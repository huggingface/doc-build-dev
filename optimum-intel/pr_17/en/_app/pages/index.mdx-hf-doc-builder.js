import{S as B,i as G,s as H,e as o,k as q,w as J,t as $,M as U,c as l,d as t,m as x,a as d,x as j,h as E,b as c,G as a,g as f,y as D,L as F,q as K,o as Q,B as V,v as W}from"../chunks/vendor-hf-doc-builder.js";import{I as X}from"../chunks/IconCopyLink-hf-doc-builder.js";function Y(N){let n,v,r,s,g,p,k,y,P,w,h,z,_,u,m,O,S,b;return p=new X({}),{c(){n=o("meta"),v=q(),r=o("h1"),s=o("a"),g=o("span"),J(p.$$.fragment),k=q(),y=o("span"),P=$("Optimum Intel"),w=q(),h=o("p"),z=$("\u{1F917} Optimum Intel is the interface between the \u{1F917} Transformers library and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures."),_=q(),u=o("p"),m=o("a"),O=$("Intel Neural Compressor"),S=$(" is an open-source library enabling the usage of the most popular compression techniques such as quantization, pruning and knowledge distillation. It supports automatic accuracy-driven tuning strategies in order for users to easily generate quantized model. The users can easily apply static, dynamic and aware-training quantization approaches while giving an expected accuracy criteria. It also supports different weight pruning techniques enabling the creation of pruned model giving a predefined sparsity target."),this.h()},l(e){const i=U('[data-svelte="svelte-1phssyn"]',document.head);n=l(i,"META",{name:!0,content:!0}),i.forEach(t),v=x(e),r=l(e,"H1",{class:!0});var I=d(r);s=l(I,"A",{id:!0,class:!0,href:!0});var T=d(s);g=l(T,"SPAN",{});var C=d(g);j(p.$$.fragment,C),C.forEach(t),T.forEach(t),k=x(I),y=l(I,"SPAN",{});var L=d(y);P=E(L,"Optimum Intel"),L.forEach(t),I.forEach(t),w=x(e),h=l(e,"P",{});var M=d(h);z=E(M,"\u{1F917} Optimum Intel is the interface between the \u{1F917} Transformers library and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures."),M.forEach(t),_=x(e),u=l(e,"P",{});var A=d(u);m=l(A,"A",{href:!0,rel:!0});var R=d(m);O=E(R,"Intel Neural Compressor"),R.forEach(t),S=E(A," is an open-source library enabling the usage of the most popular compression techniques such as quantization, pruning and knowledge distillation. It supports automatic accuracy-driven tuning strategies in order for users to easily generate quantized model. The users can easily apply static, dynamic and aware-training quantization approaches while giving an expected accuracy criteria. It also supports different weight pruning techniques enabling the creation of pruned model giving a predefined sparsity target."),A.forEach(t),this.h()},h(){c(n,"name","hf:doc:metadata"),c(n,"content",JSON.stringify(Z)),c(s,"id","optimum-intel"),c(s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s,"href","#optimum-intel"),c(r,"class","relative group"),c(m,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),c(m,"rel","nofollow")},m(e,i){a(document.head,n),f(e,v,i),f(e,r,i),a(r,s),a(s,g),D(p,g,null),a(r,k),a(r,y),a(y,P),f(e,w,i),f(e,h,i),a(h,z),f(e,_,i),f(e,u,i),a(u,m),a(m,O),a(u,S),b=!0},p:F,i(e){b||(K(p.$$.fragment,e),b=!0)},o(e){Q(p.$$.fragment,e),b=!1},d(e){t(n),e&&t(v),e&&t(r),V(p),e&&t(w),e&&t(h),e&&t(_),e&&t(u)}}}const Z={local:"optimum-intel",title:"Optimum Intel"};function ee(N){return W(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ne extends B{constructor(n){super();G(this,n,ee,Y,H,{})}}export{ne as default,Z as metadata};
