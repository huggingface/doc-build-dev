import{S as jo,i as Jo,s as Ko,e as a,k as f,w as ft,t as n,M as Vo,c as o,d as r,m as h,a as l,x as ht,h as s,b as i,F as t,g as d,y as pt,L as Zo,q as dt,o as mt,B as ct,v as tl}from"../chunks/vendor-4918fc3c.js";import{I as Ze}from"../chunks/IconCopyLink-21d338b1.js";import{C as Ca}from"../chunks/CodeBlock-99419108.js";function el(Sa){let _,Ee,E,D,ee,G,tr,re,er,ye,ut,rr,be,vt,ar,$e,y,N,ae,L,or,oe,lr,Te,gt,ir,Pe,wt,nr,Ae,u,k,M,sr,fr,Q,hr,pr,dr,v,q,mr,cr,B,ur,vr,X,gr,wr,_r,le,Er,Ie,b,x,ie,F,yr,ne,br,Oe,_t,$r,De,g,Tr,W,Pr,Ar,Y,Ir,Or,Ne,H,se,$,Et,Dr,Nr,yt,kr,xr,bt,Hr,zr,m,T,$t,Cr,Sr,Tt,Rr,Ur,Pt,Gr,Lr,P,At,Mr,Qr,It,qr,Br,Ot,Xr,Fr,A,Dt,Wr,Yr,Nt,jr,Jr,kt,Kr,Vr,I,xt,Zr,ta,Ht,ea,ra,zt,aa,ke,O,z,fe,j,oa,he,la,xe,C,ia,pe,na,sa,He,J,ze,Ct,fa,Ce,S,de,K,St,ha,pa,Rt,da,ma,c,V,Ut,Z,ca,ua,Gt,me,va,ga,tt,Lt,et,wa,_a,Mt,ce,Ea,ya,rt,Qt,at,ba,$a,qt,ue,Ta,Pa,ot,Bt,lt,Aa,Ia,Xt,ve,Oa,Se,Ft,Da,Re,it,Ue,w,Na,ge,ka,xa,we,Ha,za,Ge,nt,Le;return G=new Ze({}),L=new Ze({}),F=new Ze({}),j=new Ze({}),J=new Ca({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),it=new Ca({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),nt=new Ca({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){_=a("meta"),Ee=f(),E=a("h1"),D=a("a"),ee=a("span"),ft(G.$$.fragment),tr=f(),re=a("span"),er=n("\u{1F917} Optimum"),ye=f(),ut=a("p"),rr=n("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),be=f(),vt=a("p"),ar=n(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),$e=f(),y=a("h2"),N=a("a"),ae=a("span"),ft(L.$$.fragment),or=f(),oe=a("span"),lr=n("Integration with Hardware Partners"),Te=f(),gt=a("p"),ir=n("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Pe=f(),wt=a("p"),nr=n("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),Ae=f(),u=a("ul"),k=a("li"),M=a("a"),sr=n("Graphcore IPUs"),fr=n(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),Q=a("a"),hr=n("More information here"),pr=n("."),dr=f(),v=a("li"),q=a("a"),mr=n("Habana Gaudi Processor (HPU)"),cr=n(" - "),B=a("a"),ur=n("HPUs"),vr=n(" are designed to maximize training throughput and efficiency. "),X=a("a"),gr=n("More information here"),wr=n("."),_r=f(),le=a("li"),Er=n("More to come soon! :star:"),Ie=f(),b=a("h2"),x=a("a"),ie=a("span"),ft(F.$$.fragment),yr=f(),ne=a("span"),br=n("Optimizing models towards inference"),Oe=f(),_t=a("p"),$r=n(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),De=f(),g=a("p"),Tr=n("We currently support "),W=a("a"),Pr=n("ONNX runtime"),Ar=n(" along with "),Y=a("a"),Ir=n("Intel Neural Compressor (INC)"),Or=n("."),Ne=f(),H=a("table"),se=a("thead"),$=a("tr"),Et=a("th"),Dr=n("Features"),Nr=f(),yt=a("th"),kr=n("ONNX Runtime"),xr=f(),bt=a("th"),Hr=n("Intel Neural Compressor"),zr=f(),m=a("tbody"),T=a("tr"),$t=a("td"),Cr=n("Post-training Dynamic Quantization"),Sr=f(),Tt=a("td"),Rr=n("\u2705"),Ur=f(),Pt=a("td"),Gr=n("\u2705"),Lr=f(),P=a("tr"),At=a("td"),Mr=n("Post-training Static Quantization"),Qr=f(),It=a("td"),qr=n("\u2705"),Br=f(),Ot=a("td"),Xr=n("\u2705"),Fr=f(),A=a("tr"),Dt=a("td"),Wr=n("Quantization Aware Training (QAT)"),Yr=f(),Nt=a("td"),jr=n("Stay tuned! \u2B50"),Jr=f(),kt=a("td"),Kr=n("\u2705"),Vr=f(),I=a("tr"),xt=a("td"),Zr=n("Pruning"),ta=f(),Ht=a("td"),ea=n("N/A"),ra=f(),zt=a("td"),aa=n("\u2705"),ke=f(),O=a("h2"),z=a("a"),fe=a("span"),ft(j.$$.fragment),oa=f(),he=a("span"),la=n("Installation"),xe=f(),C=a("p"),ia=n("\u{1F917} Optimum can be installed using "),pe=a("code"),na=n("pip"),sa=n(" as follows:"),He=f(),ft(J.$$.fragment),ze=f(),Ct=a("p"),fa=n("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Ce=f(),S=a("table"),de=a("thead"),K=a("tr"),St=a("th"),ha=n("Accelerator"),pa=f(),Rt=a("th"),da=n("Installation"),ma=f(),c=a("tbody"),V=a("tr"),Ut=a("td"),Z=a("a"),ca=n("ONNX runtime"),ua=f(),Gt=a("td"),me=a("code"),va=n("python -m pip install optimum[onnxruntime]"),ga=f(),tt=a("tr"),Lt=a("td"),et=a("a"),wa=n("Intel Neural Compressor (INC)"),_a=f(),Mt=a("td"),ce=a("code"),Ea=n("python -m pip install optimum[intel]"),ya=f(),rt=a("tr"),Qt=a("td"),at=a("a"),ba=n("Graphcore IPU"),$a=f(),qt=a("td"),ue=a("code"),Ta=n("python -m pip install optimum[graphcore]"),Pa=f(),ot=a("tr"),Bt=a("td"),lt=a("a"),Aa=n("Habana Gaudi Processor (HPU)"),Ia=f(),Xt=a("td"),ve=a("code"),Oa=n("python -m pip install optimum[habana]"),Se=f(),Ft=a("p"),Da=n("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Re=f(),ft(it.$$.fragment),Ue=f(),w=a("p"),Na=n("For the acclerator-specific features, you can install them by appending "),ge=a("code"),ka=n("#egg=optimum[accelerator_type]"),xa=n(" to the "),we=a("code"),Ha=n("pip"),za=n(" command, e.g."),Ge=f(),ft(nt.$$.fragment),this.h()},l(e){const p=Vo('[data-svelte="svelte-1phssyn"]',document.head);_=o(p,"META",{name:!0,content:!0}),p.forEach(r),Ee=h(e),E=o(e,"H1",{class:!0});var Me=l(E);D=o(Me,"A",{id:!0,class:!0,href:!0});var Ra=l(D);ee=o(Ra,"SPAN",{});var Ua=l(ee);ht(G.$$.fragment,Ua),Ua.forEach(r),Ra.forEach(r),tr=h(Me),re=o(Me,"SPAN",{});var Ga=l(re);er=s(Ga,"\u{1F917} Optimum"),Ga.forEach(r),Me.forEach(r),ye=h(e),ut=o(e,"P",{});var La=l(ut);rr=s(La,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),La.forEach(r),be=h(e),vt=o(e,"P",{});var Ma=l(vt);ar=s(Ma,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Ma.forEach(r),$e=h(e),y=o(e,"H2",{class:!0});var Qe=l(y);N=o(Qe,"A",{id:!0,class:!0,href:!0});var Qa=l(N);ae=o(Qa,"SPAN",{});var qa=l(ae);ht(L.$$.fragment,qa),qa.forEach(r),Qa.forEach(r),or=h(Qe),oe=o(Qe,"SPAN",{});var Ba=l(oe);lr=s(Ba,"Integration with Hardware Partners"),Ba.forEach(r),Qe.forEach(r),Te=h(e),gt=o(e,"P",{});var Xa=l(gt);ir=s(Xa,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Xa.forEach(r),Pe=h(e),wt=o(e,"P",{});var Fa=l(wt);nr=s(Fa,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),Fa.forEach(r),Ae=h(e),u=o(e,"UL",{});var Wt=l(u);k=o(Wt,"LI",{});var _e=l(k);M=o(_e,"A",{href:!0,rel:!0});var Wa=l(M);sr=s(Wa,"Graphcore IPUs"),Wa.forEach(r),fr=s(_e," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),Q=o(_e,"A",{href:!0,rel:!0});var Ya=l(Q);hr=s(Ya,"More information here"),Ya.forEach(r),pr=s(_e,"."),_e.forEach(r),dr=h(Wt),v=o(Wt,"LI",{});var st=l(v);q=o(st,"A",{href:!0,rel:!0});var ja=l(q);mr=s(ja,"Habana Gaudi Processor (HPU)"),ja.forEach(r),cr=s(st," - "),B=o(st,"A",{href:!0,rel:!0});var Ja=l(B);ur=s(Ja,"HPUs"),Ja.forEach(r),vr=s(st," are designed to maximize training throughput and efficiency. "),X=o(st,"A",{href:!0,rel:!0});var Ka=l(X);gr=s(Ka,"More information here"),Ka.forEach(r),wr=s(st,"."),st.forEach(r),_r=h(Wt),le=o(Wt,"LI",{});var Va=l(le);Er=s(Va,"More to come soon! :star:"),Va.forEach(r),Wt.forEach(r),Ie=h(e),b=o(e,"H2",{class:!0});var qe=l(b);x=o(qe,"A",{id:!0,class:!0,href:!0});var Za=l(x);ie=o(Za,"SPAN",{});var to=l(ie);ht(F.$$.fragment,to),to.forEach(r),Za.forEach(r),yr=h(qe),ne=o(qe,"SPAN",{});var eo=l(ne);br=s(eo,"Optimizing models towards inference"),eo.forEach(r),qe.forEach(r),Oe=h(e),_t=o(e,"P",{});var ro=l(_t);$r=s(ro,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),ro.forEach(r),De=h(e),g=o(e,"P",{});var Yt=l(g);Tr=s(Yt,"We currently support "),W=o(Yt,"A",{href:!0,rel:!0});var ao=l(W);Pr=s(ao,"ONNX runtime"),ao.forEach(r),Ar=s(Yt," along with "),Y=o(Yt,"A",{href:!0,rel:!0});var oo=l(Y);Ir=s(oo,"Intel Neural Compressor (INC)"),oo.forEach(r),Or=s(Yt,"."),Yt.forEach(r),Ne=h(e),H=o(e,"TABLE",{});var Be=l(H);se=o(Be,"THEAD",{});var lo=l(se);$=o(lo,"TR",{});var jt=l($);Et=o(jt,"TH",{align:!0});var io=l(Et);Dr=s(io,"Features"),io.forEach(r),Nr=h(jt),yt=o(jt,"TH",{align:!0});var no=l(yt);kr=s(no,"ONNX Runtime"),no.forEach(r),xr=h(jt),bt=o(jt,"TH",{align:!0});var so=l(bt);Hr=s(so,"Intel Neural Compressor"),so.forEach(r),jt.forEach(r),lo.forEach(r),zr=h(Be),m=o(Be,"TBODY",{});var R=l(m);T=o(R,"TR",{});var Jt=l(T);$t=o(Jt,"TD",{align:!0});var fo=l($t);Cr=s(fo,"Post-training Dynamic Quantization"),fo.forEach(r),Sr=h(Jt),Tt=o(Jt,"TD",{align:!0});var ho=l(Tt);Rr=s(ho,"\u2705"),ho.forEach(r),Ur=h(Jt),Pt=o(Jt,"TD",{align:!0});var po=l(Pt);Gr=s(po,"\u2705"),po.forEach(r),Jt.forEach(r),Lr=h(R),P=o(R,"TR",{});var Kt=l(P);At=o(Kt,"TD",{align:!0});var mo=l(At);Mr=s(mo,"Post-training Static Quantization"),mo.forEach(r),Qr=h(Kt),It=o(Kt,"TD",{align:!0});var co=l(It);qr=s(co,"\u2705"),co.forEach(r),Br=h(Kt),Ot=o(Kt,"TD",{align:!0});var uo=l(Ot);Xr=s(uo,"\u2705"),uo.forEach(r),Kt.forEach(r),Fr=h(R),A=o(R,"TR",{});var Vt=l(A);Dt=o(Vt,"TD",{align:!0});var vo=l(Dt);Wr=s(vo,"Quantization Aware Training (QAT)"),vo.forEach(r),Yr=h(Vt),Nt=o(Vt,"TD",{align:!0});var go=l(Nt);jr=s(go,"Stay tuned! \u2B50"),go.forEach(r),Jr=h(Vt),kt=o(Vt,"TD",{align:!0});var wo=l(kt);Kr=s(wo,"\u2705"),wo.forEach(r),Vt.forEach(r),Vr=h(R),I=o(R,"TR",{});var Zt=l(I);xt=o(Zt,"TD",{align:!0});var _o=l(xt);Zr=s(_o,"Pruning"),_o.forEach(r),ta=h(Zt),Ht=o(Zt,"TD",{align:!0});var Eo=l(Ht);ea=s(Eo,"N/A"),Eo.forEach(r),ra=h(Zt),zt=o(Zt,"TD",{align:!0});var yo=l(zt);aa=s(yo,"\u2705"),yo.forEach(r),Zt.forEach(r),R.forEach(r),Be.forEach(r),ke=h(e),O=o(e,"H2",{class:!0});var Xe=l(O);z=o(Xe,"A",{id:!0,class:!0,href:!0});var bo=l(z);fe=o(bo,"SPAN",{});var $o=l(fe);ht(j.$$.fragment,$o),$o.forEach(r),bo.forEach(r),oa=h(Xe),he=o(Xe,"SPAN",{});var To=l(he);la=s(To,"Installation"),To.forEach(r),Xe.forEach(r),xe=h(e),C=o(e,"P",{});var Fe=l(C);ia=s(Fe,"\u{1F917} Optimum can be installed using "),pe=o(Fe,"CODE",{});var Po=l(pe);na=s(Po,"pip"),Po.forEach(r),sa=s(Fe," as follows:"),Fe.forEach(r),He=h(e),ht(J.$$.fragment,e),ze=h(e),Ct=o(e,"P",{});var Ao=l(Ct);fa=s(Ao,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Ao.forEach(r),Ce=h(e),S=o(e,"TABLE",{});var We=l(S);de=o(We,"THEAD",{});var Io=l(de);K=o(Io,"TR",{});var Ye=l(K);St=o(Ye,"TH",{align:!0});var Oo=l(St);ha=s(Oo,"Accelerator"),Oo.forEach(r),pa=h(Ye),Rt=o(Ye,"TH",{align:!0});var Do=l(Rt);da=s(Do,"Installation"),Do.forEach(r),Ye.forEach(r),Io.forEach(r),ma=h(We),c=o(We,"TBODY",{});var U=l(c);V=o(U,"TR",{});var je=l(V);Ut=o(je,"TD",{align:!0});var No=l(Ut);Z=o(No,"A",{href:!0,rel:!0});var ko=l(Z);ca=s(ko,"ONNX runtime"),ko.forEach(r),No.forEach(r),ua=h(je),Gt=o(je,"TD",{align:!0});var xo=l(Gt);me=o(xo,"CODE",{});var Ho=l(me);va=s(Ho,"python -m pip install optimum[onnxruntime]"),Ho.forEach(r),xo.forEach(r),je.forEach(r),ga=h(U),tt=o(U,"TR",{});var Je=l(tt);Lt=o(Je,"TD",{align:!0});var zo=l(Lt);et=o(zo,"A",{href:!0,rel:!0});var Co=l(et);wa=s(Co,"Intel Neural Compressor (INC)"),Co.forEach(r),zo.forEach(r),_a=h(Je),Mt=o(Je,"TD",{align:!0});var So=l(Mt);ce=o(So,"CODE",{});var Ro=l(ce);Ea=s(Ro,"python -m pip install optimum[intel]"),Ro.forEach(r),So.forEach(r),Je.forEach(r),ya=h(U),rt=o(U,"TR",{});var Ke=l(rt);Qt=o(Ke,"TD",{align:!0});var Uo=l(Qt);at=o(Uo,"A",{href:!0,rel:!0});var Go=l(at);ba=s(Go,"Graphcore IPU"),Go.forEach(r),Uo.forEach(r),$a=h(Ke),qt=o(Ke,"TD",{align:!0});var Lo=l(qt);ue=o(Lo,"CODE",{});var Mo=l(ue);Ta=s(Mo,"python -m pip install optimum[graphcore]"),Mo.forEach(r),Lo.forEach(r),Ke.forEach(r),Pa=h(U),ot=o(U,"TR",{});var Ve=l(ot);Bt=o(Ve,"TD",{align:!0});var Qo=l(Bt);lt=o(Qo,"A",{href:!0,rel:!0});var qo=l(lt);Aa=s(qo,"Habana Gaudi Processor (HPU)"),qo.forEach(r),Qo.forEach(r),Ia=h(Ve),Xt=o(Ve,"TD",{align:!0});var Bo=l(Xt);ve=o(Bo,"CODE",{});var Xo=l(ve);Oa=s(Xo,"python -m pip install optimum[habana]"),Xo.forEach(r),Bo.forEach(r),Ve.forEach(r),U.forEach(r),We.forEach(r),Se=h(e),Ft=o(e,"P",{});var Fo=l(Ft);Da=s(Fo,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Fo.forEach(r),Re=h(e),ht(it.$$.fragment,e),Ue=h(e),w=o(e,"P",{});var te=l(w);Na=s(te,"For the acclerator-specific features, you can install them by appending "),ge=o(te,"CODE",{});var Wo=l(ge);ka=s(Wo,"#egg=optimum[accelerator_type]"),Wo.forEach(r),xa=s(te," to the "),we=o(te,"CODE",{});var Yo=l(we);Ha=s(Yo,"pip"),Yo.forEach(r),za=s(te," command, e.g."),te.forEach(r),Ge=h(e),ht(nt.$$.fragment,e),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(rl)),i(D,"id","optimum"),i(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(D,"href","#optimum"),i(E,"class","relative group"),i(N,"id","integration-with-hardware-partners"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#integration-with-hardware-partners"),i(y,"class","relative group"),i(M,"href","https://github.com/huggingface/optimum-graphcore"),i(M,"rel","nofollow"),i(Q,"href","https://www.graphcore.ai/products/ipu"),i(Q,"rel","nofollow"),i(q,"href","https://github.com/huggingface/optimum-habana"),i(q,"rel","nofollow"),i(B,"href","https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html"),i(B,"rel","nofollow"),i(X,"href","https://habana.ai/training/"),i(X,"rel","nofollow"),i(x,"id","optimizing-models-towards-inference"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#optimizing-models-towards-inference"),i(b,"class","relative group"),i(W,"href","https://github.com/microsoft/onnxruntime"),i(W,"rel","nofollow"),i(Y,"href","https://github.com/intel/neural-compressor"),i(Y,"rel","nofollow"),i(Et,"align","center"),i(yt,"align","center"),i(bt,"align","center"),i($t,"align","center"),i(Tt,"align","center"),i(Pt,"align","center"),i(At,"align","center"),i(It,"align","center"),i(Ot,"align","center"),i(Dt,"align","center"),i(Nt,"align","center"),i(kt,"align","center"),i(xt,"align","center"),i(Ht,"align","center"),i(zt,"align","center"),i(z,"id","installation"),i(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(z,"href","#installation"),i(O,"class","relative group"),i(St,"align","left"),i(Rt,"align","left"),i(Z,"href","https://github.com/microsoft/onnxruntime"),i(Z,"rel","nofollow"),i(Ut,"align","left"),i(Gt,"align","left"),i(et,"href","https://github.com/intel/neural-compressor"),i(et,"rel","nofollow"),i(Lt,"align","left"),i(Mt,"align","left"),i(at,"href","https://www.graphcore.ai/products/ipu"),i(at,"rel","nofollow"),i(Qt,"align","left"),i(qt,"align","left"),i(lt,"href","https://habana.ai/training/"),i(lt,"rel","nofollow"),i(Bt,"align","left"),i(Xt,"align","left")},m(e,p){t(document.head,_),d(e,Ee,p),d(e,E,p),t(E,D),t(D,ee),pt(G,ee,null),t(E,tr),t(E,re),t(re,er),d(e,ye,p),d(e,ut,p),t(ut,rr),d(e,be,p),d(e,vt,p),t(vt,ar),d(e,$e,p),d(e,y,p),t(y,N),t(N,ae),pt(L,ae,null),t(y,or),t(y,oe),t(oe,lr),d(e,Te,p),d(e,gt,p),t(gt,ir),d(e,Pe,p),d(e,wt,p),t(wt,nr),d(e,Ae,p),d(e,u,p),t(u,k),t(k,M),t(M,sr),t(k,fr),t(k,Q),t(Q,hr),t(k,pr),t(u,dr),t(u,v),t(v,q),t(q,mr),t(v,cr),t(v,B),t(B,ur),t(v,vr),t(v,X),t(X,gr),t(v,wr),t(u,_r),t(u,le),t(le,Er),d(e,Ie,p),d(e,b,p),t(b,x),t(x,ie),pt(F,ie,null),t(b,yr),t(b,ne),t(ne,br),d(e,Oe,p),d(e,_t,p),t(_t,$r),d(e,De,p),d(e,g,p),t(g,Tr),t(g,W),t(W,Pr),t(g,Ar),t(g,Y),t(Y,Ir),t(g,Or),d(e,Ne,p),d(e,H,p),t(H,se),t(se,$),t($,Et),t(Et,Dr),t($,Nr),t($,yt),t(yt,kr),t($,xr),t($,bt),t(bt,Hr),t(H,zr),t(H,m),t(m,T),t(T,$t),t($t,Cr),t(T,Sr),t(T,Tt),t(Tt,Rr),t(T,Ur),t(T,Pt),t(Pt,Gr),t(m,Lr),t(m,P),t(P,At),t(At,Mr),t(P,Qr),t(P,It),t(It,qr),t(P,Br),t(P,Ot),t(Ot,Xr),t(m,Fr),t(m,A),t(A,Dt),t(Dt,Wr),t(A,Yr),t(A,Nt),t(Nt,jr),t(A,Jr),t(A,kt),t(kt,Kr),t(m,Vr),t(m,I),t(I,xt),t(xt,Zr),t(I,ta),t(I,Ht),t(Ht,ea),t(I,ra),t(I,zt),t(zt,aa),d(e,ke,p),d(e,O,p),t(O,z),t(z,fe),pt(j,fe,null),t(O,oa),t(O,he),t(he,la),d(e,xe,p),d(e,C,p),t(C,ia),t(C,pe),t(pe,na),t(C,sa),d(e,He,p),pt(J,e,p),d(e,ze,p),d(e,Ct,p),t(Ct,fa),d(e,Ce,p),d(e,S,p),t(S,de),t(de,K),t(K,St),t(St,ha),t(K,pa),t(K,Rt),t(Rt,da),t(S,ma),t(S,c),t(c,V),t(V,Ut),t(Ut,Z),t(Z,ca),t(V,ua),t(V,Gt),t(Gt,me),t(me,va),t(c,ga),t(c,tt),t(tt,Lt),t(Lt,et),t(et,wa),t(tt,_a),t(tt,Mt),t(Mt,ce),t(ce,Ea),t(c,ya),t(c,rt),t(rt,Qt),t(Qt,at),t(at,ba),t(rt,$a),t(rt,qt),t(qt,ue),t(ue,Ta),t(c,Pa),t(c,ot),t(ot,Bt),t(Bt,lt),t(lt,Aa),t(ot,Ia),t(ot,Xt),t(Xt,ve),t(ve,Oa),d(e,Se,p),d(e,Ft,p),t(Ft,Da),d(e,Re,p),pt(it,e,p),d(e,Ue,p),d(e,w,p),t(w,Na),t(w,ge),t(ge,ka),t(w,xa),t(w,we),t(we,Ha),t(w,za),d(e,Ge,p),pt(nt,e,p),Le=!0},p:Zo,i(e){Le||(dt(G.$$.fragment,e),dt(L.$$.fragment,e),dt(F.$$.fragment,e),dt(j.$$.fragment,e),dt(J.$$.fragment,e),dt(it.$$.fragment,e),dt(nt.$$.fragment,e),Le=!0)},o(e){mt(G.$$.fragment,e),mt(L.$$.fragment,e),mt(F.$$.fragment,e),mt(j.$$.fragment,e),mt(J.$$.fragment,e),mt(it.$$.fragment,e),mt(nt.$$.fragment,e),Le=!1},d(e){r(_),e&&r(Ee),e&&r(E),ct(G),e&&r(ye),e&&r(ut),e&&r(be),e&&r(vt),e&&r($e),e&&r(y),ct(L),e&&r(Te),e&&r(gt),e&&r(Pe),e&&r(wt),e&&r(Ae),e&&r(u),e&&r(Ie),e&&r(b),ct(F),e&&r(Oe),e&&r(_t),e&&r(De),e&&r(g),e&&r(Ne),e&&r(H),e&&r(ke),e&&r(O),ct(j),e&&r(xe),e&&r(C),e&&r(He),ct(J,e),e&&r(ze),e&&r(Ct),e&&r(Ce),e&&r(S),e&&r(Se),e&&r(Ft),e&&r(Re),ct(it,e),e&&r(Ue),e&&r(w),e&&r(Ge),ct(nt,e)}}}const rl={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function al(Sa){return tl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nl extends jo{constructor(_){super();Jo(this,_,al,el,Ko,{})}}export{nl as default,rl as metadata};
