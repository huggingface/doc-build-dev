import{S as xa,i as va,s as ba,e as a,k as u,w as h,t as i,M as ya,c as o,d as n,m as c,a as r,x as f,h as s,b as m,G as t,g as p,y as _,L as Ta,q as g,o as z,B as x,v as qa}from"../../chunks/vendor-hf-doc-builder.js";import{D as he}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Me}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as We}from"../../chunks/IconCopyLink-hf-doc-builder.js";function $a(Nn){let q,Ve,$,E,ye,S,Qt,Te,wt,Ge,b,Et,qe,kt,Pt,I,Dt,Ct,Be,O,k,$e,U,Nt,X,Ft,Oe,jt,Lt,Je,y,At,Re,St,It,Qe,Ut,Xt,Ke,fe,we,Ht,Ye,H,Ze,M,_e,Mt,W,Wt,et,V,tt,G,Ee,Vt,nt,B,at,J,K,Gt,ke,Bt,Jt,ot,Y,rt,R,P,Pe,Z,Kt,De,Yt,it,T,Zt,Ce,en,tn,ee,nn,an,st,te,lt,Q,D,Ne,ne,on,Fe,rn,mt,ge,sn,ut,w,C,je,ae,ln,Le,mn,ct,d,oe,un,Ae,cn,pn,ze,re,dn,N,ie,hn,se,fn,Se,_n,gn,zn,F,le,xn,Ie,vn,bn,j,me,yn,ue,Tn,Ue,qn,$n,On,L,ce,Rn,pe,Qn,Xe,wn,En,kn,A,de,Pn,He,Dn,pt;return S=new We({}),U=new We({}),H=new Me({props:{code:`from optimum.onnxruntime import ORTQuantizer

# create a quantizer from a vanilla PyTorch checkpoint by converting the model
quantizer = ORTQuantizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", from_transformers=True, task="text-classification")

quantizer.fit(...)`,highlighted:`<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># create a quantizer from a vanilla PyTorch checkpoint by converting the model</span>
quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, from_transformers=<span class="hljs-literal">True</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)

quantizer.fit(...)`}}),V=new Me({props:{code:`from optimum.onnxruntime import ORTQuantizer

# create a quantizer from a converted onnx model from the hugging face hub
quantizer = ORTQuantizer.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english", task="text-classification")

quantizer.fit(...)`,highlighted:`<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># create a quantizer from a converted onnx model from the hugging face hub</span>
quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)

quantizer.fit(...)`}}),B=new Me({props:{code:`from optimum.onnxruntime import ORTQuantizer

# This assumes a model.onnx in the path/to/model
quantizer = ORTQuantizer.from_pretrained("path/to/model")

quantizer.fit(...)`,highlighted:`<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx in the path/to/model</span>
quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)

quantizer.fit(...)`}}),Y=new Me({props:{code:`from optimum.onnxruntime import ORTQuantizer

# This assumes a that ort_model is a \`ORTModelForXXX\` class, e.g. ORTModelForSequenceClassification
quantizer = ORTQuantizer.from_pretrained(ort_model)

quantizer.fit(...)`,highlighted:`<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a that ort_model is a \`ORTModelForXXX\` class, e.g. ORTModelForSequenceClassification</span>
quantizer = ORTQuantizer.from_pretrained(ort_model)

quantizer.fit(...)`}}),Z=new We({}),te=new Me({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

# load pytorch model and convert to onnx
onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=True)

# create quantizer
quantizer = ORTQuantizer.from_pretrained(onnx_model)

# define quantize configuartion
dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

# apply the quantization configuration to the model
model_quantized_path = quantizer.export(
    output_path="model-quantized.onnx",
    quantization_config=dqconfig,
)`,highlighted:`<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># load pytorch model and convert to onnx</span>
onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># create quantizer</span>
quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># define quantize configuartion</span>
dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># apply the quantization configuration to the model</span>
model_quantized_path = quantizer.export(
    output_path=<span class="hljs-string">&quot;model-quantized.onnx&quot;</span>,
    quantization_config=dqconfig,
)`}}),ne=new We({}),ae=new We({}),oe=new he({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": PathLike"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L82"}}),re=new he({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L279",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),ie=new he({props:{name:"export",anchor:"optimum.onnxruntime.ORTQuantizer.export",parameters:[{name:"output_path",val:": typing.Union[str, os.PathLike]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.export.output_path",description:`<strong>output_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L292",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),le=new he({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L168",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),me=new he({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"from_transformers",val:": typing.Optional[bool] = False"},{name:"task",val:": typing.Optional[str] = None"},{name:"file_name",val:": typing.Optional[str] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
Is needed to load models from a private repository`,name:"use_auth_token"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.from_transformers",description:`<strong>from_transformers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Defines wether the provided <code>model_name_or_path</code> contains a vanilla Transformers checkpoint.
ORTQuantizer will then convert the model first`,name:"from_transformers"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.task",description:`<strong>task</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Transformers pipeline task for the model. Will be used to convert the model if needed.`,name:"task"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(str,",description:`<strong>file_name(<code>str</code>,</strong> <em>optional</em>) &#x2014;
Overwrites the default model file name from <code>&quot;model.onnx&quot;</code> to <code>file_name</code>. This allows you to load different model files from the same
repository or directory.`,name:"file_name(str,"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, Path]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L87",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),ce=new he({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>int</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L380",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),de=new he({props:{name:"partial_fit",anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L224",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){q=a("meta"),Ve=u(),$=a("h1"),E=a("a"),ye=a("span"),h(S.$$.fragment),Qt=u(),Te=a("span"),wt=i("Quantization"),Ge=u(),b=a("p"),Et=i("\u{1F917} Optimum provides an "),qe=a("code"),kt=i("optimum.onnxruntime"),Pt=i(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=a("a"),Dt=i("ONNX Runtime"),Ct=i(" quantization tool."),Be=u(),O=a("h2"),k=a("a"),$e=a("span"),h(U.$$.fragment),Nt=u(),X=a("span"),Ft=i("Create "),Oe=a("code"),jt=i("ORTQuantizer"),Lt=i(" class"),Je=u(),y=a("p"),At=i("The "),Re=a("code"),St=i("ORTQuantizer"),It=i(" class is used to quantize your onnx model. The class can be initialized using the "),Qe=a("code"),Ut=i("from_pretrained"),Xt=i(" method, which supports different checkpoint formats."),Ke=u(),fe=a("ol"),we=a("li"),Ht=i("Using a vanilla transformers (non converted) PyTorch checkpoint"),Ye=u(),h(H.$$.fragment),Ze=u(),M=a("ol"),_e=a("li"),Mt=i("Using a already converted onnx model from the "),W=a("a"),Wt=i("Hugging Face Hub"),et=u(),h(V.$$.fragment),tt=u(),G=a("ol"),Ee=a("li"),Vt=i("Using a already converted local onnx model."),nt=u(),h(B.$$.fragment),at=u(),J=a("ol"),K=a("li"),Gt=i("Using a already initialized "),ke=a("code"),Bt=i("ORTModelForXXX"),Jt=i(" class."),ot=u(),h(Y.$$.fragment),rt=u(),R=a("h2"),P=a("a"),Pe=a("span"),h(Z.$$.fragment),Kt=u(),De=a("span"),Yt=i("Dynmaic Quantization example"),it=u(),T=a("p"),Zt=i("The "),Ce=a("code"),en=i("ORTQuantizer"),tn=i(" class can be used to dynmaically quantize your onnx model. Below you will find an easy end-to-end example on how to dynamically quantize "),ee=a("a"),nn=i("distilbert-base-uncased-finetuned-sst-2-english"),an=i("."),st=u(),h(te.$$.fragment),lt=u(),Q=a("h2"),D=a("a"),Ne=a("span"),h(ne.$$.fragment),on=u(),Fe=a("span"),rn=i("Static Quantization example"),mt=u(),ge=a("p"),sn=i("TODO:"),ut=u(),w=a("h2"),C=a("a"),je=a("span"),h(ae.$$.fragment),ln=u(),Le=a("span"),mn=i("ORTQuantizer"),ct=u(),d=a("div"),h(oe.$$.fragment),un=u(),Ae=a("p"),cn=i("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),pn=u(),ze=a("div"),h(re.$$.fragment),dn=u(),N=a("div"),h(ie.$$.fragment),hn=u(),se=a("p"),fn=i("Quantize a model given the optimization specifications defined in "),Se=a("code"),_n=i("quantization_config"),gn=i("."),zn=u(),F=a("div"),h(le.$$.fragment),xn=u(),Ie=a("p"),vn=i("Perform the calibration step and collect the quantization ranges."),bn=u(),j=a("div"),h(me.$$.fragment),yn=u(),ue=a("p"),Tn=i("Instantiate a "),Ue=a("code"),qn=i("ORTQuantizer"),$n=i(" from a pretrained pytorch model and preprocessor."),On=u(),L=a("div"),h(ce.$$.fragment),Rn=u(),pe=a("p"),Qn=i("Create the calibration "),Xe=a("code"),wn=i("datasets.Dataset"),En=i(" to use for the post-training static quantization calibration step"),kn=u(),A=a("div"),h(de.$$.fragment),Pn=u(),He=a("p"),Dn=i("Perform the calibration step and collect the quantization ranges."),this.h()},l(e){const l=ya('[data-svelte="svelte-1phssyn"]',document.head);q=o(l,"META",{name:!0,content:!0}),l.forEach(n),Ve=c(e),$=o(e,"H1",{class:!0});var dt=r($);E=o(dt,"A",{id:!0,class:!0,href:!0});var Fn=r(E);ye=o(Fn,"SPAN",{});var jn=r(ye);f(S.$$.fragment,jn),jn.forEach(n),Fn.forEach(n),Qt=c(dt),Te=o(dt,"SPAN",{});var Ln=r(Te);wt=s(Ln,"Quantization"),Ln.forEach(n),dt.forEach(n),Ge=c(e),b=o(e,"P",{});var xe=r(b);Et=s(xe,"\u{1F917} Optimum provides an "),qe=o(xe,"CODE",{});var An=r(qe);kt=s(An,"optimum.onnxruntime"),An.forEach(n),Pt=s(xe," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=o(xe,"A",{href:!0,rel:!0});var Sn=r(I);Dt=s(Sn,"ONNX Runtime"),Sn.forEach(n),Ct=s(xe," quantization tool."),xe.forEach(n),Be=c(e),O=o(e,"H2",{class:!0});var ht=r(O);k=o(ht,"A",{id:!0,class:!0,href:!0});var In=r(k);$e=o(In,"SPAN",{});var Un=r($e);f(U.$$.fragment,Un),Un.forEach(n),In.forEach(n),Nt=c(ht),X=o(ht,"SPAN",{});var ft=r(X);Ft=s(ft,"Create "),Oe=o(ft,"CODE",{});var Xn=r(Oe);jt=s(Xn,"ORTQuantizer"),Xn.forEach(n),Lt=s(ft," class"),ft.forEach(n),ht.forEach(n),Je=c(e),y=o(e,"P",{});var ve=r(y);At=s(ve,"The "),Re=o(ve,"CODE",{});var Hn=r(Re);St=s(Hn,"ORTQuantizer"),Hn.forEach(n),It=s(ve," class is used to quantize your onnx model. The class can be initialized using the "),Qe=o(ve,"CODE",{});var Mn=r(Qe);Ut=s(Mn,"from_pretrained"),Mn.forEach(n),Xt=s(ve," method, which supports different checkpoint formats."),ve.forEach(n),Ke=c(e),fe=o(e,"OL",{});var Wn=r(fe);we=o(Wn,"LI",{});var Vn=r(we);Ht=s(Vn,"Using a vanilla transformers (non converted) PyTorch checkpoint"),Vn.forEach(n),Wn.forEach(n),Ye=c(e),f(H.$$.fragment,e),Ze=c(e),M=o(e,"OL",{start:!0});var Gn=r(M);_e=o(Gn,"LI",{});var Cn=r(_e);Mt=s(Cn,"Using a already converted onnx model from the "),W=o(Cn,"A",{href:!0,rel:!0});var Bn=r(W);Wt=s(Bn,"Hugging Face Hub"),Bn.forEach(n),Cn.forEach(n),Gn.forEach(n),et=c(e),f(V.$$.fragment,e),tt=c(e),G=o(e,"OL",{start:!0});var Jn=r(G);Ee=o(Jn,"LI",{});var Kn=r(Ee);Vt=s(Kn,"Using a already converted local onnx model."),Kn.forEach(n),Jn.forEach(n),nt=c(e),f(B.$$.fragment,e),at=c(e),J=o(e,"OL",{start:!0});var Yn=r(J);K=o(Yn,"LI",{});var _t=r(K);Gt=s(_t,"Using a already initialized "),ke=o(_t,"CODE",{});var Zn=r(ke);Bt=s(Zn,"ORTModelForXXX"),Zn.forEach(n),Jt=s(_t," class."),_t.forEach(n),Yn.forEach(n),ot=c(e),f(Y.$$.fragment,e),rt=c(e),R=o(e,"H2",{class:!0});var gt=r(R);P=o(gt,"A",{id:!0,class:!0,href:!0});var ea=r(P);Pe=o(ea,"SPAN",{});var ta=r(Pe);f(Z.$$.fragment,ta),ta.forEach(n),ea.forEach(n),Kt=c(gt),De=o(gt,"SPAN",{});var na=r(De);Yt=s(na,"Dynmaic Quantization example"),na.forEach(n),gt.forEach(n),it=c(e),T=o(e,"P",{});var be=r(T);Zt=s(be,"The "),Ce=o(be,"CODE",{});var aa=r(Ce);en=s(aa,"ORTQuantizer"),aa.forEach(n),tn=s(be," class can be used to dynmaically quantize your onnx model. Below you will find an easy end-to-end example on how to dynamically quantize "),ee=o(be,"A",{href:!0,rel:!0});var oa=r(ee);nn=s(oa,"distilbert-base-uncased-finetuned-sst-2-english"),oa.forEach(n),an=s(be,"."),be.forEach(n),st=c(e),f(te.$$.fragment,e),lt=c(e),Q=o(e,"H2",{class:!0});var zt=r(Q);D=o(zt,"A",{id:!0,class:!0,href:!0});var ra=r(D);Ne=o(ra,"SPAN",{});var ia=r(Ne);f(ne.$$.fragment,ia),ia.forEach(n),ra.forEach(n),on=c(zt),Fe=o(zt,"SPAN",{});var sa=r(Fe);rn=s(sa,"Static Quantization example"),sa.forEach(n),zt.forEach(n),mt=c(e),ge=o(e,"P",{});var la=r(ge);sn=s(la,"TODO:"),la.forEach(n),ut=c(e),w=o(e,"H2",{class:!0});var xt=r(w);C=o(xt,"A",{id:!0,class:!0,href:!0});var ma=r(C);je=o(ma,"SPAN",{});var ua=r(je);f(ae.$$.fragment,ua),ua.forEach(n),ma.forEach(n),ln=c(xt),Le=o(xt,"SPAN",{});var ca=r(Le);mn=s(ca,"ORTQuantizer"),ca.forEach(n),xt.forEach(n),ct=c(e),d=o(e,"DIV",{class:!0});var v=r(d);f(oe.$$.fragment,v),un=c(v),Ae=o(v,"P",{});var pa=r(Ae);cn=s(pa,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),pa.forEach(n),pn=c(v),ze=o(v,"DIV",{class:!0});var da=r(ze);f(re.$$.fragment,da),da.forEach(n),dn=c(v),N=o(v,"DIV",{class:!0});var vt=r(N);f(ie.$$.fragment,vt),hn=c(vt),se=o(vt,"P",{});var bt=r(se);fn=s(bt,"Quantize a model given the optimization specifications defined in "),Se=o(bt,"CODE",{});var ha=r(Se);_n=s(ha,"quantization_config"),ha.forEach(n),gn=s(bt,"."),bt.forEach(n),vt.forEach(n),zn=c(v),F=o(v,"DIV",{class:!0});var yt=r(F);f(le.$$.fragment,yt),xn=c(yt),Ie=o(yt,"P",{});var fa=r(Ie);vn=s(fa,"Perform the calibration step and collect the quantization ranges."),fa.forEach(n),yt.forEach(n),bn=c(v),j=o(v,"DIV",{class:!0});var Tt=r(j);f(me.$$.fragment,Tt),yn=c(Tt),ue=o(Tt,"P",{});var qt=r(ue);Tn=s(qt,"Instantiate a "),Ue=o(qt,"CODE",{});var _a=r(Ue);qn=s(_a,"ORTQuantizer"),_a.forEach(n),$n=s(qt," from a pretrained pytorch model and preprocessor."),qt.forEach(n),Tt.forEach(n),On=c(v),L=o(v,"DIV",{class:!0});var $t=r(L);f(ce.$$.fragment,$t),Rn=c($t),pe=o($t,"P",{});var Ot=r(pe);Qn=s(Ot,"Create the calibration "),Xe=o(Ot,"CODE",{});var ga=r(Xe);wn=s(ga,"datasets.Dataset"),ga.forEach(n),En=s(Ot," to use for the post-training static quantization calibration step"),Ot.forEach(n),$t.forEach(n),kn=c(v),A=o(v,"DIV",{class:!0});var Rt=r(A);f(de.$$.fragment,Rt),Pn=c(Rt),He=o(Rt,"P",{});var za=r(He);Dn=s(za,"Perform the calibration step and collect the quantization ranges."),za.forEach(n),Rt.forEach(n),v.forEach(n),this.h()},h(){m(q,"name","hf:doc:metadata"),m(q,"content",JSON.stringify(Oa)),m(E,"id","quantization"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#quantization"),m($,"class","relative group"),m(I,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),m(I,"rel","nofollow"),m(k,"id","create-ortquantizer-class"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#create-ortquantizer-class"),m(O,"class","relative group"),m(W,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(W,"rel","nofollow"),m(M,"start","2"),m(G,"start","3"),m(J,"start","4"),m(P,"id","dynmaic-quantization-example"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#dynmaic-quantization-example"),m(R,"class","relative group"),m(ee,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(ee,"rel","nofollow"),m(D,"id","static-quantization-example"),m(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(D,"href","#static-quantization-example"),m(Q,"class","relative group"),m(C,"id","optimum.onnxruntime.ORTQuantizer"),m(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(C,"href","#optimum.onnxruntime.ORTQuantizer"),m(w,"class","relative group"),m(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,l){t(document.head,q),p(e,Ve,l),p(e,$,l),t($,E),t(E,ye),_(S,ye,null),t($,Qt),t($,Te),t(Te,wt),p(e,Ge,l),p(e,b,l),t(b,Et),t(b,qe),t(qe,kt),t(b,Pt),t(b,I),t(I,Dt),t(b,Ct),p(e,Be,l),p(e,O,l),t(O,k),t(k,$e),_(U,$e,null),t(O,Nt),t(O,X),t(X,Ft),t(X,Oe),t(Oe,jt),t(X,Lt),p(e,Je,l),p(e,y,l),t(y,At),t(y,Re),t(Re,St),t(y,It),t(y,Qe),t(Qe,Ut),t(y,Xt),p(e,Ke,l),p(e,fe,l),t(fe,we),t(we,Ht),p(e,Ye,l),_(H,e,l),p(e,Ze,l),p(e,M,l),t(M,_e),t(_e,Mt),t(_e,W),t(W,Wt),p(e,et,l),_(V,e,l),p(e,tt,l),p(e,G,l),t(G,Ee),t(Ee,Vt),p(e,nt,l),_(B,e,l),p(e,at,l),p(e,J,l),t(J,K),t(K,Gt),t(K,ke),t(ke,Bt),t(K,Jt),p(e,ot,l),_(Y,e,l),p(e,rt,l),p(e,R,l),t(R,P),t(P,Pe),_(Z,Pe,null),t(R,Kt),t(R,De),t(De,Yt),p(e,it,l),p(e,T,l),t(T,Zt),t(T,Ce),t(Ce,en),t(T,tn),t(T,ee),t(ee,nn),t(T,an),p(e,st,l),_(te,e,l),p(e,lt,l),p(e,Q,l),t(Q,D),t(D,Ne),_(ne,Ne,null),t(Q,on),t(Q,Fe),t(Fe,rn),p(e,mt,l),p(e,ge,l),t(ge,sn),p(e,ut,l),p(e,w,l),t(w,C),t(C,je),_(ae,je,null),t(w,ln),t(w,Le),t(Le,mn),p(e,ct,l),p(e,d,l),_(oe,d,null),t(d,un),t(d,Ae),t(Ae,cn),t(d,pn),t(d,ze),_(re,ze,null),t(d,dn),t(d,N),_(ie,N,null),t(N,hn),t(N,se),t(se,fn),t(se,Se),t(Se,_n),t(se,gn),t(d,zn),t(d,F),_(le,F,null),t(F,xn),t(F,Ie),t(Ie,vn),t(d,bn),t(d,j),_(me,j,null),t(j,yn),t(j,ue),t(ue,Tn),t(ue,Ue),t(Ue,qn),t(ue,$n),t(d,On),t(d,L),_(ce,L,null),t(L,Rn),t(L,pe),t(pe,Qn),t(pe,Xe),t(Xe,wn),t(pe,En),t(d,kn),t(d,A),_(de,A,null),t(A,Pn),t(A,He),t(He,Dn),pt=!0},p:Ta,i(e){pt||(g(S.$$.fragment,e),g(U.$$.fragment,e),g(H.$$.fragment,e),g(V.$$.fragment,e),g(B.$$.fragment,e),g(Y.$$.fragment,e),g(Z.$$.fragment,e),g(te.$$.fragment,e),g(ne.$$.fragment,e),g(ae.$$.fragment,e),g(oe.$$.fragment,e),g(re.$$.fragment,e),g(ie.$$.fragment,e),g(le.$$.fragment,e),g(me.$$.fragment,e),g(ce.$$.fragment,e),g(de.$$.fragment,e),pt=!0)},o(e){z(S.$$.fragment,e),z(U.$$.fragment,e),z(H.$$.fragment,e),z(V.$$.fragment,e),z(B.$$.fragment,e),z(Y.$$.fragment,e),z(Z.$$.fragment,e),z(te.$$.fragment,e),z(ne.$$.fragment,e),z(ae.$$.fragment,e),z(oe.$$.fragment,e),z(re.$$.fragment,e),z(ie.$$.fragment,e),z(le.$$.fragment,e),z(me.$$.fragment,e),z(ce.$$.fragment,e),z(de.$$.fragment,e),pt=!1},d(e){n(q),e&&n(Ve),e&&n($),x(S),e&&n(Ge),e&&n(b),e&&n(Be),e&&n(O),x(U),e&&n(Je),e&&n(y),e&&n(Ke),e&&n(fe),e&&n(Ye),x(H,e),e&&n(Ze),e&&n(M),e&&n(et),x(V,e),e&&n(tt),e&&n(G),e&&n(nt),x(B,e),e&&n(at),e&&n(J),e&&n(ot),x(Y,e),e&&n(rt),e&&n(R),x(Z),e&&n(it),e&&n(T),e&&n(st),x(te,e),e&&n(lt),e&&n(Q),x(ne),e&&n(mt),e&&n(ge),e&&n(ut),e&&n(w),x(ae),e&&n(ct),e&&n(d),x(oe),x(re),x(ie),x(le),x(me),x(ce),x(de)}}}const Oa={local:"quantization",sections:[{local:"create-ortquantizer-class",title:"Create `ORTQuantizer` class"},{local:"dynmaic-quantization-example",title:"Dynmaic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function Ra(Nn){return qa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pa extends xa{constructor(q){super();va(this,q,Ra,$a,ba,{})}}export{Pa as default,Oa as metadata};
