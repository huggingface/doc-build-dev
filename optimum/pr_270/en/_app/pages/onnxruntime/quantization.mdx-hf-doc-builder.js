import{S as ya,i as Oa,s as $a,e as a,k as c,w as h,t as i,M as qa,c as o,d as n,m as u,a as r,x as f,h as s,b as m,G as t,g as p,y as g,L as Ra,q as _,o as z,B as x,v as Qa}from"../../chunks/vendor-hf-doc-builder.js";import{D as ge}from"../../chunks/Docstring-hf-doc-builder.js";import{C as We}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";function wa(An){let O,Ge,$,E,Oe,X,Et,$e,kt,Be,v,Nt,qe,jt,Ct,I,Pt,Dt,Je,q,k,Re,U,Ft,_e,Lt,Qe,At,Ke,T,Xt,we,It,Ut,Ee,St,Ht,Ye,ze,S,Mt,H,Wt,Vt,Ze,M,et,W,V,Gt,G,Bt,Jt,tt,B,nt,J,ke,Kt,at,K,ot,Y,Z,Yt,Ne,Zt,en,rt,ee,it,R,N,je,te,tn,Ce,nn,st,y,an,Pe,on,rn,ne,sn,ln,lt,ae,mt,Q,j,De,oe,mn,Fe,cn,ct,xe,un,ut,w,C,Le,re,pn,Ae,dn,pt,d,ie,hn,Xe,fn,gn,be,se,_n,P,le,zn,me,xn,Ie,bn,vn,Tn,D,ce,yn,Ue,On,$n,F,ue,qn,pe,Rn,Se,Qn,wn,En,L,de,kn,he,Nn,He,jn,Cn,Pn,A,fe,Dn,Me,Fn,dt;return X=new Ve({}),U=new Ve({}),M=new We({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", from_transformers=True, task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a vanilla PyTorch checkpoint by converting the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, from_transformers=<span class="hljs-literal">True</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),B=new We({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english", task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a converted ONNX model on the Hugging Face hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),K=new We({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("path/to/model")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx exists in path/to/model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)`}}),ee=new We({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForTextClassification

ort_model = ORTModelForTextClassification.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english")

quantizer = ORTQuantizer.from_pretrained(ort_model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForTextClassification

<span class="hljs-comment"># Loading ONNX Model from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForTextClassification.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)

<span class="hljs-comment"># Create a quantizer from a ORTModelForXXX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(ort_model)`}}),te=new Ve({}),ae=new We({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=True)

quantizer = ORTQuantizer.from_pretrained(onnx_model)

dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

model_quantized_path = quantizer.export(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># Load PyTorch model and convert to ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Create quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize and export the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.export(
    output_path=<span class="hljs-string">&quot;model-quantized.onnx&quot;</span>,
    quantization_config=dqconfig,
)`}}),oe=new Ve({}),re=new Ve({}),ie=new ge({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": PathLike"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L78"}}),se=new ge({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L283",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),le=new ge({props:{name:"export",anchor:"optimum.onnxruntime.ORTQuantizer.export",parameters:[{name:"output_path",val:": typing.Union[str, os.PathLike]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.export.output_path",description:`<strong>output_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L296",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),ce=new ge({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L172",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),ue=new ge({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_id",val:": typing.Union[str, os.PathLike]"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"from_transformers",val:": typing.Optional[bool] = False"},{name:"feature",val:": typing.Optional[str] = None"},{name:"file_name",val:": typing.Optional[str] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_id",description:`<strong>model_id</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
Can be either:<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a model saved as a <code>.onnx</code> file, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"model_id"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
Is needed to load models from a private repository.`,name:"use_auth_token"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.from_transformers",description:`<strong>from_transformers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Defines whether the provided <code>model_id</code> contains a vanilla Transformers checkpoint.
ORTQuantizer will then export the model first to ONNX.`,name:"from_transformers"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.feature",description:`<strong>feature</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Transformers feature for the model. Will be used to convert the model if needed. Find a list of supported features <a href="https://huggingface.co/docs/transformers/serialization#selecting-features-for-different-model-topologies" rel="nofollow">here</a>`,name:"feature"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(str,",description:`<strong>file_name(<code>str</code>,</strong> <em>optional</em>) &#x2014;
Overwrites the default model file name from <code>&quot;model.onnx&quot;</code> to <code>file_name</code>. This allows you to load different model files from the same
repository or directory.`,name:"file_name(str,"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, Path]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L83",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),de=new ge({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>int</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L384",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),fe=new ge({props:{name:"partial_fit",anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L228",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){O=a("meta"),Ge=c(),$=a("h1"),E=a("a"),Oe=a("span"),h(X.$$.fragment),Et=c(),$e=a("span"),kt=i("Quantization"),Be=c(),v=a("p"),Nt=i("\u{1F917} Optimum provides an "),qe=a("code"),jt=i("optimum.onnxruntime"),Ct=i(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=a("a"),Pt=i("ONNX Runtime"),Dt=i(" quantization tool."),Je=c(),q=a("h2"),k=a("a"),Re=a("span"),h(U.$$.fragment),Ft=c(),_e=a("span"),Lt=i("Creating an "),Qe=a("code"),At=i("ORTQuantizer"),Ke=c(),T=a("p"),Xt=i("The "),we=a("code"),It=i("ORTQuantizer"),Ut=i(" class is used to quantize your ONNX model. The class can be initialized using the "),Ee=a("code"),St=i("from_pretrained()"),Ht=i(" method, which supports different checkpoint formats."),Ye=c(),ze=a("ol"),S=a("li"),Mt=i("Using a vanilla PyTorch Transformers checkpoint from the "),H=a("a"),Wt=i("Hugging Face Hub"),Vt=i("."),Ze=c(),h(M.$$.fragment),et=c(),W=a("ol"),V=a("li"),Gt=i("Using an already converted ONNX model from the "),G=a("a"),Bt=i("Hugging Face Hub"),Jt=i("."),tt=c(),h(B.$$.fragment),nt=c(),J=a("ol"),ke=a("li"),Kt=i("Using a already converted local ONNX model."),at=c(),h(K.$$.fragment),ot=c(),Y=a("ol"),Z=a("li"),Yt=i("Using an already initialized "),Ne=a("code"),Zt=i("ORTModelForXXX"),en=i(" class."),rt=c(),h(ee.$$.fragment),it=c(),R=a("h2"),N=a("a"),je=a("span"),h(te.$$.fragment),tn=c(),Ce=a("span"),nn=i("Dynamic Quantization example"),st=c(),y=a("p"),an=i("The "),Pe=a("code"),on=i("ORTQuantizer"),rn=i(" class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),ne=a("a"),sn=i("distilbert-base-uncased-finetuned-sst-2-english"),ln=i("."),lt=c(),h(ae.$$.fragment),mt=c(),Q=a("h2"),j=a("a"),De=a("span"),h(oe.$$.fragment),mn=c(),Fe=a("span"),cn=i("Static Quantization example"),ct=c(),xe=a("p"),un=i("TODO:"),ut=c(),w=a("h2"),C=a("a"),Le=a("span"),h(re.$$.fragment),pn=c(),Ae=a("span"),dn=i("ORTQuantizer"),pt=c(),d=a("div"),h(ie.$$.fragment),hn=c(),Xe=a("p"),fn=i("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),gn=c(),be=a("div"),h(se.$$.fragment),_n=c(),P=a("div"),h(le.$$.fragment),zn=c(),me=a("p"),xn=i("Quantize a model given the optimization specifications defined in "),Ie=a("code"),bn=i("quantization_config"),vn=i("."),Tn=c(),D=a("div"),h(ce.$$.fragment),yn=c(),Ue=a("p"),On=i("Perform the calibration step and collect the quantization ranges."),$n=c(),F=a("div"),h(ue.$$.fragment),qn=c(),pe=a("p"),Rn=i("Instantiate a "),Se=a("code"),Qn=i("ORTQuantizer"),wn=i(" from a pretrained pytorch model and preprocessor."),En=c(),L=a("div"),h(de.$$.fragment),kn=c(),he=a("p"),Nn=i("Create the calibration "),He=a("code"),jn=i("datasets.Dataset"),Cn=i(" to use for the post-training static quantization calibration step"),Pn=c(),A=a("div"),h(fe.$$.fragment),Dn=c(),Me=a("p"),Fn=i("Perform the calibration step and collect the quantization ranges."),this.h()},l(e){const l=qa('[data-svelte="svelte-1phssyn"]',document.head);O=o(l,"META",{name:!0,content:!0}),l.forEach(n),Ge=u(e),$=o(e,"H1",{class:!0});var ht=r($);E=o(ht,"A",{id:!0,class:!0,href:!0});var Xn=r(E);Oe=o(Xn,"SPAN",{});var In=r(Oe);f(X.$$.fragment,In),In.forEach(n),Xn.forEach(n),Et=u(ht),$e=o(ht,"SPAN",{});var Un=r($e);kt=s(Un,"Quantization"),Un.forEach(n),ht.forEach(n),Be=u(e),v=o(e,"P",{});var ve=r(v);Nt=s(ve,"\u{1F917} Optimum provides an "),qe=o(ve,"CODE",{});var Sn=r(qe);jt=s(Sn,"optimum.onnxruntime"),Sn.forEach(n),Ct=s(ve," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=o(ve,"A",{href:!0,rel:!0});var Hn=r(I);Pt=s(Hn,"ONNX Runtime"),Hn.forEach(n),Dt=s(ve," quantization tool."),ve.forEach(n),Je=u(e),q=o(e,"H2",{class:!0});var ft=r(q);k=o(ft,"A",{id:!0,class:!0,href:!0});var Mn=r(k);Re=o(Mn,"SPAN",{});var Wn=r(Re);f(U.$$.fragment,Wn),Wn.forEach(n),Mn.forEach(n),Ft=u(ft),_e=o(ft,"SPAN",{});var Ln=r(_e);Lt=s(Ln,"Creating an "),Qe=o(Ln,"CODE",{});var Vn=r(Qe);At=s(Vn,"ORTQuantizer"),Vn.forEach(n),Ln.forEach(n),ft.forEach(n),Ke=u(e),T=o(e,"P",{});var Te=r(T);Xt=s(Te,"The "),we=o(Te,"CODE",{});var Gn=r(we);It=s(Gn,"ORTQuantizer"),Gn.forEach(n),Ut=s(Te," class is used to quantize your ONNX model. The class can be initialized using the "),Ee=o(Te,"CODE",{});var Bn=r(Ee);St=s(Bn,"from_pretrained()"),Bn.forEach(n),Ht=s(Te," method, which supports different checkpoint formats."),Te.forEach(n),Ye=u(e),ze=o(e,"OL",{});var Jn=r(ze);S=o(Jn,"LI",{});var gt=r(S);Mt=s(gt,"Using a vanilla PyTorch Transformers checkpoint from the "),H=o(gt,"A",{href:!0,rel:!0});var Kn=r(H);Wt=s(Kn,"Hugging Face Hub"),Kn.forEach(n),Vt=s(gt,"."),gt.forEach(n),Jn.forEach(n),Ze=u(e),f(M.$$.fragment,e),et=u(e),W=o(e,"OL",{start:!0});var Yn=r(W);V=o(Yn,"LI",{});var _t=r(V);Gt=s(_t,"Using an already converted ONNX model from the "),G=o(_t,"A",{href:!0,rel:!0});var Zn=r(G);Bt=s(Zn,"Hugging Face Hub"),Zn.forEach(n),Jt=s(_t,"."),_t.forEach(n),Yn.forEach(n),tt=u(e),f(B.$$.fragment,e),nt=u(e),J=o(e,"OL",{start:!0});var ea=r(J);ke=o(ea,"LI",{});var ta=r(ke);Kt=s(ta,"Using a already converted local ONNX model."),ta.forEach(n),ea.forEach(n),at=u(e),f(K.$$.fragment,e),ot=u(e),Y=o(e,"OL",{start:!0});var na=r(Y);Z=o(na,"LI",{});var zt=r(Z);Yt=s(zt,"Using an already initialized "),Ne=o(zt,"CODE",{});var aa=r(Ne);Zt=s(aa,"ORTModelForXXX"),aa.forEach(n),en=s(zt," class."),zt.forEach(n),na.forEach(n),rt=u(e),f(ee.$$.fragment,e),it=u(e),R=o(e,"H2",{class:!0});var xt=r(R);N=o(xt,"A",{id:!0,class:!0,href:!0});var oa=r(N);je=o(oa,"SPAN",{});var ra=r(je);f(te.$$.fragment,ra),ra.forEach(n),oa.forEach(n),tn=u(xt),Ce=o(xt,"SPAN",{});var ia=r(Ce);nn=s(ia,"Dynamic Quantization example"),ia.forEach(n),xt.forEach(n),st=u(e),y=o(e,"P",{});var ye=r(y);an=s(ye,"The "),Pe=o(ye,"CODE",{});var sa=r(Pe);on=s(sa,"ORTQuantizer"),sa.forEach(n),rn=s(ye," class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),ne=o(ye,"A",{href:!0,rel:!0});var la=r(ne);sn=s(la,"distilbert-base-uncased-finetuned-sst-2-english"),la.forEach(n),ln=s(ye,"."),ye.forEach(n),lt=u(e),f(ae.$$.fragment,e),mt=u(e),Q=o(e,"H2",{class:!0});var bt=r(Q);j=o(bt,"A",{id:!0,class:!0,href:!0});var ma=r(j);De=o(ma,"SPAN",{});var ca=r(De);f(oe.$$.fragment,ca),ca.forEach(n),ma.forEach(n),mn=u(bt),Fe=o(bt,"SPAN",{});var ua=r(Fe);cn=s(ua,"Static Quantization example"),ua.forEach(n),bt.forEach(n),ct=u(e),xe=o(e,"P",{});var pa=r(xe);un=s(pa,"TODO:"),pa.forEach(n),ut=u(e),w=o(e,"H2",{class:!0});var vt=r(w);C=o(vt,"A",{id:!0,class:!0,href:!0});var da=r(C);Le=o(da,"SPAN",{});var ha=r(Le);f(re.$$.fragment,ha),ha.forEach(n),da.forEach(n),pn=u(vt),Ae=o(vt,"SPAN",{});var fa=r(Ae);dn=s(fa,"ORTQuantizer"),fa.forEach(n),vt.forEach(n),pt=u(e),d=o(e,"DIV",{class:!0});var b=r(d);f(ie.$$.fragment,b),hn=u(b),Xe=o(b,"P",{});var ga=r(Xe);fn=s(ga,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),ga.forEach(n),gn=u(b),be=o(b,"DIV",{class:!0});var _a=r(be);f(se.$$.fragment,_a),_a.forEach(n),_n=u(b),P=o(b,"DIV",{class:!0});var Tt=r(P);f(le.$$.fragment,Tt),zn=u(Tt),me=o(Tt,"P",{});var yt=r(me);xn=s(yt,"Quantize a model given the optimization specifications defined in "),Ie=o(yt,"CODE",{});var za=r(Ie);bn=s(za,"quantization_config"),za.forEach(n),vn=s(yt,"."),yt.forEach(n),Tt.forEach(n),Tn=u(b),D=o(b,"DIV",{class:!0});var Ot=r(D);f(ce.$$.fragment,Ot),yn=u(Ot),Ue=o(Ot,"P",{});var xa=r(Ue);On=s(xa,"Perform the calibration step and collect the quantization ranges."),xa.forEach(n),Ot.forEach(n),$n=u(b),F=o(b,"DIV",{class:!0});var $t=r(F);f(ue.$$.fragment,$t),qn=u($t),pe=o($t,"P",{});var qt=r(pe);Rn=s(qt,"Instantiate a "),Se=o(qt,"CODE",{});var ba=r(Se);Qn=s(ba,"ORTQuantizer"),ba.forEach(n),wn=s(qt," from a pretrained pytorch model and preprocessor."),qt.forEach(n),$t.forEach(n),En=u(b),L=o(b,"DIV",{class:!0});var Rt=r(L);f(de.$$.fragment,Rt),kn=u(Rt),he=o(Rt,"P",{});var Qt=r(he);Nn=s(Qt,"Create the calibration "),He=o(Qt,"CODE",{});var va=r(He);jn=s(va,"datasets.Dataset"),va.forEach(n),Cn=s(Qt," to use for the post-training static quantization calibration step"),Qt.forEach(n),Rt.forEach(n),Pn=u(b),A=o(b,"DIV",{class:!0});var wt=r(A);f(fe.$$.fragment,wt),Dn=u(wt),Me=o(wt,"P",{});var Ta=r(Me);Fn=s(Ta,"Perform the calibration step and collect the quantization ranges."),Ta.forEach(n),wt.forEach(n),b.forEach(n),this.h()},h(){m(O,"name","hf:doc:metadata"),m(O,"content",JSON.stringify(Ea)),m(E,"id","quantization"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#quantization"),m($,"class","relative group"),m(I,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),m(I,"rel","nofollow"),m(k,"id","creating-an-ortquantizer"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#creating-an-ortquantizer"),m(q,"class","relative group"),m(H,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(H,"rel","nofollow"),m(G,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(G,"rel","nofollow"),m(W,"start","2"),m(J,"start","3"),m(Y,"start","4"),m(N,"id","dynamic-quantization-example"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#dynamic-quantization-example"),m(R,"class","relative group"),m(ne,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(ne,"rel","nofollow"),m(j,"id","static-quantization-example"),m(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(j,"href","#static-quantization-example"),m(Q,"class","relative group"),m(C,"id","optimum.onnxruntime.ORTQuantizer"),m(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(C,"href","#optimum.onnxruntime.ORTQuantizer"),m(w,"class","relative group"),m(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,l){t(document.head,O),p(e,Ge,l),p(e,$,l),t($,E),t(E,Oe),g(X,Oe,null),t($,Et),t($,$e),t($e,kt),p(e,Be,l),p(e,v,l),t(v,Nt),t(v,qe),t(qe,jt),t(v,Ct),t(v,I),t(I,Pt),t(v,Dt),p(e,Je,l),p(e,q,l),t(q,k),t(k,Re),g(U,Re,null),t(q,Ft),t(q,_e),t(_e,Lt),t(_e,Qe),t(Qe,At),p(e,Ke,l),p(e,T,l),t(T,Xt),t(T,we),t(we,It),t(T,Ut),t(T,Ee),t(Ee,St),t(T,Ht),p(e,Ye,l),p(e,ze,l),t(ze,S),t(S,Mt),t(S,H),t(H,Wt),t(S,Vt),p(e,Ze,l),g(M,e,l),p(e,et,l),p(e,W,l),t(W,V),t(V,Gt),t(V,G),t(G,Bt),t(V,Jt),p(e,tt,l),g(B,e,l),p(e,nt,l),p(e,J,l),t(J,ke),t(ke,Kt),p(e,at,l),g(K,e,l),p(e,ot,l),p(e,Y,l),t(Y,Z),t(Z,Yt),t(Z,Ne),t(Ne,Zt),t(Z,en),p(e,rt,l),g(ee,e,l),p(e,it,l),p(e,R,l),t(R,N),t(N,je),g(te,je,null),t(R,tn),t(R,Ce),t(Ce,nn),p(e,st,l),p(e,y,l),t(y,an),t(y,Pe),t(Pe,on),t(y,rn),t(y,ne),t(ne,sn),t(y,ln),p(e,lt,l),g(ae,e,l),p(e,mt,l),p(e,Q,l),t(Q,j),t(j,De),g(oe,De,null),t(Q,mn),t(Q,Fe),t(Fe,cn),p(e,ct,l),p(e,xe,l),t(xe,un),p(e,ut,l),p(e,w,l),t(w,C),t(C,Le),g(re,Le,null),t(w,pn),t(w,Ae),t(Ae,dn),p(e,pt,l),p(e,d,l),g(ie,d,null),t(d,hn),t(d,Xe),t(Xe,fn),t(d,gn),t(d,be),g(se,be,null),t(d,_n),t(d,P),g(le,P,null),t(P,zn),t(P,me),t(me,xn),t(me,Ie),t(Ie,bn),t(me,vn),t(d,Tn),t(d,D),g(ce,D,null),t(D,yn),t(D,Ue),t(Ue,On),t(d,$n),t(d,F),g(ue,F,null),t(F,qn),t(F,pe),t(pe,Rn),t(pe,Se),t(Se,Qn),t(pe,wn),t(d,En),t(d,L),g(de,L,null),t(L,kn),t(L,he),t(he,Nn),t(he,He),t(He,jn),t(he,Cn),t(d,Pn),t(d,A),g(fe,A,null),t(A,Dn),t(A,Me),t(Me,Fn),dt=!0},p:Ra,i(e){dt||(_(X.$$.fragment,e),_(U.$$.fragment,e),_(M.$$.fragment,e),_(B.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(se.$$.fragment,e),_(le.$$.fragment,e),_(ce.$$.fragment,e),_(ue.$$.fragment,e),_(de.$$.fragment,e),_(fe.$$.fragment,e),dt=!0)},o(e){z(X.$$.fragment,e),z(U.$$.fragment,e),z(M.$$.fragment,e),z(B.$$.fragment,e),z(K.$$.fragment,e),z(ee.$$.fragment,e),z(te.$$.fragment,e),z(ae.$$.fragment,e),z(oe.$$.fragment,e),z(re.$$.fragment,e),z(ie.$$.fragment,e),z(se.$$.fragment,e),z(le.$$.fragment,e),z(ce.$$.fragment,e),z(ue.$$.fragment,e),z(de.$$.fragment,e),z(fe.$$.fragment,e),dt=!1},d(e){n(O),e&&n(Ge),e&&n($),x(X),e&&n(Be),e&&n(v),e&&n(Je),e&&n(q),x(U),e&&n(Ke),e&&n(T),e&&n(Ye),e&&n(ze),e&&n(Ze),x(M,e),e&&n(et),e&&n(W),e&&n(tt),x(B,e),e&&n(nt),e&&n(J),e&&n(at),x(K,e),e&&n(ot),e&&n(Y),e&&n(rt),x(ee,e),e&&n(it),e&&n(R),x(te),e&&n(st),e&&n(y),e&&n(lt),x(ae,e),e&&n(mt),e&&n(Q),x(oe),e&&n(ct),e&&n(xe),e&&n(ut),e&&n(w),x(re),e&&n(pt),e&&n(d),x(ie),x(se),x(le),x(ce),x(ue),x(de),x(fe)}}}const Ea={local:"quantization",sections:[{local:"creating-an-ortquantizer",title:"Creating an `ORTQuantizer`"},{local:"dynamic-quantization-example",title:"Dynamic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function ka(An){return Qa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Da extends ya{constructor(O){super();Oa(this,O,ka,wa,$a,{})}}export{Da as default,Ea as metadata};
