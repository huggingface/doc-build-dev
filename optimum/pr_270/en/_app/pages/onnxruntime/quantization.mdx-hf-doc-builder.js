import{S as yn,i as $n,s as On,e as n,k as c,w as h,t as i,M as qn,c as o,d as a,m as u,a as r,x as f,h as s,b as m,G as e,g as p,y as g,L as Rn,q as _,o as b,B as z,v as Qn}from"../../chunks/vendor-hf-doc-builder.js";import{D as gt}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Wt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Vt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function wn(Aa){let $,Gt,O,E,$t,X,Ee,Ot,Ne,Bt,x,ke,qt,je,Ce,S,De,Pe,Jt,q,N,Rt,U,Fe,_t,Ae,Qt,Le,Kt,T,Xe,wt,Se,Ue,Et,Ie,He,Yt,bt,I,Me,H,We,Ve,Zt,M,te,W,V,Ge,G,Be,Je,ee,B,ae,J,Nt,Ke,ne,K,oe,Y,Z,Ye,kt,Ze,ta,re,tt,ie,R,k,jt,et,ea,Ct,aa,se,y,na,Dt,oa,ra,at,ia,sa,le,nt,me,Q,j,Pt,ot,la,Ft,ma,ce,zt,ca,ue,w,C,At,rt,ua,Lt,pa,pe,d,it,da,Xt,ha,fa,D,st,ga,St,_a,ba,vt,lt,za,P,mt,va,ct,xa,Ut,Ta,ya,$a,F,ut,Oa,pt,qa,It,Ra,Qa,wa,A,dt,Ea,ht,Na,Ht,ka,ja,Ca,L,ft,Da,Mt,Pa,de;return X=new Vt({}),U=new Vt({}),M=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", from_transformers=True, task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a vanilla PyTorch checkpoint by converting the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, from_transformers=<span class="hljs-literal">True</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),B=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english", task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a converted ONNX model on the Hugging Face hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),K=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("path/to/model")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx exists in path/to/model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)`}}),tt=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForTextClassification

ort_model = ORTModelForTextClassification.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english")

quantizer = ORTQuantizer.from_pretrained(ort_model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForTextClassification

<span class="hljs-comment"># Loading ONNX Model from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForTextClassification.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)

<span class="hljs-comment"># Create a quantizer from a ORTModelForXXX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(ort_model)`}}),et=new Vt({}),nt=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=True)

quantizer = ORTQuantizer.from_pretrained(onnx_model)

dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

model_quantized_path = quantizer.export(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># Load PyTorch model and convert to ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Create quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize and export the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.export(
    output_path=<span class="hljs-string">&quot;model-quantized.onnx&quot;</span>,
    quantization_config=dqconfig,
)`}}),ot=new Vt({}),rt=new Vt({}),it=new gt({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": typing.List[pathlib.Path]"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L80"}}),st=new gt({props:{name:"calibrate",anchor:"optimum.onnxruntime.ORTQuantizer.calibrate",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L139",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),lt=new gt({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L246",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),mt=new gt({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"output_path",val:": typing.Union[str, pathlib.Path]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"file_prefix",val:": typing.Optional[str] = 'quantized'"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.output_path",description:`<strong>output_path</strong> (<code>Union[str, Path]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.file_prefix",description:`<strong>file_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;quantized&quot;</code>) &#x2014;
The prefix used to save the quantized model.`,name:"file_prefix"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L259",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),ut=new gt({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_or_path",val:": typing.Union[str, pathlib.Path]"},{name:"file_name",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_or_path",description:`<strong>model_or_path</strong> (<code>Union[str, Path]</code>) &#x2014;
Can be either:<ul>
<li>A path to a saved exported ONNX Intermediate Representation (IR) model, e.g., \`./my_model_directory/.</li>
<li>Or a <code>ORTModelForXX</code> class, e.g., <code>ORTModelForQuestionAnswering</code>.</li>
</ul>`,name:"model_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(`Union[str,",description:"<strong>file_name(`Union[str,</strong> List[str]]<code>, *optional*) -- Overwrites the default model file name from </code>&#x201C;model.onnx&#x201D;<code>to</code>file_name`.\nThis allows you to load different model files from the same repository or directory.",name:"file_name(`Union[str,"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L95",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),dt=new gt({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"},{name:"use_auth_token",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the token generated when running <code>transformers-cli login</code> (necessary for some datasets
like ImageNet).`,name:"use_auth_token"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L351",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),ft=new gt({props:{name:"partial_calibrate",anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L193",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){$=n("meta"),Gt=c(),O=n("h1"),E=n("a"),$t=n("span"),h(X.$$.fragment),Ee=c(),Ot=n("span"),Ne=i("Quantization"),Bt=c(),x=n("p"),ke=i("\u{1F917} Optimum provides an "),qt=n("code"),je=i("optimum.onnxruntime"),Ce=i(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),S=n("a"),De=i("ONNX Runtime"),Pe=i(" quantization tool."),Jt=c(),q=n("h2"),N=n("a"),Rt=n("span"),h(U.$$.fragment),Fe=c(),_t=n("span"),Ae=i("Creating an "),Qt=n("code"),Le=i("ORTQuantizer"),Kt=c(),T=n("p"),Xe=i("The "),wt=n("code"),Se=i("ORTQuantizer"),Ue=i(" class is used to quantize your ONNX model. The class can be initialized using the "),Et=n("code"),Ie=i("from_pretrained()"),He=i(" method, which supports different checkpoint formats."),Yt=c(),bt=n("ol"),I=n("li"),Me=i("Using a vanilla PyTorch Transformers checkpoint from the "),H=n("a"),We=i("Hugging Face Hub"),Ve=i("."),Zt=c(),h(M.$$.fragment),te=c(),W=n("ol"),V=n("li"),Ge=i("Using an already converted ONNX model from the "),G=n("a"),Be=i("Hugging Face Hub"),Je=i("."),ee=c(),h(B.$$.fragment),ae=c(),J=n("ol"),Nt=n("li"),Ke=i("Using a already converted local ONNX model."),ne=c(),h(K.$$.fragment),oe=c(),Y=n("ol"),Z=n("li"),Ye=i("Using an already initialized "),kt=n("code"),Ze=i("ORTModelForXXX"),ta=i(" class."),re=c(),h(tt.$$.fragment),ie=c(),R=n("h2"),k=n("a"),jt=n("span"),h(et.$$.fragment),ea=c(),Ct=n("span"),aa=i("Dynamic Quantization example"),se=c(),y=n("p"),na=i("The "),Dt=n("code"),oa=i("ORTQuantizer"),ra=i(" class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),at=n("a"),ia=i("distilbert-base-uncased-finetuned-sst-2-english"),sa=i("."),le=c(),h(nt.$$.fragment),me=c(),Q=n("h2"),j=n("a"),Pt=n("span"),h(ot.$$.fragment),la=c(),Ft=n("span"),ma=i("Static Quantization example"),ce=c(),zt=n("p"),ca=i("TODO:"),ue=c(),w=n("h2"),C=n("a"),At=n("span"),h(rt.$$.fragment),ua=c(),Lt=n("span"),pa=i("ORTQuantizer"),pe=c(),d=n("div"),h(it.$$.fragment),da=c(),Xt=n("p"),ha=i("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),fa=c(),D=n("div"),h(st.$$.fragment),ga=c(),St=n("p"),_a=i("Perform the calibration step and collect the quantization ranges."),ba=c(),vt=n("div"),h(lt.$$.fragment),za=c(),P=n("div"),h(mt.$$.fragment),va=c(),ct=n("p"),xa=i("Quantize a model given the optimization specifications defined in "),Ut=n("code"),Ta=i("quantization_config"),ya=i("."),$a=c(),F=n("div"),h(ut.$$.fragment),Oa=c(),pt=n("p"),qa=i("Instantiate a "),It=n("code"),Ra=i("ORTQuantizer"),Qa=i(" from a pretrained pytorch model and preprocessor."),wa=c(),A=n("div"),h(dt.$$.fragment),Ea=c(),ht=n("p"),Na=i("Create the calibration "),Ht=n("code"),ka=i("datasets.Dataset"),ja=i(" to use for the post-training static quantization calibration step"),Ca=c(),L=n("div"),h(ft.$$.fragment),Da=c(),Mt=n("p"),Pa=i("Perform the calibration step and collect the quantization ranges."),this.h()},l(t){const l=qn('[data-svelte="svelte-1phssyn"]',document.head);$=o(l,"META",{name:!0,content:!0}),l.forEach(a),Gt=u(t),O=o(t,"H1",{class:!0});var he=r(O);E=o(he,"A",{id:!0,class:!0,href:!0});var La=r(E);$t=o(La,"SPAN",{});var Xa=r($t);f(X.$$.fragment,Xa),Xa.forEach(a),La.forEach(a),Ee=u(he),Ot=o(he,"SPAN",{});var Sa=r(Ot);Ne=s(Sa,"Quantization"),Sa.forEach(a),he.forEach(a),Bt=u(t),x=o(t,"P",{});var xt=r(x);ke=s(xt,"\u{1F917} Optimum provides an "),qt=o(xt,"CODE",{});var Ua=r(qt);je=s(Ua,"optimum.onnxruntime"),Ua.forEach(a),Ce=s(xt," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),S=o(xt,"A",{href:!0,rel:!0});var Ia=r(S);De=s(Ia,"ONNX Runtime"),Ia.forEach(a),Pe=s(xt," quantization tool."),xt.forEach(a),Jt=u(t),q=o(t,"H2",{class:!0});var fe=r(q);N=o(fe,"A",{id:!0,class:!0,href:!0});var Ha=r(N);Rt=o(Ha,"SPAN",{});var Ma=r(Rt);f(U.$$.fragment,Ma),Ma.forEach(a),Ha.forEach(a),Fe=u(fe),_t=o(fe,"SPAN",{});var Fa=r(_t);Ae=s(Fa,"Creating an "),Qt=o(Fa,"CODE",{});var Wa=r(Qt);Le=s(Wa,"ORTQuantizer"),Wa.forEach(a),Fa.forEach(a),fe.forEach(a),Kt=u(t),T=o(t,"P",{});var Tt=r(T);Xe=s(Tt,"The "),wt=o(Tt,"CODE",{});var Va=r(wt);Se=s(Va,"ORTQuantizer"),Va.forEach(a),Ue=s(Tt," class is used to quantize your ONNX model. The class can be initialized using the "),Et=o(Tt,"CODE",{});var Ga=r(Et);Ie=s(Ga,"from_pretrained()"),Ga.forEach(a),He=s(Tt," method, which supports different checkpoint formats."),Tt.forEach(a),Yt=u(t),bt=o(t,"OL",{});var Ba=r(bt);I=o(Ba,"LI",{});var ge=r(I);Me=s(ge,"Using a vanilla PyTorch Transformers checkpoint from the "),H=o(ge,"A",{href:!0,rel:!0});var Ja=r(H);We=s(Ja,"Hugging Face Hub"),Ja.forEach(a),Ve=s(ge,"."),ge.forEach(a),Ba.forEach(a),Zt=u(t),f(M.$$.fragment,t),te=u(t),W=o(t,"OL",{start:!0});var Ka=r(W);V=o(Ka,"LI",{});var _e=r(V);Ge=s(_e,"Using an already converted ONNX model from the "),G=o(_e,"A",{href:!0,rel:!0});var Ya=r(G);Be=s(Ya,"Hugging Face Hub"),Ya.forEach(a),Je=s(_e,"."),_e.forEach(a),Ka.forEach(a),ee=u(t),f(B.$$.fragment,t),ae=u(t),J=o(t,"OL",{start:!0});var Za=r(J);Nt=o(Za,"LI",{});var tn=r(Nt);Ke=s(tn,"Using a already converted local ONNX model."),tn.forEach(a),Za.forEach(a),ne=u(t),f(K.$$.fragment,t),oe=u(t),Y=o(t,"OL",{start:!0});var en=r(Y);Z=o(en,"LI",{});var be=r(Z);Ye=s(be,"Using an already initialized "),kt=o(be,"CODE",{});var an=r(kt);Ze=s(an,"ORTModelForXXX"),an.forEach(a),ta=s(be," class."),be.forEach(a),en.forEach(a),re=u(t),f(tt.$$.fragment,t),ie=u(t),R=o(t,"H2",{class:!0});var ze=r(R);k=o(ze,"A",{id:!0,class:!0,href:!0});var nn=r(k);jt=o(nn,"SPAN",{});var on=r(jt);f(et.$$.fragment,on),on.forEach(a),nn.forEach(a),ea=u(ze),Ct=o(ze,"SPAN",{});var rn=r(Ct);aa=s(rn,"Dynamic Quantization example"),rn.forEach(a),ze.forEach(a),se=u(t),y=o(t,"P",{});var yt=r(y);na=s(yt,"The "),Dt=o(yt,"CODE",{});var sn=r(Dt);oa=s(sn,"ORTQuantizer"),sn.forEach(a),ra=s(yt," class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),at=o(yt,"A",{href:!0,rel:!0});var ln=r(at);ia=s(ln,"distilbert-base-uncased-finetuned-sst-2-english"),ln.forEach(a),sa=s(yt,"."),yt.forEach(a),le=u(t),f(nt.$$.fragment,t),me=u(t),Q=o(t,"H2",{class:!0});var ve=r(Q);j=o(ve,"A",{id:!0,class:!0,href:!0});var mn=r(j);Pt=o(mn,"SPAN",{});var cn=r(Pt);f(ot.$$.fragment,cn),cn.forEach(a),mn.forEach(a),la=u(ve),Ft=o(ve,"SPAN",{});var un=r(Ft);ma=s(un,"Static Quantization example"),un.forEach(a),ve.forEach(a),ce=u(t),zt=o(t,"P",{});var pn=r(zt);ca=s(pn,"TODO:"),pn.forEach(a),ue=u(t),w=o(t,"H2",{class:!0});var xe=r(w);C=o(xe,"A",{id:!0,class:!0,href:!0});var dn=r(C);At=o(dn,"SPAN",{});var hn=r(At);f(rt.$$.fragment,hn),hn.forEach(a),dn.forEach(a),ua=u(xe),Lt=o(xe,"SPAN",{});var fn=r(Lt);pa=s(fn,"ORTQuantizer"),fn.forEach(a),xe.forEach(a),pe=u(t),d=o(t,"DIV",{class:!0});var v=r(d);f(it.$$.fragment,v),da=u(v),Xt=o(v,"P",{});var gn=r(Xt);ha=s(gn,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),gn.forEach(a),fa=u(v),D=o(v,"DIV",{class:!0});var Te=r(D);f(st.$$.fragment,Te),ga=u(Te),St=o(Te,"P",{});var _n=r(St);_a=s(_n,"Perform the calibration step and collect the quantization ranges."),_n.forEach(a),Te.forEach(a),ba=u(v),vt=o(v,"DIV",{class:!0});var bn=r(vt);f(lt.$$.fragment,bn),bn.forEach(a),za=u(v),P=o(v,"DIV",{class:!0});var ye=r(P);f(mt.$$.fragment,ye),va=u(ye),ct=o(ye,"P",{});var $e=r(ct);xa=s($e,"Quantize a model given the optimization specifications defined in "),Ut=o($e,"CODE",{});var zn=r(Ut);Ta=s(zn,"quantization_config"),zn.forEach(a),ya=s($e,"."),$e.forEach(a),ye.forEach(a),$a=u(v),F=o(v,"DIV",{class:!0});var Oe=r(F);f(ut.$$.fragment,Oe),Oa=u(Oe),pt=o(Oe,"P",{});var qe=r(pt);qa=s(qe,"Instantiate a "),It=o(qe,"CODE",{});var vn=r(It);Ra=s(vn,"ORTQuantizer"),vn.forEach(a),Qa=s(qe," from a pretrained pytorch model and preprocessor."),qe.forEach(a),Oe.forEach(a),wa=u(v),A=o(v,"DIV",{class:!0});var Re=r(A);f(dt.$$.fragment,Re),Ea=u(Re),ht=o(Re,"P",{});var Qe=r(ht);Na=s(Qe,"Create the calibration "),Ht=o(Qe,"CODE",{});var xn=r(Ht);ka=s(xn,"datasets.Dataset"),xn.forEach(a),ja=s(Qe," to use for the post-training static quantization calibration step"),Qe.forEach(a),Re.forEach(a),Ca=u(v),L=o(v,"DIV",{class:!0});var we=r(L);f(ft.$$.fragment,we),Da=u(we),Mt=o(we,"P",{});var Tn=r(Mt);Pa=s(Tn,"Perform the calibration step and collect the quantization ranges."),Tn.forEach(a),we.forEach(a),v.forEach(a),this.h()},h(){m($,"name","hf:doc:metadata"),m($,"content",JSON.stringify(En)),m(E,"id","quantization"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#quantization"),m(O,"class","relative group"),m(S,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),m(S,"rel","nofollow"),m(N,"id","creating-an-ortquantizer"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#creating-an-ortquantizer"),m(q,"class","relative group"),m(H,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(H,"rel","nofollow"),m(G,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(G,"rel","nofollow"),m(W,"start","2"),m(J,"start","3"),m(Y,"start","4"),m(k,"id","dynamic-quantization-example"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#dynamic-quantization-example"),m(R,"class","relative group"),m(at,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(at,"rel","nofollow"),m(j,"id","static-quantization-example"),m(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(j,"href","#static-quantization-example"),m(Q,"class","relative group"),m(C,"id","optimum.onnxruntime.ORTQuantizer"),m(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(C,"href","#optimum.onnxruntime.ORTQuantizer"),m(w,"class","relative group"),m(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,l){e(document.head,$),p(t,Gt,l),p(t,O,l),e(O,E),e(E,$t),g(X,$t,null),e(O,Ee),e(O,Ot),e(Ot,Ne),p(t,Bt,l),p(t,x,l),e(x,ke),e(x,qt),e(qt,je),e(x,Ce),e(x,S),e(S,De),e(x,Pe),p(t,Jt,l),p(t,q,l),e(q,N),e(N,Rt),g(U,Rt,null),e(q,Fe),e(q,_t),e(_t,Ae),e(_t,Qt),e(Qt,Le),p(t,Kt,l),p(t,T,l),e(T,Xe),e(T,wt),e(wt,Se),e(T,Ue),e(T,Et),e(Et,Ie),e(T,He),p(t,Yt,l),p(t,bt,l),e(bt,I),e(I,Me),e(I,H),e(H,We),e(I,Ve),p(t,Zt,l),g(M,t,l),p(t,te,l),p(t,W,l),e(W,V),e(V,Ge),e(V,G),e(G,Be),e(V,Je),p(t,ee,l),g(B,t,l),p(t,ae,l),p(t,J,l),e(J,Nt),e(Nt,Ke),p(t,ne,l),g(K,t,l),p(t,oe,l),p(t,Y,l),e(Y,Z),e(Z,Ye),e(Z,kt),e(kt,Ze),e(Z,ta),p(t,re,l),g(tt,t,l),p(t,ie,l),p(t,R,l),e(R,k),e(k,jt),g(et,jt,null),e(R,ea),e(R,Ct),e(Ct,aa),p(t,se,l),p(t,y,l),e(y,na),e(y,Dt),e(Dt,oa),e(y,ra),e(y,at),e(at,ia),e(y,sa),p(t,le,l),g(nt,t,l),p(t,me,l),p(t,Q,l),e(Q,j),e(j,Pt),g(ot,Pt,null),e(Q,la),e(Q,Ft),e(Ft,ma),p(t,ce,l),p(t,zt,l),e(zt,ca),p(t,ue,l),p(t,w,l),e(w,C),e(C,At),g(rt,At,null),e(w,ua),e(w,Lt),e(Lt,pa),p(t,pe,l),p(t,d,l),g(it,d,null),e(d,da),e(d,Xt),e(Xt,ha),e(d,fa),e(d,D),g(st,D,null),e(D,ga),e(D,St),e(St,_a),e(d,ba),e(d,vt),g(lt,vt,null),e(d,za),e(d,P),g(mt,P,null),e(P,va),e(P,ct),e(ct,xa),e(ct,Ut),e(Ut,Ta),e(ct,ya),e(d,$a),e(d,F),g(ut,F,null),e(F,Oa),e(F,pt),e(pt,qa),e(pt,It),e(It,Ra),e(pt,Qa),e(d,wa),e(d,A),g(dt,A,null),e(A,Ea),e(A,ht),e(ht,Na),e(ht,Ht),e(Ht,ka),e(ht,ja),e(d,Ca),e(d,L),g(ft,L,null),e(L,Da),e(L,Mt),e(Mt,Pa),de=!0},p:Rn,i(t){de||(_(X.$$.fragment,t),_(U.$$.fragment,t),_(M.$$.fragment,t),_(B.$$.fragment,t),_(K.$$.fragment,t),_(tt.$$.fragment,t),_(et.$$.fragment,t),_(nt.$$.fragment,t),_(ot.$$.fragment,t),_(rt.$$.fragment,t),_(it.$$.fragment,t),_(st.$$.fragment,t),_(lt.$$.fragment,t),_(mt.$$.fragment,t),_(ut.$$.fragment,t),_(dt.$$.fragment,t),_(ft.$$.fragment,t),de=!0)},o(t){b(X.$$.fragment,t),b(U.$$.fragment,t),b(M.$$.fragment,t),b(B.$$.fragment,t),b(K.$$.fragment,t),b(tt.$$.fragment,t),b(et.$$.fragment,t),b(nt.$$.fragment,t),b(ot.$$.fragment,t),b(rt.$$.fragment,t),b(it.$$.fragment,t),b(st.$$.fragment,t),b(lt.$$.fragment,t),b(mt.$$.fragment,t),b(ut.$$.fragment,t),b(dt.$$.fragment,t),b(ft.$$.fragment,t),de=!1},d(t){a($),t&&a(Gt),t&&a(O),z(X),t&&a(Bt),t&&a(x),t&&a(Jt),t&&a(q),z(U),t&&a(Kt),t&&a(T),t&&a(Yt),t&&a(bt),t&&a(Zt),z(M,t),t&&a(te),t&&a(W),t&&a(ee),z(B,t),t&&a(ae),t&&a(J),t&&a(ne),z(K,t),t&&a(oe),t&&a(Y),t&&a(re),z(tt,t),t&&a(ie),t&&a(R),z(et),t&&a(se),t&&a(y),t&&a(le),z(nt,t),t&&a(me),t&&a(Q),z(ot),t&&a(ce),t&&a(zt),t&&a(ue),t&&a(w),z(rt),t&&a(pe),t&&a(d),z(it),z(st),z(lt),z(mt),z(ut),z(dt),z(ft)}}}const En={local:"quantization",sections:[{local:"creating-an-ortquantizer",title:"Creating an `ORTQuantizer`"},{local:"dynamic-quantization-example",title:"Dynamic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function Nn(Aa){return Qn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pn extends yn{constructor($){super();$n(this,$,Nn,wn,On,{})}}export{Pn as default,En as metadata};
