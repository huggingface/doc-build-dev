import{S as ya,i as Oa,s as $a,e as a,k as p,w as h,t as i,M as qa,c as o,d as n,m as c,a as r,x as f,h as s,b as m,G as e,g as u,y as g,L as Ra,q as _,o as z,B as x,v as Qa}from"../../chunks/vendor-hf-doc-builder.js";import{D as gt}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Wt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Vt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function wa(Xn){let O,Gt,$,E,Ot,A,Ee,$t,ke,Bt,v,Ne,qt,je,De,I,Pe,Ce,Jt,q,k,Rt,U,Fe,_t,Le,Qt,Xe,Kt,T,Ae,wt,Ie,Ue,Et,He,Se,Yt,zt,H,Me,S,We,Ve,Zt,M,te,W,V,Ge,G,Be,Je,ee,B,ne,J,kt,Ke,ae,K,oe,Y,Z,Ye,Nt,Ze,tn,re,tt,ie,R,N,jt,et,en,Dt,nn,se,y,an,Pt,on,rn,nt,sn,ln,le,at,me,Q,j,Ct,ot,mn,Ft,pn,pe,xt,cn,ce,w,D,Lt,rt,un,Xt,dn,ue,d,it,hn,At,fn,gn,bt,st,_n,P,lt,zn,mt,xn,It,bn,vn,Tn,C,pt,yn,Ut,On,$n,F,ct,qn,ut,Rn,Ht,Qn,wn,En,L,dt,kn,ht,Nn,St,jn,Dn,Pn,X,ft,Cn,Mt,Fn,de;return A=new Vt({}),U=new Vt({}),M=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", from_transformers=True, task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a vanilla PyTorch checkpoint by converting the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, from_transformers=<span class="hljs-literal">True</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),B=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english", task="text-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># Create a quantizer from a converted ONNX model on the Hugging Face hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, task=<span class="hljs-string">&quot;text-classification&quot;</span>)`}}),K=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("path/to/model")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx in the path/to/model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)`}}),tt=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForTextClassification

ort_model = ORTModelForTextClassification.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english")

quantizer = ORTQuantizer.from_pretrained(ort_model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForTextClassification

<span class="hljs-comment"># loading ONNX Model from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForTextClassification.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)

<span class="hljs-comment"># Create a quantizer from a ORTModelForXXX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(ort_model)`}}),et=new Vt({}),at=new Wt({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=True)

quantizer = ORTQuantizer.from_pretrained(onnx_model)

dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

model_quantized_path = quantizer.export(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># load pytorch model and convert to onnx</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id,from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># create quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize and export the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.export(
    output_path=<span class="hljs-string">&quot;model-quantized.onnx&quot;</span>,
    quantization_config=dqconfig,
)`}}),ot=new Vt({}),rt=new Vt({}),it=new gt({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": PathLike"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L82"}}),st=new gt({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L283",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),lt=new gt({props:{name:"export",anchor:"optimum.onnxruntime.ORTQuantizer.export",parameters:[{name:"output_path",val:": typing.Union[str, os.PathLike]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.export.output_path",description:`<strong>output_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L296",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),pt=new gt({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L172",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),ct=new gt({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"from_transformers",val:": typing.Optional[bool] = False"},{name:"task",val:": typing.Optional[str] = None"},{name:"file_name",val:": typing.Optional[str] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
Is needed to load models from a private repository.`,name:"use_auth_token"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.from_transformers",description:`<strong>from_transformers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Defines whether the provided <code>model_name_or_path</code> contains a vanilla Transformers checkpoint.
ORTQuantizer will then export the model first to ONNX.`,name:"from_transformers"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.task",description:`<strong>task</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Transformers pipeline task for the model. Will be used to convert the model if needed.`,name:"task"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(str,",description:`<strong>file_name(<code>str</code>,</strong> <em>optional</em>) &#x2014;
Overwrites the default model file name from <code>&quot;model.onnx&quot;</code> to <code>file_name</code>. This allows you to load different model files from the same
repository or directory.`,name:"file_name(str,"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, Path]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L87",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),dt=new gt({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>int</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L384",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),ft=new gt({props:{name:"partial_fit",anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L228",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){O=a("meta"),Gt=p(),$=a("h1"),E=a("a"),Ot=a("span"),h(A.$$.fragment),Ee=p(),$t=a("span"),ke=i("Quantization"),Bt=p(),v=a("p"),Ne=i("\u{1F917} Optimum provides an "),qt=a("code"),je=i("optimum.onnxruntime"),De=i(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=a("a"),Pe=i("ONNX Runtime"),Ce=i(" quantization tool."),Jt=p(),q=a("h2"),k=a("a"),Rt=a("span"),h(U.$$.fragment),Fe=p(),_t=a("span"),Le=i("Creating an "),Qt=a("code"),Xe=i("ORTQuantizer"),Kt=p(),T=a("p"),Ae=i("The "),wt=a("code"),Ie=i("ORTQuantizer"),Ue=i(" class is used to quantize your ONNX model. The class can be initialized using the "),Et=a("code"),He=i("from_pretrained()"),Se=i(" method, which supports different checkpoint formats."),Yt=p(),zt=a("ol"),H=a("li"),Me=i("Using a vanilla PyTorch Transformers checkpoint from the "),S=a("a"),We=i("Hugging Face Hub"),Ve=i("."),Zt=p(),h(M.$$.fragment),te=p(),W=a("ol"),V=a("li"),Ge=i("Using an already converted ONNX model from the "),G=a("a"),Be=i("Hugging Face Hub"),Je=i("."),ee=p(),h(B.$$.fragment),ne=p(),J=a("ol"),kt=a("li"),Ke=i("Using a already converted local ONNX model."),ae=p(),h(K.$$.fragment),oe=p(),Y=a("ol"),Z=a("li"),Ye=i("Using a already initialized "),Nt=a("code"),Ze=i("ORTModelForXXX"),tn=i(" class."),re=p(),h(tt.$$.fragment),ie=p(),R=a("h2"),N=a("a"),jt=a("span"),h(et.$$.fragment),en=p(),Dt=a("span"),nn=i("Dynamic Quantization example"),se=p(),y=a("p"),an=i("The "),Pt=a("code"),on=i("ORTQuantizer"),rn=i(" class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),nt=a("a"),sn=i("distilbert-base-uncased-finetuned-sst-2-english"),ln=i("."),le=p(),h(at.$$.fragment),me=p(),Q=a("h2"),j=a("a"),Ct=a("span"),h(ot.$$.fragment),mn=p(),Ft=a("span"),pn=i("Static Quantization example"),pe=p(),xt=a("p"),cn=i("TODO:"),ce=p(),w=a("h2"),D=a("a"),Lt=a("span"),h(rt.$$.fragment),un=p(),Xt=a("span"),dn=i("ORTQuantizer"),ue=p(),d=a("div"),h(it.$$.fragment),hn=p(),At=a("p"),fn=i("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),gn=p(),bt=a("div"),h(st.$$.fragment),_n=p(),P=a("div"),h(lt.$$.fragment),zn=p(),mt=a("p"),xn=i("Quantize a model given the optimization specifications defined in "),It=a("code"),bn=i("quantization_config"),vn=i("."),Tn=p(),C=a("div"),h(pt.$$.fragment),yn=p(),Ut=a("p"),On=i("Perform the calibration step and collect the quantization ranges."),$n=p(),F=a("div"),h(ct.$$.fragment),qn=p(),ut=a("p"),Rn=i("Instantiate a "),Ht=a("code"),Qn=i("ORTQuantizer"),wn=i(" from a pretrained pytorch model and preprocessor."),En=p(),L=a("div"),h(dt.$$.fragment),kn=p(),ht=a("p"),Nn=i("Create the calibration "),St=a("code"),jn=i("datasets.Dataset"),Dn=i(" to use for the post-training static quantization calibration step"),Pn=p(),X=a("div"),h(ft.$$.fragment),Cn=p(),Mt=a("p"),Fn=i("Perform the calibration step and collect the quantization ranges."),this.h()},l(t){const l=qa('[data-svelte="svelte-1phssyn"]',document.head);O=o(l,"META",{name:!0,content:!0}),l.forEach(n),Gt=c(t),$=o(t,"H1",{class:!0});var he=r($);E=o(he,"A",{id:!0,class:!0,href:!0});var An=r(E);Ot=o(An,"SPAN",{});var In=r(Ot);f(A.$$.fragment,In),In.forEach(n),An.forEach(n),Ee=c(he),$t=o(he,"SPAN",{});var Un=r($t);ke=s(Un,"Quantization"),Un.forEach(n),he.forEach(n),Bt=c(t),v=o(t,"P",{});var vt=r(v);Ne=s(vt,"\u{1F917} Optimum provides an "),qt=o(vt,"CODE",{});var Hn=r(qt);je=s(Hn,"optimum.onnxruntime"),Hn.forEach(n),De=s(vt," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),I=o(vt,"A",{href:!0,rel:!0});var Sn=r(I);Pe=s(Sn,"ONNX Runtime"),Sn.forEach(n),Ce=s(vt," quantization tool."),vt.forEach(n),Jt=c(t),q=o(t,"H2",{class:!0});var fe=r(q);k=o(fe,"A",{id:!0,class:!0,href:!0});var Mn=r(k);Rt=o(Mn,"SPAN",{});var Wn=r(Rt);f(U.$$.fragment,Wn),Wn.forEach(n),Mn.forEach(n),Fe=c(fe),_t=o(fe,"SPAN",{});var Ln=r(_t);Le=s(Ln,"Creating an "),Qt=o(Ln,"CODE",{});var Vn=r(Qt);Xe=s(Vn,"ORTQuantizer"),Vn.forEach(n),Ln.forEach(n),fe.forEach(n),Kt=c(t),T=o(t,"P",{});var Tt=r(T);Ae=s(Tt,"The "),wt=o(Tt,"CODE",{});var Gn=r(wt);Ie=s(Gn,"ORTQuantizer"),Gn.forEach(n),Ue=s(Tt," class is used to quantize your ONNX model. The class can be initialized using the "),Et=o(Tt,"CODE",{});var Bn=r(Et);He=s(Bn,"from_pretrained()"),Bn.forEach(n),Se=s(Tt," method, which supports different checkpoint formats."),Tt.forEach(n),Yt=c(t),zt=o(t,"OL",{});var Jn=r(zt);H=o(Jn,"LI",{});var ge=r(H);Me=s(ge,"Using a vanilla PyTorch Transformers checkpoint from the "),S=o(ge,"A",{href:!0,rel:!0});var Kn=r(S);We=s(Kn,"Hugging Face Hub"),Kn.forEach(n),Ve=s(ge,"."),ge.forEach(n),Jn.forEach(n),Zt=c(t),f(M.$$.fragment,t),te=c(t),W=o(t,"OL",{start:!0});var Yn=r(W);V=o(Yn,"LI",{});var _e=r(V);Ge=s(_e,"Using an already converted ONNX model from the "),G=o(_e,"A",{href:!0,rel:!0});var Zn=r(G);Be=s(Zn,"Hugging Face Hub"),Zn.forEach(n),Je=s(_e,"."),_e.forEach(n),Yn.forEach(n),ee=c(t),f(B.$$.fragment,t),ne=c(t),J=o(t,"OL",{start:!0});var ta=r(J);kt=o(ta,"LI",{});var ea=r(kt);Ke=s(ea,"Using a already converted local ONNX model."),ea.forEach(n),ta.forEach(n),ae=c(t),f(K.$$.fragment,t),oe=c(t),Y=o(t,"OL",{start:!0});var na=r(Y);Z=o(na,"LI",{});var ze=r(Z);Ye=s(ze,"Using a already initialized "),Nt=o(ze,"CODE",{});var aa=r(Nt);Ze=s(aa,"ORTModelForXXX"),aa.forEach(n),tn=s(ze," class."),ze.forEach(n),na.forEach(n),re=c(t),f(tt.$$.fragment,t),ie=c(t),R=o(t,"H2",{class:!0});var xe=r(R);N=o(xe,"A",{id:!0,class:!0,href:!0});var oa=r(N);jt=o(oa,"SPAN",{});var ra=r(jt);f(et.$$.fragment,ra),ra.forEach(n),oa.forEach(n),en=c(xe),Dt=o(xe,"SPAN",{});var ia=r(Dt);nn=s(ia,"Dynamic Quantization example"),ia.forEach(n),xe.forEach(n),se=c(t),y=o(t,"P",{});var yt=r(y);an=s(yt,"The "),Pt=o(yt,"CODE",{});var sa=r(Pt);on=s(sa,"ORTQuantizer"),sa.forEach(n),rn=s(yt," class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),nt=o(yt,"A",{href:!0,rel:!0});var la=r(nt);sn=s(la,"distilbert-base-uncased-finetuned-sst-2-english"),la.forEach(n),ln=s(yt,"."),yt.forEach(n),le=c(t),f(at.$$.fragment,t),me=c(t),Q=o(t,"H2",{class:!0});var be=r(Q);j=o(be,"A",{id:!0,class:!0,href:!0});var ma=r(j);Ct=o(ma,"SPAN",{});var pa=r(Ct);f(ot.$$.fragment,pa),pa.forEach(n),ma.forEach(n),mn=c(be),Ft=o(be,"SPAN",{});var ca=r(Ft);pn=s(ca,"Static Quantization example"),ca.forEach(n),be.forEach(n),pe=c(t),xt=o(t,"P",{});var ua=r(xt);cn=s(ua,"TODO:"),ua.forEach(n),ce=c(t),w=o(t,"H2",{class:!0});var ve=r(w);D=o(ve,"A",{id:!0,class:!0,href:!0});var da=r(D);Lt=o(da,"SPAN",{});var ha=r(Lt);f(rt.$$.fragment,ha),ha.forEach(n),da.forEach(n),un=c(ve),Xt=o(ve,"SPAN",{});var fa=r(Xt);dn=s(fa,"ORTQuantizer"),fa.forEach(n),ve.forEach(n),ue=c(t),d=o(t,"DIV",{class:!0});var b=r(d);f(it.$$.fragment,b),hn=c(b),At=o(b,"P",{});var ga=r(At);fn=s(ga,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),ga.forEach(n),gn=c(b),bt=o(b,"DIV",{class:!0});var _a=r(bt);f(st.$$.fragment,_a),_a.forEach(n),_n=c(b),P=o(b,"DIV",{class:!0});var Te=r(P);f(lt.$$.fragment,Te),zn=c(Te),mt=o(Te,"P",{});var ye=r(mt);xn=s(ye,"Quantize a model given the optimization specifications defined in "),It=o(ye,"CODE",{});var za=r(It);bn=s(za,"quantization_config"),za.forEach(n),vn=s(ye,"."),ye.forEach(n),Te.forEach(n),Tn=c(b),C=o(b,"DIV",{class:!0});var Oe=r(C);f(pt.$$.fragment,Oe),yn=c(Oe),Ut=o(Oe,"P",{});var xa=r(Ut);On=s(xa,"Perform the calibration step and collect the quantization ranges."),xa.forEach(n),Oe.forEach(n),$n=c(b),F=o(b,"DIV",{class:!0});var $e=r(F);f(ct.$$.fragment,$e),qn=c($e),ut=o($e,"P",{});var qe=r(ut);Rn=s(qe,"Instantiate a "),Ht=o(qe,"CODE",{});var ba=r(Ht);Qn=s(ba,"ORTQuantizer"),ba.forEach(n),wn=s(qe," from a pretrained pytorch model and preprocessor."),qe.forEach(n),$e.forEach(n),En=c(b),L=o(b,"DIV",{class:!0});var Re=r(L);f(dt.$$.fragment,Re),kn=c(Re),ht=o(Re,"P",{});var Qe=r(ht);Nn=s(Qe,"Create the calibration "),St=o(Qe,"CODE",{});var va=r(St);jn=s(va,"datasets.Dataset"),va.forEach(n),Dn=s(Qe," to use for the post-training static quantization calibration step"),Qe.forEach(n),Re.forEach(n),Pn=c(b),X=o(b,"DIV",{class:!0});var we=r(X);f(ft.$$.fragment,we),Cn=c(we),Mt=o(we,"P",{});var Ta=r(Mt);Fn=s(Ta,"Perform the calibration step and collect the quantization ranges."),Ta.forEach(n),we.forEach(n),b.forEach(n),this.h()},h(){m(O,"name","hf:doc:metadata"),m(O,"content",JSON.stringify(Ea)),m(E,"id","quantization"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#quantization"),m($,"class","relative group"),m(I,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),m(I,"rel","nofollow"),m(k,"id","creating-an-ortquantizer"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#creating-an-ortquantizer"),m(q,"class","relative group"),m(S,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(S,"rel","nofollow"),m(G,"href","https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english"),m(G,"rel","nofollow"),m(W,"start","2"),m(J,"start","3"),m(Y,"start","4"),m(N,"id","dynamic-quantization-example"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#dynamic-quantization-example"),m(R,"class","relative group"),m(nt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(nt,"rel","nofollow"),m(j,"id","static-quantization-example"),m(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(j,"href","#static-quantization-example"),m(Q,"class","relative group"),m(D,"id","optimum.onnxruntime.ORTQuantizer"),m(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(D,"href","#optimum.onnxruntime.ORTQuantizer"),m(w,"class","relative group"),m(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,l){e(document.head,O),u(t,Gt,l),u(t,$,l),e($,E),e(E,Ot),g(A,Ot,null),e($,Ee),e($,$t),e($t,ke),u(t,Bt,l),u(t,v,l),e(v,Ne),e(v,qt),e(qt,je),e(v,De),e(v,I),e(I,Pe),e(v,Ce),u(t,Jt,l),u(t,q,l),e(q,k),e(k,Rt),g(U,Rt,null),e(q,Fe),e(q,_t),e(_t,Le),e(_t,Qt),e(Qt,Xe),u(t,Kt,l),u(t,T,l),e(T,Ae),e(T,wt),e(wt,Ie),e(T,Ue),e(T,Et),e(Et,He),e(T,Se),u(t,Yt,l),u(t,zt,l),e(zt,H),e(H,Me),e(H,S),e(S,We),e(H,Ve),u(t,Zt,l),g(M,t,l),u(t,te,l),u(t,W,l),e(W,V),e(V,Ge),e(V,G),e(G,Be),e(V,Je),u(t,ee,l),g(B,t,l),u(t,ne,l),u(t,J,l),e(J,kt),e(kt,Ke),u(t,ae,l),g(K,t,l),u(t,oe,l),u(t,Y,l),e(Y,Z),e(Z,Ye),e(Z,Nt),e(Nt,Ze),e(Z,tn),u(t,re,l),g(tt,t,l),u(t,ie,l),u(t,R,l),e(R,N),e(N,jt),g(et,jt,null),e(R,en),e(R,Dt),e(Dt,nn),u(t,se,l),u(t,y,l),e(y,an),e(y,Pt),e(Pt,on),e(y,rn),e(y,nt),e(nt,sn),e(y,ln),u(t,le,l),g(at,t,l),u(t,me,l),u(t,Q,l),e(Q,j),e(j,Ct),g(ot,Ct,null),e(Q,mn),e(Q,Ft),e(Ft,pn),u(t,pe,l),u(t,xt,l),e(xt,cn),u(t,ce,l),u(t,w,l),e(w,D),e(D,Lt),g(rt,Lt,null),e(w,un),e(w,Xt),e(Xt,dn),u(t,ue,l),u(t,d,l),g(it,d,null),e(d,hn),e(d,At),e(At,fn),e(d,gn),e(d,bt),g(st,bt,null),e(d,_n),e(d,P),g(lt,P,null),e(P,zn),e(P,mt),e(mt,xn),e(mt,It),e(It,bn),e(mt,vn),e(d,Tn),e(d,C),g(pt,C,null),e(C,yn),e(C,Ut),e(Ut,On),e(d,$n),e(d,F),g(ct,F,null),e(F,qn),e(F,ut),e(ut,Rn),e(ut,Ht),e(Ht,Qn),e(ut,wn),e(d,En),e(d,L),g(dt,L,null),e(L,kn),e(L,ht),e(ht,Nn),e(ht,St),e(St,jn),e(ht,Dn),e(d,Pn),e(d,X),g(ft,X,null),e(X,Cn),e(X,Mt),e(Mt,Fn),de=!0},p:Ra,i(t){de||(_(A.$$.fragment,t),_(U.$$.fragment,t),_(M.$$.fragment,t),_(B.$$.fragment,t),_(K.$$.fragment,t),_(tt.$$.fragment,t),_(et.$$.fragment,t),_(at.$$.fragment,t),_(ot.$$.fragment,t),_(rt.$$.fragment,t),_(it.$$.fragment,t),_(st.$$.fragment,t),_(lt.$$.fragment,t),_(pt.$$.fragment,t),_(ct.$$.fragment,t),_(dt.$$.fragment,t),_(ft.$$.fragment,t),de=!0)},o(t){z(A.$$.fragment,t),z(U.$$.fragment,t),z(M.$$.fragment,t),z(B.$$.fragment,t),z(K.$$.fragment,t),z(tt.$$.fragment,t),z(et.$$.fragment,t),z(at.$$.fragment,t),z(ot.$$.fragment,t),z(rt.$$.fragment,t),z(it.$$.fragment,t),z(st.$$.fragment,t),z(lt.$$.fragment,t),z(pt.$$.fragment,t),z(ct.$$.fragment,t),z(dt.$$.fragment,t),z(ft.$$.fragment,t),de=!1},d(t){n(O),t&&n(Gt),t&&n($),x(A),t&&n(Bt),t&&n(v),t&&n(Jt),t&&n(q),x(U),t&&n(Kt),t&&n(T),t&&n(Yt),t&&n(zt),t&&n(Zt),x(M,t),t&&n(te),t&&n(W),t&&n(ee),x(B,t),t&&n(ne),t&&n(J),t&&n(ae),x(K,t),t&&n(oe),t&&n(Y),t&&n(re),x(tt,t),t&&n(ie),t&&n(R),x(et),t&&n(se),t&&n(y),t&&n(le),x(at,t),t&&n(me),t&&n(Q),x(ot),t&&n(pe),t&&n(xt),t&&n(ce),t&&n(w),x(rt),t&&n(ue),t&&n(d),x(it),x(st),x(lt),x(pt),x(ct),x(dt),x(ft)}}}const Ea={local:"quantization",sections:[{local:"creating-an-ortquantizer",title:"Creating an `ORTQuantizer`"},{local:"dynamic-quantization-example",title:"Dynamic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function ka(Xn){return Qa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ca extends ya{constructor(O){super();Oa(this,O,ka,wa,$a,{})}}export{Ca as default,Ea as metadata};
