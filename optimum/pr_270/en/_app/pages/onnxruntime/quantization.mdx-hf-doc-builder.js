import{S as co,i as uo,s as ho,e as n,k as m,w as h,t as i,M as fo,c as o,d as a,m as p,a as s,x as f,h as r,b as c,G as t,g as u,y as g,L as go,q as _,o as z,B as q,v as _o}from"../../chunks/vendor-hf-doc-builder.js";import{D as xe}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ye}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ke}from"../../chunks/IconCopyLink-hf-doc-builder.js";function zo(zn){let O,pt,Q,C,Ce,H,Kt,Ne,Yt,ct,x,Zt,De,ea,ta,W,aa,na,ut,w,N,Fe,B,oa,Te,sa,Se,ia,dt,y,ra,Ae,la,ma,Pe,pa,ca,ht,$e,V,ua,Le,da,ha,ft,G,gt,J,Me,fa,_t,K,zt,R,D,Xe,Y,ga,Ie,_a,qt,T,za,Ue,qa,ba,Z,va,xa,bt,ee,vt,j,F,He,te,ya,We,Ta,xt,$,$a,Be,Oa,Qa,ae,wa,Ra,yt,ne,Tt,E,S,Ve,oe,ja,Ge,Ea,$t,v,ka,Je,Ca,Na,Ke,Da,Fa,Ye,Sa,Aa,Ot,Oe,se,Pa,Ze,La,Ma,Qt,ie,wt,re,et,Xa,Rt,le,jt,me,tt,Ia,Et,pe,kt,k,A,at,ce,Ua,nt,Ha,Ct,d,ue,Wa,ot,Ba,Va,P,de,Ga,st,Ja,Ka,Qe,he,Ya,L,fe,Za,ge,en,it,tn,an,nn,M,_e,on,ze,sn,rt,rn,ln,mn,X,qe,pn,be,cn,lt,un,dn,hn,I,ve,fn,mt,gn,Nt;return H=new ke({}),B=new ke({}),G=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForTextClassification

ort_model = ORTModelForTextClassification.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english")

quantizer = ORTQuantizer.from_pretrained(ort_model)

...

quantizer.fit(...)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForTextClassification

<span class="hljs-comment"># Loading ONNX Model from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForTextClassification.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)

<span class="hljs-comment"># Create a quantizer from a ORTModelForXXX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(ort_model)

<span class="hljs-comment"># Configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>...

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.fit(...)`}}),K=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("path/to/model")

...

quantizer.fit(...)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx exists in path/to/model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)

<span class="hljs-comment"># Configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>...

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.fit(...)`}}),Y=new ke({}),ee=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"
onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)

quantizer = ORTQuantizer.from_pretrained(onnx_model)

dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

model_quantized_path = quantizer.fit(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-comment"># Load PyTorch model and convert to ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Create quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.fit(
    output_path=<span class="hljs-string">&quot;path/to/output/model&quot;</span>,
    quantization_config=dqconfig,
)`}}),te=new ke({}),ne=new ye({props:{code:`from functools import partial
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoCalibrationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantizer = ORTQuantizer.from_pretrained(onnx_model)
qconfig = AutoQuantizationConfig.arm64(is_static=True, per_channel=False)

def preprocess_fn(ex, tokenizer):

calibration_dataset = quantizer.get_calibration_dataset(
calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)

ranges = quantizer.calibrate(

model_quantized_path = quantizer.fit(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig, AutoCalibrationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># Load PyTorch model and convert to ONNX and create Quantizer and setup config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)
<span class="hljs-meta">&gt;&gt;&gt; </span>qconfig = AutoQuantizationConfig.arm64(is_static=<span class="hljs-literal">True</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Create the calibration dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_fn</span>(<span class="hljs-params">ex, tokenizer</span>):
    <span class="hljs-keyword">return</span> tokenizer(ex[<span class="hljs-string">&quot;sentence&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>calibration_dataset = quantizer.get_calibration_dataset(
    <span class="hljs-string">&quot;glue&quot;</span>,
    dataset_config_name=<span class="hljs-string">&quot;sst2&quot;</span>,
    preprocess_function=partial(preprocess_fn, tokenizer=tokenizer),
    num_samples=<span class="hljs-number">50</span>,
    dataset_split=<span class="hljs-string">&quot;train&quot;</span>,
)
<span class="hljs-comment"># Create the calibration configuration containing the parameters related to calibration.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)

<span class="hljs-comment"># Perform the calibration step: computes the activations quantization ranges</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ranges = quantizer.calibrate(
    dataset=calibration_dataset,
    calibration_config=calibration_config,
    operators_to_quantize=qconfig.operators_to_quantize,
)

<span class="hljs-comment"># Apply static quantization on the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.fit(
    output_path=<span class="hljs-string">&quot;path/to/output/model&quot;</span>,
    calibration_tensors_range=ranges,
    quantization_config=qconfig,
)`}}),oe=new ke({}),ie=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSeq2SeqLM
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "optimum/t5-small"
onnx_model = ORTModelForSeq2SeqLM.from_pretrained(model_id)
model_path = onnx_model.model_save_dir`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-comment"># load Seq2Seq model and set model file directory</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;optimum/t5-small&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSeq2SeqLM.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_path = onnx_model.model_save_dir`}}),le=new ye({props:{code:`encoder_quantizer = ORTQuantizer.from_pretrained(model_path, file_name="encoder_model.onnx")

decoder_quantizer = ORTQuantizer.from_pretrained(model_path, file_name="decoder_model.onnx")

decoder_wp_quantizer = ORTQuantizer.from_pretrained(model_path, file_name="decoder_with_past_model.onnx")

quantizer = [encoder_quantizer, decoder_quantizer, decoder_wp_quantizer]`,highlighted:`<span class="hljs-comment"># Create encoder quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_quantizer = ORTQuantizer.from_pretrained(model_path, file_name=<span class="hljs-string">&quot;encoder_model.onnx&quot;</span>)

<span class="hljs-comment"># Create decoder quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_quantizer = ORTQuantizer.from_pretrained(model_path, file_name=<span class="hljs-string">&quot;decoder_model.onnx&quot;</span>)

<span class="hljs-comment"># Create decoder with past key values quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_wp_quantizer = ORTQuantizer.from_pretrained(model_path, file_name=<span class="hljs-string">&quot;decoder_with_past_model.onnx&quot;</span>)

<span class="hljs-comment"># Create Quantizer list</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = [encoder_quantizer, decoder_quantizer, decoder_wp_quantizer]`}}),pe=new ye({props:{code:`dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

[q.fit(output_path=".",quantization_config=dqconfig) for q in quantizer]`,highlighted:`<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>[q.fit(output_path=<span class="hljs-string">&quot;.&quot;</span>,quantization_config=dqconfig) <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> quantizer]`}}),ce=new ke({}),ue=new xe({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": typing.List[pathlib.Path]"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L79"}}),de=new xe({props:{name:"calibrate",anchor:"optimum.onnxruntime.ORTQuantizer.calibrate",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.calibrate.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L138",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),he=new xe({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L245",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),fe=new xe({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"output_path",val:": typing.Union[str, pathlib.Path]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"file_prefix",val:": typing.Optional[str] = 'quantized'"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.output_path",description:`<strong>output_path</strong> (<code>Union[str, Path]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.file_prefix",description:`<strong>file_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;quantized&quot;</code>) &#x2014;
The prefix used to save the quantized model.`,name:"file_prefix"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L258",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),_e=new xe({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_or_path",val:": typing.Union[str, pathlib.Path]"},{name:"file_name",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_or_path",description:`<strong>model_or_path</strong> (<code>Union[str, Path]</code>) &#x2014;
Can be either:<ul>
<li>A path to a saved exported ONNX Intermediate Representation (IR) model, e.g., \`./my_model_directory/.</li>
<li>Or a <code>ORTModelForXX</code> class, e.g., <code>ORTModelForQuestionAnswering</code>.</li>
</ul>`,name:"model_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(`Union[str,",description:"<strong>file_name(`Union[str,</strong> List[str]]<code>, *optional*) -- Overwrites the default model file name from </code>&#x201C;model.onnx&#x201D;<code>to</code>file_name`.\nThis allows you to load different model files from the same repository or directory.",name:"file_name(`Union[str,"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L94",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),qe=new xe({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"},{name:"use_auth_token",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the token generated when running <code>transformers-cli login</code> (necessary for some datasets
like ImageNet).`,name:"use_auth_token"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L350",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),ve=new xe({props:{name:"partial_calibrate",anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_calibrate.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/vr_270/src/optimum/onnxruntime/quantization.py#L192",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){O=n("meta"),pt=m(),Q=n("h1"),C=n("a"),Ce=n("span"),h(H.$$.fragment),Kt=m(),Ne=n("span"),Yt=i("Quantization"),ct=m(),x=n("p"),Zt=i("\u{1F917} Optimum provides an "),De=n("code"),ea=i("optimum.onnxruntime"),ta=i(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),W=n("a"),aa=i("ONNX Runtime"),na=i(" quantization tool."),ut=m(),w=n("h2"),N=n("a"),Fe=n("span"),h(B.$$.fragment),oa=m(),Te=n("span"),sa=i("Creating an "),Se=n("code"),ia=i("ORTQuantizer"),dt=m(),y=n("p"),ra=i("The "),Ae=n("code"),la=i("ORTQuantizer"),ma=i(" class is used to quantize your ONNX model. The class can be initialized using the "),Pe=n("code"),pa=i("from_pretrained()"),ca=i(" method, which supports different checkpoint formats."),ht=m(),$e=n("ol"),V=n("li"),ua=i("Using an already initialized "),Le=n("code"),da=i("ORTModelForXXX"),ha=i(" class."),ft=m(),h(G.$$.fragment),gt=m(),J=n("ol"),Me=n("li"),fa=i("Using a local ONNX model from a directory."),_t=m(),h(K.$$.fragment),zt=m(),R=n("h2"),D=n("a"),Xe=n("span"),h(Y.$$.fragment),ga=m(),Ie=n("span"),_a=i("Dynamic Quantization example"),qt=m(),T=n("p"),za=i("The "),Ue=n("code"),qa=i("ORTQuantizer"),ba=i(" class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),Z=n("a"),va=i("distilbert-base-uncased-finetuned-sst-2-english"),xa=i("."),bt=m(),h(ee.$$.fragment),vt=m(),j=n("h2"),F=n("a"),He=n("span"),h(te.$$.fragment),ya=m(),We=n("span"),Ta=i("Static Quantization example"),xt=m(),$=n("p"),$a=i("The "),Be=n("code"),Oa=i("ORTQuantizer"),Qa=i(" class can be used to statically quantize your ONNX model. Below you will find an easy end-to-end example on how to statically quantize "),ae=n("a"),wa=i("distilbert-base-uncased-finetuned-sst-2-english"),Ra=i("."),yt=m(),h(ne.$$.fragment),Tt=m(),E=n("h2"),S=n("a"),Ve=n("span"),h(oe.$$.fragment),ja=m(),Ge=n("span"),Ea=i("Quantize Seq2Seq models."),$t=m(),v=n("p"),ka=i("The "),Je=n("code"),Ca=i("ORTQuantizer"),Na=i(" currently doesn\u2019t support multi-file models, like "),Ke=n("code"),Da=i("ORTModelForSeq2SeqLM"),Fa=i(". If you want to quantize (statically/dynamically) a seq2seq model, you have to quantize each model individually using the "),Ye=n("code"),Sa=i("ORTQuantizer"),Aa=i(` class.
Below you can find a simple example on how to quantize a seq2seq model.`),Ot=m(),Oe=n("ol"),se=n("li"),Pa=i("load seq2seq model as "),Ze=n("code"),La=i("ORTModelForSeq2SeqLM"),Ma=i("."),Qt=m(),h(ie.$$.fragment),wt=m(),re=n("ol"),et=n("li"),Xa=i("Define Quantizer for encoder, decoder and decoder with past keys"),Rt=m(),h(le.$$.fragment),jt=m(),me=n("ol"),tt=n("li"),Ia=i("Quantize all models"),Et=m(),h(pe.$$.fragment),kt=m(),k=n("h2"),A=n("a"),at=n("span"),h(ce.$$.fragment),Ua=m(),nt=n("span"),Ha=i("ORTQuantizer"),Ct=m(),d=n("div"),h(ue.$$.fragment),Wa=m(),ot=n("p"),Ba=i("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),Va=m(),P=n("div"),h(de.$$.fragment),Ga=m(),st=n("p"),Ja=i("Perform the calibration step and collect the quantization ranges."),Ka=m(),Qe=n("div"),h(he.$$.fragment),Ya=m(),L=n("div"),h(fe.$$.fragment),Za=m(),ge=n("p"),en=i("Quantize a model given the optimization specifications defined in "),it=n("code"),tn=i("quantization_config"),an=i("."),nn=m(),M=n("div"),h(_e.$$.fragment),on=m(),ze=n("p"),sn=i("Instantiate a "),rt=n("code"),rn=i("ORTQuantizer"),ln=i(" from a pretrained pytorch model and preprocessor."),mn=m(),X=n("div"),h(qe.$$.fragment),pn=m(),be=n("p"),cn=i("Create the calibration "),lt=n("code"),un=i("datasets.Dataset"),dn=i(" to use for the post-training static quantization calibration step"),hn=m(),I=n("div"),h(ve.$$.fragment),fn=m(),mt=n("p"),gn=i("Perform the calibration step and collect the quantization ranges."),this.h()},l(e){const l=fo('[data-svelte="svelte-1phssyn"]',document.head);O=o(l,"META",{name:!0,content:!0}),l.forEach(a),pt=p(e),Q=o(e,"H1",{class:!0});var Dt=s(Q);C=o(Dt,"A",{id:!0,class:!0,href:!0});var qn=s(C);Ce=o(qn,"SPAN",{});var bn=s(Ce);f(H.$$.fragment,bn),bn.forEach(a),qn.forEach(a),Kt=p(Dt),Ne=o(Dt,"SPAN",{});var vn=s(Ne);Yt=r(vn,"Quantization"),vn.forEach(a),Dt.forEach(a),ct=p(e),x=o(e,"P",{});var we=s(x);Zt=r(we,"\u{1F917} Optimum provides an "),De=o(we,"CODE",{});var xn=s(De);ea=r(xn,"optimum.onnxruntime"),xn.forEach(a),ta=r(we," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),W=o(we,"A",{href:!0,rel:!0});var yn=s(W);aa=r(yn,"ONNX Runtime"),yn.forEach(a),na=r(we," quantization tool."),we.forEach(a),ut=p(e),w=o(e,"H2",{class:!0});var Ft=s(w);N=o(Ft,"A",{id:!0,class:!0,href:!0});var Tn=s(N);Fe=o(Tn,"SPAN",{});var $n=s(Fe);f(B.$$.fragment,$n),$n.forEach(a),Tn.forEach(a),oa=p(Ft),Te=o(Ft,"SPAN",{});var _n=s(Te);sa=r(_n,"Creating an "),Se=o(_n,"CODE",{});var On=s(Se);ia=r(On,"ORTQuantizer"),On.forEach(a),_n.forEach(a),Ft.forEach(a),dt=p(e),y=o(e,"P",{});var Re=s(y);ra=r(Re,"The "),Ae=o(Re,"CODE",{});var Qn=s(Ae);la=r(Qn,"ORTQuantizer"),Qn.forEach(a),ma=r(Re," class is used to quantize your ONNX model. The class can be initialized using the "),Pe=o(Re,"CODE",{});var wn=s(Pe);pa=r(wn,"from_pretrained()"),wn.forEach(a),ca=r(Re," method, which supports different checkpoint formats."),Re.forEach(a),ht=p(e),$e=o(e,"OL",{});var Rn=s($e);V=o(Rn,"LI",{});var St=s(V);ua=r(St,"Using an already initialized "),Le=o(St,"CODE",{});var jn=s(Le);da=r(jn,"ORTModelForXXX"),jn.forEach(a),ha=r(St," class."),St.forEach(a),Rn.forEach(a),ft=p(e),f(G.$$.fragment,e),gt=p(e),J=o(e,"OL",{start:!0});var En=s(J);Me=o(En,"LI",{});var kn=s(Me);fa=r(kn,"Using a local ONNX model from a directory."),kn.forEach(a),En.forEach(a),_t=p(e),f(K.$$.fragment,e),zt=p(e),R=o(e,"H2",{class:!0});var At=s(R);D=o(At,"A",{id:!0,class:!0,href:!0});var Cn=s(D);Xe=o(Cn,"SPAN",{});var Nn=s(Xe);f(Y.$$.fragment,Nn),Nn.forEach(a),Cn.forEach(a),ga=p(At),Ie=o(At,"SPAN",{});var Dn=s(Ie);_a=r(Dn,"Dynamic Quantization example"),Dn.forEach(a),At.forEach(a),qt=p(e),T=o(e,"P",{});var je=s(T);za=r(je,"The "),Ue=o(je,"CODE",{});var Fn=s(Ue);qa=r(Fn,"ORTQuantizer"),Fn.forEach(a),ba=r(je," class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),Z=o(je,"A",{href:!0,rel:!0});var Sn=s(Z);va=r(Sn,"distilbert-base-uncased-finetuned-sst-2-english"),Sn.forEach(a),xa=r(je,"."),je.forEach(a),bt=p(e),f(ee.$$.fragment,e),vt=p(e),j=o(e,"H2",{class:!0});var Pt=s(j);F=o(Pt,"A",{id:!0,class:!0,href:!0});var An=s(F);He=o(An,"SPAN",{});var Pn=s(He);f(te.$$.fragment,Pn),Pn.forEach(a),An.forEach(a),ya=p(Pt),We=o(Pt,"SPAN",{});var Ln=s(We);Ta=r(Ln,"Static Quantization example"),Ln.forEach(a),Pt.forEach(a),xt=p(e),$=o(e,"P",{});var Ee=s($);$a=r(Ee,"The "),Be=o(Ee,"CODE",{});var Mn=s(Be);Oa=r(Mn,"ORTQuantizer"),Mn.forEach(a),Qa=r(Ee," class can be used to statically quantize your ONNX model. Below you will find an easy end-to-end example on how to statically quantize "),ae=o(Ee,"A",{href:!0,rel:!0});var Xn=s(ae);wa=r(Xn,"distilbert-base-uncased-finetuned-sst-2-english"),Xn.forEach(a),Ra=r(Ee,"."),Ee.forEach(a),yt=p(e),f(ne.$$.fragment,e),Tt=p(e),E=o(e,"H2",{class:!0});var Lt=s(E);S=o(Lt,"A",{id:!0,class:!0,href:!0});var In=s(S);Ve=o(In,"SPAN",{});var Un=s(Ve);f(oe.$$.fragment,Un),Un.forEach(a),In.forEach(a),ja=p(Lt),Ge=o(Lt,"SPAN",{});var Hn=s(Ge);Ea=r(Hn,"Quantize Seq2Seq models."),Hn.forEach(a),Lt.forEach(a),$t=p(e),v=o(e,"P",{});var U=s(v);ka=r(U,"The "),Je=o(U,"CODE",{});var Wn=s(Je);Ca=r(Wn,"ORTQuantizer"),Wn.forEach(a),Na=r(U," currently doesn\u2019t support multi-file models, like "),Ke=o(U,"CODE",{});var Bn=s(Ke);Da=r(Bn,"ORTModelForSeq2SeqLM"),Bn.forEach(a),Fa=r(U,". If you want to quantize (statically/dynamically) a seq2seq model, you have to quantize each model individually using the "),Ye=o(U,"CODE",{});var Vn=s(Ye);Sa=r(Vn,"ORTQuantizer"),Vn.forEach(a),Aa=r(U,` class.
Below you can find a simple example on how to quantize a seq2seq model.`),U.forEach(a),Ot=p(e),Oe=o(e,"OL",{});var Gn=s(Oe);se=o(Gn,"LI",{});var Mt=s(se);Pa=r(Mt,"load seq2seq model as "),Ze=o(Mt,"CODE",{});var Jn=s(Ze);La=r(Jn,"ORTModelForSeq2SeqLM"),Jn.forEach(a),Ma=r(Mt,"."),Mt.forEach(a),Gn.forEach(a),Qt=p(e),f(ie.$$.fragment,e),wt=p(e),re=o(e,"OL",{start:!0});var Kn=s(re);et=o(Kn,"LI",{});var Yn=s(et);Xa=r(Yn,"Define Quantizer for encoder, decoder and decoder with past keys"),Yn.forEach(a),Kn.forEach(a),Rt=p(e),f(le.$$.fragment,e),jt=p(e),me=o(e,"OL",{start:!0});var Zn=s(me);tt=o(Zn,"LI",{});var eo=s(tt);Ia=r(eo,"Quantize all models"),eo.forEach(a),Zn.forEach(a),Et=p(e),f(pe.$$.fragment,e),kt=p(e),k=o(e,"H2",{class:!0});var Xt=s(k);A=o(Xt,"A",{id:!0,class:!0,href:!0});var to=s(A);at=o(to,"SPAN",{});var ao=s(at);f(ce.$$.fragment,ao),ao.forEach(a),to.forEach(a),Ua=p(Xt),nt=o(Xt,"SPAN",{});var no=s(nt);Ha=r(no,"ORTQuantizer"),no.forEach(a),Xt.forEach(a),Ct=p(e),d=o(e,"DIV",{class:!0});var b=s(d);f(ue.$$.fragment,b),Wa=p(b),ot=o(b,"P",{});var oo=s(ot);Ba=r(oo,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),oo.forEach(a),Va=p(b),P=o(b,"DIV",{class:!0});var It=s(P);f(de.$$.fragment,It),Ga=p(It),st=o(It,"P",{});var so=s(st);Ja=r(so,"Perform the calibration step and collect the quantization ranges."),so.forEach(a),It.forEach(a),Ka=p(b),Qe=o(b,"DIV",{class:!0});var io=s(Qe);f(he.$$.fragment,io),io.forEach(a),Ya=p(b),L=o(b,"DIV",{class:!0});var Ut=s(L);f(fe.$$.fragment,Ut),Za=p(Ut),ge=o(Ut,"P",{});var Ht=s(ge);en=r(Ht,"Quantize a model given the optimization specifications defined in "),it=o(Ht,"CODE",{});var ro=s(it);tn=r(ro,"quantization_config"),ro.forEach(a),an=r(Ht,"."),Ht.forEach(a),Ut.forEach(a),nn=p(b),M=o(b,"DIV",{class:!0});var Wt=s(M);f(_e.$$.fragment,Wt),on=p(Wt),ze=o(Wt,"P",{});var Bt=s(ze);sn=r(Bt,"Instantiate a "),rt=o(Bt,"CODE",{});var lo=s(rt);rn=r(lo,"ORTQuantizer"),lo.forEach(a),ln=r(Bt," from a pretrained pytorch model and preprocessor."),Bt.forEach(a),Wt.forEach(a),mn=p(b),X=o(b,"DIV",{class:!0});var Vt=s(X);f(qe.$$.fragment,Vt),pn=p(Vt),be=o(Vt,"P",{});var Gt=s(be);cn=r(Gt,"Create the calibration "),lt=o(Gt,"CODE",{});var mo=s(lt);un=r(mo,"datasets.Dataset"),mo.forEach(a),dn=r(Gt," to use for the post-training static quantization calibration step"),Gt.forEach(a),Vt.forEach(a),hn=p(b),I=o(b,"DIV",{class:!0});var Jt=s(I);f(ve.$$.fragment,Jt),fn=p(Jt),mt=o(Jt,"P",{});var po=s(mt);gn=r(po,"Perform the calibration step and collect the quantization ranges."),po.forEach(a),Jt.forEach(a),b.forEach(a),this.h()},h(){c(O,"name","hf:doc:metadata"),c(O,"content",JSON.stringify(qo)),c(C,"id","quantization"),c(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(C,"href","#quantization"),c(Q,"class","relative group"),c(W,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),c(W,"rel","nofollow"),c(N,"id","creating-an-ortquantizer"),c(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N,"href","#creating-an-ortquantizer"),c(w,"class","relative group"),c(J,"start","2"),c(D,"id","dynamic-quantization-example"),c(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D,"href","#dynamic-quantization-example"),c(R,"class","relative group"),c(Z,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),c(Z,"rel","nofollow"),c(F,"id","static-quantization-example"),c(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(F,"href","#static-quantization-example"),c(j,"class","relative group"),c(ae,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),c(ae,"rel","nofollow"),c(S,"id","quantize-seq2seq-models"),c(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S,"href","#quantize-seq2seq-models"),c(E,"class","relative group"),c(re,"start","2"),c(me,"start","3"),c(A,"id","optimum.onnxruntime.ORTQuantizer"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#optimum.onnxruntime.ORTQuantizer"),c(k,"class","relative group"),c(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,l){t(document.head,O),u(e,pt,l),u(e,Q,l),t(Q,C),t(C,Ce),g(H,Ce,null),t(Q,Kt),t(Q,Ne),t(Ne,Yt),u(e,ct,l),u(e,x,l),t(x,Zt),t(x,De),t(De,ea),t(x,ta),t(x,W),t(W,aa),t(x,na),u(e,ut,l),u(e,w,l),t(w,N),t(N,Fe),g(B,Fe,null),t(w,oa),t(w,Te),t(Te,sa),t(Te,Se),t(Se,ia),u(e,dt,l),u(e,y,l),t(y,ra),t(y,Ae),t(Ae,la),t(y,ma),t(y,Pe),t(Pe,pa),t(y,ca),u(e,ht,l),u(e,$e,l),t($e,V),t(V,ua),t(V,Le),t(Le,da),t(V,ha),u(e,ft,l),g(G,e,l),u(e,gt,l),u(e,J,l),t(J,Me),t(Me,fa),u(e,_t,l),g(K,e,l),u(e,zt,l),u(e,R,l),t(R,D),t(D,Xe),g(Y,Xe,null),t(R,ga),t(R,Ie),t(Ie,_a),u(e,qt,l),u(e,T,l),t(T,za),t(T,Ue),t(Ue,qa),t(T,ba),t(T,Z),t(Z,va),t(T,xa),u(e,bt,l),g(ee,e,l),u(e,vt,l),u(e,j,l),t(j,F),t(F,He),g(te,He,null),t(j,ya),t(j,We),t(We,Ta),u(e,xt,l),u(e,$,l),t($,$a),t($,Be),t(Be,Oa),t($,Qa),t($,ae),t(ae,wa),t($,Ra),u(e,yt,l),g(ne,e,l),u(e,Tt,l),u(e,E,l),t(E,S),t(S,Ve),g(oe,Ve,null),t(E,ja),t(E,Ge),t(Ge,Ea),u(e,$t,l),u(e,v,l),t(v,ka),t(v,Je),t(Je,Ca),t(v,Na),t(v,Ke),t(Ke,Da),t(v,Fa),t(v,Ye),t(Ye,Sa),t(v,Aa),u(e,Ot,l),u(e,Oe,l),t(Oe,se),t(se,Pa),t(se,Ze),t(Ze,La),t(se,Ma),u(e,Qt,l),g(ie,e,l),u(e,wt,l),u(e,re,l),t(re,et),t(et,Xa),u(e,Rt,l),g(le,e,l),u(e,jt,l),u(e,me,l),t(me,tt),t(tt,Ia),u(e,Et,l),g(pe,e,l),u(e,kt,l),u(e,k,l),t(k,A),t(A,at),g(ce,at,null),t(k,Ua),t(k,nt),t(nt,Ha),u(e,Ct,l),u(e,d,l),g(ue,d,null),t(d,Wa),t(d,ot),t(ot,Ba),t(d,Va),t(d,P),g(de,P,null),t(P,Ga),t(P,st),t(st,Ja),t(d,Ka),t(d,Qe),g(he,Qe,null),t(d,Ya),t(d,L),g(fe,L,null),t(L,Za),t(L,ge),t(ge,en),t(ge,it),t(it,tn),t(ge,an),t(d,nn),t(d,M),g(_e,M,null),t(M,on),t(M,ze),t(ze,sn),t(ze,rt),t(rt,rn),t(ze,ln),t(d,mn),t(d,X),g(qe,X,null),t(X,pn),t(X,be),t(be,cn),t(be,lt),t(lt,un),t(be,dn),t(d,hn),t(d,I),g(ve,I,null),t(I,fn),t(I,mt),t(mt,gn),Nt=!0},p:go,i(e){Nt||(_(H.$$.fragment,e),_(B.$$.fragment,e),_(G.$$.fragment,e),_(K.$$.fragment,e),_(Y.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),_(ce.$$.fragment,e),_(ue.$$.fragment,e),_(de.$$.fragment,e),_(he.$$.fragment,e),_(fe.$$.fragment,e),_(_e.$$.fragment,e),_(qe.$$.fragment,e),_(ve.$$.fragment,e),Nt=!0)},o(e){z(H.$$.fragment,e),z(B.$$.fragment,e),z(G.$$.fragment,e),z(K.$$.fragment,e),z(Y.$$.fragment,e),z(ee.$$.fragment,e),z(te.$$.fragment,e),z(ne.$$.fragment,e),z(oe.$$.fragment,e),z(ie.$$.fragment,e),z(le.$$.fragment,e),z(pe.$$.fragment,e),z(ce.$$.fragment,e),z(ue.$$.fragment,e),z(de.$$.fragment,e),z(he.$$.fragment,e),z(fe.$$.fragment,e),z(_e.$$.fragment,e),z(qe.$$.fragment,e),z(ve.$$.fragment,e),Nt=!1},d(e){a(O),e&&a(pt),e&&a(Q),q(H),e&&a(ct),e&&a(x),e&&a(ut),e&&a(w),q(B),e&&a(dt),e&&a(y),e&&a(ht),e&&a($e),e&&a(ft),q(G,e),e&&a(gt),e&&a(J),e&&a(_t),q(K,e),e&&a(zt),e&&a(R),q(Y),e&&a(qt),e&&a(T),e&&a(bt),q(ee,e),e&&a(vt),e&&a(j),q(te),e&&a(xt),e&&a($),e&&a(yt),q(ne,e),e&&a(Tt),e&&a(E),q(oe),e&&a($t),e&&a(v),e&&a(Ot),e&&a(Oe),e&&a(Qt),q(ie,e),e&&a(wt),e&&a(re),e&&a(Rt),q(le,e),e&&a(jt),e&&a(me),e&&a(Et),q(pe,e),e&&a(kt),e&&a(k),q(ce),e&&a(Ct),e&&a(d),q(ue),q(de),q(he),q(fe),q(_e),q(qe),q(ve)}}}const qo={local:"quantization",sections:[{local:"creating-an-ortquantizer",title:"Creating an `ORTQuantizer`"},{local:"dynamic-quantization-example",title:"Dynamic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"quantize-seq2seq-models",title:"Quantize Seq2Seq models."},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function bo(zn){return _o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $o extends co{constructor(O){super();uo(this,O,bo,zo,ho,{})}}export{$o as default,qo as metadata};
