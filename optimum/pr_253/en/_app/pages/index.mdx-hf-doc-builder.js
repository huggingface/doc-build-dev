import{S as nl,i as il,s as sl,e as a,k as h,w as de,t as i,M as hl,c as o,d as r,m as f,a as l,x as me,h as s,b as n,G as e,g as c,y as ue,L as fl,q as ge,o as ve,B as we,v as pl}from"../chunks/vendor-hf-doc-builder.js";import{I as ar}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ba}from"../chunks/CodeBlock-hf-doc-builder.js";function cl(Xa){let _,Tt,E,D,ot,M,or,lt,lr,At,_e,nr,Pt,Ee,ir,It,y,N,nt,L,sr,it,hr,Ot,ye,fr,Dt,be,pr,Nt,d,k,Q,cr,dr,B,mr,ur,gr,g,X,vr,wr,F,_r,Er,Y,yr,br,$r,x,j,Tr,Ar,J,Pr,Ir,Or,st,Dr,kt,b,z,ht,K,Nr,ft,kr,xt,$e,xr,zt,v,zr,V,Hr,Cr,W,Rr,Sr,Ht,H,pt,$,Te,Ur,qr,Ae,Gr,Mr,Pe,Lr,Qr,m,T,Ie,Br,Xr,Oe,Fr,Yr,De,jr,Jr,A,Ne,Kr,Vr,ke,Wr,Zr,xe,ea,ta,P,ze,ra,aa,He,oa,la,Ce,na,ia,I,Re,sa,ha,Se,fa,pa,Ue,ca,Ct,O,C,ct,Z,da,dt,ma,Rt,R,ua,mt,ga,va,St,ee,Ut,qe,wa,qt,S,ut,te,Ge,_a,Ea,Me,ya,ba,u,re,Le,ae,$a,Ta,Qe,gt,Aa,Pa,oe,Be,le,Ia,Oa,Xe,vt,Da,Na,ne,Fe,ie,ka,xa,Ye,wt,za,Ha,se,je,he,Ca,Ra,Je,_t,Sa,Gt,Ke,Ua,Mt,fe,Lt,w,qa,Et,Ga,Ma,yt,La,Qa,Qt,pe,Bt;return M=new ar({}),L=new ar({}),K=new ar({}),Z=new ar({}),ee=new Ba({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),fe=new Ba({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),pe=new Ba({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){_=a("meta"),Tt=h(),E=a("h1"),D=a("a"),ot=a("span"),de(M.$$.fragment),or=h(),lt=a("span"),lr=i("\u{1F917} Optimum"),At=h(),_e=a("p"),nr=i("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),Pt=h(),Ee=a("p"),ir=i(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),It=h(),y=a("h2"),N=a("a"),nt=a("span"),de(L.$$.fragment),sr=h(),it=a("span"),hr=i("Integration with Hardware Partners"),Ot=h(),ye=a("p"),fr=i("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Dt=h(),be=a("p"),pr=i("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),Nt=h(),d=a("ul"),k=a("li"),Q=a("a"),cr=i("Graphcore IPUs"),dr=i(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=a("a"),mr=i("More information here"),ur=i("."),gr=h(),g=a("li"),X=a("a"),vr=i("Habana Gaudi Processor (HPU)"),wr=i(" - "),F=a("a"),_r=i("HPUs"),Er=i(" are designed to maximize training throughput and efficiency. "),Y=a("a"),yr=i("More information here"),br=i("."),$r=h(),x=a("li"),j=a("a"),Tr=i("Intel"),Ar=i(" - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information "),J=a("a"),Pr=i("here"),Ir=i("."),Or=h(),st=a("li"),Dr=i("More to come soon! :star:"),kt=h(),b=a("h2"),z=a("a"),ht=a("span"),de(K.$$.fragment),Nr=h(),ft=a("span"),kr=i("Optimizing models towards inference"),xt=h(),$e=a("p"),xr=i(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),zt=h(),v=a("p"),zr=i("Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),V=a("a"),Hr=i("ONNX Runtime"),Cr=i(" along with Intel "),W=a("a"),Rr=i("Neural Compressor"),Sr=i(" (INC)."),Ht=h(),H=a("table"),pt=a("thead"),$=a("tr"),Te=a("th"),Ur=i("Features"),qr=h(),Ae=a("th"),Gr=i("ONNX Runtime"),Mr=h(),Pe=a("th"),Lr=i("Intel Neural Compressor"),Qr=h(),m=a("tbody"),T=a("tr"),Ie=a("td"),Br=i("Post-training Dynamic Quantization"),Xr=h(),Oe=a("td"),Fr=i("\u2705"),Yr=h(),De=a("td"),jr=i("\u2705"),Jr=h(),A=a("tr"),Ne=a("td"),Kr=i("Post-training Static Quantization"),Vr=h(),ke=a("td"),Wr=i("\u2705"),Zr=h(),xe=a("td"),ea=i("\u2705"),ta=h(),P=a("tr"),ze=a("td"),ra=i("Quantization Aware Training (QAT)"),aa=h(),He=a("td"),oa=i("Stay tuned! \u2B50"),la=h(),Ce=a("td"),na=i("\u2705"),ia=h(),I=a("tr"),Re=a("td"),sa=i("Pruning"),ha=h(),Se=a("td"),fa=i("N/A"),pa=h(),Ue=a("td"),ca=i("\u2705"),Ct=h(),O=a("h2"),C=a("a"),ct=a("span"),de(Z.$$.fragment),da=h(),dt=a("span"),ma=i("Installation"),Rt=h(),R=a("p"),ua=i("\u{1F917} Optimum can be installed using "),mt=a("code"),ga=i("pip"),va=i(" as follows:"),St=h(),de(ee.$$.fragment),Ut=h(),qe=a("p"),wa=i("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),qt=h(),S=a("table"),ut=a("thead"),te=a("tr"),Ge=a("th"),_a=i("Accelerator"),Ea=h(),Me=a("th"),ya=i("Installation"),ba=h(),u=a("tbody"),re=a("tr"),Le=a("td"),ae=a("a"),$a=i("ONNX runtime"),Ta=h(),Qe=a("td"),gt=a("code"),Aa=i("python -m pip install optimum[onnxruntime]"),Pa=h(),oe=a("tr"),Be=a("td"),le=a("a"),Ia=i("Intel Neural Compressor (INC)"),Oa=h(),Xe=a("td"),vt=a("code"),Da=i("python -m pip install optimum[intel]"),Na=h(),ne=a("tr"),Fe=a("td"),ie=a("a"),ka=i("Graphcore IPU"),xa=h(),Ye=a("td"),wt=a("code"),za=i("python -m pip install optimum[graphcore]"),Ha=h(),se=a("tr"),je=a("td"),he=a("a"),Ca=i("Habana Gaudi Processor (HPU)"),Ra=h(),Je=a("td"),_t=a("code"),Sa=i("python -m pip install optimum[habana]"),Gt=h(),Ke=a("p"),Ua=i("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Mt=h(),de(fe.$$.fragment),Lt=h(),w=a("p"),qa=i("For the acclerator-specific features, you can install them by appending "),Et=a("code"),Ga=i("#egg=optimum[accelerator_type]"),Ma=i(" to the "),yt=a("code"),La=i("pip"),Qa=i(" command, e.g."),Qt=h(),de(pe.$$.fragment),this.h()},l(t){const p=hl('[data-svelte="svelte-1phssyn"]',document.head);_=o(p,"META",{name:!0,content:!0}),p.forEach(r),Tt=f(t),E=o(t,"H1",{class:!0});var Xt=l(E);D=o(Xt,"A",{id:!0,class:!0,href:!0});var Fa=l(D);ot=o(Fa,"SPAN",{});var Ya=l(ot);me(M.$$.fragment,Ya),Ya.forEach(r),Fa.forEach(r),or=f(Xt),lt=o(Xt,"SPAN",{});var ja=l(lt);lr=s(ja,"\u{1F917} Optimum"),ja.forEach(r),Xt.forEach(r),At=f(t),_e=o(t,"P",{});var Ja=l(_e);nr=s(Ja,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),Ja.forEach(r),Pt=f(t),Ee=o(t,"P",{});var Ka=l(Ee);ir=s(Ka,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Ka.forEach(r),It=f(t),y=o(t,"H2",{class:!0});var Ft=l(y);N=o(Ft,"A",{id:!0,class:!0,href:!0});var Va=l(N);nt=o(Va,"SPAN",{});var Wa=l(nt);me(L.$$.fragment,Wa),Wa.forEach(r),Va.forEach(r),sr=f(Ft),it=o(Ft,"SPAN",{});var Za=l(it);hr=s(Za,"Integration with Hardware Partners"),Za.forEach(r),Ft.forEach(r),Ot=f(t),ye=o(t,"P",{});var eo=l(ye);fr=s(eo,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),eo.forEach(r),Dt=f(t),be=o(t,"P",{});var to=l(be);pr=s(to,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),to.forEach(r),Nt=f(t),d=o(t,"UL",{});var U=l(d);k=o(U,"LI",{});var bt=l(k);Q=o(bt,"A",{href:!0,rel:!0});var ro=l(Q);cr=s(ro,"Graphcore IPUs"),ro.forEach(r),dr=s(bt," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=o(bt,"A",{href:!0,rel:!0});var ao=l(B);mr=s(ao,"More information here"),ao.forEach(r),ur=s(bt,"."),bt.forEach(r),gr=f(U),g=o(U,"LI",{});var ce=l(g);X=o(ce,"A",{href:!0,rel:!0});var oo=l(X);vr=s(oo,"Habana Gaudi Processor (HPU)"),oo.forEach(r),wr=s(ce," - "),F=o(ce,"A",{href:!0,rel:!0});var lo=l(F);_r=s(lo,"HPUs"),lo.forEach(r),Er=s(ce," are designed to maximize training throughput and efficiency. "),Y=o(ce,"A",{href:!0,rel:!0});var no=l(Y);yr=s(no,"More information here"),no.forEach(r),br=s(ce,"."),ce.forEach(r),$r=f(U),x=o(U,"LI",{});var $t=l(x);j=o($t,"A",{href:!0,rel:!0});var io=l(j);Tr=s(io,"Intel"),io.forEach(r),Ar=s($t," - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information "),J=o($t,"A",{href:!0,rel:!0});var so=l(J);Pr=s(so,"here"),so.forEach(r),Ir=s($t,"."),$t.forEach(r),Or=f(U),st=o(U,"LI",{});var ho=l(st);Dr=s(ho,"More to come soon! :star:"),ho.forEach(r),U.forEach(r),kt=f(t),b=o(t,"H2",{class:!0});var Yt=l(b);z=o(Yt,"A",{id:!0,class:!0,href:!0});var fo=l(z);ht=o(fo,"SPAN",{});var po=l(ht);me(K.$$.fragment,po),po.forEach(r),fo.forEach(r),Nr=f(Yt),ft=o(Yt,"SPAN",{});var co=l(ft);kr=s(co,"Optimizing models towards inference"),co.forEach(r),Yt.forEach(r),xt=f(t),$e=o(t,"P",{});var mo=l($e);xr=s(mo,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),mo.forEach(r),zt=f(t),v=o(t,"P",{});var Ve=l(v);zr=s(Ve,"Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),V=o(Ve,"A",{href:!0,rel:!0});var uo=l(V);Hr=s(uo,"ONNX Runtime"),uo.forEach(r),Cr=s(Ve," along with Intel "),W=o(Ve,"A",{href:!0,rel:!0});var go=l(W);Rr=s(go,"Neural Compressor"),go.forEach(r),Sr=s(Ve," (INC)."),Ve.forEach(r),Ht=f(t),H=o(t,"TABLE",{});var jt=l(H);pt=o(jt,"THEAD",{});var vo=l(pt);$=o(vo,"TR",{});var We=l($);Te=o(We,"TH",{align:!0});var wo=l(Te);Ur=s(wo,"Features"),wo.forEach(r),qr=f(We),Ae=o(We,"TH",{align:!0});var _o=l(Ae);Gr=s(_o,"ONNX Runtime"),_o.forEach(r),Mr=f(We),Pe=o(We,"TH",{align:!0});var Eo=l(Pe);Lr=s(Eo,"Intel Neural Compressor"),Eo.forEach(r),We.forEach(r),vo.forEach(r),Qr=f(jt),m=o(jt,"TBODY",{});var q=l(m);T=o(q,"TR",{});var Ze=l(T);Ie=o(Ze,"TD",{align:!0});var yo=l(Ie);Br=s(yo,"Post-training Dynamic Quantization"),yo.forEach(r),Xr=f(Ze),Oe=o(Ze,"TD",{align:!0});var bo=l(Oe);Fr=s(bo,"\u2705"),bo.forEach(r),Yr=f(Ze),De=o(Ze,"TD",{align:!0});var $o=l(De);jr=s($o,"\u2705"),$o.forEach(r),Ze.forEach(r),Jr=f(q),A=o(q,"TR",{});var et=l(A);Ne=o(et,"TD",{align:!0});var To=l(Ne);Kr=s(To,"Post-training Static Quantization"),To.forEach(r),Vr=f(et),ke=o(et,"TD",{align:!0});var Ao=l(ke);Wr=s(Ao,"\u2705"),Ao.forEach(r),Zr=f(et),xe=o(et,"TD",{align:!0});var Po=l(xe);ea=s(Po,"\u2705"),Po.forEach(r),et.forEach(r),ta=f(q),P=o(q,"TR",{});var tt=l(P);ze=o(tt,"TD",{align:!0});var Io=l(ze);ra=s(Io,"Quantization Aware Training (QAT)"),Io.forEach(r),aa=f(tt),He=o(tt,"TD",{align:!0});var Oo=l(He);oa=s(Oo,"Stay tuned! \u2B50"),Oo.forEach(r),la=f(tt),Ce=o(tt,"TD",{align:!0});var Do=l(Ce);na=s(Do,"\u2705"),Do.forEach(r),tt.forEach(r),ia=f(q),I=o(q,"TR",{});var rt=l(I);Re=o(rt,"TD",{align:!0});var No=l(Re);sa=s(No,"Pruning"),No.forEach(r),ha=f(rt),Se=o(rt,"TD",{align:!0});var ko=l(Se);fa=s(ko,"N/A"),ko.forEach(r),pa=f(rt),Ue=o(rt,"TD",{align:!0});var xo=l(Ue);ca=s(xo,"\u2705"),xo.forEach(r),rt.forEach(r),q.forEach(r),jt.forEach(r),Ct=f(t),O=o(t,"H2",{class:!0});var Jt=l(O);C=o(Jt,"A",{id:!0,class:!0,href:!0});var zo=l(C);ct=o(zo,"SPAN",{});var Ho=l(ct);me(Z.$$.fragment,Ho),Ho.forEach(r),zo.forEach(r),da=f(Jt),dt=o(Jt,"SPAN",{});var Co=l(dt);ma=s(Co,"Installation"),Co.forEach(r),Jt.forEach(r),Rt=f(t),R=o(t,"P",{});var Kt=l(R);ua=s(Kt,"\u{1F917} Optimum can be installed using "),mt=o(Kt,"CODE",{});var Ro=l(mt);ga=s(Ro,"pip"),Ro.forEach(r),va=s(Kt," as follows:"),Kt.forEach(r),St=f(t),me(ee.$$.fragment,t),Ut=f(t),qe=o(t,"P",{});var So=l(qe);wa=s(So,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),So.forEach(r),qt=f(t),S=o(t,"TABLE",{});var Vt=l(S);ut=o(Vt,"THEAD",{});var Uo=l(ut);te=o(Uo,"TR",{});var Wt=l(te);Ge=o(Wt,"TH",{align:!0});var qo=l(Ge);_a=s(qo,"Accelerator"),qo.forEach(r),Ea=f(Wt),Me=o(Wt,"TH",{align:!0});var Go=l(Me);ya=s(Go,"Installation"),Go.forEach(r),Wt.forEach(r),Uo.forEach(r),ba=f(Vt),u=o(Vt,"TBODY",{});var G=l(u);re=o(G,"TR",{});var Zt=l(re);Le=o(Zt,"TD",{align:!0});var Mo=l(Le);ae=o(Mo,"A",{href:!0,rel:!0});var Lo=l(ae);$a=s(Lo,"ONNX runtime"),Lo.forEach(r),Mo.forEach(r),Ta=f(Zt),Qe=o(Zt,"TD",{align:!0});var Qo=l(Qe);gt=o(Qo,"CODE",{});var Bo=l(gt);Aa=s(Bo,"python -m pip install optimum[onnxruntime]"),Bo.forEach(r),Qo.forEach(r),Zt.forEach(r),Pa=f(G),oe=o(G,"TR",{});var er=l(oe);Be=o(er,"TD",{align:!0});var Xo=l(Be);le=o(Xo,"A",{href:!0,rel:!0});var Fo=l(le);Ia=s(Fo,"Intel Neural Compressor (INC)"),Fo.forEach(r),Xo.forEach(r),Oa=f(er),Xe=o(er,"TD",{align:!0});var Yo=l(Xe);vt=o(Yo,"CODE",{});var jo=l(vt);Da=s(jo,"python -m pip install optimum[intel]"),jo.forEach(r),Yo.forEach(r),er.forEach(r),Na=f(G),ne=o(G,"TR",{});var tr=l(ne);Fe=o(tr,"TD",{align:!0});var Jo=l(Fe);ie=o(Jo,"A",{href:!0,rel:!0});var Ko=l(ie);ka=s(Ko,"Graphcore IPU"),Ko.forEach(r),Jo.forEach(r),xa=f(tr),Ye=o(tr,"TD",{align:!0});var Vo=l(Ye);wt=o(Vo,"CODE",{});var Wo=l(wt);za=s(Wo,"python -m pip install optimum[graphcore]"),Wo.forEach(r),Vo.forEach(r),tr.forEach(r),Ha=f(G),se=o(G,"TR",{});var rr=l(se);je=o(rr,"TD",{align:!0});var Zo=l(je);he=o(Zo,"A",{href:!0,rel:!0});var el=l(he);Ca=s(el,"Habana Gaudi Processor (HPU)"),el.forEach(r),Zo.forEach(r),Ra=f(rr),Je=o(rr,"TD",{align:!0});var tl=l(Je);_t=o(tl,"CODE",{});var rl=l(_t);Sa=s(rl,"python -m pip install optimum[habana]"),rl.forEach(r),tl.forEach(r),rr.forEach(r),G.forEach(r),Vt.forEach(r),Gt=f(t),Ke=o(t,"P",{});var al=l(Ke);Ua=s(al,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),al.forEach(r),Mt=f(t),me(fe.$$.fragment,t),Lt=f(t),w=o(t,"P",{});var at=l(w);qa=s(at,"For the acclerator-specific features, you can install them by appending "),Et=o(at,"CODE",{});var ol=l(Et);Ga=s(ol,"#egg=optimum[accelerator_type]"),ol.forEach(r),Ma=s(at," to the "),yt=o(at,"CODE",{});var ll=l(yt);La=s(ll,"pip"),ll.forEach(r),Qa=s(at," command, e.g."),at.forEach(r),Qt=f(t),me(pe.$$.fragment,t),this.h()},h(){n(_,"name","hf:doc:metadata"),n(_,"content",JSON.stringify(dl)),n(D,"id","optimum"),n(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(D,"href","#optimum"),n(E,"class","relative group"),n(N,"id","integration-with-hardware-partners"),n(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(N,"href","#integration-with-hardware-partners"),n(y,"class","relative group"),n(Q,"href","https://github.com/huggingface/optimum-graphcore"),n(Q,"rel","nofollow"),n(B,"href","https://www.graphcore.ai/products/ipu"),n(B,"rel","nofollow"),n(X,"href","https://github.com/huggingface/optimum-habana"),n(X,"rel","nofollow"),n(F,"href","https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html"),n(F,"rel","nofollow"),n(Y,"href","https://habana.ai/training/"),n(Y,"rel","nofollow"),n(j,"href","https://github.com/huggingface/optimum-intel"),n(j,"rel","nofollow"),n(J,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(J,"rel","nofollow"),n(z,"id","optimizing-models-towards-inference"),n(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(z,"href","#optimizing-models-towards-inference"),n(b,"class","relative group"),n(V,"href","https://onnxruntime.ai/docs/"),n(V,"rel","nofollow"),n(W,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(W,"rel","nofollow"),n(Te,"align","center"),n(Ae,"align","center"),n(Pe,"align","center"),n(Ie,"align","center"),n(Oe,"align","center"),n(De,"align","center"),n(Ne,"align","center"),n(ke,"align","center"),n(xe,"align","center"),n(ze,"align","center"),n(He,"align","center"),n(Ce,"align","center"),n(Re,"align","center"),n(Se,"align","center"),n(Ue,"align","center"),n(C,"id","installation"),n(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(C,"href","#installation"),n(O,"class","relative group"),n(Ge,"align","left"),n(Me,"align","left"),n(ae,"href","https://onnxruntime.ai/docs/"),n(ae,"rel","nofollow"),n(Le,"align","left"),n(Qe,"align","left"),n(le,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(le,"rel","nofollow"),n(Be,"align","left"),n(Xe,"align","left"),n(ie,"href","https://www.graphcore.ai/products/ipu"),n(ie,"rel","nofollow"),n(Fe,"align","left"),n(Ye,"align","left"),n(he,"href","https://habana.ai/training/"),n(he,"rel","nofollow"),n(je,"align","left"),n(Je,"align","left")},m(t,p){e(document.head,_),c(t,Tt,p),c(t,E,p),e(E,D),e(D,ot),ue(M,ot,null),e(E,or),e(E,lt),e(lt,lr),c(t,At,p),c(t,_e,p),e(_e,nr),c(t,Pt,p),c(t,Ee,p),e(Ee,ir),c(t,It,p),c(t,y,p),e(y,N),e(N,nt),ue(L,nt,null),e(y,sr),e(y,it),e(it,hr),c(t,Ot,p),c(t,ye,p),e(ye,fr),c(t,Dt,p),c(t,be,p),e(be,pr),c(t,Nt,p),c(t,d,p),e(d,k),e(k,Q),e(Q,cr),e(k,dr),e(k,B),e(B,mr),e(k,ur),e(d,gr),e(d,g),e(g,X),e(X,vr),e(g,wr),e(g,F),e(F,_r),e(g,Er),e(g,Y),e(Y,yr),e(g,br),e(d,$r),e(d,x),e(x,j),e(j,Tr),e(x,Ar),e(x,J),e(J,Pr),e(x,Ir),e(d,Or),e(d,st),e(st,Dr),c(t,kt,p),c(t,b,p),e(b,z),e(z,ht),ue(K,ht,null),e(b,Nr),e(b,ft),e(ft,kr),c(t,xt,p),c(t,$e,p),e($e,xr),c(t,zt,p),c(t,v,p),e(v,zr),e(v,V),e(V,Hr),e(v,Cr),e(v,W),e(W,Rr),e(v,Sr),c(t,Ht,p),c(t,H,p),e(H,pt),e(pt,$),e($,Te),e(Te,Ur),e($,qr),e($,Ae),e(Ae,Gr),e($,Mr),e($,Pe),e(Pe,Lr),e(H,Qr),e(H,m),e(m,T),e(T,Ie),e(Ie,Br),e(T,Xr),e(T,Oe),e(Oe,Fr),e(T,Yr),e(T,De),e(De,jr),e(m,Jr),e(m,A),e(A,Ne),e(Ne,Kr),e(A,Vr),e(A,ke),e(ke,Wr),e(A,Zr),e(A,xe),e(xe,ea),e(m,ta),e(m,P),e(P,ze),e(ze,ra),e(P,aa),e(P,He),e(He,oa),e(P,la),e(P,Ce),e(Ce,na),e(m,ia),e(m,I),e(I,Re),e(Re,sa),e(I,ha),e(I,Se),e(Se,fa),e(I,pa),e(I,Ue),e(Ue,ca),c(t,Ct,p),c(t,O,p),e(O,C),e(C,ct),ue(Z,ct,null),e(O,da),e(O,dt),e(dt,ma),c(t,Rt,p),c(t,R,p),e(R,ua),e(R,mt),e(mt,ga),e(R,va),c(t,St,p),ue(ee,t,p),c(t,Ut,p),c(t,qe,p),e(qe,wa),c(t,qt,p),c(t,S,p),e(S,ut),e(ut,te),e(te,Ge),e(Ge,_a),e(te,Ea),e(te,Me),e(Me,ya),e(S,ba),e(S,u),e(u,re),e(re,Le),e(Le,ae),e(ae,$a),e(re,Ta),e(re,Qe),e(Qe,gt),e(gt,Aa),e(u,Pa),e(u,oe),e(oe,Be),e(Be,le),e(le,Ia),e(oe,Oa),e(oe,Xe),e(Xe,vt),e(vt,Da),e(u,Na),e(u,ne),e(ne,Fe),e(Fe,ie),e(ie,ka),e(ne,xa),e(ne,Ye),e(Ye,wt),e(wt,za),e(u,Ha),e(u,se),e(se,je),e(je,he),e(he,Ca),e(se,Ra),e(se,Je),e(Je,_t),e(_t,Sa),c(t,Gt,p),c(t,Ke,p),e(Ke,Ua),c(t,Mt,p),ue(fe,t,p),c(t,Lt,p),c(t,w,p),e(w,qa),e(w,Et),e(Et,Ga),e(w,Ma),e(w,yt),e(yt,La),e(w,Qa),c(t,Qt,p),ue(pe,t,p),Bt=!0},p:fl,i(t){Bt||(ge(M.$$.fragment,t),ge(L.$$.fragment,t),ge(K.$$.fragment,t),ge(Z.$$.fragment,t),ge(ee.$$.fragment,t),ge(fe.$$.fragment,t),ge(pe.$$.fragment,t),Bt=!0)},o(t){ve(M.$$.fragment,t),ve(L.$$.fragment,t),ve(K.$$.fragment,t),ve(Z.$$.fragment,t),ve(ee.$$.fragment,t),ve(fe.$$.fragment,t),ve(pe.$$.fragment,t),Bt=!1},d(t){r(_),t&&r(Tt),t&&r(E),we(M),t&&r(At),t&&r(_e),t&&r(Pt),t&&r(Ee),t&&r(It),t&&r(y),we(L),t&&r(Ot),t&&r(ye),t&&r(Dt),t&&r(be),t&&r(Nt),t&&r(d),t&&r(kt),t&&r(b),we(K),t&&r(xt),t&&r($e),t&&r(zt),t&&r(v),t&&r(Ht),t&&r(H),t&&r(Ct),t&&r(O),we(Z),t&&r(Rt),t&&r(R),t&&r(St),we(ee,t),t&&r(Ut),t&&r(qe),t&&r(qt),t&&r(S),t&&r(Gt),t&&r(Ke),t&&r(Mt),we(fe,t),t&&r(Lt),t&&r(w),t&&r(Qt),we(pe,t)}}}const dl={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function ml(Xa){return pl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wl extends nl{constructor(_){super();il(this,_,ml,cl,sl,{})}}export{wl as default,dl as metadata};
