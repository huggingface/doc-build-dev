import{S as Ct,i as Pt,s as Dt,e as o,k as c,w as j,t as r,M as qt,c as n,d as a,m as u,a as i,x as y,h as l,b as d,G as t,g as p,y as k,q as E,o as x,B as T,v as Ft}from"../../chunks/vendor-hf-doc-builder.js";import{T as tt}from"../../chunks/Tip-hf-doc-builder.js";import{Y as It}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ia}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as H}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as zt,M as St}from"../../chunks/Markdown-hf-doc-builder.js";function Lt(O){let m,b,f,_,w;return{c(){m=o("p"),b=r("See the image classification "),f=o("a"),_=r("task page"),w=r(" for more information about its associated models, datasets, and metrics."),this.h()},l(g){m=n(g,"P",{});var $=i(m);b=l($,"See the image classification "),f=n($,"A",{href:!0,rel:!0});var A=i(f);_=l(A,"task page"),A.forEach(a),w=l($," for more information about its associated models, datasets, and metrics."),$.forEach(a),this.h()},h(){d(f,"href","https://huggingface.co/tasks/audio-classification"),d(f,"rel","nofollow")},m(g,$){p(g,m,$),t(m,b),t(m,f),t(f,_),t(m,w)},d(g){g&&a(m)}}}function Nt(O){let m,b,f,_,w,g,$,A;return{c(){m=o("p"),b=r("If you aren\u2019t familiar with fine-tuning a model with the "),f=o("a"),_=r("Trainer"),w=r(", take a look at the basic tutorial "),g=o("a"),$=r("here"),A=r("!"),this.h()},l(P){m=n(P,"P",{});var D=i(m);b=l(D,"If you aren\u2019t familiar with fine-tuning a model with the "),f=n(D,"A",{href:!0});var F=i(f);_=l(F,"Trainer"),F.forEach(a),w=l(D,", take a look at the basic tutorial "),g=n(D,"A",{href:!0});var V=i(g);$=l(V,"here"),V.forEach(a),A=l(D,"!"),D.forEach(a),this.h()},h(){d(f,"href","/docs/transformers/pr_18141/en/main_classes/trainer#transformers.Trainer"),d(g,"href","../training#finetune-with-trainer")},m(P,D){p(P,m,D),t(m,b),t(m,f),t(f,_),t(m,w),t(m,g),t(g,$),t(m,A)},d(P){P&&a(m)}}}function Mt(O){let m,b,f,_,w,g,$,A,P,D,F,V,G,I,C,z,ee,J,De,ne,Y,qe,ie,ge,R,ae,L,U,W,K,Fe,pe,Q,_e,N,Ie,$e,X,B,te,se,ve,Z,me,S,be;return $=new H({props:{code:`from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`}}),P=new tt({props:{$$slots:{default:[Nt]},$$scope:{ctx:O}}}),S=new H({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=4,
    fp16=True,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=food["train"],
    eval_dataset=food["test"],
    tokenizer=feature_extractor,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-4</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    train_dataset=food[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=food[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){m=o("p"),b=r("Load ViT with "),f=o("a"),_=r("AutoModelForImageClassification"),w=r(". Specify the number of labels, and pass the model the mapping between label number and label class:"),g=c(),j($.$$.fragment),A=c(),j(P.$$.fragment),D=c(),F=o("p"),V=r("At this point, only three steps remain:"),G=c(),I=o("ol"),C=o("li"),z=r("Define your training hyperparameters in "),ee=o("a"),J=r("TrainingArguments"),De=r(". It is important you don\u2019t remove unused columns because this will drop the "),ne=o("code"),Y=r("image"),qe=r(" column. Without the "),ie=o("code"),ge=r("image"),R=r(" column, you can\u2019t create "),ae=o("code"),L=r("pixel_values"),U=r(". Set "),W=o("code"),K=r("remove_unused_columns=False"),Fe=r(" to prevent this behavior!"),pe=c(),Q=o("li"),_e=r("Pass the training arguments to "),N=o("a"),Ie=r("Trainer"),$e=r(" along with the model, datasets, tokenizer, and data collator."),X=c(),B=o("li"),te=r("Call "),se=o("a"),ve=r("train()"),Z=r(" to fine-tune your model."),me=c(),j(S.$$.fragment),this.h()},l(h){m=n(h,"P",{});var v=i(m);b=l(v,"Load ViT with "),f=n(v,"A",{href:!0});var re=i(f);_=l(re,"AutoModelForImageClassification"),re.forEach(a),w=l(v,". Specify the number of labels, and pass the model the mapping between label number and label class:"),v.forEach(a),g=u(h),y($.$$.fragment,h),A=u(h),y(P.$$.fragment,h),D=u(h),F=n(h,"P",{});var M=i(F);V=l(M,"At this point, only three steps remain:"),M.forEach(a),G=u(h),I=n(h,"OL",{});var le=i(I);C=n(le,"LI",{});var q=i(C);z=l(q,"Define your training hyperparameters in "),ee=n(q,"A",{href:!0});var Ge=i(ee);J=l(Ge,"TrainingArguments"),Ge.forEach(a),De=l(q,". It is important you don\u2019t remove unused columns because this will drop the "),ne=n(q,"CODE",{});var Je=i(ne);Y=l(Je,"image"),Je.forEach(a),qe=l(q," column. Without the "),ie=n(q,"CODE",{});var we=i(ie);ge=l(we,"image"),we.forEach(a),R=l(q," column, you can\u2019t create "),ae=n(q,"CODE",{});var Ye=i(ae);L=l(Ye,"pixel_values"),Ye.forEach(a),U=l(q,". Set "),W=n(q,"CODE",{});var We=i(W);K=l(We,"remove_unused_columns=False"),We.forEach(a),Fe=l(q," to prevent this behavior!"),q.forEach(a),pe=u(le),Q=n(le,"LI",{});var fe=i(Q);_e=l(fe,"Pass the training arguments to "),N=n(fe,"A",{href:!0});var oe=i(N);Ie=l(oe,"Trainer"),oe.forEach(a),$e=l(fe," along with the model, datasets, tokenizer, and data collator."),fe.forEach(a),X=u(le),B=n(le,"LI",{});var he=i(B);te=l(he,"Call "),se=n(he,"A",{href:!0});var ce=i(se);ve=l(ce,"train()"),ce.forEach(a),Z=l(he," to fine-tune your model."),he.forEach(a),le.forEach(a),me=u(h),y(S.$$.fragment,h),this.h()},h(){d(f,"href","/docs/transformers/pr_18141/en/model_doc/auto#transformers.AutoModelForImageClassification"),d(ee,"href","/docs/transformers/pr_18141/en/main_classes/trainer#transformers.TrainingArguments"),d(N,"href","/docs/transformers/pr_18141/en/main_classes/trainer#transformers.Trainer"),d(se,"href","/docs/transformers/pr_18141/en/main_classes/trainer#transformers.Trainer.train")},m(h,v){p(h,m,v),t(m,b),t(m,f),t(f,_),t(m,w),p(h,g,v),k($,h,v),p(h,A,v),k(P,h,v),p(h,D,v),p(h,F,v),t(F,V),p(h,G,v),p(h,I,v),t(I,C),t(C,z),t(C,ee),t(ee,J),t(C,De),t(C,ne),t(ne,Y),t(C,qe),t(C,ie),t(ie,ge),t(C,R),t(C,ae),t(ae,L),t(C,U),t(C,W),t(W,K),t(C,Fe),t(I,pe),t(I,Q),t(Q,_e),t(Q,N),t(N,Ie),t(Q,$e),t(I,X),t(I,B),t(B,te),t(B,se),t(se,ve),t(B,Z),p(h,me,v),k(S,h,v),be=!0},p(h,v){const re={};v&2&&(re.$$scope={dirty:v,ctx:h}),P.$set(re)},i(h){be||(E($.$$.fragment,h),E(P.$$.fragment,h),E(S.$$.fragment,h),be=!0)},o(h){x($.$$.fragment,h),x(P.$$.fragment,h),x(S.$$.fragment,h),be=!1},d(h){h&&a(m),h&&a(g),T($,h),h&&a(A),T(P,h),h&&a(D),h&&a(F),h&&a(G),h&&a(I),h&&a(me),T(S,h)}}}function Ot(O){let m,b;return m=new St({props:{$$slots:{default:[Mt]},$$scope:{ctx:O}}}),{c(){j(m.$$.fragment)},l(f){y(m.$$.fragment,f)},m(f,_){k(m,f,_),b=!0},p(f,_){const w={};_&2&&(w.$$scope={dirty:_,ctx:f}),m.$set(w)},i(f){b||(E(m.$$.fragment,f),b=!0)},o(f){x(m.$$.fragment,f),b=!1},d(f){T(m,f)}}}function Rt(O){let m,b,f,_,w;return{c(){m=o("p"),b=r("For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),f=o("a"),_=r("PyTorch notebook"),w=r("."),this.h()},l(g){m=n(g,"P",{});var $=i(m);b=l($,"For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),f=n($,"A",{href:!0,rel:!0});var A=i(f);_=l(A,"PyTorch notebook"),A.forEach(a),w=l($,"."),$.forEach(a),this.h()},h(){d(f,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb"),d(f,"rel","nofollow")},m(g,$){p(g,m,$),t(m,b),t(m,f),t(f,_),t(m,w)},d(g){g&&a(m)}}}function Ut(O){let m,b,f,_,w,g,$,A,P,D,F,V,G,I,C,z,ee,J,De,ne,Y,qe,ie,ge,R,ae,L,U,W,K,Fe,pe,Q,_e,N,Ie,$e,X,B,te,se,ve,Z,me,S,be,h,v,re,M,le,q,Ge,Je,we,Ye,We,fe,oe,he,ce,za,pa,ze,ma,je,Sa,ea,La,Na,fa,ue,ye,aa,Se,Ma,ta,Oa,ha,Ke,Ra,ca,Le,ua,ke,Ua,Ne,sa,Ba,Ha,da,Me,ga,Ee,Va,ra,Ga,Ja,_a,Oe,$a,xe,Ya,Re,Wa,Ka,va,Ue,ba,Te,Qa,Qe,Xa,Za,wa,Be,ja,de,Ae,la,He,et,oa,at,ya,Ce,ka,Pe,Ea;return g=new Ia({}),F=new It({props:{id:"tjAIM7BOYhw"}}),R=new tt({props:{$$slots:{default:[Lt]},$$scope:{ctx:O}}}),K=new Ia({}),X=new H({props:{code:`from datasets import load_dataset

food = load_dataset("food101", split="train[:5000]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>food = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`}}),Z=new H({props:{code:"food = food.train_test_split(test_size=0.2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),v=new H({props:{code:'food["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>food[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at <span class="hljs-number">0x7F52AFC8AC50</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">79</span>}`}}),oe=new H({props:{code:`labels = food["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = food[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`}}),ze=new H({props:{code:"id2label[str(79)]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">79</span>)]
<span class="hljs-string">&#x27;prime_rib&#x27;</span>`}}),Se=new Ia({}),Le=new H({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)`}}),Me=new H({props:{code:`from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomResizedCrop, Compose, Normalize, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`}}),Oe=new H({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
    del examples["image"]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(img.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">del</span> examples[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Ue=new H({props:{code:"food = food.with_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.with_transform(transforms)'}}),Be=new H({props:{code:`from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator()`}}),He=new Ia({}),Ce=new zt({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Ot]},$$scope:{ctx:O}}}),Pe=new tt({props:{$$slots:{default:[Rt]},$$scope:{ctx:O}}}),{c(){m=o("meta"),b=c(),f=o("h1"),_=o("a"),w=o("span"),j(g.$$.fragment),$=c(),A=o("span"),P=r("Image classification"),D=c(),j(F.$$.fragment),V=c(),G=o("p"),I=r("Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),C=c(),z=o("p"),ee=r("This guide will show you how to fine-tune "),J=o("a"),De=r("ViT"),ne=r(" on the "),Y=o("a"),qe=r("Food-101"),ie=r(" dataset to classify a food item in an image."),ge=c(),j(R.$$.fragment),ae=c(),L=o("h2"),U=o("a"),W=o("span"),j(K.$$.fragment),Fe=c(),pe=o("span"),Q=r("Load Food-101 dataset"),_e=c(),N=o("p"),Ie=r("Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),$e=c(),j(X.$$.fragment),B=c(),te=o("p"),se=r("Split this dataset into a train and test set:"),ve=c(),j(Z.$$.fragment),me=c(),S=o("p"),be=r("Then take a look at an example:"),h=c(),j(v.$$.fragment),re=c(),M=o("p"),le=r("The "),q=o("code"),Ge=r("image"),Je=r(" field contains a PIL image, and each "),we=o("code"),Ye=r("label"),We=r(" is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),fe=c(),j(oe.$$.fragment),he=c(),ce=o("p"),za=r("Now you can convert the label number to a label name for more information:"),pa=c(),j(ze.$$.fragment),ma=c(),je=o("p"),Sa=r("Each food class - or label - corresponds to a number; "),ea=o("code"),La=r("79"),Na=r(" indicates a prime rib in the example above."),fa=c(),ue=o("h2"),ye=o("a"),aa=o("span"),j(Se.$$.fragment),Ma=c(),ta=o("span"),Oa=r("Preprocess"),ha=c(),Ke=o("p"),Ra=r("Load the ViT feature extractor to process the image into a tensor:"),ca=c(),j(Le.$$.fragment),ua=c(),ke=o("p"),Ua=r("Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),Ne=o("a"),sa=o("code"),Ba=r("transforms"),Ha=r(" module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),da=c(),j(Me.$$.fragment),ga=c(),Ee=o("p"),Va=r("Create a preprocessing function that will apply the transforms and return the "),ra=o("code"),Ga=r("pixel_values"),Ja=r(" - the inputs to the model - of the image:"),_a=c(),j(Oe.$$.fragment),$a=c(),xe=o("p"),Ya=r("Use \u{1F917} Dataset\u2019s "),Re=o("a"),Wa=r("with_transform"),Ka=r(" method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),va=c(),j(Ue.$$.fragment),ba=c(),Te=o("p"),Qa=r("Use "),Qe=o("a"),Xa=r("DefaultDataCollator"),Za=r(" to create a batch of examples. Unlike other data collators in \u{1F917} Transformers, the DefaultDataCollator does not apply additional preprocessing such as padding."),wa=c(),j(Be.$$.fragment),ja=c(),de=o("h2"),Ae=o("a"),la=o("span"),j(He.$$.fragment),et=c(),oa=o("span"),at=r("Train"),ya=c(),j(Ce.$$.fragment),ka=c(),j(Pe.$$.fragment),this.h()},l(e){const s=qt('[data-svelte="svelte-1phssyn"]',document.head);m=n(s,"META",{name:!0,content:!0}),s.forEach(a),b=u(e),f=n(e,"H1",{class:!0});var Ve=i(f);_=n(Ve,"A",{id:!0,class:!0,href:!0});var na=i(_);w=n(na,"SPAN",{});var ia=i(w);y(g.$$.fragment,ia),ia.forEach(a),na.forEach(a),$=u(Ve),A=n(Ve,"SPAN",{});var st=i(A);P=l(st,"Image classification"),st.forEach(a),Ve.forEach(a),D=u(e),y(F.$$.fragment,e),V=u(e),G=n(e,"P",{});var rt=i(G);I=l(rt,"Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),rt.forEach(a),C=u(e),z=n(e,"P",{});var Xe=i(z);ee=l(Xe,"This guide will show you how to fine-tune "),J=n(Xe,"A",{href:!0,rel:!0});var lt=i(J);De=l(lt,"ViT"),lt.forEach(a),ne=l(Xe," on the "),Y=n(Xe,"A",{href:!0,rel:!0});var ot=i(Y);qe=l(ot,"Food-101"),ot.forEach(a),ie=l(Xe," dataset to classify a food item in an image."),Xe.forEach(a),ge=u(e),y(R.$$.fragment,e),ae=u(e),L=n(e,"H2",{class:!0});var xa=i(L);U=n(xa,"A",{id:!0,class:!0,href:!0});var nt=i(U);W=n(nt,"SPAN",{});var it=i(W);y(K.$$.fragment,it),it.forEach(a),nt.forEach(a),Fe=u(xa),pe=n(xa,"SPAN",{});var pt=i(pe);Q=l(pt,"Load Food-101 dataset"),pt.forEach(a),xa.forEach(a),_e=u(e),N=n(e,"P",{});var mt=i(N);Ie=l(mt,"Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),mt.forEach(a),$e=u(e),y(X.$$.fragment,e),B=u(e),te=n(e,"P",{});var ft=i(te);se=l(ft,"Split this dataset into a train and test set:"),ft.forEach(a),ve=u(e),y(Z.$$.fragment,e),me=u(e),S=n(e,"P",{});var ht=i(S);be=l(ht,"Then take a look at an example:"),ht.forEach(a),h=u(e),y(v.$$.fragment,e),re=u(e),M=n(e,"P",{});var Ze=i(M);le=l(Ze,"The "),q=n(Ze,"CODE",{});var ct=i(q);Ge=l(ct,"image"),ct.forEach(a),Je=l(Ze," field contains a PIL image, and each "),we=n(Ze,"CODE",{});var ut=i(we);Ye=l(ut,"label"),ut.forEach(a),We=l(Ze," is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),Ze.forEach(a),fe=u(e),y(oe.$$.fragment,e),he=u(e),ce=n(e,"P",{});var dt=i(ce);za=l(dt,"Now you can convert the label number to a label name for more information:"),dt.forEach(a),pa=u(e),y(ze.$$.fragment,e),ma=u(e),je=n(e,"P",{});var Ta=i(je);Sa=l(Ta,"Each food class - or label - corresponds to a number; "),ea=n(Ta,"CODE",{});var gt=i(ea);La=l(gt,"79"),gt.forEach(a),Na=l(Ta," indicates a prime rib in the example above."),Ta.forEach(a),fa=u(e),ue=n(e,"H2",{class:!0});var Aa=i(ue);ye=n(Aa,"A",{id:!0,class:!0,href:!0});var _t=i(ye);aa=n(_t,"SPAN",{});var $t=i(aa);y(Se.$$.fragment,$t),$t.forEach(a),_t.forEach(a),Ma=u(Aa),ta=n(Aa,"SPAN",{});var vt=i(ta);Oa=l(vt,"Preprocess"),vt.forEach(a),Aa.forEach(a),ha=u(e),Ke=n(e,"P",{});var bt=i(Ke);Ra=l(bt,"Load the ViT feature extractor to process the image into a tensor:"),bt.forEach(a),ca=u(e),y(Le.$$.fragment,e),ua=u(e),ke=n(e,"P",{});var Ca=i(ke);Ua=l(Ca,"Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),Ne=n(Ca,"A",{href:!0,rel:!0});var wt=i(Ne);sa=n(wt,"CODE",{});var jt=i(sa);Ba=l(jt,"transforms"),jt.forEach(a),wt.forEach(a),Ha=l(Ca," module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),Ca.forEach(a),da=u(e),y(Me.$$.fragment,e),ga=u(e),Ee=n(e,"P",{});var Pa=i(Ee);Va=l(Pa,"Create a preprocessing function that will apply the transforms and return the "),ra=n(Pa,"CODE",{});var yt=i(ra);Ga=l(yt,"pixel_values"),yt.forEach(a),Ja=l(Pa," - the inputs to the model - of the image:"),Pa.forEach(a),_a=u(e),y(Oe.$$.fragment,e),$a=u(e),xe=n(e,"P",{});var Da=i(xe);Ya=l(Da,"Use \u{1F917} Dataset\u2019s "),Re=n(Da,"A",{href:!0,rel:!0});var kt=i(Re);Wa=l(kt,"with_transform"),kt.forEach(a),Ka=l(Da," method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),Da.forEach(a),va=u(e),y(Ue.$$.fragment,e),ba=u(e),Te=n(e,"P",{});var qa=i(Te);Qa=l(qa,"Use "),Qe=n(qa,"A",{href:!0});var Et=i(Qe);Xa=l(Et,"DefaultDataCollator"),Et.forEach(a),Za=l(qa," to create a batch of examples. Unlike other data collators in \u{1F917} Transformers, the DefaultDataCollator does not apply additional preprocessing such as padding."),qa.forEach(a),wa=u(e),y(Be.$$.fragment,e),ja=u(e),de=n(e,"H2",{class:!0});var Fa=i(de);Ae=n(Fa,"A",{id:!0,class:!0,href:!0});var xt=i(Ae);la=n(xt,"SPAN",{});var Tt=i(la);y(He.$$.fragment,Tt),Tt.forEach(a),xt.forEach(a),et=u(Fa),oa=n(Fa,"SPAN",{});var At=i(oa);at=l(At,"Train"),At.forEach(a),Fa.forEach(a),ya=u(e),y(Ce.$$.fragment,e),ka=u(e),y(Pe.$$.fragment,e),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(Bt)),d(_,"id","image-classification"),d(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_,"href","#image-classification"),d(f,"class","relative group"),d(J,"href","https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/vit"),d(J,"rel","nofollow"),d(Y,"href","https://huggingface.co/datasets/food101"),d(Y,"rel","nofollow"),d(U,"id","load-food101-dataset"),d(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(U,"href","#load-food101-dataset"),d(L,"class","relative group"),d(ye,"id","preprocess"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#preprocess"),d(ue,"class","relative group"),d(Ne,"href","https://pytorch.org/vision/stable/transforms.html"),d(Ne,"rel","nofollow"),d(Re,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.with_transform"),d(Re,"rel","nofollow"),d(Qe,"href","/docs/transformers/pr_18141/en/main_classes/data_collator#transformers.DefaultDataCollator"),d(Ae,"id","train"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#train"),d(de,"class","relative group")},m(e,s){t(document.head,m),p(e,b,s),p(e,f,s),t(f,_),t(_,w),k(g,w,null),t(f,$),t(f,A),t(A,P),p(e,D,s),k(F,e,s),p(e,V,s),p(e,G,s),t(G,I),p(e,C,s),p(e,z,s),t(z,ee),t(z,J),t(J,De),t(z,ne),t(z,Y),t(Y,qe),t(z,ie),p(e,ge,s),k(R,e,s),p(e,ae,s),p(e,L,s),t(L,U),t(U,W),k(K,W,null),t(L,Fe),t(L,pe),t(pe,Q),p(e,_e,s),p(e,N,s),t(N,Ie),p(e,$e,s),k(X,e,s),p(e,B,s),p(e,te,s),t(te,se),p(e,ve,s),k(Z,e,s),p(e,me,s),p(e,S,s),t(S,be),p(e,h,s),k(v,e,s),p(e,re,s),p(e,M,s),t(M,le),t(M,q),t(q,Ge),t(M,Je),t(M,we),t(we,Ye),t(M,We),p(e,fe,s),k(oe,e,s),p(e,he,s),p(e,ce,s),t(ce,za),p(e,pa,s),k(ze,e,s),p(e,ma,s),p(e,je,s),t(je,Sa),t(je,ea),t(ea,La),t(je,Na),p(e,fa,s),p(e,ue,s),t(ue,ye),t(ye,aa),k(Se,aa,null),t(ue,Ma),t(ue,ta),t(ta,Oa),p(e,ha,s),p(e,Ke,s),t(Ke,Ra),p(e,ca,s),k(Le,e,s),p(e,ua,s),p(e,ke,s),t(ke,Ua),t(ke,Ne),t(Ne,sa),t(sa,Ba),t(ke,Ha),p(e,da,s),k(Me,e,s),p(e,ga,s),p(e,Ee,s),t(Ee,Va),t(Ee,ra),t(ra,Ga),t(Ee,Ja),p(e,_a,s),k(Oe,e,s),p(e,$a,s),p(e,xe,s),t(xe,Ya),t(xe,Re),t(Re,Wa),t(xe,Ka),p(e,va,s),k(Ue,e,s),p(e,ba,s),p(e,Te,s),t(Te,Qa),t(Te,Qe),t(Qe,Xa),t(Te,Za),p(e,wa,s),k(Be,e,s),p(e,ja,s),p(e,de,s),t(de,Ae),t(Ae,la),k(He,la,null),t(de,et),t(de,oa),t(oa,at),p(e,ya,s),k(Ce,e,s),p(e,ka,s),k(Pe,e,s),Ea=!0},p(e,[s]){const Ve={};s&2&&(Ve.$$scope={dirty:s,ctx:e}),R.$set(Ve);const na={};s&2&&(na.$$scope={dirty:s,ctx:e}),Ce.$set(na);const ia={};s&2&&(ia.$$scope={dirty:s,ctx:e}),Pe.$set(ia)},i(e){Ea||(E(g.$$.fragment,e),E(F.$$.fragment,e),E(R.$$.fragment,e),E(K.$$.fragment,e),E(X.$$.fragment,e),E(Z.$$.fragment,e),E(v.$$.fragment,e),E(oe.$$.fragment,e),E(ze.$$.fragment,e),E(Se.$$.fragment,e),E(Le.$$.fragment,e),E(Me.$$.fragment,e),E(Oe.$$.fragment,e),E(Ue.$$.fragment,e),E(Be.$$.fragment,e),E(He.$$.fragment,e),E(Ce.$$.fragment,e),E(Pe.$$.fragment,e),Ea=!0)},o(e){x(g.$$.fragment,e),x(F.$$.fragment,e),x(R.$$.fragment,e),x(K.$$.fragment,e),x(X.$$.fragment,e),x(Z.$$.fragment,e),x(v.$$.fragment,e),x(oe.$$.fragment,e),x(ze.$$.fragment,e),x(Se.$$.fragment,e),x(Le.$$.fragment,e),x(Me.$$.fragment,e),x(Oe.$$.fragment,e),x(Ue.$$.fragment,e),x(Be.$$.fragment,e),x(He.$$.fragment,e),x(Ce.$$.fragment,e),x(Pe.$$.fragment,e),Ea=!1},d(e){a(m),e&&a(b),e&&a(f),T(g),e&&a(D),T(F,e),e&&a(V),e&&a(G),e&&a(C),e&&a(z),e&&a(ge),T(R,e),e&&a(ae),e&&a(L),T(K),e&&a(_e),e&&a(N),e&&a($e),T(X,e),e&&a(B),e&&a(te),e&&a(ve),T(Z,e),e&&a(me),e&&a(S),e&&a(h),T(v,e),e&&a(re),e&&a(M),e&&a(fe),T(oe,e),e&&a(he),e&&a(ce),e&&a(pa),T(ze,e),e&&a(ma),e&&a(je),e&&a(fa),e&&a(ue),T(Se),e&&a(ha),e&&a(Ke),e&&a(ca),T(Le,e),e&&a(ua),e&&a(ke),e&&a(da),T(Me,e),e&&a(ga),e&&a(Ee),e&&a(_a),T(Oe,e),e&&a($a),e&&a(xe),e&&a(va),T(Ue,e),e&&a(ba),e&&a(Te),e&&a(wa),T(Be,e),e&&a(ja),e&&a(de),T(He),e&&a(ya),T(Ce,e),e&&a(ka),T(Pe,e)}}}const Bt={local:"image-classification",sections:[{local:"load-food101-dataset",title:"Load Food-101 dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Image classification"};function Ht(O){return Ft(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qt extends Ct{constructor(m){super();Pt(this,m,Ht,Ut,Dt,{})}}export{Qt as default,Bt as metadata};
