import{S as K$,i as Z$,s as eO,e as n,k as d,w as h,t as p,M as tO,c as s,d as t,m as i,a,x as f,h as c,b as r,F as o,g as l,y as m,q as _,o as g,B as v,v as oO}from"../../chunks/vendor-6b77c823.js";import{T as nO}from"../../chunks/Tip-39098574.js";import{D as y}from"../../chunks/Docstring-1088f2fb.js";import{C as G$}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as T}from"../../chunks/IconCopyLink-7a11ce68.js";function sO($l){let x,Yt,$,A,G,O,wn,K;return{c(){x=n("p"),Yt=p("You can\u2019t unpack a "),$=n("code"),A=p("ModelOutput"),G=p(" directly. Use the "),O=n("a"),wn=p("to_tuple()"),K=p(` method to convert it to a tuple
before.`),this.h()},l(Z){x=s(Z,"P",{});var M=a(x);Yt=c(M,"You can\u2019t unpack a "),$=s(M,"CODE",{});var C=a($);A=c(C,"ModelOutput"),C.forEach(t),G=c(M," directly. Use the "),O=s(M,"A",{href:!0});var tr=a(O);wn=c(tr,"to_tuple()"),tr.forEach(t),K=c(M,` method to convert it to a tuple
before.`),M.forEach(t),this.h()},h(){r(O,"href","/docs/transformers/pr_16812/en/main_classes/output#transformers.utils.ModelOutput.to_tuple")},m(Z,M){l(Z,x,M),o(x,Yt),o(x,$),o($,A),o(x,G),o(x,O),o(O,wn),o(x,K)},d(Z){Z&&t(x)}}}function aO($l){let x,Yt,$,A,G,O,wn,K,Z,M,C,tr,or,dm,im,Ol,nr,um,ql,xn,Fl,b,lm,xr,pm,cm,sr,hm,fm,$r,mm,_m,Or,gm,vm,qr,ym,Tm,Fr,bm,wm,Sr,xm,$m,Mr,Om,qm,kr,Fm,Sm,Ar,Mm,km,Cr,Am,Cm,Er,Em,Nm,Sl,q,zm,Nr,Pm,Bm,zr,Lm,Wm,Pr,jm,Dm,Br,Hm,Im,Ml,F,Vm,Lr,Qm,Rm,Wr,Xm,Um,jr,Ym,Jm,Dr,Gm,Km,kl,$n,Al,Jt,Zm,Hr,e_,t_,Cl,S,o_,Ir,n_,s_,Vr,a_,r_,Qr,d_,i_,Rr,u_,l_,El,ar,p_,Nl,ee,Gt,Xr,On,c_,Ur,h_,zl,k,qn,f_,te,m_,Yr,__,g_,Jr,v_,y_,T_,Kt,b_,Zt,Fn,w_,Sn,x_,Gr,$_,O_,Pl,oe,eo,Kr,Mn,q_,Zr,F_,Bl,ne,kn,S_,ed,M_,Ll,se,to,td,An,k_,od,A_,Wl,ae,Cn,C_,nd,E_,jl,re,oo,sd,En,N_,ad,z_,Dl,de,Nn,P_,rd,B_,Hl,ie,no,dd,zn,L_,id,W_,Il,ue,Pn,j_,ud,D_,Vl,le,so,ld,Bn,H_,pd,I_,Ql,pe,Ln,V_,cd,Q_,Rl,ce,ao,hd,Wn,R_,fd,X_,Xl,he,jn,U_,md,Y_,Ul,fe,ro,_d,Dn,J_,gd,G_,Yl,me,Hn,K_,vd,Z_,Jl,_e,io,yd,In,eg,Td,tg,Gl,ge,Vn,og,bd,ng,Kl,ve,uo,wd,Qn,sg,xd,ag,Zl,ye,Rn,rg,$d,dg,ep,Te,lo,Od,Xn,ig,qd,ug,tp,be,Un,lg,Fd,pg,op,we,po,Sd,Yn,cg,Md,hg,np,xe,Jn,fg,kd,mg,sp,$e,co,Ad,Gn,_g,Cd,gg,ap,Oe,Kn,vg,Ed,yg,rp,qe,ho,Nd,Zn,Tg,zd,bg,dp,Fe,es,wg,Pd,xg,ip,Se,fo,Bd,ts,$g,Ld,Og,up,Me,os,qg,Wd,Fg,lp,ke,mo,jd,ns,Sg,Dd,Mg,pp,Ae,ss,kg,Hd,Ag,cp,Ce,_o,Id,as,Cg,Vd,Eg,hp,Ee,rs,Ng,Qd,zg,fp,Ne,go,Rd,ds,Pg,Xd,Bg,mp,ze,is,Lg,Ud,Wg,_p,Pe,vo,Yd,us,jg,Jd,Dg,gp,Be,ls,Hg,Gd,Ig,vp,Le,yo,Kd,ps,Vg,Zd,Qg,yp,We,cs,Rg,ei,Xg,Tp,je,To,ti,hs,Ug,oi,Yg,bp,De,fs,Jg,ni,Gg,wp,He,bo,si,ms,Kg,ai,Zg,xp,Ie,_s,ev,ri,tv,$p,Ve,wo,di,gs,ov,ii,nv,Op,Qe,vs,sv,ui,av,qp,Re,xo,li,ys,rv,pi,dv,Fp,Xe,Ts,iv,ci,uv,Sp,Ue,$o,hi,bs,lv,fi,pv,Mp,Ye,ws,cv,xs,hv,mi,fv,mv,kp,Je,Oo,_i,$s,_v,gi,gv,Ap,Ge,Os,vv,qs,yv,rr,Tv,bv,Cp,Ke,qo,vi,Fs,wv,yi,xv,Ep,Ze,Ss,$v,Ti,Ov,Np,et,Fo,bi,Ms,qv,wi,Fv,zp,tt,ks,Sv,xi,Mv,Pp,ot,So,$i,As,kv,Oi,Av,Bp,nt,Cs,Cv,qi,Ev,Lp,st,Mo,Fi,Es,Nv,Si,zv,Wp,at,Ns,Pv,Mi,Bv,jp,rt,ko,ki,zs,Lv,Ai,Wv,Dp,dt,Ps,jv,Ci,Dv,Hp,it,Ao,Ei,Bs,Hv,Ni,Iv,Ip,ut,Ls,Vv,zi,Qv,Vp,lt,Co,Pi,Ws,Rv,Bi,Xv,Qp,pt,js,Uv,Li,Yv,Rp,ct,Eo,Wi,Ds,Jv,ji,Gv,Xp,ht,Hs,Kv,Di,Zv,Up,ft,No,Hi,Is,ey,Ii,ty,Yp,mt,Vs,oy,Vi,ny,Jp,_t,zo,Qi,Qs,sy,Ri,ay,Gp,gt,Rs,ry,Xi,dy,Kp,vt,Po,Ui,Xs,iy,Yi,uy,Zp,yt,Us,ly,Ji,py,ec,Tt,Bo,Gi,Ys,cy,Ki,hy,tc,bt,Js,fy,Zi,my,oc,wt,Lo,eu,Gs,_y,tu,gy,nc,xt,Ks,vy,ou,yy,sc,$t,Wo,nu,Zs,Ty,su,by,ac,Ot,ea,wy,au,xy,rc,qt,jo,ru,ta,$y,du,Oy,dc,Ft,oa,qy,iu,Fy,ic,St,Do,uu,na,Sy,lu,My,uc,Mt,sa,ky,pu,Ay,lc,kt,Ho,cu,aa,Cy,hu,Ey,pc,At,ra,Ny,fu,zy,cc,Ct,Io,mu,da,Py,_u,By,hc,Et,ia,Ly,gu,Wy,fc,Nt,Vo,vu,ua,jy,yu,Dy,mc,E,la,Hy,Tu,Iy,Vy,Qo,pa,Qy,bu,Ry,_c,zt,Ro,wu,ca,Xy,xu,Uy,gc,N,ha,Yy,$u,Jy,Gy,Xo,fa,Ky,Ou,Zy,vc,Pt,Uo,qu,ma,eT,Fu,tT,yc,z,_a,oT,Su,nT,sT,Yo,ga,aT,Mu,rT,Tc,Bt,Jo,ku,va,dT,Au,iT,bc,P,ya,uT,Cu,lT,pT,Go,Ta,cT,Eu,hT,wc,Lt,Ko,Nu,ba,fT,zu,mT,xc,B,wa,_T,Pu,gT,vT,Zo,xa,yT,Bu,TT,$c,Wt,en,Lu,$a,bT,Wu,wT,Oc,L,Oa,xT,ju,$T,OT,tn,qa,qT,Du,FT,qc,jt,on,Hu,Fa,ST,Iu,MT,Fc,W,Sa,kT,Vu,AT,CT,nn,Ma,ET,Qu,NT,Sc,Dt,sn,Ru,ka,zT,Xu,PT,Mc,j,Aa,BT,Uu,LT,WT,an,Ca,jT,Yu,DT,kc,Ht,rn,Ju,Ea,HT,Gu,IT,Ac,D,Na,VT,Ku,QT,RT,dn,za,XT,Zu,UT,Cc,It,un,el,Pa,YT,tl,JT,Ec,H,Ba,GT,ol,KT,ZT,ln,La,e2,nl,t2,Nc,Vt,pn,sl,Wa,o2,al,n2,zc,I,ja,s2,rl,a2,r2,cn,Da,d2,dl,i2,Pc,Qt,hn,il,Ha,u2,ul,l2,Bc,V,Ia,p2,ll,c2,h2,fn,Va,f2,pl,m2,Lc,Rt,mn,cl,Qa,_2,hl,g2,Wc,Q,Ra,v2,fl,y2,T2,_n,Xa,b2,ml,w2,jc,Xt,gn,_l,Ua,x2,gl,$2,Dc,R,Ya,O2,vl,q2,F2,vn,Ja,S2,yl,M2,Hc,Ut,yn,Tl,Ga,k2,bl,A2,Ic,X,Ka,C2,wl,E2,N2,Tn,Za,z2,xl,P2,Vc;return O=new T({}),xn=new G$({props:{code:`from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertForSequenceClassification
<span class="hljs-keyword">import</span> torch

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
outputs = model(**inputs, labels=labels)`}}),$n=new G$({props:{code:"outputs[:2]",highlighted:'outputs[:<span class="hljs-number">2</span>]'}}),On=new T({}),qn=new y({props:{name:"class transformers.utils.ModelOutput",anchor:"transformers.utils.ModelOutput",parameters:"",source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/utils/generic.py#L147"}}),Kt=new nO({props:{warning:!0,$$slots:{default:[sO]},$$scope:{ctx:$l}}}),Fn=new y({props:{name:"to_tuple",anchor:"transformers.utils.ModelOutput.to_tuple",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/utils/generic.py#L236"}}),Mn=new T({}),kn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutput",anchor:"transformers.modeling_outputs.BaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L24"}}),An=new T({}),Cn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPooling",anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L69"}}),En=new T({}),Nn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L162"}}),zn=new T({}),Pn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L195"}}),Bn=new T({}),Ln=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPast",anchor:"transformers.modeling_outputs.BaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L123"}}),Wn=new T({}),jn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L244"}}),Dn=new T({}),Hn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L290"}}),In=new T({}),Vn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutput",anchor:"transformers.modeling_outputs.CausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L351"}}),Qn=new T({}),Rn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L416"}}),Xn=new T({}),Un=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithPast",anchor:"transformers.modeling_outputs.CausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L380"}}),Yn=new T({}),Jn=new y({props:{name:"class transformers.modeling_outputs.MaskedLMOutput",anchor:"transformers.modeling_outputs.MaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MaskedLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L496"}}),Gn=new T({}),Kn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqLMOutput",anchor:"transformers.modeling_outputs.Seq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L525"}}),Zn=new T({}),es=new y({props:{name:"class transformers.modeling_outputs.NextSentencePredictorOutput",anchor:"transformers.modeling_outputs.NextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sequence prediction (classification) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L585"}}),ts=new T({}),os=new y({props:{name:"class transformers.modeling_outputs.SequenceClassifierOutput",anchor:"transformers.modeling_outputs.SequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L615"}}),ns=new T({}),ss=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L644"}}),as=new T({}),rs=new y({props:{name:"class transformers.modeling_outputs.MultipleChoiceModelOutput",anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L704"}}),ds=new T({}),is=new y({props:{name:"class transformers.modeling_outputs.TokenClassifierOutput",anchor:"transformers.modeling_outputs.TokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.TokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L735"}}),us=new T({}),ls=new y({props:{name:"class transformers.modeling_outputs.QuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L764"}}),ps=new T({}),cs=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L796"}}),hs=new T({}),fs=new y({props:{name:"class transformers.modeling_outputs.SemanticSegmenterOutput",anchor:"transformers.modeling_outputs.SemanticSegmenterOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) &#x2014;
Classification scores for each pixel.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>

					</div>`,name:"logits"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L859"}}),ms=new T({}),_s=new y({props:{name:"class transformers.modeling_outputs.ImageClassifierOutput",anchor:"transformers.modeling_outputs.ImageClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.ImageClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L897"}}),gs=new T({}),vs=new y({props:{name:"class transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.`,name:"hidden_states"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L925"}}),ys=new T({}),Ts=new y({props:{name:"class transformers.modeling_outputs.DepthEstimatorOutput",anchor:"transformers.modeling_outputs.DepthEstimatorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"predicted_depth",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.predicted_depth",description:`<strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>) &#x2014;
Predicted depth for each pixel.`,name:"predicted_depth"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L946"}}),bs=new T({}),ws=new y({props:{name:"class transformers.modeling_outputs.Wav2Vec2BaseModelOutput",anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"extract_features",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L976"}}),$s=new T({}),Os=new y({props:{name:"class transformers.modeling_outputs.XVectorOutput",anchor:"transformers.modeling_outputs.XVectorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"embeddings",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.XVectorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.XVectorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) &#x2014;
Classification hidden states before AMSoftmax.`,name:"logits"},{anchor:"transformers.modeling_outputs.XVectorOutput.embeddings",description:`<strong>embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) &#x2014;
Utterance embeddings used for vector similarity-based retrieval.`,name:"embeddings"},{anchor:"transformers.modeling_outputs.XVectorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.XVectorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_outputs.py#L1005"}}),Fs=new T({}),Ss=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutput",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L24"}}),Ms=new T({}),ks=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L50"}}),As=new T({}),Cs=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L84"}}),Es=new T({}),Ns=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L132"}}),zs=new T({}),Ps=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L201"}}),Bs=new T({}),Ls=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L244"}}),Ws=new T({}),js=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutput",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L304"}}),Ds=new T({}),Hs=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L369"}}),Is=new T({}),Vs=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L333"}}),Qs=new T({}),Rs=new y({props:{name:"class transformers.modeling_tf_outputs.TFMaskedLMOutput",anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L412"}}),Xs=new T({}),Us=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L441"}}),Ys=new T({}),Js=new y({props:{name:"class transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sentence prediction loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L500"}}),Gs=new T({}),Ks=new y({props:{name:"class transformers.modeling_tf_outputs.TFSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L530"}}),Zs=new T({}),ea=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L559"}}),ta=new T({}),oa=new y({props:{name:"class transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L611"}}),na=new T({}),sa=new y({props:{name:"class transformers.modeling_tf_outputs.TFTokenClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L642"}}),aa=new T({}),ra=new y({props:{name:"class transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L671"}}),da=new T({}),ia=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_tf_outputs.py#L703"}}),ua=new T({}),la=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L23"}}),pa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),ca=new T({}),ha=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, jnp.ndarray]</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L49"}}),fa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),ma=new T({}),_a=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"pooler_output",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L79"}}),ga=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),va=new T({}),ya=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L159"}}),Ta=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),ba=new T({}),wa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L205"}}),xa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),$a=new T({}),Oa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key, value
states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.
Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L266"}}),qa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Fa=new T({}),Sa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMaskedLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L307"}}),Ma=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),ka=new T({}),Aa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L336"}}),Ca=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Ea=new T({}),Na=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L393"}}),za=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Pa=new T({}),Ba=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L420"}}),La=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Wa=new T({}),ja=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L446"}}),Da=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Ha=new T({}),Ia=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L503"}}),Va=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Qa=new T({}),Ra=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L531"}}),Xa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Ua=new T({}),Ya=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L557"}}),Ja=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),Ga=new T({}),Ka=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/transformers/modeling_flax_outputs.py#L586"}}),Za=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16812/src/flax/struct.py#L120"}}),{c(){x=n("meta"),Yt=d(),$=n("h1"),A=n("a"),G=n("span"),h(O.$$.fragment),wn=d(),K=n("span"),Z=p("Model outputs"),M=d(),C=n("p"),tr=p("All models have outputs that are instances of subclasses of "),or=n("a"),dm=p("ModelOutput"),im=p(`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Ol=d(),nr=n("p"),um=p("Let\u2019s see of this looks on an example:"),ql=d(),h(xn.$$.fragment),Fl=d(),b=n("p"),lm=p("The "),xr=n("code"),pm=p("outputs"),cm=p(" object is a "),sr=n("a"),hm=p("SequenceClassifierOutput"),fm=p(`, as we can see in the
documentation of that class below, it means it has an optional `),$r=n("code"),mm=p("loss"),_m=p(", a "),Or=n("code"),gm=p("logits"),vm=p(" an optional "),qr=n("code"),ym=p("hidden_states"),Tm=p(` and
an optional `),Fr=n("code"),bm=p("attentions"),wm=p(" attribute. Here we have the "),Sr=n("code"),xm=p("loss"),$m=p(" since we passed along "),Mr=n("code"),Om=p("labels"),qm=p(`, but we don\u2019t have
`),kr=n("code"),Fm=p("hidden_states"),Sm=p(" and "),Ar=n("code"),Mm=p("attentions"),km=p(" because we didn\u2019t pass "),Cr=n("code"),Am=p("output_hidden_states=True"),Cm=p(` or
`),Er=n("code"),Em=p("output_attentions=True"),Nm=p("."),Sl=d(),q=n("p"),zm=p(`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),Nr=n("code"),Pm=p("None"),Bm=p(". Here for instance "),zr=n("code"),Lm=p("outputs.loss"),Wm=p(" is the loss computed by the model, and "),Pr=n("code"),jm=p("outputs.attentions"),Dm=p(` is
`),Br=n("code"),Hm=p("None"),Im=p("."),Ml=d(),F=n("p"),Vm=p("When considering our "),Lr=n("code"),Qm=p("outputs"),Rm=p(" object as tuple, it only considers the attributes that don\u2019t have "),Wr=n("code"),Xm=p("None"),Um=p(` values.
Here for instance, it has two elements, `),jr=n("code"),Ym=p("loss"),Jm=p(" then "),Dr=n("code"),Gm=p("logits"),Km=p(", so"),kl=d(),h($n.$$.fragment),Al=d(),Jt=n("p"),Zm=p("will return the tuple "),Hr=n("code"),e_=p("(outputs.loss, outputs.logits)"),t_=p(" for instance."),Cl=d(),S=n("p"),o_=p("When considering our "),Ir=n("code"),n_=p("outputs"),s_=p(" object as dictionary, it only considers the attributes that don\u2019t have "),Vr=n("code"),a_=p("None"),r_=p(`
values. Here for instance, it has two keys that are `),Qr=n("code"),d_=p("loss"),i_=p(" and "),Rr=n("code"),u_=p("logits"),l_=p("."),El=d(),ar=n("p"),p_=p(`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),Nl=d(),ee=n("h2"),Gt=n("a"),Xr=n("span"),h(On.$$.fragment),c_=d(),Ur=n("span"),h_=p("ModelOutput"),zl=d(),k=n("div"),h(qn.$$.fragment),f_=d(),te=n("p"),m_=p("Base class for all model outputs as dataclass. Has a "),Yr=n("code"),__=p("__getitem__"),g_=p(` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),Jr=n("code"),v_=p("None"),y_=p(` attributes. Otherwise behaves like a regular
python dictionary.`),T_=d(),h(Kt.$$.fragment),b_=d(),Zt=n("div"),h(Fn.$$.fragment),w_=d(),Sn=n("p"),x_=p("Convert self to a tuple containing all the attributes/keys that are not "),Gr=n("code"),$_=p("None"),O_=p("."),Pl=d(),oe=n("h2"),eo=n("a"),Kr=n("span"),h(Mn.$$.fragment),q_=d(),Zr=n("span"),F_=p("BaseModelOutput"),Bl=d(),ne=n("div"),h(kn.$$.fragment),S_=d(),ed=n("p"),M_=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Ll=d(),se=n("h2"),to=n("a"),td=n("span"),h(An.$$.fragment),k_=d(),od=n("span"),A_=p("BaseModelOutputWithPooling"),Wl=d(),ae=n("div"),h(Cn.$$.fragment),C_=d(),nd=n("p"),E_=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),jl=d(),re=n("h2"),oo=n("a"),sd=n("span"),h(En.$$.fragment),N_=d(),ad=n("span"),z_=p("BaseModelOutputWithCrossAttentions"),Dl=d(),de=n("div"),h(Nn.$$.fragment),P_=d(),rd=n("p"),B_=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Hl=d(),ie=n("h2"),no=n("a"),dd=n("span"),h(zn.$$.fragment),L_=d(),id=n("span"),W_=p("BaseModelOutputWithPoolingAndCrossAttentions"),Il=d(),ue=n("div"),h(Pn.$$.fragment),j_=d(),ud=n("p"),D_=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Vl=d(),le=n("h2"),so=n("a"),ld=n("span"),h(Bn.$$.fragment),H_=d(),pd=n("span"),I_=p("BaseModelOutputWithPast"),Ql=d(),pe=n("div"),h(Ln.$$.fragment),V_=d(),cd=n("p"),Q_=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Rl=d(),ce=n("h2"),ao=n("a"),hd=n("span"),h(Wn.$$.fragment),R_=d(),fd=n("span"),X_=p("BaseModelOutputWithPastAndCrossAttentions"),Xl=d(),he=n("div"),h(jn.$$.fragment),U_=d(),md=n("p"),Y_=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Ul=d(),fe=n("h2"),ro=n("a"),_d=n("span"),h(Dn.$$.fragment),J_=d(),gd=n("span"),G_=p("Seq2SeqModelOutput"),Yl=d(),me=n("div"),h(Hn.$$.fragment),K_=d(),vd=n("p"),Z_=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Jl=d(),_e=n("h2"),io=n("a"),yd=n("span"),h(In.$$.fragment),eg=d(),Td=n("span"),tg=p("CausalLMOutput"),Gl=d(),ge=n("div"),h(Vn.$$.fragment),og=d(),bd=n("p"),ng=p("Base class for causal language model (or autoregressive) outputs."),Kl=d(),ve=n("h2"),uo=n("a"),wd=n("span"),h(Qn.$$.fragment),sg=d(),xd=n("span"),ag=p("CausalLMOutputWithCrossAttentions"),Zl=d(),ye=n("div"),h(Rn.$$.fragment),rg=d(),$d=n("p"),dg=p("Base class for causal language model (or autoregressive) outputs."),ep=d(),Te=n("h2"),lo=n("a"),Od=n("span"),h(Xn.$$.fragment),ig=d(),qd=n("span"),ug=p("CausalLMOutputWithPast"),tp=d(),be=n("div"),h(Un.$$.fragment),lg=d(),Fd=n("p"),pg=p("Base class for causal language model (or autoregressive) outputs."),op=d(),we=n("h2"),po=n("a"),Sd=n("span"),h(Yn.$$.fragment),cg=d(),Md=n("span"),hg=p("MaskedLMOutput"),np=d(),xe=n("div"),h(Jn.$$.fragment),fg=d(),kd=n("p"),mg=p("Base class for masked language models outputs."),sp=d(),$e=n("h2"),co=n("a"),Ad=n("span"),h(Gn.$$.fragment),_g=d(),Cd=n("span"),gg=p("Seq2SeqLMOutput"),ap=d(),Oe=n("div"),h(Kn.$$.fragment),vg=d(),Ed=n("p"),yg=p("Base class for sequence-to-sequence language models outputs."),rp=d(),qe=n("h2"),ho=n("a"),Nd=n("span"),h(Zn.$$.fragment),Tg=d(),zd=n("span"),bg=p("NextSentencePredictorOutput"),dp=d(),Fe=n("div"),h(es.$$.fragment),wg=d(),Pd=n("p"),xg=p("Base class for outputs of models predicting if two sentences are consecutive or not."),ip=d(),Se=n("h2"),fo=n("a"),Bd=n("span"),h(ts.$$.fragment),$g=d(),Ld=n("span"),Og=p("SequenceClassifierOutput"),up=d(),Me=n("div"),h(os.$$.fragment),qg=d(),Wd=n("p"),Fg=p("Base class for outputs of sentence classification models."),lp=d(),ke=n("h2"),mo=n("a"),jd=n("span"),h(ns.$$.fragment),Sg=d(),Dd=n("span"),Mg=p("Seq2SeqSequenceClassifierOutput"),pp=d(),Ae=n("div"),h(ss.$$.fragment),kg=d(),Hd=n("p"),Ag=p("Base class for outputs of sequence-to-sequence sentence classification models."),cp=d(),Ce=n("h2"),_o=n("a"),Id=n("span"),h(as.$$.fragment),Cg=d(),Vd=n("span"),Eg=p("MultipleChoiceModelOutput"),hp=d(),Ee=n("div"),h(rs.$$.fragment),Ng=d(),Qd=n("p"),zg=p("Base class for outputs of multiple choice models."),fp=d(),Ne=n("h2"),go=n("a"),Rd=n("span"),h(ds.$$.fragment),Pg=d(),Xd=n("span"),Bg=p("TokenClassifierOutput"),mp=d(),ze=n("div"),h(is.$$.fragment),Lg=d(),Ud=n("p"),Wg=p("Base class for outputs of token classification models."),_p=d(),Pe=n("h2"),vo=n("a"),Yd=n("span"),h(us.$$.fragment),jg=d(),Jd=n("span"),Dg=p("QuestionAnsweringModelOutput"),gp=d(),Be=n("div"),h(ls.$$.fragment),Hg=d(),Gd=n("p"),Ig=p("Base class for outputs of question answering models."),vp=d(),Le=n("h2"),yo=n("a"),Kd=n("span"),h(ps.$$.fragment),Vg=d(),Zd=n("span"),Qg=p("Seq2SeqQuestionAnsweringModelOutput"),yp=d(),We=n("div"),h(cs.$$.fragment),Rg=d(),ei=n("p"),Xg=p("Base class for outputs of sequence-to-sequence question answering models."),Tp=d(),je=n("h2"),To=n("a"),ti=n("span"),h(hs.$$.fragment),Ug=d(),oi=n("span"),Yg=p("SemanticSegmenterOutput"),bp=d(),De=n("div"),h(fs.$$.fragment),Jg=d(),ni=n("p"),Gg=p("Base class for outputs of semantic segmentation models."),wp=d(),He=n("h2"),bo=n("a"),si=n("span"),h(ms.$$.fragment),Kg=d(),ai=n("span"),Zg=p("ImageClassifierOutput"),xp=d(),Ie=n("div"),h(_s.$$.fragment),ev=d(),ri=n("p"),tv=p("Base class for outputs of image classification models."),$p=d(),Ve=n("h2"),wo=n("a"),di=n("span"),h(gs.$$.fragment),ov=d(),ii=n("span"),nv=p("ImageClassifierOutputWithNoAttention"),Op=d(),Qe=n("div"),h(vs.$$.fragment),sv=d(),ui=n("p"),av=p("Base class for outputs of image classification models."),qp=d(),Re=n("h2"),xo=n("a"),li=n("span"),h(ys.$$.fragment),rv=d(),pi=n("span"),dv=p("DepthEstimatorOutput"),Fp=d(),Xe=n("div"),h(Ts.$$.fragment),iv=d(),ci=n("p"),uv=p("Base class for outputs of depth estimation models."),Sp=d(),Ue=n("h2"),$o=n("a"),hi=n("span"),h(bs.$$.fragment),lv=d(),fi=n("span"),pv=p("Wav2Vec2BaseModelOutput"),Mp=d(),Ye=n("div"),h(ws.$$.fragment),cv=d(),xs=n("p"),hv=p("Output type of "),mi=n("code"),fv=p("Wav2Vec2BaseModelOutput"),mv=p(", with potential hidden states and attentions."),kp=d(),Je=n("h2"),Oo=n("a"),_i=n("span"),h($s.$$.fragment),_v=d(),gi=n("span"),gv=p("XVectorOutput"),Ap=d(),Ge=n("div"),h(Os.$$.fragment),vv=d(),qs=n("p"),yv=p("Output type of "),rr=n("a"),Tv=p("Wav2Vec2ForXVector"),bv=p("."),Cp=d(),Ke=n("h2"),qo=n("a"),vi=n("span"),h(Fs.$$.fragment),wv=d(),yi=n("span"),xv=p("TFBaseModelOutput"),Ep=d(),Ze=n("div"),h(Ss.$$.fragment),$v=d(),Ti=n("p"),Ov=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Np=d(),et=n("h2"),Fo=n("a"),bi=n("span"),h(Ms.$$.fragment),qv=d(),wi=n("span"),Fv=p("TFBaseModelOutputWithPooling"),zp=d(),tt=n("div"),h(ks.$$.fragment),Sv=d(),xi=n("p"),Mv=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Pp=d(),ot=n("h2"),So=n("a"),$i=n("span"),h(As.$$.fragment),kv=d(),Oi=n("span"),Av=p("TFBaseModelOutputWithPoolingAndCrossAttentions"),Bp=d(),nt=n("div"),h(Cs.$$.fragment),Cv=d(),qi=n("p"),Ev=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Lp=d(),st=n("h2"),Mo=n("a"),Fi=n("span"),h(Es.$$.fragment),Nv=d(),Si=n("span"),zv=p("TFBaseModelOutputWithPast"),Wp=d(),at=n("div"),h(Ns.$$.fragment),Pv=d(),Mi=n("p"),Bv=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),jp=d(),rt=n("h2"),ko=n("a"),ki=n("span"),h(zs.$$.fragment),Lv=d(),Ai=n("span"),Wv=p("TFBaseModelOutputWithPastAndCrossAttentions"),Dp=d(),dt=n("div"),h(Ps.$$.fragment),jv=d(),Ci=n("p"),Dv=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Hp=d(),it=n("h2"),Ao=n("a"),Ei=n("span"),h(Bs.$$.fragment),Hv=d(),Ni=n("span"),Iv=p("TFSeq2SeqModelOutput"),Ip=d(),ut=n("div"),h(Ls.$$.fragment),Vv=d(),zi=n("p"),Qv=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Vp=d(),lt=n("h2"),Co=n("a"),Pi=n("span"),h(Ws.$$.fragment),Rv=d(),Bi=n("span"),Xv=p("TFCausalLMOutput"),Qp=d(),pt=n("div"),h(js.$$.fragment),Uv=d(),Li=n("p"),Yv=p("Base class for causal language model (or autoregressive) outputs."),Rp=d(),ct=n("h2"),Eo=n("a"),Wi=n("span"),h(Ds.$$.fragment),Jv=d(),ji=n("span"),Gv=p("TFCausalLMOutputWithCrossAttentions"),Xp=d(),ht=n("div"),h(Hs.$$.fragment),Kv=d(),Di=n("p"),Zv=p("Base class for causal language model (or autoregressive) outputs."),Up=d(),ft=n("h2"),No=n("a"),Hi=n("span"),h(Is.$$.fragment),ey=d(),Ii=n("span"),ty=p("TFCausalLMOutputWithPast"),Yp=d(),mt=n("div"),h(Vs.$$.fragment),oy=d(),Vi=n("p"),ny=p("Base class for causal language model (or autoregressive) outputs."),Jp=d(),_t=n("h2"),zo=n("a"),Qi=n("span"),h(Qs.$$.fragment),sy=d(),Ri=n("span"),ay=p("TFMaskedLMOutput"),Gp=d(),gt=n("div"),h(Rs.$$.fragment),ry=d(),Xi=n("p"),dy=p("Base class for masked language models outputs."),Kp=d(),vt=n("h2"),Po=n("a"),Ui=n("span"),h(Xs.$$.fragment),iy=d(),Yi=n("span"),uy=p("TFSeq2SeqLMOutput"),Zp=d(),yt=n("div"),h(Us.$$.fragment),ly=d(),Ji=n("p"),py=p("Base class for sequence-to-sequence language models outputs."),ec=d(),Tt=n("h2"),Bo=n("a"),Gi=n("span"),h(Ys.$$.fragment),cy=d(),Ki=n("span"),hy=p("TFNextSentencePredictorOutput"),tc=d(),bt=n("div"),h(Js.$$.fragment),fy=d(),Zi=n("p"),my=p("Base class for outputs of models predicting if two sentences are consecutive or not."),oc=d(),wt=n("h2"),Lo=n("a"),eu=n("span"),h(Gs.$$.fragment),_y=d(),tu=n("span"),gy=p("TFSequenceClassifierOutput"),nc=d(),xt=n("div"),h(Ks.$$.fragment),vy=d(),ou=n("p"),yy=p("Base class for outputs of sentence classification models."),sc=d(),$t=n("h2"),Wo=n("a"),nu=n("span"),h(Zs.$$.fragment),Ty=d(),su=n("span"),by=p("TFSeq2SeqSequenceClassifierOutput"),ac=d(),Ot=n("div"),h(ea.$$.fragment),wy=d(),au=n("p"),xy=p("Base class for outputs of sequence-to-sequence sentence classification models."),rc=d(),qt=n("h2"),jo=n("a"),ru=n("span"),h(ta.$$.fragment),$y=d(),du=n("span"),Oy=p("TFMultipleChoiceModelOutput"),dc=d(),Ft=n("div"),h(oa.$$.fragment),qy=d(),iu=n("p"),Fy=p("Base class for outputs of multiple choice models."),ic=d(),St=n("h2"),Do=n("a"),uu=n("span"),h(na.$$.fragment),Sy=d(),lu=n("span"),My=p("TFTokenClassifierOutput"),uc=d(),Mt=n("div"),h(sa.$$.fragment),ky=d(),pu=n("p"),Ay=p("Base class for outputs of token classification models."),lc=d(),kt=n("h2"),Ho=n("a"),cu=n("span"),h(aa.$$.fragment),Cy=d(),hu=n("span"),Ey=p("TFQuestionAnsweringModelOutput"),pc=d(),At=n("div"),h(ra.$$.fragment),Ny=d(),fu=n("p"),zy=p("Base class for outputs of question answering models."),cc=d(),Ct=n("h2"),Io=n("a"),mu=n("span"),h(da.$$.fragment),Py=d(),_u=n("span"),By=p("TFSeq2SeqQuestionAnsweringModelOutput"),hc=d(),Et=n("div"),h(ia.$$.fragment),Ly=d(),gu=n("p"),Wy=p("Base class for outputs of sequence-to-sequence question answering models."),fc=d(),Nt=n("h2"),Vo=n("a"),vu=n("span"),h(ua.$$.fragment),jy=d(),yu=n("span"),Dy=p("FlaxBaseModelOutput"),mc=d(),E=n("div"),h(la.$$.fragment),Hy=d(),Tu=n("p"),Iy=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Vy=d(),Qo=n("div"),h(pa.$$.fragment),Qy=d(),bu=n("p"),Ry=p("\u201CReturns a new object replacing the specified fields with new values."),_c=d(),zt=n("h2"),Ro=n("a"),wu=n("span"),h(ca.$$.fragment),Xy=d(),xu=n("span"),Uy=p("FlaxBaseModelOutputWithPast"),gc=d(),N=n("div"),h(ha.$$.fragment),Yy=d(),$u=n("p"),Jy=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Gy=d(),Xo=n("div"),h(fa.$$.fragment),Ky=d(),Ou=n("p"),Zy=p("\u201CReturns a new object replacing the specified fields with new values."),vc=d(),Pt=n("h2"),Uo=n("a"),qu=n("span"),h(ma.$$.fragment),eT=d(),Fu=n("span"),tT=p("FlaxBaseModelOutputWithPooling"),yc=d(),z=n("div"),h(_a.$$.fragment),oT=d(),Su=n("p"),nT=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),sT=d(),Yo=n("div"),h(ga.$$.fragment),aT=d(),Mu=n("p"),rT=p("\u201CReturns a new object replacing the specified fields with new values."),Tc=d(),Bt=n("h2"),Jo=n("a"),ku=n("span"),h(va.$$.fragment),dT=d(),Au=n("span"),iT=p("FlaxBaseModelOutputWithPastAndCrossAttentions"),bc=d(),P=n("div"),h(ya.$$.fragment),uT=d(),Cu=n("p"),lT=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),pT=d(),Go=n("div"),h(Ta.$$.fragment),cT=d(),Eu=n("p"),hT=p("\u201CReturns a new object replacing the specified fields with new values."),wc=d(),Lt=n("h2"),Ko=n("a"),Nu=n("span"),h(ba.$$.fragment),fT=d(),zu=n("span"),mT=p("FlaxSeq2SeqModelOutput"),xc=d(),B=n("div"),h(wa.$$.fragment),_T=d(),Pu=n("p"),gT=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),vT=d(),Zo=n("div"),h(xa.$$.fragment),yT=d(),Bu=n("p"),TT=p("\u201CReturns a new object replacing the specified fields with new values."),$c=d(),Wt=n("h2"),en=n("a"),Lu=n("span"),h($a.$$.fragment),bT=d(),Wu=n("span"),wT=p("FlaxCausalLMOutputWithCrossAttentions"),Oc=d(),L=n("div"),h(Oa.$$.fragment),xT=d(),ju=n("p"),$T=p("Base class for causal language model (or autoregressive) outputs."),OT=d(),tn=n("div"),h(qa.$$.fragment),qT=d(),Du=n("p"),FT=p("\u201CReturns a new object replacing the specified fields with new values."),qc=d(),jt=n("h2"),on=n("a"),Hu=n("span"),h(Fa.$$.fragment),ST=d(),Iu=n("span"),MT=p("FlaxMaskedLMOutput"),Fc=d(),W=n("div"),h(Sa.$$.fragment),kT=d(),Vu=n("p"),AT=p("Base class for masked language models outputs."),CT=d(),nn=n("div"),h(Ma.$$.fragment),ET=d(),Qu=n("p"),NT=p("\u201CReturns a new object replacing the specified fields with new values."),Sc=d(),Dt=n("h2"),sn=n("a"),Ru=n("span"),h(ka.$$.fragment),zT=d(),Xu=n("span"),PT=p("FlaxSeq2SeqLMOutput"),Mc=d(),j=n("div"),h(Aa.$$.fragment),BT=d(),Uu=n("p"),LT=p("Base class for sequence-to-sequence language models outputs."),WT=d(),an=n("div"),h(Ca.$$.fragment),jT=d(),Yu=n("p"),DT=p("\u201CReturns a new object replacing the specified fields with new values."),kc=d(),Ht=n("h2"),rn=n("a"),Ju=n("span"),h(Ea.$$.fragment),HT=d(),Gu=n("span"),IT=p("FlaxNextSentencePredictorOutput"),Ac=d(),D=n("div"),h(Na.$$.fragment),VT=d(),Ku=n("p"),QT=p("Base class for outputs of models predicting if two sentences are consecutive or not."),RT=d(),dn=n("div"),h(za.$$.fragment),XT=d(),Zu=n("p"),UT=p("\u201CReturns a new object replacing the specified fields with new values."),Cc=d(),It=n("h2"),un=n("a"),el=n("span"),h(Pa.$$.fragment),YT=d(),tl=n("span"),JT=p("FlaxSequenceClassifierOutput"),Ec=d(),H=n("div"),h(Ba.$$.fragment),GT=d(),ol=n("p"),KT=p("Base class for outputs of sentence classification models."),ZT=d(),ln=n("div"),h(La.$$.fragment),e2=d(),nl=n("p"),t2=p("\u201CReturns a new object replacing the specified fields with new values."),Nc=d(),Vt=n("h2"),pn=n("a"),sl=n("span"),h(Wa.$$.fragment),o2=d(),al=n("span"),n2=p("FlaxSeq2SeqSequenceClassifierOutput"),zc=d(),I=n("div"),h(ja.$$.fragment),s2=d(),rl=n("p"),a2=p("Base class for outputs of sequence-to-sequence sentence classification models."),r2=d(),cn=n("div"),h(Da.$$.fragment),d2=d(),dl=n("p"),i2=p("\u201CReturns a new object replacing the specified fields with new values."),Pc=d(),Qt=n("h2"),hn=n("a"),il=n("span"),h(Ha.$$.fragment),u2=d(),ul=n("span"),l2=p("FlaxMultipleChoiceModelOutput"),Bc=d(),V=n("div"),h(Ia.$$.fragment),p2=d(),ll=n("p"),c2=p("Base class for outputs of multiple choice models."),h2=d(),fn=n("div"),h(Va.$$.fragment),f2=d(),pl=n("p"),m2=p("\u201CReturns a new object replacing the specified fields with new values."),Lc=d(),Rt=n("h2"),mn=n("a"),cl=n("span"),h(Qa.$$.fragment),_2=d(),hl=n("span"),g2=p("FlaxTokenClassifierOutput"),Wc=d(),Q=n("div"),h(Ra.$$.fragment),v2=d(),fl=n("p"),y2=p("Base class for outputs of token classification models."),T2=d(),_n=n("div"),h(Xa.$$.fragment),b2=d(),ml=n("p"),w2=p("\u201CReturns a new object replacing the specified fields with new values."),jc=d(),Xt=n("h2"),gn=n("a"),_l=n("span"),h(Ua.$$.fragment),x2=d(),gl=n("span"),$2=p("FlaxQuestionAnsweringModelOutput"),Dc=d(),R=n("div"),h(Ya.$$.fragment),O2=d(),vl=n("p"),q2=p("Base class for outputs of question answering models."),F2=d(),vn=n("div"),h(Ja.$$.fragment),S2=d(),yl=n("p"),M2=p("\u201CReturns a new object replacing the specified fields with new values."),Hc=d(),Ut=n("h2"),yn=n("a"),Tl=n("span"),h(Ga.$$.fragment),k2=d(),bl=n("span"),A2=p("FlaxSeq2SeqQuestionAnsweringModelOutput"),Ic=d(),X=n("div"),h(Ka.$$.fragment),C2=d(),wl=n("p"),E2=p("Base class for outputs of sequence-to-sequence question answering models."),N2=d(),Tn=n("div"),h(Za.$$.fragment),z2=d(),xl=n("p"),P2=p("\u201CReturns a new object replacing the specified fields with new values."),this.h()},l(e){const u=tO('[data-svelte="svelte-1phssyn"]',document.head);x=s(u,"META",{name:!0,content:!0}),u.forEach(t),Yt=i(e),$=s(e,"H1",{class:!0});var er=a($);A=s(er,"A",{id:!0,class:!0,href:!0});var B2=a(A);G=s(B2,"SPAN",{});var L2=a(G);f(O.$$.fragment,L2),L2.forEach(t),B2.forEach(t),wn=i(er),K=s(er,"SPAN",{});var W2=a(K);Z=c(W2,"Model outputs"),W2.forEach(t),er.forEach(t),M=i(e),C=s(e,"P",{});var Qc=a(C);tr=c(Qc,"All models have outputs that are instances of subclasses of "),or=s(Qc,"A",{href:!0});var j2=a(or);dm=c(j2,"ModelOutput"),j2.forEach(t),im=c(Qc,`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Qc.forEach(t),Ol=i(e),nr=s(e,"P",{});var D2=a(nr);um=c(D2,"Let\u2019s see of this looks on an example:"),D2.forEach(t),ql=i(e),f(xn.$$.fragment,e),Fl=i(e),b=s(e,"P",{});var w=a(b);lm=c(w,"The "),xr=s(w,"CODE",{});var H2=a(xr);pm=c(H2,"outputs"),H2.forEach(t),cm=c(w," object is a "),sr=s(w,"A",{href:!0});var I2=a(sr);hm=c(I2,"SequenceClassifierOutput"),I2.forEach(t),fm=c(w,`, as we can see in the
documentation of that class below, it means it has an optional `),$r=s(w,"CODE",{});var V2=a($r);mm=c(V2,"loss"),V2.forEach(t),_m=c(w,", a "),Or=s(w,"CODE",{});var Q2=a(Or);gm=c(Q2,"logits"),Q2.forEach(t),vm=c(w," an optional "),qr=s(w,"CODE",{});var R2=a(qr);ym=c(R2,"hidden_states"),R2.forEach(t),Tm=c(w,` and
an optional `),Fr=s(w,"CODE",{});var X2=a(Fr);bm=c(X2,"attentions"),X2.forEach(t),wm=c(w," attribute. Here we have the "),Sr=s(w,"CODE",{});var U2=a(Sr);xm=c(U2,"loss"),U2.forEach(t),$m=c(w," since we passed along "),Mr=s(w,"CODE",{});var Y2=a(Mr);Om=c(Y2,"labels"),Y2.forEach(t),qm=c(w,`, but we don\u2019t have
`),kr=s(w,"CODE",{});var J2=a(kr);Fm=c(J2,"hidden_states"),J2.forEach(t),Sm=c(w," and "),Ar=s(w,"CODE",{});var G2=a(Ar);Mm=c(G2,"attentions"),G2.forEach(t),km=c(w," because we didn\u2019t pass "),Cr=s(w,"CODE",{});var K2=a(Cr);Am=c(K2,"output_hidden_states=True"),K2.forEach(t),Cm=c(w,` or
`),Er=s(w,"CODE",{});var Z2=a(Er);Em=c(Z2,"output_attentions=True"),Z2.forEach(t),Nm=c(w,"."),w.forEach(t),Sl=i(e),q=s(e,"P",{});var U=a(q);zm=c(U,`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),Nr=s(U,"CODE",{});var eb=a(Nr);Pm=c(eb,"None"),eb.forEach(t),Bm=c(U,". Here for instance "),zr=s(U,"CODE",{});var tb=a(zr);Lm=c(tb,"outputs.loss"),tb.forEach(t),Wm=c(U," is the loss computed by the model, and "),Pr=s(U,"CODE",{});var ob=a(Pr);jm=c(ob,"outputs.attentions"),ob.forEach(t),Dm=c(U,` is
`),Br=s(U,"CODE",{});var nb=a(Br);Hm=c(nb,"None"),nb.forEach(t),Im=c(U,"."),U.forEach(t),Ml=i(e),F=s(e,"P",{});var Y=a(F);Vm=c(Y,"When considering our "),Lr=s(Y,"CODE",{});var sb=a(Lr);Qm=c(sb,"outputs"),sb.forEach(t),Rm=c(Y," object as tuple, it only considers the attributes that don\u2019t have "),Wr=s(Y,"CODE",{});var ab=a(Wr);Xm=c(ab,"None"),ab.forEach(t),Um=c(Y,` values.
Here for instance, it has two elements, `),jr=s(Y,"CODE",{});var rb=a(jr);Ym=c(rb,"loss"),rb.forEach(t),Jm=c(Y," then "),Dr=s(Y,"CODE",{});var db=a(Dr);Gm=c(db,"logits"),db.forEach(t),Km=c(Y,", so"),Y.forEach(t),kl=i(e),f($n.$$.fragment,e),Al=i(e),Jt=s(e,"P",{});var Rc=a(Jt);Zm=c(Rc,"will return the tuple "),Hr=s(Rc,"CODE",{});var ib=a(Hr);e_=c(ib,"(outputs.loss, outputs.logits)"),ib.forEach(t),t_=c(Rc," for instance."),Rc.forEach(t),Cl=i(e),S=s(e,"P",{});var J=a(S);o_=c(J,"When considering our "),Ir=s(J,"CODE",{});var ub=a(Ir);n_=c(ub,"outputs"),ub.forEach(t),s_=c(J," object as dictionary, it only considers the attributes that don\u2019t have "),Vr=s(J,"CODE",{});var lb=a(Vr);a_=c(lb,"None"),lb.forEach(t),r_=c(J,`
values. Here for instance, it has two keys that are `),Qr=s(J,"CODE",{});var pb=a(Qr);d_=c(pb,"loss"),pb.forEach(t),i_=c(J," and "),Rr=s(J,"CODE",{});var cb=a(Rr);u_=c(cb,"logits"),cb.forEach(t),l_=c(J,"."),J.forEach(t),El=i(e),ar=s(e,"P",{});var hb=a(ar);p_=c(hb,`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),hb.forEach(t),Nl=i(e),ee=s(e,"H2",{class:!0});var Xc=a(ee);Gt=s(Xc,"A",{id:!0,class:!0,href:!0});var fb=a(Gt);Xr=s(fb,"SPAN",{});var mb=a(Xr);f(On.$$.fragment,mb),mb.forEach(t),fb.forEach(t),c_=i(Xc),Ur=s(Xc,"SPAN",{});var _b=a(Ur);h_=c(_b,"ModelOutput"),_b.forEach(t),Xc.forEach(t),zl=i(e),k=s(e,"DIV",{class:!0});var bn=a(k);f(qn.$$.fragment,bn),f_=i(bn),te=s(bn,"P",{});var dr=a(te);m_=c(dr,"Base class for all model outputs as dataclass. Has a "),Yr=s(dr,"CODE",{});var gb=a(Yr);__=c(gb,"__getitem__"),gb.forEach(t),g_=c(dr,` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),Jr=s(dr,"CODE",{});var vb=a(Jr);v_=c(vb,"None"),vb.forEach(t),y_=c(dr,` attributes. Otherwise behaves like a regular
python dictionary.`),dr.forEach(t),T_=i(bn),f(Kt.$$.fragment,bn),b_=i(bn),Zt=s(bn,"DIV",{class:!0});var Uc=a(Zt);f(Fn.$$.fragment,Uc),w_=i(Uc),Sn=s(Uc,"P",{});var Yc=a(Sn);x_=c(Yc,"Convert self to a tuple containing all the attributes/keys that are not "),Gr=s(Yc,"CODE",{});var yb=a(Gr);$_=c(yb,"None"),yb.forEach(t),O_=c(Yc,"."),Yc.forEach(t),Uc.forEach(t),bn.forEach(t),Pl=i(e),oe=s(e,"H2",{class:!0});var Jc=a(oe);eo=s(Jc,"A",{id:!0,class:!0,href:!0});var Tb=a(eo);Kr=s(Tb,"SPAN",{});var bb=a(Kr);f(Mn.$$.fragment,bb),bb.forEach(t),Tb.forEach(t),q_=i(Jc),Zr=s(Jc,"SPAN",{});var wb=a(Zr);F_=c(wb,"BaseModelOutput"),wb.forEach(t),Jc.forEach(t),Bl=i(e),ne=s(e,"DIV",{class:!0});var Gc=a(ne);f(kn.$$.fragment,Gc),S_=i(Gc),ed=s(Gc,"P",{});var xb=a(ed);M_=c(xb,"Base class for model\u2019s outputs, with potential hidden states and attentions."),xb.forEach(t),Gc.forEach(t),Ll=i(e),se=s(e,"H2",{class:!0});var Kc=a(se);to=s(Kc,"A",{id:!0,class:!0,href:!0});var $b=a(to);td=s($b,"SPAN",{});var Ob=a(td);f(An.$$.fragment,Ob),Ob.forEach(t),$b.forEach(t),k_=i(Kc),od=s(Kc,"SPAN",{});var qb=a(od);A_=c(qb,"BaseModelOutputWithPooling"),qb.forEach(t),Kc.forEach(t),Wl=i(e),ae=s(e,"DIV",{class:!0});var Zc=a(ae);f(Cn.$$.fragment,Zc),C_=i(Zc),nd=s(Zc,"P",{});var Fb=a(nd);E_=c(Fb,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Fb.forEach(t),Zc.forEach(t),jl=i(e),re=s(e,"H2",{class:!0});var eh=a(re);oo=s(eh,"A",{id:!0,class:!0,href:!0});var Sb=a(oo);sd=s(Sb,"SPAN",{});var Mb=a(sd);f(En.$$.fragment,Mb),Mb.forEach(t),Sb.forEach(t),N_=i(eh),ad=s(eh,"SPAN",{});var kb=a(ad);z_=c(kb,"BaseModelOutputWithCrossAttentions"),kb.forEach(t),eh.forEach(t),Dl=i(e),de=s(e,"DIV",{class:!0});var th=a(de);f(Nn.$$.fragment,th),P_=i(th),rd=s(th,"P",{});var Ab=a(rd);B_=c(Ab,"Base class for model\u2019s outputs, with potential hidden states and attentions."),Ab.forEach(t),th.forEach(t),Hl=i(e),ie=s(e,"H2",{class:!0});var oh=a(ie);no=s(oh,"A",{id:!0,class:!0,href:!0});var Cb=a(no);dd=s(Cb,"SPAN",{});var Eb=a(dd);f(zn.$$.fragment,Eb),Eb.forEach(t),Cb.forEach(t),L_=i(oh),id=s(oh,"SPAN",{});var Nb=a(id);W_=c(Nb,"BaseModelOutputWithPoolingAndCrossAttentions"),Nb.forEach(t),oh.forEach(t),Il=i(e),ue=s(e,"DIV",{class:!0});var nh=a(ue);f(Pn.$$.fragment,nh),j_=i(nh),ud=s(nh,"P",{});var zb=a(ud);D_=c(zb,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),zb.forEach(t),nh.forEach(t),Vl=i(e),le=s(e,"H2",{class:!0});var sh=a(le);so=s(sh,"A",{id:!0,class:!0,href:!0});var Pb=a(so);ld=s(Pb,"SPAN",{});var Bb=a(ld);f(Bn.$$.fragment,Bb),Bb.forEach(t),Pb.forEach(t),H_=i(sh),pd=s(sh,"SPAN",{});var Lb=a(pd);I_=c(Lb,"BaseModelOutputWithPast"),Lb.forEach(t),sh.forEach(t),Ql=i(e),pe=s(e,"DIV",{class:!0});var ah=a(pe);f(Ln.$$.fragment,ah),V_=i(ah),cd=s(ah,"P",{});var Wb=a(cd);Q_=c(Wb,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Wb.forEach(t),ah.forEach(t),Rl=i(e),ce=s(e,"H2",{class:!0});var rh=a(ce);ao=s(rh,"A",{id:!0,class:!0,href:!0});var jb=a(ao);hd=s(jb,"SPAN",{});var Db=a(hd);f(Wn.$$.fragment,Db),Db.forEach(t),jb.forEach(t),R_=i(rh),fd=s(rh,"SPAN",{});var Hb=a(fd);X_=c(Hb,"BaseModelOutputWithPastAndCrossAttentions"),Hb.forEach(t),rh.forEach(t),Xl=i(e),he=s(e,"DIV",{class:!0});var dh=a(he);f(jn.$$.fragment,dh),U_=i(dh),md=s(dh,"P",{});var Ib=a(md);Y_=c(Ib,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Ib.forEach(t),dh.forEach(t),Ul=i(e),fe=s(e,"H2",{class:!0});var ih=a(fe);ro=s(ih,"A",{id:!0,class:!0,href:!0});var Vb=a(ro);_d=s(Vb,"SPAN",{});var Qb=a(_d);f(Dn.$$.fragment,Qb),Qb.forEach(t),Vb.forEach(t),J_=i(ih),gd=s(ih,"SPAN",{});var Rb=a(gd);G_=c(Rb,"Seq2SeqModelOutput"),Rb.forEach(t),ih.forEach(t),Yl=i(e),me=s(e,"DIV",{class:!0});var uh=a(me);f(Hn.$$.fragment,uh),K_=i(uh),vd=s(uh,"P",{});var Xb=a(vd);Z_=c(Xb,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Xb.forEach(t),uh.forEach(t),Jl=i(e),_e=s(e,"H2",{class:!0});var lh=a(_e);io=s(lh,"A",{id:!0,class:!0,href:!0});var Ub=a(io);yd=s(Ub,"SPAN",{});var Yb=a(yd);f(In.$$.fragment,Yb),Yb.forEach(t),Ub.forEach(t),eg=i(lh),Td=s(lh,"SPAN",{});var Jb=a(Td);tg=c(Jb,"CausalLMOutput"),Jb.forEach(t),lh.forEach(t),Gl=i(e),ge=s(e,"DIV",{class:!0});var ph=a(ge);f(Vn.$$.fragment,ph),og=i(ph),bd=s(ph,"P",{});var Gb=a(bd);ng=c(Gb,"Base class for causal language model (or autoregressive) outputs."),Gb.forEach(t),ph.forEach(t),Kl=i(e),ve=s(e,"H2",{class:!0});var ch=a(ve);uo=s(ch,"A",{id:!0,class:!0,href:!0});var Kb=a(uo);wd=s(Kb,"SPAN",{});var Zb=a(wd);f(Qn.$$.fragment,Zb),Zb.forEach(t),Kb.forEach(t),sg=i(ch),xd=s(ch,"SPAN",{});var ew=a(xd);ag=c(ew,"CausalLMOutputWithCrossAttentions"),ew.forEach(t),ch.forEach(t),Zl=i(e),ye=s(e,"DIV",{class:!0});var hh=a(ye);f(Rn.$$.fragment,hh),rg=i(hh),$d=s(hh,"P",{});var tw=a($d);dg=c(tw,"Base class for causal language model (or autoregressive) outputs."),tw.forEach(t),hh.forEach(t),ep=i(e),Te=s(e,"H2",{class:!0});var fh=a(Te);lo=s(fh,"A",{id:!0,class:!0,href:!0});var ow=a(lo);Od=s(ow,"SPAN",{});var nw=a(Od);f(Xn.$$.fragment,nw),nw.forEach(t),ow.forEach(t),ig=i(fh),qd=s(fh,"SPAN",{});var sw=a(qd);ug=c(sw,"CausalLMOutputWithPast"),sw.forEach(t),fh.forEach(t),tp=i(e),be=s(e,"DIV",{class:!0});var mh=a(be);f(Un.$$.fragment,mh),lg=i(mh),Fd=s(mh,"P",{});var aw=a(Fd);pg=c(aw,"Base class for causal language model (or autoregressive) outputs."),aw.forEach(t),mh.forEach(t),op=i(e),we=s(e,"H2",{class:!0});var _h=a(we);po=s(_h,"A",{id:!0,class:!0,href:!0});var rw=a(po);Sd=s(rw,"SPAN",{});var dw=a(Sd);f(Yn.$$.fragment,dw),dw.forEach(t),rw.forEach(t),cg=i(_h),Md=s(_h,"SPAN",{});var iw=a(Md);hg=c(iw,"MaskedLMOutput"),iw.forEach(t),_h.forEach(t),np=i(e),xe=s(e,"DIV",{class:!0});var gh=a(xe);f(Jn.$$.fragment,gh),fg=i(gh),kd=s(gh,"P",{});var uw=a(kd);mg=c(uw,"Base class for masked language models outputs."),uw.forEach(t),gh.forEach(t),sp=i(e),$e=s(e,"H2",{class:!0});var vh=a($e);co=s(vh,"A",{id:!0,class:!0,href:!0});var lw=a(co);Ad=s(lw,"SPAN",{});var pw=a(Ad);f(Gn.$$.fragment,pw),pw.forEach(t),lw.forEach(t),_g=i(vh),Cd=s(vh,"SPAN",{});var cw=a(Cd);gg=c(cw,"Seq2SeqLMOutput"),cw.forEach(t),vh.forEach(t),ap=i(e),Oe=s(e,"DIV",{class:!0});var yh=a(Oe);f(Kn.$$.fragment,yh),vg=i(yh),Ed=s(yh,"P",{});var hw=a(Ed);yg=c(hw,"Base class for sequence-to-sequence language models outputs."),hw.forEach(t),yh.forEach(t),rp=i(e),qe=s(e,"H2",{class:!0});var Th=a(qe);ho=s(Th,"A",{id:!0,class:!0,href:!0});var fw=a(ho);Nd=s(fw,"SPAN",{});var mw=a(Nd);f(Zn.$$.fragment,mw),mw.forEach(t),fw.forEach(t),Tg=i(Th),zd=s(Th,"SPAN",{});var _w=a(zd);bg=c(_w,"NextSentencePredictorOutput"),_w.forEach(t),Th.forEach(t),dp=i(e),Fe=s(e,"DIV",{class:!0});var bh=a(Fe);f(es.$$.fragment,bh),wg=i(bh),Pd=s(bh,"P",{});var gw=a(Pd);xg=c(gw,"Base class for outputs of models predicting if two sentences are consecutive or not."),gw.forEach(t),bh.forEach(t),ip=i(e),Se=s(e,"H2",{class:!0});var wh=a(Se);fo=s(wh,"A",{id:!0,class:!0,href:!0});var vw=a(fo);Bd=s(vw,"SPAN",{});var yw=a(Bd);f(ts.$$.fragment,yw),yw.forEach(t),vw.forEach(t),$g=i(wh),Ld=s(wh,"SPAN",{});var Tw=a(Ld);Og=c(Tw,"SequenceClassifierOutput"),Tw.forEach(t),wh.forEach(t),up=i(e),Me=s(e,"DIV",{class:!0});var xh=a(Me);f(os.$$.fragment,xh),qg=i(xh),Wd=s(xh,"P",{});var bw=a(Wd);Fg=c(bw,"Base class for outputs of sentence classification models."),bw.forEach(t),xh.forEach(t),lp=i(e),ke=s(e,"H2",{class:!0});var $h=a(ke);mo=s($h,"A",{id:!0,class:!0,href:!0});var ww=a(mo);jd=s(ww,"SPAN",{});var xw=a(jd);f(ns.$$.fragment,xw),xw.forEach(t),ww.forEach(t),Sg=i($h),Dd=s($h,"SPAN",{});var $w=a(Dd);Mg=c($w,"Seq2SeqSequenceClassifierOutput"),$w.forEach(t),$h.forEach(t),pp=i(e),Ae=s(e,"DIV",{class:!0});var Oh=a(Ae);f(ss.$$.fragment,Oh),kg=i(Oh),Hd=s(Oh,"P",{});var Ow=a(Hd);Ag=c(Ow,"Base class for outputs of sequence-to-sequence sentence classification models."),Ow.forEach(t),Oh.forEach(t),cp=i(e),Ce=s(e,"H2",{class:!0});var qh=a(Ce);_o=s(qh,"A",{id:!0,class:!0,href:!0});var qw=a(_o);Id=s(qw,"SPAN",{});var Fw=a(Id);f(as.$$.fragment,Fw),Fw.forEach(t),qw.forEach(t),Cg=i(qh),Vd=s(qh,"SPAN",{});var Sw=a(Vd);Eg=c(Sw,"MultipleChoiceModelOutput"),Sw.forEach(t),qh.forEach(t),hp=i(e),Ee=s(e,"DIV",{class:!0});var Fh=a(Ee);f(rs.$$.fragment,Fh),Ng=i(Fh),Qd=s(Fh,"P",{});var Mw=a(Qd);zg=c(Mw,"Base class for outputs of multiple choice models."),Mw.forEach(t),Fh.forEach(t),fp=i(e),Ne=s(e,"H2",{class:!0});var Sh=a(Ne);go=s(Sh,"A",{id:!0,class:!0,href:!0});var kw=a(go);Rd=s(kw,"SPAN",{});var Aw=a(Rd);f(ds.$$.fragment,Aw),Aw.forEach(t),kw.forEach(t),Pg=i(Sh),Xd=s(Sh,"SPAN",{});var Cw=a(Xd);Bg=c(Cw,"TokenClassifierOutput"),Cw.forEach(t),Sh.forEach(t),mp=i(e),ze=s(e,"DIV",{class:!0});var Mh=a(ze);f(is.$$.fragment,Mh),Lg=i(Mh),Ud=s(Mh,"P",{});var Ew=a(Ud);Wg=c(Ew,"Base class for outputs of token classification models."),Ew.forEach(t),Mh.forEach(t),_p=i(e),Pe=s(e,"H2",{class:!0});var kh=a(Pe);vo=s(kh,"A",{id:!0,class:!0,href:!0});var Nw=a(vo);Yd=s(Nw,"SPAN",{});var zw=a(Yd);f(us.$$.fragment,zw),zw.forEach(t),Nw.forEach(t),jg=i(kh),Jd=s(kh,"SPAN",{});var Pw=a(Jd);Dg=c(Pw,"QuestionAnsweringModelOutput"),Pw.forEach(t),kh.forEach(t),gp=i(e),Be=s(e,"DIV",{class:!0});var Ah=a(Be);f(ls.$$.fragment,Ah),Hg=i(Ah),Gd=s(Ah,"P",{});var Bw=a(Gd);Ig=c(Bw,"Base class for outputs of question answering models."),Bw.forEach(t),Ah.forEach(t),vp=i(e),Le=s(e,"H2",{class:!0});var Ch=a(Le);yo=s(Ch,"A",{id:!0,class:!0,href:!0});var Lw=a(yo);Kd=s(Lw,"SPAN",{});var Ww=a(Kd);f(ps.$$.fragment,Ww),Ww.forEach(t),Lw.forEach(t),Vg=i(Ch),Zd=s(Ch,"SPAN",{});var jw=a(Zd);Qg=c(jw,"Seq2SeqQuestionAnsweringModelOutput"),jw.forEach(t),Ch.forEach(t),yp=i(e),We=s(e,"DIV",{class:!0});var Eh=a(We);f(cs.$$.fragment,Eh),Rg=i(Eh),ei=s(Eh,"P",{});var Dw=a(ei);Xg=c(Dw,"Base class for outputs of sequence-to-sequence question answering models."),Dw.forEach(t),Eh.forEach(t),Tp=i(e),je=s(e,"H2",{class:!0});var Nh=a(je);To=s(Nh,"A",{id:!0,class:!0,href:!0});var Hw=a(To);ti=s(Hw,"SPAN",{});var Iw=a(ti);f(hs.$$.fragment,Iw),Iw.forEach(t),Hw.forEach(t),Ug=i(Nh),oi=s(Nh,"SPAN",{});var Vw=a(oi);Yg=c(Vw,"SemanticSegmenterOutput"),Vw.forEach(t),Nh.forEach(t),bp=i(e),De=s(e,"DIV",{class:!0});var zh=a(De);f(fs.$$.fragment,zh),Jg=i(zh),ni=s(zh,"P",{});var Qw=a(ni);Gg=c(Qw,"Base class for outputs of semantic segmentation models."),Qw.forEach(t),zh.forEach(t),wp=i(e),He=s(e,"H2",{class:!0});var Ph=a(He);bo=s(Ph,"A",{id:!0,class:!0,href:!0});var Rw=a(bo);si=s(Rw,"SPAN",{});var Xw=a(si);f(ms.$$.fragment,Xw),Xw.forEach(t),Rw.forEach(t),Kg=i(Ph),ai=s(Ph,"SPAN",{});var Uw=a(ai);Zg=c(Uw,"ImageClassifierOutput"),Uw.forEach(t),Ph.forEach(t),xp=i(e),Ie=s(e,"DIV",{class:!0});var Bh=a(Ie);f(_s.$$.fragment,Bh),ev=i(Bh),ri=s(Bh,"P",{});var Yw=a(ri);tv=c(Yw,"Base class for outputs of image classification models."),Yw.forEach(t),Bh.forEach(t),$p=i(e),Ve=s(e,"H2",{class:!0});var Lh=a(Ve);wo=s(Lh,"A",{id:!0,class:!0,href:!0});var Jw=a(wo);di=s(Jw,"SPAN",{});var Gw=a(di);f(gs.$$.fragment,Gw),Gw.forEach(t),Jw.forEach(t),ov=i(Lh),ii=s(Lh,"SPAN",{});var Kw=a(ii);nv=c(Kw,"ImageClassifierOutputWithNoAttention"),Kw.forEach(t),Lh.forEach(t),Op=i(e),Qe=s(e,"DIV",{class:!0});var Wh=a(Qe);f(vs.$$.fragment,Wh),sv=i(Wh),ui=s(Wh,"P",{});var Zw=a(ui);av=c(Zw,"Base class for outputs of image classification models."),Zw.forEach(t),Wh.forEach(t),qp=i(e),Re=s(e,"H2",{class:!0});var jh=a(Re);xo=s(jh,"A",{id:!0,class:!0,href:!0});var e1=a(xo);li=s(e1,"SPAN",{});var t1=a(li);f(ys.$$.fragment,t1),t1.forEach(t),e1.forEach(t),rv=i(jh),pi=s(jh,"SPAN",{});var o1=a(pi);dv=c(o1,"DepthEstimatorOutput"),o1.forEach(t),jh.forEach(t),Fp=i(e),Xe=s(e,"DIV",{class:!0});var Dh=a(Xe);f(Ts.$$.fragment,Dh),iv=i(Dh),ci=s(Dh,"P",{});var n1=a(ci);uv=c(n1,"Base class for outputs of depth estimation models."),n1.forEach(t),Dh.forEach(t),Sp=i(e),Ue=s(e,"H2",{class:!0});var Hh=a(Ue);$o=s(Hh,"A",{id:!0,class:!0,href:!0});var s1=a($o);hi=s(s1,"SPAN",{});var a1=a(hi);f(bs.$$.fragment,a1),a1.forEach(t),s1.forEach(t),lv=i(Hh),fi=s(Hh,"SPAN",{});var r1=a(fi);pv=c(r1,"Wav2Vec2BaseModelOutput"),r1.forEach(t),Hh.forEach(t),Mp=i(e),Ye=s(e,"DIV",{class:!0});var Ih=a(Ye);f(ws.$$.fragment,Ih),cv=i(Ih),xs=s(Ih,"P",{});var Vh=a(xs);hv=c(Vh,"Output type of "),mi=s(Vh,"CODE",{});var d1=a(mi);fv=c(d1,"Wav2Vec2BaseModelOutput"),d1.forEach(t),mv=c(Vh,", with potential hidden states and attentions."),Vh.forEach(t),Ih.forEach(t),kp=i(e),Je=s(e,"H2",{class:!0});var Qh=a(Je);Oo=s(Qh,"A",{id:!0,class:!0,href:!0});var i1=a(Oo);_i=s(i1,"SPAN",{});var u1=a(_i);f($s.$$.fragment,u1),u1.forEach(t),i1.forEach(t),_v=i(Qh),gi=s(Qh,"SPAN",{});var l1=a(gi);gv=c(l1,"XVectorOutput"),l1.forEach(t),Qh.forEach(t),Ap=i(e),Ge=s(e,"DIV",{class:!0});var Rh=a(Ge);f(Os.$$.fragment,Rh),vv=i(Rh),qs=s(Rh,"P",{});var Xh=a(qs);yv=c(Xh,"Output type of "),rr=s(Xh,"A",{href:!0});var p1=a(rr);Tv=c(p1,"Wav2Vec2ForXVector"),p1.forEach(t),bv=c(Xh,"."),Xh.forEach(t),Rh.forEach(t),Cp=i(e),Ke=s(e,"H2",{class:!0});var Uh=a(Ke);qo=s(Uh,"A",{id:!0,class:!0,href:!0});var c1=a(qo);vi=s(c1,"SPAN",{});var h1=a(vi);f(Fs.$$.fragment,h1),h1.forEach(t),c1.forEach(t),wv=i(Uh),yi=s(Uh,"SPAN",{});var f1=a(yi);xv=c(f1,"TFBaseModelOutput"),f1.forEach(t),Uh.forEach(t),Ep=i(e),Ze=s(e,"DIV",{class:!0});var Yh=a(Ze);f(Ss.$$.fragment,Yh),$v=i(Yh),Ti=s(Yh,"P",{});var m1=a(Ti);Ov=c(m1,"Base class for model\u2019s outputs, with potential hidden states and attentions."),m1.forEach(t),Yh.forEach(t),Np=i(e),et=s(e,"H2",{class:!0});var Jh=a(et);Fo=s(Jh,"A",{id:!0,class:!0,href:!0});var _1=a(Fo);bi=s(_1,"SPAN",{});var g1=a(bi);f(Ms.$$.fragment,g1),g1.forEach(t),_1.forEach(t),qv=i(Jh),wi=s(Jh,"SPAN",{});var v1=a(wi);Fv=c(v1,"TFBaseModelOutputWithPooling"),v1.forEach(t),Jh.forEach(t),zp=i(e),tt=s(e,"DIV",{class:!0});var Gh=a(tt);f(ks.$$.fragment,Gh),Sv=i(Gh),xi=s(Gh,"P",{});var y1=a(xi);Mv=c(y1,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),y1.forEach(t),Gh.forEach(t),Pp=i(e),ot=s(e,"H2",{class:!0});var Kh=a(ot);So=s(Kh,"A",{id:!0,class:!0,href:!0});var T1=a(So);$i=s(T1,"SPAN",{});var b1=a($i);f(As.$$.fragment,b1),b1.forEach(t),T1.forEach(t),kv=i(Kh),Oi=s(Kh,"SPAN",{});var w1=a(Oi);Av=c(w1,"TFBaseModelOutputWithPoolingAndCrossAttentions"),w1.forEach(t),Kh.forEach(t),Bp=i(e),nt=s(e,"DIV",{class:!0});var Zh=a(nt);f(Cs.$$.fragment,Zh),Cv=i(Zh),qi=s(Zh,"P",{});var x1=a(qi);Ev=c(x1,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),x1.forEach(t),Zh.forEach(t),Lp=i(e),st=s(e,"H2",{class:!0});var ef=a(st);Mo=s(ef,"A",{id:!0,class:!0,href:!0});var $1=a(Mo);Fi=s($1,"SPAN",{});var O1=a(Fi);f(Es.$$.fragment,O1),O1.forEach(t),$1.forEach(t),Nv=i(ef),Si=s(ef,"SPAN",{});var q1=a(Si);zv=c(q1,"TFBaseModelOutputWithPast"),q1.forEach(t),ef.forEach(t),Wp=i(e),at=s(e,"DIV",{class:!0});var tf=a(at);f(Ns.$$.fragment,tf),Pv=i(tf),Mi=s(tf,"P",{});var F1=a(Mi);Bv=c(F1,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),F1.forEach(t),tf.forEach(t),jp=i(e),rt=s(e,"H2",{class:!0});var of=a(rt);ko=s(of,"A",{id:!0,class:!0,href:!0});var S1=a(ko);ki=s(S1,"SPAN",{});var M1=a(ki);f(zs.$$.fragment,M1),M1.forEach(t),S1.forEach(t),Lv=i(of),Ai=s(of,"SPAN",{});var k1=a(Ai);Wv=c(k1,"TFBaseModelOutputWithPastAndCrossAttentions"),k1.forEach(t),of.forEach(t),Dp=i(e),dt=s(e,"DIV",{class:!0});var nf=a(dt);f(Ps.$$.fragment,nf),jv=i(nf),Ci=s(nf,"P",{});var A1=a(Ci);Dv=c(A1,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),A1.forEach(t),nf.forEach(t),Hp=i(e),it=s(e,"H2",{class:!0});var sf=a(it);Ao=s(sf,"A",{id:!0,class:!0,href:!0});var C1=a(Ao);Ei=s(C1,"SPAN",{});var E1=a(Ei);f(Bs.$$.fragment,E1),E1.forEach(t),C1.forEach(t),Hv=i(sf),Ni=s(sf,"SPAN",{});var N1=a(Ni);Iv=c(N1,"TFSeq2SeqModelOutput"),N1.forEach(t),sf.forEach(t),Ip=i(e),ut=s(e,"DIV",{class:!0});var af=a(ut);f(Ls.$$.fragment,af),Vv=i(af),zi=s(af,"P",{});var z1=a(zi);Qv=c(z1,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),z1.forEach(t),af.forEach(t),Vp=i(e),lt=s(e,"H2",{class:!0});var rf=a(lt);Co=s(rf,"A",{id:!0,class:!0,href:!0});var P1=a(Co);Pi=s(P1,"SPAN",{});var B1=a(Pi);f(Ws.$$.fragment,B1),B1.forEach(t),P1.forEach(t),Rv=i(rf),Bi=s(rf,"SPAN",{});var L1=a(Bi);Xv=c(L1,"TFCausalLMOutput"),L1.forEach(t),rf.forEach(t),Qp=i(e),pt=s(e,"DIV",{class:!0});var df=a(pt);f(js.$$.fragment,df),Uv=i(df),Li=s(df,"P",{});var W1=a(Li);Yv=c(W1,"Base class for causal language model (or autoregressive) outputs."),W1.forEach(t),df.forEach(t),Rp=i(e),ct=s(e,"H2",{class:!0});var uf=a(ct);Eo=s(uf,"A",{id:!0,class:!0,href:!0});var j1=a(Eo);Wi=s(j1,"SPAN",{});var D1=a(Wi);f(Ds.$$.fragment,D1),D1.forEach(t),j1.forEach(t),Jv=i(uf),ji=s(uf,"SPAN",{});var H1=a(ji);Gv=c(H1,"TFCausalLMOutputWithCrossAttentions"),H1.forEach(t),uf.forEach(t),Xp=i(e),ht=s(e,"DIV",{class:!0});var lf=a(ht);f(Hs.$$.fragment,lf),Kv=i(lf),Di=s(lf,"P",{});var I1=a(Di);Zv=c(I1,"Base class for causal language model (or autoregressive) outputs."),I1.forEach(t),lf.forEach(t),Up=i(e),ft=s(e,"H2",{class:!0});var pf=a(ft);No=s(pf,"A",{id:!0,class:!0,href:!0});var V1=a(No);Hi=s(V1,"SPAN",{});var Q1=a(Hi);f(Is.$$.fragment,Q1),Q1.forEach(t),V1.forEach(t),ey=i(pf),Ii=s(pf,"SPAN",{});var R1=a(Ii);ty=c(R1,"TFCausalLMOutputWithPast"),R1.forEach(t),pf.forEach(t),Yp=i(e),mt=s(e,"DIV",{class:!0});var cf=a(mt);f(Vs.$$.fragment,cf),oy=i(cf),Vi=s(cf,"P",{});var X1=a(Vi);ny=c(X1,"Base class for causal language model (or autoregressive) outputs."),X1.forEach(t),cf.forEach(t),Jp=i(e),_t=s(e,"H2",{class:!0});var hf=a(_t);zo=s(hf,"A",{id:!0,class:!0,href:!0});var U1=a(zo);Qi=s(U1,"SPAN",{});var Y1=a(Qi);f(Qs.$$.fragment,Y1),Y1.forEach(t),U1.forEach(t),sy=i(hf),Ri=s(hf,"SPAN",{});var J1=a(Ri);ay=c(J1,"TFMaskedLMOutput"),J1.forEach(t),hf.forEach(t),Gp=i(e),gt=s(e,"DIV",{class:!0});var ff=a(gt);f(Rs.$$.fragment,ff),ry=i(ff),Xi=s(ff,"P",{});var G1=a(Xi);dy=c(G1,"Base class for masked language models outputs."),G1.forEach(t),ff.forEach(t),Kp=i(e),vt=s(e,"H2",{class:!0});var mf=a(vt);Po=s(mf,"A",{id:!0,class:!0,href:!0});var K1=a(Po);Ui=s(K1,"SPAN",{});var Z1=a(Ui);f(Xs.$$.fragment,Z1),Z1.forEach(t),K1.forEach(t),iy=i(mf),Yi=s(mf,"SPAN",{});var ex=a(Yi);uy=c(ex,"TFSeq2SeqLMOutput"),ex.forEach(t),mf.forEach(t),Zp=i(e),yt=s(e,"DIV",{class:!0});var _f=a(yt);f(Us.$$.fragment,_f),ly=i(_f),Ji=s(_f,"P",{});var tx=a(Ji);py=c(tx,"Base class for sequence-to-sequence language models outputs."),tx.forEach(t),_f.forEach(t),ec=i(e),Tt=s(e,"H2",{class:!0});var gf=a(Tt);Bo=s(gf,"A",{id:!0,class:!0,href:!0});var ox=a(Bo);Gi=s(ox,"SPAN",{});var nx=a(Gi);f(Ys.$$.fragment,nx),nx.forEach(t),ox.forEach(t),cy=i(gf),Ki=s(gf,"SPAN",{});var sx=a(Ki);hy=c(sx,"TFNextSentencePredictorOutput"),sx.forEach(t),gf.forEach(t),tc=i(e),bt=s(e,"DIV",{class:!0});var vf=a(bt);f(Js.$$.fragment,vf),fy=i(vf),Zi=s(vf,"P",{});var ax=a(Zi);my=c(ax,"Base class for outputs of models predicting if two sentences are consecutive or not."),ax.forEach(t),vf.forEach(t),oc=i(e),wt=s(e,"H2",{class:!0});var yf=a(wt);Lo=s(yf,"A",{id:!0,class:!0,href:!0});var rx=a(Lo);eu=s(rx,"SPAN",{});var dx=a(eu);f(Gs.$$.fragment,dx),dx.forEach(t),rx.forEach(t),_y=i(yf),tu=s(yf,"SPAN",{});var ix=a(tu);gy=c(ix,"TFSequenceClassifierOutput"),ix.forEach(t),yf.forEach(t),nc=i(e),xt=s(e,"DIV",{class:!0});var Tf=a(xt);f(Ks.$$.fragment,Tf),vy=i(Tf),ou=s(Tf,"P",{});var ux=a(ou);yy=c(ux,"Base class for outputs of sentence classification models."),ux.forEach(t),Tf.forEach(t),sc=i(e),$t=s(e,"H2",{class:!0});var bf=a($t);Wo=s(bf,"A",{id:!0,class:!0,href:!0});var lx=a(Wo);nu=s(lx,"SPAN",{});var px=a(nu);f(Zs.$$.fragment,px),px.forEach(t),lx.forEach(t),Ty=i(bf),su=s(bf,"SPAN",{});var cx=a(su);by=c(cx,"TFSeq2SeqSequenceClassifierOutput"),cx.forEach(t),bf.forEach(t),ac=i(e),Ot=s(e,"DIV",{class:!0});var wf=a(Ot);f(ea.$$.fragment,wf),wy=i(wf),au=s(wf,"P",{});var hx=a(au);xy=c(hx,"Base class for outputs of sequence-to-sequence sentence classification models."),hx.forEach(t),wf.forEach(t),rc=i(e),qt=s(e,"H2",{class:!0});var xf=a(qt);jo=s(xf,"A",{id:!0,class:!0,href:!0});var fx=a(jo);ru=s(fx,"SPAN",{});var mx=a(ru);f(ta.$$.fragment,mx),mx.forEach(t),fx.forEach(t),$y=i(xf),du=s(xf,"SPAN",{});var _x=a(du);Oy=c(_x,"TFMultipleChoiceModelOutput"),_x.forEach(t),xf.forEach(t),dc=i(e),Ft=s(e,"DIV",{class:!0});var $f=a(Ft);f(oa.$$.fragment,$f),qy=i($f),iu=s($f,"P",{});var gx=a(iu);Fy=c(gx,"Base class for outputs of multiple choice models."),gx.forEach(t),$f.forEach(t),ic=i(e),St=s(e,"H2",{class:!0});var Of=a(St);Do=s(Of,"A",{id:!0,class:!0,href:!0});var vx=a(Do);uu=s(vx,"SPAN",{});var yx=a(uu);f(na.$$.fragment,yx),yx.forEach(t),vx.forEach(t),Sy=i(Of),lu=s(Of,"SPAN",{});var Tx=a(lu);My=c(Tx,"TFTokenClassifierOutput"),Tx.forEach(t),Of.forEach(t),uc=i(e),Mt=s(e,"DIV",{class:!0});var qf=a(Mt);f(sa.$$.fragment,qf),ky=i(qf),pu=s(qf,"P",{});var bx=a(pu);Ay=c(bx,"Base class for outputs of token classification models."),bx.forEach(t),qf.forEach(t),lc=i(e),kt=s(e,"H2",{class:!0});var Ff=a(kt);Ho=s(Ff,"A",{id:!0,class:!0,href:!0});var wx=a(Ho);cu=s(wx,"SPAN",{});var xx=a(cu);f(aa.$$.fragment,xx),xx.forEach(t),wx.forEach(t),Cy=i(Ff),hu=s(Ff,"SPAN",{});var $x=a(hu);Ey=c($x,"TFQuestionAnsweringModelOutput"),$x.forEach(t),Ff.forEach(t),pc=i(e),At=s(e,"DIV",{class:!0});var Sf=a(At);f(ra.$$.fragment,Sf),Ny=i(Sf),fu=s(Sf,"P",{});var Ox=a(fu);zy=c(Ox,"Base class for outputs of question answering models."),Ox.forEach(t),Sf.forEach(t),cc=i(e),Ct=s(e,"H2",{class:!0});var Mf=a(Ct);Io=s(Mf,"A",{id:!0,class:!0,href:!0});var qx=a(Io);mu=s(qx,"SPAN",{});var Fx=a(mu);f(da.$$.fragment,Fx),Fx.forEach(t),qx.forEach(t),Py=i(Mf),_u=s(Mf,"SPAN",{});var Sx=a(_u);By=c(Sx,"TFSeq2SeqQuestionAnsweringModelOutput"),Sx.forEach(t),Mf.forEach(t),hc=i(e),Et=s(e,"DIV",{class:!0});var kf=a(Et);f(ia.$$.fragment,kf),Ly=i(kf),gu=s(kf,"P",{});var Mx=a(gu);Wy=c(Mx,"Base class for outputs of sequence-to-sequence question answering models."),Mx.forEach(t),kf.forEach(t),fc=i(e),Nt=s(e,"H2",{class:!0});var Af=a(Nt);Vo=s(Af,"A",{id:!0,class:!0,href:!0});var kx=a(Vo);vu=s(kx,"SPAN",{});var Ax=a(vu);f(ua.$$.fragment,Ax),Ax.forEach(t),kx.forEach(t),jy=i(Af),yu=s(Af,"SPAN",{});var Cx=a(yu);Dy=c(Cx,"FlaxBaseModelOutput"),Cx.forEach(t),Af.forEach(t),mc=i(e),E=s(e,"DIV",{class:!0});var ir=a(E);f(la.$$.fragment,ir),Hy=i(ir),Tu=s(ir,"P",{});var Ex=a(Tu);Iy=c(Ex,"Base class for model\u2019s outputs, with potential hidden states and attentions."),Ex.forEach(t),Vy=i(ir),Qo=s(ir,"DIV",{class:!0});var Cf=a(Qo);f(pa.$$.fragment,Cf),Qy=i(Cf),bu=s(Cf,"P",{});var Nx=a(bu);Ry=c(Nx,"\u201CReturns a new object replacing the specified fields with new values."),Nx.forEach(t),Cf.forEach(t),ir.forEach(t),_c=i(e),zt=s(e,"H2",{class:!0});var Ef=a(zt);Ro=s(Ef,"A",{id:!0,class:!0,href:!0});var zx=a(Ro);wu=s(zx,"SPAN",{});var Px=a(wu);f(ca.$$.fragment,Px),Px.forEach(t),zx.forEach(t),Xy=i(Ef),xu=s(Ef,"SPAN",{});var Bx=a(xu);Uy=c(Bx,"FlaxBaseModelOutputWithPast"),Bx.forEach(t),Ef.forEach(t),gc=i(e),N=s(e,"DIV",{class:!0});var ur=a(N);f(ha.$$.fragment,ur),Yy=i(ur),$u=s(ur,"P",{});var Lx=a($u);Jy=c(Lx,"Base class for model\u2019s outputs, with potential hidden states and attentions."),Lx.forEach(t),Gy=i(ur),Xo=s(ur,"DIV",{class:!0});var Nf=a(Xo);f(fa.$$.fragment,Nf),Ky=i(Nf),Ou=s(Nf,"P",{});var Wx=a(Ou);Zy=c(Wx,"\u201CReturns a new object replacing the specified fields with new values."),Wx.forEach(t),Nf.forEach(t),ur.forEach(t),vc=i(e),Pt=s(e,"H2",{class:!0});var zf=a(Pt);Uo=s(zf,"A",{id:!0,class:!0,href:!0});var jx=a(Uo);qu=s(jx,"SPAN",{});var Dx=a(qu);f(ma.$$.fragment,Dx),Dx.forEach(t),jx.forEach(t),eT=i(zf),Fu=s(zf,"SPAN",{});var Hx=a(Fu);tT=c(Hx,"FlaxBaseModelOutputWithPooling"),Hx.forEach(t),zf.forEach(t),yc=i(e),z=s(e,"DIV",{class:!0});var lr=a(z);f(_a.$$.fragment,lr),oT=i(lr),Su=s(lr,"P",{});var Ix=a(Su);nT=c(Ix,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Ix.forEach(t),sT=i(lr),Yo=s(lr,"DIV",{class:!0});var Pf=a(Yo);f(ga.$$.fragment,Pf),aT=i(Pf),Mu=s(Pf,"P",{});var Vx=a(Mu);rT=c(Vx,"\u201CReturns a new object replacing the specified fields with new values."),Vx.forEach(t),Pf.forEach(t),lr.forEach(t),Tc=i(e),Bt=s(e,"H2",{class:!0});var Bf=a(Bt);Jo=s(Bf,"A",{id:!0,class:!0,href:!0});var Qx=a(Jo);ku=s(Qx,"SPAN",{});var Rx=a(ku);f(va.$$.fragment,Rx),Rx.forEach(t),Qx.forEach(t),dT=i(Bf),Au=s(Bf,"SPAN",{});var Xx=a(Au);iT=c(Xx,"FlaxBaseModelOutputWithPastAndCrossAttentions"),Xx.forEach(t),Bf.forEach(t),bc=i(e),P=s(e,"DIV",{class:!0});var pr=a(P);f(ya.$$.fragment,pr),uT=i(pr),Cu=s(pr,"P",{});var Ux=a(Cu);lT=c(Ux,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Ux.forEach(t),pT=i(pr),Go=s(pr,"DIV",{class:!0});var Lf=a(Go);f(Ta.$$.fragment,Lf),cT=i(Lf),Eu=s(Lf,"P",{});var Yx=a(Eu);hT=c(Yx,"\u201CReturns a new object replacing the specified fields with new values."),Yx.forEach(t),Lf.forEach(t),pr.forEach(t),wc=i(e),Lt=s(e,"H2",{class:!0});var Wf=a(Lt);Ko=s(Wf,"A",{id:!0,class:!0,href:!0});var Jx=a(Ko);Nu=s(Jx,"SPAN",{});var Gx=a(Nu);f(ba.$$.fragment,Gx),Gx.forEach(t),Jx.forEach(t),fT=i(Wf),zu=s(Wf,"SPAN",{});var Kx=a(zu);mT=c(Kx,"FlaxSeq2SeqModelOutput"),Kx.forEach(t),Wf.forEach(t),xc=i(e),B=s(e,"DIV",{class:!0});var cr=a(B);f(wa.$$.fragment,cr),_T=i(cr),Pu=s(cr,"P",{});var Zx=a(Pu);gT=c(Zx,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Zx.forEach(t),vT=i(cr),Zo=s(cr,"DIV",{class:!0});var jf=a(Zo);f(xa.$$.fragment,jf),yT=i(jf),Bu=s(jf,"P",{});var e$=a(Bu);TT=c(e$,"\u201CReturns a new object replacing the specified fields with new values."),e$.forEach(t),jf.forEach(t),cr.forEach(t),$c=i(e),Wt=s(e,"H2",{class:!0});var Df=a(Wt);en=s(Df,"A",{id:!0,class:!0,href:!0});var t$=a(en);Lu=s(t$,"SPAN",{});var o$=a(Lu);f($a.$$.fragment,o$),o$.forEach(t),t$.forEach(t),bT=i(Df),Wu=s(Df,"SPAN",{});var n$=a(Wu);wT=c(n$,"FlaxCausalLMOutputWithCrossAttentions"),n$.forEach(t),Df.forEach(t),Oc=i(e),L=s(e,"DIV",{class:!0});var hr=a(L);f(Oa.$$.fragment,hr),xT=i(hr),ju=s(hr,"P",{});var s$=a(ju);$T=c(s$,"Base class for causal language model (or autoregressive) outputs."),s$.forEach(t),OT=i(hr),tn=s(hr,"DIV",{class:!0});var Hf=a(tn);f(qa.$$.fragment,Hf),qT=i(Hf),Du=s(Hf,"P",{});var a$=a(Du);FT=c(a$,"\u201CReturns a new object replacing the specified fields with new values."),a$.forEach(t),Hf.forEach(t),hr.forEach(t),qc=i(e),jt=s(e,"H2",{class:!0});var If=a(jt);on=s(If,"A",{id:!0,class:!0,href:!0});var r$=a(on);Hu=s(r$,"SPAN",{});var d$=a(Hu);f(Fa.$$.fragment,d$),d$.forEach(t),r$.forEach(t),ST=i(If),Iu=s(If,"SPAN",{});var i$=a(Iu);MT=c(i$,"FlaxMaskedLMOutput"),i$.forEach(t),If.forEach(t),Fc=i(e),W=s(e,"DIV",{class:!0});var fr=a(W);f(Sa.$$.fragment,fr),kT=i(fr),Vu=s(fr,"P",{});var u$=a(Vu);AT=c(u$,"Base class for masked language models outputs."),u$.forEach(t),CT=i(fr),nn=s(fr,"DIV",{class:!0});var Vf=a(nn);f(Ma.$$.fragment,Vf),ET=i(Vf),Qu=s(Vf,"P",{});var l$=a(Qu);NT=c(l$,"\u201CReturns a new object replacing the specified fields with new values."),l$.forEach(t),Vf.forEach(t),fr.forEach(t),Sc=i(e),Dt=s(e,"H2",{class:!0});var Qf=a(Dt);sn=s(Qf,"A",{id:!0,class:!0,href:!0});var p$=a(sn);Ru=s(p$,"SPAN",{});var c$=a(Ru);f(ka.$$.fragment,c$),c$.forEach(t),p$.forEach(t),zT=i(Qf),Xu=s(Qf,"SPAN",{});var h$=a(Xu);PT=c(h$,"FlaxSeq2SeqLMOutput"),h$.forEach(t),Qf.forEach(t),Mc=i(e),j=s(e,"DIV",{class:!0});var mr=a(j);f(Aa.$$.fragment,mr),BT=i(mr),Uu=s(mr,"P",{});var f$=a(Uu);LT=c(f$,"Base class for sequence-to-sequence language models outputs."),f$.forEach(t),WT=i(mr),an=s(mr,"DIV",{class:!0});var Rf=a(an);f(Ca.$$.fragment,Rf),jT=i(Rf),Yu=s(Rf,"P",{});var m$=a(Yu);DT=c(m$,"\u201CReturns a new object replacing the specified fields with new values."),m$.forEach(t),Rf.forEach(t),mr.forEach(t),kc=i(e),Ht=s(e,"H2",{class:!0});var Xf=a(Ht);rn=s(Xf,"A",{id:!0,class:!0,href:!0});var _$=a(rn);Ju=s(_$,"SPAN",{});var g$=a(Ju);f(Ea.$$.fragment,g$),g$.forEach(t),_$.forEach(t),HT=i(Xf),Gu=s(Xf,"SPAN",{});var v$=a(Gu);IT=c(v$,"FlaxNextSentencePredictorOutput"),v$.forEach(t),Xf.forEach(t),Ac=i(e),D=s(e,"DIV",{class:!0});var _r=a(D);f(Na.$$.fragment,_r),VT=i(_r),Ku=s(_r,"P",{});var y$=a(Ku);QT=c(y$,"Base class for outputs of models predicting if two sentences are consecutive or not."),y$.forEach(t),RT=i(_r),dn=s(_r,"DIV",{class:!0});var Uf=a(dn);f(za.$$.fragment,Uf),XT=i(Uf),Zu=s(Uf,"P",{});var T$=a(Zu);UT=c(T$,"\u201CReturns a new object replacing the specified fields with new values."),T$.forEach(t),Uf.forEach(t),_r.forEach(t),Cc=i(e),It=s(e,"H2",{class:!0});var Yf=a(It);un=s(Yf,"A",{id:!0,class:!0,href:!0});var b$=a(un);el=s(b$,"SPAN",{});var w$=a(el);f(Pa.$$.fragment,w$),w$.forEach(t),b$.forEach(t),YT=i(Yf),tl=s(Yf,"SPAN",{});var x$=a(tl);JT=c(x$,"FlaxSequenceClassifierOutput"),x$.forEach(t),Yf.forEach(t),Ec=i(e),H=s(e,"DIV",{class:!0});var gr=a(H);f(Ba.$$.fragment,gr),GT=i(gr),ol=s(gr,"P",{});var $$=a(ol);KT=c($$,"Base class for outputs of sentence classification models."),$$.forEach(t),ZT=i(gr),ln=s(gr,"DIV",{class:!0});var Jf=a(ln);f(La.$$.fragment,Jf),e2=i(Jf),nl=s(Jf,"P",{});var O$=a(nl);t2=c(O$,"\u201CReturns a new object replacing the specified fields with new values."),O$.forEach(t),Jf.forEach(t),gr.forEach(t),Nc=i(e),Vt=s(e,"H2",{class:!0});var Gf=a(Vt);pn=s(Gf,"A",{id:!0,class:!0,href:!0});var q$=a(pn);sl=s(q$,"SPAN",{});var F$=a(sl);f(Wa.$$.fragment,F$),F$.forEach(t),q$.forEach(t),o2=i(Gf),al=s(Gf,"SPAN",{});var S$=a(al);n2=c(S$,"FlaxSeq2SeqSequenceClassifierOutput"),S$.forEach(t),Gf.forEach(t),zc=i(e),I=s(e,"DIV",{class:!0});var vr=a(I);f(ja.$$.fragment,vr),s2=i(vr),rl=s(vr,"P",{});var M$=a(rl);a2=c(M$,"Base class for outputs of sequence-to-sequence sentence classification models."),M$.forEach(t),r2=i(vr),cn=s(vr,"DIV",{class:!0});var Kf=a(cn);f(Da.$$.fragment,Kf),d2=i(Kf),dl=s(Kf,"P",{});var k$=a(dl);i2=c(k$,"\u201CReturns a new object replacing the specified fields with new values."),k$.forEach(t),Kf.forEach(t),vr.forEach(t),Pc=i(e),Qt=s(e,"H2",{class:!0});var Zf=a(Qt);hn=s(Zf,"A",{id:!0,class:!0,href:!0});var A$=a(hn);il=s(A$,"SPAN",{});var C$=a(il);f(Ha.$$.fragment,C$),C$.forEach(t),A$.forEach(t),u2=i(Zf),ul=s(Zf,"SPAN",{});var E$=a(ul);l2=c(E$,"FlaxMultipleChoiceModelOutput"),E$.forEach(t),Zf.forEach(t),Bc=i(e),V=s(e,"DIV",{class:!0});var yr=a(V);f(Ia.$$.fragment,yr),p2=i(yr),ll=s(yr,"P",{});var N$=a(ll);c2=c(N$,"Base class for outputs of multiple choice models."),N$.forEach(t),h2=i(yr),fn=s(yr,"DIV",{class:!0});var em=a(fn);f(Va.$$.fragment,em),f2=i(em),pl=s(em,"P",{});var z$=a(pl);m2=c(z$,"\u201CReturns a new object replacing the specified fields with new values."),z$.forEach(t),em.forEach(t),yr.forEach(t),Lc=i(e),Rt=s(e,"H2",{class:!0});var tm=a(Rt);mn=s(tm,"A",{id:!0,class:!0,href:!0});var P$=a(mn);cl=s(P$,"SPAN",{});var B$=a(cl);f(Qa.$$.fragment,B$),B$.forEach(t),P$.forEach(t),_2=i(tm),hl=s(tm,"SPAN",{});var L$=a(hl);g2=c(L$,"FlaxTokenClassifierOutput"),L$.forEach(t),tm.forEach(t),Wc=i(e),Q=s(e,"DIV",{class:!0});var Tr=a(Q);f(Ra.$$.fragment,Tr),v2=i(Tr),fl=s(Tr,"P",{});var W$=a(fl);y2=c(W$,"Base class for outputs of token classification models."),W$.forEach(t),T2=i(Tr),_n=s(Tr,"DIV",{class:!0});var om=a(_n);f(Xa.$$.fragment,om),b2=i(om),ml=s(om,"P",{});var j$=a(ml);w2=c(j$,"\u201CReturns a new object replacing the specified fields with new values."),j$.forEach(t),om.forEach(t),Tr.forEach(t),jc=i(e),Xt=s(e,"H2",{class:!0});var nm=a(Xt);gn=s(nm,"A",{id:!0,class:!0,href:!0});var D$=a(gn);_l=s(D$,"SPAN",{});var H$=a(_l);f(Ua.$$.fragment,H$),H$.forEach(t),D$.forEach(t),x2=i(nm),gl=s(nm,"SPAN",{});var I$=a(gl);$2=c(I$,"FlaxQuestionAnsweringModelOutput"),I$.forEach(t),nm.forEach(t),Dc=i(e),R=s(e,"DIV",{class:!0});var br=a(R);f(Ya.$$.fragment,br),O2=i(br),vl=s(br,"P",{});var V$=a(vl);q2=c(V$,"Base class for outputs of question answering models."),V$.forEach(t),F2=i(br),vn=s(br,"DIV",{class:!0});var sm=a(vn);f(Ja.$$.fragment,sm),S2=i(sm),yl=s(sm,"P",{});var Q$=a(yl);M2=c(Q$,"\u201CReturns a new object replacing the specified fields with new values."),Q$.forEach(t),sm.forEach(t),br.forEach(t),Hc=i(e),Ut=s(e,"H2",{class:!0});var am=a(Ut);yn=s(am,"A",{id:!0,class:!0,href:!0});var R$=a(yn);Tl=s(R$,"SPAN",{});var X$=a(Tl);f(Ga.$$.fragment,X$),X$.forEach(t),R$.forEach(t),k2=i(am),bl=s(am,"SPAN",{});var U$=a(bl);A2=c(U$,"FlaxSeq2SeqQuestionAnsweringModelOutput"),U$.forEach(t),am.forEach(t),Ic=i(e),X=s(e,"DIV",{class:!0});var wr=a(X);f(Ka.$$.fragment,wr),C2=i(wr),wl=s(wr,"P",{});var Y$=a(wl);E2=c(Y$,"Base class for outputs of sequence-to-sequence question answering models."),Y$.forEach(t),N2=i(wr),Tn=s(wr,"DIV",{class:!0});var rm=a(Tn);f(Za.$$.fragment,rm),z2=i(rm),xl=s(rm,"P",{});var J$=a(xl);P2=c(J$,"\u201CReturns a new object replacing the specified fields with new values."),J$.forEach(t),rm.forEach(t),wr.forEach(t),this.h()},h(){r(x,"name","hf:doc:metadata"),r(x,"content",JSON.stringify(rO)),r(A,"id","model-outputs"),r(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(A,"href","#model-outputs"),r($,"class","relative group"),r(or,"href","/docs/transformers/pr_16812/en/main_classes/output#transformers.utils.ModelOutput"),r(sr,"href","/docs/transformers/pr_16812/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"),r(Gt,"id","transformers.utils.ModelOutput"),r(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Gt,"href","#transformers.utils.ModelOutput"),r(ee,"class","relative group"),r(Zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(eo,"id","transformers.modeling_outputs.BaseModelOutput"),r(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(eo,"href","#transformers.modeling_outputs.BaseModelOutput"),r(oe,"class","relative group"),r(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(to,"id","transformers.modeling_outputs.BaseModelOutputWithPooling"),r(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(to,"href","#transformers.modeling_outputs.BaseModelOutputWithPooling"),r(se,"class","relative group"),r(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(oo,"id","transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(oo,"href","#transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(re,"class","relative group"),r(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(no,"id","transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(no,"href","#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(ie,"class","relative group"),r(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(so,"id","transformers.modeling_outputs.BaseModelOutputWithPast"),r(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(so,"href","#transformers.modeling_outputs.BaseModelOutputWithPast"),r(le,"class","relative group"),r(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ao,"id","transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ao,"href","#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(ce,"class","relative group"),r(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ro,"id","transformers.modeling_outputs.Seq2SeqModelOutput"),r(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ro,"href","#transformers.modeling_outputs.Seq2SeqModelOutput"),r(fe,"class","relative group"),r(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(io,"id","transformers.modeling_outputs.CausalLMOutput"),r(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(io,"href","#transformers.modeling_outputs.CausalLMOutput"),r(_e,"class","relative group"),r(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(uo,"id","transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(uo,"href","#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(ve,"class","relative group"),r(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(lo,"id","transformers.modeling_outputs.CausalLMOutputWithPast"),r(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(lo,"href","#transformers.modeling_outputs.CausalLMOutputWithPast"),r(Te,"class","relative group"),r(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(po,"id","transformers.modeling_outputs.MaskedLMOutput"),r(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(po,"href","#transformers.modeling_outputs.MaskedLMOutput"),r(we,"class","relative group"),r(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(co,"id","transformers.modeling_outputs.Seq2SeqLMOutput"),r(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(co,"href","#transformers.modeling_outputs.Seq2SeqLMOutput"),r($e,"class","relative group"),r(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ho,"id","transformers.modeling_outputs.NextSentencePredictorOutput"),r(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ho,"href","#transformers.modeling_outputs.NextSentencePredictorOutput"),r(qe,"class","relative group"),r(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(fo,"id","transformers.modeling_outputs.SequenceClassifierOutput"),r(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(fo,"href","#transformers.modeling_outputs.SequenceClassifierOutput"),r(Se,"class","relative group"),r(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(mo,"id","transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(mo,"href","#transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(ke,"class","relative group"),r(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(_o,"id","transformers.modeling_outputs.MultipleChoiceModelOutput"),r(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(_o,"href","#transformers.modeling_outputs.MultipleChoiceModelOutput"),r(Ce,"class","relative group"),r(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(go,"id","transformers.modeling_outputs.TokenClassifierOutput"),r(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(go,"href","#transformers.modeling_outputs.TokenClassifierOutput"),r(Ne,"class","relative group"),r(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(vo,"id","transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(vo,"href","#transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(Pe,"class","relative group"),r(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(yo,"id","transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(yo,"href","#transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(Le,"class","relative group"),r(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(To,"id","transformers.modeling_outputs.SemanticSegmenterOutput"),r(To,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(To,"href","#transformers.modeling_outputs.SemanticSegmenterOutput"),r(je,"class","relative group"),r(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(bo,"id","transformers.modeling_outputs.ImageClassifierOutput"),r(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(bo,"href","#transformers.modeling_outputs.ImageClassifierOutput"),r(He,"class","relative group"),r(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(wo,"id","transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"),r(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(wo,"href","#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"),r(Ve,"class","relative group"),r(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(xo,"id","transformers.modeling_outputs.DepthEstimatorOutput"),r(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(xo,"href","#transformers.modeling_outputs.DepthEstimatorOutput"),r(Re,"class","relative group"),r(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r($o,"id","transformers.modeling_outputs.Wav2Vec2BaseModelOutput"),r($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r($o,"href","#transformers.modeling_outputs.Wav2Vec2BaseModelOutput"),r(Ue,"class","relative group"),r(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Oo,"id","transformers.modeling_outputs.XVectorOutput"),r(Oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Oo,"href","#transformers.modeling_outputs.XVectorOutput"),r(Je,"class","relative group"),r(rr,"href","/docs/transformers/pr_16812/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),r(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(qo,"id","transformers.modeling_tf_outputs.TFBaseModelOutput"),r(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(qo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutput"),r(Ke,"class","relative group"),r(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Fo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Fo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(et,"class","relative group"),r(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(So,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(So,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(So,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(ot,"class","relative group"),r(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Mo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(Mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Mo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(st,"class","relative group"),r(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ko,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ko,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(rt,"class","relative group"),r(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ao,"id","transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(Ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ao,"href","#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(it,"class","relative group"),r(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Co,"id","transformers.modeling_tf_outputs.TFCausalLMOutput"),r(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Co,"href","#transformers.modeling_tf_outputs.TFCausalLMOutput"),r(lt,"class","relative group"),r(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Eo,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Eo,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(ct,"class","relative group"),r(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(No,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(No,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(No,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(ft,"class","relative group"),r(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(zo,"id","transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(zo,"href","#transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(_t,"class","relative group"),r(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Po,"id","transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(Po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Po,"href","#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(vt,"class","relative group"),r(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Bo,"id","transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(Bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Bo,"href","#transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(Tt,"class","relative group"),r(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Lo,"id","transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(Lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Lo,"href","#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(wt,"class","relative group"),r(xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Wo,"id","transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r(Wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Wo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r($t,"class","relative group"),r(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(jo,"id","transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(jo,"href","#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(qt,"class","relative group"),r(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Do,"id","transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r(Do,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Do,"href","#transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r(St,"class","relative group"),r(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ho,"id","transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r(Ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ho,"href","#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r(kt,"class","relative group"),r(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Io,"id","transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Io,"href","#transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(Ct,"class","relative group"),r(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Vo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(Vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Vo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(Nt,"class","relative group"),r(Qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ro,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r(Ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ro,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r(zt,"class","relative group"),r(Xo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Uo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r(Uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Uo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r(Pt,"class","relative group"),r(Yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Jo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(Jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Jo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(Bt,"class","relative group"),r(Go,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ko,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(Ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ko,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(Lt,"class","relative group"),r(Zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(en,"id","transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(en,"href","#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(Wt,"class","relative group"),r(tn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(on,"id","transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(on,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(on,"href","#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(jt,"class","relative group"),r(nn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(sn,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(sn,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(Dt,"class","relative group"),r(an,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(rn,"id","transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(rn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(rn,"href","#transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(Ht,"class","relative group"),r(dn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(un,"id","transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(un,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(un,"href","#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(It,"class","relative group"),r(ln,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(pn,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(pn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(pn,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(Vt,"class","relative group"),r(cn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(hn,"id","transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(hn,"href","#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(Qt,"class","relative group"),r(fn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(mn,"id","transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(mn,"href","#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(Rt,"class","relative group"),r(_n,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(gn,"id","transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(gn,"href","#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(Xt,"class","relative group"),r(vn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(yn,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(yn,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(Ut,"class","relative group"),r(Tn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,u){o(document.head,x),l(e,Yt,u),l(e,$,u),o($,A),o(A,G),m(O,G,null),o($,wn),o($,K),o(K,Z),l(e,M,u),l(e,C,u),o(C,tr),o(C,or),o(or,dm),o(C,im),l(e,Ol,u),l(e,nr,u),o(nr,um),l(e,ql,u),m(xn,e,u),l(e,Fl,u),l(e,b,u),o(b,lm),o(b,xr),o(xr,pm),o(b,cm),o(b,sr),o(sr,hm),o(b,fm),o(b,$r),o($r,mm),o(b,_m),o(b,Or),o(Or,gm),o(b,vm),o(b,qr),o(qr,ym),o(b,Tm),o(b,Fr),o(Fr,bm),o(b,wm),o(b,Sr),o(Sr,xm),o(b,$m),o(b,Mr),o(Mr,Om),o(b,qm),o(b,kr),o(kr,Fm),o(b,Sm),o(b,Ar),o(Ar,Mm),o(b,km),o(b,Cr),o(Cr,Am),o(b,Cm),o(b,Er),o(Er,Em),o(b,Nm),l(e,Sl,u),l(e,q,u),o(q,zm),o(q,Nr),o(Nr,Pm),o(q,Bm),o(q,zr),o(zr,Lm),o(q,Wm),o(q,Pr),o(Pr,jm),o(q,Dm),o(q,Br),o(Br,Hm),o(q,Im),l(e,Ml,u),l(e,F,u),o(F,Vm),o(F,Lr),o(Lr,Qm),o(F,Rm),o(F,Wr),o(Wr,Xm),o(F,Um),o(F,jr),o(jr,Ym),o(F,Jm),o(F,Dr),o(Dr,Gm),o(F,Km),l(e,kl,u),m($n,e,u),l(e,Al,u),l(e,Jt,u),o(Jt,Zm),o(Jt,Hr),o(Hr,e_),o(Jt,t_),l(e,Cl,u),l(e,S,u),o(S,o_),o(S,Ir),o(Ir,n_),o(S,s_),o(S,Vr),o(Vr,a_),o(S,r_),o(S,Qr),o(Qr,d_),o(S,i_),o(S,Rr),o(Rr,u_),o(S,l_),l(e,El,u),l(e,ar,u),o(ar,p_),l(e,Nl,u),l(e,ee,u),o(ee,Gt),o(Gt,Xr),m(On,Xr,null),o(ee,c_),o(ee,Ur),o(Ur,h_),l(e,zl,u),l(e,k,u),m(qn,k,null),o(k,f_),o(k,te),o(te,m_),o(te,Yr),o(Yr,__),o(te,g_),o(te,Jr),o(Jr,v_),o(te,y_),o(k,T_),m(Kt,k,null),o(k,b_),o(k,Zt),m(Fn,Zt,null),o(Zt,w_),o(Zt,Sn),o(Sn,x_),o(Sn,Gr),o(Gr,$_),o(Sn,O_),l(e,Pl,u),l(e,oe,u),o(oe,eo),o(eo,Kr),m(Mn,Kr,null),o(oe,q_),o(oe,Zr),o(Zr,F_),l(e,Bl,u),l(e,ne,u),m(kn,ne,null),o(ne,S_),o(ne,ed),o(ed,M_),l(e,Ll,u),l(e,se,u),o(se,to),o(to,td),m(An,td,null),o(se,k_),o(se,od),o(od,A_),l(e,Wl,u),l(e,ae,u),m(Cn,ae,null),o(ae,C_),o(ae,nd),o(nd,E_),l(e,jl,u),l(e,re,u),o(re,oo),o(oo,sd),m(En,sd,null),o(re,N_),o(re,ad),o(ad,z_),l(e,Dl,u),l(e,de,u),m(Nn,de,null),o(de,P_),o(de,rd),o(rd,B_),l(e,Hl,u),l(e,ie,u),o(ie,no),o(no,dd),m(zn,dd,null),o(ie,L_),o(ie,id),o(id,W_),l(e,Il,u),l(e,ue,u),m(Pn,ue,null),o(ue,j_),o(ue,ud),o(ud,D_),l(e,Vl,u),l(e,le,u),o(le,so),o(so,ld),m(Bn,ld,null),o(le,H_),o(le,pd),o(pd,I_),l(e,Ql,u),l(e,pe,u),m(Ln,pe,null),o(pe,V_),o(pe,cd),o(cd,Q_),l(e,Rl,u),l(e,ce,u),o(ce,ao),o(ao,hd),m(Wn,hd,null),o(ce,R_),o(ce,fd),o(fd,X_),l(e,Xl,u),l(e,he,u),m(jn,he,null),o(he,U_),o(he,md),o(md,Y_),l(e,Ul,u),l(e,fe,u),o(fe,ro),o(ro,_d),m(Dn,_d,null),o(fe,J_),o(fe,gd),o(gd,G_),l(e,Yl,u),l(e,me,u),m(Hn,me,null),o(me,K_),o(me,vd),o(vd,Z_),l(e,Jl,u),l(e,_e,u),o(_e,io),o(io,yd),m(In,yd,null),o(_e,eg),o(_e,Td),o(Td,tg),l(e,Gl,u),l(e,ge,u),m(Vn,ge,null),o(ge,og),o(ge,bd),o(bd,ng),l(e,Kl,u),l(e,ve,u),o(ve,uo),o(uo,wd),m(Qn,wd,null),o(ve,sg),o(ve,xd),o(xd,ag),l(e,Zl,u),l(e,ye,u),m(Rn,ye,null),o(ye,rg),o(ye,$d),o($d,dg),l(e,ep,u),l(e,Te,u),o(Te,lo),o(lo,Od),m(Xn,Od,null),o(Te,ig),o(Te,qd),o(qd,ug),l(e,tp,u),l(e,be,u),m(Un,be,null),o(be,lg),o(be,Fd),o(Fd,pg),l(e,op,u),l(e,we,u),o(we,po),o(po,Sd),m(Yn,Sd,null),o(we,cg),o(we,Md),o(Md,hg),l(e,np,u),l(e,xe,u),m(Jn,xe,null),o(xe,fg),o(xe,kd),o(kd,mg),l(e,sp,u),l(e,$e,u),o($e,co),o(co,Ad),m(Gn,Ad,null),o($e,_g),o($e,Cd),o(Cd,gg),l(e,ap,u),l(e,Oe,u),m(Kn,Oe,null),o(Oe,vg),o(Oe,Ed),o(Ed,yg),l(e,rp,u),l(e,qe,u),o(qe,ho),o(ho,Nd),m(Zn,Nd,null),o(qe,Tg),o(qe,zd),o(zd,bg),l(e,dp,u),l(e,Fe,u),m(es,Fe,null),o(Fe,wg),o(Fe,Pd),o(Pd,xg),l(e,ip,u),l(e,Se,u),o(Se,fo),o(fo,Bd),m(ts,Bd,null),o(Se,$g),o(Se,Ld),o(Ld,Og),l(e,up,u),l(e,Me,u),m(os,Me,null),o(Me,qg),o(Me,Wd),o(Wd,Fg),l(e,lp,u),l(e,ke,u),o(ke,mo),o(mo,jd),m(ns,jd,null),o(ke,Sg),o(ke,Dd),o(Dd,Mg),l(e,pp,u),l(e,Ae,u),m(ss,Ae,null),o(Ae,kg),o(Ae,Hd),o(Hd,Ag),l(e,cp,u),l(e,Ce,u),o(Ce,_o),o(_o,Id),m(as,Id,null),o(Ce,Cg),o(Ce,Vd),o(Vd,Eg),l(e,hp,u),l(e,Ee,u),m(rs,Ee,null),o(Ee,Ng),o(Ee,Qd),o(Qd,zg),l(e,fp,u),l(e,Ne,u),o(Ne,go),o(go,Rd),m(ds,Rd,null),o(Ne,Pg),o(Ne,Xd),o(Xd,Bg),l(e,mp,u),l(e,ze,u),m(is,ze,null),o(ze,Lg),o(ze,Ud),o(Ud,Wg),l(e,_p,u),l(e,Pe,u),o(Pe,vo),o(vo,Yd),m(us,Yd,null),o(Pe,jg),o(Pe,Jd),o(Jd,Dg),l(e,gp,u),l(e,Be,u),m(ls,Be,null),o(Be,Hg),o(Be,Gd),o(Gd,Ig),l(e,vp,u),l(e,Le,u),o(Le,yo),o(yo,Kd),m(ps,Kd,null),o(Le,Vg),o(Le,Zd),o(Zd,Qg),l(e,yp,u),l(e,We,u),m(cs,We,null),o(We,Rg),o(We,ei),o(ei,Xg),l(e,Tp,u),l(e,je,u),o(je,To),o(To,ti),m(hs,ti,null),o(je,Ug),o(je,oi),o(oi,Yg),l(e,bp,u),l(e,De,u),m(fs,De,null),o(De,Jg),o(De,ni),o(ni,Gg),l(e,wp,u),l(e,He,u),o(He,bo),o(bo,si),m(ms,si,null),o(He,Kg),o(He,ai),o(ai,Zg),l(e,xp,u),l(e,Ie,u),m(_s,Ie,null),o(Ie,ev),o(Ie,ri),o(ri,tv),l(e,$p,u),l(e,Ve,u),o(Ve,wo),o(wo,di),m(gs,di,null),o(Ve,ov),o(Ve,ii),o(ii,nv),l(e,Op,u),l(e,Qe,u),m(vs,Qe,null),o(Qe,sv),o(Qe,ui),o(ui,av),l(e,qp,u),l(e,Re,u),o(Re,xo),o(xo,li),m(ys,li,null),o(Re,rv),o(Re,pi),o(pi,dv),l(e,Fp,u),l(e,Xe,u),m(Ts,Xe,null),o(Xe,iv),o(Xe,ci),o(ci,uv),l(e,Sp,u),l(e,Ue,u),o(Ue,$o),o($o,hi),m(bs,hi,null),o(Ue,lv),o(Ue,fi),o(fi,pv),l(e,Mp,u),l(e,Ye,u),m(ws,Ye,null),o(Ye,cv),o(Ye,xs),o(xs,hv),o(xs,mi),o(mi,fv),o(xs,mv),l(e,kp,u),l(e,Je,u),o(Je,Oo),o(Oo,_i),m($s,_i,null),o(Je,_v),o(Je,gi),o(gi,gv),l(e,Ap,u),l(e,Ge,u),m(Os,Ge,null),o(Ge,vv),o(Ge,qs),o(qs,yv),o(qs,rr),o(rr,Tv),o(qs,bv),l(e,Cp,u),l(e,Ke,u),o(Ke,qo),o(qo,vi),m(Fs,vi,null),o(Ke,wv),o(Ke,yi),o(yi,xv),l(e,Ep,u),l(e,Ze,u),m(Ss,Ze,null),o(Ze,$v),o(Ze,Ti),o(Ti,Ov),l(e,Np,u),l(e,et,u),o(et,Fo),o(Fo,bi),m(Ms,bi,null),o(et,qv),o(et,wi),o(wi,Fv),l(e,zp,u),l(e,tt,u),m(ks,tt,null),o(tt,Sv),o(tt,xi),o(xi,Mv),l(e,Pp,u),l(e,ot,u),o(ot,So),o(So,$i),m(As,$i,null),o(ot,kv),o(ot,Oi),o(Oi,Av),l(e,Bp,u),l(e,nt,u),m(Cs,nt,null),o(nt,Cv),o(nt,qi),o(qi,Ev),l(e,Lp,u),l(e,st,u),o(st,Mo),o(Mo,Fi),m(Es,Fi,null),o(st,Nv),o(st,Si),o(Si,zv),l(e,Wp,u),l(e,at,u),m(Ns,at,null),o(at,Pv),o(at,Mi),o(Mi,Bv),l(e,jp,u),l(e,rt,u),o(rt,ko),o(ko,ki),m(zs,ki,null),o(rt,Lv),o(rt,Ai),o(Ai,Wv),l(e,Dp,u),l(e,dt,u),m(Ps,dt,null),o(dt,jv),o(dt,Ci),o(Ci,Dv),l(e,Hp,u),l(e,it,u),o(it,Ao),o(Ao,Ei),m(Bs,Ei,null),o(it,Hv),o(it,Ni),o(Ni,Iv),l(e,Ip,u),l(e,ut,u),m(Ls,ut,null),o(ut,Vv),o(ut,zi),o(zi,Qv),l(e,Vp,u),l(e,lt,u),o(lt,Co),o(Co,Pi),m(Ws,Pi,null),o(lt,Rv),o(lt,Bi),o(Bi,Xv),l(e,Qp,u),l(e,pt,u),m(js,pt,null),o(pt,Uv),o(pt,Li),o(Li,Yv),l(e,Rp,u),l(e,ct,u),o(ct,Eo),o(Eo,Wi),m(Ds,Wi,null),o(ct,Jv),o(ct,ji),o(ji,Gv),l(e,Xp,u),l(e,ht,u),m(Hs,ht,null),o(ht,Kv),o(ht,Di),o(Di,Zv),l(e,Up,u),l(e,ft,u),o(ft,No),o(No,Hi),m(Is,Hi,null),o(ft,ey),o(ft,Ii),o(Ii,ty),l(e,Yp,u),l(e,mt,u),m(Vs,mt,null),o(mt,oy),o(mt,Vi),o(Vi,ny),l(e,Jp,u),l(e,_t,u),o(_t,zo),o(zo,Qi),m(Qs,Qi,null),o(_t,sy),o(_t,Ri),o(Ri,ay),l(e,Gp,u),l(e,gt,u),m(Rs,gt,null),o(gt,ry),o(gt,Xi),o(Xi,dy),l(e,Kp,u),l(e,vt,u),o(vt,Po),o(Po,Ui),m(Xs,Ui,null),o(vt,iy),o(vt,Yi),o(Yi,uy),l(e,Zp,u),l(e,yt,u),m(Us,yt,null),o(yt,ly),o(yt,Ji),o(Ji,py),l(e,ec,u),l(e,Tt,u),o(Tt,Bo),o(Bo,Gi),m(Ys,Gi,null),o(Tt,cy),o(Tt,Ki),o(Ki,hy),l(e,tc,u),l(e,bt,u),m(Js,bt,null),o(bt,fy),o(bt,Zi),o(Zi,my),l(e,oc,u),l(e,wt,u),o(wt,Lo),o(Lo,eu),m(Gs,eu,null),o(wt,_y),o(wt,tu),o(tu,gy),l(e,nc,u),l(e,xt,u),m(Ks,xt,null),o(xt,vy),o(xt,ou),o(ou,yy),l(e,sc,u),l(e,$t,u),o($t,Wo),o(Wo,nu),m(Zs,nu,null),o($t,Ty),o($t,su),o(su,by),l(e,ac,u),l(e,Ot,u),m(ea,Ot,null),o(Ot,wy),o(Ot,au),o(au,xy),l(e,rc,u),l(e,qt,u),o(qt,jo),o(jo,ru),m(ta,ru,null),o(qt,$y),o(qt,du),o(du,Oy),l(e,dc,u),l(e,Ft,u),m(oa,Ft,null),o(Ft,qy),o(Ft,iu),o(iu,Fy),l(e,ic,u),l(e,St,u),o(St,Do),o(Do,uu),m(na,uu,null),o(St,Sy),o(St,lu),o(lu,My),l(e,uc,u),l(e,Mt,u),m(sa,Mt,null),o(Mt,ky),o(Mt,pu),o(pu,Ay),l(e,lc,u),l(e,kt,u),o(kt,Ho),o(Ho,cu),m(aa,cu,null),o(kt,Cy),o(kt,hu),o(hu,Ey),l(e,pc,u),l(e,At,u),m(ra,At,null),o(At,Ny),o(At,fu),o(fu,zy),l(e,cc,u),l(e,Ct,u),o(Ct,Io),o(Io,mu),m(da,mu,null),o(Ct,Py),o(Ct,_u),o(_u,By),l(e,hc,u),l(e,Et,u),m(ia,Et,null),o(Et,Ly),o(Et,gu),o(gu,Wy),l(e,fc,u),l(e,Nt,u),o(Nt,Vo),o(Vo,vu),m(ua,vu,null),o(Nt,jy),o(Nt,yu),o(yu,Dy),l(e,mc,u),l(e,E,u),m(la,E,null),o(E,Hy),o(E,Tu),o(Tu,Iy),o(E,Vy),o(E,Qo),m(pa,Qo,null),o(Qo,Qy),o(Qo,bu),o(bu,Ry),l(e,_c,u),l(e,zt,u),o(zt,Ro),o(Ro,wu),m(ca,wu,null),o(zt,Xy),o(zt,xu),o(xu,Uy),l(e,gc,u),l(e,N,u),m(ha,N,null),o(N,Yy),o(N,$u),o($u,Jy),o(N,Gy),o(N,Xo),m(fa,Xo,null),o(Xo,Ky),o(Xo,Ou),o(Ou,Zy),l(e,vc,u),l(e,Pt,u),o(Pt,Uo),o(Uo,qu),m(ma,qu,null),o(Pt,eT),o(Pt,Fu),o(Fu,tT),l(e,yc,u),l(e,z,u),m(_a,z,null),o(z,oT),o(z,Su),o(Su,nT),o(z,sT),o(z,Yo),m(ga,Yo,null),o(Yo,aT),o(Yo,Mu),o(Mu,rT),l(e,Tc,u),l(e,Bt,u),o(Bt,Jo),o(Jo,ku),m(va,ku,null),o(Bt,dT),o(Bt,Au),o(Au,iT),l(e,bc,u),l(e,P,u),m(ya,P,null),o(P,uT),o(P,Cu),o(Cu,lT),o(P,pT),o(P,Go),m(Ta,Go,null),o(Go,cT),o(Go,Eu),o(Eu,hT),l(e,wc,u),l(e,Lt,u),o(Lt,Ko),o(Ko,Nu),m(ba,Nu,null),o(Lt,fT),o(Lt,zu),o(zu,mT),l(e,xc,u),l(e,B,u),m(wa,B,null),o(B,_T),o(B,Pu),o(Pu,gT),o(B,vT),o(B,Zo),m(xa,Zo,null),o(Zo,yT),o(Zo,Bu),o(Bu,TT),l(e,$c,u),l(e,Wt,u),o(Wt,en),o(en,Lu),m($a,Lu,null),o(Wt,bT),o(Wt,Wu),o(Wu,wT),l(e,Oc,u),l(e,L,u),m(Oa,L,null),o(L,xT),o(L,ju),o(ju,$T),o(L,OT),o(L,tn),m(qa,tn,null),o(tn,qT),o(tn,Du),o(Du,FT),l(e,qc,u),l(e,jt,u),o(jt,on),o(on,Hu),m(Fa,Hu,null),o(jt,ST),o(jt,Iu),o(Iu,MT),l(e,Fc,u),l(e,W,u),m(Sa,W,null),o(W,kT),o(W,Vu),o(Vu,AT),o(W,CT),o(W,nn),m(Ma,nn,null),o(nn,ET),o(nn,Qu),o(Qu,NT),l(e,Sc,u),l(e,Dt,u),o(Dt,sn),o(sn,Ru),m(ka,Ru,null),o(Dt,zT),o(Dt,Xu),o(Xu,PT),l(e,Mc,u),l(e,j,u),m(Aa,j,null),o(j,BT),o(j,Uu),o(Uu,LT),o(j,WT),o(j,an),m(Ca,an,null),o(an,jT),o(an,Yu),o(Yu,DT),l(e,kc,u),l(e,Ht,u),o(Ht,rn),o(rn,Ju),m(Ea,Ju,null),o(Ht,HT),o(Ht,Gu),o(Gu,IT),l(e,Ac,u),l(e,D,u),m(Na,D,null),o(D,VT),o(D,Ku),o(Ku,QT),o(D,RT),o(D,dn),m(za,dn,null),o(dn,XT),o(dn,Zu),o(Zu,UT),l(e,Cc,u),l(e,It,u),o(It,un),o(un,el),m(Pa,el,null),o(It,YT),o(It,tl),o(tl,JT),l(e,Ec,u),l(e,H,u),m(Ba,H,null),o(H,GT),o(H,ol),o(ol,KT),o(H,ZT),o(H,ln),m(La,ln,null),o(ln,e2),o(ln,nl),o(nl,t2),l(e,Nc,u),l(e,Vt,u),o(Vt,pn),o(pn,sl),m(Wa,sl,null),o(Vt,o2),o(Vt,al),o(al,n2),l(e,zc,u),l(e,I,u),m(ja,I,null),o(I,s2),o(I,rl),o(rl,a2),o(I,r2),o(I,cn),m(Da,cn,null),o(cn,d2),o(cn,dl),o(dl,i2),l(e,Pc,u),l(e,Qt,u),o(Qt,hn),o(hn,il),m(Ha,il,null),o(Qt,u2),o(Qt,ul),o(ul,l2),l(e,Bc,u),l(e,V,u),m(Ia,V,null),o(V,p2),o(V,ll),o(ll,c2),o(V,h2),o(V,fn),m(Va,fn,null),o(fn,f2),o(fn,pl),o(pl,m2),l(e,Lc,u),l(e,Rt,u),o(Rt,mn),o(mn,cl),m(Qa,cl,null),o(Rt,_2),o(Rt,hl),o(hl,g2),l(e,Wc,u),l(e,Q,u),m(Ra,Q,null),o(Q,v2),o(Q,fl),o(fl,y2),o(Q,T2),o(Q,_n),m(Xa,_n,null),o(_n,b2),o(_n,ml),o(ml,w2),l(e,jc,u),l(e,Xt,u),o(Xt,gn),o(gn,_l),m(Ua,_l,null),o(Xt,x2),o(Xt,gl),o(gl,$2),l(e,Dc,u),l(e,R,u),m(Ya,R,null),o(R,O2),o(R,vl),o(vl,q2),o(R,F2),o(R,vn),m(Ja,vn,null),o(vn,S2),o(vn,yl),o(yl,M2),l(e,Hc,u),l(e,Ut,u),o(Ut,yn),o(yn,Tl),m(Ga,Tl,null),o(Ut,k2),o(Ut,bl),o(bl,A2),l(e,Ic,u),l(e,X,u),m(Ka,X,null),o(X,C2),o(X,wl),o(wl,E2),o(X,N2),o(X,Tn),m(Za,Tn,null),o(Tn,z2),o(Tn,xl),o(xl,P2),Vc=!0},p(e,[u]){const er={};u&2&&(er.$$scope={dirty:u,ctx:e}),Kt.$set(er)},i(e){Vc||(_(O.$$.fragment,e),_(xn.$$.fragment,e),_($n.$$.fragment,e),_(On.$$.fragment,e),_(qn.$$.fragment,e),_(Kt.$$.fragment,e),_(Fn.$$.fragment,e),_(Mn.$$.fragment,e),_(kn.$$.fragment,e),_(An.$$.fragment,e),_(Cn.$$.fragment,e),_(En.$$.fragment,e),_(Nn.$$.fragment,e),_(zn.$$.fragment,e),_(Pn.$$.fragment,e),_(Bn.$$.fragment,e),_(Ln.$$.fragment,e),_(Wn.$$.fragment,e),_(jn.$$.fragment,e),_(Dn.$$.fragment,e),_(Hn.$$.fragment,e),_(In.$$.fragment,e),_(Vn.$$.fragment,e),_(Qn.$$.fragment,e),_(Rn.$$.fragment,e),_(Xn.$$.fragment,e),_(Un.$$.fragment,e),_(Yn.$$.fragment,e),_(Jn.$$.fragment,e),_(Gn.$$.fragment,e),_(Kn.$$.fragment,e),_(Zn.$$.fragment,e),_(es.$$.fragment,e),_(ts.$$.fragment,e),_(os.$$.fragment,e),_(ns.$$.fragment,e),_(ss.$$.fragment,e),_(as.$$.fragment,e),_(rs.$$.fragment,e),_(ds.$$.fragment,e),_(is.$$.fragment,e),_(us.$$.fragment,e),_(ls.$$.fragment,e),_(ps.$$.fragment,e),_(cs.$$.fragment,e),_(hs.$$.fragment,e),_(fs.$$.fragment,e),_(ms.$$.fragment,e),_(_s.$$.fragment,e),_(gs.$$.fragment,e),_(vs.$$.fragment,e),_(ys.$$.fragment,e),_(Ts.$$.fragment,e),_(bs.$$.fragment,e),_(ws.$$.fragment,e),_($s.$$.fragment,e),_(Os.$$.fragment,e),_(Fs.$$.fragment,e),_(Ss.$$.fragment,e),_(Ms.$$.fragment,e),_(ks.$$.fragment,e),_(As.$$.fragment,e),_(Cs.$$.fragment,e),_(Es.$$.fragment,e),_(Ns.$$.fragment,e),_(zs.$$.fragment,e),_(Ps.$$.fragment,e),_(Bs.$$.fragment,e),_(Ls.$$.fragment,e),_(Ws.$$.fragment,e),_(js.$$.fragment,e),_(Ds.$$.fragment,e),_(Hs.$$.fragment,e),_(Is.$$.fragment,e),_(Vs.$$.fragment,e),_(Qs.$$.fragment,e),_(Rs.$$.fragment,e),_(Xs.$$.fragment,e),_(Us.$$.fragment,e),_(Ys.$$.fragment,e),_(Js.$$.fragment,e),_(Gs.$$.fragment,e),_(Ks.$$.fragment,e),_(Zs.$$.fragment,e),_(ea.$$.fragment,e),_(ta.$$.fragment,e),_(oa.$$.fragment,e),_(na.$$.fragment,e),_(sa.$$.fragment,e),_(aa.$$.fragment,e),_(ra.$$.fragment,e),_(da.$$.fragment,e),_(ia.$$.fragment,e),_(ua.$$.fragment,e),_(la.$$.fragment,e),_(pa.$$.fragment,e),_(ca.$$.fragment,e),_(ha.$$.fragment,e),_(fa.$$.fragment,e),_(ma.$$.fragment,e),_(_a.$$.fragment,e),_(ga.$$.fragment,e),_(va.$$.fragment,e),_(ya.$$.fragment,e),_(Ta.$$.fragment,e),_(ba.$$.fragment,e),_(wa.$$.fragment,e),_(xa.$$.fragment,e),_($a.$$.fragment,e),_(Oa.$$.fragment,e),_(qa.$$.fragment,e),_(Fa.$$.fragment,e),_(Sa.$$.fragment,e),_(Ma.$$.fragment,e),_(ka.$$.fragment,e),_(Aa.$$.fragment,e),_(Ca.$$.fragment,e),_(Ea.$$.fragment,e),_(Na.$$.fragment,e),_(za.$$.fragment,e),_(Pa.$$.fragment,e),_(Ba.$$.fragment,e),_(La.$$.fragment,e),_(Wa.$$.fragment,e),_(ja.$$.fragment,e),_(Da.$$.fragment,e),_(Ha.$$.fragment,e),_(Ia.$$.fragment,e),_(Va.$$.fragment,e),_(Qa.$$.fragment,e),_(Ra.$$.fragment,e),_(Xa.$$.fragment,e),_(Ua.$$.fragment,e),_(Ya.$$.fragment,e),_(Ja.$$.fragment,e),_(Ga.$$.fragment,e),_(Ka.$$.fragment,e),_(Za.$$.fragment,e),Vc=!0)},o(e){g(O.$$.fragment,e),g(xn.$$.fragment,e),g($n.$$.fragment,e),g(On.$$.fragment,e),g(qn.$$.fragment,e),g(Kt.$$.fragment,e),g(Fn.$$.fragment,e),g(Mn.$$.fragment,e),g(kn.$$.fragment,e),g(An.$$.fragment,e),g(Cn.$$.fragment,e),g(En.$$.fragment,e),g(Nn.$$.fragment,e),g(zn.$$.fragment,e),g(Pn.$$.fragment,e),g(Bn.$$.fragment,e),g(Ln.$$.fragment,e),g(Wn.$$.fragment,e),g(jn.$$.fragment,e),g(Dn.$$.fragment,e),g(Hn.$$.fragment,e),g(In.$$.fragment,e),g(Vn.$$.fragment,e),g(Qn.$$.fragment,e),g(Rn.$$.fragment,e),g(Xn.$$.fragment,e),g(Un.$$.fragment,e),g(Yn.$$.fragment,e),g(Jn.$$.fragment,e),g(Gn.$$.fragment,e),g(Kn.$$.fragment,e),g(Zn.$$.fragment,e),g(es.$$.fragment,e),g(ts.$$.fragment,e),g(os.$$.fragment,e),g(ns.$$.fragment,e),g(ss.$$.fragment,e),g(as.$$.fragment,e),g(rs.$$.fragment,e),g(ds.$$.fragment,e),g(is.$$.fragment,e),g(us.$$.fragment,e),g(ls.$$.fragment,e),g(ps.$$.fragment,e),g(cs.$$.fragment,e),g(hs.$$.fragment,e),g(fs.$$.fragment,e),g(ms.$$.fragment,e),g(_s.$$.fragment,e),g(gs.$$.fragment,e),g(vs.$$.fragment,e),g(ys.$$.fragment,e),g(Ts.$$.fragment,e),g(bs.$$.fragment,e),g(ws.$$.fragment,e),g($s.$$.fragment,e),g(Os.$$.fragment,e),g(Fs.$$.fragment,e),g(Ss.$$.fragment,e),g(Ms.$$.fragment,e),g(ks.$$.fragment,e),g(As.$$.fragment,e),g(Cs.$$.fragment,e),g(Es.$$.fragment,e),g(Ns.$$.fragment,e),g(zs.$$.fragment,e),g(Ps.$$.fragment,e),g(Bs.$$.fragment,e),g(Ls.$$.fragment,e),g(Ws.$$.fragment,e),g(js.$$.fragment,e),g(Ds.$$.fragment,e),g(Hs.$$.fragment,e),g(Is.$$.fragment,e),g(Vs.$$.fragment,e),g(Qs.$$.fragment,e),g(Rs.$$.fragment,e),g(Xs.$$.fragment,e),g(Us.$$.fragment,e),g(Ys.$$.fragment,e),g(Js.$$.fragment,e),g(Gs.$$.fragment,e),g(Ks.$$.fragment,e),g(Zs.$$.fragment,e),g(ea.$$.fragment,e),g(ta.$$.fragment,e),g(oa.$$.fragment,e),g(na.$$.fragment,e),g(sa.$$.fragment,e),g(aa.$$.fragment,e),g(ra.$$.fragment,e),g(da.$$.fragment,e),g(ia.$$.fragment,e),g(ua.$$.fragment,e),g(la.$$.fragment,e),g(pa.$$.fragment,e),g(ca.$$.fragment,e),g(ha.$$.fragment,e),g(fa.$$.fragment,e),g(ma.$$.fragment,e),g(_a.$$.fragment,e),g(ga.$$.fragment,e),g(va.$$.fragment,e),g(ya.$$.fragment,e),g(Ta.$$.fragment,e),g(ba.$$.fragment,e),g(wa.$$.fragment,e),g(xa.$$.fragment,e),g($a.$$.fragment,e),g(Oa.$$.fragment,e),g(qa.$$.fragment,e),g(Fa.$$.fragment,e),g(Sa.$$.fragment,e),g(Ma.$$.fragment,e),g(ka.$$.fragment,e),g(Aa.$$.fragment,e),g(Ca.$$.fragment,e),g(Ea.$$.fragment,e),g(Na.$$.fragment,e),g(za.$$.fragment,e),g(Pa.$$.fragment,e),g(Ba.$$.fragment,e),g(La.$$.fragment,e),g(Wa.$$.fragment,e),g(ja.$$.fragment,e),g(Da.$$.fragment,e),g(Ha.$$.fragment,e),g(Ia.$$.fragment,e),g(Va.$$.fragment,e),g(Qa.$$.fragment,e),g(Ra.$$.fragment,e),g(Xa.$$.fragment,e),g(Ua.$$.fragment,e),g(Ya.$$.fragment,e),g(Ja.$$.fragment,e),g(Ga.$$.fragment,e),g(Ka.$$.fragment,e),g(Za.$$.fragment,e),Vc=!1},d(e){t(x),e&&t(Yt),e&&t($),v(O),e&&t(M),e&&t(C),e&&t(Ol),e&&t(nr),e&&t(ql),v(xn,e),e&&t(Fl),e&&t(b),e&&t(Sl),e&&t(q),e&&t(Ml),e&&t(F),e&&t(kl),v($n,e),e&&t(Al),e&&t(Jt),e&&t(Cl),e&&t(S),e&&t(El),e&&t(ar),e&&t(Nl),e&&t(ee),v(On),e&&t(zl),e&&t(k),v(qn),v(Kt),v(Fn),e&&t(Pl),e&&t(oe),v(Mn),e&&t(Bl),e&&t(ne),v(kn),e&&t(Ll),e&&t(se),v(An),e&&t(Wl),e&&t(ae),v(Cn),e&&t(jl),e&&t(re),v(En),e&&t(Dl),e&&t(de),v(Nn),e&&t(Hl),e&&t(ie),v(zn),e&&t(Il),e&&t(ue),v(Pn),e&&t(Vl),e&&t(le),v(Bn),e&&t(Ql),e&&t(pe),v(Ln),e&&t(Rl),e&&t(ce),v(Wn),e&&t(Xl),e&&t(he),v(jn),e&&t(Ul),e&&t(fe),v(Dn),e&&t(Yl),e&&t(me),v(Hn),e&&t(Jl),e&&t(_e),v(In),e&&t(Gl),e&&t(ge),v(Vn),e&&t(Kl),e&&t(ve),v(Qn),e&&t(Zl),e&&t(ye),v(Rn),e&&t(ep),e&&t(Te),v(Xn),e&&t(tp),e&&t(be),v(Un),e&&t(op),e&&t(we),v(Yn),e&&t(np),e&&t(xe),v(Jn),e&&t(sp),e&&t($e),v(Gn),e&&t(ap),e&&t(Oe),v(Kn),e&&t(rp),e&&t(qe),v(Zn),e&&t(dp),e&&t(Fe),v(es),e&&t(ip),e&&t(Se),v(ts),e&&t(up),e&&t(Me),v(os),e&&t(lp),e&&t(ke),v(ns),e&&t(pp),e&&t(Ae),v(ss),e&&t(cp),e&&t(Ce),v(as),e&&t(hp),e&&t(Ee),v(rs),e&&t(fp),e&&t(Ne),v(ds),e&&t(mp),e&&t(ze),v(is),e&&t(_p),e&&t(Pe),v(us),e&&t(gp),e&&t(Be),v(ls),e&&t(vp),e&&t(Le),v(ps),e&&t(yp),e&&t(We),v(cs),e&&t(Tp),e&&t(je),v(hs),e&&t(bp),e&&t(De),v(fs),e&&t(wp),e&&t(He),v(ms),e&&t(xp),e&&t(Ie),v(_s),e&&t($p),e&&t(Ve),v(gs),e&&t(Op),e&&t(Qe),v(vs),e&&t(qp),e&&t(Re),v(ys),e&&t(Fp),e&&t(Xe),v(Ts),e&&t(Sp),e&&t(Ue),v(bs),e&&t(Mp),e&&t(Ye),v(ws),e&&t(kp),e&&t(Je),v($s),e&&t(Ap),e&&t(Ge),v(Os),e&&t(Cp),e&&t(Ke),v(Fs),e&&t(Ep),e&&t(Ze),v(Ss),e&&t(Np),e&&t(et),v(Ms),e&&t(zp),e&&t(tt),v(ks),e&&t(Pp),e&&t(ot),v(As),e&&t(Bp),e&&t(nt),v(Cs),e&&t(Lp),e&&t(st),v(Es),e&&t(Wp),e&&t(at),v(Ns),e&&t(jp),e&&t(rt),v(zs),e&&t(Dp),e&&t(dt),v(Ps),e&&t(Hp),e&&t(it),v(Bs),e&&t(Ip),e&&t(ut),v(Ls),e&&t(Vp),e&&t(lt),v(Ws),e&&t(Qp),e&&t(pt),v(js),e&&t(Rp),e&&t(ct),v(Ds),e&&t(Xp),e&&t(ht),v(Hs),e&&t(Up),e&&t(ft),v(Is),e&&t(Yp),e&&t(mt),v(Vs),e&&t(Jp),e&&t(_t),v(Qs),e&&t(Gp),e&&t(gt),v(Rs),e&&t(Kp),e&&t(vt),v(Xs),e&&t(Zp),e&&t(yt),v(Us),e&&t(ec),e&&t(Tt),v(Ys),e&&t(tc),e&&t(bt),v(Js),e&&t(oc),e&&t(wt),v(Gs),e&&t(nc),e&&t(xt),v(Ks),e&&t(sc),e&&t($t),v(Zs),e&&t(ac),e&&t(Ot),v(ea),e&&t(rc),e&&t(qt),v(ta),e&&t(dc),e&&t(Ft),v(oa),e&&t(ic),e&&t(St),v(na),e&&t(uc),e&&t(Mt),v(sa),e&&t(lc),e&&t(kt),v(aa),e&&t(pc),e&&t(At),v(ra),e&&t(cc),e&&t(Ct),v(da),e&&t(hc),e&&t(Et),v(ia),e&&t(fc),e&&t(Nt),v(ua),e&&t(mc),e&&t(E),v(la),v(pa),e&&t(_c),e&&t(zt),v(ca),e&&t(gc),e&&t(N),v(ha),v(fa),e&&t(vc),e&&t(Pt),v(ma),e&&t(yc),e&&t(z),v(_a),v(ga),e&&t(Tc),e&&t(Bt),v(va),e&&t(bc),e&&t(P),v(ya),v(Ta),e&&t(wc),e&&t(Lt),v(ba),e&&t(xc),e&&t(B),v(wa),v(xa),e&&t($c),e&&t(Wt),v($a),e&&t(Oc),e&&t(L),v(Oa),v(qa),e&&t(qc),e&&t(jt),v(Fa),e&&t(Fc),e&&t(W),v(Sa),v(Ma),e&&t(Sc),e&&t(Dt),v(ka),e&&t(Mc),e&&t(j),v(Aa),v(Ca),e&&t(kc),e&&t(Ht),v(Ea),e&&t(Ac),e&&t(D),v(Na),v(za),e&&t(Cc),e&&t(It),v(Pa),e&&t(Ec),e&&t(H),v(Ba),v(La),e&&t(Nc),e&&t(Vt),v(Wa),e&&t(zc),e&&t(I),v(ja),v(Da),e&&t(Pc),e&&t(Qt),v(Ha),e&&t(Bc),e&&t(V),v(Ia),v(Va),e&&t(Lc),e&&t(Rt),v(Qa),e&&t(Wc),e&&t(Q),v(Ra),v(Xa),e&&t(jc),e&&t(Xt),v(Ua),e&&t(Dc),e&&t(R),v(Ya),v(Ja),e&&t(Hc),e&&t(Ut),v(Ga),e&&t(Ic),e&&t(X),v(Ka),v(Za)}}}const rO={local:"model-outputs",sections:[{local:"transformers.utils.ModelOutput",title:"ModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutput",title:"BaseModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutputWithPooling",title:"BaseModelOutputWithPooling"},{local:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",title:"BaseModelOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",title:"BaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPast",title:"BaseModelOutputWithPast"},{local:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",title:"BaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_outputs.Seq2SeqModelOutput",title:"Seq2SeqModelOutput"},{local:"transformers.modeling_outputs.CausalLMOutput",title:"CausalLMOutput"},{local:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",title:"CausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.CausalLMOutputWithPast",title:"CausalLMOutputWithPast"},{local:"transformers.modeling_outputs.MaskedLMOutput",title:"MaskedLMOutput"},{local:"transformers.modeling_outputs.Seq2SeqLMOutput",title:"Seq2SeqLMOutput"},{local:"transformers.modeling_outputs.NextSentencePredictorOutput",title:"NextSentencePredictorOutput"},{local:"transformers.modeling_outputs.SequenceClassifierOutput",title:"SequenceClassifierOutput"},{local:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",title:"Seq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_outputs.MultipleChoiceModelOutput",title:"MultipleChoiceModelOutput"},{local:"transformers.modeling_outputs.TokenClassifierOutput",title:"TokenClassifierOutput"},{local:"transformers.modeling_outputs.QuestionAnsweringModelOutput",title:"QuestionAnsweringModelOutput"},{local:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",title:"Seq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_outputs.SemanticSegmenterOutput",title:"SemanticSegmenterOutput"},{local:"transformers.modeling_outputs.ImageClassifierOutput",title:"ImageClassifierOutput"},{local:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",title:"ImageClassifierOutputWithNoAttention"},{local:"transformers.modeling_outputs.DepthEstimatorOutput",title:"DepthEstimatorOutput"},{local:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput",title:"Wav2Vec2BaseModelOutput"},{local:"transformers.modeling_outputs.XVectorOutput",title:"XVectorOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutput",title:"TFBaseModelOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",title:"TFBaseModelOutputWithPooling"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",title:"TFBaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",title:"TFBaseModelOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",title:"TFBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",title:"TFSeq2SeqModelOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutput",title:"TFCausalLMOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",title:"TFCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",title:"TFCausalLMOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFMaskedLMOutput",title:"TFMaskedLMOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",title:"TFSeq2SeqLMOutput"},{local:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",title:"TFNextSentencePredictorOutput"},{local:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",title:"TFSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",title:"TFSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",title:"TFMultipleChoiceModelOutput"},{local:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",title:"TFTokenClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",title:"TFQuestionAnsweringModelOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",title:"TFSeq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",title:"FlaxBaseModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",title:"FlaxBaseModelOutputWithPast"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",title:"FlaxBaseModelOutputWithPooling"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",title:"FlaxBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",title:"FlaxSeq2SeqModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",title:"FlaxCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",title:"FlaxMaskedLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",title:"FlaxSeq2SeqLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",title:"FlaxNextSentencePredictorOutput"},{local:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",title:"FlaxSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",title:"FlaxSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",title:"FlaxMultipleChoiceModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",title:"FlaxTokenClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",title:"FlaxQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",title:"FlaxSeq2SeqQuestionAnsweringModelOutput"}],title:"Model outputs"};function dO($l){return oO(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hO extends K${constructor(x){super();Z$(this,x,dO,aO,eO,{})}}export{hO as default,rO as metadata};
