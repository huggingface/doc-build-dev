import{S as h5,i as f5,s as p5,e as l,k as c,w as p,t as r,M as m5,c as s,d as o,m as h,a as i,x as m,h as a,b as f,G as e,g as d,y as u,L as u5,q as v,o as _,B as E,v as v5}from"../chunks/vendor-hf-doc-builder.js";import{I as g}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as x}from"../chunks/CodeBlock-hf-doc-builder.js";function _5(DE){let me,Fi,ue,Ge,zr,no,Ec,Lr,bc,qi,ve,Ue,Ir,co,wc,We,yc,Mr,kc,$c,Sr,Tc,Bi,Zo,gc,Hi,_e,Xe,jr,ho,Cc,Fr,Oc,Ni,er,Dc,Ri,tr,xc,Gi,Ke,qr,Pc,Ac,Br,zc,Ui,Ee,Je,Hr,fo,Lc,Nr,Ic,Wi,Qe,po,Mc,Ve,Sc,Rr,jc,Fc,qc,Bc,be,Hc,Gr,Nc,Rc,Ur,Gc,Uc,Xi,Ye,Wc,Wr,Xc,Kc,Ki,mo,Ji,Ze,Jc,Xr,Qc,Vc,Qi,uo,Vi,we,et,Kr,vo,Yc,Jr,Zc,Yi,P,eh,Qr,th,oh,Vr,rh,ah,Yr,lh,sh,Zi,tt,ih,Zr,nh,dh,en,$,ea,ta,ch,hh,oa,ra,fh,ph,aa,la,mh,uh,sa,ia,vh,_h,na,da,Eh,bh,ca,ha,wh,yh,fa,pa,kh,$h,ma,ua,Th,tn,ye,ot,va,_o,gh,_a,Ch,on,V,Oh,Ea,Dh,xh,ba,Ph,Ah,rn,rt,zh,wa,Lh,Ih,an,Eo,ln,at,Mh,ya,Sh,jh,sn,bo,nn,or,Fh,dn,wo,cn,ke,lt,ka,yo,qh,$a,Bh,hn,st,Hh,Ta,Nh,Rh,fn,rr,Gh,pn,$e,it,ga,ko,Uh,Ca,Wh,mn,nt,Xh,Oa,Kh,Jh,un,dt,Qh,Da,Vh,Yh,vn,$o,_n,ct,Zh,xa,ef,tf,En,To,bn,Te,ht,Pa,go,of,ge,rf,Aa,af,lf,za,sf,nf,wn,ft,df,Co,La,cf,hf,ff,yn,pt,pf,Ia,mf,uf,kn,Ce,mt,Ma,Oo,vf,Sa,_f,$n,A,Ef,ja,bf,wf,Fa,yf,kf,qa,$f,Tf,Tn,ut,gf,Ba,Cf,Of,gn,Do,Cn,vt,Df,Ha,xf,Pf,On,xo,Dn,ar,Af,xn,Po,Pn,Oe,_t,Na,Ao,zf,Ra,Lf,An,Et,If,zo,Mf,Sf,zn,lr,jf,Ln,sr,Ff,In,b,z,Ga,qf,Bf,Ua,Hf,Nf,Wa,Rf,Gf,Xa,Uf,Wf,Xf,L,Ka,Kf,Jf,Ja,Qf,Vf,Qa,Yf,Zf,Va,ep,tp,op,Y,Ya,rp,ap,Za,lp,sp,el,ip,np,dp,Z,tl,cp,hp,ol,fp,pp,rl,mp,up,vp,ee,al,_p,Ep,ll,bp,wp,sl,yp,kp,$p,te,il,Tp,gp,nl,Cp,Op,dl,Dp,xp,Pp,oe,cl,Ap,zp,hl,Lp,Ip,fl,Mp,Sp,jp,re,pl,Fp,qp,ml,Bp,Hp,ul,Np,Rp,Gp,ae,vl,Up,Wp,_l,Xp,Kp,El,Jp,Qp,Vp,le,bl,Yp,Zp,wl,em,tm,yl,om,rm,am,se,kl,lm,sm,$l,im,nm,Tl,dm,cm,Mn,ir,hm,Sn,I,bt,gl,fm,pm,Cl,mm,um,vm,wt,Ol,_m,Em,Dl,bm,wm,ym,yt,xl,km,$m,Pl,Tm,gm,Cm,kt,Al,Om,Dm,zl,xm,Pm,jn,nr,Am,Fn,ie,De,zm,Ll,Lm,Im,Il,Mm,Sm,jm,xe,Fm,Ml,qm,Bm,Sl,Hm,Nm,Rm,Pe,Gm,jl,Um,Wm,Fl,Xm,Km,qn,$t,Jm,ql,Qm,Vm,Bn,T,j,Ym,Bl,Zm,eu,Hl,tu,ou,Nl,ru,au,lu,F,su,Rl,iu,nu,Gl,du,cu,Ul,hu,fu,pu,Ae,mu,Wl,uu,vu,Xl,_u,Eu,bu,q,wu,Kl,yu,ku,Jl,$u,Tu,Ql,gu,Cu,Ou,B,Du,Vl,xu,Pu,Yl,Au,zu,Zl,Lu,Iu,Mu,H,Su,es,ju,Fu,ts,qu,Bu,os,Hu,Nu,Ru,N,Gu,rs,Uu,Wu,as,Xu,Ku,ls,Ju,Qu,Vu,R,Yu,ss,Zu,ev,is,tv,ov,ns,rv,av,Hn,Tt,lv,ds,sv,iv,Nn,D,G,nv,cs,dv,cv,hs,hv,fv,fs,pv,mv,uv,U,vv,ps,_v,Ev,ms,bv,wv,us,yv,kv,$v,W,Tv,vs,gv,Cv,_s,Ov,Dv,Es,xv,Pv,Av,X,zv,bs,Lv,Iv,ws,Mv,Sv,ys,jv,Fv,qv,K,Bv,ks,Hv,Nv,$s,Rv,Gv,Ts,Uv,Wv,Rn,gt,Xv,gs,Kv,Jv,Gn,dr,J,Qv,Cs,Vv,Yv,Os,Zv,e_,Ds,t_,o_,Un,cr,r_,Wn,Ct,ze,a_,xs,l_,s_,Ps,i_,n_,d_,Le,c_,As,h_,f_,zs,p_,m_,Xn,hr,u_,Kn,fr,Q,v_,Ls,__,E_,Is,b_,w_,Ms,y_,k_,Jn,Ie,Ot,Ss,Lo,$_,js,T_,Qn,Dt,g_,Fs,C_,O_,Vn,Me,xt,qs,Io,D_,Se,x_,Bs,P_,A_,Hs,z_,L_,Yn,M,I_,Ns,M_,S_,Rs,j_,F_,Gs,q_,B_,Zn,Pt,H_,Us,N_,R_,ed,At,G_,Ws,U_,W_,td,je,zt,Xs,Mo,X_,Ks,K_,od,Lt,J_,Js,Q_,V_,rd,Fe,It,Qs,So,Y_,pr,Z_,Vs,e1,ad,ne,t1,Ys,o1,r1,Zs,a1,l1,ld,Mt,s1,jo,i1,n1,sd,St,d1,ei,c1,h1,id,de,f1,ti,p1,m1,oi,u1,v1,nd,Fo,dd,qe,jt,ri,qo,_1,ai,E1,cd,Ft,b1,li,w1,y1,hd,qt,si,Be,k1,ii,$1,T1,ni,g1,C1,O1,di,w,D1,ci,x1,P1,hi,A1,z1,fi,L1,I1,pi,M1,S1,mi,j1,F1,ui,q1,B1,vi,H1,N1,_i,R1,G1,Ei,U1,W1,bi,X1,K1,fd,Bt,J1,wi,Q1,V1,pd,mr,Y1,md,Bo,ud,He,Ht,yi,Ho,Z1,ki,eE,vd,S,tE,$i,oE,rE,Ti,aE,lE,gi,sE,iE,_d,ce,Ci,nE,dE,Oi,cE,hE,Di,fE,Ed,he,pE,xi,mE,uE,Pi,vE,_E,bd,Nt,EE,No,bE,wE,wd,fe,yE,Ai,kE,$E,zi,TE,gE,yd,Ro,kd;return no=new g({}),co=new g({}),ho=new g({}),fo=new g({}),mo=new x({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),uo=new x({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, use_fast=<span class="hljs-literal">False</span>)`}}),vo=new g({}),_o=new g({}),Eo=new x({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),bo=new x({props:{code:"pip install transformers[sentencepiece]",highlighted:"pip install transformers[sentencepiece]"}}),wo=new x({props:{code:"pip install transformers sentencepiece",highlighted:"pip install transformers sentencepiece"}}),yo=new g({}),ko=new g({}),$o=new x({props:{code:"from transformers.modeling_bert import BertLayer",highlighted:"from transformers.modeling_bert import BertLayer"}}),To=new x({props:{code:"from transformers.models.bert.modeling_bert import BertLayer",highlighted:"from transformers.models.bert.modeling_bert import BertLayer"}}),go=new g({}),Oo=new g({}),Do=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs)`}}),xo=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs, return_dict=False)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs, return_dict=False)`}}),Po=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased", return_dict=False)
outputs = model(**inputs)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, return_dict=False)
outputs = model(**inputs)`}}),Ao=new g({}),Lo=new g({}),Io=new g({}),Mo=new g({}),So=new g({}),Fo=new x({props:{code:`


`,highlighted:`<span class="hljs-comment"># Let&#x27;s load our model</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment"># If you used to have this line in pytorch-pretrained-bert:</span>
loss = model(input_ids, labels=labels)

<span class="hljs-comment"># Now just use this line in \u{1F917} Transformers to extract the loss from the output tuple:</span>
outputs = model(input_ids, labels=labels)
loss = outputs[<span class="hljs-number">0</span>]

<span class="hljs-comment"># In \u{1F917} Transformers you can also have access to the logits:</span>
loss, logits = outputs[:<span class="hljs-number">2</span>]

<span class="hljs-comment"># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs`}}),qo=new g({}),Bo=new x({props:{code:`

`,highlighted:`<span class="hljs-comment">### Let&#x27;s load a model and tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment">### Do some stuff to our model and tokenizer</span>
<span class="hljs-comment"># Ex: add new tokens to the vocabulary and embeddings of our model</span>
tokenizer.add_tokens([<span class="hljs-string">&quot;[SPECIAL_TOKEN_1]&quot;</span>, <span class="hljs-string">&quot;[SPECIAL_TOKEN_2]&quot;</span>])
model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))
<span class="hljs-comment"># Train our model</span>
train(model)

<span class="hljs-comment">### Now let&#x27;s save our model and tokenizer to a directory</span>
model.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)

<span class="hljs-comment">### Reload the model and the tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)`}}),Ho=new g({}),Ro=new x({props:{code:`
`,highlighted:`<span class="hljs-comment"># Parameters:</span>
lr = <span class="hljs-number">1e-3</span>
max_grad_norm = <span class="hljs-number">1.0</span>
num_training_steps = <span class="hljs-number">1000</span>
num_warmup_steps = <span class="hljs-number">100</span>
warmup_proportion = <span class="hljs-built_in">float</span>(num_warmup_steps) / <span class="hljs-built_in">float</span>(num_training_steps)  <span class="hljs-comment"># 0.1</span>

<span class="hljs-comment">### Previously BertAdam optimizer was instantiated like this:</span>
optimizer = BertAdam(
    model.parameters(),
    lr=lr,
    schedule=<span class="hljs-string">&quot;warmup_linear&quot;</span>,
    warmup=warmup_proportion,
    num_training_steps=num_training_steps,
)
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

<span class="hljs-comment">### In \u{1F917} Transformers, optimizer and schedules are split and instantiated like this:</span>
optimizer = AdamW(
    model.parameters(), lr=lr, correct_bias=<span class="hljs-literal">False</span>
)  <span class="hljs-comment"># To reproduce BertAdam specific behavior set correct_bias=False</span>
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  <span class="hljs-comment"># PyTorch scheduler</span>
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_grad_norm
    )  <span class="hljs-comment"># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>
    optimizer.step()
    scheduler.step()`}}),{c(){me=l("meta"),Fi=c(),ue=l("h1"),Ge=l("a"),zr=l("span"),p(no.$$.fragment),Ec=c(),Lr=l("span"),bc=r("Migrating from previous packages"),qi=c(),ve=l("h2"),Ue=l("a"),Ir=l("span"),p(co.$$.fragment),wc=c(),We=l("span"),yc=r("Migrating from transformers "),Mr=l("code"),kc=r("v3.x"),$c=r(" to "),Sr=l("code"),Tc=r("v4.x"),Bi=c(),Zo=l("p"),gc=r(`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),Hi=c(),_e=l("h4"),Xe=l("a"),jr=l("span"),p(ho.$$.fragment),Cc=c(),Fr=l("span"),Oc=r("1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),Ni=c(),er=l("p"),Dc=r("The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),Ri=c(),tr=l("p"),xc=r("This introduces two breaking changes:"),Gi=c(),Ke=l("ul"),qr=l("li"),Pc=r("The handling of overflowing tokens between the python and rust tokenizers is different."),Ac=c(),Br=l("li"),zc=r("The rust tokenizers do not accept integers in the encoding methods."),Ui=c(),Ee=l("h5"),Je=l("a"),Hr=l("span"),p(fo.$$.fragment),Lc=c(),Nr=l("span"),Ic=r("How to obtain the same behavior as v3.x in v4.x"),Wi=c(),Qe=l("ul"),po=l("li"),Mc=r("The pipelines now contain additional features out of the box. See the "),Ve=l("a"),Sc=r("token-classification pipeline with the "),Rr=l("code"),jc=r("grouped_entities"),Fc=r(" flag"),qc=r("."),Bc=c(),be=l("li"),Hc=r("The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=l("code"),Nc=r("use_fast"),Rc=r(" flag by setting it to "),Ur=l("code"),Gc=r("False"),Uc=r(":"),Xi=c(),Ye=l("p"),Wc=r("In version "),Wr=l("code"),Xc=r("v3.x"),Kc=r(":"),Ki=c(),p(mo.$$.fragment),Ji=c(),Ze=l("p"),Jc=r("to obtain the same in version "),Xr=l("code"),Qc=r("v4.x"),Vc=r(":"),Qi=c(),p(uo.$$.fragment),Vi=c(),we=l("h4"),et=l("a"),Kr=l("span"),p(vo.$$.fragment),Yc=c(),Jr=l("span"),Zc=r("2. SentencePiece is removed from the required dependencies"),Yi=c(),P=l("p"),eh=r("The requirement on the SentencePiece dependency has been lifted from the "),Qr=l("code"),th=r("setup.py"),oh=r(". This is done so that we may have a channel on anaconda cloud without relying on "),Vr=l("code"),rh=r("conda-forge"),ah=r(". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Yr=l("code"),lh=r("transformers"),sh=r(" installation."),Zi=c(),tt=l("p"),ih=r("This includes the "),Zr=l("strong"),nh=r("slow"),dh=r(" versions of:"),en=c(),$=l("ul"),ea=l("li"),ta=l("code"),ch=r("XLNetTokenizer"),hh=c(),oa=l("li"),ra=l("code"),fh=r("AlbertTokenizer"),ph=c(),aa=l("li"),la=l("code"),mh=r("CamembertTokenizer"),uh=c(),sa=l("li"),ia=l("code"),vh=r("MBartTokenizer"),_h=c(),na=l("li"),da=l("code"),Eh=r("PegasusTokenizer"),bh=c(),ca=l("li"),ha=l("code"),wh=r("T5Tokenizer"),yh=c(),fa=l("li"),pa=l("code"),kh=r("ReformerTokenizer"),$h=c(),ma=l("li"),ua=l("code"),Th=r("XLMRobertaTokenizer"),tn=c(),ye=l("h5"),ot=l("a"),va=l("span"),p(_o.$$.fragment),gh=c(),_a=l("span"),Ch=r("How to obtain the same behavior as v3.x in v4.x"),on=c(),V=l("p"),Oh=r("In order to obtain the same behavior as version "),Ea=l("code"),Dh=r("v3.x"),xh=r(", you should install "),ba=l("code"),Ph=r("sentencepiece"),Ah=r(" additionally:"),rn=c(),rt=l("p"),zh=r("In version "),wa=l("code"),Lh=r("v3.x"),Ih=r(":"),an=c(),p(Eo.$$.fragment),ln=c(),at=l("p"),Mh=r("to obtain the same in version "),ya=l("code"),Sh=r("v4.x"),jh=r(":"),sn=c(),p(bo.$$.fragment),nn=c(),or=l("p"),Fh=r("or"),dn=c(),p(wo.$$.fragment),cn=c(),ke=l("h4"),lt=l("a"),ka=l("span"),p(yo.$$.fragment),qh=c(),$a=l("span"),Bh=r("3. The architecture of the repo has been updated so that each model resides in its folder"),hn=c(),st=l("p"),Hh=r("The past and foreseeable addition of new models means that the number of files in the directory "),Ta=l("code"),Nh=r("src/transformers"),Rh=r(" keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),fn=c(),rr=l("p"),Gh=r("This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),pn=c(),$e=l("h5"),it=l("a"),ga=l("span"),p(ko.$$.fragment),Uh=c(),Ca=l("span"),Wh=r("How to obtain the same behavior as v3.x in v4.x"),mn=c(),nt=l("p"),Xh=r("In order to obtain the same behavior as version "),Oa=l("code"),Kh=r("v3.x"),Jh=r(", you should update the path used to access the layers."),un=c(),dt=l("p"),Qh=r("In version "),Da=l("code"),Vh=r("v3.x"),Yh=r(":"),vn=c(),p($o.$$.fragment),_n=c(),ct=l("p"),Zh=r("to obtain the same in version "),xa=l("code"),ef=r("v4.x"),tf=r(":"),En=c(),p(To.$$.fragment),bn=c(),Te=l("h4"),ht=l("a"),Pa=l("span"),p(go.$$.fragment),of=c(),ge=l("span"),rf=r("4. Switching the "),Aa=l("code"),af=r("return_dict"),lf=r(" argument to "),za=l("code"),sf=r("True"),nf=r(" by default"),wn=c(),ft=l("p"),df=r("The "),Co=l("a"),La=l("code"),cf=r("return_dict"),hf=r(" argument"),ff=r(" enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),yn=c(),pt=l("p"),pf=r("This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Ia=l("code"),mf=r("value0, value1 = outputs"),uf=r(" will not work."),kn=c(),Ce=l("h5"),mt=l("a"),Ma=l("span"),p(Oo.$$.fragment),vf=c(),Sa=l("span"),_f=r("How to obtain the same behavior as v3.x in v4.x"),$n=c(),A=l("p"),Ef=r("In order to obtain the same behavior as version "),ja=l("code"),bf=r("v3.x"),wf=r(", you should specify the "),Fa=l("code"),yf=r("return_dict"),kf=r(" argument to "),qa=l("code"),$f=r("False"),Tf=r(", either in the model configuration or during the forward pass."),Tn=c(),ut=l("p"),gf=r("In version "),Ba=l("code"),Cf=r("v3.x"),Of=r(":"),gn=c(),p(Do.$$.fragment),Cn=c(),vt=l("p"),Df=r("to obtain the same in version "),Ha=l("code"),xf=r("v4.x"),Pf=r(":"),On=c(),p(xo.$$.fragment),Dn=c(),ar=l("p"),Af=r("or"),xn=c(),p(Po.$$.fragment),Pn=c(),Oe=l("h4"),_t=l("a"),Na=l("span"),p(Ao.$$.fragment),zf=c(),Ra=l("span"),Lf=r("5. Removed some deprecated attributes"),An=c(),Et=l("p"),If=r("Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),zo=l("a"),Mf=r("#8604"),Sf=r("."),zn=c(),lr=l("p"),jf=r("Here is a list of these attributes/methods/arguments and what their replacements should be:"),Ln=c(),sr=l("p"),Ff=r("In several models, the labels become consistent with the other models:"),In=c(),b=l("ul"),z=l("li"),Ga=l("code"),qf=r("masked_lm_labels"),Bf=r(" becomes "),Ua=l("code"),Hf=r("labels"),Nf=r(" in "),Wa=l("code"),Rf=r("AlbertForMaskedLM"),Gf=r(" and "),Xa=l("code"),Uf=r("AlbertForPreTraining"),Wf=r("."),Xf=c(),L=l("li"),Ka=l("code"),Kf=r("masked_lm_labels"),Jf=r(" becomes "),Ja=l("code"),Qf=r("labels"),Vf=r(" in "),Qa=l("code"),Yf=r("BertForMaskedLM"),Zf=r(" and "),Va=l("code"),ep=r("BertForPreTraining"),tp=r("."),op=c(),Y=l("li"),Ya=l("code"),rp=r("masked_lm_labels"),ap=r(" becomes "),Za=l("code"),lp=r("labels"),sp=r(" in "),el=l("code"),ip=r("DistilBertForMaskedLM"),np=r("."),dp=c(),Z=l("li"),tl=l("code"),cp=r("masked_lm_labels"),hp=r(" becomes "),ol=l("code"),fp=r("labels"),pp=r(" in "),rl=l("code"),mp=r("ElectraForMaskedLM"),up=r("."),vp=c(),ee=l("li"),al=l("code"),_p=r("masked_lm_labels"),Ep=r(" becomes "),ll=l("code"),bp=r("labels"),wp=r(" in "),sl=l("code"),yp=r("LongformerForMaskedLM"),kp=r("."),$p=c(),te=l("li"),il=l("code"),Tp=r("masked_lm_labels"),gp=r(" becomes "),nl=l("code"),Cp=r("labels"),Op=r(" in "),dl=l("code"),Dp=r("MobileBertForMaskedLM"),xp=r("."),Pp=c(),oe=l("li"),cl=l("code"),Ap=r("masked_lm_labels"),zp=r(" becomes "),hl=l("code"),Lp=r("labels"),Ip=r(" in "),fl=l("code"),Mp=r("RobertaForMaskedLM"),Sp=r("."),jp=c(),re=l("li"),pl=l("code"),Fp=r("lm_labels"),qp=r(" becomes "),ml=l("code"),Bp=r("labels"),Hp=r(" in "),ul=l("code"),Np=r("BartForConditionalGeneration"),Rp=r("."),Gp=c(),ae=l("li"),vl=l("code"),Up=r("lm_labels"),Wp=r(" becomes "),_l=l("code"),Xp=r("labels"),Kp=r(" in "),El=l("code"),Jp=r("GPT2DoubleHeadsModel"),Qp=r("."),Vp=c(),le=l("li"),bl=l("code"),Yp=r("lm_labels"),Zp=r(" becomes "),wl=l("code"),em=r("labels"),tm=r(" in "),yl=l("code"),om=r("OpenAIGPTDoubleHeadsModel"),rm=r("."),am=c(),se=l("li"),kl=l("code"),lm=r("lm_labels"),sm=r(" becomes "),$l=l("code"),im=r("labels"),nm=r(" in "),Tl=l("code"),dm=r("T5ForConditionalGeneration"),cm=r("."),Mn=c(),ir=l("p"),hm=r("In several models, the caching mechanism becomes consistent with the other models:"),Sn=c(),I=l("ul"),bt=l("li"),gl=l("code"),fm=r("decoder_cached_states"),pm=r(" becomes "),Cl=l("code"),mm=r("past_key_values"),um=r(" in all BART-like, FSMT and T5 models."),vm=c(),wt=l("li"),Ol=l("code"),_m=r("decoder_past_key_values"),Em=r(" becomes "),Dl=l("code"),bm=r("past_key_values"),wm=r(" in all BART-like, FSMT and T5 models."),ym=c(),yt=l("li"),xl=l("code"),km=r("past"),$m=r(" becomes "),Pl=l("code"),Tm=r("past_key_values"),gm=r(" in all CTRL models."),Cm=c(),kt=l("li"),Al=l("code"),Om=r("past"),Dm=r(" becomes "),zl=l("code"),xm=r("past_key_values"),Pm=r(" in all GPT-2 models."),jn=c(),nr=l("p"),Am=r("Regarding the tokenizer classes:"),Fn=c(),ie=l("ul"),De=l("li"),zm=r("The tokenizer attribute "),Ll=l("code"),Lm=r("max_len"),Im=r(" becomes "),Il=l("code"),Mm=r("model_max_length"),Sm=r("."),jm=c(),xe=l("li"),Fm=r("The tokenizer attribute "),Ml=l("code"),qm=r("return_lengths"),Bm=r(" becomes "),Sl=l("code"),Hm=r("return_length"),Nm=r("."),Rm=c(),Pe=l("li"),Gm=r("The tokenizer encoding argument "),jl=l("code"),Um=r("is_pretokenized"),Wm=r(" becomes "),Fl=l("code"),Xm=r("is_split_into_words"),Km=r("."),qn=c(),$t=l("p"),Jm=r("Regarding the "),ql=l("code"),Qm=r("Trainer"),Vm=r(" class:"),Bn=c(),T=l("ul"),j=l("li"),Ym=r("The "),Bl=l("code"),Zm=r("Trainer"),eu=r(" argument "),Hl=l("code"),tu=r("tb_writer"),ou=r(" is removed in favor of the callback "),Nl=l("code"),ru=r("TensorBoardCallback(tb_writer=...)"),au=r("."),lu=c(),F=l("li"),su=r("The "),Rl=l("code"),iu=r("Trainer"),nu=r(" argument "),Gl=l("code"),du=r("prediction_loss_only"),cu=r(" is removed in favor of the class argument "),Ul=l("code"),hu=r("args.prediction_loss_only"),fu=r("."),pu=c(),Ae=l("li"),mu=r("The "),Wl=l("code"),uu=r("Trainer"),vu=r(" attribute "),Xl=l("code"),_u=r("data_collator"),Eu=r(" should be a callable."),bu=c(),q=l("li"),wu=r("The "),Kl=l("code"),yu=r("Trainer"),ku=r(" method "),Jl=l("code"),$u=r("_log"),Tu=r(" is deprecated in favor of "),Ql=l("code"),gu=r("log"),Cu=r("."),Ou=c(),B=l("li"),Du=r("The "),Vl=l("code"),xu=r("Trainer"),Pu=r(" method "),Yl=l("code"),Au=r("_training_step"),zu=r(" is deprecated in favor of "),Zl=l("code"),Lu=r("training_step"),Iu=r("."),Mu=c(),H=l("li"),Su=r("The "),es=l("code"),ju=r("Trainer"),Fu=r(" method "),ts=l("code"),qu=r("_prediction_loop"),Bu=r(" is deprecated in favor of "),os=l("code"),Hu=r("prediction_loop"),Nu=r("."),Ru=c(),N=l("li"),Gu=r("The "),rs=l("code"),Uu=r("Trainer"),Wu=r(" method "),as=l("code"),Xu=r("is_local_master"),Ku=r(" is deprecated in favor of "),ls=l("code"),Ju=r("is_local_process_zero"),Qu=r("."),Vu=c(),R=l("li"),Yu=r("The "),ss=l("code"),Zu=r("Trainer"),ev=r(" method "),is=l("code"),tv=r("is_world_master"),ov=r(" is deprecated in favor of "),ns=l("code"),rv=r("is_world_process_zero"),av=r("."),Hn=c(),Tt=l("p"),lv=r("Regarding the "),ds=l("code"),sv=r("TFTrainer"),iv=r(" class:"),Nn=c(),D=l("ul"),G=l("li"),nv=r("The "),cs=l("code"),dv=r("TFTrainer"),cv=r(" argument "),hs=l("code"),hv=r("prediction_loss_only"),fv=r(" is removed in favor of the class argument "),fs=l("code"),pv=r("args.prediction_loss_only"),mv=r("."),uv=c(),U=l("li"),vv=r("The "),ps=l("code"),_v=r("Trainer"),Ev=r(" method "),ms=l("code"),bv=r("_log"),wv=r(" is deprecated in favor of "),us=l("code"),yv=r("log"),kv=r("."),$v=c(),W=l("li"),Tv=r("The "),vs=l("code"),gv=r("TFTrainer"),Cv=r(" method "),_s=l("code"),Ov=r("_prediction_loop"),Dv=r(" is deprecated in favor of "),Es=l("code"),xv=r("prediction_loop"),Pv=r("."),Av=c(),X=l("li"),zv=r("The "),bs=l("code"),Lv=r("TFTrainer"),Iv=r(" method "),ws=l("code"),Mv=r("_setup_wandb"),Sv=r(" is deprecated in favor of "),ys=l("code"),jv=r("setup_wandb"),Fv=r("."),qv=c(),K=l("li"),Bv=r("The "),ks=l("code"),Hv=r("TFTrainer"),Nv=r(" method "),$s=l("code"),Rv=r("_run_model"),Gv=r(" is deprecated in favor of "),Ts=l("code"),Uv=r("run_model"),Wv=r("."),Rn=c(),gt=l("p"),Xv=r("Regarding the "),gs=l("code"),Kv=r("TrainingArguments"),Jv=r(" class:"),Gn=c(),dr=l("ul"),J=l("li"),Qv=r("The "),Cs=l("code"),Vv=r("TrainingArguments"),Yv=r(" argument "),Os=l("code"),Zv=r("evaluate_during_training"),e_=r(" is deprecated in favor of "),Ds=l("code"),t_=r("evaluation_strategy"),o_=r("."),Un=c(),cr=l("p"),r_=r("Regarding the Transfo-XL model:"),Wn=c(),Ct=l("ul"),ze=l("li"),a_=r("The Transfo-XL configuration attribute "),xs=l("code"),l_=r("tie_weight"),s_=r(" becomes "),Ps=l("code"),i_=r("tie_words_embeddings"),n_=r("."),d_=c(),Le=l("li"),c_=r("The Transfo-XL modeling method "),As=l("code"),h_=r("reset_length"),f_=r(" becomes "),zs=l("code"),p_=r("reset_memory_length"),m_=r("."),Xn=c(),hr=l("p"),u_=r("Regarding pipelines:"),Kn=c(),fr=l("ul"),Q=l("li"),v_=r("The "),Ls=l("code"),__=r("FillMaskPipeline"),E_=r(" argument "),Is=l("code"),b_=r("topk"),w_=r(" becomes "),Ms=l("code"),y_=r("top_k"),k_=r("."),Jn=c(),Ie=l("h2"),Ot=l("a"),Ss=l("span"),p(Lo.$$.fragment),$_=c(),js=l("span"),T_=r("Migrating from pytorch-transformers to \u{1F917} Transformers"),Qn=c(),Dt=l("p"),g_=r("Here is a quick summary of what you should take care of when migrating from "),Fs=l("code"),C_=r("pytorch-transformers"),O_=r(" to \u{1F917} Transformers."),Vn=c(),Me=l("h3"),xt=l("a"),qs=l("span"),p(Io.$$.fragment),D_=c(),Se=l("span"),x_=r("Positional order of some models' keywords inputs ("),Bs=l("code"),P_=r("attention_mask"),A_=r(", "),Hs=l("code"),z_=r("token_type_ids"),L_=r("...) changed"),Yn=c(),M=l("p"),I_=r("To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Ns=l("strong"),M_=r("keywords inputs"),S_=r(" ("),Rs=l("code"),j_=r("attention_mask"),F_=r(", "),Gs=l("code"),q_=r("token_type_ids"),B_=r("\u2026) has been changed."),Zn=c(),Pt=l("p"),H_=r("If you used to call the models with keyword names for keyword arguments, e.g. "),Us=l("code"),N_=r("model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),R_=r(", this should not cause any change."),ed=c(),At=l("p"),G_=r("If you used to call the models with positional inputs for keyword arguments, e.g. "),Ws=l("code"),U_=r("model(inputs_ids, attention_mask, token_type_ids)"),W_=r(", you may have to double check the exact order of input arguments."),td=c(),je=l("h2"),zt=l("a"),Xs=l("span"),p(Mo.$$.fragment),X_=c(),Ks=l("span"),K_=r("Migrating from pytorch-pretrained-bert"),od=c(),Lt=l("p"),J_=r("Here is a quick summary of what you should take care of when migrating from "),Js=l("code"),Q_=r("pytorch-pretrained-bert"),V_=r(" to \u{1F917} Transformers"),rd=c(),Fe=l("h3"),It=l("a"),Qs=l("span"),p(So.$$.fragment),Y_=c(),pr=l("span"),Z_=r("Models always output "),Vs=l("code"),e1=r("tuples"),ad=c(),ne=l("p"),t1=r("The main breaking change when migrating from "),Ys=l("code"),o1=r("pytorch-pretrained-bert"),r1=r(" to \u{1F917} Transformers is that the models forward method always outputs a "),Zs=l("code"),a1=r("tuple"),l1=r(" with various elements depending on the model and the configuration parameters."),ld=c(),Mt=l("p"),s1=r("The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=l("a"),i1=r("documentation"),n1=r("."),sd=c(),St=l("p"),d1=r("In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ei=l("code"),c1=r("pytorch-pretrained-bert"),h1=r("."),id=c(),de=l("p"),f1=r("Here is a "),ti=l("code"),p1=r("pytorch-pretrained-bert"),m1=r(" to \u{1F917} Transformers conversion example for a "),oi=l("code"),u1=r("BertForSequenceClassification"),v1=r(" classification model:"),nd=c(),p(Fo.$$.fragment),dd=c(),qe=l("h3"),jt=l("a"),ri=l("span"),p(qo.$$.fragment),_1=c(),ai=l("span"),E1=r("Serialization"),cd=c(),Ft=l("p"),b1=r("Breaking change in the "),li=l("code"),w1=r("from_pretrained()"),y1=r("method:"),hd=c(),qt=l("ol"),si=l("li"),Be=l("p"),k1=r("Models are now set in evaluation mode by default when instantiated with the "),ii=l("code"),$1=r("from_pretrained()"),T1=r(" method. To train them don\u2019t forget to set them back in training mode ("),ni=l("code"),g1=r("model.train()"),C1=r(") to activate the dropout modules."),O1=c(),di=l("li"),w=l("p"),D1=r("The additional "),ci=l("code"),x1=r("*inputs"),P1=r(" and "),hi=l("code"),A1=r("**kwargs"),z1=r(" arguments supplied to the "),fi=l("code"),L1=r("from_pretrained()"),I1=r(" method used to be directly passed to the underlying model\u2019s class "),pi=l("code"),M1=r("__init__()"),S1=r(" method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),mi=l("code"),j1=r("BertForSequenceClassification"),F1=r(" examples. More precisely, the positional arguments "),ui=l("code"),q1=r("*inputs"),B1=r(" provided to "),vi=l("code"),H1=r("from_pretrained()"),N1=r(" are directly forwarded the model "),_i=l("code"),R1=r("__init__()"),G1=r(" method while the keyword arguments "),Ei=l("code"),U1=r("**kwargs"),W1=r(" (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),bi=l("code"),X1=r("__init__()"),K1=r(" method."),fd=c(),Bt=l("p"),J1=r("Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),wi=l("code"),Q1=r("save_pretrained(save_directory)"),V1=r(" if you were using any other serialization method before."),pd=c(),mr=l("p"),Y1=r("Here is an example:"),md=c(),p(Bo.$$.fragment),ud=c(),He=l("h3"),Ht=l("a"),yi=l("span"),p(Ho.$$.fragment),Z1=c(),ki=l("span"),eE=r("Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),vd=c(),S=l("p"),tE=r("The two optimizers previously included, "),$i=l("code"),oE=r("BertAdam"),rE=r(" and "),Ti=l("code"),aE=r("OpenAIAdam"),lE=r(", have been replaced by a single "),gi=l("code"),sE=r("AdamW"),iE=r(" optimizer which has a few differences:"),_d=c(),ce=l("ul"),Ci=l("li"),nE=r("it only implements weights decay correction,"),dE=c(),Oi=l("li"),cE=r("schedules are now externals (see below),"),hE=c(),Di=l("li"),fE=r("gradient clipping is now also external (see below)."),Ed=c(),he=l("p"),pE=r("The new optimizer "),xi=l("code"),mE=r("AdamW"),uE=r(" matches PyTorch "),Pi=l("code"),vE=r("Adam"),_E=r(" optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),bd=c(),Nt=l("p"),EE=r("The schedules are now standard "),No=l("a"),bE=r("PyTorch learning rate schedulers"),wE=r(" and not part of the optimizer anymore."),wd=c(),fe=l("p"),yE=r("Here is a conversion examples from "),Ai=l("code"),kE=r("BertAdam"),$E=r(" with a linear warmup and decay schedule to "),zi=l("code"),TE=r("AdamW"),gE=r(" and the same schedule:"),yd=c(),p(Ro.$$.fragment),this.h()},l(t){const n=m5('[data-svelte="svelte-1phssyn"]',document.head);me=s(n,"META",{name:!0,content:!0}),n.forEach(o),Fi=h(t),ue=s(t,"H1",{class:!0});var $d=i(ue);Ge=s($d,"A",{id:!0,class:!0,href:!0});var xE=i(Ge);zr=s(xE,"SPAN",{});var PE=i(zr);m(no.$$.fragment,PE),PE.forEach(o),xE.forEach(o),Ec=h($d),Lr=s($d,"SPAN",{});var AE=i(Lr);bc=a(AE,"Migrating from previous packages"),AE.forEach(o),$d.forEach(o),qi=h(t),ve=s(t,"H2",{class:!0});var Td=i(ve);Ue=s(Td,"A",{id:!0,class:!0,href:!0});var zE=i(Ue);Ir=s(zE,"SPAN",{});var LE=i(Ir);m(co.$$.fragment,LE),LE.forEach(o),zE.forEach(o),wc=h(Td),We=s(Td,"SPAN",{});var Li=i(We);yc=a(Li,"Migrating from transformers "),Mr=s(Li,"CODE",{});var IE=i(Mr);kc=a(IE,"v3.x"),IE.forEach(o),$c=a(Li," to "),Sr=s(Li,"CODE",{});var ME=i(Sr);Tc=a(ME,"v4.x"),ME.forEach(o),Li.forEach(o),Td.forEach(o),Bi=h(t),Zo=s(t,"P",{});var SE=i(Zo);gc=a(SE,`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),SE.forEach(o),Hi=h(t),_e=s(t,"H4",{class:!0});var gd=i(_e);Xe=s(gd,"A",{id:!0,class:!0,href:!0});var jE=i(Xe);jr=s(jE,"SPAN",{});var FE=i(jr);m(ho.$$.fragment,FE),FE.forEach(o),jE.forEach(o),Cc=h(gd),Fr=s(gd,"SPAN",{});var qE=i(Fr);Oc=a(qE,"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),qE.forEach(o),gd.forEach(o),Ni=h(t),er=s(t,"P",{});var BE=i(er);Dc=a(BE,"The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),BE.forEach(o),Ri=h(t),tr=s(t,"P",{});var HE=i(tr);xc=a(HE,"This introduces two breaking changes:"),HE.forEach(o),Gi=h(t),Ke=s(t,"UL",{});var Cd=i(Ke);qr=s(Cd,"LI",{});var NE=i(qr);Pc=a(NE,"The handling of overflowing tokens between the python and rust tokenizers is different."),NE.forEach(o),Ac=h(Cd),Br=s(Cd,"LI",{});var RE=i(Br);zc=a(RE,"The rust tokenizers do not accept integers in the encoding methods."),RE.forEach(o),Cd.forEach(o),Ui=h(t),Ee=s(t,"H5",{class:!0});var Od=i(Ee);Je=s(Od,"A",{id:!0,class:!0,href:!0});var GE=i(Je);Hr=s(GE,"SPAN",{});var UE=i(Hr);m(fo.$$.fragment,UE),UE.forEach(o),GE.forEach(o),Lc=h(Od),Nr=s(Od,"SPAN",{});var WE=i(Nr);Ic=a(WE,"How to obtain the same behavior as v3.x in v4.x"),WE.forEach(o),Od.forEach(o),Wi=h(t),Qe=s(t,"UL",{});var Dd=i(Qe);po=s(Dd,"LI",{});var xd=i(po);Mc=a(xd,"The pipelines now contain additional features out of the box. See the "),Ve=s(xd,"A",{href:!0});var Pd=i(Ve);Sc=a(Pd,"token-classification pipeline with the "),Rr=s(Pd,"CODE",{});var XE=i(Rr);jc=a(XE,"grouped_entities"),XE.forEach(o),Fc=a(Pd," flag"),Pd.forEach(o),qc=a(xd,"."),xd.forEach(o),Bc=h(Dd),be=s(Dd,"LI",{});var ur=i(be);Hc=a(ur,"The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=s(ur,"CODE",{});var KE=i(Gr);Nc=a(KE,"use_fast"),KE.forEach(o),Rc=a(ur," flag by setting it to "),Ur=s(ur,"CODE",{});var JE=i(Ur);Gc=a(JE,"False"),JE.forEach(o),Uc=a(ur,":"),ur.forEach(o),Dd.forEach(o),Xi=h(t),Ye=s(t,"P",{});var Ad=i(Ye);Wc=a(Ad,"In version "),Wr=s(Ad,"CODE",{});var QE=i(Wr);Xc=a(QE,"v3.x"),QE.forEach(o),Kc=a(Ad,":"),Ad.forEach(o),Ki=h(t),m(mo.$$.fragment,t),Ji=h(t),Ze=s(t,"P",{});var zd=i(Ze);Jc=a(zd,"to obtain the same in version "),Xr=s(zd,"CODE",{});var VE=i(Xr);Qc=a(VE,"v4.x"),VE.forEach(o),Vc=a(zd,":"),zd.forEach(o),Qi=h(t),m(uo.$$.fragment,t),Vi=h(t),we=s(t,"H4",{class:!0});var Ld=i(we);et=s(Ld,"A",{id:!0,class:!0,href:!0});var YE=i(et);Kr=s(YE,"SPAN",{});var ZE=i(Kr);m(vo.$$.fragment,ZE),ZE.forEach(o),YE.forEach(o),Yc=h(Ld),Jr=s(Ld,"SPAN",{});var eb=i(Jr);Zc=a(eb,"2. SentencePiece is removed from the required dependencies"),eb.forEach(o),Ld.forEach(o),Yi=h(t),P=s(t,"P",{});var Rt=i(P);eh=a(Rt,"The requirement on the SentencePiece dependency has been lifted from the "),Qr=s(Rt,"CODE",{});var tb=i(Qr);th=a(tb,"setup.py"),tb.forEach(o),oh=a(Rt,". This is done so that we may have a channel on anaconda cloud without relying on "),Vr=s(Rt,"CODE",{});var ob=i(Vr);rh=a(ob,"conda-forge"),ob.forEach(o),ah=a(Rt,". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Yr=s(Rt,"CODE",{});var rb=i(Yr);lh=a(rb,"transformers"),rb.forEach(o),sh=a(Rt," installation."),Rt.forEach(o),Zi=h(t),tt=s(t,"P",{});var Id=i(tt);ih=a(Id,"This includes the "),Zr=s(Id,"STRONG",{});var ab=i(Zr);nh=a(ab,"slow"),ab.forEach(o),dh=a(Id," versions of:"),Id.forEach(o),en=h(t),$=s(t,"UL",{});var C=i($);ea=s(C,"LI",{});var lb=i(ea);ta=s(lb,"CODE",{});var sb=i(ta);ch=a(sb,"XLNetTokenizer"),sb.forEach(o),lb.forEach(o),hh=h(C),oa=s(C,"LI",{});var ib=i(oa);ra=s(ib,"CODE",{});var nb=i(ra);fh=a(nb,"AlbertTokenizer"),nb.forEach(o),ib.forEach(o),ph=h(C),aa=s(C,"LI",{});var db=i(aa);la=s(db,"CODE",{});var cb=i(la);mh=a(cb,"CamembertTokenizer"),cb.forEach(o),db.forEach(o),uh=h(C),sa=s(C,"LI",{});var hb=i(sa);ia=s(hb,"CODE",{});var fb=i(ia);vh=a(fb,"MBartTokenizer"),fb.forEach(o),hb.forEach(o),_h=h(C),na=s(C,"LI",{});var pb=i(na);da=s(pb,"CODE",{});var mb=i(da);Eh=a(mb,"PegasusTokenizer"),mb.forEach(o),pb.forEach(o),bh=h(C),ca=s(C,"LI",{});var ub=i(ca);ha=s(ub,"CODE",{});var vb=i(ha);wh=a(vb,"T5Tokenizer"),vb.forEach(o),ub.forEach(o),yh=h(C),fa=s(C,"LI",{});var _b=i(fa);pa=s(_b,"CODE",{});var Eb=i(pa);kh=a(Eb,"ReformerTokenizer"),Eb.forEach(o),_b.forEach(o),$h=h(C),ma=s(C,"LI",{});var bb=i(ma);ua=s(bb,"CODE",{});var wb=i(ua);Th=a(wb,"XLMRobertaTokenizer"),wb.forEach(o),bb.forEach(o),C.forEach(o),tn=h(t),ye=s(t,"H5",{class:!0});var Md=i(ye);ot=s(Md,"A",{id:!0,class:!0,href:!0});var yb=i(ot);va=s(yb,"SPAN",{});var kb=i(va);m(_o.$$.fragment,kb),kb.forEach(o),yb.forEach(o),gh=h(Md),_a=s(Md,"SPAN",{});var $b=i(_a);Ch=a($b,"How to obtain the same behavior as v3.x in v4.x"),$b.forEach(o),Md.forEach(o),on=h(t),V=s(t,"P",{});var vr=i(V);Oh=a(vr,"In order to obtain the same behavior as version "),Ea=s(vr,"CODE",{});var Tb=i(Ea);Dh=a(Tb,"v3.x"),Tb.forEach(o),xh=a(vr,", you should install "),ba=s(vr,"CODE",{});var gb=i(ba);Ph=a(gb,"sentencepiece"),gb.forEach(o),Ah=a(vr," additionally:"),vr.forEach(o),rn=h(t),rt=s(t,"P",{});var Sd=i(rt);zh=a(Sd,"In version "),wa=s(Sd,"CODE",{});var Cb=i(wa);Lh=a(Cb,"v3.x"),Cb.forEach(o),Ih=a(Sd,":"),Sd.forEach(o),an=h(t),m(Eo.$$.fragment,t),ln=h(t),at=s(t,"P",{});var jd=i(at);Mh=a(jd,"to obtain the same in version "),ya=s(jd,"CODE",{});var Ob=i(ya);Sh=a(Ob,"v4.x"),Ob.forEach(o),jh=a(jd,":"),jd.forEach(o),sn=h(t),m(bo.$$.fragment,t),nn=h(t),or=s(t,"P",{});var Db=i(or);Fh=a(Db,"or"),Db.forEach(o),dn=h(t),m(wo.$$.fragment,t),cn=h(t),ke=s(t,"H4",{class:!0});var Fd=i(ke);lt=s(Fd,"A",{id:!0,class:!0,href:!0});var xb=i(lt);ka=s(xb,"SPAN",{});var Pb=i(ka);m(yo.$$.fragment,Pb),Pb.forEach(o),xb.forEach(o),qh=h(Fd),$a=s(Fd,"SPAN",{});var Ab=i($a);Bh=a(Ab,"3. The architecture of the repo has been updated so that each model resides in its folder"),Ab.forEach(o),Fd.forEach(o),hn=h(t),st=s(t,"P",{});var qd=i(st);Hh=a(qd,"The past and foreseeable addition of new models means that the number of files in the directory "),Ta=s(qd,"CODE",{});var zb=i(Ta);Nh=a(zb,"src/transformers"),zb.forEach(o),Rh=a(qd," keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),qd.forEach(o),fn=h(t),rr=s(t,"P",{});var Lb=i(rr);Gh=a(Lb,"This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),Lb.forEach(o),pn=h(t),$e=s(t,"H5",{class:!0});var Bd=i($e);it=s(Bd,"A",{id:!0,class:!0,href:!0});var Ib=i(it);ga=s(Ib,"SPAN",{});var Mb=i(ga);m(ko.$$.fragment,Mb),Mb.forEach(o),Ib.forEach(o),Uh=h(Bd),Ca=s(Bd,"SPAN",{});var Sb=i(Ca);Wh=a(Sb,"How to obtain the same behavior as v3.x in v4.x"),Sb.forEach(o),Bd.forEach(o),mn=h(t),nt=s(t,"P",{});var Hd=i(nt);Xh=a(Hd,"In order to obtain the same behavior as version "),Oa=s(Hd,"CODE",{});var jb=i(Oa);Kh=a(jb,"v3.x"),jb.forEach(o),Jh=a(Hd,", you should update the path used to access the layers."),Hd.forEach(o),un=h(t),dt=s(t,"P",{});var Nd=i(dt);Qh=a(Nd,"In version "),Da=s(Nd,"CODE",{});var Fb=i(Da);Vh=a(Fb,"v3.x"),Fb.forEach(o),Yh=a(Nd,":"),Nd.forEach(o),vn=h(t),m($o.$$.fragment,t),_n=h(t),ct=s(t,"P",{});var Rd=i(ct);Zh=a(Rd,"to obtain the same in version "),xa=s(Rd,"CODE",{});var qb=i(xa);ef=a(qb,"v4.x"),qb.forEach(o),tf=a(Rd,":"),Rd.forEach(o),En=h(t),m(To.$$.fragment,t),bn=h(t),Te=s(t,"H4",{class:!0});var Gd=i(Te);ht=s(Gd,"A",{id:!0,class:!0,href:!0});var Bb=i(ht);Pa=s(Bb,"SPAN",{});var Hb=i(Pa);m(go.$$.fragment,Hb),Hb.forEach(o),Bb.forEach(o),of=h(Gd),ge=s(Gd,"SPAN",{});var _r=i(ge);rf=a(_r,"4. Switching the "),Aa=s(_r,"CODE",{});var Nb=i(Aa);af=a(Nb,"return_dict"),Nb.forEach(o),lf=a(_r," argument to "),za=s(_r,"CODE",{});var Rb=i(za);sf=a(Rb,"True"),Rb.forEach(o),nf=a(_r," by default"),_r.forEach(o),Gd.forEach(o),wn=h(t),ft=s(t,"P",{});var Ud=i(ft);df=a(Ud,"The "),Co=s(Ud,"A",{href:!0});var CE=i(Co);La=s(CE,"CODE",{});var Gb=i(La);cf=a(Gb,"return_dict"),Gb.forEach(o),hf=a(CE," argument"),CE.forEach(o),ff=a(Ud," enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),Ud.forEach(o),yn=h(t),pt=s(t,"P",{});var Wd=i(pt);pf=a(Wd,"This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Ia=s(Wd,"CODE",{});var Ub=i(Ia);mf=a(Ub,"value0, value1 = outputs"),Ub.forEach(o),uf=a(Wd," will not work."),Wd.forEach(o),kn=h(t),Ce=s(t,"H5",{class:!0});var Xd=i(Ce);mt=s(Xd,"A",{id:!0,class:!0,href:!0});var Wb=i(mt);Ma=s(Wb,"SPAN",{});var Xb=i(Ma);m(Oo.$$.fragment,Xb),Xb.forEach(o),Wb.forEach(o),vf=h(Xd),Sa=s(Xd,"SPAN",{});var Kb=i(Sa);_f=a(Kb,"How to obtain the same behavior as v3.x in v4.x"),Kb.forEach(o),Xd.forEach(o),$n=h(t),A=s(t,"P",{});var Gt=i(A);Ef=a(Gt,"In order to obtain the same behavior as version "),ja=s(Gt,"CODE",{});var Jb=i(ja);bf=a(Jb,"v3.x"),Jb.forEach(o),wf=a(Gt,", you should specify the "),Fa=s(Gt,"CODE",{});var Qb=i(Fa);yf=a(Qb,"return_dict"),Qb.forEach(o),kf=a(Gt," argument to "),qa=s(Gt,"CODE",{});var Vb=i(qa);$f=a(Vb,"False"),Vb.forEach(o),Tf=a(Gt,", either in the model configuration or during the forward pass."),Gt.forEach(o),Tn=h(t),ut=s(t,"P",{});var Kd=i(ut);gf=a(Kd,"In version "),Ba=s(Kd,"CODE",{});var Yb=i(Ba);Cf=a(Yb,"v3.x"),Yb.forEach(o),Of=a(Kd,":"),Kd.forEach(o),gn=h(t),m(Do.$$.fragment,t),Cn=h(t),vt=s(t,"P",{});var Jd=i(vt);Df=a(Jd,"to obtain the same in version "),Ha=s(Jd,"CODE",{});var Zb=i(Ha);xf=a(Zb,"v4.x"),Zb.forEach(o),Pf=a(Jd,":"),Jd.forEach(o),On=h(t),m(xo.$$.fragment,t),Dn=h(t),ar=s(t,"P",{});var e3=i(ar);Af=a(e3,"or"),e3.forEach(o),xn=h(t),m(Po.$$.fragment,t),Pn=h(t),Oe=s(t,"H4",{class:!0});var Qd=i(Oe);_t=s(Qd,"A",{id:!0,class:!0,href:!0});var t3=i(_t);Na=s(t3,"SPAN",{});var o3=i(Na);m(Ao.$$.fragment,o3),o3.forEach(o),t3.forEach(o),zf=h(Qd),Ra=s(Qd,"SPAN",{});var r3=i(Ra);Lf=a(r3,"5. Removed some deprecated attributes"),r3.forEach(o),Qd.forEach(o),An=h(t),Et=s(t,"P",{});var Vd=i(Et);If=a(Vd,"Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),zo=s(Vd,"A",{href:!0,rel:!0});var a3=i(zo);Mf=a(a3,"#8604"),a3.forEach(o),Sf=a(Vd,"."),Vd.forEach(o),zn=h(t),lr=s(t,"P",{});var l3=i(lr);jf=a(l3,"Here is a list of these attributes/methods/arguments and what their replacements should be:"),l3.forEach(o),Ln=h(t),sr=s(t,"P",{});var s3=i(sr);Ff=a(s3,"In several models, the labels become consistent with the other models:"),s3.forEach(o),In=h(t),b=s(t,"UL",{});var y=i(b);z=s(y,"LI",{});var Ne=i(z);Ga=s(Ne,"CODE",{});var i3=i(Ga);qf=a(i3,"masked_lm_labels"),i3.forEach(o),Bf=a(Ne," becomes "),Ua=s(Ne,"CODE",{});var n3=i(Ua);Hf=a(n3,"labels"),n3.forEach(o),Nf=a(Ne," in "),Wa=s(Ne,"CODE",{});var d3=i(Wa);Rf=a(d3,"AlbertForMaskedLM"),d3.forEach(o),Gf=a(Ne," and "),Xa=s(Ne,"CODE",{});var c3=i(Xa);Uf=a(c3,"AlbertForPreTraining"),c3.forEach(o),Wf=a(Ne,"."),Ne.forEach(o),Xf=h(y),L=s(y,"LI",{});var Re=i(L);Ka=s(Re,"CODE",{});var h3=i(Ka);Kf=a(h3,"masked_lm_labels"),h3.forEach(o),Jf=a(Re," becomes "),Ja=s(Re,"CODE",{});var f3=i(Ja);Qf=a(f3,"labels"),f3.forEach(o),Vf=a(Re," in "),Qa=s(Re,"CODE",{});var p3=i(Qa);Yf=a(p3,"BertForMaskedLM"),p3.forEach(o),Zf=a(Re," and "),Va=s(Re,"CODE",{});var m3=i(Va);ep=a(m3,"BertForPreTraining"),m3.forEach(o),tp=a(Re,"."),Re.forEach(o),op=h(y),Y=s(y,"LI",{});var Go=i(Y);Ya=s(Go,"CODE",{});var u3=i(Ya);rp=a(u3,"masked_lm_labels"),u3.forEach(o),ap=a(Go," becomes "),Za=s(Go,"CODE",{});var v3=i(Za);lp=a(v3,"labels"),v3.forEach(o),sp=a(Go," in "),el=s(Go,"CODE",{});var _3=i(el);ip=a(_3,"DistilBertForMaskedLM"),_3.forEach(o),np=a(Go,"."),Go.forEach(o),dp=h(y),Z=s(y,"LI",{});var Uo=i(Z);tl=s(Uo,"CODE",{});var E3=i(tl);cp=a(E3,"masked_lm_labels"),E3.forEach(o),hp=a(Uo," becomes "),ol=s(Uo,"CODE",{});var b3=i(ol);fp=a(b3,"labels"),b3.forEach(o),pp=a(Uo," in "),rl=s(Uo,"CODE",{});var w3=i(rl);mp=a(w3,"ElectraForMaskedLM"),w3.forEach(o),up=a(Uo,"."),Uo.forEach(o),vp=h(y),ee=s(y,"LI",{});var Wo=i(ee);al=s(Wo,"CODE",{});var y3=i(al);_p=a(y3,"masked_lm_labels"),y3.forEach(o),Ep=a(Wo," becomes "),ll=s(Wo,"CODE",{});var k3=i(ll);bp=a(k3,"labels"),k3.forEach(o),wp=a(Wo," in "),sl=s(Wo,"CODE",{});var $3=i(sl);yp=a($3,"LongformerForMaskedLM"),$3.forEach(o),kp=a(Wo,"."),Wo.forEach(o),$p=h(y),te=s(y,"LI",{});var Xo=i(te);il=s(Xo,"CODE",{});var T3=i(il);Tp=a(T3,"masked_lm_labels"),T3.forEach(o),gp=a(Xo," becomes "),nl=s(Xo,"CODE",{});var g3=i(nl);Cp=a(g3,"labels"),g3.forEach(o),Op=a(Xo," in "),dl=s(Xo,"CODE",{});var C3=i(dl);Dp=a(C3,"MobileBertForMaskedLM"),C3.forEach(o),xp=a(Xo,"."),Xo.forEach(o),Pp=h(y),oe=s(y,"LI",{});var Ko=i(oe);cl=s(Ko,"CODE",{});var O3=i(cl);Ap=a(O3,"masked_lm_labels"),O3.forEach(o),zp=a(Ko," becomes "),hl=s(Ko,"CODE",{});var D3=i(hl);Lp=a(D3,"labels"),D3.forEach(o),Ip=a(Ko," in "),fl=s(Ko,"CODE",{});var x3=i(fl);Mp=a(x3,"RobertaForMaskedLM"),x3.forEach(o),Sp=a(Ko,"."),Ko.forEach(o),jp=h(y),re=s(y,"LI",{});var Jo=i(re);pl=s(Jo,"CODE",{});var P3=i(pl);Fp=a(P3,"lm_labels"),P3.forEach(o),qp=a(Jo," becomes "),ml=s(Jo,"CODE",{});var A3=i(ml);Bp=a(A3,"labels"),A3.forEach(o),Hp=a(Jo," in "),ul=s(Jo,"CODE",{});var z3=i(ul);Np=a(z3,"BartForConditionalGeneration"),z3.forEach(o),Rp=a(Jo,"."),Jo.forEach(o),Gp=h(y),ae=s(y,"LI",{});var Qo=i(ae);vl=s(Qo,"CODE",{});var L3=i(vl);Up=a(L3,"lm_labels"),L3.forEach(o),Wp=a(Qo," becomes "),_l=s(Qo,"CODE",{});var I3=i(_l);Xp=a(I3,"labels"),I3.forEach(o),Kp=a(Qo," in "),El=s(Qo,"CODE",{});var M3=i(El);Jp=a(M3,"GPT2DoubleHeadsModel"),M3.forEach(o),Qp=a(Qo,"."),Qo.forEach(o),Vp=h(y),le=s(y,"LI",{});var Vo=i(le);bl=s(Vo,"CODE",{});var S3=i(bl);Yp=a(S3,"lm_labels"),S3.forEach(o),Zp=a(Vo," becomes "),wl=s(Vo,"CODE",{});var j3=i(wl);em=a(j3,"labels"),j3.forEach(o),tm=a(Vo," in "),yl=s(Vo,"CODE",{});var F3=i(yl);om=a(F3,"OpenAIGPTDoubleHeadsModel"),F3.forEach(o),rm=a(Vo,"."),Vo.forEach(o),am=h(y),se=s(y,"LI",{});var Yo=i(se);kl=s(Yo,"CODE",{});var q3=i(kl);lm=a(q3,"lm_labels"),q3.forEach(o),sm=a(Yo," becomes "),$l=s(Yo,"CODE",{});var B3=i($l);im=a(B3,"labels"),B3.forEach(o),nm=a(Yo," in "),Tl=s(Yo,"CODE",{});var H3=i(Tl);dm=a(H3,"T5ForConditionalGeneration"),H3.forEach(o),cm=a(Yo,"."),Yo.forEach(o),y.forEach(o),Mn=h(t),ir=s(t,"P",{});var N3=i(ir);hm=a(N3,"In several models, the caching mechanism becomes consistent with the other models:"),N3.forEach(o),Sn=h(t),I=s(t,"UL",{});var Ut=i(I);bt=s(Ut,"LI",{});var Ii=i(bt);gl=s(Ii,"CODE",{});var R3=i(gl);fm=a(R3,"decoder_cached_states"),R3.forEach(o),pm=a(Ii," becomes "),Cl=s(Ii,"CODE",{});var G3=i(Cl);mm=a(G3,"past_key_values"),G3.forEach(o),um=a(Ii," in all BART-like, FSMT and T5 models."),Ii.forEach(o),vm=h(Ut),wt=s(Ut,"LI",{});var Mi=i(wt);Ol=s(Mi,"CODE",{});var U3=i(Ol);_m=a(U3,"decoder_past_key_values"),U3.forEach(o),Em=a(Mi," becomes "),Dl=s(Mi,"CODE",{});var W3=i(Dl);bm=a(W3,"past_key_values"),W3.forEach(o),wm=a(Mi," in all BART-like, FSMT and T5 models."),Mi.forEach(o),ym=h(Ut),yt=s(Ut,"LI",{});var Si=i(yt);xl=s(Si,"CODE",{});var X3=i(xl);km=a(X3,"past"),X3.forEach(o),$m=a(Si," becomes "),Pl=s(Si,"CODE",{});var K3=i(Pl);Tm=a(K3,"past_key_values"),K3.forEach(o),gm=a(Si," in all CTRL models."),Si.forEach(o),Cm=h(Ut),kt=s(Ut,"LI",{});var ji=i(kt);Al=s(ji,"CODE",{});var J3=i(Al);Om=a(J3,"past"),J3.forEach(o),Dm=a(ji," becomes "),zl=s(ji,"CODE",{});var Q3=i(zl);xm=a(Q3,"past_key_values"),Q3.forEach(o),Pm=a(ji," in all GPT-2 models."),ji.forEach(o),Ut.forEach(o),jn=h(t),nr=s(t,"P",{});var V3=i(nr);Am=a(V3,"Regarding the tokenizer classes:"),V3.forEach(o),Fn=h(t),ie=s(t,"UL",{});var Er=i(ie);De=s(Er,"LI",{});var br=i(De);zm=a(br,"The tokenizer attribute "),Ll=s(br,"CODE",{});var Y3=i(Ll);Lm=a(Y3,"max_len"),Y3.forEach(o),Im=a(br," becomes "),Il=s(br,"CODE",{});var Z3=i(Il);Mm=a(Z3,"model_max_length"),Z3.forEach(o),Sm=a(br,"."),br.forEach(o),jm=h(Er),xe=s(Er,"LI",{});var wr=i(xe);Fm=a(wr,"The tokenizer attribute "),Ml=s(wr,"CODE",{});var e4=i(Ml);qm=a(e4,"return_lengths"),e4.forEach(o),Bm=a(wr," becomes "),Sl=s(wr,"CODE",{});var t4=i(Sl);Hm=a(t4,"return_length"),t4.forEach(o),Nm=a(wr,"."),wr.forEach(o),Rm=h(Er),Pe=s(Er,"LI",{});var yr=i(Pe);Gm=a(yr,"The tokenizer encoding argument "),jl=s(yr,"CODE",{});var o4=i(jl);Um=a(o4,"is_pretokenized"),o4.forEach(o),Wm=a(yr," becomes "),Fl=s(yr,"CODE",{});var r4=i(Fl);Xm=a(r4,"is_split_into_words"),r4.forEach(o),Km=a(yr,"."),yr.forEach(o),Er.forEach(o),qn=h(t),$t=s(t,"P",{});var Yd=i($t);Jm=a(Yd,"Regarding the "),ql=s(Yd,"CODE",{});var a4=i(ql);Qm=a(a4,"Trainer"),a4.forEach(o),Vm=a(Yd," class:"),Yd.forEach(o),Bn=h(t),T=s(t,"UL",{});var O=i(T);j=s(O,"LI",{});var Wt=i(j);Ym=a(Wt,"The "),Bl=s(Wt,"CODE",{});var l4=i(Bl);Zm=a(l4,"Trainer"),l4.forEach(o),eu=a(Wt," argument "),Hl=s(Wt,"CODE",{});var s4=i(Hl);tu=a(s4,"tb_writer"),s4.forEach(o),ou=a(Wt," is removed in favor of the callback "),Nl=s(Wt,"CODE",{});var i4=i(Nl);ru=a(i4,"TensorBoardCallback(tb_writer=...)"),i4.forEach(o),au=a(Wt,"."),Wt.forEach(o),lu=h(O),F=s(O,"LI",{});var Xt=i(F);su=a(Xt,"The "),Rl=s(Xt,"CODE",{});var n4=i(Rl);iu=a(n4,"Trainer"),n4.forEach(o),nu=a(Xt," argument "),Gl=s(Xt,"CODE",{});var d4=i(Gl);du=a(d4,"prediction_loss_only"),d4.forEach(o),cu=a(Xt," is removed in favor of the class argument "),Ul=s(Xt,"CODE",{});var c4=i(Ul);hu=a(c4,"args.prediction_loss_only"),c4.forEach(o),fu=a(Xt,"."),Xt.forEach(o),pu=h(O),Ae=s(O,"LI",{});var kr=i(Ae);mu=a(kr,"The "),Wl=s(kr,"CODE",{});var h4=i(Wl);uu=a(h4,"Trainer"),h4.forEach(o),vu=a(kr," attribute "),Xl=s(kr,"CODE",{});var f4=i(Xl);_u=a(f4,"data_collator"),f4.forEach(o),Eu=a(kr," should be a callable."),kr.forEach(o),bu=h(O),q=s(O,"LI",{});var Kt=i(q);wu=a(Kt,"The "),Kl=s(Kt,"CODE",{});var p4=i(Kl);yu=a(p4,"Trainer"),p4.forEach(o),ku=a(Kt," method "),Jl=s(Kt,"CODE",{});var m4=i(Jl);$u=a(m4,"_log"),m4.forEach(o),Tu=a(Kt," is deprecated in favor of "),Ql=s(Kt,"CODE",{});var u4=i(Ql);gu=a(u4,"log"),u4.forEach(o),Cu=a(Kt,"."),Kt.forEach(o),Ou=h(O),B=s(O,"LI",{});var Jt=i(B);Du=a(Jt,"The "),Vl=s(Jt,"CODE",{});var v4=i(Vl);xu=a(v4,"Trainer"),v4.forEach(o),Pu=a(Jt," method "),Yl=s(Jt,"CODE",{});var _4=i(Yl);Au=a(_4,"_training_step"),_4.forEach(o),zu=a(Jt," is deprecated in favor of "),Zl=s(Jt,"CODE",{});var E4=i(Zl);Lu=a(E4,"training_step"),E4.forEach(o),Iu=a(Jt,"."),Jt.forEach(o),Mu=h(O),H=s(O,"LI",{});var Qt=i(H);Su=a(Qt,"The "),es=s(Qt,"CODE",{});var b4=i(es);ju=a(b4,"Trainer"),b4.forEach(o),Fu=a(Qt," method "),ts=s(Qt,"CODE",{});var w4=i(ts);qu=a(w4,"_prediction_loop"),w4.forEach(o),Bu=a(Qt," is deprecated in favor of "),os=s(Qt,"CODE",{});var y4=i(os);Hu=a(y4,"prediction_loop"),y4.forEach(o),Nu=a(Qt,"."),Qt.forEach(o),Ru=h(O),N=s(O,"LI",{});var Vt=i(N);Gu=a(Vt,"The "),rs=s(Vt,"CODE",{});var k4=i(rs);Uu=a(k4,"Trainer"),k4.forEach(o),Wu=a(Vt," method "),as=s(Vt,"CODE",{});var $4=i(as);Xu=a($4,"is_local_master"),$4.forEach(o),Ku=a(Vt," is deprecated in favor of "),ls=s(Vt,"CODE",{});var T4=i(ls);Ju=a(T4,"is_local_process_zero"),T4.forEach(o),Qu=a(Vt,"."),Vt.forEach(o),Vu=h(O),R=s(O,"LI",{});var Yt=i(R);Yu=a(Yt,"The "),ss=s(Yt,"CODE",{});var g4=i(ss);Zu=a(g4,"Trainer"),g4.forEach(o),ev=a(Yt," method "),is=s(Yt,"CODE",{});var C4=i(is);tv=a(C4,"is_world_master"),C4.forEach(o),ov=a(Yt," is deprecated in favor of "),ns=s(Yt,"CODE",{});var O4=i(ns);rv=a(O4,"is_world_process_zero"),O4.forEach(o),av=a(Yt,"."),Yt.forEach(o),O.forEach(o),Hn=h(t),Tt=s(t,"P",{});var Zd=i(Tt);lv=a(Zd,"Regarding the "),ds=s(Zd,"CODE",{});var D4=i(ds);sv=a(D4,"TFTrainer"),D4.forEach(o),iv=a(Zd," class:"),Zd.forEach(o),Nn=h(t),D=s(t,"UL",{});var pe=i(D);G=s(pe,"LI",{});var Zt=i(G);nv=a(Zt,"The "),cs=s(Zt,"CODE",{});var x4=i(cs);dv=a(x4,"TFTrainer"),x4.forEach(o),cv=a(Zt," argument "),hs=s(Zt,"CODE",{});var P4=i(hs);hv=a(P4,"prediction_loss_only"),P4.forEach(o),fv=a(Zt," is removed in favor of the class argument "),fs=s(Zt,"CODE",{});var A4=i(fs);pv=a(A4,"args.prediction_loss_only"),A4.forEach(o),mv=a(Zt,"."),Zt.forEach(o),uv=h(pe),U=s(pe,"LI",{});var eo=i(U);vv=a(eo,"The "),ps=s(eo,"CODE",{});var z4=i(ps);_v=a(z4,"Trainer"),z4.forEach(o),Ev=a(eo," method "),ms=s(eo,"CODE",{});var L4=i(ms);bv=a(L4,"_log"),L4.forEach(o),wv=a(eo," is deprecated in favor of "),us=s(eo,"CODE",{});var I4=i(us);yv=a(I4,"log"),I4.forEach(o),kv=a(eo,"."),eo.forEach(o),$v=h(pe),W=s(pe,"LI",{});var to=i(W);Tv=a(to,"The "),vs=s(to,"CODE",{});var M4=i(vs);gv=a(M4,"TFTrainer"),M4.forEach(o),Cv=a(to," method "),_s=s(to,"CODE",{});var S4=i(_s);Ov=a(S4,"_prediction_loop"),S4.forEach(o),Dv=a(to," is deprecated in favor of "),Es=s(to,"CODE",{});var j4=i(Es);xv=a(j4,"prediction_loop"),j4.forEach(o),Pv=a(to,"."),to.forEach(o),Av=h(pe),X=s(pe,"LI",{});var oo=i(X);zv=a(oo,"The "),bs=s(oo,"CODE",{});var F4=i(bs);Lv=a(F4,"TFTrainer"),F4.forEach(o),Iv=a(oo," method "),ws=s(oo,"CODE",{});var q4=i(ws);Mv=a(q4,"_setup_wandb"),q4.forEach(o),Sv=a(oo," is deprecated in favor of "),ys=s(oo,"CODE",{});var B4=i(ys);jv=a(B4,"setup_wandb"),B4.forEach(o),Fv=a(oo,"."),oo.forEach(o),qv=h(pe),K=s(pe,"LI",{});var ro=i(K);Bv=a(ro,"The "),ks=s(ro,"CODE",{});var H4=i(ks);Hv=a(H4,"TFTrainer"),H4.forEach(o),Nv=a(ro," method "),$s=s(ro,"CODE",{});var N4=i($s);Rv=a(N4,"_run_model"),N4.forEach(o),Gv=a(ro," is deprecated in favor of "),Ts=s(ro,"CODE",{});var R4=i(Ts);Uv=a(R4,"run_model"),R4.forEach(o),Wv=a(ro,"."),ro.forEach(o),pe.forEach(o),Rn=h(t),gt=s(t,"P",{});var ec=i(gt);Xv=a(ec,"Regarding the "),gs=s(ec,"CODE",{});var G4=i(gs);Kv=a(G4,"TrainingArguments"),G4.forEach(o),Jv=a(ec," class:"),ec.forEach(o),Gn=h(t),dr=s(t,"UL",{});var U4=i(dr);J=s(U4,"LI",{});var ao=i(J);Qv=a(ao,"The "),Cs=s(ao,"CODE",{});var W4=i(Cs);Vv=a(W4,"TrainingArguments"),W4.forEach(o),Yv=a(ao," argument "),Os=s(ao,"CODE",{});var X4=i(Os);Zv=a(X4,"evaluate_during_training"),X4.forEach(o),e_=a(ao," is deprecated in favor of "),Ds=s(ao,"CODE",{});var K4=i(Ds);t_=a(K4,"evaluation_strategy"),K4.forEach(o),o_=a(ao,"."),ao.forEach(o),U4.forEach(o),Un=h(t),cr=s(t,"P",{});var J4=i(cr);r_=a(J4,"Regarding the Transfo-XL model:"),J4.forEach(o),Wn=h(t),Ct=s(t,"UL",{});var tc=i(Ct);ze=s(tc,"LI",{});var $r=i(ze);a_=a($r,"The Transfo-XL configuration attribute "),xs=s($r,"CODE",{});var Q4=i(xs);l_=a(Q4,"tie_weight"),Q4.forEach(o),s_=a($r," becomes "),Ps=s($r,"CODE",{});var V4=i(Ps);i_=a(V4,"tie_words_embeddings"),V4.forEach(o),n_=a($r,"."),$r.forEach(o),d_=h(tc),Le=s(tc,"LI",{});var Tr=i(Le);c_=a(Tr,"The Transfo-XL modeling method "),As=s(Tr,"CODE",{});var Y4=i(As);h_=a(Y4,"reset_length"),Y4.forEach(o),f_=a(Tr," becomes "),zs=s(Tr,"CODE",{});var Z4=i(zs);p_=a(Z4,"reset_memory_length"),Z4.forEach(o),m_=a(Tr,"."),Tr.forEach(o),tc.forEach(o),Xn=h(t),hr=s(t,"P",{});var e2=i(hr);u_=a(e2,"Regarding pipelines:"),e2.forEach(o),Kn=h(t),fr=s(t,"UL",{});var t2=i(fr);Q=s(t2,"LI",{});var lo=i(Q);v_=a(lo,"The "),Ls=s(lo,"CODE",{});var o2=i(Ls);__=a(o2,"FillMaskPipeline"),o2.forEach(o),E_=a(lo," argument "),Is=s(lo,"CODE",{});var r2=i(Is);b_=a(r2,"topk"),r2.forEach(o),w_=a(lo," becomes "),Ms=s(lo,"CODE",{});var a2=i(Ms);y_=a(a2,"top_k"),a2.forEach(o),k_=a(lo,"."),lo.forEach(o),t2.forEach(o),Jn=h(t),Ie=s(t,"H2",{class:!0});var oc=i(Ie);Ot=s(oc,"A",{id:!0,class:!0,href:!0});var l2=i(Ot);Ss=s(l2,"SPAN",{});var s2=i(Ss);m(Lo.$$.fragment,s2),s2.forEach(o),l2.forEach(o),$_=h(oc),js=s(oc,"SPAN",{});var i2=i(js);T_=a(i2,"Migrating from pytorch-transformers to \u{1F917} Transformers"),i2.forEach(o),oc.forEach(o),Qn=h(t),Dt=s(t,"P",{});var rc=i(Dt);g_=a(rc,"Here is a quick summary of what you should take care of when migrating from "),Fs=s(rc,"CODE",{});var n2=i(Fs);C_=a(n2,"pytorch-transformers"),n2.forEach(o),O_=a(rc," to \u{1F917} Transformers."),rc.forEach(o),Vn=h(t),Me=s(t,"H3",{class:!0});var ac=i(Me);xt=s(ac,"A",{id:!0,class:!0,href:!0});var d2=i(xt);qs=s(d2,"SPAN",{});var c2=i(qs);m(Io.$$.fragment,c2),c2.forEach(o),d2.forEach(o),D_=h(ac),Se=s(ac,"SPAN",{});var gr=i(Se);x_=a(gr,"Positional order of some models' keywords inputs ("),Bs=s(gr,"CODE",{});var h2=i(Bs);P_=a(h2,"attention_mask"),h2.forEach(o),A_=a(gr,", "),Hs=s(gr,"CODE",{});var f2=i(Hs);z_=a(f2,"token_type_ids"),f2.forEach(o),L_=a(gr,"...) changed"),gr.forEach(o),ac.forEach(o),Yn=h(t),M=s(t,"P",{});var so=i(M);I_=a(so,"To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Ns=s(so,"STRONG",{});var p2=i(Ns);M_=a(p2,"keywords inputs"),p2.forEach(o),S_=a(so," ("),Rs=s(so,"CODE",{});var m2=i(Rs);j_=a(m2,"attention_mask"),m2.forEach(o),F_=a(so,", "),Gs=s(so,"CODE",{});var u2=i(Gs);q_=a(u2,"token_type_ids"),u2.forEach(o),B_=a(so,"\u2026) has been changed."),so.forEach(o),Zn=h(t),Pt=s(t,"P",{});var lc=i(Pt);H_=a(lc,"If you used to call the models with keyword names for keyword arguments, e.g. "),Us=s(lc,"CODE",{});var v2=i(Us);N_=a(v2,"model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),v2.forEach(o),R_=a(lc,", this should not cause any change."),lc.forEach(o),ed=h(t),At=s(t,"P",{});var sc=i(At);G_=a(sc,"If you used to call the models with positional inputs for keyword arguments, e.g. "),Ws=s(sc,"CODE",{});var _2=i(Ws);U_=a(_2,"model(inputs_ids, attention_mask, token_type_ids)"),_2.forEach(o),W_=a(sc,", you may have to double check the exact order of input arguments."),sc.forEach(o),td=h(t),je=s(t,"H2",{class:!0});var ic=i(je);zt=s(ic,"A",{id:!0,class:!0,href:!0});var E2=i(zt);Xs=s(E2,"SPAN",{});var b2=i(Xs);m(Mo.$$.fragment,b2),b2.forEach(o),E2.forEach(o),X_=h(ic),Ks=s(ic,"SPAN",{});var w2=i(Ks);K_=a(w2,"Migrating from pytorch-pretrained-bert"),w2.forEach(o),ic.forEach(o),od=h(t),Lt=s(t,"P",{});var nc=i(Lt);J_=a(nc,"Here is a quick summary of what you should take care of when migrating from "),Js=s(nc,"CODE",{});var y2=i(Js);Q_=a(y2,"pytorch-pretrained-bert"),y2.forEach(o),V_=a(nc," to \u{1F917} Transformers"),nc.forEach(o),rd=h(t),Fe=s(t,"H3",{class:!0});var dc=i(Fe);It=s(dc,"A",{id:!0,class:!0,href:!0});var k2=i(It);Qs=s(k2,"SPAN",{});var $2=i(Qs);m(So.$$.fragment,$2),$2.forEach(o),k2.forEach(o),Y_=h(dc),pr=s(dc,"SPAN",{});var OE=i(pr);Z_=a(OE,"Models always output "),Vs=s(OE,"CODE",{});var T2=i(Vs);e1=a(T2,"tuples"),T2.forEach(o),OE.forEach(o),dc.forEach(o),ad=h(t),ne=s(t,"P",{});var Cr=i(ne);t1=a(Cr,"The main breaking change when migrating from "),Ys=s(Cr,"CODE",{});var g2=i(Ys);o1=a(g2,"pytorch-pretrained-bert"),g2.forEach(o),r1=a(Cr," to \u{1F917} Transformers is that the models forward method always outputs a "),Zs=s(Cr,"CODE",{});var C2=i(Zs);a1=a(C2,"tuple"),C2.forEach(o),l1=a(Cr," with various elements depending on the model and the configuration parameters."),Cr.forEach(o),ld=h(t),Mt=s(t,"P",{});var cc=i(Mt);s1=a(cc,"The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=s(cc,"A",{href:!0,rel:!0});var O2=i(jo);i1=a(O2,"documentation"),O2.forEach(o),n1=a(cc,"."),cc.forEach(o),sd=h(t),St=s(t,"P",{});var hc=i(St);d1=a(hc,"In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ei=s(hc,"CODE",{});var D2=i(ei);c1=a(D2,"pytorch-pretrained-bert"),D2.forEach(o),h1=a(hc,"."),hc.forEach(o),id=h(t),de=s(t,"P",{});var Or=i(de);f1=a(Or,"Here is a "),ti=s(Or,"CODE",{});var x2=i(ti);p1=a(x2,"pytorch-pretrained-bert"),x2.forEach(o),m1=a(Or," to \u{1F917} Transformers conversion example for a "),oi=s(Or,"CODE",{});var P2=i(oi);u1=a(P2,"BertForSequenceClassification"),P2.forEach(o),v1=a(Or," classification model:"),Or.forEach(o),nd=h(t),m(Fo.$$.fragment,t),dd=h(t),qe=s(t,"H3",{class:!0});var fc=i(qe);jt=s(fc,"A",{id:!0,class:!0,href:!0});var A2=i(jt);ri=s(A2,"SPAN",{});var z2=i(ri);m(qo.$$.fragment,z2),z2.forEach(o),A2.forEach(o),_1=h(fc),ai=s(fc,"SPAN",{});var L2=i(ai);E1=a(L2,"Serialization"),L2.forEach(o),fc.forEach(o),cd=h(t),Ft=s(t,"P",{});var pc=i(Ft);b1=a(pc,"Breaking change in the "),li=s(pc,"CODE",{});var I2=i(li);w1=a(I2,"from_pretrained()"),I2.forEach(o),y1=a(pc,"method:"),pc.forEach(o),hd=h(t),qt=s(t,"OL",{});var mc=i(qt);si=s(mc,"LI",{});var M2=i(si);Be=s(M2,"P",{});var Dr=i(Be);k1=a(Dr,"Models are now set in evaluation mode by default when instantiated with the "),ii=s(Dr,"CODE",{});var S2=i(ii);$1=a(S2,"from_pretrained()"),S2.forEach(o),T1=a(Dr," method. To train them don\u2019t forget to set them back in training mode ("),ni=s(Dr,"CODE",{});var j2=i(ni);g1=a(j2,"model.train()"),j2.forEach(o),C1=a(Dr,") to activate the dropout modules."),Dr.forEach(o),M2.forEach(o),O1=h(mc),di=s(mc,"LI",{});var F2=i(di);w=s(F2,"P",{});var k=i(w);D1=a(k,"The additional "),ci=s(k,"CODE",{});var q2=i(ci);x1=a(q2,"*inputs"),q2.forEach(o),P1=a(k," and "),hi=s(k,"CODE",{});var B2=i(hi);A1=a(B2,"**kwargs"),B2.forEach(o),z1=a(k," arguments supplied to the "),fi=s(k,"CODE",{});var H2=i(fi);L1=a(H2,"from_pretrained()"),H2.forEach(o),I1=a(k," method used to be directly passed to the underlying model\u2019s class "),pi=s(k,"CODE",{});var N2=i(pi);M1=a(N2,"__init__()"),N2.forEach(o),S1=a(k," method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),mi=s(k,"CODE",{});var R2=i(mi);j1=a(R2,"BertForSequenceClassification"),R2.forEach(o),F1=a(k," examples. More precisely, the positional arguments "),ui=s(k,"CODE",{});var G2=i(ui);q1=a(G2,"*inputs"),G2.forEach(o),B1=a(k," provided to "),vi=s(k,"CODE",{});var U2=i(vi);H1=a(U2,"from_pretrained()"),U2.forEach(o),N1=a(k," are directly forwarded the model "),_i=s(k,"CODE",{});var W2=i(_i);R1=a(W2,"__init__()"),W2.forEach(o),G1=a(k," method while the keyword arguments "),Ei=s(k,"CODE",{});var X2=i(Ei);U1=a(X2,"**kwargs"),X2.forEach(o),W1=a(k," (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),bi=s(k,"CODE",{});var K2=i(bi);X1=a(K2,"__init__()"),K2.forEach(o),K1=a(k," method."),k.forEach(o),F2.forEach(o),mc.forEach(o),fd=h(t),Bt=s(t,"P",{});var uc=i(Bt);J1=a(uc,"Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),wi=s(uc,"CODE",{});var J2=i(wi);Q1=a(J2,"save_pretrained(save_directory)"),J2.forEach(o),V1=a(uc," if you were using any other serialization method before."),uc.forEach(o),pd=h(t),mr=s(t,"P",{});var Q2=i(mr);Y1=a(Q2,"Here is an example:"),Q2.forEach(o),md=h(t),m(Bo.$$.fragment,t),ud=h(t),He=s(t,"H3",{class:!0});var vc=i(He);Ht=s(vc,"A",{id:!0,class:!0,href:!0});var V2=i(Ht);yi=s(V2,"SPAN",{});var Y2=i(yi);m(Ho.$$.fragment,Y2),Y2.forEach(o),V2.forEach(o),Z1=h(vc),ki=s(vc,"SPAN",{});var Z2=i(ki);eE=a(Z2,"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),Z2.forEach(o),vc.forEach(o),vd=h(t),S=s(t,"P",{});var io=i(S);tE=a(io,"The two optimizers previously included, "),$i=s(io,"CODE",{});var e5=i($i);oE=a(e5,"BertAdam"),e5.forEach(o),rE=a(io," and "),Ti=s(io,"CODE",{});var t5=i(Ti);aE=a(t5,"OpenAIAdam"),t5.forEach(o),lE=a(io,", have been replaced by a single "),gi=s(io,"CODE",{});var o5=i(gi);sE=a(o5,"AdamW"),o5.forEach(o),iE=a(io," optimizer which has a few differences:"),io.forEach(o),_d=h(t),ce=s(t,"UL",{});var xr=i(ce);Ci=s(xr,"LI",{});var r5=i(Ci);nE=a(r5,"it only implements weights decay correction,"),r5.forEach(o),dE=h(xr),Oi=s(xr,"LI",{});var a5=i(Oi);cE=a(a5,"schedules are now externals (see below),"),a5.forEach(o),hE=h(xr),Di=s(xr,"LI",{});var l5=i(Di);fE=a(l5,"gradient clipping is now also external (see below)."),l5.forEach(o),xr.forEach(o),Ed=h(t),he=s(t,"P",{});var Pr=i(he);pE=a(Pr,"The new optimizer "),xi=s(Pr,"CODE",{});var s5=i(xi);mE=a(s5,"AdamW"),s5.forEach(o),uE=a(Pr," matches PyTorch "),Pi=s(Pr,"CODE",{});var i5=i(Pi);vE=a(i5,"Adam"),i5.forEach(o),_E=a(Pr," optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),Pr.forEach(o),bd=h(t),Nt=s(t,"P",{});var _c=i(Nt);EE=a(_c,"The schedules are now standard "),No=s(_c,"A",{href:!0,rel:!0});var n5=i(No);bE=a(n5,"PyTorch learning rate schedulers"),n5.forEach(o),wE=a(_c," and not part of the optimizer anymore."),_c.forEach(o),wd=h(t),fe=s(t,"P",{});var Ar=i(fe);yE=a(Ar,"Here is a conversion examples from "),Ai=s(Ar,"CODE",{});var d5=i(Ai);kE=a(d5,"BertAdam"),d5.forEach(o),$E=a(Ar," with a linear warmup and decay schedule to "),zi=s(Ar,"CODE",{});var c5=i(zi);TE=a(c5,"AdamW"),c5.forEach(o),gE=a(Ar," and the same schedule:"),Ar.forEach(o),yd=h(t),m(Ro.$$.fragment,t),this.h()},h(){f(me,"name","hf:doc:metadata"),f(me,"content",JSON.stringify(E5)),f(Ge,"id","migrating-from-previous-packages"),f(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ge,"href","#migrating-from-previous-packages"),f(ue,"class","relative group"),f(Ue,"id","migrating-from-transformers-v3x-to-v4x"),f(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ue,"href","#migrating-from-transformers-v3x-to-v4x"),f(ve,"class","relative group"),f(Xe,"id","1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xe,"href","#1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(_e,"class","relative group"),f(Je,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Je,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Ee,"class","relative group"),f(Ve,"href","main_classes/pipelines#transformers.TokenClassificationPipeline"),f(et,"id","2-sentencepiece-is-removed-from-the-required-dependencies"),f(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(et,"href","#2-sentencepiece-is-removed-from-the-required-dependencies"),f(we,"class","relative group"),f(ot,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ot,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ye,"class","relative group"),f(lt,"id","3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(lt,"href","#3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(ke,"class","relative group"),f(it,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(it,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f($e,"class","relative group"),f(ht,"id","4-switching-the-returndict-argument-to-true-by-default"),f(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ht,"href","#4-switching-the-returndict-argument-to-true-by-default"),f(Te,"class","relative group"),f(Co,"href","main_classes/output"),f(mt,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(mt,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Ce,"class","relative group"),f(_t,"id","5-removed-some-deprecated-attributes"),f(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_t,"href","#5-removed-some-deprecated-attributes"),f(Oe,"class","relative group"),f(zo,"href","https://github.com/huggingface/transformers/pull/8604"),f(zo,"rel","nofollow"),f(Ot,"id","migrating-from-pytorchtransformers-to-transformers"),f(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ot,"href","#migrating-from-pytorchtransformers-to-transformers"),f(Ie,"class","relative group"),f(xt,"id","positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(xt,"href","#positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(Me,"class","relative group"),f(zt,"id","migrating-from-pytorchpretrainedbert"),f(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(zt,"href","#migrating-from-pytorchpretrainedbert"),f(je,"class","relative group"),f(It,"id","models-always-output-tuples"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#models-always-output-tuples"),f(Fe,"class","relative group"),f(jo,"href","https://huggingface.co/transformers/"),f(jo,"rel","nofollow"),f(jt,"id","serialization"),f(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(jt,"href","#serialization"),f(qe,"class","relative group"),f(Ht,"id","optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ht,"href","#optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(He,"class","relative group"),f(No,"href","https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"),f(No,"rel","nofollow")},m(t,n){e(document.head,me),d(t,Fi,n),d(t,ue,n),e(ue,Ge),e(Ge,zr),u(no,zr,null),e(ue,Ec),e(ue,Lr),e(Lr,bc),d(t,qi,n),d(t,ve,n),e(ve,Ue),e(Ue,Ir),u(co,Ir,null),e(ve,wc),e(ve,We),e(We,yc),e(We,Mr),e(Mr,kc),e(We,$c),e(We,Sr),e(Sr,Tc),d(t,Bi,n),d(t,Zo,n),e(Zo,gc),d(t,Hi,n),d(t,_e,n),e(_e,Xe),e(Xe,jr),u(ho,jr,null),e(_e,Cc),e(_e,Fr),e(Fr,Oc),d(t,Ni,n),d(t,er,n),e(er,Dc),d(t,Ri,n),d(t,tr,n),e(tr,xc),d(t,Gi,n),d(t,Ke,n),e(Ke,qr),e(qr,Pc),e(Ke,Ac),e(Ke,Br),e(Br,zc),d(t,Ui,n),d(t,Ee,n),e(Ee,Je),e(Je,Hr),u(fo,Hr,null),e(Ee,Lc),e(Ee,Nr),e(Nr,Ic),d(t,Wi,n),d(t,Qe,n),e(Qe,po),e(po,Mc),e(po,Ve),e(Ve,Sc),e(Ve,Rr),e(Rr,jc),e(Ve,Fc),e(po,qc),e(Qe,Bc),e(Qe,be),e(be,Hc),e(be,Gr),e(Gr,Nc),e(be,Rc),e(be,Ur),e(Ur,Gc),e(be,Uc),d(t,Xi,n),d(t,Ye,n),e(Ye,Wc),e(Ye,Wr),e(Wr,Xc),e(Ye,Kc),d(t,Ki,n),u(mo,t,n),d(t,Ji,n),d(t,Ze,n),e(Ze,Jc),e(Ze,Xr),e(Xr,Qc),e(Ze,Vc),d(t,Qi,n),u(uo,t,n),d(t,Vi,n),d(t,we,n),e(we,et),e(et,Kr),u(vo,Kr,null),e(we,Yc),e(we,Jr),e(Jr,Zc),d(t,Yi,n),d(t,P,n),e(P,eh),e(P,Qr),e(Qr,th),e(P,oh),e(P,Vr),e(Vr,rh),e(P,ah),e(P,Yr),e(Yr,lh),e(P,sh),d(t,Zi,n),d(t,tt,n),e(tt,ih),e(tt,Zr),e(Zr,nh),e(tt,dh),d(t,en,n),d(t,$,n),e($,ea),e(ea,ta),e(ta,ch),e($,hh),e($,oa),e(oa,ra),e(ra,fh),e($,ph),e($,aa),e(aa,la),e(la,mh),e($,uh),e($,sa),e(sa,ia),e(ia,vh),e($,_h),e($,na),e(na,da),e(da,Eh),e($,bh),e($,ca),e(ca,ha),e(ha,wh),e($,yh),e($,fa),e(fa,pa),e(pa,kh),e($,$h),e($,ma),e(ma,ua),e(ua,Th),d(t,tn,n),d(t,ye,n),e(ye,ot),e(ot,va),u(_o,va,null),e(ye,gh),e(ye,_a),e(_a,Ch),d(t,on,n),d(t,V,n),e(V,Oh),e(V,Ea),e(Ea,Dh),e(V,xh),e(V,ba),e(ba,Ph),e(V,Ah),d(t,rn,n),d(t,rt,n),e(rt,zh),e(rt,wa),e(wa,Lh),e(rt,Ih),d(t,an,n),u(Eo,t,n),d(t,ln,n),d(t,at,n),e(at,Mh),e(at,ya),e(ya,Sh),e(at,jh),d(t,sn,n),u(bo,t,n),d(t,nn,n),d(t,or,n),e(or,Fh),d(t,dn,n),u(wo,t,n),d(t,cn,n),d(t,ke,n),e(ke,lt),e(lt,ka),u(yo,ka,null),e(ke,qh),e(ke,$a),e($a,Bh),d(t,hn,n),d(t,st,n),e(st,Hh),e(st,Ta),e(Ta,Nh),e(st,Rh),d(t,fn,n),d(t,rr,n),e(rr,Gh),d(t,pn,n),d(t,$e,n),e($e,it),e(it,ga),u(ko,ga,null),e($e,Uh),e($e,Ca),e(Ca,Wh),d(t,mn,n),d(t,nt,n),e(nt,Xh),e(nt,Oa),e(Oa,Kh),e(nt,Jh),d(t,un,n),d(t,dt,n),e(dt,Qh),e(dt,Da),e(Da,Vh),e(dt,Yh),d(t,vn,n),u($o,t,n),d(t,_n,n),d(t,ct,n),e(ct,Zh),e(ct,xa),e(xa,ef),e(ct,tf),d(t,En,n),u(To,t,n),d(t,bn,n),d(t,Te,n),e(Te,ht),e(ht,Pa),u(go,Pa,null),e(Te,of),e(Te,ge),e(ge,rf),e(ge,Aa),e(Aa,af),e(ge,lf),e(ge,za),e(za,sf),e(ge,nf),d(t,wn,n),d(t,ft,n),e(ft,df),e(ft,Co),e(Co,La),e(La,cf),e(Co,hf),e(ft,ff),d(t,yn,n),d(t,pt,n),e(pt,pf),e(pt,Ia),e(Ia,mf),e(pt,uf),d(t,kn,n),d(t,Ce,n),e(Ce,mt),e(mt,Ma),u(Oo,Ma,null),e(Ce,vf),e(Ce,Sa),e(Sa,_f),d(t,$n,n),d(t,A,n),e(A,Ef),e(A,ja),e(ja,bf),e(A,wf),e(A,Fa),e(Fa,yf),e(A,kf),e(A,qa),e(qa,$f),e(A,Tf),d(t,Tn,n),d(t,ut,n),e(ut,gf),e(ut,Ba),e(Ba,Cf),e(ut,Of),d(t,gn,n),u(Do,t,n),d(t,Cn,n),d(t,vt,n),e(vt,Df),e(vt,Ha),e(Ha,xf),e(vt,Pf),d(t,On,n),u(xo,t,n),d(t,Dn,n),d(t,ar,n),e(ar,Af),d(t,xn,n),u(Po,t,n),d(t,Pn,n),d(t,Oe,n),e(Oe,_t),e(_t,Na),u(Ao,Na,null),e(Oe,zf),e(Oe,Ra),e(Ra,Lf),d(t,An,n),d(t,Et,n),e(Et,If),e(Et,zo),e(zo,Mf),e(Et,Sf),d(t,zn,n),d(t,lr,n),e(lr,jf),d(t,Ln,n),d(t,sr,n),e(sr,Ff),d(t,In,n),d(t,b,n),e(b,z),e(z,Ga),e(Ga,qf),e(z,Bf),e(z,Ua),e(Ua,Hf),e(z,Nf),e(z,Wa),e(Wa,Rf),e(z,Gf),e(z,Xa),e(Xa,Uf),e(z,Wf),e(b,Xf),e(b,L),e(L,Ka),e(Ka,Kf),e(L,Jf),e(L,Ja),e(Ja,Qf),e(L,Vf),e(L,Qa),e(Qa,Yf),e(L,Zf),e(L,Va),e(Va,ep),e(L,tp),e(b,op),e(b,Y),e(Y,Ya),e(Ya,rp),e(Y,ap),e(Y,Za),e(Za,lp),e(Y,sp),e(Y,el),e(el,ip),e(Y,np),e(b,dp),e(b,Z),e(Z,tl),e(tl,cp),e(Z,hp),e(Z,ol),e(ol,fp),e(Z,pp),e(Z,rl),e(rl,mp),e(Z,up),e(b,vp),e(b,ee),e(ee,al),e(al,_p),e(ee,Ep),e(ee,ll),e(ll,bp),e(ee,wp),e(ee,sl),e(sl,yp),e(ee,kp),e(b,$p),e(b,te),e(te,il),e(il,Tp),e(te,gp),e(te,nl),e(nl,Cp),e(te,Op),e(te,dl),e(dl,Dp),e(te,xp),e(b,Pp),e(b,oe),e(oe,cl),e(cl,Ap),e(oe,zp),e(oe,hl),e(hl,Lp),e(oe,Ip),e(oe,fl),e(fl,Mp),e(oe,Sp),e(b,jp),e(b,re),e(re,pl),e(pl,Fp),e(re,qp),e(re,ml),e(ml,Bp),e(re,Hp),e(re,ul),e(ul,Np),e(re,Rp),e(b,Gp),e(b,ae),e(ae,vl),e(vl,Up),e(ae,Wp),e(ae,_l),e(_l,Xp),e(ae,Kp),e(ae,El),e(El,Jp),e(ae,Qp),e(b,Vp),e(b,le),e(le,bl),e(bl,Yp),e(le,Zp),e(le,wl),e(wl,em),e(le,tm),e(le,yl),e(yl,om),e(le,rm),e(b,am),e(b,se),e(se,kl),e(kl,lm),e(se,sm),e(se,$l),e($l,im),e(se,nm),e(se,Tl),e(Tl,dm),e(se,cm),d(t,Mn,n),d(t,ir,n),e(ir,hm),d(t,Sn,n),d(t,I,n),e(I,bt),e(bt,gl),e(gl,fm),e(bt,pm),e(bt,Cl),e(Cl,mm),e(bt,um),e(I,vm),e(I,wt),e(wt,Ol),e(Ol,_m),e(wt,Em),e(wt,Dl),e(Dl,bm),e(wt,wm),e(I,ym),e(I,yt),e(yt,xl),e(xl,km),e(yt,$m),e(yt,Pl),e(Pl,Tm),e(yt,gm),e(I,Cm),e(I,kt),e(kt,Al),e(Al,Om),e(kt,Dm),e(kt,zl),e(zl,xm),e(kt,Pm),d(t,jn,n),d(t,nr,n),e(nr,Am),d(t,Fn,n),d(t,ie,n),e(ie,De),e(De,zm),e(De,Ll),e(Ll,Lm),e(De,Im),e(De,Il),e(Il,Mm),e(De,Sm),e(ie,jm),e(ie,xe),e(xe,Fm),e(xe,Ml),e(Ml,qm),e(xe,Bm),e(xe,Sl),e(Sl,Hm),e(xe,Nm),e(ie,Rm),e(ie,Pe),e(Pe,Gm),e(Pe,jl),e(jl,Um),e(Pe,Wm),e(Pe,Fl),e(Fl,Xm),e(Pe,Km),d(t,qn,n),d(t,$t,n),e($t,Jm),e($t,ql),e(ql,Qm),e($t,Vm),d(t,Bn,n),d(t,T,n),e(T,j),e(j,Ym),e(j,Bl),e(Bl,Zm),e(j,eu),e(j,Hl),e(Hl,tu),e(j,ou),e(j,Nl),e(Nl,ru),e(j,au),e(T,lu),e(T,F),e(F,su),e(F,Rl),e(Rl,iu),e(F,nu),e(F,Gl),e(Gl,du),e(F,cu),e(F,Ul),e(Ul,hu),e(F,fu),e(T,pu),e(T,Ae),e(Ae,mu),e(Ae,Wl),e(Wl,uu),e(Ae,vu),e(Ae,Xl),e(Xl,_u),e(Ae,Eu),e(T,bu),e(T,q),e(q,wu),e(q,Kl),e(Kl,yu),e(q,ku),e(q,Jl),e(Jl,$u),e(q,Tu),e(q,Ql),e(Ql,gu),e(q,Cu),e(T,Ou),e(T,B),e(B,Du),e(B,Vl),e(Vl,xu),e(B,Pu),e(B,Yl),e(Yl,Au),e(B,zu),e(B,Zl),e(Zl,Lu),e(B,Iu),e(T,Mu),e(T,H),e(H,Su),e(H,es),e(es,ju),e(H,Fu),e(H,ts),e(ts,qu),e(H,Bu),e(H,os),e(os,Hu),e(H,Nu),e(T,Ru),e(T,N),e(N,Gu),e(N,rs),e(rs,Uu),e(N,Wu),e(N,as),e(as,Xu),e(N,Ku),e(N,ls),e(ls,Ju),e(N,Qu),e(T,Vu),e(T,R),e(R,Yu),e(R,ss),e(ss,Zu),e(R,ev),e(R,is),e(is,tv),e(R,ov),e(R,ns),e(ns,rv),e(R,av),d(t,Hn,n),d(t,Tt,n),e(Tt,lv),e(Tt,ds),e(ds,sv),e(Tt,iv),d(t,Nn,n),d(t,D,n),e(D,G),e(G,nv),e(G,cs),e(cs,dv),e(G,cv),e(G,hs),e(hs,hv),e(G,fv),e(G,fs),e(fs,pv),e(G,mv),e(D,uv),e(D,U),e(U,vv),e(U,ps),e(ps,_v),e(U,Ev),e(U,ms),e(ms,bv),e(U,wv),e(U,us),e(us,yv),e(U,kv),e(D,$v),e(D,W),e(W,Tv),e(W,vs),e(vs,gv),e(W,Cv),e(W,_s),e(_s,Ov),e(W,Dv),e(W,Es),e(Es,xv),e(W,Pv),e(D,Av),e(D,X),e(X,zv),e(X,bs),e(bs,Lv),e(X,Iv),e(X,ws),e(ws,Mv),e(X,Sv),e(X,ys),e(ys,jv),e(X,Fv),e(D,qv),e(D,K),e(K,Bv),e(K,ks),e(ks,Hv),e(K,Nv),e(K,$s),e($s,Rv),e(K,Gv),e(K,Ts),e(Ts,Uv),e(K,Wv),d(t,Rn,n),d(t,gt,n),e(gt,Xv),e(gt,gs),e(gs,Kv),e(gt,Jv),d(t,Gn,n),d(t,dr,n),e(dr,J),e(J,Qv),e(J,Cs),e(Cs,Vv),e(J,Yv),e(J,Os),e(Os,Zv),e(J,e_),e(J,Ds),e(Ds,t_),e(J,o_),d(t,Un,n),d(t,cr,n),e(cr,r_),d(t,Wn,n),d(t,Ct,n),e(Ct,ze),e(ze,a_),e(ze,xs),e(xs,l_),e(ze,s_),e(ze,Ps),e(Ps,i_),e(ze,n_),e(Ct,d_),e(Ct,Le),e(Le,c_),e(Le,As),e(As,h_),e(Le,f_),e(Le,zs),e(zs,p_),e(Le,m_),d(t,Xn,n),d(t,hr,n),e(hr,u_),d(t,Kn,n),d(t,fr,n),e(fr,Q),e(Q,v_),e(Q,Ls),e(Ls,__),e(Q,E_),e(Q,Is),e(Is,b_),e(Q,w_),e(Q,Ms),e(Ms,y_),e(Q,k_),d(t,Jn,n),d(t,Ie,n),e(Ie,Ot),e(Ot,Ss),u(Lo,Ss,null),e(Ie,$_),e(Ie,js),e(js,T_),d(t,Qn,n),d(t,Dt,n),e(Dt,g_),e(Dt,Fs),e(Fs,C_),e(Dt,O_),d(t,Vn,n),d(t,Me,n),e(Me,xt),e(xt,qs),u(Io,qs,null),e(Me,D_),e(Me,Se),e(Se,x_),e(Se,Bs),e(Bs,P_),e(Se,A_),e(Se,Hs),e(Hs,z_),e(Se,L_),d(t,Yn,n),d(t,M,n),e(M,I_),e(M,Ns),e(Ns,M_),e(M,S_),e(M,Rs),e(Rs,j_),e(M,F_),e(M,Gs),e(Gs,q_),e(M,B_),d(t,Zn,n),d(t,Pt,n),e(Pt,H_),e(Pt,Us),e(Us,N_),e(Pt,R_),d(t,ed,n),d(t,At,n),e(At,G_),e(At,Ws),e(Ws,U_),e(At,W_),d(t,td,n),d(t,je,n),e(je,zt),e(zt,Xs),u(Mo,Xs,null),e(je,X_),e(je,Ks),e(Ks,K_),d(t,od,n),d(t,Lt,n),e(Lt,J_),e(Lt,Js),e(Js,Q_),e(Lt,V_),d(t,rd,n),d(t,Fe,n),e(Fe,It),e(It,Qs),u(So,Qs,null),e(Fe,Y_),e(Fe,pr),e(pr,Z_),e(pr,Vs),e(Vs,e1),d(t,ad,n),d(t,ne,n),e(ne,t1),e(ne,Ys),e(Ys,o1),e(ne,r1),e(ne,Zs),e(Zs,a1),e(ne,l1),d(t,ld,n),d(t,Mt,n),e(Mt,s1),e(Mt,jo),e(jo,i1),e(Mt,n1),d(t,sd,n),d(t,St,n),e(St,d1),e(St,ei),e(ei,c1),e(St,h1),d(t,id,n),d(t,de,n),e(de,f1),e(de,ti),e(ti,p1),e(de,m1),e(de,oi),e(oi,u1),e(de,v1),d(t,nd,n),u(Fo,t,n),d(t,dd,n),d(t,qe,n),e(qe,jt),e(jt,ri),u(qo,ri,null),e(qe,_1),e(qe,ai),e(ai,E1),d(t,cd,n),d(t,Ft,n),e(Ft,b1),e(Ft,li),e(li,w1),e(Ft,y1),d(t,hd,n),d(t,qt,n),e(qt,si),e(si,Be),e(Be,k1),e(Be,ii),e(ii,$1),e(Be,T1),e(Be,ni),e(ni,g1),e(Be,C1),e(qt,O1),e(qt,di),e(di,w),e(w,D1),e(w,ci),e(ci,x1),e(w,P1),e(w,hi),e(hi,A1),e(w,z1),e(w,fi),e(fi,L1),e(w,I1),e(w,pi),e(pi,M1),e(w,S1),e(w,mi),e(mi,j1),e(w,F1),e(w,ui),e(ui,q1),e(w,B1),e(w,vi),e(vi,H1),e(w,N1),e(w,_i),e(_i,R1),e(w,G1),e(w,Ei),e(Ei,U1),e(w,W1),e(w,bi),e(bi,X1),e(w,K1),d(t,fd,n),d(t,Bt,n),e(Bt,J1),e(Bt,wi),e(wi,Q1),e(Bt,V1),d(t,pd,n),d(t,mr,n),e(mr,Y1),d(t,md,n),u(Bo,t,n),d(t,ud,n),d(t,He,n),e(He,Ht),e(Ht,yi),u(Ho,yi,null),e(He,Z1),e(He,ki),e(ki,eE),d(t,vd,n),d(t,S,n),e(S,tE),e(S,$i),e($i,oE),e(S,rE),e(S,Ti),e(Ti,aE),e(S,lE),e(S,gi),e(gi,sE),e(S,iE),d(t,_d,n),d(t,ce,n),e(ce,Ci),e(Ci,nE),e(ce,dE),e(ce,Oi),e(Oi,cE),e(ce,hE),e(ce,Di),e(Di,fE),d(t,Ed,n),d(t,he,n),e(he,pE),e(he,xi),e(xi,mE),e(he,uE),e(he,Pi),e(Pi,vE),e(he,_E),d(t,bd,n),d(t,Nt,n),e(Nt,EE),e(Nt,No),e(No,bE),e(Nt,wE),d(t,wd,n),d(t,fe,n),e(fe,yE),e(fe,Ai),e(Ai,kE),e(fe,$E),e(fe,zi),e(zi,TE),e(fe,gE),d(t,yd,n),u(Ro,t,n),kd=!0},p:u5,i(t){kd||(v(no.$$.fragment,t),v(co.$$.fragment,t),v(ho.$$.fragment,t),v(fo.$$.fragment,t),v(mo.$$.fragment,t),v(uo.$$.fragment,t),v(vo.$$.fragment,t),v(_o.$$.fragment,t),v(Eo.$$.fragment,t),v(bo.$$.fragment,t),v(wo.$$.fragment,t),v(yo.$$.fragment,t),v(ko.$$.fragment,t),v($o.$$.fragment,t),v(To.$$.fragment,t),v(go.$$.fragment,t),v(Oo.$$.fragment,t),v(Do.$$.fragment,t),v(xo.$$.fragment,t),v(Po.$$.fragment,t),v(Ao.$$.fragment,t),v(Lo.$$.fragment,t),v(Io.$$.fragment,t),v(Mo.$$.fragment,t),v(So.$$.fragment,t),v(Fo.$$.fragment,t),v(qo.$$.fragment,t),v(Bo.$$.fragment,t),v(Ho.$$.fragment,t),v(Ro.$$.fragment,t),kd=!0)},o(t){_(no.$$.fragment,t),_(co.$$.fragment,t),_(ho.$$.fragment,t),_(fo.$$.fragment,t),_(mo.$$.fragment,t),_(uo.$$.fragment,t),_(vo.$$.fragment,t),_(_o.$$.fragment,t),_(Eo.$$.fragment,t),_(bo.$$.fragment,t),_(wo.$$.fragment,t),_(yo.$$.fragment,t),_(ko.$$.fragment,t),_($o.$$.fragment,t),_(To.$$.fragment,t),_(go.$$.fragment,t),_(Oo.$$.fragment,t),_(Do.$$.fragment,t),_(xo.$$.fragment,t),_(Po.$$.fragment,t),_(Ao.$$.fragment,t),_(Lo.$$.fragment,t),_(Io.$$.fragment,t),_(Mo.$$.fragment,t),_(So.$$.fragment,t),_(Fo.$$.fragment,t),_(qo.$$.fragment,t),_(Bo.$$.fragment,t),_(Ho.$$.fragment,t),_(Ro.$$.fragment,t),kd=!1},d(t){o(me),t&&o(Fi),t&&o(ue),E(no),t&&o(qi),t&&o(ve),E(co),t&&o(Bi),t&&o(Zo),t&&o(Hi),t&&o(_e),E(ho),t&&o(Ni),t&&o(er),t&&o(Ri),t&&o(tr),t&&o(Gi),t&&o(Ke),t&&o(Ui),t&&o(Ee),E(fo),t&&o(Wi),t&&o(Qe),t&&o(Xi),t&&o(Ye),t&&o(Ki),E(mo,t),t&&o(Ji),t&&o(Ze),t&&o(Qi),E(uo,t),t&&o(Vi),t&&o(we),E(vo),t&&o(Yi),t&&o(P),t&&o(Zi),t&&o(tt),t&&o(en),t&&o($),t&&o(tn),t&&o(ye),E(_o),t&&o(on),t&&o(V),t&&o(rn),t&&o(rt),t&&o(an),E(Eo,t),t&&o(ln),t&&o(at),t&&o(sn),E(bo,t),t&&o(nn),t&&o(or),t&&o(dn),E(wo,t),t&&o(cn),t&&o(ke),E(yo),t&&o(hn),t&&o(st),t&&o(fn),t&&o(rr),t&&o(pn),t&&o($e),E(ko),t&&o(mn),t&&o(nt),t&&o(un),t&&o(dt),t&&o(vn),E($o,t),t&&o(_n),t&&o(ct),t&&o(En),E(To,t),t&&o(bn),t&&o(Te),E(go),t&&o(wn),t&&o(ft),t&&o(yn),t&&o(pt),t&&o(kn),t&&o(Ce),E(Oo),t&&o($n),t&&o(A),t&&o(Tn),t&&o(ut),t&&o(gn),E(Do,t),t&&o(Cn),t&&o(vt),t&&o(On),E(xo,t),t&&o(Dn),t&&o(ar),t&&o(xn),E(Po,t),t&&o(Pn),t&&o(Oe),E(Ao),t&&o(An),t&&o(Et),t&&o(zn),t&&o(lr),t&&o(Ln),t&&o(sr),t&&o(In),t&&o(b),t&&o(Mn),t&&o(ir),t&&o(Sn),t&&o(I),t&&o(jn),t&&o(nr),t&&o(Fn),t&&o(ie),t&&o(qn),t&&o($t),t&&o(Bn),t&&o(T),t&&o(Hn),t&&o(Tt),t&&o(Nn),t&&o(D),t&&o(Rn),t&&o(gt),t&&o(Gn),t&&o(dr),t&&o(Un),t&&o(cr),t&&o(Wn),t&&o(Ct),t&&o(Xn),t&&o(hr),t&&o(Kn),t&&o(fr),t&&o(Jn),t&&o(Ie),E(Lo),t&&o(Qn),t&&o(Dt),t&&o(Vn),t&&o(Me),E(Io),t&&o(Yn),t&&o(M),t&&o(Zn),t&&o(Pt),t&&o(ed),t&&o(At),t&&o(td),t&&o(je),E(Mo),t&&o(od),t&&o(Lt),t&&o(rd),t&&o(Fe),E(So),t&&o(ad),t&&o(ne),t&&o(ld),t&&o(Mt),t&&o(sd),t&&o(St),t&&o(id),t&&o(de),t&&o(nd),E(Fo,t),t&&o(dd),t&&o(qe),E(qo),t&&o(cd),t&&o(Ft),t&&o(hd),t&&o(qt),t&&o(fd),t&&o(Bt),t&&o(pd),t&&o(mr),t&&o(md),E(Bo,t),t&&o(ud),t&&o(He),E(Ho),t&&o(vd),t&&o(S),t&&o(_d),t&&o(ce),t&&o(Ed),t&&o(he),t&&o(bd),t&&o(Nt),t&&o(wd),t&&o(fe),t&&o(yd),E(Ro,t)}}}const E5={local:"migrating-from-previous-packages",sections:[{local:"migrating-from-transformers-v3x-to-v4x",sections:[{local:"1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."},{local:"2-sentencepiece-is-removed-from-the-required-dependencies",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"2. SentencePiece is removed from the required dependencies"},{local:"3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"3. The architecture of the repo has been updated so that each model resides in its folder"},{local:"4-switching-the-returndict-argument-to-true-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"4. Switching the `return_dict` argument to `True` by default"},{local:"5-removed-some-deprecated-attributes",title:"5. Removed some deprecated attributes"}],title:"Migrating from transformers `v3.x` to `v4.x`"},{local:"migrating-from-pytorchtransformers-to-transformers",sections:[{local:"positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed",title:"Positional order of some models' keywords inputs (`attention_mask`, `token_type_ids`...) changed"}],title:"Migrating from pytorch-transformers to \u{1F917} Transformers"},{local:"migrating-from-pytorchpretrainedbert",sections:[{local:"models-always-output-tuples",title:"Models always output `tuples`"},{local:"serialization",title:"Serialization"},{local:"optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules",title:"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"}],title:"Migrating from pytorch-pretrained-bert"}],title:"Migrating from previous packages"};function b5(DE){return v5(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $5 extends h5{constructor(me){super();f5(this,me,b5,_5,p5,{})}}export{$5 as default,E5 as metadata};
