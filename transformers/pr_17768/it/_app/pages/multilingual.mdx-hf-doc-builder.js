import{S as Zs,i as er,s as tr,e as a,k as d,w as g,t as n,M as lr,c as o,d as l,m as p,a as i,x as c,h as s,b as u,G as t,g as m,y as f,L as ar,q as h,o as _,B as v,v as or}from"../chunks/vendor-hf-doc-builder.js";import{I as se}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as ir}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";function nr(yn){let q,gl,C,S,dt,re,ja,pt,qa,cl,me,fl,x,Ca,ut,Ia,Pa,de,Da,Ta,hl,I,B,gt,pe,Oa,ct,Aa,_l,qe,Xa,vl,P,N,ft,ue,Sa,ht,Ba,bl,Ce,Na,kl,b,Ie,_t,Ra,Fa,Ha,Pe,vt,Ga,Qa,Wa,De,bt,Ua,Ja,Ka,Te,kt,Va,Ya,Za,Oe,Et,eo,to,lo,Ae,$t,ao,oo,io,Xe,zt,no,so,El,$,ro,Mt,mo,po,wt,uo,go,yt,co,fo,$l,R,ho,xt,_o,vo,zl,ge,Ml,F,bo,Lt,ko,Eo,wl,ce,yl,Se,$o,xl,fe,Ll,z,zo,jt,Mo,wo,qt,yo,xo,Ct,Lo,jo,jl,he,ql,H,qo,It,Co,Io,Cl,_e,Il,L,Po,ve,Do,To,Pt,Oo,Ao,Pl,D,G,Dt,be,Xo,Tt,So,Dl,Be,Bo,Tl,Q,Ne,Ot,No,Ro,Fo,Re,At,Ho,Go,Ol,Fe,Qo,Al,T,W,Xt,ke,Wo,St,Uo,Xl,He,Jo,Sl,U,Ge,Bt,Ko,Vo,Yo,Qe,Nt,Zo,ei,Bl,We,ti,Nl,O,J,Rt,Ee,li,Ft,ai,Rl,Ue,oi,Fl,K,Je,Ht,ii,ni,si,Ke,Gt,ri,mi,Hl,Ve,di,Gl,A,V,Qt,$e,pi,Wt,ui,Ql,Ye,gi,Wl,Y,Ze,Ut,ci,fi,hi,et,Jt,_i,vi,Ul,Z,bi,Kt,ki,Ei,Jl,ze,Kl,tt,$i,Vl,Me,Yl,M,zi,Vt,Mi,wi,Yt,yi,xi,Zt,Li,ji,Zl,we,ea,X,ee,el,ye,qi,tl,Ci,ta,lt,Ii,la,k,at,ll,Pi,Di,Ti,ot,al,Oi,Ai,Xi,it,ol,Si,Bi,Ni,nt,il,Ri,Fi,Hi,nl,sl,Gi,aa,te,Qi,rl,Wi,Ui,oa,xe,ia,st,Ji,na,Le,sa,w,Ki,ml,Vi,Yi,dl,Zi,en,pl,tn,ln,ra,je,ma,le,an,ul,on,nn,da;return re=new se({}),me=new ir({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/multilingual.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/pytorch/multilingual.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/tensorflow/multilingual.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/multilingual.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/pytorch/multilingual.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/tensorflow/multilingual.ipynb"}]}}),pe=new se({}),ue=new se({}),ge=new y({props:{code:`import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("xlm-clm-enfr-1024")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMTokenizer, XLMWithLMHeadModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMTokenizer.from_pretrained(<span class="hljs-string">&quot;xlm-clm-enfr-1024&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMWithLMHeadModel.from_pretrained(<span class="hljs-string">&quot;xlm-clm-enfr-1024&quot;</span>)`}}),ce=new y({props:{code:"print(tokenizer.lang2id)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.lang2id)
{<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-number">1</span>}`}}),fe=new y({props:{code:'input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(<span class="hljs-string">&quot;Wikipedia was used to&quot;</span>)])  <span class="hljs-comment"># batch size of 1</span>'}}),he=new y({props:{code:`language_id = tokenizer.lang2id["en"]  # 0
langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

# We reshape it to be of size (batch_size, sequence_length)
langs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>language_id = tokenizer.lang2id[<span class="hljs-string">&quot;en&quot;</span>]  <span class="hljs-comment"># 0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>langs = torch.tensor([language_id] * input_ids.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># torch.tensor([0, 0, 0, ..., 0])</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We reshape it to be of size (batch_size, sequence_length)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>langs = langs.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># is now of shape [1, sequence_length] (we have a batch size of 1)</span>`}}),_e=new y({props:{code:"outputs = model(input_ids, langs=langs)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids, langs=langs)'}}),be=new se({}),ke=new se({}),Ee=new se({}),$e=new se({}),ze=new y({props:{code:`from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "\u4E0D\u8981\u63D2\u624B\u5DEB\u5E2B\u7684\u4E8B\u52D9, \u56E0\u70BA\u4ED6\u5011\u662F\u5FAE\u5999\u7684, \u5F88\u5FEB\u5C31\u6703\u767C\u6012."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> M2M100ForConditionalGeneration, M2M100Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>en_text = <span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>chinese_text = <span class="hljs-string">&quot;\u4E0D\u8981\u63D2\u624B\u5DEB\u5E2B\u7684\u4E8B\u52D9, \u56E0\u70BA\u4ED6\u5011\u662F\u5FAE\u5999\u7684, \u5F88\u5FEB\u5C31\u6703\u767C\u6012.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = M2M100Tokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/m2m100_418M&quot;</span>, src_lang=<span class="hljs-string">&quot;zh&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = M2M100ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/m2m100_418M&quot;</span>)`}}),Me=new y({props:{code:'encoded_zh = tokenizer(chinese_text, return_tensors="pt")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_zh = tokenizer(chinese_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'}}),we=new y({props:{code:`generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(<span class="hljs-string">&quot;en&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Do not interfere with the matters of the witches, because they are delicate and will soon be angry.&#x27;</span>`}}),ye=new se({}),xe=new y({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "\xC4l\xE4 sekaannu velhojen asioihin, sill\xE4 ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>en_text = <span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>fi_text = <span class="hljs-string">&quot;\xC4l\xE4 sekaannu velhojen asioihin, sill\xE4 ne ovat hienovaraisia ja nopeasti vihaisia.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/mbart-large-50-many-to-many-mmt&quot;</span>, src_lang=<span class="hljs-string">&quot;fi_FI&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/mbart-large-50-many-to-many-mmt&quot;</span>)`}}),Le=new y({props:{code:'encoded_en = tokenizer(en_text, return_tensors="pt")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_en = tokenizer(en_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'}}),je=new y({props:{code:`generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id(<span class="hljs-string">&quot;en_XX&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;Don&#x27;t interfere with the wizard&#x27;s affairs, because they are subtle, will soon get angry.&quot;</span>`}}),{c(){q=a("meta"),gl=d(),C=a("h1"),S=a("a"),dt=a("span"),g(re.$$.fragment),ja=d(),pt=a("span"),qa=n("Modelli multilingua per l'inferenza"),cl=d(),g(me.$$.fragment),fl=d(),x=a("p"),Ca=n("Ci sono diversi modelli multilingua in \u{1F917} Transformers, e il loro utilizzo per l\u2019inferenza differisce da quello dei modelli monolingua. Tuttavia non "),ut=a("em"),Ia=n("tutti"),Pa=n(" gli usi dei modelli multilingua sono diversi. Alcuni modelli, come "),de=a("a"),Da=n("bert-base-multilingual-uncased"),Ta=n(", possono essere usati come un modello monolingua. Questa guida ti mostrer\xE0 come utilizzare modelli multilingue che utilizzano un modo diverso per fare l\u2019inferenza."),hl=d(),I=a("h2"),B=a("a"),gt=a("span"),g(pe.$$.fragment),Oa=d(),ct=a("span"),Aa=n("XLM"),_l=d(),qe=a("p"),Xa=n("XLM ha dieci diversi checkpoint, di cui solo uno \xE8 monolingua. I nove checkpoint rimanenti possono essere suddivisi in due categorie: i checkpoint che utilizzano i language embeddings e quelli che non le utilizzano."),vl=d(),P=a("h3"),N=a("a"),ft=a("span"),g(ue.$$.fragment),Sa=d(),ht=a("span"),Ba=n("XLM con language embeddings"),bl=d(),Ce=a("p"),Na=n("I seguenti modelli XLM utilizzano gli embeddings linguistici per specificare la lingua utilizzata per l\u2019inferenza:"),kl=d(),b=a("ul"),Ie=a("li"),_t=a("code"),Ra=n("xlm-mlm-ende-1024"),Fa=n(" (Masked language modeling, English-German)"),Ha=d(),Pe=a("li"),vt=a("code"),Ga=n("xlm-mlm-enfr-1024"),Qa=n(" (Masked language modeling, English-French)"),Wa=d(),De=a("li"),bt=a("code"),Ua=n("xlm-mlm-enro-1024"),Ja=n(" (Masked language modeling, English-Romanian)"),Ka=d(),Te=a("li"),kt=a("code"),Va=n("xlm-mlm-xnli15-1024"),Ya=n(" (Masked language modeling, XNLI languages)"),Za=d(),Oe=a("li"),Et=a("code"),eo=n("xlm-mlm-tlm-xnli15-1024"),to=n(" (Masked language modeling + translation, XNLI languages)"),lo=d(),Ae=a("li"),$t=a("code"),ao=n("xlm-clm-enfr-1024"),oo=n(" (Causal language modeling, English-French)"),io=d(),Xe=a("li"),zt=a("code"),no=n("xlm-clm-ende-1024"),so=n(" (Causal language modeling, English-German)"),El=d(),$=a("p"),ro=n("Gli embeddings linguistici sono rappresentati come un tensore delle stesse dimensioni del "),Mt=a("code"),mo=n("input_ids"),po=n(" passato al modello. I valori in questi tensori dipendono dal linguaggio usato e sono identificati dagli attributi "),wt=a("code"),uo=n("lang2id"),go=n(" e "),yt=a("code"),co=n("id2lang"),fo=n(" del tokenizer."),$l=d(),R=a("p"),ho=n("In questo esempio, carica il checkpoint "),xt=a("code"),_o=n("xlm-clm-enfr-1024"),vo=n(" (Causal language modeling, English-French):"),zl=d(),g(ge.$$.fragment),Ml=d(),F=a("p"),bo=n("L\u2019attributo "),Lt=a("code"),ko=n("lang2id"),Eo=n(" del tokenizer mostra il linguaggio del modello e il suo ids:"),wl=d(),g(ce.$$.fragment),yl=d(),Se=a("p"),$o=n("Poi, crea un esempio di input:"),xl=d(),g(fe.$$.fragment),Ll=d(),z=a("p"),zo=n("Imposta l\u2019id del linguaggio a "),jt=a("code"),Mo=n('"en"'),wo=n(" e usalo per definire il language embedding. Il language embedding \xE8 un tensore riempito con "),qt=a("code"),yo=n("0"),xo=n(" perch\xE8 questo \xE8 il language id per l\u2019inglese. Questo tensorre dovrebbe avere la stessa dimensione di "),Ct=a("code"),Lo=n("input_ids"),jo=n("."),jl=d(),g(he.$$.fragment),ql=d(),H=a("p"),qo=n("Adesso puoi inserire "),It=a("code"),Co=n("input_ids"),Io=n(" e language embedding nel modello:"),Cl=d(),g(_e.$$.fragment),Il=d(),L=a("p"),Po=n("Lo script "),ve=a("a"),Do=n("run_generation.py"),To=n(" pu\xF2 generare testo tramite i language embeddings usando i checkpoints "),Pt=a("code"),Oo=n("xlm-clm"),Ao=n("."),Pl=d(),D=a("h3"),G=a("a"),Dt=a("span"),g(be.$$.fragment),Xo=d(),Tt=a("span"),So=n("XLM senza language embeddings"),Dl=d(),Be=a("p"),Bo=n("I seguenti modelli XLM non richiedono l\u2019utilizzo dei language embeddings per fare inferenza:"),Tl=d(),Q=a("ul"),Ne=a("li"),Ot=a("code"),No=n("xlm-mlm-17-1280"),Ro=n(" (Masked language modeling, 17 languages)"),Fo=d(),Re=a("li"),At=a("code"),Ho=n("xlm-mlm-100-1280"),Go=n(" (Masked language modeling, 100 languages)"),Ol=d(),Fe=a("p"),Qo=n("Questi modelli sono utilizzati per rappresentazioni generiche di frasi, a differenza dei precedenti checkpoints XML."),Al=d(),T=a("h2"),W=a("a"),Xt=a("span"),g(ke.$$.fragment),Wo=d(),St=a("span"),Uo=n("BERT"),Xl=d(),He=a("p"),Jo=n("Il seguente modello BERT pu\xF2 essere usato per compiti multilingua:"),Sl=d(),U=a("ul"),Ge=a("li"),Bt=a("code"),Ko=n("bert-base-multilingual-uncased"),Vo=n(" (Masked language modeling + Next sentence prediction, 102 languages)"),Yo=d(),Qe=a("li"),Nt=a("code"),Zo=n("bert-base-multilingual-cased"),ei=n(" (Masked language modeling + Next sentence prediction, 104 languages)"),Bl=d(),We=a("p"),ti=n("Questi modelli non richiedono language embeddings per fare inferenza. Riescono ad identificare ul linguaggio dal contesto e inferire di conseguenza."),Nl=d(),O=a("h2"),J=a("a"),Rt=a("span"),g(Ee.$$.fragment),li=d(),Ft=a("span"),ai=n("XLM-RoBERTa"),Rl=d(),Ue=a("p"),oi=n("Il seguente modello XLM-RoBERTa pu\xF2 essere usato per compiti multilingua:"),Fl=d(),K=a("ul"),Je=a("li"),Ht=a("code"),ii=n("xlm-roberta-base"),ni=n(" (Masked language modeling, 100 languages)"),si=d(),Ke=a("li"),Gt=a("code"),ri=n("xlm-roberta-large"),mi=n(" (Masked language modeling, 100 languages)"),Hl=d(),Ve=a("p"),di=n("XLM-RoBERTa \xE8 stato addestrato su 2.5TB di dati CommonCrawl appena creati e puliti in 100 lingue. Offre notevoli vantaggi rispetto ai modelli multilingue rilasciati in precedenza, come mBERT o XLM, in compiti come la classificazione, l\u2019etichettatura delle sequenze e la risposta alle domande."),Gl=d(),A=a("h2"),V=a("a"),Qt=a("span"),g($e.$$.fragment),pi=d(),Wt=a("span"),ui=n("M2M100"),Ql=d(),Ye=a("p"),gi=n("Il seguente modello M2M100 pu\xF2 essere usato per compiti multilingua:"),Wl=d(),Y=a("ul"),Ze=a("li"),Ut=a("code"),ci=n("facebook/m2m100_418M"),fi=n(" (Translation)"),hi=d(),et=a("li"),Jt=a("code"),_i=n("facebook/m2m100_1.2B"),vi=n(" (Translation)"),Ul=d(),Z=a("p"),bi=n("In questo esempio, carica il checkpoint "),Kt=a("code"),ki=n("facebook/m2m100_418M"),Ei=n("  per tradurre dal cinese all\u2019inglese. Puoi impostare la lingua di partenza nel tokenizer:"),Jl=d(),g(ze.$$.fragment),Kl=d(),tt=a("p"),$i=n("Applica il tokenizer sul testo:"),Vl=d(),g(Me.$$.fragment),Yl=d(),M=a("p"),zi=n("M2M100 forza l\u2019id del target language come primo token generato per tradurre nel target language. Imposta il parametro "),Vt=a("code"),Mi=n("forced_bos_token_id"),wi=n(" a "),Yt=a("code"),yi=n("en"),xi=n(" nel metodo "),Zt=a("code"),Li=n("generate"),ji=n(" per tradurre in inglese:"),Zl=d(),g(we.$$.fragment),ea=d(),X=a("h2"),ee=a("a"),el=a("span"),g(ye.$$.fragment),qi=d(),tl=a("span"),Ci=n("MBart"),ta=d(),lt=a("p"),Ii=n("Il seguente modello MBart pu\xF2 essere usato per compiti multilingua:"),la=d(),k=a("ul"),at=a("li"),ll=a("code"),Pi=n("facebook/mbart-large-50-one-to-many-mmt"),Di=n(" (One-to-many multilingual machine translation, 50 languages)"),Ti=d(),ot=a("li"),al=a("code"),Oi=n("facebook/mbart-large-50-many-to-many-mmt"),Ai=n(" (Many-to-many multilingual machine translation, 50 languages)"),Xi=d(),it=a("li"),ol=a("code"),Si=n("facebook/mbart-large-50-many-to-one-mmt"),Bi=n(" (Many-to-one multilingual machine translation, 50 languages)"),Ni=d(),nt=a("li"),il=a("code"),Ri=n("facebook/mbart-large-50"),Fi=n(" (Multilingual translation, 50 languages)"),Hi=d(),nl=a("li"),sl=a("code"),Gi=n("facebook/mbart-large-cc25"),aa=d(),te=a("p"),Qi=n("In questo esempio, carica il checkpoint "),rl=a("code"),Wi=n("facebook/mbart-large-50-many-to-many-mmt"),Ui=n(" per tradurre dal finlandese all\u2019inglese. Puoi impostare la lingua di partenza nel tokenizer:"),oa=d(),g(xe.$$.fragment),ia=d(),st=a("p"),Ji=n("Applica il tokenizer sul testo:"),na=d(),g(Le.$$.fragment),sa=d(),w=a("p"),Ki=n("MBart fforza l\u2019id del target language come primo token generato per tradurre nel target language. Imposta il parametro "),ml=a("code"),Vi=n("forced_bos_token_id"),Yi=n(" a "),dl=a("code"),Zi=n("en"),en=n(" nel metodo "),pl=a("code"),tn=n("generate"),ln=n(" per tradurre in inglese:"),ra=d(),g(je.$$.fragment),ma=d(),le=a("p"),an=n("Se stai usando il checkpoint "),ul=a("code"),on=n("facebook/mbart-large-50-many-to-one-mmt"),nn=n(", non hai bisogno di forzare l\u2019id del target language come primo token generato altrimenti l\u2019uso \xE8 lo stesso."),this.h()},l(e){const r=lr('[data-svelte="svelte-1phssyn"]',document.head);q=o(r,"META",{name:!0,content:!0}),r.forEach(l),gl=p(e),C=o(e,"H1",{class:!0});var pa=i(C);S=o(pa,"A",{id:!0,class:!0,href:!0});var xn=i(S);dt=o(xn,"SPAN",{});var Ln=i(dt);c(re.$$.fragment,Ln),Ln.forEach(l),xn.forEach(l),ja=p(pa),pt=o(pa,"SPAN",{});var jn=i(pt);qa=s(jn,"Modelli multilingua per l'inferenza"),jn.forEach(l),pa.forEach(l),cl=p(e),c(me.$$.fragment,e),fl=p(e),x=o(e,"P",{});var rt=i(x);Ca=s(rt,"Ci sono diversi modelli multilingua in \u{1F917} Transformers, e il loro utilizzo per l\u2019inferenza differisce da quello dei modelli monolingua. Tuttavia non "),ut=o(rt,"EM",{});var qn=i(ut);Ia=s(qn,"tutti"),qn.forEach(l),Pa=s(rt," gli usi dei modelli multilingua sono diversi. Alcuni modelli, come "),de=o(rt,"A",{href:!0,rel:!0});var Cn=i(de);Da=s(Cn,"bert-base-multilingual-uncased"),Cn.forEach(l),Ta=s(rt,", possono essere usati come un modello monolingua. Questa guida ti mostrer\xE0 come utilizzare modelli multilingue che utilizzano un modo diverso per fare l\u2019inferenza."),rt.forEach(l),hl=p(e),I=o(e,"H2",{class:!0});var ua=i(I);B=o(ua,"A",{id:!0,class:!0,href:!0});var In=i(B);gt=o(In,"SPAN",{});var Pn=i(gt);c(pe.$$.fragment,Pn),Pn.forEach(l),In.forEach(l),Oa=p(ua),ct=o(ua,"SPAN",{});var Dn=i(ct);Aa=s(Dn,"XLM"),Dn.forEach(l),ua.forEach(l),_l=p(e),qe=o(e,"P",{});var Tn=i(qe);Xa=s(Tn,"XLM ha dieci diversi checkpoint, di cui solo uno \xE8 monolingua. I nove checkpoint rimanenti possono essere suddivisi in due categorie: i checkpoint che utilizzano i language embeddings e quelli che non le utilizzano."),Tn.forEach(l),vl=p(e),P=o(e,"H3",{class:!0});var ga=i(P);N=o(ga,"A",{id:!0,class:!0,href:!0});var On=i(N);ft=o(On,"SPAN",{});var An=i(ft);c(ue.$$.fragment,An),An.forEach(l),On.forEach(l),Sa=p(ga),ht=o(ga,"SPAN",{});var Xn=i(ht);Ba=s(Xn,"XLM con language embeddings"),Xn.forEach(l),ga.forEach(l),bl=p(e),Ce=o(e,"P",{});var Sn=i(Ce);Na=s(Sn,"I seguenti modelli XLM utilizzano gli embeddings linguistici per specificare la lingua utilizzata per l\u2019inferenza:"),Sn.forEach(l),kl=p(e),b=o(e,"UL",{});var E=i(b);Ie=o(E,"LI",{});var sn=i(Ie);_t=o(sn,"CODE",{});var Bn=i(_t);Ra=s(Bn,"xlm-mlm-ende-1024"),Bn.forEach(l),Fa=s(sn," (Masked language modeling, English-German)"),sn.forEach(l),Ha=p(E),Pe=o(E,"LI",{});var rn=i(Pe);vt=o(rn,"CODE",{});var Nn=i(vt);Ga=s(Nn,"xlm-mlm-enfr-1024"),Nn.forEach(l),Qa=s(rn," (Masked language modeling, English-French)"),rn.forEach(l),Wa=p(E),De=o(E,"LI",{});var mn=i(De);bt=o(mn,"CODE",{});var Rn=i(bt);Ua=s(Rn,"xlm-mlm-enro-1024"),Rn.forEach(l),Ja=s(mn," (Masked language modeling, English-Romanian)"),mn.forEach(l),Ka=p(E),Te=o(E,"LI",{});var dn=i(Te);kt=o(dn,"CODE",{});var Fn=i(kt);Va=s(Fn,"xlm-mlm-xnli15-1024"),Fn.forEach(l),Ya=s(dn," (Masked language modeling, XNLI languages)"),dn.forEach(l),Za=p(E),Oe=o(E,"LI",{});var pn=i(Oe);Et=o(pn,"CODE",{});var Hn=i(Et);eo=s(Hn,"xlm-mlm-tlm-xnli15-1024"),Hn.forEach(l),to=s(pn," (Masked language modeling + translation, XNLI languages)"),pn.forEach(l),lo=p(E),Ae=o(E,"LI",{});var un=i(Ae);$t=o(un,"CODE",{});var Gn=i($t);ao=s(Gn,"xlm-clm-enfr-1024"),Gn.forEach(l),oo=s(un," (Causal language modeling, English-French)"),un.forEach(l),io=p(E),Xe=o(E,"LI",{});var gn=i(Xe);zt=o(gn,"CODE",{});var Qn=i(zt);no=s(Qn,"xlm-clm-ende-1024"),Qn.forEach(l),so=s(gn," (Causal language modeling, English-German)"),gn.forEach(l),E.forEach(l),El=p(e),$=o(e,"P",{});var ae=i($);ro=s(ae,"Gli embeddings linguistici sono rappresentati come un tensore delle stesse dimensioni del "),Mt=o(ae,"CODE",{});var Wn=i(Mt);mo=s(Wn,"input_ids"),Wn.forEach(l),po=s(ae," passato al modello. I valori in questi tensori dipendono dal linguaggio usato e sono identificati dagli attributi "),wt=o(ae,"CODE",{});var Un=i(wt);uo=s(Un,"lang2id"),Un.forEach(l),go=s(ae," e "),yt=o(ae,"CODE",{});var Jn=i(yt);co=s(Jn,"id2lang"),Jn.forEach(l),fo=s(ae," del tokenizer."),ae.forEach(l),$l=p(e),R=o(e,"P",{});var ca=i(R);ho=s(ca,"In questo esempio, carica il checkpoint "),xt=o(ca,"CODE",{});var Kn=i(xt);_o=s(Kn,"xlm-clm-enfr-1024"),Kn.forEach(l),vo=s(ca," (Causal language modeling, English-French):"),ca.forEach(l),zl=p(e),c(ge.$$.fragment,e),Ml=p(e),F=o(e,"P",{});var fa=i(F);bo=s(fa,"L\u2019attributo "),Lt=o(fa,"CODE",{});var Vn=i(Lt);ko=s(Vn,"lang2id"),Vn.forEach(l),Eo=s(fa," del tokenizer mostra il linguaggio del modello e il suo ids:"),fa.forEach(l),wl=p(e),c(ce.$$.fragment,e),yl=p(e),Se=o(e,"P",{});var Yn=i(Se);$o=s(Yn,"Poi, crea un esempio di input:"),Yn.forEach(l),xl=p(e),c(fe.$$.fragment,e),Ll=p(e),z=o(e,"P",{});var oe=i(z);zo=s(oe,"Imposta l\u2019id del linguaggio a "),jt=o(oe,"CODE",{});var Zn=i(jt);Mo=s(Zn,'"en"'),Zn.forEach(l),wo=s(oe," e usalo per definire il language embedding. Il language embedding \xE8 un tensore riempito con "),qt=o(oe,"CODE",{});var es=i(qt);yo=s(es,"0"),es.forEach(l),xo=s(oe," perch\xE8 questo \xE8 il language id per l\u2019inglese. Questo tensorre dovrebbe avere la stessa dimensione di "),Ct=o(oe,"CODE",{});var ts=i(Ct);Lo=s(ts,"input_ids"),ts.forEach(l),jo=s(oe,"."),oe.forEach(l),jl=p(e),c(he.$$.fragment,e),ql=p(e),H=o(e,"P",{});var ha=i(H);qo=s(ha,"Adesso puoi inserire "),It=o(ha,"CODE",{});var ls=i(It);Co=s(ls,"input_ids"),ls.forEach(l),Io=s(ha," e language embedding nel modello:"),ha.forEach(l),Cl=p(e),c(_e.$$.fragment,e),Il=p(e),L=o(e,"P",{});var mt=i(L);Po=s(mt,"Lo script "),ve=o(mt,"A",{href:!0,rel:!0});var as=i(ve);Do=s(as,"run_generation.py"),as.forEach(l),To=s(mt," pu\xF2 generare testo tramite i language embeddings usando i checkpoints "),Pt=o(mt,"CODE",{});var os=i(Pt);Oo=s(os,"xlm-clm"),os.forEach(l),Ao=s(mt,"."),mt.forEach(l),Pl=p(e),D=o(e,"H3",{class:!0});var _a=i(D);G=o(_a,"A",{id:!0,class:!0,href:!0});var is=i(G);Dt=o(is,"SPAN",{});var ns=i(Dt);c(be.$$.fragment,ns),ns.forEach(l),is.forEach(l),Xo=p(_a),Tt=o(_a,"SPAN",{});var ss=i(Tt);So=s(ss,"XLM senza language embeddings"),ss.forEach(l),_a.forEach(l),Dl=p(e),Be=o(e,"P",{});var rs=i(Be);Bo=s(rs,"I seguenti modelli XLM non richiedono l\u2019utilizzo dei language embeddings per fare inferenza:"),rs.forEach(l),Tl=p(e),Q=o(e,"UL",{});var va=i(Q);Ne=o(va,"LI",{});var cn=i(Ne);Ot=o(cn,"CODE",{});var ms=i(Ot);No=s(ms,"xlm-mlm-17-1280"),ms.forEach(l),Ro=s(cn," (Masked language modeling, 17 languages)"),cn.forEach(l),Fo=p(va),Re=o(va,"LI",{});var fn=i(Re);At=o(fn,"CODE",{});var ds=i(At);Ho=s(ds,"xlm-mlm-100-1280"),ds.forEach(l),Go=s(fn," (Masked language modeling, 100 languages)"),fn.forEach(l),va.forEach(l),Ol=p(e),Fe=o(e,"P",{});var ps=i(Fe);Qo=s(ps,"Questi modelli sono utilizzati per rappresentazioni generiche di frasi, a differenza dei precedenti checkpoints XML."),ps.forEach(l),Al=p(e),T=o(e,"H2",{class:!0});var ba=i(T);W=o(ba,"A",{id:!0,class:!0,href:!0});var us=i(W);Xt=o(us,"SPAN",{});var gs=i(Xt);c(ke.$$.fragment,gs),gs.forEach(l),us.forEach(l),Wo=p(ba),St=o(ba,"SPAN",{});var cs=i(St);Uo=s(cs,"BERT"),cs.forEach(l),ba.forEach(l),Xl=p(e),He=o(e,"P",{});var fs=i(He);Jo=s(fs,"Il seguente modello BERT pu\xF2 essere usato per compiti multilingua:"),fs.forEach(l),Sl=p(e),U=o(e,"UL",{});var ka=i(U);Ge=o(ka,"LI",{});var hn=i(Ge);Bt=o(hn,"CODE",{});var hs=i(Bt);Ko=s(hs,"bert-base-multilingual-uncased"),hs.forEach(l),Vo=s(hn," (Masked language modeling + Next sentence prediction, 102 languages)"),hn.forEach(l),Yo=p(ka),Qe=o(ka,"LI",{});var _n=i(Qe);Nt=o(_n,"CODE",{});var _s=i(Nt);Zo=s(_s,"bert-base-multilingual-cased"),_s.forEach(l),ei=s(_n," (Masked language modeling + Next sentence prediction, 104 languages)"),_n.forEach(l),ka.forEach(l),Bl=p(e),We=o(e,"P",{});var vs=i(We);ti=s(vs,"Questi modelli non richiedono language embeddings per fare inferenza. Riescono ad identificare ul linguaggio dal contesto e inferire di conseguenza."),vs.forEach(l),Nl=p(e),O=o(e,"H2",{class:!0});var Ea=i(O);J=o(Ea,"A",{id:!0,class:!0,href:!0});var bs=i(J);Rt=o(bs,"SPAN",{});var ks=i(Rt);c(Ee.$$.fragment,ks),ks.forEach(l),bs.forEach(l),li=p(Ea),Ft=o(Ea,"SPAN",{});var Es=i(Ft);ai=s(Es,"XLM-RoBERTa"),Es.forEach(l),Ea.forEach(l),Rl=p(e),Ue=o(e,"P",{});var $s=i(Ue);oi=s($s,"Il seguente modello XLM-RoBERTa pu\xF2 essere usato per compiti multilingua:"),$s.forEach(l),Fl=p(e),K=o(e,"UL",{});var $a=i(K);Je=o($a,"LI",{});var vn=i(Je);Ht=o(vn,"CODE",{});var zs=i(Ht);ii=s(zs,"xlm-roberta-base"),zs.forEach(l),ni=s(vn," (Masked language modeling, 100 languages)"),vn.forEach(l),si=p($a),Ke=o($a,"LI",{});var bn=i(Ke);Gt=o(bn,"CODE",{});var Ms=i(Gt);ri=s(Ms,"xlm-roberta-large"),Ms.forEach(l),mi=s(bn," (Masked language modeling, 100 languages)"),bn.forEach(l),$a.forEach(l),Hl=p(e),Ve=o(e,"P",{});var ws=i(Ve);di=s(ws,"XLM-RoBERTa \xE8 stato addestrato su 2.5TB di dati CommonCrawl appena creati e puliti in 100 lingue. Offre notevoli vantaggi rispetto ai modelli multilingue rilasciati in precedenza, come mBERT o XLM, in compiti come la classificazione, l\u2019etichettatura delle sequenze e la risposta alle domande."),ws.forEach(l),Gl=p(e),A=o(e,"H2",{class:!0});var za=i(A);V=o(za,"A",{id:!0,class:!0,href:!0});var ys=i(V);Qt=o(ys,"SPAN",{});var xs=i(Qt);c($e.$$.fragment,xs),xs.forEach(l),ys.forEach(l),pi=p(za),Wt=o(za,"SPAN",{});var Ls=i(Wt);ui=s(Ls,"M2M100"),Ls.forEach(l),za.forEach(l),Ql=p(e),Ye=o(e,"P",{});var js=i(Ye);gi=s(js,"Il seguente modello M2M100 pu\xF2 essere usato per compiti multilingua:"),js.forEach(l),Wl=p(e),Y=o(e,"UL",{});var Ma=i(Y);Ze=o(Ma,"LI",{});var kn=i(Ze);Ut=o(kn,"CODE",{});var qs=i(Ut);ci=s(qs,"facebook/m2m100_418M"),qs.forEach(l),fi=s(kn," (Translation)"),kn.forEach(l),hi=p(Ma),et=o(Ma,"LI",{});var En=i(et);Jt=o(En,"CODE",{});var Cs=i(Jt);_i=s(Cs,"facebook/m2m100_1.2B"),Cs.forEach(l),vi=s(En," (Translation)"),En.forEach(l),Ma.forEach(l),Ul=p(e),Z=o(e,"P",{});var wa=i(Z);bi=s(wa,"In questo esempio, carica il checkpoint "),Kt=o(wa,"CODE",{});var Is=i(Kt);ki=s(Is,"facebook/m2m100_418M"),Is.forEach(l),Ei=s(wa,"  per tradurre dal cinese all\u2019inglese. Puoi impostare la lingua di partenza nel tokenizer:"),wa.forEach(l),Jl=p(e),c(ze.$$.fragment,e),Kl=p(e),tt=o(e,"P",{});var Ps=i(tt);$i=s(Ps,"Applica il tokenizer sul testo:"),Ps.forEach(l),Vl=p(e),c(Me.$$.fragment,e),Yl=p(e),M=o(e,"P",{});var ie=i(M);zi=s(ie,"M2M100 forza l\u2019id del target language come primo token generato per tradurre nel target language. Imposta il parametro "),Vt=o(ie,"CODE",{});var Ds=i(Vt);Mi=s(Ds,"forced_bos_token_id"),Ds.forEach(l),wi=s(ie," a "),Yt=o(ie,"CODE",{});var Ts=i(Yt);yi=s(Ts,"en"),Ts.forEach(l),xi=s(ie," nel metodo "),Zt=o(ie,"CODE",{});var Os=i(Zt);Li=s(Os,"generate"),Os.forEach(l),ji=s(ie," per tradurre in inglese:"),ie.forEach(l),Zl=p(e),c(we.$$.fragment,e),ea=p(e),X=o(e,"H2",{class:!0});var ya=i(X);ee=o(ya,"A",{id:!0,class:!0,href:!0});var As=i(ee);el=o(As,"SPAN",{});var Xs=i(el);c(ye.$$.fragment,Xs),Xs.forEach(l),As.forEach(l),qi=p(ya),tl=o(ya,"SPAN",{});var Ss=i(tl);Ci=s(Ss,"MBart"),Ss.forEach(l),ya.forEach(l),ta=p(e),lt=o(e,"P",{});var Bs=i(lt);Ii=s(Bs,"Il seguente modello MBart pu\xF2 essere usato per compiti multilingua:"),Bs.forEach(l),la=p(e),k=o(e,"UL",{});var j=i(k);at=o(j,"LI",{});var $n=i(at);ll=o($n,"CODE",{});var Ns=i(ll);Pi=s(Ns,"facebook/mbart-large-50-one-to-many-mmt"),Ns.forEach(l),Di=s($n," (One-to-many multilingual machine translation, 50 languages)"),$n.forEach(l),Ti=p(j),ot=o(j,"LI",{});var zn=i(ot);al=o(zn,"CODE",{});var Rs=i(al);Oi=s(Rs,"facebook/mbart-large-50-many-to-many-mmt"),Rs.forEach(l),Ai=s(zn," (Many-to-many multilingual machine translation, 50 languages)"),zn.forEach(l),Xi=p(j),it=o(j,"LI",{});var Mn=i(it);ol=o(Mn,"CODE",{});var Fs=i(ol);Si=s(Fs,"facebook/mbart-large-50-many-to-one-mmt"),Fs.forEach(l),Bi=s(Mn," (Many-to-one multilingual machine translation, 50 languages)"),Mn.forEach(l),Ni=p(j),nt=o(j,"LI",{});var wn=i(nt);il=o(wn,"CODE",{});var Hs=i(il);Ri=s(Hs,"facebook/mbart-large-50"),Hs.forEach(l),Fi=s(wn," (Multilingual translation, 50 languages)"),wn.forEach(l),Hi=p(j),nl=o(j,"LI",{});var Gs=i(nl);sl=o(Gs,"CODE",{});var Qs=i(sl);Gi=s(Qs,"facebook/mbart-large-cc25"),Qs.forEach(l),Gs.forEach(l),j.forEach(l),aa=p(e),te=o(e,"P",{});var xa=i(te);Qi=s(xa,"In questo esempio, carica il checkpoint "),rl=o(xa,"CODE",{});var Ws=i(rl);Wi=s(Ws,"facebook/mbart-large-50-many-to-many-mmt"),Ws.forEach(l),Ui=s(xa," per tradurre dal finlandese all\u2019inglese. Puoi impostare la lingua di partenza nel tokenizer:"),xa.forEach(l),oa=p(e),c(xe.$$.fragment,e),ia=p(e),st=o(e,"P",{});var Us=i(st);Ji=s(Us,"Applica il tokenizer sul testo:"),Us.forEach(l),na=p(e),c(Le.$$.fragment,e),sa=p(e),w=o(e,"P",{});var ne=i(w);Ki=s(ne,"MBart fforza l\u2019id del target language come primo token generato per tradurre nel target language. Imposta il parametro "),ml=o(ne,"CODE",{});var Js=i(ml);Vi=s(Js,"forced_bos_token_id"),Js.forEach(l),Yi=s(ne," a "),dl=o(ne,"CODE",{});var Ks=i(dl);Zi=s(Ks,"en"),Ks.forEach(l),en=s(ne," nel metodo "),pl=o(ne,"CODE",{});var Vs=i(pl);tn=s(Vs,"generate"),Vs.forEach(l),ln=s(ne," per tradurre in inglese:"),ne.forEach(l),ra=p(e),c(je.$$.fragment,e),ma=p(e),le=o(e,"P",{});var La=i(le);an=s(La,"Se stai usando il checkpoint "),ul=o(La,"CODE",{});var Ys=i(ul);on=s(Ys,"facebook/mbart-large-50-many-to-one-mmt"),Ys.forEach(l),nn=s(La,", non hai bisogno di forzare l\u2019id del target language come primo token generato altrimenti l\u2019uso \xE8 lo stesso."),La.forEach(l),this.h()},h(){u(q,"name","hf:doc:metadata"),u(q,"content",JSON.stringify(sr)),u(S,"id","modelli-multilingua-per-linferenza"),u(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(S,"href","#modelli-multilingua-per-linferenza"),u(C,"class","relative group"),u(de,"href","https://huggingface.co/bert-base-multilingual-uncased"),u(de,"rel","nofollow"),u(B,"id","xlm"),u(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(B,"href","#xlm"),u(I,"class","relative group"),u(N,"id","xlm-con-language-embeddings"),u(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(N,"href","#xlm-con-language-embeddings"),u(P,"class","relative group"),u(ve,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py"),u(ve,"rel","nofollow"),u(G,"id","xlm-senza-language-embeddings"),u(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(G,"href","#xlm-senza-language-embeddings"),u(D,"class","relative group"),u(W,"id","bert"),u(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(W,"href","#bert"),u(T,"class","relative group"),u(J,"id","xlmroberta"),u(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(J,"href","#xlmroberta"),u(O,"class","relative group"),u(V,"id","m2m100"),u(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(V,"href","#m2m100"),u(A,"class","relative group"),u(ee,"id","mbart"),u(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ee,"href","#mbart"),u(X,"class","relative group")},m(e,r){t(document.head,q),m(e,gl,r),m(e,C,r),t(C,S),t(S,dt),f(re,dt,null),t(C,ja),t(C,pt),t(pt,qa),m(e,cl,r),f(me,e,r),m(e,fl,r),m(e,x,r),t(x,Ca),t(x,ut),t(ut,Ia),t(x,Pa),t(x,de),t(de,Da),t(x,Ta),m(e,hl,r),m(e,I,r),t(I,B),t(B,gt),f(pe,gt,null),t(I,Oa),t(I,ct),t(ct,Aa),m(e,_l,r),m(e,qe,r),t(qe,Xa),m(e,vl,r),m(e,P,r),t(P,N),t(N,ft),f(ue,ft,null),t(P,Sa),t(P,ht),t(ht,Ba),m(e,bl,r),m(e,Ce,r),t(Ce,Na),m(e,kl,r),m(e,b,r),t(b,Ie),t(Ie,_t),t(_t,Ra),t(Ie,Fa),t(b,Ha),t(b,Pe),t(Pe,vt),t(vt,Ga),t(Pe,Qa),t(b,Wa),t(b,De),t(De,bt),t(bt,Ua),t(De,Ja),t(b,Ka),t(b,Te),t(Te,kt),t(kt,Va),t(Te,Ya),t(b,Za),t(b,Oe),t(Oe,Et),t(Et,eo),t(Oe,to),t(b,lo),t(b,Ae),t(Ae,$t),t($t,ao),t(Ae,oo),t(b,io),t(b,Xe),t(Xe,zt),t(zt,no),t(Xe,so),m(e,El,r),m(e,$,r),t($,ro),t($,Mt),t(Mt,mo),t($,po),t($,wt),t(wt,uo),t($,go),t($,yt),t(yt,co),t($,fo),m(e,$l,r),m(e,R,r),t(R,ho),t(R,xt),t(xt,_o),t(R,vo),m(e,zl,r),f(ge,e,r),m(e,Ml,r),m(e,F,r),t(F,bo),t(F,Lt),t(Lt,ko),t(F,Eo),m(e,wl,r),f(ce,e,r),m(e,yl,r),m(e,Se,r),t(Se,$o),m(e,xl,r),f(fe,e,r),m(e,Ll,r),m(e,z,r),t(z,zo),t(z,jt),t(jt,Mo),t(z,wo),t(z,qt),t(qt,yo),t(z,xo),t(z,Ct),t(Ct,Lo),t(z,jo),m(e,jl,r),f(he,e,r),m(e,ql,r),m(e,H,r),t(H,qo),t(H,It),t(It,Co),t(H,Io),m(e,Cl,r),f(_e,e,r),m(e,Il,r),m(e,L,r),t(L,Po),t(L,ve),t(ve,Do),t(L,To),t(L,Pt),t(Pt,Oo),t(L,Ao),m(e,Pl,r),m(e,D,r),t(D,G),t(G,Dt),f(be,Dt,null),t(D,Xo),t(D,Tt),t(Tt,So),m(e,Dl,r),m(e,Be,r),t(Be,Bo),m(e,Tl,r),m(e,Q,r),t(Q,Ne),t(Ne,Ot),t(Ot,No),t(Ne,Ro),t(Q,Fo),t(Q,Re),t(Re,At),t(At,Ho),t(Re,Go),m(e,Ol,r),m(e,Fe,r),t(Fe,Qo),m(e,Al,r),m(e,T,r),t(T,W),t(W,Xt),f(ke,Xt,null),t(T,Wo),t(T,St),t(St,Uo),m(e,Xl,r),m(e,He,r),t(He,Jo),m(e,Sl,r),m(e,U,r),t(U,Ge),t(Ge,Bt),t(Bt,Ko),t(Ge,Vo),t(U,Yo),t(U,Qe),t(Qe,Nt),t(Nt,Zo),t(Qe,ei),m(e,Bl,r),m(e,We,r),t(We,ti),m(e,Nl,r),m(e,O,r),t(O,J),t(J,Rt),f(Ee,Rt,null),t(O,li),t(O,Ft),t(Ft,ai),m(e,Rl,r),m(e,Ue,r),t(Ue,oi),m(e,Fl,r),m(e,K,r),t(K,Je),t(Je,Ht),t(Ht,ii),t(Je,ni),t(K,si),t(K,Ke),t(Ke,Gt),t(Gt,ri),t(Ke,mi),m(e,Hl,r),m(e,Ve,r),t(Ve,di),m(e,Gl,r),m(e,A,r),t(A,V),t(V,Qt),f($e,Qt,null),t(A,pi),t(A,Wt),t(Wt,ui),m(e,Ql,r),m(e,Ye,r),t(Ye,gi),m(e,Wl,r),m(e,Y,r),t(Y,Ze),t(Ze,Ut),t(Ut,ci),t(Ze,fi),t(Y,hi),t(Y,et),t(et,Jt),t(Jt,_i),t(et,vi),m(e,Ul,r),m(e,Z,r),t(Z,bi),t(Z,Kt),t(Kt,ki),t(Z,Ei),m(e,Jl,r),f(ze,e,r),m(e,Kl,r),m(e,tt,r),t(tt,$i),m(e,Vl,r),f(Me,e,r),m(e,Yl,r),m(e,M,r),t(M,zi),t(M,Vt),t(Vt,Mi),t(M,wi),t(M,Yt),t(Yt,yi),t(M,xi),t(M,Zt),t(Zt,Li),t(M,ji),m(e,Zl,r),f(we,e,r),m(e,ea,r),m(e,X,r),t(X,ee),t(ee,el),f(ye,el,null),t(X,qi),t(X,tl),t(tl,Ci),m(e,ta,r),m(e,lt,r),t(lt,Ii),m(e,la,r),m(e,k,r),t(k,at),t(at,ll),t(ll,Pi),t(at,Di),t(k,Ti),t(k,ot),t(ot,al),t(al,Oi),t(ot,Ai),t(k,Xi),t(k,it),t(it,ol),t(ol,Si),t(it,Bi),t(k,Ni),t(k,nt),t(nt,il),t(il,Ri),t(nt,Fi),t(k,Hi),t(k,nl),t(nl,sl),t(sl,Gi),m(e,aa,r),m(e,te,r),t(te,Qi),t(te,rl),t(rl,Wi),t(te,Ui),m(e,oa,r),f(xe,e,r),m(e,ia,r),m(e,st,r),t(st,Ji),m(e,na,r),f(Le,e,r),m(e,sa,r),m(e,w,r),t(w,Ki),t(w,ml),t(ml,Vi),t(w,Yi),t(w,dl),t(dl,Zi),t(w,en),t(w,pl),t(pl,tn),t(w,ln),m(e,ra,r),f(je,e,r),m(e,ma,r),m(e,le,r),t(le,an),t(le,ul),t(ul,on),t(le,nn),da=!0},p:ar,i(e){da||(h(re.$$.fragment,e),h(me.$$.fragment,e),h(pe.$$.fragment,e),h(ue.$$.fragment,e),h(ge.$$.fragment,e),h(ce.$$.fragment,e),h(fe.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(be.$$.fragment,e),h(ke.$$.fragment,e),h(Ee.$$.fragment,e),h($e.$$.fragment,e),h(ze.$$.fragment,e),h(Me.$$.fragment,e),h(we.$$.fragment,e),h(ye.$$.fragment,e),h(xe.$$.fragment,e),h(Le.$$.fragment,e),h(je.$$.fragment,e),da=!0)},o(e){_(re.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(ue.$$.fragment,e),_(ge.$$.fragment,e),_(ce.$$.fragment,e),_(fe.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(ke.$$.fragment,e),_(Ee.$$.fragment,e),_($e.$$.fragment,e),_(ze.$$.fragment,e),_(Me.$$.fragment,e),_(we.$$.fragment,e),_(ye.$$.fragment,e),_(xe.$$.fragment,e),_(Le.$$.fragment,e),_(je.$$.fragment,e),da=!1},d(e){l(q),e&&l(gl),e&&l(C),v(re),e&&l(cl),v(me,e),e&&l(fl),e&&l(x),e&&l(hl),e&&l(I),v(pe),e&&l(_l),e&&l(qe),e&&l(vl),e&&l(P),v(ue),e&&l(bl),e&&l(Ce),e&&l(kl),e&&l(b),e&&l(El),e&&l($),e&&l($l),e&&l(R),e&&l(zl),v(ge,e),e&&l(Ml),e&&l(F),e&&l(wl),v(ce,e),e&&l(yl),e&&l(Se),e&&l(xl),v(fe,e),e&&l(Ll),e&&l(z),e&&l(jl),v(he,e),e&&l(ql),e&&l(H),e&&l(Cl),v(_e,e),e&&l(Il),e&&l(L),e&&l(Pl),e&&l(D),v(be),e&&l(Dl),e&&l(Be),e&&l(Tl),e&&l(Q),e&&l(Ol),e&&l(Fe),e&&l(Al),e&&l(T),v(ke),e&&l(Xl),e&&l(He),e&&l(Sl),e&&l(U),e&&l(Bl),e&&l(We),e&&l(Nl),e&&l(O),v(Ee),e&&l(Rl),e&&l(Ue),e&&l(Fl),e&&l(K),e&&l(Hl),e&&l(Ve),e&&l(Gl),e&&l(A),v($e),e&&l(Ql),e&&l(Ye),e&&l(Wl),e&&l(Y),e&&l(Ul),e&&l(Z),e&&l(Jl),v(ze,e),e&&l(Kl),e&&l(tt),e&&l(Vl),v(Me,e),e&&l(Yl),e&&l(M),e&&l(Zl),v(we,e),e&&l(ea),e&&l(X),v(ye),e&&l(ta),e&&l(lt),e&&l(la),e&&l(k),e&&l(aa),e&&l(te),e&&l(oa),v(xe,e),e&&l(ia),e&&l(st),e&&l(na),v(Le,e),e&&l(sa),e&&l(w),e&&l(ra),v(je,e),e&&l(ma),e&&l(le)}}}const sr={local:"modelli-multilingua-per-linferenza",sections:[{local:"xlm",sections:[{local:"xlm-con-language-embeddings",title:"XLM con language embeddings"},{local:"xlm-senza-language-embeddings",title:"XLM senza language embeddings"}],title:"XLM"},{local:"bert",title:"BERT"},{local:"xlmroberta",title:"XLM-RoBERTa"},{local:"m2m100",title:"M2M100"},{local:"mbart",title:"MBart"}],title:"Modelli multilingua per l'inferenza"};function rr(yn){return or(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gr extends Zs{constructor(q){super();er(this,q,rr,nr,tr,{})}}export{gr as default,sr as metadata};
