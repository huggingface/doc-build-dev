import{S as jn,i as $n,s as kn,e as r,k as _,w as E,t as n,M as bn,c as i,d as e,m as g,a as p,x,h as l,b as k,G as a,g as d,y as q,q as z,o as C,B as T,v as vn,L as gn}from"../../chunks/vendor-hf-doc-builder.js";import{T as ge}from"../../chunks/Tip-hf-doc-builder.js";import{Y as hn}from"../../chunks/Youtube-hf-doc-builder.js";import{I as _e}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as as}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as _n,M as je}from"../../chunks/Markdown-hf-doc-builder.js";import"../../chunks/IconTensorflow-hf-doc-builder.js";function wn(P){let t,m,o,f,$;return{c(){t=r("p"),m=n("Consulta la [p\xE1gina de la tarea] de clasificaci\xF3n de tokens ("),o=r("a"),f=n("https://huggingface.co/tasks/token-classification"),$=n(") para obtener m\xE1s informaci\xF3n sobre otras formas de clasificaci\xF3n de tokens y sus modelos, conjuntos de datos y m\xE9tricas asociadas."),this.h()},l(j){t=i(j,"P",{});var b=p(t);m=l(b,"Consulta la [p\xE1gina de la tarea] de clasificaci\xF3n de tokens ("),o=i(b,"A",{href:!0,rel:!0});var y=p(o);f=l(y,"https://huggingface.co/tasks/token-classification"),y.forEach(e),$=l(b,") para obtener m\xE1s informaci\xF3n sobre otras formas de clasificaci\xF3n de tokens y sus modelos, conjuntos de datos y m\xE9tricas asociadas."),b.forEach(e),this.h()},h(){k(o,"href","https://huggingface.co/tasks/token-classification"),k(o,"rel","nofollow")},m(j,b){d(j,t,b),a(t,m),a(t,o),a(o,f),a(t,$)},d(j){j&&e(t)}}}function En(P){let t,m;return t=new as({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p:gn,i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function xn(P){let t,m;return t=new je({props:{$$slots:{default:[En]},$$scope:{ctx:P}}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p(o,f){const $={};f&2&&($.$$scope={dirty:f,ctx:o}),t.$set($)},i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function qn(P){let t,m;return t=new as({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p:gn,i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function zn(P){let t,m;return t=new je({props:{$$slots:{default:[qn]},$$scope:{ctx:P}}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p(o,f){const $={};f&2&&($.$$scope={dirty:f,ctx:o}),t.$set($)},i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function Cn(P){let t,m,o,f,$,j,b,y;return{c(){t=r("p"),m=n("Si no estas familiarizado con el ajuste fino de un modelo con el "),o=r("code"),f=n("Trainer"),$=n(", da un vistazo al tutorial b\xE1sico "),j=r("a"),b=n("aqu\xED"),y=n("!"),this.h()},l(D){t=i(D,"P",{});var w=p(t);m=l(w,"Si no estas familiarizado con el ajuste fino de un modelo con el "),o=i(w,"CODE",{});var O=p(o);f=l(O,"Trainer"),O.forEach(e),$=l(w,", da un vistazo al tutorial b\xE1sico "),j=i(w,"A",{href:!0});var R=p(j);b=l(R,"aqu\xED"),R.forEach(e),y=l(w,"!"),w.forEach(e),this.h()},h(){k(j,"href","../training#finetune-with-trainer")},m(D,w){d(D,t,w),a(t,m),a(t,o),a(o,f),a(t,$),a(t,j),a(j,b),a(t,y)},d(D){D&&e(t)}}}function Tn(P){let t,m,o,f,$,j,b,y,D,w,O,R,G,S,K,L,ls,U,js,ms,F,fs,W,hs,H,_s,N,V,I,J,Q,os,M,es;return b=new as({props:{code:`from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=14)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">14</span>)`}}),D=new ge({props:{$$slots:{default:[Cn]},$$scope:{ctx:P}}}),M=new as({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut["train"],
    eval_dataset=tokenized_wnut["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_wnut[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){t=r("p"),m=n("Carga el modelo DistilBERT con "),o=r("code"),f=n("AutoModelForTokenClassification"),$=n(" junto con el n\xFAmero de etiquetas previstas:"),j=_(),E(b.$$.fragment),y=_(),E(D.$$.fragment),w=_(),O=r("p"),R=n("En este punto, solo quedan tres pasos:"),G=_(),S=r("ol"),K=r("li"),L=n("Definir los hiperpar\xE1metros del entrenamiento en "),ls=r("code"),U=n("TrainingArguments"),js=n("."),ms=_(),F=r("li"),fs=n("Pasar los par\xE1metros de entrenamiento a "),W=r("code"),hs=n("Trainer"),H=n(" junto con el modelo, el conjunto de datos, el tokenizador y el lote de ejemplos."),_s=_(),N=r("li"),V=n("Llamar "),I=r("code"),J=n("train()"),Q=n(" para ajustar el modelo."),os=_(),E(M.$$.fragment)},l(h){t=i(h,"P",{});var A=p(t);m=l(A,"Carga el modelo DistilBERT con "),o=i(A,"CODE",{});var ts=p(o);f=l(ts,"AutoModelForTokenClassification"),ts.forEach(e),$=l(A," junto con el n\xFAmero de etiquetas previstas:"),A.forEach(e),j=g(h),x(b.$$.fragment,h),y=g(h),x(D.$$.fragment,h),w=g(h),O=i(h,"P",{});var Y=p(O);R=l(Y,"En este punto, solo quedan tres pasos:"),Y.forEach(e),G=g(h),S=i(h,"OL",{});var X=p(S);K=i(X,"LI",{});var B=p(K);L=l(B,"Definir los hiperpar\xE1metros del entrenamiento en "),ls=i(B,"CODE",{});var bs=p(ls);U=l(bs,"TrainingArguments"),bs.forEach(e),js=l(B,"."),B.forEach(e),ms=g(X),F=i(X,"LI",{});var Z=p(F);fs=l(Z,"Pasar los par\xE1metros de entrenamiento a "),W=i(Z,"CODE",{});var ss=p(W);hs=l(ss,"Trainer"),ss.forEach(e),H=l(Z," junto con el modelo, el conjunto de datos, el tokenizador y el lote de ejemplos."),Z.forEach(e),_s=g(X),N=i(X,"LI",{});var is=p(N);V=l(is,"Llamar "),I=i(is,"CODE",{});var ns=p(I);J=l(ns,"train()"),ns.forEach(e),Q=l(is," para ajustar el modelo."),is.forEach(e),X.forEach(e),os=g(h),x(M.$$.fragment,h)},m(h,A){d(h,t,A),a(t,m),a(t,o),a(o,f),a(t,$),d(h,j,A),q(b,h,A),d(h,y,A),q(D,h,A),d(h,w,A),d(h,O,A),a(O,R),d(h,G,A),d(h,S,A),a(S,K),a(K,L),a(K,ls),a(ls,U),a(K,js),a(S,ms),a(S,F),a(F,fs),a(F,W),a(W,hs),a(F,H),a(S,_s),a(S,N),a(N,V),a(N,I),a(I,J),a(N,Q),d(h,os,A),q(M,h,A),es=!0},p(h,A){const ts={};A&2&&(ts.$$scope={dirty:A,ctx:h}),D.$set(ts)},i(h){es||(z(b.$$.fragment,h),z(D.$$.fragment,h),z(M.$$.fragment,h),es=!0)},o(h){C(b.$$.fragment,h),C(D.$$.fragment,h),C(M.$$.fragment,h),es=!1},d(h){h&&e(t),h&&e(j),T(b,h),h&&e(y),T(D,h),h&&e(w),h&&e(O),h&&e(G),h&&e(S),h&&e(os),T(M,h)}}}function yn(P){let t,m;return t=new je({props:{$$slots:{default:[Tn]},$$scope:{ctx:P}}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p(o,f){const $={};f&2&&($.$$scope={dirty:f,ctx:o}),t.$set($)},i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function Dn(P){let t,m,o,f,$;return{c(){t=r("p"),m=n("Si no estas familiarizado con el ajuste fino de un modelo con Keras, da un vistazo al tutorial b\xE1sico "),o=r("a"),f=n("aqu\xED"),$=n("!"),this.h()},l(j){t=i(j,"P",{});var b=p(t);m=l(b,"Si no estas familiarizado con el ajuste fino de un modelo con Keras, da un vistazo al tutorial b\xE1sico "),o=i(b,"A",{href:!0});var y=p(o);f=l(y,"aqu\xED"),y.forEach(e),$=l(b,"!"),b.forEach(e),this.h()},h(){k(o,"href","training#finetune-with-keras")},m(j,b){d(j,t,b),a(t,m),a(t,o),a(o,f),a(t,$)},d(j){j&&e(t)}}}function An(P){let t,m,o,f,$,j,b,y,D,w,O,R,G,S,K,L,ls,U,js,ms,F,fs,W,hs,H,_s,N,V,I,J,Q,os,M,es,h,A,ts,Y,X,B,bs,Z,ss,is,ns,ys,ps,Ds;return S=new as({props:{code:`tf_train_set = tokenized_wnut["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_wnut["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_wnut[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),L=new ge({props:{$$slots:{default:[Dn]},$$scope:{ctx:P}}}),F=new as({props:{code:`from transformers import create_optimizer

batch_size = 16
num_train_epochs = 3
num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_steps = (<span class="hljs-built_in">len</span>(tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_train_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, lr_schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_steps=num_train_steps,
<span class="hljs-meta">... </span>    weight_decay_rate=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>)`}}),I=new as({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),Y=new as({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),ps=new as({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){t=r("p"),m=n("Para ajustar un modelo en TensorFlow, empieza por convertir tus conjuntos de datos al formato "),o=r("code"),f=n("tf.data.Dataset"),$=n(" con "),j=r("a"),b=r("code"),y=n("to_tf_dataset"),D=n(". Especifica las entradas y las etiquetas en "),w=r("code"),O=n("columns"),R=n(", si deseas mezclar el orden del conjunto de datos, el tama\xF1o del lote y el lote de ejemplos:"),G=_(),E(S.$$.fragment),K=_(),E(L.$$.fragment),ls=_(),U=r("p"),js=n("Configura una funci\xF3n de optimizaci\xF3n, un programa de tasa de aprendizaje y algunos hiperpar\xE1metros de entrenamiento:"),ms=_(),E(F.$$.fragment),fs=_(),W=r("p"),hs=n("Carga DistilBERT con "),H=r("code"),_s=n("TFAutoModelForTokenClassification"),N=n(" junto con el n\xFAmero de etiquetas previstas:"),V=_(),E(I.$$.fragment),J=_(),Q=r("p"),os=n("Configura el modelo para el entrenamiento con "),M=r("a"),es=r("code"),h=n("compile"),A=n(":"),ts=_(),E(Y.$$.fragment),X=_(),B=r("p"),bs=n("Llame a "),Z=r("a"),ss=r("code"),is=n("fit"),ns=n(" para afinar el modelo:"),ys=_(),E(ps.$$.fragment),this.h()},l(c){t=i(c,"P",{});var v=p(t);m=l(v,"Para ajustar un modelo en TensorFlow, empieza por convertir tus conjuntos de datos al formato "),o=i(v,"CODE",{});var cs=p(o);f=l(cs,"tf.data.Dataset"),cs.forEach(e),$=l(v," con "),j=i(v,"A",{href:!0,rel:!0});var Ws=p(j);b=i(Ws,"CODE",{});var ds=p(b);y=l(ds,"to_tf_dataset"),ds.forEach(e),Ws.forEach(e),D=l(v,". Especifica las entradas y las etiquetas en "),w=i(v,"CODE",{});var na=p(w);O=l(na,"columns"),na.forEach(e),R=l(v,", si deseas mezclar el orden del conjunto de datos, el tama\xF1o del lote y el lote de ejemplos:"),v.forEach(e),G=g(c),x(S.$$.fragment,c),K=g(c),x(L.$$.fragment,c),ls=g(c),U=i(c,"P",{});var As=p(U);js=l(As,"Configura una funci\xF3n de optimizaci\xF3n, un programa de tasa de aprendizaje y algunos hiperpar\xE1metros de entrenamiento:"),As.forEach(e),ms=g(c),x(F.$$.fragment,c),fs=g(c),W=i(c,"P",{});var Ps=p(W);hs=l(Ps,"Carga DistilBERT con "),H=i(Ps,"CODE",{});var la=p(H);_s=l(la,"TFAutoModelForTokenClassification"),la.forEach(e),N=l(Ps," junto con el n\xFAmero de etiquetas previstas:"),Ps.forEach(e),V=g(c),x(I.$$.fragment,c),J=g(c),Q=i(c,"P",{});var vs=p(Q);os=l(vs,"Configura el modelo para el entrenamiento con "),M=i(vs,"A",{href:!0,rel:!0});var oa=p(M);es=i(oa,"CODE",{});var ra=p(es);h=l(ra,"compile"),ra.forEach(e),oa.forEach(e),A=l(vs,":"),vs.forEach(e),ts=g(c),x(Y.$$.fragment,c),X=g(c),B=i(c,"P",{});var zs=p(B);bs=l(zs,"Llame a "),Z=i(zs,"A",{href:!0,rel:!0});var us=p(Z);ss=i(us,"CODE",{});var ws=p(ss);is=l(ws,"fit"),ws.forEach(e),us.forEach(e),ns=l(zs," para afinar el modelo:"),zs.forEach(e),ys=g(c),x(ps.$$.fragment,c),this.h()},h(){k(j,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),k(j,"rel","nofollow"),k(M,"href","https://keras.io/api/models/model_training_apis/#compile-method"),k(M,"rel","nofollow"),k(Z,"href","https://keras.io/api/models/model_training_apis/#fit-method"),k(Z,"rel","nofollow")},m(c,v){d(c,t,v),a(t,m),a(t,o),a(o,f),a(t,$),a(t,j),a(j,b),a(b,y),a(t,D),a(t,w),a(w,O),a(t,R),d(c,G,v),q(S,c,v),d(c,K,v),q(L,c,v),d(c,ls,v),d(c,U,v),a(U,js),d(c,ms,v),q(F,c,v),d(c,fs,v),d(c,W,v),a(W,hs),a(W,H),a(H,_s),a(W,N),d(c,V,v),q(I,c,v),d(c,J,v),d(c,Q,v),a(Q,os),a(Q,M),a(M,es),a(es,h),a(Q,A),d(c,ts,v),q(Y,c,v),d(c,X,v),d(c,B,v),a(B,bs),a(B,Z),a(Z,ss),a(ss,is),a(B,ns),d(c,ys,v),q(ps,c,v),Ds=!0},p(c,v){const cs={};v&2&&(cs.$$scope={dirty:v,ctx:c}),L.$set(cs)},i(c){Ds||(z(S.$$.fragment,c),z(L.$$.fragment,c),z(F.$$.fragment,c),z(I.$$.fragment,c),z(Y.$$.fragment,c),z(ps.$$.fragment,c),Ds=!0)},o(c){C(S.$$.fragment,c),C(L.$$.fragment,c),C(F.$$.fragment,c),C(I.$$.fragment,c),C(Y.$$.fragment,c),C(ps.$$.fragment,c),Ds=!1},d(c){c&&e(t),c&&e(G),T(S,c),c&&e(K),T(L,c),c&&e(ls),c&&e(U),c&&e(ms),T(F,c),c&&e(fs),c&&e(W),c&&e(V),T(I,c),c&&e(J),c&&e(Q),c&&e(ts),T(Y,c),c&&e(X),c&&e(B),c&&e(ys),T(ps,c)}}}function Pn(P){let t,m;return t=new je({props:{$$slots:{default:[An]},$$scope:{ctx:P}}}),{c(){E(t.$$.fragment)},l(o){x(t.$$.fragment,o)},m(o,f){q(t,o,f),m=!0},p(o,f){const $={};f&2&&($.$$scope={dirty:f,ctx:o}),t.$set($)},i(o){m||(z(t.$$.fragment,o),m=!0)},o(o){C(t.$$.fragment,o),m=!1},d(o){T(t,o)}}}function On(P){let t,m,o,f,$,j,b,y;return{c(){t=r("p"),m=n(`Para ver un ejemplo m\xE1s detallado de c\xF3mo afinar un modelo para la clasificaci\xF3n de tokens, da un vistazo
a `),o=r("a"),f=n("PyTorch notebook"),$=n(`
o `),j=r("a"),b=n("TensorFlow notebook"),y=n("."),this.h()},l(D){t=i(D,"P",{});var w=p(t);m=l(w,`Para ver un ejemplo m\xE1s detallado de c\xF3mo afinar un modelo para la clasificaci\xF3n de tokens, da un vistazo
a `),o=i(w,"A",{href:!0,rel:!0});var O=p(o);f=l(O,"PyTorch notebook"),O.forEach(e),$=l(w,`
o `),j=i(w,"A",{href:!0,rel:!0});var R=p(j);b=l(R,"TensorFlow notebook"),R.forEach(e),y=l(w,"."),w.forEach(e),this.h()},h(){k(o,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),k(o,"rel","nofollow"),k(j,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb"),k(j,"rel","nofollow")},m(D,w){d(D,t,w),a(t,m),a(t,o),a(o,f),a(t,$),a(t,j),a(j,b),a(t,y)},d(D){D&&e(t)}}}function Ln(P){let t,m,o,f,$,j,b,y,D,w,O,R,G,S,K,L,ls,U,js,ms,F,fs,W,hs,H,_s,N,V,I,J,Q,os,M,es,h,A,ts,Y,X,B,bs,Z,ss,is,ns,ys,ps,Ds,c,v,cs,Ws,ds,na,As,Ps,la,vs,oa,ra,zs,us,ws,$e,fa,ke,be,ve,$s,we,ha,Ee,xe,_a,qe,ze,ga,Ce,Te,ye,Hs,De,ja,Ae,Pe,Ra,Cs,Os,$a,Ys,Oe,ka,Le,Ua,Ks,Wa,Ls,Fe,ba,Se,Ne,Ha,Vs,Ya,Fs,Be,va,Ie,Me,Ka,Zs,Va,Es,Re,wa,Ue,We,Ea,He,Ye,Za,xs,Gs,Ke,Js,xa,Ve,Ze,Ge,ks,Je,qa,Qe,Xe,za,st,at,Ca,et,tt,nt,Qs,lt,Ta,ot,rt,Ga,ia,it,Ja,Xs,Qa,gs,pt,sa,ya,ct,dt,Da,ut,mt,Aa,ft,ht,Xa,aa,se,rs,_t,Pa,gt,jt,Oa,$t,kt,La,bt,vt,Fa,wt,Et,ae,Ss,ee,Ts,Ns,Sa,ea,xt,Na,qt,te,Bs,ne,Is,le;return j=new _e({}),O=new hn({props:{id:"wVHdVlPScxA"}}),H=new ge({props:{$$slots:{default:[wn]},$$scope:{ctx:P}}}),J=new _e({}),Y=new as({props:{code:`from datasets import load_dataset

wnut = load_dataset("wnut_17")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>wnut = load_dataset(<span class="hljs-string">&quot;wnut_17&quot;</span>)`}}),ss=new as({props:{code:'wnut["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>wnut[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,
 <span class="hljs-string">&#x27;ner_tags&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;tokens&#x27;</span>: [<span class="hljs-string">&#x27;@paulwalk&#x27;</span>, <span class="hljs-string">&#x27;It&#x27;</span>, <span class="hljs-string">&quot;&#x27;s&quot;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&quot;&#x27;m&quot;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Empire&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;Building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;ESB&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]
}`}}),cs=new as({props:{code:`label_list = wnut["train"].features[f"ner_tags"].feature.names
label_list`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>label_list = wnut[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">f&quot;ner_tags&quot;</span>].feature.names
<span class="hljs-meta">&gt;&gt;&gt; </span>label_list
[
    <span class="hljs-string">&quot;O&quot;</span>,
    <span class="hljs-string">&quot;B-corporation&quot;</span>,
    <span class="hljs-string">&quot;I-corporation&quot;</span>,
    <span class="hljs-string">&quot;B-creative-work&quot;</span>,
    <span class="hljs-string">&quot;I-creative-work&quot;</span>,
    <span class="hljs-string">&quot;B-group&quot;</span>,
    <span class="hljs-string">&quot;I-group&quot;</span>,
    <span class="hljs-string">&quot;B-location&quot;</span>,
    <span class="hljs-string">&quot;I-location&quot;</span>,
    <span class="hljs-string">&quot;B-person&quot;</span>,
    <span class="hljs-string">&quot;I-person&quot;</span>,
    <span class="hljs-string">&quot;B-product&quot;</span>,
    <span class="hljs-string">&quot;I-product&quot;</span>,
]`}}),Ys=new _e({}),Ks=new hn({props:{id:"iY2AZYdZAr0"}}),Vs=new as({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Zs=new as({props:{code:`tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_input = tokenizer(example[<span class="hljs-string">&quot;tokens&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens
[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;paul&#x27;</span>, <span class="hljs-string">&#x27;##walk&#x27;</span>, <span class="hljs-string">&#x27;it&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;empire&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;es&#x27;</span>, <span class="hljs-string">&#x27;##b&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),Xs=new as({props:{code:`def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Asigna los tokens a la palabra correspondiente.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:  # Asigna -100 a los tokens especiales.
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Etiqueta solo el primer token de una palabra determinada.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    tokenized_inputs = tokenizer(examples[<span class="hljs-string">&quot;tokens&quot;</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    labels = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f&quot;ner_tags&quot;</span>]):
<span class="hljs-meta">... </span>        word_ids = tokenized_inputs.word_ids(batch_index=i)  <span class="hljs-comment"># Asigna los tokens a la palabra correspondiente.</span>
<span class="hljs-meta">... </span>        previous_word_idx = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>        label_ids = []
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:  <span class="hljs-comment"># Asigna -100 a los tokens especiales.</span>
<span class="hljs-meta">... </span>            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:  <span class="hljs-comment"># Etiqueta solo el primer token de una palabra determinada.</span>
<span class="hljs-meta">... </span>                label_ids.append(label[word_idx])
<span class="hljs-meta">... </span>            <span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            previous_word_idx = word_idx
<span class="hljs-meta">... </span>        labels.append(label_ids)

<span class="hljs-meta">... </span>    tokenized_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenized_inputs`}}),aa=new as({props:{code:"tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_wnut = wnut.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)'}}),Ss=new _n({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[zn],pytorch:[xn]},$$scope:{ctx:P}}}),ea=new _e({}),Bs=new _n({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Pn],pytorch:[yn]},$$scope:{ctx:P}}}),Is=new ge({props:{$$slots:{default:[On]},$$scope:{ctx:P}}}),{c(){t=r("meta"),m=_(),o=r("h1"),f=r("a"),$=r("span"),E(j.$$.fragment),b=_(),y=r("span"),D=n("Clasificaci\xF3n de tokens"),w=_(),E(O.$$.fragment),R=_(),G=r("p"),S=n("La clasificaci\xF3n de tokens asigna una etiqueta a los tokens individuales de una frase. Una de las tareas m\xE1s comunes de clasificaci\xF3n de tokens es el Reconocimiento de Entidades Nombradas (en ingl\xE9s, NER). El NER intenta encontrar una etiqueta para cada entidad en una frase, como persona, ubicaci\xF3n, u organizaci\xF3n."),K=_(),L=r("p"),ls=n("Esta gu\xEDa te mostrar\xE1 c\xF3mo afinar el modelo (en ingl\xE9s, finetunning) "),U=r("a"),js=n("DistilBERT"),ms=n(" con el conjunto de datos "),F=r("a"),fs=n("WNUT 17"),W=n(" para detectar nuevas entidades."),hs=_(),E(H.$$.fragment),_s=_(),N=r("h2"),V=r("a"),I=r("span"),E(J.$$.fragment),Q=_(),os=r("span"),M=n("Cargar el conjunto de datos WNUT 17"),es=_(),h=r("p"),A=n("Cargue el conjunto de datos WNUT 17 de la biblioteca \u{1F917} Datasets :"),ts=_(),E(Y.$$.fragment),X=_(),B=r("p"),bs=n("Entonces, observa un ejemplo:"),Z=_(),E(ss.$$.fragment),is=_(),ns=r("p"),ys=n("Cada n\xFAmero en "),ps=r("code"),Ds=n("ner_tags"),c=n(" representa una entidad. Convierte el n\xFAmero en un nombre de etiqueta para obtener m\xE1s informaci\xF3n:"),v=_(),E(cs.$$.fragment),Ws=_(),ds=r("p"),na=n("La etiqueta "),As=r("code"),Ps=n("ner_tag"),la=n(" describe una entidad, como una empresa, un lugar, o una persona. La letra que precede a cada "),vs=r("code"),oa=n("ner_tag"),ra=n(" indica la posici\xF3n del token en la entidad:"),zs=_(),us=r("ul"),ws=r("li"),$e=n("La letra "),fa=r("code"),ke=n("B-"),be=n(" indica el comienzo de la entidad."),ve=_(),$s=r("li"),we=n("La letra "),ha=r("code"),Ee=n("I-"),xe=n(" indica que un token est\xE1 contenido dentro de la misma entidad (por ejemplo, el token "),_a=r("code"),qe=n("State"),ze=n(" es parte de la entidad "),ga=r("code"),Ce=n("Empire State Building"),Te=n(")."),ye=_(),Hs=r("li"),De=n("La letra "),ja=r("code"),Ae=n("O"),Pe=n("  indica que el token no corresponde a ninguna entidad."),Ra=_(),Cs=r("h2"),Os=r("a"),$a=r("span"),E(Ys.$$.fragment),Oe=_(),ka=r("span"),Le=n("Preprocesamiento"),Ua=_(),E(Ks.$$.fragment),Wa=_(),Ls=r("p"),Fe=n("Carga el tokenizador de DistilBERT para procesar los "),ba=r("code"),Se=n("tokens"),Ne=n(":"),Ha=_(),E(Vs.$$.fragment),Ya=_(),Fs=r("p"),Be=n("Dado que la entrada ya ha sido dividida en palabras, establece "),va=r("code"),Ie=n("is_split_into_words=True"),Me=n(" para tokenizar las palabras en subpalabras:"),Ka=_(),E(Zs.$$.fragment),Va=_(),Es=r("p"),Re=n("La incorporaci\xF3n de los tokens especiales "),wa=r("code"),Ue=n("[CLS]"),We=n(" y "),Ea=r("code"),He=n("[SEP]"),Ye=n(" y la tokenizaci\xF3n de subpalabras crea un desajuste entre la entrada y las etiquetas. Una sola palabra que corresponde a una sola etiqueta puede estar dividida en dos subpalabras. Tendr\xE1s que volver a alinear los tokens y las etiquetas:"),Za=_(),xs=r("ol"),Gs=r("li"),Ke=n("Asignar todos los tokens a la palabra correspondiente con el m\xE9todo "),Js=r("a"),xa=r("code"),Ve=n("word_ids"),Ze=n("."),Ge=_(),ks=r("li"),Je=n("Asignar la etiqueta "),qa=r("code"),Qe=n("-100"),Xe=n(" a los tokens especiales "),za=r("code"),st=n("[CLS]"),at=n(" y "),Ca=r("code"),et=n("[SEP]"),tt=n(" para que la funci\xF3n de perdida de PyTorch los ignore."),nt=_(),Qs=r("li"),lt=n("Etiquetar s\xF3lo el primer token de una palabra dada. Asignar "),Ta=r("code"),ot=n("-100"),rt=n(" a otros subtokens de la misma palabra."),Ga=_(),ia=r("p"),it=n("As\xED es como se puede crear una funci\xF3n para realinear los tokens y las etiquetas, y truncar las secuencias para que no sean m\xE1s largas que la longitud m\xE1xima de entrada de DistilBERT::"),Ja=_(),E(Xs.$$.fragment),Qa=_(),gs=r("p"),pt=n("Utiliza la funci\xF3n \u{1F917} Datasets "),sa=r("a"),ya=r("code"),ct=n("map"),dt=n(" para tokenizar y alinear las etiquetas en todo el conjunto de datos. Puedes acelerar la funci\xF3n "),Da=r("code"),ut=n("map"),mt=n(" estableciendo "),Aa=r("code"),ft=n("batched=True"),ht=n(" para procesar varios elementos del conjunto de datos a la vez:"),Xa=_(),E(aa.$$.fragment),se=_(),rs=r("p"),_t=n("Utiliza "),Pa=r("code"),gt=n("DataCollatorForTokenClassification"),jt=n(" para crear un conjunto de ejemplos. Tambi\xE9n "),Oa=r("em"),$t=n("rellenar\xE1 din\xE1micamente"),kt=n(" el texto y las etiquetas a la longitud del elemento m\xE1s largo del conjunto, para que tengan una longitud uniforme. Aunque es posible rellenar el texto con la funci\xF3n "),La=r("code"),bt=n("tokenizer"),vt=n(" estableciendo "),Fa=r("code"),wt=n("padding=True"),Et=n(", el relleno din\xE1mico es m\xE1s eficiente."),ae=_(),E(Ss.$$.fragment),ee=_(),Ts=r("h2"),Ns=r("a"),Sa=r("span"),E(ea.$$.fragment),xt=_(),Na=r("span"),qt=n("Entrenamiento"),te=_(),E(Bs.$$.fragment),ne=_(),E(Is.$$.fragment),this.h()},l(s){const u=bn('[data-svelte="svelte-1phssyn"]',document.head);t=i(u,"META",{name:!0,content:!0}),u.forEach(e),m=g(s),o=i(s,"H1",{class:!0});var ta=p(o);f=i(ta,"A",{id:!0,class:!0,href:!0});var Ba=p(f);$=i(Ba,"SPAN",{});var Ia=p($);x(j.$$.fragment,Ia),Ia.forEach(e),Ba.forEach(e),b=g(ta),y=i(ta,"SPAN",{});var Ma=p(y);D=l(Ma,"Clasificaci\xF3n de tokens"),Ma.forEach(e),ta.forEach(e),w=g(s),x(O.$$.fragment,s),R=g(s),G=i(s,"P",{});var zt=p(G);S=l(zt,"La clasificaci\xF3n de tokens asigna una etiqueta a los tokens individuales de una frase. Una de las tareas m\xE1s comunes de clasificaci\xF3n de tokens es el Reconocimiento de Entidades Nombradas (en ingl\xE9s, NER). El NER intenta encontrar una etiqueta para cada entidad en una frase, como persona, ubicaci\xF3n, u organizaci\xF3n."),zt.forEach(e),K=g(s),L=i(s,"P",{});var pa=p(L);ls=l(pa,"Esta gu\xEDa te mostrar\xE1 c\xF3mo afinar el modelo (en ingl\xE9s, finetunning) "),U=i(pa,"A",{href:!0,rel:!0});var Ct=p(U);js=l(Ct,"DistilBERT"),Ct.forEach(e),ms=l(pa," con el conjunto de datos "),F=i(pa,"A",{href:!0,rel:!0});var Tt=p(F);fs=l(Tt,"WNUT 17"),Tt.forEach(e),W=l(pa," para detectar nuevas entidades."),pa.forEach(e),hs=g(s),x(H.$$.fragment,s),_s=g(s),N=i(s,"H2",{class:!0});var oe=p(N);V=i(oe,"A",{id:!0,class:!0,href:!0});var yt=p(V);I=i(yt,"SPAN",{});var Dt=p(I);x(J.$$.fragment,Dt),Dt.forEach(e),yt.forEach(e),Q=g(oe),os=i(oe,"SPAN",{});var At=p(os);M=l(At,"Cargar el conjunto de datos WNUT 17"),At.forEach(e),oe.forEach(e),es=g(s),h=i(s,"P",{});var Pt=p(h);A=l(Pt,"Cargue el conjunto de datos WNUT 17 de la biblioteca \u{1F917} Datasets :"),Pt.forEach(e),ts=g(s),x(Y.$$.fragment,s),X=g(s),B=i(s,"P",{});var Ot=p(B);bs=l(Ot,"Entonces, observa un ejemplo:"),Ot.forEach(e),Z=g(s),x(ss.$$.fragment,s),is=g(s),ns=i(s,"P",{});var re=p(ns);ys=l(re,"Cada n\xFAmero en "),ps=i(re,"CODE",{});var Lt=p(ps);Ds=l(Lt,"ner_tags"),Lt.forEach(e),c=l(re," representa una entidad. Convierte el n\xFAmero en un nombre de etiqueta para obtener m\xE1s informaci\xF3n:"),re.forEach(e),v=g(s),x(cs.$$.fragment,s),Ws=g(s),ds=i(s,"P",{});var ca=p(ds);na=l(ca,"La etiqueta "),As=i(ca,"CODE",{});var Ft=p(As);Ps=l(Ft,"ner_tag"),Ft.forEach(e),la=l(ca," describe una entidad, como una empresa, un lugar, o una persona. La letra que precede a cada "),vs=i(ca,"CODE",{});var St=p(vs);oa=l(St,"ner_tag"),St.forEach(e),ra=l(ca," indica la posici\xF3n del token en la entidad:"),ca.forEach(e),zs=g(s),us=i(s,"UL",{});var da=p(us);ws=i(da,"LI",{});var ie=p(ws);$e=l(ie,"La letra "),fa=i(ie,"CODE",{});var Nt=p(fa);ke=l(Nt,"B-"),Nt.forEach(e),be=l(ie," indica el comienzo de la entidad."),ie.forEach(e),ve=g(da),$s=i(da,"LI",{});var Ms=p($s);we=l(Ms,"La letra "),ha=i(Ms,"CODE",{});var Bt=p(ha);Ee=l(Bt,"I-"),Bt.forEach(e),xe=l(Ms," indica que un token est\xE1 contenido dentro de la misma entidad (por ejemplo, el token "),_a=i(Ms,"CODE",{});var It=p(_a);qe=l(It,"State"),It.forEach(e),ze=l(Ms," es parte de la entidad "),ga=i(Ms,"CODE",{});var Mt=p(ga);Ce=l(Mt,"Empire State Building"),Mt.forEach(e),Te=l(Ms,")."),Ms.forEach(e),ye=g(da),Hs=i(da,"LI",{});var pe=p(Hs);De=l(pe,"La letra "),ja=i(pe,"CODE",{});var Rt=p(ja);Ae=l(Rt,"O"),Rt.forEach(e),Pe=l(pe,"  indica que el token no corresponde a ninguna entidad."),pe.forEach(e),da.forEach(e),Ra=g(s),Cs=i(s,"H2",{class:!0});var ce=p(Cs);Os=i(ce,"A",{id:!0,class:!0,href:!0});var Ut=p(Os);$a=i(Ut,"SPAN",{});var Wt=p($a);x(Ys.$$.fragment,Wt),Wt.forEach(e),Ut.forEach(e),Oe=g(ce),ka=i(ce,"SPAN",{});var Ht=p(ka);Le=l(Ht,"Preprocesamiento"),Ht.forEach(e),ce.forEach(e),Ua=g(s),x(Ks.$$.fragment,s),Wa=g(s),Ls=i(s,"P",{});var de=p(Ls);Fe=l(de,"Carga el tokenizador de DistilBERT para procesar los "),ba=i(de,"CODE",{});var Yt=p(ba);Se=l(Yt,"tokens"),Yt.forEach(e),Ne=l(de,":"),de.forEach(e),Ha=g(s),x(Vs.$$.fragment,s),Ya=g(s),Fs=i(s,"P",{});var ue=p(Fs);Be=l(ue,"Dado que la entrada ya ha sido dividida en palabras, establece "),va=i(ue,"CODE",{});var Kt=p(va);Ie=l(Kt,"is_split_into_words=True"),Kt.forEach(e),Me=l(ue," para tokenizar las palabras en subpalabras:"),ue.forEach(e),Ka=g(s),x(Zs.$$.fragment,s),Va=g(s),Es=i(s,"P",{});var ua=p(Es);Re=l(ua,"La incorporaci\xF3n de los tokens especiales "),wa=i(ua,"CODE",{});var Vt=p(wa);Ue=l(Vt,"[CLS]"),Vt.forEach(e),We=l(ua," y "),Ea=i(ua,"CODE",{});var Zt=p(Ea);He=l(Zt,"[SEP]"),Zt.forEach(e),Ye=l(ua," y la tokenizaci\xF3n de subpalabras crea un desajuste entre la entrada y las etiquetas. Una sola palabra que corresponde a una sola etiqueta puede estar dividida en dos subpalabras. Tendr\xE1s que volver a alinear los tokens y las etiquetas:"),ua.forEach(e),Za=g(s),xs=i(s,"OL",{});var ma=p(xs);Gs=i(ma,"LI",{});var me=p(Gs);Ke=l(me,"Asignar todos los tokens a la palabra correspondiente con el m\xE9todo "),Js=i(me,"A",{href:!0,rel:!0});var Gt=p(Js);xa=i(Gt,"CODE",{});var Jt=p(xa);Ve=l(Jt,"word_ids"),Jt.forEach(e),Gt.forEach(e),Ze=l(me,"."),me.forEach(e),Ge=g(ma),ks=i(ma,"LI",{});var Rs=p(ks);Je=l(Rs,"Asignar la etiqueta "),qa=i(Rs,"CODE",{});var Qt=p(qa);Qe=l(Qt,"-100"),Qt.forEach(e),Xe=l(Rs," a los tokens especiales "),za=i(Rs,"CODE",{});var Xt=p(za);st=l(Xt,"[CLS]"),Xt.forEach(e),at=l(Rs," y "),Ca=i(Rs,"CODE",{});var sn=p(Ca);et=l(sn,"[SEP]"),sn.forEach(e),tt=l(Rs," para que la funci\xF3n de perdida de PyTorch los ignore."),Rs.forEach(e),nt=g(ma),Qs=i(ma,"LI",{});var fe=p(Qs);lt=l(fe,"Etiquetar s\xF3lo el primer token de una palabra dada. Asignar "),Ta=i(fe,"CODE",{});var an=p(Ta);ot=l(an,"-100"),an.forEach(e),rt=l(fe," a otros subtokens de la misma palabra."),fe.forEach(e),ma.forEach(e),Ga=g(s),ia=i(s,"P",{});var en=p(ia);it=l(en,"As\xED es como se puede crear una funci\xF3n para realinear los tokens y las etiquetas, y truncar las secuencias para que no sean m\xE1s largas que la longitud m\xE1xima de entrada de DistilBERT::"),en.forEach(e),Ja=g(s),x(Xs.$$.fragment,s),Qa=g(s),gs=i(s,"P",{});var Us=p(gs);pt=l(Us,"Utiliza la funci\xF3n \u{1F917} Datasets "),sa=i(Us,"A",{href:!0,rel:!0});var tn=p(sa);ya=i(tn,"CODE",{});var nn=p(ya);ct=l(nn,"map"),nn.forEach(e),tn.forEach(e),dt=l(Us," para tokenizar y alinear las etiquetas en todo el conjunto de datos. Puedes acelerar la funci\xF3n "),Da=i(Us,"CODE",{});var ln=p(Da);ut=l(ln,"map"),ln.forEach(e),mt=l(Us," estableciendo "),Aa=i(Us,"CODE",{});var on=p(Aa);ft=l(on,"batched=True"),on.forEach(e),ht=l(Us," para procesar varios elementos del conjunto de datos a la vez:"),Us.forEach(e),Xa=g(s),x(aa.$$.fragment,s),se=g(s),rs=i(s,"P",{});var qs=p(rs);_t=l(qs,"Utiliza "),Pa=i(qs,"CODE",{});var rn=p(Pa);gt=l(rn,"DataCollatorForTokenClassification"),rn.forEach(e),jt=l(qs," para crear un conjunto de ejemplos. Tambi\xE9n "),Oa=i(qs,"EM",{});var pn=p(Oa);$t=l(pn,"rellenar\xE1 din\xE1micamente"),pn.forEach(e),kt=l(qs," el texto y las etiquetas a la longitud del elemento m\xE1s largo del conjunto, para que tengan una longitud uniforme. Aunque es posible rellenar el texto con la funci\xF3n "),La=i(qs,"CODE",{});var cn=p(La);bt=l(cn,"tokenizer"),cn.forEach(e),vt=l(qs," estableciendo "),Fa=i(qs,"CODE",{});var dn=p(Fa);wt=l(dn,"padding=True"),dn.forEach(e),Et=l(qs,", el relleno din\xE1mico es m\xE1s eficiente."),qs.forEach(e),ae=g(s),x(Ss.$$.fragment,s),ee=g(s),Ts=i(s,"H2",{class:!0});var he=p(Ts);Ns=i(he,"A",{id:!0,class:!0,href:!0});var un=p(Ns);Sa=i(un,"SPAN",{});var mn=p(Sa);x(ea.$$.fragment,mn),mn.forEach(e),un.forEach(e),xt=g(he),Na=i(he,"SPAN",{});var fn=p(Na);qt=l(fn,"Entrenamiento"),fn.forEach(e),he.forEach(e),te=g(s),x(Bs.$$.fragment,s),ne=g(s),x(Is.$$.fragment,s),this.h()},h(){k(t,"name","hf:doc:metadata"),k(t,"content",JSON.stringify(Fn)),k(f,"id","clasificacin-de-tokens"),k(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(f,"href","#clasificacin-de-tokens"),k(o,"class","relative group"),k(U,"href","https://huggingface.co/distilbert-base-uncased"),k(U,"rel","nofollow"),k(F,"href","https://huggingface.co/datasets/wnut_17"),k(F,"rel","nofollow"),k(V,"id","cargar-el-conjunto-de-datos-wnut-17"),k(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(V,"href","#cargar-el-conjunto-de-datos-wnut-17"),k(N,"class","relative group"),k(Os,"id","preprocesamiento"),k(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Os,"href","#preprocesamiento"),k(Cs,"class","relative group"),k(Js,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.Encoding.word_ids"),k(Js,"rel","nofollow"),k(sa,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),k(sa,"rel","nofollow"),k(Ns,"id","entrenamiento"),k(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Ns,"href","#entrenamiento"),k(Ts,"class","relative group")},m(s,u){a(document.head,t),d(s,m,u),d(s,o,u),a(o,f),a(f,$),q(j,$,null),a(o,b),a(o,y),a(y,D),d(s,w,u),q(O,s,u),d(s,R,u),d(s,G,u),a(G,S),d(s,K,u),d(s,L,u),a(L,ls),a(L,U),a(U,js),a(L,ms),a(L,F),a(F,fs),a(L,W),d(s,hs,u),q(H,s,u),d(s,_s,u),d(s,N,u),a(N,V),a(V,I),q(J,I,null),a(N,Q),a(N,os),a(os,M),d(s,es,u),d(s,h,u),a(h,A),d(s,ts,u),q(Y,s,u),d(s,X,u),d(s,B,u),a(B,bs),d(s,Z,u),q(ss,s,u),d(s,is,u),d(s,ns,u),a(ns,ys),a(ns,ps),a(ps,Ds),a(ns,c),d(s,v,u),q(cs,s,u),d(s,Ws,u),d(s,ds,u),a(ds,na),a(ds,As),a(As,Ps),a(ds,la),a(ds,vs),a(vs,oa),a(ds,ra),d(s,zs,u),d(s,us,u),a(us,ws),a(ws,$e),a(ws,fa),a(fa,ke),a(ws,be),a(us,ve),a(us,$s),a($s,we),a($s,ha),a(ha,Ee),a($s,xe),a($s,_a),a(_a,qe),a($s,ze),a($s,ga),a(ga,Ce),a($s,Te),a(us,ye),a(us,Hs),a(Hs,De),a(Hs,ja),a(ja,Ae),a(Hs,Pe),d(s,Ra,u),d(s,Cs,u),a(Cs,Os),a(Os,$a),q(Ys,$a,null),a(Cs,Oe),a(Cs,ka),a(ka,Le),d(s,Ua,u),q(Ks,s,u),d(s,Wa,u),d(s,Ls,u),a(Ls,Fe),a(Ls,ba),a(ba,Se),a(Ls,Ne),d(s,Ha,u),q(Vs,s,u),d(s,Ya,u),d(s,Fs,u),a(Fs,Be),a(Fs,va),a(va,Ie),a(Fs,Me),d(s,Ka,u),q(Zs,s,u),d(s,Va,u),d(s,Es,u),a(Es,Re),a(Es,wa),a(wa,Ue),a(Es,We),a(Es,Ea),a(Ea,He),a(Es,Ye),d(s,Za,u),d(s,xs,u),a(xs,Gs),a(Gs,Ke),a(Gs,Js),a(Js,xa),a(xa,Ve),a(Gs,Ze),a(xs,Ge),a(xs,ks),a(ks,Je),a(ks,qa),a(qa,Qe),a(ks,Xe),a(ks,za),a(za,st),a(ks,at),a(ks,Ca),a(Ca,et),a(ks,tt),a(xs,nt),a(xs,Qs),a(Qs,lt),a(Qs,Ta),a(Ta,ot),a(Qs,rt),d(s,Ga,u),d(s,ia,u),a(ia,it),d(s,Ja,u),q(Xs,s,u),d(s,Qa,u),d(s,gs,u),a(gs,pt),a(gs,sa),a(sa,ya),a(ya,ct),a(gs,dt),a(gs,Da),a(Da,ut),a(gs,mt),a(gs,Aa),a(Aa,ft),a(gs,ht),d(s,Xa,u),q(aa,s,u),d(s,se,u),d(s,rs,u),a(rs,_t),a(rs,Pa),a(Pa,gt),a(rs,jt),a(rs,Oa),a(Oa,$t),a(rs,kt),a(rs,La),a(La,bt),a(rs,vt),a(rs,Fa),a(Fa,wt),a(rs,Et),d(s,ae,u),q(Ss,s,u),d(s,ee,u),d(s,Ts,u),a(Ts,Ns),a(Ns,Sa),q(ea,Sa,null),a(Ts,xt),a(Ts,Na),a(Na,qt),d(s,te,u),q(Bs,s,u),d(s,ne,u),q(Is,s,u),le=!0},p(s,[u]){const ta={};u&2&&(ta.$$scope={dirty:u,ctx:s}),H.$set(ta);const Ba={};u&2&&(Ba.$$scope={dirty:u,ctx:s}),Ss.$set(Ba);const Ia={};u&2&&(Ia.$$scope={dirty:u,ctx:s}),Bs.$set(Ia);const Ma={};u&2&&(Ma.$$scope={dirty:u,ctx:s}),Is.$set(Ma)},i(s){le||(z(j.$$.fragment,s),z(O.$$.fragment,s),z(H.$$.fragment,s),z(J.$$.fragment,s),z(Y.$$.fragment,s),z(ss.$$.fragment,s),z(cs.$$.fragment,s),z(Ys.$$.fragment,s),z(Ks.$$.fragment,s),z(Vs.$$.fragment,s),z(Zs.$$.fragment,s),z(Xs.$$.fragment,s),z(aa.$$.fragment,s),z(Ss.$$.fragment,s),z(ea.$$.fragment,s),z(Bs.$$.fragment,s),z(Is.$$.fragment,s),le=!0)},o(s){C(j.$$.fragment,s),C(O.$$.fragment,s),C(H.$$.fragment,s),C(J.$$.fragment,s),C(Y.$$.fragment,s),C(ss.$$.fragment,s),C(cs.$$.fragment,s),C(Ys.$$.fragment,s),C(Ks.$$.fragment,s),C(Vs.$$.fragment,s),C(Zs.$$.fragment,s),C(Xs.$$.fragment,s),C(aa.$$.fragment,s),C(Ss.$$.fragment,s),C(ea.$$.fragment,s),C(Bs.$$.fragment,s),C(Is.$$.fragment,s),le=!1},d(s){e(t),s&&e(m),s&&e(o),T(j),s&&e(w),T(O,s),s&&e(R),s&&e(G),s&&e(K),s&&e(L),s&&e(hs),T(H,s),s&&e(_s),s&&e(N),T(J),s&&e(es),s&&e(h),s&&e(ts),T(Y,s),s&&e(X),s&&e(B),s&&e(Z),T(ss,s),s&&e(is),s&&e(ns),s&&e(v),T(cs,s),s&&e(Ws),s&&e(ds),s&&e(zs),s&&e(us),s&&e(Ra),s&&e(Cs),T(Ys),s&&e(Ua),T(Ks,s),s&&e(Wa),s&&e(Ls),s&&e(Ha),T(Vs,s),s&&e(Ya),s&&e(Fs),s&&e(Ka),T(Zs,s),s&&e(Va),s&&e(Es),s&&e(Za),s&&e(xs),s&&e(Ga),s&&e(ia),s&&e(Ja),T(Xs,s),s&&e(Qa),s&&e(gs),s&&e(Xa),T(aa,s),s&&e(se),s&&e(rs),s&&e(ae),T(Ss,s),s&&e(ee),s&&e(Ts),T(ea),s&&e(te),T(Bs,s),s&&e(ne),T(Is,s)}}}const Fn={local:"clasificacin-de-tokens",sections:[{local:"cargar-el-conjunto-de-datos-wnut-17",title:"Cargar el conjunto de datos WNUT 17"},{local:"preprocesamiento",title:"Preprocesamiento"},{local:"entrenamiento",title:"Entrenamiento"}],title:"Clasificaci\xF3n de tokens"};function Sn(P){return vn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hn extends jn{constructor(t){super();$n(this,t,Sn,Ln,kn,{})}}export{Hn as default,Fn as metadata};
