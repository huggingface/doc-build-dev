import{S as Di,i as yi,s as Ei,e as r,k as p,w as b,t as s,M as Fi,c as a,d as o,m,a as n,x as $,h as i,b as c,N as ki,G as e,g as _,y as P,q as w,o as x,B as D,v as ji,L as zr}from"../../chunks/vendor-hf-doc-builder.js";import{T as It}from"../../chunks/Tip-hf-doc-builder.js";import{D as F}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ar}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as H}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Cr}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Mi(y){let d,T,u,g,v;return g=new Ar({props:{code:`from transformers import DPTModel, DPTConfig

# Initializing a DPT dpt-large style configuration
configuration = DPTConfig()

# Initializing a model from the dpt-large style configuration
model = DPTModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPTModel, DPTConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DPT dpt-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the dpt-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=r("p"),T=s("Example:"),u=p(),b(g.$$.fragment)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Example:"),f.forEach(o),u=m(l),$(g.$$.fragment,l)},m(l,f){_(l,d,f),e(d,T),_(l,u,f),P(g,l,f),v=!0},p:zr,i(l){v||(w(g.$$.fragment,l),v=!0)},o(l){x(g.$$.fragment,l),v=!1},d(l){l&&o(d),l&&o(u),D(g,l)}}}function Si(y){let d,T;return{c(){d=r("p"),T=s(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(u){d=a(u,"P",{});var g=n(d);T=i(g,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),g.forEach(o)},m(u,g){_(u,d,g),e(d,T)},d(u){u&&o(d)}}}function Ii(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function Ci(y){let d,T,u,g,v;return g=new Ar({props:{code:`from transformers import DPTFeatureExtractor, DPTModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-large")
model = DPTModel.from_pretrained("Intel/dpt-large")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPTFeatureExtractor, DPTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = DPTFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPTModel.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">577</span>, <span class="hljs-number">1024</span>]`}}),{c(){d=r("p"),T=s("Example:"),u=p(),b(g.$$.fragment)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Example:"),f.forEach(o),u=m(l),$(g.$$.fragment,l)},m(l,f){_(l,d,f),e(d,T),_(l,u,f),P(g,l,f),v=!0},p:zr,i(l){v||(w(g.$$.fragment,l),v=!0)},o(l){x(g.$$.fragment,l),v=!1},d(l){l&&o(d),l&&o(u),D(g,l)}}}function zi(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function Ai(y){let d,T,u,g,v;return g=new Ar({props:{code:`from transformers import DPTFeatureExtractor, DPTForDepthEstimation
import torch
import numpy as np
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-large")
model = DPTForDepthEstimation.from_pretrained("Intel/dpt-large")

# prepare image for the model
inputs = feature_extractor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    predicted_depth = outputs.predicted_depth

# interpolate to original size
prediction = torch.nn.functional.interpolate(
    predicted_depth.unsqueeze(1),
    size=image.size[::-1],
    mode="bicubic",
    align_corners=False,
)

# visualize the prediction
output = prediction.squeeze().cpu().numpy()
formatted = (output * 255 / np.max(output)).astype("uint8")
depth = Image.fromarray(formatted)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPTFeatureExtractor, DPTForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = DPTFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPTForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">... </span>    predicted_depth = outputs.predicted_depth

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prediction = torch.nn.functional.interpolate(
<span class="hljs-meta">... </span>    predicted_depth.unsqueeze(<span class="hljs-number">1</span>),
<span class="hljs-meta">... </span>    size=image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>    mode=<span class="hljs-string">&quot;bicubic&quot;</span>,
<span class="hljs-meta">... </span>    align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># visualize the prediction</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>output = prediction.squeeze().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>formatted = (output * <span class="hljs-number">255</span> / np.<span class="hljs-built_in">max</span>(output)).astype(<span class="hljs-string">&quot;uint8&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(formatted)`}}),{c(){d=r("p"),T=s("Examples:"),u=p(),b(g.$$.fragment)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Examples:"),f.forEach(o),u=m(l),$(g.$$.fragment,l)},m(l,f){_(l,d,f),e(d,T),_(l,u,f),P(g,l,f),v=!0},p:zr,i(l){v||(w(g.$$.fragment,l),v=!0)},o(l){x(g.$$.fragment,l),v=!1},d(l){l&&o(d),l&&o(u),D(g,l)}}}function Ni(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function qi(y){let d,T,u,g,v;return g=new Ar({props:{code:`from transformers import DPTFeatureExtractor, DPTForSemanticSegmentation
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-large-ade")
model = DPTForSemanticSegmentation.from_pretrained("Intel/dpt-large-ade")

inputs = feature_extractor(images=image, return_tensors="pt")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPTFeatureExtractor, DPTForSemanticSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = DPTFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large-ade&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPTForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;Intel/dpt-large-ade&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),T=s("Examples:"),u=p(),b(g.$$.fragment)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Examples:"),f.forEach(o),u=m(l),$(g.$$.fragment,l)},m(l,f){_(l,d,f),e(d,T),_(l,u,f),P(g,l,f),v=!0},p:zr,i(l){v||(w(g.$$.fragment,l),v=!0)},o(l){x(g.$$.fragment,l),v=!1},d(l){l&&o(d),l&&o(u),D(g,l)}}}function Oi(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function Li(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function Vi(y){let d,T,u,g,v;return{c(){d=r("p"),T=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var f=n(d);T=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(f,"CODE",{});var E=n(u);g=i(E,"Module"),E.forEach(o),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(l,f){_(l,d,f),e(d,T),e(d,u),e(u,g),e(d,v)},d(l){l&&o(d)}}}function Wi(y){let d,T,u,g,v,l,f,E,Nr,Go,K,pe,ao,Ue,qr,no,Or,Yo,L,Lr,Re,Vr,Wr,Ct,Ur,Rr,Jo,zt,Br,Xo,At,so,Hr,Zo,me,vs,Qo,he,Kr,Be,Gr,Yr,er,V,Jr,He,Xr,Zr,Ke,Qr,ea,tr,G,fe,io,Ge,ta,lo,oa,or,k,Ye,ra,Y,aa,Nt,na,sa,Je,ia,la,da,J,ca,qt,pa,ma,Ot,ha,fa,ua,ue,rr,X,ge,co,Xe,ga,po,_a,ar,j,Ze,va,mo,Ta,ba,Qe,$a,Lt,Pa,wa,xa,W,et,Da,ho,ya,Ea,_e,nr,Z,ve,fo,tt,Fa,uo,ka,sr,O,ot,ja,rt,Ma,at,Sa,Ia,Ca,A,nt,za,Q,Aa,Vt,Na,qa,go,Oa,La,Va,Te,Wa,be,ir,ee,$e,_o,st,Ua,vo,Ra,lr,M,it,Ba,To,Ha,Ka,lt,Ga,dt,Ya,Ja,Xa,N,ct,Za,te,Qa,Wt,en,tn,bo,on,rn,an,Pe,nn,we,dr,oe,xe,$o,pt,sn,Po,ln,cr,S,mt,dn,wo,cn,pn,ht,mn,ft,hn,fn,un,q,ut,gn,re,_n,Ut,vn,Tn,xo,bn,$n,Pn,De,wn,ye,pr,ae,Ee,Do,gt,xn,yo,Dn,mr,I,_t,yn,Eo,En,Fn,vt,kn,Tt,jn,Mn,Sn,U,bt,In,ne,Cn,Fo,zn,An,ko,Nn,qn,On,Fe,hr,se,ke,jo,$t,Ln,Mo,Vn,fr,C,Pt,Wn,So,Un,Rn,wt,Bn,xt,Hn,Kn,Gn,R,Dt,Yn,ie,Jn,Io,Xn,Zn,Co,Qn,es,ts,je,ur,le,Me,zo,yt,os,Ao,rs,gr,z,Et,as,No,ns,ss,Ft,is,kt,ls,ds,cs,B,jt,ps,de,ms,qo,hs,fs,Oo,us,gs,_s,Se,_r;return l=new H({}),Ue=new H({}),Ge=new H({}),Ye=new F({props:{name:"class transformers.DPTConfig",anchor:"transformers.DPTConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 384"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"backbone_out_indices",val:" = [2, 5, 8, 11]"},{name:"readout_type",val:" = 'project'"},{name:"reassemble_factors",val:" = [4, 2, 1, 0.5]"},{name:"neck_hidden_sizes",val:" = [96, 192, 384, 768]"},{name:"fusion_hidden_size",val:" = 256"},{name:"head_in_index",val:" = -1"},{name:"use_batch_norm_in_fusion_residual",val:" = False"},{name:"use_auxiliary_head",val:" = True"},{name:"auxiliary_loss_weight",val:" = 0.4"},{name:"semantic_loss_ignore_index",val:" = 255"},{name:"semantic_classifier_dropout",val:" = 0.1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPTConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPTConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPTConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPTConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPTConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPTConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPTConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPTConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPTConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.DPTConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.DPTConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.DPTConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.DPTConfig.backbone_out_indices",description:`<strong>backbone_out_indices</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 5, 8, 11]</code>) &#x2014;
Indices of the intermediate hidden states to use from backbone.`,name:"backbone_out_indices"},{anchor:"transformers.DPTConfig.readout_type",description:`<strong>readout_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;project&quot;</code>) &#x2014;
The readout type to use when processing the readout token (CLS token) of the intermediate hidden states of
the ViT backbone. Can be one of [<code>&quot;ignore&quot;</code>, <code>&quot;add&quot;</code>, <code>&quot;project&quot;</code>].</p>
<ul>
<li>&#x201C;ignore&#x201D; simply ignores the CLS token.</li>
<li>&#x201C;add&#x201D; passes the information from the CLS token to all other tokens by adding the representations.</li>
<li>&#x201C;project&#x201D; passes information to the other tokens by concatenating the readout to all other tokens before
projecting the
representation to the original feature dimension D using a linear layer followed by a GELU non-linearity.</li>
</ul>`,name:"readout_type"},{anchor:"transformers.DPTConfig.reassemble_factors",description:`<strong>reassemble_factors</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 1, 0.5]</code>) &#x2014;
The up/downsampling factors of the reassemble layers.`,name:"reassemble_factors"},{anchor:"transformers.DPTConfig.neck_hidden_sizes",description:`<strong>neck_hidden_sizes</strong> (<code>List[str]</code>, <em>optional</em>, defaults to [96, 192, 384, 768]) &#x2014;
The hidden sizes to project to for the feature maps of the backbone.`,name:"neck_hidden_sizes"},{anchor:"transformers.DPTConfig.fusion_hidden_size",description:`<strong>fusion_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The number of channels before fusion.`,name:"fusion_hidden_size"},{anchor:"transformers.DPTConfig.head_in_index",description:`<strong>head_in_index</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the features to use in the heads.`,name:"head_in_index"},{anchor:"transformers.DPTConfig.use_batch_norm_in_fusion_residual",description:`<strong>use_batch_norm_in_fusion_residual</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use batch normalization in the pre-activate residual units of the fusion blocks.`,name:"use_batch_norm_in_fusion_residual"},{anchor:"transformers.DPTConfig.use_auxiliary_head",description:`<strong>use_auxiliary_head</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an auxiliary head during training.`,name:"use_auxiliary_head"},{anchor:"transformers.DPTConfig.auxiliary_loss_weight",description:`<strong>auxiliary_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Weight of the cross-entropy loss of the auxiliary head.`,name:"auxiliary_loss_weight"},{anchor:"transformers.DPTConfig.semantic_loss_ignore_index",description:`<strong>semantic_loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function of the semantic segmentation model.`,name:"semantic_loss_ignore_index"},{anchor:"transformers.DPTConfig.semantic_classifier_dropout",description:`<strong>semantic_classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the semantic classification head.`,name:"semantic_classifier_dropout"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/configuration_dpt.py#L29"}}),ue=new Cr({props:{anchor:"transformers.DPTConfig.example",$$slots:{default:[Mi]},$$scope:{ctx:y}}}),Xe=new H({}),Ze=new F({props:{name:"class transformers.DPTFeatureExtractor",anchor:"transformers.DPTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 384"},{name:"keep_aspect_ratio",val:" = False"},{name:"ensure_multiple_of",val:" = 1"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.DPTFeatureExtractor.size",description:`<strong>size</strong> (&#x2018;int&#x2019; or <code>Tuple(int)</code>, <em>optional</em>, defaults to 384) &#x2014;
Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an
integer is provided, then the input will be resized to (size, size). Only has an effect if <code>do_resize</code> is
set to <code>True</code>.`,name:"size"},{anchor:"transformers.DPTFeatureExtractor.ensure_multiple_of",description:`<strong>ensure_multiple_of</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Ensure that the input is resized to a multiple of this value. Only has an effect if <code>do_resize</code> is set to
<code>True</code>.`,name:"ensure_multiple_of"},{anchor:"transformers.DPTFeatureExtractor.keep_aspect_ratio",description:`<strong>keep_aspect_ratio</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to keep the aspect ratio of the input. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"keep_aspect_ratio"},{anchor:"transformers.DPTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.DPTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with mean and standard deviation.`,name:"do_normalize"},{anchor:"transformers.DPTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.DPTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/feature_extraction_dpt.py#L37"}}),et=new F({props:{name:"__call__",anchor:"transformers.DPTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.DPTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17770/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/feature_extraction_dpt.py#L133",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17770/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model, of shape (batch_size, num_channels, height,
width).</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17770/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),_e=new It({props:{warning:!0,$$slots:{default:[Si]},$$scope:{ctx:y}}}),tt=new H({}),ot=new F({props:{name:"class transformers.DPTModel",anchor:"transformers.DPTModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.DPTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L682"}}),nt=new F({props:{name:"forward",anchor:"transformers.DPTModel.forward",parameters:[{name:"pixel_values",val:""},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DPTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DPTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L708",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig"
>DPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Te=new It({props:{$$slots:{default:[Ii]},$$scope:{ctx:y}}}),be=new Cr({props:{anchor:"transformers.DPTModel.forward.example",$$slots:{default:[Ci]},$$scope:{ctx:y}}}),st=new H({}),it=new F({props:{name:"class transformers.DPTForDepthEstimation",anchor:"transformers.DPTForDepthEstimation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.DPTForDepthEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L863"}}),ct=new F({props:{name:"forward",anchor:"transformers.DPTForDepthEstimation.forward",parameters:[{name:"pixel_values",val:""},{name:"head_mask",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPTForDepthEstimation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DPTForDepthEstimation.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DPTForDepthEstimation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPTForDepthEstimation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPTForDepthEstimation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DPTForDepthEstimation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth depth estimation maps for computing the loss.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L878",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig"
>DPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>) \u2014 Predicted depth for each pixel.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Pe=new It({props:{$$slots:{default:[zi]},$$scope:{ctx:y}}}),we=new Cr({props:{anchor:"transformers.DPTForDepthEstimation.forward.example",$$slots:{default:[Ai]},$$scope:{ctx:y}}}),pt=new H({}),mt=new F({props:{name:"class transformers.DPTForSemanticSegmentation",anchor:"transformers.DPTForSemanticSegmentation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.DPTForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L1023"}}),ut=new F({props:{name:"forward",anchor:"transformers.DPTForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"head_mask",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPTForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DPTForSemanticSegmentation.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DPTForSemanticSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPTForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPTForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DPTForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_dpt.py#L1039",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig"
>DPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) \u2014 Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17770/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new It({props:{$$slots:{default:[Ni]},$$scope:{ctx:y}}}),ye=new Cr({props:{anchor:"transformers.DPTForSemanticSegmentation.forward.example",$$slots:{default:[qi]},$$scope:{ctx:y}}}),gt=new H({}),_t=new F({props:{name:"class transformers.FlaxDPTForSemanticSegmentation",anchor:"transformers.FlaxDPTForSemanticSegmentation",parameters:[{name:"config",val:": DPTConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDPTForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L1216"}}),bt=new F({props:{name:"__call__",anchor:"transformers.FlaxDPTForSemanticSegmentation.__call__",parameters:[{name:"pixel_values",val:""},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"}],parametersDescription:[{anchor:"transformers.FlaxDPTForSemanticSegmentation.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jax.numpy.array</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxDPTForSemanticSegmentation.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDPTForSemanticSegmentation.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDPTForSemanticSegmentation.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L701"}}),Fe=new It({props:{$$slots:{default:[Oi]},$$scope:{ctx:y}}}),$t=new H({}),Pt=new F({props:{name:"class transformers.FlaxDPTForDepthEstimation",anchor:"transformers.FlaxDPTForDepthEstimation",parameters:[{name:"config",val:": DPTConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDPTForDepthEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L1016"}}),Dt=new F({props:{name:"__call__",anchor:"transformers.FlaxDPTForDepthEstimation.__call__",parameters:[{name:"pixel_values",val:""},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"}],parametersDescription:[{anchor:"transformers.FlaxDPTForDepthEstimation.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jax.numpy.array</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxDPTForDepthEstimation.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDPTForDepthEstimation.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDPTForDepthEstimation.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L701"}}),je=new It({props:{$$slots:{default:[Li]},$$scope:{ctx:y}}}),yt=new H({}),Et=new F({props:{name:"class transformers.FlaxDPTModel",anchor:"transformers.FlaxDPTModel",parameters:[{name:"config",val:": DPTConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDPTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L792"}}),jt=new F({props:{name:"__call__",anchor:"transformers.FlaxDPTModel.__call__",parameters:[{name:"pixel_values",val:""},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"}],parametersDescription:[{anchor:"transformers.FlaxDPTModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jax.numpy.array</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See
<a href="/docs/transformers/pr_17770/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxDPTModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDPTModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDPTModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17770/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17770/src/transformers/models/dpt/modeling_flax_dpt.py#L701"}}),Se=new It({props:{$$slots:{default:[Vi]},$$scope:{ctx:y}}}),{c(){d=r("meta"),T=p(),u=r("h1"),g=r("a"),v=r("span"),b(l.$$.fragment),f=p(),E=r("span"),Nr=s("DPT"),Go=p(),K=r("h2"),pe=r("a"),ao=r("span"),b(Ue.$$.fragment),qr=p(),no=r("span"),Or=s("Overview"),Yo=p(),L=r("p"),Lr=s("The DPT model was proposed in "),Re=r("a"),Vr=s("Vision Transformers for Dense Prediction"),Wr=s(` by Ren\xE9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.
DPT is a model that leverages the `),Ct=r("a"),Ur=s("Vision Transformer (ViT)"),Rr=s(" as backbone for dense prediction tasks like semantic segmentation and depth estimation."),Jo=p(),zt=r("p"),Br=s("The abstract from the paper is the following:"),Xo=p(),At=r("p"),so=r("em"),Hr=s("We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art."),Zo=p(),me=r("img"),Qo=p(),he=r("small"),Kr=s("DPT architecture. Taken from the "),Be=r("a"),Gr=s("original paper"),Yr=s("."),er=p(),V=r("p"),Jr=s("This model was contributed by "),He=r("a"),Xr=s("nielsr"),Zr=s(". The original code can be found "),Ke=r("a"),Qr=s("here"),ea=s("."),tr=p(),G=r("h2"),fe=r("a"),io=r("span"),b(Ge.$$.fragment),ta=p(),lo=r("span"),oa=s("DPTConfig"),or=p(),k=r("div"),b(Ye.$$.fragment),ra=p(),Y=r("p"),aa=s("This is the configuration class to store the configuration of a "),Nt=r("a"),na=s("DPTModel"),sa=s(`. It is used to instantiate an DPT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the DPT
`),Je=r("a"),ia=s("Intel/dpt-large"),la=s(" architecture."),da=p(),J=r("p"),ca=s("Configuration objects inherit from "),qt=r("a"),pa=s("PretrainedConfig"),ma=s(` and can be used to control the model outputs. Read the
documentation from `),Ot=r("a"),ha=s("PretrainedConfig"),fa=s(" for more information."),ua=p(),b(ue.$$.fragment),rr=p(),X=r("h2"),ge=r("a"),co=r("span"),b(Xe.$$.fragment),ga=p(),po=r("span"),_a=s("DPTFeatureExtractor"),ar=p(),j=r("div"),b(Ze.$$.fragment),va=p(),mo=r("p"),Ta=s("Constructs a DPT feature extractor."),ba=p(),Qe=r("p"),$a=s("This feature extractor inherits from "),Lt=r("a"),Pa=s("FeatureExtractionMixin"),wa=s(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),xa=p(),W=r("div"),b(et.$$.fragment),Da=p(),ho=r("p"),ya=s("Main method to prepare for the model one or several image(s)."),Ea=p(),b(_e.$$.fragment),nr=p(),Z=r("h2"),ve=r("a"),fo=r("span"),b(tt.$$.fragment),Fa=p(),uo=r("span"),ka=s("DPTModel"),sr=p(),O=r("div"),b(ot.$$.fragment),ja=p(),rt=r("p"),Ma=s(`The bare DPT Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),at=r("a"),Sa=s("torch.nn.Module"),Ia=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ca=p(),A=r("div"),b(nt.$$.fragment),za=p(),Q=r("p"),Aa=s("The "),Vt=r("a"),Na=s("DPTModel"),qa=s(" forward method, overrides the "),go=r("code"),Oa=s("__call__"),La=s(" special method."),Va=p(),b(Te.$$.fragment),Wa=p(),b(be.$$.fragment),ir=p(),ee=r("h2"),$e=r("a"),_o=r("span"),b(st.$$.fragment),Ua=p(),vo=r("span"),Ra=s("DPTForDepthEstimation"),lr=p(),M=r("div"),b(it.$$.fragment),Ba=p(),To=r("p"),Ha=s("DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2."),Ka=p(),lt=r("p"),Ga=s("This model is a PyTorch "),dt=r("a"),Ya=s("torch.nn.Module"),Ja=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Xa=p(),N=r("div"),b(ct.$$.fragment),Za=p(),te=r("p"),Qa=s("The "),Wt=r("a"),en=s("DPTForDepthEstimation"),tn=s(" forward method, overrides the "),bo=r("code"),on=s("__call__"),rn=s(" special method."),an=p(),b(Pe.$$.fragment),nn=p(),b(we.$$.fragment),dr=p(),oe=r("h2"),xe=r("a"),$o=r("span"),b(pt.$$.fragment),sn=p(),Po=r("span"),ln=s("DPTForSemanticSegmentation"),cr=p(),S=r("div"),b(mt.$$.fragment),dn=p(),wo=r("p"),cn=s("DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),pn=p(),ht=r("p"),mn=s("This model is a PyTorch "),ft=r("a"),hn=s("torch.nn.Module"),fn=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),un=p(),q=r("div"),b(ut.$$.fragment),gn=p(),re=r("p"),_n=s("The "),Ut=r("a"),vn=s("DPTForSemanticSegmentation"),Tn=s(" forward method, overrides the "),xo=r("code"),bn=s("__call__"),$n=s(" special method."),Pn=p(),b(De.$$.fragment),wn=p(),b(ye.$$.fragment),pr=p(),ae=r("h2"),Ee=r("a"),Do=r("span"),b(gt.$$.fragment),xn=p(),yo=r("span"),Dn=s("FlaxDPTForSemanticSegmentation"),mr=p(),I=r("div"),b(_t.$$.fragment),yn=p(),Eo=r("p"),En=s("DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),Fn=p(),vt=r("p"),kn=s("This model is a Flax "),Tt=r("a"),jn=s("jax.nn.Module"),Mn=s(`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),Sn=p(),U=r("div"),b(bt.$$.fragment),In=p(),ne=r("p"),Cn=s("The "),Fo=r("code"),zn=s("FlaxDPTPreTrainedModel"),An=s(" forward method, overrides the "),ko=r("code"),Nn=s("__call__"),qn=s(" special method."),On=p(),b(Fe.$$.fragment),hr=p(),se=r("h2"),ke=r("a"),jo=r("span"),b($t.$$.fragment),Ln=p(),Mo=r("span"),Vn=s("FlaxDPTForDepthEstimation"),fr=p(),C=r("div"),b(Pt.$$.fragment),Wn=p(),So=r("p"),Un=s("DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2."),Rn=p(),wt=r("p"),Bn=s("This model is a Flax "),xt=r("a"),Hn=s("jax.nn.Module"),Kn=s(`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),Gn=p(),R=r("div"),b(Dt.$$.fragment),Yn=p(),ie=r("p"),Jn=s("The "),Io=r("code"),Xn=s("FlaxDPTPreTrainedModel"),Zn=s(" forward method, overrides the "),Co=r("code"),Qn=s("__call__"),es=s(" special method."),ts=p(),b(je.$$.fragment),ur=p(),le=r("h2"),Me=r("a"),zo=r("span"),b(yt.$$.fragment),os=p(),Ao=r("span"),rs=s("FlaxDPTModel"),gr=p(),z=r("div"),b(Et.$$.fragment),as=p(),No=r("p"),ns=s("DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),ss=p(),Ft=r("p"),is=s("This model is a Flax "),kt=r("a"),ls=s("jax.nn.Module"),ds=s(`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),cs=p(),B=r("div"),b(jt.$$.fragment),ps=p(),de=r("p"),ms=s("The "),qo=r("code"),hs=s("FlaxDPTPreTrainedModel"),fs=s(" forward method, overrides the "),Oo=r("code"),us=s("__call__"),gs=s(" special method."),_s=p(),b(Se.$$.fragment),this.h()},l(t){const h=Fi('[data-svelte="svelte-1phssyn"]',document.head);d=a(h,"META",{name:!0,content:!0}),h.forEach(o),T=m(t),u=a(t,"H1",{class:!0});var Mt=n(u);g=a(Mt,"A",{id:!0,class:!0,href:!0});var Lo=n(g);v=a(Lo,"SPAN",{});var Vo=n(v);$(l.$$.fragment,Vo),Vo.forEach(o),Lo.forEach(o),f=m(Mt),E=a(Mt,"SPAN",{});var Wo=n(E);Nr=i(Wo,"DPT"),Wo.forEach(o),Mt.forEach(o),Go=m(t),K=a(t,"H2",{class:!0});var St=n(K);pe=a(St,"A",{id:!0,class:!0,href:!0});var Uo=n(pe);ao=a(Uo,"SPAN",{});var Ro=n(ao);$(Ue.$$.fragment,Ro),Ro.forEach(o),Uo.forEach(o),qr=m(St),no=a(St,"SPAN",{});var Bo=n(no);Or=i(Bo,"Overview"),Bo.forEach(o),St.forEach(o),Yo=m(t),L=a(t,"P",{});var ce=n(L);Lr=i(ce,"The DPT model was proposed in "),Re=a(ce,"A",{href:!0,rel:!0});var Ho=n(Re);Vr=i(Ho,"Vision Transformers for Dense Prediction"),Ho.forEach(o),Wr=i(ce,` by Ren\xE9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.
DPT is a model that leverages the `),Ct=a(ce,"A",{href:!0});var Ko=n(Ct);Ur=i(Ko,"Vision Transformer (ViT)"),Ko.forEach(o),Rr=i(ce," as backbone for dense prediction tasks like semantic segmentation and depth estimation."),ce.forEach(o),Jo=m(t),zt=a(t,"P",{});var Ts=n(zt);Br=i(Ts,"The abstract from the paper is the following:"),Ts.forEach(o),Xo=m(t),At=a(t,"P",{});var bs=n(At);so=a(bs,"EM",{});var $s=n(so);Hr=i($s,"We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art."),$s.forEach(o),bs.forEach(o),Zo=m(t),me=a(t,"IMG",{src:!0,alt:!0,width:!0}),Qo=m(t),he=a(t,"SMALL",{});var vr=n(he);Kr=i(vr,"DPT architecture. Taken from the "),Be=a(vr,"A",{href:!0,target:!0});var Ps=n(Be);Gr=i(Ps,"original paper"),Ps.forEach(o),Yr=i(vr,"."),vr.forEach(o),er=m(t),V=a(t,"P",{});var Rt=n(V);Jr=i(Rt,"This model was contributed by "),He=a(Rt,"A",{href:!0,rel:!0});var ws=n(He);Xr=i(ws,"nielsr"),ws.forEach(o),Zr=i(Rt,". The original code can be found "),Ke=a(Rt,"A",{href:!0,rel:!0});var xs=n(Ke);Qr=i(xs,"here"),xs.forEach(o),ea=i(Rt,"."),Rt.forEach(o),tr=m(t),G=a(t,"H2",{class:!0});var Tr=n(G);fe=a(Tr,"A",{id:!0,class:!0,href:!0});var Ds=n(fe);io=a(Ds,"SPAN",{});var ys=n(io);$(Ge.$$.fragment,ys),ys.forEach(o),Ds.forEach(o),ta=m(Tr),lo=a(Tr,"SPAN",{});var Es=n(lo);oa=i(Es,"DPTConfig"),Es.forEach(o),Tr.forEach(o),or=m(t),k=a(t,"DIV",{class:!0});var Ie=n(k);$(Ye.$$.fragment,Ie),ra=m(Ie),Y=a(Ie,"P",{});var Bt=n(Y);aa=i(Bt,"This is the configuration class to store the configuration of a "),Nt=a(Bt,"A",{href:!0});var Fs=n(Nt);na=i(Fs,"DPTModel"),Fs.forEach(o),sa=i(Bt,`. It is used to instantiate an DPT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the DPT
`),Je=a(Bt,"A",{href:!0,rel:!0});var ks=n(Je);ia=i(ks,"Intel/dpt-large"),ks.forEach(o),la=i(Bt," architecture."),Bt.forEach(o),da=m(Ie),J=a(Ie,"P",{});var Ht=n(J);ca=i(Ht,"Configuration objects inherit from "),qt=a(Ht,"A",{href:!0});var js=n(qt);pa=i(js,"PretrainedConfig"),js.forEach(o),ma=i(Ht,` and can be used to control the model outputs. Read the
documentation from `),Ot=a(Ht,"A",{href:!0});var Ms=n(Ot);ha=i(Ms,"PretrainedConfig"),Ms.forEach(o),fa=i(Ht," for more information."),Ht.forEach(o),ua=m(Ie),$(ue.$$.fragment,Ie),Ie.forEach(o),rr=m(t),X=a(t,"H2",{class:!0});var br=n(X);ge=a(br,"A",{id:!0,class:!0,href:!0});var Ss=n(ge);co=a(Ss,"SPAN",{});var Is=n(co);$(Xe.$$.fragment,Is),Is.forEach(o),Ss.forEach(o),ga=m(br),po=a(br,"SPAN",{});var Cs=n(po);_a=i(Cs,"DPTFeatureExtractor"),Cs.forEach(o),br.forEach(o),ar=m(t),j=a(t,"DIV",{class:!0});var Ce=n(j);$(Ze.$$.fragment,Ce),va=m(Ce),mo=a(Ce,"P",{});var zs=n(mo);Ta=i(zs,"Constructs a DPT feature extractor."),zs.forEach(o),ba=m(Ce),Qe=a(Ce,"P",{});var $r=n(Qe);$a=i($r,"This feature extractor inherits from "),Lt=a($r,"A",{href:!0});var As=n(Lt);Pa=i(As,"FeatureExtractionMixin"),As.forEach(o),wa=i($r,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),$r.forEach(o),xa=m(Ce),W=a(Ce,"DIV",{class:!0});var Kt=n(W);$(et.$$.fragment,Kt),Da=m(Kt),ho=a(Kt,"P",{});var Ns=n(ho);ya=i(Ns,"Main method to prepare for the model one or several image(s)."),Ns.forEach(o),Ea=m(Kt),$(_e.$$.fragment,Kt),Kt.forEach(o),Ce.forEach(o),nr=m(t),Z=a(t,"H2",{class:!0});var Pr=n(Z);ve=a(Pr,"A",{id:!0,class:!0,href:!0});var qs=n(ve);fo=a(qs,"SPAN",{});var Os=n(fo);$(tt.$$.fragment,Os),Os.forEach(o),qs.forEach(o),Fa=m(Pr),uo=a(Pr,"SPAN",{});var Ls=n(uo);ka=i(Ls,"DPTModel"),Ls.forEach(o),Pr.forEach(o),sr=m(t),O=a(t,"DIV",{class:!0});var Gt=n(O);$(ot.$$.fragment,Gt),ja=m(Gt),rt=a(Gt,"P",{});var wr=n(rt);Ma=i(wr,`The bare DPT Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),at=a(wr,"A",{href:!0,rel:!0});var Vs=n(at);Sa=i(Vs,"torch.nn.Module"),Vs.forEach(o),Ia=i(wr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),wr.forEach(o),Ca=m(Gt),A=a(Gt,"DIV",{class:!0});var ze=n(A);$(nt.$$.fragment,ze),za=m(ze),Q=a(ze,"P",{});var Yt=n(Q);Aa=i(Yt,"The "),Vt=a(Yt,"A",{href:!0});var Ws=n(Vt);Na=i(Ws,"DPTModel"),Ws.forEach(o),qa=i(Yt," forward method, overrides the "),go=a(Yt,"CODE",{});var Us=n(go);Oa=i(Us,"__call__"),Us.forEach(o),La=i(Yt," special method."),Yt.forEach(o),Va=m(ze),$(Te.$$.fragment,ze),Wa=m(ze),$(be.$$.fragment,ze),ze.forEach(o),Gt.forEach(o),ir=m(t),ee=a(t,"H2",{class:!0});var xr=n(ee);$e=a(xr,"A",{id:!0,class:!0,href:!0});var Rs=n($e);_o=a(Rs,"SPAN",{});var Bs=n(_o);$(st.$$.fragment,Bs),Bs.forEach(o),Rs.forEach(o),Ua=m(xr),vo=a(xr,"SPAN",{});var Hs=n(vo);Ra=i(Hs,"DPTForDepthEstimation"),Hs.forEach(o),xr.forEach(o),lr=m(t),M=a(t,"DIV",{class:!0});var Ae=n(M);$(it.$$.fragment,Ae),Ba=m(Ae),To=a(Ae,"P",{});var Ks=n(To);Ha=i(Ks,"DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2."),Ks.forEach(o),Ka=m(Ae),lt=a(Ae,"P",{});var Dr=n(lt);Ga=i(Dr,"This model is a PyTorch "),dt=a(Dr,"A",{href:!0,rel:!0});var Gs=n(dt);Ya=i(Gs,"torch.nn.Module"),Gs.forEach(o),Ja=i(Dr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Dr.forEach(o),Xa=m(Ae),N=a(Ae,"DIV",{class:!0});var Ne=n(N);$(ct.$$.fragment,Ne),Za=m(Ne),te=a(Ne,"P",{});var Jt=n(te);Qa=i(Jt,"The "),Wt=a(Jt,"A",{href:!0});var Ys=n(Wt);en=i(Ys,"DPTForDepthEstimation"),Ys.forEach(o),tn=i(Jt," forward method, overrides the "),bo=a(Jt,"CODE",{});var Js=n(bo);on=i(Js,"__call__"),Js.forEach(o),rn=i(Jt," special method."),Jt.forEach(o),an=m(Ne),$(Pe.$$.fragment,Ne),nn=m(Ne),$(we.$$.fragment,Ne),Ne.forEach(o),Ae.forEach(o),dr=m(t),oe=a(t,"H2",{class:!0});var yr=n(oe);xe=a(yr,"A",{id:!0,class:!0,href:!0});var Xs=n(xe);$o=a(Xs,"SPAN",{});var Zs=n($o);$(pt.$$.fragment,Zs),Zs.forEach(o),Xs.forEach(o),sn=m(yr),Po=a(yr,"SPAN",{});var Qs=n(Po);ln=i(Qs,"DPTForSemanticSegmentation"),Qs.forEach(o),yr.forEach(o),cr=m(t),S=a(t,"DIV",{class:!0});var qe=n(S);$(mt.$$.fragment,qe),dn=m(qe),wo=a(qe,"P",{});var ei=n(wo);cn=i(ei,"DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),ei.forEach(o),pn=m(qe),ht=a(qe,"P",{});var Er=n(ht);mn=i(Er,"This model is a PyTorch "),ft=a(Er,"A",{href:!0,rel:!0});var ti=n(ft);hn=i(ti,"torch.nn.Module"),ti.forEach(o),fn=i(Er,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Er.forEach(o),un=m(qe),q=a(qe,"DIV",{class:!0});var Oe=n(q);$(ut.$$.fragment,Oe),gn=m(Oe),re=a(Oe,"P",{});var Xt=n(re);_n=i(Xt,"The "),Ut=a(Xt,"A",{href:!0});var oi=n(Ut);vn=i(oi,"DPTForSemanticSegmentation"),oi.forEach(o),Tn=i(Xt," forward method, overrides the "),xo=a(Xt,"CODE",{});var ri=n(xo);bn=i(ri,"__call__"),ri.forEach(o),$n=i(Xt," special method."),Xt.forEach(o),Pn=m(Oe),$(De.$$.fragment,Oe),wn=m(Oe),$(ye.$$.fragment,Oe),Oe.forEach(o),qe.forEach(o),pr=m(t),ae=a(t,"H2",{class:!0});var Fr=n(ae);Ee=a(Fr,"A",{id:!0,class:!0,href:!0});var ai=n(Ee);Do=a(ai,"SPAN",{});var ni=n(Do);$(gt.$$.fragment,ni),ni.forEach(o),ai.forEach(o),xn=m(Fr),yo=a(Fr,"SPAN",{});var si=n(yo);Dn=i(si,"FlaxDPTForSemanticSegmentation"),si.forEach(o),Fr.forEach(o),mr=m(t),I=a(t,"DIV",{class:!0});var Le=n(I);$(_t.$$.fragment,Le),yn=m(Le),Eo=a(Le,"P",{});var ii=n(Eo);En=i(ii,"DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),ii.forEach(o),Fn=m(Le),vt=a(Le,"P",{});var kr=n(vt);kn=i(kr,"This model is a Flax "),Tt=a(kr,"A",{href:!0,rel:!0});var li=n(Tt);jn=i(li,"jax.nn.Module"),li.forEach(o),Mn=i(kr,`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),kr.forEach(o),Sn=m(Le),U=a(Le,"DIV",{class:!0});var Zt=n(U);$(bt.$$.fragment,Zt),In=m(Zt),ne=a(Zt,"P",{});var Qt=n(ne);Cn=i(Qt,"The "),Fo=a(Qt,"CODE",{});var di=n(Fo);zn=i(di,"FlaxDPTPreTrainedModel"),di.forEach(o),An=i(Qt," forward method, overrides the "),ko=a(Qt,"CODE",{});var ci=n(ko);Nn=i(ci,"__call__"),ci.forEach(o),qn=i(Qt," special method."),Qt.forEach(o),On=m(Zt),$(Fe.$$.fragment,Zt),Zt.forEach(o),Le.forEach(o),hr=m(t),se=a(t,"H2",{class:!0});var jr=n(se);ke=a(jr,"A",{id:!0,class:!0,href:!0});var pi=n(ke);jo=a(pi,"SPAN",{});var mi=n(jo);$($t.$$.fragment,mi),mi.forEach(o),pi.forEach(o),Ln=m(jr),Mo=a(jr,"SPAN",{});var hi=n(Mo);Vn=i(hi,"FlaxDPTForDepthEstimation"),hi.forEach(o),jr.forEach(o),fr=m(t),C=a(t,"DIV",{class:!0});var Ve=n(C);$(Pt.$$.fragment,Ve),Wn=m(Ve),So=a(Ve,"P",{});var fi=n(So);Un=i(fi,"DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2."),fi.forEach(o),Rn=m(Ve),wt=a(Ve,"P",{});var Mr=n(wt);Bn=i(Mr,"This model is a Flax "),xt=a(Mr,"A",{href:!0,rel:!0});var ui=n(xt);Hn=i(ui,"jax.nn.Module"),ui.forEach(o),Kn=i(Mr,`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),Mr.forEach(o),Gn=m(Ve),R=a(Ve,"DIV",{class:!0});var eo=n(R);$(Dt.$$.fragment,eo),Yn=m(eo),ie=a(eo,"P",{});var to=n(ie);Jn=i(to,"The "),Io=a(to,"CODE",{});var gi=n(Io);Xn=i(gi,"FlaxDPTPreTrainedModel"),gi.forEach(o),Zn=i(to," forward method, overrides the "),Co=a(to,"CODE",{});var _i=n(Co);Qn=i(_i,"__call__"),_i.forEach(o),es=i(to," special method."),to.forEach(o),ts=m(eo),$(je.$$.fragment,eo),eo.forEach(o),Ve.forEach(o),ur=m(t),le=a(t,"H2",{class:!0});var Sr=n(le);Me=a(Sr,"A",{id:!0,class:!0,href:!0});var vi=n(Me);zo=a(vi,"SPAN",{});var Ti=n(zo);$(yt.$$.fragment,Ti),Ti.forEach(o),vi.forEach(o),os=m(Sr),Ao=a(Sr,"SPAN",{});var bi=n(Ao);rs=i(bi,"FlaxDPTModel"),bi.forEach(o),Sr.forEach(o),gr=m(t),z=a(t,"DIV",{class:!0});var We=n(z);$(Et.$$.fragment,We),as=m(We),No=a(We,"P",{});var $i=n(No);ns=i($i,"DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes."),$i.forEach(o),ss=m(We),Ft=a(We,"P",{});var Ir=n(Ft);is=i(Ir,"This model is a Flax "),kt=a(Ir,"A",{href:!0,rel:!0});var Pi=n(kt);ls=i(Pi,"jax.nn.Module"),Pi.forEach(o),ds=i(Ir,`
subclass. Use it as a regular Flax Module and refer to the Flax documentation for all matter related to general
usage and behavior.`),Ir.forEach(o),cs=m(We),B=a(We,"DIV",{class:!0});var oo=n(B);$(jt.$$.fragment,oo),ps=m(oo),de=a(oo,"P",{});var ro=n(de);ms=i(ro,"The "),qo=a(ro,"CODE",{});var wi=n(qo);hs=i(wi,"FlaxDPTPreTrainedModel"),wi.forEach(o),fs=i(ro," forward method, overrides the "),Oo=a(ro,"CODE",{});var xi=n(Oo);us=i(xi,"__call__"),xi.forEach(o),gs=i(ro," special method."),ro.forEach(o),_s=m(oo),$(Se.$$.fragment,oo),oo.forEach(o),We.forEach(o),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(Ui)),c(g,"id","dpt"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#dpt"),c(u,"class","relative group"),c(pe,"id","overview"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#overview"),c(K,"class","relative group"),c(Re,"href","https://arxiv.org/abs/2103.13413"),c(Re,"rel","nofollow"),c(Ct,"href","vit"),ki(me.src,vs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg")||c(me,"src",vs),c(me,"alt","drawing"),c(me,"width","600"),c(Be,"href","https://arxiv.org/abs/2103.13413"),c(Be,"target","_blank"),c(He,"href","https://huggingface.co/nielsr"),c(He,"rel","nofollow"),c(Ke,"href","https://github.com/isl-org/DPT"),c(Ke,"rel","nofollow"),c(fe,"id","transformers.DPTConfig"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#transformers.DPTConfig"),c(G,"class","relative group"),c(Nt,"href","/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTModel"),c(Je,"href","https://huggingface.co/Intel/dpt-large"),c(Je,"rel","nofollow"),c(qt,"href","/docs/transformers/pr_17770/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ot,"href","/docs/transformers/pr_17770/en/main_classes/configuration#transformers.PretrainedConfig"),c(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ge,"id","transformers.DPTFeatureExtractor"),c(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ge,"href","#transformers.DPTFeatureExtractor"),c(X,"class","relative group"),c(Lt,"href","/docs/transformers/pr_17770/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),c(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ve,"id","transformers.DPTModel"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#transformers.DPTModel"),c(Z,"class","relative group"),c(at,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(at,"rel","nofollow"),c(Vt,"href","/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTModel"),c(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c($e,"id","transformers.DPTForDepthEstimation"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#transformers.DPTForDepthEstimation"),c(ee,"class","relative group"),c(dt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(dt,"rel","nofollow"),c(Wt,"href","/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTForDepthEstimation"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(xe,"id","transformers.DPTForSemanticSegmentation"),c(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xe,"href","#transformers.DPTForSemanticSegmentation"),c(oe,"class","relative group"),c(ft,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ft,"rel","nofollow"),c(Ut,"href","/docs/transformers/pr_17770/en/model_doc/dpt#transformers.DPTForSemanticSegmentation"),c(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ee,"id","transformers.FlaxDPTForSemanticSegmentation"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#transformers.FlaxDPTForSemanticSegmentation"),c(ae,"class","relative group"),c(Tt,"href","https://jax.readthedocs.io/en/latest/jax.nn.html?highlight=nn.Module"),c(Tt,"rel","nofollow"),c(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ke,"id","transformers.FlaxDPTForDepthEstimation"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#transformers.FlaxDPTForDepthEstimation"),c(se,"class","relative group"),c(xt,"href","https://jax.readthedocs.io/en/latest/jax.nn.html?highlight=nn.Module"),c(xt,"rel","nofollow"),c(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Me,"id","transformers.FlaxDPTModel"),c(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Me,"href","#transformers.FlaxDPTModel"),c(le,"class","relative group"),c(kt,"href","https://jax.readthedocs.io/en/latest/jax.nn.html?highlight=nn.Module"),c(kt,"rel","nofollow"),c(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,d),_(t,T,h),_(t,u,h),e(u,g),e(g,v),P(l,v,null),e(u,f),e(u,E),e(E,Nr),_(t,Go,h),_(t,K,h),e(K,pe),e(pe,ao),P(Ue,ao,null),e(K,qr),e(K,no),e(no,Or),_(t,Yo,h),_(t,L,h),e(L,Lr),e(L,Re),e(Re,Vr),e(L,Wr),e(L,Ct),e(Ct,Ur),e(L,Rr),_(t,Jo,h),_(t,zt,h),e(zt,Br),_(t,Xo,h),_(t,At,h),e(At,so),e(so,Hr),_(t,Zo,h),_(t,me,h),_(t,Qo,h),_(t,he,h),e(he,Kr),e(he,Be),e(Be,Gr),e(he,Yr),_(t,er,h),_(t,V,h),e(V,Jr),e(V,He),e(He,Xr),e(V,Zr),e(V,Ke),e(Ke,Qr),e(V,ea),_(t,tr,h),_(t,G,h),e(G,fe),e(fe,io),P(Ge,io,null),e(G,ta),e(G,lo),e(lo,oa),_(t,or,h),_(t,k,h),P(Ye,k,null),e(k,ra),e(k,Y),e(Y,aa),e(Y,Nt),e(Nt,na),e(Y,sa),e(Y,Je),e(Je,ia),e(Y,la),e(k,da),e(k,J),e(J,ca),e(J,qt),e(qt,pa),e(J,ma),e(J,Ot),e(Ot,ha),e(J,fa),e(k,ua),P(ue,k,null),_(t,rr,h),_(t,X,h),e(X,ge),e(ge,co),P(Xe,co,null),e(X,ga),e(X,po),e(po,_a),_(t,ar,h),_(t,j,h),P(Ze,j,null),e(j,va),e(j,mo),e(mo,Ta),e(j,ba),e(j,Qe),e(Qe,$a),e(Qe,Lt),e(Lt,Pa),e(Qe,wa),e(j,xa),e(j,W),P(et,W,null),e(W,Da),e(W,ho),e(ho,ya),e(W,Ea),P(_e,W,null),_(t,nr,h),_(t,Z,h),e(Z,ve),e(ve,fo),P(tt,fo,null),e(Z,Fa),e(Z,uo),e(uo,ka),_(t,sr,h),_(t,O,h),P(ot,O,null),e(O,ja),e(O,rt),e(rt,Ma),e(rt,at),e(at,Sa),e(rt,Ia),e(O,Ca),e(O,A),P(nt,A,null),e(A,za),e(A,Q),e(Q,Aa),e(Q,Vt),e(Vt,Na),e(Q,qa),e(Q,go),e(go,Oa),e(Q,La),e(A,Va),P(Te,A,null),e(A,Wa),P(be,A,null),_(t,ir,h),_(t,ee,h),e(ee,$e),e($e,_o),P(st,_o,null),e(ee,Ua),e(ee,vo),e(vo,Ra),_(t,lr,h),_(t,M,h),P(it,M,null),e(M,Ba),e(M,To),e(To,Ha),e(M,Ka),e(M,lt),e(lt,Ga),e(lt,dt),e(dt,Ya),e(lt,Ja),e(M,Xa),e(M,N),P(ct,N,null),e(N,Za),e(N,te),e(te,Qa),e(te,Wt),e(Wt,en),e(te,tn),e(te,bo),e(bo,on),e(te,rn),e(N,an),P(Pe,N,null),e(N,nn),P(we,N,null),_(t,dr,h),_(t,oe,h),e(oe,xe),e(xe,$o),P(pt,$o,null),e(oe,sn),e(oe,Po),e(Po,ln),_(t,cr,h),_(t,S,h),P(mt,S,null),e(S,dn),e(S,wo),e(wo,cn),e(S,pn),e(S,ht),e(ht,mn),e(ht,ft),e(ft,hn),e(ht,fn),e(S,un),e(S,q),P(ut,q,null),e(q,gn),e(q,re),e(re,_n),e(re,Ut),e(Ut,vn),e(re,Tn),e(re,xo),e(xo,bn),e(re,$n),e(q,Pn),P(De,q,null),e(q,wn),P(ye,q,null),_(t,pr,h),_(t,ae,h),e(ae,Ee),e(Ee,Do),P(gt,Do,null),e(ae,xn),e(ae,yo),e(yo,Dn),_(t,mr,h),_(t,I,h),P(_t,I,null),e(I,yn),e(I,Eo),e(Eo,En),e(I,Fn),e(I,vt),e(vt,kn),e(vt,Tt),e(Tt,jn),e(vt,Mn),e(I,Sn),e(I,U),P(bt,U,null),e(U,In),e(U,ne),e(ne,Cn),e(ne,Fo),e(Fo,zn),e(ne,An),e(ne,ko),e(ko,Nn),e(ne,qn),e(U,On),P(Fe,U,null),_(t,hr,h),_(t,se,h),e(se,ke),e(ke,jo),P($t,jo,null),e(se,Ln),e(se,Mo),e(Mo,Vn),_(t,fr,h),_(t,C,h),P(Pt,C,null),e(C,Wn),e(C,So),e(So,Un),e(C,Rn),e(C,wt),e(wt,Bn),e(wt,xt),e(xt,Hn),e(wt,Kn),e(C,Gn),e(C,R),P(Dt,R,null),e(R,Yn),e(R,ie),e(ie,Jn),e(ie,Io),e(Io,Xn),e(ie,Zn),e(ie,Co),e(Co,Qn),e(ie,es),e(R,ts),P(je,R,null),_(t,ur,h),_(t,le,h),e(le,Me),e(Me,zo),P(yt,zo,null),e(le,os),e(le,Ao),e(Ao,rs),_(t,gr,h),_(t,z,h),P(Et,z,null),e(z,as),e(z,No),e(No,ns),e(z,ss),e(z,Ft),e(Ft,is),e(Ft,kt),e(kt,ls),e(Ft,ds),e(z,cs),e(z,B),P(jt,B,null),e(B,ps),e(B,de),e(de,ms),e(de,qo),e(qo,hs),e(de,fs),e(de,Oo),e(Oo,us),e(de,gs),e(B,_s),P(Se,B,null),_r=!0},p(t,[h]){const Mt={};h&2&&(Mt.$$scope={dirty:h,ctx:t}),ue.$set(Mt);const Lo={};h&2&&(Lo.$$scope={dirty:h,ctx:t}),_e.$set(Lo);const Vo={};h&2&&(Vo.$$scope={dirty:h,ctx:t}),Te.$set(Vo);const Wo={};h&2&&(Wo.$$scope={dirty:h,ctx:t}),be.$set(Wo);const St={};h&2&&(St.$$scope={dirty:h,ctx:t}),Pe.$set(St);const Uo={};h&2&&(Uo.$$scope={dirty:h,ctx:t}),we.$set(Uo);const Ro={};h&2&&(Ro.$$scope={dirty:h,ctx:t}),De.$set(Ro);const Bo={};h&2&&(Bo.$$scope={dirty:h,ctx:t}),ye.$set(Bo);const ce={};h&2&&(ce.$$scope={dirty:h,ctx:t}),Fe.$set(ce);const Ho={};h&2&&(Ho.$$scope={dirty:h,ctx:t}),je.$set(Ho);const Ko={};h&2&&(Ko.$$scope={dirty:h,ctx:t}),Se.$set(Ko)},i(t){_r||(w(l.$$.fragment,t),w(Ue.$$.fragment,t),w(Ge.$$.fragment,t),w(Ye.$$.fragment,t),w(ue.$$.fragment,t),w(Xe.$$.fragment,t),w(Ze.$$.fragment,t),w(et.$$.fragment,t),w(_e.$$.fragment,t),w(tt.$$.fragment,t),w(ot.$$.fragment,t),w(nt.$$.fragment,t),w(Te.$$.fragment,t),w(be.$$.fragment,t),w(st.$$.fragment,t),w(it.$$.fragment,t),w(ct.$$.fragment,t),w(Pe.$$.fragment,t),w(we.$$.fragment,t),w(pt.$$.fragment,t),w(mt.$$.fragment,t),w(ut.$$.fragment,t),w(De.$$.fragment,t),w(ye.$$.fragment,t),w(gt.$$.fragment,t),w(_t.$$.fragment,t),w(bt.$$.fragment,t),w(Fe.$$.fragment,t),w($t.$$.fragment,t),w(Pt.$$.fragment,t),w(Dt.$$.fragment,t),w(je.$$.fragment,t),w(yt.$$.fragment,t),w(Et.$$.fragment,t),w(jt.$$.fragment,t),w(Se.$$.fragment,t),_r=!0)},o(t){x(l.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),x(Ye.$$.fragment,t),x(ue.$$.fragment,t),x(Xe.$$.fragment,t),x(Ze.$$.fragment,t),x(et.$$.fragment,t),x(_e.$$.fragment,t),x(tt.$$.fragment,t),x(ot.$$.fragment,t),x(nt.$$.fragment,t),x(Te.$$.fragment,t),x(be.$$.fragment,t),x(st.$$.fragment,t),x(it.$$.fragment,t),x(ct.$$.fragment,t),x(Pe.$$.fragment,t),x(we.$$.fragment,t),x(pt.$$.fragment,t),x(mt.$$.fragment,t),x(ut.$$.fragment,t),x(De.$$.fragment,t),x(ye.$$.fragment,t),x(gt.$$.fragment,t),x(_t.$$.fragment,t),x(bt.$$.fragment,t),x(Fe.$$.fragment,t),x($t.$$.fragment,t),x(Pt.$$.fragment,t),x(Dt.$$.fragment,t),x(je.$$.fragment,t),x(yt.$$.fragment,t),x(Et.$$.fragment,t),x(jt.$$.fragment,t),x(Se.$$.fragment,t),_r=!1},d(t){o(d),t&&o(T),t&&o(u),D(l),t&&o(Go),t&&o(K),D(Ue),t&&o(Yo),t&&o(L),t&&o(Jo),t&&o(zt),t&&o(Xo),t&&o(At),t&&o(Zo),t&&o(me),t&&o(Qo),t&&o(he),t&&o(er),t&&o(V),t&&o(tr),t&&o(G),D(Ge),t&&o(or),t&&o(k),D(Ye),D(ue),t&&o(rr),t&&o(X),D(Xe),t&&o(ar),t&&o(j),D(Ze),D(et),D(_e),t&&o(nr),t&&o(Z),D(tt),t&&o(sr),t&&o(O),D(ot),D(nt),D(Te),D(be),t&&o(ir),t&&o(ee),D(st),t&&o(lr),t&&o(M),D(it),D(ct),D(Pe),D(we),t&&o(dr),t&&o(oe),D(pt),t&&o(cr),t&&o(S),D(mt),D(ut),D(De),D(ye),t&&o(pr),t&&o(ae),D(gt),t&&o(mr),t&&o(I),D(_t),D(bt),D(Fe),t&&o(hr),t&&o(se),D($t),t&&o(fr),t&&o(C),D(Pt),D(Dt),D(je),t&&o(ur),t&&o(le),D(yt),t&&o(gr),t&&o(z),D(Et),D(jt),D(Se)}}}const Ui={local:"dpt",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPTConfig",title:"DPTConfig"},{local:"transformers.DPTFeatureExtractor",title:"DPTFeatureExtractor"},{local:"transformers.DPTModel",title:"DPTModel"},{local:"transformers.DPTForDepthEstimation",title:"DPTForDepthEstimation"},{local:"transformers.DPTForSemanticSegmentation",title:"DPTForSemanticSegmentation"},{local:"transformers.FlaxDPTForSemanticSegmentation",title:"FlaxDPTForSemanticSegmentation"},{local:"transformers.FlaxDPTForDepthEstimation",title:"FlaxDPTForDepthEstimation"},{local:"transformers.FlaxDPTModel",title:"FlaxDPTModel"}],title:"DPT"};function Ri(y){return ji(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xi extends Di{constructor(d){super();yi(this,d,Ri,Wi,Ei,{})}}export{Xi as default,Ui as metadata};
