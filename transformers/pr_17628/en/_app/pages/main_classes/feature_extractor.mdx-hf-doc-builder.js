import{S as Ho,i as Go,s as Jo,e as a,k as c,w as g,t as s,M as Yo,c as o,d as r,m as d,a as n,x as _,h as i,b as l,G as e,g as E,y as v,q as x,o as y,B as b,v as Ko,L as Qo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Uo}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Xo}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Nt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Zo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function en(Q){let m,T,f,h,F;return{c(){m=a("p"),T=s("Passing "),f=a("code"),h=s("use_auth_token=True"),F=s(" is required when you want to use a private model.")},l(u){m=o(u,"P",{});var $=n(m);T=i($,"Passing "),f=o($,"CODE",{});var L=n(f);h=i(L,"use_auth_token=True"),L.forEach(r),F=i($," is required when you want to use a private model."),$.forEach(r)},m(u,$){E(u,m,$),e(m,T),e(m,f),e(f,h),e(m,F)},d(u){u&&r(m)}}}function tn(Q){let m,T,f,h,F;return h=new Xo({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),T=s("Examples:"),f=c(),g(h.$$.fragment)},l(u){m=o(u,"P",{});var $=n(m);T=i($,"Examples:"),$.forEach(r),f=d(u),_(h.$$.fragment,u)},m(u,$){E(u,m,$),e(m,T),E(u,f,$),v(h,u,$),F=!0},p:Qo,i(u){F||(x(h.$$.fragment,u),F=!0)},o(u){y(h.$$.fragment,u),F=!1},d(u){u&&r(m),u&&r(f),b(h,u)}}}function rn(Q){let m,T,f,h,F,u,$,L;return{c(){m=a("p"),T=s("If the "),f=a("code"),h=s("processed_features"),F=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),u=a("code"),$=s("return_tensors"),L=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(O){m=o(O,"P",{});var z=n(m);T=i(z,"If the "),f=o(z,"CODE",{});var q=n(f);h=i(q,"processed_features"),q.forEach(r),F=i(z,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),u=o(z,"CODE",{});var Ve=n(u);$=i(Ve,"return_tensors"),Ve.forEach(r),L=i(z,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),z.forEach(r)},m(O,z){E(O,m,z),e(m,T),e(m,f),e(f,h),e(m,F),e(m,u),e(u,$),e(m,L)},d(O){O&&r(m)}}}function an(Q){let m,T,f,h,F,u,$,L,O,z,q,Ve,et,cr,dr,tt,lr,mr,St,V,X,rt,ve,ur,at,pr,Bt,M,xe,fr,ot,hr,gr,N,ye,_r,B,vr,We,xr,yr,nt,br,Er,Re,$r,wr,Fr,Z,Tr,ee,Ir,te,be,kr,W,Pr,st,zr,qr,Ue,Mr,Dr,Ct,R,re,it,Ee,Lr,ct,Nr,At,C,$e,Sr,dt,Br,Cr,S,we,Ar,lt,jr,Or,U,Vr,mt,Wr,Rr,ut,Ur,Hr,Gr,ae,jt,H,oe,pt,Fe,Jr,ft,Yr,Ot,k,Te,Kr,G,Qr,He,Xr,Zr,ht,ea,ta,ra,gt,aa,oa,ne,Ie,na,_t,sa,ia,se,ke,ca,Pe,da,vt,la,ma,Vt,J,ie,xt,ze,ua,yt,pa,Wt,w,qe,fa,bt,ha,ga,ce,Me,_a,De,va,Et,xa,ya,ba,de,Le,Ea,Y,$a,$t,wa,Fa,wt,Ta,Ia,ka,le,Ne,Pa,D,za,Ft,qa,Ma,Tt,Da,La,It,Na,Sa,kt,Ba,Ca,Aa,me,Se,ja,K,Oa,Pt,Va,Wa,zt,Ra,Ua,Ha,ue,Be,Ga,Ce,Ja,qt,Ya,Ka,Qa,pe,Ae,Xa,je,Za,Mt,eo,to,Rt;return u=new Nt({}),ve=new Nt({}),xe=new P({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L204"}}),ye=new P({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L228",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),Z=new Uo({props:{$$slots:{default:[en]},$$scope:{ctx:Q}}}),ee=new Zo({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[tn]},$$scope:{ctx:Q}}}),be=new P({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your feature extractor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_17628/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L312"}}),Ee=new Nt({}),$e=new P({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_sequence_utils.py#L30"}}),we=new P({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17628/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17628/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_sequence_utils.py#L53"}}),ae=new Uo({props:{$$slots:{default:[rn]},$$scope:{ctx:Q}}}),Fe=new Nt({}),Te=new P({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L62"}}),Ie=new P({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/pr_17628/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/pr_17628/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L116"}}),ke=new P({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/feature_extraction_utils.py#L181",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),ze=new Nt({}),qe=new P({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L77"}}),Me=new P({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L264"}}),Le=new P({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L119"}}),Ne=new P({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L166"}}),Se=new P({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L202"}}),Be=new P({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L133"}}),Ae=new P({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/vr_17628/src/transformers/image_utils.py#L89"}}),{c(){m=a("meta"),T=c(),f=a("h1"),h=a("a"),F=a("span"),g(u.$$.fragment),$=c(),L=a("span"),O=s("Feature Extractor"),z=c(),q=a("p"),Ve=s(`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),et=a("em"),cr=s("e.g."),dr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),tt=a("em"),lr=s("e.g."),mr=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),St=c(),V=a("h2"),X=a("a"),rt=a("span"),g(ve.$$.fragment),ur=c(),at=a("span"),pr=s("FeatureExtractionMixin"),Bt=c(),M=a("div"),g(xe.$$.fragment),fr=c(),ot=a("p"),hr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),gr=c(),N=a("div"),g(ye.$$.fragment),_r=c(),B=a("p"),vr=s("Instantiate a type of "),We=a("a"),xr=s("FeatureExtractionMixin"),yr=s(" from a feature extractor, "),nt=a("em"),br=s("e.g."),Er=s(` a
derived class of `),Re=a("a"),$r=s("SequenceFeatureExtractor"),wr=s("."),Fr=c(),g(Z.$$.fragment),Tr=c(),g(ee.$$.fragment),Ir=c(),te=a("div"),g(be.$$.fragment),kr=c(),W=a("p"),Pr=s("Save a feature_extractor object to the directory "),st=a("code"),zr=s("save_directory"),qr=s(`, so that it can be re-loaded using the
`),Ue=a("a"),Mr=s("from_pretrained()"),Dr=s(" class method."),Ct=c(),R=a("h2"),re=a("a"),it=a("span"),g(Ee.$$.fragment),Lr=c(),ct=a("span"),Nr=s("SequenceFeatureExtractor"),At=c(),C=a("div"),g($e.$$.fragment),Sr=c(),dt=a("p"),Br=s("This is a general feature extraction class for speech recognition."),Cr=c(),S=a("div"),g(we.$$.fragment),Ar=c(),lt=a("p"),jr=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),Or=c(),U=a("p"),Vr=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),mt=a("code"),Wr=s("self.padding_side"),Rr=s(`,
`),ut=a("code"),Ur=s("self.padding_value"),Hr=s(")"),Gr=c(),g(ae.$$.fragment),jt=c(),H=a("h2"),oe=a("a"),pt=a("span"),g(Fe.$$.fragment),Jr=c(),ft=a("span"),Yr=s("BatchFeature"),Ot=c(),k=a("div"),g(Te.$$.fragment),Kr=c(),G=a("p"),Qr=s("Holds the output of the "),He=a("a"),Xr=s("pad()"),Zr=s(" and feature extractor specific "),ht=a("code"),ea=s("__call__"),ta=s(" methods."),ra=c(),gt=a("p"),aa=s("This class is derived from a python dictionary and can be used as a dictionary."),oa=c(),ne=a("div"),g(Ie.$$.fragment),na=c(),_t=a("p"),sa=s("Convert the inner content to tensors."),ia=c(),se=a("div"),g(ke.$$.fragment),ca=c(),Pe=a("p"),da=s("Send all values to device by calling "),vt=a("code"),la=s("v.to(device)"),ma=s(" (PyTorch only)."),Vt=c(),J=a("h2"),ie=a("a"),xt=a("span"),g(ze.$$.fragment),ua=c(),yt=a("span"),pa=s("ImageFeatureExtractionMixin"),Wt=c(),w=a("div"),g(qe.$$.fragment),fa=c(),bt=a("p"),ha=s("Mixin that contain utilities for preparing image features."),ga=c(),ce=a("div"),g(Me.$$.fragment),_a=c(),De=a("p"),va=s("Crops "),Et=a("code"),xa=s("image"),ya=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),ba=c(),de=a("div"),g(Le.$$.fragment),Ea=c(),Y=a("p"),$a=s("Converts "),$t=a("code"),wa=s("image"),Fa=s(" to RGB format. Note that this will trigger a conversion of "),wt=a("code"),Ta=s("image"),Ia=s(" to a PIL Image."),ka=c(),le=a("div"),g(Ne.$$.fragment),Pa=c(),D=a("p"),za=s("Normalizes "),Ft=a("code"),qa=s("image"),Ma=s(" with "),Tt=a("code"),Da=s("mean"),La=s(" and "),It=a("code"),Na=s("std"),Sa=s(". Note that this will trigger a conversion of "),kt=a("code"),Ba=s("image"),Ca=s(` to a NumPy array
if it\u2019s a PIL Image.`),Aa=c(),me=a("div"),g(Se.$$.fragment),ja=c(),K=a("p"),Oa=s("Resizes "),Pt=a("code"),Va=s("image"),Wa=s(". Note that this will trigger a conversion of "),zt=a("code"),Ra=s("image"),Ua=s(" to a PIL Image."),Ha=c(),ue=a("div"),g(Be.$$.fragment),Ga=c(),Ce=a("p"),Ja=s("Converts "),qt=a("code"),Ya=s("image"),Ka=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Qa=c(),pe=a("div"),g(Ae.$$.fragment),Xa=c(),je=a("p"),Za=s("Converts "),Mt=a("code"),eo=s("image"),to=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const p=Yo('[data-svelte="svelte-1phssyn"]',document.head);m=o(p,"META",{name:!0,content:!0}),p.forEach(r),T=d(t),f=o(t,"H1",{class:!0});var Oe=n(f);h=o(Oe,"A",{id:!0,class:!0,href:!0});var Dt=n(h);F=o(Dt,"SPAN",{});var Lt=n(F);_(u.$$.fragment,Lt),Lt.forEach(r),Dt.forEach(r),$=d(Oe),L=o(Oe,"SPAN",{});var ro=n(L);O=i(ro,"Feature Extractor"),ro.forEach(r),Oe.forEach(r),z=d(t),q=o(t,"P",{});var Ge=n(q);Ve=i(Ge,`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),et=o(Ge,"EM",{});var ao=n(et);cr=i(ao,"e.g."),ao.forEach(r),dr=i(Ge,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),tt=o(Ge,"EM",{});var oo=n(tt);lr=i(oo,"e.g."),oo.forEach(r),mr=i(Ge,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Ge.forEach(r),St=d(t),V=o(t,"H2",{class:!0});var Ut=n(V);X=o(Ut,"A",{id:!0,class:!0,href:!0});var no=n(X);rt=o(no,"SPAN",{});var so=n(rt);_(ve.$$.fragment,so),so.forEach(r),no.forEach(r),ur=d(Ut),at=o(Ut,"SPAN",{});var io=n(at);pr=i(io,"FeatureExtractionMixin"),io.forEach(r),Ut.forEach(r),Bt=d(t),M=o(t,"DIV",{class:!0});var fe=n(M);_(xe.$$.fragment,fe),fr=d(fe),ot=o(fe,"P",{});var co=n(ot);hr=i(co,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),co.forEach(r),gr=d(fe),N=o(fe,"DIV",{class:!0});var he=n(N);_(ye.$$.fragment,he),_r=d(he),B=o(he,"P",{});var ge=n(B);vr=i(ge,"Instantiate a type of "),We=o(ge,"A",{href:!0});var lo=n(We);xr=i(lo,"FeatureExtractionMixin"),lo.forEach(r),yr=i(ge," from a feature extractor, "),nt=o(ge,"EM",{});var mo=n(nt);br=i(mo,"e.g."),mo.forEach(r),Er=i(ge,` a
derived class of `),Re=o(ge,"A",{href:!0});var uo=n(Re);$r=i(uo,"SequenceFeatureExtractor"),uo.forEach(r),wr=i(ge,"."),ge.forEach(r),Fr=d(he),_(Z.$$.fragment,he),Tr=d(he),_(ee.$$.fragment,he),he.forEach(r),Ir=d(fe),te=o(fe,"DIV",{class:!0});var Ht=n(te);_(be.$$.fragment,Ht),kr=d(Ht),W=o(Ht,"P",{});var Je=n(W);Pr=i(Je,"Save a feature_extractor object to the directory "),st=o(Je,"CODE",{});var po=n(st);zr=i(po,"save_directory"),po.forEach(r),qr=i(Je,`, so that it can be re-loaded using the
`),Ue=o(Je,"A",{href:!0});var fo=n(Ue);Mr=i(fo,"from_pretrained()"),fo.forEach(r),Dr=i(Je," class method."),Je.forEach(r),Ht.forEach(r),fe.forEach(r),Ct=d(t),R=o(t,"H2",{class:!0});var Gt=n(R);re=o(Gt,"A",{id:!0,class:!0,href:!0});var ho=n(re);it=o(ho,"SPAN",{});var go=n(it);_(Ee.$$.fragment,go),go.forEach(r),ho.forEach(r),Lr=d(Gt),ct=o(Gt,"SPAN",{});var _o=n(ct);Nr=i(_o,"SequenceFeatureExtractor"),_o.forEach(r),Gt.forEach(r),At=d(t),C=o(t,"DIV",{class:!0});var Ye=n(C);_($e.$$.fragment,Ye),Sr=d(Ye),dt=o(Ye,"P",{});var vo=n(dt);Br=i(vo,"This is a general feature extraction class for speech recognition."),vo.forEach(r),Cr=d(Ye),S=o(Ye,"DIV",{class:!0});var _e=n(S);_(we.$$.fragment,_e),Ar=d(_e),lt=o(_e,"P",{});var xo=n(lt);jr=i(xo,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),xo.forEach(r),Or=d(_e),U=o(_e,"P",{});var Ke=n(U);Vr=i(Ke,"Padding side (left/right) padding values are defined at the feature extractor level (with "),mt=o(Ke,"CODE",{});var yo=n(mt);Wr=i(yo,"self.padding_side"),yo.forEach(r),Rr=i(Ke,`,
`),ut=o(Ke,"CODE",{});var bo=n(ut);Ur=i(bo,"self.padding_value"),bo.forEach(r),Hr=i(Ke,")"),Ke.forEach(r),Gr=d(_e),_(ae.$$.fragment,_e),_e.forEach(r),Ye.forEach(r),jt=d(t),H=o(t,"H2",{class:!0});var Jt=n(H);oe=o(Jt,"A",{id:!0,class:!0,href:!0});var Eo=n(oe);pt=o(Eo,"SPAN",{});var $o=n(pt);_(Fe.$$.fragment,$o),$o.forEach(r),Eo.forEach(r),Jr=d(Jt),ft=o(Jt,"SPAN",{});var wo=n(ft);Yr=i(wo,"BatchFeature"),wo.forEach(r),Jt.forEach(r),Ot=d(t),k=o(t,"DIV",{class:!0});var A=n(k);_(Te.$$.fragment,A),Kr=d(A),G=o(A,"P",{});var Qe=n(G);Qr=i(Qe,"Holds the output of the "),He=o(Qe,"A",{href:!0});var Fo=n(He);Xr=i(Fo,"pad()"),Fo.forEach(r),Zr=i(Qe," and feature extractor specific "),ht=o(Qe,"CODE",{});var To=n(ht);ea=i(To,"__call__"),To.forEach(r),ta=i(Qe," methods."),Qe.forEach(r),ra=d(A),gt=o(A,"P",{});var Io=n(gt);aa=i(Io,"This class is derived from a python dictionary and can be used as a dictionary."),Io.forEach(r),oa=d(A),ne=o(A,"DIV",{class:!0});var Yt=n(ne);_(Ie.$$.fragment,Yt),na=d(Yt),_t=o(Yt,"P",{});var ko=n(_t);sa=i(ko,"Convert the inner content to tensors."),ko.forEach(r),Yt.forEach(r),ia=d(A),se=o(A,"DIV",{class:!0});var Kt=n(se);_(ke.$$.fragment,Kt),ca=d(Kt),Pe=o(Kt,"P",{});var Qt=n(Pe);da=i(Qt,"Send all values to device by calling "),vt=o(Qt,"CODE",{});var Po=n(vt);la=i(Po,"v.to(device)"),Po.forEach(r),ma=i(Qt," (PyTorch only)."),Qt.forEach(r),Kt.forEach(r),A.forEach(r),Vt=d(t),J=o(t,"H2",{class:!0});var Xt=n(J);ie=o(Xt,"A",{id:!0,class:!0,href:!0});var zo=n(ie);xt=o(zo,"SPAN",{});var qo=n(xt);_(ze.$$.fragment,qo),qo.forEach(r),zo.forEach(r),ua=d(Xt),yt=o(Xt,"SPAN",{});var Mo=n(yt);pa=i(Mo,"ImageFeatureExtractionMixin"),Mo.forEach(r),Xt.forEach(r),Wt=d(t),w=o(t,"DIV",{class:!0});var I=n(w);_(qe.$$.fragment,I),fa=d(I),bt=o(I,"P",{});var Do=n(bt);ha=i(Do,"Mixin that contain utilities for preparing image features."),Do.forEach(r),ga=d(I),ce=o(I,"DIV",{class:!0});var Zt=n(ce);_(Me.$$.fragment,Zt),_a=d(Zt),De=o(Zt,"P",{});var er=n(De);va=i(er,"Crops "),Et=o(er,"CODE",{});var Lo=n(Et);xa=i(Lo,"image"),Lo.forEach(r),ya=i(er,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),er.forEach(r),Zt.forEach(r),ba=d(I),de=o(I,"DIV",{class:!0});var tr=n(de);_(Le.$$.fragment,tr),Ea=d(tr),Y=o(tr,"P",{});var Xe=n(Y);$a=i(Xe,"Converts "),$t=o(Xe,"CODE",{});var No=n($t);wa=i(No,"image"),No.forEach(r),Fa=i(Xe," to RGB format. Note that this will trigger a conversion of "),wt=o(Xe,"CODE",{});var So=n(wt);Ta=i(So,"image"),So.forEach(r),Ia=i(Xe," to a PIL Image."),Xe.forEach(r),tr.forEach(r),ka=d(I),le=o(I,"DIV",{class:!0});var rr=n(le);_(Ne.$$.fragment,rr),Pa=d(rr),D=o(rr,"P",{});var j=n(D);za=i(j,"Normalizes "),Ft=o(j,"CODE",{});var Bo=n(Ft);qa=i(Bo,"image"),Bo.forEach(r),Ma=i(j," with "),Tt=o(j,"CODE",{});var Co=n(Tt);Da=i(Co,"mean"),Co.forEach(r),La=i(j," and "),It=o(j,"CODE",{});var Ao=n(It);Na=i(Ao,"std"),Ao.forEach(r),Sa=i(j,". Note that this will trigger a conversion of "),kt=o(j,"CODE",{});var jo=n(kt);Ba=i(jo,"image"),jo.forEach(r),Ca=i(j,` to a NumPy array
if it\u2019s a PIL Image.`),j.forEach(r),rr.forEach(r),Aa=d(I),me=o(I,"DIV",{class:!0});var ar=n(me);_(Se.$$.fragment,ar),ja=d(ar),K=o(ar,"P",{});var Ze=n(K);Oa=i(Ze,"Resizes "),Pt=o(Ze,"CODE",{});var Oo=n(Pt);Va=i(Oo,"image"),Oo.forEach(r),Wa=i(Ze,". Note that this will trigger a conversion of "),zt=o(Ze,"CODE",{});var Vo=n(zt);Ra=i(Vo,"image"),Vo.forEach(r),Ua=i(Ze," to a PIL Image."),Ze.forEach(r),ar.forEach(r),Ha=d(I),ue=o(I,"DIV",{class:!0});var or=n(ue);_(Be.$$.fragment,or),Ga=d(or),Ce=o(or,"P",{});var nr=n(Ce);Ja=i(nr,"Converts "),qt=o(nr,"CODE",{});var Wo=n(qt);Ya=i(Wo,"image"),Wo.forEach(r),Ka=i(nr,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),nr.forEach(r),or.forEach(r),Qa=d(I),pe=o(I,"DIV",{class:!0});var sr=n(pe);_(Ae.$$.fragment,sr),Xa=d(sr),je=o(sr,"P",{});var ir=n(je);Za=i(ir,"Converts "),Mt=o(ir,"CODE",{});var Ro=n(Mt);eo=i(Ro,"image"),Ro.forEach(r),to=i(ir,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),ir.forEach(r),sr.forEach(r),I.forEach(r),this.h()},h(){l(m,"name","hf:doc:metadata"),l(m,"content",JSON.stringify(on)),l(h,"id","feature-extractor"),l(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(h,"href","#feature-extractor"),l(f,"class","relative group"),l(X,"id","transformers.FeatureExtractionMixin"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#transformers.FeatureExtractionMixin"),l(V,"class","relative group"),l(We,"href","/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(Re,"href","/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ue,"href","/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained"),l(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(re,"id","transformers.SequenceFeatureExtractor"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#transformers.SequenceFeatureExtractor"),l(R,"class","relative group"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"id","transformers.BatchFeature"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#transformers.BatchFeature"),l(H,"class","relative group"),l(He,"href","/docs/transformers/pr_17628/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),l(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"id","transformers.ImageFeatureExtractionMixin"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.ImageFeatureExtractionMixin"),l(J,"class","relative group"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,m),E(t,T,p),E(t,f,p),e(f,h),e(h,F),v(u,F,null),e(f,$),e(f,L),e(L,O),E(t,z,p),E(t,q,p),e(q,Ve),e(q,et),e(et,cr),e(q,dr),e(q,tt),e(tt,lr),e(q,mr),E(t,St,p),E(t,V,p),e(V,X),e(X,rt),v(ve,rt,null),e(V,ur),e(V,at),e(at,pr),E(t,Bt,p),E(t,M,p),v(xe,M,null),e(M,fr),e(M,ot),e(ot,hr),e(M,gr),e(M,N),v(ye,N,null),e(N,_r),e(N,B),e(B,vr),e(B,We),e(We,xr),e(B,yr),e(B,nt),e(nt,br),e(B,Er),e(B,Re),e(Re,$r),e(B,wr),e(N,Fr),v(Z,N,null),e(N,Tr),v(ee,N,null),e(M,Ir),e(M,te),v(be,te,null),e(te,kr),e(te,W),e(W,Pr),e(W,st),e(st,zr),e(W,qr),e(W,Ue),e(Ue,Mr),e(W,Dr),E(t,Ct,p),E(t,R,p),e(R,re),e(re,it),v(Ee,it,null),e(R,Lr),e(R,ct),e(ct,Nr),E(t,At,p),E(t,C,p),v($e,C,null),e(C,Sr),e(C,dt),e(dt,Br),e(C,Cr),e(C,S),v(we,S,null),e(S,Ar),e(S,lt),e(lt,jr),e(S,Or),e(S,U),e(U,Vr),e(U,mt),e(mt,Wr),e(U,Rr),e(U,ut),e(ut,Ur),e(U,Hr),e(S,Gr),v(ae,S,null),E(t,jt,p),E(t,H,p),e(H,oe),e(oe,pt),v(Fe,pt,null),e(H,Jr),e(H,ft),e(ft,Yr),E(t,Ot,p),E(t,k,p),v(Te,k,null),e(k,Kr),e(k,G),e(G,Qr),e(G,He),e(He,Xr),e(G,Zr),e(G,ht),e(ht,ea),e(G,ta),e(k,ra),e(k,gt),e(gt,aa),e(k,oa),e(k,ne),v(Ie,ne,null),e(ne,na),e(ne,_t),e(_t,sa),e(k,ia),e(k,se),v(ke,se,null),e(se,ca),e(se,Pe),e(Pe,da),e(Pe,vt),e(vt,la),e(Pe,ma),E(t,Vt,p),E(t,J,p),e(J,ie),e(ie,xt),v(ze,xt,null),e(J,ua),e(J,yt),e(yt,pa),E(t,Wt,p),E(t,w,p),v(qe,w,null),e(w,fa),e(w,bt),e(bt,ha),e(w,ga),e(w,ce),v(Me,ce,null),e(ce,_a),e(ce,De),e(De,va),e(De,Et),e(Et,xa),e(De,ya),e(w,ba),e(w,de),v(Le,de,null),e(de,Ea),e(de,Y),e(Y,$a),e(Y,$t),e($t,wa),e(Y,Fa),e(Y,wt),e(wt,Ta),e(Y,Ia),e(w,ka),e(w,le),v(Ne,le,null),e(le,Pa),e(le,D),e(D,za),e(D,Ft),e(Ft,qa),e(D,Ma),e(D,Tt),e(Tt,Da),e(D,La),e(D,It),e(It,Na),e(D,Sa),e(D,kt),e(kt,Ba),e(D,Ca),e(w,Aa),e(w,me),v(Se,me,null),e(me,ja),e(me,K),e(K,Oa),e(K,Pt),e(Pt,Va),e(K,Wa),e(K,zt),e(zt,Ra),e(K,Ua),e(w,Ha),e(w,ue),v(Be,ue,null),e(ue,Ga),e(ue,Ce),e(Ce,Ja),e(Ce,qt),e(qt,Ya),e(Ce,Ka),e(w,Qa),e(w,pe),v(Ae,pe,null),e(pe,Xa),e(pe,je),e(je,Za),e(je,Mt),e(Mt,eo),e(je,to),Rt=!0},p(t,[p]){const Oe={};p&2&&(Oe.$$scope={dirty:p,ctx:t}),Z.$set(Oe);const Dt={};p&2&&(Dt.$$scope={dirty:p,ctx:t}),ee.$set(Dt);const Lt={};p&2&&(Lt.$$scope={dirty:p,ctx:t}),ae.$set(Lt)},i(t){Rt||(x(u.$$.fragment,t),x(ve.$$.fragment,t),x(xe.$$.fragment,t),x(ye.$$.fragment,t),x(Z.$$.fragment,t),x(ee.$$.fragment,t),x(be.$$.fragment,t),x(Ee.$$.fragment,t),x($e.$$.fragment,t),x(we.$$.fragment,t),x(ae.$$.fragment,t),x(Fe.$$.fragment,t),x(Te.$$.fragment,t),x(Ie.$$.fragment,t),x(ke.$$.fragment,t),x(ze.$$.fragment,t),x(qe.$$.fragment,t),x(Me.$$.fragment,t),x(Le.$$.fragment,t),x(Ne.$$.fragment,t),x(Se.$$.fragment,t),x(Be.$$.fragment,t),x(Ae.$$.fragment,t),Rt=!0)},o(t){y(u.$$.fragment,t),y(ve.$$.fragment,t),y(xe.$$.fragment,t),y(ye.$$.fragment,t),y(Z.$$.fragment,t),y(ee.$$.fragment,t),y(be.$$.fragment,t),y(Ee.$$.fragment,t),y($e.$$.fragment,t),y(we.$$.fragment,t),y(ae.$$.fragment,t),y(Fe.$$.fragment,t),y(Te.$$.fragment,t),y(Ie.$$.fragment,t),y(ke.$$.fragment,t),y(ze.$$.fragment,t),y(qe.$$.fragment,t),y(Me.$$.fragment,t),y(Le.$$.fragment,t),y(Ne.$$.fragment,t),y(Se.$$.fragment,t),y(Be.$$.fragment,t),y(Ae.$$.fragment,t),Rt=!1},d(t){r(m),t&&r(T),t&&r(f),b(u),t&&r(z),t&&r(q),t&&r(St),t&&r(V),b(ve),t&&r(Bt),t&&r(M),b(xe),b(ye),b(Z),b(ee),b(be),t&&r(Ct),t&&r(R),b(Ee),t&&r(At),t&&r(C),b($e),b(we),b(ae),t&&r(jt),t&&r(H),b(Fe),t&&r(Ot),t&&r(k),b(Te),b(Ie),b(ke),t&&r(Vt),t&&r(J),b(ze),t&&r(Wt),t&&r(w),b(qe),b(Me),b(Le),b(Ne),b(Se),b(Be),b(Ae)}}}const on={local:"feature-extractor",sections:[{local:"transformers.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function nn(Q){return Ko(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pn extends Ho{constructor(m){super();Go(this,m,nn,an,Jo,{})}}export{pn as default,on as metadata};
