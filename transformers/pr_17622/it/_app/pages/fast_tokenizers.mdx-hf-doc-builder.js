import{S as _t,i as $t,s as Tt,e as o,k as c,w as C,t as l,M as jt,c as r,d as t,m as f,a as s,x as F,h as p,b as z,G as a,g as n,y as O,L as Pt,q as B,o as x,B as D,v as wt}from"../chunks/vendor-hf-doc-builder.js";import{I as Ze}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Pe}from"../chunks/CodeBlock-hf-doc-builder.js";function Et(et){let d,ae,k,v,Q,w,we,W,Ee,ie,m,be,H,ye,qe,E,Se,Ae,oe,I,Ne,re,b,se,J,Ce,ne,h,_,G,y,Fe,R,Oe,le,$,Be,X,xe,De,pe,q,ce,T,Ie,U,Je,Ue,fe,g,j,Y,S,Le,Z,Ke,ze,L,Ve,me,A,ue,u,Me,ee,Qe,We,te,He,Ge,de,N,ke,P,Re,K,Xe,Ye,he;return w=new Ze({}),b=new Pe({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

tokenizer.pre_tokenizer = Whitespace()
files = [...]
tokenizer.train(files, trainer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`}}),y=new Ze({}),q=new Pe({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`}}),S=new Ze({}),A=new Pe({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),N=new Pe({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`}}),{c(){d=o("meta"),ae=c(),k=o("h1"),v=o("a"),Q=o("span"),C(w.$$.fragment),we=c(),W=o("span"),Ee=l("Usare i tokenizzatori di \u{1F917} Tokenizers"),ie=c(),m=o("p"),be=l("Il tokenizzatore "),H=o("code"),ye=l("PreTrainedTokenizerFast"),qe=l(" dipende dalla libreria "),E=o("a"),Se=l("\u{1F917} Tokenizers"),Ae=l(`. I tokenizzatori
disponibili nella libreria \u{1F917} Tokenizers possono essere caricati facilmente con \u{1F917} Transformers.`),oe=c(),I=o("p"),Ne=l("Prima di entrare nei dettagli, iniziamo creando un tokenizzatore con poche righe di codice:"),re=c(),C(b.$$.fragment),se=c(),J=o("p"),Ce=l("Adesso abbiamo un tokenizzatore addestrato sui file che abbiamo definito. Da qui possiamo sia continuare a usarlo in quel runtume o salvarlo in un file JSON per riutilizzarlo in seguito."),ne=c(),h=o("h2"),_=o("a"),G=o("span"),C(y.$$.fragment),Fe=c(),R=o("span"),Oe=l("Caricare un tokenizzatore da un oggetto"),le=c(),$=o("p"),Be=l(`Vediamo ora come utilizzare questo oggetto con la libreria \u{1F917} Transformers. La classe
`),X=o("code"),xe=l("PreTrainedTokenizerFast"),De=l(` pu\xF2 essere istanziata in modo semplice, accettando il tokenizzatore che abbiamo definito
come argomento:`),pe=c(),C(q.$$.fragment),ce=c(),T=o("p"),Ie=l("Questo oggetto pu\xF2 ora essere usato con tutti i metodi condivisi dai tokenizzatori in \u{1F917} Transformers! Vai alla "),U=o("a"),Je=l("documentazione dei tokenizzatori"),Ue=l(" per maggiori informazioni."),fe=c(),g=o("h2"),j=o("a"),Y=o("span"),C(S.$$.fragment),Le=c(),Z=o("span"),Ke=l("Caricare da un file JSON"),ze=c(),L=o("p"),Ve=l("Per caricare un tokenizzatore da un file JSON, iniziamo salvando il nostro tokenizzatore:"),me=c(),C(A.$$.fragment),ue=c(),u=o("p"),Me=l("Il percorso in cui abbiamo salvato il file pu\xF2 essere passato al metodo di inizializzazione di "),ee=o("code"),Qe=l("PreTrainedTokenizerFast"),We=l(` utilizzando il
parametro `),te=o("code"),He=l("tokenizer_file"),Ge=l(":"),de=c(),C(N.$$.fragment),ke=c(),P=o("p"),Re=l("Questo oggetto pu\xF2 ora essere usato con tutti i metodi condivisi dai tokenizzatori in \u{1F917} Transformers! Vai alla "),K=o("a"),Xe=l("documentazione dei tokenizzatori"),Ye=l(" per maggiori informazioni."),this.h()},l(e){const i=jt('[data-svelte="svelte-1phssyn"]',document.head);d=r(i,"META",{name:!0,content:!0}),i.forEach(t),ae=f(e),k=r(e,"H1",{class:!0});var ge=s(k);v=r(ge,"A",{id:!0,class:!0,href:!0});var tt=s(v);Q=r(tt,"SPAN",{});var at=s(Q);F(w.$$.fragment,at),at.forEach(t),tt.forEach(t),we=f(ge),W=r(ge,"SPAN",{});var it=s(W);Ee=p(it,"Usare i tokenizzatori di \u{1F917} Tokenizers"),it.forEach(t),ge.forEach(t),ie=f(e),m=r(e,"P",{});var V=s(m);be=p(V,"Il tokenizzatore "),H=r(V,"CODE",{});var ot=s(H);ye=p(ot,"PreTrainedTokenizerFast"),ot.forEach(t),qe=p(V," dipende dalla libreria "),E=r(V,"A",{href:!0,rel:!0});var rt=s(E);Se=p(rt,"\u{1F917} Tokenizers"),rt.forEach(t),Ae=p(V,`. I tokenizzatori
disponibili nella libreria \u{1F917} Tokenizers possono essere caricati facilmente con \u{1F917} Transformers.`),V.forEach(t),oe=f(e),I=r(e,"P",{});var st=s(I);Ne=p(st,"Prima di entrare nei dettagli, iniziamo creando un tokenizzatore con poche righe di codice:"),st.forEach(t),re=f(e),F(b.$$.fragment,e),se=f(e),J=r(e,"P",{});var nt=s(J);Ce=p(nt,"Adesso abbiamo un tokenizzatore addestrato sui file che abbiamo definito. Da qui possiamo sia continuare a usarlo in quel runtume o salvarlo in un file JSON per riutilizzarlo in seguito."),nt.forEach(t),ne=f(e),h=r(e,"H2",{class:!0});var ve=s(h);_=r(ve,"A",{id:!0,class:!0,href:!0});var lt=s(_);G=r(lt,"SPAN",{});var pt=s(G);F(y.$$.fragment,pt),pt.forEach(t),lt.forEach(t),Fe=f(ve),R=r(ve,"SPAN",{});var ct=s(R);Oe=p(ct,"Caricare un tokenizzatore da un oggetto"),ct.forEach(t),ve.forEach(t),le=f(e),$=r(e,"P",{});var _e=s($);Be=p(_e,`Vediamo ora come utilizzare questo oggetto con la libreria \u{1F917} Transformers. La classe
`),X=r(_e,"CODE",{});var ft=s(X);xe=p(ft,"PreTrainedTokenizerFast"),ft.forEach(t),De=p(_e,` pu\xF2 essere istanziata in modo semplice, accettando il tokenizzatore che abbiamo definito
come argomento:`),_e.forEach(t),pe=f(e),F(q.$$.fragment,e),ce=f(e),T=r(e,"P",{});var $e=s(T);Ie=p($e,"Questo oggetto pu\xF2 ora essere usato con tutti i metodi condivisi dai tokenizzatori in \u{1F917} Transformers! Vai alla "),U=r($e,"A",{href:!0});var zt=s(U);Je=p(zt,"documentazione dei tokenizzatori"),zt.forEach(t),Ue=p($e," per maggiori informazioni."),$e.forEach(t),fe=f(e),g=r(e,"H2",{class:!0});var Te=s(g);j=r(Te,"A",{id:!0,class:!0,href:!0});var mt=s(j);Y=r(mt,"SPAN",{});var ut=s(Y);F(S.$$.fragment,ut),ut.forEach(t),mt.forEach(t),Le=f(Te),Z=r(Te,"SPAN",{});var dt=s(Z);Ke=p(dt,"Caricare da un file JSON"),dt.forEach(t),Te.forEach(t),ze=f(e),L=r(e,"P",{});var kt=s(L);Ve=p(kt,"Per caricare un tokenizzatore da un file JSON, iniziamo salvando il nostro tokenizzatore:"),kt.forEach(t),me=f(e),F(A.$$.fragment,e),ue=f(e),u=r(e,"P",{});var M=s(u);Me=p(M,"Il percorso in cui abbiamo salvato il file pu\xF2 essere passato al metodo di inizializzazione di "),ee=r(M,"CODE",{});var ht=s(ee);Qe=p(ht,"PreTrainedTokenizerFast"),ht.forEach(t),We=p(M,` utilizzando il
parametro `),te=r(M,"CODE",{});var gt=s(te);He=p(gt,"tokenizer_file"),gt.forEach(t),Ge=p(M,":"),M.forEach(t),de=f(e),F(N.$$.fragment,e),ke=f(e),P=r(e,"P",{});var je=s(P);Re=p(je,"Questo oggetto pu\xF2 ora essere usato con tutti i metodi condivisi dai tokenizzatori in \u{1F917} Transformers! Vai alla "),K=r(je,"A",{href:!0});var vt=s(K);Xe=p(vt,"documentazione dei tokenizzatori"),vt.forEach(t),Ye=p(je," per maggiori informazioni."),je.forEach(t),this.h()},h(){z(d,"name","hf:doc:metadata"),z(d,"content",JSON.stringify(bt)),z(v,"id","usare-i-tokenizzatori-di-tokenizers"),z(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(v,"href","#usare-i-tokenizzatori-di-tokenizers"),z(k,"class","relative group"),z(E,"href","https://huggingface.co/docs/tokenizers"),z(E,"rel","nofollow"),z(_,"id","caricare-un-tokenizzatore-da-un-oggetto"),z(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(_,"href","#caricare-un-tokenizzatore-da-un-oggetto"),z(h,"class","relative group"),z(U,"href","main_classes/tokenizer"),z(j,"id","caricare-da-un-file-json"),z(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(j,"href","#caricare-da-un-file-json"),z(g,"class","relative group"),z(K,"href","main_classes/tokenizer")},m(e,i){a(document.head,d),n(e,ae,i),n(e,k,i),a(k,v),a(v,Q),O(w,Q,null),a(k,we),a(k,W),a(W,Ee),n(e,ie,i),n(e,m,i),a(m,be),a(m,H),a(H,ye),a(m,qe),a(m,E),a(E,Se),a(m,Ae),n(e,oe,i),n(e,I,i),a(I,Ne),n(e,re,i),O(b,e,i),n(e,se,i),n(e,J,i),a(J,Ce),n(e,ne,i),n(e,h,i),a(h,_),a(_,G),O(y,G,null),a(h,Fe),a(h,R),a(R,Oe),n(e,le,i),n(e,$,i),a($,Be),a($,X),a(X,xe),a($,De),n(e,pe,i),O(q,e,i),n(e,ce,i),n(e,T,i),a(T,Ie),a(T,U),a(U,Je),a(T,Ue),n(e,fe,i),n(e,g,i),a(g,j),a(j,Y),O(S,Y,null),a(g,Le),a(g,Z),a(Z,Ke),n(e,ze,i),n(e,L,i),a(L,Ve),n(e,me,i),O(A,e,i),n(e,ue,i),n(e,u,i),a(u,Me),a(u,ee),a(ee,Qe),a(u,We),a(u,te),a(te,He),a(u,Ge),n(e,de,i),O(N,e,i),n(e,ke,i),n(e,P,i),a(P,Re),a(P,K),a(K,Xe),a(P,Ye),he=!0},p:Pt,i(e){he||(B(w.$$.fragment,e),B(b.$$.fragment,e),B(y.$$.fragment,e),B(q.$$.fragment,e),B(S.$$.fragment,e),B(A.$$.fragment,e),B(N.$$.fragment,e),he=!0)},o(e){x(w.$$.fragment,e),x(b.$$.fragment,e),x(y.$$.fragment,e),x(q.$$.fragment,e),x(S.$$.fragment,e),x(A.$$.fragment,e),x(N.$$.fragment,e),he=!1},d(e){t(d),e&&t(ae),e&&t(k),D(w),e&&t(ie),e&&t(m),e&&t(oe),e&&t(I),e&&t(re),D(b,e),e&&t(se),e&&t(J),e&&t(ne),e&&t(h),D(y),e&&t(le),e&&t($),e&&t(pe),D(q,e),e&&t(ce),e&&t(T),e&&t(fe),e&&t(g),D(S),e&&t(ze),e&&t(L),e&&t(me),D(A,e),e&&t(ue),e&&t(u),e&&t(de),D(N,e),e&&t(ke),e&&t(P)}}}const bt={local:"usare-i-tokenizzatori-di-tokenizers",sections:[{local:"caricare-un-tokenizzatore-da-un-oggetto",title:"Caricare un tokenizzatore da un oggetto"},{local:"caricare-da-un-file-json",title:"Caricare da un file JSON"}],title:"Usare i tokenizzatori di \u{1F917} Tokenizers"};function yt(et){return wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nt extends _t{constructor(d){super();$t(this,d,yt,Et,Tt,{})}}export{Nt as default,bt as metadata};
