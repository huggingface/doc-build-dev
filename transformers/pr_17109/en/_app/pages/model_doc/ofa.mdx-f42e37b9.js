import{S as Pr,i as Sr,s as Lr,e as n,k as c,w as F,t as s,M as Nr,c as a,d as t,m as l,a as r,x as k,h as i,b as d,F as e,g as f,y as w,q as A,o as T,B as O,v as Ir,L as Er}from"../../chunks/vendor-6b77c823.js";import{T as Cr}from"../../chunks/Tip-39098574.js";import{D as Be}from"../../chunks/Docstring-1088f2fb.js";import{C as Mr}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Re}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as qr}from"../../chunks/ExampleCodeBlock-5212b321.js";function jr(S){let m,b,g,u,v;return{c(){m=n("p"),b=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(h){m=a(h,"P",{});var _=r(m);b=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(_,"CODE",{});var M=r(g);u=i(M,"Module"),M.forEach(t),v=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(h,_){f(h,m,_),e(m,b),e(m,g),e(g,u),e(m,v)},d(h){h&&t(m)}}}function Gr(S){let m,b,g,u,v;return u=new Mr({props:{code:`from transformers import OFATokenizer, OFAModel
import torch

tokenizer = OFATokenizer.from_pretrained("OFA-Sys/OFA-base")
model = OFAModel.from_pretrained("OFA-Sys/OFA-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OFATokenizer, OFAModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = OFATokenizer.from_pretrained(<span class="hljs-string">&quot;OFA-Sys/OFA-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OFAModel.from_pretrained(<span class="hljs-string">&quot;OFA-Sys/OFA-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){m=n("p"),b=s("Example:"),g=c(),F(u.$$.fragment)},l(h){m=a(h,"P",{});var _=r(m);b=i(_,"Example:"),_.forEach(t),g=l(h),k(u.$$.fragment,h)},m(h,_){f(h,m,_),e(m,b),f(h,g,_),w(u,h,_),v=!0},p:Er,i(h){v||(A(u.$$.fragment,h),v=!0)},o(h){T(u.$$.fragment,h),v=!1},d(h){h&&t(m),h&&t(g),O(u,h)}}}function Dr(S){let m,b,g,u,v;return{c(){m=n("p"),b=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(h){m=a(h,"P",{});var _=r(m);b=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(_,"CODE",{});var M=r(g);u=i(M,"Module"),M.forEach(t),v=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(h,_){f(h,m,_),e(m,b),e(m,g),e(g,u),e(m,v)},d(h){h&&t(m)}}}function Br(S){let m,b,g,u,v;return u=new Mr({props:{code:`model = OFAForConditionalGeneration.from_pretrained(ckpt_dir)
tokenizer = OFATokenizer.from_pretrained(ckpt_dir)

txt = " what is the description of the image?"
inputs = tokenizer([txt], max_length=1024, return_tensors="pt")["input_ids"]
img = Image.open(path_to_image)
patch_img = patch_resize_transform(img).unsqueeze(0)

gen = model.generate(inputs, patch_img=patch_img, num_beams=4)
print(tokenizer.decode(gen, skip_special_tokens=True, clean_up_tokenization_spaces=False))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model = OFAForConditionalGeneration.from_pretrained(ckpt_dir)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = OFATokenizer.from_pretrained(ckpt_dir)

<span class="hljs-meta">&gt;&gt;&gt; </span>txt = <span class="hljs-string">&quot; what is the description of the image?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([txt], max_length=<span class="hljs-number">1024</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(path_to_image)
<span class="hljs-meta">&gt;&gt;&gt; </span>patch_img = patch_resize_transform(img).unsqueeze(<span class="hljs-number">0</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>gen = model.generate(inputs, patch_img=patch_img, num_beams=<span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(gen, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>))`}}),{c(){m=n("p"),b=s("Image captioning example:"),g=c(),F(u.$$.fragment)},l(h){m=a(h,"P",{});var _=r(m);b=i(_,"Image captioning example:"),_.forEach(t),g=l(h),k(u.$$.fragment,h)},m(h,_){f(h,m,_),e(m,b),f(h,g,_),w(u,h,_),v=!0},p:Er,i(h){v||(A(u.$$.fragment,h),v=!0)},o(h){T(u.$$.fragment,h),v=!1},d(h){h&&t(m),h&&t(g),O(u,h)}}}function Rr(S){let m,b,g,u,v,h,_,M,Ot,Uo,j,W,uo,fe,yt,go,$t,Jo,Q,zt,me,xt,Ct,Zo,He,qt,Wo,Ve,Ue,Et,ue,Mt,Qo,Je,Pt,Yo,C,_o,vo,St,Lt,bo,Fo,Nt,It,ko,wo,jt,Gt,Ao,To,Dt,Ko,ge,Bt,_e,Rt,Xo,L,Ht,Ze,Vt,Ut,ve,Jt,Zt,et,G,Y,Oo,be,Wt,yo,Qt,ot,P,Fe,Yt,D,Kt,We,Xt,en,ke,on,tn,nn,B,an,Qe,rn,sn,Ye,dn,cn,tt,R,K,$o,we,ln,zo,hn,nt,z,Ae,pn,xo,fn,mn,X,Ke,un,gn,Xe,_n,vn,bn,Te,Fn,eo,kn,wn,at,H,ee,Co,Oe,An,qo,Tn,rt,x,ye,On,$e,yn,Eo,$n,zn,xn,oe,oo,Cn,qn,to,En,Mn,Pn,ze,Sn,no,Ln,Nn,st,V,te,Mo,xe,In,Po,jn,it,y,Ce,Gn,qe,Dn,ao,Bn,Rn,Hn,Ee,Vn,Me,Un,Jn,Zn,So,Wn,Qn,q,Pe,Yn,U,Kn,ro,Xn,ea,Lo,oa,ta,na,ne,aa,ae,dt,J,re,No,Se,ra,Io,sa,ct,$,Le,ia,Ne,da,so,ca,la,ha,Ie,pa,je,fa,ma,ua,jo,ga,_a,E,Ge,va,Z,ba,io,Fa,ka,Go,wa,Aa,Ta,se,Oa,ie,lt;return h=new Re({}),fe=new Re({}),be=new Re({}),Fe=new Be({props:{name:"class transformers.OFAConfig",anchor:"transformers.OFAConfig",parameters:[{name:"vocab_size",val:" = 59457"},{name:"max_position_embeddings",val:" = 1024"},{name:"encoder_layers",val:" = 4"},{name:"encoder_ffn_dim",val:" = 2048"},{name:"encoder_attention_heads",val:" = 8"},{name:"decoder_layers",val:" = 4"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 8"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"is_encoder_decoder",val:" = True"},{name:"activation_function",val:" = 'gelu'"},{name:"d_model",val:" = 512"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"classifier_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = False"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"forced_eos_token_id",val:" = 2"},{name:"encoder_normalize_before",val:" = True"},{name:"decoder_normalize_before",val:" = True"},{name:"normformer",val:" = True"},{name:"encoder_drop_path_rate",val:" = 0.0"},{name:"decoder_drop_path_rate",val:" = 0.0"},{name:"layernorm_embedding",val:" = True"},{name:"patch_layernorm_embedding",val:" = True"},{name:"resnet_type",val:" = 'resnet101'"},{name:"resnet_model_path",val:" = None"},{name:"resnet_drop_path_rate",val:" = 0.0"},{name:"token_bucket_size",val:" = 256"},{name:"image_bucket_size",val:" = 42"},{name:"add_type_embedding",val:" = True"},{name:"share_decoder_input_output_embed",val:" = True"},{name:"attn_scale_factor",val:" = 2.0"},{name:"code_layernorm_embedding",val:" = True"},{name:"code_image_size",val:" = 128"},{name:"entangle_position_embedding",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OFAConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the OFA model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAModel">~OFAModel</a> or <code>~TFOFAModel</code>.`,name:"vocab_size"},{anchor:"transformers.OFAConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimension of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.OFAConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.OFAConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.OFAConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.OFAConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.OFAConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.OFAConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.OFAConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.OFAConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OFAConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OFAConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.OFAConfig.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for classifier.`,name:"classifier_dropout"},{anchor:"transformers.OFAConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OFAConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
encoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the encoder. See the [LayerDrop paper](see <a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>)
for more details.
decoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the decoder. See the [LayerDrop paper](see <a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>)
for more details.`,name:"init_std"},{anchor:"transformers.OFAConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/configuration_ofa.py#L33"}}),we=new Re({}),Ae=new Be({props:{name:"class transformers.OFATokenizer",anchor:"transformers.OFATokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"errors",val:" = 'replace'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"add_prefix_space",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/tokenization_ofa.py#L47"}}),Oe=new Re({}),ye=new Be({props:{name:"class transformers.OFATokenizerFast",anchor:"transformers.OFATokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"errors",val:" = 'replace'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"add_prefix_space",val:" = False"},{name:"trim_offsets",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/tokenization_ofa_fast.py#L48"}}),xe=new Re({}),Ce=new Be({props:{name:"class transformers.OFAModel",anchor:"transformers.OFAModel",parameters:[{name:"config",val:": OFAConfig"}],parametersDescription:[{anchor:"transformers.OFAModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAConfig">~OFAConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/pr_17109/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.OFAModel.config",description:"<strong>config</strong> (OFAConfig) &#x2014; OFA configuration.",name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/modeling_ofa.py#L1607"}}),Pe=new Be({props:{name:"forward",anchor:"transformers.OFAModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"patch_images",val:" = None"},{name:"patch_images_2",val:" = None"},{name:"patch_masks",val:" = None"},{name:"token_embeddings",val:" = None"},{name:"sample_patch_num",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"code_masks",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"use_cache",val:" = False"},{name:"output_attentions",val:" = False"},{name:"output_hidden_states",val:" = False"}],parametersDescription:[{anchor:"transformers.OFAModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014;
indices of input sequence tokens in the vocabular, and padding will be ignored by default;</p>
<p>indices can be obtained using <a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizer">~OFATokenizer</a>.`,name:"input_ids"},{anchor:"transformers.OFAModel.forward.patch_images",description:`<strong>patch_images</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>) &#x2014;
the resized image, which are transformed by the default operations.`,name:"patch_images"},{anchor:"transformers.OFAModel.forward.patch_images_2",description:`<strong>patch_images_2</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>) &#x2014;
the second (if it exists) image.`,name:"patch_images_2"},{anchor:"transformers.OFAModel.forward.patch_masks",description:"<strong>patch_masks</strong> (<code>torch.BoolTensor</code>) &#x2014; the patches to be masked.",name:"patch_masks"},{anchor:"transformers.OFAModel.forward.token_embeddings",description:"<strong>token_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, seq_len, embed_dim)</code>) &#x2014; token embeddings.",name:"token_embeddings"},{anchor:"transformers.OFAModel.forward.sample_patch_num",description:"<strong>sample_patch_num</strong> (<code>int</code>) &#x2014; the number of patches to sample.",name:"sample_patch_num"},{anchor:"transformers.OFAModel.forward.decoder_input_ids",description:"<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; indices of the sequence in the vocabulary.",name:"decoder_input_ids"},{anchor:"transformers.OFAModel.forward.code_masks",description:"<strong>code_masks</strong> (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; masks only for code generation.",name:"code_masks"},{anchor:"transformers.OFAModel.forward.attention_mask",description:"<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; attention mask for decoding.",name:"attention_mask"},{anchor:"transformers.OFAModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>OFAEncoderOutput</code>) &#x2014;
encoder outputs with hidden states, positional embeddings, and padding masks.`,name:"encoder_outputs"},{anchor:"transformers.OFAModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(bsz, num_heads, tgt_len, head_size)</code>) and 2 additional tensors of shape <code>(bsz, num_heads, src_len, head_size)</code>.`,name:"past_key_values"},{anchor:"transformers.OFAModel.forward.use_cache",description:"<strong>use_cache</strong> (<code>bool</code>) &#x2014; whether to use cache for faster inference.",name:"use_cache"},{anchor:"transformers.OFAModel.forward.output_attentions",description:"<strong>output_attentions</strong> (<code>bool</code>) &#x2014; whether to output attention weights.",name:"output_attentions"},{anchor:"transformers.OFAModel.forward.output_hidden_states",description:"<strong>output_hidden_states</strong> (<code>bool</code>) &#x2014; whether to output hidden states.",name:"output_hidden_states"},{anchor:"transformers.OFAModel.forward.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; unused. Keep it for generation only.",name:"return_dict"},{anchor:"transformers.OFAModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.</p>
<p>Args &#x2014;
input_ids (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>):
indices of input sequence tokens in the vocabular, and padding will be ignored by default;</p>
<p>indices can be obtained using <a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizer">~OFATokenizer</a>.</p>
<p>patch_images (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>):
the resized image, which are transformed by the default operations.
patch_images_2 (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>):
the second (if it exists) image.
patch_masks (<code>torch.BoolTensor</code>): the patches to be masked.
token_embeddings (<code>torch.FloatTensor</code> of shape <code>(bsz, seq_len, embed_dim)</code>): token embeddings.
sample_patch_num (<code>int</code>): the number of patches to sample.
decoder_input_ids (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>):
indices of the sequence in the vocabulary.
code_masks (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>): masks only for code generation.
attention_mask (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>): attention mask for decoding.
encoder_outputs (<code>OFAEncoderOutput</code>):
encoder outputs with hidden states, positional embeddings, and padding masks.
past_key_values (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed):
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(bsz, num_heads, tgt_len, head_size)</code>) and 2 additional tensors of shape <code>(bsz, num_heads, src_len, head_size)</code>.
use_cache (<code>bool</code>): whether to use cache for faster inference.
output_attentions (<code>bool</code>): whether to output attention weights.
output_hidden_states (<code>bool</code>): whether to output hidden states.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/modeling_ofa.py#L1653",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17109/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAConfig"
>OFAConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17109/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ne=new Cr({props:{$$slots:{default:[jr]},$$scope:{ctx:S}}}),ae=new qr({props:{anchor:"transformers.OFAModel.forward.example",$$slots:{default:[Gr]},$$scope:{ctx:S}}}),Se=new Re({}),Le=new Be({props:{name:"class transformers.OFAForConditionalGeneration",anchor:"transformers.OFAForConditionalGeneration",parameters:[{name:"config",val:": OFAConfig"}],parametersDescription:[{anchor:"transformers.OFAForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAConfig">~OFAConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/pr_17109/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.OFAForConditionalGeneration.config",description:"<strong>config</strong> (OFAConfig) &#x2014; OFA configuration.",name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/modeling_ofa.py#L1759"}}),Ge=new Be({props:{name:"forward",anchor:"transformers.OFAForConditionalGeneration.forward",parameters:[{name:"input_ids",val:" = None"},{name:"patch_images",val:" = None"},{name:"patch_images_2",val:" = None"},{name:"patch_masks",val:" = None"},{name:"token_embeddings",val:" = None"},{name:"sample_patch_num",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"code_masks",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"use_cache",val:" = False"},{name:"output_attentions",val:" = False"},{name:"output_hidden_states",val:" = False"},{name:"return_dict",val:" = False"},{name:"labels",val:" = None"}],parametersDescription:[{anchor:"transformers.OFAForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014;
indices of input sequence tokens in the vocabular, and padding will be ignored by default;</p>
<p>indices can be obtained using <a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizer">~OFATokenizer</a>.`,name:"input_ids"},{anchor:"transformers.OFAForConditionalGeneration.forward.patch_images",description:`<strong>patch_images</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>) &#x2014;
the resized image, which are transformed by the default operations.`,name:"patch_images"},{anchor:"transformers.OFAForConditionalGeneration.forward.patch_images_2",description:`<strong>patch_images_2</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>) &#x2014;
the second (if it exists) image.`,name:"patch_images_2"},{anchor:"transformers.OFAForConditionalGeneration.forward.patch_masks",description:"<strong>patch_masks</strong> (<code>torch.BoolTensor</code>) &#x2014; the patches to be masked.",name:"patch_masks"},{anchor:"transformers.OFAForConditionalGeneration.forward.token_embeddings",description:"<strong>token_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(bsz, seq_len, embed_dim)</code>) &#x2014; token embeddings.",name:"token_embeddings"},{anchor:"transformers.OFAForConditionalGeneration.forward.sample_patch_num",description:"<strong>sample_patch_num</strong> (<code>int</code>) &#x2014; the number of patches to sample.",name:"sample_patch_num"},{anchor:"transformers.OFAForConditionalGeneration.forward.decoder_input_ids",description:"<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; indices of the sequence in the vocabulary.",name:"decoder_input_ids"},{anchor:"transformers.OFAForConditionalGeneration.forward.code_masks",description:"<strong>code_masks</strong> (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; masks only for code generation.",name:"code_masks"},{anchor:"transformers.OFAForConditionalGeneration.forward.attention_mask",description:"<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>) &#x2014; attention mask for decoding.",name:"attention_mask"},{anchor:"transformers.OFAForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>OFAEncoderOutput</code>) &#x2014;
encoder outputs with hidden states, positional embeddings, and padding masks.`,name:"encoder_outputs"},{anchor:"transformers.OFAForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(bsz, num_heads, tgt_len, head_size)</code>) and 2 additional tensors of shape <code>(bsz, num_heads, src_len, head_size)</code>.`,name:"past_key_values"},{anchor:"transformers.OFAForConditionalGeneration.forward.use_cache",description:"<strong>use_cache</strong> (<code>bool</code>) &#x2014; whether to use cache for faster inference.",name:"use_cache"},{anchor:"transformers.OFAForConditionalGeneration.forward.output_attentions",description:"<strong>output_attentions</strong> (<code>bool</code>) &#x2014; whether to output attention weights.",name:"output_attentions"},{anchor:"transformers.OFAForConditionalGeneration.forward.output_hidden_states",description:"<strong>output_hidden_states</strong> (<code>bool</code>) &#x2014; whether to output hidden states.",name:"output_hidden_states"},{anchor:"transformers.OFAForConditionalGeneration.forward.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; unused. Keep it for generation only.",name:"return_dict"},{anchor:"transformers.OFAForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.</p>
<p>Args &#x2014;
input_ids (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>):
indices of input sequence tokens in the vocabulary, and padding will be ignored by default;</p>
<p>indices can be obtained using <a href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizer">~OFATokenizer</a>.</p>
<p>patch_images (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>):
the resized image, which are transformed by the default operations.
patch_images_2 (<code>torch.FloatTensor</code> of shape <code>(bsz, 3, height, width)</code>):
the second (if it exists) image.
patch_masks (<code>torch.BoolTensor</code>): the patches to be masked.
token_embeddings (<code>torch.FloatTensor</code> of shape <code>(bsz, seq_len, embed_dim)</code>): token embeddings.
sample_patch_num (<code>int</code>): the number of patches to sample.
decoder_input_ids (<code>torch.LongTensor</code> of shape <code>(bsz, seq_len)</code>):
indices of the sequence in the vocabulary.
code_masks (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>): masks only for code generation.
attention_mask (<code>torch.Tensor</code> of shape <code>(bsz, seq_len)</code>): attention mask for decoding.
encoder_outputs (<code>OFAEncoderOutput</code>):
encoder outputs with hidden states, positional embeddings, and padding masks.
past_key_values (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed):
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(bsz, num_heads, tgt_len, head_size)</code>) and 2 additional tensors of shape <code>(bsz, num_heads, src_len, head_size)</code>.
use_cache (<code>bool</code>): whether to use cache for faster inference.
output_attentions (<code>bool</code>): whether to output attention weights.
output_hidden_states (<code>bool</code>): whether to output hidden states.
return_dict (<code>bool</code>): unused. Keep it for generation only.
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17109/src/transformers/models/ofa/modeling_ofa.py#L1814",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17109/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAConfig"
>OFAConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
<p>Seq2SeqModelOutput (<code>ModelOutput</code>): model outputs with hidden states and attentions.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17109/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new Cr({props:{$$slots:{default:[Dr]},$$scope:{ctx:S}}}),ie=new qr({props:{anchor:"transformers.OFAForConditionalGeneration.forward.example",$$slots:{default:[Br]},$$scope:{ctx:S}}}),{c(){m=n("meta"),b=c(),g=n("h1"),u=n("a"),v=n("span"),F(h.$$.fragment),_=c(),M=n("span"),Ot=s("OFA"),Uo=c(),j=n("h2"),W=n("a"),uo=n("span"),F(fe.$$.fragment),yt=c(),go=n("span"),$t=s("Overview"),Jo=c(),Q=n("p"),zt=s("The OFA model was proposed in "),me=n("a"),xt=s("Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"),Ct=s("  by Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA is a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework."),Zo=c(),He=n("p"),qt=s("The abstract from the paper is the following:"),Wo=c(),Ve=n("p"),Ue=n("em"),Et=s("In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework based on the encoder-decoder architecture. OFA performs pretraining and finetuning with task instructions and introduces no extra task-specific layers for finetuning. Experimental results show that OFA achieves new state-of-the-arts on a series of multimodal tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ / RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we demonstrate that OFA reaches comparable performance with uni-modal pretrained models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks, including NLU, NLG, and image classification, and it effectively transfers to unseen tasks and domains. Code shall be released soon at this "),ue=n("a"),Mt=s("http URL"),Qo=c(),Je=n("p"),Pt=s("Tips:"),Yo=c(),C=n("ul"),_o=n("li"),vo=n("p"),St=s("OFA is a model that accepts images, texts and/or bounding boxes as inputs and outputs texts, images (discrete codes), and/or bounding boxes."),Lt=c(),bo=n("li"),Fo=n("p"),Nt=s("OFA formalizes tasks as sequence-to-sequence transformation, and thus it can use conditional generation to perform tasks like image captioning, VQA, text-to-image generation, referring expression comprehension, etc."),It=c(),ko=n("li"),wo=n("p"),jt=s("OFA performs best on image captioning and referring expression comprehension."),Gt=c(),Ao=n("li"),To=n("p"),Dt=s("OFA can be finetuned on either cross-modal or unimodal tasks."),Ko=c(),ge=n("p"),Bt=s("Relevant checkpoints can be found at "),_e=n("a"),Rt=s("https://huggingface.co/OFA-Sys"),Xo=c(),L=n("p"),Ht=s("This model was contributed by "),Ze=n("a"),Vt=s("JustinLin610"),Ut=s(". The original code can be found "),ve=n("a"),Jt=s("here"),Zt=s("."),et=c(),G=n("h2"),Y=n("a"),Oo=n("span"),F(be.$$.fragment),Wt=c(),yo=n("span"),Qt=s("OFAConfig"),ot=c(),P=n("div"),F(Fe.$$.fragment),Yt=c(),D=n("p"),Kt=s("This is the configuration class to store the configuration of a "),We=n("a"),Xt=s("~OFAModel"),en=s(`. It is used to instantiate an OFA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the OFA `),ke=n("a"),on=s("ofa-base"),tn=s(`
architecture.`),nn=c(),B=n("p"),an=s("Configuration objects inherit from "),Qe=n("a"),rn=s("PretrainedConfig"),sn=s(` and can be used to control the model outputs. Read the
documentation from `),Ye=n("a"),dn=s("PretrainedConfig"),cn=s(" for more information."),tt=c(),R=n("h2"),K=n("a"),$o=n("span"),F(we.$$.fragment),ln=c(),zo=n("span"),hn=s("OFATokenizer"),nt=c(),z=n("div"),F(Ae.$$.fragment),pn=c(),xo=n("p"),fn=s("Construct a OFA tokenizer."),mn=c(),X=n("p"),Ke=n("a"),un=s("~OFATokenizer"),gn=s(" is identical to "),Xe=n("a"),_n=s("BartTokenizer"),vn=s(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),bn=c(),Te=n("p"),Fn=s("Refer to superclass "),eo=n("a"),kn=s("BartTokenizer"),wn=s(" for usage examples and documentation concerning parameters."),at=c(),H=n("h2"),ee=n("a"),Co=n("span"),F(Oe.$$.fragment),An=c(),qo=n("span"),Tn=s("OFATokenizerFast"),rt=c(),x=n("div"),F(ye.$$.fragment),On=c(),$e=n("p"),yn=s("Construct a \u201Cfast\u201D OFA tokenizer (backed by HuggingFace\u2019s "),Eo=n("em"),$n=s("tokenizers"),zn=s(" library)."),xn=c(),oe=n("p"),oo=n("a"),Cn=s("~OFATokenizerFast"),qn=s(" is identical to "),to=n("a"),En=s("BartTokenizerFast"),Mn=s(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),Pn=c(),ze=n("p"),Sn=s("Refer to superclass "),no=n("a"),Ln=s("BartTokenizerFast"),Nn=s(" for usage examples and documentation concerning parameters."),st=c(),V=n("h2"),te=n("a"),Mo=n("span"),F(xe.$$.fragment),In=c(),Po=n("span"),jn=s("OFAModel"),it=c(),y=n("div"),F(Ce.$$.fragment),Gn=c(),qe=n("p"),Dn=s(`The bare OFA Model outputting raw hidden-states without any specific head on top.
This model inherits from `),ao=n("a"),Bn=s("PreTrainedModel"),Rn=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hn=c(),Ee=n("p"),Vn=s("This model is also a PyTorch "),Me=n("a"),Un=s("torch.nn.Module"),Jn=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zn=c(),So=n("p"),Wn=s("The OFA model built with an encoder and a decoder only, without any classification head."),Qn=c(),q=n("div"),F(Pe.$$.fragment),Yn=c(),U=n("p"),Kn=s("The "),ro=n("a"),Xn=s("OFAModel"),ea=s(" forward method, overrides the "),Lo=n("code"),oa=s("__call__"),ta=s(" special method."),na=c(),F(ne.$$.fragment),aa=c(),F(ae.$$.fragment),dt=c(),J=n("h2"),re=n("a"),No=n("span"),F(Se.$$.fragment),ra=c(),Io=n("span"),sa=s("OFAForConditionalGeneration"),ct=c(),$=n("div"),F(Le.$$.fragment),ia=c(),Ne=n("p"),da=s(`The OFA Model with a language modeling head. Can be used for conditional generation.
This model inherits from `),so=n("a"),ca=s("PreTrainedModel"),la=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ha=c(),Ie=n("p"),pa=s("This model is also a PyTorch "),je=n("a"),fa=s("torch.nn.Module"),ma=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ua=c(),jo=n("p"),ga=s("The OFA model for conditional generation, which can adapt to most tasks due to the unified nature of OFA."),_a=c(),E=n("div"),F(Ge.$$.fragment),va=c(),Z=n("p"),ba=s("The "),io=n("a"),Fa=s("OFAForConditionalGeneration"),ka=s(" forward method, overrides the "),Go=n("code"),wa=s("__call__"),Aa=s(" special method."),Ta=c(),F(se.$$.fragment),Oa=c(),F(ie.$$.fragment),this.h()},l(o){const p=Nr('[data-svelte="svelte-1phssyn"]',document.head);m=a(p,"META",{name:!0,content:!0}),p.forEach(t),b=l(o),g=a(o,"H1",{class:!0});var De=r(g);u=a(De,"A",{id:!0,class:!0,href:!0});var Do=r(u);v=a(Do,"SPAN",{});var Bo=r(v);k(h.$$.fragment,Bo),Bo.forEach(t),Do.forEach(t),_=l(De),M=a(De,"SPAN",{});var Ro=r(M);Ot=i(Ro,"OFA"),Ro.forEach(t),De.forEach(t),Uo=l(o),j=a(o,"H2",{class:!0});var ht=r(j);W=a(ht,"A",{id:!0,class:!0,href:!0});var za=r(W);uo=a(za,"SPAN",{});var xa=r(uo);k(fe.$$.fragment,xa),xa.forEach(t),za.forEach(t),yt=l(ht),go=a(ht,"SPAN",{});var Ca=r(go);$t=i(Ca,"Overview"),Ca.forEach(t),ht.forEach(t),Jo=l(o),Q=a(o,"P",{});var pt=r(Q);zt=i(pt,"The OFA model was proposed in "),me=a(pt,"A",{href:!0,rel:!0});var qa=r(me);xt=i(qa,"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"),qa.forEach(t),Ct=i(pt,"  by Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA is a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework."),pt.forEach(t),Zo=l(o),He=a(o,"P",{});var Ea=r(He);qt=i(Ea,"The abstract from the paper is the following:"),Ea.forEach(t),Wo=l(o),Ve=a(o,"P",{});var Ma=r(Ve);Ue=a(Ma,"EM",{});var ya=r(Ue);Et=i(ya,"In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework based on the encoder-decoder architecture. OFA performs pretraining and finetuning with task instructions and introduces no extra task-specific layers for finetuning. Experimental results show that OFA achieves new state-of-the-arts on a series of multimodal tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ / RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we demonstrate that OFA reaches comparable performance with uni-modal pretrained models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks, including NLU, NLG, and image classification, and it effectively transfers to unseen tasks and domains. Code shall be released soon at this "),ue=a(ya,"A",{href:!0,rel:!0});var Pa=r(ue);Mt=i(Pa,"http URL"),Pa.forEach(t),ya.forEach(t),Ma.forEach(t),Qo=l(o),Je=a(o,"P",{});var Sa=r(Je);Pt=i(Sa,"Tips:"),Sa.forEach(t),Yo=l(o),C=a(o,"UL",{});var de=r(C);_o=a(de,"LI",{});var La=r(_o);vo=a(La,"P",{});var Na=r(vo);St=i(Na,"OFA is a model that accepts images, texts and/or bounding boxes as inputs and outputs texts, images (discrete codes), and/or bounding boxes."),Na.forEach(t),La.forEach(t),Lt=l(de),bo=a(de,"LI",{});var Ia=r(bo);Fo=a(Ia,"P",{});var ja=r(Fo);Nt=i(ja,"OFA formalizes tasks as sequence-to-sequence transformation, and thus it can use conditional generation to perform tasks like image captioning, VQA, text-to-image generation, referring expression comprehension, etc."),ja.forEach(t),Ia.forEach(t),It=l(de),ko=a(de,"LI",{});var Ga=r(ko);wo=a(Ga,"P",{});var Da=r(wo);jt=i(Da,"OFA performs best on image captioning and referring expression comprehension."),Da.forEach(t),Ga.forEach(t),Gt=l(de),Ao=a(de,"LI",{});var Ba=r(Ao);To=a(Ba,"P",{});var Ra=r(To);Dt=i(Ra,"OFA can be finetuned on either cross-modal or unimodal tasks."),Ra.forEach(t),Ba.forEach(t),de.forEach(t),Ko=l(o),ge=a(o,"P",{});var $a=r(ge);Bt=i($a,"Relevant checkpoints can be found at "),_e=a($a,"A",{href:!0,rel:!0});var Ha=r(_e);Rt=i(Ha,"https://huggingface.co/OFA-Sys"),Ha.forEach(t),$a.forEach(t),Xo=l(o),L=a(o,"P",{});var co=r(L);Ht=i(co,"This model was contributed by "),Ze=a(co,"A",{href:!0});var Va=r(Ze);Vt=i(Va,"JustinLin610"),Va.forEach(t),Ut=i(co,". The original code can be found "),ve=a(co,"A",{href:!0,rel:!0});var Ua=r(ve);Jt=i(Ua,"here"),Ua.forEach(t),Zt=i(co,"."),co.forEach(t),et=l(o),G=a(o,"H2",{class:!0});var ft=r(G);Y=a(ft,"A",{id:!0,class:!0,href:!0});var Ja=r(Y);Oo=a(Ja,"SPAN",{});var Za=r(Oo);k(be.$$.fragment,Za),Za.forEach(t),Ja.forEach(t),Wt=l(ft),yo=a(ft,"SPAN",{});var Wa=r(yo);Qt=i(Wa,"OFAConfig"),Wa.forEach(t),ft.forEach(t),ot=l(o),P=a(o,"DIV",{class:!0});var lo=r(P);k(Fe.$$.fragment,lo),Yt=l(lo),D=a(lo,"P",{});var ho=r(D);Kt=i(ho,"This is the configuration class to store the configuration of a "),We=a(ho,"A",{href:!0});var Qa=r(We);Xt=i(Qa,"~OFAModel"),Qa.forEach(t),en=i(ho,`. It is used to instantiate an OFA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the OFA `),ke=a(ho,"A",{href:!0,rel:!0});var Ya=r(ke);on=i(Ya,"ofa-base"),Ya.forEach(t),tn=i(ho,`
architecture.`),ho.forEach(t),nn=l(lo),B=a(lo,"P",{});var po=r(B);an=i(po,"Configuration objects inherit from "),Qe=a(po,"A",{href:!0});var Ka=r(Qe);rn=i(Ka,"PretrainedConfig"),Ka.forEach(t),sn=i(po,` and can be used to control the model outputs. Read the
documentation from `),Ye=a(po,"A",{href:!0});var Xa=r(Ye);dn=i(Xa,"PretrainedConfig"),Xa.forEach(t),cn=i(po," for more information."),po.forEach(t),lo.forEach(t),tt=l(o),R=a(o,"H2",{class:!0});var mt=r(R);K=a(mt,"A",{id:!0,class:!0,href:!0});var er=r(K);$o=a(er,"SPAN",{});var or=r($o);k(we.$$.fragment,or),or.forEach(t),er.forEach(t),ln=l(mt),zo=a(mt,"SPAN",{});var tr=r(zo);hn=i(tr,"OFATokenizer"),tr.forEach(t),mt.forEach(t),nt=l(o),z=a(o,"DIV",{class:!0});var ce=r(z);k(Ae.$$.fragment,ce),pn=l(ce),xo=a(ce,"P",{});var nr=r(xo);fn=i(nr,"Construct a OFA tokenizer."),nr.forEach(t),mn=l(ce),X=a(ce,"P",{});var Ho=r(X);Ke=a(Ho,"A",{href:!0});var ar=r(Ke);un=i(ar,"~OFATokenizer"),ar.forEach(t),gn=i(Ho," is identical to "),Xe=a(Ho,"A",{href:!0});var rr=r(Xe);_n=i(rr,"BartTokenizer"),rr.forEach(t),vn=i(Ho,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),Ho.forEach(t),bn=l(ce),Te=a(ce,"P",{});var ut=r(Te);Fn=i(ut,"Refer to superclass "),eo=a(ut,"A",{href:!0});var sr=r(eo);kn=i(sr,"BartTokenizer"),sr.forEach(t),wn=i(ut," for usage examples and documentation concerning parameters."),ut.forEach(t),ce.forEach(t),at=l(o),H=a(o,"H2",{class:!0});var gt=r(H);ee=a(gt,"A",{id:!0,class:!0,href:!0});var ir=r(ee);Co=a(ir,"SPAN",{});var dr=r(Co);k(Oe.$$.fragment,dr),dr.forEach(t),ir.forEach(t),An=l(gt),qo=a(gt,"SPAN",{});var cr=r(qo);Tn=i(cr,"OFATokenizerFast"),cr.forEach(t),gt.forEach(t),rt=l(o),x=a(o,"DIV",{class:!0});var le=r(x);k(ye.$$.fragment,le),On=l(le),$e=a(le,"P",{});var _t=r($e);yn=i(_t,"Construct a \u201Cfast\u201D OFA tokenizer (backed by HuggingFace\u2019s "),Eo=a(_t,"EM",{});var lr=r(Eo);$n=i(lr,"tokenizers"),lr.forEach(t),zn=i(_t," library)."),_t.forEach(t),xn=l(le),oe=a(le,"P",{});var Vo=r(oe);oo=a(Vo,"A",{href:!0});var hr=r(oo);Cn=i(hr,"~OFATokenizerFast"),hr.forEach(t),qn=i(Vo," is identical to "),to=a(Vo,"A",{href:!0});var pr=r(to);En=i(pr,"BartTokenizerFast"),pr.forEach(t),Mn=i(Vo,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),Vo.forEach(t),Pn=l(le),ze=a(le,"P",{});var vt=r(ze);Sn=i(vt,"Refer to superclass "),no=a(vt,"A",{href:!0});var fr=r(no);Ln=i(fr,"BartTokenizerFast"),fr.forEach(t),Nn=i(vt," for usage examples and documentation concerning parameters."),vt.forEach(t),le.forEach(t),st=l(o),V=a(o,"H2",{class:!0});var bt=r(V);te=a(bt,"A",{id:!0,class:!0,href:!0});var mr=r(te);Mo=a(mr,"SPAN",{});var ur=r(Mo);k(xe.$$.fragment,ur),ur.forEach(t),mr.forEach(t),In=l(bt),Po=a(bt,"SPAN",{});var gr=r(Po);jn=i(gr,"OFAModel"),gr.forEach(t),bt.forEach(t),it=l(o),y=a(o,"DIV",{class:!0});var N=r(y);k(Ce.$$.fragment,N),Gn=l(N),qe=a(N,"P",{});var Ft=r(qe);Dn=i(Ft,`The bare OFA Model outputting raw hidden-states without any specific head on top.
This model inherits from `),ao=a(Ft,"A",{href:!0});var _r=r(ao);Bn=i(_r,"PreTrainedModel"),_r.forEach(t),Rn=i(Ft,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ft.forEach(t),Hn=l(N),Ee=a(N,"P",{});var kt=r(Ee);Vn=i(kt,"This model is also a PyTorch "),Me=a(kt,"A",{href:!0,rel:!0});var vr=r(Me);Un=i(vr,"torch.nn.Module"),vr.forEach(t),Jn=i(kt,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kt.forEach(t),Zn=l(N),So=a(N,"P",{});var br=r(So);Wn=i(br,"The OFA model built with an encoder and a decoder only, without any classification head."),br.forEach(t),Qn=l(N),q=a(N,"DIV",{class:!0});var he=r(q);k(Pe.$$.fragment,he),Yn=l(he),U=a(he,"P",{});var fo=r(U);Kn=i(fo,"The "),ro=a(fo,"A",{href:!0});var Fr=r(ro);Xn=i(Fr,"OFAModel"),Fr.forEach(t),ea=i(fo," forward method, overrides the "),Lo=a(fo,"CODE",{});var kr=r(Lo);oa=i(kr,"__call__"),kr.forEach(t),ta=i(fo," special method."),fo.forEach(t),na=l(he),k(ne.$$.fragment,he),aa=l(he),k(ae.$$.fragment,he),he.forEach(t),N.forEach(t),dt=l(o),J=a(o,"H2",{class:!0});var wt=r(J);re=a(wt,"A",{id:!0,class:!0,href:!0});var wr=r(re);No=a(wr,"SPAN",{});var Ar=r(No);k(Se.$$.fragment,Ar),Ar.forEach(t),wr.forEach(t),ra=l(wt),Io=a(wt,"SPAN",{});var Tr=r(Io);sa=i(Tr,"OFAForConditionalGeneration"),Tr.forEach(t),wt.forEach(t),ct=l(o),$=a(o,"DIV",{class:!0});var I=r($);k(Le.$$.fragment,I),ia=l(I),Ne=a(I,"P",{});var At=r(Ne);da=i(At,`The OFA Model with a language modeling head. Can be used for conditional generation.
This model inherits from `),so=a(At,"A",{href:!0});var Or=r(so);ca=i(Or,"PreTrainedModel"),Or.forEach(t),la=i(At,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),At.forEach(t),ha=l(I),Ie=a(I,"P",{});var Tt=r(Ie);pa=i(Tt,"This model is also a PyTorch "),je=a(Tt,"A",{href:!0,rel:!0});var yr=r(je);fa=i(yr,"torch.nn.Module"),yr.forEach(t),ma=i(Tt,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Tt.forEach(t),ua=l(I),jo=a(I,"P",{});var $r=r(jo);ga=i($r,"The OFA model for conditional generation, which can adapt to most tasks due to the unified nature of OFA."),$r.forEach(t),_a=l(I),E=a(I,"DIV",{class:!0});var pe=r(E);k(Ge.$$.fragment,pe),va=l(pe),Z=a(pe,"P",{});var mo=r(Z);ba=i(mo,"The "),io=a(mo,"A",{href:!0});var zr=r(io);Fa=i(zr,"OFAForConditionalGeneration"),zr.forEach(t),ka=i(mo," forward method, overrides the "),Go=a(mo,"CODE",{});var xr=r(Go);wa=i(xr,"__call__"),xr.forEach(t),Aa=i(mo," special method."),mo.forEach(t),Ta=l(pe),k(se.$$.fragment,pe),Oa=l(pe),k(ie.$$.fragment,pe),pe.forEach(t),I.forEach(t),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(Hr)),d(u,"id","ofa"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#ofa"),d(g,"class","relative group"),d(W,"id","overview"),d(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(W,"href","#overview"),d(j,"class","relative group"),d(me,"href","http://arxiv.org/abs/2202.03052"),d(me,"rel","nofollow"),d(ue,"href","https://github.com/OFA-Sys/OFA"),d(ue,"rel","nofollow"),d(_e,"href","https://huggingface.co/OFA-Sys"),d(_e,"rel","nofollow"),d(Ze,"href","<https://huggingface.co/JustinLin610"),d(ve,"href","https://github.com/OFA-Sys/OFA"),d(ve,"rel","nofollow"),d(Y,"id","transformers.OFAConfig"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#transformers.OFAConfig"),d(G,"class","relative group"),d(We,"href","/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAModel"),d(ke,"href","https://huggingface.co/ofa-base"),d(ke,"rel","nofollow"),d(Qe,"href","/docs/transformers/pr_17109/en/main_classes/configuration#transformers.PretrainedConfig"),d(Ye,"href","/docs/transformers/pr_17109/en/main_classes/configuration#transformers.PretrainedConfig"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(K,"id","transformers.OFATokenizer"),d(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K,"href","#transformers.OFATokenizer"),d(R,"class","relative group"),d(Ke,"href","/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizer"),d(Xe,"href","/docs/transformers/pr_17109/en/model_doc/bart#transformers.BartTokenizer"),d(eo,"href","/docs/transformers/pr_17109/en/model_doc/bart#transformers.BartTokenizer"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ee,"id","transformers.OFATokenizerFast"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#transformers.OFATokenizerFast"),d(H,"class","relative group"),d(oo,"href","/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFATokenizerFast"),d(to,"href","/docs/transformers/pr_17109/en/model_doc/bart#transformers.BartTokenizerFast"),d(no,"href","/docs/transformers/pr_17109/en/model_doc/bart#transformers.BartTokenizerFast"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(te,"id","transformers.OFAModel"),d(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(te,"href","#transformers.OFAModel"),d(V,"class","relative group"),d(ao,"href","/docs/transformers/pr_17109/en/main_classes/model#transformers.PreTrainedModel"),d(Me,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Me,"rel","nofollow"),d(ro,"href","/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAModel"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(re,"id","transformers.OFAForConditionalGeneration"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#transformers.OFAForConditionalGeneration"),d(J,"class","relative group"),d(so,"href","/docs/transformers/pr_17109/en/main_classes/model#transformers.PreTrainedModel"),d(je,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(je,"rel","nofollow"),d(io,"href","/docs/transformers/pr_17109/en/model_doc/ofa#transformers.OFAForConditionalGeneration"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,p){e(document.head,m),f(o,b,p),f(o,g,p),e(g,u),e(u,v),w(h,v,null),e(g,_),e(g,M),e(M,Ot),f(o,Uo,p),f(o,j,p),e(j,W),e(W,uo),w(fe,uo,null),e(j,yt),e(j,go),e(go,$t),f(o,Jo,p),f(o,Q,p),e(Q,zt),e(Q,me),e(me,xt),e(Q,Ct),f(o,Zo,p),f(o,He,p),e(He,qt),f(o,Wo,p),f(o,Ve,p),e(Ve,Ue),e(Ue,Et),e(Ue,ue),e(ue,Mt),f(o,Qo,p),f(o,Je,p),e(Je,Pt),f(o,Yo,p),f(o,C,p),e(C,_o),e(_o,vo),e(vo,St),e(C,Lt),e(C,bo),e(bo,Fo),e(Fo,Nt),e(C,It),e(C,ko),e(ko,wo),e(wo,jt),e(C,Gt),e(C,Ao),e(Ao,To),e(To,Dt),f(o,Ko,p),f(o,ge,p),e(ge,Bt),e(ge,_e),e(_e,Rt),f(o,Xo,p),f(o,L,p),e(L,Ht),e(L,Ze),e(Ze,Vt),e(L,Ut),e(L,ve),e(ve,Jt),e(L,Zt),f(o,et,p),f(o,G,p),e(G,Y),e(Y,Oo),w(be,Oo,null),e(G,Wt),e(G,yo),e(yo,Qt),f(o,ot,p),f(o,P,p),w(Fe,P,null),e(P,Yt),e(P,D),e(D,Kt),e(D,We),e(We,Xt),e(D,en),e(D,ke),e(ke,on),e(D,tn),e(P,nn),e(P,B),e(B,an),e(B,Qe),e(Qe,rn),e(B,sn),e(B,Ye),e(Ye,dn),e(B,cn),f(o,tt,p),f(o,R,p),e(R,K),e(K,$o),w(we,$o,null),e(R,ln),e(R,zo),e(zo,hn),f(o,nt,p),f(o,z,p),w(Ae,z,null),e(z,pn),e(z,xo),e(xo,fn),e(z,mn),e(z,X),e(X,Ke),e(Ke,un),e(X,gn),e(X,Xe),e(Xe,_n),e(X,vn),e(z,bn),e(z,Te),e(Te,Fn),e(Te,eo),e(eo,kn),e(Te,wn),f(o,at,p),f(o,H,p),e(H,ee),e(ee,Co),w(Oe,Co,null),e(H,An),e(H,qo),e(qo,Tn),f(o,rt,p),f(o,x,p),w(ye,x,null),e(x,On),e(x,$e),e($e,yn),e($e,Eo),e(Eo,$n),e($e,zn),e(x,xn),e(x,oe),e(oe,oo),e(oo,Cn),e(oe,qn),e(oe,to),e(to,En),e(oe,Mn),e(x,Pn),e(x,ze),e(ze,Sn),e(ze,no),e(no,Ln),e(ze,Nn),f(o,st,p),f(o,V,p),e(V,te),e(te,Mo),w(xe,Mo,null),e(V,In),e(V,Po),e(Po,jn),f(o,it,p),f(o,y,p),w(Ce,y,null),e(y,Gn),e(y,qe),e(qe,Dn),e(qe,ao),e(ao,Bn),e(qe,Rn),e(y,Hn),e(y,Ee),e(Ee,Vn),e(Ee,Me),e(Me,Un),e(Ee,Jn),e(y,Zn),e(y,So),e(So,Wn),e(y,Qn),e(y,q),w(Pe,q,null),e(q,Yn),e(q,U),e(U,Kn),e(U,ro),e(ro,Xn),e(U,ea),e(U,Lo),e(Lo,oa),e(U,ta),e(q,na),w(ne,q,null),e(q,aa),w(ae,q,null),f(o,dt,p),f(o,J,p),e(J,re),e(re,No),w(Se,No,null),e(J,ra),e(J,Io),e(Io,sa),f(o,ct,p),f(o,$,p),w(Le,$,null),e($,ia),e($,Ne),e(Ne,da),e(Ne,so),e(so,ca),e(Ne,la),e($,ha),e($,Ie),e(Ie,pa),e(Ie,je),e(je,fa),e(Ie,ma),e($,ua),e($,jo),e(jo,ga),e($,_a),e($,E),w(Ge,E,null),e(E,va),e(E,Z),e(Z,ba),e(Z,io),e(io,Fa),e(Z,ka),e(Z,Go),e(Go,wa),e(Z,Aa),e(E,Ta),w(se,E,null),e(E,Oa),w(ie,E,null),lt=!0},p(o,[p]){const De={};p&2&&(De.$$scope={dirty:p,ctx:o}),ne.$set(De);const Do={};p&2&&(Do.$$scope={dirty:p,ctx:o}),ae.$set(Do);const Bo={};p&2&&(Bo.$$scope={dirty:p,ctx:o}),se.$set(Bo);const Ro={};p&2&&(Ro.$$scope={dirty:p,ctx:o}),ie.$set(Ro)},i(o){lt||(A(h.$$.fragment,o),A(fe.$$.fragment,o),A(be.$$.fragment,o),A(Fe.$$.fragment,o),A(we.$$.fragment,o),A(Ae.$$.fragment,o),A(Oe.$$.fragment,o),A(ye.$$.fragment,o),A(xe.$$.fragment,o),A(Ce.$$.fragment,o),A(Pe.$$.fragment,o),A(ne.$$.fragment,o),A(ae.$$.fragment,o),A(Se.$$.fragment,o),A(Le.$$.fragment,o),A(Ge.$$.fragment,o),A(se.$$.fragment,o),A(ie.$$.fragment,o),lt=!0)},o(o){T(h.$$.fragment,o),T(fe.$$.fragment,o),T(be.$$.fragment,o),T(Fe.$$.fragment,o),T(we.$$.fragment,o),T(Ae.$$.fragment,o),T(Oe.$$.fragment,o),T(ye.$$.fragment,o),T(xe.$$.fragment,o),T(Ce.$$.fragment,o),T(Pe.$$.fragment,o),T(ne.$$.fragment,o),T(ae.$$.fragment,o),T(Se.$$.fragment,o),T(Le.$$.fragment,o),T(Ge.$$.fragment,o),T(se.$$.fragment,o),T(ie.$$.fragment,o),lt=!1},d(o){t(m),o&&t(b),o&&t(g),O(h),o&&t(Uo),o&&t(j),O(fe),o&&t(Jo),o&&t(Q),o&&t(Zo),o&&t(He),o&&t(Wo),o&&t(Ve),o&&t(Qo),o&&t(Je),o&&t(Yo),o&&t(C),o&&t(Ko),o&&t(ge),o&&t(Xo),o&&t(L),o&&t(et),o&&t(G),O(be),o&&t(ot),o&&t(P),O(Fe),o&&t(tt),o&&t(R),O(we),o&&t(nt),o&&t(z),O(Ae),o&&t(at),o&&t(H),O(Oe),o&&t(rt),o&&t(x),O(ye),o&&t(st),o&&t(V),O(xe),o&&t(it),o&&t(y),O(Ce),O(Pe),O(ne),O(ae),o&&t(dt),o&&t(J),O(Se),o&&t(ct),o&&t($),O(Le),O(Ge),O(se),O(ie)}}}const Hr={local:"ofa",sections:[{local:"overview",title:"Overview"},{local:"transformers.OFAConfig",title:"OFAConfig"},{local:"transformers.OFATokenizer",title:"OFATokenizer"},{local:"transformers.OFATokenizerFast",title:"OFATokenizerFast"},{local:"transformers.OFAModel",title:"OFAModel"},{local:"transformers.OFAForConditionalGeneration",title:"OFAForConditionalGeneration"}],title:"OFA"};function Vr(S){return Ir(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kr extends Pr{constructor(m){super();Sr(this,m,Vr,Rr,Lr,{})}}export{Kr as default,Hr as metadata};
