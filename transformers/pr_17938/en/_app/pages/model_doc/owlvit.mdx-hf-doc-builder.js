import{S as Vp,i as xp,s as yp,e as a,k as h,w as T,t as s,M as kp,c as n,d as o,m as f,a as i,x as b,h as r,b as c,G as e,g as _,y as $,q as O,o as V,B as x,v as jp,L as ft}from"../../chunks/vendor-hf-doc-builder.js";import{T as ao}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ve}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Z}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as ht}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ep(j){let d,v,u,p,w;return p=new Ve({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with adirik/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the adirik/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=a("p"),v=s("Example:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Example:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function zp(j){let d,v,u,p,w;return p=new Ve({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with adirik/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the adirik/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=a("p"),v=s("Example:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Example:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Pp(j){let d,v;return{c(){d=a("p"),v=s(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(u){d=n(u,"P",{});var p=i(d);v=r(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(u,p){_(u,d,p),e(d,v)},d(u){u&&o(d)}}}function Mp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function qp(j){let d,v,u,p,w;return p=new Ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Cp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function Ip(j){let d,v,u,p,w;return p=new Ve({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Fp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function Lp(j){let d,v,u,p,w;return p=new Ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Dp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function Ap(j){let d,v,u,p,w;return p=new Ve({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooled_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooled_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Wp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function Np(j){let d,v,u,p,w;return p=new Ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooled_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooled_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Sp(j){let d,v,u,p,w;return{c(){d=a("p"),v=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=r(E,"Module"),E.forEach(o),w=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,d,m),e(d,v),e(d,u),e(u,p),e(d,w)},d(l){l&&o(d)}}}function Bp(j){let d,v,u,p,w;return p=new Ve({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape # [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][0], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape # [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][<span class="hljs-number">0</span>], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),{c(){d=a("p"),v=s("Examples:"),u=h(),T(p.$$.fragment)},l(l){d=n(l,"P",{});var m=i(d);v=r(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,d,m),e(d,v),_(l,u,m),$(p,l,m),w=!0},p:ft,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(d),l&&o(u),x(p,l)}}}function Rp(j){let d,v,u,p,w,l,m,E,pa,wr,Q,xe,as,ut,ma,ns,ha,vr,ye,fa,gt,ua,ga,Tr,no,_a,br,io,is,wa,$r,ee,ke,ls,_t,va,ds,Ta,Or,lo,ba,Vr,z,$a,co,Oa,Va,po,xa,ya,mo,ka,ja,ho,Ea,za,fo,Pa,Ma,uo,qa,Ca,go,Ia,Fa,xr,wt,yr,J,La,vt,Da,Aa,Tt,Wa,Na,kr,te,je,cs,bt,Sa,ps,Ba,jr,L,$t,Ra,Ee,_o,Ha,Ua,wo,Ga,Xa,Za,oe,Ja,vo,Ka,Ya,To,Qa,en,tn,ze,Ot,on,Vt,sn,bo,rn,an,Er,se,Pe,ms,xt,nn,hs,ln,zr,D,yt,dn,re,cn,$o,pn,mn,kt,hn,fn,un,ae,gn,Oo,_n,wn,Vo,vn,Tn,bn,Me,Pr,ne,qe,fs,jt,$n,us,On,Mr,A,Et,Vn,ie,xn,xo,yn,kn,zt,jn,En,zn,le,Pn,yo,Mn,qn,ko,Cn,In,Fn,Ce,qr,de,Ie,gs,Pt,Ln,_s,Dn,Cr,W,Mt,An,ws,Wn,Nn,qt,Sn,jo,Bn,Rn,Hn,K,Ct,Un,vs,Gn,Xn,Fe,Ir,ce,Le,Ts,It,Zn,bs,Jn,Fr,N,Ft,Kn,C,Yn,Eo,Qn,ei,zo,ti,oi,Po,si,ri,$s,ai,ni,Mo,ii,li,di,De,Lt,ci,Dt,pi,qo,mi,hi,fi,Ae,At,ui,Wt,gi,Co,_i,wi,Lr,pe,We,Os,Nt,vi,Vs,Ti,Dr,S,St,bi,B,Bt,$i,me,Oi,Io,Vi,xi,xs,yi,ki,ji,Ne,Ei,Se,zi,R,Rt,Pi,he,Mi,Fo,qi,Ci,ys,Ii,Fi,Li,Be,Di,Re,Ai,H,Ht,Wi,fe,Ni,Lo,Si,Bi,ks,Ri,Hi,Ui,He,Gi,Ue,Ar,ue,Ge,js,Ut,Xi,Es,Zi,Wr,ge,Gt,Ji,U,Xt,Ki,_e,Yi,Do,Qi,el,zs,tl,ol,sl,Xe,rl,Ze,Nr,we,Je,Ps,Zt,al,Ms,nl,Sr,ve,Jt,il,G,Kt,ll,Te,dl,Ao,cl,pl,qs,ml,hl,fl,Ke,ul,Ye,Br,be,Qe,Cs,Yt,gl,Is,_l,Rr,$e,Qt,wl,I,eo,vl,Oe,Tl,Wo,bl,$l,Fs,Ol,Vl,xl,et,yl,Ls,y,Ds,kl,jl,No,El,zl,As,Pl,Ml,Ws,ql,Cl,Ns,Il,Fl,Ss,Ll,Dl,Bs,Al,Wl,Rs,Nl,Sl,Hs,Bl,Rl,Us,Hl,Ul,Gs,Gl,Xl,Xs,Zl,Jl,Zs,Kl,Yl,Js,Ql,ed,Ks,td,od,So,sd,rd,Ys,ad,nd,Qs,id,ld,Bo,dd,cd,er,pd,md,tr,hd,fd,M,ud,or,gd,_d,sr,wd,vd,Ro,Td,bd,rr,$d,Od,ar,Vd,xd,Ho,yd,kd,jd,Ed,tt,Hr;return l=new Z({}),ut=new Z({}),_t=new Z({}),wt=new Ve({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("adirik/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt", padding=True
)

outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][0], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][<span class="hljs-number">0</span>], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),bt=new Z({}),$t=new P({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.OwlViTConfig.vision_config_dict",description:`<strong>vision_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config_dict"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L246"}}),Ot=new P({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L311",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),xt=new Z({}),yt=new P({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L34"}}),Me=new ht({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Ep]},$$scope:{ctx:j}}}),jt=new Z({}),Et=new P({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L142"}}),Ce=new ht({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[zp]},$$scope:{ctx:j}}}),Pt=new Z({}),Mt=new P({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 768"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 768"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"rescale",val:" = True"},{name:"do_convert_rgb",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the shorter edge of the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"},{anchor:"transformers.OwlViTFeatureExtractor.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to rescale input images to between 0-1 range. <code>PIL.Image.Image</code> inputs are automatically
scaled.`,name:"rescale"},{anchor:"transformers.OwlViTFeatureExtractor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to convert <code>PIL.Image.Image</code> into <code>RGB</code> format.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/feature_extraction_owlvit.py#L44"}}),Ct=new P({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17938/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/feature_extraction_owlvit.py#L143",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Fe=new ao({props:{warning:!0,$$slots:{default:[Pp]},$$scope:{ctx:j}}}),It=new Z({}),Ft=new P({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),Lt=new P({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L141"}}),At=new P({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L148"}}),Nt=new Z({}),St=new P({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17938/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L868"}}),Bt=new P({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L996",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output(Tuple<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new ao({props:{$$slots:{default:[Mp]},$$scope:{ctx:j}}}),Se=new ht({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[qp]},$$scope:{ctx:j}}}),Rt=new P({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L903",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new ao({props:{$$slots:{default:[Cp]},$$scope:{ctx:j}}}),Re=new ht({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[Ip]},$$scope:{ctx:j}}}),Ht=new P({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L947",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),He=new ao({props:{$$slots:{default:[Fp]},$$scope:{ctx:j}}}),Ue=new ht({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Lp]},$$scope:{ctx:j}}}),Ut=new Z({}),Gt=new P({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L710"}}),Xt=new P({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L725",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xe=new ao({props:{$$slots:{default:[Dp]},$$scope:{ctx:j}}}),Ze=new ht({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[Ap]},$$scope:{ctx:j}}}),Zt=new Z({}),Jt=new P({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L819"}}),Kt=new P({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L832",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ke=new ao({props:{$$slots:{default:[Wp]},$$scope:{ctx:j}}}),Ye=new ht({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Np]},$$scope:{ctx:j}}}),Yt=new Z({}),Qt=new P({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L1187"}}),eo=new P({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": Tensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L1310",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),et=new ao({props:{$$slots:{default:[Sp]},$$scope:{ctx:j}}}),tt=new ht({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Bp]},$$scope:{ctx:j}}}),{c(){d=a("meta"),v=h(),u=a("h1"),p=a("a"),w=a("span"),T(l.$$.fragment),m=h(),E=a("span"),pa=s("OWL-ViT"),wr=h(),Q=a("h2"),xe=a("a"),as=a("span"),T(ut.$$.fragment),ma=h(),ns=a("span"),ha=s("Overview"),vr=h(),ye=a("p"),fa=s("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),gt=a("a"),ua=s("Simple Open-Vocabulary Object Detection with Vision Transformers"),ga=s(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Tr=h(),no=a("p"),_a=s("The abstract from the paper is the following:"),br=h(),io=a("p"),is=a("em"),wa=s("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),$r=h(),ee=a("h2"),ke=a("a"),ls=a("span"),T(_t.$$.fragment),va=h(),ds=a("span"),Ta=s("Usage"),Or=h(),lo=a("p"),ba=s("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Vr=h(),z=a("p"),$a=s("The "),co=a("a"),Oa=s("OwlViTFeatureExtractor"),Va=s(" can be used to resize (or rescale) and normalize images for the model and the "),po=a("a"),xa=s("CLIPTokenizer"),ya=s(" is used to encode the text. The "),mo=a("a"),ka=s("OwlViTProcessor"),ja=s(" wraps "),ho=a("a"),Ea=s("OwlViTFeatureExtractor"),za=s(" and "),fo=a("a"),Pa=s("CLIPTokenizer"),Ma=s(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),uo=a("a"),qa=s("OwlViTProcessor"),Ca=s(" and "),go=a("a"),Ia=s("OwlViTForObjectDetection"),Fa=s("."),xr=h(),T(wt.$$.fragment),yr=h(),J=a("p"),La=s("This model was contributed by "),vt=a("a"),Da=s("adirik"),Aa=s(". The original code can be found "),Tt=a("a"),Wa=s("here"),Na=s("."),kr=h(),te=a("h2"),je=a("a"),cs=a("span"),T(bt.$$.fragment),Sa=h(),ps=a("span"),Ba=s("OwlViTConfig"),jr=h(),L=a("div"),T($t.$$.fragment),Ra=h(),Ee=a("p"),_o=a("a"),Ha=s("OwlViTConfig"),Ua=s(" is the configuration class to store the configuration of an "),wo=a("a"),Ga=s("OwlViTModel"),Xa=s(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),Za=h(),oe=a("p"),Ja=s("Configuration objects inherit from "),vo=a("a"),Ka=s("PretrainedConfig"),Ya=s(` and can be used to control the model outputs. Read the
documentation from `),To=a("a"),Qa=s("PretrainedConfig"),en=s(" for more information."),tn=h(),ze=a("div"),T(Ot.$$.fragment),on=h(),Vt=a("p"),sn=s("Instantiate a "),bo=a("a"),rn=s("OwlViTConfig"),an=s(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),Er=h(),se=a("h2"),Pe=a("a"),ms=a("span"),T(xt.$$.fragment),nn=h(),hs=a("span"),ln=s("OwlViTTextConfig"),zr=h(),D=a("div"),T(yt.$$.fragment),dn=h(),re=a("p"),cn=s("This is the configuration class to store the configuration of a "),$o=a("a"),pn=s("OwlViTModel"),mn=s(`. It is used to instantiate an
OwlViT model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the OwlViT
`),kt=a("a"),hn=s("adirik/owlvit-base-patch32"),fn=s(" architecture."),un=h(),ae=a("p"),gn=s("Configuration objects inherit from "),Oo=a("a"),_n=s("PretrainedConfig"),wn=s(` and can be used to control the model outputs. Read the
documentation from `),Vo=a("a"),vn=s("PretrainedConfig"),Tn=s(" for more information."),bn=h(),T(Me.$$.fragment),Pr=h(),ne=a("h2"),qe=a("a"),fs=a("span"),T(jt.$$.fragment),$n=h(),us=a("span"),On=s("OwlViTVisionConfig"),Mr=h(),A=a("div"),T(Et.$$.fragment),Vn=h(),ie=a("p"),xn=s("This is the configuration class to store the configuration of an "),xo=a("a"),yn=s("OwlViTVisionModel"),kn=s(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=a("a"),jn=s("google/owlvit-base-patch32"),En=s(" architecture."),zn=h(),le=a("p"),Pn=s("Configuration objects inherit from "),yo=a("a"),Mn=s("PretrainedConfig"),qn=s(` and can be used to control the model outputs. Read the
documentation from `),ko=a("a"),Cn=s("PretrainedConfig"),In=s(" for more information."),Fn=h(),T(Ce.$$.fragment),qr=h(),de=a("h2"),Ie=a("a"),gs=a("span"),T(Pt.$$.fragment),Ln=h(),_s=a("span"),Dn=s("OwlViTFeatureExtractor"),Cr=h(),W=a("div"),T(Mt.$$.fragment),An=h(),ws=a("p"),Wn=s("Constructs an OWL-ViT feature extractor."),Nn=h(),qt=a("p"),Sn=s("This feature extractor inherits from "),jo=a("a"),Bn=s("FeatureExtractionMixin"),Rn=s(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Hn=h(),K=a("div"),T(Ct.$$.fragment),Un=h(),vs=a("p"),Gn=s("Main method to prepare for the model one or several image(s)."),Xn=h(),T(Fe.$$.fragment),Ir=h(),ce=a("h2"),Le=a("a"),Ts=a("span"),T(It.$$.fragment),Zn=h(),bs=a("span"),Jn=s("OwlViTProcessor"),Fr=h(),N=a("div"),T(Ft.$$.fragment),Kn=h(),C=a("p"),Yn=s("Constructs an OWL-ViT processor which wraps "),Eo=a("a"),Qn=s("OwlViTFeatureExtractor"),ei=s(" and "),zo=a("a"),ti=s("CLIPTokenizer"),oi=s("/"),Po=a("a"),si=s("CLIPTokenizerFast"),ri=s(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),$s=a("code"),ai=s("__call__()"),ni=s(" and "),Mo=a("a"),ii=s("decode()"),li=s(" for more information."),di=h(),De=a("div"),T(Lt.$$.fragment),ci=h(),Dt=a("p"),pi=s("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=a("a"),mi=s("batch_decode()"),hi=s(`. Please
refer to the docstring of this method for more information.`),fi=h(),Ae=a("div"),T(At.$$.fragment),ui=h(),Wt=a("p"),gi=s("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=a("a"),_i=s("decode()"),wi=s(`. Please refer to
the docstring of this method for more information.`),Lr=h(),pe=a("h2"),We=a("a"),Os=a("span"),T(Nt.$$.fragment),vi=h(),Vs=a("span"),Ti=s("OwlViTModel"),Dr=h(),S=a("div"),T(St.$$.fragment),bi=h(),B=a("div"),T(Bt.$$.fragment),$i=h(),me=a("p"),Oi=s("The "),Io=a("a"),Vi=s("OwlViTModel"),xi=s(" forward method, overrides the "),xs=a("code"),yi=s("__call__"),ki=s(" special method."),ji=h(),T(Ne.$$.fragment),Ei=h(),T(Se.$$.fragment),zi=h(),R=a("div"),T(Rt.$$.fragment),Pi=h(),he=a("p"),Mi=s("The "),Fo=a("a"),qi=s("OwlViTModel"),Ci=s(" forward method, overrides the "),ys=a("code"),Ii=s("__call__"),Fi=s(" special method."),Li=h(),T(Be.$$.fragment),Di=h(),T(Re.$$.fragment),Ai=h(),H=a("div"),T(Ht.$$.fragment),Wi=h(),fe=a("p"),Ni=s("The "),Lo=a("a"),Si=s("OwlViTModel"),Bi=s(" forward method, overrides the "),ks=a("code"),Ri=s("__call__"),Hi=s(" special method."),Ui=h(),T(He.$$.fragment),Gi=h(),T(Ue.$$.fragment),Ar=h(),ue=a("h2"),Ge=a("a"),js=a("span"),T(Ut.$$.fragment),Xi=h(),Es=a("span"),Zi=s("OwlViTTextModel"),Wr=h(),ge=a("div"),T(Gt.$$.fragment),Ji=h(),U=a("div"),T(Xt.$$.fragment),Ki=h(),_e=a("p"),Yi=s("The "),Do=a("a"),Qi=s("OwlViTTextModel"),el=s(" forward method, overrides the "),zs=a("code"),tl=s("__call__"),ol=s(" special method."),sl=h(),T(Xe.$$.fragment),rl=h(),T(Ze.$$.fragment),Nr=h(),we=a("h2"),Je=a("a"),Ps=a("span"),T(Zt.$$.fragment),al=h(),Ms=a("span"),nl=s("OwlViTVisionModel"),Sr=h(),ve=a("div"),T(Jt.$$.fragment),il=h(),G=a("div"),T(Kt.$$.fragment),ll=h(),Te=a("p"),dl=s("The "),Ao=a("a"),cl=s("OwlViTVisionModel"),pl=s(" forward method, overrides the "),qs=a("code"),ml=s("__call__"),hl=s(" special method."),fl=h(),T(Ke.$$.fragment),ul=h(),T(Ye.$$.fragment),Br=h(),be=a("h2"),Qe=a("a"),Cs=a("span"),T(Yt.$$.fragment),gl=h(),Is=a("span"),_l=s("OwlViTForObjectDetection"),Rr=h(),$e=a("div"),T(Qt.$$.fragment),wl=h(),I=a("div"),T(eo.$$.fragment),vl=h(),Oe=a("p"),Tl=s("The "),Wo=a("a"),bl=s("OwlViTForObjectDetection"),$l=s(" forward method, overrides the "),Fs=a("code"),Ol=s("__call__"),Vl=s(" special method."),xl=h(),T(et.$$.fragment),yl=h(),Ls=a("ul"),y=a("li"),Ds=a("strong"),kl=s("Output"),jl=s(" type of "),No=a("a"),El=s("OwlViTForObjectDetection"),zl=s(`.
loss (`),As=a("code"),Pl=s("torch.FloatTensor"),Ml=s(" of shape "),Ws=a("code"),ql=s("(1,)"),Cl=s(", "),Ns=a("em"),Il=s("optional"),Fl=s(", returned when "),Ss=a("code"),Ll=s("labels"),Dl=s(` are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.
loss_dict (`),Bs=a("code"),Al=s("Dict"),Wl=s(", "),Rs=a("em"),Nl=s("optional"),Sl=s(`) \u2014 A dictionary containing the individual losses. Useful for logging.
logits (`),Hs=a("code"),Bl=s("torch.FloatTensor"),Rl=s(" of shape "),Us=a("code"),Hl=s("(batch_size, num_patches, num_queries)"),Ul=s(`) \u2014 Classification logits (including no-object) for all queries.
pred_boxes (`),Gs=a("code"),Gl=s("torch.FloatTensor"),Xl=s(" of shape "),Xs=a("code"),Zl=s("(batch_size, num_patches, 4)"),Jl=s(`) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use `),Zs=a("code"),Kl=s("post_process()"),Yl=s(` to retrieve the unnormalized
bounding boxes.
text_embeds(`),Js=a("code"),Ql=s("torch.FloatTensor"),ed=s(" of shape "),Ks=a("code"),td=s("(batch_size, num_max_text_queries, output_dim"),od=s(") \u2014 The text embeddings obtained by applying the projection layer to the pooled output of "),So=a("a"),sd=s("OwlViTTextModel"),rd=s(`.
image_embeds(`),Ys=a("code"),ad=s("torch.FloatTensor"),nd=s(" of shape "),Qs=a("code"),id=s("(batch_size, patch_size, patch_size, output_dim"),ld=s(") \u2014 Pooled output of "),Bo=a("a"),dd=s("OwlViTVisionModel"),cd=s(`. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.
class_embeds (`),er=a("code"),pd=s("torch.FloatTensor"),md=s(" of shape "),tr=a("code"),hd=s("(batch_size, num_patches, hidden_size)"),fd=s(`) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)`),M=a("strong"),ud=s(`2.
text_model_last_hidden_states (`),or=a("code"),gd=s("torch.FloatTensor"),_d=s(" of shape "),sr=a("code"),wd=s("(batch_size, sequence_length, hidden_size)"),vd=s(")) \u2014 Last hidden states extracted from the "),Ro=a("a"),Td=s("OwlViTTextModel"),bd=s(`.
vision_model_last_hidden_states (`),rr=a("code"),$d=s("torch.FloatTensor"),Od=s(" of shape "),ar=a("code"),Vd=s("(batch_size, num_patches + 1, hidden_size)"),xd=s(")) \u2014 Last hidden states extracted from the "),Ho=a("a"),yd=s("OwlViTVisionModel"),kd=s(`. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)`),jd=s("2."),Ed=h(),T(tt.$$.fragment),this.h()},l(t){const g=kp('[data-svelte="svelte-1phssyn"]',document.head);d=n(g,"META",{name:!0,content:!0}),g.forEach(o),v=f(t),u=n(t,"H1",{class:!0});var to=i(u);p=n(to,"A",{id:!0,class:!0,href:!0});var nr=i(p);w=n(nr,"SPAN",{});var ir=i(w);b(l.$$.fragment,ir),ir.forEach(o),nr.forEach(o),m=f(to),E=n(to,"SPAN",{});var lr=i(E);pa=r(lr,"OWL-ViT"),lr.forEach(o),to.forEach(o),wr=f(t),Q=n(t,"H2",{class:!0});var oo=i(Q);xe=n(oo,"A",{id:!0,class:!0,href:!0});var dr=i(xe);as=n(dr,"SPAN",{});var cr=i(as);b(ut.$$.fragment,cr),cr.forEach(o),dr.forEach(o),ma=f(oo),ns=n(oo,"SPAN",{});var pr=i(ns);ha=r(pr,"Overview"),pr.forEach(o),oo.forEach(o),vr=f(t),ye=n(t,"P",{});var so=i(ye);fa=r(so,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),gt=n(so,"A",{href:!0,rel:!0});var mr=i(gt);ua=r(mr,"Simple Open-Vocabulary Object Detection with Vision Transformers"),mr.forEach(o),ga=r(so," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),so.forEach(o),Tr=f(t),no=n(t,"P",{});var hr=i(no);_a=r(hr,"The abstract from the paper is the following:"),hr.forEach(o),br=f(t),io=n(t,"P",{});var fr=i(io);is=n(fr,"EM",{});var ur=i(is);wa=r(ur,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),ur.forEach(o),fr.forEach(o),$r=f(t),ee=n(t,"H2",{class:!0});var ro=i(ee);ke=n(ro,"A",{id:!0,class:!0,href:!0});var gr=i(ke);ls=n(gr,"SPAN",{});var zd=i(ls);b(_t.$$.fragment,zd),zd.forEach(o),gr.forEach(o),va=f(ro),ds=n(ro,"SPAN",{});var Pd=i(ds);Ta=r(Pd,"Usage"),Pd.forEach(o),ro.forEach(o),Or=f(t),lo=n(t,"P",{});var Md=i(lo);ba=r(Md,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Md.forEach(o),Vr=f(t),z=n(t,"P",{});var q=i(z);$a=r(q,"The "),co=n(q,"A",{href:!0});var qd=i(co);Oa=r(qd,"OwlViTFeatureExtractor"),qd.forEach(o),Va=r(q," can be used to resize (or rescale) and normalize images for the model and the "),po=n(q,"A",{href:!0});var Cd=i(po);xa=r(Cd,"CLIPTokenizer"),Cd.forEach(o),ya=r(q," is used to encode the text. The "),mo=n(q,"A",{href:!0});var Id=i(mo);ka=r(Id,"OwlViTProcessor"),Id.forEach(o),ja=r(q," wraps "),ho=n(q,"A",{href:!0});var Fd=i(ho);Ea=r(Fd,"OwlViTFeatureExtractor"),Fd.forEach(o),za=r(q," and "),fo=n(q,"A",{href:!0});var Ld=i(fo);Pa=r(Ld,"CLIPTokenizer"),Ld.forEach(o),Ma=r(q," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),uo=n(q,"A",{href:!0});var Dd=i(uo);qa=r(Dd,"OwlViTProcessor"),Dd.forEach(o),Ca=r(q," and "),go=n(q,"A",{href:!0});var Ad=i(go);Ia=r(Ad,"OwlViTForObjectDetection"),Ad.forEach(o),Fa=r(q,"."),q.forEach(o),xr=f(t),b(wt.$$.fragment,t),yr=f(t),J=n(t,"P",{});var Uo=i(J);La=r(Uo,"This model was contributed by "),vt=n(Uo,"A",{href:!0,rel:!0});var Wd=i(vt);Da=r(Wd,"adirik"),Wd.forEach(o),Aa=r(Uo,". The original code can be found "),Tt=n(Uo,"A",{href:!0,rel:!0});var Nd=i(Tt);Wa=r(Nd,"here"),Nd.forEach(o),Na=r(Uo,"."),Uo.forEach(o),kr=f(t),te=n(t,"H2",{class:!0});var Ur=i(te);je=n(Ur,"A",{id:!0,class:!0,href:!0});var Sd=i(je);cs=n(Sd,"SPAN",{});var Bd=i(cs);b(bt.$$.fragment,Bd),Bd.forEach(o),Sd.forEach(o),Sa=f(Ur),ps=n(Ur,"SPAN",{});var Rd=i(ps);Ba=r(Rd,"OwlViTConfig"),Rd.forEach(o),Ur.forEach(o),jr=f(t),L=n(t,"DIV",{class:!0});var ot=i(L);b($t.$$.fragment,ot),Ra=f(ot),Ee=n(ot,"P",{});var _r=i(Ee);_o=n(_r,"A",{href:!0});var Hd=i(_o);Ha=r(Hd,"OwlViTConfig"),Hd.forEach(o),Ua=r(_r," is the configuration class to store the configuration of an "),wo=n(_r,"A",{href:!0});var Ud=i(wo);Ga=r(Ud,"OwlViTModel"),Ud.forEach(o),Xa=r(_r,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),_r.forEach(o),Za=f(ot),oe=n(ot,"P",{});var Go=i(oe);Ja=r(Go,"Configuration objects inherit from "),vo=n(Go,"A",{href:!0});var Gd=i(vo);Ka=r(Gd,"PretrainedConfig"),Gd.forEach(o),Ya=r(Go,` and can be used to control the model outputs. Read the
documentation from `),To=n(Go,"A",{href:!0});var Xd=i(To);Qa=r(Xd,"PretrainedConfig"),Xd.forEach(o),en=r(Go," for more information."),Go.forEach(o),tn=f(ot),ze=n(ot,"DIV",{class:!0});var Gr=i(ze);b(Ot.$$.fragment,Gr),on=f(Gr),Vt=n(Gr,"P",{});var Xr=i(Vt);sn=r(Xr,"Instantiate a "),bo=n(Xr,"A",{href:!0});var Zd=i(bo);rn=r(Zd,"OwlViTConfig"),Zd.forEach(o),an=r(Xr,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),Xr.forEach(o),Gr.forEach(o),ot.forEach(o),Er=f(t),se=n(t,"H2",{class:!0});var Zr=i(se);Pe=n(Zr,"A",{id:!0,class:!0,href:!0});var Jd=i(Pe);ms=n(Jd,"SPAN",{});var Kd=i(ms);b(xt.$$.fragment,Kd),Kd.forEach(o),Jd.forEach(o),nn=f(Zr),hs=n(Zr,"SPAN",{});var Yd=i(hs);ln=r(Yd,"OwlViTTextConfig"),Yd.forEach(o),Zr.forEach(o),zr=f(t),D=n(t,"DIV",{class:!0});var st=i(D);b(yt.$$.fragment,st),dn=f(st),re=n(st,"P",{});var Xo=i(re);cn=r(Xo,"This is the configuration class to store the configuration of a "),$o=n(Xo,"A",{href:!0});var Qd=i($o);pn=r(Qd,"OwlViTModel"),Qd.forEach(o),mn=r(Xo,`. It is used to instantiate an
OwlViT model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the OwlViT
`),kt=n(Xo,"A",{href:!0,rel:!0});var ec=i(kt);hn=r(ec,"adirik/owlvit-base-patch32"),ec.forEach(o),fn=r(Xo," architecture."),Xo.forEach(o),un=f(st),ae=n(st,"P",{});var Zo=i(ae);gn=r(Zo,"Configuration objects inherit from "),Oo=n(Zo,"A",{href:!0});var tc=i(Oo);_n=r(tc,"PretrainedConfig"),tc.forEach(o),wn=r(Zo,` and can be used to control the model outputs. Read the
documentation from `),Vo=n(Zo,"A",{href:!0});var oc=i(Vo);vn=r(oc,"PretrainedConfig"),oc.forEach(o),Tn=r(Zo," for more information."),Zo.forEach(o),bn=f(st),b(Me.$$.fragment,st),st.forEach(o),Pr=f(t),ne=n(t,"H2",{class:!0});var Jr=i(ne);qe=n(Jr,"A",{id:!0,class:!0,href:!0});var sc=i(qe);fs=n(sc,"SPAN",{});var rc=i(fs);b(jt.$$.fragment,rc),rc.forEach(o),sc.forEach(o),$n=f(Jr),us=n(Jr,"SPAN",{});var ac=i(us);On=r(ac,"OwlViTVisionConfig"),ac.forEach(o),Jr.forEach(o),Mr=f(t),A=n(t,"DIV",{class:!0});var rt=i(A);b(Et.$$.fragment,rt),Vn=f(rt),ie=n(rt,"P",{});var Jo=i(ie);xn=r(Jo,"This is the configuration class to store the configuration of an "),xo=n(Jo,"A",{href:!0});var nc=i(xo);yn=r(nc,"OwlViTVisionModel"),nc.forEach(o),kn=r(Jo,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=n(Jo,"A",{href:!0,rel:!0});var ic=i(zt);jn=r(ic,"google/owlvit-base-patch32"),ic.forEach(o),En=r(Jo," architecture."),Jo.forEach(o),zn=f(rt),le=n(rt,"P",{});var Ko=i(le);Pn=r(Ko,"Configuration objects inherit from "),yo=n(Ko,"A",{href:!0});var lc=i(yo);Mn=r(lc,"PretrainedConfig"),lc.forEach(o),qn=r(Ko,` and can be used to control the model outputs. Read the
documentation from `),ko=n(Ko,"A",{href:!0});var dc=i(ko);Cn=r(dc,"PretrainedConfig"),dc.forEach(o),In=r(Ko," for more information."),Ko.forEach(o),Fn=f(rt),b(Ce.$$.fragment,rt),rt.forEach(o),qr=f(t),de=n(t,"H2",{class:!0});var Kr=i(de);Ie=n(Kr,"A",{id:!0,class:!0,href:!0});var cc=i(Ie);gs=n(cc,"SPAN",{});var pc=i(gs);b(Pt.$$.fragment,pc),pc.forEach(o),cc.forEach(o),Ln=f(Kr),_s=n(Kr,"SPAN",{});var mc=i(_s);Dn=r(mc,"OwlViTFeatureExtractor"),mc.forEach(o),Kr.forEach(o),Cr=f(t),W=n(t,"DIV",{class:!0});var at=i(W);b(Mt.$$.fragment,at),An=f(at),ws=n(at,"P",{});var hc=i(ws);Wn=r(hc,"Constructs an OWL-ViT feature extractor."),hc.forEach(o),Nn=f(at),qt=n(at,"P",{});var Yr=i(qt);Sn=r(Yr,"This feature extractor inherits from "),jo=n(Yr,"A",{href:!0});var fc=i(jo);Bn=r(fc,"FeatureExtractionMixin"),fc.forEach(o),Rn=r(Yr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Yr.forEach(o),Hn=f(at),K=n(at,"DIV",{class:!0});var Yo=i(K);b(Ct.$$.fragment,Yo),Un=f(Yo),vs=n(Yo,"P",{});var uc=i(vs);Gn=r(uc,"Main method to prepare for the model one or several image(s)."),uc.forEach(o),Xn=f(Yo),b(Fe.$$.fragment,Yo),Yo.forEach(o),at.forEach(o),Ir=f(t),ce=n(t,"H2",{class:!0});var Qr=i(ce);Le=n(Qr,"A",{id:!0,class:!0,href:!0});var gc=i(Le);Ts=n(gc,"SPAN",{});var _c=i(Ts);b(It.$$.fragment,_c),_c.forEach(o),gc.forEach(o),Zn=f(Qr),bs=n(Qr,"SPAN",{});var wc=i(bs);Jn=r(wc,"OwlViTProcessor"),wc.forEach(o),Qr.forEach(o),Fr=f(t),N=n(t,"DIV",{class:!0});var nt=i(N);b(Ft.$$.fragment,nt),Kn=f(nt),C=n(nt,"P",{});var X=i(C);Yn=r(X,"Constructs an OWL-ViT processor which wraps "),Eo=n(X,"A",{href:!0});var vc=i(Eo);Qn=r(vc,"OwlViTFeatureExtractor"),vc.forEach(o),ei=r(X," and "),zo=n(X,"A",{href:!0});var Tc=i(zo);ti=r(Tc,"CLIPTokenizer"),Tc.forEach(o),oi=r(X,"/"),Po=n(X,"A",{href:!0});var bc=i(Po);si=r(bc,"CLIPTokenizerFast"),bc.forEach(o),ri=r(X,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),$s=n(X,"CODE",{});var $c=i($s);ai=r($c,"__call__()"),$c.forEach(o),ni=r(X," and "),Mo=n(X,"A",{href:!0});var Oc=i(Mo);ii=r(Oc,"decode()"),Oc.forEach(o),li=r(X," for more information."),X.forEach(o),di=f(nt),De=n(nt,"DIV",{class:!0});var ea=i(De);b(Lt.$$.fragment,ea),ci=f(ea),Dt=n(ea,"P",{});var ta=i(Dt);pi=r(ta,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=n(ta,"A",{href:!0});var Vc=i(qo);mi=r(Vc,"batch_decode()"),Vc.forEach(o),hi=r(ta,`. Please
refer to the docstring of this method for more information.`),ta.forEach(o),ea.forEach(o),fi=f(nt),Ae=n(nt,"DIV",{class:!0});var oa=i(Ae);b(At.$$.fragment,oa),ui=f(oa),Wt=n(oa,"P",{});var sa=i(Wt);gi=r(sa,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=n(sa,"A",{href:!0});var xc=i(Co);_i=r(xc,"decode()"),xc.forEach(o),wi=r(sa,`. Please refer to
the docstring of this method for more information.`),sa.forEach(o),oa.forEach(o),nt.forEach(o),Lr=f(t),pe=n(t,"H2",{class:!0});var ra=i(pe);We=n(ra,"A",{id:!0,class:!0,href:!0});var yc=i(We);Os=n(yc,"SPAN",{});var kc=i(Os);b(Nt.$$.fragment,kc),kc.forEach(o),yc.forEach(o),vi=f(ra),Vs=n(ra,"SPAN",{});var jc=i(Vs);Ti=r(jc,"OwlViTModel"),jc.forEach(o),ra.forEach(o),Dr=f(t),S=n(t,"DIV",{class:!0});var it=i(S);b(St.$$.fragment,it),bi=f(it),B=n(it,"DIV",{class:!0});var lt=i(B);b(Bt.$$.fragment,lt),$i=f(lt),me=n(lt,"P",{});var Qo=i(me);Oi=r(Qo,"The "),Io=n(Qo,"A",{href:!0});var Ec=i(Io);Vi=r(Ec,"OwlViTModel"),Ec.forEach(o),xi=r(Qo," forward method, overrides the "),xs=n(Qo,"CODE",{});var zc=i(xs);yi=r(zc,"__call__"),zc.forEach(o),ki=r(Qo," special method."),Qo.forEach(o),ji=f(lt),b(Ne.$$.fragment,lt),Ei=f(lt),b(Se.$$.fragment,lt),lt.forEach(o),zi=f(it),R=n(it,"DIV",{class:!0});var dt=i(R);b(Rt.$$.fragment,dt),Pi=f(dt),he=n(dt,"P",{});var es=i(he);Mi=r(es,"The "),Fo=n(es,"A",{href:!0});var Pc=i(Fo);qi=r(Pc,"OwlViTModel"),Pc.forEach(o),Ci=r(es," forward method, overrides the "),ys=n(es,"CODE",{});var Mc=i(ys);Ii=r(Mc,"__call__"),Mc.forEach(o),Fi=r(es," special method."),es.forEach(o),Li=f(dt),b(Be.$$.fragment,dt),Di=f(dt),b(Re.$$.fragment,dt),dt.forEach(o),Ai=f(it),H=n(it,"DIV",{class:!0});var ct=i(H);b(Ht.$$.fragment,ct),Wi=f(ct),fe=n(ct,"P",{});var ts=i(fe);Ni=r(ts,"The "),Lo=n(ts,"A",{href:!0});var qc=i(Lo);Si=r(qc,"OwlViTModel"),qc.forEach(o),Bi=r(ts," forward method, overrides the "),ks=n(ts,"CODE",{});var Cc=i(ks);Ri=r(Cc,"__call__"),Cc.forEach(o),Hi=r(ts," special method."),ts.forEach(o),Ui=f(ct),b(He.$$.fragment,ct),Gi=f(ct),b(Ue.$$.fragment,ct),ct.forEach(o),it.forEach(o),Ar=f(t),ue=n(t,"H2",{class:!0});var aa=i(ue);Ge=n(aa,"A",{id:!0,class:!0,href:!0});var Ic=i(Ge);js=n(Ic,"SPAN",{});var Fc=i(js);b(Ut.$$.fragment,Fc),Fc.forEach(o),Ic.forEach(o),Xi=f(aa),Es=n(aa,"SPAN",{});var Lc=i(Es);Zi=r(Lc,"OwlViTTextModel"),Lc.forEach(o),aa.forEach(o),Wr=f(t),ge=n(t,"DIV",{class:!0});var na=i(ge);b(Gt.$$.fragment,na),Ji=f(na),U=n(na,"DIV",{class:!0});var pt=i(U);b(Xt.$$.fragment,pt),Ki=f(pt),_e=n(pt,"P",{});var os=i(_e);Yi=r(os,"The "),Do=n(os,"A",{href:!0});var Dc=i(Do);Qi=r(Dc,"OwlViTTextModel"),Dc.forEach(o),el=r(os," forward method, overrides the "),zs=n(os,"CODE",{});var Ac=i(zs);tl=r(Ac,"__call__"),Ac.forEach(o),ol=r(os," special method."),os.forEach(o),sl=f(pt),b(Xe.$$.fragment,pt),rl=f(pt),b(Ze.$$.fragment,pt),pt.forEach(o),na.forEach(o),Nr=f(t),we=n(t,"H2",{class:!0});var ia=i(we);Je=n(ia,"A",{id:!0,class:!0,href:!0});var Wc=i(Je);Ps=n(Wc,"SPAN",{});var Nc=i(Ps);b(Zt.$$.fragment,Nc),Nc.forEach(o),Wc.forEach(o),al=f(ia),Ms=n(ia,"SPAN",{});var Sc=i(Ms);nl=r(Sc,"OwlViTVisionModel"),Sc.forEach(o),ia.forEach(o),Sr=f(t),ve=n(t,"DIV",{class:!0});var la=i(ve);b(Jt.$$.fragment,la),il=f(la),G=n(la,"DIV",{class:!0});var mt=i(G);b(Kt.$$.fragment,mt),ll=f(mt),Te=n(mt,"P",{});var ss=i(Te);dl=r(ss,"The "),Ao=n(ss,"A",{href:!0});var Bc=i(Ao);cl=r(Bc,"OwlViTVisionModel"),Bc.forEach(o),pl=r(ss," forward method, overrides the "),qs=n(ss,"CODE",{});var Rc=i(qs);ml=r(Rc,"__call__"),Rc.forEach(o),hl=r(ss," special method."),ss.forEach(o),fl=f(mt),b(Ke.$$.fragment,mt),ul=f(mt),b(Ye.$$.fragment,mt),mt.forEach(o),la.forEach(o),Br=f(t),be=n(t,"H2",{class:!0});var da=i(be);Qe=n(da,"A",{id:!0,class:!0,href:!0});var Hc=i(Qe);Cs=n(Hc,"SPAN",{});var Uc=i(Cs);b(Yt.$$.fragment,Uc),Uc.forEach(o),Hc.forEach(o),gl=f(da),Is=n(da,"SPAN",{});var Gc=i(Is);_l=r(Gc,"OwlViTForObjectDetection"),Gc.forEach(o),da.forEach(o),Rr=f(t),$e=n(t,"DIV",{class:!0});var ca=i($e);b(Qt.$$.fragment,ca),wl=f(ca),I=n(ca,"DIV",{class:!0});var Y=i(I);b(eo.$$.fragment,Y),vl=f(Y),Oe=n(Y,"P",{});var rs=i(Oe);Tl=r(rs,"The "),Wo=n(rs,"A",{href:!0});var Xc=i(Wo);bl=r(Xc,"OwlViTForObjectDetection"),Xc.forEach(o),$l=r(rs," forward method, overrides the "),Fs=n(rs,"CODE",{});var Zc=i(Fs);Ol=r(Zc,"__call__"),Zc.forEach(o),Vl=r(rs," special method."),rs.forEach(o),xl=f(Y),b(et.$$.fragment,Y),yl=f(Y),Ls=n(Y,"UL",{});var Jc=i(Ls);y=n(Jc,"LI",{});var k=i(y);Ds=n(k,"STRONG",{});var Kc=i(Ds);kl=r(Kc,"Output"),Kc.forEach(o),jl=r(k," type of "),No=n(k,"A",{href:!0});var Yc=i(No);El=r(Yc,"OwlViTForObjectDetection"),Yc.forEach(o),zl=r(k,`.
loss (`),As=n(k,"CODE",{});var Qc=i(As);Pl=r(Qc,"torch.FloatTensor"),Qc.forEach(o),Ml=r(k," of shape "),Ws=n(k,"CODE",{});var ep=i(Ws);ql=r(ep,"(1,)"),ep.forEach(o),Cl=r(k,", "),Ns=n(k,"EM",{});var tp=i(Ns);Il=r(tp,"optional"),tp.forEach(o),Fl=r(k,", returned when "),Ss=n(k,"CODE",{});var op=i(Ss);Ll=r(op,"labels"),op.forEach(o),Dl=r(k,` are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.
loss_dict (`),Bs=n(k,"CODE",{});var sp=i(Bs);Al=r(sp,"Dict"),sp.forEach(o),Wl=r(k,", "),Rs=n(k,"EM",{});var rp=i(Rs);Nl=r(rp,"optional"),rp.forEach(o),Sl=r(k,`) \u2014 A dictionary containing the individual losses. Useful for logging.
logits (`),Hs=n(k,"CODE",{});var ap=i(Hs);Bl=r(ap,"torch.FloatTensor"),ap.forEach(o),Rl=r(k," of shape "),Us=n(k,"CODE",{});var np=i(Us);Hl=r(np,"(batch_size, num_patches, num_queries)"),np.forEach(o),Ul=r(k,`) \u2014 Classification logits (including no-object) for all queries.
pred_boxes (`),Gs=n(k,"CODE",{});var ip=i(Gs);Gl=r(ip,"torch.FloatTensor"),ip.forEach(o),Xl=r(k," of shape "),Xs=n(k,"CODE",{});var lp=i(Xs);Zl=r(lp,"(batch_size, num_patches, 4)"),lp.forEach(o),Jl=r(k,`) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use `),Zs=n(k,"CODE",{});var dp=i(Zs);Kl=r(dp,"post_process()"),dp.forEach(o),Yl=r(k,` to retrieve the unnormalized
bounding boxes.
text_embeds(`),Js=n(k,"CODE",{});var cp=i(Js);Ql=r(cp,"torch.FloatTensor"),cp.forEach(o),ed=r(k," of shape "),Ks=n(k,"CODE",{});var pp=i(Ks);td=r(pp,"(batch_size, num_max_text_queries, output_dim"),pp.forEach(o),od=r(k,") \u2014 The text embeddings obtained by applying the projection layer to the pooled output of "),So=n(k,"A",{href:!0});var mp=i(So);sd=r(mp,"OwlViTTextModel"),mp.forEach(o),rd=r(k,`.
image_embeds(`),Ys=n(k,"CODE",{});var hp=i(Ys);ad=r(hp,"torch.FloatTensor"),hp.forEach(o),nd=r(k," of shape "),Qs=n(k,"CODE",{});var fp=i(Qs);id=r(fp,"(batch_size, patch_size, patch_size, output_dim"),fp.forEach(o),ld=r(k,") \u2014 Pooled output of "),Bo=n(k,"A",{href:!0});var up=i(Bo);dd=r(up,"OwlViTVisionModel"),up.forEach(o),cd=r(k,`. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.
class_embeds (`),er=n(k,"CODE",{});var gp=i(er);pd=r(gp,"torch.FloatTensor"),gp.forEach(o),md=r(k," of shape "),tr=n(k,"CODE",{});var _p=i(tr);hd=r(_p,"(batch_size, num_patches, hidden_size)"),_p.forEach(o),fd=r(k,`) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)`),M=n(k,"STRONG",{});var F=i(M);ud=r(F,`2.
text_model_last_hidden_states (`),or=n(F,"CODE",{});var wp=i(or);gd=r(wp,"torch.FloatTensor"),wp.forEach(o),_d=r(F," of shape "),sr=n(F,"CODE",{});var vp=i(sr);wd=r(vp,"(batch_size, sequence_length, hidden_size)"),vp.forEach(o),vd=r(F,")) \u2014 Last hidden states extracted from the "),Ro=n(F,"A",{href:!0});var Tp=i(Ro);Td=r(Tp,"OwlViTTextModel"),Tp.forEach(o),bd=r(F,`.
vision_model_last_hidden_states (`),rr=n(F,"CODE",{});var bp=i(rr);$d=r(bp,"torch.FloatTensor"),bp.forEach(o),Od=r(F," of shape "),ar=n(F,"CODE",{});var $p=i(ar);Vd=r($p,"(batch_size, num_patches + 1, hidden_size)"),$p.forEach(o),xd=r(F,")) \u2014 Last hidden states extracted from the "),Ho=n(F,"A",{href:!0});var Op=i(Ho);yd=r(Op,"OwlViTVisionModel"),Op.forEach(o),kd=r(F,`. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)`),F.forEach(o),jd=r(k,"2."),k.forEach(o),Jc.forEach(o),Ed=f(Y),b(tt.$$.fragment,Y),Y.forEach(o),ca.forEach(o),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(Hp)),c(p,"id","owlvit"),c(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p,"href","#owlvit"),c(u,"class","relative group"),c(xe,"id","overview"),c(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xe,"href","#overview"),c(Q,"class","relative group"),c(gt,"href","https://arxiv.org/abs/2205.06230"),c(gt,"rel","nofollow"),c(ke,"id","usage"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#usage"),c(ee,"class","relative group"),c(co,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),c(po,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),c(mo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor"),c(ho,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),c(fo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),c(uo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor"),c(go,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),c(vt,"href","https://huggingface.co/adirik"),c(vt,"rel","nofollow"),c(Tt,"href","https://github.com/google-research/scenic/tree/a41d24676f64a2158bfcd7cb79b0a87673aa875b/scenic/projects/owl_vit"),c(Tt,"rel","nofollow"),c(je,"id","transformers.OwlViTConfig"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#transformers.OwlViTConfig"),c(te,"class","relative group"),c(_o,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"),c(wo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),c(vo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(To,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(bo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"),c(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Pe,"id","transformers.OwlViTTextConfig"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.OwlViTTextConfig"),c(se,"class","relative group"),c($o,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),c(kt,"href","https://huggingface.co/adirik/owlvit-base-patch32"),c(kt,"rel","nofollow"),c(Oo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(Vo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qe,"id","transformers.OwlViTVisionConfig"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.OwlViTVisionConfig"),c(ne,"class","relative group"),c(xo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),c(zt,"href","https://huggingface.co/google/owlvit-base-patch32"),c(zt,"rel","nofollow"),c(yo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(ko,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),c(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ie,"id","transformers.OwlViTFeatureExtractor"),c(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ie,"href","#transformers.OwlViTFeatureExtractor"),c(de,"class","relative group"),c(jo,"href","/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),c(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Le,"id","transformers.OwlViTProcessor"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.OwlViTProcessor"),c(ce,"class","relative group"),c(Eo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),c(zo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),c(Po,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(Mo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),c(qo,"href","/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode"),c(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Co,"href","/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode"),c(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(We,"id","transformers.OwlViTModel"),c(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(We,"href","#transformers.OwlViTModel"),c(pe,"class","relative group"),c(Io,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),c(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Fo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),c(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Lo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),c(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ge,"id","transformers.OwlViTTextModel"),c(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ge,"href","#transformers.OwlViTTextModel"),c(ue,"class","relative group"),c(Do,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"),c(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Je,"id","transformers.OwlViTVisionModel"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#transformers.OwlViTVisionModel"),c(we,"class","relative group"),c(Ao,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),c(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qe,"id","transformers.OwlViTForObjectDetection"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.OwlViTForObjectDetection"),c(be,"class","relative group"),c(Wo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),c(No,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),c(So,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"),c(Bo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),c(Ro,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"),c(Ho,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),c(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,d),_(t,v,g),_(t,u,g),e(u,p),e(p,w),$(l,w,null),e(u,m),e(u,E),e(E,pa),_(t,wr,g),_(t,Q,g),e(Q,xe),e(xe,as),$(ut,as,null),e(Q,ma),e(Q,ns),e(ns,ha),_(t,vr,g),_(t,ye,g),e(ye,fa),e(ye,gt),e(gt,ua),e(ye,ga),_(t,Tr,g),_(t,no,g),e(no,_a),_(t,br,g),_(t,io,g),e(io,is),e(is,wa),_(t,$r,g),_(t,ee,g),e(ee,ke),e(ke,ls),$(_t,ls,null),e(ee,va),e(ee,ds),e(ds,Ta),_(t,Or,g),_(t,lo,g),e(lo,ba),_(t,Vr,g),_(t,z,g),e(z,$a),e(z,co),e(co,Oa),e(z,Va),e(z,po),e(po,xa),e(z,ya),e(z,mo),e(mo,ka),e(z,ja),e(z,ho),e(ho,Ea),e(z,za),e(z,fo),e(fo,Pa),e(z,Ma),e(z,uo),e(uo,qa),e(z,Ca),e(z,go),e(go,Ia),e(z,Fa),_(t,xr,g),$(wt,t,g),_(t,yr,g),_(t,J,g),e(J,La),e(J,vt),e(vt,Da),e(J,Aa),e(J,Tt),e(Tt,Wa),e(J,Na),_(t,kr,g),_(t,te,g),e(te,je),e(je,cs),$(bt,cs,null),e(te,Sa),e(te,ps),e(ps,Ba),_(t,jr,g),_(t,L,g),$($t,L,null),e(L,Ra),e(L,Ee),e(Ee,_o),e(_o,Ha),e(Ee,Ua),e(Ee,wo),e(wo,Ga),e(Ee,Xa),e(L,Za),e(L,oe),e(oe,Ja),e(oe,vo),e(vo,Ka),e(oe,Ya),e(oe,To),e(To,Qa),e(oe,en),e(L,tn),e(L,ze),$(Ot,ze,null),e(ze,on),e(ze,Vt),e(Vt,sn),e(Vt,bo),e(bo,rn),e(Vt,an),_(t,Er,g),_(t,se,g),e(se,Pe),e(Pe,ms),$(xt,ms,null),e(se,nn),e(se,hs),e(hs,ln),_(t,zr,g),_(t,D,g),$(yt,D,null),e(D,dn),e(D,re),e(re,cn),e(re,$o),e($o,pn),e(re,mn),e(re,kt),e(kt,hn),e(re,fn),e(D,un),e(D,ae),e(ae,gn),e(ae,Oo),e(Oo,_n),e(ae,wn),e(ae,Vo),e(Vo,vn),e(ae,Tn),e(D,bn),$(Me,D,null),_(t,Pr,g),_(t,ne,g),e(ne,qe),e(qe,fs),$(jt,fs,null),e(ne,$n),e(ne,us),e(us,On),_(t,Mr,g),_(t,A,g),$(Et,A,null),e(A,Vn),e(A,ie),e(ie,xn),e(ie,xo),e(xo,yn),e(ie,kn),e(ie,zt),e(zt,jn),e(ie,En),e(A,zn),e(A,le),e(le,Pn),e(le,yo),e(yo,Mn),e(le,qn),e(le,ko),e(ko,Cn),e(le,In),e(A,Fn),$(Ce,A,null),_(t,qr,g),_(t,de,g),e(de,Ie),e(Ie,gs),$(Pt,gs,null),e(de,Ln),e(de,_s),e(_s,Dn),_(t,Cr,g),_(t,W,g),$(Mt,W,null),e(W,An),e(W,ws),e(ws,Wn),e(W,Nn),e(W,qt),e(qt,Sn),e(qt,jo),e(jo,Bn),e(qt,Rn),e(W,Hn),e(W,K),$(Ct,K,null),e(K,Un),e(K,vs),e(vs,Gn),e(K,Xn),$(Fe,K,null),_(t,Ir,g),_(t,ce,g),e(ce,Le),e(Le,Ts),$(It,Ts,null),e(ce,Zn),e(ce,bs),e(bs,Jn),_(t,Fr,g),_(t,N,g),$(Ft,N,null),e(N,Kn),e(N,C),e(C,Yn),e(C,Eo),e(Eo,Qn),e(C,ei),e(C,zo),e(zo,ti),e(C,oi),e(C,Po),e(Po,si),e(C,ri),e(C,$s),e($s,ai),e(C,ni),e(C,Mo),e(Mo,ii),e(C,li),e(N,di),e(N,De),$(Lt,De,null),e(De,ci),e(De,Dt),e(Dt,pi),e(Dt,qo),e(qo,mi),e(Dt,hi),e(N,fi),e(N,Ae),$(At,Ae,null),e(Ae,ui),e(Ae,Wt),e(Wt,gi),e(Wt,Co),e(Co,_i),e(Wt,wi),_(t,Lr,g),_(t,pe,g),e(pe,We),e(We,Os),$(Nt,Os,null),e(pe,vi),e(pe,Vs),e(Vs,Ti),_(t,Dr,g),_(t,S,g),$(St,S,null),e(S,bi),e(S,B),$(Bt,B,null),e(B,$i),e(B,me),e(me,Oi),e(me,Io),e(Io,Vi),e(me,xi),e(me,xs),e(xs,yi),e(me,ki),e(B,ji),$(Ne,B,null),e(B,Ei),$(Se,B,null),e(S,zi),e(S,R),$(Rt,R,null),e(R,Pi),e(R,he),e(he,Mi),e(he,Fo),e(Fo,qi),e(he,Ci),e(he,ys),e(ys,Ii),e(he,Fi),e(R,Li),$(Be,R,null),e(R,Di),$(Re,R,null),e(S,Ai),e(S,H),$(Ht,H,null),e(H,Wi),e(H,fe),e(fe,Ni),e(fe,Lo),e(Lo,Si),e(fe,Bi),e(fe,ks),e(ks,Ri),e(fe,Hi),e(H,Ui),$(He,H,null),e(H,Gi),$(Ue,H,null),_(t,Ar,g),_(t,ue,g),e(ue,Ge),e(Ge,js),$(Ut,js,null),e(ue,Xi),e(ue,Es),e(Es,Zi),_(t,Wr,g),_(t,ge,g),$(Gt,ge,null),e(ge,Ji),e(ge,U),$(Xt,U,null),e(U,Ki),e(U,_e),e(_e,Yi),e(_e,Do),e(Do,Qi),e(_e,el),e(_e,zs),e(zs,tl),e(_e,ol),e(U,sl),$(Xe,U,null),e(U,rl),$(Ze,U,null),_(t,Nr,g),_(t,we,g),e(we,Je),e(Je,Ps),$(Zt,Ps,null),e(we,al),e(we,Ms),e(Ms,nl),_(t,Sr,g),_(t,ve,g),$(Jt,ve,null),e(ve,il),e(ve,G),$(Kt,G,null),e(G,ll),e(G,Te),e(Te,dl),e(Te,Ao),e(Ao,cl),e(Te,pl),e(Te,qs),e(qs,ml),e(Te,hl),e(G,fl),$(Ke,G,null),e(G,ul),$(Ye,G,null),_(t,Br,g),_(t,be,g),e(be,Qe),e(Qe,Cs),$(Yt,Cs,null),e(be,gl),e(be,Is),e(Is,_l),_(t,Rr,g),_(t,$e,g),$(Qt,$e,null),e($e,wl),e($e,I),$(eo,I,null),e(I,vl),e(I,Oe),e(Oe,Tl),e(Oe,Wo),e(Wo,bl),e(Oe,$l),e(Oe,Fs),e(Fs,Ol),e(Oe,Vl),e(I,xl),$(et,I,null),e(I,yl),e(I,Ls),e(Ls,y),e(y,Ds),e(Ds,kl),e(y,jl),e(y,No),e(No,El),e(y,zl),e(y,As),e(As,Pl),e(y,Ml),e(y,Ws),e(Ws,ql),e(y,Cl),e(y,Ns),e(Ns,Il),e(y,Fl),e(y,Ss),e(Ss,Ll),e(y,Dl),e(y,Bs),e(Bs,Al),e(y,Wl),e(y,Rs),e(Rs,Nl),e(y,Sl),e(y,Hs),e(Hs,Bl),e(y,Rl),e(y,Us),e(Us,Hl),e(y,Ul),e(y,Gs),e(Gs,Gl),e(y,Xl),e(y,Xs),e(Xs,Zl),e(y,Jl),e(y,Zs),e(Zs,Kl),e(y,Yl),e(y,Js),e(Js,Ql),e(y,ed),e(y,Ks),e(Ks,td),e(y,od),e(y,So),e(So,sd),e(y,rd),e(y,Ys),e(Ys,ad),e(y,nd),e(y,Qs),e(Qs,id),e(y,ld),e(y,Bo),e(Bo,dd),e(y,cd),e(y,er),e(er,pd),e(y,md),e(y,tr),e(tr,hd),e(y,fd),e(y,M),e(M,ud),e(M,or),e(or,gd),e(M,_d),e(M,sr),e(sr,wd),e(M,vd),e(M,Ro),e(Ro,Td),e(M,bd),e(M,rr),e(rr,$d),e(M,Od),e(M,ar),e(ar,Vd),e(M,xd),e(M,Ho),e(Ho,yd),e(M,kd),e(y,jd),e(I,Ed),$(tt,I,null),Hr=!0},p(t,[g]){const to={};g&2&&(to.$$scope={dirty:g,ctx:t}),Me.$set(to);const nr={};g&2&&(nr.$$scope={dirty:g,ctx:t}),Ce.$set(nr);const ir={};g&2&&(ir.$$scope={dirty:g,ctx:t}),Fe.$set(ir);const lr={};g&2&&(lr.$$scope={dirty:g,ctx:t}),Ne.$set(lr);const oo={};g&2&&(oo.$$scope={dirty:g,ctx:t}),Se.$set(oo);const dr={};g&2&&(dr.$$scope={dirty:g,ctx:t}),Be.$set(dr);const cr={};g&2&&(cr.$$scope={dirty:g,ctx:t}),Re.$set(cr);const pr={};g&2&&(pr.$$scope={dirty:g,ctx:t}),He.$set(pr);const so={};g&2&&(so.$$scope={dirty:g,ctx:t}),Ue.$set(so);const mr={};g&2&&(mr.$$scope={dirty:g,ctx:t}),Xe.$set(mr);const hr={};g&2&&(hr.$$scope={dirty:g,ctx:t}),Ze.$set(hr);const fr={};g&2&&(fr.$$scope={dirty:g,ctx:t}),Ke.$set(fr);const ur={};g&2&&(ur.$$scope={dirty:g,ctx:t}),Ye.$set(ur);const ro={};g&2&&(ro.$$scope={dirty:g,ctx:t}),et.$set(ro);const gr={};g&2&&(gr.$$scope={dirty:g,ctx:t}),tt.$set(gr)},i(t){Hr||(O(l.$$.fragment,t),O(ut.$$.fragment,t),O(_t.$$.fragment,t),O(wt.$$.fragment,t),O(bt.$$.fragment,t),O($t.$$.fragment,t),O(Ot.$$.fragment,t),O(xt.$$.fragment,t),O(yt.$$.fragment,t),O(Me.$$.fragment,t),O(jt.$$.fragment,t),O(Et.$$.fragment,t),O(Ce.$$.fragment,t),O(Pt.$$.fragment,t),O(Mt.$$.fragment,t),O(Ct.$$.fragment,t),O(Fe.$$.fragment,t),O(It.$$.fragment,t),O(Ft.$$.fragment,t),O(Lt.$$.fragment,t),O(At.$$.fragment,t),O(Nt.$$.fragment,t),O(St.$$.fragment,t),O(Bt.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Rt.$$.fragment,t),O(Be.$$.fragment,t),O(Re.$$.fragment,t),O(Ht.$$.fragment,t),O(He.$$.fragment,t),O(Ue.$$.fragment,t),O(Ut.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Xe.$$.fragment,t),O(Ze.$$.fragment,t),O(Zt.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Ke.$$.fragment,t),O(Ye.$$.fragment,t),O(Yt.$$.fragment,t),O(Qt.$$.fragment,t),O(eo.$$.fragment,t),O(et.$$.fragment,t),O(tt.$$.fragment,t),Hr=!0)},o(t){V(l.$$.fragment,t),V(ut.$$.fragment,t),V(_t.$$.fragment,t),V(wt.$$.fragment,t),V(bt.$$.fragment,t),V($t.$$.fragment,t),V(Ot.$$.fragment,t),V(xt.$$.fragment,t),V(yt.$$.fragment,t),V(Me.$$.fragment,t),V(jt.$$.fragment,t),V(Et.$$.fragment,t),V(Ce.$$.fragment,t),V(Pt.$$.fragment,t),V(Mt.$$.fragment,t),V(Ct.$$.fragment,t),V(Fe.$$.fragment,t),V(It.$$.fragment,t),V(Ft.$$.fragment,t),V(Lt.$$.fragment,t),V(At.$$.fragment,t),V(Nt.$$.fragment,t),V(St.$$.fragment,t),V(Bt.$$.fragment,t),V(Ne.$$.fragment,t),V(Se.$$.fragment,t),V(Rt.$$.fragment,t),V(Be.$$.fragment,t),V(Re.$$.fragment,t),V(Ht.$$.fragment,t),V(He.$$.fragment,t),V(Ue.$$.fragment,t),V(Ut.$$.fragment,t),V(Gt.$$.fragment,t),V(Xt.$$.fragment,t),V(Xe.$$.fragment,t),V(Ze.$$.fragment,t),V(Zt.$$.fragment,t),V(Jt.$$.fragment,t),V(Kt.$$.fragment,t),V(Ke.$$.fragment,t),V(Ye.$$.fragment,t),V(Yt.$$.fragment,t),V(Qt.$$.fragment,t),V(eo.$$.fragment,t),V(et.$$.fragment,t),V(tt.$$.fragment,t),Hr=!1},d(t){o(d),t&&o(v),t&&o(u),x(l),t&&o(wr),t&&o(Q),x(ut),t&&o(vr),t&&o(ye),t&&o(Tr),t&&o(no),t&&o(br),t&&o(io),t&&o($r),t&&o(ee),x(_t),t&&o(Or),t&&o(lo),t&&o(Vr),t&&o(z),t&&o(xr),x(wt,t),t&&o(yr),t&&o(J),t&&o(kr),t&&o(te),x(bt),t&&o(jr),t&&o(L),x($t),x(Ot),t&&o(Er),t&&o(se),x(xt),t&&o(zr),t&&o(D),x(yt),x(Me),t&&o(Pr),t&&o(ne),x(jt),t&&o(Mr),t&&o(A),x(Et),x(Ce),t&&o(qr),t&&o(de),x(Pt),t&&o(Cr),t&&o(W),x(Mt),x(Ct),x(Fe),t&&o(Ir),t&&o(ce),x(It),t&&o(Fr),t&&o(N),x(Ft),x(Lt),x(At),t&&o(Lr),t&&o(pe),x(Nt),t&&o(Dr),t&&o(S),x(St),x(Bt),x(Ne),x(Se),x(Rt),x(Be),x(Re),x(Ht),x(He),x(Ue),t&&o(Ar),t&&o(ue),x(Ut),t&&o(Wr),t&&o(ge),x(Gt),x(Xt),x(Xe),x(Ze),t&&o(Nr),t&&o(we),x(Zt),t&&o(Sr),t&&o(ve),x(Jt),x(Kt),x(Ke),x(Ye),t&&o(Br),t&&o(be),x(Yt),t&&o(Rr),t&&o($e),x(Qt),x(eo),x(et),x(tt)}}}const Hp={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function Up(j){return jp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qp extends Vp{constructor(d){super();xp(this,d,Up,Rp,yp,{})}}export{Qp as default,Hp as metadata};
