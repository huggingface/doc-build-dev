import{S as ep,i as tp,s as op,e as a,k as h,w as T,t as r,M as rp,c as n,d as o,m as f,a as i,x as b,h as s,b as d,G as e,g as _,y as $,q as O,o as V,B as x,v as sp,L as mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as ro}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as $e}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as G}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function ap(j){let c,v,u,p,w;return p=new $e({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with adirik/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the adirik/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=a("p"),v=r("Example:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Example:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function np(j){let c,v,u,p,w;return p=new $e({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with adirik/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the adirik/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the adirik/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=a("p"),v=r("Example:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Example:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function ip(j){let c,v;return{c(){c=a("p"),v=r(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(u){c=n(u,"P",{});var p=i(c);v=s(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(u,p){_(u,c,p),e(c,v)},d(u){u&&o(c)}}}function lp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function cp(j){let c,v,u,p,w;return p=new $e({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function dp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function pp(j){let c,v,u,p,w;return p=new $e({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function mp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function hp(j){let c,v,u,p,w;return p=new $e({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function fp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function up(j){let c,v,u,p,w;return p=new $e({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooled_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooled_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function gp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function _p(j){let c,v,u,p,w;return p=new $e({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooled_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooled_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function wp(j){let c,v,u,p,w;return{c(){c=a("p"),v=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(m,"CODE",{});var E=i(u);p=s(E,"Module"),E.forEach(o),w=s(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(l,m){_(l,c,m),e(c,v),e(c,u),e(u,p),e(c,w)},d(l){l&&o(c)}}}function vp(j){let c,v,u,p,w;return p=new $e({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

model = OwlViTModel.from_pretrained("adirik/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape # [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][0], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape # [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][<span class="hljs-number">0</span>], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),{c(){c=a("p"),v=r("Examples:"),u=h(),T(p.$$.fragment)},l(l){c=n(l,"P",{});var m=i(c);v=s(m,"Examples:"),m.forEach(o),u=f(l),b(p.$$.fragment,l)},m(l,m){_(l,c,m),e(c,v),_(l,u,m),$(p,l,m),w=!0},p:mt,i(l){w||(O(p.$$.fragment,l),w=!0)},o(l){V(p.$$.fragment,l),w=!1},d(l){l&&o(c),l&&o(u),x(p,l)}}}function Tp(j){let c,v,u,p,w,l,m,E,sa,ps,K,Oe,tr,ht,aa,or,na,ms,Ve,ia,ft,la,ca,hs,so,da,fs,ao,rr,pa,us,Y,xe,sr,ut,ma,ar,ha,gs,no,fa,_s,z,ua,io,ga,_a,lo,wa,va,co,Ta,ba,po,$a,Oa,mo,Va,xa,ho,ya,ka,fo,ja,Ea,ws,gt,vs,X,za,_t,Pa,Ma,wt,qa,Ca,Ts,Q,ye,nr,vt,Ia,ir,Fa,bs,I,Tt,La,ke,uo,Da,Aa,go,Wa,Na,Sa,ee,Ba,_o,Ha,Ra,wo,Ua,Ga,Xa,je,bt,Za,$t,Ja,vo,Ka,Ya,$s,te,Ee,lr,Ot,Qa,cr,en,Os,F,Vt,tn,oe,on,To,rn,sn,xt,an,nn,ln,re,cn,bo,dn,pn,$o,mn,hn,fn,ze,Vs,se,Pe,dr,yt,un,pr,gn,xs,L,kt,_n,ae,wn,Oo,vn,Tn,jt,bn,$n,On,ne,Vn,Vo,xn,yn,xo,kn,jn,En,Me,ys,ie,qe,mr,Et,zn,hr,Pn,ks,D,zt,Mn,fr,qn,Cn,Pt,In,yo,Fn,Ln,Dn,Z,Mt,An,ur,Wn,Nn,Ce,js,le,Ie,gr,qt,Sn,_r,Bn,Es,A,Ct,Hn,q,Rn,ko,Un,Gn,jo,Xn,Zn,Eo,Jn,Kn,wr,Yn,Qn,zo,ei,ti,oi,Fe,It,ri,Ft,si,Po,ai,ni,ii,Le,Lt,li,Dt,ci,Mo,di,pi,zs,ce,De,vr,At,mi,Tr,hi,Ps,W,Wt,fi,N,Nt,ui,de,gi,qo,_i,wi,br,vi,Ti,bi,Ae,$i,We,Oi,S,St,Vi,pe,xi,Co,yi,ki,$r,ji,Ei,zi,Ne,Pi,Se,Mi,B,Bt,qi,me,Ci,Io,Ii,Fi,Or,Li,Di,Ai,Be,Wi,He,Ms,he,Re,Vr,Ht,Ni,xr,Si,qs,fe,Rt,Bi,H,Ut,Hi,ue,Ri,Fo,Ui,Gi,yr,Xi,Zi,Ji,Ue,Ki,Ge,Cs,ge,Xe,kr,Gt,Yi,jr,Qi,Is,_e,Xt,el,R,Zt,tl,we,ol,Lo,rl,sl,Er,al,nl,il,Ze,ll,Je,Fs,ve,Ke,zr,Jt,cl,Pr,dl,Ls,Te,Kt,pl,C,Yt,ml,be,hl,Do,fl,ul,Mr,gl,_l,wl,Ye,vl,qr,y,Cr,Tl,bl,Ao,$l,Ol,Ir,Vl,xl,Fr,yl,kl,Lr,jl,El,Dr,zl,Pl,Ar,Ml,ql,Wr,Cl,Il,Nr,Fl,Ll,Sr,Dl,Al,Br,Wl,Nl,Hr,Sl,Bl,Rr,Hl,Rl,Ur,Ul,Gl,Gr,Xl,Zl,Wo,Jl,Kl,Xr,Yl,Ql,Zr,ec,tc,No,oc,rc,Jr,sc,ac,Kr,nc,ic,Yr,lc,cc,dc,Qe,Ds;return l=new G({}),ht=new G({}),ut=new G({}),gt=new $e({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("adirik/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("adirik/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt", padding=True
)

outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][0], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;adirik/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][<span class="hljs-number">0</span>], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),vt=new G({}),Tt=new P({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.OwlViTConfig.vision_config_dict",description:`<strong>vision_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config_dict"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L246"}}),bt=new P({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L311",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),Ot=new G({}),Vt=new P({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L34"}}),ze=new pt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[ap]},$$scope:{ctx:j}}}),yt=new G({}),kt=new P({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/configuration_owlvit.py#L142"}}),Me=new pt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[np]},$$scope:{ctx:j}}}),Et=new G({}),zt=new P({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 768"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 768"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"rescale",val:" = True"},{name:"do_convert_rgb",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the shorter edge of the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"},{anchor:"transformers.OwlViTFeatureExtractor.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to rescale input images to between 0-1 range. <code>PIL.Image.Image</code> inputs are automatically
scaled.`,name:"rescale"},{anchor:"transformers.OwlViTFeatureExtractor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to convert <code>PIL.Image.Image</code> into <code>RGB</code> format.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/feature_extraction_owlvit.py#L44"}}),Mt=new P({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17938/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/feature_extraction_owlvit.py#L143",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ce=new ro({props:{warning:!0,$$slots:{default:[ip]},$$scope:{ctx:j}}}),qt=new G({}),Ct=new P({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),It=new P({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L141"}}),Lt=new P({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/processing_owlvit.py#L148"}}),At=new G({}),Wt=new P({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17938/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L861"}}),Nt=new P({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L989",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output(Tuple<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ae=new ro({props:{$$slots:{default:[lp]},$$scope:{ctx:j}}}),We=new pt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[cp]},$$scope:{ctx:j}}}),St=new P({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L896",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new ro({props:{$$slots:{default:[dp]},$$scope:{ctx:j}}}),Se=new pt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[pp]},$$scope:{ctx:j}}}),Bt=new P({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L940",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new ro({props:{$$slots:{default:[mp]},$$scope:{ctx:j}}}),He=new pt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[hp]},$$scope:{ctx:j}}}),Ht=new G({}),Rt=new P({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L703"}}),Ut=new P({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L718",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new ro({props:{$$slots:{default:[fp]},$$scope:{ctx:j}}}),Ge=new pt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[up]},$$scope:{ctx:j}}}),Gt=new G({}),Xt=new P({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L812"}}),Zt=new P({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17938/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L825",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17938/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new ro({props:{$$slots:{default:[gp]},$$scope:{ctx:j}}}),Je=new pt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[_p]},$$scope:{ctx:j}}}),Jt=new G({}),Kt=new P({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L1180"}}),Yt=new P({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": Tensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17938/src/transformers/models/owlvit/modeling_owlvit.py#L1303",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new ro({props:{$$slots:{default:[wp]},$$scope:{ctx:j}}}),Qe=new pt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[vp]},$$scope:{ctx:j}}}),{c(){c=a("meta"),v=h(),u=a("h1"),p=a("a"),w=a("span"),T(l.$$.fragment),m=h(),E=a("span"),sa=r("OWL-ViT"),ps=h(),K=a("h2"),Oe=a("a"),tr=a("span"),T(ht.$$.fragment),aa=h(),or=a("span"),na=r("Overview"),ms=h(),Ve=a("p"),ia=r("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=a("a"),la=r("Simple Open-Vocabulary Object Detection with Vision Transformers"),ca=r(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),hs=h(),so=a("p"),da=r("The abstract from the paper is the following:"),fs=h(),ao=a("p"),rr=a("em"),pa=r("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),us=h(),Y=a("h2"),xe=a("a"),sr=a("span"),T(ut.$$.fragment),ma=h(),ar=a("span"),ha=r("Usage"),gs=h(),no=a("p"),fa=r("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),_s=h(),z=a("p"),ua=r("The "),io=a("a"),ga=r("OwlViTFeatureExtractor"),_a=r(" can be used to resize (or rescale) and normalize images for the model and the "),lo=a("a"),wa=r("CLIPTokenizer"),va=r(" is used to encode the text. The "),co=a("a"),Ta=r("OwlViTProcessor"),ba=r(" wraps "),po=a("a"),$a=r("OwlViTFeatureExtractor"),Oa=r(" and "),mo=a("a"),Va=r("CLIPTokenizer"),xa=r(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),ho=a("a"),ya=r("OwlViTProcessor"),ka=r(" and "),fo=a("a"),ja=r("OwlViTForObjectDetection"),Ea=r("."),ws=h(),T(gt.$$.fragment),vs=h(),X=a("p"),za=r("This model was contributed by "),_t=a("a"),Pa=r("adirik"),Ma=r(". The original code can be found "),wt=a("a"),qa=r("here"),Ca=r("."),Ts=h(),Q=a("h2"),ye=a("a"),nr=a("span"),T(vt.$$.fragment),Ia=h(),ir=a("span"),Fa=r("OwlViTConfig"),bs=h(),I=a("div"),T(Tt.$$.fragment),La=h(),ke=a("p"),uo=a("a"),Da=r("OwlViTConfig"),Aa=r(" is the configuration class to store the configuration of an "),go=a("a"),Wa=r("OwlViTModel"),Na=r(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),Sa=h(),ee=a("p"),Ba=r("Configuration objects inherit from "),_o=a("a"),Ha=r("PretrainedConfig"),Ra=r(` and can be used to control the model outputs. Read the
documentation from `),wo=a("a"),Ua=r("PretrainedConfig"),Ga=r(" for more information."),Xa=h(),je=a("div"),T(bt.$$.fragment),Za=h(),$t=a("p"),Ja=r("Instantiate a "),vo=a("a"),Ka=r("OwlViTConfig"),Ya=r(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),$s=h(),te=a("h2"),Ee=a("a"),lr=a("span"),T(Ot.$$.fragment),Qa=h(),cr=a("span"),en=r("OwlViTTextConfig"),Os=h(),F=a("div"),T(Vt.$$.fragment),tn=h(),oe=a("p"),on=r("This is the configuration class to store the configuration of a "),To=a("a"),rn=r("OwlViTModel"),sn=r(`. It is used to instantiate an
OwlViT model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the OwlViT
`),xt=a("a"),an=r("adirik/owlvit-base-patch32"),nn=r(" architecture."),ln=h(),re=a("p"),cn=r("Configuration objects inherit from "),bo=a("a"),dn=r("PretrainedConfig"),pn=r(` and can be used to control the model outputs. Read the
documentation from `),$o=a("a"),mn=r("PretrainedConfig"),hn=r(" for more information."),fn=h(),T(ze.$$.fragment),Vs=h(),se=a("h2"),Pe=a("a"),dr=a("span"),T(yt.$$.fragment),un=h(),pr=a("span"),gn=r("OwlViTVisionConfig"),xs=h(),L=a("div"),T(kt.$$.fragment),_n=h(),ae=a("p"),wn=r("This is the configuration class to store the configuration of an "),Oo=a("a"),vn=r("OwlViTVisionModel"),Tn=r(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),jt=a("a"),bn=r("google/owlvit-base-patch32"),$n=r(" architecture."),On=h(),ne=a("p"),Vn=r("Configuration objects inherit from "),Vo=a("a"),xn=r("PretrainedConfig"),yn=r(` and can be used to control the model outputs. Read the
documentation from `),xo=a("a"),kn=r("PretrainedConfig"),jn=r(" for more information."),En=h(),T(Me.$$.fragment),ys=h(),ie=a("h2"),qe=a("a"),mr=a("span"),T(Et.$$.fragment),zn=h(),hr=a("span"),Pn=r("OwlViTFeatureExtractor"),ks=h(),D=a("div"),T(zt.$$.fragment),Mn=h(),fr=a("p"),qn=r("Constructs an OWL-ViT feature extractor."),Cn=h(),Pt=a("p"),In=r("This feature extractor inherits from "),yo=a("a"),Fn=r("FeatureExtractionMixin"),Ln=r(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Dn=h(),Z=a("div"),T(Mt.$$.fragment),An=h(),ur=a("p"),Wn=r("Main method to prepare for the model one or several image(s)."),Nn=h(),T(Ce.$$.fragment),js=h(),le=a("h2"),Ie=a("a"),gr=a("span"),T(qt.$$.fragment),Sn=h(),_r=a("span"),Bn=r("OwlViTProcessor"),Es=h(),A=a("div"),T(Ct.$$.fragment),Hn=h(),q=a("p"),Rn=r("Constructs an OWL-ViT processor which wraps "),ko=a("a"),Un=r("OwlViTFeatureExtractor"),Gn=r(" and "),jo=a("a"),Xn=r("CLIPTokenizer"),Zn=r("/"),Eo=a("a"),Jn=r("CLIPTokenizerFast"),Kn=r(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),wr=a("code"),Yn=r("__call__()"),Qn=r(" and "),zo=a("a"),ei=r("decode()"),ti=r(" for more information."),oi=h(),Fe=a("div"),T(It.$$.fragment),ri=h(),Ft=a("p"),si=r("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Po=a("a"),ai=r("batch_decode()"),ni=r(`. Please
refer to the docstring of this method for more information.`),ii=h(),Le=a("div"),T(Lt.$$.fragment),li=h(),Dt=a("p"),ci=r("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Mo=a("a"),di=r("decode()"),pi=r(`. Please refer to
the docstring of this method for more information.`),zs=h(),ce=a("h2"),De=a("a"),vr=a("span"),T(At.$$.fragment),mi=h(),Tr=a("span"),hi=r("OwlViTModel"),Ps=h(),W=a("div"),T(Wt.$$.fragment),fi=h(),N=a("div"),T(Nt.$$.fragment),ui=h(),de=a("p"),gi=r("The "),qo=a("a"),_i=r("OwlViTModel"),wi=r(" forward method, overrides the "),br=a("code"),vi=r("__call__"),Ti=r(" special method."),bi=h(),T(Ae.$$.fragment),$i=h(),T(We.$$.fragment),Oi=h(),S=a("div"),T(St.$$.fragment),Vi=h(),pe=a("p"),xi=r("The "),Co=a("a"),yi=r("OwlViTModel"),ki=r(" forward method, overrides the "),$r=a("code"),ji=r("__call__"),Ei=r(" special method."),zi=h(),T(Ne.$$.fragment),Pi=h(),T(Se.$$.fragment),Mi=h(),B=a("div"),T(Bt.$$.fragment),qi=h(),me=a("p"),Ci=r("The "),Io=a("a"),Ii=r("OwlViTModel"),Fi=r(" forward method, overrides the "),Or=a("code"),Li=r("__call__"),Di=r(" special method."),Ai=h(),T(Be.$$.fragment),Wi=h(),T(He.$$.fragment),Ms=h(),he=a("h2"),Re=a("a"),Vr=a("span"),T(Ht.$$.fragment),Ni=h(),xr=a("span"),Si=r("OwlViTTextModel"),qs=h(),fe=a("div"),T(Rt.$$.fragment),Bi=h(),H=a("div"),T(Ut.$$.fragment),Hi=h(),ue=a("p"),Ri=r("The "),Fo=a("a"),Ui=r("OwlViTTextModel"),Gi=r(" forward method, overrides the "),yr=a("code"),Xi=r("__call__"),Zi=r(" special method."),Ji=h(),T(Ue.$$.fragment),Ki=h(),T(Ge.$$.fragment),Cs=h(),ge=a("h2"),Xe=a("a"),kr=a("span"),T(Gt.$$.fragment),Yi=h(),jr=a("span"),Qi=r("OwlViTVisionModel"),Is=h(),_e=a("div"),T(Xt.$$.fragment),el=h(),R=a("div"),T(Zt.$$.fragment),tl=h(),we=a("p"),ol=r("The "),Lo=a("a"),rl=r("OwlViTVisionModel"),sl=r(" forward method, overrides the "),Er=a("code"),al=r("__call__"),nl=r(" special method."),il=h(),T(Ze.$$.fragment),ll=h(),T(Je.$$.fragment),Fs=h(),ve=a("h2"),Ke=a("a"),zr=a("span"),T(Jt.$$.fragment),cl=h(),Pr=a("span"),dl=r("OwlViTForObjectDetection"),Ls=h(),Te=a("div"),T(Kt.$$.fragment),pl=h(),C=a("div"),T(Yt.$$.fragment),ml=h(),be=a("p"),hl=r("The "),Do=a("a"),fl=r("OwlViTForObjectDetection"),ul=r(" forward method, overrides the "),Mr=a("code"),gl=r("__call__"),_l=r(" special method."),wl=h(),T(Ye.$$.fragment),vl=h(),qr=a("ul"),y=a("li"),Cr=a("strong"),Tl=r("Output"),bl=r(" type of "),Ao=a("a"),$l=r("OwlViTForObjectDetection"),Ol=r(`.
loss (`),Ir=a("code"),Vl=r("torch.FloatTensor"),xl=r(" of shape "),Fr=a("code"),yl=r("(1,)"),kl=r(", "),Lr=a("em"),jl=r("optional"),El=r(", returned when "),Dr=a("code"),zl=r("labels"),Pl=r(` are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.
loss_dict (`),Ar=a("code"),Ml=r("Dict"),ql=r(", "),Wr=a("em"),Cl=r("optional"),Il=r(`) \u2014 A dictionary containing the individual losses. Useful for logging.
logits (`),Nr=a("code"),Fl=r("torch.FloatTensor"),Ll=r(" of shape "),Sr=a("code"),Dl=r("(batch_size, num_patches, num_queries)"),Al=r(`) \u2014 Classification logits (including no-object) for all queries.
pred_boxes (`),Br=a("code"),Wl=r("torch.FloatTensor"),Nl=r(" of shape "),Hr=a("code"),Sl=r("(batch_size, num_patches, 4)"),Bl=r(`) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use `),Rr=a("code"),Hl=r("post_process()"),Rl=r(` to retrieve the unnormalized
bounding boxes.
text_embeds(`),Ur=a("code"),Ul=r("torch.FloatTensor"),Gl=r(" of shape "),Gr=a("code"),Xl=r("(batch_size, num_max_text_queries, output_dim"),Zl=r(") \u2014 The text embeddings obtained by applying the projection layer to the pooled output of "),Wo=a("a"),Jl=r("OwlViTTextModel"),Kl=r(`.
image_embeds(`),Xr=a("code"),Yl=r("torch.FloatTensor"),Ql=r(" of shape "),Zr=a("code"),ec=r("(batch_size, patch_size, patch_size, output_dim"),tc=r(") \u2014 Pooled output of "),No=a("a"),oc=r("OwlViTVisionModel"),rc=r(`. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.
class_embeds (`),Jr=a("code"),sc=r("torch.FloatTensor"),ac=r(" of shape "),Kr=a("code"),nc=r("(batch_size, num_patches, hidden_size)"),ic=r(", "),Yr=a("em"),lc=r("optional"),cc=r(`) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.`),dc=h(),T(Qe.$$.fragment),this.h()},l(t){const g=rp('[data-svelte="svelte-1phssyn"]',document.head);c=n(g,"META",{name:!0,content:!0}),g.forEach(o),v=f(t),u=n(t,"H1",{class:!0});var Qt=i(u);p=n(Qt,"A",{id:!0,class:!0,href:!0});var Qr=i(p);w=n(Qr,"SPAN",{});var es=i(w);b(l.$$.fragment,es),es.forEach(o),Qr.forEach(o),m=f(Qt),E=n(Qt,"SPAN",{});var ts=i(E);sa=s(ts,"OWL-ViT"),ts.forEach(o),Qt.forEach(o),ps=f(t),K=n(t,"H2",{class:!0});var eo=i(K);Oe=n(eo,"A",{id:!0,class:!0,href:!0});var os=i(Oe);tr=n(os,"SPAN",{});var rs=i(tr);b(ht.$$.fragment,rs),rs.forEach(o),os.forEach(o),aa=f(eo),or=n(eo,"SPAN",{});var ss=i(or);na=s(ss,"Overview"),ss.forEach(o),eo.forEach(o),ms=f(t),Ve=n(t,"P",{});var to=i(Ve);ia=s(to,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=n(to,"A",{href:!0,rel:!0});var as=i(ft);la=s(as,"Simple Open-Vocabulary Object Detection with Vision Transformers"),as.forEach(o),ca=s(to," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),to.forEach(o),hs=f(t),so=n(t,"P",{});var ns=i(so);da=s(ns,"The abstract from the paper is the following:"),ns.forEach(o),fs=f(t),ao=n(t,"P",{});var is=i(ao);rr=n(is,"EM",{});var ls=i(rr);pa=s(ls,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),ls.forEach(o),is.forEach(o),us=f(t),Y=n(t,"H2",{class:!0});var oo=i(Y);xe=n(oo,"A",{id:!0,class:!0,href:!0});var cs=i(xe);sr=n(cs,"SPAN",{});var pc=i(sr);b(ut.$$.fragment,pc),pc.forEach(o),cs.forEach(o),ma=f(oo),ar=n(oo,"SPAN",{});var mc=i(ar);ha=s(mc,"Usage"),mc.forEach(o),oo.forEach(o),gs=f(t),no=n(t,"P",{});var hc=i(no);fa=s(hc,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),hc.forEach(o),_s=f(t),z=n(t,"P",{});var M=i(z);ua=s(M,"The "),io=n(M,"A",{href:!0});var fc=i(io);ga=s(fc,"OwlViTFeatureExtractor"),fc.forEach(o),_a=s(M," can be used to resize (or rescale) and normalize images for the model and the "),lo=n(M,"A",{href:!0});var uc=i(lo);wa=s(uc,"CLIPTokenizer"),uc.forEach(o),va=s(M," is used to encode the text. The "),co=n(M,"A",{href:!0});var gc=i(co);Ta=s(gc,"OwlViTProcessor"),gc.forEach(o),ba=s(M," wraps "),po=n(M,"A",{href:!0});var _c=i(po);$a=s(_c,"OwlViTFeatureExtractor"),_c.forEach(o),Oa=s(M," and "),mo=n(M,"A",{href:!0});var wc=i(mo);Va=s(wc,"CLIPTokenizer"),wc.forEach(o),xa=s(M," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),ho=n(M,"A",{href:!0});var vc=i(ho);ya=s(vc,"OwlViTProcessor"),vc.forEach(o),ka=s(M," and "),fo=n(M,"A",{href:!0});var Tc=i(fo);ja=s(Tc,"OwlViTForObjectDetection"),Tc.forEach(o),Ea=s(M,"."),M.forEach(o),ws=f(t),b(gt.$$.fragment,t),vs=f(t),X=n(t,"P",{});var So=i(X);za=s(So,"This model was contributed by "),_t=n(So,"A",{href:!0,rel:!0});var bc=i(_t);Pa=s(bc,"adirik"),bc.forEach(o),Ma=s(So,". The original code can be found "),wt=n(So,"A",{href:!0,rel:!0});var $c=i(wt);qa=s($c,"here"),$c.forEach(o),Ca=s(So,"."),So.forEach(o),Ts=f(t),Q=n(t,"H2",{class:!0});var As=i(Q);ye=n(As,"A",{id:!0,class:!0,href:!0});var Oc=i(ye);nr=n(Oc,"SPAN",{});var Vc=i(nr);b(vt.$$.fragment,Vc),Vc.forEach(o),Oc.forEach(o),Ia=f(As),ir=n(As,"SPAN",{});var xc=i(ir);Fa=s(xc,"OwlViTConfig"),xc.forEach(o),As.forEach(o),bs=f(t),I=n(t,"DIV",{class:!0});var et=i(I);b(Tt.$$.fragment,et),La=f(et),ke=n(et,"P",{});var ds=i(ke);uo=n(ds,"A",{href:!0});var yc=i(uo);Da=s(yc,"OwlViTConfig"),yc.forEach(o),Aa=s(ds," is the configuration class to store the configuration of an "),go=n(ds,"A",{href:!0});var kc=i(go);Wa=s(kc,"OwlViTModel"),kc.forEach(o),Na=s(ds,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),ds.forEach(o),Sa=f(et),ee=n(et,"P",{});var Bo=i(ee);Ba=s(Bo,"Configuration objects inherit from "),_o=n(Bo,"A",{href:!0});var jc=i(_o);Ha=s(jc,"PretrainedConfig"),jc.forEach(o),Ra=s(Bo,` and can be used to control the model outputs. Read the
documentation from `),wo=n(Bo,"A",{href:!0});var Ec=i(wo);Ua=s(Ec,"PretrainedConfig"),Ec.forEach(o),Ga=s(Bo," for more information."),Bo.forEach(o),Xa=f(et),je=n(et,"DIV",{class:!0});var Ws=i(je);b(bt.$$.fragment,Ws),Za=f(Ws),$t=n(Ws,"P",{});var Ns=i($t);Ja=s(Ns,"Instantiate a "),vo=n(Ns,"A",{href:!0});var zc=i(vo);Ka=s(zc,"OwlViTConfig"),zc.forEach(o),Ya=s(Ns,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),Ns.forEach(o),Ws.forEach(o),et.forEach(o),$s=f(t),te=n(t,"H2",{class:!0});var Ss=i(te);Ee=n(Ss,"A",{id:!0,class:!0,href:!0});var Pc=i(Ee);lr=n(Pc,"SPAN",{});var Mc=i(lr);b(Ot.$$.fragment,Mc),Mc.forEach(o),Pc.forEach(o),Qa=f(Ss),cr=n(Ss,"SPAN",{});var qc=i(cr);en=s(qc,"OwlViTTextConfig"),qc.forEach(o),Ss.forEach(o),Os=f(t),F=n(t,"DIV",{class:!0});var tt=i(F);b(Vt.$$.fragment,tt),tn=f(tt),oe=n(tt,"P",{});var Ho=i(oe);on=s(Ho,"This is the configuration class to store the configuration of a "),To=n(Ho,"A",{href:!0});var Cc=i(To);rn=s(Cc,"OwlViTModel"),Cc.forEach(o),sn=s(Ho,`. It is used to instantiate an
OwlViT model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the OwlViT
`),xt=n(Ho,"A",{href:!0,rel:!0});var Ic=i(xt);an=s(Ic,"adirik/owlvit-base-patch32"),Ic.forEach(o),nn=s(Ho," architecture."),Ho.forEach(o),ln=f(tt),re=n(tt,"P",{});var Ro=i(re);cn=s(Ro,"Configuration objects inherit from "),bo=n(Ro,"A",{href:!0});var Fc=i(bo);dn=s(Fc,"PretrainedConfig"),Fc.forEach(o),pn=s(Ro,` and can be used to control the model outputs. Read the
documentation from `),$o=n(Ro,"A",{href:!0});var Lc=i($o);mn=s(Lc,"PretrainedConfig"),Lc.forEach(o),hn=s(Ro," for more information."),Ro.forEach(o),fn=f(tt),b(ze.$$.fragment,tt),tt.forEach(o),Vs=f(t),se=n(t,"H2",{class:!0});var Bs=i(se);Pe=n(Bs,"A",{id:!0,class:!0,href:!0});var Dc=i(Pe);dr=n(Dc,"SPAN",{});var Ac=i(dr);b(yt.$$.fragment,Ac),Ac.forEach(o),Dc.forEach(o),un=f(Bs),pr=n(Bs,"SPAN",{});var Wc=i(pr);gn=s(Wc,"OwlViTVisionConfig"),Wc.forEach(o),Bs.forEach(o),xs=f(t),L=n(t,"DIV",{class:!0});var ot=i(L);b(kt.$$.fragment,ot),_n=f(ot),ae=n(ot,"P",{});var Uo=i(ae);wn=s(Uo,"This is the configuration class to store the configuration of an "),Oo=n(Uo,"A",{href:!0});var Nc=i(Oo);vn=s(Nc,"OwlViTVisionModel"),Nc.forEach(o),Tn=s(Uo,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),jt=n(Uo,"A",{href:!0,rel:!0});var Sc=i(jt);bn=s(Sc,"google/owlvit-base-patch32"),Sc.forEach(o),$n=s(Uo," architecture."),Uo.forEach(o),On=f(ot),ne=n(ot,"P",{});var Go=i(ne);Vn=s(Go,"Configuration objects inherit from "),Vo=n(Go,"A",{href:!0});var Bc=i(Vo);xn=s(Bc,"PretrainedConfig"),Bc.forEach(o),yn=s(Go,` and can be used to control the model outputs. Read the
documentation from `),xo=n(Go,"A",{href:!0});var Hc=i(xo);kn=s(Hc,"PretrainedConfig"),Hc.forEach(o),jn=s(Go," for more information."),Go.forEach(o),En=f(ot),b(Me.$$.fragment,ot),ot.forEach(o),ys=f(t),ie=n(t,"H2",{class:!0});var Hs=i(ie);qe=n(Hs,"A",{id:!0,class:!0,href:!0});var Rc=i(qe);mr=n(Rc,"SPAN",{});var Uc=i(mr);b(Et.$$.fragment,Uc),Uc.forEach(o),Rc.forEach(o),zn=f(Hs),hr=n(Hs,"SPAN",{});var Gc=i(hr);Pn=s(Gc,"OwlViTFeatureExtractor"),Gc.forEach(o),Hs.forEach(o),ks=f(t),D=n(t,"DIV",{class:!0});var rt=i(D);b(zt.$$.fragment,rt),Mn=f(rt),fr=n(rt,"P",{});var Xc=i(fr);qn=s(Xc,"Constructs an OWL-ViT feature extractor."),Xc.forEach(o),Cn=f(rt),Pt=n(rt,"P",{});var Rs=i(Pt);In=s(Rs,"This feature extractor inherits from "),yo=n(Rs,"A",{href:!0});var Zc=i(yo);Fn=s(Zc,"FeatureExtractionMixin"),Zc.forEach(o),Ln=s(Rs,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Rs.forEach(o),Dn=f(rt),Z=n(rt,"DIV",{class:!0});var Xo=i(Z);b(Mt.$$.fragment,Xo),An=f(Xo),ur=n(Xo,"P",{});var Jc=i(ur);Wn=s(Jc,"Main method to prepare for the model one or several image(s)."),Jc.forEach(o),Nn=f(Xo),b(Ce.$$.fragment,Xo),Xo.forEach(o),rt.forEach(o),js=f(t),le=n(t,"H2",{class:!0});var Us=i(le);Ie=n(Us,"A",{id:!0,class:!0,href:!0});var Kc=i(Ie);gr=n(Kc,"SPAN",{});var Yc=i(gr);b(qt.$$.fragment,Yc),Yc.forEach(o),Kc.forEach(o),Sn=f(Us),_r=n(Us,"SPAN",{});var Qc=i(_r);Bn=s(Qc,"OwlViTProcessor"),Qc.forEach(o),Us.forEach(o),Es=f(t),A=n(t,"DIV",{class:!0});var st=i(A);b(Ct.$$.fragment,st),Hn=f(st),q=n(st,"P",{});var U=i(q);Rn=s(U,"Constructs an OWL-ViT processor which wraps "),ko=n(U,"A",{href:!0});var ed=i(ko);Un=s(ed,"OwlViTFeatureExtractor"),ed.forEach(o),Gn=s(U," and "),jo=n(U,"A",{href:!0});var td=i(jo);Xn=s(td,"CLIPTokenizer"),td.forEach(o),Zn=s(U,"/"),Eo=n(U,"A",{href:!0});var od=i(Eo);Jn=s(od,"CLIPTokenizerFast"),od.forEach(o),Kn=s(U,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),wr=n(U,"CODE",{});var rd=i(wr);Yn=s(rd,"__call__()"),rd.forEach(o),Qn=s(U," and "),zo=n(U,"A",{href:!0});var sd=i(zo);ei=s(sd,"decode()"),sd.forEach(o),ti=s(U," for more information."),U.forEach(o),oi=f(st),Fe=n(st,"DIV",{class:!0});var Gs=i(Fe);b(It.$$.fragment,Gs),ri=f(Gs),Ft=n(Gs,"P",{});var Xs=i(Ft);si=s(Xs,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Po=n(Xs,"A",{href:!0});var ad=i(Po);ai=s(ad,"batch_decode()"),ad.forEach(o),ni=s(Xs,`. Please
refer to the docstring of this method for more information.`),Xs.forEach(o),Gs.forEach(o),ii=f(st),Le=n(st,"DIV",{class:!0});var Zs=i(Le);b(Lt.$$.fragment,Zs),li=f(Zs),Dt=n(Zs,"P",{});var Js=i(Dt);ci=s(Js,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Mo=n(Js,"A",{href:!0});var nd=i(Mo);di=s(nd,"decode()"),nd.forEach(o),pi=s(Js,`. Please refer to
the docstring of this method for more information.`),Js.forEach(o),Zs.forEach(o),st.forEach(o),zs=f(t),ce=n(t,"H2",{class:!0});var Ks=i(ce);De=n(Ks,"A",{id:!0,class:!0,href:!0});var id=i(De);vr=n(id,"SPAN",{});var ld=i(vr);b(At.$$.fragment,ld),ld.forEach(o),id.forEach(o),mi=f(Ks),Tr=n(Ks,"SPAN",{});var cd=i(Tr);hi=s(cd,"OwlViTModel"),cd.forEach(o),Ks.forEach(o),Ps=f(t),W=n(t,"DIV",{class:!0});var at=i(W);b(Wt.$$.fragment,at),fi=f(at),N=n(at,"DIV",{class:!0});var nt=i(N);b(Nt.$$.fragment,nt),ui=f(nt),de=n(nt,"P",{});var Zo=i(de);gi=s(Zo,"The "),qo=n(Zo,"A",{href:!0});var dd=i(qo);_i=s(dd,"OwlViTModel"),dd.forEach(o),wi=s(Zo," forward method, overrides the "),br=n(Zo,"CODE",{});var pd=i(br);vi=s(pd,"__call__"),pd.forEach(o),Ti=s(Zo," special method."),Zo.forEach(o),bi=f(nt),b(Ae.$$.fragment,nt),$i=f(nt),b(We.$$.fragment,nt),nt.forEach(o),Oi=f(at),S=n(at,"DIV",{class:!0});var it=i(S);b(St.$$.fragment,it),Vi=f(it),pe=n(it,"P",{});var Jo=i(pe);xi=s(Jo,"The "),Co=n(Jo,"A",{href:!0});var md=i(Co);yi=s(md,"OwlViTModel"),md.forEach(o),ki=s(Jo," forward method, overrides the "),$r=n(Jo,"CODE",{});var hd=i($r);ji=s(hd,"__call__"),hd.forEach(o),Ei=s(Jo," special method."),Jo.forEach(o),zi=f(it),b(Ne.$$.fragment,it),Pi=f(it),b(Se.$$.fragment,it),it.forEach(o),Mi=f(at),B=n(at,"DIV",{class:!0});var lt=i(B);b(Bt.$$.fragment,lt),qi=f(lt),me=n(lt,"P",{});var Ko=i(me);Ci=s(Ko,"The "),Io=n(Ko,"A",{href:!0});var fd=i(Io);Ii=s(fd,"OwlViTModel"),fd.forEach(o),Fi=s(Ko," forward method, overrides the "),Or=n(Ko,"CODE",{});var ud=i(Or);Li=s(ud,"__call__"),ud.forEach(o),Di=s(Ko," special method."),Ko.forEach(o),Ai=f(lt),b(Be.$$.fragment,lt),Wi=f(lt),b(He.$$.fragment,lt),lt.forEach(o),at.forEach(o),Ms=f(t),he=n(t,"H2",{class:!0});var Ys=i(he);Re=n(Ys,"A",{id:!0,class:!0,href:!0});var gd=i(Re);Vr=n(gd,"SPAN",{});var _d=i(Vr);b(Ht.$$.fragment,_d),_d.forEach(o),gd.forEach(o),Ni=f(Ys),xr=n(Ys,"SPAN",{});var wd=i(xr);Si=s(wd,"OwlViTTextModel"),wd.forEach(o),Ys.forEach(o),qs=f(t),fe=n(t,"DIV",{class:!0});var Qs=i(fe);b(Rt.$$.fragment,Qs),Bi=f(Qs),H=n(Qs,"DIV",{class:!0});var ct=i(H);b(Ut.$$.fragment,ct),Hi=f(ct),ue=n(ct,"P",{});var Yo=i(ue);Ri=s(Yo,"The "),Fo=n(Yo,"A",{href:!0});var vd=i(Fo);Ui=s(vd,"OwlViTTextModel"),vd.forEach(o),Gi=s(Yo," forward method, overrides the "),yr=n(Yo,"CODE",{});var Td=i(yr);Xi=s(Td,"__call__"),Td.forEach(o),Zi=s(Yo," special method."),Yo.forEach(o),Ji=f(ct),b(Ue.$$.fragment,ct),Ki=f(ct),b(Ge.$$.fragment,ct),ct.forEach(o),Qs.forEach(o),Cs=f(t),ge=n(t,"H2",{class:!0});var ea=i(ge);Xe=n(ea,"A",{id:!0,class:!0,href:!0});var bd=i(Xe);kr=n(bd,"SPAN",{});var $d=i(kr);b(Gt.$$.fragment,$d),$d.forEach(o),bd.forEach(o),Yi=f(ea),jr=n(ea,"SPAN",{});var Od=i(jr);Qi=s(Od,"OwlViTVisionModel"),Od.forEach(o),ea.forEach(o),Is=f(t),_e=n(t,"DIV",{class:!0});var ta=i(_e);b(Xt.$$.fragment,ta),el=f(ta),R=n(ta,"DIV",{class:!0});var dt=i(R);b(Zt.$$.fragment,dt),tl=f(dt),we=n(dt,"P",{});var Qo=i(we);ol=s(Qo,"The "),Lo=n(Qo,"A",{href:!0});var Vd=i(Lo);rl=s(Vd,"OwlViTVisionModel"),Vd.forEach(o),sl=s(Qo," forward method, overrides the "),Er=n(Qo,"CODE",{});var xd=i(Er);al=s(xd,"__call__"),xd.forEach(o),nl=s(Qo," special method."),Qo.forEach(o),il=f(dt),b(Ze.$$.fragment,dt),ll=f(dt),b(Je.$$.fragment,dt),dt.forEach(o),ta.forEach(o),Fs=f(t),ve=n(t,"H2",{class:!0});var oa=i(ve);Ke=n(oa,"A",{id:!0,class:!0,href:!0});var yd=i(Ke);zr=n(yd,"SPAN",{});var kd=i(zr);b(Jt.$$.fragment,kd),kd.forEach(o),yd.forEach(o),cl=f(oa),Pr=n(oa,"SPAN",{});var jd=i(Pr);dl=s(jd,"OwlViTForObjectDetection"),jd.forEach(o),oa.forEach(o),Ls=f(t),Te=n(t,"DIV",{class:!0});var ra=i(Te);b(Kt.$$.fragment,ra),pl=f(ra),C=n(ra,"DIV",{class:!0});var J=i(C);b(Yt.$$.fragment,J),ml=f(J),be=n(J,"P",{});var er=i(be);hl=s(er,"The "),Do=n(er,"A",{href:!0});var Ed=i(Do);fl=s(Ed,"OwlViTForObjectDetection"),Ed.forEach(o),ul=s(er," forward method, overrides the "),Mr=n(er,"CODE",{});var zd=i(Mr);gl=s(zd,"__call__"),zd.forEach(o),_l=s(er," special method."),er.forEach(o),wl=f(J),b(Ye.$$.fragment,J),vl=f(J),qr=n(J,"UL",{});var Pd=i(qr);y=n(Pd,"LI",{});var k=i(y);Cr=n(k,"STRONG",{});var Md=i(Cr);Tl=s(Md,"Output"),Md.forEach(o),bl=s(k," type of "),Ao=n(k,"A",{href:!0});var qd=i(Ao);$l=s(qd,"OwlViTForObjectDetection"),qd.forEach(o),Ol=s(k,`.
loss (`),Ir=n(k,"CODE",{});var Cd=i(Ir);Vl=s(Cd,"torch.FloatTensor"),Cd.forEach(o),xl=s(k," of shape "),Fr=n(k,"CODE",{});var Id=i(Fr);yl=s(Id,"(1,)"),Id.forEach(o),kl=s(k,", "),Lr=n(k,"EM",{});var Fd=i(Lr);jl=s(Fd,"optional"),Fd.forEach(o),El=s(k,", returned when "),Dr=n(k,"CODE",{});var Ld=i(Dr);zl=s(Ld,"labels"),Ld.forEach(o),Pl=s(k,` are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.
loss_dict (`),Ar=n(k,"CODE",{});var Dd=i(Ar);Ml=s(Dd,"Dict"),Dd.forEach(o),ql=s(k,", "),Wr=n(k,"EM",{});var Ad=i(Wr);Cl=s(Ad,"optional"),Ad.forEach(o),Il=s(k,`) \u2014 A dictionary containing the individual losses. Useful for logging.
logits (`),Nr=n(k,"CODE",{});var Wd=i(Nr);Fl=s(Wd,"torch.FloatTensor"),Wd.forEach(o),Ll=s(k," of shape "),Sr=n(k,"CODE",{});var Nd=i(Sr);Dl=s(Nd,"(batch_size, num_patches, num_queries)"),Nd.forEach(o),Al=s(k,`) \u2014 Classification logits (including no-object) for all queries.
pred_boxes (`),Br=n(k,"CODE",{});var Sd=i(Br);Wl=s(Sd,"torch.FloatTensor"),Sd.forEach(o),Nl=s(k," of shape "),Hr=n(k,"CODE",{});var Bd=i(Hr);Sl=s(Bd,"(batch_size, num_patches, 4)"),Bd.forEach(o),Bl=s(k,`) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use `),Rr=n(k,"CODE",{});var Hd=i(Rr);Hl=s(Hd,"post_process()"),Hd.forEach(o),Rl=s(k,` to retrieve the unnormalized
bounding boxes.
text_embeds(`),Ur=n(k,"CODE",{});var Rd=i(Ur);Ul=s(Rd,"torch.FloatTensor"),Rd.forEach(o),Gl=s(k," of shape "),Gr=n(k,"CODE",{});var Ud=i(Gr);Xl=s(Ud,"(batch_size, num_max_text_queries, output_dim"),Ud.forEach(o),Zl=s(k,") \u2014 The text embeddings obtained by applying the projection layer to the pooled output of "),Wo=n(k,"A",{href:!0});var Gd=i(Wo);Jl=s(Gd,"OwlViTTextModel"),Gd.forEach(o),Kl=s(k,`.
image_embeds(`),Xr=n(k,"CODE",{});var Xd=i(Xr);Yl=s(Xd,"torch.FloatTensor"),Xd.forEach(o),Ql=s(k," of shape "),Zr=n(k,"CODE",{});var Zd=i(Zr);ec=s(Zd,"(batch_size, patch_size, patch_size, output_dim"),Zd.forEach(o),tc=s(k,") \u2014 Pooled output of "),No=n(k,"A",{href:!0});var Jd=i(No);oc=s(Jd,"OwlViTVisionModel"),Jd.forEach(o),rc=s(k,`. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.
class_embeds (`),Jr=n(k,"CODE",{});var Kd=i(Jr);sc=s(Kd,"torch.FloatTensor"),Kd.forEach(o),ac=s(k," of shape "),Kr=n(k,"CODE",{});var Yd=i(Kr);nc=s(Yd,"(batch_size, num_patches, hidden_size)"),Yd.forEach(o),ic=s(k,", "),Yr=n(k,"EM",{});var Qd=i(Yr);lc=s(Qd,"optional"),Qd.forEach(o),cc=s(k,`) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.`),k.forEach(o),Pd.forEach(o),dc=f(J),b(Qe.$$.fragment,J),J.forEach(o),ra.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(bp)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(u,"class","relative group"),d(Oe,"id","overview"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#overview"),d(K,"class","relative group"),d(ft,"href","https://arxiv.org/abs/2205.06230"),d(ft,"rel","nofollow"),d(xe,"id","usage"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#usage"),d(Y,"class","relative group"),d(io,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(lo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),d(co,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(po,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(mo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),d(ho,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(fo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(_t,"href","https://huggingface.co/adirik"),d(_t,"rel","nofollow"),d(wt,"href","https://github.com/google-research/scenic/tree/a41d24676f64a2158bfcd7cb79b0a87673aa875b/scenic/projects/owl_vit"),d(wt,"rel","nofollow"),d(ye,"id","transformers.OwlViTConfig"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#transformers.OwlViTConfig"),d(Q,"class","relative group"),d(uo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"),d(go,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),d(_o,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d(wo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d(vo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTConfig"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ee,"id","transformers.OwlViTTextConfig"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#transformers.OwlViTTextConfig"),d(te,"class","relative group"),d(To,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),d(xt,"href","https://huggingface.co/adirik/owlvit-base-patch32"),d(xt,"rel","nofollow"),d(bo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d($o,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pe,"id","transformers.OwlViTVisionConfig"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#transformers.OwlViTVisionConfig"),d(se,"class","relative group"),d(Oo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(jt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(jt,"rel","nofollow"),d(Vo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d(xo,"href","/docs/transformers/pr_17938/en/main_classes/configuration#transformers.PretrainedConfig"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.OwlViTFeatureExtractor"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.OwlViTFeatureExtractor"),d(ie,"class","relative group"),d(yo,"href","/docs/transformers/pr_17938/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ie,"id","transformers.OwlViTProcessor"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#transformers.OwlViTProcessor"),d(le,"class","relative group"),d(ko,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(jo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizer"),d(Eo,"href","/docs/transformers/pr_17938/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(zo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(Po,"href","/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Mo,"href","/docs/transformers/pr_17938/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(De,"id","transformers.OwlViTModel"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#transformers.OwlViTModel"),d(ce,"class","relative group"),d(qo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Co,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Io,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTModel"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.OwlViTTextModel"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.OwlViTTextModel"),d(he,"class","relative group"),d(Fo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.OwlViTVisionModel"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.OwlViTVisionModel"),d(ge,"class","relative group"),d(Lo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.OwlViTForObjectDetection"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.OwlViTForObjectDetection"),d(ve,"class","relative group"),d(Do,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(Ao,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(Wo,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(No,"href","/docs/transformers/pr_17938/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,c),_(t,v,g),_(t,u,g),e(u,p),e(p,w),$(l,w,null),e(u,m),e(u,E),e(E,sa),_(t,ps,g),_(t,K,g),e(K,Oe),e(Oe,tr),$(ht,tr,null),e(K,aa),e(K,or),e(or,na),_(t,ms,g),_(t,Ve,g),e(Ve,ia),e(Ve,ft),e(ft,la),e(Ve,ca),_(t,hs,g),_(t,so,g),e(so,da),_(t,fs,g),_(t,ao,g),e(ao,rr),e(rr,pa),_(t,us,g),_(t,Y,g),e(Y,xe),e(xe,sr),$(ut,sr,null),e(Y,ma),e(Y,ar),e(ar,ha),_(t,gs,g),_(t,no,g),e(no,fa),_(t,_s,g),_(t,z,g),e(z,ua),e(z,io),e(io,ga),e(z,_a),e(z,lo),e(lo,wa),e(z,va),e(z,co),e(co,Ta),e(z,ba),e(z,po),e(po,$a),e(z,Oa),e(z,mo),e(mo,Va),e(z,xa),e(z,ho),e(ho,ya),e(z,ka),e(z,fo),e(fo,ja),e(z,Ea),_(t,ws,g),$(gt,t,g),_(t,vs,g),_(t,X,g),e(X,za),e(X,_t),e(_t,Pa),e(X,Ma),e(X,wt),e(wt,qa),e(X,Ca),_(t,Ts,g),_(t,Q,g),e(Q,ye),e(ye,nr),$(vt,nr,null),e(Q,Ia),e(Q,ir),e(ir,Fa),_(t,bs,g),_(t,I,g),$(Tt,I,null),e(I,La),e(I,ke),e(ke,uo),e(uo,Da),e(ke,Aa),e(ke,go),e(go,Wa),e(ke,Na),e(I,Sa),e(I,ee),e(ee,Ba),e(ee,_o),e(_o,Ha),e(ee,Ra),e(ee,wo),e(wo,Ua),e(ee,Ga),e(I,Xa),e(I,je),$(bt,je,null),e(je,Za),e(je,$t),e($t,Ja),e($t,vo),e(vo,Ka),e($t,Ya),_(t,$s,g),_(t,te,g),e(te,Ee),e(Ee,lr),$(Ot,lr,null),e(te,Qa),e(te,cr),e(cr,en),_(t,Os,g),_(t,F,g),$(Vt,F,null),e(F,tn),e(F,oe),e(oe,on),e(oe,To),e(To,rn),e(oe,sn),e(oe,xt),e(xt,an),e(oe,nn),e(F,ln),e(F,re),e(re,cn),e(re,bo),e(bo,dn),e(re,pn),e(re,$o),e($o,mn),e(re,hn),e(F,fn),$(ze,F,null),_(t,Vs,g),_(t,se,g),e(se,Pe),e(Pe,dr),$(yt,dr,null),e(se,un),e(se,pr),e(pr,gn),_(t,xs,g),_(t,L,g),$(kt,L,null),e(L,_n),e(L,ae),e(ae,wn),e(ae,Oo),e(Oo,vn),e(ae,Tn),e(ae,jt),e(jt,bn),e(ae,$n),e(L,On),e(L,ne),e(ne,Vn),e(ne,Vo),e(Vo,xn),e(ne,yn),e(ne,xo),e(xo,kn),e(ne,jn),e(L,En),$(Me,L,null),_(t,ys,g),_(t,ie,g),e(ie,qe),e(qe,mr),$(Et,mr,null),e(ie,zn),e(ie,hr),e(hr,Pn),_(t,ks,g),_(t,D,g),$(zt,D,null),e(D,Mn),e(D,fr),e(fr,qn),e(D,Cn),e(D,Pt),e(Pt,In),e(Pt,yo),e(yo,Fn),e(Pt,Ln),e(D,Dn),e(D,Z),$(Mt,Z,null),e(Z,An),e(Z,ur),e(ur,Wn),e(Z,Nn),$(Ce,Z,null),_(t,js,g),_(t,le,g),e(le,Ie),e(Ie,gr),$(qt,gr,null),e(le,Sn),e(le,_r),e(_r,Bn),_(t,Es,g),_(t,A,g),$(Ct,A,null),e(A,Hn),e(A,q),e(q,Rn),e(q,ko),e(ko,Un),e(q,Gn),e(q,jo),e(jo,Xn),e(q,Zn),e(q,Eo),e(Eo,Jn),e(q,Kn),e(q,wr),e(wr,Yn),e(q,Qn),e(q,zo),e(zo,ei),e(q,ti),e(A,oi),e(A,Fe),$(It,Fe,null),e(Fe,ri),e(Fe,Ft),e(Ft,si),e(Ft,Po),e(Po,ai),e(Ft,ni),e(A,ii),e(A,Le),$(Lt,Le,null),e(Le,li),e(Le,Dt),e(Dt,ci),e(Dt,Mo),e(Mo,di),e(Dt,pi),_(t,zs,g),_(t,ce,g),e(ce,De),e(De,vr),$(At,vr,null),e(ce,mi),e(ce,Tr),e(Tr,hi),_(t,Ps,g),_(t,W,g),$(Wt,W,null),e(W,fi),e(W,N),$(Nt,N,null),e(N,ui),e(N,de),e(de,gi),e(de,qo),e(qo,_i),e(de,wi),e(de,br),e(br,vi),e(de,Ti),e(N,bi),$(Ae,N,null),e(N,$i),$(We,N,null),e(W,Oi),e(W,S),$(St,S,null),e(S,Vi),e(S,pe),e(pe,xi),e(pe,Co),e(Co,yi),e(pe,ki),e(pe,$r),e($r,ji),e(pe,Ei),e(S,zi),$(Ne,S,null),e(S,Pi),$(Se,S,null),e(W,Mi),e(W,B),$(Bt,B,null),e(B,qi),e(B,me),e(me,Ci),e(me,Io),e(Io,Ii),e(me,Fi),e(me,Or),e(Or,Li),e(me,Di),e(B,Ai),$(Be,B,null),e(B,Wi),$(He,B,null),_(t,Ms,g),_(t,he,g),e(he,Re),e(Re,Vr),$(Ht,Vr,null),e(he,Ni),e(he,xr),e(xr,Si),_(t,qs,g),_(t,fe,g),$(Rt,fe,null),e(fe,Bi),e(fe,H),$(Ut,H,null),e(H,Hi),e(H,ue),e(ue,Ri),e(ue,Fo),e(Fo,Ui),e(ue,Gi),e(ue,yr),e(yr,Xi),e(ue,Zi),e(H,Ji),$(Ue,H,null),e(H,Ki),$(Ge,H,null),_(t,Cs,g),_(t,ge,g),e(ge,Xe),e(Xe,kr),$(Gt,kr,null),e(ge,Yi),e(ge,jr),e(jr,Qi),_(t,Is,g),_(t,_e,g),$(Xt,_e,null),e(_e,el),e(_e,R),$(Zt,R,null),e(R,tl),e(R,we),e(we,ol),e(we,Lo),e(Lo,rl),e(we,sl),e(we,Er),e(Er,al),e(we,nl),e(R,il),$(Ze,R,null),e(R,ll),$(Je,R,null),_(t,Fs,g),_(t,ve,g),e(ve,Ke),e(Ke,zr),$(Jt,zr,null),e(ve,cl),e(ve,Pr),e(Pr,dl),_(t,Ls,g),_(t,Te,g),$(Kt,Te,null),e(Te,pl),e(Te,C),$(Yt,C,null),e(C,ml),e(C,be),e(be,hl),e(be,Do),e(Do,fl),e(be,ul),e(be,Mr),e(Mr,gl),e(be,_l),e(C,wl),$(Ye,C,null),e(C,vl),e(C,qr),e(qr,y),e(y,Cr),e(Cr,Tl),e(y,bl),e(y,Ao),e(Ao,$l),e(y,Ol),e(y,Ir),e(Ir,Vl),e(y,xl),e(y,Fr),e(Fr,yl),e(y,kl),e(y,Lr),e(Lr,jl),e(y,El),e(y,Dr),e(Dr,zl),e(y,Pl),e(y,Ar),e(Ar,Ml),e(y,ql),e(y,Wr),e(Wr,Cl),e(y,Il),e(y,Nr),e(Nr,Fl),e(y,Ll),e(y,Sr),e(Sr,Dl),e(y,Al),e(y,Br),e(Br,Wl),e(y,Nl),e(y,Hr),e(Hr,Sl),e(y,Bl),e(y,Rr),e(Rr,Hl),e(y,Rl),e(y,Ur),e(Ur,Ul),e(y,Gl),e(y,Gr),e(Gr,Xl),e(y,Zl),e(y,Wo),e(Wo,Jl),e(y,Kl),e(y,Xr),e(Xr,Yl),e(y,Ql),e(y,Zr),e(Zr,ec),e(y,tc),e(y,No),e(No,oc),e(y,rc),e(y,Jr),e(Jr,sc),e(y,ac),e(y,Kr),e(Kr,nc),e(y,ic),e(y,Yr),e(Yr,lc),e(y,cc),e(C,dc),$(Qe,C,null),Ds=!0},p(t,[g]){const Qt={};g&2&&(Qt.$$scope={dirty:g,ctx:t}),ze.$set(Qt);const Qr={};g&2&&(Qr.$$scope={dirty:g,ctx:t}),Me.$set(Qr);const es={};g&2&&(es.$$scope={dirty:g,ctx:t}),Ce.$set(es);const ts={};g&2&&(ts.$$scope={dirty:g,ctx:t}),Ae.$set(ts);const eo={};g&2&&(eo.$$scope={dirty:g,ctx:t}),We.$set(eo);const os={};g&2&&(os.$$scope={dirty:g,ctx:t}),Ne.$set(os);const rs={};g&2&&(rs.$$scope={dirty:g,ctx:t}),Se.$set(rs);const ss={};g&2&&(ss.$$scope={dirty:g,ctx:t}),Be.$set(ss);const to={};g&2&&(to.$$scope={dirty:g,ctx:t}),He.$set(to);const as={};g&2&&(as.$$scope={dirty:g,ctx:t}),Ue.$set(as);const ns={};g&2&&(ns.$$scope={dirty:g,ctx:t}),Ge.$set(ns);const is={};g&2&&(is.$$scope={dirty:g,ctx:t}),Ze.$set(is);const ls={};g&2&&(ls.$$scope={dirty:g,ctx:t}),Je.$set(ls);const oo={};g&2&&(oo.$$scope={dirty:g,ctx:t}),Ye.$set(oo);const cs={};g&2&&(cs.$$scope={dirty:g,ctx:t}),Qe.$set(cs)},i(t){Ds||(O(l.$$.fragment,t),O(ht.$$.fragment,t),O(ut.$$.fragment,t),O(gt.$$.fragment,t),O(vt.$$.fragment,t),O(Tt.$$.fragment,t),O(bt.$$.fragment,t),O(Ot.$$.fragment,t),O(Vt.$$.fragment,t),O(ze.$$.fragment,t),O(yt.$$.fragment,t),O(kt.$$.fragment,t),O(Me.$$.fragment,t),O(Et.$$.fragment,t),O(zt.$$.fragment,t),O(Mt.$$.fragment,t),O(Ce.$$.fragment,t),O(qt.$$.fragment,t),O(Ct.$$.fragment,t),O(It.$$.fragment,t),O(Lt.$$.fragment,t),O(At.$$.fragment,t),O(Wt.$$.fragment,t),O(Nt.$$.fragment,t),O(Ae.$$.fragment,t),O(We.$$.fragment,t),O(St.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Bt.$$.fragment,t),O(Be.$$.fragment,t),O(He.$$.fragment,t),O(Ht.$$.fragment,t),O(Rt.$$.fragment,t),O(Ut.$$.fragment,t),O(Ue.$$.fragment,t),O(Ge.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Zt.$$.fragment,t),O(Ze.$$.fragment,t),O(Je.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Yt.$$.fragment,t),O(Ye.$$.fragment,t),O(Qe.$$.fragment,t),Ds=!0)},o(t){V(l.$$.fragment,t),V(ht.$$.fragment,t),V(ut.$$.fragment,t),V(gt.$$.fragment,t),V(vt.$$.fragment,t),V(Tt.$$.fragment,t),V(bt.$$.fragment,t),V(Ot.$$.fragment,t),V(Vt.$$.fragment,t),V(ze.$$.fragment,t),V(yt.$$.fragment,t),V(kt.$$.fragment,t),V(Me.$$.fragment,t),V(Et.$$.fragment,t),V(zt.$$.fragment,t),V(Mt.$$.fragment,t),V(Ce.$$.fragment,t),V(qt.$$.fragment,t),V(Ct.$$.fragment,t),V(It.$$.fragment,t),V(Lt.$$.fragment,t),V(At.$$.fragment,t),V(Wt.$$.fragment,t),V(Nt.$$.fragment,t),V(Ae.$$.fragment,t),V(We.$$.fragment,t),V(St.$$.fragment,t),V(Ne.$$.fragment,t),V(Se.$$.fragment,t),V(Bt.$$.fragment,t),V(Be.$$.fragment,t),V(He.$$.fragment,t),V(Ht.$$.fragment,t),V(Rt.$$.fragment,t),V(Ut.$$.fragment,t),V(Ue.$$.fragment,t),V(Ge.$$.fragment,t),V(Gt.$$.fragment,t),V(Xt.$$.fragment,t),V(Zt.$$.fragment,t),V(Ze.$$.fragment,t),V(Je.$$.fragment,t),V(Jt.$$.fragment,t),V(Kt.$$.fragment,t),V(Yt.$$.fragment,t),V(Ye.$$.fragment,t),V(Qe.$$.fragment,t),Ds=!1},d(t){o(c),t&&o(v),t&&o(u),x(l),t&&o(ps),t&&o(K),x(ht),t&&o(ms),t&&o(Ve),t&&o(hs),t&&o(so),t&&o(fs),t&&o(ao),t&&o(us),t&&o(Y),x(ut),t&&o(gs),t&&o(no),t&&o(_s),t&&o(z),t&&o(ws),x(gt,t),t&&o(vs),t&&o(X),t&&o(Ts),t&&o(Q),x(vt),t&&o(bs),t&&o(I),x(Tt),x(bt),t&&o($s),t&&o(te),x(Ot),t&&o(Os),t&&o(F),x(Vt),x(ze),t&&o(Vs),t&&o(se),x(yt),t&&o(xs),t&&o(L),x(kt),x(Me),t&&o(ys),t&&o(ie),x(Et),t&&o(ks),t&&o(D),x(zt),x(Mt),x(Ce),t&&o(js),t&&o(le),x(qt),t&&o(Es),t&&o(A),x(Ct),x(It),x(Lt),t&&o(zs),t&&o(ce),x(At),t&&o(Ps),t&&o(W),x(Wt),x(Nt),x(Ae),x(We),x(St),x(Ne),x(Se),x(Bt),x(Be),x(He),t&&o(Ms),t&&o(he),x(Ht),t&&o(qs),t&&o(fe),x(Rt),x(Ut),x(Ue),x(Ge),t&&o(Cs),t&&o(ge),x(Gt),t&&o(Is),t&&o(_e),x(Xt),x(Zt),x(Ze),x(Je),t&&o(Fs),t&&o(ve),x(Jt),t&&o(Ls),t&&o(Te),x(Kt),x(Yt),x(Ye),x(Qe)}}}const bp={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function $p(j){return sp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ep extends ep{constructor(c){super();tp(this,c,$p,Tp,op,{})}}export{Ep as default,bp as metadata};
