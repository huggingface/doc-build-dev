import{S as Ct,i as Ut,s as Ft,e as o,k as h,w as H,t as l,M as Mt,c as i,d as t,m as c,a as r,x as G,h as p,b as f,G as a,g as s,y as D,L as St,q as R,o as J,B as O,v as Bt}from"../chunks/vendor-hf-doc-builder.js";import{I as ke}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Tt}from"../chunks/CodeBlock-hf-doc-builder.js";function qt(it){let u,re,m,v,K,A,Te,Q,Ce,se,S,Ue,le,d,w,W,g,Fe,Y,Me,pe,B,Se,fe,q,Be,he,b,qe,I,Le,Ne,ce,_,E,Z,X,je,ee,ze,ue,L,Ve,me,N,He,de,k,_e,j,Ge,Pe,T,ve,x,De,C,Re,Je,we,P,y,te,U,Oe,ae,Ke,be,F,Qe,M,We,Ee,z,V,Ye,$,Ze,ne,et,tt,oe,at,xe;return A=new ke({}),g=new ke({}),X=new ke({}),k=new Tt({props:{code:"pip install intel_extension_for_pytorch==1.10.100+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">100</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),T=new Tt({props:{code:"pip install intel_extension_for_pytorch==1.11.200+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">11</span>.<span class="hljs-number">200</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),U=new ke({}),{c(){u=o("meta"),re=h(),m=o("h1"),v=o("a"),K=o("span"),H(A.$$.fragment),Te=h(),Q=o("span"),Ce=l("Efficient Training on CPU"),se=h(),S=o("p"),Ue=l("This guide focuses on training large models efficiently on CPU."),le=h(),d=o("h2"),w=o("a"),W=o("span"),H(g.$$.fragment),Fe=h(),Y=o("span"),Me=l("Mixed precision with IPEX"),pe=h(),B=o("p"),Se=l("IPEX optimizes for CPU with AVX-512 or above. It can also functionally work for CPUs with AVX2. So, both Intel CPUs and AMD CPUs are likely to result in a better performance under IPEX. IPEX provides performance optimizations for both Float32 and BFloat16. Float32 is used by default and the following parts will show the usage of BFloat16."),fe=h(),q=o("p"),Be=l("Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),he=h(),b=o("p"),qe=l("Check more detailed information for "),I=o("a"),Le=l("Auto Mixed Precision"),Ne=l("."),ce=h(),_=o("h3"),E=o("a"),Z=o("span"),H(X.$$.fragment),je=h(),ee=o("span"),ze=l("IPEX installation:"),ue=h(),L=o("p"),Ve=l("IPEX release is following PyTorch, to install via pip:"),me=h(),N=o("p"),He=l("For PyTorch-1.10:"),de=h(),H(k.$$.fragment),_e=h(),j=o("p"),Ge=l("For PyTorch-1.11:"),Pe=h(),H(T.$$.fragment),ve=h(),x=o("p"),De=l("Check more approaches for "),C=o("a"),Re=l("IPEX installation"),Je=l("."),we=h(),P=o("h3"),y=o("a"),te=o("span"),H(U.$$.fragment),Oe=h(),ae=o("span"),Ke=l("Usage in Trainer"),be=l("\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),F=o("p"),Qe=l("Take an example of the use cases on "),M=o("a"),We=l("Transformers question-answering"),Ee=h(),z=o("ul"),V=o("li"),Ye=l("Training with IPEX using BF16 auto mixed precision on CPU:"),$=o("pre"),Ze=l(` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ne=o("b"),et=l("--use_ipex \\"),tt=l(`
`),oe=o("b"),at=l("--bf16 --no_cuda"),this.h()},l(e){const n=Mt('[data-svelte="svelte-1phssyn"]',document.head);u=i(n,"META",{name:!0,content:!0}),n.forEach(t),re=c(e),m=i(e,"H1",{class:!0});var ye=r(m);v=i(ye,"A",{id:!0,class:!0,href:!0});var rt=r(v);K=i(rt,"SPAN",{});var st=r(K);G(A.$$.fragment,st),st.forEach(t),rt.forEach(t),Te=c(ye),Q=i(ye,"SPAN",{});var lt=r(Q);Ce=p(lt,"Efficient Training on CPU"),lt.forEach(t),ye.forEach(t),se=c(e),S=i(e,"P",{});var pt=r(S);Ue=p(pt,"This guide focuses on training large models efficiently on CPU."),pt.forEach(t),le=c(e),d=i(e,"H2",{class:!0});var $e=r(d);w=i($e,"A",{id:!0,class:!0,href:!0});var ft=r(w);W=i(ft,"SPAN",{});var ht=r(W);G(g.$$.fragment,ht),ht.forEach(t),ft.forEach(t),Fe=c($e),Y=i($e,"SPAN",{});var ct=r(Y);Me=p(ct,"Mixed precision with IPEX"),ct.forEach(t),$e.forEach(t),pe=c(e),B=i(e,"P",{});var ut=r(B);Se=p(ut,"IPEX optimizes for CPU with AVX-512 or above. It can also functionally work for CPUs with AVX2. So, both Intel CPUs and AMD CPUs are likely to result in a better performance under IPEX. IPEX provides performance optimizations for both Float32 and BFloat16. Float32 is used by default and the following parts will show the usage of BFloat16."),ut.forEach(t),fe=c(e),q=i(e,"P",{});var mt=r(q);Be=p(mt,"Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),mt.forEach(t),he=c(e),b=i(e,"P",{});var Ae=r(b);qe=p(Ae,"Check more detailed information for "),I=i(Ae,"A",{href:!0,rel:!0});var dt=r(I);Le=p(dt,"Auto Mixed Precision"),dt.forEach(t),Ne=p(Ae,"."),Ae.forEach(t),ce=c(e),_=i(e,"H3",{class:!0});var ge=r(_);E=i(ge,"A",{id:!0,class:!0,href:!0});var _t=r(E);Z=i(_t,"SPAN",{});var Pt=r(Z);G(X.$$.fragment,Pt),Pt.forEach(t),_t.forEach(t),je=c(ge),ee=i(ge,"SPAN",{});var vt=r(ee);ze=p(vt,"IPEX installation:"),vt.forEach(t),ge.forEach(t),ue=c(e),L=i(e,"P",{});var wt=r(L);Ve=p(wt,"IPEX release is following PyTorch, to install via pip:"),wt.forEach(t),me=c(e),N=i(e,"P",{});var bt=r(N);He=p(bt,"For PyTorch-1.10:"),bt.forEach(t),de=c(e),G(k.$$.fragment,e),_e=c(e),j=i(e,"P",{});var Et=r(j);Ge=p(Et,"For PyTorch-1.11:"),Et.forEach(t),Pe=c(e),G(T.$$.fragment,e),ve=c(e),x=i(e,"P",{});var Ie=r(x);De=p(Ie,"Check more approaches for "),C=i(Ie,"A",{href:!0,rel:!0});var xt=r(C);Re=p(xt,"IPEX installation"),xt.forEach(t),Je=p(Ie,"."),Ie.forEach(t),we=c(e),P=i(e,"H3",{class:!0});var Xe=r(P);y=i(Xe,"A",{id:!0,class:!0,href:!0});var yt=r(y);te=i(yt,"SPAN",{});var $t=r(te);G(U.$$.fragment,$t),$t.forEach(t),yt.forEach(t),Oe=c(Xe),ae=i(Xe,"SPAN",{});var At=r(ae);Ke=p(At,"Usage in Trainer"),At.forEach(t),Xe.forEach(t),be=p(e,"\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),F=i(e,"P",{});var nt=r(F);Qe=p(nt,"Take an example of the use cases on "),M=i(nt,"A",{href:!0,rel:!0});var gt=r(M);We=p(gt,"Transformers question-answering"),gt.forEach(t),nt.forEach(t),Ee=c(e),z=i(e,"UL",{});var It=r(z);V=i(It,"LI",{});var ot=r(V);Ye=p(ot,"Training with IPEX using BF16 auto mixed precision on CPU:"),$=i(ot,"PRE",{});var ie=r($);Ze=p(ie,` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ne=i(ie,"B",{});var Xt=r(ne);et=p(Xt,"--use_ipex \\"),Xt.forEach(t),tt=p(ie,`
`),oe=i(ie,"B",{});var kt=r(oe);at=p(kt,"--bf16 --no_cuda"),kt.forEach(t),ie.forEach(t),ot.forEach(t),It.forEach(t),this.h()},h(){f(u,"name","hf:doc:metadata"),f(u,"content",JSON.stringify(Lt)),f(v,"id","efficient-training-on-cpu"),f(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(v,"href","#efficient-training-on-cpu"),f(m,"class","relative group"),f(w,"id","mixed-precision-with-ipex"),f(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(w,"href","#mixed-precision-with-ipex"),f(d,"class","relative group"),f(I,"href","https://intel.github.io/intel-extension-for-pytorch/1.11.200/tutorials/features/amp.html"),f(I,"rel","nofollow"),f(E,"id","ipex-installation"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#ipex-installation"),f(_,"class","relative group"),f(C,"href","https://intel.github.io/intel-extension-for-pytorch/1.11.200/tutorials/installation.html"),f(C,"rel","nofollow"),f(y,"id","usage-in-trainer"),f(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(y,"href","#usage-in-trainer"),f(P,"class","relative group"),f(M,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),f(M,"rel","nofollow")},m(e,n){a(document.head,u),s(e,re,n),s(e,m,n),a(m,v),a(v,K),D(A,K,null),a(m,Te),a(m,Q),a(Q,Ce),s(e,se,n),s(e,S,n),a(S,Ue),s(e,le,n),s(e,d,n),a(d,w),a(w,W),D(g,W,null),a(d,Fe),a(d,Y),a(Y,Me),s(e,pe,n),s(e,B,n),a(B,Se),s(e,fe,n),s(e,q,n),a(q,Be),s(e,he,n),s(e,b,n),a(b,qe),a(b,I),a(I,Le),a(b,Ne),s(e,ce,n),s(e,_,n),a(_,E),a(E,Z),D(X,Z,null),a(_,je),a(_,ee),a(ee,ze),s(e,ue,n),s(e,L,n),a(L,Ve),s(e,me,n),s(e,N,n),a(N,He),s(e,de,n),D(k,e,n),s(e,_e,n),s(e,j,n),a(j,Ge),s(e,Pe,n),D(T,e,n),s(e,ve,n),s(e,x,n),a(x,De),a(x,C),a(C,Re),a(x,Je),s(e,we,n),s(e,P,n),a(P,y),a(y,te),D(U,te,null),a(P,Oe),a(P,ae),a(ae,Ke),s(e,be,n),s(e,F,n),a(F,Qe),a(F,M),a(M,We),s(e,Ee,n),s(e,z,n),a(z,V),a(V,Ye),a(V,$),a($,Ze),a($,ne),a(ne,et),a($,tt),a($,oe),a(oe,at),xe=!0},p:St,i(e){xe||(R(A.$$.fragment,e),R(g.$$.fragment,e),R(X.$$.fragment,e),R(k.$$.fragment,e),R(T.$$.fragment,e),R(U.$$.fragment,e),xe=!0)},o(e){J(A.$$.fragment,e),J(g.$$.fragment,e),J(X.$$.fragment,e),J(k.$$.fragment,e),J(T.$$.fragment,e),J(U.$$.fragment,e),xe=!1},d(e){t(u),e&&t(re),e&&t(m),O(A),e&&t(se),e&&t(S),e&&t(le),e&&t(d),O(g),e&&t(pe),e&&t(B),e&&t(fe),e&&t(q),e&&t(he),e&&t(b),e&&t(ce),e&&t(_),O(X),e&&t(ue),e&&t(L),e&&t(me),e&&t(N),e&&t(de),O(k,e),e&&t(_e),e&&t(j),e&&t(Pe),O(T,e),e&&t(ve),e&&t(x),e&&t(we),e&&t(P),O(U),e&&t(be),e&&t(F),e&&t(Ee),e&&t(z)}}}const Lt={local:"efficient-training-on-cpu",sections:[{local:"mixed-precision-with-ipex",sections:[{local:"ipex-installation",title:"IPEX installation:"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Mixed precision with IPEX"}],title:"Efficient Training on CPU"};function Nt(it){return Bt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ht extends Ct{constructor(u){super();Ut(this,u,Nt,qt,Ft,{})}}export{Ht as default,Lt as metadata};
