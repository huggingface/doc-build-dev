import{S as At,i as kt,s as It,e as i,k as f,w as H,t as p,M as Xt,c as r,d as t,m as c,a as o,x as G,h,b as l,G as n,g as s,y as R,L as Ct,q as V,o as J,B as O,v as Mt}from"../chunks/vendor-hf-doc-builder.js";import{I as Ae}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Tt}from"../chunks/CodeBlock-hf-doc-builder.js";function Ut(nt){let u,re,m,P,D,g,ke,K,Ie,oe,q,Xe,se,d,w,Q,T,Ce,W,Me,le,B,Ue,pe,b,Se,A,qe,Be,he,_,x,Y,k,Fe,Z,Le,fe,F,Ne,ce,L,je,ue,I,me,N,ze,de,X,_e,E,He,C,Ge,Re,ve,v,y,ee,M,Ve,te,Je,Pe,U,Oe,S,De,we,j,z,Ke,$,Qe,ne,We,Ye,ae,Ze,be;return g=new Ae({}),T=new Ae({}),k=new Ae({}),I=new Tt({props:{code:"pip install intel_extension_for_pytorch==1.10.100+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">100</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),X=new Tt({props:{code:"pip install intel_extension_for_pytorch==1.11.200+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">11</span>.<span class="hljs-number">200</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),M=new Ae({}),{c(){u=i("meta"),re=f(),m=i("h1"),P=i("a"),D=i("span"),H(g.$$.fragment),ke=f(),K=i("span"),Ie=p("Efficient Training on CPU"),oe=f(),q=i("p"),Xe=p("This guide focuses on training large models efficiently on CPU."),se=f(),d=i("h2"),w=i("a"),Q=i("span"),H(T.$$.fragment),Ce=f(),W=i("span"),Me=p("Mixed precision with IPEX"),le=f(),B=i("p"),Ue=p("Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators have been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),pe=f(),b=i("p"),Se=p("Check more detailed information for "),A=i("a"),qe=p("Auto Mixed Precision"),Be=p("."),he=f(),_=i("h3"),x=i("a"),Y=i("span"),H(k.$$.fragment),Fe=f(),Z=i("span"),Le=p("IPEX installation:"),fe=f(),F=i("p"),Ne=p("IPEX release is following PyTorch, to install via pip:"),ce=f(),L=i("p"),je=p("For PyTorch-1.10:"),ue=f(),H(I.$$.fragment),me=f(),N=i("p"),ze=p("For PyTorch-1.11:"),de=f(),H(X.$$.fragment),_e=f(),E=i("p"),He=p("Check more approaches for "),C=i("a"),Ge=p("IPEX installation"),Re=p("."),ve=f(),v=i("h3"),y=i("a"),ee=i("span"),H(M.$$.fragment),Ve=f(),te=i("span"),Je=p("Usage in Trainer"),Pe=p("\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),U=i("p"),Oe=p("Take an example of the use cases on "),S=i("a"),De=p("Transformers question-answering"),we=f(),j=i("ul"),z=i("li"),Ke=p("Training with IPEX using BF16 auto mixed precision on CPU:"),$=i("pre"),Qe=p(` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ne=i("b"),We=p("--use_ipex \\"),Ye=p(`
`),ae=i("b"),Ze=p("--bf16 --no_cuda"),this.h()},l(e){const a=Xt('[data-svelte="svelte-1phssyn"]',document.head);u=r(a,"META",{name:!0,content:!0}),a.forEach(t),re=c(e),m=r(e,"H1",{class:!0});var xe=o(m);P=r(xe,"A",{id:!0,class:!0,href:!0});var at=o(P);D=r(at,"SPAN",{});var it=o(D);G(g.$$.fragment,it),it.forEach(t),at.forEach(t),ke=c(xe),K=r(xe,"SPAN",{});var rt=o(K);Ie=h(rt,"Efficient Training on CPU"),rt.forEach(t),xe.forEach(t),oe=c(e),q=r(e,"P",{});var ot=o(q);Xe=h(ot,"This guide focuses on training large models efficiently on CPU."),ot.forEach(t),se=c(e),d=r(e,"H2",{class:!0});var Ee=o(d);w=r(Ee,"A",{id:!0,class:!0,href:!0});var st=o(w);Q=r(st,"SPAN",{});var lt=o(Q);G(T.$$.fragment,lt),lt.forEach(t),st.forEach(t),Ce=c(Ee),W=r(Ee,"SPAN",{});var pt=o(W);Me=h(pt,"Mixed precision with IPEX"),pt.forEach(t),Ee.forEach(t),le=c(e),B=r(e,"P",{});var ht=o(B);Ue=h(ht,"Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators have been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),ht.forEach(t),pe=c(e),b=r(e,"P",{});var ye=o(b);Se=h(ye,"Check more detailed information for "),A=r(ye,"A",{href:!0,rel:!0});var ft=o(A);qe=h(ft,"Auto Mixed Precision"),ft.forEach(t),Be=h(ye,"."),ye.forEach(t),he=c(e),_=r(e,"H3",{class:!0});var $e=o(_);x=r($e,"A",{id:!0,class:!0,href:!0});var ct=o(x);Y=r(ct,"SPAN",{});var ut=o(Y);G(k.$$.fragment,ut),ut.forEach(t),ct.forEach(t),Fe=c($e),Z=r($e,"SPAN",{});var mt=o(Z);Le=h(mt,"IPEX installation:"),mt.forEach(t),$e.forEach(t),fe=c(e),F=r(e,"P",{});var dt=o(F);Ne=h(dt,"IPEX release is following PyTorch, to install via pip:"),dt.forEach(t),ce=c(e),L=r(e,"P",{});var _t=o(L);je=h(_t,"For PyTorch-1.10:"),_t.forEach(t),ue=c(e),G(I.$$.fragment,e),me=c(e),N=r(e,"P",{});var vt=o(N);ze=h(vt,"For PyTorch-1.11:"),vt.forEach(t),de=c(e),G(X.$$.fragment,e),_e=c(e),E=r(e,"P",{});var ge=o(E);He=h(ge,"Check more approaches for "),C=r(ge,"A",{href:!0,rel:!0});var Pt=o(C);Ge=h(Pt,"IPEX installation"),Pt.forEach(t),Re=h(ge,"."),ge.forEach(t),ve=c(e),v=r(e,"H3",{class:!0});var Te=o(v);y=r(Te,"A",{id:!0,class:!0,href:!0});var wt=o(y);ee=r(wt,"SPAN",{});var bt=o(ee);G(M.$$.fragment,bt),bt.forEach(t),wt.forEach(t),Ve=c(Te),te=r(Te,"SPAN",{});var xt=o(te);Je=h(xt,"Usage in Trainer"),xt.forEach(t),Te.forEach(t),Pe=h(e,"\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),U=r(e,"P",{});var et=o(U);Oe=h(et,"Take an example of the use cases on "),S=r(et,"A",{href:!0,rel:!0});var Et=o(S);De=h(Et,"Transformers question-answering"),Et.forEach(t),et.forEach(t),we=c(e),j=r(e,"UL",{});var yt=o(j);z=r(yt,"LI",{});var tt=o(z);Ke=h(tt,"Training with IPEX using BF16 auto mixed precision on CPU:"),$=r(tt,"PRE",{});var ie=o($);Qe=h(ie,` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ne=r(ie,"B",{});var $t=o(ne);We=h($t,"--use_ipex \\"),$t.forEach(t),Ye=h(ie,`
`),ae=r(ie,"B",{});var gt=o(ae);Ze=h(gt,"--bf16 --no_cuda"),gt.forEach(t),ie.forEach(t),tt.forEach(t),yt.forEach(t),this.h()},h(){l(u,"name","hf:doc:metadata"),l(u,"content",JSON.stringify(St)),l(P,"id","efficient-training-on-cpu"),l(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(P,"href","#efficient-training-on-cpu"),l(m,"class","relative group"),l(w,"id","mixed-precision-with-ipex"),l(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(w,"href","#mixed-precision-with-ipex"),l(d,"class","relative group"),l(A,"href","https://intel.github.io/intel-extension-for-pytorch/1.11.200/tutorials/features/amp.html"),l(A,"rel","nofollow"),l(x,"id","ipex-installation"),l(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(x,"href","#ipex-installation"),l(_,"class","relative group"),l(C,"href","https://intel.github.io/intel-extension-for-pytorch/1.11.200/tutorials/installation.html"),l(C,"rel","nofollow"),l(y,"id","usage-in-trainer"),l(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(y,"href","#usage-in-trainer"),l(v,"class","relative group"),l(S,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),l(S,"rel","nofollow")},m(e,a){n(document.head,u),s(e,re,a),s(e,m,a),n(m,P),n(P,D),R(g,D,null),n(m,ke),n(m,K),n(K,Ie),s(e,oe,a),s(e,q,a),n(q,Xe),s(e,se,a),s(e,d,a),n(d,w),n(w,Q),R(T,Q,null),n(d,Ce),n(d,W),n(W,Me),s(e,le,a),s(e,B,a),n(B,Ue),s(e,pe,a),s(e,b,a),n(b,Se),n(b,A),n(A,qe),n(b,Be),s(e,he,a),s(e,_,a),n(_,x),n(x,Y),R(k,Y,null),n(_,Fe),n(_,Z),n(Z,Le),s(e,fe,a),s(e,F,a),n(F,Ne),s(e,ce,a),s(e,L,a),n(L,je),s(e,ue,a),R(I,e,a),s(e,me,a),s(e,N,a),n(N,ze),s(e,de,a),R(X,e,a),s(e,_e,a),s(e,E,a),n(E,He),n(E,C),n(C,Ge),n(E,Re),s(e,ve,a),s(e,v,a),n(v,y),n(y,ee),R(M,ee,null),n(v,Ve),n(v,te),n(te,Je),s(e,Pe,a),s(e,U,a),n(U,Oe),n(U,S),n(S,De),s(e,we,a),s(e,j,a),n(j,z),n(z,Ke),n(z,$),n($,Qe),n($,ne),n(ne,We),n($,Ye),n($,ae),n(ae,Ze),be=!0},p:Ct,i(e){be||(V(g.$$.fragment,e),V(T.$$.fragment,e),V(k.$$.fragment,e),V(I.$$.fragment,e),V(X.$$.fragment,e),V(M.$$.fragment,e),be=!0)},o(e){J(g.$$.fragment,e),J(T.$$.fragment,e),J(k.$$.fragment,e),J(I.$$.fragment,e),J(X.$$.fragment,e),J(M.$$.fragment,e),be=!1},d(e){t(u),e&&t(re),e&&t(m),O(g),e&&t(oe),e&&t(q),e&&t(se),e&&t(d),O(T),e&&t(le),e&&t(B),e&&t(pe),e&&t(b),e&&t(he),e&&t(_),O(k),e&&t(fe),e&&t(F),e&&t(ce),e&&t(L),e&&t(ue),O(I,e),e&&t(me),e&&t(N),e&&t(de),O(X,e),e&&t(_e),e&&t(E),e&&t(ve),e&&t(v),O(M),e&&t(Pe),e&&t(U),e&&t(we),e&&t(j)}}}const St={local:"efficient-training-on-cpu",sections:[{local:"mixed-precision-with-ipex",sections:[{local:"ipex-installation",title:"IPEX installation:"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Mixed precision with IPEX"}],title:"Efficient Training on CPU"};function qt(nt){return Mt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nt extends At{constructor(u){super();kt(this,u,qt,Ut,It,{})}}export{Nt as default,St as metadata};
