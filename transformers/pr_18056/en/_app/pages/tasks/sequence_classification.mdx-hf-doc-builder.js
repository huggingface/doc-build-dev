import{S as La,i as Oa,s as Wa,e as i,k as g,w as x,t as o,M as Ba,c as p,d as a,m as b,a as f,x as z,h as n,b as w,G as e,g as m,y as q,q as A,o as C,B as D,v as Na,L as Ma}from"../../chunks/vendor-hf-doc-builder.js";import{T as ce}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Ra}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ce}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ft}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Ia,M as De}from"../../chunks/Markdown-hf-doc-builder.js";function Ua(S){let s,h,r,u,$;return{c(){s=i("p"),h=o("See the text classification "),r=i("a"),u=o("task page"),$=o(" for more information about other forms of text classification and their associated models, datasets, and metrics."),this.h()},l(_){s=p(_,"P",{});var v=f(s);h=n(v,"See the text classification "),r=p(v,"A",{href:!0,rel:!0});var E=f(r);u=n(E,"task page"),E.forEach(a),$=n(v," for more information about other forms of text classification and their associated models, datasets, and metrics."),v.forEach(a),this.h()},h(){w(r,"href","https://huggingface.co/tasks/text-classification"),w(r,"rel","nofollow")},m(_,v){m(_,s,v),e(s,h),e(s,r),e(r,u),e(s,$)},d(_){_&&a(s)}}}function Ga(S){let s,h;return s=new ft({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p:Ma,i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function Ha(S){let s,h;return s=new De({props:{$$slots:{default:[Ga]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function Ya(S){let s,h;return s=new ft({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p:Ma,i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function Ka(S){let s,h;return s=new De({props:{$$slots:{default:[Ya]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function Va(S){let s,h,r,u,$,_,v,E;return{c(){s=i("p"),h=o("If you aren\u2019t familiar with fine-tuning a model with the "),r=i("a"),u=o("Trainer"),$=o(", take a look at the basic tutorial "),_=i("a"),v=o("here"),E=o("!"),this.h()},l(j){s=p(j,"P",{});var T=f(s);h=n(T,"If you aren\u2019t familiar with fine-tuning a model with the "),r=p(T,"A",{href:!0});var P=f(r);u=n(P,"Trainer"),P.forEach(a),$=n(T,", take a look at the basic tutorial "),_=p(T,"A",{href:!0});var W=f(_);v=n(W,"here"),W.forEach(a),E=n(T,"!"),T.forEach(a),this.h()},h(){w(r,"href","/docs/transformers/pr_18056/en/main_classes/trainer#transformers.Trainer"),w(_,"href","../training#finetune-with-trainer")},m(j,T){m(j,s,T),e(s,h),e(s,r),e(r,u),e(s,$),e(s,_),e(_,v),e(s,E)},d(j){j&&a(s)}}}function Ja(S){let s,h,r,u,$,_,v;return{c(){s=i("p"),h=i("a"),r=o("Trainer"),u=o(" will apply dynamic padding by default when you pass "),$=i("code"),_=o("tokenizer"),v=o(" to it. In this case, you don\u2019t need to specify a data collator explicitly."),this.h()},l(E){s=p(E,"P",{});var j=f(s);h=p(j,"A",{href:!0});var T=f(h);r=n(T,"Trainer"),T.forEach(a),u=n(j," will apply dynamic padding by default when you pass "),$=p(j,"CODE",{});var P=f($);_=n(P,"tokenizer"),P.forEach(a),v=n(j," to it. In this case, you don\u2019t need to specify a data collator explicitly."),j.forEach(a),this.h()},h(){w(h,"href","/docs/transformers/pr_18056/en/main_classes/trainer#transformers.Trainer")},m(E,j){m(E,s,j),e(s,h),e(h,r),e(s,u),e(s,$),e($,_),e(s,v)},d(E){E&&a(s)}}}function Qa(S){let s,h,r,u,$,_,v,E,j,T,P,W,B,N,I,R,K,et,ut,at,M,Q,nt,st,X,dt,F,L,V,O,gt,H,J,ct,U,mt;return v=new ft({props:{code:`from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),j=new ce({props:{$$slots:{default:[Va]},$$scope:{ctx:S}}}),J=new ft({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),U=new ce({props:{$$slots:{default:[Ja]},$$scope:{ctx:S}}}),{c(){s=i("p"),h=o("Load DistilBERT with "),r=i("a"),u=o("AutoModelForSequenceClassification"),$=o(" along with the number of expected labels:"),_=g(),x(v.$$.fragment),E=g(),x(j.$$.fragment),T=g(),P=i("p"),W=o("At this point, only three steps remain:"),B=g(),N=i("ol"),I=i("li"),R=o("Define your training hyperparameters in "),K=i("a"),et=o("TrainingArguments"),ut=o("."),at=g(),M=i("li"),Q=o("Pass the training arguments to "),nt=i("a"),st=o("Trainer"),X=o(" along with the model, dataset, tokenizer, and data collator."),dt=g(),F=i("li"),L=o("Call "),V=i("a"),O=o("train()"),gt=o(" to fine-tune your model."),H=g(),x(J.$$.fragment),ct=g(),x(U.$$.fragment),this.h()},l(c){s=p(c,"P",{});var y=f(s);h=n(y,"Load DistilBERT with "),r=p(y,"A",{href:!0});var G=f(r);u=n(G,"AutoModelForSequenceClassification"),G.forEach(a),$=n(y," along with the number of expected labels:"),y.forEach(a),_=b(c),z(v.$$.fragment,c),E=b(c),z(j.$$.fragment,c),T=b(c),P=p(c,"P",{});var Z=f(P);W=n(Z,"At this point, only three steps remain:"),Z.forEach(a),B=b(c),N=p(c,"OL",{});var Y=f(N);I=p(Y,"LI",{});var rt=f(I);R=n(rt,"Define your training hyperparameters in "),K=p(rt,"A",{href:!0});var lt=f(K);et=n(lt,"TrainingArguments"),lt.forEach(a),ut=n(rt,"."),rt.forEach(a),at=b(Y),M=p(Y,"LI",{});var ht=f(M);Q=n(ht,"Pass the training arguments to "),nt=p(ht,"A",{href:!0});var it=f(nt);st=n(it,"Trainer"),it.forEach(a),X=n(ht," along with the model, dataset, tokenizer, and data collator."),ht.forEach(a),dt=b(Y),F=p(Y,"LI",{});var tt=f(F);L=n(tt,"Call "),V=p(tt,"A",{href:!0});var _t=f(V);O=n(_t,"train()"),_t.forEach(a),gt=n(tt," to fine-tune your model."),tt.forEach(a),Y.forEach(a),H=b(c),z(J.$$.fragment,c),ct=b(c),z(U.$$.fragment,c),this.h()},h(){w(r,"href","/docs/transformers/pr_18056/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),w(K,"href","/docs/transformers/pr_18056/en/main_classes/trainer#transformers.TrainingArguments"),w(nt,"href","/docs/transformers/pr_18056/en/main_classes/trainer#transformers.Trainer"),w(V,"href","/docs/transformers/pr_18056/en/main_classes/trainer#transformers.Trainer.train")},m(c,y){m(c,s,y),e(s,h),e(s,r),e(r,u),e(s,$),m(c,_,y),q(v,c,y),m(c,E,y),q(j,c,y),m(c,T,y),m(c,P,y),e(P,W),m(c,B,y),m(c,N,y),e(N,I),e(I,R),e(I,K),e(K,et),e(I,ut),e(N,at),e(N,M),e(M,Q),e(M,nt),e(nt,st),e(M,X),e(N,dt),e(N,F),e(F,L),e(F,V),e(V,O),e(F,gt),m(c,H,y),q(J,c,y),m(c,ct,y),q(U,c,y),mt=!0},p(c,y){const G={};y&2&&(G.$$scope={dirty:y,ctx:c}),j.$set(G);const Z={};y&2&&(Z.$$scope={dirty:y,ctx:c}),U.$set(Z)},i(c){mt||(A(v.$$.fragment,c),A(j.$$.fragment,c),A(J.$$.fragment,c),A(U.$$.fragment,c),mt=!0)},o(c){C(v.$$.fragment,c),C(j.$$.fragment,c),C(J.$$.fragment,c),C(U.$$.fragment,c),mt=!1},d(c){c&&a(s),c&&a(_),D(v,c),c&&a(E),D(j,c),c&&a(T),c&&a(P),c&&a(B),c&&a(N),c&&a(H),D(J,c),c&&a(ct),D(U,c)}}}function Xa(S){let s,h;return s=new De({props:{$$slots:{default:[Qa]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function Za(S){let s,h,r,u,$;return{c(){s=i("p"),h=o("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=i("a"),u=o("here"),$=o("!"),this.h()},l(_){s=p(_,"P",{});var v=f(s);h=n(v,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=p(v,"A",{href:!0});var E=f(r);u=n(E,"here"),E.forEach(a),$=n(v,"!"),v.forEach(a),this.h()},h(){w(r,"href","training#finetune-with-keras")},m(_,v){m(_,s,v),e(s,h),e(s,r),e(r,u),e(s,$)},d(_){_&&a(s)}}}function ts(S){let s,h,r,u,$,_,v,E,j,T,P,W,B,N,I,R,K,et,ut,at,M,Q,nt,st,X,dt,F,L,V,O,gt,H,J,ct,U,mt,c,y,G,Z,Y,rt,lt,ht,it,tt,_t;return B=new ft({props:{code:`tf_train_set = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_imdb["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),I=new ce({props:{$$slots:{default:[Za]},$$scope:{ctx:S}}}),at=new ft({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),L=new ft({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),c=new ft({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),tt=new ft({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),h=o("To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=i("code"),u=o("tf.data.Dataset"),$=o(" format with "),_=i("a"),v=o("to_tf_dataset"),E=o(". Specify inputs and labels in "),j=i("code"),T=o("columns"),P=o(", whether to shuffle the dataset order, batch size, and the data collator:"),W=g(),x(B.$$.fragment),N=g(),x(I.$$.fragment),R=g(),K=i("p"),et=o("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ut=g(),x(at.$$.fragment),M=g(),Q=i("p"),nt=o("Load DistilBERT with "),st=i("a"),X=o("TFAutoModelForSequenceClassification"),dt=o(" along with the number of expected labels:"),F=g(),x(L.$$.fragment),V=g(),O=i("p"),gt=o("Configure the model for training with "),H=i("a"),J=i("code"),ct=o("compile"),U=o(":"),mt=g(),x(c.$$.fragment),y=g(),G=i("p"),Z=o("Call "),Y=i("a"),rt=i("code"),lt=o("fit"),ht=o(" to fine-tune the model:"),it=g(),x(tt.$$.fragment),this.h()},l(l){s=p(l,"P",{});var k=f(s);h=n(k,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=p(k,"CODE",{});var bt=f(r);u=n(bt,"tf.data.Dataset"),bt.forEach(a),$=n(k," format with "),_=p(k,"A",{href:!0,rel:!0});var Ut=f(_);v=n(Ut,"to_tf_dataset"),Ut.forEach(a),E=n(k,". Specify inputs and labels in "),j=p(k,"CODE",{});var Gt=f(j);T=n(Gt,"columns"),Gt.forEach(a),P=n(k,", whether to shuffle the dataset order, batch size, and the data collator:"),k.forEach(a),W=b(l),z(B.$$.fragment,l),N=b(l),z(I.$$.fragment,l),R=b(l),K=p(l,"P",{});var Ht=f(K);et=n(Ht,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Ht.forEach(a),ut=b(l),z(at.$$.fragment,l),M=b(l),Q=p(l,"P",{});var ot=f(Q);nt=n(ot,"Load DistilBERT with "),st=p(ot,"A",{href:!0});var jt=f(st);X=n(jt,"TFAutoModelForSequenceClassification"),jt.forEach(a),dt=n(ot," along with the number of expected labels:"),ot.forEach(a),F=b(l),z(L.$$.fragment,l),V=b(l),O=p(l,"P",{});var yt=f(O);gt=n(yt,"Configure the model for training with "),H=p(yt,"A",{href:!0,rel:!0});var Yt=f(H);J=p(Yt,"CODE",{});var Et=f(J);ct=n(Et,"compile"),Et.forEach(a),Yt.forEach(a),U=n(yt,":"),yt.forEach(a),mt=b(l),z(c.$$.fragment,l),y=b(l),G=p(l,"P",{});var Tt=f(G);Z=n(Tt,"Call "),Y=p(Tt,"A",{href:!0,rel:!0});var Kt=f(Y);rt=p(Kt,"CODE",{});var xt=f(rt);lt=n(xt,"fit"),xt.forEach(a),Kt.forEach(a),ht=n(Tt," to fine-tune the model:"),Tt.forEach(a),it=b(l),z(tt.$$.fragment,l),this.h()},h(){w(_,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),w(_,"rel","nofollow"),w(st,"href","/docs/transformers/pr_18056/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),w(H,"href","https://keras.io/api/models/model_training_apis/#compile-method"),w(H,"rel","nofollow"),w(Y,"href","https://keras.io/api/models/model_training_apis/#fit-method"),w(Y,"rel","nofollow")},m(l,k){m(l,s,k),e(s,h),e(s,r),e(r,u),e(s,$),e(s,_),e(_,v),e(s,E),e(s,j),e(j,T),e(s,P),m(l,W,k),q(B,l,k),m(l,N,k),q(I,l,k),m(l,R,k),m(l,K,k),e(K,et),m(l,ut,k),q(at,l,k),m(l,M,k),m(l,Q,k),e(Q,nt),e(Q,st),e(st,X),e(Q,dt),m(l,F,k),q(L,l,k),m(l,V,k),m(l,O,k),e(O,gt),e(O,H),e(H,J),e(J,ct),e(O,U),m(l,mt,k),q(c,l,k),m(l,y,k),m(l,G,k),e(G,Z),e(G,Y),e(Y,rt),e(rt,lt),e(G,ht),m(l,it,k),q(tt,l,k),_t=!0},p(l,k){const bt={};k&2&&(bt.$$scope={dirty:k,ctx:l}),I.$set(bt)},i(l){_t||(A(B.$$.fragment,l),A(I.$$.fragment,l),A(at.$$.fragment,l),A(L.$$.fragment,l),A(c.$$.fragment,l),A(tt.$$.fragment,l),_t=!0)},o(l){C(B.$$.fragment,l),C(I.$$.fragment,l),C(at.$$.fragment,l),C(L.$$.fragment,l),C(c.$$.fragment,l),C(tt.$$.fragment,l),_t=!1},d(l){l&&a(s),l&&a(W),D(B,l),l&&a(N),D(I,l),l&&a(R),l&&a(K),l&&a(ut),D(at,l),l&&a(M),l&&a(Q),l&&a(F),D(L,l),l&&a(V),l&&a(O),l&&a(mt),D(c,l),l&&a(y),l&&a(G),l&&a(it),D(tt,l)}}}function es(S){let s,h;return s=new De({props:{$$slots:{default:[ts]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){q(s,r,u),h=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){h||(A(s.$$.fragment,r),h=!0)},o(r){C(s.$$.fragment,r),h=!1},d(r){D(s,r)}}}function as(S){let s,h,r,u,$,_,v,E;return{c(){s=i("p"),h=o(`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),r=i("a"),u=o("PyTorch notebook"),$=o(`
or `),_=i("a"),v=o("TensorFlow notebook"),E=o("."),this.h()},l(j){s=p(j,"P",{});var T=f(s);h=n(T,`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),r=p(T,"A",{href:!0,rel:!0});var P=f(r);u=n(P,"PyTorch notebook"),P.forEach(a),$=n(T,`
or `),_=p(T,"A",{href:!0,rel:!0});var W=f(_);v=n(W,"TensorFlow notebook"),W.forEach(a),E=n(T,"."),T.forEach(a),this.h()},h(){w(r,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),w(r,"rel","nofollow"),w(_,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),w(_,"rel","nofollow")},m(j,T){m(j,s,T),e(s,h),e(s,r),e(r,u),e(s,$),e(s,_),e(_,v),e(s,E)},d(j){j&&a(s)}}}function ss(S){let s,h,r,u,$,_,v,E,j,T,P,W,B,N,I,R,K,et,ut,at,M,Q,nt,st,X,dt,F,L,V,O,gt,H,J,ct,U,mt,c,y,G,Z,Y,rt,lt,ht,it,tt,_t,l,k,bt,Ut,Gt,Ht,ot,jt,yt,Yt,Et,Tt,Kt,xt,Pe,Se,me,vt,zt,Qt,It,Fe,Xt,Ie,he,qt,Me,Zt,Le,Oe,ue,Mt,de,At,We,te,Be,Ne,_e,Lt,$e,$t,Re,Ot,Ue,Ge,ee,He,Ye,ae,Ke,Ve,ge,Wt,be,pt,Je,Vt,Qe,Xe,se,Ze,ta,re,ea,aa,oe,sa,ra,we,Ct,ve,kt,Dt,ne,Bt,oa,le,na,ke,Pt,je,St,ye;return _=new Ce({}),P=new Ra({props:{id:"leNG9fN9FQU"}}),X=new ce({props:{$$slots:{default:[Ua]},$$scope:{ctx:S}}}),O=new Ce({}),y=new ft({props:{code:`from datasets import load_dataset

imdb = load_dataset("imdb")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`}}),lt=new ft({props:{code:'imdb["test"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clich\xE9d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`}}),It=new Ce({}),Mt=new ft({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Lt=new ft({props:{code:`def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),Wt=new ft({props:{code:"tokenized_imdb = imdb.map(preprocess_function, batched=True)",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),Ct=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ka],pytorch:[Ha]},$$scope:{ctx:S}}}),Bt=new Ce({}),Pt=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[es],pytorch:[Xa]},$$scope:{ctx:S}}}),St=new ce({props:{$$slots:{default:[as]},$$scope:{ctx:S}}}),{c(){s=i("meta"),h=g(),r=i("h1"),u=i("a"),$=i("span"),x(_.$$.fragment),v=g(),E=i("span"),j=o("Text classification"),T=g(),x(P.$$.fragment),W=g(),B=i("p"),N=o("Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),I=g(),R=i("p"),K=o("This guide will show you how to fine-tune "),et=i("a"),ut=o("DistilBERT"),at=o(" on the "),M=i("a"),Q=o("IMDb"),nt=o(" dataset to determine whether a movie review is positive or negative."),st=g(),x(X.$$.fragment),dt=g(),F=i("h2"),L=i("a"),V=i("span"),x(O.$$.fragment),gt=g(),H=i("span"),J=o("Load IMDb dataset"),ct=g(),U=i("p"),mt=o("Load the IMDb dataset from the \u{1F917} Datasets library:"),c=g(),x(y.$$.fragment),G=g(),Z=i("p"),Y=o("Then take a look at an example:"),rt=g(),x(lt.$$.fragment),ht=g(),it=i("p"),tt=o("There are two fields in this dataset:"),_t=g(),l=i("ul"),k=i("li"),bt=i("code"),Ut=o("text"),Gt=o(": a string containing the text of the movie review."),Ht=g(),ot=i("li"),jt=i("code"),yt=o("label"),Yt=o(": a value that can either be "),Et=i("code"),Tt=o("0"),Kt=o(" for a negative review or "),xt=i("code"),Pe=o("1"),Se=o(" for a positive review."),me=g(),vt=i("h2"),zt=i("a"),Qt=i("span"),x(It.$$.fragment),Fe=g(),Xt=i("span"),Ie=o("Preprocess"),he=g(),qt=i("p"),Me=o("Load the DistilBERT tokenizer to process the "),Zt=i("code"),Le=o("text"),Oe=o(" field:"),ue=g(),x(Mt.$$.fragment),de=g(),At=i("p"),We=o("Create a preprocessing function to tokenize "),te=i("code"),Be=o("text"),Ne=o(" and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),_e=g(),x(Lt.$$.fragment),$e=g(),$t=i("p"),Re=o("Use \u{1F917} Datasets "),Ot=i("a"),Ue=o("map"),Ge=o(" function to apply the preprocessing function over the entire dataset. You can speed up the "),ee=i("code"),He=o("map"),Ye=o(" function by setting "),ae=i("code"),Ke=o("batched=True"),Ve=o(" to process multiple elements of the dataset at once:"),ge=g(),x(Wt.$$.fragment),be=g(),pt=i("p"),Je=o("Use "),Vt=i("a"),Qe=o("DataCollatorWithPadding"),Xe=o(" to create a batch of examples. It will also "),se=i("em"),Ze=o("dynamically pad"),ta=o(" your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),re=i("code"),ea=o("tokenizer"),aa=o(" function by setting "),oe=i("code"),sa=o("padding=True"),ra=o(", dynamic padding is more efficient."),we=g(),x(Ct.$$.fragment),ve=g(),kt=i("h2"),Dt=i("a"),ne=i("span"),x(Bt.$$.fragment),oa=g(),le=i("span"),na=o("Train"),ke=g(),x(Pt.$$.fragment),je=g(),x(St.$$.fragment),this.h()},l(t){const d=Ba('[data-svelte="svelte-1phssyn"]',document.head);s=p(d,"META",{name:!0,content:!0}),d.forEach(a),h=b(t),r=p(t,"H1",{class:!0});var Nt=f(r);u=p(Nt,"A",{id:!0,class:!0,href:!0});var ie=f(u);$=p(ie,"SPAN",{});var pe=f($);z(_.$$.fragment,pe),pe.forEach(a),ie.forEach(a),v=b(Nt),E=p(Nt,"SPAN",{});var fe=f(E);j=n(fe,"Text classification"),fe.forEach(a),Nt.forEach(a),T=b(t),z(P.$$.fragment,t),W=b(t),B=p(t,"P",{});var ia=f(B);N=n(ia,"Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),ia.forEach(a),I=b(t),R=p(t,"P",{});var Jt=f(R);K=n(Jt,"This guide will show you how to fine-tune "),et=p(Jt,"A",{href:!0,rel:!0});var pa=f(et);ut=n(pa,"DistilBERT"),pa.forEach(a),at=n(Jt," on the "),M=p(Jt,"A",{href:!0,rel:!0});var fa=f(M);Q=n(fa,"IMDb"),fa.forEach(a),nt=n(Jt," dataset to determine whether a movie review is positive or negative."),Jt.forEach(a),st=b(t),z(X.$$.fragment,t),dt=b(t),F=p(t,"H2",{class:!0});var Ee=f(F);L=p(Ee,"A",{id:!0,class:!0,href:!0});var ca=f(L);V=p(ca,"SPAN",{});var ma=f(V);z(O.$$.fragment,ma),ma.forEach(a),ca.forEach(a),gt=b(Ee),H=p(Ee,"SPAN",{});var ha=f(H);J=n(ha,"Load IMDb dataset"),ha.forEach(a),Ee.forEach(a),ct=b(t),U=p(t,"P",{});var ua=f(U);mt=n(ua,"Load the IMDb dataset from the \u{1F917} Datasets library:"),ua.forEach(a),c=b(t),z(y.$$.fragment,t),G=b(t),Z=p(t,"P",{});var da=f(Z);Y=n(da,"Then take a look at an example:"),da.forEach(a),rt=b(t),z(lt.$$.fragment,t),ht=b(t),it=p(t,"P",{});var _a=f(it);tt=n(_a,"There are two fields in this dataset:"),_a.forEach(a),_t=b(t),l=p(t,"UL",{});var Te=f(l);k=p(Te,"LI",{});var la=f(k);bt=p(la,"CODE",{});var $a=f(bt);Ut=n($a,"text"),$a.forEach(a),Gt=n(la,": a string containing the text of the movie review."),la.forEach(a),Ht=b(Te),ot=p(Te,"LI",{});var Rt=f(ot);jt=p(Rt,"CODE",{});var ga=f(jt);yt=n(ga,"label"),ga.forEach(a),Yt=n(Rt,": a value that can either be "),Et=p(Rt,"CODE",{});var ba=f(Et);Tt=n(ba,"0"),ba.forEach(a),Kt=n(Rt," for a negative review or "),xt=p(Rt,"CODE",{});var wa=f(xt);Pe=n(wa,"1"),wa.forEach(a),Se=n(Rt," for a positive review."),Rt.forEach(a),Te.forEach(a),me=b(t),vt=p(t,"H2",{class:!0});var xe=f(vt);zt=p(xe,"A",{id:!0,class:!0,href:!0});var va=f(zt);Qt=p(va,"SPAN",{});var ka=f(Qt);z(It.$$.fragment,ka),ka.forEach(a),va.forEach(a),Fe=b(xe),Xt=p(xe,"SPAN",{});var ja=f(Xt);Ie=n(ja,"Preprocess"),ja.forEach(a),xe.forEach(a),he=b(t),qt=p(t,"P",{});var ze=f(qt);Me=n(ze,"Load the DistilBERT tokenizer to process the "),Zt=p(ze,"CODE",{});var ya=f(Zt);Le=n(ya,"text"),ya.forEach(a),Oe=n(ze," field:"),ze.forEach(a),ue=b(t),z(Mt.$$.fragment,t),de=b(t),At=p(t,"P",{});var qe=f(At);We=n(qe,"Create a preprocessing function to tokenize "),te=p(qe,"CODE",{});var Ea=f(te);Be=n(Ea,"text"),Ea.forEach(a),Ne=n(qe," and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),qe.forEach(a),_e=b(t),z(Lt.$$.fragment,t),$e=b(t),$t=p(t,"P",{});var Ft=f($t);Re=n(Ft,"Use \u{1F917} Datasets "),Ot=p(Ft,"A",{href:!0,rel:!0});var Ta=f(Ot);Ue=n(Ta,"map"),Ta.forEach(a),Ge=n(Ft," function to apply the preprocessing function over the entire dataset. You can speed up the "),ee=p(Ft,"CODE",{});var xa=f(ee);He=n(xa,"map"),xa.forEach(a),Ye=n(Ft," function by setting "),ae=p(Ft,"CODE",{});var za=f(ae);Ke=n(za,"batched=True"),za.forEach(a),Ve=n(Ft," to process multiple elements of the dataset at once:"),Ft.forEach(a),ge=b(t),z(Wt.$$.fragment,t),be=b(t),pt=p(t,"P",{});var wt=f(pt);Je=n(wt,"Use "),Vt=p(wt,"A",{href:!0});var qa=f(Vt);Qe=n(qa,"DataCollatorWithPadding"),qa.forEach(a),Xe=n(wt," to create a batch of examples. It will also "),se=p(wt,"EM",{});var Aa=f(se);Ze=n(Aa,"dynamically pad"),Aa.forEach(a),ta=n(wt," your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),re=p(wt,"CODE",{});var Ca=f(re);ea=n(Ca,"tokenizer"),Ca.forEach(a),aa=n(wt," function by setting "),oe=p(wt,"CODE",{});var Da=f(oe);sa=n(Da,"padding=True"),Da.forEach(a),ra=n(wt,", dynamic padding is more efficient."),wt.forEach(a),we=b(t),z(Ct.$$.fragment,t),ve=b(t),kt=p(t,"H2",{class:!0});var Ae=f(kt);Dt=p(Ae,"A",{id:!0,class:!0,href:!0});var Pa=f(Dt);ne=p(Pa,"SPAN",{});var Sa=f(ne);z(Bt.$$.fragment,Sa),Sa.forEach(a),Pa.forEach(a),oa=b(Ae),le=p(Ae,"SPAN",{});var Fa=f(le);na=n(Fa,"Train"),Fa.forEach(a),Ae.forEach(a),ke=b(t),z(Pt.$$.fragment,t),je=b(t),z(St.$$.fragment,t),this.h()},h(){w(s,"name","hf:doc:metadata"),w(s,"content",JSON.stringify(rs)),w(u,"id","text-classification"),w(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(u,"href","#text-classification"),w(r,"class","relative group"),w(et,"href","https://huggingface.co/distilbert-base-uncased"),w(et,"rel","nofollow"),w(M,"href","https://huggingface.co/datasets/imdb"),w(M,"rel","nofollow"),w(L,"id","load-imdb-dataset"),w(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(L,"href","#load-imdb-dataset"),w(F,"class","relative group"),w(zt,"id","preprocess"),w(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(zt,"href","#preprocess"),w(vt,"class","relative group"),w(Ot,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),w(Ot,"rel","nofollow"),w(Vt,"href","/docs/transformers/pr_18056/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),w(Dt,"id","train"),w(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Dt,"href","#train"),w(kt,"class","relative group")},m(t,d){e(document.head,s),m(t,h,d),m(t,r,d),e(r,u),e(u,$),q(_,$,null),e(r,v),e(r,E),e(E,j),m(t,T,d),q(P,t,d),m(t,W,d),m(t,B,d),e(B,N),m(t,I,d),m(t,R,d),e(R,K),e(R,et),e(et,ut),e(R,at),e(R,M),e(M,Q),e(R,nt),m(t,st,d),q(X,t,d),m(t,dt,d),m(t,F,d),e(F,L),e(L,V),q(O,V,null),e(F,gt),e(F,H),e(H,J),m(t,ct,d),m(t,U,d),e(U,mt),m(t,c,d),q(y,t,d),m(t,G,d),m(t,Z,d),e(Z,Y),m(t,rt,d),q(lt,t,d),m(t,ht,d),m(t,it,d),e(it,tt),m(t,_t,d),m(t,l,d),e(l,k),e(k,bt),e(bt,Ut),e(k,Gt),e(l,Ht),e(l,ot),e(ot,jt),e(jt,yt),e(ot,Yt),e(ot,Et),e(Et,Tt),e(ot,Kt),e(ot,xt),e(xt,Pe),e(ot,Se),m(t,me,d),m(t,vt,d),e(vt,zt),e(zt,Qt),q(It,Qt,null),e(vt,Fe),e(vt,Xt),e(Xt,Ie),m(t,he,d),m(t,qt,d),e(qt,Me),e(qt,Zt),e(Zt,Le),e(qt,Oe),m(t,ue,d),q(Mt,t,d),m(t,de,d),m(t,At,d),e(At,We),e(At,te),e(te,Be),e(At,Ne),m(t,_e,d),q(Lt,t,d),m(t,$e,d),m(t,$t,d),e($t,Re),e($t,Ot),e(Ot,Ue),e($t,Ge),e($t,ee),e(ee,He),e($t,Ye),e($t,ae),e(ae,Ke),e($t,Ve),m(t,ge,d),q(Wt,t,d),m(t,be,d),m(t,pt,d),e(pt,Je),e(pt,Vt),e(Vt,Qe),e(pt,Xe),e(pt,se),e(se,Ze),e(pt,ta),e(pt,re),e(re,ea),e(pt,aa),e(pt,oe),e(oe,sa),e(pt,ra),m(t,we,d),q(Ct,t,d),m(t,ve,d),m(t,kt,d),e(kt,Dt),e(Dt,ne),q(Bt,ne,null),e(kt,oa),e(kt,le),e(le,na),m(t,ke,d),q(Pt,t,d),m(t,je,d),q(St,t,d),ye=!0},p(t,[d]){const Nt={};d&2&&(Nt.$$scope={dirty:d,ctx:t}),X.$set(Nt);const ie={};d&2&&(ie.$$scope={dirty:d,ctx:t}),Ct.$set(ie);const pe={};d&2&&(pe.$$scope={dirty:d,ctx:t}),Pt.$set(pe);const fe={};d&2&&(fe.$$scope={dirty:d,ctx:t}),St.$set(fe)},i(t){ye||(A(_.$$.fragment,t),A(P.$$.fragment,t),A(X.$$.fragment,t),A(O.$$.fragment,t),A(y.$$.fragment,t),A(lt.$$.fragment,t),A(It.$$.fragment,t),A(Mt.$$.fragment,t),A(Lt.$$.fragment,t),A(Wt.$$.fragment,t),A(Ct.$$.fragment,t),A(Bt.$$.fragment,t),A(Pt.$$.fragment,t),A(St.$$.fragment,t),ye=!0)},o(t){C(_.$$.fragment,t),C(P.$$.fragment,t),C(X.$$.fragment,t),C(O.$$.fragment,t),C(y.$$.fragment,t),C(lt.$$.fragment,t),C(It.$$.fragment,t),C(Mt.$$.fragment,t),C(Lt.$$.fragment,t),C(Wt.$$.fragment,t),C(Ct.$$.fragment,t),C(Bt.$$.fragment,t),C(Pt.$$.fragment,t),C(St.$$.fragment,t),ye=!1},d(t){a(s),t&&a(h),t&&a(r),D(_),t&&a(T),D(P,t),t&&a(W),t&&a(B),t&&a(I),t&&a(R),t&&a(st),D(X,t),t&&a(dt),t&&a(F),D(O),t&&a(ct),t&&a(U),t&&a(c),D(y,t),t&&a(G),t&&a(Z),t&&a(rt),D(lt,t),t&&a(ht),t&&a(it),t&&a(_t),t&&a(l),t&&a(me),t&&a(vt),D(It),t&&a(he),t&&a(qt),t&&a(ue),D(Mt,t),t&&a(de),t&&a(At),t&&a(_e),D(Lt,t),t&&a($e),t&&a($t),t&&a(ge),D(Wt,t),t&&a(be),t&&a(pt),t&&a(we),D(Ct,t),t&&a(ve),t&&a(kt),D(Bt),t&&a(ke),D(Pt,t),t&&a(je),D(St,t)}}}const rs={local:"text-classification",sections:[{local:"load-imdb-dataset",title:"Load IMDb dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Text classification"};function os(S){return Na(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ms extends La{constructor(s){super();Oa(this,s,os,ss,Wa,{})}}export{ms as default,rs as metadata};
