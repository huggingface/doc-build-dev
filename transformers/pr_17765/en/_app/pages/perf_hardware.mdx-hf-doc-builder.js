import{S as zn,i as Fn,s as Qn,e as n,k as h,w,t as l,M as Zn,c as s,d as t,m as c,a as i,x as P,h as p,b as f,G as o,g as r,y,L as Jn,q as _,o as k,B as b,v as Kn}from"../chunks/vendor-hf-doc-builder.js";import{I as st}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as it}from"../chunks/CodeBlock-hf-doc-builder.js";function es(Ba){let E,rt,g,$,Ae,B,po,Se,ho,lt,I,co,H,fo,uo,pt,se,mo,ht,U,A,Ve,O,vo,Le,wo,ct,ie,Po,ft,N,S,xe,X,yo,De,_o,dt,re,ko,ut,M,je,bo,Eo,mt,le,go,vt,pe,Uo,wt,he,No,Pt,ce,Co,yt,fe,Go,_t,de,$o,kt,W,Te,Io,Ao,bt,ue,So,Et,me,Vo,gt,ve,Lo,Ut,C,V,Be,R,xo,He,Do,Nt,we,jo,Ct,q,Gt,Pe,To,$t,Y,It,ye,Bo,At,z,St,_e,Ho,Vt,F,Lt,v,Oo,Oe,Xo,Mo,Xe,Wo,Ro,xt,ke,qo,Dt,be,Yo,jt,G,L,Me,Q,zo,We,Fo,Tt,Z,J,Qo,Zo,Bt,x,Jo,K,Ko,ea,Ht,Ee,Re,ta,Ot,u,oa,qe,aa,na,Ye,sa,ia,ze,ra,la,Xt,ge,pa,Mt,Ue,ha,Wt,D,Fe,ee,Qe,ca,fa,Ne,da,ua,te,oe,Ze,ma,va,Ce,wa,Pa,ae,Je,ya,_a,Ge,ka,Rt,j,ba,Ke,Ea,ga,qt,$e,Ua,Yt,ne,zt,d,Na,et,Ca,Ga,tt,$a,Ia,ot,Aa,Sa,at,Va,La,nt,xa,Ft;return B=new st({}),O=new st({}),X=new st({}),R=new st({}),q=new it({props:{code:"nvidia-smi topo -m",highlighted:'<span class="hljs-symbol">nvidia</span>-<span class="hljs-keyword">smi</span> topo -m'}}),Y=new it({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      NV2     0-23            N/A
GPU1    NV2      X      0-23            N/A`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      NV2     <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A
<span class="hljs-attribute">GPU1</span>    NV2      X      <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A`}}),z=new it({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      PHB     0-11            N/A
GPU1    PHB      X      0-11            N/A`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      PHB     <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A
<span class="hljs-attribute">GPU1</span>    PHB      X      <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A`}}),F=new it({props:{code:`  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks`,highlighted:`  X    = Self
  SYS  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> interconnect between PCIe Host Bridges <span class="hljs-keyword">within</span> <span class="hljs-keyword">a</span> NUMA node
  PHB  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">a</span> PCIe Host Bridge (typically <span class="hljs-keyword">the</span> CPU)
  PXB  = Connection traversing multiple PCIe bridges (<span class="hljs-keyword">without</span> traversing <span class="hljs-keyword">the</span> PCIe Host Bridge)
  PIX  = Connection traversing <span class="hljs-keyword">at</span> most <span class="hljs-keyword">a</span> single PCIe bridge
  NV<span class="hljs-comment">#  = Connection traversing a bonded set of # NVLinks</span>`}}),Q=new st({}),ne=new it({props:{code:`



`,highlighted:`<span class="hljs-comment"># DDP w/ NVLink</span>

<span class="hljs-built_in">rm</span> -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: 101.9003, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: 1.963, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.69}

<span class="hljs-comment"># DDP w/o NVLink</span>

<span class="hljs-built_in">rm</span> -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: 131.4367, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: 1.522, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.69}`}}),{c(){E=n("meta"),rt=h(),g=n("h1"),$=n("a"),Ae=n("span"),w(B.$$.fragment),po=h(),Se=n("span"),ho=l("Custom hardware for training"),lt=h(),I=n("p"),co=l("The hardware you use to run model training and inference can have a big effect on performance. For a deep dive into GPUs make sure to check out Tim Dettmer\u2019s excellent "),H=n("a"),fo=l("blog post"),uo=l("."),pt=h(),se=n("p"),mo=l("Let\u2019s have a look at some practical advice for GPU setups."),ht=h(),U=n("h2"),A=n("a"),Ve=n("span"),w(O.$$.fragment),vo=h(),Le=n("span"),wo=l("GPU"),ct=l(`

When you train bigger models you have essentially three options:
- bigger GPUs
- more GPUs
- more CPU and NVMe (offloaded to by [DeepSpeed-Infinity](main_classes/deepspeed#nvme-support))
`),ie=n("p"),Po=l("Let\u2019s start at the case where you have a single GPU."),ft=h(),N=n("h3"),S=n("a"),xe=n("span"),w(X.$$.fragment),yo=h(),De=n("span"),_o=l("Power and Cooling"),dt=h(),re=n("p"),ko=l("If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),ut=h(),M=n("p"),je=n("strong"),bo=l("Power"),Eo=l(":"),mt=h(),le=n("p"),go=l("Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),vt=h(),pe=n("p"),Uo=l("Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),wt=h(),he=n("p"),No=l("Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),Pt=h(),ce=n("p"),Co=l("Low end cards may use 6-Pin connectors, which supply up to 75W of power."),yt=h(),fe=n("p"),Go=l("Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),_t=h(),de=n("p"),$o=l("And of course the PSU needs to have enough unused Watts to power the card."),kt=h(),W=n("p"),Te=n("strong"),Io=l("Cooling"),Ao=l(":"),bt=h(),ue=n("p"),So=l("When a GPU gets overheated it will start throttling down and will not deliver full performance and it can even shutdown if it gets too hot."),Et=h(),me=n("p"),Vo=l("It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very high temperature is likely to reduce the lifespan of a GPU."),gt=h(),ve=n("p"),Lo=l("Next let\u2019s have a look at one of the most important aspects when having multiple GPUs: connectivity."),Ut=h(),C=n("h3"),V=n("a"),Be=n("span"),w(R.$$.fragment),xo=h(),He=n("span"),Do=l("Multi-GPU Connectivity"),Nt=h(),we=n("p"),jo=l("If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:"),Ct=h(),w(q.$$.fragment),Gt=h(),Pe=n("p"),To=l("and it will tell you how the GPUs are inter-connected. On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),$t=h(),w(Y.$$.fragment),It=h(),ye=n("p"),Bo=l("on a different machine w/o NVLink we may see:"),At=h(),w(z.$$.fragment),St=h(),_e=n("p"),Ho=l("The report includes this legend:"),Vt=h(),w(F.$$.fragment),Lt=h(),v=n("p"),Oo=l("So the first report "),Oe=n("code"),Xo=l("NV2"),Mo=l(" tells us the GPUs are interconnected with 2 NVLinks, and the second report "),Xe=n("code"),Wo=l("PHB"),Ro=l(" we have a typical consumer-level PCIe+Bridge setup."),xt=h(),ke=n("p"),qo=l("Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),Dt=h(),be=n("p"),Yo=l("Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),jt=h(),G=n("h4"),L=n("a"),Me=n("span"),w(Q.$$.fragment),zo=h(),We=n("span"),Fo=l("NVlink"),Tt=h(),Z=n("p"),J=n("a"),Qo=l("NVLink"),Zo=l(" is a wire-based serial multi-lane near-range communications link developed by Nvidia."),Bt=h(),x=n("p"),Jo=l("Each new generation provides a faster bandwidth, e.g. here is a quote from "),K=n("a"),Ko=l("Nvidia Ampere GA102 GPU Architecture"),ea=l(":"),Ht=h(),Ee=n("blockquote"),Re=n("p"),ta=l(`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),Ot=h(),u=n("p"),oa=l("So the higher "),qe=n("code"),aa=l("X"),na=l(" you get in the report of "),Ye=n("code"),sa=l("NVX"),ia=l(" in the output of "),ze=n("code"),ra=l("nvidia-smi topo -m"),la=l(" the better. The generation will depend on your GPU architecture."),Xt=h(),ge=n("p"),pa=l("Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),Mt=h(),Ue=n("p"),ha=l("The results are:"),Wt=h(),D=n("table"),Fe=n("thead"),ee=n("tr"),Qe=n("th"),ca=l("NVlink"),fa=h(),Ne=n("th"),da=l("Time"),ua=h(),te=n("tbody"),oe=n("tr"),Ze=n("td"),ma=l("Y"),va=h(),Ce=n("td"),wa=l("101s"),Pa=h(),ae=n("tr"),Je=n("td"),ya=l("N"),_a=h(),Ge=n("td"),ka=l("131s"),Rt=h(),j=n("p"),ba=l("You can see that NVLink completes the training ~23% faster. In the second benchmark we use "),Ke=n("code"),Ea=l("NCCL_P2P_DISABLE=1"),ga=l(" to tell the GPUs not to use NVLink."),qt=h(),$e=n("p"),Ua=l("Here is the full benchmark code and outputs:"),Yt=h(),w(ne.$$.fragment),zt=h(),d=n("p"),Na=l("Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),et=n("code"),Ca=l("NV2"),Ga=l(" in "),tt=n("code"),$a=l("nvidia-smi topo -m"),Ia=l(`)
Software: `),ot=n("code"),Aa=l("pytorch-1.8-to-be"),Sa=l(" + "),at=n("code"),Va=l("cuda-11.0"),La=l(" / "),nt=n("code"),xa=l("transformers==4.3.0.dev0"),this.h()},l(e){const a=Zn('[data-svelte="svelte-1phssyn"]',document.head);E=s(a,"META",{name:!0,content:!0}),a.forEach(t),rt=c(e),g=s(e,"H1",{class:!0});var Qt=i(g);$=s(Qt,"A",{id:!0,class:!0,href:!0});var Ha=i($);Ae=s(Ha,"SPAN",{});var Oa=i(Ae);P(B.$$.fragment,Oa),Oa.forEach(t),Ha.forEach(t),po=c(Qt),Se=s(Qt,"SPAN",{});var Xa=i(Se);ho=p(Xa,"Custom hardware for training"),Xa.forEach(t),Qt.forEach(t),lt=c(e),I=s(e,"P",{});var Zt=i(I);co=p(Zt,"The hardware you use to run model training and inference can have a big effect on performance. For a deep dive into GPUs make sure to check out Tim Dettmer\u2019s excellent "),H=s(Zt,"A",{href:!0,rel:!0});var Ma=i(H);fo=p(Ma,"blog post"),Ma.forEach(t),uo=p(Zt,"."),Zt.forEach(t),pt=c(e),se=s(e,"P",{});var Wa=i(se);mo=p(Wa,"Let\u2019s have a look at some practical advice for GPU setups."),Wa.forEach(t),ht=c(e),U=s(e,"H2",{class:!0});var Jt=i(U);A=s(Jt,"A",{id:!0,class:!0,href:!0});var Ra=i(A);Ve=s(Ra,"SPAN",{});var qa=i(Ve);P(O.$$.fragment,qa),qa.forEach(t),Ra.forEach(t),vo=c(Jt),Le=s(Jt,"SPAN",{});var Ya=i(Le);wo=p(Ya,"GPU"),Ya.forEach(t),Jt.forEach(t),ct=p(e,`

When you train bigger models you have essentially three options:
- bigger GPUs
- more GPUs
- more CPU and NVMe (offloaded to by [DeepSpeed-Infinity](main_classes/deepspeed#nvme-support))
`),ie=s(e,"P",{});var za=i(ie);Po=p(za,"Let\u2019s start at the case where you have a single GPU."),za.forEach(t),ft=c(e),N=s(e,"H3",{class:!0});var Kt=i(N);S=s(Kt,"A",{id:!0,class:!0,href:!0});var Fa=i(S);xe=s(Fa,"SPAN",{});var Qa=i(xe);P(X.$$.fragment,Qa),Qa.forEach(t),Fa.forEach(t),yo=c(Kt),De=s(Kt,"SPAN",{});var Za=i(De);_o=p(Za,"Power and Cooling"),Za.forEach(t),Kt.forEach(t),dt=c(e),re=s(e,"P",{});var Ja=i(re);ko=p(Ja,"If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),Ja.forEach(t),ut=c(e),M=s(e,"P",{});var Da=i(M);je=s(Da,"STRONG",{});var Ka=i(je);bo=p(Ka,"Power"),Ka.forEach(t),Eo=p(Da,":"),Da.forEach(t),mt=c(e),le=s(e,"P",{});var en=i(le);go=p(en,"Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),en.forEach(t),vt=c(e),pe=s(e,"P",{});var tn=i(pe);Uo=p(tn,"Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),tn.forEach(t),wt=c(e),he=s(e,"P",{});var on=i(he);No=p(on,"Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),on.forEach(t),Pt=c(e),ce=s(e,"P",{});var an=i(ce);Co=p(an,"Low end cards may use 6-Pin connectors, which supply up to 75W of power."),an.forEach(t),yt=c(e),fe=s(e,"P",{});var nn=i(fe);Go=p(nn,"Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),nn.forEach(t),_t=c(e),de=s(e,"P",{});var sn=i(de);$o=p(sn,"And of course the PSU needs to have enough unused Watts to power the card."),sn.forEach(t),kt=c(e),W=s(e,"P",{});var ja=i(W);Te=s(ja,"STRONG",{});var rn=i(Te);Io=p(rn,"Cooling"),rn.forEach(t),Ao=p(ja,":"),ja.forEach(t),bt=c(e),ue=s(e,"P",{});var ln=i(ue);So=p(ln,"When a GPU gets overheated it will start throttling down and will not deliver full performance and it can even shutdown if it gets too hot."),ln.forEach(t),Et=c(e),me=s(e,"P",{});var pn=i(me);Vo=p(pn,"It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very high temperature is likely to reduce the lifespan of a GPU."),pn.forEach(t),gt=c(e),ve=s(e,"P",{});var hn=i(ve);Lo=p(hn,"Next let\u2019s have a look at one of the most important aspects when having multiple GPUs: connectivity."),hn.forEach(t),Ut=c(e),C=s(e,"H3",{class:!0});var eo=i(C);V=s(eo,"A",{id:!0,class:!0,href:!0});var cn=i(V);Be=s(cn,"SPAN",{});var fn=i(Be);P(R.$$.fragment,fn),fn.forEach(t),cn.forEach(t),xo=c(eo),He=s(eo,"SPAN",{});var dn=i(He);Do=p(dn,"Multi-GPU Connectivity"),dn.forEach(t),eo.forEach(t),Nt=c(e),we=s(e,"P",{});var un=i(we);jo=p(un,"If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:"),un.forEach(t),Ct=c(e),P(q.$$.fragment,e),Gt=c(e),Pe=s(e,"P",{});var mn=i(Pe);To=p(mn,"and it will tell you how the GPUs are inter-connected. On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),mn.forEach(t),$t=c(e),P(Y.$$.fragment,e),It=c(e),ye=s(e,"P",{});var vn=i(ye);Bo=p(vn,"on a different machine w/o NVLink we may see:"),vn.forEach(t),At=c(e),P(z.$$.fragment,e),St=c(e),_e=s(e,"P",{});var wn=i(_e);Ho=p(wn,"The report includes this legend:"),wn.forEach(t),Vt=c(e),P(F.$$.fragment,e),Lt=c(e),v=s(e,"P",{});var Ie=i(v);Oo=p(Ie,"So the first report "),Oe=s(Ie,"CODE",{});var Pn=i(Oe);Xo=p(Pn,"NV2"),Pn.forEach(t),Mo=p(Ie," tells us the GPUs are interconnected with 2 NVLinks, and the second report "),Xe=s(Ie,"CODE",{});var yn=i(Xe);Wo=p(yn,"PHB"),yn.forEach(t),Ro=p(Ie," we have a typical consumer-level PCIe+Bridge setup."),Ie.forEach(t),xt=c(e),ke=s(e,"P",{});var _n=i(ke);qo=p(_n,"Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),_n.forEach(t),Dt=c(e),be=s(e,"P",{});var kn=i(be);Yo=p(kn,"Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),kn.forEach(t),jt=c(e),G=s(e,"H4",{class:!0});var to=i(G);L=s(to,"A",{id:!0,class:!0,href:!0});var bn=i(L);Me=s(bn,"SPAN",{});var En=i(Me);P(Q.$$.fragment,En),En.forEach(t),bn.forEach(t),zo=c(to),We=s(to,"SPAN",{});var gn=i(We);Fo=p(gn,"NVlink"),gn.forEach(t),to.forEach(t),Tt=c(e),Z=s(e,"P",{});var Ta=i(Z);J=s(Ta,"A",{href:!0,rel:!0});var Un=i(J);Qo=p(Un,"NVLink"),Un.forEach(t),Zo=p(Ta," is a wire-based serial multi-lane near-range communications link developed by Nvidia."),Ta.forEach(t),Bt=c(e),x=s(e,"P",{});var oo=i(x);Jo=p(oo,"Each new generation provides a faster bandwidth, e.g. here is a quote from "),K=s(oo,"A",{href:!0,rel:!0});var Nn=i(K);Ko=p(Nn,"Nvidia Ampere GA102 GPU Architecture"),Nn.forEach(t),ea=p(oo,":"),oo.forEach(t),Ht=c(e),Ee=s(e,"BLOCKQUOTE",{});var Cn=i(Ee);Re=s(Cn,"P",{});var Gn=i(Re);ta=p(Gn,`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),Gn.forEach(t),Cn.forEach(t),Ot=c(e),u=s(e,"P",{});var T=i(u);oa=p(T,"So the higher "),qe=s(T,"CODE",{});var $n=i(qe);aa=p($n,"X"),$n.forEach(t),na=p(T," you get in the report of "),Ye=s(T,"CODE",{});var In=i(Ye);sa=p(In,"NVX"),In.forEach(t),ia=p(T," in the output of "),ze=s(T,"CODE",{});var An=i(ze);ra=p(An,"nvidia-smi topo -m"),An.forEach(t),la=p(T," the better. The generation will depend on your GPU architecture."),T.forEach(t),Xt=c(e),ge=s(e,"P",{});var Sn=i(ge);pa=p(Sn,"Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),Sn.forEach(t),Mt=c(e),Ue=s(e,"P",{});var Vn=i(Ue);ha=p(Vn,"The results are:"),Vn.forEach(t),Wt=c(e),D=s(e,"TABLE",{});var ao=i(D);Fe=s(ao,"THEAD",{});var Ln=i(Fe);ee=s(Ln,"TR",{});var no=i(ee);Qe=s(no,"TH",{});var xn=i(Qe);ca=p(xn,"NVlink"),xn.forEach(t),fa=c(no),Ne=s(no,"TH",{align:!0});var Dn=i(Ne);da=p(Dn,"Time"),Dn.forEach(t),no.forEach(t),Ln.forEach(t),ua=c(ao),te=s(ao,"TBODY",{});var so=i(te);oe=s(so,"TR",{});var io=i(oe);Ze=s(io,"TD",{});var jn=i(Ze);ma=p(jn,"Y"),jn.forEach(t),va=c(io),Ce=s(io,"TD",{align:!0});var Tn=i(Ce);wa=p(Tn,"101s"),Tn.forEach(t),io.forEach(t),Pa=c(so),ae=s(so,"TR",{});var ro=i(ae);Je=s(ro,"TD",{});var Bn=i(Je);ya=p(Bn,"N"),Bn.forEach(t),_a=c(ro),Ge=s(ro,"TD",{align:!0});var Hn=i(Ge);ka=p(Hn,"131s"),Hn.forEach(t),ro.forEach(t),so.forEach(t),ao.forEach(t),Rt=c(e),j=s(e,"P",{});var lo=i(j);ba=p(lo,"You can see that NVLink completes the training ~23% faster. In the second benchmark we use "),Ke=s(lo,"CODE",{});var On=i(Ke);Ea=p(On,"NCCL_P2P_DISABLE=1"),On.forEach(t),ga=p(lo," to tell the GPUs not to use NVLink."),lo.forEach(t),qt=c(e),$e=s(e,"P",{});var Xn=i($e);Ua=p(Xn,"Here is the full benchmark code and outputs:"),Xn.forEach(t),Yt=c(e),P(ne.$$.fragment,e),zt=c(e),d=s(e,"P",{});var m=i(d);Na=p(m,"Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),et=s(m,"CODE",{});var Mn=i(et);Ca=p(Mn,"NV2"),Mn.forEach(t),Ga=p(m," in "),tt=s(m,"CODE",{});var Wn=i(tt);$a=p(Wn,"nvidia-smi topo -m"),Wn.forEach(t),Ia=p(m,`)
Software: `),ot=s(m,"CODE",{});var Rn=i(ot);Aa=p(Rn,"pytorch-1.8-to-be"),Rn.forEach(t),Sa=p(m," + "),at=s(m,"CODE",{});var qn=i(at);Va=p(qn,"cuda-11.0"),qn.forEach(t),La=p(m," / "),nt=s(m,"CODE",{});var Yn=i(nt);xa=p(Yn,"transformers==4.3.0.dev0"),Yn.forEach(t),m.forEach(t),this.h()},h(){f(E,"name","hf:doc:metadata"),f(E,"content",JSON.stringify(ts)),f($,"id","custom-hardware-for-training"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#custom-hardware-for-training"),f(g,"class","relative group"),f(H,"href","https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/"),f(H,"rel","nofollow"),f(A,"id","gpu"),f(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(A,"href","#gpu"),f(U,"class","relative group"),f(S,"id","power-and-cooling"),f(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(S,"href","#power-and-cooling"),f(N,"class","relative group"),f(V,"id","multigpu-connectivity"),f(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(V,"href","#multigpu-connectivity"),f(C,"class","relative group"),f(L,"id","nvlink"),f(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(L,"href","#nvlink"),f(G,"class","relative group"),f(J,"href","https://en.wikipedia.org/wiki/NVLink"),f(J,"rel","nofollow"),f(K,"href","https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf"),f(K,"rel","nofollow"),f(Ne,"align","right"),f(Ce,"align","right"),f(Ge,"align","right")},m(e,a){o(document.head,E),r(e,rt,a),r(e,g,a),o(g,$),o($,Ae),y(B,Ae,null),o(g,po),o(g,Se),o(Se,ho),r(e,lt,a),r(e,I,a),o(I,co),o(I,H),o(H,fo),o(I,uo),r(e,pt,a),r(e,se,a),o(se,mo),r(e,ht,a),r(e,U,a),o(U,A),o(A,Ve),y(O,Ve,null),o(U,vo),o(U,Le),o(Le,wo),r(e,ct,a),r(e,ie,a),o(ie,Po),r(e,ft,a),r(e,N,a),o(N,S),o(S,xe),y(X,xe,null),o(N,yo),o(N,De),o(De,_o),r(e,dt,a),r(e,re,a),o(re,ko),r(e,ut,a),r(e,M,a),o(M,je),o(je,bo),o(M,Eo),r(e,mt,a),r(e,le,a),o(le,go),r(e,vt,a),r(e,pe,a),o(pe,Uo),r(e,wt,a),r(e,he,a),o(he,No),r(e,Pt,a),r(e,ce,a),o(ce,Co),r(e,yt,a),r(e,fe,a),o(fe,Go),r(e,_t,a),r(e,de,a),o(de,$o),r(e,kt,a),r(e,W,a),o(W,Te),o(Te,Io),o(W,Ao),r(e,bt,a),r(e,ue,a),o(ue,So),r(e,Et,a),r(e,me,a),o(me,Vo),r(e,gt,a),r(e,ve,a),o(ve,Lo),r(e,Ut,a),r(e,C,a),o(C,V),o(V,Be),y(R,Be,null),o(C,xo),o(C,He),o(He,Do),r(e,Nt,a),r(e,we,a),o(we,jo),r(e,Ct,a),y(q,e,a),r(e,Gt,a),r(e,Pe,a),o(Pe,To),r(e,$t,a),y(Y,e,a),r(e,It,a),r(e,ye,a),o(ye,Bo),r(e,At,a),y(z,e,a),r(e,St,a),r(e,_e,a),o(_e,Ho),r(e,Vt,a),y(F,e,a),r(e,Lt,a),r(e,v,a),o(v,Oo),o(v,Oe),o(Oe,Xo),o(v,Mo),o(v,Xe),o(Xe,Wo),o(v,Ro),r(e,xt,a),r(e,ke,a),o(ke,qo),r(e,Dt,a),r(e,be,a),o(be,Yo),r(e,jt,a),r(e,G,a),o(G,L),o(L,Me),y(Q,Me,null),o(G,zo),o(G,We),o(We,Fo),r(e,Tt,a),r(e,Z,a),o(Z,J),o(J,Qo),o(Z,Zo),r(e,Bt,a),r(e,x,a),o(x,Jo),o(x,K),o(K,Ko),o(x,ea),r(e,Ht,a),r(e,Ee,a),o(Ee,Re),o(Re,ta),r(e,Ot,a),r(e,u,a),o(u,oa),o(u,qe),o(qe,aa),o(u,na),o(u,Ye),o(Ye,sa),o(u,ia),o(u,ze),o(ze,ra),o(u,la),r(e,Xt,a),r(e,ge,a),o(ge,pa),r(e,Mt,a),r(e,Ue,a),o(Ue,ha),r(e,Wt,a),r(e,D,a),o(D,Fe),o(Fe,ee),o(ee,Qe),o(Qe,ca),o(ee,fa),o(ee,Ne),o(Ne,da),o(D,ua),o(D,te),o(te,oe),o(oe,Ze),o(Ze,ma),o(oe,va),o(oe,Ce),o(Ce,wa),o(te,Pa),o(te,ae),o(ae,Je),o(Je,ya),o(ae,_a),o(ae,Ge),o(Ge,ka),r(e,Rt,a),r(e,j,a),o(j,ba),o(j,Ke),o(Ke,Ea),o(j,ga),r(e,qt,a),r(e,$e,a),o($e,Ua),r(e,Yt,a),y(ne,e,a),r(e,zt,a),r(e,d,a),o(d,Na),o(d,et),o(et,Ca),o(d,Ga),o(d,tt),o(tt,$a),o(d,Ia),o(d,ot),o(ot,Aa),o(d,Sa),o(d,at),o(at,Va),o(d,La),o(d,nt),o(nt,xa),Ft=!0},p:Jn,i(e){Ft||(_(B.$$.fragment,e),_(O.$$.fragment,e),_(X.$$.fragment,e),_(R.$$.fragment,e),_(q.$$.fragment,e),_(Y.$$.fragment,e),_(z.$$.fragment,e),_(F.$$.fragment,e),_(Q.$$.fragment,e),_(ne.$$.fragment,e),Ft=!0)},o(e){k(B.$$.fragment,e),k(O.$$.fragment,e),k(X.$$.fragment,e),k(R.$$.fragment,e),k(q.$$.fragment,e),k(Y.$$.fragment,e),k(z.$$.fragment,e),k(F.$$.fragment,e),k(Q.$$.fragment,e),k(ne.$$.fragment,e),Ft=!1},d(e){t(E),e&&t(rt),e&&t(g),b(B),e&&t(lt),e&&t(I),e&&t(pt),e&&t(se),e&&t(ht),e&&t(U),b(O),e&&t(ct),e&&t(ie),e&&t(ft),e&&t(N),b(X),e&&t(dt),e&&t(re),e&&t(ut),e&&t(M),e&&t(mt),e&&t(le),e&&t(vt),e&&t(pe),e&&t(wt),e&&t(he),e&&t(Pt),e&&t(ce),e&&t(yt),e&&t(fe),e&&t(_t),e&&t(de),e&&t(kt),e&&t(W),e&&t(bt),e&&t(ue),e&&t(Et),e&&t(me),e&&t(gt),e&&t(ve),e&&t(Ut),e&&t(C),b(R),e&&t(Nt),e&&t(we),e&&t(Ct),b(q,e),e&&t(Gt),e&&t(Pe),e&&t($t),b(Y,e),e&&t(It),e&&t(ye),e&&t(At),b(z,e),e&&t(St),e&&t(_e),e&&t(Vt),b(F,e),e&&t(Lt),e&&t(v),e&&t(xt),e&&t(ke),e&&t(Dt),e&&t(be),e&&t(jt),e&&t(G),b(Q),e&&t(Tt),e&&t(Z),e&&t(Bt),e&&t(x),e&&t(Ht),e&&t(Ee),e&&t(Ot),e&&t(u),e&&t(Xt),e&&t(ge),e&&t(Mt),e&&t(Ue),e&&t(Wt),e&&t(D),e&&t(Rt),e&&t(j),e&&t(qt),e&&t($e),e&&t(Yt),b(ne,e),e&&t(zt),e&&t(d)}}}const ts={local:"custom-hardware-for-training",sections:[{local:"gpu",sections:[{local:"power-and-cooling",title:"Power and Cooling"},{local:"multigpu-connectivity",sections:[{local:"nvlink",title:"NVlink"}],title:"Multi-GPU Connectivity"}],title:"GPU"}],title:"Custom hardware for training"};function os(Ba){return Kn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class is extends zn{constructor(E){super();Fn(this,E,os,es,Qn,{})}}export{is as default,ts as metadata};
