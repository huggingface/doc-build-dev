import{S as zc,i as Ec,s as Mc,e as r,k as f,w as T,t as i,M as qc,c as n,d as o,m as h,a,x as b,h as l,b as d,G as e,g as _,y as $,q as O,o as V,B as x,v as Cc,L as mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as no}from"../../chunks/Tip-hf-doc-builder.js";import{D as k}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Te}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as R}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ic(y){let c,v,g,p,w;return p=new Te({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),v=i("Example:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Example:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Fc(y){let c,v,g,p,w;return p=new Te({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),v=i("Example:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Example:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Lc(y){let c,v;return{c(){c=r("p"),v=i(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){c=n(g,"P",{});var p=a(c);v=l(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(g,p){_(g,c,p),e(c,v)},d(g){g&&o(c)}}}function Ac(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Dc(y){let c,v,g,p,w;return p=new Te({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Wc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Nc(y){let c,v,g,p,w;return p=new Te({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Sc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Bc(y){let c,v,g,p,w;return p=new Te({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Hc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Rc(y){let c,v,g,p,w;return p=new Te({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Uc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Gc(y){let c,v,g,p,w;return p=new Te({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Xc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Zc(y){let c,v,g,p,w;return p=new Te({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape # [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][i], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape # [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][i], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Jc(y){let c,v,g,p,w,s,m,j,Nr,Rs,Z,be,es,ft,Sr,ts,Br,Us,$e,Hr,ht,Rr,Ur,Gs,ao,Gr,Xs,io,os,Xr,Zs,J,Oe,ss,gt,Zr,rs,Jr,Js,Ve,Kr,lo,Yr,Qr,Ks,P,co,en,tn,po,on,sn,mo,rn,nn,fo,an,ln,ho,cn,dn,go,pn,mn,uo,fn,hn,Ys,ut,Qs,U,gn,_t,un,_n,wt,wn,vn,er,K,xe,ns,vt,Tn,as,bn,tr,q,Tt,$n,ye,_o,On,Vn,wo,xn,yn,jn,Y,kn,vo,Pn,zn,To,En,Mn,qn,je,bt,Cn,$t,In,bo,Fn,Ln,or,Q,ke,is,Ot,An,ls,Dn,sr,C,Vt,Wn,ee,Nn,$o,Sn,Bn,xt,Hn,Rn,Un,te,Gn,Oo,Xn,Zn,Vo,Jn,Kn,Yn,Pe,rr,oe,ze,cs,yt,Qn,ds,ea,nr,I,jt,ta,se,oa,xo,sa,ra,kt,na,aa,ia,re,la,yo,ca,da,jo,pa,ma,fa,Ee,ar,ne,Me,ps,Pt,ha,ms,ga,ir,F,zt,ua,fs,_a,wa,Et,va,ko,Ta,ba,$a,G,Mt,Oa,hs,Va,xa,qe,lr,ae,Ce,gs,qt,ya,us,ja,cr,z,Ct,ka,E,Pa,Po,za,Ea,zo,Ma,qa,Eo,Ca,Ia,_s,Fa,La,Mo,Aa,Da,Wa,Ie,It,Na,Ft,Sa,qo,Ba,Ha,Ra,Fe,Lt,Ua,At,Ga,Co,Xa,Za,Ja,Le,Dt,Ka,Wt,Ya,ws,Qa,ei,dr,ie,Ae,vs,Nt,ti,Ts,oi,pr,L,St,si,A,Bt,ri,le,ni,Io,ai,ii,bs,li,ci,di,De,pi,We,mi,D,Ht,fi,ce,hi,Fo,gi,ui,$s,_i,wi,vi,Ne,Ti,Se,bi,W,Rt,$i,de,Oi,Lo,Vi,xi,Os,yi,ji,ki,Be,Pi,He,mr,pe,Re,Vs,Ut,zi,xs,Ei,fr,me,Gt,Mi,N,Xt,qi,fe,Ci,Ao,Ii,Fi,ys,Li,Ai,Di,Ue,Wi,Ge,hr,he,Xe,js,Zt,Ni,ks,Si,gr,ge,Jt,Bi,S,Kt,Hi,ue,Ri,Do,Ui,Gi,Ps,Xi,Zi,Ji,Ze,Ki,Je,ur,_e,Ke,zs,Yt,Yi,Es,Qi,_r,we,Qt,el,B,eo,tl,ve,ol,Wo,sl,rl,Ms,nl,al,il,Ye,ll,Qe,wr;return s=new R({}),ft=new R({}),gt=new R({}),ut=new Te({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")

outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][i], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][i], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),vt=new R({}),Tt=new k({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.OwlViTConfig.vision_config_dict",description:`<strong>vision_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config_dict"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/configuration_owlvit.py#L245"}}),bt=new k({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/configuration_owlvit.py#L310",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),Ot=new R({}),Vt=new k({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel">OwlViTTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/configuration_owlvit.py#L34"}}),Pe=new pt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Ic]},$$scope:{ctx:y}}}),yt=new R({}),jt=new k({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/configuration_owlvit.py#L142"}}),Ee=new pt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[Fc]},$$scope:{ctx:y}}}),Pt=new R({}),zt=new k({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 768"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"crop_size",val:" = 768"},{name:"do_center_crop",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Resize the shorter edge of the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:"<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;",name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>. Desired output size when applying
center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/feature_extraction_owlvit.py#L43"}}),Mt=new k({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W) or (H, W, C),
where C is a number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_18257/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/feature_extraction_owlvit.py#L134",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_18257/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_18257/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),qe=new no({props:{warning:!0,$$slots:{default:[Lc]},$$scope:{ctx:y}}}),qt=new R({}),Ct=new k({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),It=new k({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/processing_owlvit.py#L149"}}),Lt=new k({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/processing_owlvit.py#L156"}}),Dt=new k({props:{name:"post_process",anchor:"transformers.OwlViTProcessor.post_process",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/processing_owlvit.py#L142"}}),Nt=new R({}),St=new k({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_18257/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L873"}}),Bt=new k({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_18257/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L1003",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output</strong> (Tuple<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new no({props:{$$slots:{default:[Ac]},$$scope:{ctx:y}}}),We=new pt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_18257/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L908",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new no({props:{$$slots:{default:[Wc]},$$scope:{ctx:y}}}),Se=new pt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[Nc]},$$scope:{ctx:y}}}),Rt=new k({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_18257/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L953",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new no({props:{$$slots:{default:[Sc]},$$scope:{ctx:y}}}),He=new pt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),Ut=new R({}),Gt=new k({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L712"}}),Xt=new k({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_18257/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L727",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_18257/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_18257/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new no({props:{$$slots:{default:[Hc]},$$scope:{ctx:y}}}),Ge=new pt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Zt=new R({}),Jt=new k({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L822"}}),Kt=new k({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_18257/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L835",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_18257/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_18257/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new no({props:{$$slots:{default:[Uc]},$$scope:{ctx:y}}}),Je=new pt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Gc]},$$scope:{ctx:y}}}),Yt=new R({}),Qt=new k({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L1154"}}),eo=new k({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": Tensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_18257/src/transformers/models/owlvit/modeling_owlvit.py#L1282",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</li>
<li><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) \u2014 A dictionary containing the individual losses. Useful for logging.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, num_queries)</code>) \u2014 Classification logits (including no-object) for all queries.</li>
<li><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, 4)</code>) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>post_process()</code> to retrieve the unnormalized
bounding boxes.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_size, patch_size, output_dim</code>) \u2014 Pooled output of <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.</li>
<li><strong>class_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.</li>
<li><strong>text_model_last_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_last_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches + 1, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)**2.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new no({props:{$$slots:{default:[Xc]},$$scope:{ctx:y}}}),Qe=new pt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Zc]},$$scope:{ctx:y}}}),{c(){c=r("meta"),v=f(),g=r("h1"),p=r("a"),w=r("span"),T(s.$$.fragment),m=f(),j=r("span"),Nr=i("OWL-ViT"),Rs=f(),Z=r("h2"),be=r("a"),es=r("span"),T(ft.$$.fragment),Sr=f(),ts=r("span"),Br=i("Overview"),Us=f(),$e=r("p"),Hr=i("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ht=r("a"),Rr=i("Simple Open-Vocabulary Object Detection with Vision Transformers"),Ur=i(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Gs=f(),ao=r("p"),Gr=i("The abstract from the paper is the following:"),Xs=f(),io=r("p"),os=r("em"),Xr=i("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Zs=f(),J=r("h2"),Oe=r("a"),ss=r("span"),T(gt.$$.fragment),Zr=f(),rs=r("span"),Jr=i("Usage"),Js=f(),Ve=r("p"),Kr=i("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),lo=r("a"),Yr=i("CLIP"),Qr=i(" as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Ks=f(),P=r("p"),co=r("a"),en=i("OwlViTFeatureExtractor"),tn=i(" can be used to resize (or rescale) and normalize images for the model and "),po=r("a"),on=i("CLIPTokenizer"),sn=i(" is used to encode the text. "),mo=r("a"),rn=i("OwlViTProcessor"),nn=i(" wraps "),fo=r("a"),an=i("OwlViTFeatureExtractor"),ln=i(" and "),ho=r("a"),cn=i("CLIPTokenizer"),dn=i(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),go=r("a"),pn=i("OwlViTProcessor"),mn=i(" and "),uo=r("a"),fn=i("OwlViTForObjectDetection"),hn=i("."),Ys=f(),T(ut.$$.fragment),Qs=f(),U=r("p"),gn=i("This model was contributed by "),_t=r("a"),un=i("adirik"),_n=i(". The original code can be found "),wt=r("a"),wn=i("here"),vn=i("."),er=f(),K=r("h2"),xe=r("a"),ns=r("span"),T(vt.$$.fragment),Tn=f(),as=r("span"),bn=i("OwlViTConfig"),tr=f(),q=r("div"),T(Tt.$$.fragment),$n=f(),ye=r("p"),_o=r("a"),On=i("OwlViTConfig"),Vn=i(" is the configuration class to store the configuration of an "),wo=r("a"),xn=i("OwlViTModel"),yn=i(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),jn=f(),Y=r("p"),kn=i("Configuration objects inherit from "),vo=r("a"),Pn=i("PretrainedConfig"),zn=i(` and can be used to control the model outputs. Read the
documentation from `),To=r("a"),En=i("PretrainedConfig"),Mn=i(" for more information."),qn=f(),je=r("div"),T(bt.$$.fragment),Cn=f(),$t=r("p"),In=i("Instantiate a "),bo=r("a"),Fn=i("OwlViTConfig"),Ln=i(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),or=f(),Q=r("h2"),ke=r("a"),is=r("span"),T(Ot.$$.fragment),An=f(),ls=r("span"),Dn=i("OwlViTTextConfig"),sr=f(),C=r("div"),T(Vt.$$.fragment),Wn=f(),ee=r("p"),Nn=i("This is the configuration class to store the configuration of an "),$o=r("a"),Sn=i("OwlViTTextModel"),Bn=i(`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),xt=r("a"),Hn=i("google/owlvit-base-patch32"),Rn=i(" architecture."),Un=f(),te=r("p"),Gn=i("Configuration objects inherit from "),Oo=r("a"),Xn=i("PretrainedConfig"),Zn=i(` and can be used to control the model outputs. Read the
documentation from `),Vo=r("a"),Jn=i("PretrainedConfig"),Kn=i(" for more information."),Yn=f(),T(Pe.$$.fragment),rr=f(),oe=r("h2"),ze=r("a"),cs=r("span"),T(yt.$$.fragment),Qn=f(),ds=r("span"),ea=i("OwlViTVisionConfig"),nr=f(),I=r("div"),T(jt.$$.fragment),ta=f(),se=r("p"),oa=i("This is the configuration class to store the configuration of an "),xo=r("a"),sa=i("OwlViTVisionModel"),ra=i(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),kt=r("a"),na=i("google/owlvit-base-patch32"),aa=i(" architecture."),ia=f(),re=r("p"),la=i("Configuration objects inherit from "),yo=r("a"),ca=i("PretrainedConfig"),da=i(` and can be used to control the model outputs. Read the
documentation from `),jo=r("a"),pa=i("PretrainedConfig"),ma=i(" for more information."),fa=f(),T(Ee.$$.fragment),ar=f(),ne=r("h2"),Me=r("a"),ps=r("span"),T(Pt.$$.fragment),ha=f(),ms=r("span"),ga=i("OwlViTFeatureExtractor"),ir=f(),F=r("div"),T(zt.$$.fragment),ua=f(),fs=r("p"),_a=i("Constructs an OWL-ViT feature extractor."),wa=f(),Et=r("p"),va=i("This feature extractor inherits from "),ko=r("a"),Ta=i("FeatureExtractionMixin"),ba=i(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),$a=f(),G=r("div"),T(Mt.$$.fragment),Oa=f(),hs=r("p"),Va=i("Main method to prepare for the model one or several image(s)."),xa=f(),T(qe.$$.fragment),lr=f(),ae=r("h2"),Ce=r("a"),gs=r("span"),T(qt.$$.fragment),ya=f(),us=r("span"),ja=i("OwlViTProcessor"),cr=f(),z=r("div"),T(Ct.$$.fragment),ka=f(),E=r("p"),Pa=i("Constructs an OWL-ViT processor which wraps "),Po=r("a"),za=i("OwlViTFeatureExtractor"),Ea=i(" and "),zo=r("a"),Ma=i("CLIPTokenizer"),qa=i("/"),Eo=r("a"),Ca=i("CLIPTokenizerFast"),Ia=i(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),_s=r("code"),Fa=i("__call__()"),La=i(" and "),Mo=r("a"),Aa=i("decode()"),Da=i(" for more information."),Wa=f(),Ie=r("div"),T(It.$$.fragment),Na=f(),Ft=r("p"),Sa=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=r("a"),Ba=i("batch_decode()"),Ha=i(`. Please
refer to the docstring of this method for more information.`),Ra=f(),Fe=r("div"),T(Lt.$$.fragment),Ua=f(),At=r("p"),Ga=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=r("a"),Xa=i("decode()"),Za=i(`. Please refer to
the docstring of this method for more information.`),Ja=f(),Le=r("div"),T(Dt.$$.fragment),Ka=f(),Wt=r("p"),Ya=i("This method forwards all its arguments to "),ws=r("code"),Qa=i("OwlViTFeatureExtractor.post_process()"),ei=i(`. Please refer to the
docstring of this method for more information.`),dr=f(),ie=r("h2"),Ae=r("a"),vs=r("span"),T(Nt.$$.fragment),ti=f(),Ts=r("span"),oi=i("OwlViTModel"),pr=f(),L=r("div"),T(St.$$.fragment),si=f(),A=r("div"),T(Bt.$$.fragment),ri=f(),le=r("p"),ni=i("The "),Io=r("a"),ai=i("OwlViTModel"),ii=i(" forward method, overrides the "),bs=r("code"),li=i("__call__"),ci=i(" special method."),di=f(),T(De.$$.fragment),pi=f(),T(We.$$.fragment),mi=f(),D=r("div"),T(Ht.$$.fragment),fi=f(),ce=r("p"),hi=i("The "),Fo=r("a"),gi=i("OwlViTModel"),ui=i(" forward method, overrides the "),$s=r("code"),_i=i("__call__"),wi=i(" special method."),vi=f(),T(Ne.$$.fragment),Ti=f(),T(Se.$$.fragment),bi=f(),W=r("div"),T(Rt.$$.fragment),$i=f(),de=r("p"),Oi=i("The "),Lo=r("a"),Vi=i("OwlViTModel"),xi=i(" forward method, overrides the "),Os=r("code"),yi=i("__call__"),ji=i(" special method."),ki=f(),T(Be.$$.fragment),Pi=f(),T(He.$$.fragment),mr=f(),pe=r("h2"),Re=r("a"),Vs=r("span"),T(Ut.$$.fragment),zi=f(),xs=r("span"),Ei=i("OwlViTTextModel"),fr=f(),me=r("div"),T(Gt.$$.fragment),Mi=f(),N=r("div"),T(Xt.$$.fragment),qi=f(),fe=r("p"),Ci=i("The "),Ao=r("a"),Ii=i("OwlViTTextModel"),Fi=i(" forward method, overrides the "),ys=r("code"),Li=i("__call__"),Ai=i(" special method."),Di=f(),T(Ue.$$.fragment),Wi=f(),T(Ge.$$.fragment),hr=f(),he=r("h2"),Xe=r("a"),js=r("span"),T(Zt.$$.fragment),Ni=f(),ks=r("span"),Si=i("OwlViTVisionModel"),gr=f(),ge=r("div"),T(Jt.$$.fragment),Bi=f(),S=r("div"),T(Kt.$$.fragment),Hi=f(),ue=r("p"),Ri=i("The "),Do=r("a"),Ui=i("OwlViTVisionModel"),Gi=i(" forward method, overrides the "),Ps=r("code"),Xi=i("__call__"),Zi=i(" special method."),Ji=f(),T(Ze.$$.fragment),Ki=f(),T(Je.$$.fragment),ur=f(),_e=r("h2"),Ke=r("a"),zs=r("span"),T(Yt.$$.fragment),Yi=f(),Es=r("span"),Qi=i("OwlViTForObjectDetection"),_r=f(),we=r("div"),T(Qt.$$.fragment),el=f(),B=r("div"),T(eo.$$.fragment),tl=f(),ve=r("p"),ol=i("The "),Wo=r("a"),sl=i("OwlViTForObjectDetection"),rl=i(" forward method, overrides the "),Ms=r("code"),nl=i("__call__"),al=i(" special method."),il=f(),T(Ye.$$.fragment),ll=f(),T(Qe.$$.fragment),this.h()},l(t){const u=qc('[data-svelte="svelte-1phssyn"]',document.head);c=n(u,"META",{name:!0,content:!0}),u.forEach(o),v=h(t),g=n(t,"H1",{class:!0});var to=a(g);p=n(to,"A",{id:!0,class:!0,href:!0});var qs=a(p);w=n(qs,"SPAN",{});var Cs=a(w);b(s.$$.fragment,Cs),Cs.forEach(o),qs.forEach(o),m=h(to),j=n(to,"SPAN",{});var Is=a(j);Nr=l(Is,"OWL-ViT"),Is.forEach(o),to.forEach(o),Rs=h(t),Z=n(t,"H2",{class:!0});var oo=a(Z);be=n(oo,"A",{id:!0,class:!0,href:!0});var Fs=a(be);es=n(Fs,"SPAN",{});var Ls=a(es);b(ft.$$.fragment,Ls),Ls.forEach(o),Fs.forEach(o),Sr=h(oo),ts=n(oo,"SPAN",{});var As=a(ts);Br=l(As,"Overview"),As.forEach(o),oo.forEach(o),Us=h(t),$e=n(t,"P",{});var so=a($e);Hr=l(so,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ht=n(so,"A",{href:!0,rel:!0});var Ds=a(ht);Rr=l(Ds,"Simple Open-Vocabulary Object Detection with Vision Transformers"),Ds.forEach(o),Ur=l(so," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),so.forEach(o),Gs=h(t),ao=n(t,"P",{});var Ws=a(ao);Gr=l(Ws,"The abstract from the paper is the following:"),Ws.forEach(o),Xs=h(t),io=n(t,"P",{});var Ns=a(io);os=n(Ns,"EM",{});var Ss=a(os);Xr=l(Ss,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Ss.forEach(o),Ns.forEach(o),Zs=h(t),J=n(t,"H2",{class:!0});var ro=a(J);Oe=n(ro,"A",{id:!0,class:!0,href:!0});var Bs=a(Oe);ss=n(Bs,"SPAN",{});var cl=a(ss);b(gt.$$.fragment,cl),cl.forEach(o),Bs.forEach(o),Zr=h(ro),rs=n(ro,"SPAN",{});var dl=a(rs);Jr=l(dl,"Usage"),dl.forEach(o),ro.forEach(o),Js=h(t),Ve=n(t,"P",{});var vr=a(Ve);Kr=l(vr,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),lo=n(vr,"A",{href:!0});var pl=a(lo);Yr=l(pl,"CLIP"),pl.forEach(o),Qr=l(vr," as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),vr.forEach(o),Ks=h(t),P=n(t,"P",{});var M=a(P);co=n(M,"A",{href:!0});var ml=a(co);en=l(ml,"OwlViTFeatureExtractor"),ml.forEach(o),tn=l(M," can be used to resize (or rescale) and normalize images for the model and "),po=n(M,"A",{href:!0});var fl=a(po);on=l(fl,"CLIPTokenizer"),fl.forEach(o),sn=l(M," is used to encode the text. "),mo=n(M,"A",{href:!0});var hl=a(mo);rn=l(hl,"OwlViTProcessor"),hl.forEach(o),nn=l(M," wraps "),fo=n(M,"A",{href:!0});var gl=a(fo);an=l(gl,"OwlViTFeatureExtractor"),gl.forEach(o),ln=l(M," and "),ho=n(M,"A",{href:!0});var ul=a(ho);cn=l(ul,"CLIPTokenizer"),ul.forEach(o),dn=l(M," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),go=n(M,"A",{href:!0});var _l=a(go);pn=l(_l,"OwlViTProcessor"),_l.forEach(o),mn=l(M," and "),uo=n(M,"A",{href:!0});var wl=a(uo);fn=l(wl,"OwlViTForObjectDetection"),wl.forEach(o),hn=l(M,"."),M.forEach(o),Ys=h(t),b(ut.$$.fragment,t),Qs=h(t),U=n(t,"P",{});var No=a(U);gn=l(No,"This model was contributed by "),_t=n(No,"A",{href:!0,rel:!0});var vl=a(_t);un=l(vl,"adirik"),vl.forEach(o),_n=l(No,". The original code can be found "),wt=n(No,"A",{href:!0,rel:!0});var Tl=a(wt);wn=l(Tl,"here"),Tl.forEach(o),vn=l(No,"."),No.forEach(o),er=h(t),K=n(t,"H2",{class:!0});var Tr=a(K);xe=n(Tr,"A",{id:!0,class:!0,href:!0});var bl=a(xe);ns=n(bl,"SPAN",{});var $l=a(ns);b(vt.$$.fragment,$l),$l.forEach(o),bl.forEach(o),Tn=h(Tr),as=n(Tr,"SPAN",{});var Ol=a(as);bn=l(Ol,"OwlViTConfig"),Ol.forEach(o),Tr.forEach(o),tr=h(t),q=n(t,"DIV",{class:!0});var et=a(q);b(Tt.$$.fragment,et),$n=h(et),ye=n(et,"P",{});var Hs=a(ye);_o=n(Hs,"A",{href:!0});var Vl=a(_o);On=l(Vl,"OwlViTConfig"),Vl.forEach(o),Vn=l(Hs," is the configuration class to store the configuration of an "),wo=n(Hs,"A",{href:!0});var xl=a(wo);xn=l(xl,"OwlViTModel"),xl.forEach(o),yn=l(Hs,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),Hs.forEach(o),jn=h(et),Y=n(et,"P",{});var So=a(Y);kn=l(So,"Configuration objects inherit from "),vo=n(So,"A",{href:!0});var yl=a(vo);Pn=l(yl,"PretrainedConfig"),yl.forEach(o),zn=l(So,` and can be used to control the model outputs. Read the
documentation from `),To=n(So,"A",{href:!0});var jl=a(To);En=l(jl,"PretrainedConfig"),jl.forEach(o),Mn=l(So," for more information."),So.forEach(o),qn=h(et),je=n(et,"DIV",{class:!0});var br=a(je);b(bt.$$.fragment,br),Cn=h(br),$t=n(br,"P",{});var $r=a($t);In=l($r,"Instantiate a "),bo=n($r,"A",{href:!0});var kl=a(bo);Fn=l(kl,"OwlViTConfig"),kl.forEach(o),Ln=l($r,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),$r.forEach(o),br.forEach(o),et.forEach(o),or=h(t),Q=n(t,"H2",{class:!0});var Or=a(Q);ke=n(Or,"A",{id:!0,class:!0,href:!0});var Pl=a(ke);is=n(Pl,"SPAN",{});var zl=a(is);b(Ot.$$.fragment,zl),zl.forEach(o),Pl.forEach(o),An=h(Or),ls=n(Or,"SPAN",{});var El=a(ls);Dn=l(El,"OwlViTTextConfig"),El.forEach(o),Or.forEach(o),sr=h(t),C=n(t,"DIV",{class:!0});var tt=a(C);b(Vt.$$.fragment,tt),Wn=h(tt),ee=n(tt,"P",{});var Bo=a(ee);Nn=l(Bo,"This is the configuration class to store the configuration of an "),$o=n(Bo,"A",{href:!0});var Ml=a($o);Sn=l(Ml,"OwlViTTextModel"),Ml.forEach(o),Bn=l(Bo,`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),xt=n(Bo,"A",{href:!0,rel:!0});var ql=a(xt);Hn=l(ql,"google/owlvit-base-patch32"),ql.forEach(o),Rn=l(Bo," architecture."),Bo.forEach(o),Un=h(tt),te=n(tt,"P",{});var Ho=a(te);Gn=l(Ho,"Configuration objects inherit from "),Oo=n(Ho,"A",{href:!0});var Cl=a(Oo);Xn=l(Cl,"PretrainedConfig"),Cl.forEach(o),Zn=l(Ho,` and can be used to control the model outputs. Read the
documentation from `),Vo=n(Ho,"A",{href:!0});var Il=a(Vo);Jn=l(Il,"PretrainedConfig"),Il.forEach(o),Kn=l(Ho," for more information."),Ho.forEach(o),Yn=h(tt),b(Pe.$$.fragment,tt),tt.forEach(o),rr=h(t),oe=n(t,"H2",{class:!0});var Vr=a(oe);ze=n(Vr,"A",{id:!0,class:!0,href:!0});var Fl=a(ze);cs=n(Fl,"SPAN",{});var Ll=a(cs);b(yt.$$.fragment,Ll),Ll.forEach(o),Fl.forEach(o),Qn=h(Vr),ds=n(Vr,"SPAN",{});var Al=a(ds);ea=l(Al,"OwlViTVisionConfig"),Al.forEach(o),Vr.forEach(o),nr=h(t),I=n(t,"DIV",{class:!0});var ot=a(I);b(jt.$$.fragment,ot),ta=h(ot),se=n(ot,"P",{});var Ro=a(se);oa=l(Ro,"This is the configuration class to store the configuration of an "),xo=n(Ro,"A",{href:!0});var Dl=a(xo);sa=l(Dl,"OwlViTVisionModel"),Dl.forEach(o),ra=l(Ro,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),kt=n(Ro,"A",{href:!0,rel:!0});var Wl=a(kt);na=l(Wl,"google/owlvit-base-patch32"),Wl.forEach(o),aa=l(Ro," architecture."),Ro.forEach(o),ia=h(ot),re=n(ot,"P",{});var Uo=a(re);la=l(Uo,"Configuration objects inherit from "),yo=n(Uo,"A",{href:!0});var Nl=a(yo);ca=l(Nl,"PretrainedConfig"),Nl.forEach(o),da=l(Uo,` and can be used to control the model outputs. Read the
documentation from `),jo=n(Uo,"A",{href:!0});var Sl=a(jo);pa=l(Sl,"PretrainedConfig"),Sl.forEach(o),ma=l(Uo," for more information."),Uo.forEach(o),fa=h(ot),b(Ee.$$.fragment,ot),ot.forEach(o),ar=h(t),ne=n(t,"H2",{class:!0});var xr=a(ne);Me=n(xr,"A",{id:!0,class:!0,href:!0});var Bl=a(Me);ps=n(Bl,"SPAN",{});var Hl=a(ps);b(Pt.$$.fragment,Hl),Hl.forEach(o),Bl.forEach(o),ha=h(xr),ms=n(xr,"SPAN",{});var Rl=a(ms);ga=l(Rl,"OwlViTFeatureExtractor"),Rl.forEach(o),xr.forEach(o),ir=h(t),F=n(t,"DIV",{class:!0});var st=a(F);b(zt.$$.fragment,st),ua=h(st),fs=n(st,"P",{});var Ul=a(fs);_a=l(Ul,"Constructs an OWL-ViT feature extractor."),Ul.forEach(o),wa=h(st),Et=n(st,"P",{});var yr=a(Et);va=l(yr,"This feature extractor inherits from "),ko=n(yr,"A",{href:!0});var Gl=a(ko);Ta=l(Gl,"FeatureExtractionMixin"),Gl.forEach(o),ba=l(yr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),yr.forEach(o),$a=h(st),G=n(st,"DIV",{class:!0});var Go=a(G);b(Mt.$$.fragment,Go),Oa=h(Go),hs=n(Go,"P",{});var Xl=a(hs);Va=l(Xl,"Main method to prepare for the model one or several image(s)."),Xl.forEach(o),xa=h(Go),b(qe.$$.fragment,Go),Go.forEach(o),st.forEach(o),lr=h(t),ae=n(t,"H2",{class:!0});var jr=a(ae);Ce=n(jr,"A",{id:!0,class:!0,href:!0});var Zl=a(Ce);gs=n(Zl,"SPAN",{});var Jl=a(gs);b(qt.$$.fragment,Jl),Jl.forEach(o),Zl.forEach(o),ya=h(jr),us=n(jr,"SPAN",{});var Kl=a(us);ja=l(Kl,"OwlViTProcessor"),Kl.forEach(o),jr.forEach(o),cr=h(t),z=n(t,"DIV",{class:!0});var X=a(z);b(Ct.$$.fragment,X),ka=h(X),E=n(X,"P",{});var H=a(E);Pa=l(H,"Constructs an OWL-ViT processor which wraps "),Po=n(H,"A",{href:!0});var Yl=a(Po);za=l(Yl,"OwlViTFeatureExtractor"),Yl.forEach(o),Ea=l(H," and "),zo=n(H,"A",{href:!0});var Ql=a(zo);Ma=l(Ql,"CLIPTokenizer"),Ql.forEach(o),qa=l(H,"/"),Eo=n(H,"A",{href:!0});var ec=a(Eo);Ca=l(ec,"CLIPTokenizerFast"),ec.forEach(o),Ia=l(H,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),_s=n(H,"CODE",{});var tc=a(_s);Fa=l(tc,"__call__()"),tc.forEach(o),La=l(H," and "),Mo=n(H,"A",{href:!0});var oc=a(Mo);Aa=l(oc,"decode()"),oc.forEach(o),Da=l(H," for more information."),H.forEach(o),Wa=h(X),Ie=n(X,"DIV",{class:!0});var kr=a(Ie);b(It.$$.fragment,kr),Na=h(kr),Ft=n(kr,"P",{});var Pr=a(Ft);Sa=l(Pr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=n(Pr,"A",{href:!0});var sc=a(qo);Ba=l(sc,"batch_decode()"),sc.forEach(o),Ha=l(Pr,`. Please
refer to the docstring of this method for more information.`),Pr.forEach(o),kr.forEach(o),Ra=h(X),Fe=n(X,"DIV",{class:!0});var zr=a(Fe);b(Lt.$$.fragment,zr),Ua=h(zr),At=n(zr,"P",{});var Er=a(At);Ga=l(Er,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=n(Er,"A",{href:!0});var rc=a(Co);Xa=l(rc,"decode()"),rc.forEach(o),Za=l(Er,`. Please refer to
the docstring of this method for more information.`),Er.forEach(o),zr.forEach(o),Ja=h(X),Le=n(X,"DIV",{class:!0});var Mr=a(Le);b(Dt.$$.fragment,Mr),Ka=h(Mr),Wt=n(Mr,"P",{});var qr=a(Wt);Ya=l(qr,"This method forwards all its arguments to "),ws=n(qr,"CODE",{});var nc=a(ws);Qa=l(nc,"OwlViTFeatureExtractor.post_process()"),nc.forEach(o),ei=l(qr,`. Please refer to the
docstring of this method for more information.`),qr.forEach(o),Mr.forEach(o),X.forEach(o),dr=h(t),ie=n(t,"H2",{class:!0});var Cr=a(ie);Ae=n(Cr,"A",{id:!0,class:!0,href:!0});var ac=a(Ae);vs=n(ac,"SPAN",{});var ic=a(vs);b(Nt.$$.fragment,ic),ic.forEach(o),ac.forEach(o),ti=h(Cr),Ts=n(Cr,"SPAN",{});var lc=a(Ts);oi=l(lc,"OwlViTModel"),lc.forEach(o),Cr.forEach(o),pr=h(t),L=n(t,"DIV",{class:!0});var rt=a(L);b(St.$$.fragment,rt),si=h(rt),A=n(rt,"DIV",{class:!0});var nt=a(A);b(Bt.$$.fragment,nt),ri=h(nt),le=n(nt,"P",{});var Xo=a(le);ni=l(Xo,"The "),Io=n(Xo,"A",{href:!0});var cc=a(Io);ai=l(cc,"OwlViTModel"),cc.forEach(o),ii=l(Xo," forward method, overrides the "),bs=n(Xo,"CODE",{});var dc=a(bs);li=l(dc,"__call__"),dc.forEach(o),ci=l(Xo," special method."),Xo.forEach(o),di=h(nt),b(De.$$.fragment,nt),pi=h(nt),b(We.$$.fragment,nt),nt.forEach(o),mi=h(rt),D=n(rt,"DIV",{class:!0});var at=a(D);b(Ht.$$.fragment,at),fi=h(at),ce=n(at,"P",{});var Zo=a(ce);hi=l(Zo,"The "),Fo=n(Zo,"A",{href:!0});var pc=a(Fo);gi=l(pc,"OwlViTModel"),pc.forEach(o),ui=l(Zo," forward method, overrides the "),$s=n(Zo,"CODE",{});var mc=a($s);_i=l(mc,"__call__"),mc.forEach(o),wi=l(Zo," special method."),Zo.forEach(o),vi=h(at),b(Ne.$$.fragment,at),Ti=h(at),b(Se.$$.fragment,at),at.forEach(o),bi=h(rt),W=n(rt,"DIV",{class:!0});var it=a(W);b(Rt.$$.fragment,it),$i=h(it),de=n(it,"P",{});var Jo=a(de);Oi=l(Jo,"The "),Lo=n(Jo,"A",{href:!0});var fc=a(Lo);Vi=l(fc,"OwlViTModel"),fc.forEach(o),xi=l(Jo," forward method, overrides the "),Os=n(Jo,"CODE",{});var hc=a(Os);yi=l(hc,"__call__"),hc.forEach(o),ji=l(Jo," special method."),Jo.forEach(o),ki=h(it),b(Be.$$.fragment,it),Pi=h(it),b(He.$$.fragment,it),it.forEach(o),rt.forEach(o),mr=h(t),pe=n(t,"H2",{class:!0});var Ir=a(pe);Re=n(Ir,"A",{id:!0,class:!0,href:!0});var gc=a(Re);Vs=n(gc,"SPAN",{});var uc=a(Vs);b(Ut.$$.fragment,uc),uc.forEach(o),gc.forEach(o),zi=h(Ir),xs=n(Ir,"SPAN",{});var _c=a(xs);Ei=l(_c,"OwlViTTextModel"),_c.forEach(o),Ir.forEach(o),fr=h(t),me=n(t,"DIV",{class:!0});var Fr=a(me);b(Gt.$$.fragment,Fr),Mi=h(Fr),N=n(Fr,"DIV",{class:!0});var lt=a(N);b(Xt.$$.fragment,lt),qi=h(lt),fe=n(lt,"P",{});var Ko=a(fe);Ci=l(Ko,"The "),Ao=n(Ko,"A",{href:!0});var wc=a(Ao);Ii=l(wc,"OwlViTTextModel"),wc.forEach(o),Fi=l(Ko," forward method, overrides the "),ys=n(Ko,"CODE",{});var vc=a(ys);Li=l(vc,"__call__"),vc.forEach(o),Ai=l(Ko," special method."),Ko.forEach(o),Di=h(lt),b(Ue.$$.fragment,lt),Wi=h(lt),b(Ge.$$.fragment,lt),lt.forEach(o),Fr.forEach(o),hr=h(t),he=n(t,"H2",{class:!0});var Lr=a(he);Xe=n(Lr,"A",{id:!0,class:!0,href:!0});var Tc=a(Xe);js=n(Tc,"SPAN",{});var bc=a(js);b(Zt.$$.fragment,bc),bc.forEach(o),Tc.forEach(o),Ni=h(Lr),ks=n(Lr,"SPAN",{});var $c=a(ks);Si=l($c,"OwlViTVisionModel"),$c.forEach(o),Lr.forEach(o),gr=h(t),ge=n(t,"DIV",{class:!0});var Ar=a(ge);b(Jt.$$.fragment,Ar),Bi=h(Ar),S=n(Ar,"DIV",{class:!0});var ct=a(S);b(Kt.$$.fragment,ct),Hi=h(ct),ue=n(ct,"P",{});var Yo=a(ue);Ri=l(Yo,"The "),Do=n(Yo,"A",{href:!0});var Oc=a(Do);Ui=l(Oc,"OwlViTVisionModel"),Oc.forEach(o),Gi=l(Yo," forward method, overrides the "),Ps=n(Yo,"CODE",{});var Vc=a(Ps);Xi=l(Vc,"__call__"),Vc.forEach(o),Zi=l(Yo," special method."),Yo.forEach(o),Ji=h(ct),b(Ze.$$.fragment,ct),Ki=h(ct),b(Je.$$.fragment,ct),ct.forEach(o),Ar.forEach(o),ur=h(t),_e=n(t,"H2",{class:!0});var Dr=a(_e);Ke=n(Dr,"A",{id:!0,class:!0,href:!0});var xc=a(Ke);zs=n(xc,"SPAN",{});var yc=a(zs);b(Yt.$$.fragment,yc),yc.forEach(o),xc.forEach(o),Yi=h(Dr),Es=n(Dr,"SPAN",{});var jc=a(Es);Qi=l(jc,"OwlViTForObjectDetection"),jc.forEach(o),Dr.forEach(o),_r=h(t),we=n(t,"DIV",{class:!0});var Wr=a(we);b(Qt.$$.fragment,Wr),el=h(Wr),B=n(Wr,"DIV",{class:!0});var dt=a(B);b(eo.$$.fragment,dt),tl=h(dt),ve=n(dt,"P",{});var Qo=a(ve);ol=l(Qo,"The "),Wo=n(Qo,"A",{href:!0});var kc=a(Wo);sl=l(kc,"OwlViTForObjectDetection"),kc.forEach(o),rl=l(Qo," forward method, overrides the "),Ms=n(Qo,"CODE",{});var Pc=a(Ms);nl=l(Pc,"__call__"),Pc.forEach(o),al=l(Qo," special method."),Qo.forEach(o),il=h(dt),b(Ye.$$.fragment,dt),ll=h(dt),b(Qe.$$.fragment,dt),dt.forEach(o),Wr.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(Kc)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(g,"class","relative group"),d(be,"id","overview"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#overview"),d(Z,"class","relative group"),d(ht,"href","https://arxiv.org/abs/2205.06230"),d(ht,"rel","nofollow"),d(Oe,"id","usage"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#usage"),d(J,"class","relative group"),d(lo,"href","clip"),d(co,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(po,"href","/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer"),d(mo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(fo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(ho,"href","/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer"),d(go,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(uo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(_t,"href","https://huggingface.co/adirik"),d(_t,"rel","nofollow"),d(wt,"href","https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit"),d(wt,"rel","nofollow"),d(xe,"id","transformers.OwlViTConfig"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#transformers.OwlViTConfig"),d(K,"class","relative group"),d(_o,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTConfig"),d(wo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTModel"),d(vo,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(To,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(bo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTConfig"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"id","transformers.OwlViTTextConfig"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#transformers.OwlViTTextConfig"),d(Q,"class","relative group"),d($o,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(xt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(xt,"rel","nofollow"),d(Oo,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(Vo,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ze,"id","transformers.OwlViTVisionConfig"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#transformers.OwlViTVisionConfig"),d(oe,"class","relative group"),d(xo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(kt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(kt,"rel","nofollow"),d(yo,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(jo,"href","/docs/transformers/pr_18257/en/main_classes/configuration#transformers.PretrainedConfig"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Me,"id","transformers.OwlViTFeatureExtractor"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#transformers.OwlViTFeatureExtractor"),d(ne,"class","relative group"),d(ko,"href","/docs/transformers/pr_18257/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.OwlViTProcessor"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.OwlViTProcessor"),d(ae,"class","relative group"),d(Po,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(zo,"href","/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizer"),d(Eo,"href","/docs/transformers/pr_18257/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(Mo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(qo,"href","/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Co,"href","/docs/transformers/pr_18257/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"id","transformers.OwlViTModel"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#transformers.OwlViTModel"),d(ie,"class","relative group"),d(Io,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTModel"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.OwlViTTextModel"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.OwlViTTextModel"),d(pe,"class","relative group"),d(Ao,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.OwlViTVisionModel"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.OwlViTVisionModel"),d(he,"class","relative group"),d(Do,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.OwlViTForObjectDetection"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.OwlViTForObjectDetection"),d(_e,"class","relative group"),d(Wo,"href","/docs/transformers/pr_18257/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,c),_(t,v,u),_(t,g,u),e(g,p),e(p,w),$(s,w,null),e(g,m),e(g,j),e(j,Nr),_(t,Rs,u),_(t,Z,u),e(Z,be),e(be,es),$(ft,es,null),e(Z,Sr),e(Z,ts),e(ts,Br),_(t,Us,u),_(t,$e,u),e($e,Hr),e($e,ht),e(ht,Rr),e($e,Ur),_(t,Gs,u),_(t,ao,u),e(ao,Gr),_(t,Xs,u),_(t,io,u),e(io,os),e(os,Xr),_(t,Zs,u),_(t,J,u),e(J,Oe),e(Oe,ss),$(gt,ss,null),e(J,Zr),e(J,rs),e(rs,Jr),_(t,Js,u),_(t,Ve,u),e(Ve,Kr),e(Ve,lo),e(lo,Yr),e(Ve,Qr),_(t,Ks,u),_(t,P,u),e(P,co),e(co,en),e(P,tn),e(P,po),e(po,on),e(P,sn),e(P,mo),e(mo,rn),e(P,nn),e(P,fo),e(fo,an),e(P,ln),e(P,ho),e(ho,cn),e(P,dn),e(P,go),e(go,pn),e(P,mn),e(P,uo),e(uo,fn),e(P,hn),_(t,Ys,u),$(ut,t,u),_(t,Qs,u),_(t,U,u),e(U,gn),e(U,_t),e(_t,un),e(U,_n),e(U,wt),e(wt,wn),e(U,vn),_(t,er,u),_(t,K,u),e(K,xe),e(xe,ns),$(vt,ns,null),e(K,Tn),e(K,as),e(as,bn),_(t,tr,u),_(t,q,u),$(Tt,q,null),e(q,$n),e(q,ye),e(ye,_o),e(_o,On),e(ye,Vn),e(ye,wo),e(wo,xn),e(ye,yn),e(q,jn),e(q,Y),e(Y,kn),e(Y,vo),e(vo,Pn),e(Y,zn),e(Y,To),e(To,En),e(Y,Mn),e(q,qn),e(q,je),$(bt,je,null),e(je,Cn),e(je,$t),e($t,In),e($t,bo),e(bo,Fn),e($t,Ln),_(t,or,u),_(t,Q,u),e(Q,ke),e(ke,is),$(Ot,is,null),e(Q,An),e(Q,ls),e(ls,Dn),_(t,sr,u),_(t,C,u),$(Vt,C,null),e(C,Wn),e(C,ee),e(ee,Nn),e(ee,$o),e($o,Sn),e(ee,Bn),e(ee,xt),e(xt,Hn),e(ee,Rn),e(C,Un),e(C,te),e(te,Gn),e(te,Oo),e(Oo,Xn),e(te,Zn),e(te,Vo),e(Vo,Jn),e(te,Kn),e(C,Yn),$(Pe,C,null),_(t,rr,u),_(t,oe,u),e(oe,ze),e(ze,cs),$(yt,cs,null),e(oe,Qn),e(oe,ds),e(ds,ea),_(t,nr,u),_(t,I,u),$(jt,I,null),e(I,ta),e(I,se),e(se,oa),e(se,xo),e(xo,sa),e(se,ra),e(se,kt),e(kt,na),e(se,aa),e(I,ia),e(I,re),e(re,la),e(re,yo),e(yo,ca),e(re,da),e(re,jo),e(jo,pa),e(re,ma),e(I,fa),$(Ee,I,null),_(t,ar,u),_(t,ne,u),e(ne,Me),e(Me,ps),$(Pt,ps,null),e(ne,ha),e(ne,ms),e(ms,ga),_(t,ir,u),_(t,F,u),$(zt,F,null),e(F,ua),e(F,fs),e(fs,_a),e(F,wa),e(F,Et),e(Et,va),e(Et,ko),e(ko,Ta),e(Et,ba),e(F,$a),e(F,G),$(Mt,G,null),e(G,Oa),e(G,hs),e(hs,Va),e(G,xa),$(qe,G,null),_(t,lr,u),_(t,ae,u),e(ae,Ce),e(Ce,gs),$(qt,gs,null),e(ae,ya),e(ae,us),e(us,ja),_(t,cr,u),_(t,z,u),$(Ct,z,null),e(z,ka),e(z,E),e(E,Pa),e(E,Po),e(Po,za),e(E,Ea),e(E,zo),e(zo,Ma),e(E,qa),e(E,Eo),e(Eo,Ca),e(E,Ia),e(E,_s),e(_s,Fa),e(E,La),e(E,Mo),e(Mo,Aa),e(E,Da),e(z,Wa),e(z,Ie),$(It,Ie,null),e(Ie,Na),e(Ie,Ft),e(Ft,Sa),e(Ft,qo),e(qo,Ba),e(Ft,Ha),e(z,Ra),e(z,Fe),$(Lt,Fe,null),e(Fe,Ua),e(Fe,At),e(At,Ga),e(At,Co),e(Co,Xa),e(At,Za),e(z,Ja),e(z,Le),$(Dt,Le,null),e(Le,Ka),e(Le,Wt),e(Wt,Ya),e(Wt,ws),e(ws,Qa),e(Wt,ei),_(t,dr,u),_(t,ie,u),e(ie,Ae),e(Ae,vs),$(Nt,vs,null),e(ie,ti),e(ie,Ts),e(Ts,oi),_(t,pr,u),_(t,L,u),$(St,L,null),e(L,si),e(L,A),$(Bt,A,null),e(A,ri),e(A,le),e(le,ni),e(le,Io),e(Io,ai),e(le,ii),e(le,bs),e(bs,li),e(le,ci),e(A,di),$(De,A,null),e(A,pi),$(We,A,null),e(L,mi),e(L,D),$(Ht,D,null),e(D,fi),e(D,ce),e(ce,hi),e(ce,Fo),e(Fo,gi),e(ce,ui),e(ce,$s),e($s,_i),e(ce,wi),e(D,vi),$(Ne,D,null),e(D,Ti),$(Se,D,null),e(L,bi),e(L,W),$(Rt,W,null),e(W,$i),e(W,de),e(de,Oi),e(de,Lo),e(Lo,Vi),e(de,xi),e(de,Os),e(Os,yi),e(de,ji),e(W,ki),$(Be,W,null),e(W,Pi),$(He,W,null),_(t,mr,u),_(t,pe,u),e(pe,Re),e(Re,Vs),$(Ut,Vs,null),e(pe,zi),e(pe,xs),e(xs,Ei),_(t,fr,u),_(t,me,u),$(Gt,me,null),e(me,Mi),e(me,N),$(Xt,N,null),e(N,qi),e(N,fe),e(fe,Ci),e(fe,Ao),e(Ao,Ii),e(fe,Fi),e(fe,ys),e(ys,Li),e(fe,Ai),e(N,Di),$(Ue,N,null),e(N,Wi),$(Ge,N,null),_(t,hr,u),_(t,he,u),e(he,Xe),e(Xe,js),$(Zt,js,null),e(he,Ni),e(he,ks),e(ks,Si),_(t,gr,u),_(t,ge,u),$(Jt,ge,null),e(ge,Bi),e(ge,S),$(Kt,S,null),e(S,Hi),e(S,ue),e(ue,Ri),e(ue,Do),e(Do,Ui),e(ue,Gi),e(ue,Ps),e(Ps,Xi),e(ue,Zi),e(S,Ji),$(Ze,S,null),e(S,Ki),$(Je,S,null),_(t,ur,u),_(t,_e,u),e(_e,Ke),e(Ke,zs),$(Yt,zs,null),e(_e,Yi),e(_e,Es),e(Es,Qi),_(t,_r,u),_(t,we,u),$(Qt,we,null),e(we,el),e(we,B),$(eo,B,null),e(B,tl),e(B,ve),e(ve,ol),e(ve,Wo),e(Wo,sl),e(ve,rl),e(ve,Ms),e(Ms,nl),e(ve,al),e(B,il),$(Ye,B,null),e(B,ll),$(Qe,B,null),wr=!0},p(t,[u]){const to={};u&2&&(to.$$scope={dirty:u,ctx:t}),Pe.$set(to);const qs={};u&2&&(qs.$$scope={dirty:u,ctx:t}),Ee.$set(qs);const Cs={};u&2&&(Cs.$$scope={dirty:u,ctx:t}),qe.$set(Cs);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:t}),De.$set(Is);const oo={};u&2&&(oo.$$scope={dirty:u,ctx:t}),We.$set(oo);const Fs={};u&2&&(Fs.$$scope={dirty:u,ctx:t}),Ne.$set(Fs);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:t}),Se.$set(Ls);const As={};u&2&&(As.$$scope={dirty:u,ctx:t}),Be.$set(As);const so={};u&2&&(so.$$scope={dirty:u,ctx:t}),He.$set(so);const Ds={};u&2&&(Ds.$$scope={dirty:u,ctx:t}),Ue.$set(Ds);const Ws={};u&2&&(Ws.$$scope={dirty:u,ctx:t}),Ge.$set(Ws);const Ns={};u&2&&(Ns.$$scope={dirty:u,ctx:t}),Ze.$set(Ns);const Ss={};u&2&&(Ss.$$scope={dirty:u,ctx:t}),Je.$set(Ss);const ro={};u&2&&(ro.$$scope={dirty:u,ctx:t}),Ye.$set(ro);const Bs={};u&2&&(Bs.$$scope={dirty:u,ctx:t}),Qe.$set(Bs)},i(t){wr||(O(s.$$.fragment,t),O(ft.$$.fragment,t),O(gt.$$.fragment,t),O(ut.$$.fragment,t),O(vt.$$.fragment,t),O(Tt.$$.fragment,t),O(bt.$$.fragment,t),O(Ot.$$.fragment,t),O(Vt.$$.fragment,t),O(Pe.$$.fragment,t),O(yt.$$.fragment,t),O(jt.$$.fragment,t),O(Ee.$$.fragment,t),O(Pt.$$.fragment,t),O(zt.$$.fragment,t),O(Mt.$$.fragment,t),O(qe.$$.fragment,t),O(qt.$$.fragment,t),O(Ct.$$.fragment,t),O(It.$$.fragment,t),O(Lt.$$.fragment,t),O(Dt.$$.fragment,t),O(Nt.$$.fragment,t),O(St.$$.fragment,t),O(Bt.$$.fragment,t),O(De.$$.fragment,t),O(We.$$.fragment,t),O(Ht.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Rt.$$.fragment,t),O(Be.$$.fragment,t),O(He.$$.fragment,t),O(Ut.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Ue.$$.fragment,t),O(Ge.$$.fragment,t),O(Zt.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Ze.$$.fragment,t),O(Je.$$.fragment,t),O(Yt.$$.fragment,t),O(Qt.$$.fragment,t),O(eo.$$.fragment,t),O(Ye.$$.fragment,t),O(Qe.$$.fragment,t),wr=!0)},o(t){V(s.$$.fragment,t),V(ft.$$.fragment,t),V(gt.$$.fragment,t),V(ut.$$.fragment,t),V(vt.$$.fragment,t),V(Tt.$$.fragment,t),V(bt.$$.fragment,t),V(Ot.$$.fragment,t),V(Vt.$$.fragment,t),V(Pe.$$.fragment,t),V(yt.$$.fragment,t),V(jt.$$.fragment,t),V(Ee.$$.fragment,t),V(Pt.$$.fragment,t),V(zt.$$.fragment,t),V(Mt.$$.fragment,t),V(qe.$$.fragment,t),V(qt.$$.fragment,t),V(Ct.$$.fragment,t),V(It.$$.fragment,t),V(Lt.$$.fragment,t),V(Dt.$$.fragment,t),V(Nt.$$.fragment,t),V(St.$$.fragment,t),V(Bt.$$.fragment,t),V(De.$$.fragment,t),V(We.$$.fragment,t),V(Ht.$$.fragment,t),V(Ne.$$.fragment,t),V(Se.$$.fragment,t),V(Rt.$$.fragment,t),V(Be.$$.fragment,t),V(He.$$.fragment,t),V(Ut.$$.fragment,t),V(Gt.$$.fragment,t),V(Xt.$$.fragment,t),V(Ue.$$.fragment,t),V(Ge.$$.fragment,t),V(Zt.$$.fragment,t),V(Jt.$$.fragment,t),V(Kt.$$.fragment,t),V(Ze.$$.fragment,t),V(Je.$$.fragment,t),V(Yt.$$.fragment,t),V(Qt.$$.fragment,t),V(eo.$$.fragment,t),V(Ye.$$.fragment,t),V(Qe.$$.fragment,t),wr=!1},d(t){o(c),t&&o(v),t&&o(g),x(s),t&&o(Rs),t&&o(Z),x(ft),t&&o(Us),t&&o($e),t&&o(Gs),t&&o(ao),t&&o(Xs),t&&o(io),t&&o(Zs),t&&o(J),x(gt),t&&o(Js),t&&o(Ve),t&&o(Ks),t&&o(P),t&&o(Ys),x(ut,t),t&&o(Qs),t&&o(U),t&&o(er),t&&o(K),x(vt),t&&o(tr),t&&o(q),x(Tt),x(bt),t&&o(or),t&&o(Q),x(Ot),t&&o(sr),t&&o(C),x(Vt),x(Pe),t&&o(rr),t&&o(oe),x(yt),t&&o(nr),t&&o(I),x(jt),x(Ee),t&&o(ar),t&&o(ne),x(Pt),t&&o(ir),t&&o(F),x(zt),x(Mt),x(qe),t&&o(lr),t&&o(ae),x(qt),t&&o(cr),t&&o(z),x(Ct),x(It),x(Lt),x(Dt),t&&o(dr),t&&o(ie),x(Nt),t&&o(pr),t&&o(L),x(St),x(Bt),x(De),x(We),x(Ht),x(Ne),x(Se),x(Rt),x(Be),x(He),t&&o(mr),t&&o(pe),x(Ut),t&&o(fr),t&&o(me),x(Gt),x(Xt),x(Ue),x(Ge),t&&o(hr),t&&o(he),x(Zt),t&&o(gr),t&&o(ge),x(Jt),x(Kt),x(Ze),x(Je),t&&o(ur),t&&o(_e),x(Yt),t&&o(_r),t&&o(we),x(Qt),x(eo),x(Ye),x(Qe)}}}const Kc={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function Yc(y){return Cc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nd extends zc{constructor(c){super();Ec(this,c,Yc,Jc,Mc,{})}}export{nd as default,Kc as metadata};
