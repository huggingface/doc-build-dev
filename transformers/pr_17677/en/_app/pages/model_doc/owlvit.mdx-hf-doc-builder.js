import{S as wc,i as vc,s as Tc,e as r,k as f,w as T,t as i,M as bc,c as n,d as o,m as h,a,x as b,h as l,b as d,G as e,g as _,y as $,q as O,o as V,B as x,v as $c,L as pt}from"../../chunks/vendor-hf-doc-builder.js";import{T as oo}from"../../chunks/Tip-hf-doc-builder.js";import{D as z}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ve}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as R}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as dt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Oc(y){let c,v,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),v=i("Example:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Example:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Vc(y){let c,v,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),v=i("Example:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Example:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function xc(y){let c,v;return{c(){c=r("p"),v=i(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){c=n(g,"P",{});var p=a(c);v=l(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(g,p){_(g,c,p),e(c,v)},d(g){g&&o(c)}}}function yc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function jc(y){let c,v,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function kc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function zc(y){let c,v,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Pc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Ec(y){let c,v,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Mc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function qc(y){let c,v,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Cc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Ic(y){let c,v,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Fc(y){let c,v,g,p,w;return{c(){c=r("p"),v=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,v),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Lc(y){let c,v,g,p,w;return p=new ve({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape # [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][i], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape # [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][i], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),{c(){c=r("p"),v=i("Examples:"),g=f(),T(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);v=l(m,"Examples:"),m.forEach(o),g=h(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,v),_(s,g,m),$(p,s,m),w=!0},p:pt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){V(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),x(p,s)}}}function Ac(y){let c,v,g,p,w,s,m,j,Ir,Ns,X,Te,Ko,mt,Fr,Yo,Lr,Ss,be,Ar,ft,Dr,Wr,Bs,so,Nr,Hs,ro,Qo,Sr,Rs,Z,$e,es,ht,Br,ts,Hr,Us,Oe,Rr,no,Ur,Gr,Gs,k,ao,Xr,Zr,io,Jr,Kr,lo,Yr,Qr,co,en,tn,po,on,sn,mo,rn,nn,fo,an,ln,Xs,gt,Zs,U,cn,ut,dn,pn,_t,mn,fn,Js,J,Ve,os,wt,hn,ss,gn,Ks,M,vt,un,xe,ho,_n,wn,go,vn,Tn,bn,K,$n,uo,On,Vn,_o,xn,yn,jn,ye,Tt,kn,bt,zn,wo,Pn,En,Ys,Y,je,rs,$t,Mn,ns,qn,Qs,q,Ot,Cn,Q,In,vo,Fn,Ln,Vt,An,Dn,Wn,ee,Nn,To,Sn,Bn,bo,Hn,Rn,Un,ke,er,te,ze,as,xt,Gn,is,Xn,tr,C,yt,Zn,oe,Jn,$o,Kn,Yn,jt,Qn,ea,ta,se,oa,Oo,sa,ra,Vo,na,aa,ia,Pe,or,re,Ee,ls,kt,la,cs,ca,sr,I,zt,da,ds,pa,ma,Pt,fa,xo,ha,ga,ua,G,Et,_a,ps,wa,va,Me,rr,ne,qe,ms,Mt,Ta,fs,ba,nr,F,qt,$a,P,Oa,yo,Va,xa,jo,ya,ja,ko,ka,za,hs,Pa,Ea,zo,Ma,qa,Ca,Ce,Ct,Ia,It,Fa,Po,La,Aa,Da,Ie,Ft,Wa,Lt,Na,Eo,Sa,Ba,ar,ae,Fe,gs,At,Ha,us,Ra,ir,L,Dt,Ua,A,Wt,Ga,ie,Xa,Mo,Za,Ja,_s,Ka,Ya,Qa,Le,ei,Ae,ti,D,Nt,oi,le,si,qo,ri,ni,ws,ai,ii,li,De,ci,We,di,W,St,pi,ce,mi,Co,fi,hi,vs,gi,ui,_i,Ne,wi,Se,lr,de,Be,Ts,Bt,vi,bs,Ti,cr,pe,Ht,bi,N,Rt,$i,me,Oi,Io,Vi,xi,$s,yi,ji,ki,He,zi,Re,dr,fe,Ue,Os,Ut,Pi,Vs,Ei,pr,he,Gt,Mi,S,Xt,qi,ge,Ci,Fo,Ii,Fi,xs,Li,Ai,Di,Ge,Wi,Xe,mr,ue,Ze,ys,Zt,Ni,js,Si,fr,_e,Jt,Bi,B,Kt,Hi,we,Ri,Lo,Ui,Gi,ks,Xi,Zi,Ji,Je,Ki,Ke,hr;return s=new R({}),mt=new R({}),ht=new R({}),gt=new ve({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")

outputs = model(**inputs)
logits = outputs["logits"]  # Prediction logits of shape [batch_size, num_patches, num_max_text_queries]
boxes = outputs["pred_boxes"]  # Object box boundaries of shape [batch_size, num_patches, 4]

batch_size = boxes.shape[0]
for i in range(batch_size):  # Loop over sets of images and text queries
    boxes = outputs["pred_boxes"][i]
    logits = torch.max(outputs["logits"][i], dim=-1)
    scores = torch.sigmoid(logits.values)
    labels = logits.indices`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs[<span class="hljs-string">&quot;logits&quot;</span>]  <span class="hljs-comment"># Prediction logits of shape [batch_size, num_patches, num_max_text_queries]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>]  <span class="hljs-comment"># Object box boundaries of shape [batch_size, num_patches, 4]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = boxes.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):  <span class="hljs-comment"># Loop over sets of images and text queries</span>
<span class="hljs-meta">... </span>    boxes = outputs[<span class="hljs-string">&quot;pred_boxes&quot;</span>][i]
<span class="hljs-meta">... </span>    logits = torch.<span class="hljs-built_in">max</span>(outputs[<span class="hljs-string">&quot;logits&quot;</span>][i], dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    scores = torch.sigmoid(logits.values)
<span class="hljs-meta">... </span>    labels = logits.indices`}}),wt=new R({}),vt=new z({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.OwlViTConfig.vision_config_dict",description:`<strong>vision_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config_dict"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/configuration_owlvit.py#L245"}}),Tt=new z({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/configuration_owlvit.py#L310",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),$t=new R({}),Ot=new z({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel">OwlViTTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/configuration_owlvit.py#L34"}}),ke=new dt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Oc]},$$scope:{ctx:y}}}),xt=new R({}),yt=new z({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/configuration_owlvit.py#L142"}}),Pe=new dt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[Vc]},$$scope:{ctx:y}}}),kt=new R({}),zt=new z({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 768"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"crop_size",val:" = 768"},{name:"do_center_crop",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Resize the shorter edge of the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:"<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;",name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>. Desired output size when applying
center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/feature_extraction_owlvit.py#L44"}}),Et=new z({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W) or (H, W, C),
where C is a number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17677/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/feature_extraction_owlvit.py#L134",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17677/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17677/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Me=new oo({props:{warning:!0,$$slots:{default:[xc]},$$scope:{ctx:y}}}),Mt=new R({}),qt=new z({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),Ct=new z({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/processing_owlvit.py#L142"}}),Ft=new z({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/processing_owlvit.py#L149"}}),At=new R({}),Dt=new z({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17677/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L873"}}),Wt=new z({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17677/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L1003",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output</strong> (Tuple<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Le=new oo({props:{$$slots:{default:[yc]},$$scope:{ctx:y}}}),Ae=new dt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[jc]},$$scope:{ctx:y}}}),Nt=new z({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17677/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L908",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),De=new oo({props:{$$slots:{default:[kc]},$$scope:{ctx:y}}}),We=new dt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[zc]},$$scope:{ctx:y}}}),St=new z({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17677/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L953",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new oo({props:{$$slots:{default:[Pc]},$$scope:{ctx:y}}}),Se=new dt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Ec]},$$scope:{ctx:y}}}),Bt=new R({}),Ht=new z({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L712"}}),Rt=new z({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17677/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L727",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17677/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17677/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),He=new oo({props:{$$slots:{default:[Mc]},$$scope:{ctx:y}}}),Re=new dt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[qc]},$$scope:{ctx:y}}}),Ut=new R({}),Gt=new z({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L822"}}),Xt=new z({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17677/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L835",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17677/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17677/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new oo({props:{$$slots:{default:[Cc]},$$scope:{ctx:y}}}),Xe=new dt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Ic]},$$scope:{ctx:y}}}),Zt=new R({}),Jt=new z({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L1154"}}),Kt=new z({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": Tensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17677/src/transformers/models/owlvit/modeling_owlvit.py#L1282",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</li>
<li><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) \u2014 A dictionary containing the individual losses. Useful for logging.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, num_queries)</code>) \u2014 Classification logits (including no-object) for all queries.</li>
<li><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, 4)</code>) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>post_process()</code> to retrieve the unnormalized
bounding boxes.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_size, patch_size, output_dim</code>) \u2014 Pooled output of <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.</li>
<li><strong>class_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.</li>
<li><strong>text_model_last_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_last_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches + 1, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)**2.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new oo({props:{$$slots:{default:[Fc]},$$scope:{ctx:y}}}),Ke=new dt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Lc]},$$scope:{ctx:y}}}),{c(){c=r("meta"),v=f(),g=r("h1"),p=r("a"),w=r("span"),T(s.$$.fragment),m=f(),j=r("span"),Ir=i("OWL-ViT"),Ns=f(),X=r("h2"),Te=r("a"),Ko=r("span"),T(mt.$$.fragment),Fr=f(),Yo=r("span"),Lr=i("Overview"),Ss=f(),be=r("p"),Ar=i("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=r("a"),Dr=i("Simple Open-Vocabulary Object Detection with Vision Transformers"),Wr=i(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Bs=f(),so=r("p"),Nr=i("The abstract from the paper is the following:"),Hs=f(),ro=r("p"),Qo=r("em"),Sr=i("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Rs=f(),Z=r("h2"),$e=r("a"),es=r("span"),T(ht.$$.fragment),Br=f(),ts=r("span"),Hr=i("Usage"),Us=f(),Oe=r("p"),Rr=i("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),no=r("a"),Ur=i("CLIP"),Gr=i(" as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Gs=f(),k=r("p"),ao=r("a"),Xr=i("OwlViTFeatureExtractor"),Zr=i(" can be used to resize (or rescale) and normalize images for the model and "),io=r("a"),Jr=i("CLIPTokenizer"),Kr=i(" is used to encode the text. "),lo=r("a"),Yr=i("OwlViTProcessor"),Qr=i(" wraps "),co=r("a"),en=i("OwlViTFeatureExtractor"),tn=i(" and "),po=r("a"),on=i("CLIPTokenizer"),sn=i(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),mo=r("a"),rn=i("OwlViTProcessor"),nn=i(" and "),fo=r("a"),an=i("OwlViTForObjectDetection"),ln=i("."),Xs=f(),T(gt.$$.fragment),Zs=f(),U=r("p"),cn=i("This model was contributed by "),ut=r("a"),dn=i("adirik"),pn=i(". The original code can be found "),_t=r("a"),mn=i("here"),fn=i("."),Js=f(),J=r("h2"),Ve=r("a"),os=r("span"),T(wt.$$.fragment),hn=f(),ss=r("span"),gn=i("OwlViTConfig"),Ks=f(),M=r("div"),T(vt.$$.fragment),un=f(),xe=r("p"),ho=r("a"),_n=i("OwlViTConfig"),wn=i(" is the configuration class to store the configuration of an "),go=r("a"),vn=i("OwlViTModel"),Tn=i(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),bn=f(),K=r("p"),$n=i("Configuration objects inherit from "),uo=r("a"),On=i("PretrainedConfig"),Vn=i(` and can be used to control the model outputs. Read the
documentation from `),_o=r("a"),xn=i("PretrainedConfig"),yn=i(" for more information."),jn=f(),ye=r("div"),T(Tt.$$.fragment),kn=f(),bt=r("p"),zn=i("Instantiate a "),wo=r("a"),Pn=i("OwlViTConfig"),En=i(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),Ys=f(),Y=r("h2"),je=r("a"),rs=r("span"),T($t.$$.fragment),Mn=f(),ns=r("span"),qn=i("OwlViTTextConfig"),Qs=f(),q=r("div"),T(Ot.$$.fragment),Cn=f(),Q=r("p"),In=i("This is the configuration class to store the configuration of an "),vo=r("a"),Fn=i("OwlViTTextModel"),Ln=i(`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),Vt=r("a"),An=i("google/owlvit-base-patch32"),Dn=i(" architecture."),Wn=f(),ee=r("p"),Nn=i("Configuration objects inherit from "),To=r("a"),Sn=i("PretrainedConfig"),Bn=i(` and can be used to control the model outputs. Read the
documentation from `),bo=r("a"),Hn=i("PretrainedConfig"),Rn=i(" for more information."),Un=f(),T(ke.$$.fragment),er=f(),te=r("h2"),ze=r("a"),as=r("span"),T(xt.$$.fragment),Gn=f(),is=r("span"),Xn=i("OwlViTVisionConfig"),tr=f(),C=r("div"),T(yt.$$.fragment),Zn=f(),oe=r("p"),Jn=i("This is the configuration class to store the configuration of an "),$o=r("a"),Kn=i("OwlViTVisionModel"),Yn=i(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),jt=r("a"),Qn=i("google/owlvit-base-patch32"),ea=i(" architecture."),ta=f(),se=r("p"),oa=i("Configuration objects inherit from "),Oo=r("a"),sa=i("PretrainedConfig"),ra=i(` and can be used to control the model outputs. Read the
documentation from `),Vo=r("a"),na=i("PretrainedConfig"),aa=i(" for more information."),ia=f(),T(Pe.$$.fragment),or=f(),re=r("h2"),Ee=r("a"),ls=r("span"),T(kt.$$.fragment),la=f(),cs=r("span"),ca=i("OwlViTFeatureExtractor"),sr=f(),I=r("div"),T(zt.$$.fragment),da=f(),ds=r("p"),pa=i("Constructs an OWL-ViT feature extractor."),ma=f(),Pt=r("p"),fa=i("This feature extractor inherits from "),xo=r("a"),ha=i("FeatureExtractionMixin"),ga=i(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),ua=f(),G=r("div"),T(Et.$$.fragment),_a=f(),ps=r("p"),wa=i("Main method to prepare for the model one or several image(s)."),va=f(),T(Me.$$.fragment),rr=f(),ne=r("h2"),qe=r("a"),ms=r("span"),T(Mt.$$.fragment),Ta=f(),fs=r("span"),ba=i("OwlViTProcessor"),nr=f(),F=r("div"),T(qt.$$.fragment),$a=f(),P=r("p"),Oa=i("Constructs an OWL-ViT processor which wraps "),yo=r("a"),Va=i("OwlViTFeatureExtractor"),xa=i(" and "),jo=r("a"),ya=i("CLIPTokenizer"),ja=i("/"),ko=r("a"),ka=i("CLIPTokenizerFast"),za=i(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),hs=r("code"),Pa=i("__call__()"),Ea=i(" and "),zo=r("a"),Ma=i("decode()"),qa=i(" for more information."),Ca=f(),Ce=r("div"),T(Ct.$$.fragment),Ia=f(),It=r("p"),Fa=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Po=r("a"),La=i("batch_decode()"),Aa=i(`. Please
refer to the docstring of this method for more information.`),Da=f(),Ie=r("div"),T(Ft.$$.fragment),Wa=f(),Lt=r("p"),Na=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Eo=r("a"),Sa=i("decode()"),Ba=i(`. Please refer to
the docstring of this method for more information.`),ar=f(),ae=r("h2"),Fe=r("a"),gs=r("span"),T(At.$$.fragment),Ha=f(),us=r("span"),Ra=i("OwlViTModel"),ir=f(),L=r("div"),T(Dt.$$.fragment),Ua=f(),A=r("div"),T(Wt.$$.fragment),Ga=f(),ie=r("p"),Xa=i("The "),Mo=r("a"),Za=i("OwlViTModel"),Ja=i(" forward method, overrides the "),_s=r("code"),Ka=i("__call__"),Ya=i(" special method."),Qa=f(),T(Le.$$.fragment),ei=f(),T(Ae.$$.fragment),ti=f(),D=r("div"),T(Nt.$$.fragment),oi=f(),le=r("p"),si=i("The "),qo=r("a"),ri=i("OwlViTModel"),ni=i(" forward method, overrides the "),ws=r("code"),ai=i("__call__"),ii=i(" special method."),li=f(),T(De.$$.fragment),ci=f(),T(We.$$.fragment),di=f(),W=r("div"),T(St.$$.fragment),pi=f(),ce=r("p"),mi=i("The "),Co=r("a"),fi=i("OwlViTModel"),hi=i(" forward method, overrides the "),vs=r("code"),gi=i("__call__"),ui=i(" special method."),_i=f(),T(Ne.$$.fragment),wi=f(),T(Se.$$.fragment),lr=f(),de=r("h2"),Be=r("a"),Ts=r("span"),T(Bt.$$.fragment),vi=f(),bs=r("span"),Ti=i("OwlViTTextModel"),cr=f(),pe=r("div"),T(Ht.$$.fragment),bi=f(),N=r("div"),T(Rt.$$.fragment),$i=f(),me=r("p"),Oi=i("The "),Io=r("a"),Vi=i("OwlViTTextModel"),xi=i(" forward method, overrides the "),$s=r("code"),yi=i("__call__"),ji=i(" special method."),ki=f(),T(He.$$.fragment),zi=f(),T(Re.$$.fragment),dr=f(),fe=r("h2"),Ue=r("a"),Os=r("span"),T(Ut.$$.fragment),Pi=f(),Vs=r("span"),Ei=i("OwlViTVisionModel"),pr=f(),he=r("div"),T(Gt.$$.fragment),Mi=f(),S=r("div"),T(Xt.$$.fragment),qi=f(),ge=r("p"),Ci=i("The "),Fo=r("a"),Ii=i("OwlViTVisionModel"),Fi=i(" forward method, overrides the "),xs=r("code"),Li=i("__call__"),Ai=i(" special method."),Di=f(),T(Ge.$$.fragment),Wi=f(),T(Xe.$$.fragment),mr=f(),ue=r("h2"),Ze=r("a"),ys=r("span"),T(Zt.$$.fragment),Ni=f(),js=r("span"),Si=i("OwlViTForObjectDetection"),fr=f(),_e=r("div"),T(Jt.$$.fragment),Bi=f(),B=r("div"),T(Kt.$$.fragment),Hi=f(),we=r("p"),Ri=i("The "),Lo=r("a"),Ui=i("OwlViTForObjectDetection"),Gi=i(" forward method, overrides the "),ks=r("code"),Xi=i("__call__"),Zi=i(" special method."),Ji=f(),T(Je.$$.fragment),Ki=f(),T(Ke.$$.fragment),this.h()},l(t){const u=bc('[data-svelte="svelte-1phssyn"]',document.head);c=n(u,"META",{name:!0,content:!0}),u.forEach(o),v=h(t),g=n(t,"H1",{class:!0});var Yt=a(g);p=n(Yt,"A",{id:!0,class:!0,href:!0});var zs=a(p);w=n(zs,"SPAN",{});var Ps=a(w);b(s.$$.fragment,Ps),Ps.forEach(o),zs.forEach(o),m=h(Yt),j=n(Yt,"SPAN",{});var Es=a(j);Ir=l(Es,"OWL-ViT"),Es.forEach(o),Yt.forEach(o),Ns=h(t),X=n(t,"H2",{class:!0});var Qt=a(X);Te=n(Qt,"A",{id:!0,class:!0,href:!0});var Ms=a(Te);Ko=n(Ms,"SPAN",{});var qs=a(Ko);b(mt.$$.fragment,qs),qs.forEach(o),Ms.forEach(o),Fr=h(Qt),Yo=n(Qt,"SPAN",{});var Cs=a(Yo);Lr=l(Cs,"Overview"),Cs.forEach(o),Qt.forEach(o),Ss=h(t),be=n(t,"P",{});var eo=a(be);Ar=l(eo,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=n(eo,"A",{href:!0,rel:!0});var Is=a(ft);Dr=l(Is,"Simple Open-Vocabulary Object Detection with Vision Transformers"),Is.forEach(o),Wr=l(eo," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),eo.forEach(o),Bs=h(t),so=n(t,"P",{});var Fs=a(so);Nr=l(Fs,"The abstract from the paper is the following:"),Fs.forEach(o),Hs=h(t),ro=n(t,"P",{});var Ls=a(ro);Qo=n(Ls,"EM",{});var As=a(Qo);Sr=l(As,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),As.forEach(o),Ls.forEach(o),Rs=h(t),Z=n(t,"H2",{class:!0});var to=a(Z);$e=n(to,"A",{id:!0,class:!0,href:!0});var Ds=a($e);es=n(Ds,"SPAN",{});var Yi=a(es);b(ht.$$.fragment,Yi),Yi.forEach(o),Ds.forEach(o),Br=h(to),ts=n(to,"SPAN",{});var Qi=a(ts);Hr=l(Qi,"Usage"),Qi.forEach(o),to.forEach(o),Us=h(t),Oe=n(t,"P",{});var gr=a(Oe);Rr=l(gr,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),no=n(gr,"A",{href:!0});var el=a(no);Ur=l(el,"CLIP"),el.forEach(o),Gr=l(gr," as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),gr.forEach(o),Gs=h(t),k=n(t,"P",{});var E=a(k);ao=n(E,"A",{href:!0});var tl=a(ao);Xr=l(tl,"OwlViTFeatureExtractor"),tl.forEach(o),Zr=l(E," can be used to resize (or rescale) and normalize images for the model and "),io=n(E,"A",{href:!0});var ol=a(io);Jr=l(ol,"CLIPTokenizer"),ol.forEach(o),Kr=l(E," is used to encode the text. "),lo=n(E,"A",{href:!0});var sl=a(lo);Yr=l(sl,"OwlViTProcessor"),sl.forEach(o),Qr=l(E," wraps "),co=n(E,"A",{href:!0});var rl=a(co);en=l(rl,"OwlViTFeatureExtractor"),rl.forEach(o),tn=l(E," and "),po=n(E,"A",{href:!0});var nl=a(po);on=l(nl,"CLIPTokenizer"),nl.forEach(o),sn=l(E," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),mo=n(E,"A",{href:!0});var al=a(mo);rn=l(al,"OwlViTProcessor"),al.forEach(o),nn=l(E," and "),fo=n(E,"A",{href:!0});var il=a(fo);an=l(il,"OwlViTForObjectDetection"),il.forEach(o),ln=l(E,"."),E.forEach(o),Xs=h(t),b(gt.$$.fragment,t),Zs=h(t),U=n(t,"P",{});var Ao=a(U);cn=l(Ao,"This model was contributed by "),ut=n(Ao,"A",{href:!0,rel:!0});var ll=a(ut);dn=l(ll,"adirik"),ll.forEach(o),pn=l(Ao,". The original code can be found "),_t=n(Ao,"A",{href:!0,rel:!0});var cl=a(_t);mn=l(cl,"here"),cl.forEach(o),fn=l(Ao,"."),Ao.forEach(o),Js=h(t),J=n(t,"H2",{class:!0});var ur=a(J);Ve=n(ur,"A",{id:!0,class:!0,href:!0});var dl=a(Ve);os=n(dl,"SPAN",{});var pl=a(os);b(wt.$$.fragment,pl),pl.forEach(o),dl.forEach(o),hn=h(ur),ss=n(ur,"SPAN",{});var ml=a(ss);gn=l(ml,"OwlViTConfig"),ml.forEach(o),ur.forEach(o),Ks=h(t),M=n(t,"DIV",{class:!0});var Ye=a(M);b(vt.$$.fragment,Ye),un=h(Ye),xe=n(Ye,"P",{});var Ws=a(xe);ho=n(Ws,"A",{href:!0});var fl=a(ho);_n=l(fl,"OwlViTConfig"),fl.forEach(o),wn=l(Ws," is the configuration class to store the configuration of an "),go=n(Ws,"A",{href:!0});var hl=a(go);vn=l(hl,"OwlViTModel"),hl.forEach(o),Tn=l(Ws,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),Ws.forEach(o),bn=h(Ye),K=n(Ye,"P",{});var Do=a(K);$n=l(Do,"Configuration objects inherit from "),uo=n(Do,"A",{href:!0});var gl=a(uo);On=l(gl,"PretrainedConfig"),gl.forEach(o),Vn=l(Do,` and can be used to control the model outputs. Read the
documentation from `),_o=n(Do,"A",{href:!0});var ul=a(_o);xn=l(ul,"PretrainedConfig"),ul.forEach(o),yn=l(Do," for more information."),Do.forEach(o),jn=h(Ye),ye=n(Ye,"DIV",{class:!0});var _r=a(ye);b(Tt.$$.fragment,_r),kn=h(_r),bt=n(_r,"P",{});var wr=a(bt);zn=l(wr,"Instantiate a "),wo=n(wr,"A",{href:!0});var _l=a(wo);Pn=l(_l,"OwlViTConfig"),_l.forEach(o),En=l(wr,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),wr.forEach(o),_r.forEach(o),Ye.forEach(o),Ys=h(t),Y=n(t,"H2",{class:!0});var vr=a(Y);je=n(vr,"A",{id:!0,class:!0,href:!0});var wl=a(je);rs=n(wl,"SPAN",{});var vl=a(rs);b($t.$$.fragment,vl),vl.forEach(o),wl.forEach(o),Mn=h(vr),ns=n(vr,"SPAN",{});var Tl=a(ns);qn=l(Tl,"OwlViTTextConfig"),Tl.forEach(o),vr.forEach(o),Qs=h(t),q=n(t,"DIV",{class:!0});var Qe=a(q);b(Ot.$$.fragment,Qe),Cn=h(Qe),Q=n(Qe,"P",{});var Wo=a(Q);In=l(Wo,"This is the configuration class to store the configuration of an "),vo=n(Wo,"A",{href:!0});var bl=a(vo);Fn=l(bl,"OwlViTTextModel"),bl.forEach(o),Ln=l(Wo,`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),Vt=n(Wo,"A",{href:!0,rel:!0});var $l=a(Vt);An=l($l,"google/owlvit-base-patch32"),$l.forEach(o),Dn=l(Wo," architecture."),Wo.forEach(o),Wn=h(Qe),ee=n(Qe,"P",{});var No=a(ee);Nn=l(No,"Configuration objects inherit from "),To=n(No,"A",{href:!0});var Ol=a(To);Sn=l(Ol,"PretrainedConfig"),Ol.forEach(o),Bn=l(No,` and can be used to control the model outputs. Read the
documentation from `),bo=n(No,"A",{href:!0});var Vl=a(bo);Hn=l(Vl,"PretrainedConfig"),Vl.forEach(o),Rn=l(No," for more information."),No.forEach(o),Un=h(Qe),b(ke.$$.fragment,Qe),Qe.forEach(o),er=h(t),te=n(t,"H2",{class:!0});var Tr=a(te);ze=n(Tr,"A",{id:!0,class:!0,href:!0});var xl=a(ze);as=n(xl,"SPAN",{});var yl=a(as);b(xt.$$.fragment,yl),yl.forEach(o),xl.forEach(o),Gn=h(Tr),is=n(Tr,"SPAN",{});var jl=a(is);Xn=l(jl,"OwlViTVisionConfig"),jl.forEach(o),Tr.forEach(o),tr=h(t),C=n(t,"DIV",{class:!0});var et=a(C);b(yt.$$.fragment,et),Zn=h(et),oe=n(et,"P",{});var So=a(oe);Jn=l(So,"This is the configuration class to store the configuration of an "),$o=n(So,"A",{href:!0});var kl=a($o);Kn=l(kl,"OwlViTVisionModel"),kl.forEach(o),Yn=l(So,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),jt=n(So,"A",{href:!0,rel:!0});var zl=a(jt);Qn=l(zl,"google/owlvit-base-patch32"),zl.forEach(o),ea=l(So," architecture."),So.forEach(o),ta=h(et),se=n(et,"P",{});var Bo=a(se);oa=l(Bo,"Configuration objects inherit from "),Oo=n(Bo,"A",{href:!0});var Pl=a(Oo);sa=l(Pl,"PretrainedConfig"),Pl.forEach(o),ra=l(Bo,` and can be used to control the model outputs. Read the
documentation from `),Vo=n(Bo,"A",{href:!0});var El=a(Vo);na=l(El,"PretrainedConfig"),El.forEach(o),aa=l(Bo," for more information."),Bo.forEach(o),ia=h(et),b(Pe.$$.fragment,et),et.forEach(o),or=h(t),re=n(t,"H2",{class:!0});var br=a(re);Ee=n(br,"A",{id:!0,class:!0,href:!0});var Ml=a(Ee);ls=n(Ml,"SPAN",{});var ql=a(ls);b(kt.$$.fragment,ql),ql.forEach(o),Ml.forEach(o),la=h(br),cs=n(br,"SPAN",{});var Cl=a(cs);ca=l(Cl,"OwlViTFeatureExtractor"),Cl.forEach(o),br.forEach(o),sr=h(t),I=n(t,"DIV",{class:!0});var tt=a(I);b(zt.$$.fragment,tt),da=h(tt),ds=n(tt,"P",{});var Il=a(ds);pa=l(Il,"Constructs an OWL-ViT feature extractor."),Il.forEach(o),ma=h(tt),Pt=n(tt,"P",{});var $r=a(Pt);fa=l($r,"This feature extractor inherits from "),xo=n($r,"A",{href:!0});var Fl=a(xo);ha=l(Fl,"FeatureExtractionMixin"),Fl.forEach(o),ga=l($r,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),$r.forEach(o),ua=h(tt),G=n(tt,"DIV",{class:!0});var Ho=a(G);b(Et.$$.fragment,Ho),_a=h(Ho),ps=n(Ho,"P",{});var Ll=a(ps);wa=l(Ll,"Main method to prepare for the model one or several image(s)."),Ll.forEach(o),va=h(Ho),b(Me.$$.fragment,Ho),Ho.forEach(o),tt.forEach(o),rr=h(t),ne=n(t,"H2",{class:!0});var Or=a(ne);qe=n(Or,"A",{id:!0,class:!0,href:!0});var Al=a(qe);ms=n(Al,"SPAN",{});var Dl=a(ms);b(Mt.$$.fragment,Dl),Dl.forEach(o),Al.forEach(o),Ta=h(Or),fs=n(Or,"SPAN",{});var Wl=a(fs);ba=l(Wl,"OwlViTProcessor"),Wl.forEach(o),Or.forEach(o),nr=h(t),F=n(t,"DIV",{class:!0});var ot=a(F);b(qt.$$.fragment,ot),$a=h(ot),P=n(ot,"P",{});var H=a(P);Oa=l(H,"Constructs an OWL-ViT processor which wraps "),yo=n(H,"A",{href:!0});var Nl=a(yo);Va=l(Nl,"OwlViTFeatureExtractor"),Nl.forEach(o),xa=l(H," and "),jo=n(H,"A",{href:!0});var Sl=a(jo);ya=l(Sl,"CLIPTokenizer"),Sl.forEach(o),ja=l(H,"/"),ko=n(H,"A",{href:!0});var Bl=a(ko);ka=l(Bl,"CLIPTokenizerFast"),Bl.forEach(o),za=l(H,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),hs=n(H,"CODE",{});var Hl=a(hs);Pa=l(Hl,"__call__()"),Hl.forEach(o),Ea=l(H," and "),zo=n(H,"A",{href:!0});var Rl=a(zo);Ma=l(Rl,"decode()"),Rl.forEach(o),qa=l(H," for more information."),H.forEach(o),Ca=h(ot),Ce=n(ot,"DIV",{class:!0});var Vr=a(Ce);b(Ct.$$.fragment,Vr),Ia=h(Vr),It=n(Vr,"P",{});var xr=a(It);Fa=l(xr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Po=n(xr,"A",{href:!0});var Ul=a(Po);La=l(Ul,"batch_decode()"),Ul.forEach(o),Aa=l(xr,`. Please
refer to the docstring of this method for more information.`),xr.forEach(o),Vr.forEach(o),Da=h(ot),Ie=n(ot,"DIV",{class:!0});var yr=a(Ie);b(Ft.$$.fragment,yr),Wa=h(yr),Lt=n(yr,"P",{});var jr=a(Lt);Na=l(jr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Eo=n(jr,"A",{href:!0});var Gl=a(Eo);Sa=l(Gl,"decode()"),Gl.forEach(o),Ba=l(jr,`. Please refer to
the docstring of this method for more information.`),jr.forEach(o),yr.forEach(o),ot.forEach(o),ar=h(t),ae=n(t,"H2",{class:!0});var kr=a(ae);Fe=n(kr,"A",{id:!0,class:!0,href:!0});var Xl=a(Fe);gs=n(Xl,"SPAN",{});var Zl=a(gs);b(At.$$.fragment,Zl),Zl.forEach(o),Xl.forEach(o),Ha=h(kr),us=n(kr,"SPAN",{});var Jl=a(us);Ra=l(Jl,"OwlViTModel"),Jl.forEach(o),kr.forEach(o),ir=h(t),L=n(t,"DIV",{class:!0});var st=a(L);b(Dt.$$.fragment,st),Ua=h(st),A=n(st,"DIV",{class:!0});var rt=a(A);b(Wt.$$.fragment,rt),Ga=h(rt),ie=n(rt,"P",{});var Ro=a(ie);Xa=l(Ro,"The "),Mo=n(Ro,"A",{href:!0});var Kl=a(Mo);Za=l(Kl,"OwlViTModel"),Kl.forEach(o),Ja=l(Ro," forward method, overrides the "),_s=n(Ro,"CODE",{});var Yl=a(_s);Ka=l(Yl,"__call__"),Yl.forEach(o),Ya=l(Ro," special method."),Ro.forEach(o),Qa=h(rt),b(Le.$$.fragment,rt),ei=h(rt),b(Ae.$$.fragment,rt),rt.forEach(o),ti=h(st),D=n(st,"DIV",{class:!0});var nt=a(D);b(Nt.$$.fragment,nt),oi=h(nt),le=n(nt,"P",{});var Uo=a(le);si=l(Uo,"The "),qo=n(Uo,"A",{href:!0});var Ql=a(qo);ri=l(Ql,"OwlViTModel"),Ql.forEach(o),ni=l(Uo," forward method, overrides the "),ws=n(Uo,"CODE",{});var ec=a(ws);ai=l(ec,"__call__"),ec.forEach(o),ii=l(Uo," special method."),Uo.forEach(o),li=h(nt),b(De.$$.fragment,nt),ci=h(nt),b(We.$$.fragment,nt),nt.forEach(o),di=h(st),W=n(st,"DIV",{class:!0});var at=a(W);b(St.$$.fragment,at),pi=h(at),ce=n(at,"P",{});var Go=a(ce);mi=l(Go,"The "),Co=n(Go,"A",{href:!0});var tc=a(Co);fi=l(tc,"OwlViTModel"),tc.forEach(o),hi=l(Go," forward method, overrides the "),vs=n(Go,"CODE",{});var oc=a(vs);gi=l(oc,"__call__"),oc.forEach(o),ui=l(Go," special method."),Go.forEach(o),_i=h(at),b(Ne.$$.fragment,at),wi=h(at),b(Se.$$.fragment,at),at.forEach(o),st.forEach(o),lr=h(t),de=n(t,"H2",{class:!0});var zr=a(de);Be=n(zr,"A",{id:!0,class:!0,href:!0});var sc=a(Be);Ts=n(sc,"SPAN",{});var rc=a(Ts);b(Bt.$$.fragment,rc),rc.forEach(o),sc.forEach(o),vi=h(zr),bs=n(zr,"SPAN",{});var nc=a(bs);Ti=l(nc,"OwlViTTextModel"),nc.forEach(o),zr.forEach(o),cr=h(t),pe=n(t,"DIV",{class:!0});var Pr=a(pe);b(Ht.$$.fragment,Pr),bi=h(Pr),N=n(Pr,"DIV",{class:!0});var it=a(N);b(Rt.$$.fragment,it),$i=h(it),me=n(it,"P",{});var Xo=a(me);Oi=l(Xo,"The "),Io=n(Xo,"A",{href:!0});var ac=a(Io);Vi=l(ac,"OwlViTTextModel"),ac.forEach(o),xi=l(Xo," forward method, overrides the "),$s=n(Xo,"CODE",{});var ic=a($s);yi=l(ic,"__call__"),ic.forEach(o),ji=l(Xo," special method."),Xo.forEach(o),ki=h(it),b(He.$$.fragment,it),zi=h(it),b(Re.$$.fragment,it),it.forEach(o),Pr.forEach(o),dr=h(t),fe=n(t,"H2",{class:!0});var Er=a(fe);Ue=n(Er,"A",{id:!0,class:!0,href:!0});var lc=a(Ue);Os=n(lc,"SPAN",{});var cc=a(Os);b(Ut.$$.fragment,cc),cc.forEach(o),lc.forEach(o),Pi=h(Er),Vs=n(Er,"SPAN",{});var dc=a(Vs);Ei=l(dc,"OwlViTVisionModel"),dc.forEach(o),Er.forEach(o),pr=h(t),he=n(t,"DIV",{class:!0});var Mr=a(he);b(Gt.$$.fragment,Mr),Mi=h(Mr),S=n(Mr,"DIV",{class:!0});var lt=a(S);b(Xt.$$.fragment,lt),qi=h(lt),ge=n(lt,"P",{});var Zo=a(ge);Ci=l(Zo,"The "),Fo=n(Zo,"A",{href:!0});var pc=a(Fo);Ii=l(pc,"OwlViTVisionModel"),pc.forEach(o),Fi=l(Zo," forward method, overrides the "),xs=n(Zo,"CODE",{});var mc=a(xs);Li=l(mc,"__call__"),mc.forEach(o),Ai=l(Zo," special method."),Zo.forEach(o),Di=h(lt),b(Ge.$$.fragment,lt),Wi=h(lt),b(Xe.$$.fragment,lt),lt.forEach(o),Mr.forEach(o),mr=h(t),ue=n(t,"H2",{class:!0});var qr=a(ue);Ze=n(qr,"A",{id:!0,class:!0,href:!0});var fc=a(Ze);ys=n(fc,"SPAN",{});var hc=a(ys);b(Zt.$$.fragment,hc),hc.forEach(o),fc.forEach(o),Ni=h(qr),js=n(qr,"SPAN",{});var gc=a(js);Si=l(gc,"OwlViTForObjectDetection"),gc.forEach(o),qr.forEach(o),fr=h(t),_e=n(t,"DIV",{class:!0});var Cr=a(_e);b(Jt.$$.fragment,Cr),Bi=h(Cr),B=n(Cr,"DIV",{class:!0});var ct=a(B);b(Kt.$$.fragment,ct),Hi=h(ct),we=n(ct,"P",{});var Jo=a(we);Ri=l(Jo,"The "),Lo=n(Jo,"A",{href:!0});var uc=a(Lo);Ui=l(uc,"OwlViTForObjectDetection"),uc.forEach(o),Gi=l(Jo," forward method, overrides the "),ks=n(Jo,"CODE",{});var _c=a(ks);Xi=l(_c,"__call__"),_c.forEach(o),Zi=l(Jo," special method."),Jo.forEach(o),Ji=h(ct),b(Je.$$.fragment,ct),Ki=h(ct),b(Ke.$$.fragment,ct),ct.forEach(o),Cr.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(Dc)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(g,"class","relative group"),d(Te,"id","overview"),d(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Te,"href","#overview"),d(X,"class","relative group"),d(ft,"href","https://arxiv.org/abs/2205.06230"),d(ft,"rel","nofollow"),d($e,"id","usage"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#usage"),d(Z,"class","relative group"),d(no,"href","clip"),d(ao,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(io,"href","/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer"),d(lo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(co,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(po,"href","/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer"),d(mo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(fo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(ut,"href","https://huggingface.co/adirik"),d(ut,"rel","nofollow"),d(_t,"href","https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit"),d(_t,"rel","nofollow"),d(Ve,"id","transformers.OwlViTConfig"),d(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ve,"href","#transformers.OwlViTConfig"),d(J,"class","relative group"),d(ho,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTConfig"),d(go,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTModel"),d(uo,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(_o,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(wo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTConfig"),d(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"id","transformers.OwlViTTextConfig"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.OwlViTTextConfig"),d(Y,"class","relative group"),d(vo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(Vt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(Vt,"rel","nofollow"),d(To,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(bo,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ze,"id","transformers.OwlViTVisionConfig"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#transformers.OwlViTVisionConfig"),d(te,"class","relative group"),d($o,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(jt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(jt,"rel","nofollow"),d(Oo,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(Vo,"href","/docs/transformers/pr_17677/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ee,"id","transformers.OwlViTFeatureExtractor"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#transformers.OwlViTFeatureExtractor"),d(re,"class","relative group"),d(xo,"href","/docs/transformers/pr_17677/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.OwlViTProcessor"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.OwlViTProcessor"),d(ne,"class","relative group"),d(yo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(jo,"href","/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizer"),d(ko,"href","/docs/transformers/pr_17677/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(zo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(Po,"href","/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode"),d(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Eo,"href","/docs/transformers/pr_17677/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fe,"id","transformers.OwlViTModel"),d(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fe,"href","#transformers.OwlViTModel"),d(ae,"class","relative group"),d(Mo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTModel"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Co,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Be,"id","transformers.OwlViTTextModel"),d(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Be,"href","#transformers.OwlViTTextModel"),d(de,"class","relative group"),d(Io,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ue,"id","transformers.OwlViTVisionModel"),d(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ue,"href","#transformers.OwlViTVisionModel"),d(fe,"class","relative group"),d(Fo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ze,"id","transformers.OwlViTForObjectDetection"),d(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ze,"href","#transformers.OwlViTForObjectDetection"),d(ue,"class","relative group"),d(Lo,"href","/docs/transformers/pr_17677/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,c),_(t,v,u),_(t,g,u),e(g,p),e(p,w),$(s,w,null),e(g,m),e(g,j),e(j,Ir),_(t,Ns,u),_(t,X,u),e(X,Te),e(Te,Ko),$(mt,Ko,null),e(X,Fr),e(X,Yo),e(Yo,Lr),_(t,Ss,u),_(t,be,u),e(be,Ar),e(be,ft),e(ft,Dr),e(be,Wr),_(t,Bs,u),_(t,so,u),e(so,Nr),_(t,Hs,u),_(t,ro,u),e(ro,Qo),e(Qo,Sr),_(t,Rs,u),_(t,Z,u),e(Z,$e),e($e,es),$(ht,es,null),e(Z,Br),e(Z,ts),e(ts,Hr),_(t,Us,u),_(t,Oe,u),e(Oe,Rr),e(Oe,no),e(no,Ur),e(Oe,Gr),_(t,Gs,u),_(t,k,u),e(k,ao),e(ao,Xr),e(k,Zr),e(k,io),e(io,Jr),e(k,Kr),e(k,lo),e(lo,Yr),e(k,Qr),e(k,co),e(co,en),e(k,tn),e(k,po),e(po,on),e(k,sn),e(k,mo),e(mo,rn),e(k,nn),e(k,fo),e(fo,an),e(k,ln),_(t,Xs,u),$(gt,t,u),_(t,Zs,u),_(t,U,u),e(U,cn),e(U,ut),e(ut,dn),e(U,pn),e(U,_t),e(_t,mn),e(U,fn),_(t,Js,u),_(t,J,u),e(J,Ve),e(Ve,os),$(wt,os,null),e(J,hn),e(J,ss),e(ss,gn),_(t,Ks,u),_(t,M,u),$(vt,M,null),e(M,un),e(M,xe),e(xe,ho),e(ho,_n),e(xe,wn),e(xe,go),e(go,vn),e(xe,Tn),e(M,bn),e(M,K),e(K,$n),e(K,uo),e(uo,On),e(K,Vn),e(K,_o),e(_o,xn),e(K,yn),e(M,jn),e(M,ye),$(Tt,ye,null),e(ye,kn),e(ye,bt),e(bt,zn),e(bt,wo),e(wo,Pn),e(bt,En),_(t,Ys,u),_(t,Y,u),e(Y,je),e(je,rs),$($t,rs,null),e(Y,Mn),e(Y,ns),e(ns,qn),_(t,Qs,u),_(t,q,u),$(Ot,q,null),e(q,Cn),e(q,Q),e(Q,In),e(Q,vo),e(vo,Fn),e(Q,Ln),e(Q,Vt),e(Vt,An),e(Q,Dn),e(q,Wn),e(q,ee),e(ee,Nn),e(ee,To),e(To,Sn),e(ee,Bn),e(ee,bo),e(bo,Hn),e(ee,Rn),e(q,Un),$(ke,q,null),_(t,er,u),_(t,te,u),e(te,ze),e(ze,as),$(xt,as,null),e(te,Gn),e(te,is),e(is,Xn),_(t,tr,u),_(t,C,u),$(yt,C,null),e(C,Zn),e(C,oe),e(oe,Jn),e(oe,$o),e($o,Kn),e(oe,Yn),e(oe,jt),e(jt,Qn),e(oe,ea),e(C,ta),e(C,se),e(se,oa),e(se,Oo),e(Oo,sa),e(se,ra),e(se,Vo),e(Vo,na),e(se,aa),e(C,ia),$(Pe,C,null),_(t,or,u),_(t,re,u),e(re,Ee),e(Ee,ls),$(kt,ls,null),e(re,la),e(re,cs),e(cs,ca),_(t,sr,u),_(t,I,u),$(zt,I,null),e(I,da),e(I,ds),e(ds,pa),e(I,ma),e(I,Pt),e(Pt,fa),e(Pt,xo),e(xo,ha),e(Pt,ga),e(I,ua),e(I,G),$(Et,G,null),e(G,_a),e(G,ps),e(ps,wa),e(G,va),$(Me,G,null),_(t,rr,u),_(t,ne,u),e(ne,qe),e(qe,ms),$(Mt,ms,null),e(ne,Ta),e(ne,fs),e(fs,ba),_(t,nr,u),_(t,F,u),$(qt,F,null),e(F,$a),e(F,P),e(P,Oa),e(P,yo),e(yo,Va),e(P,xa),e(P,jo),e(jo,ya),e(P,ja),e(P,ko),e(ko,ka),e(P,za),e(P,hs),e(hs,Pa),e(P,Ea),e(P,zo),e(zo,Ma),e(P,qa),e(F,Ca),e(F,Ce),$(Ct,Ce,null),e(Ce,Ia),e(Ce,It),e(It,Fa),e(It,Po),e(Po,La),e(It,Aa),e(F,Da),e(F,Ie),$(Ft,Ie,null),e(Ie,Wa),e(Ie,Lt),e(Lt,Na),e(Lt,Eo),e(Eo,Sa),e(Lt,Ba),_(t,ar,u),_(t,ae,u),e(ae,Fe),e(Fe,gs),$(At,gs,null),e(ae,Ha),e(ae,us),e(us,Ra),_(t,ir,u),_(t,L,u),$(Dt,L,null),e(L,Ua),e(L,A),$(Wt,A,null),e(A,Ga),e(A,ie),e(ie,Xa),e(ie,Mo),e(Mo,Za),e(ie,Ja),e(ie,_s),e(_s,Ka),e(ie,Ya),e(A,Qa),$(Le,A,null),e(A,ei),$(Ae,A,null),e(L,ti),e(L,D),$(Nt,D,null),e(D,oi),e(D,le),e(le,si),e(le,qo),e(qo,ri),e(le,ni),e(le,ws),e(ws,ai),e(le,ii),e(D,li),$(De,D,null),e(D,ci),$(We,D,null),e(L,di),e(L,W),$(St,W,null),e(W,pi),e(W,ce),e(ce,mi),e(ce,Co),e(Co,fi),e(ce,hi),e(ce,vs),e(vs,gi),e(ce,ui),e(W,_i),$(Ne,W,null),e(W,wi),$(Se,W,null),_(t,lr,u),_(t,de,u),e(de,Be),e(Be,Ts),$(Bt,Ts,null),e(de,vi),e(de,bs),e(bs,Ti),_(t,cr,u),_(t,pe,u),$(Ht,pe,null),e(pe,bi),e(pe,N),$(Rt,N,null),e(N,$i),e(N,me),e(me,Oi),e(me,Io),e(Io,Vi),e(me,xi),e(me,$s),e($s,yi),e(me,ji),e(N,ki),$(He,N,null),e(N,zi),$(Re,N,null),_(t,dr,u),_(t,fe,u),e(fe,Ue),e(Ue,Os),$(Ut,Os,null),e(fe,Pi),e(fe,Vs),e(Vs,Ei),_(t,pr,u),_(t,he,u),$(Gt,he,null),e(he,Mi),e(he,S),$(Xt,S,null),e(S,qi),e(S,ge),e(ge,Ci),e(ge,Fo),e(Fo,Ii),e(ge,Fi),e(ge,xs),e(xs,Li),e(ge,Ai),e(S,Di),$(Ge,S,null),e(S,Wi),$(Xe,S,null),_(t,mr,u),_(t,ue,u),e(ue,Ze),e(Ze,ys),$(Zt,ys,null),e(ue,Ni),e(ue,js),e(js,Si),_(t,fr,u),_(t,_e,u),$(Jt,_e,null),e(_e,Bi),e(_e,B),$(Kt,B,null),e(B,Hi),e(B,we),e(we,Ri),e(we,Lo),e(Lo,Ui),e(we,Gi),e(we,ks),e(ks,Xi),e(we,Zi),e(B,Ji),$(Je,B,null),e(B,Ki),$(Ke,B,null),hr=!0},p(t,[u]){const Yt={};u&2&&(Yt.$$scope={dirty:u,ctx:t}),ke.$set(Yt);const zs={};u&2&&(zs.$$scope={dirty:u,ctx:t}),Pe.$set(zs);const Ps={};u&2&&(Ps.$$scope={dirty:u,ctx:t}),Me.$set(Ps);const Es={};u&2&&(Es.$$scope={dirty:u,ctx:t}),Le.$set(Es);const Qt={};u&2&&(Qt.$$scope={dirty:u,ctx:t}),Ae.$set(Qt);const Ms={};u&2&&(Ms.$$scope={dirty:u,ctx:t}),De.$set(Ms);const qs={};u&2&&(qs.$$scope={dirty:u,ctx:t}),We.$set(qs);const Cs={};u&2&&(Cs.$$scope={dirty:u,ctx:t}),Ne.$set(Cs);const eo={};u&2&&(eo.$$scope={dirty:u,ctx:t}),Se.$set(eo);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:t}),He.$set(Is);const Fs={};u&2&&(Fs.$$scope={dirty:u,ctx:t}),Re.$set(Fs);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:t}),Ge.$set(Ls);const As={};u&2&&(As.$$scope={dirty:u,ctx:t}),Xe.$set(As);const to={};u&2&&(to.$$scope={dirty:u,ctx:t}),Je.$set(to);const Ds={};u&2&&(Ds.$$scope={dirty:u,ctx:t}),Ke.$set(Ds)},i(t){hr||(O(s.$$.fragment,t),O(mt.$$.fragment,t),O(ht.$$.fragment,t),O(gt.$$.fragment,t),O(wt.$$.fragment,t),O(vt.$$.fragment,t),O(Tt.$$.fragment,t),O($t.$$.fragment,t),O(Ot.$$.fragment,t),O(ke.$$.fragment,t),O(xt.$$.fragment,t),O(yt.$$.fragment,t),O(Pe.$$.fragment,t),O(kt.$$.fragment,t),O(zt.$$.fragment,t),O(Et.$$.fragment,t),O(Me.$$.fragment,t),O(Mt.$$.fragment,t),O(qt.$$.fragment,t),O(Ct.$$.fragment,t),O(Ft.$$.fragment,t),O(At.$$.fragment,t),O(Dt.$$.fragment,t),O(Wt.$$.fragment,t),O(Le.$$.fragment,t),O(Ae.$$.fragment,t),O(Nt.$$.fragment,t),O(De.$$.fragment,t),O(We.$$.fragment,t),O(St.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Bt.$$.fragment,t),O(Ht.$$.fragment,t),O(Rt.$$.fragment,t),O(He.$$.fragment,t),O(Re.$$.fragment,t),O(Ut.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Ge.$$.fragment,t),O(Xe.$$.fragment,t),O(Zt.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Je.$$.fragment,t),O(Ke.$$.fragment,t),hr=!0)},o(t){V(s.$$.fragment,t),V(mt.$$.fragment,t),V(ht.$$.fragment,t),V(gt.$$.fragment,t),V(wt.$$.fragment,t),V(vt.$$.fragment,t),V(Tt.$$.fragment,t),V($t.$$.fragment,t),V(Ot.$$.fragment,t),V(ke.$$.fragment,t),V(xt.$$.fragment,t),V(yt.$$.fragment,t),V(Pe.$$.fragment,t),V(kt.$$.fragment,t),V(zt.$$.fragment,t),V(Et.$$.fragment,t),V(Me.$$.fragment,t),V(Mt.$$.fragment,t),V(qt.$$.fragment,t),V(Ct.$$.fragment,t),V(Ft.$$.fragment,t),V(At.$$.fragment,t),V(Dt.$$.fragment,t),V(Wt.$$.fragment,t),V(Le.$$.fragment,t),V(Ae.$$.fragment,t),V(Nt.$$.fragment,t),V(De.$$.fragment,t),V(We.$$.fragment,t),V(St.$$.fragment,t),V(Ne.$$.fragment,t),V(Se.$$.fragment,t),V(Bt.$$.fragment,t),V(Ht.$$.fragment,t),V(Rt.$$.fragment,t),V(He.$$.fragment,t),V(Re.$$.fragment,t),V(Ut.$$.fragment,t),V(Gt.$$.fragment,t),V(Xt.$$.fragment,t),V(Ge.$$.fragment,t),V(Xe.$$.fragment,t),V(Zt.$$.fragment,t),V(Jt.$$.fragment,t),V(Kt.$$.fragment,t),V(Je.$$.fragment,t),V(Ke.$$.fragment,t),hr=!1},d(t){o(c),t&&o(v),t&&o(g),x(s),t&&o(Ns),t&&o(X),x(mt),t&&o(Ss),t&&o(be),t&&o(Bs),t&&o(so),t&&o(Hs),t&&o(ro),t&&o(Rs),t&&o(Z),x(ht),t&&o(Us),t&&o(Oe),t&&o(Gs),t&&o(k),t&&o(Xs),x(gt,t),t&&o(Zs),t&&o(U),t&&o(Js),t&&o(J),x(wt),t&&o(Ks),t&&o(M),x(vt),x(Tt),t&&o(Ys),t&&o(Y),x($t),t&&o(Qs),t&&o(q),x(Ot),x(ke),t&&o(er),t&&o(te),x(xt),t&&o(tr),t&&o(C),x(yt),x(Pe),t&&o(or),t&&o(re),x(kt),t&&o(sr),t&&o(I),x(zt),x(Et),x(Me),t&&o(rr),t&&o(ne),x(Mt),t&&o(nr),t&&o(F),x(qt),x(Ct),x(Ft),t&&o(ar),t&&o(ae),x(At),t&&o(ir),t&&o(L),x(Dt),x(Wt),x(Le),x(Ae),x(Nt),x(De),x(We),x(St),x(Ne),x(Se),t&&o(lr),t&&o(de),x(Bt),t&&o(cr),t&&o(pe),x(Ht),x(Rt),x(He),x(Re),t&&o(dr),t&&o(fe),x(Ut),t&&o(pr),t&&o(he),x(Gt),x(Xt),x(Ge),x(Xe),t&&o(mr),t&&o(ue),x(Zt),t&&o(fr),t&&o(_e),x(Jt),x(Kt),x(Je),x(Ke)}}}const Dc={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function Wc(y){return $c(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gc extends wc{constructor(c){super();vc(this,c,Wc,Ac,Tc,{})}}export{Gc as default,Dc as metadata};
