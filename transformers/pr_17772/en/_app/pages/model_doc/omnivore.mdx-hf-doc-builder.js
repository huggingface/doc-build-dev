import{S as Ja,i as Xa,s as Za,e as n,k as f,w,t as r,M as Qa,c as s,d as t,m as h,a as i,x as $,h as a,b as d,G as e,g as u,y as O,q as x,o as E,B as y,v as Ya,L as Qr}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ka}from"../../chunks/Tip-hf-doc-builder.js";import{D as ao}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Yr}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as no}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Zr}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function en(M){let m,b,v,p,_;return p=new Yr({props:{code:`from transformers import OmnivoreModel, OmnivoreConfig

# Initializing an Omnivore omnivore-tiny-224 style configuration
configuration = OmnivoreConfig()
# Initializing a model from the omnivore-tiny-224 style configuration
model = OmnivoreModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreModel, OmnivoreConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing an Omnivore omnivore-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OmnivoreConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the omnivore-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Qr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){E(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),y(p,l)}}}function on(M){let m,b,v,p,_;return{c(){m=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),p=r("Module"),_=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(g,"CODE",{});var I=i(v);p=a(I,"Module"),I.forEach(t),_=a(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(l,g){u(l,m,g),e(m,b),e(m,v),e(v,p),e(m,_)},d(l){l&&t(m)}}}function tn(M){let m,b,v,p,_;return p=new Yr({props:{code:`from transformers import OmnivoreFeatureExtractor, OmnivoreModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = OmnivoreFeatureExtractor.from_pretrained("anugunj/omnivore-swinT")
model = OmnivoreModel.from_pretrained("anugunj/omnivore-swinT")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreFeatureExtractor, OmnivoreModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = OmnivoreFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreModel.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Qr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){E(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),y(p,l)}}}function rn(M){let m,b,v,p,_;return{c(){m=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),p=r("Module"),_=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(g,"CODE",{});var I=i(v);p=a(I,"Module"),I.forEach(t),_=a(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(l,g){u(l,m,g),e(m,b),e(m,v),e(v,p),e(m,_)},d(l){l&&t(m)}}}function an(M){let m,b,v,p,_;return p=new Yr({props:{code:`from transformers import OmnivoreFeatureExtractor, OmnivoreForVisionClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = OmnivoreFeatureExtractor.from_pretrained("anugunj/omnivore-swinT")
model = OmnivoreForVisionClassification.from_pretrained("anugunj/omnivore-swinT")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreFeatureExtractor, OmnivoreForVisionClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = OmnivoreFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreForVisionClassification.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Qr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){E(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),y(p,l)}}}function nn(M){let m,b,v,p,_,l,g,I,rt,zo,D,J,so,he,at,io,nt,Ao,X,st,pe,it,lt,Io,Se,dt,Po,Le,lo,mt,No,De,ct,qo,F,P,ft,Ve,ht,pt,ue,ut,gt,ge,vt,_t,bt,T,wt,Be,$t,Ot,Re,xt,Et,mo,yt,Tt,co,kt,Ct,fo,jt,Mt,ho,Ft,zt,At,ve,It,Ue,Pt,Nt,qt,N,St,po,Lt,Dt,uo,Vt,Bt,go,Rt,Ut,So,L,Wt,_e,Gt,Ht,be,Kt,Jt,Lo,V,Z,vo,we,Xt,_o,Zt,Do,C,$e,Qt,B,Yt,We,er,or,Oe,tr,rr,ar,R,nr,Ge,sr,ir,He,lr,dr,mr,Q,Vo,U,Y,bo,xe,cr,wo,fr,Bo,q,Ee,hr,$o,pr,ur,ye,gr,Ke,vr,_r,Ro,W,ee,Oo,Te,br,xo,wr,Uo,S,ke,$r,Ce,Or,je,xr,Er,yr,z,Me,Tr,G,kr,Je,Cr,jr,Eo,Mr,Fr,zr,oe,Ar,te,Wo,H,re,yo,Fe,Ir,To,Pr,Go,j,ze,Nr,ko,qr,Sr,Ae,Lr,Ie,Dr,Vr,Br,A,Pe,Rr,K,Ur,Xe,Wr,Gr,Co,Hr,Kr,Jr,ae,Xr,ne,Ho;return l=new no({}),he=new no({}),we=new no({}),$e=new ao({props:{name:"class transformers.OmnivoreConfig",anchor:"transformers.OmnivoreConfig",parameters:[{name:"num_image_labels",val:" = 1000"},{name:"num_video_labels",val:" = 400"},{name:"num_rgbd_labels",val:" = 19"},{name:"input_channels",val:" = 3"},{name:"patch_size",val:" = [2, 4, 4]"},{name:"embed_dim",val:" = 96"},{name:"depths",val:" = [2, 2, 6, 2]"},{name:"num_heads",val:" = [3, 6, 12, 24]"},{name:"window_size",val:" = [8, 7, 7]"},{name:"mlp_ratio",val:" = 4.0"},{name:"qkv_bias",val:" = True"},{name:"qk_scale",val:" = None"},{name:"dropout_rate",val:" = 0.0"},{name:"attention_dropout_rate",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.2"},{name:"patch_norm",val:" = True"},{name:"frozen_stages",val:" = -1"},{name:"depth_mode",val:" = 'summed_rgb_d_tokens'"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreConfig.num_image_labels",description:`<strong>num_image_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 1000) &#x2014;
The number of labels for image head.`,name:"num_image_labels"},{anchor:"transformers.OmnivoreConfig.num_video_labels",description:`<strong>num_video_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 400) &#x2014;
The number of labels for video head.`,name:"num_video_labels"},{anchor:"transformers.OmnivoreConfig.num_rgbd_labels",description:`<strong>num_rgbd_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 19) &#x2014;
The number of labels for rgbd head.`,name:"num_rgbd_labels"},{anchor:"transformers.OmnivoreConfig.input_channels",description:`<strong>input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"input_channels"},{anchor:"transformers.OmnivoreConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code> | <code>List[int]</code>, <em>optional</em>, defaults to [4, 4, 4]) &#x2014;
Patch size to use in the patch embedding layer.`,name:"patch_size"},{anchor:"transformers.OmnivoreConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Number of linear projection output channels.`,name:"embed_dim"},{anchor:"transformers.OmnivoreConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [2, 2, 6, 2],) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.OmnivoreConfig.num_heads",description:`<strong>num_heads</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 6, 12, 24]</code>) &#x2014;
Number of attention head of each stage.`,name:"num_heads"},{anchor:"transformers.OmnivoreConfig.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Size of the window used by the Swin Transformer model.`,name:"window_size"},{anchor:"transformers.OmnivoreConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the
encoder blocks.`,name:"mlp_ratios"},{anchor:"transformers.OmnivoreConfig.attention_dropout_rate",description:`<strong>attention_dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout_rate"},{anchor:"transformers.OmnivoreConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the patch embeddings probabilities and projections in attention.`,name:"dropout_rate"},{anchor:"transformers.OmnivoreConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.0, 0.1]</code>) &#x2014;
The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.`,name:"drop_path_rate"},{anchor:"transformers.OmnivoreConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
The bias bool for query, key and value in attentions`,name:"qkv_bias"},{anchor:"transformers.OmnivoreConfig.qk_scale",description:`<strong>qk_scale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Override default qk scale of head_dim ** -0.5 if set.`,name:"qk_scale"},{anchor:"transformers.OmnivoreConfig.norm_layer",description:`<strong>norm_layer</strong> (<code>nn.Module</code>, <em>optional</em>, defaults to <code>nn.LayerNorm</code>) &#x2014;
Normalization layer for the model`,name:"norm_layer"},{anchor:"transformers.OmnivoreConfig.patch_norm",description:`<strong>patch_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;
If True, add normalization after patch embedding.`,name:"patch_norm"},{anchor:"transformers.OmnivoreConfig.frozen_stages",description:`<strong>frozen_stages</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
Stages to be frozen (stop grad and set eval mode) -1 means not freezing any parameters.`,name:"frozen_stages"},{anchor:"transformers.OmnivoreConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/configuration_omnivore.py#L28"}}),Q=new Zr({props:{anchor:"transformers.OmnivoreConfig.example",$$slots:{default:[en]},$$scope:{ctx:M}}}),xe=new no({}),Ee=new ao({props:{name:"class transformers.OmnivoreFeatureExtractor",anchor:"transformers.OmnivoreFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 224"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = [0.485, 0.456, 0.406]"},{name:"image_std",val:" = [0.229, 0.224, 0.225]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shortest edge of the input to int(256/224 *<code>size</code>).`,name:"do_resize"},{anchor:"transformers.OmnivoreFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple(int)</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an
integer is provided, then shorter side of input will be resized to &#x2018;size&#x2019;.`,name:"size"},{anchor:"transformers.OmnivoreFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OmnivoreFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to center crop the input to <code>size</code>.`,name:"do_center_crop"},{anchor:"transformers.OmnivoreFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with mean and standard deviation.`,name:"do_normalize"},{anchor:"transformers.OmnivoreFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OmnivoreFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/feature_extraction_omnivore.py#L36"}}),Te=new no({}),ke=new ao({props:{name:"class transformers.OmnivoreModel",anchor:"transformers.OmnivoreModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig">OmnivoreConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17772/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L826"}}),Me=new ao({props:{name:"forward",anchor:"transformers.OmnivoreModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OmnivoreModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, time, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17772/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.OmnivoreModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OmnivoreModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17772/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L833",returnDescription:`
<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig"
>OmnivoreConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new Ka({props:{$$slots:{default:[on]},$$scope:{ctx:M}}}),te=new Zr({props:{anchor:"transformers.OmnivoreModel.forward.example",$$slots:{default:[tn]},$$scope:{ctx:M}}}),Fe=new no({}),ze=new ao({props:{name:"class transformers.OmnivoreForVisionClassification",anchor:"transformers.OmnivoreForVisionClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreForVisionClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig">OmnivoreConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17772/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L878"}}),Pe=new ao({props:{name:"forward",anchor:"transformers.OmnivoreForVisionClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"input_type",val:": str = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OmnivoreForVisionClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, time, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17772/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.OmnivoreForVisionClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OmnivoreForVisionClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17772/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.OmnivoreForVisionClassification.forward.input_type",description:`<strong>input_type</strong> (<code>str</code>) &#x2014;
Which classification head to use for the classification of given pixel_values`,name:"input_type"},{anchor:"transformers.OmnivoreForVisionClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p>
<p>Example &#x2014;`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L893",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17772/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig"
>OmnivoreConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17772/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ae=new Ka({props:{$$slots:{default:[rn]},$$scope:{ctx:M}}}),ne=new Zr({props:{anchor:"transformers.OmnivoreForVisionClassification.forward.example",$$slots:{default:[an]},$$scope:{ctx:M}}}),{c(){m=n("meta"),b=f(),v=n("h1"),p=n("a"),_=n("span"),w(l.$$.fragment),g=f(),I=n("span"),rt=r("Omnivore"),zo=f(),D=n("h2"),J=n("a"),so=n("span"),w(he.$$.fragment),at=f(),io=n("span"),nt=r("Overview"),Ao=f(),X=n("p"),st=r("The Omnivore Model was proposed in "),pe=n("a"),it=r("Omnivore: A Single Model for Many Visual Modalities"),lt=r(` by Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra.
The main idea here is generalization of vision transformer model across different modalities. The authors propose a single model
for multiple classification tasks such as image classification, video classification and 3D scence classification.`),Io=f(),Se=n("p"),dt=r("The abstract from the paper is the following:"),Po=f(),Le=n("p"),lo=n("em"),mt=r(`Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images,
videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view
3D data using exactly the same model parameters. Our \u2018Omnivore\u2019 model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard
datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on
ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision
tasks and generalize across modalities. Omnivore\u2019s shared visual representation naturally enables cross-modal recognition without
access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.`),No=f(),De=n("p"),ct=r("Tips:"),qo=f(),F=n("ul"),P=n("li"),ft=r("All released checkpoints were pre-trained and fine-tuned on three datasets "),Ve=n("a"),ht=r("ImageNet-1k"),pt=r(", "),ue=n("a"),ut=r("Kinetics"),gt=r(" and "),ge=n("a"),vt=r("SUN RGB-D"),_t=r(` respectively.
There are five checkpoints available are trained on said dataset. In single epoch training for the model it was trained on one epoch each for ImageNet-1k and Kinetics with ten epochs for SUN RGB-D.`),bt=f(),T=n("li"),wt=r("The authors of Omnivore released 5 trained Omnivore models, which you can directly plug into "),Be=n("a"),$t=r("OmnivoreModel"),Ot=r(" or "),Re=n("a"),xt=r("OmnivoreForVisionClassification"),Et=r(`.
The 5 variants available are (all trained on images of size 224x224, video of 32 frames of size 224x224 and RGBD images of size 224x224):
`),mo=n("em"),yt=r("facebook/omnivore-swinT"),Tt=r(", "),co=n("em"),kt=r("facebook/omnivore-swinS**, "),Ct=r("facebook/omnivore-swinB"),fo=n("em"),jt=r(", "),Mt=r("facebook/omnivore-swinB-in21k"),ho=n("em"),Ft=r(`and
`),zt=r("facebook/omnivore-swinL-in21k*."),At=f(),ve=n("li"),It=r("Note that one should use "),Ue=n("a"),Pt=r("OmnivoreFeatureExtractor"),Nt=r(" in order to prepare images, videos and RGBD images for the model."),qt=f(),N=n("li"),St=r("The input to the model needs to be provided with a corresponding type, i.e. "),po=n("em"),Lt=r("\u201Cimages\u201D"),Dt=r(", "),uo=n("em"),Vt=r("\u201Cvideos\u201D"),Bt=r(" or "),go=n("em"),Rt=r("\u201Crgbd\u201D"),Ut=r(" in order to use the classification head for that modality."),So=f(),L=n("p"),Wt=r("This model was contributed by "),_e=n("a"),Gt=r("anugunj"),Ht=r(". The original code can be found "),be=n("a"),Kt=r("here"),Jt=r("."),Lo=f(),V=n("h2"),Z=n("a"),vo=n("span"),w(we.$$.fragment),Xt=f(),_o=n("span"),Zt=r("OmnivoreConfig"),Do=f(),C=n("div"),w($e.$$.fragment),Qt=f(),B=n("p"),Yt=r("This is the configuration class to store the configuration of an "),We=n("a"),er=r("OmnivoreModel"),or=r(`. It is used to instantiate an
Omnivore model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Omnivore
`),Oe=n("a"),tr=r("anugunj/omnivore"),rr=r(" architecture."),ar=f(),R=n("p"),nr=r("Configuration objects inherit from "),Ge=n("a"),sr=r("PretrainedConfig"),ir=r(` and can be used to control the model outputs. Read the
documentation from `),He=n("a"),lr=r("PretrainedConfig"),dr=r(" for more information."),mr=f(),w(Q.$$.fragment),Vo=f(),U=n("h2"),Y=n("a"),bo=n("span"),w(xe.$$.fragment),cr=f(),wo=n("span"),fr=r("OmnivoreFeatureExtractor"),Bo=f(),q=n("div"),w(Ee.$$.fragment),hr=f(),$o=n("p"),pr=r("Constructs an Omnivore feature extractor."),ur=f(),ye=n("p"),gr=r("This feature extractor inherits from "),Ke=n("a"),vr=r("FeatureExtractionMixin"),_r=r(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Ro=f(),W=n("h2"),ee=n("a"),Oo=n("span"),w(Te.$$.fragment),br=f(),xo=n("span"),wr=r("OmnivoreModel"),Uo=f(),S=n("div"),w(ke.$$.fragment),$r=f(),Ce=n("p"),Or=r(`The bare Omnivore model outputting raw features without any specific head on top.
This model is a PyTorch `),je=n("a"),xr=r("torch.nn.Module"),Er=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),yr=f(),z=n("div"),w(Me.$$.fragment),Tr=f(),G=n("p"),kr=r("The "),Je=n("a"),Cr=r("OmnivoreModel"),jr=r(" forward method, overrides the "),Eo=n("code"),Mr=r("__call__"),Fr=r(" special method."),zr=f(),w(oe.$$.fragment),Ar=f(),w(te.$$.fragment),Wo=f(),H=n("h2"),re=n("a"),yo=n("span"),w(Fe.$$.fragment),Ir=f(),To=n("span"),Pr=r("OmnivoreForVisionClassification"),Go=f(),j=n("div"),w(ze.$$.fragment),Nr=f(),ko=n("p"),qr=r(`Omnivore Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Sr=f(),Ae=n("p"),Lr=r("This model is a PyTorch "),Ie=n("a"),Dr=r("torch.nn.Module"),Vr=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Br=f(),A=n("div"),w(Pe.$$.fragment),Rr=f(),K=n("p"),Ur=r("The "),Xe=n("a"),Wr=r("OmnivoreForVisionClassification"),Gr=r(" forward method, overrides the "),Co=n("code"),Hr=r("__call__"),Kr=r(" special method."),Jr=f(),w(ae.$$.fragment),Xr=f(),w(ne.$$.fragment),this.h()},l(o){const c=Qa('[data-svelte="svelte-1phssyn"]',document.head);m=s(c,"META",{name:!0,content:!0}),c.forEach(t),b=h(o),v=s(o,"H1",{class:!0});var Ne=i(v);p=s(Ne,"A",{id:!0,class:!0,href:!0});var jo=i(p);_=s(jo,"SPAN",{});var Mo=i(_);$(l.$$.fragment,Mo),Mo.forEach(t),jo.forEach(t),g=h(Ne),I=s(Ne,"SPAN",{});var Fo=i(I);rt=a(Fo,"Omnivore"),Fo.forEach(t),Ne.forEach(t),zo=h(o),D=s(o,"H2",{class:!0});var qe=i(D);J=s(qe,"A",{id:!0,class:!0,href:!0});var ea=i(J);so=s(ea,"SPAN",{});var oa=i(so);$(he.$$.fragment,oa),oa.forEach(t),ea.forEach(t),at=h(qe),io=s(qe,"SPAN",{});var ta=i(io);nt=a(ta,"Overview"),ta.forEach(t),qe.forEach(t),Ao=h(o),X=s(o,"P",{});var Ko=i(X);st=a(Ko,"The Omnivore Model was proposed in "),pe=s(Ko,"A",{href:!0,rel:!0});var ra=i(pe);it=a(ra,"Omnivore: A Single Model for Many Visual Modalities"),ra.forEach(t),lt=a(Ko,` by Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra.
The main idea here is generalization of vision transformer model across different modalities. The authors propose a single model
for multiple classification tasks such as image classification, video classification and 3D scence classification.`),Ko.forEach(t),Io=h(o),Se=s(o,"P",{});var aa=i(Se);dt=a(aa,"The abstract from the paper is the following:"),aa.forEach(t),Po=h(o),Le=s(o,"P",{});var na=i(Le);lo=s(na,"EM",{});var sa=i(lo);mt=a(sa,`Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images,
videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view
3D data using exactly the same model parameters. Our \u2018Omnivore\u2019 model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard
datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on
ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision
tasks and generalize across modalities. Omnivore\u2019s shared visual representation naturally enables cross-modal recognition without
access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.`),sa.forEach(t),na.forEach(t),No=h(o),De=s(o,"P",{});var ia=i(De);ct=a(ia,"Tips:"),ia.forEach(t),qo=h(o),F=s(o,"UL",{});var se=i(F);P=s(se,"LI",{});var ie=i(P);ft=a(ie,"All released checkpoints were pre-trained and fine-tuned on three datasets "),Ve=s(ie,"A",{href:!0});var la=i(Ve);ht=a(la,"ImageNet-1k"),la.forEach(t),pt=a(ie,", "),ue=s(ie,"A",{href:!0,rel:!0});var da=i(ue);ut=a(da,"Kinetics"),da.forEach(t),gt=a(ie," and "),ge=s(ie,"A",{href:!0,rel:!0});var ma=i(ge);vt=a(ma,"SUN RGB-D"),ma.forEach(t),_t=a(ie,` respectively.
There are five checkpoints available are trained on said dataset. In single epoch training for the model it was trained on one epoch each for ImageNet-1k and Kinetics with ten epochs for SUN RGB-D.`),ie.forEach(t),bt=h(se),T=s(se,"LI",{});var k=i(T);wt=a(k,"The authors of Omnivore released 5 trained Omnivore models, which you can directly plug into "),Be=s(k,"A",{href:!0});var ca=i(Be);$t=a(ca,"OmnivoreModel"),ca.forEach(t),Ot=a(k," or "),Re=s(k,"A",{href:!0});var fa=i(Re);xt=a(fa,"OmnivoreForVisionClassification"),fa.forEach(t),Et=a(k,`.
The 5 variants available are (all trained on images of size 224x224, video of 32 frames of size 224x224 and RGBD images of size 224x224):
`),mo=s(k,"EM",{});var ha=i(mo);yt=a(ha,"facebook/omnivore-swinT"),ha.forEach(t),Tt=a(k,", "),co=s(k,"EM",{});var pa=i(co);kt=a(pa,"facebook/omnivore-swinS**, "),pa.forEach(t),Ct=a(k,"facebook/omnivore-swinB"),fo=s(k,"EM",{});var ua=i(fo);jt=a(ua,", "),ua.forEach(t),Mt=a(k,"facebook/omnivore-swinB-in21k"),ho=s(k,"EM",{});var ga=i(ho);Ft=a(ga,`and
`),ga.forEach(t),zt=a(k,"facebook/omnivore-swinL-in21k*."),k.forEach(t),At=h(se),ve=s(se,"LI",{});var Jo=i(ve);It=a(Jo,"Note that one should use "),Ue=s(Jo,"A",{href:!0});var va=i(Ue);Pt=a(va,"OmnivoreFeatureExtractor"),va.forEach(t),Nt=a(Jo," in order to prepare images, videos and RGBD images for the model."),Jo.forEach(t),qt=h(se),N=s(se,"LI",{});var le=i(N);St=a(le,"The input to the model needs to be provided with a corresponding type, i.e. "),po=s(le,"EM",{});var _a=i(po);Lt=a(_a,"\u201Cimages\u201D"),_a.forEach(t),Dt=a(le,", "),uo=s(le,"EM",{});var ba=i(uo);Vt=a(ba,"\u201Cvideos\u201D"),ba.forEach(t),Bt=a(le," or "),go=s(le,"EM",{});var wa=i(go);Rt=a(wa,"\u201Crgbd\u201D"),wa.forEach(t),Ut=a(le," in order to use the classification head for that modality."),le.forEach(t),se.forEach(t),So=h(o),L=s(o,"P",{});var Ze=i(L);Wt=a(Ze,"This model was contributed by "),_e=s(Ze,"A",{href:!0,rel:!0});var $a=i(_e);Gt=a($a,"anugunj"),$a.forEach(t),Ht=a(Ze,". The original code can be found "),be=s(Ze,"A",{href:!0,rel:!0});var Oa=i(be);Kt=a(Oa,"here"),Oa.forEach(t),Jt=a(Ze,"."),Ze.forEach(t),Lo=h(o),V=s(o,"H2",{class:!0});var Xo=i(V);Z=s(Xo,"A",{id:!0,class:!0,href:!0});var xa=i(Z);vo=s(xa,"SPAN",{});var Ea=i(vo);$(we.$$.fragment,Ea),Ea.forEach(t),xa.forEach(t),Xt=h(Xo),_o=s(Xo,"SPAN",{});var ya=i(_o);Zt=a(ya,"OmnivoreConfig"),ya.forEach(t),Xo.forEach(t),Do=h(o),C=s(o,"DIV",{class:!0});var de=i(C);$($e.$$.fragment,de),Qt=h(de),B=s(de,"P",{});var Qe=i(B);Yt=a(Qe,"This is the configuration class to store the configuration of an "),We=s(Qe,"A",{href:!0});var Ta=i(We);er=a(Ta,"OmnivoreModel"),Ta.forEach(t),or=a(Qe,`. It is used to instantiate an
Omnivore model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Omnivore
`),Oe=s(Qe,"A",{href:!0,rel:!0});var ka=i(Oe);tr=a(ka,"anugunj/omnivore"),ka.forEach(t),rr=a(Qe," architecture."),Qe.forEach(t),ar=h(de),R=s(de,"P",{});var Ye=i(R);nr=a(Ye,"Configuration objects inherit from "),Ge=s(Ye,"A",{href:!0});var Ca=i(Ge);sr=a(Ca,"PretrainedConfig"),Ca.forEach(t),ir=a(Ye,` and can be used to control the model outputs. Read the
documentation from `),He=s(Ye,"A",{href:!0});var ja=i(He);lr=a(ja,"PretrainedConfig"),ja.forEach(t),dr=a(Ye," for more information."),Ye.forEach(t),mr=h(de),$(Q.$$.fragment,de),de.forEach(t),Vo=h(o),U=s(o,"H2",{class:!0});var Zo=i(U);Y=s(Zo,"A",{id:!0,class:!0,href:!0});var Ma=i(Y);bo=s(Ma,"SPAN",{});var Fa=i(bo);$(xe.$$.fragment,Fa),Fa.forEach(t),Ma.forEach(t),cr=h(Zo),wo=s(Zo,"SPAN",{});var za=i(wo);fr=a(za,"OmnivoreFeatureExtractor"),za.forEach(t),Zo.forEach(t),Bo=h(o),q=s(o,"DIV",{class:!0});var eo=i(q);$(Ee.$$.fragment,eo),hr=h(eo),$o=s(eo,"P",{});var Aa=i($o);pr=a(Aa,"Constructs an Omnivore feature extractor."),Aa.forEach(t),ur=h(eo),ye=s(eo,"P",{});var Qo=i(ye);gr=a(Qo,"This feature extractor inherits from "),Ke=s(Qo,"A",{href:!0});var Ia=i(Ke);vr=a(Ia,"FeatureExtractionMixin"),Ia.forEach(t),_r=a(Qo,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Qo.forEach(t),eo.forEach(t),Ro=h(o),W=s(o,"H2",{class:!0});var Yo=i(W);ee=s(Yo,"A",{id:!0,class:!0,href:!0});var Pa=i(ee);Oo=s(Pa,"SPAN",{});var Na=i(Oo);$(Te.$$.fragment,Na),Na.forEach(t),Pa.forEach(t),br=h(Yo),xo=s(Yo,"SPAN",{});var qa=i(xo);wr=a(qa,"OmnivoreModel"),qa.forEach(t),Yo.forEach(t),Uo=h(o),S=s(o,"DIV",{class:!0});var oo=i(S);$(ke.$$.fragment,oo),$r=h(oo),Ce=s(oo,"P",{});var et=i(Ce);Or=a(et,`The bare Omnivore model outputting raw features without any specific head on top.
This model is a PyTorch `),je=s(et,"A",{href:!0,rel:!0});var Sa=i(je);xr=a(Sa,"torch.nn.Module"),Sa.forEach(t),Er=a(et,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),et.forEach(t),yr=h(oo),z=s(oo,"DIV",{class:!0});var me=i(z);$(Me.$$.fragment,me),Tr=h(me),G=s(me,"P",{});var to=i(G);kr=a(to,"The "),Je=s(to,"A",{href:!0});var La=i(Je);Cr=a(La,"OmnivoreModel"),La.forEach(t),jr=a(to," forward method, overrides the "),Eo=s(to,"CODE",{});var Da=i(Eo);Mr=a(Da,"__call__"),Da.forEach(t),Fr=a(to," special method."),to.forEach(t),zr=h(me),$(oe.$$.fragment,me),Ar=h(me),$(te.$$.fragment,me),me.forEach(t),oo.forEach(t),Wo=h(o),H=s(o,"H2",{class:!0});var ot=i(H);re=s(ot,"A",{id:!0,class:!0,href:!0});var Va=i(re);yo=s(Va,"SPAN",{});var Ba=i(yo);$(Fe.$$.fragment,Ba),Ba.forEach(t),Va.forEach(t),Ir=h(ot),To=s(ot,"SPAN",{});var Ra=i(To);Pr=a(Ra,"OmnivoreForVisionClassification"),Ra.forEach(t),ot.forEach(t),Go=h(o),j=s(o,"DIV",{class:!0});var ce=i(j);$(ze.$$.fragment,ce),Nr=h(ce),ko=s(ce,"P",{});var Ua=i(ko);qr=a(Ua,`Omnivore Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ua.forEach(t),Sr=h(ce),Ae=s(ce,"P",{});var tt=i(Ae);Lr=a(tt,"This model is a PyTorch "),Ie=s(tt,"A",{href:!0,rel:!0});var Wa=i(Ie);Dr=a(Wa,"torch.nn.Module"),Wa.forEach(t),Vr=a(tt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),tt.forEach(t),Br=h(ce),A=s(ce,"DIV",{class:!0});var fe=i(A);$(Pe.$$.fragment,fe),Rr=h(fe),K=s(fe,"P",{});var ro=i(K);Ur=a(ro,"The "),Xe=s(ro,"A",{href:!0});var Ga=i(Xe);Wr=a(Ga,"OmnivoreForVisionClassification"),Ga.forEach(t),Gr=a(ro," forward method, overrides the "),Co=s(ro,"CODE",{});var Ha=i(Co);Hr=a(Ha,"__call__"),Ha.forEach(t),Kr=a(ro," special method."),ro.forEach(t),Jr=h(fe),$(ae.$$.fragment,fe),Xr=h(fe),$(ne.$$.fragment,fe),fe.forEach(t),ce.forEach(t),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(sn)),d(p,"id","omnivore"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#omnivore"),d(v,"class","relative group"),d(J,"id","overview"),d(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J,"href","#overview"),d(D,"class","relative group"),d(pe,"href","https://arxiv.org/abs/2201.08377"),d(pe,"rel","nofollow"),d(Ve,"href","(https://huggingface.co/datasets/imagenet-1k)"),d(ue,"href","https://www.deepmind.com/open-source/kinetics"),d(ue,"rel","nofollow"),d(ge,"href","https://rgbd.cs.princeton.edu/"),d(ge,"rel","nofollow"),d(Be,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(Re,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreForVisionClassification"),d(Ue,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreFeatureExtractor"),d(_e,"href","https://huggingface.co/anugunj"),d(_e,"rel","nofollow"),d(be,"href","https://github.com/facebookresearch/omnivore"),d(be,"rel","nofollow"),d(Z,"id","transformers.OmnivoreConfig"),d(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Z,"href","#transformers.OmnivoreConfig"),d(V,"class","relative group"),d(We,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(Oe,"href","https://huggingface.co/anugunj/omnivore"),d(Oe,"rel","nofollow"),d(Ge,"href","/docs/transformers/pr_17772/en/main_classes/configuration#transformers.PretrainedConfig"),d(He,"href","/docs/transformers/pr_17772/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"id","transformers.OmnivoreFeatureExtractor"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#transformers.OmnivoreFeatureExtractor"),d(U,"class","relative group"),d(Ke,"href","/docs/transformers/pr_17772/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ee,"id","transformers.OmnivoreModel"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#transformers.OmnivoreModel"),d(W,"class","relative group"),d(je,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(je,"rel","nofollow"),d(Je,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(re,"id","transformers.OmnivoreForVisionClassification"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#transformers.OmnivoreForVisionClassification"),d(H,"class","relative group"),d(Ie,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ie,"rel","nofollow"),d(Xe,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreForVisionClassification"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,c){e(document.head,m),u(o,b,c),u(o,v,c),e(v,p),e(p,_),O(l,_,null),e(v,g),e(v,I),e(I,rt),u(o,zo,c),u(o,D,c),e(D,J),e(J,so),O(he,so,null),e(D,at),e(D,io),e(io,nt),u(o,Ao,c),u(o,X,c),e(X,st),e(X,pe),e(pe,it),e(X,lt),u(o,Io,c),u(o,Se,c),e(Se,dt),u(o,Po,c),u(o,Le,c),e(Le,lo),e(lo,mt),u(o,No,c),u(o,De,c),e(De,ct),u(o,qo,c),u(o,F,c),e(F,P),e(P,ft),e(P,Ve),e(Ve,ht),e(P,pt),e(P,ue),e(ue,ut),e(P,gt),e(P,ge),e(ge,vt),e(P,_t),e(F,bt),e(F,T),e(T,wt),e(T,Be),e(Be,$t),e(T,Ot),e(T,Re),e(Re,xt),e(T,Et),e(T,mo),e(mo,yt),e(T,Tt),e(T,co),e(co,kt),e(T,Ct),e(T,fo),e(fo,jt),e(T,Mt),e(T,ho),e(ho,Ft),e(T,zt),e(F,At),e(F,ve),e(ve,It),e(ve,Ue),e(Ue,Pt),e(ve,Nt),e(F,qt),e(F,N),e(N,St),e(N,po),e(po,Lt),e(N,Dt),e(N,uo),e(uo,Vt),e(N,Bt),e(N,go),e(go,Rt),e(N,Ut),u(o,So,c),u(o,L,c),e(L,Wt),e(L,_e),e(_e,Gt),e(L,Ht),e(L,be),e(be,Kt),e(L,Jt),u(o,Lo,c),u(o,V,c),e(V,Z),e(Z,vo),O(we,vo,null),e(V,Xt),e(V,_o),e(_o,Zt),u(o,Do,c),u(o,C,c),O($e,C,null),e(C,Qt),e(C,B),e(B,Yt),e(B,We),e(We,er),e(B,or),e(B,Oe),e(Oe,tr),e(B,rr),e(C,ar),e(C,R),e(R,nr),e(R,Ge),e(Ge,sr),e(R,ir),e(R,He),e(He,lr),e(R,dr),e(C,mr),O(Q,C,null),u(o,Vo,c),u(o,U,c),e(U,Y),e(Y,bo),O(xe,bo,null),e(U,cr),e(U,wo),e(wo,fr),u(o,Bo,c),u(o,q,c),O(Ee,q,null),e(q,hr),e(q,$o),e($o,pr),e(q,ur),e(q,ye),e(ye,gr),e(ye,Ke),e(Ke,vr),e(ye,_r),u(o,Ro,c),u(o,W,c),e(W,ee),e(ee,Oo),O(Te,Oo,null),e(W,br),e(W,xo),e(xo,wr),u(o,Uo,c),u(o,S,c),O(ke,S,null),e(S,$r),e(S,Ce),e(Ce,Or),e(Ce,je),e(je,xr),e(Ce,Er),e(S,yr),e(S,z),O(Me,z,null),e(z,Tr),e(z,G),e(G,kr),e(G,Je),e(Je,Cr),e(G,jr),e(G,Eo),e(Eo,Mr),e(G,Fr),e(z,zr),O(oe,z,null),e(z,Ar),O(te,z,null),u(o,Wo,c),u(o,H,c),e(H,re),e(re,yo),O(Fe,yo,null),e(H,Ir),e(H,To),e(To,Pr),u(o,Go,c),u(o,j,c),O(ze,j,null),e(j,Nr),e(j,ko),e(ko,qr),e(j,Sr),e(j,Ae),e(Ae,Lr),e(Ae,Ie),e(Ie,Dr),e(Ae,Vr),e(j,Br),e(j,A),O(Pe,A,null),e(A,Rr),e(A,K),e(K,Ur),e(K,Xe),e(Xe,Wr),e(K,Gr),e(K,Co),e(Co,Hr),e(K,Kr),e(A,Jr),O(ae,A,null),e(A,Xr),O(ne,A,null),Ho=!0},p(o,[c]){const Ne={};c&2&&(Ne.$$scope={dirty:c,ctx:o}),Q.$set(Ne);const jo={};c&2&&(jo.$$scope={dirty:c,ctx:o}),oe.$set(jo);const Mo={};c&2&&(Mo.$$scope={dirty:c,ctx:o}),te.$set(Mo);const Fo={};c&2&&(Fo.$$scope={dirty:c,ctx:o}),ae.$set(Fo);const qe={};c&2&&(qe.$$scope={dirty:c,ctx:o}),ne.$set(qe)},i(o){Ho||(x(l.$$.fragment,o),x(he.$$.fragment,o),x(we.$$.fragment,o),x($e.$$.fragment,o),x(Q.$$.fragment,o),x(xe.$$.fragment,o),x(Ee.$$.fragment,o),x(Te.$$.fragment,o),x(ke.$$.fragment,o),x(Me.$$.fragment,o),x(oe.$$.fragment,o),x(te.$$.fragment,o),x(Fe.$$.fragment,o),x(ze.$$.fragment,o),x(Pe.$$.fragment,o),x(ae.$$.fragment,o),x(ne.$$.fragment,o),Ho=!0)},o(o){E(l.$$.fragment,o),E(he.$$.fragment,o),E(we.$$.fragment,o),E($e.$$.fragment,o),E(Q.$$.fragment,o),E(xe.$$.fragment,o),E(Ee.$$.fragment,o),E(Te.$$.fragment,o),E(ke.$$.fragment,o),E(Me.$$.fragment,o),E(oe.$$.fragment,o),E(te.$$.fragment,o),E(Fe.$$.fragment,o),E(ze.$$.fragment,o),E(Pe.$$.fragment,o),E(ae.$$.fragment,o),E(ne.$$.fragment,o),Ho=!1},d(o){t(m),o&&t(b),o&&t(v),y(l),o&&t(zo),o&&t(D),y(he),o&&t(Ao),o&&t(X),o&&t(Io),o&&t(Se),o&&t(Po),o&&t(Le),o&&t(No),o&&t(De),o&&t(qo),o&&t(F),o&&t(So),o&&t(L),o&&t(Lo),o&&t(V),y(we),o&&t(Do),o&&t(C),y($e),y(Q),o&&t(Vo),o&&t(U),y(xe),o&&t(Bo),o&&t(q),y(Ee),o&&t(Ro),o&&t(W),y(Te),o&&t(Uo),o&&t(S),y(ke),y(Me),y(oe),y(te),o&&t(Wo),o&&t(H),y(Fe),o&&t(Go),o&&t(j),y(ze),y(Pe),y(ae),y(ne)}}}const sn={local:"omnivore",sections:[{local:"overview",title:"Overview"},{local:"transformers.OmnivoreConfig",title:"OmnivoreConfig"},{local:"transformers.OmnivoreFeatureExtractor",title:"OmnivoreFeatureExtractor"},{local:"transformers.OmnivoreModel",title:"OmnivoreModel"},{local:"transformers.OmnivoreForVisionClassification",title:"OmnivoreForVisionClassification"}],title:"Omnivore"};function ln(M){return Ya(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class un extends Ja{constructor(m){super();Xa(this,m,ln,nn,Za,{})}}export{un as default,sn as metadata};
