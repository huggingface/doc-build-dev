import{S as Wa,i as Ga,s as Ha,e as n,k as f,w,t as r,M as Ka,c as s,d as t,m as h,a as i,x as $,h as a,b as d,G as e,g as u,y as O,q as x,o as y,B as E,v as Ja,L as Kr}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ua}from"../../chunks/Tip-hf-doc-builder.js";import{D as ro}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Jr}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ao}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Hr}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Xa(M){let m,b,v,p,_;return p=new Jr({props:{code:`from transformers import OmnivoreModel, OmnivoreConfig

# Initializing a Omnivore omnivore-tiny-224 style configuration
configuration = OmnivoreConfig()
# Initializing a model from the omnivore-tiny-224 style configuration
model = OmnivoreModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreModel, OmnivoreConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Omnivore omnivore-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OmnivoreConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the omnivore-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Kr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){y(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),E(p,l)}}}function Za(M){let m,b,v,p,_;return{c(){m=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),p=r("Module"),_=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(g,"CODE",{});var A=i(v);p=a(A,"Module"),A.forEach(t),_=a(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(l,g){u(l,m,g),e(m,b),e(m,v),e(v,p),e(m,_)},d(l){l&&t(m)}}}function Qa(M){let m,b,v,p,_;return p=new Jr({props:{code:`from transformers import OmnivoreFeatureExtractor, OmnivoreModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = OmnivoreFeatureExtractor.from_pretrained("anugunj/omnivore-swinT")
model = OmnivoreModel.from_pretrained("anugunj/omnivore-swinT")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreFeatureExtractor, OmnivoreModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = OmnivoreFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreModel.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Kr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){y(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),E(p,l)}}}function Ya(M){let m,b,v,p,_;return{c(){m=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),p=r("Module"),_=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(g,"CODE",{});var A=i(v);p=a(A,"Module"),A.forEach(t),_=a(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(l,g){u(l,m,g),e(m,b),e(m,v),e(v,p),e(m,_)},d(l){l&&t(m)}}}function en(M){let m,b,v,p,_;return p=new Jr({props:{code:`from transformers import OmnivoreFeatureExtractor, OmnivoreForVisionClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = OmnivoreFeatureExtractor.from_pretrained("anugunj/omnivore-swinT")
model = OmnivoreForVisionClassification.from_pretrained("anugunj/omnivore-swinT")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OmnivoreFeatureExtractor, OmnivoreForVisionClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = OmnivoreFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OmnivoreForVisionClassification.from_pretrained(<span class="hljs-string">&quot;anugunj/omnivore-swinT&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){m=n("p"),b=r("Example:"),v=f(),w(p.$$.fragment)},l(l){m=s(l,"P",{});var g=i(m);b=a(g,"Example:"),g.forEach(t),v=h(l),$(p.$$.fragment,l)},m(l,g){u(l,m,g),e(m,b),u(l,v,g),O(p,l,g),_=!0},p:Kr,i(l){_||(x(p.$$.fragment,l),_=!0)},o(l){y(p.$$.fragment,l),_=!1},d(l){l&&t(m),l&&t(v),E(p,l)}}}function on(M){let m,b,v,p,_,l,g,A,ot,Fo,L,J,no,fe,tt,so,rt,zo,X,at,he,nt,st,Ao,Ne,it,Io,qe,io,lt,Po,Se,dt,No,S,I,mt,De,ct,ft,pe,ht,pt,ue,ut,gt,vt,T,_t,Le,bt,wt,Ve,$t,Ot,lo,xt,yt,mo,Et,Tt,co,kt,Ct,fo,jt,Mt,Be,Ft,zt,At,P,It,ho,Pt,Nt,po,qt,St,uo,Dt,Lt,qo,D,Vt,ge,Bt,Rt,ve,Ut,Wt,So,V,Z,go,_e,Gt,vo,Ht,Do,C,be,Kt,B,Jt,Re,Xt,Zt,we,Qt,Yt,er,R,or,Ue,tr,rr,We,ar,nr,sr,Q,Lo,U,Y,_o,$e,ir,bo,lr,Vo,N,Oe,dr,wo,mr,cr,xe,fr,Ge,hr,pr,Bo,W,ee,$o,ye,ur,Oo,gr,Ro,q,Ee,vr,Te,_r,ke,br,wr,$r,F,Ce,Or,G,xr,He,yr,Er,xo,Tr,kr,Cr,oe,jr,te,Uo,H,re,yo,je,Mr,Eo,Fr,Wo,j,Me,zr,To,Ar,Ir,Fe,Pr,ze,Nr,qr,Sr,z,Ae,Dr,K,Lr,Ke,Vr,Br,ko,Rr,Ur,Wr,ae,Gr,ne,Go;return l=new ao({}),fe=new ao({}),_e=new ao({}),be=new ro({props:{name:"class transformers.OmnivoreConfig",anchor:"transformers.OmnivoreConfig",parameters:[{name:"num_image_labels",val:" = 1000"},{name:"num_video_labels",val:" = 400"},{name:"num_rgbd_labels",val:" = 19"},{name:"input_channels",val:" = 3"},{name:"patch_size",val:" = [2, 4, 4]"},{name:"embed_dim",val:" = 96"},{name:"depths",val:" = [2, 2, 6, 2]"},{name:"num_heads",val:" = [3, 6, 12, 24]"},{name:"window_size",val:" = [8, 7, 7]"},{name:"mlp_ratio",val:" = 4.0"},{name:"qkv_bias",val:" = True"},{name:"qk_scale",val:" = None"},{name:"dropout_rate",val:" = 0.0"},{name:"attention_dropout_rate",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.2"},{name:"patch_norm",val:" = True"},{name:"frozen_stages",val:" = -1"},{name:"depth_mode",val:" = 'summed_rgb_d_tokens'"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreConfig.num_image_labels",description:`<strong>num_image_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 1000) &#x2014;
The number of labels for image head.`,name:"num_image_labels"},{anchor:"transformers.OmnivoreConfig.num_video_labels",description:`<strong>num_video_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 400) &#x2014;
The number of labels for video head.`,name:"num_video_labels"},{anchor:"transformers.OmnivoreConfig.num_rgbd_labels",description:`<strong>num_rgbd_labels</strong> (<code>int</code>, <em>optional</em>, defaults to 19) &#x2014;
The number of labels for rgbd head.`,name:"num_rgbd_labels"},{anchor:"transformers.OmnivoreConfig.input_channels",description:`<strong>input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"input_channels"},{anchor:"transformers.OmnivoreConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code> | <code>List[int]</code>, <em>optional</em>, defaults to [4, 4, 4]) &#x2014;
Patch size to use in the patch embedding layer.`,name:"patch_size"},{anchor:"transformers.OmnivoreConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Number of linear projection output channels.`,name:"embed_dim"},{anchor:"transformers.OmnivoreConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [2, 2, 6, 2],) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.OmnivoreConfig.num_heads",description:`<strong>num_heads</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [3, 6, 12, 24]) &#x2014;
Number of attention head of each stage.`,name:"num_heads"},{anchor:"transformers.OmnivoreConfig.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Size of the window used by swin transformer in the model,`,name:"window_size"},{anchor:"transformers.OmnivoreConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the
encoder blocks.`,name:"mlp_ratios"},{anchor:"transformers.OmnivoreConfig.attention_dropout_rate",description:`<strong>attention_dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout_rate"},{anchor:"transformers.OmnivoreConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the patch embeddings probabilities and projections in attention.`,name:"dropout_rate"},{anchor:"transformers.OmnivoreConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.0, 0.1]</code>) &#x2014;
The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.`,name:"drop_path_rate"},{anchor:"transformers.OmnivoreConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
The bias bool for query, key and value in attentions`,name:"qkv_bias"},{anchor:"transformers.OmnivoreConfig.qk_scale",description:`<strong>qk_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to None) &#x2014;
Override default qk scale of head_dim ** -0.5 if set.`,name:"qk_scale"},{anchor:"transformers.OmnivoreConfig.norm_layer",description:`<strong>norm_layer</strong> (<code>nn.Module</code>, <em>optional</em>, defaults to nn.LayerNorm) &#x2014;
Normalization layer for the model`,name:"norm_layer"},{anchor:"transformers.OmnivoreConfig.patch_norm",description:`<strong>patch_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;
If True, add normalization after patch embedding.`,name:"patch_norm"},{anchor:"transformers.OmnivoreConfig.frozen_stages",description:`<strong>frozen_stages</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
Stages to be frozen (stop grad and set eval mode) -1 means not freezing any parameters.`,name:"frozen_stages"},{anchor:"transformers.OmnivoreConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/configuration_omnivore.py#L28"}}),Q=new Hr({props:{anchor:"transformers.OmnivoreConfig.example",$$slots:{default:[Xa]},$$scope:{ctx:M}}}),$e=new ao({}),Oe=new ro({props:{name:"class transformers.OmnivoreFeatureExtractor",anchor:"transformers.OmnivoreFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 224"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = [0.485, 0.456, 0.406]"},{name:"image_std",val:" = [0.229, 0.224, 0.225]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shortest edge of the input to int(256/224 *<code>size</code>).`,name:"do_resize"},{anchor:"transformers.OmnivoreFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple(int)</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an
integer is provided, then shorter side of input will be resized to &#x2018;size&#x2019;.`,name:"size"},{anchor:"transformers.OmnivoreFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OmnivoreFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to center crop the input to <code>size</code>.`,name:"do_center_crop"},{anchor:"transformers.OmnivoreFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with mean and standard deviation.`,name:"do_normalize"},{anchor:"transformers.OmnivoreFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OmnivoreFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/feature_extraction_omnivore.py#L36"}}),ye=new ao({}),Ee=new ro({props:{name:"class transformers.OmnivoreModel",anchor:"transformers.OmnivoreModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig">OmnivoreConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17772/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L826"}}),Ce=new ro({props:{name:"forward",anchor:"transformers.OmnivoreModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OmnivoreModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17772/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.OmnivoreModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OmnivoreModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17772/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L833",returnDescription:`
<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig"
>OmnivoreConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new Ua({props:{$$slots:{default:[Za]},$$scope:{ctx:M}}}),te=new Hr({props:{anchor:"transformers.OmnivoreModel.forward.example",$$slots:{default:[Qa]},$$scope:{ctx:M}}}),je=new ao({}),Me=new ro({props:{name:"class transformers.OmnivoreForVisionClassification",anchor:"transformers.OmnivoreForVisionClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OmnivoreForVisionClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig">OmnivoreConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17772/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L878"}}),Ae=new ro({props:{name:"forward",anchor:"transformers.OmnivoreForVisionClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"input_type",val:": str = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OmnivoreForVisionClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_17772/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.OmnivoreForVisionClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OmnivoreForVisionClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17772/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.OmnivoreForVisionClassification.forward.input_type",description:`<strong>input_type</strong> (<code>str</code>) &#x2014;
Which classification head to use for the classification of given pixel_values`,name:"input_type"},{anchor:"transformers.OmnivoreForVisionClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p>
<p>Example &#x2014;`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17772/src/transformers/models/omnivore/modeling_omnivore.py#L893",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17772/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreConfig"
>OmnivoreConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17772/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ae=new Ua({props:{$$slots:{default:[Ya]},$$scope:{ctx:M}}}),ne=new Hr({props:{anchor:"transformers.OmnivoreForVisionClassification.forward.example",$$slots:{default:[en]},$$scope:{ctx:M}}}),{c(){m=n("meta"),b=f(),v=n("h1"),p=n("a"),_=n("span"),w(l.$$.fragment),g=f(),A=n("span"),ot=r("Omnivore"),Fo=f(),L=n("h2"),J=n("a"),no=n("span"),w(fe.$$.fragment),tt=f(),so=n("span"),rt=r("Overview"),zo=f(),X=n("p"),at=r("The Omnivore Model was proposed in "),he=n("a"),nt=r("Omnivore: A Single Model for Many Visual Modalities"),st=r(` by Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra.
The main idea here is generalization of vision transformer model across different modalities. The authors propose a single model
for multiple classification tasks such as image classification, video classification and 3D scence classification.`),Ao=f(),Ne=n("p"),it=r("The abstract from the paper is the following:"),Io=f(),qe=n("p"),io=n("em"),lt=r(`Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images,
videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view
3D data using exactly the same model parameters. Our \u2018Omnivore\u2019 model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard
datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on
ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision
tasks and generalize across modalities. Omnivore\u2019s shared visual representation naturally enables cross-modal recognition without
access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.`),Po=f(),Se=n("p"),dt=r("Tips:"),No=f(),S=n("ul"),I=n("li"),mt=r("All released checkpoints were pre-trained and fine-tuned on three datasets "),De=n("a"),ct=r("ImageNet-1k"),ft=r(", "),pe=n("a"),ht=r("Kinetics"),pt=r(" and "),ue=n("a"),ut=r("SUN RGB-D"),gt=r(` respectively.
There are five checkpoints available are trained on said dataset. In single epoch training for the model it was trained on one epoch each for ImageNet-1k and Kinetics with ten epochs for SUN RGB-D.`),vt=f(),T=n("li"),_t=r("The authors of Omnivore released 5 trained Omnivore models, which you can directly plug into "),Le=n("a"),bt=r("OmnivoreModel"),wt=r(" or "),Ve=n("a"),$t=r("OmnivoreForVisionClassification"),Ot=r(`.
The 5 variants available are (all trained on images of size 224x224, video of 32 frames of size 224x224 and RGBD images of size 224x224):
`),lo=n("em"),xt=r("facebook/omnivore-swinT"),yt=r(", "),mo=n("em"),Et=r("facebook/omnivore-swinS**, "),Tt=r("facebook/omnivore-swinB"),co=n("em"),kt=r(", "),Ct=r("facebook/omnivore-swinB-in21k"),fo=n("em"),jt=r(`and
`),Mt=r("facebook/omnivore-swinL-in21k*. Note that one should use "),Be=n("a"),Ft=r("OmnivoreFeatureExtractor"),zt=r(" in order to prepare images, vidoes and RGBD images for the model."),At=f(),P=n("li"),It=r("The input to the model needs to be provided with type of input, i.e. "),ho=n("em"),Pt=r("\u201Cimages\u201D"),Nt=r(", "),po=n("em"),qt=r("\u201Cvideos\u201D"),St=r(" or "),uo=n("em"),Dt=r("\u201Crgbd\u201D"),Lt=r(" to use the classification head for that modality."),qo=f(),D=n("p"),Vt=r("This model was contributed by "),ge=n("a"),Bt=r("anugunj"),Rt=r(". The original code can be found "),ve=n("a"),Ut=r("here"),Wt=r("."),So=f(),V=n("h2"),Z=n("a"),go=n("span"),w(_e.$$.fragment),Gt=f(),vo=n("span"),Ht=r("OmnivoreConfig"),Do=f(),C=n("div"),w(be.$$.fragment),Kt=f(),B=n("p"),Jt=r("This is the configuration class to store the configuration of a "),Re=n("a"),Xt=r("OmnivoreModel"),Zt=r(`. It is used to instantiate an
Omnivore model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Omnivore
`),we=n("a"),Qt=r("anugunj/omnivore"),Yt=r(" architecture."),er=f(),R=n("p"),or=r("Configuration objects inherit from "),Ue=n("a"),tr=r("PretrainedConfig"),rr=r(` and can be used to control the model outputs. Read the
documentation from `),We=n("a"),ar=r("PretrainedConfig"),nr=r(" for more information."),sr=f(),w(Q.$$.fragment),Lo=f(),U=n("h2"),Y=n("a"),_o=n("span"),w($e.$$.fragment),ir=f(),bo=n("span"),lr=r("OmnivoreFeatureExtractor"),Vo=f(),N=n("div"),w(Oe.$$.fragment),dr=f(),wo=n("p"),mr=r("Constructs a Omnivore feature extractor."),cr=f(),xe=n("p"),fr=r("This feature extractor inherits from "),Ge=n("a"),hr=r("FeatureExtractionMixin"),pr=r(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Bo=f(),W=n("h2"),ee=n("a"),$o=n("span"),w(ye.$$.fragment),ur=f(),Oo=n("span"),gr=r("OmnivoreModel"),Ro=f(),q=n("div"),w(Ee.$$.fragment),vr=f(),Te=n("p"),_r=r(`The bare Omnivore model outputting raw features without any specific head on top.
This model is a PyTorch `),ke=n("a"),br=r("torch.nn.Module"),wr=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),$r=f(),F=n("div"),w(Ce.$$.fragment),Or=f(),G=n("p"),xr=r("The "),He=n("a"),yr=r("OmnivoreModel"),Er=r(" forward method, overrides the "),xo=n("code"),Tr=r("__call__"),kr=r(" special method."),Cr=f(),w(oe.$$.fragment),jr=f(),w(te.$$.fragment),Uo=f(),H=n("h2"),re=n("a"),yo=n("span"),w(je.$$.fragment),Mr=f(),Eo=n("span"),Fr=r("OmnivoreForVisionClassification"),Wo=f(),j=n("div"),w(Me.$$.fragment),zr=f(),To=n("p"),Ar=r(`Omnivore Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ir=f(),Fe=n("p"),Pr=r("This model is a PyTorch "),ze=n("a"),Nr=r("torch.nn.Module"),qr=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sr=f(),z=n("div"),w(Ae.$$.fragment),Dr=f(),K=n("p"),Lr=r("The "),Ke=n("a"),Vr=r("OmnivoreForVisionClassification"),Br=r(" forward method, overrides the "),ko=n("code"),Rr=r("__call__"),Ur=r(" special method."),Wr=f(),w(ae.$$.fragment),Gr=f(),w(ne.$$.fragment),this.h()},l(o){const c=Ka('[data-svelte="svelte-1phssyn"]',document.head);m=s(c,"META",{name:!0,content:!0}),c.forEach(t),b=h(o),v=s(o,"H1",{class:!0});var Ie=i(v);p=s(Ie,"A",{id:!0,class:!0,href:!0});var Co=i(p);_=s(Co,"SPAN",{});var jo=i(_);$(l.$$.fragment,jo),jo.forEach(t),Co.forEach(t),g=h(Ie),A=s(Ie,"SPAN",{});var Mo=i(A);ot=a(Mo,"Omnivore"),Mo.forEach(t),Ie.forEach(t),Fo=h(o),L=s(o,"H2",{class:!0});var Pe=i(L);J=s(Pe,"A",{id:!0,class:!0,href:!0});var Xr=i(J);no=s(Xr,"SPAN",{});var Zr=i(no);$(fe.$$.fragment,Zr),Zr.forEach(t),Xr.forEach(t),tt=h(Pe),so=s(Pe,"SPAN",{});var Qr=i(so);rt=a(Qr,"Overview"),Qr.forEach(t),Pe.forEach(t),zo=h(o),X=s(o,"P",{});var Ho=i(X);at=a(Ho,"The Omnivore Model was proposed in "),he=s(Ho,"A",{href:!0,rel:!0});var Yr=i(he);nt=a(Yr,"Omnivore: A Single Model for Many Visual Modalities"),Yr.forEach(t),st=a(Ho,` by Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra.
The main idea here is generalization of vision transformer model across different modalities. The authors propose a single model
for multiple classification tasks such as image classification, video classification and 3D scence classification.`),Ho.forEach(t),Ao=h(o),Ne=s(o,"P",{});var ea=i(Ne);it=a(ea,"The abstract from the paper is the following:"),ea.forEach(t),Io=h(o),qe=s(o,"P",{});var oa=i(qe);io=s(oa,"EM",{});var ta=i(io);lt=a(ta,`Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images,
videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view
3D data using exactly the same model parameters. Our \u2018Omnivore\u2019 model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard
datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on
ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision
tasks and generalize across modalities. Omnivore\u2019s shared visual representation naturally enables cross-modal recognition without
access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.`),ta.forEach(t),oa.forEach(t),Po=h(o),Se=s(o,"P",{});var ra=i(Se);dt=a(ra,"Tips:"),ra.forEach(t),No=h(o),S=s(o,"UL",{});var Je=i(S);I=s(Je,"LI",{});var se=i(I);mt=a(se,"All released checkpoints were pre-trained and fine-tuned on three datasets "),De=s(se,"A",{href:!0});var aa=i(De);ct=a(aa,"ImageNet-1k"),aa.forEach(t),ft=a(se,", "),pe=s(se,"A",{href:!0,rel:!0});var na=i(pe);ht=a(na,"Kinetics"),na.forEach(t),pt=a(se," and "),ue=s(se,"A",{href:!0,rel:!0});var sa=i(ue);ut=a(sa,"SUN RGB-D"),sa.forEach(t),gt=a(se,` respectively.
There are five checkpoints available are trained on said dataset. In single epoch training for the model it was trained on one epoch each for ImageNet-1k and Kinetics with ten epochs for SUN RGB-D.`),se.forEach(t),vt=h(Je),T=s(Je,"LI",{});var k=i(T);_t=a(k,"The authors of Omnivore released 5 trained Omnivore models, which you can directly plug into "),Le=s(k,"A",{href:!0});var ia=i(Le);bt=a(ia,"OmnivoreModel"),ia.forEach(t),wt=a(k," or "),Ve=s(k,"A",{href:!0});var la=i(Ve);$t=a(la,"OmnivoreForVisionClassification"),la.forEach(t),Ot=a(k,`.
The 5 variants available are (all trained on images of size 224x224, video of 32 frames of size 224x224 and RGBD images of size 224x224):
`),lo=s(k,"EM",{});var da=i(lo);xt=a(da,"facebook/omnivore-swinT"),da.forEach(t),yt=a(k,", "),mo=s(k,"EM",{});var ma=i(mo);Et=a(ma,"facebook/omnivore-swinS**, "),ma.forEach(t),Tt=a(k,"facebook/omnivore-swinB"),co=s(k,"EM",{});var ca=i(co);kt=a(ca,", "),ca.forEach(t),Ct=a(k,"facebook/omnivore-swinB-in21k"),fo=s(k,"EM",{});var fa=i(fo);jt=a(fa,`and
`),fa.forEach(t),Mt=a(k,"facebook/omnivore-swinL-in21k*. Note that one should use "),Be=s(k,"A",{href:!0});var ha=i(Be);Ft=a(ha,"OmnivoreFeatureExtractor"),ha.forEach(t),zt=a(k," in order to prepare images, vidoes and RGBD images for the model."),k.forEach(t),At=h(Je),P=s(Je,"LI",{});var ie=i(P);It=a(ie,"The input to the model needs to be provided with type of input, i.e. "),ho=s(ie,"EM",{});var pa=i(ho);Pt=a(pa,"\u201Cimages\u201D"),pa.forEach(t),Nt=a(ie,", "),po=s(ie,"EM",{});var ua=i(po);qt=a(ua,"\u201Cvideos\u201D"),ua.forEach(t),St=a(ie," or "),uo=s(ie,"EM",{});var ga=i(uo);Dt=a(ga,"\u201Crgbd\u201D"),ga.forEach(t),Lt=a(ie," to use the classification head for that modality."),ie.forEach(t),Je.forEach(t),qo=h(o),D=s(o,"P",{});var Xe=i(D);Vt=a(Xe,"This model was contributed by "),ge=s(Xe,"A",{href:!0,rel:!0});var va=i(ge);Bt=a(va,"anugunj"),va.forEach(t),Rt=a(Xe,". The original code can be found "),ve=s(Xe,"A",{href:!0,rel:!0});var _a=i(ve);Ut=a(_a,"here"),_a.forEach(t),Wt=a(Xe,"."),Xe.forEach(t),So=h(o),V=s(o,"H2",{class:!0});var Ko=i(V);Z=s(Ko,"A",{id:!0,class:!0,href:!0});var ba=i(Z);go=s(ba,"SPAN",{});var wa=i(go);$(_e.$$.fragment,wa),wa.forEach(t),ba.forEach(t),Gt=h(Ko),vo=s(Ko,"SPAN",{});var $a=i(vo);Ht=a($a,"OmnivoreConfig"),$a.forEach(t),Ko.forEach(t),Do=h(o),C=s(o,"DIV",{class:!0});var le=i(C);$(be.$$.fragment,le),Kt=h(le),B=s(le,"P",{});var Ze=i(B);Jt=a(Ze,"This is the configuration class to store the configuration of a "),Re=s(Ze,"A",{href:!0});var Oa=i(Re);Xt=a(Oa,"OmnivoreModel"),Oa.forEach(t),Zt=a(Ze,`. It is used to instantiate an
Omnivore model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Omnivore
`),we=s(Ze,"A",{href:!0,rel:!0});var xa=i(we);Qt=a(xa,"anugunj/omnivore"),xa.forEach(t),Yt=a(Ze," architecture."),Ze.forEach(t),er=h(le),R=s(le,"P",{});var Qe=i(R);or=a(Qe,"Configuration objects inherit from "),Ue=s(Qe,"A",{href:!0});var ya=i(Ue);tr=a(ya,"PretrainedConfig"),ya.forEach(t),rr=a(Qe,` and can be used to control the model outputs. Read the
documentation from `),We=s(Qe,"A",{href:!0});var Ea=i(We);ar=a(Ea,"PretrainedConfig"),Ea.forEach(t),nr=a(Qe," for more information."),Qe.forEach(t),sr=h(le),$(Q.$$.fragment,le),le.forEach(t),Lo=h(o),U=s(o,"H2",{class:!0});var Jo=i(U);Y=s(Jo,"A",{id:!0,class:!0,href:!0});var Ta=i(Y);_o=s(Ta,"SPAN",{});var ka=i(_o);$($e.$$.fragment,ka),ka.forEach(t),Ta.forEach(t),ir=h(Jo),bo=s(Jo,"SPAN",{});var Ca=i(bo);lr=a(Ca,"OmnivoreFeatureExtractor"),Ca.forEach(t),Jo.forEach(t),Vo=h(o),N=s(o,"DIV",{class:!0});var Ye=i(N);$(Oe.$$.fragment,Ye),dr=h(Ye),wo=s(Ye,"P",{});var ja=i(wo);mr=a(ja,"Constructs a Omnivore feature extractor."),ja.forEach(t),cr=h(Ye),xe=s(Ye,"P",{});var Xo=i(xe);fr=a(Xo,"This feature extractor inherits from "),Ge=s(Xo,"A",{href:!0});var Ma=i(Ge);hr=a(Ma,"FeatureExtractionMixin"),Ma.forEach(t),pr=a(Xo,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Xo.forEach(t),Ye.forEach(t),Bo=h(o),W=s(o,"H2",{class:!0});var Zo=i(W);ee=s(Zo,"A",{id:!0,class:!0,href:!0});var Fa=i(ee);$o=s(Fa,"SPAN",{});var za=i($o);$(ye.$$.fragment,za),za.forEach(t),Fa.forEach(t),ur=h(Zo),Oo=s(Zo,"SPAN",{});var Aa=i(Oo);gr=a(Aa,"OmnivoreModel"),Aa.forEach(t),Zo.forEach(t),Ro=h(o),q=s(o,"DIV",{class:!0});var eo=i(q);$(Ee.$$.fragment,eo),vr=h(eo),Te=s(eo,"P",{});var Qo=i(Te);_r=a(Qo,`The bare Omnivore model outputting raw features without any specific head on top.
This model is a PyTorch `),ke=s(Qo,"A",{href:!0,rel:!0});var Ia=i(ke);br=a(Ia,"torch.nn.Module"),Ia.forEach(t),wr=a(Qo,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qo.forEach(t),$r=h(eo),F=s(eo,"DIV",{class:!0});var de=i(F);$(Ce.$$.fragment,de),Or=h(de),G=s(de,"P",{});var oo=i(G);xr=a(oo,"The "),He=s(oo,"A",{href:!0});var Pa=i(He);yr=a(Pa,"OmnivoreModel"),Pa.forEach(t),Er=a(oo," forward method, overrides the "),xo=s(oo,"CODE",{});var Na=i(xo);Tr=a(Na,"__call__"),Na.forEach(t),kr=a(oo," special method."),oo.forEach(t),Cr=h(de),$(oe.$$.fragment,de),jr=h(de),$(te.$$.fragment,de),de.forEach(t),eo.forEach(t),Uo=h(o),H=s(o,"H2",{class:!0});var Yo=i(H);re=s(Yo,"A",{id:!0,class:!0,href:!0});var qa=i(re);yo=s(qa,"SPAN",{});var Sa=i(yo);$(je.$$.fragment,Sa),Sa.forEach(t),qa.forEach(t),Mr=h(Yo),Eo=s(Yo,"SPAN",{});var Da=i(Eo);Fr=a(Da,"OmnivoreForVisionClassification"),Da.forEach(t),Yo.forEach(t),Wo=h(o),j=s(o,"DIV",{class:!0});var me=i(j);$(Me.$$.fragment,me),zr=h(me),To=s(me,"P",{});var La=i(To);Ar=a(La,`Omnivore Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),La.forEach(t),Ir=h(me),Fe=s(me,"P",{});var et=i(Fe);Pr=a(et,"This model is a PyTorch "),ze=s(et,"A",{href:!0,rel:!0});var Va=i(ze);Nr=a(Va,"torch.nn.Module"),Va.forEach(t),qr=a(et,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),et.forEach(t),Sr=h(me),z=s(me,"DIV",{class:!0});var ce=i(z);$(Ae.$$.fragment,ce),Dr=h(ce),K=s(ce,"P",{});var to=i(K);Lr=a(to,"The "),Ke=s(to,"A",{href:!0});var Ba=i(Ke);Vr=a(Ba,"OmnivoreForVisionClassification"),Ba.forEach(t),Br=a(to," forward method, overrides the "),ko=s(to,"CODE",{});var Ra=i(ko);Rr=a(Ra,"__call__"),Ra.forEach(t),Ur=a(to," special method."),to.forEach(t),Wr=h(ce),$(ae.$$.fragment,ce),Gr=h(ce),$(ne.$$.fragment,ce),ce.forEach(t),me.forEach(t),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(tn)),d(p,"id","omnivore"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#omnivore"),d(v,"class","relative group"),d(J,"id","overview"),d(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J,"href","#overview"),d(L,"class","relative group"),d(he,"href","https://arxiv.org/abs/2201.08377"),d(he,"rel","nofollow"),d(De,"href","(https://huggingface.co/datasets/imagenet-1k)"),d(pe,"href","https://www.deepmind.com/open-source/kinetics"),d(pe,"rel","nofollow"),d(ue,"href","https://rgbd.cs.princeton.edu/"),d(ue,"rel","nofollow"),d(Le,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(Ve,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreForVisionClassification"),d(Be,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreFeatureExtractor"),d(ge,"href","https://huggingface.co/anugunj"),d(ge,"rel","nofollow"),d(ve,"href","https://github.com/facebookresearch/omnivore"),d(ve,"rel","nofollow"),d(Z,"id","transformers.OmnivoreConfig"),d(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Z,"href","#transformers.OmnivoreConfig"),d(V,"class","relative group"),d(Re,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(we,"href","https://huggingface.co/anugunj/omnivore"),d(we,"rel","nofollow"),d(Ue,"href","/docs/transformers/pr_17772/en/main_classes/configuration#transformers.PretrainedConfig"),d(We,"href","/docs/transformers/pr_17772/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"id","transformers.OmnivoreFeatureExtractor"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#transformers.OmnivoreFeatureExtractor"),d(U,"class","relative group"),d(Ge,"href","/docs/transformers/pr_17772/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ee,"id","transformers.OmnivoreModel"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#transformers.OmnivoreModel"),d(W,"class","relative group"),d(ke,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(ke,"rel","nofollow"),d(He,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreModel"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(re,"id","transformers.OmnivoreForVisionClassification"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#transformers.OmnivoreForVisionClassification"),d(H,"class","relative group"),d(ze,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(ze,"rel","nofollow"),d(Ke,"href","/docs/transformers/pr_17772/en/model_doc/omnivore#transformers.OmnivoreForVisionClassification"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,c){e(document.head,m),u(o,b,c),u(o,v,c),e(v,p),e(p,_),O(l,_,null),e(v,g),e(v,A),e(A,ot),u(o,Fo,c),u(o,L,c),e(L,J),e(J,no),O(fe,no,null),e(L,tt),e(L,so),e(so,rt),u(o,zo,c),u(o,X,c),e(X,at),e(X,he),e(he,nt),e(X,st),u(o,Ao,c),u(o,Ne,c),e(Ne,it),u(o,Io,c),u(o,qe,c),e(qe,io),e(io,lt),u(o,Po,c),u(o,Se,c),e(Se,dt),u(o,No,c),u(o,S,c),e(S,I),e(I,mt),e(I,De),e(De,ct),e(I,ft),e(I,pe),e(pe,ht),e(I,pt),e(I,ue),e(ue,ut),e(I,gt),e(S,vt),e(S,T),e(T,_t),e(T,Le),e(Le,bt),e(T,wt),e(T,Ve),e(Ve,$t),e(T,Ot),e(T,lo),e(lo,xt),e(T,yt),e(T,mo),e(mo,Et),e(T,Tt),e(T,co),e(co,kt),e(T,Ct),e(T,fo),e(fo,jt),e(T,Mt),e(T,Be),e(Be,Ft),e(T,zt),e(S,At),e(S,P),e(P,It),e(P,ho),e(ho,Pt),e(P,Nt),e(P,po),e(po,qt),e(P,St),e(P,uo),e(uo,Dt),e(P,Lt),u(o,qo,c),u(o,D,c),e(D,Vt),e(D,ge),e(ge,Bt),e(D,Rt),e(D,ve),e(ve,Ut),e(D,Wt),u(o,So,c),u(o,V,c),e(V,Z),e(Z,go),O(_e,go,null),e(V,Gt),e(V,vo),e(vo,Ht),u(o,Do,c),u(o,C,c),O(be,C,null),e(C,Kt),e(C,B),e(B,Jt),e(B,Re),e(Re,Xt),e(B,Zt),e(B,we),e(we,Qt),e(B,Yt),e(C,er),e(C,R),e(R,or),e(R,Ue),e(Ue,tr),e(R,rr),e(R,We),e(We,ar),e(R,nr),e(C,sr),O(Q,C,null),u(o,Lo,c),u(o,U,c),e(U,Y),e(Y,_o),O($e,_o,null),e(U,ir),e(U,bo),e(bo,lr),u(o,Vo,c),u(o,N,c),O(Oe,N,null),e(N,dr),e(N,wo),e(wo,mr),e(N,cr),e(N,xe),e(xe,fr),e(xe,Ge),e(Ge,hr),e(xe,pr),u(o,Bo,c),u(o,W,c),e(W,ee),e(ee,$o),O(ye,$o,null),e(W,ur),e(W,Oo),e(Oo,gr),u(o,Ro,c),u(o,q,c),O(Ee,q,null),e(q,vr),e(q,Te),e(Te,_r),e(Te,ke),e(ke,br),e(Te,wr),e(q,$r),e(q,F),O(Ce,F,null),e(F,Or),e(F,G),e(G,xr),e(G,He),e(He,yr),e(G,Er),e(G,xo),e(xo,Tr),e(G,kr),e(F,Cr),O(oe,F,null),e(F,jr),O(te,F,null),u(o,Uo,c),u(o,H,c),e(H,re),e(re,yo),O(je,yo,null),e(H,Mr),e(H,Eo),e(Eo,Fr),u(o,Wo,c),u(o,j,c),O(Me,j,null),e(j,zr),e(j,To),e(To,Ar),e(j,Ir),e(j,Fe),e(Fe,Pr),e(Fe,ze),e(ze,Nr),e(Fe,qr),e(j,Sr),e(j,z),O(Ae,z,null),e(z,Dr),e(z,K),e(K,Lr),e(K,Ke),e(Ke,Vr),e(K,Br),e(K,ko),e(ko,Rr),e(K,Ur),e(z,Wr),O(ae,z,null),e(z,Gr),O(ne,z,null),Go=!0},p(o,[c]){const Ie={};c&2&&(Ie.$$scope={dirty:c,ctx:o}),Q.$set(Ie);const Co={};c&2&&(Co.$$scope={dirty:c,ctx:o}),oe.$set(Co);const jo={};c&2&&(jo.$$scope={dirty:c,ctx:o}),te.$set(jo);const Mo={};c&2&&(Mo.$$scope={dirty:c,ctx:o}),ae.$set(Mo);const Pe={};c&2&&(Pe.$$scope={dirty:c,ctx:o}),ne.$set(Pe)},i(o){Go||(x(l.$$.fragment,o),x(fe.$$.fragment,o),x(_e.$$.fragment,o),x(be.$$.fragment,o),x(Q.$$.fragment,o),x($e.$$.fragment,o),x(Oe.$$.fragment,o),x(ye.$$.fragment,o),x(Ee.$$.fragment,o),x(Ce.$$.fragment,o),x(oe.$$.fragment,o),x(te.$$.fragment,o),x(je.$$.fragment,o),x(Me.$$.fragment,o),x(Ae.$$.fragment,o),x(ae.$$.fragment,o),x(ne.$$.fragment,o),Go=!0)},o(o){y(l.$$.fragment,o),y(fe.$$.fragment,o),y(_e.$$.fragment,o),y(be.$$.fragment,o),y(Q.$$.fragment,o),y($e.$$.fragment,o),y(Oe.$$.fragment,o),y(ye.$$.fragment,o),y(Ee.$$.fragment,o),y(Ce.$$.fragment,o),y(oe.$$.fragment,o),y(te.$$.fragment,o),y(je.$$.fragment,o),y(Me.$$.fragment,o),y(Ae.$$.fragment,o),y(ae.$$.fragment,o),y(ne.$$.fragment,o),Go=!1},d(o){t(m),o&&t(b),o&&t(v),E(l),o&&t(Fo),o&&t(L),E(fe),o&&t(zo),o&&t(X),o&&t(Ao),o&&t(Ne),o&&t(Io),o&&t(qe),o&&t(Po),o&&t(Se),o&&t(No),o&&t(S),o&&t(qo),o&&t(D),o&&t(So),o&&t(V),E(_e),o&&t(Do),o&&t(C),E(be),E(Q),o&&t(Lo),o&&t(U),E($e),o&&t(Vo),o&&t(N),E(Oe),o&&t(Bo),o&&t(W),E(ye),o&&t(Ro),o&&t(q),E(Ee),E(Ce),E(oe),E(te),o&&t(Uo),o&&t(H),E(je),o&&t(Wo),o&&t(j),E(Me),E(Ae),E(ae),E(ne)}}}const tn={local:"omnivore",sections:[{local:"overview",title:"Overview"},{local:"transformers.OmnivoreConfig",title:"OmnivoreConfig"},{local:"transformers.OmnivoreFeatureExtractor",title:"OmnivoreFeatureExtractor"},{local:"transformers.OmnivoreModel",title:"OmnivoreModel"},{local:"transformers.OmnivoreForVisionClassification",title:"OmnivoreForVisionClassification"}],title:"Omnivore"};function rn(M){return Ja(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cn extends Wa{constructor(m){super();Ga(this,m,rn,on,Ha,{})}}export{cn as default,tn as metadata};
