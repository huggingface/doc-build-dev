import{S as Is,i as Us,s as Ns,e as i,k,w as S,t as n,M as Bs,c as p,d as a,m as w,a as h,x as y,h as o,b as q,G as r,g as u,y as E,q as T,o as x,B as z,v as Ws,L as Ft}from"../../chunks/vendor-hf-doc-builder.js";import{T as At}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Os}from"../../chunks/Youtube-hf-doc-builder.js";import{I as zt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as X}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as fs,M as He}from"../../chunks/Markdown-hf-doc-builder.js";function Ys(F){let t,l,s,f,_;return{c(){t=i("p"),l=n("See the translation "),s=i("a"),f=n("task page"),_=n(" for more information about its associated models, datasets, and metrics."),this.h()},l($){t=p($,"P",{});var g=h(t);l=o(g,"See the translation "),s=p(g,"A",{href:!0,rel:!0});var v=h(s);f=o(v,"task page"),v.forEach(a),_=o(g," for more information about its associated models, datasets, and metrics."),g.forEach(a),this.h()},h(){q(s,"href","https://huggingface.co/tasks/translation"),q(s,"rel","nofollow")},m($,g){u($,t,g),r(t,l),r(t,s),r(s,f),r(t,_)},d($){$&&a(t)}}}function Hs(F){let t,l,s,f,_,$,g,v;return g=new X({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),{c(){t=i("p"),l=n("Load T5 with "),s=i("a"),f=n("AutoModelForSeq2SeqLM"),_=n(":"),$=k(),S(g.$$.fragment),this.h()},l(d){t=p(d,"P",{});var b=h(t);l=o(b,"Load T5 with "),s=p(b,"A",{href:!0});var P=h(s);f=o(P,"AutoModelForSeq2SeqLM"),P.forEach(a),_=o(b,":"),b.forEach(a),$=w(d),y(g.$$.fragment,d),this.h()},h(){q(s,"href","/docs/transformers/pr_18169/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM")},m(d,b){u(d,t,b),r(t,l),r(t,s),r(s,f),r(t,_),u(d,$,b),E(g,d,b),v=!0},p:Ft,i(d){v||(T(g.$$.fragment,d),v=!0)},o(d){x(g.$$.fragment,d),v=!1},d(d){d&&a(t),d&&a($),z(g,d)}}}function Zs(F){let t,l;return t=new He({props:{$$slots:{default:[Hs]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Js(F){let t,l,s,f,_,$,g,v;return g=new X({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),{c(){t=i("p"),l=n("Load T5 with "),s=i("a"),f=n("TFAutoModelForSeq2SeqLM"),_=n(":"),$=k(),S(g.$$.fragment),this.h()},l(d){t=p(d,"P",{});var b=h(t);l=o(b,"Load T5 with "),s=p(b,"A",{href:!0});var P=h(s);f=o(P,"TFAutoModelForSeq2SeqLM"),P.forEach(a),_=o(b,":"),b.forEach(a),$=w(d),y(g.$$.fragment,d),this.h()},h(){q(s,"href","/docs/transformers/pr_18169/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM")},m(d,b){u(d,t,b),r(t,l),r(t,s),r(s,f),r(t,_),u(d,$,b),E(g,d,b),v=!0},p:Ft,i(d){v||(T(g.$$.fragment,d),v=!0)},o(d){x(g.$$.fragment,d),v=!1},d(d){d&&a(t),d&&a($),z(g,d)}}}function Ks(F){let t,l;return t=new He({props:{$$slots:{default:[Js]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Rs(F){let t,l;return t=new X({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p:Ft,i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Gs(F){let t,l;return t=new He({props:{$$slots:{default:[Rs]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Xs(F){let t,l;return t=new X({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p:Ft,i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Qs(F){let t,l;return t=new He({props:{$$slots:{default:[Xs]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function Vs(F){let t,l,s,f,_,$,g,v;return{c(){t=i("p"),l=n("If you aren\u2019t familiar with fine-tuning a model with the "),s=i("a"),f=n("Trainer"),_=n(", take a look at the basic tutorial "),$=i("a"),g=n("here"),v=n("!"),this.h()},l(d){t=p(d,"P",{});var b=h(t);l=o(b,"If you aren\u2019t familiar with fine-tuning a model with the "),s=p(b,"A",{href:!0});var P=h(s);f=o(P,"Trainer"),P.forEach(a),_=o(b,", take a look at the basic tutorial "),$=p(b,"A",{href:!0});var I=h($);g=o(I,"here"),I.forEach(a),v=o(b,"!"),b.forEach(a),this.h()},h(){q(s,"href","/docs/transformers/pr_18169/en/main_classes/trainer#transformers.Trainer"),q($,"href","../training#finetune-with-trainer")},m(d,b){u(d,t,b),r(t,l),r(t,s),r(s,f),r(t,_),r(t,$),r($,g),r(t,v)},d(d){d&&a(t)}}}function ea(F){let t,l,s,f,_,$,g,v,d,b,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,se;return t=new At({props:{$$slots:{default:[Vs]},$$scope:{ctx:F}}}),C=new X({props:{code:`from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){S(t.$$.fragment),l=k(),s=i("p"),f=n("At this point, only three steps remain:"),_=k(),$=i("ol"),g=i("li"),v=n("Define your training hyperparameters in "),d=i("a"),b=n("Seq2SeqTrainingArguments"),P=n("."),I=k(),M=i("li"),re=n("Pass the training arguments to "),O=i("a"),U=n("Seq2SeqTrainer"),Q=n(" along with the model, dataset, tokenizer, and data collator."),Z=k(),Y=i("li"),J=n("Call "),N=i("a"),H=n("train()"),oe=n(" to fine-tune your model."),B=k(),S(C.$$.fragment),this.h()},l(j){y(t.$$.fragment,j),l=w(j),s=p(j,"P",{});var L=h(s);f=o(L,"At this point, only three steps remain:"),L.forEach(a),_=w(j),$=p(j,"OL",{});var D=h($);g=p(D,"LI",{});var W=h(g);v=o(W,"Define your training hyperparameters in "),d=p(W,"A",{href:!0});var K=h(d);b=o(K,"Seq2SeqTrainingArguments"),K.forEach(a),P=o(W,"."),W.forEach(a),I=w(D),M=p(D,"LI",{});var V=h(M);re=o(V,"Pass the training arguments to "),O=p(V,"A",{href:!0});var ee=h(O);U=o(ee,"Seq2SeqTrainer"),ee.forEach(a),Q=o(V," along with the model, dataset, tokenizer, and data collator."),V.forEach(a),Z=w(D),Y=p(D,"LI",{});var R=h(Y);J=o(R,"Call "),N=p(R,"A",{href:!0});var ae=h(N);H=o(ae,"train()"),ae.forEach(a),oe=o(R," to fine-tune your model."),R.forEach(a),D.forEach(a),B=w(j),y(C.$$.fragment,j),this.h()},h(){q(d,"href","/docs/transformers/pr_18169/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),q(O,"href","/docs/transformers/pr_18169/en/main_classes/trainer#transformers.Seq2SeqTrainer"),q(N,"href","/docs/transformers/pr_18169/en/main_classes/trainer#transformers.Trainer.train")},m(j,L){E(t,j,L),u(j,l,L),u(j,s,L),r(s,f),u(j,_,L),u(j,$,L),r($,g),r(g,v),r(g,d),r(d,b),r(g,P),r($,I),r($,M),r(M,re),r(M,O),r(O,U),r(M,Q),r($,Z),r($,Y),r(Y,J),r(Y,N),r(N,H),r(Y,oe),u(j,B,L),E(C,j,L),se=!0},p(j,L){const D={};L&2&&(D.$$scope={dirty:L,ctx:j}),t.$set(D)},i(j){se||(T(t.$$.fragment,j),T(C.$$.fragment,j),se=!0)},o(j){x(t.$$.fragment,j),x(C.$$.fragment,j),se=!1},d(j){z(t,j),j&&a(l),j&&a(s),j&&a(_),j&&a($),j&&a(B),z(C,j)}}}function ta(F){let t,l;return t=new He({props:{$$slots:{default:[ea]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function sa(F){let t,l,s,f,_;return{c(){t=i("p"),l=n("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),s=i("a"),f=n("here"),_=n("!"),this.h()},l($){t=p($,"P",{});var g=h(t);l=o(g,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),s=p(g,"A",{href:!0});var v=h(s);f=o(v,"here"),v.forEach(a),_=o(g,"!"),g.forEach(a),this.h()},h(){q(s,"href","training#finetune-with-keras")},m($,g){u($,t,g),r(t,l),r(t,s),r(s,f),r(t,_)},d($){$&&a(t)}}}function aa(F){let t,l,s,f,_,$,g,v,d,b,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,se,j,L,D,W,K,V,ee,R,ae,Ee,ce,G,ue;return M=new X({props:{code:`tf_train_set = tokenized_books["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = tokenized_books["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_books[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = tokenized_books[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),O=new At({props:{$$slots:{default:[sa]},$$scope:{ctx:F}}}),J=new X({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),D=new X({props:{code:"model.compile(optimizer=optimizer)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),G=new X({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){t=i("p"),l=n("To fine-tune a model in TensorFlow, start by converting your datasets to the "),s=i("code"),f=n("tf.data.Dataset"),_=n(" format with "),$=i("a"),g=n("to_tf_dataset"),v=n(". Specify inputs and labels in "),d=i("code"),b=n("columns"),P=n(", whether to shuffle the dataset order, batch size, and the data collator:"),I=k(),S(M.$$.fragment),re=k(),S(O.$$.fragment),U=k(),Q=i("p"),Z=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Y=k(),S(J.$$.fragment),N=k(),H=i("p"),oe=n("Configure the model for training with "),B=i("a"),C=i("code"),se=n("compile"),j=n(":"),L=k(),S(D.$$.fragment),W=k(),K=i("p"),V=n("Call "),ee=i("a"),R=i("code"),ae=n("fit"),Ee=n(" to fine-tune the model:"),ce=k(),S(G.$$.fragment),this.h()},l(m){t=p(m,"P",{});var A=h(t);l=o(A,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),s=p(A,"CODE",{});var ie=h(s);f=o(ie,"tf.data.Dataset"),ie.forEach(a),_=o(A," format with "),$=p(A,"A",{href:!0,rel:!0});var pe=h($);g=o(pe,"to_tf_dataset"),pe.forEach(a),v=o(A,". Specify inputs and labels in "),d=p(A,"CODE",{});var Te=h(d);b=o(Te,"columns"),Te.forEach(a),P=o(A,", whether to shuffle the dataset order, batch size, and the data collator:"),A.forEach(a),I=w(m),y(M.$$.fragment,m),re=w(m),y(O.$$.fragment,m),U=w(m),Q=p(m,"P",{});var $e=h(Q);Z=o($e,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),$e.forEach(a),Y=w(m),y(J.$$.fragment,m),N=w(m),H=p(m,"P",{});var ke=h(H);oe=o(ke,"Configure the model for training with "),B=p(ke,"A",{href:!0,rel:!0});var xe=h(B);C=p(xe,"CODE",{});var fe=h(C);se=o(fe,"compile"),fe.forEach(a),xe.forEach(a),j=o(ke,":"),ke.forEach(a),L=w(m),y(D.$$.fragment,m),W=w(m),K=p(m,"P",{});var de=h(K);V=o(de,"Call "),ee=p(de,"A",{href:!0,rel:!0});var le=h(ee);R=p(le,"CODE",{});var Ie=h(R);ae=o(Ie,"fit"),Ie.forEach(a),le.forEach(a),Ee=o(de," to fine-tune the model:"),de.forEach(a),ce=w(m),y(G.$$.fragment,m),this.h()},h(){q($,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),q($,"rel","nofollow"),q(B,"href","https://keras.io/api/models/model_training_apis/#compile-method"),q(B,"rel","nofollow"),q(ee,"href","https://keras.io/api/models/model_training_apis/#fit-method"),q(ee,"rel","nofollow")},m(m,A){u(m,t,A),r(t,l),r(t,s),r(s,f),r(t,_),r(t,$),r($,g),r(t,v),r(t,d),r(d,b),r(t,P),u(m,I,A),E(M,m,A),u(m,re,A),E(O,m,A),u(m,U,A),u(m,Q,A),r(Q,Z),u(m,Y,A),E(J,m,A),u(m,N,A),u(m,H,A),r(H,oe),r(H,B),r(B,C),r(C,se),r(H,j),u(m,L,A),E(D,m,A),u(m,W,A),u(m,K,A),r(K,V),r(K,ee),r(ee,R),r(R,ae),r(K,Ee),u(m,ce,A),E(G,m,A),ue=!0},p(m,A){const ie={};A&2&&(ie.$$scope={dirty:A,ctx:m}),O.$set(ie)},i(m){ue||(T(M.$$.fragment,m),T(O.$$.fragment,m),T(J.$$.fragment,m),T(D.$$.fragment,m),T(G.$$.fragment,m),ue=!0)},o(m){x(M.$$.fragment,m),x(O.$$.fragment,m),x(J.$$.fragment,m),x(D.$$.fragment,m),x(G.$$.fragment,m),ue=!1},d(m){m&&a(t),m&&a(I),z(M,m),m&&a(re),z(O,m),m&&a(U),m&&a(Q),m&&a(Y),z(J,m),m&&a(N),m&&a(H),m&&a(L),z(D,m),m&&a(W),m&&a(K),m&&a(ce),z(G,m)}}}function ra(F){let t,l;return t=new He({props:{$$slots:{default:[aa]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,f){E(t,s,f),l=!0},p(s,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:s}),t.$set(_)},i(s){l||(T(t.$$.fragment,s),l=!0)},o(s){x(t.$$.fragment,s),l=!1},d(s){z(t,s)}}}function na(F){let t,l,s,f,_,$,g,v;return{c(){t=i("p"),l=n(`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),s=i("a"),f=n("PyTorch notebook"),_=n(`
or `),$=i("a"),g=n("TensorFlow notebook"),v=n("."),this.h()},l(d){t=p(d,"P",{});var b=h(t);l=o(b,`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),s=p(b,"A",{href:!0,rel:!0});var P=h(s);f=o(P,"PyTorch notebook"),P.forEach(a),_=o(b,`
or `),$=p(b,"A",{href:!0,rel:!0});var I=h($);g=o(I,"TensorFlow notebook"),I.forEach(a),v=o(b,"."),b.forEach(a),this.h()},h(){q(s,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb"),q(s,"rel","nofollow"),q($,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb"),q($,"rel","nofollow")},m(d,b){u(d,t,b),r(t,l),r(t,s),r(s,f),r(t,_),r(t,$),r($,g),r(t,v)},d(d){d&&a(t)}}}function oa(F){let t,l,s,f,_,$,g,v,d,b,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,se,j,L,D,W,K,V,ee,R,ae,Ee,ce,G,ue,m,A,ie,pe,Te,$e,ke,xe,fe,de,le,Ie,Ze,Pt,Lt,pt,_e,we,Je,ze,Dt,Ke,Mt,ft,Ae,mt,Ue,Ct,ht,Fe,ct,Ne,Ot,ut,me,Re,It,Ut,Ge,Nt,Bt,Pe,Wt,Xe,Yt,Ht,$t,Le,dt,ne,Zt,De,Jt,Kt,Qe,Rt,Gt,Ve,Xt,Qt,_t,Me,gt,be,kt,te,Vt,Be,es,ts,et,ss,as,tt,rs,ns,st,os,ls,wt,je,bt,ge,qe,at,Ce,is,rt,ps,jt,ve,qt,Se,vt;return $=new zt({}),P=new Os({props:{id:"1JvfrvZgi6c"}}),C=new At({props:{$$slots:{default:[Ys]},$$scope:{ctx:F}}}),W=new zt({}),G=new X({props:{code:`from datasets import load_dataset

books = load_dataset("opus_books", "en-fr")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`}}),pe=new X({props:{code:'books = books["train"].train_test_split(test_size=0.2)',highlighted:'books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),fe=new X({props:{code:'books["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau \xE9lev\xE9 ne mesurait que quelques toises, et bient\xF4t nous f\xFBmes rentr\xE9s dans notre \xE9l\xE9ment.&#x27;</span>}}`}}),ze=new zt({}),Ae=new Os({props:{id:"XAR8jnZZuUs"}}),Fe=new X({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Le=new X({props:{code:`source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>        labels = tokenizer(targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`}}),Me=new X({props:{code:"tokenized_books = books.map(preprocess_function, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),be=new fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ks],pytorch:[Zs]},$$scope:{ctx:F}}}),je=new fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qs],pytorch:[Gs]},$$scope:{ctx:F}}}),Ce=new zt({}),ve=new fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ra],pytorch:[ta]},$$scope:{ctx:F}}}),Se=new At({props:{$$slots:{default:[na]},$$scope:{ctx:F}}}),{c(){t=i("meta"),l=k(),s=i("h1"),f=i("a"),_=i("span"),S($.$$.fragment),g=k(),v=i("span"),d=n("Translation"),b=k(),S(P.$$.fragment),I=k(),M=i("p"),re=n("Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),O=k(),U=i("p"),Q=n("This guide will show you how to fine-tune "),Z=i("a"),Y=n("T5"),J=n(" on the English-French subset of the "),N=i("a"),H=n("OPUS Books"),oe=n(" dataset to translate English text to French."),B=k(),S(C.$$.fragment),se=k(),j=i("h2"),L=i("a"),D=i("span"),S(W.$$.fragment),K=k(),V=i("span"),ee=n("Load OPUS Books dataset"),R=k(),ae=i("p"),Ee=n("Load the OPUS Books dataset from the \u{1F917} Datasets library:"),ce=k(),S(G.$$.fragment),ue=k(),m=i("p"),A=n("Split this dataset into a train and test set:"),ie=k(),S(pe.$$.fragment),Te=k(),$e=i("p"),ke=n("Then take a look at an example:"),xe=k(),S(fe.$$.fragment),de=k(),le=i("p"),Ie=n("The "),Ze=i("code"),Pt=n("translation"),Lt=n(" field is a dictionary containing the English and French translations of the text."),pt=k(),_e=i("h2"),we=i("a"),Je=i("span"),S(ze.$$.fragment),Dt=k(),Ke=i("span"),Mt=n("Preprocess"),ft=k(),S(Ae.$$.fragment),mt=k(),Ue=i("p"),Ct=n("Load the T5 tokenizer to process the language pairs:"),ht=k(),S(Fe.$$.fragment),ct=k(),Ne=i("p"),Ot=n("The preprocessing function needs to:"),ut=k(),me=i("ol"),Re=i("li"),It=n("Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Ut=k(),Ge=i("li"),Nt=n("Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),Bt=k(),Pe=i("li"),Wt=n("Truncate sequences to be no longer than the maximum length set by the "),Xe=i("code"),Yt=n("max_length"),Ht=n(" parameter."),$t=k(),S(Le.$$.fragment),dt=k(),ne=i("p"),Zt=n("Use \u{1F917} Datasets "),De=i("a"),Jt=n("map"),Kt=n(" function to apply the preprocessing function over the entire dataset. You can speed up the "),Qe=i("code"),Rt=n("map"),Gt=n(" function by setting "),Ve=i("code"),Xt=n("batched=True"),Qt=n(" to process multiple elements of the dataset at once:"),_t=k(),S(Me.$$.fragment),gt=k(),S(be.$$.fragment),kt=k(),te=i("p"),Vt=n("Use "),Be=i("a"),es=n("DataCollatorForSeq2Seq"),ts=n(" to create a batch of examples. It will also "),et=i("em"),ss=n("dynamically pad"),as=n(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),tt=i("code"),rs=n("tokenizer"),ns=n(" function by setting "),st=i("code"),os=n("padding=True"),ls=n(", dynamic padding is more efficient."),wt=k(),S(je.$$.fragment),bt=k(),ge=i("h2"),qe=i("a"),at=i("span"),S(Ce.$$.fragment),is=k(),rt=i("span"),ps=n("Train"),jt=k(),S(ve.$$.fragment),qt=k(),S(Se.$$.fragment),this.h()},l(e){const c=Bs('[data-svelte="svelte-1phssyn"]',document.head);t=p(c,"META",{name:!0,content:!0}),c.forEach(a),l=w(e),s=p(e,"H1",{class:!0});var Oe=h(s);f=p(Oe,"A",{id:!0,class:!0,href:!0});var nt=h(f);_=p(nt,"SPAN",{});var ot=h(_);y($.$$.fragment,ot),ot.forEach(a),nt.forEach(a),g=w(Oe),v=p(Oe,"SPAN",{});var lt=h(v);d=o(lt,"Translation"),lt.forEach(a),Oe.forEach(a),b=w(e),y(P.$$.fragment,e),I=w(e),M=p(e,"P",{});var it=h(M);re=o(it,"Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),it.forEach(a),O=w(e),U=p(e,"P",{});var We=h(U);Q=o(We,"This guide will show you how to fine-tune "),Z=p(We,"A",{href:!0,rel:!0});var ms=h(Z);Y=o(ms,"T5"),ms.forEach(a),J=o(We," on the English-French subset of the "),N=p(We,"A",{href:!0,rel:!0});var hs=h(N);H=o(hs,"OPUS Books"),hs.forEach(a),oe=o(We," dataset to translate English text to French."),We.forEach(a),B=w(e),y(C.$$.fragment,e),se=w(e),j=p(e,"H2",{class:!0});var St=h(j);L=p(St,"A",{id:!0,class:!0,href:!0});var cs=h(L);D=p(cs,"SPAN",{});var us=h(D);y(W.$$.fragment,us),us.forEach(a),cs.forEach(a),K=w(St),V=p(St,"SPAN",{});var $s=h(V);ee=o($s,"Load OPUS Books dataset"),$s.forEach(a),St.forEach(a),R=w(e),ae=p(e,"P",{});var ds=h(ae);Ee=o(ds,"Load the OPUS Books dataset from the \u{1F917} Datasets library:"),ds.forEach(a),ce=w(e),y(G.$$.fragment,e),ue=w(e),m=p(e,"P",{});var _s=h(m);A=o(_s,"Split this dataset into a train and test set:"),_s.forEach(a),ie=w(e),y(pe.$$.fragment,e),Te=w(e),$e=p(e,"P",{});var gs=h($e);ke=o(gs,"Then take a look at an example:"),gs.forEach(a),xe=w(e),y(fe.$$.fragment,e),de=w(e),le=p(e,"P",{});var yt=h(le);Ie=o(yt,"The "),Ze=p(yt,"CODE",{});var ks=h(Ze);Pt=o(ks,"translation"),ks.forEach(a),Lt=o(yt," field is a dictionary containing the English and French translations of the text."),yt.forEach(a),pt=w(e),_e=p(e,"H2",{class:!0});var Et=h(_e);we=p(Et,"A",{id:!0,class:!0,href:!0});var ws=h(we);Je=p(ws,"SPAN",{});var bs=h(Je);y(ze.$$.fragment,bs),bs.forEach(a),ws.forEach(a),Dt=w(Et),Ke=p(Et,"SPAN",{});var js=h(Ke);Mt=o(js,"Preprocess"),js.forEach(a),Et.forEach(a),ft=w(e),y(Ae.$$.fragment,e),mt=w(e),Ue=p(e,"P",{});var qs=h(Ue);Ct=o(qs,"Load the T5 tokenizer to process the language pairs:"),qs.forEach(a),ht=w(e),y(Fe.$$.fragment,e),ct=w(e),Ne=p(e,"P",{});var vs=h(Ne);Ot=o(vs,"The preprocessing function needs to:"),vs.forEach(a),ut=w(e),me=p(e,"OL",{});var Ye=h(me);Re=p(Ye,"LI",{});var Ss=h(Re);It=o(Ss,"Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Ss.forEach(a),Ut=w(Ye),Ge=p(Ye,"LI",{});var ys=h(Ge);Nt=o(ys,"Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),ys.forEach(a),Bt=w(Ye),Pe=p(Ye,"LI",{});var Tt=h(Pe);Wt=o(Tt,"Truncate sequences to be no longer than the maximum length set by the "),Xe=p(Tt,"CODE",{});var Es=h(Xe);Yt=o(Es,"max_length"),Es.forEach(a),Ht=o(Tt," parameter."),Tt.forEach(a),Ye.forEach(a),$t=w(e),y(Le.$$.fragment,e),dt=w(e),ne=p(e,"P",{});var ye=h(ne);Zt=o(ye,"Use \u{1F917} Datasets "),De=p(ye,"A",{href:!0,rel:!0});var Ts=h(De);Jt=o(Ts,"map"),Ts.forEach(a),Kt=o(ye," function to apply the preprocessing function over the entire dataset. You can speed up the "),Qe=p(ye,"CODE",{});var xs=h(Qe);Rt=o(xs,"map"),xs.forEach(a),Gt=o(ye," function by setting "),Ve=p(ye,"CODE",{});var zs=h(Ve);Xt=o(zs,"batched=True"),zs.forEach(a),Qt=o(ye," to process multiple elements of the dataset at once:"),ye.forEach(a),_t=w(e),y(Me.$$.fragment,e),gt=w(e),y(be.$$.fragment,e),kt=w(e),te=p(e,"P",{});var he=h(te);Vt=o(he,"Use "),Be=p(he,"A",{href:!0});var As=h(Be);es=o(As,"DataCollatorForSeq2Seq"),As.forEach(a),ts=o(he," to create a batch of examples. It will also "),et=p(he,"EM",{});var Fs=h(et);ss=o(Fs,"dynamically pad"),Fs.forEach(a),as=o(he," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),tt=p(he,"CODE",{});var Ps=h(tt);rs=o(Ps,"tokenizer"),Ps.forEach(a),ns=o(he," function by setting "),st=p(he,"CODE",{});var Ls=h(st);os=o(Ls,"padding=True"),Ls.forEach(a),ls=o(he,", dynamic padding is more efficient."),he.forEach(a),wt=w(e),y(je.$$.fragment,e),bt=w(e),ge=p(e,"H2",{class:!0});var xt=h(ge);qe=p(xt,"A",{id:!0,class:!0,href:!0});var Ds=h(qe);at=p(Ds,"SPAN",{});var Ms=h(at);y(Ce.$$.fragment,Ms),Ms.forEach(a),Ds.forEach(a),is=w(xt),rt=p(xt,"SPAN",{});var Cs=h(rt);ps=o(Cs,"Train"),Cs.forEach(a),xt.forEach(a),jt=w(e),y(ve.$$.fragment,e),qt=w(e),y(Se.$$.fragment,e),this.h()},h(){q(t,"name","hf:doc:metadata"),q(t,"content",JSON.stringify(la)),q(f,"id","translation"),q(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(f,"href","#translation"),q(s,"class","relative group"),q(Z,"href","https://huggingface.co/t5-small"),q(Z,"rel","nofollow"),q(N,"href","https://huggingface.co/datasets/opus_books"),q(N,"rel","nofollow"),q(L,"id","load-opus-books-dataset"),q(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(L,"href","#load-opus-books-dataset"),q(j,"class","relative group"),q(we,"id","preprocess"),q(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(we,"href","#preprocess"),q(_e,"class","relative group"),q(De,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),q(De,"rel","nofollow"),q(Be,"href","/docs/transformers/pr_18169/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq"),q(qe,"id","train"),q(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(qe,"href","#train"),q(ge,"class","relative group")},m(e,c){r(document.head,t),u(e,l,c),u(e,s,c),r(s,f),r(f,_),E($,_,null),r(s,g),r(s,v),r(v,d),u(e,b,c),E(P,e,c),u(e,I,c),u(e,M,c),r(M,re),u(e,O,c),u(e,U,c),r(U,Q),r(U,Z),r(Z,Y),r(U,J),r(U,N),r(N,H),r(U,oe),u(e,B,c),E(C,e,c),u(e,se,c),u(e,j,c),r(j,L),r(L,D),E(W,D,null),r(j,K),r(j,V),r(V,ee),u(e,R,c),u(e,ae,c),r(ae,Ee),u(e,ce,c),E(G,e,c),u(e,ue,c),u(e,m,c),r(m,A),u(e,ie,c),E(pe,e,c),u(e,Te,c),u(e,$e,c),r($e,ke),u(e,xe,c),E(fe,e,c),u(e,de,c),u(e,le,c),r(le,Ie),r(le,Ze),r(Ze,Pt),r(le,Lt),u(e,pt,c),u(e,_e,c),r(_e,we),r(we,Je),E(ze,Je,null),r(_e,Dt),r(_e,Ke),r(Ke,Mt),u(e,ft,c),E(Ae,e,c),u(e,mt,c),u(e,Ue,c),r(Ue,Ct),u(e,ht,c),E(Fe,e,c),u(e,ct,c),u(e,Ne,c),r(Ne,Ot),u(e,ut,c),u(e,me,c),r(me,Re),r(Re,It),r(me,Ut),r(me,Ge),r(Ge,Nt),r(me,Bt),r(me,Pe),r(Pe,Wt),r(Pe,Xe),r(Xe,Yt),r(Pe,Ht),u(e,$t,c),E(Le,e,c),u(e,dt,c),u(e,ne,c),r(ne,Zt),r(ne,De),r(De,Jt),r(ne,Kt),r(ne,Qe),r(Qe,Rt),r(ne,Gt),r(ne,Ve),r(Ve,Xt),r(ne,Qt),u(e,_t,c),E(Me,e,c),u(e,gt,c),E(be,e,c),u(e,kt,c),u(e,te,c),r(te,Vt),r(te,Be),r(Be,es),r(te,ts),r(te,et),r(et,ss),r(te,as),r(te,tt),r(tt,rs),r(te,ns),r(te,st),r(st,os),r(te,ls),u(e,wt,c),E(je,e,c),u(e,bt,c),u(e,ge,c),r(ge,qe),r(qe,at),E(Ce,at,null),r(ge,is),r(ge,rt),r(rt,ps),u(e,jt,c),E(ve,e,c),u(e,qt,c),E(Se,e,c),vt=!0},p(e,[c]){const Oe={};c&2&&(Oe.$$scope={dirty:c,ctx:e}),C.$set(Oe);const nt={};c&2&&(nt.$$scope={dirty:c,ctx:e}),be.$set(nt);const ot={};c&2&&(ot.$$scope={dirty:c,ctx:e}),je.$set(ot);const lt={};c&2&&(lt.$$scope={dirty:c,ctx:e}),ve.$set(lt);const it={};c&2&&(it.$$scope={dirty:c,ctx:e}),Se.$set(it)},i(e){vt||(T($.$$.fragment,e),T(P.$$.fragment,e),T(C.$$.fragment,e),T(W.$$.fragment,e),T(G.$$.fragment,e),T(pe.$$.fragment,e),T(fe.$$.fragment,e),T(ze.$$.fragment,e),T(Ae.$$.fragment,e),T(Fe.$$.fragment,e),T(Le.$$.fragment,e),T(Me.$$.fragment,e),T(be.$$.fragment,e),T(je.$$.fragment,e),T(Ce.$$.fragment,e),T(ve.$$.fragment,e),T(Se.$$.fragment,e),vt=!0)},o(e){x($.$$.fragment,e),x(P.$$.fragment,e),x(C.$$.fragment,e),x(W.$$.fragment,e),x(G.$$.fragment,e),x(pe.$$.fragment,e),x(fe.$$.fragment,e),x(ze.$$.fragment,e),x(Ae.$$.fragment,e),x(Fe.$$.fragment,e),x(Le.$$.fragment,e),x(Me.$$.fragment,e),x(be.$$.fragment,e),x(je.$$.fragment,e),x(Ce.$$.fragment,e),x(ve.$$.fragment,e),x(Se.$$.fragment,e),vt=!1},d(e){a(t),e&&a(l),e&&a(s),z($),e&&a(b),z(P,e),e&&a(I),e&&a(M),e&&a(O),e&&a(U),e&&a(B),z(C,e),e&&a(se),e&&a(j),z(W),e&&a(R),e&&a(ae),e&&a(ce),z(G,e),e&&a(ue),e&&a(m),e&&a(ie),z(pe,e),e&&a(Te),e&&a($e),e&&a(xe),z(fe,e),e&&a(de),e&&a(le),e&&a(pt),e&&a(_e),z(ze),e&&a(ft),z(Ae,e),e&&a(mt),e&&a(Ue),e&&a(ht),z(Fe,e),e&&a(ct),e&&a(Ne),e&&a(ut),e&&a(me),e&&a($t),z(Le,e),e&&a(dt),e&&a(ne),e&&a(_t),z(Me,e),e&&a(gt),z(be,e),e&&a(kt),e&&a(te),e&&a(wt),z(je,e),e&&a(bt),e&&a(ge),z(Ce),e&&a(jt),z(ve,e),e&&a(qt),z(Se,e)}}}const la={local:"translation",sections:[{local:"load-opus-books-dataset",title:"Load OPUS Books dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Translation"};function ia(F){return Ws(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $a extends Is{constructor(t){super();Us(this,t,ia,oa,Ns,{})}}export{$a as default,la as metadata};
