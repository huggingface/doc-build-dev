import{S as gn,i as _n,s as kn,e as r,k as d,w as v,t as l,M as vn,c as o,d as s,m as c,a as n,x as $,h as p,b as i,F as t,g as m,y as b,q as P,o as w,B as N,v as $n,L as Bt}from"../../chunks/vendor-6b77c823.js";import{D as C}from"../../chunks/Docstring-1088f2fb.js";import{C as Rt}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as W}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Gt}from"../../chunks/ExampleCodeBlock-5212b321.js";function bn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetModel

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetModel.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),{c(){f=r("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=o(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Pn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetEncoder.from_pretrained("patrickvonplaten/xprophetnet-large-uncased-standalone")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=r("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=o(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function wn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetDecoder.from_pretrained(
    "patrickvonplaten/xprophetnet-large-uncased-standalone", add_cross_attention=False
)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetDecoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=r("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=o(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Nn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForConditionalGeneration.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),{c(){f=r("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=o(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Ln(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForCausalLM.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
import torch

tokenizer_enc = XLMRobertaTokenizer.from_pretrained("xlm-roberta-large")
tokenizer_dec = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "xlm-roberta-large", "microsoft/xprophetnet-large-wiki100-cased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec("us rejects charges against its ambassador in bolivia", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = XLMRobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;xlm-roberta-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;xlm-roberta-large&quot;</span>, <span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(<span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){f=r("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=o(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Mn(q){let f,L,_,u,k,a,g,mt,Cs,Ot,A,ut,Ds,As,ue,Ss,Is,Ht,S,U,ft,fe,Fs,gt,Gs,Vt,Y,Bs,ge,Rs,Os,Wt,We,Hs,Ut,Ue,Vs,Yt,Ye,_t,Ws,Jt,J,Us,_e,Ys,Js,Qt,I,Q,kt,ke,Qs,vt,Zs,Zt,F,ve,Ks,$e,er,Je,tr,sr,Kt,G,Z,$t,be,rr,bt,or,es,M,Pe,nr,E,ar,Qe,ir,lr,Ze,pr,dr,we,cr,hr,mr,Ne,ur,Ke,fr,gr,_r,D,Le,kr,Pt,vr,$r,Me,et,br,wt,Pr,wr,tt,Nr,Nt,Lr,Mr,K,xe,xr,Lt,qr,Er,ee,qe,Xr,Mt,yr,jr,te,Ee,zr,Xe,Tr,xt,Cr,Dr,ts,B,se,qt,ye,Ar,Et,Sr,ss,X,je,Ir,ze,Fr,st,Gr,Br,Rr,re,rs,R,oe,Xt,Te,Or,yt,Hr,os,y,Ce,Vr,De,Wr,rt,Ur,Yr,Jr,ne,ns,O,ae,jt,Ae,Qr,zt,Zr,as,j,Se,Kr,Ie,eo,ot,to,so,ro,ie,is,H,le,Tt,Fe,oo,Ct,no,ls,z,Ge,ao,Be,io,nt,lo,po,co,pe,ps,V,de,Dt,Re,ho,At,mo,ds,T,Oe,uo,He,fo,at,go,_o,ko,ce,cs;return a=new W({}),fe=new W({}),ke=new W({}),ve=new C({props:{name:"class transformers.XLMProphetNetConfig",anchor:"transformers.XLMProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py#L29"}}),be=new W({}),Pe=new C({props:{name:"class transformers.XLMProphetNetTokenizer",anchor:"transformers.XLMProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '[SEP]'"},{name:"eos_token",val:" = '[SEP]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"unk_token",val:" = '[UNK]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.XLMProphetNetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.XLMProphetNetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.XLMProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.XLMProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.XLMProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.XLMProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.XLMProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.XLMProphetNetTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L57"}}),Le=new C({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L314",returnDescription:`
<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),xe=new C({props:{name:"convert_tokens_to_string",anchor:"transformers.XLMProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L292"}}),qe=new C({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L241",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ee=new C({props:{name:"get_special_tokens_mask",anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L213",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ye=new W({}),je=new C({props:{name:"class transformers.XLMProphetNetModel",anchor:"transformers.XLMProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L86"}}),re=new Gt({props:{anchor:"transformers.XLMProphetNetModel.example",$$slots:{default:[bn]},$$scope:{ctx:q}}}),Te=new W({}),Ce=new C({props:{name:"class transformers.XLMProphetNetEncoder",anchor:"transformers.XLMProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L38"}}),ne=new Gt({props:{anchor:"transformers.XLMProphetNetEncoder.example",$$slots:{default:[Pn]},$$scope:{ctx:q}}}),Ae=new W({}),Se=new C({props:{name:"class transformers.XLMProphetNetDecoder",anchor:"transformers.XLMProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L61"}}),ie=new Gt({props:{anchor:"transformers.XLMProphetNetDecoder.example",$$slots:{default:[wn]},$$scope:{ctx:q}}}),Fe=new W({}),Ge=new C({props:{name:"class transformers.XLMProphetNetForConditionalGeneration",anchor:"transformers.XLMProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L112"}}),pe=new Gt({props:{anchor:"transformers.XLMProphetNetForConditionalGeneration.example",$$slots:{default:[Nn]},$$scope:{ctx:q}}}),Re=new W({}),Oe=new C({props:{name:"class transformers.XLMProphetNetForCausalLM",anchor:"transformers.XLMProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L138"}}),ce=new Gt({props:{anchor:"transformers.XLMProphetNetForCausalLM.example",$$slots:{default:[Ln]},$$scope:{ctx:q}}}),{c(){f=r("meta"),L=d(),_=r("h1"),u=r("a"),k=r("span"),v(a.$$.fragment),g=d(),mt=r("span"),Cs=l("XLM-ProphetNet"),Ot=d(),A=r("p"),ut=r("strong"),Ds=l("DISCLAIMER:"),As=l(" If you see something strange, file a "),ue=r("a"),Ss=l("Github Issue"),Is=l(` and assign
@patrickvonplaten`),Ht=d(),S=r("h2"),U=r("a"),ft=r("span"),v(fe.$$.fragment),Fs=d(),gt=r("span"),Gs=l("Overview"),Vt=d(),Y=r("p"),Bs=l("The XLM-ProphetNet model was proposed in "),ge=r("a"),Rs=l("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Os=l(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Wt=d(),We=r("p"),Hs=l(`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),Ut=d(),Ue=r("p"),Vs=l("The abstract from the paper is the following:"),Yt=d(),Ye=r("p"),_t=r("em"),Ws=l(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Jt=d(),J=r("p"),Us=l("The Authors\u2019 code can be found "),_e=r("a"),Ys=l("here"),Js=l("."),Qt=d(),I=r("h2"),Q=r("a"),kt=r("span"),v(ke.$$.fragment),Qs=d(),vt=r("span"),Zs=l("XLMProphetNetConfig"),Zt=d(),F=r("div"),v(ve.$$.fragment),Ks=d(),$e=r("p"),er=l("This class overrides "),Je=r("a"),tr=l("ProphetNetConfig"),sr=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Kt=d(),G=r("h2"),Z=r("a"),$t=r("span"),v(be.$$.fragment),rr=d(),bt=r("span"),or=l("XLMProphetNetTokenizer"),es=d(),M=r("div"),v(Pe.$$.fragment),nr=d(),E=r("p"),ar=l("Adapted from "),Qe=r("a"),ir=l("RobertaTokenizer"),lr=l(" and "),Ze=r("a"),pr=l("XLNetTokenizer"),dr=l(`. Based on
`),we=r("a"),cr=l("SentencePiece"),hr=l("."),mr=d(),Ne=r("p"),ur=l("This tokenizer inherits from "),Ke=r("a"),fr=l("PreTrainedTokenizer"),gr=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),_r=d(),D=r("div"),v(Le.$$.fragment),kr=d(),Pt=r("p"),vr=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),$r=d(),Me=r("ul"),et=r("li"),br=l("single sequence: "),wt=r("code"),Pr=l("X [SEP]"),wr=d(),tt=r("li"),Nr=l("pair of sequences: "),Nt=r("code"),Lr=l("A [SEP] B [SEP]"),Mr=d(),K=r("div"),v(xe.$$.fragment),xr=d(),Lt=r("p"),qr=l("Converts a sequence of tokens (strings for sub-words) in a single string."),Er=d(),ee=r("div"),v(qe.$$.fragment),Xr=d(),Mt=r("p"),yr=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),jr=d(),te=r("div"),v(Ee.$$.fragment),zr=d(),Xe=r("p"),Tr=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),xt=r("code"),Cr=l("prepare_for_model"),Dr=l(" method."),ts=d(),B=r("h2"),se=r("a"),qt=r("span"),v(ye.$$.fragment),Ar=d(),Et=r("span"),Sr=l("XLMProphetNetModel"),ss=d(),X=r("div"),v(je.$$.fragment),Ir=d(),ze=r("p"),Fr=l("This class overrides "),st=r("a"),Gr=l("ProphetNetModel"),Br=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Rr=d(),v(re.$$.fragment),rs=d(),R=r("h2"),oe=r("a"),Xt=r("span"),v(Te.$$.fragment),Or=d(),yt=r("span"),Hr=l("XLMProphetNetEncoder"),os=d(),y=r("div"),v(Ce.$$.fragment),Vr=d(),De=r("p"),Wr=l("This class overrides "),rt=r("a"),Ur=l("ProphetNetEncoder"),Yr=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Jr=d(),v(ne.$$.fragment),ns=d(),O=r("h2"),ae=r("a"),jt=r("span"),v(Ae.$$.fragment),Qr=d(),zt=r("span"),Zr=l("XLMProphetNetDecoder"),as=d(),j=r("div"),v(Se.$$.fragment),Kr=d(),Ie=r("p"),eo=l("This class overrides "),ot=r("a"),to=l("ProphetNetDecoder"),so=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ro=d(),v(ie.$$.fragment),is=d(),H=r("h2"),le=r("a"),Tt=r("span"),v(Fe.$$.fragment),oo=d(),Ct=r("span"),no=l("XLMProphetNetForConditionalGeneration"),ls=d(),z=r("div"),v(Ge.$$.fragment),ao=d(),Be=r("p"),io=l("This class overrides "),nt=r("a"),lo=l("ProphetNetForConditionalGeneration"),po=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),co=d(),v(pe.$$.fragment),ps=d(),V=r("h2"),de=r("a"),Dt=r("span"),v(Re.$$.fragment),ho=d(),At=r("span"),mo=l("XLMProphetNetForCausalLM"),ds=d(),T=r("div"),v(Oe.$$.fragment),uo=d(),He=r("p"),fo=l("This class overrides "),at=r("a"),go=l("ProphetNetForCausalLM"),_o=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),ko=d(),v(ce.$$.fragment),this.h()},l(e){const h=vn('[data-svelte="svelte-1phssyn"]',document.head);f=o(h,"META",{name:!0,content:!0}),h.forEach(s),L=c(e),_=o(e,"H1",{class:!0});var Ve=n(_);u=o(Ve,"A",{id:!0,class:!0,href:!0});var St=n(u);k=o(St,"SPAN",{});var It=n(k);$(a.$$.fragment,It),It.forEach(s),St.forEach(s),g=c(Ve),mt=o(Ve,"SPAN",{});var Ft=n(mt);Cs=p(Ft,"XLM-ProphetNet"),Ft.forEach(s),Ve.forEach(s),Ot=c(e),A=o(e,"P",{});var he=n(A);ut=o(he,"STRONG",{});var bo=n(ut);Ds=p(bo,"DISCLAIMER:"),bo.forEach(s),As=p(he," If you see something strange, file a "),ue=o(he,"A",{href:!0,rel:!0});var Po=n(ue);Ss=p(Po,"Github Issue"),Po.forEach(s),Is=p(he,` and assign
@patrickvonplaten`),he.forEach(s),Ht=c(e),S=o(e,"H2",{class:!0});var hs=n(S);U=o(hs,"A",{id:!0,class:!0,href:!0});var wo=n(U);ft=o(wo,"SPAN",{});var No=n(ft);$(fe.$$.fragment,No),No.forEach(s),wo.forEach(s),Fs=c(hs),gt=o(hs,"SPAN",{});var Lo=n(gt);Gs=p(Lo,"Overview"),Lo.forEach(s),hs.forEach(s),Vt=c(e),Y=o(e,"P",{});var ms=n(Y);Bs=p(ms,"The XLM-ProphetNet model was proposed in "),ge=o(ms,"A",{href:!0,rel:!0});var Mo=n(ge);Rs=p(Mo,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Mo.forEach(s),Os=p(ms,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),ms.forEach(s),Wt=c(e),We=o(e,"P",{});var xo=n(We);Hs=p(xo,`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),xo.forEach(s),Ut=c(e),Ue=o(e,"P",{});var qo=n(Ue);Vs=p(qo,"The abstract from the paper is the following:"),qo.forEach(s),Yt=c(e),Ye=o(e,"P",{});var Eo=n(Ye);_t=o(Eo,"EM",{});var Xo=n(_t);Ws=p(Xo,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Xo.forEach(s),Eo.forEach(s),Jt=c(e),J=o(e,"P",{});var us=n(J);Us=p(us,"The Authors\u2019 code can be found "),_e=o(us,"A",{href:!0,rel:!0});var yo=n(_e);Ys=p(yo,"here"),yo.forEach(s),Js=p(us,"."),us.forEach(s),Qt=c(e),I=o(e,"H2",{class:!0});var fs=n(I);Q=o(fs,"A",{id:!0,class:!0,href:!0});var jo=n(Q);kt=o(jo,"SPAN",{});var zo=n(kt);$(ke.$$.fragment,zo),zo.forEach(s),jo.forEach(s),Qs=c(fs),vt=o(fs,"SPAN",{});var To=n(vt);Zs=p(To,"XLMProphetNetConfig"),To.forEach(s),fs.forEach(s),Zt=c(e),F=o(e,"DIV",{class:!0});var gs=n(F);$(ve.$$.fragment,gs),Ks=c(gs),$e=o(gs,"P",{});var _s=n($e);er=p(_s,"This class overrides "),Je=o(_s,"A",{href:!0});var Co=n(Je);tr=p(Co,"ProphetNetConfig"),Co.forEach(s),sr=p(_s,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),_s.forEach(s),gs.forEach(s),Kt=c(e),G=o(e,"H2",{class:!0});var ks=n(G);Z=o(ks,"A",{id:!0,class:!0,href:!0});var Do=n(Z);$t=o(Do,"SPAN",{});var Ao=n($t);$(be.$$.fragment,Ao),Ao.forEach(s),Do.forEach(s),rr=c(ks),bt=o(ks,"SPAN",{});var So=n(bt);or=p(So,"XLMProphetNetTokenizer"),So.forEach(s),ks.forEach(s),es=c(e),M=o(e,"DIV",{class:!0});var x=n(M);$(Pe.$$.fragment,x),nr=c(x),E=o(x,"P",{});var me=n(E);ar=p(me,"Adapted from "),Qe=o(me,"A",{href:!0});var Io=n(Qe);ir=p(Io,"RobertaTokenizer"),Io.forEach(s),lr=p(me," and "),Ze=o(me,"A",{href:!0});var Fo=n(Ze);pr=p(Fo,"XLNetTokenizer"),Fo.forEach(s),dr=p(me,`. Based on
`),we=o(me,"A",{href:!0,rel:!0});var Go=n(we);cr=p(Go,"SentencePiece"),Go.forEach(s),hr=p(me,"."),me.forEach(s),mr=c(x),Ne=o(x,"P",{});var vs=n(Ne);ur=p(vs,"This tokenizer inherits from "),Ke=o(vs,"A",{href:!0});var Bo=n(Ke);fr=p(Bo,"PreTrainedTokenizer"),Bo.forEach(s),gr=p(vs,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),vs.forEach(s),_r=c(x),D=o(x,"DIV",{class:!0});var it=n(D);$(Le.$$.fragment,it),kr=c(it),Pt=o(it,"P",{});var Ro=n(Pt);vr=p(Ro,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),Ro.forEach(s),$r=c(it),Me=o(it,"UL",{});var $s=n(Me);et=o($s,"LI",{});var vo=n(et);br=p(vo,"single sequence: "),wt=o(vo,"CODE",{});var Oo=n(wt);Pr=p(Oo,"X [SEP]"),Oo.forEach(s),vo.forEach(s),wr=c($s),tt=o($s,"LI",{});var $o=n(tt);Nr=p($o,"pair of sequences: "),Nt=o($o,"CODE",{});var Ho=n(Nt);Lr=p(Ho,"A [SEP] B [SEP]"),Ho.forEach(s),$o.forEach(s),$s.forEach(s),it.forEach(s),Mr=c(x),K=o(x,"DIV",{class:!0});var bs=n(K);$(xe.$$.fragment,bs),xr=c(bs),Lt=o(bs,"P",{});var Vo=n(Lt);qr=p(Vo,"Converts a sequence of tokens (strings for sub-words) in a single string."),Vo.forEach(s),bs.forEach(s),Er=c(x),ee=o(x,"DIV",{class:!0});var Ps=n(ee);$(qe.$$.fragment,Ps),Xr=c(Ps),Mt=o(Ps,"P",{});var Wo=n(Mt);yr=p(Wo,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),Wo.forEach(s),Ps.forEach(s),jr=c(x),te=o(x,"DIV",{class:!0});var ws=n(te);$(Ee.$$.fragment,ws),zr=c(ws),Xe=o(ws,"P",{});var Ns=n(Xe);Tr=p(Ns,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),xt=o(Ns,"CODE",{});var Uo=n(xt);Cr=p(Uo,"prepare_for_model"),Uo.forEach(s),Dr=p(Ns," method."),Ns.forEach(s),ws.forEach(s),x.forEach(s),ts=c(e),B=o(e,"H2",{class:!0});var Ls=n(B);se=o(Ls,"A",{id:!0,class:!0,href:!0});var Yo=n(se);qt=o(Yo,"SPAN",{});var Jo=n(qt);$(ye.$$.fragment,Jo),Jo.forEach(s),Yo.forEach(s),Ar=c(Ls),Et=o(Ls,"SPAN",{});var Qo=n(Et);Sr=p(Qo,"XLMProphetNetModel"),Qo.forEach(s),Ls.forEach(s),ss=c(e),X=o(e,"DIV",{class:!0});var lt=n(X);$(je.$$.fragment,lt),Ir=c(lt),ze=o(lt,"P",{});var Ms=n(ze);Fr=p(Ms,"This class overrides "),st=o(Ms,"A",{href:!0});var Zo=n(st);Gr=p(Zo,"ProphetNetModel"),Zo.forEach(s),Br=p(Ms,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ms.forEach(s),Rr=c(lt),$(re.$$.fragment,lt),lt.forEach(s),rs=c(e),R=o(e,"H2",{class:!0});var xs=n(R);oe=o(xs,"A",{id:!0,class:!0,href:!0});var Ko=n(oe);Xt=o(Ko,"SPAN",{});var en=n(Xt);$(Te.$$.fragment,en),en.forEach(s),Ko.forEach(s),Or=c(xs),yt=o(xs,"SPAN",{});var tn=n(yt);Hr=p(tn,"XLMProphetNetEncoder"),tn.forEach(s),xs.forEach(s),os=c(e),y=o(e,"DIV",{class:!0});var pt=n(y);$(Ce.$$.fragment,pt),Vr=c(pt),De=o(pt,"P",{});var qs=n(De);Wr=p(qs,"This class overrides "),rt=o(qs,"A",{href:!0});var sn=n(rt);Ur=p(sn,"ProphetNetEncoder"),sn.forEach(s),Yr=p(qs,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),qs.forEach(s),Jr=c(pt),$(ne.$$.fragment,pt),pt.forEach(s),ns=c(e),O=o(e,"H2",{class:!0});var Es=n(O);ae=o(Es,"A",{id:!0,class:!0,href:!0});var rn=n(ae);jt=o(rn,"SPAN",{});var on=n(jt);$(Ae.$$.fragment,on),on.forEach(s),rn.forEach(s),Qr=c(Es),zt=o(Es,"SPAN",{});var nn=n(zt);Zr=p(nn,"XLMProphetNetDecoder"),nn.forEach(s),Es.forEach(s),as=c(e),j=o(e,"DIV",{class:!0});var dt=n(j);$(Se.$$.fragment,dt),Kr=c(dt),Ie=o(dt,"P",{});var Xs=n(Ie);eo=p(Xs,"This class overrides "),ot=o(Xs,"A",{href:!0});var an=n(ot);to=p(an,"ProphetNetDecoder"),an.forEach(s),so=p(Xs,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Xs.forEach(s),ro=c(dt),$(ie.$$.fragment,dt),dt.forEach(s),is=c(e),H=o(e,"H2",{class:!0});var ys=n(H);le=o(ys,"A",{id:!0,class:!0,href:!0});var ln=n(le);Tt=o(ln,"SPAN",{});var pn=n(Tt);$(Fe.$$.fragment,pn),pn.forEach(s),ln.forEach(s),oo=c(ys),Ct=o(ys,"SPAN",{});var dn=n(Ct);no=p(dn,"XLMProphetNetForConditionalGeneration"),dn.forEach(s),ys.forEach(s),ls=c(e),z=o(e,"DIV",{class:!0});var ct=n(z);$(Ge.$$.fragment,ct),ao=c(ct),Be=o(ct,"P",{});var js=n(Be);io=p(js,"This class overrides "),nt=o(js,"A",{href:!0});var cn=n(nt);lo=p(cn,"ProphetNetForConditionalGeneration"),cn.forEach(s),po=p(js,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),js.forEach(s),co=c(ct),$(pe.$$.fragment,ct),ct.forEach(s),ps=c(e),V=o(e,"H2",{class:!0});var zs=n(V);de=o(zs,"A",{id:!0,class:!0,href:!0});var hn=n(de);Dt=o(hn,"SPAN",{});var mn=n(Dt);$(Re.$$.fragment,mn),mn.forEach(s),hn.forEach(s),ho=c(zs),At=o(zs,"SPAN",{});var un=n(At);mo=p(un,"XLMProphetNetForCausalLM"),un.forEach(s),zs.forEach(s),ds=c(e),T=o(e,"DIV",{class:!0});var ht=n(T);$(Oe.$$.fragment,ht),uo=c(ht),He=o(ht,"P",{});var Ts=n(He);fo=p(Ts,"This class overrides "),at=o(Ts,"A",{href:!0});var fn=n(at);go=p(fn,"ProphetNetForCausalLM"),fn.forEach(s),_o=p(Ts,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ts.forEach(s),ko=c(ht),$(ce.$$.fragment,ht),ht.forEach(s),this.h()},h(){i(f,"name","hf:doc:metadata"),i(f,"content",JSON.stringify(xn)),i(u,"id","xlmprophetnet"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#xlmprophetnet"),i(_,"class","relative group"),i(ue,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(ue,"rel","nofollow"),i(U,"id","overview"),i(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(U,"href","#overview"),i(S,"class","relative group"),i(ge,"href","https://arxiv.org/abs/2001.04063"),i(ge,"rel","nofollow"),i(_e,"href","https://github.com/microsoft/ProphetNet"),i(_e,"rel","nofollow"),i(Q,"id","transformers.XLMProphetNetConfig"),i(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Q,"href","#transformers.XLMProphetNetConfig"),i(I,"class","relative group"),i(Je,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetConfig"),i(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Z,"id","transformers.XLMProphetNetTokenizer"),i(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Z,"href","#transformers.XLMProphetNetTokenizer"),i(G,"class","relative group"),i(Qe,"href","/docs/transformers/pr_16919/en/model_doc/roberta#transformers.RobertaTokenizer"),i(Ze,"href","/docs/transformers/pr_16919/en/model_doc/xlnet#transformers.XLNetTokenizer"),i(we,"href","https://github.com/google/sentencepiece"),i(we,"rel","nofollow"),i(Ke,"href","/docs/transformers/pr_16919/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(se,"id","transformers.XLMProphetNetModel"),i(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(se,"href","#transformers.XLMProphetNetModel"),i(B,"class","relative group"),i(st,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(oe,"id","transformers.XLMProphetNetEncoder"),i(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(oe,"href","#transformers.XLMProphetNetEncoder"),i(R,"class","relative group"),i(rt,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ae,"id","transformers.XLMProphetNetDecoder"),i(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ae,"href","#transformers.XLMProphetNetDecoder"),i(O,"class","relative group"),i(ot,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(le,"id","transformers.XLMProphetNetForConditionalGeneration"),i(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(le,"href","#transformers.XLMProphetNetForConditionalGeneration"),i(H,"class","relative group"),i(nt,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(de,"id","transformers.XLMProphetNetForCausalLM"),i(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(de,"href","#transformers.XLMProphetNetForCausalLM"),i(V,"class","relative group"),i(at,"href","/docs/transformers/pr_16919/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,h){t(document.head,f),m(e,L,h),m(e,_,h),t(_,u),t(u,k),b(a,k,null),t(_,g),t(_,mt),t(mt,Cs),m(e,Ot,h),m(e,A,h),t(A,ut),t(ut,Ds),t(A,As),t(A,ue),t(ue,Ss),t(A,Is),m(e,Ht,h),m(e,S,h),t(S,U),t(U,ft),b(fe,ft,null),t(S,Fs),t(S,gt),t(gt,Gs),m(e,Vt,h),m(e,Y,h),t(Y,Bs),t(Y,ge),t(ge,Rs),t(Y,Os),m(e,Wt,h),m(e,We,h),t(We,Hs),m(e,Ut,h),m(e,Ue,h),t(Ue,Vs),m(e,Yt,h),m(e,Ye,h),t(Ye,_t),t(_t,Ws),m(e,Jt,h),m(e,J,h),t(J,Us),t(J,_e),t(_e,Ys),t(J,Js),m(e,Qt,h),m(e,I,h),t(I,Q),t(Q,kt),b(ke,kt,null),t(I,Qs),t(I,vt),t(vt,Zs),m(e,Zt,h),m(e,F,h),b(ve,F,null),t(F,Ks),t(F,$e),t($e,er),t($e,Je),t(Je,tr),t($e,sr),m(e,Kt,h),m(e,G,h),t(G,Z),t(Z,$t),b(be,$t,null),t(G,rr),t(G,bt),t(bt,or),m(e,es,h),m(e,M,h),b(Pe,M,null),t(M,nr),t(M,E),t(E,ar),t(E,Qe),t(Qe,ir),t(E,lr),t(E,Ze),t(Ze,pr),t(E,dr),t(E,we),t(we,cr),t(E,hr),t(M,mr),t(M,Ne),t(Ne,ur),t(Ne,Ke),t(Ke,fr),t(Ne,gr),t(M,_r),t(M,D),b(Le,D,null),t(D,kr),t(D,Pt),t(Pt,vr),t(D,$r),t(D,Me),t(Me,et),t(et,br),t(et,wt),t(wt,Pr),t(Me,wr),t(Me,tt),t(tt,Nr),t(tt,Nt),t(Nt,Lr),t(M,Mr),t(M,K),b(xe,K,null),t(K,xr),t(K,Lt),t(Lt,qr),t(M,Er),t(M,ee),b(qe,ee,null),t(ee,Xr),t(ee,Mt),t(Mt,yr),t(M,jr),t(M,te),b(Ee,te,null),t(te,zr),t(te,Xe),t(Xe,Tr),t(Xe,xt),t(xt,Cr),t(Xe,Dr),m(e,ts,h),m(e,B,h),t(B,se),t(se,qt),b(ye,qt,null),t(B,Ar),t(B,Et),t(Et,Sr),m(e,ss,h),m(e,X,h),b(je,X,null),t(X,Ir),t(X,ze),t(ze,Fr),t(ze,st),t(st,Gr),t(ze,Br),t(X,Rr),b(re,X,null),m(e,rs,h),m(e,R,h),t(R,oe),t(oe,Xt),b(Te,Xt,null),t(R,Or),t(R,yt),t(yt,Hr),m(e,os,h),m(e,y,h),b(Ce,y,null),t(y,Vr),t(y,De),t(De,Wr),t(De,rt),t(rt,Ur),t(De,Yr),t(y,Jr),b(ne,y,null),m(e,ns,h),m(e,O,h),t(O,ae),t(ae,jt),b(Ae,jt,null),t(O,Qr),t(O,zt),t(zt,Zr),m(e,as,h),m(e,j,h),b(Se,j,null),t(j,Kr),t(j,Ie),t(Ie,eo),t(Ie,ot),t(ot,to),t(Ie,so),t(j,ro),b(ie,j,null),m(e,is,h),m(e,H,h),t(H,le),t(le,Tt),b(Fe,Tt,null),t(H,oo),t(H,Ct),t(Ct,no),m(e,ls,h),m(e,z,h),b(Ge,z,null),t(z,ao),t(z,Be),t(Be,io),t(Be,nt),t(nt,lo),t(Be,po),t(z,co),b(pe,z,null),m(e,ps,h),m(e,V,h),t(V,de),t(de,Dt),b(Re,Dt,null),t(V,ho),t(V,At),t(At,mo),m(e,ds,h),m(e,T,h),b(Oe,T,null),t(T,uo),t(T,He),t(He,fo),t(He,at),t(at,go),t(He,_o),t(T,ko),b(ce,T,null),cs=!0},p(e,[h]){const Ve={};h&2&&(Ve.$$scope={dirty:h,ctx:e}),re.$set(Ve);const St={};h&2&&(St.$$scope={dirty:h,ctx:e}),ne.$set(St);const It={};h&2&&(It.$$scope={dirty:h,ctx:e}),ie.$set(It);const Ft={};h&2&&(Ft.$$scope={dirty:h,ctx:e}),pe.$set(Ft);const he={};h&2&&(he.$$scope={dirty:h,ctx:e}),ce.$set(he)},i(e){cs||(P(a.$$.fragment,e),P(fe.$$.fragment,e),P(ke.$$.fragment,e),P(ve.$$.fragment,e),P(be.$$.fragment,e),P(Pe.$$.fragment,e),P(Le.$$.fragment,e),P(xe.$$.fragment,e),P(qe.$$.fragment,e),P(Ee.$$.fragment,e),P(ye.$$.fragment,e),P(je.$$.fragment,e),P(re.$$.fragment,e),P(Te.$$.fragment,e),P(Ce.$$.fragment,e),P(ne.$$.fragment,e),P(Ae.$$.fragment,e),P(Se.$$.fragment,e),P(ie.$$.fragment,e),P(Fe.$$.fragment,e),P(Ge.$$.fragment,e),P(pe.$$.fragment,e),P(Re.$$.fragment,e),P(Oe.$$.fragment,e),P(ce.$$.fragment,e),cs=!0)},o(e){w(a.$$.fragment,e),w(fe.$$.fragment,e),w(ke.$$.fragment,e),w(ve.$$.fragment,e),w(be.$$.fragment,e),w(Pe.$$.fragment,e),w(Le.$$.fragment,e),w(xe.$$.fragment,e),w(qe.$$.fragment,e),w(Ee.$$.fragment,e),w(ye.$$.fragment,e),w(je.$$.fragment,e),w(re.$$.fragment,e),w(Te.$$.fragment,e),w(Ce.$$.fragment,e),w(ne.$$.fragment,e),w(Ae.$$.fragment,e),w(Se.$$.fragment,e),w(ie.$$.fragment,e),w(Fe.$$.fragment,e),w(Ge.$$.fragment,e),w(pe.$$.fragment,e),w(Re.$$.fragment,e),w(Oe.$$.fragment,e),w(ce.$$.fragment,e),cs=!1},d(e){s(f),e&&s(L),e&&s(_),N(a),e&&s(Ot),e&&s(A),e&&s(Ht),e&&s(S),N(fe),e&&s(Vt),e&&s(Y),e&&s(Wt),e&&s(We),e&&s(Ut),e&&s(Ue),e&&s(Yt),e&&s(Ye),e&&s(Jt),e&&s(J),e&&s(Qt),e&&s(I),N(ke),e&&s(Zt),e&&s(F),N(ve),e&&s(Kt),e&&s(G),N(be),e&&s(es),e&&s(M),N(Pe),N(Le),N(xe),N(qe),N(Ee),e&&s(ts),e&&s(B),N(ye),e&&s(ss),e&&s(X),N(je),N(re),e&&s(rs),e&&s(R),N(Te),e&&s(os),e&&s(y),N(Ce),N(ne),e&&s(ns),e&&s(O),N(Ae),e&&s(as),e&&s(j),N(Se),N(ie),e&&s(is),e&&s(H),N(Fe),e&&s(ls),e&&s(z),N(Ge),N(pe),e&&s(ps),e&&s(V),N(Re),e&&s(ds),e&&s(T),N(Oe),N(ce)}}}const xn={local:"xlmprophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.XLMProphetNetConfig",title:"XLMProphetNetConfig"},{local:"transformers.XLMProphetNetTokenizer",title:"XLMProphetNetTokenizer"},{local:"transformers.XLMProphetNetModel",title:"XLMProphetNetModel"},{local:"transformers.XLMProphetNetEncoder",title:"XLMProphetNetEncoder"},{local:"transformers.XLMProphetNetDecoder",title:"XLMProphetNetDecoder"},{local:"transformers.XLMProphetNetForConditionalGeneration",title:"XLMProphetNetForConditionalGeneration"},{local:"transformers.XLMProphetNetForCausalLM",title:"XLMProphetNetForCausalLM"}],title:"XLM-ProphetNet"};function qn(q){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tn extends gn{constructor(f){super();_n(this,f,qn,Mn,kn,{})}}export{Tn as default,xn as metadata};
