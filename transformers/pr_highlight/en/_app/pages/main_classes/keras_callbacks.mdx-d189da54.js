import{S as mt,i as ut,s as ft,e as s,k as m,w as $,t as i,M as bt,c as n,d as o,m as u,a as l,x as y,h as d,b as f,F as t,g as k,y as w,q as C,o as x,B as T,v as gt,L as Ve}from"../../chunks/vendor-6b77c823.js";import{D as ht}from"../../chunks/Docstring-1088f2fb.js";import{C as Je}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Re}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Ge}from"../../chunks/ExampleCodeBlock-5212b321.js";function _t(K){let r,p;return r=new Je({props:{code:`from datasets import load_metric

rouge_metric = load_metric("rouge")


def rouge_fn(predictions, labels):
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels)
    return {key: value.mid.fmeasure * 100 for key, value in result.items()}`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

rouge_metric = load_metric(<span class="hljs-string">&quot;rouge&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">rouge_fn</span>(<span class="hljs-params">predictions, labels</span>):
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    result = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels)
    <span class="hljs-keyword">return</span> {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}`}}),{c(){$(r.$$.fragment)},l(a){y(r.$$.fragment,a)},m(a,b){w(r,a,b),p=!0},p:Ve,i(a){p||(C(r.$$.fragment,a),p=!0)},o(a){x(r.$$.fragment,a),p=!1},d(a){T(r,a)}}}function kt(K){let r,p;return r=new Je({props:{code:"{'rouge1': 37.4199, 'rouge2': 13.9768, 'rougeL': 34.361, 'rougeLsum': 35.0781",highlighted:'{&#x27;rouge1&#x27;: <span class="hljs-number">37.4199</span>, &#x27;rouge2&#x27;: <span class="hljs-number">13.9768</span>, &#x27;rougeL&#x27;: <span class="hljs-number">34.361</span>, &#x27;rougeLsum&#x27;: <span class="hljs-number">35.0781</span>'}}),{c(){$(r.$$.fragment)},l(a){y(r.$$.fragment,a)},m(a,b){w(r,a,b),p=!0},p:Ve,i(a){p||(C(r.$$.fragment,a),p=!0)},o(a){x(r.$$.fragment,a),p=!1},d(a){T(r,a)}}}function vt(K){let r,p;return r=new Je({props:{code:`from transformers.keras_callbacks import PushToHubCallback

push_to_hub_callback = PushToHubCallback(
    output_dir="./model_save",
    tokenizer=tokenizer,
    hub_model_id="gpt5-7xlarge",
)

model.fit(train_dataset, callbacks=[push_to_hub_callback])`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

push_to_hub_callback = PushToHubCallback(
    output_dir=<span class="hljs-string">&quot;./model_save&quot;</span>,
    tokenizer=tokenizer,
    hub_model_id=<span class="hljs-string">&quot;gpt5-7xlarge&quot;</span>,
)

model.fit(train_dataset, callbacks=[push_to_hub_callback])`}}),{c(){$(r.$$.fragment)},l(a){y(r.$$.fragment,a)},m(a,b){w(r,a,b),p=!0},p:Ve,i(a){p||(C(r.$$.fragment,a),p=!0)},o(a){x(r.$$.fragment,a),p=!1},d(a){T(r,a)}}}function $t(K){let r,p,a,b,R,N,be,G,ge,ce,B,_e,ie,E,q,V,L,ke,J,ve,de,h,S,$e,g,ye,Q,we,Ce,X,xe,Te,Y,Pe,Ee,Z,He,ze,Ke,ee,qe,Oe,O,je,te,Me,De,j,pe,H,M,ae,I,Ne,oe,Le,he,v,A,Se,z,Ie,re,Ae,Ue,se,Be,We,Fe,D,me;return N=new Re({}),L=new Re({}),S=new ht({props:{name:"class transformers.KerasMetricCallback",anchor:"transformers.KerasMetricCallback",parameters:[{name:"metric_fn",val:": typing.Callable"},{name:"eval_dataset",val:": typing.Union[tensorflow.python.data.ops.dataset_ops.DatasetV2, numpy.ndarray, tensorflow.python.framework.ops.Tensor, tuple, dict]"},{name:"output_cols",val:": typing.Optional[typing.List[str]] = None"},{name:"label_cols",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"predict_with_generate",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.KerasMetricCallback.metric_fn",description:`<strong>metric_fn</strong> (<code>Callable</code>) &#x2014;
Metric function provided by the user. It will be called with two arguments - <code>predictions</code> and <code>labels</code>.
These contain the model&#x2019;s outputs and matching labels from the dataset. It should return a dict mapping
metric names to numerical values.`,name:"metric_fn"},{anchor:"transformers.KerasMetricCallback.eval_dataset",description:`<strong>eval_dataset</strong> (<code>tf.data.Dataset</code> or <code>dict</code> or <code>tuple</code> or <code>np.ndarray</code> or <code>tf.Tensor</code>) &#x2014;
Validation data to be used to generate predictions for the <code>metric_fn</code>.`,name:"eval_dataset"},{anchor:"transformers.KerasMetricCallback.output_cols",description:"<strong>output_cols</strong> (`List[str], <em>optional</em>) &#x2014;\nA list of columns to be retained from the model output as the predictions. Defaults to all.",name:"output_cols"},{anchor:"transformers.KerasMetricCallback.label_cols",description:`<strong>label_cols</strong> (&#x2019;<code>List[str]</code>, <em>optional</em>&#x2019;) &#x2014;
A list of columns to be retained from the input dataset as the labels. Will be autodetected if this is not
supplied.`,name:"label_cols"},{anchor:"transformers.KerasMetricCallback.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size. Only used when the data is not a pre-batched <code>tf.data.Dataset</code>.`,name:"batch_size"},{anchor:"transformers.KerasMetricCallback.predict_with_generate",description:`<strong>predict_with_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether we should use <code>model.generate()</code> to get outputs for the model.`,name:"predict_with_generate"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/keras_callbacks.py#L22"}}),O=new Ge({props:{anchor:"transformers.KerasMetricCallback.example",$$slots:{default:[_t]},$$scope:{ctx:K}}}),j=new Ge({props:{anchor:"transformers.KerasMetricCallback.example-2",$$slots:{default:[kt]},$$scope:{ctx:K}}}),I=new Re({}),A=new ht({props:{name:"class transformers.PushToHubCallback",anchor:"transformers.PushToHubCallback",parameters:[{name:"output_dir",val:": typing.Union[str, pathlib.Path]"},{name:"save_strategy",val:": typing.Union[str, transformers.trainer_utils.IntervalStrategy] = 'epoch'"},{name:"save_steps",val:": typing.Optional[int] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"hub_model_id",val:": typing.Optional[str] = None"},{name:"hub_token",val:": typing.Optional[str] = None"},{name:"checkpoint",val:": bool = False"},{name:"**model_card_args",val:""}],parametersDescription:[{anchor:"transformers.PushToHubCallback.output_dir",description:`<strong>output_dir</strong> (<code>str</code>) &#x2014;
The output directory where the model predictions and checkpoints will be written and synced with the
repository on the Hub.`,name:"output_dir"},{anchor:"transformers.PushToHubCallback.save_strategy",description:`<strong>save_strategy</strong> (<code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/trainer_utils#transformers.IntervalStrategy">IntervalStrategy</a>, <em>optional</em>, defaults to <code>&quot;epoch&quot;</code>) &#x2014;
The checkpoint save strategy to adopt during training. Possible values are:</p>
<ul>
<li><code>&quot;no&quot;</code>: Save is done at the end of training.</li>
<li><code>&quot;epoch&quot;</code>: Save is done at the end of each epoch.</li>
<li><code>&quot;steps&quot;</code>: Save is done every <code>save_steps</code></li>
</ul>`,name:"save_strategy"},{anchor:"transformers.PushToHubCallback.save_steps",description:`<strong>save_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of steps between saves when using the &#x201C;steps&#x201D; <code>save_strategy</code>.`,name:"save_steps"},{anchor:"transformers.PushToHubCallback.tokenizer",description:`<strong>tokenizer</strong> (<code>PreTrainedTokenizerBase</code>, <em>optional</em>) &#x2014;
The tokenizer used by the model. If supplied, will be uploaded to the repo alongside the weights.`,name:"tokenizer"},{anchor:"transformers.PushToHubCallback.hub_model_id",description:`<strong>hub_model_id</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the repository to keep in sync with the local <code>output_dir</code>. It can be a simple model ID in
which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,
for instance <code>&quot;user_name/model&quot;</code>, which allows you to push to an organization you are a member of with
<code>&quot;organization_name/model&quot;</code>.</p>
<p>Will default to to the name of <code>output_dir</code>.`,name:"hub_model_id"},{anchor:"transformers.PushToHubCallback.hub_token",description:`<strong>hub_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with
<code>huggingface-cli login</code>.`,name:"hub_token"},{anchor:"transformers.PushToHubCallback.checkpoint",description:`<strong>checkpoint</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to save full training checkpoints (including epoch and optimizer state) to allow training to be
resumed. Only usable when <code>save_strategy</code> is <code>&quot;epoch&quot;</code>.`,name:"checkpoint"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/keras_callbacks.py#L242"}}),D=new Ge({props:{anchor:"transformers.PushToHubCallback.example",$$slots:{default:[vt]},$$scope:{ctx:K}}}),{c(){r=s("meta"),p=m(),a=s("h1"),b=s("a"),R=s("span"),$(N.$$.fragment),be=m(),G=s("span"),ge=i("Keras callbacks"),ce=m(),B=s("p"),_e=i(`When training a Transformers model with Keras, there are some library-specific callbacks available to automate common
tasks:`),ie=m(),E=s("h2"),q=s("a"),V=s("span"),$(L.$$.fragment),ke=m(),J=s("span"),ve=i("KerasMetricCallback"),de=m(),h=s("div"),$(S.$$.fragment),$e=m(),g=s("p"),ye=i(`Callback to compute metrics at the end of every epoch. Unlike normal Keras metrics, these do not need to be
compilable by TF. It is particularly useful for common NLP metrics like BLEU and ROUGE that require string
operations or generation loops that cannot be compiled. Predictions (or generations) will be computed on the
`),Q=s("code"),we=i("eval_dataset"),Ce=i(" before being passed to the "),X=s("code"),xe=i("metric_fn"),Te=i(" in "),Y=s("code"),Pe=i("np.ndarray"),Ee=i(" format. The "),Z=s("code"),He=i("metric_fn"),ze=i(` should compute
metrics and return a dict mapping metric names to metric values.`),Ke=m(),ee=s("p"),qe=i(`We provide an example of a suitable metric_fn that computes ROUGE scores for a summarization model below. Note that
this example skips some post-processing for readability and simplicity, and should probably not be used as-is!`),Oe=m(),$(O.$$.fragment),je=m(),te=s("p"),Me=i("The above function will return a dict containing values which will be logged like any other Keras metric:"),De=m(),$(j.$$.fragment),pe=m(),H=s("h2"),M=s("a"),ae=s("span"),$(I.$$.fragment),Ne=m(),oe=s("span"),Le=i("PushToHubCallback"),he=m(),v=s("div"),$(A.$$.fragment),Se=m(),z=s("p"),Ie=i(`Callback that will save and push the model to the Hub regularly. By default, it pushes once per epoch, but this can
be changed with the `),re=s("code"),Ae=i("save_strategy"),Ue=i(` argument. Pushed models can be accessed like any other model on the hub, such
as with the `),se=s("code"),Be=i("from_pretrained"),We=i(" method."),Fe=m(),$(D.$$.fragment),this.h()},l(e){const c=bt('[data-svelte="svelte-1phssyn"]',document.head);r=n(c,"META",{name:!0,content:!0}),c.forEach(o),p=u(e),a=n(e,"H1",{class:!0});var U=l(a);b=n(U,"A",{id:!0,class:!0,href:!0});var ne=l(b);R=n(ne,"SPAN",{});var le=l(R);y(N.$$.fragment,le),le.forEach(o),ne.forEach(o),be=u(U),G=n(U,"SPAN",{});var Qe=l(G);ge=d(Qe,"Keras callbacks"),Qe.forEach(o),U.forEach(o),ce=u(e),B=n(e,"P",{});var Xe=l(B);_e=d(Xe,`When training a Transformers model with Keras, there are some library-specific callbacks available to automate common
tasks:`),Xe.forEach(o),ie=u(e),E=n(e,"H2",{class:!0});var ue=l(E);q=n(ue,"A",{id:!0,class:!0,href:!0});var Ye=l(q);V=n(Ye,"SPAN",{});var Ze=l(V);y(L.$$.fragment,Ze),Ze.forEach(o),Ye.forEach(o),ke=u(ue),J=n(ue,"SPAN",{});var et=l(J);ve=d(et,"KerasMetricCallback"),et.forEach(o),ue.forEach(o),de=u(e),h=n(e,"DIV",{class:!0});var _=l(h);y(S.$$.fragment,_),$e=u(_),g=n(_,"P",{});var P=l(g);ye=d(P,`Callback to compute metrics at the end of every epoch. Unlike normal Keras metrics, these do not need to be
compilable by TF. It is particularly useful for common NLP metrics like BLEU and ROUGE that require string
operations or generation loops that cannot be compiled. Predictions (or generations) will be computed on the
`),Q=n(P,"CODE",{});var tt=l(Q);we=d(tt,"eval_dataset"),tt.forEach(o),Ce=d(P," before being passed to the "),X=n(P,"CODE",{});var at=l(X);xe=d(at,"metric_fn"),at.forEach(o),Te=d(P," in "),Y=n(P,"CODE",{});var ot=l(Y);Pe=d(ot,"np.ndarray"),ot.forEach(o),Ee=d(P," format. The "),Z=n(P,"CODE",{});var rt=l(Z);He=d(rt,"metric_fn"),rt.forEach(o),ze=d(P,` should compute
metrics and return a dict mapping metric names to metric values.`),P.forEach(o),Ke=u(_),ee=n(_,"P",{});var st=l(ee);qe=d(st,`We provide an example of a suitable metric_fn that computes ROUGE scores for a summarization model below. Note that
this example skips some post-processing for readability and simplicity, and should probably not be used as-is!`),st.forEach(o),Oe=u(_),y(O.$$.fragment,_),je=u(_),te=n(_,"P",{});var nt=l(te);Me=d(nt,"The above function will return a dict containing values which will be logged like any other Keras metric:"),nt.forEach(o),De=u(_),y(j.$$.fragment,_),_.forEach(o),pe=u(e),H=n(e,"H2",{class:!0});var fe=l(H);M=n(fe,"A",{id:!0,class:!0,href:!0});var lt=l(M);ae=n(lt,"SPAN",{});var ct=l(ae);y(I.$$.fragment,ct),ct.forEach(o),lt.forEach(o),Ne=u(fe),oe=n(fe,"SPAN",{});var it=l(oe);Le=d(it,"PushToHubCallback"),it.forEach(o),fe.forEach(o),he=u(e),v=n(e,"DIV",{class:!0});var W=l(v);y(A.$$.fragment,W),Se=u(W),z=n(W,"P",{});var F=l(z);Ie=d(F,`Callback that will save and push the model to the Hub regularly. By default, it pushes once per epoch, but this can
be changed with the `),re=n(F,"CODE",{});var dt=l(re);Ae=d(dt,"save_strategy"),dt.forEach(o),Ue=d(F,` argument. Pushed models can be accessed like any other model on the hub, such
as with the `),se=n(F,"CODE",{});var pt=l(se);Be=d(pt,"from_pretrained"),pt.forEach(o),We=d(F," method."),F.forEach(o),Fe=u(W),y(D.$$.fragment,W),W.forEach(o),this.h()},h(){f(r,"name","hf:doc:metadata"),f(r,"content",JSON.stringify(yt)),f(b,"id","keras-callbacks"),f(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(b,"href","#keras-callbacks"),f(a,"class","relative group"),f(q,"id","transformers.KerasMetricCallback"),f(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(q,"href","#transformers.KerasMetricCallback"),f(E,"class","relative group"),f(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(M,"id","transformers.PushToHubCallback"),f(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(M,"href","#transformers.PushToHubCallback"),f(H,"class","relative group"),f(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,r),k(e,p,c),k(e,a,c),t(a,b),t(b,R),w(N,R,null),t(a,be),t(a,G),t(G,ge),k(e,ce,c),k(e,B,c),t(B,_e),k(e,ie,c),k(e,E,c),t(E,q),t(q,V),w(L,V,null),t(E,ke),t(E,J),t(J,ve),k(e,de,c),k(e,h,c),w(S,h,null),t(h,$e),t(h,g),t(g,ye),t(g,Q),t(Q,we),t(g,Ce),t(g,X),t(X,xe),t(g,Te),t(g,Y),t(Y,Pe),t(g,Ee),t(g,Z),t(Z,He),t(g,ze),t(h,Ke),t(h,ee),t(ee,qe),t(h,Oe),w(O,h,null),t(h,je),t(h,te),t(te,Me),t(h,De),w(j,h,null),k(e,pe,c),k(e,H,c),t(H,M),t(M,ae),w(I,ae,null),t(H,Ne),t(H,oe),t(oe,Le),k(e,he,c),k(e,v,c),w(A,v,null),t(v,Se),t(v,z),t(z,Ie),t(z,re),t(re,Ae),t(z,Ue),t(z,se),t(se,Be),t(z,We),t(v,Fe),w(D,v,null),me=!0},p(e,[c]){const U={};c&2&&(U.$$scope={dirty:c,ctx:e}),O.$set(U);const ne={};c&2&&(ne.$$scope={dirty:c,ctx:e}),j.$set(ne);const le={};c&2&&(le.$$scope={dirty:c,ctx:e}),D.$set(le)},i(e){me||(C(N.$$.fragment,e),C(L.$$.fragment,e),C(S.$$.fragment,e),C(O.$$.fragment,e),C(j.$$.fragment,e),C(I.$$.fragment,e),C(A.$$.fragment,e),C(D.$$.fragment,e),me=!0)},o(e){x(N.$$.fragment,e),x(L.$$.fragment,e),x(S.$$.fragment,e),x(O.$$.fragment,e),x(j.$$.fragment,e),x(I.$$.fragment,e),x(A.$$.fragment,e),x(D.$$.fragment,e),me=!1},d(e){o(r),e&&o(p),e&&o(a),T(N),e&&o(ce),e&&o(B),e&&o(ie),e&&o(E),T(L),e&&o(de),e&&o(h),T(S),T(O),T(j),e&&o(pe),e&&o(H),T(I),e&&o(he),e&&o(v),T(A),T(D)}}}const yt={local:"keras-callbacks",sections:[{local:"transformers.KerasMetricCallback",title:"KerasMetricCallback"},{local:"transformers.PushToHubCallback",title:"PushToHubCallback"}],title:"Keras callbacks"};function wt(K){return gt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ht extends mt{constructor(r){super();ut(this,r,wt,$t,ft,{})}}export{Ht as default,yt as metadata};
