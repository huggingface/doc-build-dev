import{S as zf,i as Pf,s as Ff,e as l,k as h,w as k,t as o,M as Sf,c as i,d as s,m as d,a as p,x as y,h as n,b as $,G as t,g as u,y as b,q as E,o as T,B as A,v as Mf,L as ge}from"../chunks/vendor-hf-doc-builder.js";import{T as Js}from"../chunks/Tip-hf-doc-builder.js";import{Y as qf}from"../chunks/Youtube-hf-doc-builder.js";import{I as Pe}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as L}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as Cf}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as as,M as ce}from"../chunks/Markdown-hf-doc-builder.js";function If(z){let a,m;return{c(){a=l("p"),m=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var c=p(a);m=n(c,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),c.forEach(s)},m(r,c){u(r,a,c),t(a,m)},d(r){r&&s(a)}}}function Nf(z){let a,m,r,c,g,_,v,C;return{c(){a=l("p"),m=o("For more details about the "),r=l("a"),c=o("pipeline()"),g=o(" and associated tasks, refer to the documentation "),_=l("a"),v=o("here"),C=o("."),this.h()},l(j){a=i(j,"P",{});var S=p(a);m=n(S,"For more details about the "),r=i(S,"A",{href:!0});var I=p(r);c=n(I,"pipeline()"),I.forEach(s),g=n(S," and associated tasks, refer to the documentation "),_=i(S,"A",{href:!0});var N=p(_);v=n(N,"here"),N.forEach(s),C=n(S,"."),S.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(_,"href","./main_classes/pipelines")},m(j,S){u(j,a,S),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C)},d(j){j&&s(a)}}}function Of(z){let a,m;return a=new L({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Df(z){let a,m;return a=new ce({props:{$$slots:{default:[Of]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Hf(z){let a,m;return a=new L({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Lf(z){let a,m;return a=new ce({props:{$$slots:{default:[Hf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Wf(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W;return D=new L({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("AutoModelForSequenceClassification"),g=o(" and "),_=l("a"),v=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),j=l("code"),S=o("AutoClass"),I=o(" below):"),N=h(),k(D.$$.fragment),this.h()},l(q){a=i(q,"P",{});var M=p(a);m=n(M,"Use the "),r=i(M,"A",{href:!0});var w=p(r);c=n(w,"AutoModelForSequenceClassification"),w.forEach(s),g=n(M," and "),_=i(M,"A",{href:!0});var F=p(_);v=n(F,"AutoTokenizer"),F.forEach(s),C=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),j=i(M,"CODE",{});var R=p(j);S=n(R,"AutoClass"),R.forEach(s),I=n(M," below):"),M.forEach(s),N=d(q),y(D.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer")},m(q,M){u(q,a,M),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C),t(a,j),t(j,S),t(a,I),u(q,N,M),b(D,q,M),W=!0},p:ge,i(q){W||(E(D.$$.fragment,q),W=!0)},o(q){T(D.$$.fragment,q),W=!1},d(q){q&&s(a),q&&s(N),A(D,q)}}}function Uf(z){let a,m;return a=new ce({props:{$$slots:{default:[Wf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Rf(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W;return D=new L({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("TFAutoModelForSequenceClassification"),g=o(" and "),_=l("a"),v=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),j=l("code"),S=o("TFAutoClass"),I=o(" below):"),N=h(),k(D.$$.fragment),this.h()},l(q){a=i(q,"P",{});var M=p(a);m=n(M,"Use the "),r=i(M,"A",{href:!0});var w=p(r);c=n(w,"TFAutoModelForSequenceClassification"),w.forEach(s),g=n(M," and "),_=i(M,"A",{href:!0});var F=p(_);v=n(F,"AutoTokenizer"),F.forEach(s),C=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),j=i(M,"CODE",{});var R=p(j);S=n(R,"TFAutoClass"),R.forEach(s),I=n(M," below):"),M.forEach(s),N=d(q),y(D.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer")},m(q,M){u(q,a,M),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C),t(a,j),t(j,S),t(a,I),u(q,N,M),b(D,q,M),W=!0},p:ge,i(q){W||(E(D.$$.fragment,q),W=!0)},o(q){T(D.$$.fragment,q),W=!1},d(q){q&&s(a),q&&s(N),A(D,q)}}}function Gf(z){let a,m;return a=new ce({props:{$$slots:{default:[Rf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Bf(z){let a,m;return a=new L({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Yf(z){let a,m;return a=new ce({props:{$$slots:{default:[Bf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Jf(z){let a,m;return a=new L({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Qf(z){let a,m;return a=new ce({props:{$$slots:{default:[Jf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Vf(z){let a,m,r,c,g,_,v,C;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),g=o(" for which "),_=l("a"),v=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(j){a=i(j,"P",{});var S=p(a);m=n(S,"See the "),r=i(S,"A",{href:!0});var I=p(r);c=n(I,"task summary"),I.forEach(s),g=n(S," for which "),_=i(S,"A",{href:!0});var N=p(_);v=n(N,"AutoModel"),N.forEach(s),C=n(S," class to use for which task."),S.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(j,S){u(j,a,S),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C)},d(j){j&&s(a)}}}function Kf(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W,q,M,w,F,R,U,J,Y,se,Q,G,ee,V,K,he,re,$e,oe,te,ne,_e,P,O,le;return M=new L({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Js({props:{$$slots:{default:[Vf]},$$scope:{ctx:z}}}),ee=new L({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),O=new L({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("AutoModel"),g=o(" like you would load an "),_=l("a"),v=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),j=l("a"),S=o("AutoModel"),I=o(" for the task. Since you are doing text - or sequence - classification, load "),N=l("a"),D=o("AutoModelForSequenceClassification"),W=o(":"),q=h(),k(M.$$.fragment),w=h(),k(F.$$.fragment),R=h(),U=l("p"),J=o("Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),Y=l("code"),se=o("**"),Q=o(":"),G=h(),k(ee.$$.fragment),V=h(),K=l("p"),he=o("The model outputs the final activations in the "),re=l("code"),$e=o("logits"),oe=o(" attribute. Apply the softmax function to the "),te=l("code"),ne=o("logits"),_e=o(" to retrieve the probabilities:"),P=h(),k(O.$$.fragment),this.h()},l(x){a=i(x,"P",{});var H=p(a);m=n(H,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(H,"A",{href:!0});var ie=p(r);c=n(ie,"AutoModel"),ie.forEach(s),g=n(H," like you would load an "),_=i(H,"A",{href:!0});var Fe=p(_);v=n(Fe,"AutoTokenizer"),Fe.forEach(s),C=n(H,". The only difference is selecting the correct "),j=i(H,"A",{href:!0});var de=p(j);S=n(de,"AutoModel"),de.forEach(s),I=n(H," for the task. Since you are doing text - or sequence - classification, load "),N=i(H,"A",{href:!0});var we=p(N);D=n(we,"AutoModelForSequenceClassification"),we.forEach(s),W=n(H,":"),H.forEach(s),q=d(x),y(M.$$.fragment,x),w=d(x),y(F.$$.fragment,x),R=d(x),U=i(x,"P",{});var pe=p(U);J=n(pe,"Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),Y=i(pe,"CODE",{});var We=p(Y);se=n(We,"**"),We.forEach(s),Q=n(pe,":"),pe.forEach(s),G=d(x),y(ee.$$.fragment,x),V=d(x),K=i(x,"P",{});var ve=p(K);he=n(ve,"The model outputs the final activations in the "),re=i(ve,"CODE",{});var rs=p(re);$e=n(rs,"logits"),rs.forEach(s),oe=n(ve," attribute. Apply the softmax function to the "),te=i(ve,"CODE",{});var kt=p(te);ne=n(kt,"logits"),kt.forEach(s),_e=n(ve," to retrieve the probabilities:"),ve.forEach(s),P=d(x),y(O.$$.fragment,x),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(j,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),$(N,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(x,H){u(x,a,H),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C),t(a,j),t(j,S),t(a,I),t(a,N),t(N,D),t(a,W),u(x,q,H),b(M,x,H),u(x,w,H),b(F,x,H),u(x,R,H),u(x,U,H),t(U,J),t(U,Y),t(Y,se),t(U,Q),u(x,G,H),b(ee,x,H),u(x,V,H),u(x,K,H),t(K,he),t(K,re),t(re,$e),t(K,oe),t(K,te),t(te,ne),t(K,_e),u(x,P,H),b(O,x,H),le=!0},p(x,H){const ie={};H&2&&(ie.$$scope={dirty:H,ctx:x}),F.$set(ie)},i(x){le||(E(M.$$.fragment,x),E(F.$$.fragment,x),E(ee.$$.fragment,x),E(O.$$.fragment,x),le=!0)},o(x){T(M.$$.fragment,x),T(F.$$.fragment,x),T(ee.$$.fragment,x),T(O.$$.fragment,x),le=!1},d(x){x&&s(a),x&&s(q),A(M,x),x&&s(w),A(F,x),x&&s(R),x&&s(U),x&&s(G),A(ee,x),x&&s(V),x&&s(K),x&&s(P),A(O,x)}}}function Zf(z){let a,m;return a=new ce({props:{$$slots:{default:[Kf]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function Xf(z){let a,m,r,c,g,_,v,C;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),g=o(" for which "),_=l("a"),v=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(j){a=i(j,"P",{});var S=p(a);m=n(S,"See the "),r=i(S,"A",{href:!0});var I=p(r);c=n(I,"task summary"),I.forEach(s),g=n(S," for which "),_=i(S,"A",{href:!0});var N=p(_);v=n(N,"AutoModel"),N.forEach(s),C=n(S," class to use for which task."),S.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(j,S){u(j,a,S),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C)},d(j){j&&s(a)}}}function eu(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W,q,M,w,F,R,U,J,Y,se,Q,G,ee,V,K,he,re,$e,oe,te,ne,_e;return M=new L({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Js({props:{$$slots:{default:[Xf]},$$scope:{ctx:z}}}),se=new L({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ne=new L({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("TFAutoModel"),g=o(" like you would load an "),_=l("a"),v=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),j=l("a"),S=o("TFAutoModel"),I=o(" for the task. Since you are doing text - or sequence - classification, load "),N=l("a"),D=o("TFAutoModelForSequenceClassification"),W=o(":"),q=h(),k(M.$$.fragment),w=h(),k(F.$$.fragment),R=h(),U=l("p"),J=o("Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Y=h(),k(se.$$.fragment),Q=h(),G=l("p"),ee=o("The model outputs the final activations in the "),V=l("code"),K=o("logits"),he=o(" attribute. Apply the softmax function to the "),re=l("code"),$e=o("logits"),oe=o(" to retrieve the probabilities:"),te=h(),k(ne.$$.fragment),this.h()},l(P){a=i(P,"P",{});var O=p(a);m=n(O,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(O,"A",{href:!0});var le=p(r);c=n(le,"TFAutoModel"),le.forEach(s),g=n(O," like you would load an "),_=i(O,"A",{href:!0});var x=p(_);v=n(x,"AutoTokenizer"),x.forEach(s),C=n(O,". The only difference is selecting the correct "),j=i(O,"A",{href:!0});var H=p(j);S=n(H,"TFAutoModel"),H.forEach(s),I=n(O," for the task. Since you are doing text - or sequence - classification, load "),N=i(O,"A",{href:!0});var ie=p(N);D=n(ie,"TFAutoModelForSequenceClassification"),ie.forEach(s),W=n(O,":"),O.forEach(s),q=d(P),y(M.$$.fragment,P),w=d(P),y(F.$$.fragment,P),R=d(P),U=i(P,"P",{});var Fe=p(U);J=n(Fe,"Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Fe.forEach(s),Y=d(P),y(se.$$.fragment,P),Q=d(P),G=i(P,"P",{});var de=p(G);ee=n(de,"The model outputs the final activations in the "),V=i(de,"CODE",{});var we=p(V);K=n(we,"logits"),we.forEach(s),he=n(de," attribute. Apply the softmax function to the "),re=i(de,"CODE",{});var pe=p(re);$e=n(pe,"logits"),pe.forEach(s),oe=n(de," to retrieve the probabilities:"),de.forEach(s),te=d(P),y(ne.$$.fragment,P),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(j,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),$(N,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(P,O){u(P,a,O),t(a,m),t(a,r),t(r,c),t(a,g),t(a,_),t(_,v),t(a,C),t(a,j),t(j,S),t(a,I),t(a,N),t(N,D),t(a,W),u(P,q,O),b(M,P,O),u(P,w,O),b(F,P,O),u(P,R,O),u(P,U,O),t(U,J),u(P,Y,O),b(se,P,O),u(P,Q,O),u(P,G,O),t(G,ee),t(G,V),t(V,K),t(G,he),t(G,re),t(re,$e),t(G,oe),u(P,te,O),b(ne,P,O),_e=!0},p(P,O){const le={};O&2&&(le.$$scope={dirty:O,ctx:P}),F.$set(le)},i(P){_e||(E(M.$$.fragment,P),E(F.$$.fragment,P),E(se.$$.fragment,P),E(ne.$$.fragment,P),_e=!0)},o(P){T(M.$$.fragment,P),T(F.$$.fragment,P),T(se.$$.fragment,P),T(ne.$$.fragment,P),_e=!1},d(P){P&&s(a),P&&s(q),A(M,P),P&&s(w),A(F,P),P&&s(R),P&&s(U),P&&s(Y),A(se,P),P&&s(Q),P&&s(G),P&&s(te),A(ne,P)}}}function tu(z){let a,m;return a=new ce({props:{$$slots:{default:[eu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function su(z){let a,m,r,c,g;return{c(){a=l("p"),m=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=l("em"),c=o("before"),g=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(_){a=i(_,"P",{});var v=p(a);m=n(v,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=i(v,"EM",{});var C=p(r);c=n(C,"before"),C.forEach(s),g=n(v,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),v.forEach(s)},m(_,v){u(_,a,v),t(a,m),t(a,r),t(r,c),t(a,g)},d(_){_&&s(a)}}}function au(z){let a,m,r,c,g;return{c(){a=l("p"),m=o(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=l("code"),c=o("None"),g=o(" are ignored.")},l(_){a=i(_,"P",{});var v=p(a);m=n(v,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=i(v,"CODE",{});var C=p(r);c=n(C,"None"),C.forEach(s),g=n(v," are ignored."),v.forEach(s)},m(_,v){u(_,a,v),t(a,m),t(a,r),t(r,c),t(a,g)},d(_){_&&s(a)}}}function ru(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W,q,M;return v=new L({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),q=new L({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("PreTrainedModel.save_pretrained()"),g=o(":"),_=h(),k(v.$$.fragment),C=h(),j=l("p"),S=o("When you are ready to use the model again, reload it with "),I=l("a"),N=o("PreTrainedModel.from_pretrained()"),D=o(":"),W=h(),k(q.$$.fragment),this.h()},l(w){a=i(w,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"PreTrainedModel.save_pretrained()"),R.forEach(s),g=n(F,":"),F.forEach(s),_=d(w),y(v.$$.fragment,w),C=d(w),j=i(w,"P",{});var U=p(j);S=n(U,"When you are ready to use the model again, reload it with "),I=i(U,"A",{href:!0});var J=p(I);N=n(J,"PreTrainedModel.from_pretrained()"),J.forEach(s),D=n(U,":"),U.forEach(s),W=d(w),y(q.$$.fragment,w),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(I,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(w,F){u(w,a,F),t(a,m),t(a,r),t(r,c),t(a,g),u(w,_,F),b(v,w,F),u(w,C,F),u(w,j,F),t(j,S),t(j,I),t(I,N),t(j,D),u(w,W,F),b(q,w,F),M=!0},p:ge,i(w){M||(E(v.$$.fragment,w),E(q.$$.fragment,w),M=!0)},o(w){T(v.$$.fragment,w),T(q.$$.fragment,w),M=!1},d(w){w&&s(a),w&&s(_),A(v,w),w&&s(C),w&&s(j),w&&s(W),A(q,w)}}}function ou(z){let a,m;return a=new ce({props:{$$slots:{default:[ru]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function nu(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W,q,M;return v=new L({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),q=new L({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("TFPreTrainedModel.save_pretrained()"),g=o(":"),_=h(),k(v.$$.fragment),C=h(),j=l("p"),S=o("When you are ready to use the model again, reload it with "),I=l("a"),N=o("TFPreTrainedModel.from_pretrained()"),D=o(":"),W=h(),k(q.$$.fragment),this.h()},l(w){a=i(w,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"TFPreTrainedModel.save_pretrained()"),R.forEach(s),g=n(F,":"),F.forEach(s),_=d(w),y(v.$$.fragment,w),C=d(w),j=i(w,"P",{});var U=p(j);S=n(U,"When you are ready to use the model again, reload it with "),I=i(U,"A",{href:!0});var J=p(I);N=n(J,"TFPreTrainedModel.from_pretrained()"),J.forEach(s),D=n(U,":"),U.forEach(s),W=d(w),y(q.$$.fragment,w),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(I,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(w,F){u(w,a,F),t(a,m),t(a,r),t(r,c),t(a,g),u(w,_,F),b(v,w,F),u(w,C,F),u(w,j,F),t(j,S),t(j,I),t(I,N),t(j,D),u(w,W,F),b(q,w,F),M=!0},p:ge,i(w){M||(E(v.$$.fragment,w),E(q.$$.fragment,w),M=!0)},o(w){T(v.$$.fragment,w),T(q.$$.fragment,w),M=!1},d(w){w&&s(a),w&&s(_),A(v,w),w&&s(C),w&&s(j),w&&s(W),A(q,w)}}}function lu(z){let a,m;return a=new ce({props:{$$slots:{default:[nu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function iu(z){let a,m;return a=new L({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function pu(z){let a,m;return a=new ce({props:{$$slots:{default:[iu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function fu(z){let a,m;return a=new L({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p:ge,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function uu(z){let a,m;return a=new ce({props:{$$slots:{default:[fu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function mu(z){let a,m,r,c,g;return c=new L({props:{code:`from transformers import DistilBertForSequenceClassification

my_model = DistilBertForSequenceClassification(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = DistilBertForSequenceClassification(my_config)`}}),{c(){a=l("p"),m=o("Create a model from your custom configuration:"),r=h(),k(c.$$.fragment)},l(_){a=i(_,"P",{});var v=p(a);m=n(v,"Create a model from your custom configuration:"),v.forEach(s),r=d(_),y(c.$$.fragment,_)},m(_,v){u(_,a,v),t(a,m),u(_,r,v),b(c,_,v),g=!0},p:ge,i(_){g||(E(c.$$.fragment,_),g=!0)},o(_){T(c.$$.fragment,_),g=!1},d(_){_&&s(a),_&&s(r),A(c,_)}}}function cu(z){let a,m;return a=new ce({props:{$$slots:{default:[mu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function hu(z){let a,m,r,c,g;return c=new L({props:{code:`from transformers import TFDistilBertForSequenceClassification

my_model = TFDistilBertForSequenceClassification(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = TFDistilBertForSequenceClassification(my_config)`}}),{c(){a=l("p"),m=o("Create a model from your custom configuration:"),r=h(),k(c.$$.fragment)},l(_){a=i(_,"P",{});var v=p(a);m=n(v,"Create a model from your custom configuration:"),v.forEach(s),r=d(_),y(c.$$.fragment,_)},m(_,v){u(_,a,v),t(a,m),u(_,r,v),b(c,_,v),g=!0},p:ge,i(_){g||(E(c.$$.fragment,_),g=!0)},o(_){T(c.$$.fragment,_),g=!1},d(_){_&&s(a),_&&s(r),A(c,_)}}}function du(z){let a,m;return a=new ce({props:{$$slots:{default:[hu]},$$scope:{ctx:z}}}),{c(){k(a.$$.fragment)},l(r){y(a.$$.fragment,r)},m(r,c){b(a,r,c),m=!0},p(r,c){const g={};c&2&&(g.$$scope={dirty:c,ctx:r}),a.$set(g)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){A(a,r)}}}function $u(z){let a,m,r,c,g,_,v,C,j,S,I,N,D,W,q,M,w,F,R,U,J,Y,se,Q,G,ee,V,K,he,re,$e,oe,te,ne,_e,P,O,le,x,H,ie,Fe,de,we,pe,We,ve,rs,kt,B,Qs,zo,Po,Vs,Fo,So,Ks,Mo,Co,Zs,Io,No,Xs,Oo,Do,ea,Ho,Lo,ta,Wo,Uo,sa,Ro,Ya,yt,aa,Go,Bo,Ja,ke,ra,Yo,Jo,oa,Qo,Vo,na,Ko,Qa,bt,la,Zo,Xo,Va,Ue,ia,en,tn,pa,sn,Ka,Re,Za,Se,Ge,fa,Et,an,ua,rn,Xa,Be,on,os,nn,ln,er,ns,pn,tr,Ye,sr,Je,fn,ls,un,mn,ar,Tt,rr,ye,cn,At,hn,dn,ma,$n,_n,or,jt,nr,Qe,gn,is,wn,vn,lr,xt,ir,be,kn,ps,yn,bn,qt,En,Tn,pr,zt,fr,Ve,An,fs,jn,xn,ur,Pt,mr,Ee,qn,Ft,zn,Pn,St,Fn,Sn,cr,Mt,hr,Ke,Mn,ca,Cn,In,dr,Ct,$r,Ze,Nn,ha,On,Dn,_r,It,gr,Xe,Hn,us,Ln,Wn,wr,Me,et,da,Nt,Un,$a,Rn,vr,fe,Gn,ms,Bn,Yn,Ot,Jn,Qn,cs,Vn,Kn,Dt,Zn,Xn,kr,Ht,yr,tt,br,Te,el,hs,tl,sl,_a,al,rl,Er,Lt,Tr,Ae,ol,ds,nl,ll,$s,il,pl,Ar,Ce,st,ga,Wt,fl,wa,ul,jr,Ut,xr,Z,ml,_s,cl,hl,gs,dl,$l,ws,_l,gl,vs,wl,vl,va,kl,yl,ks,bl,El,qr,je,Tl,ka,Al,jl,ys,xl,ql,zr,Ie,at,ya,Rt,zl,ba,Pl,Pr,xe,Fl,Ea,Sl,Ml,bs,Cl,Il,Fr,rt,Nl,Es,Ol,Dl,Sr,Gt,Mr,ot,Hl,Ta,Ll,Wl,Cr,Ts,Ul,Ir,Bt,Nr,As,Rl,Or,nt,js,xs,Gl,Bl,Yl,qs,zs,Jl,Ql,Dr,lt,Vl,Ps,Kl,Zl,Hr,it,Lr,pt,Xl,Fs,ei,ti,Wr,Ne,ft,Aa,Yt,si,ja,ai,Ur,ut,Rr,mt,Gr,X,ri,Jt,xa,oi,ni,Qt,qa,li,ii,Ss,pi,fi,za,ui,mi,Vt,ci,hi,Ms,di,$i,Br,ct,Yr,Oe,ht,Pa,Kt,_i,Fa,gi,Jr,dt,Qr,qe,wi,Sa,vi,ki,Ma,yi,bi,Vr,$t,Kr,De,_t,Ca,Zt,Ei,Ia,Ti,Zr,Cs,Ai,Xr,Is,ji,eo,Xt,to,gt,so,wt,xi,Ns,qi,zi,ao,He,vt,Na,es,Pi,Oa,Fi,ro,Os,Si,oo;return _=new Pe({}),I=new Cf({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),Y=new Js({props:{$$slots:{default:[If]},$$scope:{ctx:z}}}),V=new Pe({}),O=new qf({props:{id:"tiZFewofSLM"}}),Re=new Js({props:{$$slots:{default:[Nf]},$$scope:{ctx:z}}}),Et=new Pe({}),Ye=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Lf],pytorch:[Df]},$$scope:{ctx:z}}}),Tt=new L({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),jt=new L({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),xt=new L({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),zt=new L({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Pt=new L({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Mt=new L({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ct=new L({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),It=new L({props:{code:`result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I TURN A JOIN A COUNT&#x27;</span>]`}}),Nt=new Pe({}),Ht=new L({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),tt=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Gf],pytorch:[Uf]},$$scope:{ctx:z}}}),Lt=new L({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),Wt=new Pe({}),Ut=new qf({props:{id:"AhChOFRegn4"}}),Rt=new Pe({}),Gt=new L({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Bt=new L({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),it=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qf],pytorch:[Yf]},$$scope:{ctx:z}}}),Yt=new Pe({}),ut=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tu],pytorch:[Zf]},$$scope:{ctx:z}}}),mt=new Js({props:{$$slots:{default:[su]},$$scope:{ctx:z}}}),ct=new Js({props:{$$slots:{default:[au]},$$scope:{ctx:z}}}),Kt=new Pe({}),dt=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[lu],pytorch:[ou]},$$scope:{ctx:z}}}),$t=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[uu],pytorch:[pu]},$$scope:{ctx:z}}}),Zt=new Pe({}),Xt=new L({props:{code:`from transformers import DistilBertConfig

my_config = DistilBertConfig(n_heads=12)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = DistilBertConfig(n_heads=<span class="hljs-number">12</span>)`}}),gt=new as({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[du],pytorch:[cu]},$$scope:{ctx:z}}}),es=new Pe({}),{c(){a=l("meta"),m=h(),r=l("h1"),c=l("a"),g=l("span"),k(_.$$.fragment),v=h(),C=l("span"),j=o("Quick tour"),S=h(),k(I.$$.fragment),N=h(),D=l("p"),W=o("Get up and running with \u{1F917} Transformers! Start using the "),q=l("a"),M=o("pipeline()"),w=o(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=l("a"),R=o("AutoClass"),U=o(" to solve your text, vision or audio task."),J=h(),k(Y.$$.fragment),se=h(),Q=l("h2"),G=l("a"),ee=l("span"),k(V.$$.fragment),K=h(),he=l("span"),re=o("Pipeline"),$e=h(),oe=l("p"),te=l("a"),ne=o("pipeline()"),_e=o(" is the easiest way to use a pretrained model for a given task."),P=h(),k(O.$$.fragment),le=h(),x=l("p"),H=o("The "),ie=l("a"),Fe=o("pipeline()"),de=o(" supports many common tasks out-of-the-box:"),we=h(),pe=l("p"),We=l("strong"),ve=o("Text"),rs=o(":"),kt=h(),B=l("ul"),Qs=l("li"),zo=o("Sentiment analysis: classify the polarity of a given text."),Po=h(),Vs=l("li"),Fo=o("Text generation (in English): generate text from a given input."),So=h(),Ks=l("li"),Mo=o("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Co=h(),Zs=l("li"),Io=o("Question answering: extract the answer from the context, given some context and a question."),No=h(),Xs=l("li"),Oo=o("Fill-mask: fill in the blank given a text with masked words."),Do=h(),ea=l("li"),Ho=o("Summarization: generate a summary of a long sequence of text or document."),Lo=h(),ta=l("li"),Wo=o("Translation: translate text into another language."),Uo=h(),sa=l("li"),Ro=o("Feature extraction: create a tensor representation of the text."),Ya=h(),yt=l("p"),aa=l("strong"),Go=o("Image"),Bo=o(":"),Ja=h(),ke=l("ul"),ra=l("li"),Yo=o("Image classification: classify an image."),Jo=h(),oa=l("li"),Qo=o("Image segmentation: classify every pixel in an image."),Vo=h(),na=l("li"),Ko=o("Object detection: detect objects within an image."),Qa=h(),bt=l("p"),la=l("strong"),Zo=o("Audio"),Xo=o(":"),Va=h(),Ue=l("ul"),ia=l("li"),en=o("Audio classification: assign a label to a given segment of audio."),tn=h(),pa=l("li"),sn=o("Automatic speech recognition (ASR): transcribe audio data into text."),Ka=h(),k(Re.$$.fragment),Za=h(),Se=l("h3"),Ge=l("a"),fa=l("span"),k(Et.$$.fragment),an=h(),ua=l("span"),rn=o("Pipeline usage"),Xa=h(),Be=l("p"),on=o("In the following example, you will use the "),os=l("a"),nn=o("pipeline()"),ln=o(" for sentiment analysis."),er=h(),ns=l("p"),pn=o("Install the following dependencies if you haven\u2019t already:"),tr=h(),k(Ye.$$.fragment),sr=h(),Je=l("p"),fn=o("Import "),ls=l("a"),un=o("pipeline()"),mn=o(" and specify the task you want to complete:"),ar=h(),k(Tt.$$.fragment),rr=h(),ye=l("p"),cn=o("The pipeline downloads and caches a default "),At=l("a"),hn=o("pretrained model"),dn=o(" and tokenizer for sentiment analysis. Now you can use the "),ma=l("code"),$n=o("classifier"),_n=o(" on your target text:"),or=h(),k(jt.$$.fragment),nr=h(),Qe=l("p"),gn=o("For more than one sentence, pass a list of sentences to the "),is=l("a"),wn=o("pipeline()"),vn=o(" which returns a list of dictionaries:"),lr=h(),k(xt.$$.fragment),ir=h(),be=l("p"),kn=o("The "),ps=l("a"),yn=o("pipeline()"),bn=o(" can also iterate over an entire dataset. Start by installing the "),qt=l("a"),En=o("\u{1F917} Datasets"),Tn=o(" library:"),pr=h(),k(zt.$$.fragment),fr=h(),Ve=l("p"),An=o("Create a "),fs=l("a"),jn=o("pipeline()"),xn=o(" with the task you want to solve for and the model you want to use."),ur=h(),k(Pt.$$.fragment),mr=h(),Ee=l("p"),qn=o("Next, load a dataset (see the \u{1F917} Datasets "),Ft=l("a"),zn=o("Quick Start"),Pn=o(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),St=l("a"),Fn=o("MInDS-14"),Sn=o(" dataset:"),cr=h(),k(Mt.$$.fragment),hr=h(),Ke=l("p"),Mn=o(`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ca=l("code"),Cn=o("facebook/wav2vec2-base-960h"),In=o(" was trained on."),dr=h(),k(Ct.$$.fragment),$r=h(),Ze=l("p"),Nn=o("Audio files are automatically loaded and resampled when calling the "),ha=l("code"),On=o('"audio"'),Dn=o(` column.
Let\u2019s extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:`),_r=h(),k(It.$$.fragment),gr=h(),Xe=l("p"),Hn=o("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),us=l("a"),Ln=o("pipeline documentation"),Wn=o(" for more information."),wr=h(),Me=l("h3"),et=l("a"),da=l("span"),k(Nt.$$.fragment),Un=h(),$a=l("span"),Rn=o("Use another model and tokenizer in the pipeline"),vr=h(),fe=l("p"),Gn=o("The "),ms=l("a"),Bn=o("pipeline()"),Yn=o(" can accommodate any model from the "),Ot=l("a"),Jn=o("Model Hub"),Qn=o(", making it easy to adapt the "),cs=l("a"),Vn=o("pipeline()"),Kn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Dt=l("a"),Zn=o("BERT model"),Xn=o(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),kr=h(),k(Ht.$$.fragment),yr=h(),k(tt.$$.fragment),br=h(),Te=l("p"),el=o("Then you can specify the model and tokenizer in the "),hs=l("a"),tl=o("pipeline()"),sl=o(", and apply the "),_a=l("code"),al=o("classifier"),rl=o(" on your target text:"),Er=h(),k(Lt.$$.fragment),Tr=h(),Ae=l("p"),ol=o("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),ds=l("a"),nl=o("fine-tuning tutorial"),ll=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),$s=l("a"),il=o("here"),pl=o(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),Ar=h(),Ce=l("h2"),st=l("a"),ga=l("span"),k(Wt.$$.fragment),fl=h(),wa=l("span"),ul=o("AutoClass"),jr=h(),k(Ut.$$.fragment),xr=h(),Z=l("p"),ml=o("Under the hood, the "),_s=l("a"),cl=o("AutoModelForSequenceClassification"),hl=o(" and "),gs=l("a"),dl=o("AutoTokenizer"),$l=o(" classes work together to power the "),ws=l("a"),_l=o("pipeline()"),gl=o(". An "),vs=l("a"),wl=o("AutoClass"),vl=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),va=l("code"),kl=o("AutoClass"),yl=o(" for your task and it\u2019s associated tokenizer with "),ks=l("a"),bl=o("AutoTokenizer"),El=o("."),qr=h(),je=l("p"),Tl=o("Let\u2019s return to our example and see how you can use the "),ka=l("code"),Al=o("AutoClass"),jl=o(" to replicate the results of the "),ys=l("a"),xl=o("pipeline()"),ql=o("."),zr=h(),Ie=l("h3"),at=l("a"),ya=l("span"),k(Rt.$$.fragment),zl=h(),ba=l("span"),Pl=o("AutoTokenizer"),Pr=h(),xe=l("p"),Fl=o("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),Ea=l("em"),Sl=o("tokens"),Ml=o(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),bs=l("a"),Cl=o("here"),Il=o("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),Fr=h(),rt=l("p"),Nl=o("Load a tokenizer with "),Es=l("a"),Ol=o("AutoTokenizer"),Dl=o(":"),Sr=h(),k(Gt.$$.fragment),Mr=h(),ot=l("p"),Hl=o("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),Ta=l("em"),Ll=o("vocabulary"),Wl=o("."),Cr=h(),Ts=l("p"),Ul=o("Pass your text to the tokenizer:"),Ir=h(),k(Bt.$$.fragment),Nr=h(),As=l("p"),Rl=o("The tokenizer will return a dictionary containing:"),Or=h(),nt=l("ul"),js=l("li"),xs=l("a"),Gl=o("input_ids"),Bl=o(": numerical representions of your tokens."),Yl=h(),qs=l("li"),zs=l("a"),Jl=o("atttention_mask"),Ql=o(": indicates which tokens should be attended to."),Dr=h(),lt=l("p"),Vl=o("Just like the "),Ps=l("a"),Kl=o("pipeline()"),Zl=o(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Hr=h(),k(it.$$.fragment),Lr=h(),pt=l("p"),Xl=o("Read the "),Fs=l("a"),ei=o("preprocessing"),ti=o(" tutorial for more details about tokenization."),Wr=h(),Ne=l("h3"),ft=l("a"),Aa=l("span"),k(Yt.$$.fragment),si=h(),ja=l("span"),ai=o("AutoModel"),Ur=h(),k(ut.$$.fragment),Rr=h(),k(mt.$$.fragment),Gr=h(),X=l("p"),ri=o("Models are a standard "),Jt=l("a"),xa=l("code"),oi=o("torch.nn.Module"),ni=o(" or a "),Qt=l("a"),qa=l("code"),li=o("tf.keras.Model"),ii=o(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),Ss=l("a"),pi=o("Trainer"),fi=o(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),za=l("code"),ui=o("fit"),mi=o(" method from "),Vt=l("a"),ci=o("Keras"),hi=o(". Refer to the "),Ms=l("a"),di=o("training tutorial"),$i=o(" for more details."),Br=h(),k(ct.$$.fragment),Yr=h(),Oe=l("h3"),ht=l("a"),Pa=l("span"),k(Kt.$$.fragment),_i=h(),Fa=l("span"),gi=o("Save a model"),Jr=h(),k(dt.$$.fragment),Qr=h(),qe=l("p"),wi=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Sa=l("code"),vi=o("from_pt"),ki=o(" or "),Ma=l("code"),yi=o("from_tf"),bi=o(" parameter can convert the model from one framework to the other:"),Vr=h(),k($t.$$.fragment),Kr=h(),De=l("h2"),_t=l("a"),Ca=l("span"),k(Zt.$$.fragment),Ei=h(),Ia=l("span"),Ti=o("Custom model builds"),Zr=h(),Cs=l("p"),Ai=o("You can modify the model\u2019s configuration class to change how a model is built. The configuration specifies a model\u2019s attributes, such as the number of hidden layers or attention heads. You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and you\u2019ll need to train the model before you can use it to get meaningful results."),Xr=h(),Is=l("p"),ji=o("Start by importing a model\u2019s configuration class, and then you can change the number of attention heads, for instance:"),eo=h(),k(Xt.$$.fragment),to=h(),k(gt.$$.fragment),so=h(),wt=l("p"),xi=o("Take a look at the "),Ns=l("a"),qi=o("Create a custom architecture"),zi=o(" guide for more information about building custom configurations."),ao=h(),He=l("h2"),vt=l("a"),Na=l("span"),k(es.$$.fragment),Pi=h(),Oa=l("span"),Fi=o("What's next?"),ro=h(),Os=l("p"),Si=o("Now that you\u2019ve completed the \u{1F917} Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you\u2019re interested in learning more about \u{1F917} Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!"),this.h()},l(e){const f=Sf('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),m=d(e),r=i(e,"H1",{class:!0});var ts=p(r);c=i(ts,"A",{id:!0,class:!0,href:!0});var Da=p(c);g=i(Da,"SPAN",{});var Ha=p(g);y(_.$$.fragment,Ha),Ha.forEach(s),Da.forEach(s),v=d(ts),C=i(ts,"SPAN",{});var La=p(C);j=n(La,"Quick tour"),La.forEach(s),ts.forEach(s),S=d(e),y(I.$$.fragment,e),N=d(e),D=i(e,"P",{});var Le=p(D);W=n(Le,"Get up and running with \u{1F917} Transformers! Start using the "),q=i(Le,"A",{href:!0});var Wa=p(q);M=n(Wa,"pipeline()"),Wa.forEach(s),w=n(Le," for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=i(Le,"A",{href:!0});var Ua=p(F);R=n(Ua,"AutoClass"),Ua.forEach(s),U=n(Le," to solve your text, vision or audio task."),Le.forEach(s),J=d(e),y(Y.$$.fragment,e),se=d(e),Q=i(e,"H2",{class:!0});var ss=p(Q);G=i(ss,"A",{id:!0,class:!0,href:!0});var Ra=p(G);ee=i(Ra,"SPAN",{});var Ga=p(ee);y(V.$$.fragment,Ga),Ga.forEach(s),Ra.forEach(s),K=d(ss),he=i(ss,"SPAN",{});var Ba=p(he);re=n(Ba,"Pipeline"),Ba.forEach(s),ss.forEach(s),$e=d(e),oe=i(e,"P",{});var Mi=p(oe);te=i(Mi,"A",{href:!0});var Hi=p(te);ne=n(Hi,"pipeline()"),Hi.forEach(s),_e=n(Mi," is the easiest way to use a pretrained model for a given task."),Mi.forEach(s),P=d(e),y(O.$$.fragment,e),le=d(e),x=i(e,"P",{});var no=p(x);H=n(no,"The "),ie=i(no,"A",{href:!0});var Li=p(ie);Fe=n(Li,"pipeline()"),Li.forEach(s),de=n(no," supports many common tasks out-of-the-box:"),no.forEach(s),we=d(e),pe=i(e,"P",{});var Ci=p(pe);We=i(Ci,"STRONG",{});var Wi=p(We);ve=n(Wi,"Text"),Wi.forEach(s),rs=n(Ci,":"),Ci.forEach(s),kt=d(e),B=i(e,"UL",{});var ae=p(B);Qs=i(ae,"LI",{});var Ui=p(Qs);zo=n(Ui,"Sentiment analysis: classify the polarity of a given text."),Ui.forEach(s),Po=d(ae),Vs=i(ae,"LI",{});var Ri=p(Vs);Fo=n(Ri,"Text generation (in English): generate text from a given input."),Ri.forEach(s),So=d(ae),Ks=i(ae,"LI",{});var Gi=p(Ks);Mo=n(Gi,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Gi.forEach(s),Co=d(ae),Zs=i(ae,"LI",{});var Bi=p(Zs);Io=n(Bi,"Question answering: extract the answer from the context, given some context and a question."),Bi.forEach(s),No=d(ae),Xs=i(ae,"LI",{});var Yi=p(Xs);Oo=n(Yi,"Fill-mask: fill in the blank given a text with masked words."),Yi.forEach(s),Do=d(ae),ea=i(ae,"LI",{});var Ji=p(ea);Ho=n(Ji,"Summarization: generate a summary of a long sequence of text or document."),Ji.forEach(s),Lo=d(ae),ta=i(ae,"LI",{});var Qi=p(ta);Wo=n(Qi,"Translation: translate text into another language."),Qi.forEach(s),Uo=d(ae),sa=i(ae,"LI",{});var Vi=p(sa);Ro=n(Vi,"Feature extraction: create a tensor representation of the text."),Vi.forEach(s),ae.forEach(s),Ya=d(e),yt=i(e,"P",{});var Ii=p(yt);aa=i(Ii,"STRONG",{});var Ki=p(aa);Go=n(Ki,"Image"),Ki.forEach(s),Bo=n(Ii,":"),Ii.forEach(s),Ja=d(e),ke=i(e,"UL",{});var Ds=p(ke);ra=i(Ds,"LI",{});var Zi=p(ra);Yo=n(Zi,"Image classification: classify an image."),Zi.forEach(s),Jo=d(Ds),oa=i(Ds,"LI",{});var Xi=p(oa);Qo=n(Xi,"Image segmentation: classify every pixel in an image."),Xi.forEach(s),Vo=d(Ds),na=i(Ds,"LI",{});var ep=p(na);Ko=n(ep,"Object detection: detect objects within an image."),ep.forEach(s),Ds.forEach(s),Qa=d(e),bt=i(e,"P",{});var Ni=p(bt);la=i(Ni,"STRONG",{});var tp=p(la);Zo=n(tp,"Audio"),tp.forEach(s),Xo=n(Ni,":"),Ni.forEach(s),Va=d(e),Ue=i(e,"UL",{});var lo=p(Ue);ia=i(lo,"LI",{});var sp=p(ia);en=n(sp,"Audio classification: assign a label to a given segment of audio."),sp.forEach(s),tn=d(lo),pa=i(lo,"LI",{});var ap=p(pa);sn=n(ap,"Automatic speech recognition (ASR): transcribe audio data into text."),ap.forEach(s),lo.forEach(s),Ka=d(e),y(Re.$$.fragment,e),Za=d(e),Se=i(e,"H3",{class:!0});var io=p(Se);Ge=i(io,"A",{id:!0,class:!0,href:!0});var rp=p(Ge);fa=i(rp,"SPAN",{});var op=p(fa);y(Et.$$.fragment,op),op.forEach(s),rp.forEach(s),an=d(io),ua=i(io,"SPAN",{});var np=p(ua);rn=n(np,"Pipeline usage"),np.forEach(s),io.forEach(s),Xa=d(e),Be=i(e,"P",{});var po=p(Be);on=n(po,"In the following example, you will use the "),os=i(po,"A",{href:!0});var lp=p(os);nn=n(lp,"pipeline()"),lp.forEach(s),ln=n(po," for sentiment analysis."),po.forEach(s),er=d(e),ns=i(e,"P",{});var ip=p(ns);pn=n(ip,"Install the following dependencies if you haven\u2019t already:"),ip.forEach(s),tr=d(e),y(Ye.$$.fragment,e),sr=d(e),Je=i(e,"P",{});var fo=p(Je);fn=n(fo,"Import "),ls=i(fo,"A",{href:!0});var pp=p(ls);un=n(pp,"pipeline()"),pp.forEach(s),mn=n(fo," and specify the task you want to complete:"),fo.forEach(s),ar=d(e),y(Tt.$$.fragment,e),rr=d(e),ye=i(e,"P",{});var Hs=p(ye);cn=n(Hs,"The pipeline downloads and caches a default "),At=i(Hs,"A",{href:!0,rel:!0});var fp=p(At);hn=n(fp,"pretrained model"),fp.forEach(s),dn=n(Hs," and tokenizer for sentiment analysis. Now you can use the "),ma=i(Hs,"CODE",{});var up=p(ma);$n=n(up,"classifier"),up.forEach(s),_n=n(Hs," on your target text:"),Hs.forEach(s),or=d(e),y(jt.$$.fragment,e),nr=d(e),Qe=i(e,"P",{});var uo=p(Qe);gn=n(uo,"For more than one sentence, pass a list of sentences to the "),is=i(uo,"A",{href:!0});var mp=p(is);wn=n(mp,"pipeline()"),mp.forEach(s),vn=n(uo," which returns a list of dictionaries:"),uo.forEach(s),lr=d(e),y(xt.$$.fragment,e),ir=d(e),be=i(e,"P",{});var Ls=p(be);kn=n(Ls,"The "),ps=i(Ls,"A",{href:!0});var cp=p(ps);yn=n(cp,"pipeline()"),cp.forEach(s),bn=n(Ls," can also iterate over an entire dataset. Start by installing the "),qt=i(Ls,"A",{href:!0,rel:!0});var hp=p(qt);En=n(hp,"\u{1F917} Datasets"),hp.forEach(s),Tn=n(Ls," library:"),Ls.forEach(s),pr=d(e),y(zt.$$.fragment,e),fr=d(e),Ve=i(e,"P",{});var mo=p(Ve);An=n(mo,"Create a "),fs=i(mo,"A",{href:!0});var dp=p(fs);jn=n(dp,"pipeline()"),dp.forEach(s),xn=n(mo," with the task you want to solve for and the model you want to use."),mo.forEach(s),ur=d(e),y(Pt.$$.fragment,e),mr=d(e),Ee=i(e,"P",{});var Ws=p(Ee);qn=n(Ws,"Next, load a dataset (see the \u{1F917} Datasets "),Ft=i(Ws,"A",{href:!0,rel:!0});var $p=p(Ft);zn=n($p,"Quick Start"),$p.forEach(s),Pn=n(Ws," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),St=i(Ws,"A",{href:!0,rel:!0});var _p=p(St);Fn=n(_p,"MInDS-14"),_p.forEach(s),Sn=n(Ws," dataset:"),Ws.forEach(s),cr=d(e),y(Mt.$$.fragment,e),hr=d(e),Ke=i(e,"P",{});var co=p(Ke);Mn=n(co,`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ca=i(co,"CODE",{});var gp=p(ca);Cn=n(gp,"facebook/wav2vec2-base-960h"),gp.forEach(s),In=n(co," was trained on."),co.forEach(s),dr=d(e),y(Ct.$$.fragment,e),$r=d(e),Ze=i(e,"P",{});var ho=p(Ze);Nn=n(ho,"Audio files are automatically loaded and resampled when calling the "),ha=i(ho,"CODE",{});var wp=p(ha);On=n(wp,'"audio"'),wp.forEach(s),Dn=n(ho,` column.
Let\u2019s extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:`),ho.forEach(s),_r=d(e),y(It.$$.fragment,e),gr=d(e),Xe=i(e,"P",{});var $o=p(Xe);Hn=n($o,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),us=i($o,"A",{href:!0});var vp=p(us);Ln=n(vp,"pipeline documentation"),vp.forEach(s),Wn=n($o," for more information."),$o.forEach(s),wr=d(e),Me=i(e,"H3",{class:!0});var _o=p(Me);et=i(_o,"A",{id:!0,class:!0,href:!0});var kp=p(et);da=i(kp,"SPAN",{});var yp=p(da);y(Nt.$$.fragment,yp),yp.forEach(s),kp.forEach(s),Un=d(_o),$a=i(_o,"SPAN",{});var bp=p($a);Rn=n(bp,"Use another model and tokenizer in the pipeline"),bp.forEach(s),_o.forEach(s),vr=d(e),fe=i(e,"P",{});var ze=p(fe);Gn=n(ze,"The "),ms=i(ze,"A",{href:!0});var Ep=p(ms);Bn=n(Ep,"pipeline()"),Ep.forEach(s),Yn=n(ze," can accommodate any model from the "),Ot=i(ze,"A",{href:!0,rel:!0});var Tp=p(Ot);Jn=n(Tp,"Model Hub"),Tp.forEach(s),Qn=n(ze,", making it easy to adapt the "),cs=i(ze,"A",{href:!0});var Ap=p(cs);Vn=n(Ap,"pipeline()"),Ap.forEach(s),Kn=n(ze," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Dt=i(ze,"A",{href:!0,rel:!0});var jp=p(Dt);Zn=n(jp,"BERT model"),jp.forEach(s),Xn=n(ze," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ze.forEach(s),kr=d(e),y(Ht.$$.fragment,e),yr=d(e),y(tt.$$.fragment,e),br=d(e),Te=i(e,"P",{});var Us=p(Te);el=n(Us,"Then you can specify the model and tokenizer in the "),hs=i(Us,"A",{href:!0});var xp=p(hs);tl=n(xp,"pipeline()"),xp.forEach(s),sl=n(Us,", and apply the "),_a=i(Us,"CODE",{});var qp=p(_a);al=n(qp,"classifier"),qp.forEach(s),rl=n(Us," on your target text:"),Us.forEach(s),Er=d(e),y(Lt.$$.fragment,e),Tr=d(e),Ae=i(e,"P",{});var Rs=p(Ae);ol=n(Rs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),ds=i(Rs,"A",{href:!0});var zp=p(ds);nl=n(zp,"fine-tuning tutorial"),zp.forEach(s),ll=n(Rs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),$s=i(Rs,"A",{href:!0});var Pp=p($s);il=n(Pp,"here"),Pp.forEach(s),pl=n(Rs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),Rs.forEach(s),Ar=d(e),Ce=i(e,"H2",{class:!0});var go=p(Ce);st=i(go,"A",{id:!0,class:!0,href:!0});var Fp=p(st);ga=i(Fp,"SPAN",{});var Sp=p(ga);y(Wt.$$.fragment,Sp),Sp.forEach(s),Fp.forEach(s),fl=d(go),wa=i(go,"SPAN",{});var Mp=p(wa);ul=n(Mp,"AutoClass"),Mp.forEach(s),go.forEach(s),jr=d(e),y(Ut.$$.fragment,e),xr=d(e),Z=i(e,"P",{});var ue=p(Z);ml=n(ue,"Under the hood, the "),_s=i(ue,"A",{href:!0});var Cp=p(_s);cl=n(Cp,"AutoModelForSequenceClassification"),Cp.forEach(s),hl=n(ue," and "),gs=i(ue,"A",{href:!0});var Ip=p(gs);dl=n(Ip,"AutoTokenizer"),Ip.forEach(s),$l=n(ue," classes work together to power the "),ws=i(ue,"A",{href:!0});var Np=p(ws);_l=n(Np,"pipeline()"),Np.forEach(s),gl=n(ue,". An "),vs=i(ue,"A",{href:!0});var Op=p(vs);wl=n(Op,"AutoClass"),Op.forEach(s),vl=n(ue," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),va=i(ue,"CODE",{});var Dp=p(va);kl=n(Dp,"AutoClass"),Dp.forEach(s),yl=n(ue," for your task and it\u2019s associated tokenizer with "),ks=i(ue,"A",{href:!0});var Hp=p(ks);bl=n(Hp,"AutoTokenizer"),Hp.forEach(s),El=n(ue,"."),ue.forEach(s),qr=d(e),je=i(e,"P",{});var Gs=p(je);Tl=n(Gs,"Let\u2019s return to our example and see how you can use the "),ka=i(Gs,"CODE",{});var Lp=p(ka);Al=n(Lp,"AutoClass"),Lp.forEach(s),jl=n(Gs," to replicate the results of the "),ys=i(Gs,"A",{href:!0});var Wp=p(ys);xl=n(Wp,"pipeline()"),Wp.forEach(s),ql=n(Gs,"."),Gs.forEach(s),zr=d(e),Ie=i(e,"H3",{class:!0});var wo=p(Ie);at=i(wo,"A",{id:!0,class:!0,href:!0});var Up=p(at);ya=i(Up,"SPAN",{});var Rp=p(ya);y(Rt.$$.fragment,Rp),Rp.forEach(s),Up.forEach(s),zl=d(wo),ba=i(wo,"SPAN",{});var Gp=p(ba);Pl=n(Gp,"AutoTokenizer"),Gp.forEach(s),wo.forEach(s),Pr=d(e),xe=i(e,"P",{});var Bs=p(xe);Fl=n(Bs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),Ea=i(Bs,"EM",{});var Bp=p(Ea);Sl=n(Bp,"tokens"),Bp.forEach(s),Ml=n(Bs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),bs=i(Bs,"A",{href:!0});var Yp=p(bs);Cl=n(Yp,"here"),Yp.forEach(s),Il=n(Bs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),Bs.forEach(s),Fr=d(e),rt=i(e,"P",{});var vo=p(rt);Nl=n(vo,"Load a tokenizer with "),Es=i(vo,"A",{href:!0});var Jp=p(Es);Ol=n(Jp,"AutoTokenizer"),Jp.forEach(s),Dl=n(vo,":"),vo.forEach(s),Sr=d(e),y(Gt.$$.fragment,e),Mr=d(e),ot=i(e,"P",{});var ko=p(ot);Hl=n(ko,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),Ta=i(ko,"EM",{});var Qp=p(Ta);Ll=n(Qp,"vocabulary"),Qp.forEach(s),Wl=n(ko,"."),ko.forEach(s),Cr=d(e),Ts=i(e,"P",{});var Vp=p(Ts);Ul=n(Vp,"Pass your text to the tokenizer:"),Vp.forEach(s),Ir=d(e),y(Bt.$$.fragment,e),Nr=d(e),As=i(e,"P",{});var Kp=p(As);Rl=n(Kp,"The tokenizer will return a dictionary containing:"),Kp.forEach(s),Or=d(e),nt=i(e,"UL",{});var yo=p(nt);js=i(yo,"LI",{});var Oi=p(js);xs=i(Oi,"A",{href:!0});var Zp=p(xs);Gl=n(Zp,"input_ids"),Zp.forEach(s),Bl=n(Oi,": numerical representions of your tokens."),Oi.forEach(s),Yl=d(yo),qs=i(yo,"LI",{});var Di=p(qs);zs=i(Di,"A",{href:!0});var Xp=p(zs);Jl=n(Xp,"atttention_mask"),Xp.forEach(s),Ql=n(Di,": indicates which tokens should be attended to."),Di.forEach(s),yo.forEach(s),Dr=d(e),lt=i(e,"P",{});var bo=p(lt);Vl=n(bo,"Just like the "),Ps=i(bo,"A",{href:!0});var ef=p(Ps);Kl=n(ef,"pipeline()"),ef.forEach(s),Zl=n(bo,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),bo.forEach(s),Hr=d(e),y(it.$$.fragment,e),Lr=d(e),pt=i(e,"P",{});var Eo=p(pt);Xl=n(Eo,"Read the "),Fs=i(Eo,"A",{href:!0});var tf=p(Fs);ei=n(tf,"preprocessing"),tf.forEach(s),ti=n(Eo," tutorial for more details about tokenization."),Eo.forEach(s),Wr=d(e),Ne=i(e,"H3",{class:!0});var To=p(Ne);ft=i(To,"A",{id:!0,class:!0,href:!0});var sf=p(ft);Aa=i(sf,"SPAN",{});var af=p(Aa);y(Yt.$$.fragment,af),af.forEach(s),sf.forEach(s),si=d(To),ja=i(To,"SPAN",{});var rf=p(ja);ai=n(rf,"AutoModel"),rf.forEach(s),To.forEach(s),Ur=d(e),y(ut.$$.fragment,e),Rr=d(e),y(mt.$$.fragment,e),Gr=d(e),X=i(e,"P",{});var me=p(X);ri=n(me,"Models are a standard "),Jt=i(me,"A",{href:!0,rel:!0});var of=p(Jt);xa=i(of,"CODE",{});var nf=p(xa);oi=n(nf,"torch.nn.Module"),nf.forEach(s),of.forEach(s),ni=n(me," or a "),Qt=i(me,"A",{href:!0,rel:!0});var lf=p(Qt);qa=i(lf,"CODE",{});var pf=p(qa);li=n(pf,"tf.keras.Model"),pf.forEach(s),lf.forEach(s),ii=n(me," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),Ss=i(me,"A",{href:!0});var ff=p(Ss);pi=n(ff,"Trainer"),ff.forEach(s),fi=n(me," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),za=i(me,"CODE",{});var uf=p(za);ui=n(uf,"fit"),uf.forEach(s),mi=n(me," method from "),Vt=i(me,"A",{href:!0,rel:!0});var mf=p(Vt);ci=n(mf,"Keras"),mf.forEach(s),hi=n(me,". Refer to the "),Ms=i(me,"A",{href:!0});var cf=p(Ms);di=n(cf,"training tutorial"),cf.forEach(s),$i=n(me," for more details."),me.forEach(s),Br=d(e),y(ct.$$.fragment,e),Yr=d(e),Oe=i(e,"H3",{class:!0});var Ao=p(Oe);ht=i(Ao,"A",{id:!0,class:!0,href:!0});var hf=p(ht);Pa=i(hf,"SPAN",{});var df=p(Pa);y(Kt.$$.fragment,df),df.forEach(s),hf.forEach(s),_i=d(Ao),Fa=i(Ao,"SPAN",{});var $f=p(Fa);gi=n($f,"Save a model"),$f.forEach(s),Ao.forEach(s),Jr=d(e),y(dt.$$.fragment,e),Qr=d(e),qe=i(e,"P",{});var Ys=p(qe);wi=n(Ys,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Sa=i(Ys,"CODE",{});var _f=p(Sa);vi=n(_f,"from_pt"),_f.forEach(s),ki=n(Ys," or "),Ma=i(Ys,"CODE",{});var gf=p(Ma);yi=n(gf,"from_tf"),gf.forEach(s),bi=n(Ys," parameter can convert the model from one framework to the other:"),Ys.forEach(s),Vr=d(e),y($t.$$.fragment,e),Kr=d(e),De=i(e,"H2",{class:!0});var jo=p(De);_t=i(jo,"A",{id:!0,class:!0,href:!0});var wf=p(_t);Ca=i(wf,"SPAN",{});var vf=p(Ca);y(Zt.$$.fragment,vf),vf.forEach(s),wf.forEach(s),Ei=d(jo),Ia=i(jo,"SPAN",{});var kf=p(Ia);Ti=n(kf,"Custom model builds"),kf.forEach(s),jo.forEach(s),Zr=d(e),Cs=i(e,"P",{});var yf=p(Cs);Ai=n(yf,"You can modify the model\u2019s configuration class to change how a model is built. The configuration specifies a model\u2019s attributes, such as the number of hidden layers or attention heads. You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and you\u2019ll need to train the model before you can use it to get meaningful results."),yf.forEach(s),Xr=d(e),Is=i(e,"P",{});var bf=p(Is);ji=n(bf,"Start by importing a model\u2019s configuration class, and then you can change the number of attention heads, for instance:"),bf.forEach(s),eo=d(e),y(Xt.$$.fragment,e),to=d(e),y(gt.$$.fragment,e),so=d(e),wt=i(e,"P",{});var xo=p(wt);xi=n(xo,"Take a look at the "),Ns=i(xo,"A",{href:!0});var Ef=p(Ns);qi=n(Ef,"Create a custom architecture"),Ef.forEach(s),zi=n(xo," guide for more information about building custom configurations."),xo.forEach(s),ao=d(e),He=i(e,"H2",{class:!0});var qo=p(He);vt=i(qo,"A",{id:!0,class:!0,href:!0});var Tf=p(vt);Na=i(Tf,"SPAN",{});var Af=p(Na);y(es.$$.fragment,Af),Af.forEach(s),Tf.forEach(s),Pi=d(qo),Oa=i(qo,"SPAN",{});var jf=p(Oa);Fi=n(jf,"What's next?"),jf.forEach(s),qo.forEach(s),ro=d(e),Os=i(e,"P",{});var xf=p(Os);Si=n(xf,"Now that you\u2019ve completed the \u{1F917} Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you\u2019re interested in learning more about \u{1F917} Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!"),xf.forEach(s),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(_u)),$(c,"id","quick-tour"),$(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(c,"href","#quick-tour"),$(r,"class","relative group"),$(q,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(F,"href","./model_doc/auto"),$(G,"id","pipeline"),$(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(G,"href","#pipeline"),$(Q,"class","relative group"),$(te,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(ie,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Ge,"id","pipeline-usage"),$(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ge,"href","#pipeline-usage"),$(Se,"class","relative group"),$(os,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(ls,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(At,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(At,"rel","nofollow"),$(is,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(ps,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(qt,"href","https://huggingface.co/docs/datasets/"),$(qt,"rel","nofollow"),$(fs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Ft,"href","https://huggingface.co/docs/datasets/quickstart.html"),$(Ft,"rel","nofollow"),$(St,"href","https://huggingface.co/datasets/PolyAI/minds14"),$(St,"rel","nofollow"),$(us,"href","./main_classes/pipelines"),$(et,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(et,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Me,"class","relative group"),$(ms,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Ot,"href","https://huggingface.co/models"),$(Ot,"rel","nofollow"),$(cs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Dt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(Dt,"rel","nofollow"),$(hs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(ds,"href","./training"),$($s,"href","./model_sharing"),$(st,"id","autoclass"),$(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(st,"href","#autoclass"),$(Ce,"class","relative group"),$(_s,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(gs,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(ws,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(vs,"href","./model_doc/auto"),$(ks,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(ys,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(at,"id","autotokenizer"),$(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(at,"href","#autotokenizer"),$(Ie,"class","relative group"),$(bs,"href","./tokenizer_summary"),$(Es,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(xs,"href","./glossary#input-ids"),$(zs,"href",".glossary#attention-mask"),$(Ps,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Fs,"href","./preprocessing"),$(ft,"id","automodel"),$(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ft,"href","#automodel"),$(Ne,"class","relative group"),$(Jt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Jt,"rel","nofollow"),$(Qt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Qt,"rel","nofollow"),$(Ss,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),$(Vt,"href","https://keras.io/"),$(Vt,"rel","nofollow"),$(Ms,"href","./training"),$(ht,"id","save-a-model"),$(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ht,"href","#save-a-model"),$(Oe,"class","relative group"),$(_t,"id","custom-model-builds"),$(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(_t,"href","#custom-model-builds"),$(De,"class","relative group"),$(Ns,"href","./create_a_model"),$(vt,"id","whats-next"),$(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(vt,"href","#whats-next"),$(He,"class","relative group")},m(e,f){t(document.head,a),u(e,m,f),u(e,r,f),t(r,c),t(c,g),b(_,g,null),t(r,v),t(r,C),t(C,j),u(e,S,f),b(I,e,f),u(e,N,f),u(e,D,f),t(D,W),t(D,q),t(q,M),t(D,w),t(D,F),t(F,R),t(D,U),u(e,J,f),b(Y,e,f),u(e,se,f),u(e,Q,f),t(Q,G),t(G,ee),b(V,ee,null),t(Q,K),t(Q,he),t(he,re),u(e,$e,f),u(e,oe,f),t(oe,te),t(te,ne),t(oe,_e),u(e,P,f),b(O,e,f),u(e,le,f),u(e,x,f),t(x,H),t(x,ie),t(ie,Fe),t(x,de),u(e,we,f),u(e,pe,f),t(pe,We),t(We,ve),t(pe,rs),u(e,kt,f),u(e,B,f),t(B,Qs),t(Qs,zo),t(B,Po),t(B,Vs),t(Vs,Fo),t(B,So),t(B,Ks),t(Ks,Mo),t(B,Co),t(B,Zs),t(Zs,Io),t(B,No),t(B,Xs),t(Xs,Oo),t(B,Do),t(B,ea),t(ea,Ho),t(B,Lo),t(B,ta),t(ta,Wo),t(B,Uo),t(B,sa),t(sa,Ro),u(e,Ya,f),u(e,yt,f),t(yt,aa),t(aa,Go),t(yt,Bo),u(e,Ja,f),u(e,ke,f),t(ke,ra),t(ra,Yo),t(ke,Jo),t(ke,oa),t(oa,Qo),t(ke,Vo),t(ke,na),t(na,Ko),u(e,Qa,f),u(e,bt,f),t(bt,la),t(la,Zo),t(bt,Xo),u(e,Va,f),u(e,Ue,f),t(Ue,ia),t(ia,en),t(Ue,tn),t(Ue,pa),t(pa,sn),u(e,Ka,f),b(Re,e,f),u(e,Za,f),u(e,Se,f),t(Se,Ge),t(Ge,fa),b(Et,fa,null),t(Se,an),t(Se,ua),t(ua,rn),u(e,Xa,f),u(e,Be,f),t(Be,on),t(Be,os),t(os,nn),t(Be,ln),u(e,er,f),u(e,ns,f),t(ns,pn),u(e,tr,f),b(Ye,e,f),u(e,sr,f),u(e,Je,f),t(Je,fn),t(Je,ls),t(ls,un),t(Je,mn),u(e,ar,f),b(Tt,e,f),u(e,rr,f),u(e,ye,f),t(ye,cn),t(ye,At),t(At,hn),t(ye,dn),t(ye,ma),t(ma,$n),t(ye,_n),u(e,or,f),b(jt,e,f),u(e,nr,f),u(e,Qe,f),t(Qe,gn),t(Qe,is),t(is,wn),t(Qe,vn),u(e,lr,f),b(xt,e,f),u(e,ir,f),u(e,be,f),t(be,kn),t(be,ps),t(ps,yn),t(be,bn),t(be,qt),t(qt,En),t(be,Tn),u(e,pr,f),b(zt,e,f),u(e,fr,f),u(e,Ve,f),t(Ve,An),t(Ve,fs),t(fs,jn),t(Ve,xn),u(e,ur,f),b(Pt,e,f),u(e,mr,f),u(e,Ee,f),t(Ee,qn),t(Ee,Ft),t(Ft,zn),t(Ee,Pn),t(Ee,St),t(St,Fn),t(Ee,Sn),u(e,cr,f),b(Mt,e,f),u(e,hr,f),u(e,Ke,f),t(Ke,Mn),t(Ke,ca),t(ca,Cn),t(Ke,In),u(e,dr,f),b(Ct,e,f),u(e,$r,f),u(e,Ze,f),t(Ze,Nn),t(Ze,ha),t(ha,On),t(Ze,Dn),u(e,_r,f),b(It,e,f),u(e,gr,f),u(e,Xe,f),t(Xe,Hn),t(Xe,us),t(us,Ln),t(Xe,Wn),u(e,wr,f),u(e,Me,f),t(Me,et),t(et,da),b(Nt,da,null),t(Me,Un),t(Me,$a),t($a,Rn),u(e,vr,f),u(e,fe,f),t(fe,Gn),t(fe,ms),t(ms,Bn),t(fe,Yn),t(fe,Ot),t(Ot,Jn),t(fe,Qn),t(fe,cs),t(cs,Vn),t(fe,Kn),t(fe,Dt),t(Dt,Zn),t(fe,Xn),u(e,kr,f),b(Ht,e,f),u(e,yr,f),b(tt,e,f),u(e,br,f),u(e,Te,f),t(Te,el),t(Te,hs),t(hs,tl),t(Te,sl),t(Te,_a),t(_a,al),t(Te,rl),u(e,Er,f),b(Lt,e,f),u(e,Tr,f),u(e,Ae,f),t(Ae,ol),t(Ae,ds),t(ds,nl),t(Ae,ll),t(Ae,$s),t($s,il),t(Ae,pl),u(e,Ar,f),u(e,Ce,f),t(Ce,st),t(st,ga),b(Wt,ga,null),t(Ce,fl),t(Ce,wa),t(wa,ul),u(e,jr,f),b(Ut,e,f),u(e,xr,f),u(e,Z,f),t(Z,ml),t(Z,_s),t(_s,cl),t(Z,hl),t(Z,gs),t(gs,dl),t(Z,$l),t(Z,ws),t(ws,_l),t(Z,gl),t(Z,vs),t(vs,wl),t(Z,vl),t(Z,va),t(va,kl),t(Z,yl),t(Z,ks),t(ks,bl),t(Z,El),u(e,qr,f),u(e,je,f),t(je,Tl),t(je,ka),t(ka,Al),t(je,jl),t(je,ys),t(ys,xl),t(je,ql),u(e,zr,f),u(e,Ie,f),t(Ie,at),t(at,ya),b(Rt,ya,null),t(Ie,zl),t(Ie,ba),t(ba,Pl),u(e,Pr,f),u(e,xe,f),t(xe,Fl),t(xe,Ea),t(Ea,Sl),t(xe,Ml),t(xe,bs),t(bs,Cl),t(xe,Il),u(e,Fr,f),u(e,rt,f),t(rt,Nl),t(rt,Es),t(Es,Ol),t(rt,Dl),u(e,Sr,f),b(Gt,e,f),u(e,Mr,f),u(e,ot,f),t(ot,Hl),t(ot,Ta),t(Ta,Ll),t(ot,Wl),u(e,Cr,f),u(e,Ts,f),t(Ts,Ul),u(e,Ir,f),b(Bt,e,f),u(e,Nr,f),u(e,As,f),t(As,Rl),u(e,Or,f),u(e,nt,f),t(nt,js),t(js,xs),t(xs,Gl),t(js,Bl),t(nt,Yl),t(nt,qs),t(qs,zs),t(zs,Jl),t(qs,Ql),u(e,Dr,f),u(e,lt,f),t(lt,Vl),t(lt,Ps),t(Ps,Kl),t(lt,Zl),u(e,Hr,f),b(it,e,f),u(e,Lr,f),u(e,pt,f),t(pt,Xl),t(pt,Fs),t(Fs,ei),t(pt,ti),u(e,Wr,f),u(e,Ne,f),t(Ne,ft),t(ft,Aa),b(Yt,Aa,null),t(Ne,si),t(Ne,ja),t(ja,ai),u(e,Ur,f),b(ut,e,f),u(e,Rr,f),b(mt,e,f),u(e,Gr,f),u(e,X,f),t(X,ri),t(X,Jt),t(Jt,xa),t(xa,oi),t(X,ni),t(X,Qt),t(Qt,qa),t(qa,li),t(X,ii),t(X,Ss),t(Ss,pi),t(X,fi),t(X,za),t(za,ui),t(X,mi),t(X,Vt),t(Vt,ci),t(X,hi),t(X,Ms),t(Ms,di),t(X,$i),u(e,Br,f),b(ct,e,f),u(e,Yr,f),u(e,Oe,f),t(Oe,ht),t(ht,Pa),b(Kt,Pa,null),t(Oe,_i),t(Oe,Fa),t(Fa,gi),u(e,Jr,f),b(dt,e,f),u(e,Qr,f),u(e,qe,f),t(qe,wi),t(qe,Sa),t(Sa,vi),t(qe,ki),t(qe,Ma),t(Ma,yi),t(qe,bi),u(e,Vr,f),b($t,e,f),u(e,Kr,f),u(e,De,f),t(De,_t),t(_t,Ca),b(Zt,Ca,null),t(De,Ei),t(De,Ia),t(Ia,Ti),u(e,Zr,f),u(e,Cs,f),t(Cs,Ai),u(e,Xr,f),u(e,Is,f),t(Is,ji),u(e,eo,f),b(Xt,e,f),u(e,to,f),b(gt,e,f),u(e,so,f),u(e,wt,f),t(wt,xi),t(wt,Ns),t(Ns,qi),t(wt,zi),u(e,ao,f),u(e,He,f),t(He,vt),t(vt,Na),b(es,Na,null),t(He,Pi),t(He,Oa),t(Oa,Fi),u(e,ro,f),u(e,Os,f),t(Os,Si),oo=!0},p(e,[f]){const ts={};f&2&&(ts.$$scope={dirty:f,ctx:e}),Y.$set(ts);const Da={};f&2&&(Da.$$scope={dirty:f,ctx:e}),Re.$set(Da);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:e}),Ye.$set(Ha);const La={};f&2&&(La.$$scope={dirty:f,ctx:e}),tt.$set(La);const Le={};f&2&&(Le.$$scope={dirty:f,ctx:e}),it.$set(Le);const Wa={};f&2&&(Wa.$$scope={dirty:f,ctx:e}),ut.$set(Wa);const Ua={};f&2&&(Ua.$$scope={dirty:f,ctx:e}),mt.$set(Ua);const ss={};f&2&&(ss.$$scope={dirty:f,ctx:e}),ct.$set(ss);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:e}),dt.$set(Ra);const Ga={};f&2&&(Ga.$$scope={dirty:f,ctx:e}),$t.$set(Ga);const Ba={};f&2&&(Ba.$$scope={dirty:f,ctx:e}),gt.$set(Ba)},i(e){oo||(E(_.$$.fragment,e),E(I.$$.fragment,e),E(Y.$$.fragment,e),E(V.$$.fragment,e),E(O.$$.fragment,e),E(Re.$$.fragment,e),E(Et.$$.fragment,e),E(Ye.$$.fragment,e),E(Tt.$$.fragment,e),E(jt.$$.fragment,e),E(xt.$$.fragment,e),E(zt.$$.fragment,e),E(Pt.$$.fragment,e),E(Mt.$$.fragment,e),E(Ct.$$.fragment,e),E(It.$$.fragment,e),E(Nt.$$.fragment,e),E(Ht.$$.fragment,e),E(tt.$$.fragment,e),E(Lt.$$.fragment,e),E(Wt.$$.fragment,e),E(Ut.$$.fragment,e),E(Rt.$$.fragment,e),E(Gt.$$.fragment,e),E(Bt.$$.fragment,e),E(it.$$.fragment,e),E(Yt.$$.fragment,e),E(ut.$$.fragment,e),E(mt.$$.fragment,e),E(ct.$$.fragment,e),E(Kt.$$.fragment,e),E(dt.$$.fragment,e),E($t.$$.fragment,e),E(Zt.$$.fragment,e),E(Xt.$$.fragment,e),E(gt.$$.fragment,e),E(es.$$.fragment,e),oo=!0)},o(e){T(_.$$.fragment,e),T(I.$$.fragment,e),T(Y.$$.fragment,e),T(V.$$.fragment,e),T(O.$$.fragment,e),T(Re.$$.fragment,e),T(Et.$$.fragment,e),T(Ye.$$.fragment,e),T(Tt.$$.fragment,e),T(jt.$$.fragment,e),T(xt.$$.fragment,e),T(zt.$$.fragment,e),T(Pt.$$.fragment,e),T(Mt.$$.fragment,e),T(Ct.$$.fragment,e),T(It.$$.fragment,e),T(Nt.$$.fragment,e),T(Ht.$$.fragment,e),T(tt.$$.fragment,e),T(Lt.$$.fragment,e),T(Wt.$$.fragment,e),T(Ut.$$.fragment,e),T(Rt.$$.fragment,e),T(Gt.$$.fragment,e),T(Bt.$$.fragment,e),T(it.$$.fragment,e),T(Yt.$$.fragment,e),T(ut.$$.fragment,e),T(mt.$$.fragment,e),T(ct.$$.fragment,e),T(Kt.$$.fragment,e),T(dt.$$.fragment,e),T($t.$$.fragment,e),T(Zt.$$.fragment,e),T(Xt.$$.fragment,e),T(gt.$$.fragment,e),T(es.$$.fragment,e),oo=!1},d(e){s(a),e&&s(m),e&&s(r),A(_),e&&s(S),A(I,e),e&&s(N),e&&s(D),e&&s(J),A(Y,e),e&&s(se),e&&s(Q),A(V),e&&s($e),e&&s(oe),e&&s(P),A(O,e),e&&s(le),e&&s(x),e&&s(we),e&&s(pe),e&&s(kt),e&&s(B),e&&s(Ya),e&&s(yt),e&&s(Ja),e&&s(ke),e&&s(Qa),e&&s(bt),e&&s(Va),e&&s(Ue),e&&s(Ka),A(Re,e),e&&s(Za),e&&s(Se),A(Et),e&&s(Xa),e&&s(Be),e&&s(er),e&&s(ns),e&&s(tr),A(Ye,e),e&&s(sr),e&&s(Je),e&&s(ar),A(Tt,e),e&&s(rr),e&&s(ye),e&&s(or),A(jt,e),e&&s(nr),e&&s(Qe),e&&s(lr),A(xt,e),e&&s(ir),e&&s(be),e&&s(pr),A(zt,e),e&&s(fr),e&&s(Ve),e&&s(ur),A(Pt,e),e&&s(mr),e&&s(Ee),e&&s(cr),A(Mt,e),e&&s(hr),e&&s(Ke),e&&s(dr),A(Ct,e),e&&s($r),e&&s(Ze),e&&s(_r),A(It,e),e&&s(gr),e&&s(Xe),e&&s(wr),e&&s(Me),A(Nt),e&&s(vr),e&&s(fe),e&&s(kr),A(Ht,e),e&&s(yr),A(tt,e),e&&s(br),e&&s(Te),e&&s(Er),A(Lt,e),e&&s(Tr),e&&s(Ae),e&&s(Ar),e&&s(Ce),A(Wt),e&&s(jr),A(Ut,e),e&&s(xr),e&&s(Z),e&&s(qr),e&&s(je),e&&s(zr),e&&s(Ie),A(Rt),e&&s(Pr),e&&s(xe),e&&s(Fr),e&&s(rt),e&&s(Sr),A(Gt,e),e&&s(Mr),e&&s(ot),e&&s(Cr),e&&s(Ts),e&&s(Ir),A(Bt,e),e&&s(Nr),e&&s(As),e&&s(Or),e&&s(nt),e&&s(Dr),e&&s(lt),e&&s(Hr),A(it,e),e&&s(Lr),e&&s(pt),e&&s(Wr),e&&s(Ne),A(Yt),e&&s(Ur),A(ut,e),e&&s(Rr),A(mt,e),e&&s(Gr),e&&s(X),e&&s(Br),A(ct,e),e&&s(Yr),e&&s(Oe),A(Kt),e&&s(Jr),A(dt,e),e&&s(Qr),e&&s(qe),e&&s(Vr),A($t,e),e&&s(Kr),e&&s(De),A(Zt),e&&s(Zr),e&&s(Cs),e&&s(Xr),e&&s(Is),e&&s(eo),A(Xt,e),e&&s(to),A(gt,e),e&&s(so),e&&s(wt),e&&s(ao),e&&s(He),A(es),e&&s(ro),e&&s(Os)}}}const _u={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"},{local:"custom-model-builds",title:"Custom model builds"},{local:"whats-next",title:"What's next?"}],title:"Quick tour"};function gu(z){return Mf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Au extends zf{constructor(a){super();Pf(this,a,gu,$u,Ff,{})}}export{Au as default,_u as metadata};
