import{S as oi,i as ni,s as li,e as l,k as v,t as o,c as i,a as p,m as y,h as n,d as s,b as $,g as u,G as t,Q as Ma,q as E,l as Wp,n as wo,o as z,B as P,p as bo,w as F,y as x,j as Zp,K as Xp,U as Yp,x as C,V as ef,T as tf,Y as Gp,Z as Vp,M as sf,v as af,L as ta}from"../chunks/vendor-hf-doc-builder.js";import{T as ns}from"../chunks/Tip-hf-doc-builder.js";import{Y as Up}from"../chunks/Youtube-hf-doc-builder.js";import{I as Ne}from"../chunks/IconCopyLink-hf-doc-builder.js";import{a as Qp,C as U}from"../chunks/CodeBlock-hf-doc-builder.js";import{g as Jp,I as rf,a as of,F as ko,M as ct}from"../chunks/Markdown-hf-doc-builder.js";import{D as nf}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";function Lp(b,a,m){const r=b.slice();return r[8]=a[m],r[10]=m,r}function Rp(b){let a,m,r;var h=b[8].icon;function d(_){return{props:{classNames:"mr-1.5"}}}return h&&(a=new h(d())),{c(){a&&F(a.$$.fragment),m=Wp()},l(_){a&&C(a.$$.fragment,_),m=Wp()},m(_,k){a&&x(a,_,k),u(_,m,k),r=!0},p(_,k){if(h!==(h=_[8].icon)){if(a){wo();const w=a;z(w.$$.fragment,1,0,()=>{P(w,1)}),bo()}h?(a=new h(d()),F(a.$$.fragment),E(a.$$.fragment,1),x(a,m.parentNode,m)):a=null}},i(_){r||(a&&E(a.$$.fragment,_),r=!0)},o(_){a&&z(a.$$.fragment,_),r=!1},d(_){_&&s(m),a&&P(a,_)}}}function Bp(b){let a,m,r,h=b[8].name+"",d,_,k,w,c,g,T,A=b[8].icon&&Rp(b);function N(){return b[6](b[8])}return{c(){a=l("button"),A&&A.c(),m=v(),r=l("p"),d=o(h),k=v(),this.h()},l(D){a=i(D,"BUTTON",{class:!0});var q=p(a);A&&A.l(q),m=y(q),r=i(q,"P",{class:!0});var M=p(r);d=n(M,h),M.forEach(s),k=y(q),q.forEach(s),this.h()},h(){$(r,"class",_="!m-0 "+b[8].classNames),$(a,"class",w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(b[10]?"r":"l")+" "+(b[8].group!==b[1]&&"text-gray-500 filter grayscale"))},m(D,q){u(D,a,q),A&&A.m(a,null),t(a,m),t(a,r),t(r,d),t(a,k),c=!0,g||(T=Ma(a,"click",N),g=!0)},p(D,q){b=D,b[8].icon?A?(A.p(b,q),q&1&&E(A,1)):(A=Rp(b),A.c(),E(A,1),A.m(a,m)):A&&(wo(),z(A,1,1,()=>{A=null}),bo()),(!c||q&1)&&h!==(h=b[8].name+"")&&Zp(d,h),(!c||q&1&&_!==(_="!m-0 "+b[8].classNames))&&$(r,"class",_),(!c||q&3&&w!==(w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(b[10]?"r":"l")+" "+(b[8].group!==b[1]&&"text-gray-500 filter grayscale")))&&$(a,"class",w)},i(D){c||(E(A),c=!0)},o(D){z(A),c=!1},d(D){D&&s(a),A&&A.d(),g=!1,T()}}}function lf(b){let a,m,r,h=b[3].filter(b[5]),d=[];for(let k=0;k<h.length;k+=1)d[k]=Bp(Lp(b,h,k));const _=k=>z(d[k],1,1,()=>{d[k]=null});return{c(){a=l("div"),m=l("div");for(let k=0;k<d.length;k+=1)d[k].c();this.h()},l(k){a=i(k,"DIV",{});var w=p(a);m=i(w,"DIV",{class:!0});var c=p(m);for(let g=0;g<d.length;g+=1)d[g].l(c);c.forEach(s),w.forEach(s),this.h()},h(){$(m,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(k,w){u(k,a,w),t(a,m);for(let c=0;c<d.length;c+=1)d[c].m(m,null);r=!0},p(k,[w]){if(w&27){h=k[3].filter(k[5]);let c;for(c=0;c<h.length;c+=1){const g=Lp(k,h,c);d[c]?(d[c].p(g,w),E(d[c],1)):(d[c]=Bp(g),d[c].c(),E(d[c],1),d[c].m(m,null))}for(wo(),c=h.length;c<d.length;c+=1)_(c);bo()}},i(k){if(!r){for(let w=0;w<h.length;w+=1)E(d[w]);r=!0}},o(k){d=d.filter(Boolean);for(let w=0;w<d.length;w+=1)z(d[w]);r=!1},d(k){k&&s(a),Xp(d,k)}}}function pf(b,a,m){let r,{ids:h}=a;const d=h.join("-"),_=Jp(d);Yp(b,_,T=>m(1,r=T));const k=[{id:"pt",classNames:"",icon:rf,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:of,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function w(T){ef(_,r=T,r)}const c=T=>h.includes(T.id),g=T=>w(T.group);return b.$$set=T=>{"ids"in T&&m(0,h=T.ids)},[h,r,_,k,w,c,g]}class Kp extends oi{constructor(a){super();ni(this,a,pf,lf,li,{ids:0})}}function ff(b){let a,m,r,h,d,_,k=b[1].highlighted+"",w;return m=new Qp({props:{classNames:"transition duration-200 ease-in-out "+(b[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:b[1].code}}),d=new Kp({props:{ids:b[4]}}),{c(){a=l("div"),F(m.$$.fragment),r=v(),h=l("pre"),F(d.$$.fragment),_=new Gp,this.h()},l(c){a=i(c,"DIV",{class:!0});var g=p(a);C(m.$$.fragment,g),g.forEach(s),r=y(c),h=i(c,"PRE",{});var T=p(h);C(d.$$.fragment,T),_=Vp(T),T.forEach(s),this.h()},h(){$(a,"class","absolute top-2.5 right-4"),_.a=null},m(c,g){u(c,a,g),x(m,a,null),u(c,r,g),u(c,h,g),x(d,h,null),_.m(k,h),w=!0},p(c,g){const T={};g&4&&(T.classNames="transition duration-200 ease-in-out "+(c[2]&&"opacity-0")),g&2&&(T.value=c[1].code),m.$set(T),(!w||g&2)&&k!==(k=c[1].highlighted+"")&&_.p(k)},i(c){w||(E(m.$$.fragment,c),E(d.$$.fragment,c),w=!0)},o(c){z(m.$$.fragment,c),z(d.$$.fragment,c),w=!1},d(c){c&&s(a),P(m),c&&s(r),c&&s(h),P(d)}}}function uf(b){let a,m,r,h,d,_,k=b[0].highlighted+"",w;return m=new Qp({props:{classNames:"transition duration-200 ease-in-out "+(b[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:b[0].code}}),d=new Kp({props:{ids:b[4]}}),{c(){a=l("div"),F(m.$$.fragment),r=v(),h=l("pre"),F(d.$$.fragment),_=new Gp,this.h()},l(c){a=i(c,"DIV",{class:!0});var g=p(a);C(m.$$.fragment,g),g.forEach(s),r=y(c),h=i(c,"PRE",{});var T=p(h);C(d.$$.fragment,T),_=Vp(T),T.forEach(s),this.h()},h(){$(a,"class","absolute top-2.5 right-4"),_.a=null},m(c,g){u(c,a,g),x(m,a,null),u(c,r,g),u(c,h,g),x(d,h,null),_.m(k,h),w=!0},p(c,g){const T={};g&4&&(T.classNames="transition duration-200 ease-in-out "+(c[2]&&"opacity-0")),g&1&&(T.value=c[0].code),m.$set(T),(!w||g&1)&&k!==(k=c[0].highlighted+"")&&_.p(k)},i(c){w||(E(m.$$.fragment,c),E(d.$$.fragment,c),w=!0)},o(c){z(m.$$.fragment,c),z(d.$$.fragment,c),w=!1},d(c){c&&s(a),P(m),c&&s(r),c&&s(h),P(d)}}}function mf(b){let a,m,r,h,d,_;const k=[uf,ff],w=[];function c(g,T){return g[3]==="group1"?0:1}return m=c(b),r=w[m]=k[m](b),{c(){a=l("div"),r.c(),this.h()},l(g){a=i(g,"DIV",{class:!0});var T=p(a);r.l(T),T.forEach(s),this.h()},h(){$(a,"class","code-block relative")},m(g,T){u(g,a,T),w[m].m(a,null),h=!0,d||(_=[Ma(a,"mouseover",b[6]),Ma(a,"focus",b[6]),Ma(a,"mouseout",b[7]),Ma(a,"focus",b[7])],d=!0)},p(g,[T]){let A=m;m=c(g),m===A?w[m].p(g,T):(wo(),z(w[A],1,1,()=>{w[A]=null}),bo(),r=w[m],r?r.p(g,T):(r=w[m]=k[m](g),r.c()),E(r,1),r.m(a,null))},i(g){h||(E(r),h=!0)},o(g){z(r),h=!1},d(g){g&&s(a),w[m].d(),d=!1,tf(_)}}}function cf(b,a,m){let r,{group1:h}=a,{group2:d}=a;const _=[h.id,d.id],k=_.join("-"),w=Jp(k);Yp(b,w,A=>m(3,r=A));let c=!0;function g(){m(2,c=!1)}function T(){m(2,c=!0)}return b.$$set=A=>{"group1"in A&&m(0,h=A.group1),"group2"in A&&m(1,d=A.group2)},[h,d,c,r,_,w,g,T]}class hf extends oi{constructor(a){super();ni(this,a,cf,mf,li,{group1:0,group2:1})}}function df(b){let a,m;return{c(){a=l("p"),m=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var h=p(a);m=n(h,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),h.forEach(s)},m(r,h){u(r,a,h),t(a,m)},d(r){r&&s(a)}}}function _f(b){let a,m,r,h,d,_,k,w;return{c(){a=l("p"),m=o("For more details about the tasks supported by "),r=l("a"),h=o("pipeline()"),d=o(", take a look at the "),_=l("a"),k=o("pipelines API reference"),w=o("."),this.h()},l(c){a=i(c,"P",{});var g=p(a);m=n(g,"For more details about the tasks supported by "),r=i(g,"A",{href:!0});var T=p(r);h=n(T,"pipeline()"),T.forEach(s),d=n(g,", take a look at the "),_=i(g,"A",{href:!0});var A=p(_);k=n(A,"pipelines API reference"),A.forEach(s),w=n(g,"."),g.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(_,"href","./main_classes/pipelines")},m(c,g){u(c,a,g),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w)},d(c){c&&s(a)}}}function $f(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D;return N=new U({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use "),r=l("a"),h=o("AutoModelForSequenceClassification"),d=o(" and "),_=l("a"),k=o("AutoTokenizer"),w=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),c=l("code"),g=o("AutoClass"),T=o(" in the next section):"),A=v(),F(N.$$.fragment),this.h()},l(q){a=i(q,"P",{});var M=p(a);m=n(M,"Use "),r=i(M,"A",{href:!0});var j=p(r);h=n(j,"AutoModelForSequenceClassification"),j.forEach(s),d=n(M," and "),_=i(M,"A",{href:!0});var O=p(_);k=n(O,"AutoTokenizer"),O.forEach(s),w=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),c=i(M,"CODE",{});var R=p(c);g=n(R,"AutoClass"),R.forEach(s),T=n(M," in the next section):"),M.forEach(s),A=y(q),C(N.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer")},m(q,M){u(q,a,M),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w),t(a,c),t(c,g),t(a,T),u(q,A,M),x(N,q,M),D=!0},p:ta,i(q){D||(E(N.$$.fragment,q),D=!0)},o(q){z(N.$$.fragment,q),D=!1},d(q){q&&s(a),q&&s(A),P(N,q)}}}function gf(b){let a,m;return a=new ct({props:{$$slots:{default:[$f]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function vf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D;return N=new U({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use "),r=l("a"),h=o("TFAutoModelForSequenceClassification"),d=o(" and "),_=l("a"),k=o("AutoTokenizer"),w=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),c=l("code"),g=o("TFAutoClass"),T=o(" in the next section):"),A=v(),F(N.$$.fragment),this.h()},l(q){a=i(q,"P",{});var M=p(a);m=n(M,"Use "),r=i(M,"A",{href:!0});var j=p(r);h=n(j,"TFAutoModelForSequenceClassification"),j.forEach(s),d=n(M," and "),_=i(M,"A",{href:!0});var O=p(_);k=n(O,"AutoTokenizer"),O.forEach(s),w=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),c=i(M,"CODE",{});var R=p(c);g=n(R,"TFAutoClass"),R.forEach(s),T=n(M," in the next section):"),M.forEach(s),A=y(q),C(N.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer")},m(q,M){u(q,a,M),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w),t(a,c),t(c,g),t(a,T),u(q,A,M),x(N,q,M),D=!0},p:ta,i(q){D||(E(N.$$.fragment,q),D=!0)},o(q){z(N.$$.fragment,q),D=!1},d(q){q&&s(a),q&&s(A),P(N,q)}}}function yf(b){let a,m;return a=new ct({props:{$$slots:{default:[vf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function kf(b){let a,m,r,h,d,_,k,w,c,g,T;return{c(){a=l("p"),m=o("Check out the "),r=l("a"),h=o("preprocess"),d=o(" tutorial for more details about tokenization, and how to use an "),_=l("a"),k=o("AutoFeatureExtractor"),w=o(" and "),c=l("a"),g=o("AutoProcessor"),T=o(" to preprocess image, audio, and multimodal inputs."),this.h()},l(A){a=i(A,"P",{});var N=p(a);m=n(N,"Check out the "),r=i(N,"A",{href:!0});var D=p(r);h=n(D,"preprocess"),D.forEach(s),d=n(N," tutorial for more details about tokenization, and how to use an "),_=i(N,"A",{href:!0});var q=p(_);k=n(q,"AutoFeatureExtractor"),q.forEach(s),w=n(N," and "),c=i(N,"A",{href:!0});var M=p(c);g=n(M,"AutoProcessor"),M.forEach(s),T=n(N," to preprocess image, audio, and multimodal inputs."),N.forEach(s),this.h()},h(){$(r,"href","./preprocessing"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoFeatureExtractor"),$(c,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoProcessor")},m(A,N){u(A,a,N),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w),t(a,c),t(c,g),t(a,T)},d(A){A&&s(a)}}}function wf(b){let a,m,r,h,d,_,k,w;return{c(){a=l("p"),m=o("See the "),r=l("a"),h=o("task summary"),d=o(" for tasks supported by an "),_=l("a"),k=o("AutoModel"),w=o(" class."),this.h()},l(c){a=i(c,"P",{});var g=p(a);m=n(g,"See the "),r=i(g,"A",{href:!0});var T=p(r);h=n(T,"task summary"),T.forEach(s),d=n(g," for tasks supported by an "),_=i(g,"A",{href:!0});var A=p(_);k=n(A,"AutoModel"),A.forEach(s),w=n(g," class."),g.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(c,g){u(c,a,g),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w)},d(c){c&&s(a)}}}function bf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D,q,M,j,O,R,L,J,se,Y,ie,B,Z,ae,V,le,X,de,ee,re,Q,pe,S,H,G;return M=new U({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),O=new ns({props:{$$slots:{default:[wf]},$$scope:{ctx:b}}}),Z=new U({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),H=new U({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),h=o("AutoModel"),d=o(" like you would load an "),_=l("a"),k=o("AutoTokenizer"),w=o(". The only difference is selecting the correct "),c=l("a"),g=o("AutoModel"),T=o(" for the task. For text (or sequence) classification, you should load "),A=l("a"),N=o("AutoModelForSequenceClassification"),D=o(":"),q=v(),F(M.$$.fragment),j=v(),F(O.$$.fragment),R=v(),L=l("p"),J=o("Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),se=l("code"),Y=o("**"),ie=o(":"),B=v(),F(Z.$$.fragment),ae=v(),V=l("p"),le=o("The model outputs the final activations in the "),X=l("code"),de=o("logits"),ee=o(" attribute. Apply the softmax function to the "),re=l("code"),Q=o("logits"),pe=o(" to retrieve the probabilities:"),S=v(),F(H.$$.fragment),this.h()},l(I){a=i(I,"P",{});var W=p(a);m=n(W,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(W,"A",{href:!0});var fe=p(r);h=n(fe,"AutoModel"),fe.forEach(s),d=n(W," like you would load an "),_=i(W,"A",{href:!0});var qe=p(_);k=n(qe,"AutoTokenizer"),qe.forEach(s),w=n(W,". The only difference is selecting the correct "),c=i(W,"A",{href:!0});var oe=p(c);g=n(oe,"AutoModel"),oe.forEach(s),T=n(W," for the task. For text (or sequence) classification, you should load "),A=i(W,"A",{href:!0});var ue=p(A);N=n(ue,"AutoModelForSequenceClassification"),ue.forEach(s),D=n(W,":"),W.forEach(s),q=y(I),C(M.$$.fragment,I),j=y(I),C(O.$$.fragment,I),R=y(I),L=i(I,"P",{});var _e=p(L);J=n(_e,"Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),se=i(_e,"CODE",{});var ht=p(se);Y=n(ht,"**"),ht.forEach(s),ie=n(_e,":"),_e.forEach(s),B=y(I),C(Z.$$.fragment,I),ae=y(I),V=i(I,"P",{});var te=p(V);le=n(te,"The model outputs the final activations in the "),X=i(te,"CODE",{});var dt=p(X);de=n(dt,"logits"),dt.forEach(s),ee=n(te," attribute. Apply the softmax function to the "),re=i(te,"CODE",{});var ze=p(re);Q=n(ze,"logits"),ze.forEach(s),pe=n(te," to retrieve the probabilities:"),te.forEach(s),S=y(I),C(H.$$.fragment,I),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(c,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),$(A,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(I,W){u(I,a,W),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w),t(a,c),t(c,g),t(a,T),t(a,A),t(A,N),t(a,D),u(I,q,W),x(M,I,W),u(I,j,W),x(O,I,W),u(I,R,W),u(I,L,W),t(L,J),t(L,se),t(se,Y),t(L,ie),u(I,B,W),x(Z,I,W),u(I,ae,W),u(I,V,W),t(V,le),t(V,X),t(X,de),t(V,ee),t(V,re),t(re,Q),t(V,pe),u(I,S,W),x(H,I,W),G=!0},p(I,W){const fe={};W&2&&(fe.$$scope={dirty:W,ctx:I}),O.$set(fe)},i(I){G||(E(M.$$.fragment,I),E(O.$$.fragment,I),E(Z.$$.fragment,I),E(H.$$.fragment,I),G=!0)},o(I){z(M.$$.fragment,I),z(O.$$.fragment,I),z(Z.$$.fragment,I),z(H.$$.fragment,I),G=!1},d(I){I&&s(a),I&&s(q),P(M,I),I&&s(j),P(O,I),I&&s(R),I&&s(L),I&&s(B),P(Z,I),I&&s(ae),I&&s(V),I&&s(S),P(H,I)}}}function Tf(b){let a,m;return a=new ct({props:{$$slots:{default:[bf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function Af(b){let a,m,r,h,d,_,k,w;return{c(){a=l("p"),m=o("See the "),r=l("a"),h=o("task summary"),d=o(" for tasks supported by an "),_=l("a"),k=o("AutoModel"),w=o(" class."),this.h()},l(c){a=i(c,"P",{});var g=p(a);m=n(g,"See the "),r=i(g,"A",{href:!0});var T=p(r);h=n(T,"task summary"),T.forEach(s),d=n(g," for tasks supported by an "),_=i(g,"A",{href:!0});var A=p(_);k=n(A,"AutoModel"),A.forEach(s),w=n(g," class."),g.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(c,g){u(c,a,g),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w)},d(c){c&&s(a)}}}function jf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D,q,M,j,O,R,L,J,se,Y,ie,B,Z,ae,V,le,X,de,ee,re,Q,pe;return M=new U({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),O=new ns({props:{$$slots:{default:[Af]},$$scope:{ctx:b}}}),Y=new U({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),Q=new U({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),h=o("TFAutoModel"),d=o(" like you would load an "),_=l("a"),k=o("AutoTokenizer"),w=o(". The only difference is selecting the correct "),c=l("a"),g=o("TFAutoModel"),T=o(" for the task. For text (or sequence) classification, you should load "),A=l("a"),N=o("TFAutoModelForSequenceClassification"),D=o(":"),q=v(),F(M.$$.fragment),j=v(),F(O.$$.fragment),R=v(),L=l("p"),J=o("Now pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),se=v(),F(Y.$$.fragment),ie=v(),B=l("p"),Z=o("The model outputs the final activations in the "),ae=l("code"),V=o("logits"),le=o(" attribute. Apply the softmax function to the "),X=l("code"),de=o("logits"),ee=o(" to retrieve the probabilities:"),re=v(),F(Q.$$.fragment),this.h()},l(S){a=i(S,"P",{});var H=p(a);m=n(H,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(H,"A",{href:!0});var G=p(r);h=n(G,"TFAutoModel"),G.forEach(s),d=n(H," like you would load an "),_=i(H,"A",{href:!0});var I=p(_);k=n(I,"AutoTokenizer"),I.forEach(s),w=n(H,". The only difference is selecting the correct "),c=i(H,"A",{href:!0});var W=p(c);g=n(W,"TFAutoModel"),W.forEach(s),T=n(H," for the task. For text (or sequence) classification, you should load "),A=i(H,"A",{href:!0});var fe=p(A);N=n(fe,"TFAutoModelForSequenceClassification"),fe.forEach(s),D=n(H,":"),H.forEach(s),q=y(S),C(M.$$.fragment,S),j=y(S),C(O.$$.fragment,S),R=y(S),L=i(S,"P",{});var qe=p(L);J=n(qe,"Now pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),qe.forEach(s),se=y(S),C(Y.$$.fragment,S),ie=y(S),B=i(S,"P",{});var oe=p(B);Z=n(oe,"The model outputs the final activations in the "),ae=i(oe,"CODE",{});var ue=p(ae);V=n(ue,"logits"),ue.forEach(s),le=n(oe," attribute. Apply the softmax function to the "),X=i(oe,"CODE",{});var _e=p(X);de=n(_e,"logits"),_e.forEach(s),ee=n(oe," to retrieve the probabilities:"),oe.forEach(s),re=y(S),C(Q.$$.fragment,S),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),$(_,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(c,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),$(A,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(S,H){u(S,a,H),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w),t(a,c),t(c,g),t(a,T),t(a,A),t(A,N),t(a,D),u(S,q,H),x(M,S,H),u(S,j,H),x(O,S,H),u(S,R,H),u(S,L,H),t(L,J),u(S,se,H),x(Y,S,H),u(S,ie,H),u(S,B,H),t(B,Z),t(B,ae),t(ae,V),t(B,le),t(B,X),t(X,de),t(B,ee),u(S,re,H),x(Q,S,H),pe=!0},p(S,H){const G={};H&2&&(G.$$scope={dirty:H,ctx:S}),O.$set(G)},i(S){pe||(E(M.$$.fragment,S),E(O.$$.fragment,S),E(Y.$$.fragment,S),E(Q.$$.fragment,S),pe=!0)},o(S){z(M.$$.fragment,S),z(O.$$.fragment,S),z(Y.$$.fragment,S),z(Q.$$.fragment,S),pe=!1},d(S){S&&s(a),S&&s(q),P(M,S),S&&s(j),P(O,S),S&&s(R),S&&s(L),S&&s(se),P(Y,S),S&&s(ie),S&&s(B),S&&s(re),P(Q,S)}}}function Ef(b){let a,m;return a=new ct({props:{$$slots:{default:[jf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function qf(b){let a,m,r,h,d;return{c(){a=l("p"),m=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) output the tensors "),r=l("em"),h=o("before"),d=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored.`)},l(_){a=i(_,"P",{});var k=p(a);m=n(k,"All \u{1F917} Transformers models (PyTorch or TensorFlow) output the tensors "),r=i(k,"EM",{});var w=p(r);h=n(w,"before"),w.forEach(s),d=n(k,` the final activation
function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored.`),k.forEach(s)},m(_,k){u(_,a,k),t(a,m),t(a,r),t(r,h),t(a,d)},d(_){_&&s(a)}}}function zf(b){let a,m,r,h,d,_,k,w;return{c(){a=l("p"),m=o("For TensorFlow, convert the dataset into a TensorFlow compatible format with "),r=l("code"),h=o("to_tf_dataset"),d=o(". Then you can compile and fit your model with the usual Keras methods. Take a look at the "),_=l("a"),k=o("fine-tuning tutorial"),w=o(" for more details."),this.h()},l(c){a=i(c,"P",{});var g=p(a);m=n(g,"For TensorFlow, convert the dataset into a TensorFlow compatible format with "),r=i(g,"CODE",{});var T=p(r);h=n(T,"to_tf_dataset"),T.forEach(s),d=n(g,". Then you can compile and fit your model with the usual Keras methods. Take a look at the "),_=i(g,"A",{href:!0});var A=p(_);k=n(A,"fine-tuning tutorial"),A.forEach(s),w=n(g," for more details."),g.forEach(s),this.h()},h(){$(_,"href","./training#convert-dataset-to-tensorflow-format")},m(c,g){u(c,a,g),t(a,m),t(a,r),t(r,h),t(a,d),t(a,_),t(_,k),t(a,w)},d(c){c&&s(a)}}}function Pf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D,q,M;return k=new U({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),q=new U({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),h=o("PreTrainedModel.save_pretrained()"),d=o(":"),_=v(),F(k.$$.fragment),w=v(),c=l("p"),g=o("When you are ready to use the model again, reload it with "),T=l("a"),A=o("PreTrainedModel.from_pretrained()"),N=o(":"),D=v(),F(q.$$.fragment),this.h()},l(j){a=i(j,"P",{});var O=p(a);m=n(O,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(O,"A",{href:!0});var R=p(r);h=n(R,"PreTrainedModel.save_pretrained()"),R.forEach(s),d=n(O,":"),O.forEach(s),_=y(j),C(k.$$.fragment,j),w=y(j),c=i(j,"P",{});var L=p(c);g=n(L,"When you are ready to use the model again, reload it with "),T=i(L,"A",{href:!0});var J=p(T);A=n(J,"PreTrainedModel.from_pretrained()"),J.forEach(s),N=n(L,":"),L.forEach(s),D=y(j),C(q.$$.fragment,j),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(T,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(j,O){u(j,a,O),t(a,m),t(a,r),t(r,h),t(a,d),u(j,_,O),x(k,j,O),u(j,w,O),u(j,c,O),t(c,g),t(c,T),t(T,A),t(c,N),u(j,D,O),x(q,j,O),M=!0},p:ta,i(j){M||(E(k.$$.fragment,j),E(q.$$.fragment,j),M=!0)},o(j){z(k.$$.fragment,j),z(q.$$.fragment,j),M=!1},d(j){j&&s(a),j&&s(_),P(k,j),j&&s(w),j&&s(c),j&&s(D),P(q,j)}}}function Ff(b){let a,m;return a=new ct({props:{$$slots:{default:[Pf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function xf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D,q,M;return k=new U({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),q=new U({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),h=o("TFPreTrainedModel.save_pretrained()"),d=o(":"),_=v(),F(k.$$.fragment),w=v(),c=l("p"),g=o("When you are ready to use the model again, reload it with "),T=l("a"),A=o("TFPreTrainedModel.from_pretrained()"),N=o(":"),D=v(),F(q.$$.fragment),this.h()},l(j){a=i(j,"P",{});var O=p(a);m=n(O,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(O,"A",{href:!0});var R=p(r);h=n(R,"TFPreTrainedModel.save_pretrained()"),R.forEach(s),d=n(O,":"),O.forEach(s),_=y(j),C(k.$$.fragment,j),w=y(j),c=i(j,"P",{});var L=p(c);g=n(L,"When you are ready to use the model again, reload it with "),T=i(L,"A",{href:!0});var J=p(T);A=n(J,"TFPreTrainedModel.from_pretrained()"),J.forEach(s),N=n(L,":"),L.forEach(s),D=y(j),C(q.$$.fragment,j),this.h()},h(){$(r,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(T,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(j,O){u(j,a,O),t(a,m),t(a,r),t(r,h),t(a,d),u(j,_,O),x(k,j,O),u(j,w,O),u(j,c,O),t(c,g),t(c,T),t(T,A),t(c,N),u(j,D,O),x(q,j,O),M=!0},p:ta,i(j){M||(E(k.$$.fragment,j),E(q.$$.fragment,j),M=!0)},o(j){z(k.$$.fragment,j),z(q.$$.fragment,j),M=!1},d(j){j&&s(a),j&&s(_),P(k,j),j&&s(w),j&&s(c),j&&s(D),P(q,j)}}}function Cf(b){let a,m;return a=new ct({props:{$$slots:{default:[xf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function Mf(b){let a,m;return a=new U({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p:ta,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function Sf(b){let a,m;return a=new ct({props:{$$slots:{default:[Mf]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function If(b){let a,m;return a=new U({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p:ta,i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function Of(b){let a,m;return a=new ct({props:{$$slots:{default:[If]},$$scope:{ctx:b}}}),{c(){F(a.$$.fragment)},l(r){C(a.$$.fragment,r)},m(r,h){x(a,r,h),m=!0},p(r,h){const d={};h&2&&(d.$$scope={dirty:h,ctx:r}),a.$set(d)},i(r){m||(E(a.$$.fragment,r),m=!0)},o(r){z(a.$$.fragment,r),m=!1},d(r){P(a,r)}}}function Nf(b){let a,m,r,h,d,_,k,w,c,g,T,A,N,D,q,M,j,O,R,L,J,se,Y,ie,B,Z,ae,V,le,X,de,ee,re,Q,pe,S,H,G,I,W,fe,qe,oe,ue,_e,ht,te,dt,ze,To,Sa,_t,Ia,ls,Ao,Oa,$t,Na,Pe,De,sa,gt,jo,aa,Eo,Da,ge,qo,is,zo,Po,ps,Fo,xo,Ha,vt,Wa,He,Ua,We,Co,fs,Mo,So,La,yt,Ra,me,Io,us,Oo,No,kt,Do,Ho,ra,Wo,Uo,Ba,wt,Ya,Ue,Lo,ms,Ro,Bo,Ga,bt,Va,Le,Yo,cs,Go,Vo,Qa,Tt,Ja,ve,Qo,At,Jo,Ko,jt,Zo,Xo,Ka,Et,Za,Re,en,qt,oa,tn,sn,Xa,zt,er,Be,an,na,rn,on,tr,Pt,sr,Ye,nn,hs,ln,pn,ar,Fe,Ge,la,Ft,fn,ia,un,rr,ne,mn,ds,cn,hn,xt,dn,_n,_s,$n,gn,Ct,vn,yn,or,Mt,nr,Ve,lr,ye,kn,$s,wn,bn,pa,Tn,An,ir,St,pr,ke,jn,gs,En,qn,vs,zn,Pn,fr,xe,Qe,fa,It,Fn,ua,xn,ur,Ot,mr,K,Cn,ys,Mn,Sn,ks,In,On,ws,Nn,Dn,bs,Hn,Wn,ma,Un,Ln,cr,we,Rn,ca,Bn,Yn,Ts,Gn,Vn,hr,Ce,Je,ha,Nt,Qn,da,Jn,dr,Ke,Kn,As,Zn,Xn,_r,Ze,el,js,tl,sl,$r,Dt,gr,Es,al,vr,Ht,yr,qs,rl,kr,Xe,zs,Ps,ol,nl,ll,Fs,xs,il,pl,wr,Cs,fl,br,Wt,Tr,et,Ar,Me,tt,_a,Ut,ul,$a,ml,jr,st,Er,at,qr,Se,rt,ga,Lt,cl,va,hl,zr,Ms,dl,Pr,Ss,_l,Fr,Rt,xr,Is,$l,Cr,Bt,Mr,ot,gl,Os,vl,yl,Sr,Ie,nt,ya,Yt,kl,ka,wl,Ir,ce,bl,Gt,wa,Tl,Al,Vt,ba,jl,El,Ns,ql,zl,Or,lt,Nr,be,Pl,Ds,Fl,xl,Qt,Cl,Ml,Dr,Jt,Hr,Hs,Sl,Wr,Kt,Ur,Ws,Il,Lr,Zt,Rr,Te,Ol,Us,Nl,Dl,Xt,Ta,Hl,Wl,Ul,Br,Ls,Ll,Yr,es,Gr,Ae,Rl,Rs,Bl,Yl,Bs,Gl,Vl,Vr,ts,Qr,Oe,it,Aa,ss,Ql,ja,Jl,Jr,pt,Kr,je,Kl,Ea,Zl,Xl,qa,ei,ti,Zr,ft,Xr;return _=new Ne({}),T=new nf({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),te=new ns({props:{$$slots:{default:[df]},$$scope:{ctx:b}}}),_t=new U({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),$t=new U({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),gt=new Ne({}),vt=new Up({props:{id:"tiZFewofSLM"}}),He=new ns({props:{$$slots:{default:[_f]},$$scope:{ctx:b}}}),yt=new U({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),wt=new U({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),bt=new U({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Tt=new U({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Et=new U({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),zt=new U({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),Pt=new U({props:{code:`result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I TURN A JOIN A COUNT&#x27;</span>]`}}),Ft=new Ne({}),Mt=new U({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ve=new ko({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[yf],pytorch:[gf]},$$scope:{ctx:b}}}),St=new U({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),It=new Ne({}),Ot=new Up({props:{id:"AhChOFRegn4"}}),Nt=new Ne({}),Dt=new U({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Ht=new U({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Wt=new hf({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),et=new ns({props:{$$slots:{default:[kf]},$$scope:{ctx:b}}}),Ut=new Ne({}),st=new ko({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ef],pytorch:[Tf]},$$scope:{ctx:b}}}),at=new ns({props:{$$slots:{default:[qf]},$$scope:{ctx:b}}}),Lt=new Ne({}),Rt=new U({props:{code:`from transformers import DistilBertConfig

my_config = DistilBertConfig(n_heads=12)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = DistilBertConfig(n_heads=<span class="hljs-number">12</span>)`}}),Bt=new U({props:{code:`from transformers import DistilBertForSequenceClassification

my_model = DistilBertForSequenceClassification(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = DistilBertForSequenceClassification(my_config)`}}),Yt=new Ne({}),lt=new ns({props:{$$slots:{default:[zf]},$$scope:{ctx:b}}}),Jt=new U({props:{code:`from datasets import load_dataset

dataset = load_dataset("yelp_review_full")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)`}}),Kt=new U({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)`}}),Zt=new U({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`}}),es=new U({props:{code:`from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)`}}),ts=new U({props:{code:"trainer.train()",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()'}}),ss=new Ne({}),pt=new ko({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Cf],pytorch:[Ff]},$$scope:{ctx:b}}}),ft=new ko({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Of],pytorch:[Sf]},$$scope:{ctx:b}}}),{c(){a=l("meta"),m=v(),r=l("h1"),h=l("a"),d=l("span"),F(_.$$.fragment),k=v(),w=l("span"),c=o("Quick tour"),g=v(),F(T.$$.fragment),A=v(),N=l("p"),D=o("The \u{1F917} Transformers library is built around four major concepts to maximize user-friendliness and customization for research experiments:"),q=v(),M=l("ul"),j=l("li"),O=o("Start using the "),R=l("a"),L=o("pipeline()"),J=o(" for rapid inference."),se=v(),Y=l("li"),ie=o("Load a pretrained model from the "),B=l("a"),Z=o("Hub"),ae=o("."),V=v(),le=l("li"),X=o("Customize a base configuration, tokenizer, and model to modify how a model is built."),de=v(),ee=l("li"),re=o("Use the "),Q=l("a"),pe=o("Trainer"),S=o(" class - an optimized training loop for \u{1F917} Transformers models - to train a model."),H=v(),G=l("p"),I=o("Whether you\u2019re a new user or an experienced developer, this quick tour will help you get started and show you how to use our library. If you\u2019re a beginner, we recommend starting with our tutorials or "),W=l("a"),fe=o("course"),qe=o(" for a more in-depth introduction."),oe=v(),ue=l("p"),_e=o("Let\u2019s take a look at each of these features, and get up and running!"),ht=v(),F(te.$$.fragment),dt=v(),ze=l("p"),To=o("Before you begin, make sure you have \u{1F917} Transformers installed:"),Sa=v(),F(_t.$$.fragment),Ia=v(),ls=l("p"),Ao=o("You\u2019ll also want to install \u{1F917} Datasets to quickly load a dataset to train on:"),Oa=v(),F($t.$$.fragment),Na=v(),Pe=l("h2"),De=l("a"),sa=l("span"),F(gt.$$.fragment),jo=v(),aa=l("span"),Eo=o("Pipeline"),Da=v(),ge=l("p"),qo=o("The "),is=l("a"),zo=o("pipeline()"),Po=o(" is the easiest way to use a pretrained model for inference. You can use the "),ps=l("a"),Fo=o("pipeline()"),xo=o(" out-of-the-box for many tasks such as text generation, image classification, automatic speech recognition, and many more."),Ha=v(),F(vt.$$.fragment),Wa=v(),F(He.$$.fragment),Ua=v(),We=l("p"),Co=o("To use a "),fs=l("a"),Mo=o("pipeline()"),So=o(", create an instance of it and specify a task you want to use it for:"),La=v(),F(yt.$$.fragment),Ra=v(),me=l("p"),Io=o("The "),us=l("a"),Oo=o("pipeline()"),No=o(" downloads and caches a default "),kt=l("a"),Do=o("pretrained model"),Ho=o(" and tokenizer for sentiment analysis. Now you can use the "),ra=l("code"),Wo=o("classifier"),Uo=o(" on your target text:"),Ba=v(),F(wt.$$.fragment),Ya=v(),Ue=l("p"),Lo=o("If you have more than one input, pass your inputs as a list to the "),ms=l("a"),Ro=o("pipeline()"),Bo=o(" to return a list of dictionaries:"),Ga=v(),F(bt.$$.fragment),Va=v(),Le=l("p"),Yo=o("The "),cs=l("a"),Go=o("pipeline()"),Vo=o(" can also iterate over an entire dataset for a task like automatic speech recognition:"),Qa=v(),F(Tt.$$.fragment),Ja=v(),ve=l("p"),Qo=o("Load an audio dataset (see the \u{1F917} Datasets "),At=l("a"),Jo=o("Quick Start"),Ko=o(" for more details) you\u2019d like to iterate over. For example, load the "),jt=l("a"),Zo=o("MInDS-14"),Xo=o(" dataset:"),Ka=v(),F(Et.$$.fragment),Za=v(),Re=l("p"),en=o(`You need to make sure the sampling rate of the dataset matches the sampling
rate `),qt=l("a"),oa=l("code"),tn=o("facebook/wav2vec2-base-960h"),sn=o(" was trained on:"),Xa=v(),F(zt.$$.fragment),er=v(),Be=l("p"),an=o("The audio files are automatically loaded and resampled when calling the "),na=l("code"),rn=o("audio"),on=o(` column.
Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:`),tr=v(),F(Pt.$$.fragment),sr=v(),Ye=l("p"),nn=o("For larger datasets where the inputs are big (like in speech or vision), you\u2019ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the "),hs=l("a"),ln=o("pipeline API reference"),pn=o(" for more information."),ar=v(),Fe=l("h3"),Ge=l("a"),la=l("span"),F(Ft.$$.fragment),fn=v(),ia=l("span"),un=o("Use another model and tokenizer in the pipeline"),rr=v(),ne=l("p"),mn=o("The "),ds=l("a"),cn=o("pipeline()"),hn=o(" can accommodate any model from the "),xt=l("a"),dn=o("Hub"),_n=o(", making it easy to adapt the "),_s=l("a"),$n=o("pipeline()"),gn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ct=l("a"),vn=o("BERT model"),yn=o(" fine-tuned for sentiment analysis you can use for French text:"),or=v(),F(Mt.$$.fragment),nr=v(),F(Ve.$$.fragment),lr=v(),ye=l("p"),kn=o("Specify the model and tokenizer in the "),$s=l("a"),wn=o("pipeline()"),bn=o(", and now you can apply the "),pa=l("code"),Tn=o("classifier"),An=o(" on French text:"),ir=v(),F(St.$$.fragment),pr=v(),ke=l("p"),jn=o("If you can\u2019t find a model for your use-case, you\u2019ll need to fine-tune a pretrained model on your data. Take a look at our "),gs=l("a"),En=o("fine-tuning tutorial"),qn=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider "),vs=l("a"),zn=o("sharing"),Pn=o(" the model with the community on the Hub to democratize machine learning for everyone! \u{1F917}"),fr=v(),xe=l("h2"),Qe=l("a"),fa=l("span"),F(It.$$.fragment),Fn=v(),ua=l("span"),xn=o("AutoClass"),ur=v(),F(Ot.$$.fragment),mr=v(),K=l("p"),Cn=o("Under the hood, the "),ys=l("a"),Mn=o("AutoModelForSequenceClassification"),Sn=o(" and "),ks=l("a"),In=o("AutoTokenizer"),On=o(" classes work together to power the "),ws=l("a"),Nn=o("pipeline()"),Dn=o(" you used above. An "),bs=l("a"),Hn=o("AutoClass"),Wn=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ma=l("code"),Un=o("AutoClass"),Ln=o(" for your task and it\u2019s associated preprocessing class."),cr=v(),we=l("p"),Rn=o("Let\u2019s return to the example from the previous section and see how you can use the "),ca=l("code"),Bn=o("AutoClass"),Yn=o(" to replicate the results of the "),Ts=l("a"),Gn=o("pipeline()"),Vn=o("."),hr=v(),Ce=l("h3"),Je=l("a"),ha=l("span"),F(Nt.$$.fragment),Qn=v(),da=l("span"),Jn=o("AutoTokenizer"),dr=v(),Ke=l("p"),Kn=o("A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the "),As=l("a"),Zn=o("tokenizer summary"),Xn=o("). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),_r=v(),Ze=l("p"),el=o("Load a tokenizer with "),js=l("a"),tl=o("AutoTokenizer"),sl=o(":"),$r=v(),F(Dt.$$.fragment),gr=v(),Es=l("p"),al=o("Pass your text to the tokenizer:"),vr=v(),F(Ht.$$.fragment),yr=v(),qs=l("p"),rl=o("The tokenizer returns a dictionary containing:"),kr=v(),Xe=l("ul"),zs=l("li"),Ps=l("a"),ol=o("input_ids"),nl=o(": numerical representions of your tokens."),ll=v(),Fs=l("li"),xs=l("a"),il=o("atttention_mask"),pl=o(": indicates which tokens should be attended to."),wr=v(),Cs=l("p"),fl=o("A tokenizer accepts a list of inputs, and it can also pad and truncate the text to return a batch with uniform length:"),br=v(),F(Wt.$$.fragment),Tr=v(),F(et.$$.fragment),Ar=v(),Me=l("h3"),tt=l("a"),_a=l("span"),F(Ut.$$.fragment),ul=v(),$a=l("span"),ml=o("AutoModel"),jr=v(),F(st.$$.fragment),Er=v(),F(at.$$.fragment),qr=v(),Se=l("h2"),rt=l("a"),ga=l("span"),F(Lt.$$.fragment),cl=v(),va=l("span"),hl=o("Custom model builds"),zr=v(),Ms=l("p"),dl=o("To change how a model is built, you can modify the model\u2019s configuration class. The configuration specifies a model\u2019s attributes such as the number of hidden layers or attention heads. When you initialize a model from a custom configuration class, you are starting from scratch. The model attributes are randomly initialized and you\u2019ll need to train the model first before you can use it to get any meaningful results."),Pr=v(),Ss=l("p"),_l=o("Start by importing a model\u2019s configuration class, and then you can change the number of attention heads for instance:"),Fr=v(),F(Rt.$$.fragment),xr=v(),Is=l("p"),$l=o("Create a model from your custom configuration:"),Cr=v(),F(Bt.$$.fragment),Mr=v(),ot=l("p"),gl=o("Take a look at the "),Os=l("a"),vl=o("Create a custom architecture"),yl=o(" guide for more information about building custom configurations."),Sr=v(),Ie=l("h2"),nt=l("a"),ya=l("span"),F(Yt.$$.fragment),kl=v(),ka=l("span"),wl=o("Trainer"),Ir=v(),ce=l("p"),bl=o("All models are a standard "),Gt=l("a"),wa=l("code"),Tl=o("torch.nn.Module"),Al=o(" or a "),Vt=l("a"),ba=l("code"),jl=o("tf.keras.Model"),El=o(" so you can use them in any standard training loop. However, to make things easier, \u{1F917} Transformers provides a "),Ns=l("a"),ql=o("Trainer"),zl=o(" class for PyTorch which adds functionality for distributed training, mixed precision, and more."),Or=v(),F(lt.$$.fragment),Nr=v(),be=l("p"),Pl=o("Before you can use the "),Ds=l("a"),Fl=o("Trainer"),xl=o(", load and prepare a dataset. This example uses the "),Qt=l("a"),Cl=o("Yelp Review"),Ml=o(" dataset:"),Dr=v(),F(Jt.$$.fragment),Hr=v(),Hs=l("p"),Sl=o("Tokenize the dataset:"),Wr=v(),F(Kt.$$.fragment),Ur=v(),Ws=l("p"),Il=o("Import your model and the expected number of labels:"),Lr=v(),F(Zt.$$.fragment),Rr=v(),Te=l("p"),Ol=o("Create a "),Us=l("a"),Nl=o("TrainingArguments"),Dl=o(" class which holds all the available hyperparameters, and flags for activating different training options. You can adjust the learning rate, whether you want to use mixed-precision training, and options for pushing a model to the Hub. Take a look at the "),Xt=l("a"),Ta=l("code"),Hl=o("TrainingArguments"),Wl=o(" API reference"),Ul=o(" for a full list of options."),Br=v(),Ls=l("p"),Ll=o("In this example, you\u2019ll use the default values and save the model checkpoints to an output directory:"),Yr=v(),F(es.$$.fragment),Gr=v(),Ae=l("p"),Rl=o("Now create a "),Rs=l("a"),Bl=o("Trainer"),Yl=o(" with the model, training arguments, and train and test datasets. Then call "),Bs=l("a"),Gl=o("train()"),Vl=o(" to fine-tune your model:"),Vr=v(),F(ts.$$.fragment),Qr=v(),Oe=l("h3"),it=l("a"),Aa=l("span"),F(ss.$$.fragment),Ql=v(),ja=l("span"),Jl=o("Save a model"),Jr=v(),F(pt.$$.fragment),Kr=v(),je=l("p"),Kl=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Ea=l("code"),Zl=o("from_pt"),Xl=o(" or "),qa=l("code"),ei=o("from_tf"),ti=o(" parameter can convert the model from one framework to the other:"),Zr=v(),F(ft.$$.fragment),this.h()},l(e){const f=sf('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),m=y(e),r=i(e,"H1",{class:!0});var as=p(r);h=i(as,"A",{id:!0,class:!0,href:!0});var za=p(h);d=i(za,"SPAN",{});var Pa=p(d);C(_.$$.fragment,Pa),Pa.forEach(s),za.forEach(s),k=y(as),w=i(as,"SPAN",{});var Fa=p(w);c=n(Fa,"Quick tour"),Fa.forEach(s),as.forEach(s),g=y(e),C(T.$$.fragment,e),A=y(e),N=i(e,"P",{});var xa=p(N);D=n(xa,"The \u{1F917} Transformers library is built around four major concepts to maximize user-friendliness and customization for research experiments:"),xa.forEach(s),q=y(e),M=i(e,"UL",{});var $e=p(M);j=i($e,"LI",{});var rs=p(j);O=n(rs,"Start using the "),R=i(rs,"A",{href:!0});var Ca=p(R);L=n(Ca,"pipeline()"),Ca.forEach(s),J=n(rs," for rapid inference."),rs.forEach(s),se=y($e),Y=i($e,"LI",{});var os=p(Y);ie=n(os,"Load a pretrained model from the "),B=i(os,"A",{href:!0,rel:!0});var ii=p(B);Z=n(ii,"Hub"),ii.forEach(s),ae=n(os,"."),os.forEach(s),V=y($e),le=i($e,"LI",{});var pi=p(le);X=n(pi,"Customize a base configuration, tokenizer, and model to modify how a model is built."),pi.forEach(s),de=y($e),ee=i($e,"LI",{});var eo=p(ee);re=n(eo,"Use the "),Q=i(eo,"A",{href:!0});var fi=p(Q);pe=n(fi,"Trainer"),fi.forEach(s),S=n(eo," class - an optimized training loop for \u{1F917} Transformers models - to train a model."),eo.forEach(s),$e.forEach(s),H=y(e),G=i(e,"P",{});var to=p(G);I=n(to,"Whether you\u2019re a new user or an experienced developer, this quick tour will help you get started and show you how to use our library. If you\u2019re a beginner, we recommend starting with our tutorials or "),W=i(to,"A",{href:!0,rel:!0});var ui=p(W);fe=n(ui,"course"),ui.forEach(s),qe=n(to," for a more in-depth introduction."),to.forEach(s),oe=y(e),ue=i(e,"P",{});var mi=p(ue);_e=n(mi,"Let\u2019s take a look at each of these features, and get up and running!"),mi.forEach(s),ht=y(e),C(te.$$.fragment,e),dt=y(e),ze=i(e,"P",{});var ci=p(ze);To=n(ci,"Before you begin, make sure you have \u{1F917} Transformers installed:"),ci.forEach(s),Sa=y(e),C(_t.$$.fragment,e),Ia=y(e),ls=i(e,"P",{});var hi=p(ls);Ao=n(hi,"You\u2019ll also want to install \u{1F917} Datasets to quickly load a dataset to train on:"),hi.forEach(s),Oa=y(e),C($t.$$.fragment,e),Na=y(e),Pe=i(e,"H2",{class:!0});var so=p(Pe);De=i(so,"A",{id:!0,class:!0,href:!0});var di=p(De);sa=i(di,"SPAN",{});var _i=p(sa);C(gt.$$.fragment,_i),_i.forEach(s),di.forEach(s),jo=y(so),aa=i(so,"SPAN",{});var $i=p(aa);Eo=n($i,"Pipeline"),$i.forEach(s),so.forEach(s),Da=y(e),ge=i(e,"P",{});var Ys=p(ge);qo=n(Ys,"The "),is=i(Ys,"A",{href:!0});var gi=p(is);zo=n(gi,"pipeline()"),gi.forEach(s),Po=n(Ys," is the easiest way to use a pretrained model for inference. You can use the "),ps=i(Ys,"A",{href:!0});var vi=p(ps);Fo=n(vi,"pipeline()"),vi.forEach(s),xo=n(Ys," out-of-the-box for many tasks such as text generation, image classification, automatic speech recognition, and many more."),Ys.forEach(s),Ha=y(e),C(vt.$$.fragment,e),Wa=y(e),C(He.$$.fragment,e),Ua=y(e),We=i(e,"P",{});var ao=p(We);Co=n(ao,"To use a "),fs=i(ao,"A",{href:!0});var yi=p(fs);Mo=n(yi,"pipeline()"),yi.forEach(s),So=n(ao,", create an instance of it and specify a task you want to use it for:"),ao.forEach(s),La=y(e),C(yt.$$.fragment,e),Ra=y(e),me=i(e,"P",{});var ut=p(me);Io=n(ut,"The "),us=i(ut,"A",{href:!0});var ki=p(us);Oo=n(ki,"pipeline()"),ki.forEach(s),No=n(ut," downloads and caches a default "),kt=i(ut,"A",{href:!0,rel:!0});var wi=p(kt);Do=n(wi,"pretrained model"),wi.forEach(s),Ho=n(ut," and tokenizer for sentiment analysis. Now you can use the "),ra=i(ut,"CODE",{});var bi=p(ra);Wo=n(bi,"classifier"),bi.forEach(s),Uo=n(ut," on your target text:"),ut.forEach(s),Ba=y(e),C(wt.$$.fragment,e),Ya=y(e),Ue=i(e,"P",{});var ro=p(Ue);Lo=n(ro,"If you have more than one input, pass your inputs as a list to the "),ms=i(ro,"A",{href:!0});var Ti=p(ms);Ro=n(Ti,"pipeline()"),Ti.forEach(s),Bo=n(ro," to return a list of dictionaries:"),ro.forEach(s),Ga=y(e),C(bt.$$.fragment,e),Va=y(e),Le=i(e,"P",{});var oo=p(Le);Yo=n(oo,"The "),cs=i(oo,"A",{href:!0});var Ai=p(cs);Go=n(Ai,"pipeline()"),Ai.forEach(s),Vo=n(oo," can also iterate over an entire dataset for a task like automatic speech recognition:"),oo.forEach(s),Qa=y(e),C(Tt.$$.fragment,e),Ja=y(e),ve=i(e,"P",{});var Gs=p(ve);Qo=n(Gs,"Load an audio dataset (see the \u{1F917} Datasets "),At=i(Gs,"A",{href:!0,rel:!0});var ji=p(At);Jo=n(ji,"Quick Start"),ji.forEach(s),Ko=n(Gs," for more details) you\u2019d like to iterate over. For example, load the "),jt=i(Gs,"A",{href:!0,rel:!0});var Ei=p(jt);Zo=n(Ei,"MInDS-14"),Ei.forEach(s),Xo=n(Gs," dataset:"),Gs.forEach(s),Ka=y(e),C(Et.$$.fragment,e),Za=y(e),Re=i(e,"P",{});var no=p(Re);en=n(no,`You need to make sure the sampling rate of the dataset matches the sampling
rate `),qt=i(no,"A",{href:!0,rel:!0});var qi=p(qt);oa=i(qi,"CODE",{});var zi=p(oa);tn=n(zi,"facebook/wav2vec2-base-960h"),zi.forEach(s),qi.forEach(s),sn=n(no," was trained on:"),no.forEach(s),Xa=y(e),C(zt.$$.fragment,e),er=y(e),Be=i(e,"P",{});var lo=p(Be);an=n(lo,"The audio files are automatically loaded and resampled when calling the "),na=i(lo,"CODE",{});var Pi=p(na);rn=n(Pi,"audio"),Pi.forEach(s),on=n(lo,` column.
Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:`),lo.forEach(s),tr=y(e),C(Pt.$$.fragment,e),sr=y(e),Ye=i(e,"P",{});var io=p(Ye);nn=n(io,"For larger datasets where the inputs are big (like in speech or vision), you\u2019ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the "),hs=i(io,"A",{href:!0});var Fi=p(hs);ln=n(Fi,"pipeline API reference"),Fi.forEach(s),pn=n(io," for more information."),io.forEach(s),ar=y(e),Fe=i(e,"H3",{class:!0});var po=p(Fe);Ge=i(po,"A",{id:!0,class:!0,href:!0});var xi=p(Ge);la=i(xi,"SPAN",{});var Ci=p(la);C(Ft.$$.fragment,Ci),Ci.forEach(s),xi.forEach(s),fn=y(po),ia=i(po,"SPAN",{});var Mi=p(ia);un=n(Mi,"Use another model and tokenizer in the pipeline"),Mi.forEach(s),po.forEach(s),rr=y(e),ne=i(e,"P",{});var Ee=p(ne);mn=n(Ee,"The "),ds=i(Ee,"A",{href:!0});var Si=p(ds);cn=n(Si,"pipeline()"),Si.forEach(s),hn=n(Ee," can accommodate any model from the "),xt=i(Ee,"A",{href:!0,rel:!0});var Ii=p(xt);dn=n(Ii,"Hub"),Ii.forEach(s),_n=n(Ee,", making it easy to adapt the "),_s=i(Ee,"A",{href:!0});var Oi=p(_s);$n=n(Oi,"pipeline()"),Oi.forEach(s),gn=n(Ee," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ct=i(Ee,"A",{href:!0,rel:!0});var Ni=p(Ct);vn=n(Ni,"BERT model"),Ni.forEach(s),yn=n(Ee," fine-tuned for sentiment analysis you can use for French text:"),Ee.forEach(s),or=y(e),C(Mt.$$.fragment,e),nr=y(e),C(Ve.$$.fragment,e),lr=y(e),ye=i(e,"P",{});var Vs=p(ye);kn=n(Vs,"Specify the model and tokenizer in the "),$s=i(Vs,"A",{href:!0});var Di=p($s);wn=n(Di,"pipeline()"),Di.forEach(s),bn=n(Vs,", and now you can apply the "),pa=i(Vs,"CODE",{});var Hi=p(pa);Tn=n(Hi,"classifier"),Hi.forEach(s),An=n(Vs," on French text:"),Vs.forEach(s),ir=y(e),C(St.$$.fragment,e),pr=y(e),ke=i(e,"P",{});var Qs=p(ke);jn=n(Qs,"If you can\u2019t find a model for your use-case, you\u2019ll need to fine-tune a pretrained model on your data. Take a look at our "),gs=i(Qs,"A",{href:!0});var Wi=p(gs);En=n(Wi,"fine-tuning tutorial"),Wi.forEach(s),qn=n(Qs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider "),vs=i(Qs,"A",{href:!0});var Ui=p(vs);zn=n(Ui,"sharing"),Ui.forEach(s),Pn=n(Qs," the model with the community on the Hub to democratize machine learning for everyone! \u{1F917}"),Qs.forEach(s),fr=y(e),xe=i(e,"H2",{class:!0});var fo=p(xe);Qe=i(fo,"A",{id:!0,class:!0,href:!0});var Li=p(Qe);fa=i(Li,"SPAN",{});var Ri=p(fa);C(It.$$.fragment,Ri),Ri.forEach(s),Li.forEach(s),Fn=y(fo),ua=i(fo,"SPAN",{});var Bi=p(ua);xn=n(Bi,"AutoClass"),Bi.forEach(s),fo.forEach(s),ur=y(e),C(Ot.$$.fragment,e),mr=y(e),K=i(e,"P",{});var he=p(K);Cn=n(he,"Under the hood, the "),ys=i(he,"A",{href:!0});var Yi=p(ys);Mn=n(Yi,"AutoModelForSequenceClassification"),Yi.forEach(s),Sn=n(he," and "),ks=i(he,"A",{href:!0});var Gi=p(ks);In=n(Gi,"AutoTokenizer"),Gi.forEach(s),On=n(he," classes work together to power the "),ws=i(he,"A",{href:!0});var Vi=p(ws);Nn=n(Vi,"pipeline()"),Vi.forEach(s),Dn=n(he," you used above. An "),bs=i(he,"A",{href:!0});var Qi=p(bs);Hn=n(Qi,"AutoClass"),Qi.forEach(s),Wn=n(he," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ma=i(he,"CODE",{});var Ji=p(ma);Un=n(Ji,"AutoClass"),Ji.forEach(s),Ln=n(he," for your task and it\u2019s associated preprocessing class."),he.forEach(s),cr=y(e),we=i(e,"P",{});var Js=p(we);Rn=n(Js,"Let\u2019s return to the example from the previous section and see how you can use the "),ca=i(Js,"CODE",{});var Ki=p(ca);Bn=n(Ki,"AutoClass"),Ki.forEach(s),Yn=n(Js," to replicate the results of the "),Ts=i(Js,"A",{href:!0});var Zi=p(Ts);Gn=n(Zi,"pipeline()"),Zi.forEach(s),Vn=n(Js,"."),Js.forEach(s),hr=y(e),Ce=i(e,"H3",{class:!0});var uo=p(Ce);Je=i(uo,"A",{id:!0,class:!0,href:!0});var Xi=p(Je);ha=i(Xi,"SPAN",{});var ep=p(ha);C(Nt.$$.fragment,ep),ep.forEach(s),Xi.forEach(s),Qn=y(uo),da=i(uo,"SPAN",{});var tp=p(da);Jn=n(tp,"AutoTokenizer"),tp.forEach(s),uo.forEach(s),dr=y(e),Ke=i(e,"P",{});var mo=p(Ke);Kn=n(mo,"A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the "),As=i(mo,"A",{href:!0});var sp=p(As);Zn=n(sp,"tokenizer summary"),sp.forEach(s),Xn=n(mo,"). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),mo.forEach(s),_r=y(e),Ze=i(e,"P",{});var co=p(Ze);el=n(co,"Load a tokenizer with "),js=i(co,"A",{href:!0});var ap=p(js);tl=n(ap,"AutoTokenizer"),ap.forEach(s),sl=n(co,":"),co.forEach(s),$r=y(e),C(Dt.$$.fragment,e),gr=y(e),Es=i(e,"P",{});var rp=p(Es);al=n(rp,"Pass your text to the tokenizer:"),rp.forEach(s),vr=y(e),C(Ht.$$.fragment,e),yr=y(e),qs=i(e,"P",{});var op=p(qs);rl=n(op,"The tokenizer returns a dictionary containing:"),op.forEach(s),kr=y(e),Xe=i(e,"UL",{});var ho=p(Xe);zs=i(ho,"LI",{});var si=p(zs);Ps=i(si,"A",{href:!0});var np=p(Ps);ol=n(np,"input_ids"),np.forEach(s),nl=n(si,": numerical representions of your tokens."),si.forEach(s),ll=y(ho),Fs=i(ho,"LI",{});var ai=p(Fs);xs=i(ai,"A",{href:!0});var lp=p(xs);il=n(lp,"atttention_mask"),lp.forEach(s),pl=n(ai,": indicates which tokens should be attended to."),ai.forEach(s),ho.forEach(s),wr=y(e),Cs=i(e,"P",{});var ip=p(Cs);fl=n(ip,"A tokenizer accepts a list of inputs, and it can also pad and truncate the text to return a batch with uniform length:"),ip.forEach(s),br=y(e),C(Wt.$$.fragment,e),Tr=y(e),C(et.$$.fragment,e),Ar=y(e),Me=i(e,"H3",{class:!0});var _o=p(Me);tt=i(_o,"A",{id:!0,class:!0,href:!0});var pp=p(tt);_a=i(pp,"SPAN",{});var fp=p(_a);C(Ut.$$.fragment,fp),fp.forEach(s),pp.forEach(s),ul=y(_o),$a=i(_o,"SPAN",{});var up=p($a);ml=n(up,"AutoModel"),up.forEach(s),_o.forEach(s),jr=y(e),C(st.$$.fragment,e),Er=y(e),C(at.$$.fragment,e),qr=y(e),Se=i(e,"H2",{class:!0});var $o=p(Se);rt=i($o,"A",{id:!0,class:!0,href:!0});var mp=p(rt);ga=i(mp,"SPAN",{});var cp=p(ga);C(Lt.$$.fragment,cp),cp.forEach(s),mp.forEach(s),cl=y($o),va=i($o,"SPAN",{});var hp=p(va);hl=n(hp,"Custom model builds"),hp.forEach(s),$o.forEach(s),zr=y(e),Ms=i(e,"P",{});var dp=p(Ms);dl=n(dp,"To change how a model is built, you can modify the model\u2019s configuration class. The configuration specifies a model\u2019s attributes such as the number of hidden layers or attention heads. When you initialize a model from a custom configuration class, you are starting from scratch. The model attributes are randomly initialized and you\u2019ll need to train the model first before you can use it to get any meaningful results."),dp.forEach(s),Pr=y(e),Ss=i(e,"P",{});var _p=p(Ss);_l=n(_p,"Start by importing a model\u2019s configuration class, and then you can change the number of attention heads for instance:"),_p.forEach(s),Fr=y(e),C(Rt.$$.fragment,e),xr=y(e),Is=i(e,"P",{});var $p=p(Is);$l=n($p,"Create a model from your custom configuration:"),$p.forEach(s),Cr=y(e),C(Bt.$$.fragment,e),Mr=y(e),ot=i(e,"P",{});var go=p(ot);gl=n(go,"Take a look at the "),Os=i(go,"A",{href:!0});var gp=p(Os);vl=n(gp,"Create a custom architecture"),gp.forEach(s),yl=n(go," guide for more information about building custom configurations."),go.forEach(s),Sr=y(e),Ie=i(e,"H2",{class:!0});var vo=p(Ie);nt=i(vo,"A",{id:!0,class:!0,href:!0});var vp=p(nt);ya=i(vp,"SPAN",{});var yp=p(ya);C(Yt.$$.fragment,yp),yp.forEach(s),vp.forEach(s),kl=y(vo),ka=i(vo,"SPAN",{});var kp=p(ka);wl=n(kp,"Trainer"),kp.forEach(s),vo.forEach(s),Ir=y(e),ce=i(e,"P",{});var mt=p(ce);bl=n(mt,"All models are a standard "),Gt=i(mt,"A",{href:!0,rel:!0});var wp=p(Gt);wa=i(wp,"CODE",{});var bp=p(wa);Tl=n(bp,"torch.nn.Module"),bp.forEach(s),wp.forEach(s),Al=n(mt," or a "),Vt=i(mt,"A",{href:!0,rel:!0});var Tp=p(Vt);ba=i(Tp,"CODE",{});var Ap=p(ba);jl=n(Ap,"tf.keras.Model"),Ap.forEach(s),Tp.forEach(s),El=n(mt," so you can use them in any standard training loop. However, to make things easier, \u{1F917} Transformers provides a "),Ns=i(mt,"A",{href:!0});var jp=p(Ns);ql=n(jp,"Trainer"),jp.forEach(s),zl=n(mt," class for PyTorch which adds functionality for distributed training, mixed precision, and more."),mt.forEach(s),Or=y(e),C(lt.$$.fragment,e),Nr=y(e),be=i(e,"P",{});var Ks=p(be);Pl=n(Ks,"Before you can use the "),Ds=i(Ks,"A",{href:!0});var Ep=p(Ds);Fl=n(Ep,"Trainer"),Ep.forEach(s),xl=n(Ks,", load and prepare a dataset. This example uses the "),Qt=i(Ks,"A",{href:!0,rel:!0});var qp=p(Qt);Cl=n(qp,"Yelp Review"),qp.forEach(s),Ml=n(Ks," dataset:"),Ks.forEach(s),Dr=y(e),C(Jt.$$.fragment,e),Hr=y(e),Hs=i(e,"P",{});var zp=p(Hs);Sl=n(zp,"Tokenize the dataset:"),zp.forEach(s),Wr=y(e),C(Kt.$$.fragment,e),Ur=y(e),Ws=i(e,"P",{});var Pp=p(Ws);Il=n(Pp,"Import your model and the expected number of labels:"),Pp.forEach(s),Lr=y(e),C(Zt.$$.fragment,e),Rr=y(e),Te=i(e,"P",{});var Zs=p(Te);Ol=n(Zs,"Create a "),Us=i(Zs,"A",{href:!0});var Fp=p(Us);Nl=n(Fp,"TrainingArguments"),Fp.forEach(s),Dl=n(Zs," class which holds all the available hyperparameters, and flags for activating different training options. You can adjust the learning rate, whether you want to use mixed-precision training, and options for pushing a model to the Hub. Take a look at the "),Xt=i(Zs,"A",{href:!0});var ri=p(Xt);Ta=i(ri,"CODE",{});var xp=p(Ta);Hl=n(xp,"TrainingArguments"),xp.forEach(s),Wl=n(ri," API reference"),ri.forEach(s),Ul=n(Zs," for a full list of options."),Zs.forEach(s),Br=y(e),Ls=i(e,"P",{});var Cp=p(Ls);Ll=n(Cp,"In this example, you\u2019ll use the default values and save the model checkpoints to an output directory:"),Cp.forEach(s),Yr=y(e),C(es.$$.fragment,e),Gr=y(e),Ae=i(e,"P",{});var Xs=p(Ae);Rl=n(Xs,"Now create a "),Rs=i(Xs,"A",{href:!0});var Mp=p(Rs);Bl=n(Mp,"Trainer"),Mp.forEach(s),Yl=n(Xs," with the model, training arguments, and train and test datasets. Then call "),Bs=i(Xs,"A",{href:!0});var Sp=p(Bs);Gl=n(Sp,"train()"),Sp.forEach(s),Vl=n(Xs," to fine-tune your model:"),Xs.forEach(s),Vr=y(e),C(ts.$$.fragment,e),Qr=y(e),Oe=i(e,"H3",{class:!0});var yo=p(Oe);it=i(yo,"A",{id:!0,class:!0,href:!0});var Ip=p(it);Aa=i(Ip,"SPAN",{});var Op=p(Aa);C(ss.$$.fragment,Op),Op.forEach(s),Ip.forEach(s),Ql=y(yo),ja=i(yo,"SPAN",{});var Np=p(ja);Jl=n(Np,"Save a model"),Np.forEach(s),yo.forEach(s),Jr=y(e),C(pt.$$.fragment,e),Kr=y(e),je=i(e,"P",{});var ea=p(je);Kl=n(ea,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Ea=i(ea,"CODE",{});var Dp=p(Ea);Zl=n(Dp,"from_pt"),Dp.forEach(s),Xl=n(ea," or "),qa=i(ea,"CODE",{});var Hp=p(qa);ei=n(Hp,"from_tf"),Hp.forEach(s),ti=n(ea," parameter can convert the model from one framework to the other:"),ea.forEach(s),Zr=y(e),C(ft.$$.fragment,e),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(Df)),$(h,"id","quick-tour"),$(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(h,"href","#quick-tour"),$(r,"class","relative group"),$(R,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(B,"href","https://huggingface.co/models"),$(B,"rel","nofollow"),$(Q,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),$(W,"href","https://huggingface.co/course/chapter1/1"),$(W,"rel","nofollow"),$(De,"id","pipeline"),$(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(De,"href","#pipeline"),$(Pe,"class","relative group"),$(is,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(ps,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(fs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(us,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(kt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(kt,"rel","nofollow"),$(ms,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(cs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(At,"href","https://huggingface.co/docs/datasets/quickstart#audio"),$(At,"rel","nofollow"),$(jt,"href","https://huggingface.co/datasets/PolyAI/minds14"),$(jt,"rel","nofollow"),$(qt,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),$(qt,"rel","nofollow"),$(hs,"href","./main_classes/pipelines"),$(Ge,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ge,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Fe,"class","relative group"),$(ds,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(xt,"href","https://huggingface.co/models"),$(xt,"rel","nofollow"),$(_s,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Ct,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(Ct,"rel","nofollow"),$($s,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(gs,"href","./training"),$(vs,"href","./model_sharing"),$(Qe,"id","autoclass"),$(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Qe,"href","#autoclass"),$(xe,"class","relative group"),$(ys,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(ks,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(ws,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(bs,"href","./model_doc/auto"),$(Ts,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),$(Je,"id","autotokenizer"),$(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Je,"href","#autotokenizer"),$(Ce,"class","relative group"),$(As,"href","./tokenizer_summary"),$(js,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),$(Ps,"href","./glossary#input-ids"),$(xs,"href",".glossary#attention-mask"),$(tt,"id","automodel"),$(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(tt,"href","#automodel"),$(Me,"class","relative group"),$(rt,"id","custom-model-builds"),$(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(rt,"href","#custom-model-builds"),$(Se,"class","relative group"),$(Os,"href","./create_a_model"),$(nt,"id","trainer"),$(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(nt,"href","#trainer"),$(Ie,"class","relative group"),$(Gt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Gt,"rel","nofollow"),$(Vt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Vt,"rel","nofollow"),$(Ns,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),$(Ds,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),$(Qt,"href","https://huggingface.co/datasets/yelp_review_full"),$(Qt,"rel","nofollow"),$(Us,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.TrainingArguments"),$(Xt,"href","./main_classes/trainer#transformers.TrainingArguments"),$(Rs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),$(Bs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer.train"),$(it,"id","save-a-model"),$(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(it,"href","#save-a-model"),$(Oe,"class","relative group")},m(e,f){t(document.head,a),u(e,m,f),u(e,r,f),t(r,h),t(h,d),x(_,d,null),t(r,k),t(r,w),t(w,c),u(e,g,f),x(T,e,f),u(e,A,f),u(e,N,f),t(N,D),u(e,q,f),u(e,M,f),t(M,j),t(j,O),t(j,R),t(R,L),t(j,J),t(M,se),t(M,Y),t(Y,ie),t(Y,B),t(B,Z),t(Y,ae),t(M,V),t(M,le),t(le,X),t(M,de),t(M,ee),t(ee,re),t(ee,Q),t(Q,pe),t(ee,S),u(e,H,f),u(e,G,f),t(G,I),t(G,W),t(W,fe),t(G,qe),u(e,oe,f),u(e,ue,f),t(ue,_e),u(e,ht,f),x(te,e,f),u(e,dt,f),u(e,ze,f),t(ze,To),u(e,Sa,f),x(_t,e,f),u(e,Ia,f),u(e,ls,f),t(ls,Ao),u(e,Oa,f),x($t,e,f),u(e,Na,f),u(e,Pe,f),t(Pe,De),t(De,sa),x(gt,sa,null),t(Pe,jo),t(Pe,aa),t(aa,Eo),u(e,Da,f),u(e,ge,f),t(ge,qo),t(ge,is),t(is,zo),t(ge,Po),t(ge,ps),t(ps,Fo),t(ge,xo),u(e,Ha,f),x(vt,e,f),u(e,Wa,f),x(He,e,f),u(e,Ua,f),u(e,We,f),t(We,Co),t(We,fs),t(fs,Mo),t(We,So),u(e,La,f),x(yt,e,f),u(e,Ra,f),u(e,me,f),t(me,Io),t(me,us),t(us,Oo),t(me,No),t(me,kt),t(kt,Do),t(me,Ho),t(me,ra),t(ra,Wo),t(me,Uo),u(e,Ba,f),x(wt,e,f),u(e,Ya,f),u(e,Ue,f),t(Ue,Lo),t(Ue,ms),t(ms,Ro),t(Ue,Bo),u(e,Ga,f),x(bt,e,f),u(e,Va,f),u(e,Le,f),t(Le,Yo),t(Le,cs),t(cs,Go),t(Le,Vo),u(e,Qa,f),x(Tt,e,f),u(e,Ja,f),u(e,ve,f),t(ve,Qo),t(ve,At),t(At,Jo),t(ve,Ko),t(ve,jt),t(jt,Zo),t(ve,Xo),u(e,Ka,f),x(Et,e,f),u(e,Za,f),u(e,Re,f),t(Re,en),t(Re,qt),t(qt,oa),t(oa,tn),t(Re,sn),u(e,Xa,f),x(zt,e,f),u(e,er,f),u(e,Be,f),t(Be,an),t(Be,na),t(na,rn),t(Be,on),u(e,tr,f),x(Pt,e,f),u(e,sr,f),u(e,Ye,f),t(Ye,nn),t(Ye,hs),t(hs,ln),t(Ye,pn),u(e,ar,f),u(e,Fe,f),t(Fe,Ge),t(Ge,la),x(Ft,la,null),t(Fe,fn),t(Fe,ia),t(ia,un),u(e,rr,f),u(e,ne,f),t(ne,mn),t(ne,ds),t(ds,cn),t(ne,hn),t(ne,xt),t(xt,dn),t(ne,_n),t(ne,_s),t(_s,$n),t(ne,gn),t(ne,Ct),t(Ct,vn),t(ne,yn),u(e,or,f),x(Mt,e,f),u(e,nr,f),x(Ve,e,f),u(e,lr,f),u(e,ye,f),t(ye,kn),t(ye,$s),t($s,wn),t(ye,bn),t(ye,pa),t(pa,Tn),t(ye,An),u(e,ir,f),x(St,e,f),u(e,pr,f),u(e,ke,f),t(ke,jn),t(ke,gs),t(gs,En),t(ke,qn),t(ke,vs),t(vs,zn),t(ke,Pn),u(e,fr,f),u(e,xe,f),t(xe,Qe),t(Qe,fa),x(It,fa,null),t(xe,Fn),t(xe,ua),t(ua,xn),u(e,ur,f),x(Ot,e,f),u(e,mr,f),u(e,K,f),t(K,Cn),t(K,ys),t(ys,Mn),t(K,Sn),t(K,ks),t(ks,In),t(K,On),t(K,ws),t(ws,Nn),t(K,Dn),t(K,bs),t(bs,Hn),t(K,Wn),t(K,ma),t(ma,Un),t(K,Ln),u(e,cr,f),u(e,we,f),t(we,Rn),t(we,ca),t(ca,Bn),t(we,Yn),t(we,Ts),t(Ts,Gn),t(we,Vn),u(e,hr,f),u(e,Ce,f),t(Ce,Je),t(Je,ha),x(Nt,ha,null),t(Ce,Qn),t(Ce,da),t(da,Jn),u(e,dr,f),u(e,Ke,f),t(Ke,Kn),t(Ke,As),t(As,Zn),t(Ke,Xn),u(e,_r,f),u(e,Ze,f),t(Ze,el),t(Ze,js),t(js,tl),t(Ze,sl),u(e,$r,f),x(Dt,e,f),u(e,gr,f),u(e,Es,f),t(Es,al),u(e,vr,f),x(Ht,e,f),u(e,yr,f),u(e,qs,f),t(qs,rl),u(e,kr,f),u(e,Xe,f),t(Xe,zs),t(zs,Ps),t(Ps,ol),t(zs,nl),t(Xe,ll),t(Xe,Fs),t(Fs,xs),t(xs,il),t(Fs,pl),u(e,wr,f),u(e,Cs,f),t(Cs,fl),u(e,br,f),x(Wt,e,f),u(e,Tr,f),x(et,e,f),u(e,Ar,f),u(e,Me,f),t(Me,tt),t(tt,_a),x(Ut,_a,null),t(Me,ul),t(Me,$a),t($a,ml),u(e,jr,f),x(st,e,f),u(e,Er,f),x(at,e,f),u(e,qr,f),u(e,Se,f),t(Se,rt),t(rt,ga),x(Lt,ga,null),t(Se,cl),t(Se,va),t(va,hl),u(e,zr,f),u(e,Ms,f),t(Ms,dl),u(e,Pr,f),u(e,Ss,f),t(Ss,_l),u(e,Fr,f),x(Rt,e,f),u(e,xr,f),u(e,Is,f),t(Is,$l),u(e,Cr,f),x(Bt,e,f),u(e,Mr,f),u(e,ot,f),t(ot,gl),t(ot,Os),t(Os,vl),t(ot,yl),u(e,Sr,f),u(e,Ie,f),t(Ie,nt),t(nt,ya),x(Yt,ya,null),t(Ie,kl),t(Ie,ka),t(ka,wl),u(e,Ir,f),u(e,ce,f),t(ce,bl),t(ce,Gt),t(Gt,wa),t(wa,Tl),t(ce,Al),t(ce,Vt),t(Vt,ba),t(ba,jl),t(ce,El),t(ce,Ns),t(Ns,ql),t(ce,zl),u(e,Or,f),x(lt,e,f),u(e,Nr,f),u(e,be,f),t(be,Pl),t(be,Ds),t(Ds,Fl),t(be,xl),t(be,Qt),t(Qt,Cl),t(be,Ml),u(e,Dr,f),x(Jt,e,f),u(e,Hr,f),u(e,Hs,f),t(Hs,Sl),u(e,Wr,f),x(Kt,e,f),u(e,Ur,f),u(e,Ws,f),t(Ws,Il),u(e,Lr,f),x(Zt,e,f),u(e,Rr,f),u(e,Te,f),t(Te,Ol),t(Te,Us),t(Us,Nl),t(Te,Dl),t(Te,Xt),t(Xt,Ta),t(Ta,Hl),t(Xt,Wl),t(Te,Ul),u(e,Br,f),u(e,Ls,f),t(Ls,Ll),u(e,Yr,f),x(es,e,f),u(e,Gr,f),u(e,Ae,f),t(Ae,Rl),t(Ae,Rs),t(Rs,Bl),t(Ae,Yl),t(Ae,Bs),t(Bs,Gl),t(Ae,Vl),u(e,Vr,f),x(ts,e,f),u(e,Qr,f),u(e,Oe,f),t(Oe,it),t(it,Aa),x(ss,Aa,null),t(Oe,Ql),t(Oe,ja),t(ja,Jl),u(e,Jr,f),x(pt,e,f),u(e,Kr,f),u(e,je,f),t(je,Kl),t(je,Ea),t(Ea,Zl),t(je,Xl),t(je,qa),t(qa,ei),t(je,ti),u(e,Zr,f),x(ft,e,f),Xr=!0},p(e,[f]){const as={};f&2&&(as.$$scope={dirty:f,ctx:e}),te.$set(as);const za={};f&2&&(za.$$scope={dirty:f,ctx:e}),He.$set(za);const Pa={};f&2&&(Pa.$$scope={dirty:f,ctx:e}),Ve.$set(Pa);const Fa={};f&2&&(Fa.$$scope={dirty:f,ctx:e}),et.$set(Fa);const xa={};f&2&&(xa.$$scope={dirty:f,ctx:e}),st.$set(xa);const $e={};f&2&&($e.$$scope={dirty:f,ctx:e}),at.$set($e);const rs={};f&2&&(rs.$$scope={dirty:f,ctx:e}),lt.$set(rs);const Ca={};f&2&&(Ca.$$scope={dirty:f,ctx:e}),pt.$set(Ca);const os={};f&2&&(os.$$scope={dirty:f,ctx:e}),ft.$set(os)},i(e){Xr||(E(_.$$.fragment,e),E(T.$$.fragment,e),E(te.$$.fragment,e),E(_t.$$.fragment,e),E($t.$$.fragment,e),E(gt.$$.fragment,e),E(vt.$$.fragment,e),E(He.$$.fragment,e),E(yt.$$.fragment,e),E(wt.$$.fragment,e),E(bt.$$.fragment,e),E(Tt.$$.fragment,e),E(Et.$$.fragment,e),E(zt.$$.fragment,e),E(Pt.$$.fragment,e),E(Ft.$$.fragment,e),E(Mt.$$.fragment,e),E(Ve.$$.fragment,e),E(St.$$.fragment,e),E(It.$$.fragment,e),E(Ot.$$.fragment,e),E(Nt.$$.fragment,e),E(Dt.$$.fragment,e),E(Ht.$$.fragment,e),E(Wt.$$.fragment,e),E(et.$$.fragment,e),E(Ut.$$.fragment,e),E(st.$$.fragment,e),E(at.$$.fragment,e),E(Lt.$$.fragment,e),E(Rt.$$.fragment,e),E(Bt.$$.fragment,e),E(Yt.$$.fragment,e),E(lt.$$.fragment,e),E(Jt.$$.fragment,e),E(Kt.$$.fragment,e),E(Zt.$$.fragment,e),E(es.$$.fragment,e),E(ts.$$.fragment,e),E(ss.$$.fragment,e),E(pt.$$.fragment,e),E(ft.$$.fragment,e),Xr=!0)},o(e){z(_.$$.fragment,e),z(T.$$.fragment,e),z(te.$$.fragment,e),z(_t.$$.fragment,e),z($t.$$.fragment,e),z(gt.$$.fragment,e),z(vt.$$.fragment,e),z(He.$$.fragment,e),z(yt.$$.fragment,e),z(wt.$$.fragment,e),z(bt.$$.fragment,e),z(Tt.$$.fragment,e),z(Et.$$.fragment,e),z(zt.$$.fragment,e),z(Pt.$$.fragment,e),z(Ft.$$.fragment,e),z(Mt.$$.fragment,e),z(Ve.$$.fragment,e),z(St.$$.fragment,e),z(It.$$.fragment,e),z(Ot.$$.fragment,e),z(Nt.$$.fragment,e),z(Dt.$$.fragment,e),z(Ht.$$.fragment,e),z(Wt.$$.fragment,e),z(et.$$.fragment,e),z(Ut.$$.fragment,e),z(st.$$.fragment,e),z(at.$$.fragment,e),z(Lt.$$.fragment,e),z(Rt.$$.fragment,e),z(Bt.$$.fragment,e),z(Yt.$$.fragment,e),z(lt.$$.fragment,e),z(Jt.$$.fragment,e),z(Kt.$$.fragment,e),z(Zt.$$.fragment,e),z(es.$$.fragment,e),z(ts.$$.fragment,e),z(ss.$$.fragment,e),z(pt.$$.fragment,e),z(ft.$$.fragment,e),Xr=!1},d(e){s(a),e&&s(m),e&&s(r),P(_),e&&s(g),P(T,e),e&&s(A),e&&s(N),e&&s(q),e&&s(M),e&&s(H),e&&s(G),e&&s(oe),e&&s(ue),e&&s(ht),P(te,e),e&&s(dt),e&&s(ze),e&&s(Sa),P(_t,e),e&&s(Ia),e&&s(ls),e&&s(Oa),P($t,e),e&&s(Na),e&&s(Pe),P(gt),e&&s(Da),e&&s(ge),e&&s(Ha),P(vt,e),e&&s(Wa),P(He,e),e&&s(Ua),e&&s(We),e&&s(La),P(yt,e),e&&s(Ra),e&&s(me),e&&s(Ba),P(wt,e),e&&s(Ya),e&&s(Ue),e&&s(Ga),P(bt,e),e&&s(Va),e&&s(Le),e&&s(Qa),P(Tt,e),e&&s(Ja),e&&s(ve),e&&s(Ka),P(Et,e),e&&s(Za),e&&s(Re),e&&s(Xa),P(zt,e),e&&s(er),e&&s(Be),e&&s(tr),P(Pt,e),e&&s(sr),e&&s(Ye),e&&s(ar),e&&s(Fe),P(Ft),e&&s(rr),e&&s(ne),e&&s(or),P(Mt,e),e&&s(nr),P(Ve,e),e&&s(lr),e&&s(ye),e&&s(ir),P(St,e),e&&s(pr),e&&s(ke),e&&s(fr),e&&s(xe),P(It),e&&s(ur),P(Ot,e),e&&s(mr),e&&s(K),e&&s(cr),e&&s(we),e&&s(hr),e&&s(Ce),P(Nt),e&&s(dr),e&&s(Ke),e&&s(_r),e&&s(Ze),e&&s($r),P(Dt,e),e&&s(gr),e&&s(Es),e&&s(vr),P(Ht,e),e&&s(yr),e&&s(qs),e&&s(kr),e&&s(Xe),e&&s(wr),e&&s(Cs),e&&s(br),P(Wt,e),e&&s(Tr),P(et,e),e&&s(Ar),e&&s(Me),P(Ut),e&&s(jr),P(st,e),e&&s(Er),P(at,e),e&&s(qr),e&&s(Se),P(Lt),e&&s(zr),e&&s(Ms),e&&s(Pr),e&&s(Ss),e&&s(Fr),P(Rt,e),e&&s(xr),e&&s(Is),e&&s(Cr),P(Bt,e),e&&s(Mr),e&&s(ot),e&&s(Sr),e&&s(Ie),P(Yt),e&&s(Ir),e&&s(ce),e&&s(Or),P(lt,e),e&&s(Nr),e&&s(be),e&&s(Dr),P(Jt,e),e&&s(Hr),e&&s(Hs),e&&s(Wr),P(Kt,e),e&&s(Ur),e&&s(Ws),e&&s(Lr),P(Zt,e),e&&s(Rr),e&&s(Te),e&&s(Br),e&&s(Ls),e&&s(Yr),P(es,e),e&&s(Gr),e&&s(Ae),e&&s(Vr),P(ts,e),e&&s(Qr),e&&s(Oe),P(ss),e&&s(Jr),P(pt,e),e&&s(Kr),e&&s(je),e&&s(Zr),P(ft,e)}}}const Df={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"}],title:"AutoClass"},{local:"custom-model-builds",title:"Custom model builds"},{local:"trainer",sections:[{local:"save-a-model",title:"Save a model"}],title:"Trainer"}],title:"Quick tour"};function Hf(b){return af(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vf extends oi{constructor(a){super();ni(this,a,Hf,Nf,li,{})}}export{Vf as default,Df as metadata};
