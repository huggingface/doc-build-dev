import{S as Qi,i as Ji,s as Ki,e as n,k as h,t as a,c as l,a as i,m as d,h as r,d as t,b as c,g as f,G as s,Q as Ya,q as A,l as Bf,n as Yo,o as E,B as z,p as Go,w as x,y as F,j as su,K as au,U as Qf,x as S,V as ru,T as ou,Y as Jf,Z as Kf,M as nu,v as lu}from"../chunks/vendor-hf-doc-builder.js";import{T as Ga}from"../chunks/Tip-hf-doc-builder.js";import{Y as Rf}from"../chunks/Youtube-hf-doc-builder.js";import{I as Pe}from"../chunks/IconCopyLink-hf-doc-builder.js";import{a as Zf,C as D}from"../chunks/CodeBlock-hf-doc-builder.js";import{g as Xf,I as iu,a as pu,F as fu,M as eu}from"../chunks/Markdown-hf-doc-builder.js";import{D as uu}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";function Yf(T,o,_){const u=T.slice();return u[8]=o[_],u[10]=_,u}function Gf(T){let o,_,u;var v=T[8].icon;function y($){return{props:{classNames:"mr-1.5"}}}return v&&(o=new v(y())),{c(){o&&x(o.$$.fragment),_=Bf()},l($){o&&S(o.$$.fragment,$),_=Bf()},m($,b){o&&F(o,$,b),f($,_,b),u=!0},p($,b){if(v!==(v=$[8].icon)){if(o){Yo();const k=o;E(k.$$.fragment,1,0,()=>{z(k,1)}),Go()}v?(o=new v(y()),x(o.$$.fragment),A(o.$$.fragment,1),F(o,_.parentNode,_)):o=null}},i($){u||(o&&A(o.$$.fragment,$),u=!0)},o($){o&&E(o.$$.fragment,$),u=!1},d($){$&&t(_),o&&z(o,$)}}}function Vf(T){let o,_,u,v=T[8].name+"",y,$,b,k,m,g,w,j=T[8].icon&&Gf(T);function O(){return T[6](T[8])}return{c(){o=n("button"),j&&j.c(),_=h(),u=n("p"),y=a(v),b=h(),this.h()},l(H){o=l(H,"BUTTON",{class:!0});var N=i(o);j&&j.l(N),_=d(N),u=l(N,"P",{class:!0});var I=i(u);y=r(I,v),I.forEach(t),b=d(N),N.forEach(t),this.h()},h(){c(u,"class",$="!m-0 "+T[8].classNames),c(o,"class",k="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(T[10]?"r":"l")+" "+(T[8].group!==T[1]&&"text-gray-500 filter grayscale"))},m(H,N){f(H,o,N),j&&j.m(o,null),s(o,_),s(o,u),s(u,y),s(o,b),m=!0,g||(w=Ya(o,"click",O),g=!0)},p(H,N){T=H,T[8].icon?j?(j.p(T,N),N&1&&A(j,1)):(j=Gf(T),j.c(),A(j,1),j.m(o,_)):j&&(Yo(),E(j,1,1,()=>{j=null}),Go()),(!m||N&1)&&v!==(v=T[8].name+"")&&su(y,v),(!m||N&1&&$!==($="!m-0 "+T[8].classNames))&&c(u,"class",$),(!m||N&3&&k!==(k="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(T[10]?"r":"l")+" "+(T[8].group!==T[1]&&"text-gray-500 filter grayscale")))&&c(o,"class",k)},i(H){m||(A(j),m=!0)},o(H){E(j),m=!1},d(H){H&&t(o),j&&j.d(),g=!1,w()}}}function mu(T){let o,_,u,v=T[3].filter(T[5]),y=[];for(let b=0;b<v.length;b+=1)y[b]=Vf(Yf(T,v,b));const $=b=>E(y[b],1,1,()=>{y[b]=null});return{c(){o=n("div"),_=n("div");for(let b=0;b<y.length;b+=1)y[b].c();this.h()},l(b){o=l(b,"DIV",{});var k=i(o);_=l(k,"DIV",{class:!0});var m=i(_);for(let g=0;g<y.length;g+=1)y[g].l(m);m.forEach(t),k.forEach(t),this.h()},h(){c(_,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(b,k){f(b,o,k),s(o,_);for(let m=0;m<y.length;m+=1)y[m].m(_,null);u=!0},p(b,[k]){if(k&27){v=b[3].filter(b[5]);let m;for(m=0;m<v.length;m+=1){const g=Yf(b,v,m);y[m]?(y[m].p(g,k),A(y[m],1)):(y[m]=Vf(g),y[m].c(),A(y[m],1),y[m].m(_,null))}for(Yo(),m=v.length;m<y.length;m+=1)$(m);Go()}},i(b){if(!u){for(let k=0;k<v.length;k+=1)A(y[k]);u=!0}},o(b){y=y.filter(Boolean);for(let k=0;k<y.length;k+=1)E(y[k]);u=!1},d(b){b&&t(o),au(y,b)}}}function cu(T,o,_){let u,{ids:v}=o;const y=v.join("-"),$=Xf(y);Qf(T,$,w=>_(1,u=w));const b=[{id:"pt",classNames:"",icon:iu,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:pu,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function k(w){ru($,u=w,u)}const m=w=>v.includes(w.id),g=w=>k(w.group);return T.$$set=w=>{"ids"in w&&_(0,v=w.ids)},[v,u,$,b,k,m,g]}class tu extends Qi{constructor(o){super();Ji(this,o,cu,mu,Ki,{ids:0})}}function hu(T){let o,_,u,v,y,$,b=T[1].highlighted+"",k;return _=new Zf({props:{classNames:"transition duration-200 ease-in-out "+(T[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:T[1].code}}),y=new tu({props:{ids:T[4]}}),{c(){o=n("div"),x(_.$$.fragment),u=h(),v=n("pre"),x(y.$$.fragment),$=new Jf,this.h()},l(m){o=l(m,"DIV",{class:!0});var g=i(o);S(_.$$.fragment,g),g.forEach(t),u=d(m),v=l(m,"PRE",{});var w=i(v);S(y.$$.fragment,w),$=Kf(w),w.forEach(t),this.h()},h(){c(o,"class","absolute top-2.5 right-4"),$.a=null},m(m,g){f(m,o,g),F(_,o,null),f(m,u,g),f(m,v,g),F(y,v,null),$.m(b,v),k=!0},p(m,g){const w={};g&4&&(w.classNames="transition duration-200 ease-in-out "+(m[2]&&"opacity-0")),g&2&&(w.value=m[1].code),_.$set(w),(!k||g&2)&&b!==(b=m[1].highlighted+"")&&$.p(b)},i(m){k||(A(_.$$.fragment,m),A(y.$$.fragment,m),k=!0)},o(m){E(_.$$.fragment,m),E(y.$$.fragment,m),k=!1},d(m){m&&t(o),z(_),m&&t(u),m&&t(v),z(y)}}}function du(T){let o,_,u,v,y,$,b=T[0].highlighted+"",k;return _=new Zf({props:{classNames:"transition duration-200 ease-in-out "+(T[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:T[0].code}}),y=new tu({props:{ids:T[4]}}),{c(){o=n("div"),x(_.$$.fragment),u=h(),v=n("pre"),x(y.$$.fragment),$=new Jf,this.h()},l(m){o=l(m,"DIV",{class:!0});var g=i(o);S(_.$$.fragment,g),g.forEach(t),u=d(m),v=l(m,"PRE",{});var w=i(v);S(y.$$.fragment,w),$=Kf(w),w.forEach(t),this.h()},h(){c(o,"class","absolute top-2.5 right-4"),$.a=null},m(m,g){f(m,o,g),F(_,o,null),f(m,u,g),f(m,v,g),F(y,v,null),$.m(b,v),k=!0},p(m,g){const w={};g&4&&(w.classNames="transition duration-200 ease-in-out "+(m[2]&&"opacity-0")),g&1&&(w.value=m[0].code),_.$set(w),(!k||g&1)&&b!==(b=m[0].highlighted+"")&&$.p(b)},i(m){k||(A(_.$$.fragment,m),A(y.$$.fragment,m),k=!0)},o(m){E(_.$$.fragment,m),E(y.$$.fragment,m),k=!1},d(m){m&&t(o),z(_),m&&t(u),m&&t(v),z(y)}}}function _u(T){let o,_,u,v,y,$;const b=[du,hu],k=[];function m(g,w){return g[3]==="group1"?0:1}return _=m(T),u=k[_]=b[_](T),{c(){o=n("div"),u.c(),this.h()},l(g){o=l(g,"DIV",{class:!0});var w=i(o);u.l(w),w.forEach(t),this.h()},h(){c(o,"class","code-block relative")},m(g,w){f(g,o,w),k[_].m(o,null),v=!0,y||($=[Ya(o,"mouseover",T[6]),Ya(o,"focus",T[6]),Ya(o,"mouseout",T[7]),Ya(o,"focus",T[7])],y=!0)},p(g,[w]){let j=_;_=m(g),_===j?k[_].p(g,w):(Yo(),E(k[j],1,1,()=>{k[j]=null}),Go(),u=k[_],u?u.p(g,w):(u=k[_]=b[_](g),u.c()),A(u,1),u.m(o,null))},i(g){v||(A(u),v=!0)},o(g){E(u),v=!1},d(g){g&&t(o),k[_].d(),y=!1,ou($)}}}function gu(T,o,_){let u,{group1:v}=o,{group2:y}=o;const $=[v.id,y.id],b=$.join("-"),k=Xf(b);Qf(T,k,j=>_(3,u=j));let m=!0;function g(){_(2,m=!1)}function w(){_(2,m=!0)}return T.$$set=j=>{"group1"in j&&_(0,v=j.group1),"group2"in j&&_(1,y=j.group2)},[v,y,m,u,$,k,g,w]}class Ro extends Qi{constructor(o){super();Ji(this,o,gu,_u,Ki,{group1:0,group2:1})}}function $u(T){let o,_,u,v,y,$,b,k;return{c(){o=n("p"),_=a("For more details about tasks supported by "),u=n("a"),v=a("pipeline()"),y=a(", take a look at the "),$=n("a"),b=a("pipelines API reference"),k=a("."),this.h()},l(m){o=l(m,"P",{});var g=i(o);_=r(g,"For more details about tasks supported by "),u=l(g,"A",{href:!0});var w=i(u);v=r(w,"pipeline()"),w.forEach(t),y=r(g,", take a look at the "),$=l(g,"A",{href:!0});var j=i($);b=r(j,"pipelines API reference"),j.forEach(t),k=r(g,"."),g.forEach(t),this.h()},h(){c(u,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c($,"href","./main_classes/pipelines")},m(m,g){f(m,o,g),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k)},d(m){m&&t(o)}}}function vu(T){let o,_,u,v,y,$,b,k,m,g,w;return{c(){o=n("p"),_=a("Check out the "),u=n("a"),v=a("preprocess"),y=a(" tutorial for more details about tokenization and how to use an "),$=n("a"),b=a("AutoFeatureExtractor"),k=a(" and "),m=n("a"),g=a("AutoProcessor"),w=a(" to preprocess image, audio, and multimodal inputs."),this.h()},l(j){o=l(j,"P",{});var O=i(o);_=r(O,"Check out the "),u=l(O,"A",{href:!0});var H=i(u);v=r(H,"preprocess"),H.forEach(t),y=r(O," tutorial for more details about tokenization and how to use an "),$=l(O,"A",{href:!0});var N=i($);b=r(N,"AutoFeatureExtractor"),N.forEach(t),k=r(O," and "),m=l(O,"A",{href:!0});var I=i(m);g=r(I,"AutoProcessor"),I.forEach(t),w=r(O," to preprocess image, audio, and multimodal inputs."),O.forEach(t),this.h()},h(){c(u,"href","./preprocessing"),c($,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoFeatureExtractor"),c(m,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoProcessor")},m(j,O){f(j,o,O),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k),s(o,m),s(m,g),s(o,w)},d(j){j&&t(o)}}}function yu(T){let o,_,u,v,y,$,b,k;return{c(){o=n("p"),_=a("See the "),u=n("a"),v=a("task summary"),y=a(" for tasks supported by an "),$=n("a"),b=a("AutoModel"),k=a(" class."),this.h()},l(m){o=l(m,"P",{});var g=i(o);_=r(g,"See the "),u=l(g,"A",{href:!0});var w=i(u);v=r(w,"task summary"),w.forEach(t),y=r(g," for tasks supported by an "),$=l(g,"A",{href:!0});var j=i($);b=r(j,"AutoModel"),j.forEach(t),k=r(g," class."),g.forEach(t),this.h()},h(){c(u,"href","./task_summary"),c($,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(m,g){f(m,o,g),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k)},d(m){m&&t(o)}}}function ku(T){let o,_,u,v,y,$,b,k,m,g,w,j,O,H,N,I,V,L,Q,U,he,ee,B,le,W,K,te,Y,oe,Z,de,X,se,G,ie,q,C,R;return I=new D({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),L=new Ga({props:{$$slots:{default:[yu]},$$scope:{ctx:T}}}),K=new D({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),C=new D({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){o=n("p"),_=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances, which means you can load an "),u=n("a"),v=a("AutoModel"),y=a(" like you would load an "),$=n("a"),b=a("AutoTokenizer"),k=a(". The only difference is selecting the correct "),m=n("a"),g=a("AutoModel"),w=a(" for the task. For text (or sequence) classification, load "),j=n("a"),O=a("AutoModelForSequenceClassification"),H=a(":"),N=h(),x(I.$$.fragment),V=h(),x(L.$$.fragment),Q=h(),U=n("p"),he=a("Now pass your preprocessed batch of inputs directly to the model, and unpack the dictionary by adding "),ee=n("code"),B=a("**"),le=a(":"),W=h(),x(K.$$.fragment),te=h(),Y=n("p"),oe=a("The model outputs the final activations in the "),Z=n("code"),de=a("logits"),X=a(" attribute. Apply a softmax function to the "),se=n("code"),G=a("logits"),ie=a(" to retrieve the probabilities:"),q=h(),x(C.$$.fragment),this.h()},l(P){o=l(P,"P",{});var M=i(o);_=r(M,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances, which means you can load an "),u=l(M,"A",{href:!0});var pe=i(u);v=r(pe,"AutoModel"),pe.forEach(t),y=r(M," like you would load an "),$=l(M,"A",{href:!0});var ze=i($);b=r(ze,"AutoTokenizer"),ze.forEach(t),k=r(M,". The only difference is selecting the correct "),m=l(M,"A",{href:!0});var ae=i(m);g=r(ae,"AutoModel"),ae.forEach(t),w=r(M," for the task. For text (or sequence) classification, load "),j=l(M,"A",{href:!0});var fe=i(j);O=r(fe,"AutoModelForSequenceClassification"),fe.forEach(t),H=r(M,":"),M.forEach(t),N=d(P),S(I.$$.fragment,P),V=d(P),S(L.$$.fragment,P),Q=d(P),U=l(P,"P",{});var _e=i(U);he=r(_e,"Now pass your preprocessed batch of inputs directly to the model, and unpack the dictionary by adding "),ee=l(_e,"CODE",{});var dt=i(ee);B=r(dt,"**"),dt.forEach(t),le=r(_e,":"),_e.forEach(t),W=d(P),S(K.$$.fragment,P),te=d(P),Y=l(P,"P",{});var ne=i(Y);oe=r(ne,"The model outputs the final activations in the "),Z=l(ne,"CODE",{});var fs=i(Z);de=r(fs,"logits"),fs.forEach(t),X=r(ne," attribute. Apply a softmax function to the "),se=l(ne,"CODE",{});var _t=i(se);G=r(_t,"logits"),_t.forEach(t),ie=r(ne," to retrieve the probabilities:"),ne.forEach(t),q=d(P),S(C.$$.fragment,P),this.h()},h(){c(u,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),c($,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),c(m,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel"),c(j,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(P,M){f(P,o,M),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k),s(o,m),s(m,g),s(o,w),s(o,j),s(j,O),s(o,H),f(P,N,M),F(I,P,M),f(P,V,M),F(L,P,M),f(P,Q,M),f(P,U,M),s(U,he),s(U,ee),s(ee,B),s(U,le),f(P,W,M),F(K,P,M),f(P,te,M),f(P,Y,M),s(Y,oe),s(Y,Z),s(Z,de),s(Y,X),s(Y,se),s(se,G),s(Y,ie),f(P,q,M),F(C,P,M),R=!0},p(P,M){const pe={};M&2&&(pe.$$scope={dirty:M,ctx:P}),L.$set(pe)},i(P){R||(A(I.$$.fragment,P),A(L.$$.fragment,P),A(K.$$.fragment,P),A(C.$$.fragment,P),R=!0)},o(P){E(I.$$.fragment,P),E(L.$$.fragment,P),E(K.$$.fragment,P),E(C.$$.fragment,P),R=!1},d(P){P&&t(o),P&&t(N),z(I,P),P&&t(V),z(L,P),P&&t(Q),P&&t(U),P&&t(W),z(K,P),P&&t(te),P&&t(Y),P&&t(q),z(C,P)}}}function bu(T){let o,_;return o=new eu({props:{$$slots:{default:[ku]},$$scope:{ctx:T}}}),{c(){x(o.$$.fragment)},l(u){S(o.$$.fragment,u)},m(u,v){F(o,u,v),_=!0},p(u,v){const y={};v&2&&(y.$$scope={dirty:v,ctx:u}),o.$set(y)},i(u){_||(A(o.$$.fragment,u),_=!0)},o(u){E(o.$$.fragment,u),_=!1},d(u){z(o,u)}}}function wu(T){let o,_,u,v,y,$,b,k;return{c(){o=n("p"),_=a("See the "),u=n("a"),v=a("task summary"),y=a(" for tasks supported by an "),$=n("a"),b=a("AutoModel"),k=a(" class."),this.h()},l(m){o=l(m,"P",{});var g=i(o);_=r(g,"See the "),u=l(g,"A",{href:!0});var w=i(u);v=r(w,"task summary"),w.forEach(t),y=r(g," for tasks supported by an "),$=l(g,"A",{href:!0});var j=i($);b=r(j,"AutoModel"),j.forEach(t),k=r(g," class."),g.forEach(t),this.h()},h(){c(u,"href","./task_summary"),c($,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModel")},m(m,g){f(m,o,g),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k)},d(m){m&&t(o)}}}function Tu(T){let o,_,u,v,y,$,b,k,m,g,w,j,O,H,N,I,V,L,Q,U,he,ee,B,le,W,K,te,Y,oe,Z,de,X,se,G,ie;return I=new D({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),L=new Ga({props:{$$slots:{default:[wu]},$$scope:{ctx:T}}}),B=new D({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),G=new D({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){o=n("p"),_=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances, which means you can load an "),u=n("a"),v=a("TFAutoModel"),y=a(" like you would load an "),$=n("a"),b=a("AutoTokenizer"),k=a(". The only difference is selecting the correct "),m=n("a"),g=a("TFAutoModel"),w=a(" for the task. For text (or sequence) classification, load "),j=n("a"),O=a("TFAutoModelForSequenceClassification"),H=a(":"),N=h(),x(I.$$.fragment),V=h(),x(L.$$.fragment),Q=h(),U=n("p"),he=a("Now pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),ee=h(),x(B.$$.fragment),le=h(),W=n("p"),K=a("The model outputs the final activations in the "),te=n("code"),Y=a("logits"),oe=a(" attribute. Apply a softmax function to the "),Z=n("code"),de=a("logits"),X=a(" to retrieve the probabilities:"),se=h(),x(G.$$.fragment),this.h()},l(q){o=l(q,"P",{});var C=i(o);_=r(C,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances, which means you can load an "),u=l(C,"A",{href:!0});var R=i(u);v=r(R,"TFAutoModel"),R.forEach(t),y=r(C," like you would load an "),$=l(C,"A",{href:!0});var P=i($);b=r(P,"AutoTokenizer"),P.forEach(t),k=r(C,". The only difference is selecting the correct "),m=l(C,"A",{href:!0});var M=i(m);g=r(M,"TFAutoModel"),M.forEach(t),w=r(C," for the task. For text (or sequence) classification, load "),j=l(C,"A",{href:!0});var pe=i(j);O=r(pe,"TFAutoModelForSequenceClassification"),pe.forEach(t),H=r(C,":"),C.forEach(t),N=d(q),S(I.$$.fragment,q),V=d(q),S(L.$$.fragment,q),Q=d(q),U=l(q,"P",{});var ze=i(U);he=r(ze,"Now pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),ze.forEach(t),ee=d(q),S(B.$$.fragment,q),le=d(q),W=l(q,"P",{});var ae=i(W);K=r(ae,"The model outputs the final activations in the "),te=l(ae,"CODE",{});var fe=i(te);Y=r(fe,"logits"),fe.forEach(t),oe=r(ae," attribute. Apply a softmax function to the "),Z=l(ae,"CODE",{});var _e=i(Z);de=r(_e,"logits"),_e.forEach(t),X=r(ae," to retrieve the probabilities:"),ae.forEach(t),se=d(q),S(G.$$.fragment,q),this.h()},h(){c(u,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),c($,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),c(m,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModel"),c(j,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(q,C){f(q,o,C),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k),s(o,m),s(m,g),s(o,w),s(o,j),s(j,O),s(o,H),f(q,N,C),F(I,q,C),f(q,V,C),F(L,q,C),f(q,Q,C),f(q,U,C),s(U,he),f(q,ee,C),F(B,q,C),f(q,le,C),f(q,W,C),s(W,K),s(W,te),s(te,Y),s(W,oe),s(W,Z),s(Z,de),s(W,X),f(q,se,C),F(G,q,C),ie=!0},p(q,C){const R={};C&2&&(R.$$scope={dirty:C,ctx:q}),L.$set(R)},i(q){ie||(A(I.$$.fragment,q),A(L.$$.fragment,q),A(B.$$.fragment,q),A(G.$$.fragment,q),ie=!0)},o(q){E(I.$$.fragment,q),E(L.$$.fragment,q),E(B.$$.fragment,q),E(G.$$.fragment,q),ie=!1},d(q){q&&t(o),q&&t(N),z(I,q),q&&t(V),z(L,q),q&&t(Q),q&&t(U),q&&t(ee),z(B,q),q&&t(le),q&&t(W),q&&t(se),z(G,q)}}}function ju(T){let o,_;return o=new eu({props:{$$slots:{default:[Tu]},$$scope:{ctx:T}}}),{c(){x(o.$$.fragment)},l(u){S(o.$$.fragment,u)},m(u,v){F(o,u,v),_=!0},p(u,v){const y={};v&2&&(y.$$scope={dirty:v,ctx:u}),o.$set(y)},i(u){_||(A(o.$$.fragment,u),_=!0)},o(u){E(o.$$.fragment,u),_=!1},d(u){z(o,u)}}}function Au(T){let o,_,u,v,y,$,b,k;return{c(){o=n("p"),_=a("For TensorFlow, convert the dataset into a TensorFlow compatible format with "),u=n("code"),v=a("to_tf_dataset"),y=a(". Then you can compile and fit your model with the usual Keras methods. Take a look at the "),$=n("a"),b=a("fine-tuning tutorial"),k=a(" for more details."),this.h()},l(m){o=l(m,"P",{});var g=i(o);_=r(g,"For TensorFlow, convert the dataset into a TensorFlow compatible format with "),u=l(g,"CODE",{});var w=i(u);v=r(w,"to_tf_dataset"),w.forEach(t),y=r(g,". Then you can compile and fit your model with the usual Keras methods. Take a look at the "),$=l(g,"A",{href:!0});var j=i($);b=r(j,"fine-tuning tutorial"),j.forEach(t),k=r(g," for more details."),g.forEach(t),this.h()},h(){c($,"href","./training#convert-dataset-to-tensorflow-format")},m(m,g){f(m,o,g),s(o,_),s(o,u),s(u,v),s(o,y),s(o,$),s($,b),s(o,k)},d(m){m&&t(o)}}}function Eu(T){let o,_,u,v,y,$,b,k,m,g,w,j,O,H,N,I,V,L,Q,U,he,ee,B,le,W,K,te,Y,oe,Z,de,X,se,G,ie,q,C,R,P,M,pe,ze,ae,fe,_e,dt,ne,fs,_t,gt,Va,us,Vo,Qa,$t,Ja,xe,He,ua,vt,Qo,ma,Jo,Ka,ge,Ko,ms,Zo,Xo,cs,en,tn,Za,yt,Xa,We,er,Le,sn,hs,an,rn,tr,kt,sr,ue,on,ds,nn,ln,bt,pn,fn,ca,un,mn,ar,wt,rr,Ue,cn,_s,hn,dn,or,Tt,nr,Be,_n,gs,gn,$n,lr,jt,ir,$e,vn,At,yn,kn,Et,bn,wn,pr,qt,fr,Re,Tn,Pt,ha,jn,An,ur,zt,mr,Ye,En,da,qn,Pn,cr,xt,hr,Ge,zn,$s,xn,Fn,dr,Fe,Ve,_a,Ft,Sn,ga,Cn,_r,re,Mn,vs,In,Nn,St,On,Dn,ys,Hn,Wn,Ct,Ln,Un,gr,Mt,$r,ve,Bn,ks,Rn,Yn,bs,Gn,Vn,vr,It,yr,ye,Qn,ws,Jn,Kn,$a,Zn,Xn,kr,Nt,br,ke,el,Ts,tl,sl,js,al,rl,wr,Se,Qe,va,Ot,ol,ya,nl,Tr,Dt,jr,J,ll,As,il,pl,Es,fl,ul,qs,ml,cl,Ps,hl,dl,ka,_l,gl,Ar,be,$l,ba,vl,yl,zs,kl,bl,Er,Ce,Je,wa,Ht,wl,Ta,Tl,qr,Ke,jl,xs,Al,El,Pr,Ze,zr,Xe,ql,Fs,Pl,zl,xr,Wt,Fr,Ss,xl,Sr,Lt,Cr,Cs,Fl,Mr,et,Ms,Is,Sl,Cl,Ml,Ns,Os,Il,Nl,Ir,Ds,Ol,Nr,Ut,Or,Me,tt,ja,Bt,Dl,Aa,Hl,Dr,st,Hr,we,Wl,Ea,Ll,Ul,qa,Bl,Rl,Wr,Ie,at,Pa,Rt,Yl,za,Gl,Lr,Hs,Vl,Ur,Ws,Ql,Br,Yt,Rr,Ls,Jl,Yr,Gt,Gr,rt,Kl,Us,Zl,Xl,Vr,Ne,ot,xa,Vt,ei,Fa,ti,Qr,me,si,Qt,Sa,ai,ri,Jt,Ca,oi,ni,Bs,li,ii,Jr,nt,Kr,Te,pi,Rs,fi,ui,Kt,mi,ci,Zr,Zt,Xr,lt,hi,Ma,di,_i,eo,Xt,to,Ys,gi,so,es,ao,je,$i,Gs,vi,yi,ts,Ia,ki,bi,wi,ro,Vs,Ti,oo,ss,no,Ae,ji,Qs,Ai,Ei,Js,qi,Pi,lo,as,io,Oe,it,Na,rs,zi,Oa,xi,po,pt,Fi,Ks,Si,Ci,fo,os,uo,ft,Mi,Zs,Ii,Ni,mo,ns,co,Ee,Oi,Da,Di,Hi,Ha,Wi,Li,ho,ls,_o,De,ut,Wa,is,Ui,La,Bi,go,Xs,Ri,$o;return $=new Pe({}),w=new uu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),gt=new D({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),$t=new D({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),vt=new Pe({}),yt=new Rf({props:{id:"tiZFewofSLM"}}),We=new Ga({props:{$$slots:{default:[$u]},$$scope:{ctx:T}}}),kt=new D({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),wt=new D({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Tt=new D({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),jt=new D({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),qt=new D({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),zt=new D({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),xt=new D({props:{code:`result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I TURN A JOIN A COUNT&#x27;</span>]`}}),Ft=new Pe({}),Mt=new D({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),It=new Ro({props:{group1:{id:"pt",code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),Nt=new D({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),Ot=new Pe({}),Dt=new Rf({props:{id:"AhChOFRegn4"}}),Ht=new Pe({}),Ze=new Ga({props:{$$slots:{default:[vu]},$$scope:{ctx:T}}}),Wt=new D({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Lt=new D({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ut=new Ro({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),Bt=new Pe({}),st=new fu({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ju],pytorch:[bu]},$$scope:{ctx:T}}}),Rt=new Pe({}),Yt=new D({props:{code:`from transformers import DistilBertConfig

my_config = DistilBertConfig(n_heads=12)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = DistilBertConfig(n_heads=<span class="hljs-number">12</span>)`}}),Gt=new D({props:{code:`from transformers import DistilBertForSequenceClassification

my_model = DistilBertForSequenceClassification(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = DistilBertForSequenceClassification(my_config)`}}),Vt=new Pe({}),nt=new Ga({props:{$$slots:{default:[Au]},$$scope:{ctx:T}}}),Zt=new D({props:{code:`from datasets import load_dataset

dataset = load_dataset("yelp_review_full")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)`}}),Xt=new D({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)`}}),es=new D({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`}}),ss=new D({props:{code:`from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)`}}),as=new D({props:{code:"trainer.train()",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()'}}),rs=new Pe({}),os=new D({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)
tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)
===PT-TF-SPLIT
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),ns=new Ro({props:{group1:{id:"pt",code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},group2:{id:"tf",code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),ls=new Ro({props:{group1:{id:"pt",code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),is=new Pe({}),{c(){o=n("meta"),_=h(),u=n("h1"),v=n("a"),y=n("span"),x($.$$.fragment),b=h(),k=n("span"),m=a("Quick tour"),g=h(),x(w.$$.fragment),j=h(),O=n("p"),H=a("The \u{1F917} Transformers library is built around four main ideas to maximize user-friendliness and customization for research and experiments:"),N=h(),I=n("ul"),V=n("li"),L=a("Perform inference with the "),Q=n("a"),U=a("pipeline()"),he=a("."),ee=h(),B=n("li"),le=a("Load and use pretrained models from the "),W=n("a"),K=a("Hub"),te=a("."),Y=h(),oe=n("li"),Z=a("Build a custom model by modifying the base configuration, preprocessing, and model classes."),de=h(),X=n("li"),se=a("Train a model with the "),G=n("a"),ie=a("Trainer"),q=a(" class, an optimized training loop for \u{1F917} Transformers models."),C=h(),R=n("p"),P=a("Whether you\u2019re a new user or an experienced developer, this quick tour will help you get started and show you the most important features. If you\u2019re a beginner, we recommend starting with our tutorials or the "),M=n("a"),pe=a("Hugging Face course"),ze=a(" for a more in-depth introduction."),ae=h(),fe=n("p"),_e=a("Let\u2019s take a look at each of these features and get up and running!"),dt=h(),ne=n("p"),fs=a("Before you begin, make sure you have \u{1F917} Transformers installed:"),_t=h(),x(gt.$$.fragment),Va=h(),us=n("p"),Vo=a("You\u2019ll also want to install \u{1F917} Datasets to load a dataset to train on quickly:"),Qa=h(),x($t.$$.fragment),Ja=h(),xe=n("h2"),He=n("a"),ua=n("span"),x(vt.$$.fragment),Qo=h(),ma=n("span"),Jo=a("Pipeline"),Ka=h(),ge=n("p"),Ko=a("The "),ms=n("a"),Zo=a("pipeline()"),Xo=a(" is the easiest way to use a pretrained model for inference. You can use the "),cs=n("a"),en=a("pipeline()"),tn=a(" out-of-the-box for many tasks such as text generation, image classification, automatic speech recognition, and many more."),Za=h(),x(yt.$$.fragment),Xa=h(),x(We.$$.fragment),er=h(),Le=n("p"),sn=a("To use a "),hs=n("a"),an=a("pipeline()"),rn=a(", create an instance of it and specify a task you want to complete, like sentiment analysis, for example:"),tr=h(),x(kt.$$.fragment),sr=h(),ue=n("p"),on=a("For sentiment analysis, the "),ds=n("a"),nn=a("pipeline()"),ln=a(" downloads and caches a default "),bt=n("a"),pn=a("pretrained model"),fn=a(" and tokenizer. Now you can use the "),ca=n("code"),un=a("classifier"),mn=a(" on your target text:"),ar=h(),x(wt.$$.fragment),rr=h(),Ue=n("p"),cn=a("If you have more than one input, pass your inputs as a list to the "),_s=n("a"),hn=a("pipeline()"),dn=a(" to return a list of dictionaries:"),or=h(),x(Tt.$$.fragment),nr=h(),Be=n("p"),_n=a("The "),gs=n("a"),gn=a("pipeline()"),$n=a(" can also iterate over an entire dataset for a task like automatic speech recognition:"),lr=h(),x(jt.$$.fragment),ir=h(),$e=n("p"),vn=a("Load an audio dataset (see the \u{1F917} Datasets "),At=n("a"),yn=a("Quickstart"),kn=a(" for more details) you\u2019d like to iterate over. For example, load the "),Et=n("a"),bn=a("MInDS-14"),wn=a(" dataset:"),pr=h(),x(qt.$$.fragment),fr=h(),Re=n("p"),Tn=a(`You need to make sure the sampling rate of the dataset matches the sampling
rate the model, `),Pt=n("a"),ha=n("code"),jn=a("facebook/wav2vec2-base-960h"),An=a(", was trained on:"),ur=h(),x(zt.$$.fragment),mr=h(),Ye=n("p"),En=a("The audio files are automatically loaded and resampled when you call the "),da=n("code"),qn=a("audio"),Pn=a(` column.
Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:`),cr=h(),x(xt.$$.fragment),hr=h(),Ge=n("p"),zn=a("For larger datasets with big inputs (like speech or vision), you\u2019ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the "),$s=n("a"),xn=a("pipeline API reference"),Fn=a(" for more information."),dr=h(),Fe=n("h3"),Ve=n("a"),_a=n("span"),x(Ft.$$.fragment),Sn=h(),ga=n("span"),Cn=a("Use another model and tokenizer in the pipeline"),_r=h(),re=n("p"),Mn=a("The "),vs=n("a"),In=a("pipeline()"),Nn=a(" can accommodate any model from the "),St=n("a"),On=a("Hub"),Dn=a(", making it easy to adapt the "),ys=n("a"),Hn=a("pipeline()"),Wn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ct=n("a"),Ln=a("BERT model"),Un=a(" fine-tuned for sentiment analysis that you can use for French text:"),gr=h(),x(Mt.$$.fragment),$r=h(),ve=n("p"),Bn=a("Now load the pretrained model and it\u2019s associated tokenizer with "),ks=n("a"),Rn=a("AutoModelForSequenceClassification"),Yn=a(" and "),bs=n("a"),Gn=a("AutoTokenizer"),Vn=a(":"),vr=h(),x(It.$$.fragment),yr=h(),ye=n("p"),Qn=a("Specify the model and tokenizer in the "),ws=n("a"),Jn=a("pipeline()"),Kn=a(", and apply the "),$a=n("code"),Zn=a("classifier"),Xn=a(" to the text:"),kr=h(),x(Nt.$$.fragment),br=h(),ke=n("p"),el=a("If you can\u2019t find a model for your use-case, you\u2019ll need to fine-tune a pretrained model on your data. Take a look at our "),Ts=n("a"),tl=a("fine-tuning tutorial"),sl=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider "),js=n("a"),al=a("sharing"),rl=a(" the model with the community on the Hub to democratize machine learning for everyone! \u{1F917}"),wr=h(),Se=n("h2"),Qe=n("a"),va=n("span"),x(Ot.$$.fragment),ol=h(),ya=n("span"),nl=a("AutoClass"),Tr=h(),x(Dt.$$.fragment),jr=h(),J=n("p"),ll=a("Under the hood, the "),As=n("a"),il=a("AutoModelForSequenceClassification"),pl=a(" and "),Es=n("a"),fl=a("AutoTokenizer"),ul=a(" classes work together to power the "),qs=n("a"),ml=a("pipeline()"),cl=a(" you used in the example above. An "),Ps=n("a"),hl=a("AutoClass"),dl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate "),ka=n("code"),_l=a("AutoClass"),gl=a(" for your task and its associated preprocessing class."),Ar=h(),be=n("p"),$l=a("Let\u2019s return to the example from the previous section and see how you can use the "),ba=n("code"),vl=a("AutoClass"),yl=a(" to replicate the results of the "),zs=n("a"),kl=a("pipeline()"),bl=a("."),Er=h(),Ce=n("h3"),Je=n("a"),wa=n("span"),x(Ht.$$.fragment),wl=h(),Ta=n("span"),Tl=a("AutoTokenizer"),qr=h(),Ke=n("p"),jl=a("A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the "),xs=n("a"),Al=a("tokenizer summary"),El=a("). The most important thing to remember is to instantiate a tokenizer with the same name as the model you\u2019re using. This ensures you\u2019re applying the same tokenization rules a model was pretrained with."),Pr=h(),x(Ze.$$.fragment),zr=h(),Xe=n("p"),ql=a("Load a tokenizer with "),Fs=n("a"),Pl=a("AutoTokenizer"),zl=a(":"),xr=h(),x(Wt.$$.fragment),Fr=h(),Ss=n("p"),xl=a("Pass your text to the tokenizer:"),Sr=h(),x(Lt.$$.fragment),Cr=h(),Cs=n("p"),Fl=a("The tokenizer returns a dictionary containing:"),Mr=h(),et=n("ul"),Ms=n("li"),Is=n("a"),Sl=a("input_ids"),Cl=a(": numerical representions of your tokens."),Ml=h(),Ns=n("li"),Os=n("a"),Il=a("atttention_mask"),Nl=a(": indicates which tokens should be attended to."),Ir=h(),Ds=n("p"),Ol=a("A tokenizer accepts a list of inputs, and it can also pad and truncate the text to return a batch with uniform length:"),Nr=h(),x(Ut.$$.fragment),Or=h(),Me=n("h3"),tt=n("a"),ja=n("span"),x(Bt.$$.fragment),Dl=h(),Aa=n("span"),Hl=a("AutoModel"),Dr=h(),x(st.$$.fragment),Hr=h(),we=n("p"),Wl=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) output the tensors "),Ea=n("em"),Ll=a("before"),Ul=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses, so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice, or a string), and attributes that are `),qa=n("code"),Bl=a("None"),Rl=a(" are ignored."),Wr=h(),Ie=n("h2"),at=n("a"),Pa=n("span"),x(Rt.$$.fragment),Yl=h(),za=n("span"),Gl=a("Custom model builds"),Lr=h(),Hs=n("p"),Vl=a("You can modify the model\u2019s configuration class to change how a model is built. The configuration specifies a model\u2019s attributes, such as the number of hidden layers or attention heads. When you initialize a model from a custom configuration class, you are starting from scratch. The model attributes are randomly initialized, and you\u2019ll need to train the model before you can use it to get meaningful results."),Ur=h(),Ws=n("p"),Ql=a("Start by importing a model\u2019s configuration class, and then you can change the number of attention heads, for instance:"),Br=h(),x(Yt.$$.fragment),Rr=h(),Ls=n("p"),Jl=a("Create a model from your custom configuration:"),Yr=h(),x(Gt.$$.fragment),Gr=h(),rt=n("p"),Kl=a("Take a look at the "),Us=n("a"),Zl=a("Create a custom architecture"),Xl=a(" guide for more information about building custom configurations."),Vr=h(),Ne=n("h2"),ot=n("a"),xa=n("span"),x(Vt.$$.fragment),ei=h(),Fa=n("span"),ti=a("Trainer"),Qr=h(),me=n("p"),si=a("All models are a standard "),Qt=n("a"),Sa=n("code"),ai=a("torch.nn.Module"),ri=a(" or a "),Jt=n("a"),Ca=n("code"),oi=a("tf.keras.Model"),ni=a(" so you can use them in any standard training loop. However, to make things easier, \u{1F917} Transformers provides a "),Bs=n("a"),li=a("Trainer"),ii=a(" class for PyTorch, which adds functionality for distributed training, mixed precision, and more."),Jr=h(),x(nt.$$.fragment),Kr=h(),Te=n("p"),pi=a("Before you can use the "),Rs=n("a"),fi=a("Trainer"),ui=a(", load and prepare a dataset. This example uses the "),Kt=n("a"),mi=a("Yelp Review"),ci=a(" dataset:"),Zr=h(),x(Zt.$$.fragment),Xr=h(),lt=n("p"),hi=a("Tokenize the dataset with "),Ma=n("code"),di=a("map"),_i=a(":"),eo=h(),x(Xt.$$.fragment),to=h(),Ys=n("p"),gi=a("Import your model and the expected number of labels:"),so=h(),x(es.$$.fragment),ao=h(),je=n("p"),$i=a("The "),Gs=n("a"),vi=a("TrainingArguments"),yi=a(" class holds all the available hyperparameters and flags for activating different training options. You can adjust the learning rate, whether you want to use mixed-precision training and options for pushing a model to the Hub, and many more. Take a look at the "),ts=n("a"),Ia=n("code"),ki=a("TrainingArguments"),bi=a(" API reference"),wi=a(" for a full list of options."),ro=h(),Vs=n("p"),Ti=a("In this example, you\u2019ll use the default values and save the model checkpoints to an output directory:"),oo=h(),x(ss.$$.fragment),no=h(),Ae=n("p"),ji=a("Now create a "),Qs=n("a"),Ai=a("Trainer"),Ei=a(" with the model, training arguments, and train and test datasets. Then call "),Js=n("a"),qi=a("train()"),Pi=a(" to fine-tune your model:"),lo=h(),x(as.$$.fragment),io=h(),Oe=n("h3"),it=n("a"),Na=n("span"),x(rs.$$.fragment),zi=h(),Oa=n("span"),xi=a("Save a model"),po=h(),pt=n("p"),Fi=a("Once your model is fine-tuned, you can save it with its tokenizer using "),Ks=n("a"),Si=a("PreTrainedModel.save_pretrained()"),Ci=a(":"),fo=h(),x(os.$$.fragment),uo=h(),ft=n("p"),Mi=a("When you are ready to use the model again, reload it with "),Zs=n("a"),Ii=a("PreTrainedModel.from_pretrained()"),Ni=a(":"),mo=h(),x(ns.$$.fragment),co=h(),Ee=n("p"),Oi=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Da=n("code"),Di=a("from_pt"),Hi=a(" or "),Ha=n("code"),Wi=a("from_tf"),Li=a(" parameter can convert the model from one framework to the other:"),ho=h(),x(ls.$$.fragment),_o=h(),De=n("h2"),ut=n("a"),Wa=n("span"),x(is.$$.fragment),Ui=h(),La=n("span"),Bi=a("What's next?"),go=h(),Xs=n("p"),Ri=a("Now that you\u2019ve completed the \u{1F917} Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you\u2019re interested in learning more about \u{1F917} Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!"),this.h()},l(e){const p=nu('[data-svelte="svelte-1phssyn"]',document.head);o=l(p,"META",{name:!0,content:!0}),p.forEach(t),_=d(e),u=l(e,"H1",{class:!0});var ps=i(u);v=l(ps,"A",{id:!0,class:!0,href:!0});var Ua=i(v);y=l(Ua,"SPAN",{});var Ba=i(y);S($.$$.fragment,Ba),Ba.forEach(t),Ua.forEach(t),b=d(ps),k=l(ps,"SPAN",{});var Ra=i(k);m=r(Ra,"Quick tour"),Ra.forEach(t),ps.forEach(t),g=d(e),S(w.$$.fragment,e),j=d(e),O=l(e,"P",{});var Zi=i(O);H=r(Zi,"The \u{1F917} Transformers library is built around four main ideas to maximize user-friendliness and customization for research and experiments:"),Zi.forEach(t),N=d(e),I=l(e,"UL",{});var mt=i(I);V=l(mt,"LI",{});var vo=i(V);L=r(vo,"Perform inference with the "),Q=l(vo,"A",{href:!0});var Xi=i(Q);U=r(Xi,"pipeline()"),Xi.forEach(t),he=r(vo,"."),vo.forEach(t),ee=d(mt),B=l(mt,"LI",{});var yo=i(B);le=r(yo,"Load and use pretrained models from the "),W=l(yo,"A",{href:!0,rel:!0});var ep=i(W);K=r(ep,"Hub"),ep.forEach(t),te=r(yo,"."),yo.forEach(t),Y=d(mt),oe=l(mt,"LI",{});var tp=i(oe);Z=r(tp,"Build a custom model by modifying the base configuration, preprocessing, and model classes."),tp.forEach(t),de=d(mt),X=l(mt,"LI",{});var ko=i(X);se=r(ko,"Train a model with the "),G=l(ko,"A",{href:!0});var sp=i(G);ie=r(sp,"Trainer"),sp.forEach(t),q=r(ko," class, an optimized training loop for \u{1F917} Transformers models."),ko.forEach(t),mt.forEach(t),C=d(e),R=l(e,"P",{});var bo=i(R);P=r(bo,"Whether you\u2019re a new user or an experienced developer, this quick tour will help you get started and show you the most important features. If you\u2019re a beginner, we recommend starting with our tutorials or the "),M=l(bo,"A",{href:!0,rel:!0});var ap=i(M);pe=r(ap,"Hugging Face course"),ap.forEach(t),ze=r(bo," for a more in-depth introduction."),bo.forEach(t),ae=d(e),fe=l(e,"P",{});var rp=i(fe);_e=r(rp,"Let\u2019s take a look at each of these features and get up and running!"),rp.forEach(t),dt=d(e),ne=l(e,"P",{});var op=i(ne);fs=r(op,"Before you begin, make sure you have \u{1F917} Transformers installed:"),op.forEach(t),_t=d(e),S(gt.$$.fragment,e),Va=d(e),us=l(e,"P",{});var np=i(us);Vo=r(np,"You\u2019ll also want to install \u{1F917} Datasets to load a dataset to train on quickly:"),np.forEach(t),Qa=d(e),S($t.$$.fragment,e),Ja=d(e),xe=l(e,"H2",{class:!0});var wo=i(xe);He=l(wo,"A",{id:!0,class:!0,href:!0});var lp=i(He);ua=l(lp,"SPAN",{});var ip=i(ua);S(vt.$$.fragment,ip),ip.forEach(t),lp.forEach(t),Qo=d(wo),ma=l(wo,"SPAN",{});var pp=i(ma);Jo=r(pp,"Pipeline"),pp.forEach(t),wo.forEach(t),Ka=d(e),ge=l(e,"P",{});var ea=i(ge);Ko=r(ea,"The "),ms=l(ea,"A",{href:!0});var fp=i(ms);Zo=r(fp,"pipeline()"),fp.forEach(t),Xo=r(ea," is the easiest way to use a pretrained model for inference. You can use the "),cs=l(ea,"A",{href:!0});var up=i(cs);en=r(up,"pipeline()"),up.forEach(t),tn=r(ea," out-of-the-box for many tasks such as text generation, image classification, automatic speech recognition, and many more."),ea.forEach(t),Za=d(e),S(yt.$$.fragment,e),Xa=d(e),S(We.$$.fragment,e),er=d(e),Le=l(e,"P",{});var To=i(Le);sn=r(To,"To use a "),hs=l(To,"A",{href:!0});var mp=i(hs);an=r(mp,"pipeline()"),mp.forEach(t),rn=r(To,", create an instance of it and specify a task you want to complete, like sentiment analysis, for example:"),To.forEach(t),tr=d(e),S(kt.$$.fragment,e),sr=d(e),ue=l(e,"P",{});var ct=i(ue);on=r(ct,"For sentiment analysis, the "),ds=l(ct,"A",{href:!0});var cp=i(ds);nn=r(cp,"pipeline()"),cp.forEach(t),ln=r(ct," downloads and caches a default "),bt=l(ct,"A",{href:!0,rel:!0});var hp=i(bt);pn=r(hp,"pretrained model"),hp.forEach(t),fn=r(ct," and tokenizer. Now you can use the "),ca=l(ct,"CODE",{});var dp=i(ca);un=r(dp,"classifier"),dp.forEach(t),mn=r(ct," on your target text:"),ct.forEach(t),ar=d(e),S(wt.$$.fragment,e),rr=d(e),Ue=l(e,"P",{});var jo=i(Ue);cn=r(jo,"If you have more than one input, pass your inputs as a list to the "),_s=l(jo,"A",{href:!0});var _p=i(_s);hn=r(_p,"pipeline()"),_p.forEach(t),dn=r(jo," to return a list of dictionaries:"),jo.forEach(t),or=d(e),S(Tt.$$.fragment,e),nr=d(e),Be=l(e,"P",{});var Ao=i(Be);_n=r(Ao,"The "),gs=l(Ao,"A",{href:!0});var gp=i(gs);gn=r(gp,"pipeline()"),gp.forEach(t),$n=r(Ao," can also iterate over an entire dataset for a task like automatic speech recognition:"),Ao.forEach(t),lr=d(e),S(jt.$$.fragment,e),ir=d(e),$e=l(e,"P",{});var ta=i($e);vn=r(ta,"Load an audio dataset (see the \u{1F917} Datasets "),At=l(ta,"A",{href:!0,rel:!0});var $p=i(At);yn=r($p,"Quickstart"),$p.forEach(t),kn=r(ta," for more details) you\u2019d like to iterate over. For example, load the "),Et=l(ta,"A",{href:!0,rel:!0});var vp=i(Et);bn=r(vp,"MInDS-14"),vp.forEach(t),wn=r(ta," dataset:"),ta.forEach(t),pr=d(e),S(qt.$$.fragment,e),fr=d(e),Re=l(e,"P",{});var Eo=i(Re);Tn=r(Eo,`You need to make sure the sampling rate of the dataset matches the sampling
rate the model, `),Pt=l(Eo,"A",{href:!0,rel:!0});var yp=i(Pt);ha=l(yp,"CODE",{});var kp=i(ha);jn=r(kp,"facebook/wav2vec2-base-960h"),kp.forEach(t),yp.forEach(t),An=r(Eo,", was trained on:"),Eo.forEach(t),ur=d(e),S(zt.$$.fragment,e),mr=d(e),Ye=l(e,"P",{});var qo=i(Ye);En=r(qo,"The audio files are automatically loaded and resampled when you call the "),da=l(qo,"CODE",{});var bp=i(da);qn=r(bp,"audio"),bp.forEach(t),Pn=r(qo,` column.
Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:`),qo.forEach(t),cr=d(e),S(xt.$$.fragment,e),hr=d(e),Ge=l(e,"P",{});var Po=i(Ge);zn=r(Po,"For larger datasets with big inputs (like speech or vision), you\u2019ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the "),$s=l(Po,"A",{href:!0});var wp=i($s);xn=r(wp,"pipeline API reference"),wp.forEach(t),Fn=r(Po," for more information."),Po.forEach(t),dr=d(e),Fe=l(e,"H3",{class:!0});var zo=i(Fe);Ve=l(zo,"A",{id:!0,class:!0,href:!0});var Tp=i(Ve);_a=l(Tp,"SPAN",{});var jp=i(_a);S(Ft.$$.fragment,jp),jp.forEach(t),Tp.forEach(t),Sn=d(zo),ga=l(zo,"SPAN",{});var Ap=i(ga);Cn=r(Ap,"Use another model and tokenizer in the pipeline"),Ap.forEach(t),zo.forEach(t),_r=d(e),re=l(e,"P",{});var qe=i(re);Mn=r(qe,"The "),vs=l(qe,"A",{href:!0});var Ep=i(vs);In=r(Ep,"pipeline()"),Ep.forEach(t),Nn=r(qe," can accommodate any model from the "),St=l(qe,"A",{href:!0,rel:!0});var qp=i(St);On=r(qp,"Hub"),qp.forEach(t),Dn=r(qe,", making it easy to adapt the "),ys=l(qe,"A",{href:!0});var Pp=i(ys);Hn=r(Pp,"pipeline()"),Pp.forEach(t),Wn=r(qe," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ct=l(qe,"A",{href:!0,rel:!0});var zp=i(Ct);Ln=r(zp,"BERT model"),zp.forEach(t),Un=r(qe," fine-tuned for sentiment analysis that you can use for French text:"),qe.forEach(t),gr=d(e),S(Mt.$$.fragment,e),$r=d(e),ve=l(e,"P",{});var sa=i(ve);Bn=r(sa,"Now load the pretrained model and it\u2019s associated tokenizer with "),ks=l(sa,"A",{href:!0});var xp=i(ks);Rn=r(xp,"AutoModelForSequenceClassification"),xp.forEach(t),Yn=r(sa," and "),bs=l(sa,"A",{href:!0});var Fp=i(bs);Gn=r(Fp,"AutoTokenizer"),Fp.forEach(t),Vn=r(sa,":"),sa.forEach(t),vr=d(e),S(It.$$.fragment,e),yr=d(e),ye=l(e,"P",{});var aa=i(ye);Qn=r(aa,"Specify the model and tokenizer in the "),ws=l(aa,"A",{href:!0});var Sp=i(ws);Jn=r(Sp,"pipeline()"),Sp.forEach(t),Kn=r(aa,", and apply the "),$a=l(aa,"CODE",{});var Cp=i($a);Zn=r(Cp,"classifier"),Cp.forEach(t),Xn=r(aa," to the text:"),aa.forEach(t),kr=d(e),S(Nt.$$.fragment,e),br=d(e),ke=l(e,"P",{});var ra=i(ke);el=r(ra,"If you can\u2019t find a model for your use-case, you\u2019ll need to fine-tune a pretrained model on your data. Take a look at our "),Ts=l(ra,"A",{href:!0});var Mp=i(Ts);tl=r(Mp,"fine-tuning tutorial"),Mp.forEach(t),sl=r(ra," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider "),js=l(ra,"A",{href:!0});var Ip=i(js);al=r(Ip,"sharing"),Ip.forEach(t),rl=r(ra," the model with the community on the Hub to democratize machine learning for everyone! \u{1F917}"),ra.forEach(t),wr=d(e),Se=l(e,"H2",{class:!0});var xo=i(Se);Qe=l(xo,"A",{id:!0,class:!0,href:!0});var Np=i(Qe);va=l(Np,"SPAN",{});var Op=i(va);S(Ot.$$.fragment,Op),Op.forEach(t),Np.forEach(t),ol=d(xo),ya=l(xo,"SPAN",{});var Dp=i(ya);nl=r(Dp,"AutoClass"),Dp.forEach(t),xo.forEach(t),Tr=d(e),S(Dt.$$.fragment,e),jr=d(e),J=l(e,"P",{});var ce=i(J);ll=r(ce,"Under the hood, the "),As=l(ce,"A",{href:!0});var Hp=i(As);il=r(Hp,"AutoModelForSequenceClassification"),Hp.forEach(t),pl=r(ce," and "),Es=l(ce,"A",{href:!0});var Wp=i(Es);fl=r(Wp,"AutoTokenizer"),Wp.forEach(t),ul=r(ce," classes work together to power the "),qs=l(ce,"A",{href:!0});var Lp=i(qs);ml=r(Lp,"pipeline()"),Lp.forEach(t),cl=r(ce," you used in the example above. An "),Ps=l(ce,"A",{href:!0});var Up=i(Ps);hl=r(Up,"AutoClass"),Up.forEach(t),dl=r(ce," is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate "),ka=l(ce,"CODE",{});var Bp=i(ka);_l=r(Bp,"AutoClass"),Bp.forEach(t),gl=r(ce," for your task and its associated preprocessing class."),ce.forEach(t),Ar=d(e),be=l(e,"P",{});var oa=i(be);$l=r(oa,"Let\u2019s return to the example from the previous section and see how you can use the "),ba=l(oa,"CODE",{});var Rp=i(ba);vl=r(Rp,"AutoClass"),Rp.forEach(t),yl=r(oa," to replicate the results of the "),zs=l(oa,"A",{href:!0});var Yp=i(zs);kl=r(Yp,"pipeline()"),Yp.forEach(t),bl=r(oa,"."),oa.forEach(t),Er=d(e),Ce=l(e,"H3",{class:!0});var Fo=i(Ce);Je=l(Fo,"A",{id:!0,class:!0,href:!0});var Gp=i(Je);wa=l(Gp,"SPAN",{});var Vp=i(wa);S(Ht.$$.fragment,Vp),Vp.forEach(t),Gp.forEach(t),wl=d(Fo),Ta=l(Fo,"SPAN",{});var Qp=i(Ta);Tl=r(Qp,"AutoTokenizer"),Qp.forEach(t),Fo.forEach(t),qr=d(e),Ke=l(e,"P",{});var So=i(Ke);jl=r(So,"A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the "),xs=l(So,"A",{href:!0});var Jp=i(xs);Al=r(Jp,"tokenizer summary"),Jp.forEach(t),El=r(So,"). The most important thing to remember is to instantiate a tokenizer with the same name as the model you\u2019re using. This ensures you\u2019re applying the same tokenization rules a model was pretrained with."),So.forEach(t),Pr=d(e),S(Ze.$$.fragment,e),zr=d(e),Xe=l(e,"P",{});var Co=i(Xe);ql=r(Co,"Load a tokenizer with "),Fs=l(Co,"A",{href:!0});var Kp=i(Fs);Pl=r(Kp,"AutoTokenizer"),Kp.forEach(t),zl=r(Co,":"),Co.forEach(t),xr=d(e),S(Wt.$$.fragment,e),Fr=d(e),Ss=l(e,"P",{});var Zp=i(Ss);xl=r(Zp,"Pass your text to the tokenizer:"),Zp.forEach(t),Sr=d(e),S(Lt.$$.fragment,e),Cr=d(e),Cs=l(e,"P",{});var Xp=i(Cs);Fl=r(Xp,"The tokenizer returns a dictionary containing:"),Xp.forEach(t),Mr=d(e),et=l(e,"UL",{});var Mo=i(et);Ms=l(Mo,"LI",{});var Yi=i(Ms);Is=l(Yi,"A",{href:!0});var ef=i(Is);Sl=r(ef,"input_ids"),ef.forEach(t),Cl=r(Yi,": numerical representions of your tokens."),Yi.forEach(t),Ml=d(Mo),Ns=l(Mo,"LI",{});var Gi=i(Ns);Os=l(Gi,"A",{href:!0});var tf=i(Os);Il=r(tf,"atttention_mask"),tf.forEach(t),Nl=r(Gi,": indicates which tokens should be attended to."),Gi.forEach(t),Mo.forEach(t),Ir=d(e),Ds=l(e,"P",{});var sf=i(Ds);Ol=r(sf,"A tokenizer accepts a list of inputs, and it can also pad and truncate the text to return a batch with uniform length:"),sf.forEach(t),Nr=d(e),S(Ut.$$.fragment,e),Or=d(e),Me=l(e,"H3",{class:!0});var Io=i(Me);tt=l(Io,"A",{id:!0,class:!0,href:!0});var af=i(tt);ja=l(af,"SPAN",{});var rf=i(ja);S(Bt.$$.fragment,rf),rf.forEach(t),af.forEach(t),Dl=d(Io),Aa=l(Io,"SPAN",{});var of=i(Aa);Hl=r(of,"AutoModel"),of.forEach(t),Io.forEach(t),Dr=d(e),S(st.$$.fragment,e),Hr=d(e),we=l(e,"P",{});var na=i(we);Wl=r(na,"All \u{1F917} Transformers models (PyTorch or TensorFlow) output the tensors "),Ea=l(na,"EM",{});var nf=i(Ea);Ll=r(nf,"before"),nf.forEach(t),Ul=r(na,` the final activation
function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses, so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice, or a string), and attributes that are `),qa=l(na,"CODE",{});var lf=i(qa);Bl=r(lf,"None"),lf.forEach(t),Rl=r(na," are ignored."),na.forEach(t),Wr=d(e),Ie=l(e,"H2",{class:!0});var No=i(Ie);at=l(No,"A",{id:!0,class:!0,href:!0});var pf=i(at);Pa=l(pf,"SPAN",{});var ff=i(Pa);S(Rt.$$.fragment,ff),ff.forEach(t),pf.forEach(t),Yl=d(No),za=l(No,"SPAN",{});var uf=i(za);Gl=r(uf,"Custom model builds"),uf.forEach(t),No.forEach(t),Lr=d(e),Hs=l(e,"P",{});var mf=i(Hs);Vl=r(mf,"You can modify the model\u2019s configuration class to change how a model is built. The configuration specifies a model\u2019s attributes, such as the number of hidden layers or attention heads. When you initialize a model from a custom configuration class, you are starting from scratch. The model attributes are randomly initialized, and you\u2019ll need to train the model before you can use it to get meaningful results."),mf.forEach(t),Ur=d(e),Ws=l(e,"P",{});var cf=i(Ws);Ql=r(cf,"Start by importing a model\u2019s configuration class, and then you can change the number of attention heads, for instance:"),cf.forEach(t),Br=d(e),S(Yt.$$.fragment,e),Rr=d(e),Ls=l(e,"P",{});var hf=i(Ls);Jl=r(hf,"Create a model from your custom configuration:"),hf.forEach(t),Yr=d(e),S(Gt.$$.fragment,e),Gr=d(e),rt=l(e,"P",{});var Oo=i(rt);Kl=r(Oo,"Take a look at the "),Us=l(Oo,"A",{href:!0});var df=i(Us);Zl=r(df,"Create a custom architecture"),df.forEach(t),Xl=r(Oo," guide for more information about building custom configurations."),Oo.forEach(t),Vr=d(e),Ne=l(e,"H2",{class:!0});var Do=i(Ne);ot=l(Do,"A",{id:!0,class:!0,href:!0});var _f=i(ot);xa=l(_f,"SPAN",{});var gf=i(xa);S(Vt.$$.fragment,gf),gf.forEach(t),_f.forEach(t),ei=d(Do),Fa=l(Do,"SPAN",{});var $f=i(Fa);ti=r($f,"Trainer"),$f.forEach(t),Do.forEach(t),Qr=d(e),me=l(e,"P",{});var ht=i(me);si=r(ht,"All models are a standard "),Qt=l(ht,"A",{href:!0,rel:!0});var vf=i(Qt);Sa=l(vf,"CODE",{});var yf=i(Sa);ai=r(yf,"torch.nn.Module"),yf.forEach(t),vf.forEach(t),ri=r(ht," or a "),Jt=l(ht,"A",{href:!0,rel:!0});var kf=i(Jt);Ca=l(kf,"CODE",{});var bf=i(Ca);oi=r(bf,"tf.keras.Model"),bf.forEach(t),kf.forEach(t),ni=r(ht," so you can use them in any standard training loop. However, to make things easier, \u{1F917} Transformers provides a "),Bs=l(ht,"A",{href:!0});var wf=i(Bs);li=r(wf,"Trainer"),wf.forEach(t),ii=r(ht," class for PyTorch, which adds functionality for distributed training, mixed precision, and more."),ht.forEach(t),Jr=d(e),S(nt.$$.fragment,e),Kr=d(e),Te=l(e,"P",{});var la=i(Te);pi=r(la,"Before you can use the "),Rs=l(la,"A",{href:!0});var Tf=i(Rs);fi=r(Tf,"Trainer"),Tf.forEach(t),ui=r(la,", load and prepare a dataset. This example uses the "),Kt=l(la,"A",{href:!0,rel:!0});var jf=i(Kt);mi=r(jf,"Yelp Review"),jf.forEach(t),ci=r(la," dataset:"),la.forEach(t),Zr=d(e),S(Zt.$$.fragment,e),Xr=d(e),lt=l(e,"P",{});var Ho=i(lt);hi=r(Ho,"Tokenize the dataset with "),Ma=l(Ho,"CODE",{});var Af=i(Ma);di=r(Af,"map"),Af.forEach(t),_i=r(Ho,":"),Ho.forEach(t),eo=d(e),S(Xt.$$.fragment,e),to=d(e),Ys=l(e,"P",{});var Ef=i(Ys);gi=r(Ef,"Import your model and the expected number of labels:"),Ef.forEach(t),so=d(e),S(es.$$.fragment,e),ao=d(e),je=l(e,"P",{});var ia=i(je);$i=r(ia,"The "),Gs=l(ia,"A",{href:!0});var qf=i(Gs);vi=r(qf,"TrainingArguments"),qf.forEach(t),yi=r(ia," class holds all the available hyperparameters and flags for activating different training options. You can adjust the learning rate, whether you want to use mixed-precision training and options for pushing a model to the Hub, and many more. Take a look at the "),ts=l(ia,"A",{href:!0});var Vi=i(ts);Ia=l(Vi,"CODE",{});var Pf=i(Ia);ki=r(Pf,"TrainingArguments"),Pf.forEach(t),bi=r(Vi," API reference"),Vi.forEach(t),wi=r(ia," for a full list of options."),ia.forEach(t),ro=d(e),Vs=l(e,"P",{});var zf=i(Vs);Ti=r(zf,"In this example, you\u2019ll use the default values and save the model checkpoints to an output directory:"),zf.forEach(t),oo=d(e),S(ss.$$.fragment,e),no=d(e),Ae=l(e,"P",{});var pa=i(Ae);ji=r(pa,"Now create a "),Qs=l(pa,"A",{href:!0});var xf=i(Qs);Ai=r(xf,"Trainer"),xf.forEach(t),Ei=r(pa," with the model, training arguments, and train and test datasets. Then call "),Js=l(pa,"A",{href:!0});var Ff=i(Js);qi=r(Ff,"train()"),Ff.forEach(t),Pi=r(pa," to fine-tune your model:"),pa.forEach(t),lo=d(e),S(as.$$.fragment,e),io=d(e),Oe=l(e,"H3",{class:!0});var Wo=i(Oe);it=l(Wo,"A",{id:!0,class:!0,href:!0});var Sf=i(it);Na=l(Sf,"SPAN",{});var Cf=i(Na);S(rs.$$.fragment,Cf),Cf.forEach(t),Sf.forEach(t),zi=d(Wo),Oa=l(Wo,"SPAN",{});var Mf=i(Oa);xi=r(Mf,"Save a model"),Mf.forEach(t),Wo.forEach(t),po=d(e),pt=l(e,"P",{});var Lo=i(pt);Fi=r(Lo,"Once your model is fine-tuned, you can save it with its tokenizer using "),Ks=l(Lo,"A",{href:!0});var If=i(Ks);Si=r(If,"PreTrainedModel.save_pretrained()"),If.forEach(t),Ci=r(Lo,":"),Lo.forEach(t),fo=d(e),S(os.$$.fragment,e),uo=d(e),ft=l(e,"P",{});var Uo=i(ft);Mi=r(Uo,"When you are ready to use the model again, reload it with "),Zs=l(Uo,"A",{href:!0});var Nf=i(Zs);Ii=r(Nf,"PreTrainedModel.from_pretrained()"),Nf.forEach(t),Ni=r(Uo,":"),Uo.forEach(t),mo=d(e),S(ns.$$.fragment,e),co=d(e),Ee=l(e,"P",{});var fa=i(Ee);Oi=r(fa,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),Da=l(fa,"CODE",{});var Of=i(Da);Di=r(Of,"from_pt"),Of.forEach(t),Hi=r(fa," or "),Ha=l(fa,"CODE",{});var Df=i(Ha);Wi=r(Df,"from_tf"),Df.forEach(t),Li=r(fa," parameter can convert the model from one framework to the other:"),fa.forEach(t),ho=d(e),S(ls.$$.fragment,e),_o=d(e),De=l(e,"H2",{class:!0});var Bo=i(De);ut=l(Bo,"A",{id:!0,class:!0,href:!0});var Hf=i(ut);Wa=l(Hf,"SPAN",{});var Wf=i(Wa);S(is.$$.fragment,Wf),Wf.forEach(t),Hf.forEach(t),Ui=d(Bo),La=l(Bo,"SPAN",{});var Lf=i(La);Bi=r(Lf,"What's next?"),Lf.forEach(t),Bo.forEach(t),go=d(e),Xs=l(e,"P",{});var Uf=i(Xs);Ri=r(Uf,"Now that you\u2019ve completed the \u{1F917} Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you\u2019re interested in learning more about \u{1F917} Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!"),Uf.forEach(t),this.h()},h(){c(o,"name","hf:doc:metadata"),c(o,"content",JSON.stringify(qu)),c(v,"id","quick-tour"),c(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v,"href","#quick-tour"),c(u,"class","relative group"),c(Q,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(W,"href","https://huggingface.co/models"),c(W,"rel","nofollow"),c(G,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),c(M,"href","https://huggingface.co/course/chapter1/1"),c(M,"rel","nofollow"),c(He,"id","pipeline"),c(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(He,"href","#pipeline"),c(xe,"class","relative group"),c(ms,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(cs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(hs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(ds,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(bt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),c(bt,"rel","nofollow"),c(_s,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(gs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(At,"href","https://huggingface.co/docs/datasets/quickstart#audio"),c(At,"rel","nofollow"),c(Et,"href","https://huggingface.co/datasets/PolyAI/minds14"),c(Et,"rel","nofollow"),c(Pt,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),c(Pt,"rel","nofollow"),c($s,"href","./main_classes/pipelines"),c(Ve,"id","use-another-model-and-tokenizer-in-the-pipeline"),c(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ve,"href","#use-another-model-and-tokenizer-in-the-pipeline"),c(Fe,"class","relative group"),c(vs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(St,"href","https://huggingface.co/models"),c(St,"rel","nofollow"),c(ys,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(Ct,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),c(Ct,"rel","nofollow"),c(ks,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),c(bs,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),c(ws,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(Ts,"href","./training"),c(js,"href","./model_sharing"),c(Qe,"id","autoclass"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#autoclass"),c(Se,"class","relative group"),c(As,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),c(Es,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),c(qs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(Ps,"href","./model_doc/auto"),c(zs,"href","/docs/transformers/pr_18115/en/main_classes/pipelines#transformers.pipeline"),c(Je,"id","autotokenizer"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#autotokenizer"),c(Ce,"class","relative group"),c(xs,"href","./tokenizer_summary"),c(Fs,"href","/docs/transformers/pr_18115/en/model_doc/auto#transformers.AutoTokenizer"),c(Is,"href","./glossary#input-ids"),c(Os,"href",".glossary#attention-mask"),c(tt,"id","automodel"),c(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tt,"href","#automodel"),c(Me,"class","relative group"),c(at,"id","custom-model-builds"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#custom-model-builds"),c(Ie,"class","relative group"),c(Us,"href","./create_a_model"),c(ot,"id","trainer"),c(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ot,"href","#trainer"),c(Ne,"class","relative group"),c(Qt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Qt,"rel","nofollow"),c(Jt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Jt,"rel","nofollow"),c(Bs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),c(Rs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),c(Kt,"href","https://huggingface.co/datasets/yelp_review_full"),c(Kt,"rel","nofollow"),c(Gs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.TrainingArguments"),c(ts,"href","./main_classes/trainer#transformers.TrainingArguments"),c(Qs,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer"),c(Js,"href","/docs/transformers/pr_18115/en/main_classes/trainer#transformers.Trainer.train"),c(it,"id","save-a-model"),c(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(it,"href","#save-a-model"),c(Oe,"class","relative group"),c(Ks,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(Zs,"href","/docs/transformers/pr_18115/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),c(ut,"id","whats-next"),c(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ut,"href","#whats-next"),c(De,"class","relative group")},m(e,p){s(document.head,o),f(e,_,p),f(e,u,p),s(u,v),s(v,y),F($,y,null),s(u,b),s(u,k),s(k,m),f(e,g,p),F(w,e,p),f(e,j,p),f(e,O,p),s(O,H),f(e,N,p),f(e,I,p),s(I,V),s(V,L),s(V,Q),s(Q,U),s(V,he),s(I,ee),s(I,B),s(B,le),s(B,W),s(W,K),s(B,te),s(I,Y),s(I,oe),s(oe,Z),s(I,de),s(I,X),s(X,se),s(X,G),s(G,ie),s(X,q),f(e,C,p),f(e,R,p),s(R,P),s(R,M),s(M,pe),s(R,ze),f(e,ae,p),f(e,fe,p),s(fe,_e),f(e,dt,p),f(e,ne,p),s(ne,fs),f(e,_t,p),F(gt,e,p),f(e,Va,p),f(e,us,p),s(us,Vo),f(e,Qa,p),F($t,e,p),f(e,Ja,p),f(e,xe,p),s(xe,He),s(He,ua),F(vt,ua,null),s(xe,Qo),s(xe,ma),s(ma,Jo),f(e,Ka,p),f(e,ge,p),s(ge,Ko),s(ge,ms),s(ms,Zo),s(ge,Xo),s(ge,cs),s(cs,en),s(ge,tn),f(e,Za,p),F(yt,e,p),f(e,Xa,p),F(We,e,p),f(e,er,p),f(e,Le,p),s(Le,sn),s(Le,hs),s(hs,an),s(Le,rn),f(e,tr,p),F(kt,e,p),f(e,sr,p),f(e,ue,p),s(ue,on),s(ue,ds),s(ds,nn),s(ue,ln),s(ue,bt),s(bt,pn),s(ue,fn),s(ue,ca),s(ca,un),s(ue,mn),f(e,ar,p),F(wt,e,p),f(e,rr,p),f(e,Ue,p),s(Ue,cn),s(Ue,_s),s(_s,hn),s(Ue,dn),f(e,or,p),F(Tt,e,p),f(e,nr,p),f(e,Be,p),s(Be,_n),s(Be,gs),s(gs,gn),s(Be,$n),f(e,lr,p),F(jt,e,p),f(e,ir,p),f(e,$e,p),s($e,vn),s($e,At),s(At,yn),s($e,kn),s($e,Et),s(Et,bn),s($e,wn),f(e,pr,p),F(qt,e,p),f(e,fr,p),f(e,Re,p),s(Re,Tn),s(Re,Pt),s(Pt,ha),s(ha,jn),s(Re,An),f(e,ur,p),F(zt,e,p),f(e,mr,p),f(e,Ye,p),s(Ye,En),s(Ye,da),s(da,qn),s(Ye,Pn),f(e,cr,p),F(xt,e,p),f(e,hr,p),f(e,Ge,p),s(Ge,zn),s(Ge,$s),s($s,xn),s(Ge,Fn),f(e,dr,p),f(e,Fe,p),s(Fe,Ve),s(Ve,_a),F(Ft,_a,null),s(Fe,Sn),s(Fe,ga),s(ga,Cn),f(e,_r,p),f(e,re,p),s(re,Mn),s(re,vs),s(vs,In),s(re,Nn),s(re,St),s(St,On),s(re,Dn),s(re,ys),s(ys,Hn),s(re,Wn),s(re,Ct),s(Ct,Ln),s(re,Un),f(e,gr,p),F(Mt,e,p),f(e,$r,p),f(e,ve,p),s(ve,Bn),s(ve,ks),s(ks,Rn),s(ve,Yn),s(ve,bs),s(bs,Gn),s(ve,Vn),f(e,vr,p),F(It,e,p),f(e,yr,p),f(e,ye,p),s(ye,Qn),s(ye,ws),s(ws,Jn),s(ye,Kn),s(ye,$a),s($a,Zn),s(ye,Xn),f(e,kr,p),F(Nt,e,p),f(e,br,p),f(e,ke,p),s(ke,el),s(ke,Ts),s(Ts,tl),s(ke,sl),s(ke,js),s(js,al),s(ke,rl),f(e,wr,p),f(e,Se,p),s(Se,Qe),s(Qe,va),F(Ot,va,null),s(Se,ol),s(Se,ya),s(ya,nl),f(e,Tr,p),F(Dt,e,p),f(e,jr,p),f(e,J,p),s(J,ll),s(J,As),s(As,il),s(J,pl),s(J,Es),s(Es,fl),s(J,ul),s(J,qs),s(qs,ml),s(J,cl),s(J,Ps),s(Ps,hl),s(J,dl),s(J,ka),s(ka,_l),s(J,gl),f(e,Ar,p),f(e,be,p),s(be,$l),s(be,ba),s(ba,vl),s(be,yl),s(be,zs),s(zs,kl),s(be,bl),f(e,Er,p),f(e,Ce,p),s(Ce,Je),s(Je,wa),F(Ht,wa,null),s(Ce,wl),s(Ce,Ta),s(Ta,Tl),f(e,qr,p),f(e,Ke,p),s(Ke,jl),s(Ke,xs),s(xs,Al),s(Ke,El),f(e,Pr,p),F(Ze,e,p),f(e,zr,p),f(e,Xe,p),s(Xe,ql),s(Xe,Fs),s(Fs,Pl),s(Xe,zl),f(e,xr,p),F(Wt,e,p),f(e,Fr,p),f(e,Ss,p),s(Ss,xl),f(e,Sr,p),F(Lt,e,p),f(e,Cr,p),f(e,Cs,p),s(Cs,Fl),f(e,Mr,p),f(e,et,p),s(et,Ms),s(Ms,Is),s(Is,Sl),s(Ms,Cl),s(et,Ml),s(et,Ns),s(Ns,Os),s(Os,Il),s(Ns,Nl),f(e,Ir,p),f(e,Ds,p),s(Ds,Ol),f(e,Nr,p),F(Ut,e,p),f(e,Or,p),f(e,Me,p),s(Me,tt),s(tt,ja),F(Bt,ja,null),s(Me,Dl),s(Me,Aa),s(Aa,Hl),f(e,Dr,p),F(st,e,p),f(e,Hr,p),f(e,we,p),s(we,Wl),s(we,Ea),s(Ea,Ll),s(we,Ul),s(we,qa),s(qa,Bl),s(we,Rl),f(e,Wr,p),f(e,Ie,p),s(Ie,at),s(at,Pa),F(Rt,Pa,null),s(Ie,Yl),s(Ie,za),s(za,Gl),f(e,Lr,p),f(e,Hs,p),s(Hs,Vl),f(e,Ur,p),f(e,Ws,p),s(Ws,Ql),f(e,Br,p),F(Yt,e,p),f(e,Rr,p),f(e,Ls,p),s(Ls,Jl),f(e,Yr,p),F(Gt,e,p),f(e,Gr,p),f(e,rt,p),s(rt,Kl),s(rt,Us),s(Us,Zl),s(rt,Xl),f(e,Vr,p),f(e,Ne,p),s(Ne,ot),s(ot,xa),F(Vt,xa,null),s(Ne,ei),s(Ne,Fa),s(Fa,ti),f(e,Qr,p),f(e,me,p),s(me,si),s(me,Qt),s(Qt,Sa),s(Sa,ai),s(me,ri),s(me,Jt),s(Jt,Ca),s(Ca,oi),s(me,ni),s(me,Bs),s(Bs,li),s(me,ii),f(e,Jr,p),F(nt,e,p),f(e,Kr,p),f(e,Te,p),s(Te,pi),s(Te,Rs),s(Rs,fi),s(Te,ui),s(Te,Kt),s(Kt,mi),s(Te,ci),f(e,Zr,p),F(Zt,e,p),f(e,Xr,p),f(e,lt,p),s(lt,hi),s(lt,Ma),s(Ma,di),s(lt,_i),f(e,eo,p),F(Xt,e,p),f(e,to,p),f(e,Ys,p),s(Ys,gi),f(e,so,p),F(es,e,p),f(e,ao,p),f(e,je,p),s(je,$i),s(je,Gs),s(Gs,vi),s(je,yi),s(je,ts),s(ts,Ia),s(Ia,ki),s(ts,bi),s(je,wi),f(e,ro,p),f(e,Vs,p),s(Vs,Ti),f(e,oo,p),F(ss,e,p),f(e,no,p),f(e,Ae,p),s(Ae,ji),s(Ae,Qs),s(Qs,Ai),s(Ae,Ei),s(Ae,Js),s(Js,qi),s(Ae,Pi),f(e,lo,p),F(as,e,p),f(e,io,p),f(e,Oe,p),s(Oe,it),s(it,Na),F(rs,Na,null),s(Oe,zi),s(Oe,Oa),s(Oa,xi),f(e,po,p),f(e,pt,p),s(pt,Fi),s(pt,Ks),s(Ks,Si),s(pt,Ci),f(e,fo,p),F(os,e,p),f(e,uo,p),f(e,ft,p),s(ft,Mi),s(ft,Zs),s(Zs,Ii),s(ft,Ni),f(e,mo,p),F(ns,e,p),f(e,co,p),f(e,Ee,p),s(Ee,Oi),s(Ee,Da),s(Da,Di),s(Ee,Hi),s(Ee,Ha),s(Ha,Wi),s(Ee,Li),f(e,ho,p),F(ls,e,p),f(e,_o,p),f(e,De,p),s(De,ut),s(ut,Wa),F(is,Wa,null),s(De,Ui),s(De,La),s(La,Bi),f(e,go,p),f(e,Xs,p),s(Xs,Ri),$o=!0},p(e,[p]){const ps={};p&2&&(ps.$$scope={dirty:p,ctx:e}),We.$set(ps);const Ua={};p&2&&(Ua.$$scope={dirty:p,ctx:e}),Ze.$set(Ua);const Ba={};p&2&&(Ba.$$scope={dirty:p,ctx:e}),st.$set(Ba);const Ra={};p&2&&(Ra.$$scope={dirty:p,ctx:e}),nt.$set(Ra)},i(e){$o||(A($.$$.fragment,e),A(w.$$.fragment,e),A(gt.$$.fragment,e),A($t.$$.fragment,e),A(vt.$$.fragment,e),A(yt.$$.fragment,e),A(We.$$.fragment,e),A(kt.$$.fragment,e),A(wt.$$.fragment,e),A(Tt.$$.fragment,e),A(jt.$$.fragment,e),A(qt.$$.fragment,e),A(zt.$$.fragment,e),A(xt.$$.fragment,e),A(Ft.$$.fragment,e),A(Mt.$$.fragment,e),A(It.$$.fragment,e),A(Nt.$$.fragment,e),A(Ot.$$.fragment,e),A(Dt.$$.fragment,e),A(Ht.$$.fragment,e),A(Ze.$$.fragment,e),A(Wt.$$.fragment,e),A(Lt.$$.fragment,e),A(Ut.$$.fragment,e),A(Bt.$$.fragment,e),A(st.$$.fragment,e),A(Rt.$$.fragment,e),A(Yt.$$.fragment,e),A(Gt.$$.fragment,e),A(Vt.$$.fragment,e),A(nt.$$.fragment,e),A(Zt.$$.fragment,e),A(Xt.$$.fragment,e),A(es.$$.fragment,e),A(ss.$$.fragment,e),A(as.$$.fragment,e),A(rs.$$.fragment,e),A(os.$$.fragment,e),A(ns.$$.fragment,e),A(ls.$$.fragment,e),A(is.$$.fragment,e),$o=!0)},o(e){E($.$$.fragment,e),E(w.$$.fragment,e),E(gt.$$.fragment,e),E($t.$$.fragment,e),E(vt.$$.fragment,e),E(yt.$$.fragment,e),E(We.$$.fragment,e),E(kt.$$.fragment,e),E(wt.$$.fragment,e),E(Tt.$$.fragment,e),E(jt.$$.fragment,e),E(qt.$$.fragment,e),E(zt.$$.fragment,e),E(xt.$$.fragment,e),E(Ft.$$.fragment,e),E(Mt.$$.fragment,e),E(It.$$.fragment,e),E(Nt.$$.fragment,e),E(Ot.$$.fragment,e),E(Dt.$$.fragment,e),E(Ht.$$.fragment,e),E(Ze.$$.fragment,e),E(Wt.$$.fragment,e),E(Lt.$$.fragment,e),E(Ut.$$.fragment,e),E(Bt.$$.fragment,e),E(st.$$.fragment,e),E(Rt.$$.fragment,e),E(Yt.$$.fragment,e),E(Gt.$$.fragment,e),E(Vt.$$.fragment,e),E(nt.$$.fragment,e),E(Zt.$$.fragment,e),E(Xt.$$.fragment,e),E(es.$$.fragment,e),E(ss.$$.fragment,e),E(as.$$.fragment,e),E(rs.$$.fragment,e),E(os.$$.fragment,e),E(ns.$$.fragment,e),E(ls.$$.fragment,e),E(is.$$.fragment,e),$o=!1},d(e){t(o),e&&t(_),e&&t(u),z($),e&&t(g),z(w,e),e&&t(j),e&&t(O),e&&t(N),e&&t(I),e&&t(C),e&&t(R),e&&t(ae),e&&t(fe),e&&t(dt),e&&t(ne),e&&t(_t),z(gt,e),e&&t(Va),e&&t(us),e&&t(Qa),z($t,e),e&&t(Ja),e&&t(xe),z(vt),e&&t(Ka),e&&t(ge),e&&t(Za),z(yt,e),e&&t(Xa),z(We,e),e&&t(er),e&&t(Le),e&&t(tr),z(kt,e),e&&t(sr),e&&t(ue),e&&t(ar),z(wt,e),e&&t(rr),e&&t(Ue),e&&t(or),z(Tt,e),e&&t(nr),e&&t(Be),e&&t(lr),z(jt,e),e&&t(ir),e&&t($e),e&&t(pr),z(qt,e),e&&t(fr),e&&t(Re),e&&t(ur),z(zt,e),e&&t(mr),e&&t(Ye),e&&t(cr),z(xt,e),e&&t(hr),e&&t(Ge),e&&t(dr),e&&t(Fe),z(Ft),e&&t(_r),e&&t(re),e&&t(gr),z(Mt,e),e&&t($r),e&&t(ve),e&&t(vr),z(It,e),e&&t(yr),e&&t(ye),e&&t(kr),z(Nt,e),e&&t(br),e&&t(ke),e&&t(wr),e&&t(Se),z(Ot),e&&t(Tr),z(Dt,e),e&&t(jr),e&&t(J),e&&t(Ar),e&&t(be),e&&t(Er),e&&t(Ce),z(Ht),e&&t(qr),e&&t(Ke),e&&t(Pr),z(Ze,e),e&&t(zr),e&&t(Xe),e&&t(xr),z(Wt,e),e&&t(Fr),e&&t(Ss),e&&t(Sr),z(Lt,e),e&&t(Cr),e&&t(Cs),e&&t(Mr),e&&t(et),e&&t(Ir),e&&t(Ds),e&&t(Nr),z(Ut,e),e&&t(Or),e&&t(Me),z(Bt),e&&t(Dr),z(st,e),e&&t(Hr),e&&t(we),e&&t(Wr),e&&t(Ie),z(Rt),e&&t(Lr),e&&t(Hs),e&&t(Ur),e&&t(Ws),e&&t(Br),z(Yt,e),e&&t(Rr),e&&t(Ls),e&&t(Yr),z(Gt,e),e&&t(Gr),e&&t(rt),e&&t(Vr),e&&t(Ne),z(Vt),e&&t(Qr),e&&t(me),e&&t(Jr),z(nt,e),e&&t(Kr),e&&t(Te),e&&t(Zr),z(Zt,e),e&&t(Xr),e&&t(lt),e&&t(eo),z(Xt,e),e&&t(to),e&&t(Ys),e&&t(so),z(es,e),e&&t(ao),e&&t(je),e&&t(ro),e&&t(Vs),e&&t(oo),z(ss,e),e&&t(no),e&&t(Ae),e&&t(lo),z(as,e),e&&t(io),e&&t(Oe),z(rs),e&&t(po),e&&t(pt),e&&t(fo),z(os,e),e&&t(uo),e&&t(ft),e&&t(mo),z(ns,e),e&&t(co),e&&t(Ee),e&&t(ho),z(ls,e),e&&t(_o),e&&t(De),z(is),e&&t(go),e&&t(Xs)}}}const qu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"}],title:"AutoClass"},{local:"custom-model-builds",title:"Custom model builds"},{local:"trainer",sections:[{local:"save-a-model",title:"Save a model"}],title:"Trainer"},{local:"whats-next",title:"What's next?"}],title:"Quick tour"};function Pu(T){return lu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nu extends Qi{constructor(o){super();Ji(this,o,Pu,Eu,Ki,{})}}export{Nu as default,qu as metadata};
