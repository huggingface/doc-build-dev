import{S as mn,i as un,s as dn,e as r,k as _,w as x,t as l,M as _n,c as i,d as e,m as g,a as p,x as v,h as o,b as j,G as t,g as f,y as E,q as y,o as T,B as z,v as gn,L as hn}from"../../chunks/vendor-hf-doc-builder.js";import{T as ue}from"../../chunks/Tip-hf-doc-builder.js";import{Y as cn}from"../../chunks/Youtube-hf-doc-builder.js";import{I as me}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as X}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as fn,M as de}from"../../chunks/Markdown-hf-doc-builder.js";function $n(P){let a,m,n,u,k;return{c(){a=r("p"),m=l("See the token classification "),n=r("a"),u=l("task page"),k=l(" for more information about other forms of token classification and their associated models, datasets, and metrics."),this.h()},l($){a=i($,"P",{});var w=p(a);m=o(w,"See the token classification "),n=i(w,"A",{href:!0,rel:!0});var A=p(n);u=o(A,"task page"),A.forEach(e),k=o(w," for more information about other forms of token classification and their associated models, datasets, and metrics."),w.forEach(e),this.h()},h(){j(n,"href","https://huggingface.co/tasks/token-classification"),j(n,"rel","nofollow")},m($,w){f($,a,w),t(a,m),t(a,n),t(n,u),t(a,k)},d($){$&&e(a)}}}function jn(P){let a,m;return a=new X({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p:hn,i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function kn(P){let a,m;return a=new de({props:{$$slots:{default:[jn]},$$scope:{ctx:P}}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p(n,u){const k={};u&2&&(k.$$scope={dirty:u,ctx:n}),a.$set(k)},i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function wn(P){let a,m;return a=new X({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p:hn,i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function bn(P){let a,m;return a=new de({props:{$$slots:{default:[wn]},$$scope:{ctx:P}}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p(n,u){const k={};u&2&&(k.$$scope={dirty:u,ctx:n}),a.$set(k)},i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function xn(P){let a,m,n,u,k,$,w,A;return{c(){a=r("p"),m=l("If you aren\u2019t familiar with fine-tuning a model with the "),n=r("a"),u=l("Trainer"),k=l(", take a look at the basic tutorial "),$=r("a"),w=l("here"),A=l("!"),this.h()},l(C){a=i(C,"P",{});var q=p(a);m=o(q,"If you aren\u2019t familiar with fine-tuning a model with the "),n=i(q,"A",{href:!0});var F=p(n);u=o(F,"Trainer"),F.forEach(e),k=o(q,", take a look at the basic tutorial "),$=i(q,"A",{href:!0});var M=p($);w=o(M,"here"),M.forEach(e),A=o(q,"!"),q.forEach(e),this.h()},h(){j(n,"href","/docs/transformers/pr_18100/en/main_classes/trainer#transformers.Trainer"),j($,"href","../training#finetune-with-trainer")},m(C,q){f(C,a,q),t(a,m),t(a,n),t(n,u),t(a,k),t(a,$),t($,w),t(a,A)},d(C){C&&e(a)}}}function vn(P){let a,m,n,u,k,$,w,A,C,q,F,M,R,U,O,W,K,ss,ms,ts,L,G,os,es,J,us,S,I,V,N,_s,Y,Z,cs;return w=new X({props:{code:`from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=14)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">14</span>)`}}),C=new ue({props:{$$slots:{default:[xn]},$$scope:{ctx:P}}}),Z=new X({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut["train"],
    eval_dataset=tokenized_wnut["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_wnut[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){a=r("p"),m=l("Load DistilBERT with "),n=r("a"),u=l("AutoModelForTokenClassification"),k=l(" along with the number of expected labels:"),$=_(),x(w.$$.fragment),A=_(),x(C.$$.fragment),q=_(),F=r("p"),M=l("At this point, only three steps remain:"),R=_(),U=r("ol"),O=r("li"),W=l("Define your training hyperparameters in "),K=r("a"),ss=l("TrainingArguments"),ms=l("."),ts=_(),L=r("li"),G=l("Pass the training arguments to "),os=r("a"),es=l("Trainer"),J=l(" along with the model, dataset, tokenizer, and data collator."),us=_(),S=r("li"),I=l("Call "),V=r("a"),N=l("train()"),_s=l(" to fine-tune your model."),Y=_(),x(Z.$$.fragment),this.h()},l(d){a=i(d,"P",{});var D=p(a);m=o(D,"Load DistilBERT with "),n=i(D,"A",{href:!0});var H=p(n);u=o(H,"AutoModelForTokenClassification"),H.forEach(e),k=o(D," along with the number of expected labels:"),D.forEach(e),$=g(d),v(w.$$.fragment,d),A=g(d),v(C.$$.fragment,d),q=g(d),F=i(d,"P",{});var as=p(F);M=o(as,"At this point, only three steps remain:"),as.forEach(e),R=g(d),U=i(d,"OL",{});var B=p(U);O=i(B,"LI",{});var ns=p(O);W=o(ns,"Define your training hyperparameters in "),K=i(ns,"A",{href:!0});var rs=p(K);ss=o(rs,"TrainingArguments"),rs.forEach(e),ms=o(ns,"."),ns.forEach(e),ts=g(B),L=i(B,"LI",{});var ls=p(L);G=o(ls,"Pass the training arguments to "),os=i(ls,"A",{href:!0});var is=p(os);es=o(is,"Trainer"),is.forEach(e),J=o(ls," along with the model, dataset, tokenizer, and data collator."),ls.forEach(e),us=g(B),S=i(B,"LI",{});var fs=p(S);I=o(fs,"Call "),V=i(fs,"A",{href:!0});var Q=p(V);N=o(Q,"train()"),Q.forEach(e),_s=o(fs," to fine-tune your model."),fs.forEach(e),B.forEach(e),Y=g(d),v(Z.$$.fragment,d),this.h()},h(){j(n,"href","/docs/transformers/pr_18100/en/model_doc/auto#transformers.AutoModelForTokenClassification"),j(K,"href","/docs/transformers/pr_18100/en/main_classes/trainer#transformers.TrainingArguments"),j(os,"href","/docs/transformers/pr_18100/en/main_classes/trainer#transformers.Trainer"),j(V,"href","/docs/transformers/pr_18100/en/main_classes/trainer#transformers.Trainer.train")},m(d,D){f(d,a,D),t(a,m),t(a,n),t(n,u),t(a,k),f(d,$,D),E(w,d,D),f(d,A,D),E(C,d,D),f(d,q,D),f(d,F,D),t(F,M),f(d,R,D),f(d,U,D),t(U,O),t(O,W),t(O,K),t(K,ss),t(O,ms),t(U,ts),t(U,L),t(L,G),t(L,os),t(os,es),t(L,J),t(U,us),t(U,S),t(S,I),t(S,V),t(V,N),t(S,_s),f(d,Y,D),E(Z,d,D),cs=!0},p(d,D){const H={};D&2&&(H.$$scope={dirty:D,ctx:d}),C.$set(H)},i(d){cs||(y(w.$$.fragment,d),y(C.$$.fragment,d),y(Z.$$.fragment,d),cs=!0)},o(d){T(w.$$.fragment,d),T(C.$$.fragment,d),T(Z.$$.fragment,d),cs=!1},d(d){d&&e(a),d&&e($),z(w,d),d&&e(A),z(C,d),d&&e(q),d&&e(F),d&&e(R),d&&e(U),d&&e(Y),z(Z,d)}}}function En(P){let a,m;return a=new de({props:{$$slots:{default:[vn]},$$scope:{ctx:P}}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p(n,u){const k={};u&2&&(k.$$scope={dirty:u,ctx:n}),a.$set(k)},i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function yn(P){let a,m,n,u,k;return{c(){a=r("p"),m=l("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),n=r("a"),u=l("here"),k=l("!"),this.h()},l($){a=i($,"P",{});var w=p(a);m=o(w,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),n=i(w,"A",{href:!0});var A=p(n);u=o(A,"here"),A.forEach(e),k=o(w,"!"),w.forEach(e),this.h()},h(){j(n,"href","training#finetune-with-keras")},m($,w){f($,a,w),t(a,m),t(a,n),t(n,u),t(a,k)},d($){$&&e(a)}}}function Tn(P){let a,m,n,u,k,$,w,A,C,q,F,M,R,U,O,W,K,ss,ms,ts,L,G,os,es,J,us,S,I,V,N,_s,Y,Z,cs,d,D,H,as,B,ns,rs,ls,is,fs,Q,gs,js;return R=new X({props:{code:`tf_train_set = tokenized_wnut["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_wnut["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_wnut[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),O=new ue({props:{$$slots:{default:[yn]},$$scope:{ctx:P}}}),ts=new X({props:{code:`from transformers import create_optimizer

batch_size = 16
num_train_epochs = 3
num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_steps = (<span class="hljs-built_in">len</span>(tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_train_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, lr_schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_steps=num_train_steps,
<span class="hljs-meta">... </span>    weight_decay_rate=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>)`}}),I=new X({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),H=new X({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),gs=new X({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){a=r("p"),m=l("To fine-tune a model in TensorFlow, start by converting your datasets to the "),n=r("code"),u=l("tf.data.Dataset"),k=l(" format with "),$=r("a"),w=l("to_tf_dataset"),A=l(". Specify inputs and labels in "),C=r("code"),q=l("columns"),F=l(", whether to shuffle the dataset order, batch size, and the data collator:"),M=_(),x(R.$$.fragment),U=_(),x(O.$$.fragment),W=_(),K=r("p"),ss=l("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ms=_(),x(ts.$$.fragment),L=_(),G=r("p"),os=l("Load DistilBERT with "),es=r("a"),J=l("TFAutoModelForTokenClassification"),us=l(" along with the number of expected labels:"),S=_(),x(I.$$.fragment),V=_(),N=r("p"),_s=l("Configure the model for training with "),Y=r("a"),Z=r("code"),cs=l("compile"),d=l(":"),D=_(),x(H.$$.fragment),as=_(),B=r("p"),ns=l("Call "),rs=r("a"),ls=r("code"),is=l("fit"),fs=l(" to fine-tune the model:"),Q=_(),x(gs.$$.fragment),this.h()},l(c){a=i(c,"P",{});var b=p(a);m=o(b,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),n=i(b,"CODE",{});var ks=p(n);u=o(ks,"tf.data.Dataset"),ks.forEach(e),k=o(b," format with "),$=i(b,"A",{href:!0,rel:!0});var ws=p($);w=o(ws,"to_tf_dataset"),ws.forEach(e),A=o(b,". Specify inputs and labels in "),C=i(b,"CODE",{});var Us=p(C);q=o(Us,"columns"),Us.forEach(e),F=o(b,", whether to shuffle the dataset order, batch size, and the data collator:"),b.forEach(e),M=g(c),v(R.$$.fragment,c),U=g(c),v(O.$$.fragment,c),W=g(c),K=i(c,"P",{});var hs=p(K);ss=o(hs,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),hs.forEach(e),ms=g(c),v(ts.$$.fragment,c),L=g(c),G=i(c,"P",{});var qs=p(G);os=o(qs,"Load DistilBERT with "),es=i(qs,"A",{href:!0});var Cs=p(es);J=o(Cs,"TFAutoModelForTokenClassification"),Cs.forEach(e),us=o(qs," along with the number of expected labels:"),qs.forEach(e),S=g(c),v(I.$$.fragment,c),V=g(c),N=i(c,"P",{});var As=p(N);_s=o(As,"Configure the model for training with "),Y=i(As,"A",{href:!0,rel:!0});var nt=p(Y);Z=i(nt,"CODE",{});var Ds=p(Z);cs=o(Ds,"compile"),Ds.forEach(e),nt.forEach(e),d=o(As,":"),As.forEach(e),D=g(c),v(H.$$.fragment,c),as=g(c),B=i(c,"P",{});var Ps=p(B);ns=o(Ps,"Call "),rs=i(Ps,"A",{href:!0,rel:!0});var lt=p(rs);ls=i(lt,"CODE",{});var Ws=p(ls);is=o(Ws,"fit"),Ws.forEach(e),lt.forEach(e),fs=o(Ps," to fine-tune the model:"),Ps.forEach(e),Q=g(c),v(gs.$$.fragment,c),this.h()},h(){j($,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),j($,"rel","nofollow"),j(es,"href","/docs/transformers/pr_18100/en/model_doc/auto#transformers.TFAutoModelForTokenClassification"),j(Y,"href","https://keras.io/api/models/model_training_apis/#compile-method"),j(Y,"rel","nofollow"),j(rs,"href","https://keras.io/api/models/model_training_apis/#fit-method"),j(rs,"rel","nofollow")},m(c,b){f(c,a,b),t(a,m),t(a,n),t(n,u),t(a,k),t(a,$),t($,w),t(a,A),t(a,C),t(C,q),t(a,F),f(c,M,b),E(R,c,b),f(c,U,b),E(O,c,b),f(c,W,b),f(c,K,b),t(K,ss),f(c,ms,b),E(ts,c,b),f(c,L,b),f(c,G,b),t(G,os),t(G,es),t(es,J),t(G,us),f(c,S,b),E(I,c,b),f(c,V,b),f(c,N,b),t(N,_s),t(N,Y),t(Y,Z),t(Z,cs),t(N,d),f(c,D,b),E(H,c,b),f(c,as,b),f(c,B,b),t(B,ns),t(B,rs),t(rs,ls),t(ls,is),t(B,fs),f(c,Q,b),E(gs,c,b),js=!0},p(c,b){const ks={};b&2&&(ks.$$scope={dirty:b,ctx:c}),O.$set(ks)},i(c){js||(y(R.$$.fragment,c),y(O.$$.fragment,c),y(ts.$$.fragment,c),y(I.$$.fragment,c),y(H.$$.fragment,c),y(gs.$$.fragment,c),js=!0)},o(c){T(R.$$.fragment,c),T(O.$$.fragment,c),T(ts.$$.fragment,c),T(I.$$.fragment,c),T(H.$$.fragment,c),T(gs.$$.fragment,c),js=!1},d(c){c&&e(a),c&&e(M),z(R,c),c&&e(U),z(O,c),c&&e(W),c&&e(K),c&&e(ms),z(ts,c),c&&e(L),c&&e(G),c&&e(S),z(I,c),c&&e(V),c&&e(N),c&&e(D),z(H,c),c&&e(as),c&&e(B),c&&e(Q),z(gs,c)}}}function zn(P){let a,m;return a=new de({props:{$$slots:{default:[Tn]},$$scope:{ctx:P}}}),{c(){x(a.$$.fragment)},l(n){v(a.$$.fragment,n)},m(n,u){E(a,n,u),m=!0},p(n,u){const k={};u&2&&(k.$$scope={dirty:u,ctx:n}),a.$set(k)},i(n){m||(y(a.$$.fragment,n),m=!0)},o(n){T(a.$$.fragment,n),m=!1},d(n){z(a,n)}}}function qn(P){let a,m,n,u,k,$,w,A;return{c(){a=r("p"),m=l(`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),n=r("a"),u=l("PyTorch notebook"),k=l(`
or `),$=r("a"),w=l("TensorFlow notebook"),A=l("."),this.h()},l(C){a=i(C,"P",{});var q=p(a);m=o(q,`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),n=i(q,"A",{href:!0,rel:!0});var F=p(n);u=o(F,"PyTorch notebook"),F.forEach(e),k=o(q,`
or `),$=i(q,"A",{href:!0,rel:!0});var M=p($);w=o(M,"TensorFlow notebook"),M.forEach(e),A=o(q,"."),q.forEach(e),this.h()},h(){j(n,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),j(n,"rel","nofollow"),j($,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb"),j($,"rel","nofollow")},m(C,q){f(C,a,q),t(a,m),t(a,n),t(n,u),t(a,k),t(a,$),t($,w),t(a,A)},d(C){C&&e(a)}}}function Cn(P){let a,m,n,u,k,$,w,A,C,q,F,M,R,U,O,W,K,ss,ms,ts,L,G,os,es,J,us,S,I,V,N,_s,Y,Z,cs,d,D,H,as,B,ns,rs,ls,is,fs,Q,gs,js,c,b,ks,ws,Us,hs,qs,Cs,As,nt,Ds,Ps,lt,Ws,bs,ot,dt,_e,ge,$e,xs,_t,je,ke,gt,we,be,$t,xe,ve,Ee,rt,jt,ye,Te,Mt,Ts,Fs,kt,Ys,ze,wt,qe,Rt,Hs,Ut,Ss,Ce,bt,Ae,De,Wt,Ks,Yt,Os,Pe,xt,Fe,Se,Ht,Vs,Kt,vs,Oe,vt,Le,Ie,Et,Ne,Be,Vt,Es,Zs,Me,Gs,yt,Re,Ue,We,$s,Ye,Tt,He,Ke,zt,Ve,Ze,qt,Ge,Je,Qe,Js,Xe,Ct,sa,ta,Zt,it,ea,Gt,Qs,Jt,ds,aa,Xs,na,la,At,oa,ra,Dt,ia,pa,Qt,st,Xt,ps,ca,pt,fa,ha,Pt,ma,ua,Ft,da,_a,St,ga,$a,se,Ls,te,zs,Is,Ot,tt,ja,Lt,ka,ee,Ns,ae,Bs,ne;return $=new me({}),F=new cn({props:{id:"wVHdVlPScxA"}}),J=new ue({props:{$$slots:{default:[$n]},$$scope:{ctx:P}}}),N=new me({}),as=new X({props:{code:`from datasets import load_dataset

wnut = load_dataset("wnut_17")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>wnut = load_dataset(<span class="hljs-string">&quot;wnut_17&quot;</span>)`}}),is=new X({props:{code:'wnut["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>wnut[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,
 <span class="hljs-string">&#x27;ner_tags&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;tokens&#x27;</span>: [<span class="hljs-string">&#x27;@paulwalk&#x27;</span>, <span class="hljs-string">&#x27;It&#x27;</span>, <span class="hljs-string">&quot;&#x27;s&quot;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&quot;&#x27;m&quot;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Empire&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;Building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;ESB&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]
}`}}),ws=new X({props:{code:`label_list = wnut["train"].features[f"ner_tags"].feature.names
label_list`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>label_list = wnut[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">f&quot;ner_tags&quot;</span>].feature.names
<span class="hljs-meta">&gt;&gt;&gt; </span>label_list
[
    <span class="hljs-string">&quot;O&quot;</span>,
    <span class="hljs-string">&quot;B-corporation&quot;</span>,
    <span class="hljs-string">&quot;I-corporation&quot;</span>,
    <span class="hljs-string">&quot;B-creative-work&quot;</span>,
    <span class="hljs-string">&quot;I-creative-work&quot;</span>,
    <span class="hljs-string">&quot;B-group&quot;</span>,
    <span class="hljs-string">&quot;I-group&quot;</span>,
    <span class="hljs-string">&quot;B-location&quot;</span>,
    <span class="hljs-string">&quot;I-location&quot;</span>,
    <span class="hljs-string">&quot;B-person&quot;</span>,
    <span class="hljs-string">&quot;I-person&quot;</span>,
    <span class="hljs-string">&quot;B-product&quot;</span>,
    <span class="hljs-string">&quot;I-product&quot;</span>,
]`}}),Ys=new me({}),Hs=new cn({props:{id:"iY2AZYdZAr0"}}),Ks=new X({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Vs=new X({props:{code:`tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_input = tokenizer(example[<span class="hljs-string">&quot;tokens&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens
[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;paul&#x27;</span>, <span class="hljs-string">&#x27;##walk&#x27;</span>, <span class="hljs-string">&#x27;it&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;empire&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;es&#x27;</span>, <span class="hljs-string">&#x27;##b&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),Qs=new X({props:{code:`def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:  # Set the special tokens to -100.
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    tokenized_inputs = tokenizer(examples[<span class="hljs-string">&quot;tokens&quot;</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    labels = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f&quot;ner_tags&quot;</span>]):
<span class="hljs-meta">... </span>        word_ids = tokenized_inputs.word_ids(batch_index=i)  <span class="hljs-comment"># Map tokens to their respective word.</span>
<span class="hljs-meta">... </span>        previous_word_idx = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>        label_ids = []
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:  <span class="hljs-comment"># Set the special tokens to -100.</span>
<span class="hljs-meta">... </span>            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:  <span class="hljs-comment"># Only label the first token of a given word.</span>
<span class="hljs-meta">... </span>                label_ids.append(label[word_idx])
<span class="hljs-meta">... </span>            <span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            previous_word_idx = word_idx
<span class="hljs-meta">... </span>        labels.append(label_ids)

<span class="hljs-meta">... </span>    tokenized_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenized_inputs`}}),st=new X({props:{code:"tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_wnut = wnut.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)'}}),Ls=new fn({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[bn],pytorch:[kn]},$$scope:{ctx:P}}}),tt=new me({}),Ns=new fn({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[zn],pytorch:[En]},$$scope:{ctx:P}}}),Bs=new ue({props:{$$slots:{default:[qn]},$$scope:{ctx:P}}}),{c(){a=r("meta"),m=_(),n=r("h1"),u=r("a"),k=r("span"),x($.$$.fragment),w=_(),A=r("span"),C=l("Token classification"),q=_(),x(F.$$.fragment),M=_(),R=r("p"),U=l("Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."),O=_(),W=r("p"),K=l("This guide will show you how to fine-tune "),ss=r("a"),ms=l("DistilBERT"),ts=l(" on the "),L=r("a"),G=l("WNUT 17"),os=l(" dataset to detect new entities."),es=_(),x(J.$$.fragment),us=_(),S=r("h2"),I=r("a"),V=r("span"),x(N.$$.fragment),_s=_(),Y=r("span"),Z=l("Load WNUT 17 dataset"),cs=_(),d=r("p"),D=l("Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),H=_(),x(as.$$.fragment),B=_(),ns=r("p"),rs=l("Then take a look at an example:"),ls=_(),x(is.$$.fragment),fs=_(),Q=r("p"),gs=l("Each number in "),js=r("code"),c=l("ner_tags"),b=l(" represents an entity. Convert the number to a label name for more information:"),ks=_(),x(ws.$$.fragment),Us=_(),hs=r("p"),qs=l("The "),Cs=r("code"),As=l("ner_tag"),nt=l(" describes an entity, such as a corporation, location, or person. The letter that prefixes each "),Ds=r("code"),Ps=l("ner_tag"),lt=l(" indicates the token position of the entity:"),Ws=_(),bs=r("ul"),ot=r("li"),dt=r("code"),_e=l("B-"),ge=l(" indicates the beginning of an entity."),$e=_(),xs=r("li"),_t=r("code"),je=l("I-"),ke=l(" indicates a token is contained inside the same entity (e.g., the "),gt=r("code"),we=l("State"),be=l(` token is a part of an entity like
`),$t=r("code"),xe=l("Empire State Building"),ve=l(")."),Ee=_(),rt=r("li"),jt=r("code"),ye=l("0"),Te=l(" indicates the token doesn\u2019t correspond to any entity."),Mt=_(),Ts=r("h2"),Fs=r("a"),kt=r("span"),x(Ys.$$.fragment),ze=_(),wt=r("span"),qe=l("Preprocess"),Rt=_(),x(Hs.$$.fragment),Ut=_(),Ss=r("p"),Ce=l("Load the DistilBERT tokenizer to process the "),bt=r("code"),Ae=l("tokens"),De=l(":"),Wt=_(),x(Ks.$$.fragment),Yt=_(),Os=r("p"),Pe=l("Since the input has already been split into words, set "),xt=r("code"),Fe=l("is_split_into_words=True"),Se=l(" to tokenize the words into subwords:"),Ht=_(),x(Vs.$$.fragment),Kt=_(),vs=r("p"),Oe=l("Adding the special tokens "),vt=r("code"),Le=l("[CLS]"),Ie=l(" and "),Et=r("code"),Ne=l("[SEP]"),Be=l(" and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:"),Vt=_(),Es=r("ol"),Zs=r("li"),Me=l("Mapping all tokens to their corresponding word with the "),Gs=r("a"),yt=r("code"),Re=l("word_ids"),Ue=l(" method."),We=_(),$s=r("li"),Ye=l("Assigning the label "),Tt=r("code"),He=l("-100"),Ke=l(" to the special tokens "),zt=r("code"),Ve=l("[CLS]"),Ze=l(" and "),qt=r("code"),Ge=l("[SEP]"),Je=l(` so the PyTorch loss function ignores
them.`),Qe=_(),Js=r("li"),Xe=l("Only labeling the first token of a given word. Assign "),Ct=r("code"),sa=l("-100"),ta=l(" to other subtokens from the same word."),Zt=_(),it=r("p"),ea=l("Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT\u2019s maximum input length::"),Gt=_(),x(Qs.$$.fragment),Jt=_(),ds=r("p"),aa=l("Use \u{1F917} Datasets "),Xs=r("a"),na=l("map"),la=l(" function to tokenize and align the labels over the entire dataset. You can speed up the "),At=r("code"),oa=l("map"),ra=l(" function by setting "),Dt=r("code"),ia=l("batched=True"),pa=l(" to process multiple elements of the dataset at once:"),Qt=_(),x(st.$$.fragment),Xt=_(),ps=r("p"),ca=l("Use "),pt=r("a"),fa=l("DataCollatorForTokenClassification"),ha=l(" to create a batch of examples. It will also "),Pt=r("em"),ma=l("dynamically pad"),ua=l(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),Ft=r("code"),da=l("tokenizer"),_a=l(" function by setting "),St=r("code"),ga=l("padding=True"),$a=l(", dynamic padding is more efficient."),se=_(),x(Ls.$$.fragment),te=_(),zs=r("h2"),Is=r("a"),Ot=r("span"),x(tt.$$.fragment),ja=_(),Lt=r("span"),ka=l("Train"),ee=_(),x(Ns.$$.fragment),ae=_(),x(Bs.$$.fragment),this.h()},l(s){const h=_n('[data-svelte="svelte-1phssyn"]',document.head);a=i(h,"META",{name:!0,content:!0}),h.forEach(e),m=g(s),n=i(s,"H1",{class:!0});var et=p(n);u=i(et,"A",{id:!0,class:!0,href:!0});var It=p(u);k=i(It,"SPAN",{});var Nt=p(k);v($.$$.fragment,Nt),Nt.forEach(e),It.forEach(e),w=g(et),A=i(et,"SPAN",{});var Bt=p(A);C=o(Bt,"Token classification"),Bt.forEach(e),et.forEach(e),q=g(s),v(F.$$.fragment,s),M=g(s),R=i(s,"P",{});var xa=p(R);U=o(xa,"Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."),xa.forEach(e),O=g(s),W=i(s,"P",{});var ct=p(W);K=o(ct,"This guide will show you how to fine-tune "),ss=i(ct,"A",{href:!0,rel:!0});var va=p(ss);ms=o(va,"DistilBERT"),va.forEach(e),ts=o(ct," on the "),L=i(ct,"A",{href:!0,rel:!0});var Ea=p(L);G=o(Ea,"WNUT 17"),Ea.forEach(e),os=o(ct," dataset to detect new entities."),ct.forEach(e),es=g(s),v(J.$$.fragment,s),us=g(s),S=i(s,"H2",{class:!0});var le=p(S);I=i(le,"A",{id:!0,class:!0,href:!0});var ya=p(I);V=i(ya,"SPAN",{});var Ta=p(V);v(N.$$.fragment,Ta),Ta.forEach(e),ya.forEach(e),_s=g(le),Y=i(le,"SPAN",{});var za=p(Y);Z=o(za,"Load WNUT 17 dataset"),za.forEach(e),le.forEach(e),cs=g(s),d=i(s,"P",{});var qa=p(d);D=o(qa,"Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),qa.forEach(e),H=g(s),v(as.$$.fragment,s),B=g(s),ns=i(s,"P",{});var Ca=p(ns);rs=o(Ca,"Then take a look at an example:"),Ca.forEach(e),ls=g(s),v(is.$$.fragment,s),fs=g(s),Q=i(s,"P",{});var oe=p(Q);gs=o(oe,"Each number in "),js=i(oe,"CODE",{});var Aa=p(js);c=o(Aa,"ner_tags"),Aa.forEach(e),b=o(oe," represents an entity. Convert the number to a label name for more information:"),oe.forEach(e),ks=g(s),v(ws.$$.fragment,s),Us=g(s),hs=i(s,"P",{});var ft=p(hs);qs=o(ft,"The "),Cs=i(ft,"CODE",{});var Da=p(Cs);As=o(Da,"ner_tag"),Da.forEach(e),nt=o(ft," describes an entity, such as a corporation, location, or person. The letter that prefixes each "),Ds=i(ft,"CODE",{});var Pa=p(Ds);Ps=o(Pa,"ner_tag"),Pa.forEach(e),lt=o(ft," indicates the token position of the entity:"),ft.forEach(e),Ws=g(s),bs=i(s,"UL",{});var ht=p(bs);ot=i(ht,"LI",{});var wa=p(ot);dt=i(wa,"CODE",{});var Fa=p(dt);_e=o(Fa,"B-"),Fa.forEach(e),ge=o(wa," indicates the beginning of an entity."),wa.forEach(e),$e=g(ht),xs=i(ht,"LI",{});var at=p(xs);_t=i(at,"CODE",{});var Sa=p(_t);je=o(Sa,"I-"),Sa.forEach(e),ke=o(at," indicates a token is contained inside the same entity (e.g., the "),gt=i(at,"CODE",{});var Oa=p(gt);we=o(Oa,"State"),Oa.forEach(e),be=o(at,` token is a part of an entity like
`),$t=i(at,"CODE",{});var La=p($t);xe=o(La,"Empire State Building"),La.forEach(e),ve=o(at,")."),at.forEach(e),Ee=g(ht),rt=i(ht,"LI",{});var ba=p(rt);jt=i(ba,"CODE",{});var Ia=p(jt);ye=o(Ia,"0"),Ia.forEach(e),Te=o(ba," indicates the token doesn\u2019t correspond to any entity."),ba.forEach(e),ht.forEach(e),Mt=g(s),Ts=i(s,"H2",{class:!0});var re=p(Ts);Fs=i(re,"A",{id:!0,class:!0,href:!0});var Na=p(Fs);kt=i(Na,"SPAN",{});var Ba=p(kt);v(Ys.$$.fragment,Ba),Ba.forEach(e),Na.forEach(e),ze=g(re),wt=i(re,"SPAN",{});var Ma=p(wt);qe=o(Ma,"Preprocess"),Ma.forEach(e),re.forEach(e),Rt=g(s),v(Hs.$$.fragment,s),Ut=g(s),Ss=i(s,"P",{});var ie=p(Ss);Ce=o(ie,"Load the DistilBERT tokenizer to process the "),bt=i(ie,"CODE",{});var Ra=p(bt);Ae=o(Ra,"tokens"),Ra.forEach(e),De=o(ie,":"),ie.forEach(e),Wt=g(s),v(Ks.$$.fragment,s),Yt=g(s),Os=i(s,"P",{});var pe=p(Os);Pe=o(pe,"Since the input has already been split into words, set "),xt=i(pe,"CODE",{});var Ua=p(xt);Fe=o(Ua,"is_split_into_words=True"),Ua.forEach(e),Se=o(pe," to tokenize the words into subwords:"),pe.forEach(e),Ht=g(s),v(Vs.$$.fragment,s),Kt=g(s),vs=i(s,"P",{});var mt=p(vs);Oe=o(mt,"Adding the special tokens "),vt=i(mt,"CODE",{});var Wa=p(vt);Le=o(Wa,"[CLS]"),Wa.forEach(e),Ie=o(mt," and "),Et=i(mt,"CODE",{});var Ya=p(Et);Ne=o(Ya,"[SEP]"),Ya.forEach(e),Be=o(mt," and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:"),mt.forEach(e),Vt=g(s),Es=i(s,"OL",{});var ut=p(Es);Zs=i(ut,"LI",{});var ce=p(Zs);Me=o(ce,"Mapping all tokens to their corresponding word with the "),Gs=i(ce,"A",{href:!0,rel:!0});var Ha=p(Gs);yt=i(Ha,"CODE",{});var Ka=p(yt);Re=o(Ka,"word_ids"),Ka.forEach(e),Ha.forEach(e),Ue=o(ce," method."),ce.forEach(e),We=g(ut),$s=i(ut,"LI",{});var Ms=p($s);Ye=o(Ms,"Assigning the label "),Tt=i(Ms,"CODE",{});var Va=p(Tt);He=o(Va,"-100"),Va.forEach(e),Ke=o(Ms," to the special tokens "),zt=i(Ms,"CODE",{});var Za=p(zt);Ve=o(Za,"[CLS]"),Za.forEach(e),Ze=o(Ms," and "),qt=i(Ms,"CODE",{});var Ga=p(qt);Ge=o(Ga,"[SEP]"),Ga.forEach(e),Je=o(Ms,` so the PyTorch loss function ignores
them.`),Ms.forEach(e),Qe=g(ut),Js=i(ut,"LI",{});var fe=p(Js);Xe=o(fe,"Only labeling the first token of a given word. Assign "),Ct=i(fe,"CODE",{});var Ja=p(Ct);sa=o(Ja,"-100"),Ja.forEach(e),ta=o(fe," to other subtokens from the same word."),fe.forEach(e),ut.forEach(e),Zt=g(s),it=i(s,"P",{});var Qa=p(it);ea=o(Qa,"Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT\u2019s maximum input length::"),Qa.forEach(e),Gt=g(s),v(Qs.$$.fragment,s),Jt=g(s),ds=i(s,"P",{});var Rs=p(ds);aa=o(Rs,"Use \u{1F917} Datasets "),Xs=i(Rs,"A",{href:!0,rel:!0});var Xa=p(Xs);na=o(Xa,"map"),Xa.forEach(e),la=o(Rs," function to tokenize and align the labels over the entire dataset. You can speed up the "),At=i(Rs,"CODE",{});var sn=p(At);oa=o(sn,"map"),sn.forEach(e),ra=o(Rs," function by setting "),Dt=i(Rs,"CODE",{});var tn=p(Dt);ia=o(tn,"batched=True"),tn.forEach(e),pa=o(Rs," to process multiple elements of the dataset at once:"),Rs.forEach(e),Qt=g(s),v(st.$$.fragment,s),Xt=g(s),ps=i(s,"P",{});var ys=p(ps);ca=o(ys,"Use "),pt=i(ys,"A",{href:!0});var en=p(pt);fa=o(en,"DataCollatorForTokenClassification"),en.forEach(e),ha=o(ys," to create a batch of examples. It will also "),Pt=i(ys,"EM",{});var an=p(Pt);ma=o(an,"dynamically pad"),an.forEach(e),ua=o(ys," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),Ft=i(ys,"CODE",{});var nn=p(Ft);da=o(nn,"tokenizer"),nn.forEach(e),_a=o(ys," function by setting "),St=i(ys,"CODE",{});var ln=p(St);ga=o(ln,"padding=True"),ln.forEach(e),$a=o(ys,", dynamic padding is more efficient."),ys.forEach(e),se=g(s),v(Ls.$$.fragment,s),te=g(s),zs=i(s,"H2",{class:!0});var he=p(zs);Is=i(he,"A",{id:!0,class:!0,href:!0});var on=p(Is);Ot=i(on,"SPAN",{});var rn=p(Ot);v(tt.$$.fragment,rn),rn.forEach(e),on.forEach(e),ja=g(he),Lt=i(he,"SPAN",{});var pn=p(Lt);ka=o(pn,"Train"),pn.forEach(e),he.forEach(e),ee=g(s),v(Ns.$$.fragment,s),ae=g(s),v(Bs.$$.fragment,s),this.h()},h(){j(a,"name","hf:doc:metadata"),j(a,"content",JSON.stringify(An)),j(u,"id","token-classification"),j(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(u,"href","#token-classification"),j(n,"class","relative group"),j(ss,"href","https://huggingface.co/distilbert-base-uncased"),j(ss,"rel","nofollow"),j(L,"href","https://huggingface.co/datasets/wnut_17"),j(L,"rel","nofollow"),j(I,"id","load-wnut-17-dataset"),j(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(I,"href","#load-wnut-17-dataset"),j(S,"class","relative group"),j(Fs,"id","preprocess"),j(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Fs,"href","#preprocess"),j(Ts,"class","relative group"),j(Gs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.Encoding.word_ids"),j(Gs,"rel","nofollow"),j(Xs,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),j(Xs,"rel","nofollow"),j(pt,"href","/docs/transformers/pr_18100/en/main_classes/data_collator#transformers.DataCollatorForTokenClassification"),j(Is,"id","train"),j(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Is,"href","#train"),j(zs,"class","relative group")},m(s,h){t(document.head,a),f(s,m,h),f(s,n,h),t(n,u),t(u,k),E($,k,null),t(n,w),t(n,A),t(A,C),f(s,q,h),E(F,s,h),f(s,M,h),f(s,R,h),t(R,U),f(s,O,h),f(s,W,h),t(W,K),t(W,ss),t(ss,ms),t(W,ts),t(W,L),t(L,G),t(W,os),f(s,es,h),E(J,s,h),f(s,us,h),f(s,S,h),t(S,I),t(I,V),E(N,V,null),t(S,_s),t(S,Y),t(Y,Z),f(s,cs,h),f(s,d,h),t(d,D),f(s,H,h),E(as,s,h),f(s,B,h),f(s,ns,h),t(ns,rs),f(s,ls,h),E(is,s,h),f(s,fs,h),f(s,Q,h),t(Q,gs),t(Q,js),t(js,c),t(Q,b),f(s,ks,h),E(ws,s,h),f(s,Us,h),f(s,hs,h),t(hs,qs),t(hs,Cs),t(Cs,As),t(hs,nt),t(hs,Ds),t(Ds,Ps),t(hs,lt),f(s,Ws,h),f(s,bs,h),t(bs,ot),t(ot,dt),t(dt,_e),t(ot,ge),t(bs,$e),t(bs,xs),t(xs,_t),t(_t,je),t(xs,ke),t(xs,gt),t(gt,we),t(xs,be),t(xs,$t),t($t,xe),t(xs,ve),t(bs,Ee),t(bs,rt),t(rt,jt),t(jt,ye),t(rt,Te),f(s,Mt,h),f(s,Ts,h),t(Ts,Fs),t(Fs,kt),E(Ys,kt,null),t(Ts,ze),t(Ts,wt),t(wt,qe),f(s,Rt,h),E(Hs,s,h),f(s,Ut,h),f(s,Ss,h),t(Ss,Ce),t(Ss,bt),t(bt,Ae),t(Ss,De),f(s,Wt,h),E(Ks,s,h),f(s,Yt,h),f(s,Os,h),t(Os,Pe),t(Os,xt),t(xt,Fe),t(Os,Se),f(s,Ht,h),E(Vs,s,h),f(s,Kt,h),f(s,vs,h),t(vs,Oe),t(vs,vt),t(vt,Le),t(vs,Ie),t(vs,Et),t(Et,Ne),t(vs,Be),f(s,Vt,h),f(s,Es,h),t(Es,Zs),t(Zs,Me),t(Zs,Gs),t(Gs,yt),t(yt,Re),t(Zs,Ue),t(Es,We),t(Es,$s),t($s,Ye),t($s,Tt),t(Tt,He),t($s,Ke),t($s,zt),t(zt,Ve),t($s,Ze),t($s,qt),t(qt,Ge),t($s,Je),t(Es,Qe),t(Es,Js),t(Js,Xe),t(Js,Ct),t(Ct,sa),t(Js,ta),f(s,Zt,h),f(s,it,h),t(it,ea),f(s,Gt,h),E(Qs,s,h),f(s,Jt,h),f(s,ds,h),t(ds,aa),t(ds,Xs),t(Xs,na),t(ds,la),t(ds,At),t(At,oa),t(ds,ra),t(ds,Dt),t(Dt,ia),t(ds,pa),f(s,Qt,h),E(st,s,h),f(s,Xt,h),f(s,ps,h),t(ps,ca),t(ps,pt),t(pt,fa),t(ps,ha),t(ps,Pt),t(Pt,ma),t(ps,ua),t(ps,Ft),t(Ft,da),t(ps,_a),t(ps,St),t(St,ga),t(ps,$a),f(s,se,h),E(Ls,s,h),f(s,te,h),f(s,zs,h),t(zs,Is),t(Is,Ot),E(tt,Ot,null),t(zs,ja),t(zs,Lt),t(Lt,ka),f(s,ee,h),E(Ns,s,h),f(s,ae,h),E(Bs,s,h),ne=!0},p(s,[h]){const et={};h&2&&(et.$$scope={dirty:h,ctx:s}),J.$set(et);const It={};h&2&&(It.$$scope={dirty:h,ctx:s}),Ls.$set(It);const Nt={};h&2&&(Nt.$$scope={dirty:h,ctx:s}),Ns.$set(Nt);const Bt={};h&2&&(Bt.$$scope={dirty:h,ctx:s}),Bs.$set(Bt)},i(s){ne||(y($.$$.fragment,s),y(F.$$.fragment,s),y(J.$$.fragment,s),y(N.$$.fragment,s),y(as.$$.fragment,s),y(is.$$.fragment,s),y(ws.$$.fragment,s),y(Ys.$$.fragment,s),y(Hs.$$.fragment,s),y(Ks.$$.fragment,s),y(Vs.$$.fragment,s),y(Qs.$$.fragment,s),y(st.$$.fragment,s),y(Ls.$$.fragment,s),y(tt.$$.fragment,s),y(Ns.$$.fragment,s),y(Bs.$$.fragment,s),ne=!0)},o(s){T($.$$.fragment,s),T(F.$$.fragment,s),T(J.$$.fragment,s),T(N.$$.fragment,s),T(as.$$.fragment,s),T(is.$$.fragment,s),T(ws.$$.fragment,s),T(Ys.$$.fragment,s),T(Hs.$$.fragment,s),T(Ks.$$.fragment,s),T(Vs.$$.fragment,s),T(Qs.$$.fragment,s),T(st.$$.fragment,s),T(Ls.$$.fragment,s),T(tt.$$.fragment,s),T(Ns.$$.fragment,s),T(Bs.$$.fragment,s),ne=!1},d(s){e(a),s&&e(m),s&&e(n),z($),s&&e(q),z(F,s),s&&e(M),s&&e(R),s&&e(O),s&&e(W),s&&e(es),z(J,s),s&&e(us),s&&e(S),z(N),s&&e(cs),s&&e(d),s&&e(H),z(as,s),s&&e(B),s&&e(ns),s&&e(ls),z(is,s),s&&e(fs),s&&e(Q),s&&e(ks),z(ws,s),s&&e(Us),s&&e(hs),s&&e(Ws),s&&e(bs),s&&e(Mt),s&&e(Ts),z(Ys),s&&e(Rt),z(Hs,s),s&&e(Ut),s&&e(Ss),s&&e(Wt),z(Ks,s),s&&e(Yt),s&&e(Os),s&&e(Ht),z(Vs,s),s&&e(Kt),s&&e(vs),s&&e(Vt),s&&e(Es),s&&e(Zt),s&&e(it),s&&e(Gt),z(Qs,s),s&&e(Jt),s&&e(ds),s&&e(Qt),z(st,s),s&&e(Xt),s&&e(ps),s&&e(se),z(Ls,s),s&&e(te),s&&e(zs),z(tt),s&&e(ee),z(Ns,s),s&&e(ae),z(Bs,s)}}}const An={local:"token-classification",sections:[{local:"load-wnut-17-dataset",title:"Load WNUT 17 dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Token classification"};function Dn(P){return gn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nn extends mn{constructor(a){super();un(this,a,Dn,Cn,dn,{})}}export{Nn as default,An as metadata};
