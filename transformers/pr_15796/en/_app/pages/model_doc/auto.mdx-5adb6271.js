import{S as u_t,i as b_t,s as v_t,e as a,k as l,w as f,t as o,L as T_t,c as n,d as t,m as i,a as s,x as m,h as r,b as c,J as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-9e2b328e.js";import{T as _3r}from"../../chunks/Tip-76f97a76.js";import{D as E}from"../../chunks/Docstring-50fd6873.js";import{C as w}from"../../chunks/CodeBlock-88e23343.js";import{I as z}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function F_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,o4,yf,ye,io,Bi,Pn,r4,$n,In,t4,ki,jn,a4,xi,wf,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),o4=o(")."),yf=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),r4=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),t4=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),a4=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var D0=s(ie);me=r(D0,"NewModelConfig"),D0.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);ue=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var q0=s(wi);Ef=r(q0,"model_type"),q0.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var G0=s(Ai);Li=r(G0,'"new-model"'),G0.forEach(t),o4=r(ge,")."),ge.forEach(t),yf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),r4=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var O0=s($n);In=r(O0,"PreTrainedModel"),O0.forEach(t),t4=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),a4=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var X0=s(xi);wf=r(X0,"NewModelConfig"),X0.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,o4),b(co,yf,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Pn),e(ye,r4),e(ye,$n),e($n,In),e(ye,t4),e(ye,ki),e(ki,jn),e(ye,a4),e(ye,xi),e(xi,wf),e(ye,$a)},d(co){co&&t(J),co&&t(yf),co&&t(ye)}}}function C_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function M_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function E_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,o4,yf,ye,io,Bi,Pn,r4,$n,In,t4,ki,jn,a4,xi,wf,$a,co,ge,D0,Ri,q0,G0,qo,Ia,O0,Af,X0,mxe,tLe,Si,Lf,$V,n4,gxe,IV,hxe,aLe,Nn,pxe,jV,_xe,uxe,NV,bxe,vxe,nLe,s4,sLe,z0,Txe,lLe,Bf,iLe,Pi,kf,DV,l4,Fxe,qV,Cxe,dLe,Go,i4,Mxe,d4,Exe,V0,yxe,wxe,Axe,c4,Lxe,GV,Bxe,kxe,xxe,fo,f4,Rxe,OV,Sxe,Pxe,$i,$xe,XV,Ixe,jxe,zV,Nxe,Dxe,qxe,v,xf,VV,Gxe,Oxe,W0,Xxe,zxe,Vxe,Rf,WV,Wxe,Qxe,Q0,Hxe,Uxe,Jxe,Sf,QV,Yxe,Kxe,H0,Zxe,eRe,oRe,Pf,HV,rRe,tRe,U0,aRe,nRe,sRe,$f,UV,lRe,iRe,J0,dRe,cRe,fRe,If,JV,mRe,gRe,Y0,hRe,pRe,_Re,jf,YV,uRe,bRe,K0,vRe,TRe,FRe,Nf,KV,CRe,MRe,Z0,ERe,yRe,wRe,Df,ZV,ARe,LRe,eL,BRe,kRe,xRe,qf,eW,RRe,SRe,oL,PRe,$Re,IRe,Gf,oW,jRe,NRe,rL,DRe,qRe,GRe,Of,rW,ORe,XRe,tL,zRe,VRe,WRe,Xf,tW,QRe,HRe,aL,URe,JRe,YRe,zf,aW,KRe,ZRe,nL,eSe,oSe,rSe,Vf,nW,tSe,aSe,sL,nSe,sSe,lSe,Wf,sW,iSe,dSe,lL,cSe,fSe,mSe,Qf,lW,gSe,hSe,iL,pSe,_Se,uSe,Hf,iW,bSe,vSe,dL,TSe,FSe,CSe,Uf,dW,MSe,ESe,cL,ySe,wSe,ASe,Jf,cW,LSe,BSe,fL,kSe,xSe,RSe,Yf,fW,SSe,PSe,mL,$Se,ISe,jSe,Kf,mW,NSe,DSe,gL,qSe,GSe,OSe,Zf,gW,XSe,zSe,hL,VSe,WSe,QSe,em,hW,HSe,USe,pL,JSe,YSe,KSe,om,pW,ZSe,ePe,_L,oPe,rPe,tPe,rm,_W,aPe,nPe,uL,sPe,lPe,iPe,tm,uW,dPe,cPe,bL,fPe,mPe,gPe,am,bW,hPe,pPe,vL,_Pe,uPe,bPe,nm,vW,vPe,TPe,TL,FPe,CPe,MPe,sm,TW,EPe,yPe,FL,wPe,APe,LPe,lm,FW,BPe,kPe,CL,xPe,RPe,SPe,im,CW,PPe,$Pe,ML,IPe,jPe,NPe,dm,MW,DPe,qPe,EL,GPe,OPe,XPe,cm,EW,zPe,VPe,yL,WPe,QPe,HPe,fm,yW,UPe,JPe,wL,YPe,KPe,ZPe,mm,wW,e$e,o$e,AL,r$e,t$e,a$e,gm,AW,n$e,s$e,LL,l$e,i$e,d$e,hm,LW,c$e,f$e,BL,m$e,g$e,h$e,pm,BW,p$e,_$e,kL,u$e,b$e,v$e,_m,kW,T$e,F$e,xL,C$e,M$e,E$e,um,xW,y$e,w$e,RL,A$e,L$e,B$e,bm,RW,k$e,x$e,SL,R$e,S$e,P$e,vm,SW,$$e,I$e,PL,j$e,N$e,D$e,Tm,PW,q$e,G$e,$L,O$e,X$e,z$e,Fm,$W,V$e,W$e,IL,Q$e,H$e,U$e,Cm,IW,J$e,Y$e,jL,K$e,Z$e,eIe,Mm,jW,oIe,rIe,NL,tIe,aIe,nIe,Em,NW,sIe,lIe,DL,iIe,dIe,cIe,ym,DW,fIe,mIe,qL,gIe,hIe,pIe,wm,qW,_Ie,uIe,GL,bIe,vIe,TIe,Am,GW,FIe,CIe,OL,MIe,EIe,yIe,Lm,OW,wIe,AIe,XL,LIe,BIe,kIe,Bm,XW,xIe,RIe,zL,SIe,PIe,$Ie,km,zW,IIe,jIe,VL,NIe,DIe,qIe,xm,VW,GIe,OIe,WL,XIe,zIe,VIe,Rm,WW,WIe,QIe,QL,HIe,UIe,JIe,Sm,QW,YIe,KIe,HL,ZIe,eje,oje,Pm,HW,rje,tje,UL,aje,nje,sje,$m,UW,lje,ije,JL,dje,cje,fje,Im,JW,mje,gje,YL,hje,pje,_je,jm,YW,uje,bje,KL,vje,Tje,Fje,Nm,KW,Cje,Mje,ZL,Eje,yje,wje,Dm,ZW,Aje,Lje,e8,Bje,kje,xje,qm,eQ,Rje,Sje,o8,Pje,$je,Ije,Gm,oQ,jje,Nje,r8,Dje,qje,Gje,Om,rQ,Oje,Xje,t8,zje,Vje,Wje,Xm,tQ,Qje,Hje,a8,Uje,Jje,Yje,zm,aQ,Kje,Zje,n8,eNe,oNe,rNe,Vm,nQ,tNe,aNe,s8,nNe,sNe,lNe,Wm,sQ,iNe,dNe,l8,cNe,fNe,mNe,Qm,lQ,gNe,hNe,i8,pNe,_Ne,uNe,Hm,iQ,bNe,vNe,d8,TNe,FNe,CNe,Um,dQ,MNe,ENe,c8,yNe,wNe,ANe,Jm,cQ,LNe,BNe,f8,kNe,xNe,RNe,Ym,fQ,SNe,PNe,m8,$Ne,INe,jNe,Km,mQ,NNe,DNe,g8,qNe,GNe,ONe,Zm,gQ,XNe,zNe,h8,VNe,WNe,QNe,eg,hQ,HNe,UNe,p8,JNe,YNe,KNe,og,pQ,ZNe,eDe,_8,oDe,rDe,tDe,rg,_Q,aDe,nDe,u8,sDe,lDe,iDe,tg,uQ,dDe,cDe,b8,fDe,mDe,gDe,ag,bQ,hDe,pDe,v8,_De,uDe,bDe,ng,vQ,vDe,TDe,T8,FDe,CDe,MDe,sg,TQ,EDe,yDe,F8,wDe,ADe,LDe,lg,FQ,BDe,kDe,C8,xDe,RDe,SDe,ig,CQ,PDe,$De,M8,IDe,jDe,NDe,dg,MQ,DDe,qDe,E8,GDe,ODe,XDe,cg,EQ,zDe,VDe,y8,WDe,QDe,HDe,fg,yQ,UDe,JDe,w8,YDe,KDe,ZDe,mg,wQ,eqe,oqe,A8,rqe,tqe,aqe,gg,AQ,nqe,sqe,L8,lqe,iqe,dqe,LQ,cqe,fqe,m4,mqe,hg,g4,gqe,BQ,hqe,cLe,Ii,pg,kQ,h4,pqe,xQ,_qe,fLe,Oo,p4,uqe,_4,bqe,B8,vqe,Tqe,Fqe,u4,Cqe,RQ,Mqe,Eqe,yqe,mo,b4,wqe,SQ,Aqe,Lqe,ja,Bqe,PQ,kqe,xqe,$Q,Rqe,Sqe,IQ,Pqe,$qe,Iqe,M,Dn,jQ,jqe,Nqe,k8,Dqe,qqe,x8,Gqe,Oqe,Xqe,qn,NQ,zqe,Vqe,R8,Wqe,Qqe,S8,Hqe,Uqe,Jqe,Gn,DQ,Yqe,Kqe,P8,Zqe,eGe,$8,oGe,rGe,tGe,_g,qQ,aGe,nGe,I8,sGe,lGe,iGe,On,GQ,dGe,cGe,j8,fGe,mGe,N8,gGe,hGe,pGe,ug,OQ,_Ge,uGe,D8,bGe,vGe,TGe,bg,XQ,FGe,CGe,q8,MGe,EGe,yGe,vg,zQ,wGe,AGe,G8,LGe,BGe,kGe,Xn,VQ,xGe,RGe,O8,SGe,PGe,X8,$Ge,IGe,jGe,zn,WQ,NGe,DGe,z8,qGe,GGe,V8,OGe,XGe,zGe,Vn,QQ,VGe,WGe,W8,QGe,HGe,Q8,UGe,JGe,YGe,Tg,HQ,KGe,ZGe,H8,eOe,oOe,rOe,Fg,UQ,tOe,aOe,U8,nOe,sOe,lOe,Wn,JQ,iOe,dOe,J8,cOe,fOe,Y8,mOe,gOe,hOe,Cg,YQ,pOe,_Oe,K8,uOe,bOe,vOe,Qn,KQ,TOe,FOe,Z8,COe,MOe,eB,EOe,yOe,wOe,Hn,ZQ,AOe,LOe,oB,BOe,kOe,rB,xOe,ROe,SOe,Un,eH,POe,$Oe,tB,IOe,jOe,oH,NOe,DOe,qOe,Mg,rH,GOe,OOe,aB,XOe,zOe,VOe,Jn,tH,WOe,QOe,nB,HOe,UOe,sB,JOe,YOe,KOe,Eg,aH,ZOe,eXe,lB,oXe,rXe,tXe,Yn,nH,aXe,nXe,iB,sXe,lXe,dB,iXe,dXe,cXe,Kn,sH,fXe,mXe,cB,gXe,hXe,fB,pXe,_Xe,uXe,Zn,lH,bXe,vXe,mB,TXe,FXe,gB,CXe,MXe,EXe,yg,iH,yXe,wXe,hB,AXe,LXe,BXe,es,dH,kXe,xXe,pB,RXe,SXe,_B,PXe,$Xe,IXe,wg,cH,jXe,NXe,uB,DXe,qXe,GXe,os,fH,OXe,XXe,bB,zXe,VXe,vB,WXe,QXe,HXe,rs,mH,UXe,JXe,TB,YXe,KXe,FB,ZXe,eze,oze,ts,gH,rze,tze,CB,aze,nze,MB,sze,lze,ize,as,hH,dze,cze,EB,fze,mze,yB,gze,hze,pze,Ag,pH,_ze,uze,wB,bze,vze,Tze,ns,_H,Fze,Cze,AB,Mze,Eze,LB,yze,wze,Aze,ss,uH,Lze,Bze,BB,kze,xze,kB,Rze,Sze,Pze,ls,bH,$ze,Ize,xB,jze,Nze,RB,Dze,qze,Gze,is,vH,Oze,Xze,SB,zze,Vze,PB,Wze,Qze,Hze,ds,TH,Uze,Jze,$B,Yze,Kze,IB,Zze,eVe,oVe,cs,FH,rVe,tVe,jB,aVe,nVe,NB,sVe,lVe,iVe,Lg,CH,dVe,cVe,DB,fVe,mVe,gVe,fs,MH,hVe,pVe,qB,_Ve,uVe,GB,bVe,vVe,TVe,Bg,EH,FVe,CVe,OB,MVe,EVe,yVe,kg,yH,wVe,AVe,XB,LVe,BVe,kVe,ms,wH,xVe,RVe,zB,SVe,PVe,VB,$Ve,IVe,jVe,gs,AH,NVe,DVe,WB,qVe,GVe,QB,OVe,XVe,zVe,xg,LH,VVe,WVe,HB,QVe,HVe,UVe,hs,BH,JVe,YVe,UB,KVe,ZVe,JB,eWe,oWe,rWe,ps,kH,tWe,aWe,YB,nWe,sWe,KB,lWe,iWe,dWe,_s,xH,cWe,fWe,ZB,mWe,gWe,ek,hWe,pWe,_We,us,RH,uWe,bWe,ok,vWe,TWe,rk,FWe,CWe,MWe,bs,SH,EWe,yWe,tk,wWe,AWe,ak,LWe,BWe,kWe,Rg,PH,xWe,RWe,nk,SWe,PWe,$We,Sg,$H,IWe,jWe,sk,NWe,DWe,qWe,Pg,IH,GWe,OWe,lk,XWe,zWe,VWe,$g,jH,WWe,QWe,ik,HWe,UWe,JWe,vs,NH,YWe,KWe,dk,ZWe,eQe,ck,oQe,rQe,tQe,Ig,DH,aQe,nQe,fk,sQe,lQe,iQe,Ts,qH,dQe,cQe,mk,fQe,mQe,gk,gQe,hQe,pQe,Fs,GH,_Qe,uQe,hk,bQe,vQe,pk,TQe,FQe,CQe,Cs,OH,MQe,EQe,_k,yQe,wQe,uk,AQe,LQe,BQe,Ms,XH,kQe,xQe,bk,RQe,SQe,vk,PQe,$Qe,IQe,Es,zH,jQe,NQe,Tk,DQe,qQe,Fk,GQe,OQe,XQe,jg,VH,zQe,VQe,Ck,WQe,QQe,HQe,Ng,WH,UQe,JQe,Mk,YQe,KQe,ZQe,ys,QH,eHe,oHe,Ek,rHe,tHe,yk,aHe,nHe,sHe,ws,HH,lHe,iHe,wk,dHe,cHe,Ak,fHe,mHe,gHe,As,UH,hHe,pHe,Lk,_He,uHe,Bk,bHe,vHe,THe,Dg,JH,FHe,CHe,kk,MHe,EHe,yHe,qg,YH,wHe,AHe,xk,LHe,BHe,kHe,Gg,KH,xHe,RHe,Rk,SHe,PHe,$He,Og,ZH,IHe,jHe,Sk,NHe,DHe,qHe,Ls,eU,GHe,OHe,Pk,XHe,zHe,$k,VHe,WHe,QHe,Xg,oU,HHe,UHe,Ik,JHe,YHe,KHe,zg,rU,ZHe,eUe,jk,oUe,rUe,tUe,Bs,tU,aUe,nUe,Nk,sUe,lUe,Dk,iUe,dUe,cUe,ks,aU,fUe,mUe,qk,gUe,hUe,Gk,pUe,_Ue,uUe,nU,bUe,vUe,v4,TUe,Vg,T4,FUe,sU,CUe,mLe,ji,Wg,lU,F4,MUe,iU,EUe,gLe,Xo,C4,yUe,M4,wUe,Ok,AUe,LUe,BUe,E4,kUe,dU,xUe,RUe,SUe,Le,y4,PUe,cU,$Ue,IUe,Na,jUe,fU,NUe,DUe,mU,qUe,GUe,gU,OUe,XUe,zUe,se,Qg,hU,VUe,WUe,Xk,QUe,HUe,UUe,Hg,pU,JUe,YUe,zk,KUe,ZUe,eJe,Ug,_U,oJe,rJe,Vk,tJe,aJe,nJe,Jg,uU,sJe,lJe,Wk,iJe,dJe,cJe,Yg,bU,fJe,mJe,Qk,gJe,hJe,pJe,Kg,vU,_Je,uJe,Hk,bJe,vJe,TJe,Zg,TU,FJe,CJe,Uk,MJe,EJe,yJe,eh,FU,wJe,AJe,Jk,LJe,BJe,kJe,oh,CU,xJe,RJe,Yk,SJe,PJe,$Je,rh,MU,IJe,jJe,Kk,NJe,DJe,qJe,th,EU,GJe,OJe,Zk,XJe,zJe,VJe,ah,yU,WJe,QJe,ex,HJe,UJe,JJe,nh,wU,YJe,KJe,ox,ZJe,eYe,oYe,sh,AU,rYe,tYe,rx,aYe,nYe,sYe,lh,LU,lYe,iYe,tx,dYe,cYe,fYe,ih,mYe,BU,gYe,hYe,w4,pYe,dh,A4,_Ye,kU,uYe,hLe,Ni,ch,xU,L4,bYe,RU,vYe,pLe,zo,B4,TYe,k4,FYe,ax,CYe,MYe,EYe,x4,yYe,SU,wYe,AYe,LYe,Be,R4,BYe,PU,kYe,xYe,Di,RYe,$U,SYe,PYe,IU,$Ye,IYe,jYe,we,fh,jU,NYe,DYe,nx,qYe,GYe,OYe,mh,NU,XYe,zYe,sx,VYe,WYe,QYe,gh,DU,HYe,UYe,lx,JYe,YYe,KYe,hh,qU,ZYe,eKe,ix,oKe,rKe,tKe,ph,GU,aKe,nKe,dx,sKe,lKe,iKe,_h,OU,dKe,cKe,cx,fKe,mKe,gKe,uh,XU,hKe,pKe,fx,_Ke,uKe,bKe,bh,zU,vKe,TKe,mx,FKe,CKe,MKe,vh,EKe,VU,yKe,wKe,S4,AKe,Th,P4,LKe,WU,BKe,_Le,qi,Fh,QU,$4,kKe,HU,xKe,uLe,Vo,I4,RKe,Gi,SKe,UU,PKe,$Ke,JU,IKe,jKe,NKe,j4,DKe,YU,qKe,GKe,OKe,Nr,N4,XKe,KU,zKe,VKe,Oi,WKe,ZU,QKe,HKe,eJ,UKe,JKe,YKe,oJ,KKe,ZKe,D4,eZe,ke,q4,oZe,rJ,rZe,tZe,Da,aZe,tJ,nZe,sZe,aJ,lZe,iZe,nJ,dZe,cZe,fZe,F,Ch,sJ,mZe,gZe,gx,hZe,pZe,_Ze,Mh,lJ,uZe,bZe,hx,vZe,TZe,FZe,Eh,iJ,CZe,MZe,px,EZe,yZe,wZe,yh,dJ,AZe,LZe,_x,BZe,kZe,xZe,wh,cJ,RZe,SZe,ux,PZe,$Ze,IZe,Ah,fJ,jZe,NZe,bx,DZe,qZe,GZe,Lh,mJ,OZe,XZe,vx,zZe,VZe,WZe,Bh,gJ,QZe,HZe,Tx,UZe,JZe,YZe,kh,hJ,KZe,ZZe,Fx,eeo,oeo,reo,xh,pJ,teo,aeo,Cx,neo,seo,leo,Rh,_J,ieo,deo,Mx,ceo,feo,meo,Sh,uJ,geo,heo,Ex,peo,_eo,ueo,Ph,bJ,beo,veo,yx,Teo,Feo,Ceo,$h,vJ,Meo,Eeo,wx,yeo,weo,Aeo,Ih,TJ,Leo,Beo,Ax,keo,xeo,Reo,jh,FJ,Seo,Peo,Lx,$eo,Ieo,jeo,Nh,CJ,Neo,Deo,Bx,qeo,Geo,Oeo,Dh,MJ,Xeo,zeo,kx,Veo,Weo,Qeo,qh,EJ,Heo,Ueo,xx,Jeo,Yeo,Keo,Gh,yJ,Zeo,eoo,Rx,ooo,roo,too,Oh,wJ,aoo,noo,Sx,soo,loo,ioo,Xh,AJ,doo,coo,Px,foo,moo,goo,zh,LJ,hoo,poo,$x,_oo,uoo,boo,Vh,BJ,voo,Too,Ix,Foo,Coo,Moo,Wh,kJ,Eoo,yoo,jx,woo,Aoo,Loo,xs,xJ,Boo,koo,Nx,xoo,Roo,Dx,Soo,Poo,$oo,Qh,RJ,Ioo,joo,qx,Noo,Doo,qoo,Hh,SJ,Goo,Ooo,Gx,Xoo,zoo,Voo,Uh,PJ,Woo,Qoo,Ox,Hoo,Uoo,Joo,Jh,$J,Yoo,Koo,Xx,Zoo,ero,oro,Yh,IJ,rro,tro,zx,aro,nro,sro,Kh,jJ,lro,iro,Vx,dro,cro,fro,Zh,NJ,mro,gro,Wx,hro,pro,_ro,ep,DJ,uro,bro,Qx,vro,Tro,Fro,op,qJ,Cro,Mro,Hx,Ero,yro,wro,rp,GJ,Aro,Lro,Ux,Bro,kro,xro,tp,OJ,Rro,Sro,Jx,Pro,$ro,Iro,ap,XJ,jro,Nro,Yx,Dro,qro,Gro,np,zJ,Oro,Xro,Kx,zro,Vro,Wro,sp,VJ,Qro,Hro,Zx,Uro,Jro,Yro,lp,WJ,Kro,Zro,eR,eto,oto,rto,ip,QJ,tto,ato,oR,nto,sto,lto,dp,HJ,ito,dto,rR,cto,fto,mto,cp,UJ,gto,hto,tR,pto,_to,uto,fp,JJ,bto,vto,aR,Tto,Fto,Cto,mp,YJ,Mto,Eto,nR,yto,wto,Ato,gp,KJ,Lto,Bto,sR,kto,xto,Rto,hp,ZJ,Sto,Pto,lR,$to,Ito,jto,pp,eY,Nto,Dto,iR,qto,Gto,Oto,_p,oY,Xto,zto,dR,Vto,Wto,Qto,up,rY,Hto,Uto,cR,Jto,Yto,Kto,bp,tY,Zto,eao,fR,oao,rao,tao,vp,aY,aao,nao,mR,sao,lao,iao,Tp,nY,dao,cao,gR,fao,mao,gao,Fp,sY,hao,pao,hR,_ao,uao,bao,Cp,lY,vao,Tao,pR,Fao,Cao,Mao,Mp,iY,Eao,yao,_R,wao,Aao,Lao,Ep,dY,Bao,kao,uR,xao,Rao,Sao,yp,cY,Pao,$ao,bR,Iao,jao,Nao,wp,fY,Dao,qao,vR,Gao,Oao,Xao,Ap,mY,zao,Vao,TR,Wao,Qao,Hao,Lp,gY,Uao,Jao,FR,Yao,Kao,Zao,Bp,hY,eno,ono,CR,rno,tno,ano,kp,pY,nno,sno,MR,lno,ino,dno,xp,_Y,cno,fno,ER,mno,gno,hno,Rp,uY,pno,_no,yR,uno,bno,vno,Sp,bY,Tno,Fno,wR,Cno,Mno,Eno,Pp,vY,yno,wno,AR,Ano,Lno,Bno,$p,TY,kno,xno,LR,Rno,Sno,Pno,Ip,FY,$no,Ino,BR,jno,Nno,Dno,jp,CY,qno,Gno,kR,Ono,Xno,zno,Np,MY,Vno,Wno,xR,Qno,Hno,Uno,Dp,EY,Jno,Yno,RR,Kno,Zno,eso,qp,yY,oso,rso,SR,tso,aso,nso,Gp,wY,sso,lso,PR,iso,dso,cso,Op,AY,fso,mso,$R,gso,hso,pso,Xp,LY,_so,uso,IR,bso,vso,Tso,zp,BY,Fso,Cso,jR,Mso,Eso,yso,Vp,kY,wso,Aso,NR,Lso,Bso,kso,Wp,xY,xso,Rso,DR,Sso,Pso,$so,Qp,RY,Iso,jso,qR,Nso,Dso,qso,Hp,SY,Gso,Oso,GR,Xso,zso,Vso,Up,PY,Wso,Qso,OR,Hso,Uso,Jso,Jp,$Y,Yso,Kso,XR,Zso,elo,olo,Yp,rlo,IY,tlo,alo,jY,nlo,slo,NY,llo,ilo,G4,bLe,Xi,Kp,DY,O4,dlo,qY,clo,vLe,Wo,X4,flo,zi,mlo,GY,glo,hlo,OY,plo,_lo,ulo,z4,blo,XY,vlo,Tlo,Flo,Dr,V4,Clo,zY,Mlo,Elo,Vi,ylo,VY,wlo,Alo,WY,Llo,Blo,klo,QY,xlo,Rlo,W4,Slo,xe,Q4,Plo,HY,$lo,Ilo,qa,jlo,UY,Nlo,Dlo,JY,qlo,Glo,YY,Olo,Xlo,zlo,x,Zp,KY,Vlo,Wlo,zR,Qlo,Hlo,Ulo,e_,ZY,Jlo,Ylo,VR,Klo,Zlo,eio,o_,eK,oio,rio,WR,tio,aio,nio,r_,oK,sio,lio,QR,iio,dio,cio,t_,rK,fio,mio,HR,gio,hio,pio,a_,tK,_io,uio,UR,bio,vio,Tio,n_,aK,Fio,Cio,JR,Mio,Eio,yio,s_,nK,wio,Aio,YR,Lio,Bio,kio,l_,sK,xio,Rio,KR,Sio,Pio,$io,i_,lK,Iio,jio,ZR,Nio,Dio,qio,d_,iK,Gio,Oio,eS,Xio,zio,Vio,c_,dK,Wio,Qio,oS,Hio,Uio,Jio,f_,cK,Yio,Kio,rS,Zio,edo,odo,m_,fK,rdo,tdo,tS,ado,ndo,sdo,g_,mK,ldo,ido,aS,ddo,cdo,fdo,h_,gK,mdo,gdo,nS,hdo,pdo,_do,p_,hK,udo,bdo,sS,vdo,Tdo,Fdo,__,pK,Cdo,Mdo,lS,Edo,ydo,wdo,u_,_K,Ado,Ldo,iS,Bdo,kdo,xdo,b_,uK,Rdo,Sdo,dS,Pdo,$do,Ido,v_,bK,jdo,Ndo,cS,Ddo,qdo,Gdo,T_,vK,Odo,Xdo,fS,zdo,Vdo,Wdo,F_,TK,Qdo,Hdo,mS,Udo,Jdo,Ydo,C_,FK,Kdo,Zdo,gS,eco,oco,rco,M_,CK,tco,aco,hS,nco,sco,lco,E_,MK,ico,dco,pS,cco,fco,mco,y_,EK,gco,hco,_S,pco,_co,uco,w_,yK,bco,vco,uS,Tco,Fco,Cco,A_,wK,Mco,Eco,bS,yco,wco,Aco,L_,AK,Lco,Bco,vS,kco,xco,Rco,B_,LK,Sco,Pco,TS,$co,Ico,jco,k_,BK,Nco,Dco,FS,qco,Gco,Oco,x_,kK,Xco,zco,CS,Vco,Wco,Qco,R_,xK,Hco,Uco,MS,Jco,Yco,Kco,S_,RK,Zco,efo,ES,ofo,rfo,tfo,P_,SK,afo,nfo,yS,sfo,lfo,ifo,$_,PK,dfo,cfo,wS,ffo,mfo,gfo,I_,$K,hfo,pfo,AS,_fo,ufo,bfo,j_,vfo,IK,Tfo,Ffo,jK,Cfo,Mfo,NK,Efo,yfo,H4,TLe,Wi,N_,DK,U4,wfo,qK,Afo,FLe,Qo,J4,Lfo,Qi,Bfo,GK,kfo,xfo,OK,Rfo,Sfo,Pfo,Y4,$fo,XK,Ifo,jfo,Nfo,qr,K4,Dfo,zK,qfo,Gfo,Hi,Ofo,VK,Xfo,zfo,WK,Vfo,Wfo,Qfo,QK,Hfo,Ufo,Z4,Jfo,Re,eM,Yfo,HK,Kfo,Zfo,Ga,emo,UK,omo,rmo,JK,tmo,amo,YK,nmo,smo,lmo,$,D_,KK,imo,dmo,LS,cmo,fmo,mmo,q_,ZK,gmo,hmo,BS,pmo,_mo,umo,G_,eZ,bmo,vmo,kS,Tmo,Fmo,Cmo,O_,oZ,Mmo,Emo,xS,ymo,wmo,Amo,X_,rZ,Lmo,Bmo,RS,kmo,xmo,Rmo,z_,tZ,Smo,Pmo,SS,$mo,Imo,jmo,V_,aZ,Nmo,Dmo,PS,qmo,Gmo,Omo,W_,nZ,Xmo,zmo,$S,Vmo,Wmo,Qmo,Q_,sZ,Hmo,Umo,IS,Jmo,Ymo,Kmo,H_,lZ,Zmo,ego,jS,ogo,rgo,tgo,U_,iZ,ago,ngo,NS,sgo,lgo,igo,J_,dZ,dgo,cgo,DS,fgo,mgo,ggo,Y_,cZ,hgo,pgo,qS,_go,ugo,bgo,K_,fZ,vgo,Tgo,GS,Fgo,Cgo,Mgo,Z_,mZ,Ego,ygo,OS,wgo,Ago,Lgo,eu,gZ,Bgo,kgo,XS,xgo,Rgo,Sgo,ou,hZ,Pgo,$go,zS,Igo,jgo,Ngo,ru,pZ,Dgo,qgo,VS,Ggo,Ogo,Xgo,tu,_Z,zgo,Vgo,WS,Wgo,Qgo,Hgo,au,uZ,Ugo,Jgo,QS,Ygo,Kgo,Zgo,nu,bZ,eho,oho,HS,rho,tho,aho,su,vZ,nho,sho,US,lho,iho,dho,lu,TZ,cho,fho,JS,mho,gho,hho,iu,FZ,pho,_ho,YS,uho,bho,vho,du,CZ,Tho,Fho,KS,Cho,Mho,Eho,cu,MZ,yho,who,ZS,Aho,Lho,Bho,fu,EZ,kho,xho,eP,Rho,Sho,Pho,mu,yZ,$ho,Iho,oP,jho,Nho,Dho,gu,wZ,qho,Gho,rP,Oho,Xho,zho,hu,AZ,Vho,Who,tP,Qho,Hho,Uho,pu,LZ,Jho,Yho,aP,Kho,Zho,epo,_u,BZ,opo,rpo,nP,tpo,apo,npo,uu,kZ,spo,lpo,sP,ipo,dpo,cpo,bu,xZ,fpo,mpo,lP,gpo,hpo,ppo,vu,_po,RZ,upo,bpo,SZ,vpo,Tpo,PZ,Fpo,Cpo,oM,CLe,Ui,Tu,$Z,rM,Mpo,IZ,Epo,MLe,Ho,tM,ypo,Ji,wpo,jZ,Apo,Lpo,NZ,Bpo,kpo,xpo,aM,Rpo,DZ,Spo,Ppo,$po,Gr,nM,Ipo,qZ,jpo,Npo,Yi,Dpo,GZ,qpo,Gpo,OZ,Opo,Xpo,zpo,XZ,Vpo,Wpo,sM,Qpo,Se,lM,Hpo,zZ,Upo,Jpo,Oa,Ypo,VZ,Kpo,Zpo,WZ,e_o,o_o,QZ,r_o,t_o,a_o,I,Fu,HZ,n_o,s_o,iP,l_o,i_o,d_o,Cu,UZ,c_o,f_o,dP,m_o,g_o,h_o,Mu,JZ,p_o,__o,cP,u_o,b_o,v_o,Eu,YZ,T_o,F_o,fP,C_o,M_o,E_o,yu,KZ,y_o,w_o,mP,A_o,L_o,B_o,wu,ZZ,k_o,x_o,gP,R_o,S_o,P_o,Au,eee,$_o,I_o,hP,j_o,N_o,D_o,Lu,oee,q_o,G_o,pP,O_o,X_o,z_o,Bu,ree,V_o,W_o,_P,Q_o,H_o,U_o,ku,tee,J_o,Y_o,uP,K_o,Z_o,euo,xu,aee,ouo,ruo,bP,tuo,auo,nuo,Ru,nee,suo,luo,vP,iuo,duo,cuo,Su,see,fuo,muo,TP,guo,huo,puo,Pu,lee,_uo,uuo,FP,buo,vuo,Tuo,$u,iee,Fuo,Cuo,CP,Muo,Euo,yuo,Iu,dee,wuo,Auo,MP,Luo,Buo,kuo,ju,cee,xuo,Ruo,EP,Suo,Puo,$uo,Nu,fee,Iuo,juo,yP,Nuo,Duo,quo,Du,mee,Guo,Ouo,wP,Xuo,zuo,Vuo,qu,gee,Wuo,Quo,AP,Huo,Uuo,Juo,Gu,hee,Yuo,Kuo,LP,Zuo,e1o,o1o,Ou,pee,r1o,t1o,BP,a1o,n1o,s1o,Xu,_ee,l1o,i1o,kP,d1o,c1o,f1o,zu,uee,m1o,g1o,xP,h1o,p1o,_1o,Vu,bee,u1o,b1o,RP,v1o,T1o,F1o,Wu,vee,C1o,M1o,SP,E1o,y1o,w1o,Qu,Tee,A1o,L1o,PP,B1o,k1o,x1o,Hu,Fee,R1o,S1o,$P,P1o,$1o,I1o,Uu,Cee,j1o,N1o,IP,D1o,q1o,G1o,Ju,Mee,O1o,X1o,Eee,z1o,V1o,W1o,Yu,yee,Q1o,H1o,jP,U1o,J1o,Y1o,Ku,wee,K1o,Z1o,NP,ebo,obo,rbo,Zu,Aee,tbo,abo,DP,nbo,sbo,lbo,e1,Lee,ibo,dbo,qP,cbo,fbo,mbo,o1,gbo,Bee,hbo,pbo,kee,_bo,ubo,xee,bbo,vbo,iM,ELe,Ki,r1,Ree,dM,Tbo,See,Fbo,yLe,Uo,cM,Cbo,Zi,Mbo,Pee,Ebo,ybo,$ee,wbo,Abo,Lbo,fM,Bbo,Iee,kbo,xbo,Rbo,Or,mM,Sbo,jee,Pbo,$bo,ed,Ibo,Nee,jbo,Nbo,Dee,Dbo,qbo,Gbo,qee,Obo,Xbo,gM,zbo,Pe,hM,Vbo,Gee,Wbo,Qbo,Xa,Hbo,Oee,Ubo,Jbo,Xee,Ybo,Kbo,zee,Zbo,e5o,o5o,ae,t1,Vee,r5o,t5o,GP,a5o,n5o,s5o,a1,Wee,l5o,i5o,OP,d5o,c5o,f5o,n1,Qee,m5o,g5o,XP,h5o,p5o,_5o,s1,Hee,u5o,b5o,zP,v5o,T5o,F5o,l1,Uee,C5o,M5o,VP,E5o,y5o,w5o,i1,Jee,A5o,L5o,WP,B5o,k5o,x5o,d1,Yee,R5o,S5o,QP,P5o,$5o,I5o,c1,Kee,j5o,N5o,HP,D5o,q5o,G5o,f1,Zee,O5o,X5o,UP,z5o,V5o,W5o,m1,eoe,Q5o,H5o,JP,U5o,J5o,Y5o,g1,ooe,K5o,Z5o,YP,e2o,o2o,r2o,h1,roe,t2o,a2o,KP,n2o,s2o,l2o,p1,toe,i2o,d2o,ZP,c2o,f2o,m2o,_1,aoe,g2o,h2o,e$,p2o,_2o,u2o,u1,noe,b2o,v2o,o$,T2o,F2o,C2o,b1,soe,M2o,E2o,r$,y2o,w2o,A2o,v1,L2o,loe,B2o,k2o,ioe,x2o,R2o,doe,S2o,P2o,pM,wLe,od,T1,coe,_M,$2o,foe,I2o,ALe,Jo,uM,j2o,rd,N2o,moe,D2o,q2o,goe,G2o,O2o,X2o,bM,z2o,hoe,V2o,W2o,Q2o,Xr,vM,H2o,poe,U2o,J2o,td,Y2o,_oe,K2o,Z2o,uoe,evo,ovo,rvo,boe,tvo,avo,TM,nvo,$e,FM,svo,voe,lvo,ivo,za,dvo,Toe,cvo,fvo,Foe,mvo,gvo,Coe,hvo,pvo,_vo,A,F1,Moe,uvo,bvo,t$,vvo,Tvo,Fvo,C1,Eoe,Cvo,Mvo,a$,Evo,yvo,wvo,M1,yoe,Avo,Lvo,n$,Bvo,kvo,xvo,E1,woe,Rvo,Svo,s$,Pvo,$vo,Ivo,y1,Aoe,jvo,Nvo,l$,Dvo,qvo,Gvo,w1,Loe,Ovo,Xvo,i$,zvo,Vvo,Wvo,A1,Boe,Qvo,Hvo,d$,Uvo,Jvo,Yvo,L1,koe,Kvo,Zvo,c$,e6o,o6o,r6o,B1,xoe,t6o,a6o,f$,n6o,s6o,l6o,k1,Roe,i6o,d6o,m$,c6o,f6o,m6o,x1,Soe,g6o,h6o,g$,p6o,_6o,u6o,R1,Poe,b6o,v6o,h$,T6o,F6o,C6o,S1,$oe,M6o,E6o,p$,y6o,w6o,A6o,P1,Ioe,L6o,B6o,_$,k6o,x6o,R6o,$1,joe,S6o,P6o,u$,$6o,I6o,j6o,I1,Noe,N6o,D6o,b$,q6o,G6o,O6o,j1,Doe,X6o,z6o,v$,V6o,W6o,Q6o,N1,qoe,H6o,U6o,T$,J6o,Y6o,K6o,D1,Goe,Z6o,eTo,F$,oTo,rTo,tTo,q1,Ooe,aTo,nTo,C$,sTo,lTo,iTo,G1,Xoe,dTo,cTo,M$,fTo,mTo,gTo,O1,zoe,hTo,pTo,E$,_To,uTo,bTo,X1,Voe,vTo,TTo,y$,FTo,CTo,MTo,z1,Woe,ETo,yTo,w$,wTo,ATo,LTo,V1,Qoe,BTo,kTo,A$,xTo,RTo,STo,W1,Hoe,PTo,$To,L$,ITo,jTo,NTo,Q1,Uoe,DTo,qTo,B$,GTo,OTo,XTo,H1,Joe,zTo,VTo,k$,WTo,QTo,HTo,U1,Yoe,UTo,JTo,x$,YTo,KTo,ZTo,J1,Koe,e7o,o7o,R$,r7o,t7o,a7o,Y1,Zoe,n7o,s7o,S$,l7o,i7o,d7o,K1,ere,c7o,f7o,P$,m7o,g7o,h7o,Z1,ore,p7o,_7o,$$,u7o,b7o,v7o,eb,rre,T7o,F7o,I$,C7o,M7o,E7o,ob,tre,y7o,w7o,j$,A7o,L7o,B7o,rb,are,k7o,x7o,N$,R7o,S7o,P7o,tb,nre,$7o,I7o,D$,j7o,N7o,D7o,ab,sre,q7o,G7o,q$,O7o,X7o,z7o,nb,lre,V7o,W7o,G$,Q7o,H7o,U7o,sb,ire,J7o,Y7o,O$,K7o,Z7o,eFo,lb,dre,oFo,rFo,X$,tFo,aFo,nFo,ib,cre,sFo,lFo,z$,iFo,dFo,cFo,db,fre,fFo,mFo,V$,gFo,hFo,pFo,cb,mre,_Fo,uFo,W$,bFo,vFo,TFo,fb,gre,FFo,CFo,Q$,MFo,EFo,yFo,mb,wFo,hre,AFo,LFo,pre,BFo,kFo,_re,xFo,RFo,CM,LLe,ad,gb,ure,MM,SFo,bre,PFo,BLe,Yo,EM,$Fo,nd,IFo,vre,jFo,NFo,Tre,DFo,qFo,GFo,yM,OFo,Fre,XFo,zFo,VFo,zr,wM,WFo,Cre,QFo,HFo,sd,UFo,Mre,JFo,YFo,Ere,KFo,ZFo,e9o,yre,o9o,r9o,AM,t9o,Ie,LM,a9o,wre,n9o,s9o,Va,l9o,Are,i9o,d9o,Lre,c9o,f9o,Bre,m9o,g9o,h9o,G,hb,kre,p9o,_9o,H$,u9o,b9o,v9o,pb,xre,T9o,F9o,U$,C9o,M9o,E9o,_b,Rre,y9o,w9o,J$,A9o,L9o,B9o,ub,Sre,k9o,x9o,Y$,R9o,S9o,P9o,bb,Pre,$9o,I9o,K$,j9o,N9o,D9o,vb,$re,q9o,G9o,Z$,O9o,X9o,z9o,Tb,Ire,V9o,W9o,eI,Q9o,H9o,U9o,Fb,jre,J9o,Y9o,oI,K9o,Z9o,eCo,Cb,Nre,oCo,rCo,rI,tCo,aCo,nCo,Mb,Dre,sCo,lCo,tI,iCo,dCo,cCo,Eb,qre,fCo,mCo,aI,gCo,hCo,pCo,yb,Gre,_Co,uCo,nI,bCo,vCo,TCo,wb,Ore,FCo,CCo,sI,MCo,ECo,yCo,Ab,Xre,wCo,ACo,lI,LCo,BCo,kCo,Lb,zre,xCo,RCo,iI,SCo,PCo,$Co,Bb,Vre,ICo,jCo,dI,NCo,DCo,qCo,kb,Wre,GCo,OCo,cI,XCo,zCo,VCo,xb,Qre,WCo,QCo,fI,HCo,UCo,JCo,Rb,Hre,YCo,KCo,mI,ZCo,e4o,o4o,Sb,Ure,r4o,t4o,gI,a4o,n4o,s4o,Pb,Jre,l4o,i4o,hI,d4o,c4o,f4o,$b,Yre,m4o,g4o,pI,h4o,p4o,_4o,Ib,Kre,u4o,b4o,_I,v4o,T4o,F4o,jb,Zre,C4o,M4o,uI,E4o,y4o,w4o,Nb,ete,A4o,L4o,bI,B4o,k4o,x4o,Db,ote,R4o,S4o,vI,P4o,$4o,I4o,qb,rte,j4o,N4o,TI,D4o,q4o,G4o,Gb,O4o,tte,X4o,z4o,ate,V4o,W4o,nte,Q4o,H4o,BM,kLe,ld,Ob,ste,kM,U4o,lte,J4o,xLe,Ko,xM,Y4o,id,K4o,ite,Z4o,eMo,dte,oMo,rMo,tMo,RM,aMo,cte,nMo,sMo,lMo,Vr,SM,iMo,fte,dMo,cMo,dd,fMo,mte,mMo,gMo,gte,hMo,pMo,_Mo,hte,uMo,bMo,PM,vMo,je,$M,TMo,pte,FMo,CMo,Wa,MMo,_te,EMo,yMo,ute,wMo,AMo,bte,LMo,BMo,kMo,na,Xb,vte,xMo,RMo,FI,SMo,PMo,$Mo,zb,Tte,IMo,jMo,CI,NMo,DMo,qMo,Vb,Fte,GMo,OMo,MI,XMo,zMo,VMo,Wb,Cte,WMo,QMo,EI,HMo,UMo,JMo,Qb,Mte,YMo,KMo,yI,ZMo,eEo,oEo,Hb,rEo,Ete,tEo,aEo,yte,nEo,sEo,wte,lEo,iEo,IM,RLe,cd,Ub,Ate,jM,dEo,Lte,cEo,SLe,Zo,NM,fEo,fd,mEo,Bte,gEo,hEo,kte,pEo,_Eo,uEo,DM,bEo,xte,vEo,TEo,FEo,Wr,qM,CEo,Rte,MEo,EEo,md,yEo,Ste,wEo,AEo,Pte,LEo,BEo,kEo,$te,xEo,REo,GM,SEo,Ne,OM,PEo,Ite,$Eo,IEo,Qa,jEo,jte,NEo,DEo,Nte,qEo,GEo,Dte,OEo,XEo,zEo,D,Jb,qte,VEo,WEo,wI,QEo,HEo,UEo,Yb,Gte,JEo,YEo,AI,KEo,ZEo,e3o,Kb,Ote,o3o,r3o,LI,t3o,a3o,n3o,Zb,Xte,s3o,l3o,BI,i3o,d3o,c3o,e5,zte,f3o,m3o,kI,g3o,h3o,p3o,o5,Vte,_3o,u3o,xI,b3o,v3o,T3o,r5,Wte,F3o,C3o,RI,M3o,E3o,y3o,t5,Qte,w3o,A3o,SI,L3o,B3o,k3o,a5,Hte,x3o,R3o,PI,S3o,P3o,$3o,n5,Ute,I3o,j3o,$I,N3o,D3o,q3o,s5,Jte,G3o,O3o,II,X3o,z3o,V3o,l5,Yte,W3o,Q3o,jI,H3o,U3o,J3o,i5,Kte,Y3o,K3o,NI,Z3o,eyo,oyo,d5,Zte,ryo,tyo,DI,ayo,nyo,syo,c5,eae,lyo,iyo,qI,dyo,cyo,fyo,f5,oae,myo,gyo,GI,hyo,pyo,_yo,m5,rae,uyo,byo,OI,vyo,Tyo,Fyo,g5,tae,Cyo,Myo,XI,Eyo,yyo,wyo,h5,aae,Ayo,Lyo,zI,Byo,kyo,xyo,p5,nae,Ryo,Syo,VI,Pyo,$yo,Iyo,_5,sae,jyo,Nyo,WI,Dyo,qyo,Gyo,u5,lae,Oyo,Xyo,QI,zyo,Vyo,Wyo,b5,iae,Qyo,Hyo,HI,Uyo,Jyo,Yyo,v5,dae,Kyo,Zyo,UI,ewo,owo,rwo,T5,cae,two,awo,JI,nwo,swo,lwo,F5,fae,iwo,dwo,YI,cwo,fwo,mwo,C5,mae,gwo,hwo,KI,pwo,_wo,uwo,M5,gae,bwo,vwo,ZI,Two,Fwo,Cwo,E5,hae,Mwo,Ewo,ej,ywo,wwo,Awo,y5,pae,Lwo,Bwo,oj,kwo,xwo,Rwo,w5,_ae,Swo,Pwo,rj,$wo,Iwo,jwo,A5,uae,Nwo,Dwo,tj,qwo,Gwo,Owo,L5,Xwo,bae,zwo,Vwo,vae,Wwo,Qwo,Tae,Hwo,Uwo,XM,PLe,gd,B5,Fae,zM,Jwo,Cae,Ywo,$Le,er,VM,Kwo,hd,Zwo,Mae,eAo,oAo,Eae,rAo,tAo,aAo,WM,nAo,yae,sAo,lAo,iAo,Qr,QM,dAo,wae,cAo,fAo,pd,mAo,Aae,gAo,hAo,Lae,pAo,_Ao,uAo,Bae,bAo,vAo,HM,TAo,De,UM,FAo,kae,CAo,MAo,Ha,EAo,xae,yAo,wAo,Rae,AAo,LAo,Sae,BAo,kAo,xAo,R,k5,Pae,RAo,SAo,aj,PAo,$Ao,IAo,x5,$ae,jAo,NAo,nj,DAo,qAo,GAo,R5,Iae,OAo,XAo,sj,zAo,VAo,WAo,S5,jae,QAo,HAo,lj,UAo,JAo,YAo,P5,Nae,KAo,ZAo,ij,e0o,o0o,r0o,$5,Dae,t0o,a0o,dj,n0o,s0o,l0o,I5,qae,i0o,d0o,cj,c0o,f0o,m0o,j5,Gae,g0o,h0o,fj,p0o,_0o,u0o,N5,Oae,b0o,v0o,mj,T0o,F0o,C0o,D5,Xae,M0o,E0o,gj,y0o,w0o,A0o,q5,zae,L0o,B0o,hj,k0o,x0o,R0o,G5,Vae,S0o,P0o,pj,$0o,I0o,j0o,O5,Wae,N0o,D0o,_j,q0o,G0o,O0o,X5,Qae,X0o,z0o,uj,V0o,W0o,Q0o,z5,Hae,H0o,U0o,bj,J0o,Y0o,K0o,V5,Uae,Z0o,eLo,vj,oLo,rLo,tLo,W5,Jae,aLo,nLo,Tj,sLo,lLo,iLo,Q5,Yae,dLo,cLo,Fj,fLo,mLo,gLo,H5,Kae,hLo,pLo,Cj,_Lo,uLo,bLo,U5,Zae,vLo,TLo,Mj,FLo,CLo,MLo,J5,ene,ELo,yLo,Ej,wLo,ALo,LLo,Y5,one,BLo,kLo,yj,xLo,RLo,SLo,K5,rne,PLo,$Lo,wj,ILo,jLo,NLo,Z5,tne,DLo,qLo,Aj,GLo,OLo,XLo,e2,ane,zLo,VLo,Lj,WLo,QLo,HLo,o2,nne,ULo,JLo,Bj,YLo,KLo,ZLo,r2,sne,e8o,o8o,kj,r8o,t8o,a8o,t2,lne,n8o,s8o,xj,l8o,i8o,d8o,a2,ine,c8o,f8o,Rj,m8o,g8o,h8o,n2,dne,p8o,_8o,Sj,u8o,b8o,v8o,s2,cne,T8o,F8o,Pj,C8o,M8o,E8o,l2,fne,y8o,w8o,$j,A8o,L8o,B8o,i2,mne,k8o,x8o,Ij,R8o,S8o,P8o,d2,gne,$8o,I8o,jj,j8o,N8o,D8o,c2,hne,q8o,G8o,Nj,O8o,X8o,z8o,f2,pne,V8o,W8o,Dj,Q8o,H8o,U8o,m2,_ne,J8o,Y8o,qj,K8o,Z8o,eBo,g2,une,oBo,rBo,Gj,tBo,aBo,nBo,h2,sBo,bne,lBo,iBo,vne,dBo,cBo,Tne,fBo,mBo,JM,ILe,_d,p2,Fne,YM,gBo,Cne,hBo,jLe,or,KM,pBo,ud,_Bo,Mne,uBo,bBo,Ene,vBo,TBo,FBo,ZM,CBo,yne,MBo,EBo,yBo,Hr,eE,wBo,wne,ABo,LBo,bd,BBo,Ane,kBo,xBo,Lne,RBo,SBo,PBo,Bne,$Bo,IBo,oE,jBo,qe,rE,NBo,kne,DBo,qBo,Ua,GBo,xne,OBo,XBo,Rne,zBo,VBo,Sne,WBo,QBo,HBo,Pne,_2,$ne,UBo,JBo,Oj,YBo,KBo,ZBo,u2,eko,Ine,oko,rko,jne,tko,ako,Nne,nko,sko,tE,NLe,vd,b2,Dne,aE,lko,qne,iko,DLe,rr,nE,dko,Td,cko,Gne,fko,mko,One,gko,hko,pko,sE,_ko,Xne,uko,bko,vko,Ur,lE,Tko,zne,Fko,Cko,Fd,Mko,Vne,Eko,yko,Wne,wko,Ako,Lko,Qne,Bko,kko,iE,xko,Ge,dE,Rko,Hne,Sko,Pko,Ja,$ko,Une,Iko,jko,Jne,Nko,Dko,Yne,qko,Gko,Oko,be,v2,Kne,Xko,zko,Xj,Vko,Wko,Qko,T2,Zne,Hko,Uko,zj,Jko,Yko,Kko,Rs,ese,Zko,exo,Vj,oxo,rxo,Wj,txo,axo,nxo,F2,ose,sxo,lxo,Qj,ixo,dxo,cxo,la,rse,fxo,mxo,Hj,gxo,hxo,Uj,pxo,_xo,Jj,uxo,bxo,vxo,C2,tse,Txo,Fxo,Yj,Cxo,Mxo,Exo,M2,ase,yxo,wxo,Kj,Axo,Lxo,Bxo,E2,nse,kxo,xxo,Zj,Rxo,Sxo,Pxo,y2,sse,$xo,Ixo,eN,jxo,Nxo,Dxo,w2,qxo,lse,Gxo,Oxo,ise,Xxo,zxo,dse,Vxo,Wxo,cE,qLe,Cd,A2,cse,fE,Qxo,fse,Hxo,GLe,tr,mE,Uxo,Md,Jxo,mse,Yxo,Kxo,gse,Zxo,eRo,oRo,gE,rRo,hse,tRo,aRo,nRo,Jr,hE,sRo,pse,lRo,iRo,Ed,dRo,_se,cRo,fRo,use,mRo,gRo,hRo,bse,pRo,_Ro,pE,uRo,Oe,_E,bRo,vse,vRo,TRo,Ya,FRo,Tse,CRo,MRo,Fse,ERo,yRo,Cse,wRo,ARo,LRo,Mse,L2,Ese,BRo,kRo,oN,xRo,RRo,SRo,B2,PRo,yse,$Ro,IRo,wse,jRo,NRo,Ase,DRo,qRo,uE,OLe,yd,k2,Lse,bE,GRo,Bse,ORo,XLe,ar,vE,XRo,wd,zRo,kse,VRo,WRo,xse,QRo,HRo,URo,TE,JRo,Rse,YRo,KRo,ZRo,Yr,FE,eSo,Sse,oSo,rSo,Ad,tSo,Pse,aSo,nSo,$se,sSo,lSo,iSo,Ise,dSo,cSo,CE,fSo,Xe,ME,mSo,jse,gSo,hSo,Ka,pSo,Nse,_So,uSo,Dse,bSo,vSo,qse,TSo,FSo,CSo,ao,x2,Gse,MSo,ESo,rN,ySo,wSo,ASo,R2,Ose,LSo,BSo,tN,kSo,xSo,RSo,S2,Xse,SSo,PSo,aN,$So,ISo,jSo,P2,zse,NSo,DSo,nN,qSo,GSo,OSo,$2,Vse,XSo,zSo,sN,VSo,WSo,QSo,I2,Wse,HSo,USo,lN,JSo,YSo,KSo,j2,Qse,ZSo,ePo,iN,oPo,rPo,tPo,N2,aPo,Hse,nPo,sPo,Use,lPo,iPo,Jse,dPo,cPo,EE,zLe,Ld,D2,Yse,yE,fPo,Kse,mPo,VLe,nr,wE,gPo,Bd,hPo,Zse,pPo,_Po,ele,uPo,bPo,vPo,AE,TPo,ole,FPo,CPo,MPo,Kr,LE,EPo,rle,yPo,wPo,kd,APo,tle,LPo,BPo,ale,kPo,xPo,RPo,nle,SPo,PPo,BE,$Po,ze,kE,IPo,sle,jPo,NPo,Za,DPo,lle,qPo,GPo,ile,OPo,XPo,dle,zPo,VPo,WPo,xd,q2,cle,QPo,HPo,dN,UPo,JPo,YPo,G2,fle,KPo,ZPo,cN,e$o,o$o,r$o,O2,mle,t$o,a$o,fN,n$o,s$o,l$o,X2,i$o,gle,d$o,c$o,hle,f$o,m$o,ple,g$o,h$o,xE,WLe,Rd,z2,_le,RE,p$o,ule,_$o,QLe,sr,SE,u$o,Sd,b$o,ble,v$o,T$o,vle,F$o,C$o,M$o,PE,E$o,Tle,y$o,w$o,A$o,Zr,$E,L$o,Fle,B$o,k$o,Pd,x$o,Cle,R$o,S$o,Mle,P$o,$$o,I$o,Ele,j$o,N$o,IE,D$o,Ve,jE,q$o,yle,G$o,O$o,en,X$o,wle,z$o,V$o,Ale,W$o,Q$o,Lle,H$o,U$o,J$o,no,V2,Ble,Y$o,K$o,mN,Z$o,eIo,oIo,W2,kle,rIo,tIo,gN,aIo,nIo,sIo,Q2,xle,lIo,iIo,hN,dIo,cIo,fIo,H2,Rle,mIo,gIo,pN,hIo,pIo,_Io,U2,Sle,uIo,bIo,_N,vIo,TIo,FIo,J2,Ple,CIo,MIo,uN,EIo,yIo,wIo,Y2,$le,AIo,LIo,bN,BIo,kIo,xIo,K2,RIo,Ile,SIo,PIo,jle,$Io,IIo,Nle,jIo,NIo,NE,HLe,$d,Z2,Dle,DE,DIo,qle,qIo,ULe,lr,qE,GIo,Id,OIo,Gle,XIo,zIo,Ole,VIo,WIo,QIo,GE,HIo,Xle,UIo,JIo,YIo,et,OE,KIo,zle,ZIo,ejo,jd,ojo,Vle,rjo,tjo,Wle,ajo,njo,sjo,Qle,ljo,ijo,XE,djo,We,zE,cjo,Hle,fjo,mjo,on,gjo,Ule,hjo,pjo,Jle,_jo,ujo,Yle,bjo,vjo,Tjo,VE,ev,Kle,Fjo,Cjo,vN,Mjo,Ejo,yjo,ov,Zle,wjo,Ajo,TN,Ljo,Bjo,kjo,rv,xjo,eie,Rjo,Sjo,oie,Pjo,$jo,rie,Ijo,jjo,WE,JLe,Nd,tv,tie,QE,Njo,aie,Djo,YLe,ir,HE,qjo,Dd,Gjo,nie,Ojo,Xjo,sie,zjo,Vjo,Wjo,UE,Qjo,lie,Hjo,Ujo,Jjo,ot,JE,Yjo,iie,Kjo,Zjo,qd,eNo,die,oNo,rNo,cie,tNo,aNo,nNo,fie,sNo,lNo,YE,iNo,Qe,KE,dNo,mie,cNo,fNo,rn,mNo,gie,gNo,hNo,hie,pNo,_No,pie,uNo,bNo,vNo,Gd,av,_ie,TNo,FNo,FN,CNo,MNo,ENo,nv,uie,yNo,wNo,CN,ANo,LNo,BNo,sv,bie,kNo,xNo,MN,RNo,SNo,PNo,lv,$No,vie,INo,jNo,Tie,NNo,DNo,Fie,qNo,GNo,ZE,KLe,Od,iv,Cie,e3,ONo,Mie,XNo,ZLe,dr,o3,zNo,Xd,VNo,Eie,WNo,QNo,yie,HNo,UNo,JNo,r3,YNo,wie,KNo,ZNo,eDo,rt,t3,oDo,Aie,rDo,tDo,zd,aDo,Lie,nDo,sDo,Bie,lDo,iDo,dDo,kie,cDo,fDo,a3,mDo,He,n3,gDo,xie,hDo,pDo,tn,_Do,Rie,uDo,bDo,Sie,vDo,TDo,Pie,FDo,CDo,MDo,Vd,dv,$ie,EDo,yDo,EN,wDo,ADo,LDo,cv,Iie,BDo,kDo,yN,xDo,RDo,SDo,fv,jie,PDo,$Do,wN,IDo,jDo,NDo,mv,DDo,Nie,qDo,GDo,Die,ODo,XDo,qie,zDo,VDo,s3,e8e,Wd,gv,Gie,l3,WDo,Oie,QDo,o8e,cr,i3,HDo,Qd,UDo,Xie,JDo,YDo,zie,KDo,ZDo,eqo,d3,oqo,Vie,rqo,tqo,aqo,tt,c3,nqo,Wie,sqo,lqo,Hd,iqo,Qie,dqo,cqo,Hie,fqo,mqo,gqo,Uie,hqo,pqo,f3,_qo,Ue,m3,uqo,Jie,bqo,vqo,an,Tqo,Yie,Fqo,Cqo,Kie,Mqo,Eqo,Zie,yqo,wqo,Aqo,ede,hv,ode,Lqo,Bqo,AN,kqo,xqo,Rqo,pv,Sqo,rde,Pqo,$qo,tde,Iqo,jqo,ade,Nqo,Dqo,g3,r8e,Ud,_v,nde,h3,qqo,sde,Gqo,t8e,fr,p3,Oqo,Jd,Xqo,lde,zqo,Vqo,ide,Wqo,Qqo,Hqo,_3,Uqo,dde,Jqo,Yqo,Kqo,at,u3,Zqo,cde,eGo,oGo,Yd,rGo,fde,tGo,aGo,mde,nGo,sGo,lGo,gde,iGo,dGo,b3,cGo,Je,v3,fGo,hde,mGo,gGo,nn,hGo,pde,pGo,_Go,_de,uGo,bGo,ude,vGo,TGo,FGo,bde,uv,vde,CGo,MGo,LN,EGo,yGo,wGo,bv,AGo,Tde,LGo,BGo,Fde,kGo,xGo,Cde,RGo,SGo,T3,a8e,Kd,vv,Mde,F3,PGo,Ede,$Go,n8e,mr,C3,IGo,Zd,jGo,yde,NGo,DGo,wde,qGo,GGo,OGo,M3,XGo,Ade,zGo,VGo,WGo,nt,E3,QGo,Lde,HGo,UGo,ec,JGo,Bde,YGo,KGo,kde,ZGo,eOo,oOo,xde,rOo,tOo,y3,aOo,Ye,w3,nOo,Rde,sOo,lOo,sn,iOo,Sde,dOo,cOo,Pde,fOo,mOo,$de,gOo,hOo,pOo,A3,Tv,Ide,_Oo,uOo,BN,bOo,vOo,TOo,Fv,jde,FOo,COo,kN,MOo,EOo,yOo,Cv,wOo,Nde,AOo,LOo,Dde,BOo,kOo,qde,xOo,ROo,L3,s8e,oc,Mv,Gde,B3,SOo,Ode,POo,l8e,gr,k3,$Oo,rc,IOo,Xde,jOo,NOo,zde,DOo,qOo,GOo,x3,OOo,Vde,XOo,zOo,VOo,st,R3,WOo,Wde,QOo,HOo,tc,UOo,Qde,JOo,YOo,Hde,KOo,ZOo,eXo,Ude,oXo,rXo,S3,tXo,go,P3,aXo,Jde,nXo,sXo,ln,lXo,Yde,iXo,dXo,Kde,cXo,fXo,Zde,mXo,gXo,hXo,B,Ev,ece,pXo,_Xo,xN,uXo,bXo,vXo,yv,oce,TXo,FXo,RN,CXo,MXo,EXo,wv,rce,yXo,wXo,SN,AXo,LXo,BXo,Av,tce,kXo,xXo,PN,RXo,SXo,PXo,Lv,ace,$Xo,IXo,$N,jXo,NXo,DXo,Bv,nce,qXo,GXo,IN,OXo,XXo,zXo,kv,sce,VXo,WXo,jN,QXo,HXo,UXo,xv,lce,JXo,YXo,NN,KXo,ZXo,ezo,Rv,ice,ozo,rzo,DN,tzo,azo,nzo,Sv,dce,szo,lzo,qN,izo,dzo,czo,Pv,cce,fzo,mzo,GN,gzo,hzo,pzo,$v,fce,_zo,uzo,ON,bzo,vzo,Tzo,Iv,mce,Fzo,Czo,XN,Mzo,Ezo,yzo,jv,gce,wzo,Azo,zN,Lzo,Bzo,kzo,Nv,hce,xzo,Rzo,VN,Szo,Pzo,$zo,Ss,pce,Izo,jzo,WN,Nzo,Dzo,QN,qzo,Gzo,Ozo,Dv,_ce,Xzo,zzo,HN,Vzo,Wzo,Qzo,qv,uce,Hzo,Uzo,UN,Jzo,Yzo,Kzo,Gv,bce,Zzo,eVo,JN,oVo,rVo,tVo,Ov,vce,aVo,nVo,YN,sVo,lVo,iVo,Xv,Tce,dVo,cVo,KN,fVo,mVo,gVo,zv,Fce,hVo,pVo,ZN,_Vo,uVo,bVo,Vv,Cce,vVo,TVo,eD,FVo,CVo,MVo,Wv,Mce,EVo,yVo,oD,wVo,AVo,LVo,Qv,Ece,BVo,kVo,rD,xVo,RVo,SVo,Hv,yce,PVo,$Vo,tD,IVo,jVo,NVo,Uv,wce,DVo,qVo,aD,GVo,OVo,XVo,Jv,Ace,zVo,VVo,nD,WVo,QVo,HVo,Yv,Lce,UVo,JVo,sD,YVo,KVo,ZVo,Kv,Bce,eWo,oWo,lD,rWo,tWo,aWo,Zv,kce,nWo,sWo,iD,lWo,iWo,dWo,e6,xce,cWo,fWo,dD,mWo,gWo,hWo,o6,Rce,pWo,_Wo,cD,uWo,bWo,vWo,r6,Sce,TWo,FWo,fD,CWo,MWo,EWo,t6,Pce,yWo,wWo,mD,AWo,LWo,BWo,a6,$ce,kWo,xWo,gD,RWo,SWo,PWo,n6,Ice,$Wo,IWo,hD,jWo,NWo,DWo,s6,jce,qWo,GWo,pD,OWo,XWo,zWo,l6,Nce,VWo,WWo,_D,QWo,HWo,UWo,i6,Dce,JWo,YWo,uD,KWo,ZWo,eQo,d6,qce,oQo,rQo,bD,tQo,aQo,nQo,Gce,sQo,lQo,$3,i8e,ac,c6,Oce,I3,iQo,Xce,dQo,d8e,hr,j3,cQo,nc,fQo,zce,mQo,gQo,Vce,hQo,pQo,_Qo,N3,uQo,Wce,bQo,vQo,TQo,lt,D3,FQo,Qce,CQo,MQo,sc,EQo,Hce,yQo,wQo,Uce,AQo,LQo,BQo,Jce,kQo,xQo,q3,RQo,ho,G3,SQo,Yce,PQo,$Qo,dn,IQo,Kce,jQo,NQo,Zce,DQo,qQo,efe,GQo,OQo,XQo,H,f6,ofe,zQo,VQo,vD,WQo,QQo,HQo,m6,rfe,UQo,JQo,TD,YQo,KQo,ZQo,g6,tfe,eHo,oHo,FD,rHo,tHo,aHo,h6,afe,nHo,sHo,CD,lHo,iHo,dHo,p6,nfe,cHo,fHo,MD,mHo,gHo,hHo,_6,sfe,pHo,_Ho,ED,uHo,bHo,vHo,u6,lfe,THo,FHo,yD,CHo,MHo,EHo,b6,ife,yHo,wHo,wD,AHo,LHo,BHo,v6,dfe,kHo,xHo,AD,RHo,SHo,PHo,T6,cfe,$Ho,IHo,LD,jHo,NHo,DHo,F6,ffe,qHo,GHo,BD,OHo,XHo,zHo,C6,mfe,VHo,WHo,kD,QHo,HHo,UHo,M6,gfe,JHo,YHo,xD,KHo,ZHo,eUo,E6,hfe,oUo,rUo,RD,tUo,aUo,nUo,y6,pfe,sUo,lUo,SD,iUo,dUo,cUo,w6,_fe,fUo,mUo,PD,gUo,hUo,pUo,A6,ufe,_Uo,uUo,$D,bUo,vUo,TUo,L6,bfe,FUo,CUo,ID,MUo,EUo,yUo,B6,vfe,wUo,AUo,jD,LUo,BUo,kUo,k6,Tfe,xUo,RUo,ND,SUo,PUo,$Uo,x6,Ffe,IUo,jUo,DD,NUo,DUo,qUo,R6,Cfe,GUo,OUo,qD,XUo,zUo,VUo,Mfe,WUo,QUo,O3,c8e,lc,S6,Efe,X3,HUo,yfe,UUo,f8e,pr,z3,JUo,ic,YUo,wfe,KUo,ZUo,Afe,eJo,oJo,rJo,V3,tJo,Lfe,aJo,nJo,sJo,it,W3,lJo,Bfe,iJo,dJo,dc,cJo,kfe,fJo,mJo,xfe,gJo,hJo,pJo,Rfe,_Jo,uJo,Q3,bJo,po,H3,vJo,Sfe,TJo,FJo,cn,CJo,Pfe,MJo,EJo,$fe,yJo,wJo,Ife,AJo,LJo,BJo,he,P6,jfe,kJo,xJo,GD,RJo,SJo,PJo,$6,Nfe,$Jo,IJo,OD,jJo,NJo,DJo,I6,Dfe,qJo,GJo,XD,OJo,XJo,zJo,j6,qfe,VJo,WJo,zD,QJo,HJo,UJo,N6,Gfe,JJo,YJo,VD,KJo,ZJo,eYo,D6,Ofe,oYo,rYo,WD,tYo,aYo,nYo,q6,Xfe,sYo,lYo,QD,iYo,dYo,cYo,G6,zfe,fYo,mYo,HD,gYo,hYo,pYo,O6,Vfe,_Yo,uYo,UD,bYo,vYo,TYo,X6,Wfe,FYo,CYo,JD,MYo,EYo,yYo,Qfe,wYo,AYo,U3,m8e,cc,z6,Hfe,J3,LYo,Ufe,BYo,g8e,_r,Y3,kYo,fc,xYo,Jfe,RYo,SYo,Yfe,PYo,$Yo,IYo,K3,jYo,Kfe,NYo,DYo,qYo,dt,Z3,GYo,Zfe,OYo,XYo,mc,zYo,eme,VYo,WYo,ome,QYo,HYo,UYo,rme,JYo,YYo,ey,KYo,_o,oy,ZYo,tme,eKo,oKo,fn,rKo,ame,tKo,aKo,nme,nKo,sKo,sme,lKo,iKo,dKo,lme,V6,ime,cKo,fKo,YD,mKo,gKo,hKo,dme,pKo,_Ko,ry,h8e,gc,W6,cme,ty,uKo,fme,bKo,p8e,ur,ay,vKo,hc,TKo,mme,FKo,CKo,gme,MKo,EKo,yKo,ny,wKo,hme,AKo,LKo,BKo,ct,sy,kKo,pme,xKo,RKo,pc,SKo,_me,PKo,$Ko,ume,IKo,jKo,NKo,bme,DKo,qKo,ly,GKo,uo,iy,OKo,vme,XKo,zKo,mn,VKo,Tme,WKo,QKo,Fme,HKo,UKo,Cme,JKo,YKo,KKo,Y,Q6,Mme,ZKo,eZo,KD,oZo,rZo,tZo,H6,Eme,aZo,nZo,ZD,sZo,lZo,iZo,U6,yme,dZo,cZo,eq,fZo,mZo,gZo,J6,wme,hZo,pZo,oq,_Zo,uZo,bZo,Y6,Ame,vZo,TZo,rq,FZo,CZo,MZo,K6,Lme,EZo,yZo,tq,wZo,AZo,LZo,Z6,Bme,BZo,kZo,aq,xZo,RZo,SZo,eT,kme,PZo,$Zo,nq,IZo,jZo,NZo,oT,xme,DZo,qZo,sq,GZo,OZo,XZo,rT,Rme,zZo,VZo,lq,WZo,QZo,HZo,tT,Sme,UZo,JZo,iq,YZo,KZo,ZZo,aT,Pme,eer,oer,dq,rer,ter,aer,nT,$me,ner,ser,cq,ler,ier,der,sT,Ime,cer,fer,fq,mer,ger,her,lT,jme,per,_er,mq,uer,ber,ver,iT,Nme,Ter,Fer,gq,Cer,Mer,Eer,dT,Dme,yer,wer,hq,Aer,Ler,Ber,cT,qme,ker,xer,pq,Rer,Ser,Per,fT,Gme,$er,Ier,_q,jer,Ner,Der,mT,Ome,qer,Ger,uq,Oer,Xer,zer,Xme,Ver,Wer,dy,_8e,_c,gT,zme,cy,Qer,Vme,Her,u8e,br,fy,Uer,uc,Jer,Wme,Yer,Ker,Qme,Zer,eor,oor,my,ror,Hme,tor,aor,nor,ft,gy,sor,Ume,lor,ior,bc,dor,Jme,cor,mor,Yme,gor,hor,por,Kme,_or,uor,hy,bor,bo,py,vor,Zme,Tor,For,gn,Cor,ege,Mor,Eor,oge,yor,wor,rge,Aor,Lor,Bor,pe,hT,tge,kor,xor,bq,Ror,Sor,Por,pT,age,$or,Ior,vq,jor,Nor,Dor,_T,nge,qor,Gor,Tq,Oor,Xor,zor,uT,sge,Vor,Wor,Fq,Qor,Hor,Uor,bT,lge,Jor,Yor,Cq,Kor,Zor,err,vT,ige,orr,rrr,Mq,trr,arr,nrr,TT,dge,srr,lrr,Eq,irr,drr,crr,FT,cge,frr,mrr,yq,grr,hrr,prr,CT,fge,_rr,urr,wq,brr,vrr,Trr,MT,mge,Frr,Crr,Aq,Mrr,Err,yrr,gge,wrr,Arr,_y,b8e,vc,ET,hge,uy,Lrr,pge,Brr,v8e,vr,by,krr,Tc,xrr,_ge,Rrr,Srr,uge,Prr,$rr,Irr,vy,jrr,bge,Nrr,Drr,qrr,mt,Ty,Grr,vge,Orr,Xrr,Fc,zrr,Tge,Vrr,Wrr,Fge,Qrr,Hrr,Urr,Cge,Jrr,Yrr,Fy,Krr,vo,Cy,Zrr,Mge,etr,otr,hn,rtr,Ege,ttr,atr,yge,ntr,str,wge,ltr,itr,dtr,X,yT,Age,ctr,ftr,Lq,mtr,gtr,htr,wT,Lge,ptr,_tr,Bq,utr,btr,vtr,AT,Bge,Ttr,Ftr,kq,Ctr,Mtr,Etr,LT,kge,ytr,wtr,xq,Atr,Ltr,Btr,BT,xge,ktr,xtr,Rq,Rtr,Str,Ptr,kT,Rge,$tr,Itr,Sq,jtr,Ntr,Dtr,xT,Sge,qtr,Gtr,Pq,Otr,Xtr,ztr,RT,Pge,Vtr,Wtr,$q,Qtr,Htr,Utr,ST,$ge,Jtr,Ytr,Iq,Ktr,Ztr,ear,PT,Ige,oar,rar,jq,tar,aar,nar,$T,jge,sar,lar,Nq,iar,dar,car,IT,Nge,far,mar,Dq,gar,har,par,jT,Dge,_ar,uar,qq,bar,Tar,Far,NT,qge,Car,Mar,Gq,Ear,yar,war,DT,Gge,Aar,Lar,Oq,Bar,kar,xar,qT,Oge,Rar,Sar,Xq,Par,$ar,Iar,GT,Xge,jar,Nar,zq,Dar,qar,Gar,OT,zge,Oar,Xar,Vq,zar,Var,War,XT,Vge,Qar,Har,Wq,Uar,Jar,Yar,zT,Wge,Kar,Zar,Qq,enr,onr,rnr,VT,Qge,tnr,anr,Hq,nnr,snr,lnr,WT,Hge,inr,dnr,Uq,cnr,fnr,mnr,QT,Uge,gnr,hnr,Jq,pnr,_nr,unr,HT,Jge,bnr,vnr,Yq,Tnr,Fnr,Cnr,UT,Yge,Mnr,Enr,Kq,ynr,wnr,Anr,Kge,Lnr,Bnr,My,T8e,Cc,JT,Zge,Ey,knr,ehe,xnr,F8e,Tr,yy,Rnr,Mc,Snr,ohe,Pnr,$nr,rhe,Inr,jnr,Nnr,wy,Dnr,the,qnr,Gnr,Onr,gt,Ay,Xnr,ahe,znr,Vnr,Ec,Wnr,nhe,Qnr,Hnr,she,Unr,Jnr,Ynr,lhe,Knr,Znr,Ly,esr,To,By,osr,ihe,rsr,tsr,pn,asr,dhe,nsr,ssr,che,lsr,isr,fhe,dsr,csr,fsr,te,YT,mhe,msr,gsr,Zq,hsr,psr,_sr,KT,ghe,usr,bsr,eG,vsr,Tsr,Fsr,ZT,hhe,Csr,Msr,oG,Esr,ysr,wsr,e7,phe,Asr,Lsr,rG,Bsr,ksr,xsr,o7,_he,Rsr,Ssr,tG,Psr,$sr,Isr,r7,uhe,jsr,Nsr,aG,Dsr,qsr,Gsr,t7,bhe,Osr,Xsr,nG,zsr,Vsr,Wsr,a7,vhe,Qsr,Hsr,sG,Usr,Jsr,Ysr,n7,The,Ksr,Zsr,lG,elr,olr,rlr,s7,Fhe,tlr,alr,iG,nlr,slr,llr,l7,Che,ilr,dlr,dG,clr,flr,mlr,i7,Mhe,glr,hlr,cG,plr,_lr,ulr,d7,Ehe,blr,vlr,fG,Tlr,Flr,Clr,c7,yhe,Mlr,Elr,mG,ylr,wlr,Alr,f7,whe,Llr,Blr,gG,klr,xlr,Rlr,m7,Ahe,Slr,Plr,hG,$lr,Ilr,jlr,g7,Lhe,Nlr,Dlr,pG,qlr,Glr,Olr,Bhe,Xlr,zlr,ky,C8e,yc,h7,khe,xy,Vlr,xhe,Wlr,M8e,Fr,Ry,Qlr,wc,Hlr,Rhe,Ulr,Jlr,She,Ylr,Klr,Zlr,Sy,eir,Phe,oir,rir,tir,ht,Py,air,$he,nir,sir,Ac,lir,Ihe,iir,dir,jhe,cir,fir,mir,Nhe,gir,hir,$y,pir,Fo,Iy,_ir,Dhe,uir,bir,_n,vir,qhe,Tir,Fir,Ghe,Cir,Mir,Ohe,Eir,yir,wir,Xhe,p7,zhe,Air,Lir,_G,Bir,kir,xir,Vhe,Rir,Sir,jy,E8e,Lc,_7,Whe,Ny,Pir,Qhe,$ir,y8e,Cr,Dy,Iir,Bc,jir,Hhe,Nir,Dir,Uhe,qir,Gir,Oir,qy,Xir,Jhe,zir,Vir,Wir,pt,Gy,Qir,Yhe,Hir,Uir,kc,Jir,Khe,Yir,Kir,Zhe,Zir,edr,odr,epe,rdr,tdr,Oy,adr,Co,Xy,ndr,ope,sdr,ldr,un,idr,rpe,ddr,cdr,tpe,fdr,mdr,ape,gdr,hdr,pdr,K,u7,npe,_dr,udr,uG,bdr,vdr,Tdr,b7,spe,Fdr,Cdr,bG,Mdr,Edr,ydr,v7,lpe,wdr,Adr,vG,Ldr,Bdr,kdr,T7,ipe,xdr,Rdr,TG,Sdr,Pdr,$dr,F7,dpe,Idr,jdr,FG,Ndr,Ddr,qdr,C7,cpe,Gdr,Odr,CG,Xdr,zdr,Vdr,M7,fpe,Wdr,Qdr,MG,Hdr,Udr,Jdr,E7,mpe,Ydr,Kdr,EG,Zdr,ecr,ocr,y7,gpe,rcr,tcr,yG,acr,ncr,scr,w7,hpe,lcr,icr,wG,dcr,ccr,fcr,A7,ppe,mcr,gcr,AG,hcr,pcr,_cr,L7,_pe,ucr,bcr,LG,vcr,Tcr,Fcr,B7,upe,Ccr,Mcr,BG,Ecr,ycr,wcr,k7,bpe,Acr,Lcr,kG,Bcr,kcr,xcr,x7,vpe,Rcr,Scr,xG,Pcr,$cr,Icr,R7,Tpe,jcr,Ncr,RG,Dcr,qcr,Gcr,S7,Fpe,Ocr,Xcr,SG,zcr,Vcr,Wcr,P7,Cpe,Qcr,Hcr,PG,Ucr,Jcr,Ycr,$7,Mpe,Kcr,Zcr,$G,efr,ofr,rfr,I7,Epe,tfr,afr,IG,nfr,sfr,lfr,ype,ifr,dfr,zy,w8e,xc,j7,wpe,Vy,cfr,Ape,ffr,A8e,Mr,Wy,mfr,Rc,gfr,Lpe,hfr,pfr,Bpe,_fr,ufr,bfr,Qy,vfr,kpe,Tfr,Ffr,Cfr,_t,Hy,Mfr,xpe,Efr,yfr,Sc,wfr,Rpe,Afr,Lfr,Spe,Bfr,kfr,xfr,Ppe,Rfr,Sfr,Uy,Pfr,Mo,Jy,$fr,$pe,Ifr,jfr,bn,Nfr,Ipe,Dfr,qfr,jpe,Gfr,Ofr,Npe,Xfr,zfr,Vfr,Z,N7,Dpe,Wfr,Qfr,jG,Hfr,Ufr,Jfr,D7,qpe,Yfr,Kfr,NG,Zfr,emr,omr,q7,Gpe,rmr,tmr,DG,amr,nmr,smr,G7,Ope,lmr,imr,qG,dmr,cmr,fmr,O7,Xpe,mmr,gmr,GG,hmr,pmr,_mr,X7,zpe,umr,bmr,OG,vmr,Tmr,Fmr,z7,Vpe,Cmr,Mmr,XG,Emr,ymr,wmr,V7,Wpe,Amr,Lmr,zG,Bmr,kmr,xmr,W7,Qpe,Rmr,Smr,VG,Pmr,$mr,Imr,Q7,Hpe,jmr,Nmr,WG,Dmr,qmr,Gmr,H7,Upe,Omr,Xmr,QG,zmr,Vmr,Wmr,U7,Jpe,Qmr,Hmr,HG,Umr,Jmr,Ymr,J7,Ype,Kmr,Zmr,UG,egr,ogr,rgr,Y7,Kpe,tgr,agr,JG,ngr,sgr,lgr,K7,Zpe,igr,dgr,YG,cgr,fgr,mgr,Z7,e_e,ggr,hgr,KG,pgr,_gr,ugr,eF,o_e,bgr,vgr,ZG,Tgr,Fgr,Cgr,oF,r_e,Mgr,Egr,eO,ygr,wgr,Agr,rF,t_e,Lgr,Bgr,oO,kgr,xgr,Rgr,a_e,Sgr,Pgr,Yy,L8e,Pc,tF,n_e,Ky,$gr,s_e,Igr,B8e,Er,Zy,jgr,$c,Ngr,l_e,Dgr,qgr,i_e,Ggr,Ogr,Xgr,ew,zgr,d_e,Vgr,Wgr,Qgr,ut,ow,Hgr,c_e,Ugr,Jgr,Ic,Ygr,f_e,Kgr,Zgr,m_e,ehr,ohr,rhr,g_e,thr,ahr,rw,nhr,Eo,tw,shr,h_e,lhr,ihr,vn,dhr,p_e,chr,fhr,__e,mhr,ghr,u_e,hhr,phr,_hr,b_e,aF,v_e,uhr,bhr,rO,vhr,Thr,Fhr,T_e,Chr,Mhr,aw,k8e,jc,nF,F_e,nw,Ehr,C_e,yhr,x8e,yr,sw,whr,Nc,Ahr,M_e,Lhr,Bhr,E_e,khr,xhr,Rhr,lw,Shr,y_e,Phr,$hr,Ihr,bt,iw,jhr,w_e,Nhr,Dhr,Dc,qhr,A_e,Ghr,Ohr,L_e,Xhr,zhr,Vhr,B_e,Whr,Qhr,dw,Hhr,yo,cw,Uhr,k_e,Jhr,Yhr,Tn,Khr,x_e,Zhr,epr,R_e,opr,rpr,S_e,tpr,apr,npr,P_e,sF,$_e,spr,lpr,tO,ipr,dpr,cpr,I_e,fpr,mpr,fw,R8e,qc,lF,j_e,mw,gpr,N_e,hpr,S8e,wr,gw,ppr,Gc,_pr,D_e,upr,bpr,q_e,vpr,Tpr,Fpr,hw,Cpr,G_e,Mpr,Epr,ypr,vt,pw,wpr,O_e,Apr,Lpr,Oc,Bpr,X_e,kpr,xpr,z_e,Rpr,Spr,Ppr,V_e,$pr,Ipr,_w,jpr,wo,uw,Npr,W_e,Dpr,qpr,Fn,Gpr,Q_e,Opr,Xpr,H_e,zpr,Vpr,U_e,Wpr,Qpr,Hpr,V,iF,J_e,Upr,Jpr,aO,Ypr,Kpr,Zpr,dF,Y_e,e_r,o_r,nO,r_r,t_r,a_r,cF,K_e,n_r,s_r,sO,l_r,i_r,d_r,fF,Z_e,c_r,f_r,lO,m_r,g_r,h_r,mF,eue,p_r,__r,iO,u_r,b_r,v_r,gF,oue,T_r,F_r,dO,C_r,M_r,E_r,hF,rue,y_r,w_r,cO,A_r,L_r,B_r,pF,tue,k_r,x_r,fO,R_r,S_r,P_r,_F,aue,$_r,I_r,mO,j_r,N_r,D_r,uF,nue,q_r,G_r,gO,O_r,X_r,z_r,bF,sue,V_r,W_r,hO,Q_r,H_r,U_r,vF,lue,J_r,Y_r,pO,K_r,Z_r,eur,TF,iue,our,rur,_O,tur,aur,nur,FF,due,sur,lur,uO,iur,dur,cur,CF,cue,fur,mur,bO,gur,hur,pur,MF,fue,_ur,uur,vO,bur,vur,Tur,EF,mue,Fur,Cur,TO,Mur,Eur,yur,yF,gue,wur,Aur,FO,Lur,Bur,kur,wF,hue,xur,Rur,CO,Sur,Pur,$ur,AF,pue,Iur,jur,MO,Nur,Dur,qur,LF,_ue,Gur,Our,EO,Xur,zur,Vur,BF,uue,Wur,Qur,yO,Hur,Uur,Jur,kF,bue,Yur,Kur,wO,Zur,e1r,o1r,xF,vue,r1r,t1r,AO,a1r,n1r,s1r,Tue,l1r,i1r,bw,P8e,Xc,RF,Fue,vw,d1r,Cue,c1r,$8e,Ar,Tw,f1r,zc,m1r,Mue,g1r,h1r,Eue,p1r,_1r,u1r,Fw,b1r,yue,v1r,T1r,F1r,Tt,Cw,C1r,wue,M1r,E1r,Vc,y1r,Aue,w1r,A1r,Lue,L1r,B1r,k1r,Bue,x1r,R1r,Mw,S1r,Ao,Ew,P1r,kue,$1r,I1r,Cn,j1r,xue,N1r,D1r,Rue,q1r,G1r,Sue,O1r,X1r,z1r,Mn,SF,Pue,V1r,W1r,LO,Q1r,H1r,U1r,PF,$ue,J1r,Y1r,BO,K1r,Z1r,ebr,$F,Iue,obr,rbr,kO,tbr,abr,nbr,IF,jue,sbr,lbr,xO,ibr,dbr,cbr,Nue,fbr,mbr,yw,I8e,Wc,jF,Due,ww,gbr,que,hbr,j8e,Lr,Aw,pbr,Qc,_br,Gue,ubr,bbr,Oue,vbr,Tbr,Fbr,Lw,Cbr,Xue,Mbr,Ebr,ybr,Ft,Bw,wbr,zue,Abr,Lbr,Hc,Bbr,Vue,kbr,xbr,Wue,Rbr,Sbr,Pbr,Que,$br,Ibr,kw,jbr,Lo,xw,Nbr,Hue,Dbr,qbr,En,Gbr,Uue,Obr,Xbr,Jue,zbr,Vbr,Yue,Wbr,Qbr,Hbr,fe,NF,Kue,Ubr,Jbr,RO,Ybr,Kbr,Zbr,DF,Zue,e5r,o5r,SO,r5r,t5r,a5r,qF,e1e,n5r,s5r,PO,l5r,i5r,d5r,GF,o1e,c5r,f5r,$O,m5r,g5r,h5r,OF,r1e,p5r,_5r,IO,u5r,b5r,v5r,XF,t1e,T5r,F5r,jO,C5r,M5r,E5r,zF,a1e,y5r,w5r,NO,A5r,L5r,B5r,VF,n1e,k5r,x5r,DO,R5r,S5r,P5r,WF,s1e,$5r,I5r,qO,j5r,N5r,D5r,QF,l1e,q5r,G5r,GO,O5r,X5r,z5r,HF,i1e,V5r,W5r,OO,Q5r,H5r,U5r,d1e,J5r,Y5r,Rw,N8e,Uc,UF,c1e,Sw,K5r,f1e,Z5r,D8e,Br,Pw,e2r,Jc,o2r,m1e,r2r,t2r,g1e,a2r,n2r,s2r,$w,l2r,h1e,i2r,d2r,c2r,Ct,Iw,f2r,p1e,m2r,g2r,Yc,h2r,_1e,p2r,_2r,u1e,u2r,b2r,v2r,b1e,T2r,F2r,jw,C2r,Bo,Nw,M2r,v1e,E2r,y2r,yn,w2r,T1e,A2r,L2r,F1e,B2r,k2r,C1e,x2r,R2r,S2r,ve,JF,M1e,P2r,$2r,XO,I2r,j2r,N2r,YF,E1e,D2r,q2r,zO,G2r,O2r,X2r,KF,y1e,z2r,V2r,VO,W2r,Q2r,H2r,ZF,w1e,U2r,J2r,WO,Y2r,K2r,Z2r,e9,A1e,evr,ovr,QO,rvr,tvr,avr,o9,L1e,nvr,svr,HO,lvr,ivr,dvr,r9,B1e,cvr,fvr,UO,mvr,gvr,hvr,t9,k1e,pvr,_vr,JO,uvr,bvr,vvr,a9,x1e,Tvr,Fvr,YO,Cvr,Mvr,Evr,R1e,yvr,wvr,Dw,q8e,Kc,n9,S1e,qw,Avr,P1e,Lvr,G8e,kr,Gw,Bvr,Zc,kvr,$1e,xvr,Rvr,I1e,Svr,Pvr,$vr,Ow,Ivr,j1e,jvr,Nvr,Dvr,Mt,Xw,qvr,N1e,Gvr,Ovr,ef,Xvr,D1e,zvr,Vvr,q1e,Wvr,Qvr,Hvr,G1e,Uvr,Jvr,zw,Yvr,ko,Vw,Kvr,O1e,Zvr,e6r,wn,o6r,X1e,r6r,t6r,z1e,a6r,n6r,V1e,s6r,l6r,i6r,Te,s9,W1e,d6r,c6r,KO,f6r,m6r,g6r,l9,Q1e,h6r,p6r,ZO,_6r,u6r,b6r,i9,H1e,v6r,T6r,eX,F6r,C6r,M6r,d9,U1e,E6r,y6r,oX,w6r,A6r,L6r,c9,J1e,B6r,k6r,rX,x6r,R6r,S6r,f9,Y1e,P6r,$6r,tX,I6r,j6r,N6r,m9,K1e,D6r,q6r,aX,G6r,O6r,X6r,g9,Z1e,z6r,V6r,nX,W6r,Q6r,H6r,h9,ebe,U6r,J6r,sX,Y6r,K6r,Z6r,obe,eTr,oTr,Ww,O8e,of,p9,rbe,Qw,rTr,tbe,tTr,X8e,xr,Hw,aTr,rf,nTr,abe,sTr,lTr,nbe,iTr,dTr,cTr,Uw,fTr,sbe,mTr,gTr,hTr,Et,Jw,pTr,lbe,_Tr,uTr,tf,bTr,ibe,vTr,TTr,dbe,FTr,CTr,MTr,cbe,ETr,yTr,Yw,wTr,xo,Kw,ATr,fbe,LTr,BTr,An,kTr,mbe,xTr,RTr,gbe,STr,PTr,hbe,$Tr,ITr,jTr,Fe,_9,pbe,NTr,DTr,lX,qTr,GTr,OTr,u9,_be,XTr,zTr,iX,VTr,WTr,QTr,b9,ube,HTr,UTr,dX,JTr,YTr,KTr,v9,bbe,ZTr,e7r,cX,o7r,r7r,t7r,T9,vbe,a7r,n7r,fX,s7r,l7r,i7r,F9,Tbe,d7r,c7r,mX,f7r,m7r,g7r,C9,Fbe,h7r,p7r,gX,_7r,u7r,b7r,M9,Cbe,v7r,T7r,hX,F7r,C7r,M7r,E9,Mbe,E7r,y7r,pX,w7r,A7r,L7r,Ebe,B7r,k7r,Zw,z8e,af,y9,ybe,eA,x7r,wbe,R7r,V8e,Rr,oA,S7r,nf,P7r,Abe,$7r,I7r,Lbe,j7r,N7r,D7r,rA,q7r,Bbe,G7r,O7r,X7r,yt,tA,z7r,kbe,V7r,W7r,sf,Q7r,xbe,H7r,U7r,Rbe,J7r,Y7r,K7r,Sbe,Z7r,eFr,aA,oFr,Ro,nA,rFr,Pbe,tFr,aFr,Ln,nFr,$be,sFr,lFr,Ibe,iFr,dFr,jbe,cFr,fFr,mFr,Ce,w9,Nbe,gFr,hFr,_X,pFr,_Fr,uFr,A9,Dbe,bFr,vFr,uX,TFr,FFr,CFr,L9,qbe,MFr,EFr,bX,yFr,wFr,AFr,B9,Gbe,LFr,BFr,vX,kFr,xFr,RFr,k9,Obe,SFr,PFr,TX,$Fr,IFr,jFr,x9,Xbe,NFr,DFr,FX,qFr,GFr,OFr,R9,zbe,XFr,zFr,CX,VFr,WFr,QFr,S9,Vbe,HFr,UFr,MX,JFr,YFr,KFr,P9,Wbe,ZFr,e9r,EX,o9r,r9r,t9r,Qbe,a9r,n9r,sA,W8e,lf,$9,Hbe,lA,s9r,Ube,l9r,Q8e,Sr,iA,i9r,df,d9r,Jbe,c9r,f9r,Ybe,m9r,g9r,h9r,dA,p9r,Kbe,_9r,u9r,b9r,wt,cA,v9r,Zbe,T9r,F9r,cf,C9r,e5e,M9r,E9r,o5e,y9r,w9r,A9r,r5e,L9r,B9r,fA,k9r,So,mA,x9r,t5e,R9r,S9r,Bn,P9r,a5e,$9r,I9r,n5e,j9r,N9r,s5e,D9r,q9r,G9r,so,I9,l5e,O9r,X9r,yX,z9r,V9r,W9r,j9,i5e,Q9r,H9r,wX,U9r,J9r,Y9r,N9,d5e,K9r,Z9r,AX,eCr,oCr,rCr,D9,c5e,tCr,aCr,LX,nCr,sCr,lCr,q9,f5e,iCr,dCr,BX,cCr,fCr,mCr,G9,m5e,gCr,hCr,kX,pCr,_Cr,uCr,O9,g5e,bCr,vCr,xX,TCr,FCr,CCr,h5e,MCr,ECr,gA,H8e,ff,X9,p5e,hA,yCr,_5e,wCr,U8e,Pr,pA,ACr,mf,LCr,u5e,BCr,kCr,b5e,xCr,RCr,SCr,_A,PCr,v5e,$Cr,ICr,jCr,At,uA,NCr,T5e,DCr,qCr,gf,GCr,F5e,OCr,XCr,C5e,zCr,VCr,WCr,M5e,QCr,HCr,bA,UCr,Po,vA,JCr,E5e,YCr,KCr,kn,ZCr,y5e,e4r,o4r,w5e,r4r,t4r,A5e,a4r,n4r,s4r,lo,z9,L5e,l4r,i4r,RX,d4r,c4r,f4r,V9,B5e,m4r,g4r,SX,h4r,p4r,_4r,W9,k5e,u4r,b4r,PX,v4r,T4r,F4r,Q9,x5e,C4r,M4r,$X,E4r,y4r,w4r,H9,R5e,A4r,L4r,IX,B4r,k4r,x4r,U9,S5e,R4r,S4r,jX,P4r,$4r,I4r,J9,P5e,j4r,N4r,NX,D4r,q4r,G4r,$5e,O4r,X4r,TA,J8e,hf,Y9,I5e,FA,z4r,j5e,V4r,Y8e,$r,CA,W4r,pf,Q4r,N5e,H4r,U4r,D5e,J4r,Y4r,K4r,MA,Z4r,q5e,eMr,oMr,rMr,Lt,EA,tMr,G5e,aMr,nMr,_f,sMr,O5e,lMr,iMr,X5e,dMr,cMr,fMr,z5e,mMr,gMr,yA,hMr,$o,wA,pMr,V5e,_Mr,uMr,xn,bMr,W5e,vMr,TMr,Q5e,FMr,CMr,H5e,MMr,EMr,yMr,U5e,K9,J5e,wMr,AMr,DX,LMr,BMr,kMr,Y5e,xMr,RMr,AA,K8e,uf,Z9,K5e,LA,SMr,Z5e,PMr,Z8e,Ir,BA,$Mr,bf,IMr,e2e,jMr,NMr,o2e,DMr,qMr,GMr,kA,OMr,r2e,XMr,zMr,VMr,Bt,xA,WMr,t2e,QMr,HMr,vf,UMr,a2e,JMr,YMr,n2e,KMr,ZMr,eEr,s2e,oEr,rEr,RA,tEr,Io,SA,aEr,l2e,nEr,sEr,Rn,lEr,i2e,iEr,dEr,d2e,cEr,fEr,c2e,mEr,gEr,hEr,PA,eC,f2e,pEr,_Er,qX,uEr,bEr,vEr,oC,m2e,TEr,FEr,GX,CEr,MEr,EEr,g2e,yEr,wEr,$A,eBe,Tf,rC,h2e,IA,AEr,p2e,LEr,oBe,jr,jA,BEr,Ff,kEr,_2e,xEr,REr,u2e,SEr,PEr,$Er,NA,IEr,b2e,jEr,NEr,DEr,kt,DA,qEr,v2e,GEr,OEr,Cf,XEr,T2e,zEr,VEr,F2e,WEr,QEr,HEr,C2e,UEr,JEr,qA,YEr,jo,GA,KEr,M2e,ZEr,e3r,Sn,o3r,E2e,r3r,t3r,y2e,a3r,n3r,w2e,s3r,l3r,i3r,A2e,tC,L2e,d3r,c3r,OX,f3r,m3r,g3r,B2e,h3r,p3r,OA,rBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),n4=new z({}),s4=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new _3r({props:{warning:"&lcub;true}",$$slots:{default:[F_t]},$$scope:{ctx:yi}}}),l4=new z({}),i4=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/configuration_auto.py#L515"}}),f4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/configuration_auto.py#L538",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),m4=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),g4=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/configuration_auto.py#L660",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),h4=new z({}),p4=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/tokenization_auto.py#L351"}}),b4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15796/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),v4=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),T4=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),F4=new z({}),C4=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),y4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15796/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ih=new _3r({props:{$$slots:{default:[C_t]},$$scope:{ctx:yi}}}),w4=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),A4=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),L4=new z({}),B4=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/processing_auto.py#L71"}}),R4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),vh=new _3r({props:{$$slots:{default:[M_t]},$$scope:{ctx:yi}}}),S4=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),P4=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),$4=new z({}),I4=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L672"}}),N4=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),D4=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),q4=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G4=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),O4=new z({}),X4=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L679"}}),V4=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),W4=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),Q4=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H4=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U4=new z({}),J4=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L694"}}),K4=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Z4=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),eM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oM=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rM=new z({}),tM=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L701"}}),nM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),sM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),lM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dM=new z({}),cM=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L708"}}),mM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),gM=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),hM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pM=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_M=new z({}),uM=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L717"}}),vM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),TM=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),FM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CM=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),MM=new z({}),EM=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L751"}}),wM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),AM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),LM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),kM=new z({}),xM=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L758"}}),SM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),PM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),$M=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jM=new z({}),NM=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L744"}}),qM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),GM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),OM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zM=new z({}),VM=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L726"}}),QM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),HM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),UM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YM=new z({}),KM=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L733"}}),eE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),oE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),rE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aE=new z({}),nE=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L767"}}),lE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iE=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),dE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cE=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fE=new z({}),mE=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L797"}}),hE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),pE=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),_E=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uE=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bE=new z({}),vE=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L804"}}),FE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),CE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),ME=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),EE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yE=new z({}),wE=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L827"}}),LE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),BE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),kE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),RE=new z({}),SE=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L811"}}),$E=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),IE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),jE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),NE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DE=new z({}),qE=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L818"}}),OE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),XE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),zE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),WE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),QE=new z({}),HE=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L836"}}),JE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),YE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),KE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ZE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e3=new z({}),o3=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L843"}}),t3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),a3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),n3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l3=new z({}),i3=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L790"}}),c3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),f3=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),m3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g3=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h3=new z({}),p3=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L774"}}),u3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),b3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),v3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F3=new z({}),C3=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_auto.py#L781"}}),E3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),y3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),w3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B3=new z({}),k3=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),R3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),S3=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),P3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$3=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I3=new z({}),j3=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),D3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),G3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X3=new z({}),z3=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),W3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Q3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),H3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),U3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),J3=new z({}),Y3=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),Z3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ey=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),oy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ry=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ty=new z({}),ay=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),sy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),iy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cy=new z({}),fy=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),gy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),hy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),py=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_y=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uy=new z({}),by=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),Ty=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Cy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),My=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ey=new z({}),yy=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),Ay=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),By=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ky=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xy=new z({}),Ry=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),Py=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Iy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ny=new z({}),Dy=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),Gy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Xy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vy=new z({}),Wy=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),Hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Jy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ky=new z({}),Zy=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),tw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nw=new z({}),sw=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),cw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mw=new z({}),gw=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),pw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),_w=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),uw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vw=new z({}),Tw=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),Cw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),Mw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),Ew=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ww=new z({}),Aw=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),Bw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),kw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),xw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sw=new z({}),Pw=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),Iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),jw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),Nw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qw=new z({}),Gw=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),Xw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),zw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),Vw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qw=new z({}),Hw=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),Jw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),Kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),eA=new z({}),oA=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),tA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),nA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lA=new z({}),iA=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),cA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),fA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),mA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hA=new z({}),pA=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),uA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),vA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),TA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),FA=new z({}),CA=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),EA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),wA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),AA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),LA=new z({}),BA=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),xA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),SA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$A=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),IA=new z({}),jA=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),DA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),qA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),GA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15796/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15796/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15796/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),OA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),o4=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),r4=o("AutoConfig"),$n=o(", "),In=a("a"),t4=o("AutoModel"),ki=o(`, and
`),jn=a("a"),a4=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),D0=o("will create a model that is an instance of "),Ri=a("a"),q0=o("BertModel"),G0=o("."),qo=l(),Ia=a("p"),O0=o("There is one class of "),Af=a("code"),X0=o("AutoModel"),mxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),tLe=l(),Si=a("h2"),Lf=a("a"),$V=a("span"),f(n4.$$.fragment),gxe=l(),IV=a("span"),hxe=o("Extending the Auto Classes"),aLe=l(),Nn=a("p"),pxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=a("code"),_xe=o("NewModel"),uxe=o(", make sure you have a "),NV=a("code"),bxe=o("NewModelConfig"),vxe=o(` then you can add those to the auto
classes like this:`),nLe=l(),f(s4.$$.fragment),sLe=l(),z0=a("p"),Txe=o("You will then be able to use the auto classes like you would usually do!"),lLe=l(),f(Bf.$$.fragment),iLe=l(),Pi=a("h2"),kf=a("a"),DV=a("span"),f(l4.$$.fragment),Fxe=l(),qV=a("span"),Cxe=o("AutoConfig"),dLe=l(),Go=a("div"),f(i4.$$.fragment),Mxe=l(),d4=a("p"),Exe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V0=a("a"),yxe=o("from_pretrained()"),wxe=o(" class method."),Axe=l(),c4=a("p"),Lxe=o("This class cannot be instantiated directly using "),GV=a("code"),Bxe=o("__init__()"),kxe=o(" (throws an error)."),xxe=l(),fo=a("div"),f(f4.$$.fragment),Rxe=l(),OV=a("p"),Sxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Pxe=l(),$i=a("p"),$xe=o("The configuration class to instantiate is selected based on the "),XV=a("code"),Ixe=o("model_type"),jxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=a("code"),Nxe=o("pretrained_model_name_or_path"),Dxe=o(":"),qxe=l(),v=a("ul"),xf=a("li"),VV=a("strong"),Gxe=o("albert"),Oxe=o(" \u2014 "),W0=a("a"),Xxe=o("AlbertConfig"),zxe=o(" (ALBERT model)"),Vxe=l(),Rf=a("li"),WV=a("strong"),Wxe=o("bart"),Qxe=o(" \u2014 "),Q0=a("a"),Hxe=o("BartConfig"),Uxe=o(" (BART model)"),Jxe=l(),Sf=a("li"),QV=a("strong"),Yxe=o("beit"),Kxe=o(" \u2014 "),H0=a("a"),Zxe=o("BeitConfig"),eRe=o(" (BEiT model)"),oRe=l(),Pf=a("li"),HV=a("strong"),rRe=o("bert"),tRe=o(" \u2014 "),U0=a("a"),aRe=o("BertConfig"),nRe=o(" (BERT model)"),sRe=l(),$f=a("li"),UV=a("strong"),lRe=o("bert-generation"),iRe=o(" \u2014 "),J0=a("a"),dRe=o("BertGenerationConfig"),cRe=o(" (Bert Generation model)"),fRe=l(),If=a("li"),JV=a("strong"),mRe=o("big_bird"),gRe=o(" \u2014 "),Y0=a("a"),hRe=o("BigBirdConfig"),pRe=o(" (BigBird model)"),_Re=l(),jf=a("li"),YV=a("strong"),uRe=o("bigbird_pegasus"),bRe=o(" \u2014 "),K0=a("a"),vRe=o("BigBirdPegasusConfig"),TRe=o(" (BigBirdPegasus model)"),FRe=l(),Nf=a("li"),KV=a("strong"),CRe=o("blenderbot"),MRe=o(" \u2014 "),Z0=a("a"),ERe=o("BlenderbotConfig"),yRe=o(" (Blenderbot model)"),wRe=l(),Df=a("li"),ZV=a("strong"),ARe=o("blenderbot-small"),LRe=o(" \u2014 "),eL=a("a"),BRe=o("BlenderbotSmallConfig"),kRe=o(" (BlenderbotSmall model)"),xRe=l(),qf=a("li"),eW=a("strong"),RRe=o("camembert"),SRe=o(" \u2014 "),oL=a("a"),PRe=o("CamembertConfig"),$Re=o(" (CamemBERT model)"),IRe=l(),Gf=a("li"),oW=a("strong"),jRe=o("canine"),NRe=o(" \u2014 "),rL=a("a"),DRe=o("CanineConfig"),qRe=o(" (Canine model)"),GRe=l(),Of=a("li"),rW=a("strong"),ORe=o("clip"),XRe=o(" \u2014 "),tL=a("a"),zRe=o("CLIPConfig"),VRe=o(" (CLIP model)"),WRe=l(),Xf=a("li"),tW=a("strong"),QRe=o("convbert"),HRe=o(" \u2014 "),aL=a("a"),URe=o("ConvBertConfig"),JRe=o(" (ConvBERT model)"),YRe=l(),zf=a("li"),aW=a("strong"),KRe=o("convnext"),ZRe=o(" \u2014 "),nL=a("a"),eSe=o("ConvNextConfig"),oSe=o(" (ConvNext model)"),rSe=l(),Vf=a("li"),nW=a("strong"),tSe=o("ctrl"),aSe=o(" \u2014 "),sL=a("a"),nSe=o("CTRLConfig"),sSe=o(" (CTRL model)"),lSe=l(),Wf=a("li"),sW=a("strong"),iSe=o("deberta"),dSe=o(" \u2014 "),lL=a("a"),cSe=o("DebertaConfig"),fSe=o(" (DeBERTa model)"),mSe=l(),Qf=a("li"),lW=a("strong"),gSe=o("deberta-v2"),hSe=o(" \u2014 "),iL=a("a"),pSe=o("DebertaV2Config"),_Se=o(" (DeBERTa-v2 model)"),uSe=l(),Hf=a("li"),iW=a("strong"),bSe=o("deit"),vSe=o(" \u2014 "),dL=a("a"),TSe=o("DeiTConfig"),FSe=o(" (DeiT model)"),CSe=l(),Uf=a("li"),dW=a("strong"),MSe=o("detr"),ESe=o(" \u2014 "),cL=a("a"),ySe=o("DetrConfig"),wSe=o(" (DETR model)"),ASe=l(),Jf=a("li"),cW=a("strong"),LSe=o("distilbert"),BSe=o(" \u2014 "),fL=a("a"),kSe=o("DistilBertConfig"),xSe=o(" (DistilBERT model)"),RSe=l(),Yf=a("li"),fW=a("strong"),SSe=o("dpr"),PSe=o(" \u2014 "),mL=a("a"),$Se=o("DPRConfig"),ISe=o(" (DPR model)"),jSe=l(),Kf=a("li"),mW=a("strong"),NSe=o("electra"),DSe=o(" \u2014 "),gL=a("a"),qSe=o("ElectraConfig"),GSe=o(" (ELECTRA model)"),OSe=l(),Zf=a("li"),gW=a("strong"),XSe=o("encoder-decoder"),zSe=o(" \u2014 "),hL=a("a"),VSe=o("EncoderDecoderConfig"),WSe=o(" (Encoder decoder model)"),QSe=l(),em=a("li"),hW=a("strong"),HSe=o("flaubert"),USe=o(" \u2014 "),pL=a("a"),JSe=o("FlaubertConfig"),YSe=o(" (FlauBERT model)"),KSe=l(),om=a("li"),pW=a("strong"),ZSe=o("fnet"),ePe=o(" \u2014 "),_L=a("a"),oPe=o("FNetConfig"),rPe=o(" (FNet model)"),tPe=l(),rm=a("li"),_W=a("strong"),aPe=o("fsmt"),nPe=o(" \u2014 "),uL=a("a"),sPe=o("FSMTConfig"),lPe=o(" (FairSeq Machine-Translation model)"),iPe=l(),tm=a("li"),uW=a("strong"),dPe=o("funnel"),cPe=o(" \u2014 "),bL=a("a"),fPe=o("FunnelConfig"),mPe=o(" (Funnel Transformer model)"),gPe=l(),am=a("li"),bW=a("strong"),hPe=o("gpt2"),pPe=o(" \u2014 "),vL=a("a"),_Pe=o("GPT2Config"),uPe=o(" (OpenAI GPT-2 model)"),bPe=l(),nm=a("li"),vW=a("strong"),vPe=o("gpt_neo"),TPe=o(" \u2014 "),TL=a("a"),FPe=o("GPTNeoConfig"),CPe=o(" (GPT Neo model)"),MPe=l(),sm=a("li"),TW=a("strong"),EPe=o("gptj"),yPe=o(" \u2014 "),FL=a("a"),wPe=o("GPTJConfig"),APe=o(" (GPT-J model)"),LPe=l(),lm=a("li"),FW=a("strong"),BPe=o("hubert"),kPe=o(" \u2014 "),CL=a("a"),xPe=o("HubertConfig"),RPe=o(" (Hubert model)"),SPe=l(),im=a("li"),CW=a("strong"),PPe=o("ibert"),$Pe=o(" \u2014 "),ML=a("a"),IPe=o("IBertConfig"),jPe=o(" (I-BERT model)"),NPe=l(),dm=a("li"),MW=a("strong"),DPe=o("imagegpt"),qPe=o(" \u2014 "),EL=a("a"),GPe=o("ImageGPTConfig"),OPe=o(" (ImageGPT model)"),XPe=l(),cm=a("li"),EW=a("strong"),zPe=o("layoutlm"),VPe=o(" \u2014 "),yL=a("a"),WPe=o("LayoutLMConfig"),QPe=o(" (LayoutLM model)"),HPe=l(),fm=a("li"),yW=a("strong"),UPe=o("layoutlmv2"),JPe=o(" \u2014 "),wL=a("a"),YPe=o("LayoutLMv2Config"),KPe=o(" (LayoutLMv2 model)"),ZPe=l(),mm=a("li"),wW=a("strong"),e$e=o("led"),o$e=o(" \u2014 "),AL=a("a"),r$e=o("LEDConfig"),t$e=o(" (LED model)"),a$e=l(),gm=a("li"),AW=a("strong"),n$e=o("longformer"),s$e=o(" \u2014 "),LL=a("a"),l$e=o("LongformerConfig"),i$e=o(" (Longformer model)"),d$e=l(),hm=a("li"),LW=a("strong"),c$e=o("luke"),f$e=o(" \u2014 "),BL=a("a"),m$e=o("LukeConfig"),g$e=o(" (LUKE model)"),h$e=l(),pm=a("li"),BW=a("strong"),p$e=o("lxmert"),_$e=o(" \u2014 "),kL=a("a"),u$e=o("LxmertConfig"),b$e=o(" (LXMERT model)"),v$e=l(),_m=a("li"),kW=a("strong"),T$e=o("m2m_100"),F$e=o(" \u2014 "),xL=a("a"),C$e=o("M2M100Config"),M$e=o(" (M2M100 model)"),E$e=l(),um=a("li"),xW=a("strong"),y$e=o("marian"),w$e=o(" \u2014 "),RL=a("a"),A$e=o("MarianConfig"),L$e=o(" (Marian model)"),B$e=l(),bm=a("li"),RW=a("strong"),k$e=o("mbart"),x$e=o(" \u2014 "),SL=a("a"),R$e=o("MBartConfig"),S$e=o(" (mBART model)"),P$e=l(),vm=a("li"),SW=a("strong"),$$e=o("megatron-bert"),I$e=o(" \u2014 "),PL=a("a"),j$e=o("MegatronBertConfig"),N$e=o(" (MegatronBert model)"),D$e=l(),Tm=a("li"),PW=a("strong"),q$e=o("mobilebert"),G$e=o(" \u2014 "),$L=a("a"),O$e=o("MobileBertConfig"),X$e=o(" (MobileBERT model)"),z$e=l(),Fm=a("li"),$W=a("strong"),V$e=o("mpnet"),W$e=o(" \u2014 "),IL=a("a"),Q$e=o("MPNetConfig"),H$e=o(" (MPNet model)"),U$e=l(),Cm=a("li"),IW=a("strong"),J$e=o("mt5"),Y$e=o(" \u2014 "),jL=a("a"),K$e=o("MT5Config"),Z$e=o(" (mT5 model)"),eIe=l(),Mm=a("li"),jW=a("strong"),oIe=o("nystromformer"),rIe=o(" \u2014 "),NL=a("a"),tIe=o("NystromformerConfig"),aIe=o(" (Nystromformer model)"),nIe=l(),Em=a("li"),NW=a("strong"),sIe=o("openai-gpt"),lIe=o(" \u2014 "),DL=a("a"),iIe=o("OpenAIGPTConfig"),dIe=o(" (OpenAI GPT model)"),cIe=l(),ym=a("li"),DW=a("strong"),fIe=o("pegasus"),mIe=o(" \u2014 "),qL=a("a"),gIe=o("PegasusConfig"),hIe=o(" (Pegasus model)"),pIe=l(),wm=a("li"),qW=a("strong"),_Ie=o("perceiver"),uIe=o(" \u2014 "),GL=a("a"),bIe=o("PerceiverConfig"),vIe=o(" (Perceiver model)"),TIe=l(),Am=a("li"),GW=a("strong"),FIe=o("plbart"),CIe=o(" \u2014 "),OL=a("a"),MIe=o("PLBartConfig"),EIe=o(" (PLBart model)"),yIe=l(),Lm=a("li"),OW=a("strong"),wIe=o("poolformer"),AIe=o(" \u2014 "),XL=a("a"),LIe=o("PoolFormerConfig"),BIe=o(" (PoolFormer model)"),kIe=l(),Bm=a("li"),XW=a("strong"),xIe=o("prophetnet"),RIe=o(" \u2014 "),zL=a("a"),SIe=o("ProphetNetConfig"),PIe=o(" (ProphetNet model)"),$Ie=l(),km=a("li"),zW=a("strong"),IIe=o("qdqbert"),jIe=o(" \u2014 "),VL=a("a"),NIe=o("QDQBertConfig"),DIe=o(" (QDQBert model)"),qIe=l(),xm=a("li"),VW=a("strong"),GIe=o("rag"),OIe=o(" \u2014 "),WL=a("a"),XIe=o("RagConfig"),zIe=o(" (RAG model)"),VIe=l(),Rm=a("li"),WW=a("strong"),WIe=o("realm"),QIe=o(" \u2014 "),QL=a("a"),HIe=o("RealmConfig"),UIe=o(" (Realm model)"),JIe=l(),Sm=a("li"),QW=a("strong"),YIe=o("reformer"),KIe=o(" \u2014 "),HL=a("a"),ZIe=o("ReformerConfig"),eje=o(" (Reformer model)"),oje=l(),Pm=a("li"),HW=a("strong"),rje=o("rembert"),tje=o(" \u2014 "),UL=a("a"),aje=o("RemBertConfig"),nje=o(" (RemBERT model)"),sje=l(),$m=a("li"),UW=a("strong"),lje=o("retribert"),ije=o(" \u2014 "),JL=a("a"),dje=o("RetriBertConfig"),cje=o(" (RetriBERT model)"),fje=l(),Im=a("li"),JW=a("strong"),mje=o("roberta"),gje=o(" \u2014 "),YL=a("a"),hje=o("RobertaConfig"),pje=o(" (RoBERTa model)"),_je=l(),jm=a("li"),YW=a("strong"),uje=o("roformer"),bje=o(" \u2014 "),KL=a("a"),vje=o("RoFormerConfig"),Tje=o(" (RoFormer model)"),Fje=l(),Nm=a("li"),KW=a("strong"),Cje=o("segformer"),Mje=o(" \u2014 "),ZL=a("a"),Eje=o("SegformerConfig"),yje=o(" (SegFormer model)"),wje=l(),Dm=a("li"),ZW=a("strong"),Aje=o("sew"),Lje=o(" \u2014 "),e8=a("a"),Bje=o("SEWConfig"),kje=o(" (SEW model)"),xje=l(),qm=a("li"),eQ=a("strong"),Rje=o("sew-d"),Sje=o(" \u2014 "),o8=a("a"),Pje=o("SEWDConfig"),$je=o(" (SEW-D model)"),Ije=l(),Gm=a("li"),oQ=a("strong"),jje=o("speech-encoder-decoder"),Nje=o(" \u2014 "),r8=a("a"),Dje=o("SpeechEncoderDecoderConfig"),qje=o(" (Speech Encoder decoder model)"),Gje=l(),Om=a("li"),rQ=a("strong"),Oje=o("speech_to_text"),Xje=o(" \u2014 "),t8=a("a"),zje=o("Speech2TextConfig"),Vje=o(" (Speech2Text model)"),Wje=l(),Xm=a("li"),tQ=a("strong"),Qje=o("speech_to_text_2"),Hje=o(" \u2014 "),a8=a("a"),Uje=o("Speech2Text2Config"),Jje=o(" (Speech2Text2 model)"),Yje=l(),zm=a("li"),aQ=a("strong"),Kje=o("splinter"),Zje=o(" \u2014 "),n8=a("a"),eNe=o("SplinterConfig"),oNe=o(" (Splinter model)"),rNe=l(),Vm=a("li"),nQ=a("strong"),tNe=o("squeezebert"),aNe=o(" \u2014 "),s8=a("a"),nNe=o("SqueezeBertConfig"),sNe=o(" (SqueezeBERT model)"),lNe=l(),Wm=a("li"),sQ=a("strong"),iNe=o("swin"),dNe=o(" \u2014 "),l8=a("a"),cNe=o("SwinConfig"),fNe=o(" (Swin model)"),mNe=l(),Qm=a("li"),lQ=a("strong"),gNe=o("t5"),hNe=o(" \u2014 "),i8=a("a"),pNe=o("T5Config"),_Ne=o(" (T5 model)"),uNe=l(),Hm=a("li"),iQ=a("strong"),bNe=o("tapas"),vNe=o(" \u2014 "),d8=a("a"),TNe=o("TapasConfig"),FNe=o(" (TAPAS model)"),CNe=l(),Um=a("li"),dQ=a("strong"),MNe=o("transfo-xl"),ENe=o(" \u2014 "),c8=a("a"),yNe=o("TransfoXLConfig"),wNe=o(" (Transformer-XL model)"),ANe=l(),Jm=a("li"),cQ=a("strong"),LNe=o("trocr"),BNe=o(" \u2014 "),f8=a("a"),kNe=o("TrOCRConfig"),xNe=o(" (TrOCR model)"),RNe=l(),Ym=a("li"),fQ=a("strong"),SNe=o("unispeech"),PNe=o(" \u2014 "),m8=a("a"),$Ne=o("UniSpeechConfig"),INe=o(" (UniSpeech model)"),jNe=l(),Km=a("li"),mQ=a("strong"),NNe=o("unispeech-sat"),DNe=o(" \u2014 "),g8=a("a"),qNe=o("UniSpeechSatConfig"),GNe=o(" (UniSpeechSat model)"),ONe=l(),Zm=a("li"),gQ=a("strong"),XNe=o("vilt"),zNe=o(" \u2014 "),h8=a("a"),VNe=o("ViltConfig"),WNe=o(" (ViLT model)"),QNe=l(),eg=a("li"),hQ=a("strong"),HNe=o("vision-encoder-decoder"),UNe=o(" \u2014 "),p8=a("a"),JNe=o("VisionEncoderDecoderConfig"),YNe=o(" (Vision Encoder decoder model)"),KNe=l(),og=a("li"),pQ=a("strong"),ZNe=o("vision-text-dual-encoder"),eDe=o(" \u2014 "),_8=a("a"),oDe=o("VisionTextDualEncoderConfig"),rDe=o(" (VisionTextDualEncoder model)"),tDe=l(),rg=a("li"),_Q=a("strong"),aDe=o("visual_bert"),nDe=o(" \u2014 "),u8=a("a"),sDe=o("VisualBertConfig"),lDe=o(" (VisualBert model)"),iDe=l(),tg=a("li"),uQ=a("strong"),dDe=o("vit"),cDe=o(" \u2014 "),b8=a("a"),fDe=o("ViTConfig"),mDe=o(" (ViT model)"),gDe=l(),ag=a("li"),bQ=a("strong"),hDe=o("vit_mae"),pDe=o(" \u2014 "),v8=a("a"),_De=o("ViTMAEConfig"),uDe=o(" (ViTMAE model)"),bDe=l(),ng=a("li"),vQ=a("strong"),vDe=o("wav2vec2"),TDe=o(" \u2014 "),T8=a("a"),FDe=o("Wav2Vec2Config"),CDe=o(" (Wav2Vec2 model)"),MDe=l(),sg=a("li"),TQ=a("strong"),EDe=o("wavlm"),yDe=o(" \u2014 "),F8=a("a"),wDe=o("WavLMConfig"),ADe=o(" (WavLM model)"),LDe=l(),lg=a("li"),FQ=a("strong"),BDe=o("xglm"),kDe=o(" \u2014 "),C8=a("a"),xDe=o("XGLMConfig"),RDe=o(" (XGLM model)"),SDe=l(),ig=a("li"),CQ=a("strong"),PDe=o("xlm"),$De=o(" \u2014 "),M8=a("a"),IDe=o("XLMConfig"),jDe=o(" (XLM model)"),NDe=l(),dg=a("li"),MQ=a("strong"),DDe=o("xlm-prophetnet"),qDe=o(" \u2014 "),E8=a("a"),GDe=o("XLMProphetNetConfig"),ODe=o(" (XLMProphetNet model)"),XDe=l(),cg=a("li"),EQ=a("strong"),zDe=o("xlm-roberta"),VDe=o(" \u2014 "),y8=a("a"),WDe=o("XLMRobertaConfig"),QDe=o(" (XLM-RoBERTa model)"),HDe=l(),fg=a("li"),yQ=a("strong"),UDe=o("xlm-roberta-xl"),JDe=o(" \u2014 "),w8=a("a"),YDe=o("XLMRobertaXLConfig"),KDe=o(" (XLM-RoBERTa-XL model)"),ZDe=l(),mg=a("li"),wQ=a("strong"),eqe=o("xlnet"),oqe=o(" \u2014 "),A8=a("a"),rqe=o("XLNetConfig"),tqe=o(" (XLNet model)"),aqe=l(),gg=a("li"),AQ=a("strong"),nqe=o("yoso"),sqe=o(" \u2014 "),L8=a("a"),lqe=o("YosoConfig"),iqe=o(" (YOSO model)"),dqe=l(),LQ=a("p"),cqe=o("Examples:"),fqe=l(),f(m4.$$.fragment),mqe=l(),hg=a("div"),f(g4.$$.fragment),gqe=l(),BQ=a("p"),hqe=o("Register a new configuration for this class."),cLe=l(),Ii=a("h2"),pg=a("a"),kQ=a("span"),f(h4.$$.fragment),pqe=l(),xQ=a("span"),_qe=o("AutoTokenizer"),fLe=l(),Oo=a("div"),f(p4.$$.fragment),uqe=l(),_4=a("p"),bqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B8=a("a"),vqe=o("AutoTokenizer.from_pretrained()"),Tqe=o(" class method."),Fqe=l(),u4=a("p"),Cqe=o("This class cannot be instantiated directly using "),RQ=a("code"),Mqe=o("__init__()"),Eqe=o(" (throws an error)."),yqe=l(),mo=a("div"),f(b4.$$.fragment),wqe=l(),SQ=a("p"),Aqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Lqe=l(),ja=a("p"),Bqe=o("The tokenizer class to instantiate is selected based on the "),PQ=a("code"),kqe=o("model_type"),xqe=o(` property of the config object (either
passed as an argument or loaded from `),$Q=a("code"),Rqe=o("pretrained_model_name_or_path"),Sqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=a("code"),Pqe=o("pretrained_model_name_or_path"),$qe=o(":"),Iqe=l(),M=a("ul"),Dn=a("li"),jQ=a("strong"),jqe=o("albert"),Nqe=o(" \u2014 "),k8=a("a"),Dqe=o("AlbertTokenizer"),qqe=o(" or "),x8=a("a"),Gqe=o("AlbertTokenizerFast"),Oqe=o(" (ALBERT model)"),Xqe=l(),qn=a("li"),NQ=a("strong"),zqe=o("bart"),Vqe=o(" \u2014 "),R8=a("a"),Wqe=o("BartTokenizer"),Qqe=o(" or "),S8=a("a"),Hqe=o("BartTokenizerFast"),Uqe=o(" (BART model)"),Jqe=l(),Gn=a("li"),DQ=a("strong"),Yqe=o("barthez"),Kqe=o(" \u2014 "),P8=a("a"),Zqe=o("BarthezTokenizer"),eGe=o(" or "),$8=a("a"),oGe=o("BarthezTokenizerFast"),rGe=o(" (BARThez model)"),tGe=l(),_g=a("li"),qQ=a("strong"),aGe=o("bartpho"),nGe=o(" \u2014 "),I8=a("a"),sGe=o("BartphoTokenizer"),lGe=o(" (BARTpho model)"),iGe=l(),On=a("li"),GQ=a("strong"),dGe=o("bert"),cGe=o(" \u2014 "),j8=a("a"),fGe=o("BertTokenizer"),mGe=o(" or "),N8=a("a"),gGe=o("BertTokenizerFast"),hGe=o(" (BERT model)"),pGe=l(),ug=a("li"),OQ=a("strong"),_Ge=o("bert-generation"),uGe=o(" \u2014 "),D8=a("a"),bGe=o("BertGenerationTokenizer"),vGe=o(" (Bert Generation model)"),TGe=l(),bg=a("li"),XQ=a("strong"),FGe=o("bert-japanese"),CGe=o(" \u2014 "),q8=a("a"),MGe=o("BertJapaneseTokenizer"),EGe=o(" (BertJapanese model)"),yGe=l(),vg=a("li"),zQ=a("strong"),wGe=o("bertweet"),AGe=o(" \u2014 "),G8=a("a"),LGe=o("BertweetTokenizer"),BGe=o(" (Bertweet model)"),kGe=l(),Xn=a("li"),VQ=a("strong"),xGe=o("big_bird"),RGe=o(" \u2014 "),O8=a("a"),SGe=o("BigBirdTokenizer"),PGe=o(" or "),X8=a("a"),$Ge=o("BigBirdTokenizerFast"),IGe=o(" (BigBird model)"),jGe=l(),zn=a("li"),WQ=a("strong"),NGe=o("bigbird_pegasus"),DGe=o(" \u2014 "),z8=a("a"),qGe=o("PegasusTokenizer"),GGe=o(" or "),V8=a("a"),OGe=o("PegasusTokenizerFast"),XGe=o(" (BigBirdPegasus model)"),zGe=l(),Vn=a("li"),QQ=a("strong"),VGe=o("blenderbot"),WGe=o(" \u2014 "),W8=a("a"),QGe=o("BlenderbotTokenizer"),HGe=o(" or "),Q8=a("a"),UGe=o("BlenderbotTokenizerFast"),JGe=o(" (Blenderbot model)"),YGe=l(),Tg=a("li"),HQ=a("strong"),KGe=o("blenderbot-small"),ZGe=o(" \u2014 "),H8=a("a"),eOe=o("BlenderbotSmallTokenizer"),oOe=o(" (BlenderbotSmall model)"),rOe=l(),Fg=a("li"),UQ=a("strong"),tOe=o("byt5"),aOe=o(" \u2014 "),U8=a("a"),nOe=o("ByT5Tokenizer"),sOe=o(" (ByT5 model)"),lOe=l(),Wn=a("li"),JQ=a("strong"),iOe=o("camembert"),dOe=o(" \u2014 "),J8=a("a"),cOe=o("CamembertTokenizer"),fOe=o(" or "),Y8=a("a"),mOe=o("CamembertTokenizerFast"),gOe=o(" (CamemBERT model)"),hOe=l(),Cg=a("li"),YQ=a("strong"),pOe=o("canine"),_Oe=o(" \u2014 "),K8=a("a"),uOe=o("CanineTokenizer"),bOe=o(" (Canine model)"),vOe=l(),Qn=a("li"),KQ=a("strong"),TOe=o("clip"),FOe=o(" \u2014 "),Z8=a("a"),COe=o("CLIPTokenizer"),MOe=o(" or "),eB=a("a"),EOe=o("CLIPTokenizerFast"),yOe=o(" (CLIP model)"),wOe=l(),Hn=a("li"),ZQ=a("strong"),AOe=o("convbert"),LOe=o(" \u2014 "),oB=a("a"),BOe=o("ConvBertTokenizer"),kOe=o(" or "),rB=a("a"),xOe=o("ConvBertTokenizerFast"),ROe=o(" (ConvBERT model)"),SOe=l(),Un=a("li"),eH=a("strong"),POe=o("cpm"),$Oe=o(" \u2014 "),tB=a("a"),IOe=o("CpmTokenizer"),jOe=o(" or "),oH=a("code"),NOe=o("CpmTokenizerFast"),DOe=o(" (CPM model)"),qOe=l(),Mg=a("li"),rH=a("strong"),GOe=o("ctrl"),OOe=o(" \u2014 "),aB=a("a"),XOe=o("CTRLTokenizer"),zOe=o(" (CTRL model)"),VOe=l(),Jn=a("li"),tH=a("strong"),WOe=o("deberta"),QOe=o(" \u2014 "),nB=a("a"),HOe=o("DebertaTokenizer"),UOe=o(" or "),sB=a("a"),JOe=o("DebertaTokenizerFast"),YOe=o(" (DeBERTa model)"),KOe=l(),Eg=a("li"),aH=a("strong"),ZOe=o("deberta-v2"),eXe=o(" \u2014 "),lB=a("a"),oXe=o("DebertaV2Tokenizer"),rXe=o(" (DeBERTa-v2 model)"),tXe=l(),Yn=a("li"),nH=a("strong"),aXe=o("distilbert"),nXe=o(" \u2014 "),iB=a("a"),sXe=o("DistilBertTokenizer"),lXe=o(" or "),dB=a("a"),iXe=o("DistilBertTokenizerFast"),dXe=o(" (DistilBERT model)"),cXe=l(),Kn=a("li"),sH=a("strong"),fXe=o("dpr"),mXe=o(" \u2014 "),cB=a("a"),gXe=o("DPRQuestionEncoderTokenizer"),hXe=o(" or "),fB=a("a"),pXe=o("DPRQuestionEncoderTokenizerFast"),_Xe=o(" (DPR model)"),uXe=l(),Zn=a("li"),lH=a("strong"),bXe=o("electra"),vXe=o(" \u2014 "),mB=a("a"),TXe=o("ElectraTokenizer"),FXe=o(" or "),gB=a("a"),CXe=o("ElectraTokenizerFast"),MXe=o(" (ELECTRA model)"),EXe=l(),yg=a("li"),iH=a("strong"),yXe=o("flaubert"),wXe=o(" \u2014 "),hB=a("a"),AXe=o("FlaubertTokenizer"),LXe=o(" (FlauBERT model)"),BXe=l(),es=a("li"),dH=a("strong"),kXe=o("fnet"),xXe=o(" \u2014 "),pB=a("a"),RXe=o("FNetTokenizer"),SXe=o(" or "),_B=a("a"),PXe=o("FNetTokenizerFast"),$Xe=o(" (FNet model)"),IXe=l(),wg=a("li"),cH=a("strong"),jXe=o("fsmt"),NXe=o(" \u2014 "),uB=a("a"),DXe=o("FSMTTokenizer"),qXe=o(" (FairSeq Machine-Translation model)"),GXe=l(),os=a("li"),fH=a("strong"),OXe=o("funnel"),XXe=o(" \u2014 "),bB=a("a"),zXe=o("FunnelTokenizer"),VXe=o(" or "),vB=a("a"),WXe=o("FunnelTokenizerFast"),QXe=o(" (Funnel Transformer model)"),HXe=l(),rs=a("li"),mH=a("strong"),UXe=o("gpt2"),JXe=o(" \u2014 "),TB=a("a"),YXe=o("GPT2Tokenizer"),KXe=o(" or "),FB=a("a"),ZXe=o("GPT2TokenizerFast"),eze=o(" (OpenAI GPT-2 model)"),oze=l(),ts=a("li"),gH=a("strong"),rze=o("gpt_neo"),tze=o(" \u2014 "),CB=a("a"),aze=o("GPT2Tokenizer"),nze=o(" or "),MB=a("a"),sze=o("GPT2TokenizerFast"),lze=o(" (GPT Neo model)"),ize=l(),as=a("li"),hH=a("strong"),dze=o("herbert"),cze=o(" \u2014 "),EB=a("a"),fze=o("HerbertTokenizer"),mze=o(" or "),yB=a("a"),gze=o("HerbertTokenizerFast"),hze=o(" (HerBERT model)"),pze=l(),Ag=a("li"),pH=a("strong"),_ze=o("hubert"),uze=o(" \u2014 "),wB=a("a"),bze=o("Wav2Vec2CTCTokenizer"),vze=o(" (Hubert model)"),Tze=l(),ns=a("li"),_H=a("strong"),Fze=o("ibert"),Cze=o(" \u2014 "),AB=a("a"),Mze=o("RobertaTokenizer"),Eze=o(" or "),LB=a("a"),yze=o("RobertaTokenizerFast"),wze=o(" (I-BERT model)"),Aze=l(),ss=a("li"),uH=a("strong"),Lze=o("layoutlm"),Bze=o(" \u2014 "),BB=a("a"),kze=o("LayoutLMTokenizer"),xze=o(" or "),kB=a("a"),Rze=o("LayoutLMTokenizerFast"),Sze=o(" (LayoutLM model)"),Pze=l(),ls=a("li"),bH=a("strong"),$ze=o("layoutlmv2"),Ize=o(" \u2014 "),xB=a("a"),jze=o("LayoutLMv2Tokenizer"),Nze=o(" or "),RB=a("a"),Dze=o("LayoutLMv2TokenizerFast"),qze=o(" (LayoutLMv2 model)"),Gze=l(),is=a("li"),vH=a("strong"),Oze=o("layoutxlm"),Xze=o(" \u2014 "),SB=a("a"),zze=o("LayoutXLMTokenizer"),Vze=o(" or "),PB=a("a"),Wze=o("LayoutXLMTokenizerFast"),Qze=o(" (LayoutXLM model)"),Hze=l(),ds=a("li"),TH=a("strong"),Uze=o("led"),Jze=o(" \u2014 "),$B=a("a"),Yze=o("LEDTokenizer"),Kze=o(" or "),IB=a("a"),Zze=o("LEDTokenizerFast"),eVe=o(" (LED model)"),oVe=l(),cs=a("li"),FH=a("strong"),rVe=o("longformer"),tVe=o(" \u2014 "),jB=a("a"),aVe=o("LongformerTokenizer"),nVe=o(" or "),NB=a("a"),sVe=o("LongformerTokenizerFast"),lVe=o(" (Longformer model)"),iVe=l(),Lg=a("li"),CH=a("strong"),dVe=o("luke"),cVe=o(" \u2014 "),DB=a("a"),fVe=o("LukeTokenizer"),mVe=o(" (LUKE model)"),gVe=l(),fs=a("li"),MH=a("strong"),hVe=o("lxmert"),pVe=o(" \u2014 "),qB=a("a"),_Ve=o("LxmertTokenizer"),uVe=o(" or "),GB=a("a"),bVe=o("LxmertTokenizerFast"),vVe=o(" (LXMERT model)"),TVe=l(),Bg=a("li"),EH=a("strong"),FVe=o("m2m_100"),CVe=o(" \u2014 "),OB=a("a"),MVe=o("M2M100Tokenizer"),EVe=o(" (M2M100 model)"),yVe=l(),kg=a("li"),yH=a("strong"),wVe=o("marian"),AVe=o(" \u2014 "),XB=a("a"),LVe=o("MarianTokenizer"),BVe=o(" (Marian model)"),kVe=l(),ms=a("li"),wH=a("strong"),xVe=o("mbart"),RVe=o(" \u2014 "),zB=a("a"),SVe=o("MBartTokenizer"),PVe=o(" or "),VB=a("a"),$Ve=o("MBartTokenizerFast"),IVe=o(" (mBART model)"),jVe=l(),gs=a("li"),AH=a("strong"),NVe=o("mbart50"),DVe=o(" \u2014 "),WB=a("a"),qVe=o("MBart50Tokenizer"),GVe=o(" or "),QB=a("a"),OVe=o("MBart50TokenizerFast"),XVe=o(" (mBART-50 model)"),zVe=l(),xg=a("li"),LH=a("strong"),VVe=o("mluke"),WVe=o(" \u2014 "),HB=a("a"),QVe=o("MLukeTokenizer"),HVe=o(" (mLUKE model)"),UVe=l(),hs=a("li"),BH=a("strong"),JVe=o("mobilebert"),YVe=o(" \u2014 "),UB=a("a"),KVe=o("MobileBertTokenizer"),ZVe=o(" or "),JB=a("a"),eWe=o("MobileBertTokenizerFast"),oWe=o(" (MobileBERT model)"),rWe=l(),ps=a("li"),kH=a("strong"),tWe=o("mpnet"),aWe=o(" \u2014 "),YB=a("a"),nWe=o("MPNetTokenizer"),sWe=o(" or "),KB=a("a"),lWe=o("MPNetTokenizerFast"),iWe=o(" (MPNet model)"),dWe=l(),_s=a("li"),xH=a("strong"),cWe=o("mt5"),fWe=o(" \u2014 "),ZB=a("a"),mWe=o("MT5Tokenizer"),gWe=o(" or "),ek=a("a"),hWe=o("MT5TokenizerFast"),pWe=o(" (mT5 model)"),_We=l(),us=a("li"),RH=a("strong"),uWe=o("openai-gpt"),bWe=o(" \u2014 "),ok=a("a"),vWe=o("OpenAIGPTTokenizer"),TWe=o(" or "),rk=a("a"),FWe=o("OpenAIGPTTokenizerFast"),CWe=o(" (OpenAI GPT model)"),MWe=l(),bs=a("li"),SH=a("strong"),EWe=o("pegasus"),yWe=o(" \u2014 "),tk=a("a"),wWe=o("PegasusTokenizer"),AWe=o(" or "),ak=a("a"),LWe=o("PegasusTokenizerFast"),BWe=o(" (Pegasus model)"),kWe=l(),Rg=a("li"),PH=a("strong"),xWe=o("perceiver"),RWe=o(" \u2014 "),nk=a("a"),SWe=o("PerceiverTokenizer"),PWe=o(" (Perceiver model)"),$We=l(),Sg=a("li"),$H=a("strong"),IWe=o("phobert"),jWe=o(" \u2014 "),sk=a("a"),NWe=o("PhobertTokenizer"),DWe=o(" (PhoBERT model)"),qWe=l(),Pg=a("li"),IH=a("strong"),GWe=o("plbart"),OWe=o(" \u2014 "),lk=a("a"),XWe=o("PLBartTokenizer"),zWe=o(" (PLBart model)"),VWe=l(),$g=a("li"),jH=a("strong"),WWe=o("prophetnet"),QWe=o(" \u2014 "),ik=a("a"),HWe=o("ProphetNetTokenizer"),UWe=o(" (ProphetNet model)"),JWe=l(),vs=a("li"),NH=a("strong"),YWe=o("qdqbert"),KWe=o(" \u2014 "),dk=a("a"),ZWe=o("BertTokenizer"),eQe=o(" or "),ck=a("a"),oQe=o("BertTokenizerFast"),rQe=o(" (QDQBert model)"),tQe=l(),Ig=a("li"),DH=a("strong"),aQe=o("rag"),nQe=o(" \u2014 "),fk=a("a"),sQe=o("RagTokenizer"),lQe=o(" (RAG model)"),iQe=l(),Ts=a("li"),qH=a("strong"),dQe=o("reformer"),cQe=o(" \u2014 "),mk=a("a"),fQe=o("ReformerTokenizer"),mQe=o(" or "),gk=a("a"),gQe=o("ReformerTokenizerFast"),hQe=o(" (Reformer model)"),pQe=l(),Fs=a("li"),GH=a("strong"),_Qe=o("rembert"),uQe=o(" \u2014 "),hk=a("a"),bQe=o("RemBertTokenizer"),vQe=o(" or "),pk=a("a"),TQe=o("RemBertTokenizerFast"),FQe=o(" (RemBERT model)"),CQe=l(),Cs=a("li"),OH=a("strong"),MQe=o("retribert"),EQe=o(" \u2014 "),_k=a("a"),yQe=o("RetriBertTokenizer"),wQe=o(" or "),uk=a("a"),AQe=o("RetriBertTokenizerFast"),LQe=o(" (RetriBERT model)"),BQe=l(),Ms=a("li"),XH=a("strong"),kQe=o("roberta"),xQe=o(" \u2014 "),bk=a("a"),RQe=o("RobertaTokenizer"),SQe=o(" or "),vk=a("a"),PQe=o("RobertaTokenizerFast"),$Qe=o(" (RoBERTa model)"),IQe=l(),Es=a("li"),zH=a("strong"),jQe=o("roformer"),NQe=o(" \u2014 "),Tk=a("a"),DQe=o("RoFormerTokenizer"),qQe=o(" or "),Fk=a("a"),GQe=o("RoFormerTokenizerFast"),OQe=o(" (RoFormer model)"),XQe=l(),jg=a("li"),VH=a("strong"),zQe=o("speech_to_text"),VQe=o(" \u2014 "),Ck=a("a"),WQe=o("Speech2TextTokenizer"),QQe=o(" (Speech2Text model)"),HQe=l(),Ng=a("li"),WH=a("strong"),UQe=o("speech_to_text_2"),JQe=o(" \u2014 "),Mk=a("a"),YQe=o("Speech2Text2Tokenizer"),KQe=o(" (Speech2Text2 model)"),ZQe=l(),ys=a("li"),QH=a("strong"),eHe=o("splinter"),oHe=o(" \u2014 "),Ek=a("a"),rHe=o("SplinterTokenizer"),tHe=o(" or "),yk=a("a"),aHe=o("SplinterTokenizerFast"),nHe=o(" (Splinter model)"),sHe=l(),ws=a("li"),HH=a("strong"),lHe=o("squeezebert"),iHe=o(" \u2014 "),wk=a("a"),dHe=o("SqueezeBertTokenizer"),cHe=o(" or "),Ak=a("a"),fHe=o("SqueezeBertTokenizerFast"),mHe=o(" (SqueezeBERT model)"),gHe=l(),As=a("li"),UH=a("strong"),hHe=o("t5"),pHe=o(" \u2014 "),Lk=a("a"),_He=o("T5Tokenizer"),uHe=o(" or "),Bk=a("a"),bHe=o("T5TokenizerFast"),vHe=o(" (T5 model)"),THe=l(),Dg=a("li"),JH=a("strong"),FHe=o("tapas"),CHe=o(" \u2014 "),kk=a("a"),MHe=o("TapasTokenizer"),EHe=o(" (TAPAS model)"),yHe=l(),qg=a("li"),YH=a("strong"),wHe=o("transfo-xl"),AHe=o(" \u2014 "),xk=a("a"),LHe=o("TransfoXLTokenizer"),BHe=o(" (Transformer-XL model)"),kHe=l(),Gg=a("li"),KH=a("strong"),xHe=o("wav2vec2"),RHe=o(" \u2014 "),Rk=a("a"),SHe=o("Wav2Vec2CTCTokenizer"),PHe=o(" (Wav2Vec2 model)"),$He=l(),Og=a("li"),ZH=a("strong"),IHe=o("wav2vec2_phoneme"),jHe=o(" \u2014 "),Sk=a("a"),NHe=o("Wav2Vec2PhonemeCTCTokenizer"),DHe=o(" (Wav2Vec2Phoneme model)"),qHe=l(),Ls=a("li"),eU=a("strong"),GHe=o("xglm"),OHe=o(" \u2014 "),Pk=a("a"),XHe=o("XGLMTokenizer"),zHe=o(" or "),$k=a("a"),VHe=o("XGLMTokenizerFast"),WHe=o(" (XGLM model)"),QHe=l(),Xg=a("li"),oU=a("strong"),HHe=o("xlm"),UHe=o(" \u2014 "),Ik=a("a"),JHe=o("XLMTokenizer"),YHe=o(" (XLM model)"),KHe=l(),zg=a("li"),rU=a("strong"),ZHe=o("xlm-prophetnet"),eUe=o(" \u2014 "),jk=a("a"),oUe=o("XLMProphetNetTokenizer"),rUe=o(" (XLMProphetNet model)"),tUe=l(),Bs=a("li"),tU=a("strong"),aUe=o("xlm-roberta"),nUe=o(" \u2014 "),Nk=a("a"),sUe=o("XLMRobertaTokenizer"),lUe=o(" or "),Dk=a("a"),iUe=o("XLMRobertaTokenizerFast"),dUe=o(" (XLM-RoBERTa model)"),cUe=l(),ks=a("li"),aU=a("strong"),fUe=o("xlnet"),mUe=o(" \u2014 "),qk=a("a"),gUe=o("XLNetTokenizer"),hUe=o(" or "),Gk=a("a"),pUe=o("XLNetTokenizerFast"),_Ue=o(" (XLNet model)"),uUe=l(),nU=a("p"),bUe=o("Examples:"),vUe=l(),f(v4.$$.fragment),TUe=l(),Vg=a("div"),f(T4.$$.fragment),FUe=l(),sU=a("p"),CUe=o("Register a new tokenizer in this mapping."),mLe=l(),ji=a("h2"),Wg=a("a"),lU=a("span"),f(F4.$$.fragment),MUe=l(),iU=a("span"),EUe=o("AutoFeatureExtractor"),gLe=l(),Xo=a("div"),f(C4.$$.fragment),yUe=l(),M4=a("p"),wUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=a("a"),AUe=o("AutoFeatureExtractor.from_pretrained()"),LUe=o(" class method."),BUe=l(),E4=a("p"),kUe=o("This class cannot be instantiated directly using "),dU=a("code"),xUe=o("__init__()"),RUe=o(" (throws an error)."),SUe=l(),Le=a("div"),f(y4.$$.fragment),PUe=l(),cU=a("p"),$Ue=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),IUe=l(),Na=a("p"),jUe=o("The feature extractor class to instantiate is selected based on the "),fU=a("code"),NUe=o("model_type"),DUe=o(` property of the config object
(either passed as an argument or loaded from `),mU=a("code"),qUe=o("pretrained_model_name_or_path"),GUe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=a("code"),OUe=o("pretrained_model_name_or_path"),XUe=o(":"),zUe=l(),se=a("ul"),Qg=a("li"),hU=a("strong"),VUe=o("beit"),WUe=o(" \u2014 "),Xk=a("a"),QUe=o("BeitFeatureExtractor"),HUe=o(" (BEiT model)"),UUe=l(),Hg=a("li"),pU=a("strong"),JUe=o("clip"),YUe=o(" \u2014 "),zk=a("a"),KUe=o("CLIPFeatureExtractor"),ZUe=o(" (CLIP model)"),eJe=l(),Ug=a("li"),_U=a("strong"),oJe=o("convnext"),rJe=o(" \u2014 "),Vk=a("a"),tJe=o("ConvNextFeatureExtractor"),aJe=o(" (ConvNext model)"),nJe=l(),Jg=a("li"),uU=a("strong"),sJe=o("deit"),lJe=o(" \u2014 "),Wk=a("a"),iJe=o("DeiTFeatureExtractor"),dJe=o(" (DeiT model)"),cJe=l(),Yg=a("li"),bU=a("strong"),fJe=o("detr"),mJe=o(" \u2014 "),Qk=a("a"),gJe=o("DetrFeatureExtractor"),hJe=o(" (DETR model)"),pJe=l(),Kg=a("li"),vU=a("strong"),_Je=o("hubert"),uJe=o(" \u2014 "),Hk=a("a"),bJe=o("Wav2Vec2FeatureExtractor"),vJe=o(" (Hubert model)"),TJe=l(),Zg=a("li"),TU=a("strong"),FJe=o("layoutlmv2"),CJe=o(" \u2014 "),Uk=a("a"),MJe=o("LayoutLMv2FeatureExtractor"),EJe=o(" (LayoutLMv2 model)"),yJe=l(),eh=a("li"),FU=a("strong"),wJe=o("perceiver"),AJe=o(" \u2014 "),Jk=a("a"),LJe=o("PerceiverFeatureExtractor"),BJe=o(" (Perceiver model)"),kJe=l(),oh=a("li"),CU=a("strong"),xJe=o("poolformer"),RJe=o(" \u2014 "),Yk=a("a"),SJe=o("PoolFormerFeatureExtractor"),PJe=o(" (PoolFormer model)"),$Je=l(),rh=a("li"),MU=a("strong"),IJe=o("segformer"),jJe=o(" \u2014 "),Kk=a("a"),NJe=o("SegformerFeatureExtractor"),DJe=o(" (SegFormer model)"),qJe=l(),th=a("li"),EU=a("strong"),GJe=o("speech_to_text"),OJe=o(" \u2014 "),Zk=a("a"),XJe=o("Speech2TextFeatureExtractor"),zJe=o(" (Speech2Text model)"),VJe=l(),ah=a("li"),yU=a("strong"),WJe=o("swin"),QJe=o(" \u2014 "),ex=a("a"),HJe=o("ViTFeatureExtractor"),UJe=o(" (Swin model)"),JJe=l(),nh=a("li"),wU=a("strong"),YJe=o("vit"),KJe=o(" \u2014 "),ox=a("a"),ZJe=o("ViTFeatureExtractor"),eYe=o(" (ViT model)"),oYe=l(),sh=a("li"),AU=a("strong"),rYe=o("vit_mae"),tYe=o(" \u2014 "),rx=a("a"),aYe=o("ViTFeatureExtractor"),nYe=o(" (ViTMAE model)"),sYe=l(),lh=a("li"),LU=a("strong"),lYe=o("wav2vec2"),iYe=o(" \u2014 "),tx=a("a"),dYe=o("Wav2Vec2FeatureExtractor"),cYe=o(" (Wav2Vec2 model)"),fYe=l(),f(ih.$$.fragment),mYe=l(),BU=a("p"),gYe=o("Examples:"),hYe=l(),f(w4.$$.fragment),pYe=l(),dh=a("div"),f(A4.$$.fragment),_Ye=l(),kU=a("p"),uYe=o("Register a new feature extractor for this class."),hLe=l(),Ni=a("h2"),ch=a("a"),xU=a("span"),f(L4.$$.fragment),bYe=l(),RU=a("span"),vYe=o("AutoProcessor"),pLe=l(),zo=a("div"),f(B4.$$.fragment),TYe=l(),k4=a("p"),FYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=a("a"),CYe=o("AutoProcessor.from_pretrained()"),MYe=o(" class method."),EYe=l(),x4=a("p"),yYe=o("This class cannot be instantiated directly using "),SU=a("code"),wYe=o("__init__()"),AYe=o(" (throws an error)."),LYe=l(),Be=a("div"),f(R4.$$.fragment),BYe=l(),PU=a("p"),kYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),xYe=l(),Di=a("p"),RYe=o("The processor class to instantiate is selected based on the "),$U=a("code"),SYe=o("model_type"),PYe=o(` property of the config object (either
passed as an argument or loaded from `),IU=a("code"),$Ye=o("pretrained_model_name_or_path"),IYe=o(" if possible):"),jYe=l(),we=a("ul"),fh=a("li"),jU=a("strong"),NYe=o("clip"),DYe=o(" \u2014 "),nx=a("a"),qYe=o("CLIPProcessor"),GYe=o(" (CLIP model)"),OYe=l(),mh=a("li"),NU=a("strong"),XYe=o("layoutlmv2"),zYe=o(" \u2014 "),sx=a("a"),VYe=o("LayoutLMv2Processor"),WYe=o(" (LayoutLMv2 model)"),QYe=l(),gh=a("li"),DU=a("strong"),HYe=o("layoutxlm"),UYe=o(" \u2014 "),lx=a("a"),JYe=o("LayoutXLMProcessor"),YYe=o(" (LayoutXLM model)"),KYe=l(),hh=a("li"),qU=a("strong"),ZYe=o("speech_to_text"),eKe=o(" \u2014 "),ix=a("a"),oKe=o("Speech2TextProcessor"),rKe=o(" (Speech2Text model)"),tKe=l(),ph=a("li"),GU=a("strong"),aKe=o("speech_to_text_2"),nKe=o(" \u2014 "),dx=a("a"),sKe=o("Speech2Text2Processor"),lKe=o(" (Speech2Text2 model)"),iKe=l(),_h=a("li"),OU=a("strong"),dKe=o("trocr"),cKe=o(" \u2014 "),cx=a("a"),fKe=o("TrOCRProcessor"),mKe=o(" (TrOCR model)"),gKe=l(),uh=a("li"),XU=a("strong"),hKe=o("vision-text-dual-encoder"),pKe=o(" \u2014 "),fx=a("a"),_Ke=o("VisionTextDualEncoderProcessor"),uKe=o(" (VisionTextDualEncoder model)"),bKe=l(),bh=a("li"),zU=a("strong"),vKe=o("wav2vec2"),TKe=o(" \u2014 "),mx=a("a"),FKe=o("Wav2Vec2Processor"),CKe=o(" (Wav2Vec2 model)"),MKe=l(),f(vh.$$.fragment),EKe=l(),VU=a("p"),yKe=o("Examples:"),wKe=l(),f(S4.$$.fragment),AKe=l(),Th=a("div"),f(P4.$$.fragment),LKe=l(),WU=a("p"),BKe=o("Register a new processor for this class."),_Le=l(),qi=a("h2"),Fh=a("a"),QU=a("span"),f($4.$$.fragment),kKe=l(),HU=a("span"),xKe=o("AutoModel"),uLe=l(),Vo=a("div"),f(I4.$$.fragment),RKe=l(),Gi=a("p"),SKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=a("code"),PKe=o("from_pretrained()"),$Ke=o("class method or the "),JU=a("code"),IKe=o("from_config()"),jKe=o(`class
method.`),NKe=l(),j4=a("p"),DKe=o("This class cannot be instantiated directly using "),YU=a("code"),qKe=o("__init__()"),GKe=o(" (throws an error)."),OKe=l(),Nr=a("div"),f(N4.$$.fragment),XKe=l(),KU=a("p"),zKe=o("Instantiates one of the base model classes of the library from a configuration."),VKe=l(),Oi=a("p"),WKe=o(`Note:
Loading a model from its configuration file does `),ZU=a("strong"),QKe=o("not"),HKe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=a("code"),UKe=o("from_pretrained()"),JKe=o("to load the model weights."),YKe=l(),oJ=a("p"),KKe=o("Examples:"),ZKe=l(),f(D4.$$.fragment),eZe=l(),ke=a("div"),f(q4.$$.fragment),oZe=l(),rJ=a("p"),rZe=o("Instantiate one of the base model classes of the library from a pretrained model."),tZe=l(),Da=a("p"),aZe=o("The model class to instantiate is selected based on the "),tJ=a("code"),nZe=o("model_type"),sZe=o(` property of the config object (either
passed as an argument or loaded from `),aJ=a("code"),lZe=o("pretrained_model_name_or_path"),iZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=a("code"),dZe=o("pretrained_model_name_or_path"),cZe=o(":"),fZe=l(),F=a("ul"),Ch=a("li"),sJ=a("strong"),mZe=o("albert"),gZe=o(" \u2014 "),gx=a("a"),hZe=o("AlbertModel"),pZe=o(" (ALBERT model)"),_Ze=l(),Mh=a("li"),lJ=a("strong"),uZe=o("bart"),bZe=o(" \u2014 "),hx=a("a"),vZe=o("BartModel"),TZe=o(" (BART model)"),FZe=l(),Eh=a("li"),iJ=a("strong"),CZe=o("beit"),MZe=o(" \u2014 "),px=a("a"),EZe=o("BeitModel"),yZe=o(" (BEiT model)"),wZe=l(),yh=a("li"),dJ=a("strong"),AZe=o("bert"),LZe=o(" \u2014 "),_x=a("a"),BZe=o("BertModel"),kZe=o(" (BERT model)"),xZe=l(),wh=a("li"),cJ=a("strong"),RZe=o("bert-generation"),SZe=o(" \u2014 "),ux=a("a"),PZe=o("BertGenerationEncoder"),$Ze=o(" (Bert Generation model)"),IZe=l(),Ah=a("li"),fJ=a("strong"),jZe=o("big_bird"),NZe=o(" \u2014 "),bx=a("a"),DZe=o("BigBirdModel"),qZe=o(" (BigBird model)"),GZe=l(),Lh=a("li"),mJ=a("strong"),OZe=o("bigbird_pegasus"),XZe=o(" \u2014 "),vx=a("a"),zZe=o("BigBirdPegasusModel"),VZe=o(" (BigBirdPegasus model)"),WZe=l(),Bh=a("li"),gJ=a("strong"),QZe=o("blenderbot"),HZe=o(" \u2014 "),Tx=a("a"),UZe=o("BlenderbotModel"),JZe=o(" (Blenderbot model)"),YZe=l(),kh=a("li"),hJ=a("strong"),KZe=o("blenderbot-small"),ZZe=o(" \u2014 "),Fx=a("a"),eeo=o("BlenderbotSmallModel"),oeo=o(" (BlenderbotSmall model)"),reo=l(),xh=a("li"),pJ=a("strong"),teo=o("camembert"),aeo=o(" \u2014 "),Cx=a("a"),neo=o("CamembertModel"),seo=o(" (CamemBERT model)"),leo=l(),Rh=a("li"),_J=a("strong"),ieo=o("canine"),deo=o(" \u2014 "),Mx=a("a"),ceo=o("CanineModel"),feo=o(" (Canine model)"),meo=l(),Sh=a("li"),uJ=a("strong"),geo=o("clip"),heo=o(" \u2014 "),Ex=a("a"),peo=o("CLIPModel"),_eo=o(" (CLIP model)"),ueo=l(),Ph=a("li"),bJ=a("strong"),beo=o("convbert"),veo=o(" \u2014 "),yx=a("a"),Teo=o("ConvBertModel"),Feo=o(" (ConvBERT model)"),Ceo=l(),$h=a("li"),vJ=a("strong"),Meo=o("convnext"),Eeo=o(" \u2014 "),wx=a("a"),yeo=o("ConvNextModel"),weo=o(" (ConvNext model)"),Aeo=l(),Ih=a("li"),TJ=a("strong"),Leo=o("ctrl"),Beo=o(" \u2014 "),Ax=a("a"),keo=o("CTRLModel"),xeo=o(" (CTRL model)"),Reo=l(),jh=a("li"),FJ=a("strong"),Seo=o("deberta"),Peo=o(" \u2014 "),Lx=a("a"),$eo=o("DebertaModel"),Ieo=o(" (DeBERTa model)"),jeo=l(),Nh=a("li"),CJ=a("strong"),Neo=o("deberta-v2"),Deo=o(" \u2014 "),Bx=a("a"),qeo=o("DebertaV2Model"),Geo=o(" (DeBERTa-v2 model)"),Oeo=l(),Dh=a("li"),MJ=a("strong"),Xeo=o("deit"),zeo=o(" \u2014 "),kx=a("a"),Veo=o("DeiTModel"),Weo=o(" (DeiT model)"),Qeo=l(),qh=a("li"),EJ=a("strong"),Heo=o("detr"),Ueo=o(" \u2014 "),xx=a("a"),Jeo=o("DetrModel"),Yeo=o(" (DETR model)"),Keo=l(),Gh=a("li"),yJ=a("strong"),Zeo=o("distilbert"),eoo=o(" \u2014 "),Rx=a("a"),ooo=o("DistilBertModel"),roo=o(" (DistilBERT model)"),too=l(),Oh=a("li"),wJ=a("strong"),aoo=o("dpr"),noo=o(" \u2014 "),Sx=a("a"),soo=o("DPRQuestionEncoder"),loo=o(" (DPR model)"),ioo=l(),Xh=a("li"),AJ=a("strong"),doo=o("electra"),coo=o(" \u2014 "),Px=a("a"),foo=o("ElectraModel"),moo=o(" (ELECTRA model)"),goo=l(),zh=a("li"),LJ=a("strong"),hoo=o("flaubert"),poo=o(" \u2014 "),$x=a("a"),_oo=o("FlaubertModel"),uoo=o(" (FlauBERT model)"),boo=l(),Vh=a("li"),BJ=a("strong"),voo=o("fnet"),Too=o(" \u2014 "),Ix=a("a"),Foo=o("FNetModel"),Coo=o(" (FNet model)"),Moo=l(),Wh=a("li"),kJ=a("strong"),Eoo=o("fsmt"),yoo=o(" \u2014 "),jx=a("a"),woo=o("FSMTModel"),Aoo=o(" (FairSeq Machine-Translation model)"),Loo=l(),xs=a("li"),xJ=a("strong"),Boo=o("funnel"),koo=o(" \u2014 "),Nx=a("a"),xoo=o("FunnelModel"),Roo=o(" or "),Dx=a("a"),Soo=o("FunnelBaseModel"),Poo=o(" (Funnel Transformer model)"),$oo=l(),Qh=a("li"),RJ=a("strong"),Ioo=o("gpt2"),joo=o(" \u2014 "),qx=a("a"),Noo=o("GPT2Model"),Doo=o(" (OpenAI GPT-2 model)"),qoo=l(),Hh=a("li"),SJ=a("strong"),Goo=o("gpt_neo"),Ooo=o(" \u2014 "),Gx=a("a"),Xoo=o("GPTNeoModel"),zoo=o(" (GPT Neo model)"),Voo=l(),Uh=a("li"),PJ=a("strong"),Woo=o("gptj"),Qoo=o(" \u2014 "),Ox=a("a"),Hoo=o("GPTJModel"),Uoo=o(" (GPT-J model)"),Joo=l(),Jh=a("li"),$J=a("strong"),Yoo=o("hubert"),Koo=o(" \u2014 "),Xx=a("a"),Zoo=o("HubertModel"),ero=o(" (Hubert model)"),oro=l(),Yh=a("li"),IJ=a("strong"),rro=o("ibert"),tro=o(" \u2014 "),zx=a("a"),aro=o("IBertModel"),nro=o(" (I-BERT model)"),sro=l(),Kh=a("li"),jJ=a("strong"),lro=o("imagegpt"),iro=o(" \u2014 "),Vx=a("a"),dro=o("ImageGPTModel"),cro=o(" (ImageGPT model)"),fro=l(),Zh=a("li"),NJ=a("strong"),mro=o("layoutlm"),gro=o(" \u2014 "),Wx=a("a"),hro=o("LayoutLMModel"),pro=o(" (LayoutLM model)"),_ro=l(),ep=a("li"),DJ=a("strong"),uro=o("layoutlmv2"),bro=o(" \u2014 "),Qx=a("a"),vro=o("LayoutLMv2Model"),Tro=o(" (LayoutLMv2 model)"),Fro=l(),op=a("li"),qJ=a("strong"),Cro=o("led"),Mro=o(" \u2014 "),Hx=a("a"),Ero=o("LEDModel"),yro=o(" (LED model)"),wro=l(),rp=a("li"),GJ=a("strong"),Aro=o("longformer"),Lro=o(" \u2014 "),Ux=a("a"),Bro=o("LongformerModel"),kro=o(" (Longformer model)"),xro=l(),tp=a("li"),OJ=a("strong"),Rro=o("luke"),Sro=o(" \u2014 "),Jx=a("a"),Pro=o("LukeModel"),$ro=o(" (LUKE model)"),Iro=l(),ap=a("li"),XJ=a("strong"),jro=o("lxmert"),Nro=o(" \u2014 "),Yx=a("a"),Dro=o("LxmertModel"),qro=o(" (LXMERT model)"),Gro=l(),np=a("li"),zJ=a("strong"),Oro=o("m2m_100"),Xro=o(" \u2014 "),Kx=a("a"),zro=o("M2M100Model"),Vro=o(" (M2M100 model)"),Wro=l(),sp=a("li"),VJ=a("strong"),Qro=o("marian"),Hro=o(" \u2014 "),Zx=a("a"),Uro=o("MarianModel"),Jro=o(" (Marian model)"),Yro=l(),lp=a("li"),WJ=a("strong"),Kro=o("mbart"),Zro=o(" \u2014 "),eR=a("a"),eto=o("MBartModel"),oto=o(" (mBART model)"),rto=l(),ip=a("li"),QJ=a("strong"),tto=o("megatron-bert"),ato=o(" \u2014 "),oR=a("a"),nto=o("MegatronBertModel"),sto=o(" (MegatronBert model)"),lto=l(),dp=a("li"),HJ=a("strong"),ito=o("mobilebert"),dto=o(" \u2014 "),rR=a("a"),cto=o("MobileBertModel"),fto=o(" (MobileBERT model)"),mto=l(),cp=a("li"),UJ=a("strong"),gto=o("mpnet"),hto=o(" \u2014 "),tR=a("a"),pto=o("MPNetModel"),_to=o(" (MPNet model)"),uto=l(),fp=a("li"),JJ=a("strong"),bto=o("mt5"),vto=o(" \u2014 "),aR=a("a"),Tto=o("MT5Model"),Fto=o(" (mT5 model)"),Cto=l(),mp=a("li"),YJ=a("strong"),Mto=o("nystromformer"),Eto=o(" \u2014 "),nR=a("a"),yto=o("NystromformerModel"),wto=o(" (Nystromformer model)"),Ato=l(),gp=a("li"),KJ=a("strong"),Lto=o("openai-gpt"),Bto=o(" \u2014 "),sR=a("a"),kto=o("OpenAIGPTModel"),xto=o(" (OpenAI GPT model)"),Rto=l(),hp=a("li"),ZJ=a("strong"),Sto=o("pegasus"),Pto=o(" \u2014 "),lR=a("a"),$to=o("PegasusModel"),Ito=o(" (Pegasus model)"),jto=l(),pp=a("li"),eY=a("strong"),Nto=o("perceiver"),Dto=o(" \u2014 "),iR=a("a"),qto=o("PerceiverModel"),Gto=o(" (Perceiver model)"),Oto=l(),_p=a("li"),oY=a("strong"),Xto=o("plbart"),zto=o(" \u2014 "),dR=a("a"),Vto=o("PLBartModel"),Wto=o(" (PLBart model)"),Qto=l(),up=a("li"),rY=a("strong"),Hto=o("poolformer"),Uto=o(" \u2014 "),cR=a("a"),Jto=o("PoolFormerModel"),Yto=o(" (PoolFormer model)"),Kto=l(),bp=a("li"),tY=a("strong"),Zto=o("prophetnet"),eao=o(" \u2014 "),fR=a("a"),oao=o("ProphetNetModel"),rao=o(" (ProphetNet model)"),tao=l(),vp=a("li"),aY=a("strong"),aao=o("qdqbert"),nao=o(" \u2014 "),mR=a("a"),sao=o("QDQBertModel"),lao=o(" (QDQBert model)"),iao=l(),Tp=a("li"),nY=a("strong"),dao=o("reformer"),cao=o(" \u2014 "),gR=a("a"),fao=o("ReformerModel"),mao=o(" (Reformer model)"),gao=l(),Fp=a("li"),sY=a("strong"),hao=o("rembert"),pao=o(" \u2014 "),hR=a("a"),_ao=o("RemBertModel"),uao=o(" (RemBERT model)"),bao=l(),Cp=a("li"),lY=a("strong"),vao=o("retribert"),Tao=o(" \u2014 "),pR=a("a"),Fao=o("RetriBertModel"),Cao=o(" (RetriBERT model)"),Mao=l(),Mp=a("li"),iY=a("strong"),Eao=o("roberta"),yao=o(" \u2014 "),_R=a("a"),wao=o("RobertaModel"),Aao=o(" (RoBERTa model)"),Lao=l(),Ep=a("li"),dY=a("strong"),Bao=o("roformer"),kao=o(" \u2014 "),uR=a("a"),xao=o("RoFormerModel"),Rao=o(" (RoFormer model)"),Sao=l(),yp=a("li"),cY=a("strong"),Pao=o("segformer"),$ao=o(" \u2014 "),bR=a("a"),Iao=o("SegformerModel"),jao=o(" (SegFormer model)"),Nao=l(),wp=a("li"),fY=a("strong"),Dao=o("sew"),qao=o(" \u2014 "),vR=a("a"),Gao=o("SEWModel"),Oao=o(" (SEW model)"),Xao=l(),Ap=a("li"),mY=a("strong"),zao=o("sew-d"),Vao=o(" \u2014 "),TR=a("a"),Wao=o("SEWDModel"),Qao=o(" (SEW-D model)"),Hao=l(),Lp=a("li"),gY=a("strong"),Uao=o("speech_to_text"),Jao=o(" \u2014 "),FR=a("a"),Yao=o("Speech2TextModel"),Kao=o(" (Speech2Text model)"),Zao=l(),Bp=a("li"),hY=a("strong"),eno=o("splinter"),ono=o(" \u2014 "),CR=a("a"),rno=o("SplinterModel"),tno=o(" (Splinter model)"),ano=l(),kp=a("li"),pY=a("strong"),nno=o("squeezebert"),sno=o(" \u2014 "),MR=a("a"),lno=o("SqueezeBertModel"),ino=o(" (SqueezeBERT model)"),dno=l(),xp=a("li"),_Y=a("strong"),cno=o("swin"),fno=o(" \u2014 "),ER=a("a"),mno=o("SwinModel"),gno=o(" (Swin model)"),hno=l(),Rp=a("li"),uY=a("strong"),pno=o("t5"),_no=o(" \u2014 "),yR=a("a"),uno=o("T5Model"),bno=o(" (T5 model)"),vno=l(),Sp=a("li"),bY=a("strong"),Tno=o("tapas"),Fno=o(" \u2014 "),wR=a("a"),Cno=o("TapasModel"),Mno=o(" (TAPAS model)"),Eno=l(),Pp=a("li"),vY=a("strong"),yno=o("transfo-xl"),wno=o(" \u2014 "),AR=a("a"),Ano=o("TransfoXLModel"),Lno=o(" (Transformer-XL model)"),Bno=l(),$p=a("li"),TY=a("strong"),kno=o("unispeech"),xno=o(" \u2014 "),LR=a("a"),Rno=o("UniSpeechModel"),Sno=o(" (UniSpeech model)"),Pno=l(),Ip=a("li"),FY=a("strong"),$no=o("unispeech-sat"),Ino=o(" \u2014 "),BR=a("a"),jno=o("UniSpeechSatModel"),Nno=o(" (UniSpeechSat model)"),Dno=l(),jp=a("li"),CY=a("strong"),qno=o("vilt"),Gno=o(" \u2014 "),kR=a("a"),Ono=o("ViltModel"),Xno=o(" (ViLT model)"),zno=l(),Np=a("li"),MY=a("strong"),Vno=o("vision-text-dual-encoder"),Wno=o(" \u2014 "),xR=a("a"),Qno=o("VisionTextDualEncoderModel"),Hno=o(" (VisionTextDualEncoder model)"),Uno=l(),Dp=a("li"),EY=a("strong"),Jno=o("visual_bert"),Yno=o(" \u2014 "),RR=a("a"),Kno=o("VisualBertModel"),Zno=o(" (VisualBert model)"),eso=l(),qp=a("li"),yY=a("strong"),oso=o("vit"),rso=o(" \u2014 "),SR=a("a"),tso=o("ViTModel"),aso=o(" (ViT model)"),nso=l(),Gp=a("li"),wY=a("strong"),sso=o("vit_mae"),lso=o(" \u2014 "),PR=a("a"),iso=o("ViTMAEModel"),dso=o(" (ViTMAE model)"),cso=l(),Op=a("li"),AY=a("strong"),fso=o("wav2vec2"),mso=o(" \u2014 "),$R=a("a"),gso=o("Wav2Vec2Model"),hso=o(" (Wav2Vec2 model)"),pso=l(),Xp=a("li"),LY=a("strong"),_so=o("wavlm"),uso=o(" \u2014 "),IR=a("a"),bso=o("WavLMModel"),vso=o(" (WavLM model)"),Tso=l(),zp=a("li"),BY=a("strong"),Fso=o("xglm"),Cso=o(" \u2014 "),jR=a("a"),Mso=o("XGLMModel"),Eso=o(" (XGLM model)"),yso=l(),Vp=a("li"),kY=a("strong"),wso=o("xlm"),Aso=o(" \u2014 "),NR=a("a"),Lso=o("XLMModel"),Bso=o(" (XLM model)"),kso=l(),Wp=a("li"),xY=a("strong"),xso=o("xlm-prophetnet"),Rso=o(" \u2014 "),DR=a("a"),Sso=o("XLMProphetNetModel"),Pso=o(" (XLMProphetNet model)"),$so=l(),Qp=a("li"),RY=a("strong"),Iso=o("xlm-roberta"),jso=o(" \u2014 "),qR=a("a"),Nso=o("XLMRobertaModel"),Dso=o(" (XLM-RoBERTa model)"),qso=l(),Hp=a("li"),SY=a("strong"),Gso=o("xlm-roberta-xl"),Oso=o(" \u2014 "),GR=a("a"),Xso=o("XLMRobertaXLModel"),zso=o(" (XLM-RoBERTa-XL model)"),Vso=l(),Up=a("li"),PY=a("strong"),Wso=o("xlnet"),Qso=o(" \u2014 "),OR=a("a"),Hso=o("XLNetModel"),Uso=o(" (XLNet model)"),Jso=l(),Jp=a("li"),$Y=a("strong"),Yso=o("yoso"),Kso=o(" \u2014 "),XR=a("a"),Zso=o("YosoModel"),elo=o(" (YOSO model)"),olo=l(),Yp=a("p"),rlo=o("The model is set in evaluation mode by default using "),IY=a("code"),tlo=o("model.eval()"),alo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=a("code"),nlo=o("model.train()"),slo=l(),NY=a("p"),llo=o("Examples:"),ilo=l(),f(G4.$$.fragment),bLe=l(),Xi=a("h2"),Kp=a("a"),DY=a("span"),f(O4.$$.fragment),dlo=l(),qY=a("span"),clo=o("AutoModelForPreTraining"),vLe=l(),Wo=a("div"),f(X4.$$.fragment),flo=l(),zi=a("p"),mlo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=a("code"),glo=o("from_pretrained()"),hlo=o("class method or the "),OY=a("code"),plo=o("from_config()"),_lo=o(`class
method.`),ulo=l(),z4=a("p"),blo=o("This class cannot be instantiated directly using "),XY=a("code"),vlo=o("__init__()"),Tlo=o(" (throws an error)."),Flo=l(),Dr=a("div"),f(V4.$$.fragment),Clo=l(),zY=a("p"),Mlo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Elo=l(),Vi=a("p"),ylo=o(`Note:
Loading a model from its configuration file does `),VY=a("strong"),wlo=o("not"),Alo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=a("code"),Llo=o("from_pretrained()"),Blo=o("to load the model weights."),klo=l(),QY=a("p"),xlo=o("Examples:"),Rlo=l(),f(W4.$$.fragment),Slo=l(),xe=a("div"),f(Q4.$$.fragment),Plo=l(),HY=a("p"),$lo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ilo=l(),qa=a("p"),jlo=o("The model class to instantiate is selected based on the "),UY=a("code"),Nlo=o("model_type"),Dlo=o(` property of the config object (either
passed as an argument or loaded from `),JY=a("code"),qlo=o("pretrained_model_name_or_path"),Glo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=a("code"),Olo=o("pretrained_model_name_or_path"),Xlo=o(":"),zlo=l(),x=a("ul"),Zp=a("li"),KY=a("strong"),Vlo=o("albert"),Wlo=o(" \u2014 "),zR=a("a"),Qlo=o("AlbertForPreTraining"),Hlo=o(" (ALBERT model)"),Ulo=l(),e_=a("li"),ZY=a("strong"),Jlo=o("bart"),Ylo=o(" \u2014 "),VR=a("a"),Klo=o("BartForConditionalGeneration"),Zlo=o(" (BART model)"),eio=l(),o_=a("li"),eK=a("strong"),oio=o("bert"),rio=o(" \u2014 "),WR=a("a"),tio=o("BertForPreTraining"),aio=o(" (BERT model)"),nio=l(),r_=a("li"),oK=a("strong"),sio=o("big_bird"),lio=o(" \u2014 "),QR=a("a"),iio=o("BigBirdForPreTraining"),dio=o(" (BigBird model)"),cio=l(),t_=a("li"),rK=a("strong"),fio=o("camembert"),mio=o(" \u2014 "),HR=a("a"),gio=o("CamembertForMaskedLM"),hio=o(" (CamemBERT model)"),pio=l(),a_=a("li"),tK=a("strong"),_io=o("ctrl"),uio=o(" \u2014 "),UR=a("a"),bio=o("CTRLLMHeadModel"),vio=o(" (CTRL model)"),Tio=l(),n_=a("li"),aK=a("strong"),Fio=o("deberta"),Cio=o(" \u2014 "),JR=a("a"),Mio=o("DebertaForMaskedLM"),Eio=o(" (DeBERTa model)"),yio=l(),s_=a("li"),nK=a("strong"),wio=o("deberta-v2"),Aio=o(" \u2014 "),YR=a("a"),Lio=o("DebertaV2ForMaskedLM"),Bio=o(" (DeBERTa-v2 model)"),kio=l(),l_=a("li"),sK=a("strong"),xio=o("distilbert"),Rio=o(" \u2014 "),KR=a("a"),Sio=o("DistilBertForMaskedLM"),Pio=o(" (DistilBERT model)"),$io=l(),i_=a("li"),lK=a("strong"),Iio=o("electra"),jio=o(" \u2014 "),ZR=a("a"),Nio=o("ElectraForPreTraining"),Dio=o(" (ELECTRA model)"),qio=l(),d_=a("li"),iK=a("strong"),Gio=o("flaubert"),Oio=o(" \u2014 "),eS=a("a"),Xio=o("FlaubertWithLMHeadModel"),zio=o(" (FlauBERT model)"),Vio=l(),c_=a("li"),dK=a("strong"),Wio=o("fnet"),Qio=o(" \u2014 "),oS=a("a"),Hio=o("FNetForPreTraining"),Uio=o(" (FNet model)"),Jio=l(),f_=a("li"),cK=a("strong"),Yio=o("fsmt"),Kio=o(" \u2014 "),rS=a("a"),Zio=o("FSMTForConditionalGeneration"),edo=o(" (FairSeq Machine-Translation model)"),odo=l(),m_=a("li"),fK=a("strong"),rdo=o("funnel"),tdo=o(" \u2014 "),tS=a("a"),ado=o("FunnelForPreTraining"),ndo=o(" (Funnel Transformer model)"),sdo=l(),g_=a("li"),mK=a("strong"),ldo=o("gpt2"),ido=o(" \u2014 "),aS=a("a"),ddo=o("GPT2LMHeadModel"),cdo=o(" (OpenAI GPT-2 model)"),fdo=l(),h_=a("li"),gK=a("strong"),mdo=o("ibert"),gdo=o(" \u2014 "),nS=a("a"),hdo=o("IBertForMaskedLM"),pdo=o(" (I-BERT model)"),_do=l(),p_=a("li"),hK=a("strong"),udo=o("layoutlm"),bdo=o(" \u2014 "),sS=a("a"),vdo=o("LayoutLMForMaskedLM"),Tdo=o(" (LayoutLM model)"),Fdo=l(),__=a("li"),pK=a("strong"),Cdo=o("longformer"),Mdo=o(" \u2014 "),lS=a("a"),Edo=o("LongformerForMaskedLM"),ydo=o(" (Longformer model)"),wdo=l(),u_=a("li"),_K=a("strong"),Ado=o("lxmert"),Ldo=o(" \u2014 "),iS=a("a"),Bdo=o("LxmertForPreTraining"),kdo=o(" (LXMERT model)"),xdo=l(),b_=a("li"),uK=a("strong"),Rdo=o("megatron-bert"),Sdo=o(" \u2014 "),dS=a("a"),Pdo=o("MegatronBertForPreTraining"),$do=o(" (MegatronBert model)"),Ido=l(),v_=a("li"),bK=a("strong"),jdo=o("mobilebert"),Ndo=o(" \u2014 "),cS=a("a"),Ddo=o("MobileBertForPreTraining"),qdo=o(" (MobileBERT model)"),Gdo=l(),T_=a("li"),vK=a("strong"),Odo=o("mpnet"),Xdo=o(" \u2014 "),fS=a("a"),zdo=o("MPNetForMaskedLM"),Vdo=o(" (MPNet model)"),Wdo=l(),F_=a("li"),TK=a("strong"),Qdo=o("openai-gpt"),Hdo=o(" \u2014 "),mS=a("a"),Udo=o("OpenAIGPTLMHeadModel"),Jdo=o(" (OpenAI GPT model)"),Ydo=l(),C_=a("li"),FK=a("strong"),Kdo=o("retribert"),Zdo=o(" \u2014 "),gS=a("a"),eco=o("RetriBertModel"),oco=o(" (RetriBERT model)"),rco=l(),M_=a("li"),CK=a("strong"),tco=o("roberta"),aco=o(" \u2014 "),hS=a("a"),nco=o("RobertaForMaskedLM"),sco=o(" (RoBERTa model)"),lco=l(),E_=a("li"),MK=a("strong"),ico=o("squeezebert"),dco=o(" \u2014 "),pS=a("a"),cco=o("SqueezeBertForMaskedLM"),fco=o(" (SqueezeBERT model)"),mco=l(),y_=a("li"),EK=a("strong"),gco=o("t5"),hco=o(" \u2014 "),_S=a("a"),pco=o("T5ForConditionalGeneration"),_co=o(" (T5 model)"),uco=l(),w_=a("li"),yK=a("strong"),bco=o("tapas"),vco=o(" \u2014 "),uS=a("a"),Tco=o("TapasForMaskedLM"),Fco=o(" (TAPAS model)"),Cco=l(),A_=a("li"),wK=a("strong"),Mco=o("transfo-xl"),Eco=o(" \u2014 "),bS=a("a"),yco=o("TransfoXLLMHeadModel"),wco=o(" (Transformer-XL model)"),Aco=l(),L_=a("li"),AK=a("strong"),Lco=o("unispeech"),Bco=o(" \u2014 "),vS=a("a"),kco=o("UniSpeechForPreTraining"),xco=o(" (UniSpeech model)"),Rco=l(),B_=a("li"),LK=a("strong"),Sco=o("unispeech-sat"),Pco=o(" \u2014 "),TS=a("a"),$co=o("UniSpeechSatForPreTraining"),Ico=o(" (UniSpeechSat model)"),jco=l(),k_=a("li"),BK=a("strong"),Nco=o("visual_bert"),Dco=o(" \u2014 "),FS=a("a"),qco=o("VisualBertForPreTraining"),Gco=o(" (VisualBert model)"),Oco=l(),x_=a("li"),kK=a("strong"),Xco=o("vit_mae"),zco=o(" \u2014 "),CS=a("a"),Vco=o("ViTMAEForPreTraining"),Wco=o(" (ViTMAE model)"),Qco=l(),R_=a("li"),xK=a("strong"),Hco=o("wav2vec2"),Uco=o(" \u2014 "),MS=a("a"),Jco=o("Wav2Vec2ForPreTraining"),Yco=o(" (Wav2Vec2 model)"),Kco=l(),S_=a("li"),RK=a("strong"),Zco=o("xlm"),efo=o(" \u2014 "),ES=a("a"),ofo=o("XLMWithLMHeadModel"),rfo=o(" (XLM model)"),tfo=l(),P_=a("li"),SK=a("strong"),afo=o("xlm-roberta"),nfo=o(" \u2014 "),yS=a("a"),sfo=o("XLMRobertaForMaskedLM"),lfo=o(" (XLM-RoBERTa model)"),ifo=l(),$_=a("li"),PK=a("strong"),dfo=o("xlm-roberta-xl"),cfo=o(" \u2014 "),wS=a("a"),ffo=o("XLMRobertaXLForMaskedLM"),mfo=o(" (XLM-RoBERTa-XL model)"),gfo=l(),I_=a("li"),$K=a("strong"),hfo=o("xlnet"),pfo=o(" \u2014 "),AS=a("a"),_fo=o("XLNetLMHeadModel"),ufo=o(" (XLNet model)"),bfo=l(),j_=a("p"),vfo=o("The model is set in evaluation mode by default using "),IK=a("code"),Tfo=o("model.eval()"),Ffo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=a("code"),Cfo=o("model.train()"),Mfo=l(),NK=a("p"),Efo=o("Examples:"),yfo=l(),f(H4.$$.fragment),TLe=l(),Wi=a("h2"),N_=a("a"),DK=a("span"),f(U4.$$.fragment),wfo=l(),qK=a("span"),Afo=o("AutoModelForCausalLM"),FLe=l(),Qo=a("div"),f(J4.$$.fragment),Lfo=l(),Qi=a("p"),Bfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=a("code"),kfo=o("from_pretrained()"),xfo=o("class method or the "),OK=a("code"),Rfo=o("from_config()"),Sfo=o(`class
method.`),Pfo=l(),Y4=a("p"),$fo=o("This class cannot be instantiated directly using "),XK=a("code"),Ifo=o("__init__()"),jfo=o(" (throws an error)."),Nfo=l(),qr=a("div"),f(K4.$$.fragment),Dfo=l(),zK=a("p"),qfo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Gfo=l(),Hi=a("p"),Ofo=o(`Note:
Loading a model from its configuration file does `),VK=a("strong"),Xfo=o("not"),zfo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=a("code"),Vfo=o("from_pretrained()"),Wfo=o("to load the model weights."),Qfo=l(),QK=a("p"),Hfo=o("Examples:"),Ufo=l(),f(Z4.$$.fragment),Jfo=l(),Re=a("div"),f(eM.$$.fragment),Yfo=l(),HK=a("p"),Kfo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Zfo=l(),Ga=a("p"),emo=o("The model class to instantiate is selected based on the "),UK=a("code"),omo=o("model_type"),rmo=o(` property of the config object (either
passed as an argument or loaded from `),JK=a("code"),tmo=o("pretrained_model_name_or_path"),amo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=a("code"),nmo=o("pretrained_model_name_or_path"),smo=o(":"),lmo=l(),$=a("ul"),D_=a("li"),KK=a("strong"),imo=o("bart"),dmo=o(" \u2014 "),LS=a("a"),cmo=o("BartForCausalLM"),fmo=o(" (BART model)"),mmo=l(),q_=a("li"),ZK=a("strong"),gmo=o("bert"),hmo=o(" \u2014 "),BS=a("a"),pmo=o("BertLMHeadModel"),_mo=o(" (BERT model)"),umo=l(),G_=a("li"),eZ=a("strong"),bmo=o("bert-generation"),vmo=o(" \u2014 "),kS=a("a"),Tmo=o("BertGenerationDecoder"),Fmo=o(" (Bert Generation model)"),Cmo=l(),O_=a("li"),oZ=a("strong"),Mmo=o("big_bird"),Emo=o(" \u2014 "),xS=a("a"),ymo=o("BigBirdForCausalLM"),wmo=o(" (BigBird model)"),Amo=l(),X_=a("li"),rZ=a("strong"),Lmo=o("bigbird_pegasus"),Bmo=o(" \u2014 "),RS=a("a"),kmo=o("BigBirdPegasusForCausalLM"),xmo=o(" (BigBirdPegasus model)"),Rmo=l(),z_=a("li"),tZ=a("strong"),Smo=o("blenderbot"),Pmo=o(" \u2014 "),SS=a("a"),$mo=o("BlenderbotForCausalLM"),Imo=o(" (Blenderbot model)"),jmo=l(),V_=a("li"),aZ=a("strong"),Nmo=o("blenderbot-small"),Dmo=o(" \u2014 "),PS=a("a"),qmo=o("BlenderbotSmallForCausalLM"),Gmo=o(" (BlenderbotSmall model)"),Omo=l(),W_=a("li"),nZ=a("strong"),Xmo=o("camembert"),zmo=o(" \u2014 "),$S=a("a"),Vmo=o("CamembertForCausalLM"),Wmo=o(" (CamemBERT model)"),Qmo=l(),Q_=a("li"),sZ=a("strong"),Hmo=o("ctrl"),Umo=o(" \u2014 "),IS=a("a"),Jmo=o("CTRLLMHeadModel"),Ymo=o(" (CTRL model)"),Kmo=l(),H_=a("li"),lZ=a("strong"),Zmo=o("electra"),ego=o(" \u2014 "),jS=a("a"),ogo=o("ElectraForCausalLM"),rgo=o(" (ELECTRA model)"),tgo=l(),U_=a("li"),iZ=a("strong"),ago=o("gpt2"),ngo=o(" \u2014 "),NS=a("a"),sgo=o("GPT2LMHeadModel"),lgo=o(" (OpenAI GPT-2 model)"),igo=l(),J_=a("li"),dZ=a("strong"),dgo=o("gpt_neo"),cgo=o(" \u2014 "),DS=a("a"),fgo=o("GPTNeoForCausalLM"),mgo=o(" (GPT Neo model)"),ggo=l(),Y_=a("li"),cZ=a("strong"),hgo=o("gptj"),pgo=o(" \u2014 "),qS=a("a"),_go=o("GPTJForCausalLM"),ugo=o(" (GPT-J model)"),bgo=l(),K_=a("li"),fZ=a("strong"),vgo=o("marian"),Tgo=o(" \u2014 "),GS=a("a"),Fgo=o("MarianForCausalLM"),Cgo=o(" (Marian model)"),Mgo=l(),Z_=a("li"),mZ=a("strong"),Ego=o("mbart"),ygo=o(" \u2014 "),OS=a("a"),wgo=o("MBartForCausalLM"),Ago=o(" (mBART model)"),Lgo=l(),eu=a("li"),gZ=a("strong"),Bgo=o("megatron-bert"),kgo=o(" \u2014 "),XS=a("a"),xgo=o("MegatronBertForCausalLM"),Rgo=o(" (MegatronBert model)"),Sgo=l(),ou=a("li"),hZ=a("strong"),Pgo=o("openai-gpt"),$go=o(" \u2014 "),zS=a("a"),Igo=o("OpenAIGPTLMHeadModel"),jgo=o(" (OpenAI GPT model)"),Ngo=l(),ru=a("li"),pZ=a("strong"),Dgo=o("pegasus"),qgo=o(" \u2014 "),VS=a("a"),Ggo=o("PegasusForCausalLM"),Ogo=o(" (Pegasus model)"),Xgo=l(),tu=a("li"),_Z=a("strong"),zgo=o("plbart"),Vgo=o(" \u2014 "),WS=a("a"),Wgo=o("PLBartForCausalLM"),Qgo=o(" (PLBart model)"),Hgo=l(),au=a("li"),uZ=a("strong"),Ugo=o("prophetnet"),Jgo=o(" \u2014 "),QS=a("a"),Ygo=o("ProphetNetForCausalLM"),Kgo=o(" (ProphetNet model)"),Zgo=l(),nu=a("li"),bZ=a("strong"),eho=o("qdqbert"),oho=o(" \u2014 "),HS=a("a"),rho=o("QDQBertLMHeadModel"),tho=o(" (QDQBert model)"),aho=l(),su=a("li"),vZ=a("strong"),nho=o("reformer"),sho=o(" \u2014 "),US=a("a"),lho=o("ReformerModelWithLMHead"),iho=o(" (Reformer model)"),dho=l(),lu=a("li"),TZ=a("strong"),cho=o("rembert"),fho=o(" \u2014 "),JS=a("a"),mho=o("RemBertForCausalLM"),gho=o(" (RemBERT model)"),hho=l(),iu=a("li"),FZ=a("strong"),pho=o("roberta"),_ho=o(" \u2014 "),YS=a("a"),uho=o("RobertaForCausalLM"),bho=o(" (RoBERTa model)"),vho=l(),du=a("li"),CZ=a("strong"),Tho=o("roformer"),Fho=o(" \u2014 "),KS=a("a"),Cho=o("RoFormerForCausalLM"),Mho=o(" (RoFormer model)"),Eho=l(),cu=a("li"),MZ=a("strong"),yho=o("speech_to_text_2"),who=o(" \u2014 "),ZS=a("a"),Aho=o("Speech2Text2ForCausalLM"),Lho=o(" (Speech2Text2 model)"),Bho=l(),fu=a("li"),EZ=a("strong"),kho=o("transfo-xl"),xho=o(" \u2014 "),eP=a("a"),Rho=o("TransfoXLLMHeadModel"),Sho=o(" (Transformer-XL model)"),Pho=l(),mu=a("li"),yZ=a("strong"),$ho=o("trocr"),Iho=o(" \u2014 "),oP=a("a"),jho=o("TrOCRForCausalLM"),Nho=o(" (TrOCR model)"),Dho=l(),gu=a("li"),wZ=a("strong"),qho=o("xglm"),Gho=o(" \u2014 "),rP=a("a"),Oho=o("XGLMForCausalLM"),Xho=o(" (XGLM model)"),zho=l(),hu=a("li"),AZ=a("strong"),Vho=o("xlm"),Who=o(" \u2014 "),tP=a("a"),Qho=o("XLMWithLMHeadModel"),Hho=o(" (XLM model)"),Uho=l(),pu=a("li"),LZ=a("strong"),Jho=o("xlm-prophetnet"),Yho=o(" \u2014 "),aP=a("a"),Kho=o("XLMProphetNetForCausalLM"),Zho=o(" (XLMProphetNet model)"),epo=l(),_u=a("li"),BZ=a("strong"),opo=o("xlm-roberta"),rpo=o(" \u2014 "),nP=a("a"),tpo=o("XLMRobertaForCausalLM"),apo=o(" (XLM-RoBERTa model)"),npo=l(),uu=a("li"),kZ=a("strong"),spo=o("xlm-roberta-xl"),lpo=o(" \u2014 "),sP=a("a"),ipo=o("XLMRobertaXLForCausalLM"),dpo=o(" (XLM-RoBERTa-XL model)"),cpo=l(),bu=a("li"),xZ=a("strong"),fpo=o("xlnet"),mpo=o(" \u2014 "),lP=a("a"),gpo=o("XLNetLMHeadModel"),hpo=o(" (XLNet model)"),ppo=l(),vu=a("p"),_po=o("The model is set in evaluation mode by default using "),RZ=a("code"),upo=o("model.eval()"),bpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=a("code"),vpo=o("model.train()"),Tpo=l(),PZ=a("p"),Fpo=o("Examples:"),Cpo=l(),f(oM.$$.fragment),CLe=l(),Ui=a("h2"),Tu=a("a"),$Z=a("span"),f(rM.$$.fragment),Mpo=l(),IZ=a("span"),Epo=o("AutoModelForMaskedLM"),MLe=l(),Ho=a("div"),f(tM.$$.fragment),ypo=l(),Ji=a("p"),wpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=a("code"),Apo=o("from_pretrained()"),Lpo=o("class method or the "),NZ=a("code"),Bpo=o("from_config()"),kpo=o(`class
method.`),xpo=l(),aM=a("p"),Rpo=o("This class cannot be instantiated directly using "),DZ=a("code"),Spo=o("__init__()"),Ppo=o(" (throws an error)."),$po=l(),Gr=a("div"),f(nM.$$.fragment),Ipo=l(),qZ=a("p"),jpo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Npo=l(),Yi=a("p"),Dpo=o(`Note:
Loading a model from its configuration file does `),GZ=a("strong"),qpo=o("not"),Gpo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=a("code"),Opo=o("from_pretrained()"),Xpo=o("to load the model weights."),zpo=l(),XZ=a("p"),Vpo=o("Examples:"),Wpo=l(),f(sM.$$.fragment),Qpo=l(),Se=a("div"),f(lM.$$.fragment),Hpo=l(),zZ=a("p"),Upo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Jpo=l(),Oa=a("p"),Ypo=o("The model class to instantiate is selected based on the "),VZ=a("code"),Kpo=o("model_type"),Zpo=o(` property of the config object (either
passed as an argument or loaded from `),WZ=a("code"),e_o=o("pretrained_model_name_or_path"),o_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=a("code"),r_o=o("pretrained_model_name_or_path"),t_o=o(":"),a_o=l(),I=a("ul"),Fu=a("li"),HZ=a("strong"),n_o=o("albert"),s_o=o(" \u2014 "),iP=a("a"),l_o=o("AlbertForMaskedLM"),i_o=o(" (ALBERT model)"),d_o=l(),Cu=a("li"),UZ=a("strong"),c_o=o("bart"),f_o=o(" \u2014 "),dP=a("a"),m_o=o("BartForConditionalGeneration"),g_o=o(" (BART model)"),h_o=l(),Mu=a("li"),JZ=a("strong"),p_o=o("bert"),__o=o(" \u2014 "),cP=a("a"),u_o=o("BertForMaskedLM"),b_o=o(" (BERT model)"),v_o=l(),Eu=a("li"),YZ=a("strong"),T_o=o("big_bird"),F_o=o(" \u2014 "),fP=a("a"),C_o=o("BigBirdForMaskedLM"),M_o=o(" (BigBird model)"),E_o=l(),yu=a("li"),KZ=a("strong"),y_o=o("camembert"),w_o=o(" \u2014 "),mP=a("a"),A_o=o("CamembertForMaskedLM"),L_o=o(" (CamemBERT model)"),B_o=l(),wu=a("li"),ZZ=a("strong"),k_o=o("convbert"),x_o=o(" \u2014 "),gP=a("a"),R_o=o("ConvBertForMaskedLM"),S_o=o(" (ConvBERT model)"),P_o=l(),Au=a("li"),eee=a("strong"),$_o=o("deberta"),I_o=o(" \u2014 "),hP=a("a"),j_o=o("DebertaForMaskedLM"),N_o=o(" (DeBERTa model)"),D_o=l(),Lu=a("li"),oee=a("strong"),q_o=o("deberta-v2"),G_o=o(" \u2014 "),pP=a("a"),O_o=o("DebertaV2ForMaskedLM"),X_o=o(" (DeBERTa-v2 model)"),z_o=l(),Bu=a("li"),ree=a("strong"),V_o=o("distilbert"),W_o=o(" \u2014 "),_P=a("a"),Q_o=o("DistilBertForMaskedLM"),H_o=o(" (DistilBERT model)"),U_o=l(),ku=a("li"),tee=a("strong"),J_o=o("electra"),Y_o=o(" \u2014 "),uP=a("a"),K_o=o("ElectraForMaskedLM"),Z_o=o(" (ELECTRA model)"),euo=l(),xu=a("li"),aee=a("strong"),ouo=o("flaubert"),ruo=o(" \u2014 "),bP=a("a"),tuo=o("FlaubertWithLMHeadModel"),auo=o(" (FlauBERT model)"),nuo=l(),Ru=a("li"),nee=a("strong"),suo=o("fnet"),luo=o(" \u2014 "),vP=a("a"),iuo=o("FNetForMaskedLM"),duo=o(" (FNet model)"),cuo=l(),Su=a("li"),see=a("strong"),fuo=o("funnel"),muo=o(" \u2014 "),TP=a("a"),guo=o("FunnelForMaskedLM"),huo=o(" (Funnel Transformer model)"),puo=l(),Pu=a("li"),lee=a("strong"),_uo=o("ibert"),uuo=o(" \u2014 "),FP=a("a"),buo=o("IBertForMaskedLM"),vuo=o(" (I-BERT model)"),Tuo=l(),$u=a("li"),iee=a("strong"),Fuo=o("layoutlm"),Cuo=o(" \u2014 "),CP=a("a"),Muo=o("LayoutLMForMaskedLM"),Euo=o(" (LayoutLM model)"),yuo=l(),Iu=a("li"),dee=a("strong"),wuo=o("longformer"),Auo=o(" \u2014 "),MP=a("a"),Luo=o("LongformerForMaskedLM"),Buo=o(" (Longformer model)"),kuo=l(),ju=a("li"),cee=a("strong"),xuo=o("mbart"),Ruo=o(" \u2014 "),EP=a("a"),Suo=o("MBartForConditionalGeneration"),Puo=o(" (mBART model)"),$uo=l(),Nu=a("li"),fee=a("strong"),Iuo=o("megatron-bert"),juo=o(" \u2014 "),yP=a("a"),Nuo=o("MegatronBertForMaskedLM"),Duo=o(" (MegatronBert model)"),quo=l(),Du=a("li"),mee=a("strong"),Guo=o("mobilebert"),Ouo=o(" \u2014 "),wP=a("a"),Xuo=o("MobileBertForMaskedLM"),zuo=o(" (MobileBERT model)"),Vuo=l(),qu=a("li"),gee=a("strong"),Wuo=o("mpnet"),Quo=o(" \u2014 "),AP=a("a"),Huo=o("MPNetForMaskedLM"),Uuo=o(" (MPNet model)"),Juo=l(),Gu=a("li"),hee=a("strong"),Yuo=o("nystromformer"),Kuo=o(" \u2014 "),LP=a("a"),Zuo=o("NystromformerForMaskedLM"),e1o=o(" (Nystromformer model)"),o1o=l(),Ou=a("li"),pee=a("strong"),r1o=o("perceiver"),t1o=o(" \u2014 "),BP=a("a"),a1o=o("PerceiverForMaskedLM"),n1o=o(" (Perceiver model)"),s1o=l(),Xu=a("li"),_ee=a("strong"),l1o=o("qdqbert"),i1o=o(" \u2014 "),kP=a("a"),d1o=o("QDQBertForMaskedLM"),c1o=o(" (QDQBert model)"),f1o=l(),zu=a("li"),uee=a("strong"),m1o=o("reformer"),g1o=o(" \u2014 "),xP=a("a"),h1o=o("ReformerForMaskedLM"),p1o=o(" (Reformer model)"),_1o=l(),Vu=a("li"),bee=a("strong"),u1o=o("rembert"),b1o=o(" \u2014 "),RP=a("a"),v1o=o("RemBertForMaskedLM"),T1o=o(" (RemBERT model)"),F1o=l(),Wu=a("li"),vee=a("strong"),C1o=o("roberta"),M1o=o(" \u2014 "),SP=a("a"),E1o=o("RobertaForMaskedLM"),y1o=o(" (RoBERTa model)"),w1o=l(),Qu=a("li"),Tee=a("strong"),A1o=o("roformer"),L1o=o(" \u2014 "),PP=a("a"),B1o=o("RoFormerForMaskedLM"),k1o=o(" (RoFormer model)"),x1o=l(),Hu=a("li"),Fee=a("strong"),R1o=o("squeezebert"),S1o=o(" \u2014 "),$P=a("a"),P1o=o("SqueezeBertForMaskedLM"),$1o=o(" (SqueezeBERT model)"),I1o=l(),Uu=a("li"),Cee=a("strong"),j1o=o("tapas"),N1o=o(" \u2014 "),IP=a("a"),D1o=o("TapasForMaskedLM"),q1o=o(" (TAPAS model)"),G1o=l(),Ju=a("li"),Mee=a("strong"),O1o=o("wav2vec2"),X1o=o(" \u2014 "),Eee=a("code"),z1o=o("Wav2Vec2ForMaskedLM"),V1o=o("(Wav2Vec2 model)"),W1o=l(),Yu=a("li"),yee=a("strong"),Q1o=o("xlm"),H1o=o(" \u2014 "),jP=a("a"),U1o=o("XLMWithLMHeadModel"),J1o=o(" (XLM model)"),Y1o=l(),Ku=a("li"),wee=a("strong"),K1o=o("xlm-roberta"),Z1o=o(" \u2014 "),NP=a("a"),ebo=o("XLMRobertaForMaskedLM"),obo=o(" (XLM-RoBERTa model)"),rbo=l(),Zu=a("li"),Aee=a("strong"),tbo=o("xlm-roberta-xl"),abo=o(" \u2014 "),DP=a("a"),nbo=o("XLMRobertaXLForMaskedLM"),sbo=o(" (XLM-RoBERTa-XL model)"),lbo=l(),e1=a("li"),Lee=a("strong"),ibo=o("yoso"),dbo=o(" \u2014 "),qP=a("a"),cbo=o("YosoForMaskedLM"),fbo=o(" (YOSO model)"),mbo=l(),o1=a("p"),gbo=o("The model is set in evaluation mode by default using "),Bee=a("code"),hbo=o("model.eval()"),pbo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=a("code"),_bo=o("model.train()"),ubo=l(),xee=a("p"),bbo=o("Examples:"),vbo=l(),f(iM.$$.fragment),ELe=l(),Ki=a("h2"),r1=a("a"),Ree=a("span"),f(dM.$$.fragment),Tbo=l(),See=a("span"),Fbo=o("AutoModelForSeq2SeqLM"),yLe=l(),Uo=a("div"),f(cM.$$.fragment),Cbo=l(),Zi=a("p"),Mbo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=a("code"),Ebo=o("from_pretrained()"),ybo=o("class method or the "),$ee=a("code"),wbo=o("from_config()"),Abo=o(`class
method.`),Lbo=l(),fM=a("p"),Bbo=o("This class cannot be instantiated directly using "),Iee=a("code"),kbo=o("__init__()"),xbo=o(" (throws an error)."),Rbo=l(),Or=a("div"),f(mM.$$.fragment),Sbo=l(),jee=a("p"),Pbo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$bo=l(),ed=a("p"),Ibo=o(`Note:
Loading a model from its configuration file does `),Nee=a("strong"),jbo=o("not"),Nbo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=a("code"),Dbo=o("from_pretrained()"),qbo=o("to load the model weights."),Gbo=l(),qee=a("p"),Obo=o("Examples:"),Xbo=l(),f(gM.$$.fragment),zbo=l(),Pe=a("div"),f(hM.$$.fragment),Vbo=l(),Gee=a("p"),Wbo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Qbo=l(),Xa=a("p"),Hbo=o("The model class to instantiate is selected based on the "),Oee=a("code"),Ubo=o("model_type"),Jbo=o(` property of the config object (either
passed as an argument or loaded from `),Xee=a("code"),Ybo=o("pretrained_model_name_or_path"),Kbo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=a("code"),Zbo=o("pretrained_model_name_or_path"),e5o=o(":"),o5o=l(),ae=a("ul"),t1=a("li"),Vee=a("strong"),r5o=o("bart"),t5o=o(" \u2014 "),GP=a("a"),a5o=o("BartForConditionalGeneration"),n5o=o(" (BART model)"),s5o=l(),a1=a("li"),Wee=a("strong"),l5o=o("bigbird_pegasus"),i5o=o(" \u2014 "),OP=a("a"),d5o=o("BigBirdPegasusForConditionalGeneration"),c5o=o(" (BigBirdPegasus model)"),f5o=l(),n1=a("li"),Qee=a("strong"),m5o=o("blenderbot"),g5o=o(" \u2014 "),XP=a("a"),h5o=o("BlenderbotForConditionalGeneration"),p5o=o(" (Blenderbot model)"),_5o=l(),s1=a("li"),Hee=a("strong"),u5o=o("blenderbot-small"),b5o=o(" \u2014 "),zP=a("a"),v5o=o("BlenderbotSmallForConditionalGeneration"),T5o=o(" (BlenderbotSmall model)"),F5o=l(),l1=a("li"),Uee=a("strong"),C5o=o("encoder-decoder"),M5o=o(" \u2014 "),VP=a("a"),E5o=o("EncoderDecoderModel"),y5o=o(" (Encoder decoder model)"),w5o=l(),i1=a("li"),Jee=a("strong"),A5o=o("fsmt"),L5o=o(" \u2014 "),WP=a("a"),B5o=o("FSMTForConditionalGeneration"),k5o=o(" (FairSeq Machine-Translation model)"),x5o=l(),d1=a("li"),Yee=a("strong"),R5o=o("led"),S5o=o(" \u2014 "),QP=a("a"),P5o=o("LEDForConditionalGeneration"),$5o=o(" (LED model)"),I5o=l(),c1=a("li"),Kee=a("strong"),j5o=o("m2m_100"),N5o=o(" \u2014 "),HP=a("a"),D5o=o("M2M100ForConditionalGeneration"),q5o=o(" (M2M100 model)"),G5o=l(),f1=a("li"),Zee=a("strong"),O5o=o("marian"),X5o=o(" \u2014 "),UP=a("a"),z5o=o("MarianMTModel"),V5o=o(" (Marian model)"),W5o=l(),m1=a("li"),eoe=a("strong"),Q5o=o("mbart"),H5o=o(" \u2014 "),JP=a("a"),U5o=o("MBartForConditionalGeneration"),J5o=o(" (mBART model)"),Y5o=l(),g1=a("li"),ooe=a("strong"),K5o=o("mt5"),Z5o=o(" \u2014 "),YP=a("a"),e2o=o("MT5ForConditionalGeneration"),o2o=o(" (mT5 model)"),r2o=l(),h1=a("li"),roe=a("strong"),t2o=o("pegasus"),a2o=o(" \u2014 "),KP=a("a"),n2o=o("PegasusForConditionalGeneration"),s2o=o(" (Pegasus model)"),l2o=l(),p1=a("li"),toe=a("strong"),i2o=o("plbart"),d2o=o(" \u2014 "),ZP=a("a"),c2o=o("PLBartForConditionalGeneration"),f2o=o(" (PLBart model)"),m2o=l(),_1=a("li"),aoe=a("strong"),g2o=o("prophetnet"),h2o=o(" \u2014 "),e$=a("a"),p2o=o("ProphetNetForConditionalGeneration"),_2o=o(" (ProphetNet model)"),u2o=l(),u1=a("li"),noe=a("strong"),b2o=o("t5"),v2o=o(" \u2014 "),o$=a("a"),T2o=o("T5ForConditionalGeneration"),F2o=o(" (T5 model)"),C2o=l(),b1=a("li"),soe=a("strong"),M2o=o("xlm-prophetnet"),E2o=o(" \u2014 "),r$=a("a"),y2o=o("XLMProphetNetForConditionalGeneration"),w2o=o(" (XLMProphetNet model)"),A2o=l(),v1=a("p"),L2o=o("The model is set in evaluation mode by default using "),loe=a("code"),B2o=o("model.eval()"),k2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=a("code"),x2o=o("model.train()"),R2o=l(),doe=a("p"),S2o=o("Examples:"),P2o=l(),f(pM.$$.fragment),wLe=l(),od=a("h2"),T1=a("a"),coe=a("span"),f(_M.$$.fragment),$2o=l(),foe=a("span"),I2o=o("AutoModelForSequenceClassification"),ALe=l(),Jo=a("div"),f(uM.$$.fragment),j2o=l(),rd=a("p"),N2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=a("code"),D2o=o("from_pretrained()"),q2o=o("class method or the "),goe=a("code"),G2o=o("from_config()"),O2o=o(`class
method.`),X2o=l(),bM=a("p"),z2o=o("This class cannot be instantiated directly using "),hoe=a("code"),V2o=o("__init__()"),W2o=o(" (throws an error)."),Q2o=l(),Xr=a("div"),f(vM.$$.fragment),H2o=l(),poe=a("p"),U2o=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),J2o=l(),td=a("p"),Y2o=o(`Note:
Loading a model from its configuration file does `),_oe=a("strong"),K2o=o("not"),Z2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=a("code"),evo=o("from_pretrained()"),ovo=o("to load the model weights."),rvo=l(),boe=a("p"),tvo=o("Examples:"),avo=l(),f(TM.$$.fragment),nvo=l(),$e=a("div"),f(FM.$$.fragment),svo=l(),voe=a("p"),lvo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),ivo=l(),za=a("p"),dvo=o("The model class to instantiate is selected based on the "),Toe=a("code"),cvo=o("model_type"),fvo=o(` property of the config object (either
passed as an argument or loaded from `),Foe=a("code"),mvo=o("pretrained_model_name_or_path"),gvo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=a("code"),hvo=o("pretrained_model_name_or_path"),pvo=o(":"),_vo=l(),A=a("ul"),F1=a("li"),Moe=a("strong"),uvo=o("albert"),bvo=o(" \u2014 "),t$=a("a"),vvo=o("AlbertForSequenceClassification"),Tvo=o(" (ALBERT model)"),Fvo=l(),C1=a("li"),Eoe=a("strong"),Cvo=o("bart"),Mvo=o(" \u2014 "),a$=a("a"),Evo=o("BartForSequenceClassification"),yvo=o(" (BART model)"),wvo=l(),M1=a("li"),yoe=a("strong"),Avo=o("bert"),Lvo=o(" \u2014 "),n$=a("a"),Bvo=o("BertForSequenceClassification"),kvo=o(" (BERT model)"),xvo=l(),E1=a("li"),woe=a("strong"),Rvo=o("big_bird"),Svo=o(" \u2014 "),s$=a("a"),Pvo=o("BigBirdForSequenceClassification"),$vo=o(" (BigBird model)"),Ivo=l(),y1=a("li"),Aoe=a("strong"),jvo=o("bigbird_pegasus"),Nvo=o(" \u2014 "),l$=a("a"),Dvo=o("BigBirdPegasusForSequenceClassification"),qvo=o(" (BigBirdPegasus model)"),Gvo=l(),w1=a("li"),Loe=a("strong"),Ovo=o("camembert"),Xvo=o(" \u2014 "),i$=a("a"),zvo=o("CamembertForSequenceClassification"),Vvo=o(" (CamemBERT model)"),Wvo=l(),A1=a("li"),Boe=a("strong"),Qvo=o("canine"),Hvo=o(" \u2014 "),d$=a("a"),Uvo=o("CanineForSequenceClassification"),Jvo=o(" (Canine model)"),Yvo=l(),L1=a("li"),koe=a("strong"),Kvo=o("convbert"),Zvo=o(" \u2014 "),c$=a("a"),e6o=o("ConvBertForSequenceClassification"),o6o=o(" (ConvBERT model)"),r6o=l(),B1=a("li"),xoe=a("strong"),t6o=o("ctrl"),a6o=o(" \u2014 "),f$=a("a"),n6o=o("CTRLForSequenceClassification"),s6o=o(" (CTRL model)"),l6o=l(),k1=a("li"),Roe=a("strong"),i6o=o("deberta"),d6o=o(" \u2014 "),m$=a("a"),c6o=o("DebertaForSequenceClassification"),f6o=o(" (DeBERTa model)"),m6o=l(),x1=a("li"),Soe=a("strong"),g6o=o("deberta-v2"),h6o=o(" \u2014 "),g$=a("a"),p6o=o("DebertaV2ForSequenceClassification"),_6o=o(" (DeBERTa-v2 model)"),u6o=l(),R1=a("li"),Poe=a("strong"),b6o=o("distilbert"),v6o=o(" \u2014 "),h$=a("a"),T6o=o("DistilBertForSequenceClassification"),F6o=o(" (DistilBERT model)"),C6o=l(),S1=a("li"),$oe=a("strong"),M6o=o("electra"),E6o=o(" \u2014 "),p$=a("a"),y6o=o("ElectraForSequenceClassification"),w6o=o(" (ELECTRA model)"),A6o=l(),P1=a("li"),Ioe=a("strong"),L6o=o("flaubert"),B6o=o(" \u2014 "),_$=a("a"),k6o=o("FlaubertForSequenceClassification"),x6o=o(" (FlauBERT model)"),R6o=l(),$1=a("li"),joe=a("strong"),S6o=o("fnet"),P6o=o(" \u2014 "),u$=a("a"),$6o=o("FNetForSequenceClassification"),I6o=o(" (FNet model)"),j6o=l(),I1=a("li"),Noe=a("strong"),N6o=o("funnel"),D6o=o(" \u2014 "),b$=a("a"),q6o=o("FunnelForSequenceClassification"),G6o=o(" (Funnel Transformer model)"),O6o=l(),j1=a("li"),Doe=a("strong"),X6o=o("gpt2"),z6o=o(" \u2014 "),v$=a("a"),V6o=o("GPT2ForSequenceClassification"),W6o=o(" (OpenAI GPT-2 model)"),Q6o=l(),N1=a("li"),qoe=a("strong"),H6o=o("gpt_neo"),U6o=o(" \u2014 "),T$=a("a"),J6o=o("GPTNeoForSequenceClassification"),Y6o=o(" (GPT Neo model)"),K6o=l(),D1=a("li"),Goe=a("strong"),Z6o=o("gptj"),eTo=o(" \u2014 "),F$=a("a"),oTo=o("GPTJForSequenceClassification"),rTo=o(" (GPT-J model)"),tTo=l(),q1=a("li"),Ooe=a("strong"),aTo=o("ibert"),nTo=o(" \u2014 "),C$=a("a"),sTo=o("IBertForSequenceClassification"),lTo=o(" (I-BERT model)"),iTo=l(),G1=a("li"),Xoe=a("strong"),dTo=o("layoutlm"),cTo=o(" \u2014 "),M$=a("a"),fTo=o("LayoutLMForSequenceClassification"),mTo=o(" (LayoutLM model)"),gTo=l(),O1=a("li"),zoe=a("strong"),hTo=o("layoutlmv2"),pTo=o(" \u2014 "),E$=a("a"),_To=o("LayoutLMv2ForSequenceClassification"),uTo=o(" (LayoutLMv2 model)"),bTo=l(),X1=a("li"),Voe=a("strong"),vTo=o("led"),TTo=o(" \u2014 "),y$=a("a"),FTo=o("LEDForSequenceClassification"),CTo=o(" (LED model)"),MTo=l(),z1=a("li"),Woe=a("strong"),ETo=o("longformer"),yTo=o(" \u2014 "),w$=a("a"),wTo=o("LongformerForSequenceClassification"),ATo=o(" (Longformer model)"),LTo=l(),V1=a("li"),Qoe=a("strong"),BTo=o("mbart"),kTo=o(" \u2014 "),A$=a("a"),xTo=o("MBartForSequenceClassification"),RTo=o(" (mBART model)"),STo=l(),W1=a("li"),Hoe=a("strong"),PTo=o("megatron-bert"),$To=o(" \u2014 "),L$=a("a"),ITo=o("MegatronBertForSequenceClassification"),jTo=o(" (MegatronBert model)"),NTo=l(),Q1=a("li"),Uoe=a("strong"),DTo=o("mobilebert"),qTo=o(" \u2014 "),B$=a("a"),GTo=o("MobileBertForSequenceClassification"),OTo=o(" (MobileBERT model)"),XTo=l(),H1=a("li"),Joe=a("strong"),zTo=o("mpnet"),VTo=o(" \u2014 "),k$=a("a"),WTo=o("MPNetForSequenceClassification"),QTo=o(" (MPNet model)"),HTo=l(),U1=a("li"),Yoe=a("strong"),UTo=o("nystromformer"),JTo=o(" \u2014 "),x$=a("a"),YTo=o("NystromformerForSequenceClassification"),KTo=o(" (Nystromformer model)"),ZTo=l(),J1=a("li"),Koe=a("strong"),e7o=o("openai-gpt"),o7o=o(" \u2014 "),R$=a("a"),r7o=o("OpenAIGPTForSequenceClassification"),t7o=o(" (OpenAI GPT model)"),a7o=l(),Y1=a("li"),Zoe=a("strong"),n7o=o("perceiver"),s7o=o(" \u2014 "),S$=a("a"),l7o=o("PerceiverForSequenceClassification"),i7o=o(" (Perceiver model)"),d7o=l(),K1=a("li"),ere=a("strong"),c7o=o("plbart"),f7o=o(" \u2014 "),P$=a("a"),m7o=o("PLBartForSequenceClassification"),g7o=o(" (PLBart model)"),h7o=l(),Z1=a("li"),ore=a("strong"),p7o=o("qdqbert"),_7o=o(" \u2014 "),$$=a("a"),u7o=o("QDQBertForSequenceClassification"),b7o=o(" (QDQBert model)"),v7o=l(),eb=a("li"),rre=a("strong"),T7o=o("reformer"),F7o=o(" \u2014 "),I$=a("a"),C7o=o("ReformerForSequenceClassification"),M7o=o(" (Reformer model)"),E7o=l(),ob=a("li"),tre=a("strong"),y7o=o("rembert"),w7o=o(" \u2014 "),j$=a("a"),A7o=o("RemBertForSequenceClassification"),L7o=o(" (RemBERT model)"),B7o=l(),rb=a("li"),are=a("strong"),k7o=o("roberta"),x7o=o(" \u2014 "),N$=a("a"),R7o=o("RobertaForSequenceClassification"),S7o=o(" (RoBERTa model)"),P7o=l(),tb=a("li"),nre=a("strong"),$7o=o("roformer"),I7o=o(" \u2014 "),D$=a("a"),j7o=o("RoFormerForSequenceClassification"),N7o=o(" (RoFormer model)"),D7o=l(),ab=a("li"),sre=a("strong"),q7o=o("squeezebert"),G7o=o(" \u2014 "),q$=a("a"),O7o=o("SqueezeBertForSequenceClassification"),X7o=o(" (SqueezeBERT model)"),z7o=l(),nb=a("li"),lre=a("strong"),V7o=o("tapas"),W7o=o(" \u2014 "),G$=a("a"),Q7o=o("TapasForSequenceClassification"),H7o=o(" (TAPAS model)"),U7o=l(),sb=a("li"),ire=a("strong"),J7o=o("transfo-xl"),Y7o=o(" \u2014 "),O$=a("a"),K7o=o("TransfoXLForSequenceClassification"),Z7o=o(" (Transformer-XL model)"),eFo=l(),lb=a("li"),dre=a("strong"),oFo=o("xlm"),rFo=o(" \u2014 "),X$=a("a"),tFo=o("XLMForSequenceClassification"),aFo=o(" (XLM model)"),nFo=l(),ib=a("li"),cre=a("strong"),sFo=o("xlm-roberta"),lFo=o(" \u2014 "),z$=a("a"),iFo=o("XLMRobertaForSequenceClassification"),dFo=o(" (XLM-RoBERTa model)"),cFo=l(),db=a("li"),fre=a("strong"),fFo=o("xlm-roberta-xl"),mFo=o(" \u2014 "),V$=a("a"),gFo=o("XLMRobertaXLForSequenceClassification"),hFo=o(" (XLM-RoBERTa-XL model)"),pFo=l(),cb=a("li"),mre=a("strong"),_Fo=o("xlnet"),uFo=o(" \u2014 "),W$=a("a"),bFo=o("XLNetForSequenceClassification"),vFo=o(" (XLNet model)"),TFo=l(),fb=a("li"),gre=a("strong"),FFo=o("yoso"),CFo=o(" \u2014 "),Q$=a("a"),MFo=o("YosoForSequenceClassification"),EFo=o(" (YOSO model)"),yFo=l(),mb=a("p"),wFo=o("The model is set in evaluation mode by default using "),hre=a("code"),AFo=o("model.eval()"),LFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=a("code"),BFo=o("model.train()"),kFo=l(),_re=a("p"),xFo=o("Examples:"),RFo=l(),f(CM.$$.fragment),LLe=l(),ad=a("h2"),gb=a("a"),ure=a("span"),f(MM.$$.fragment),SFo=l(),bre=a("span"),PFo=o("AutoModelForMultipleChoice"),BLe=l(),Yo=a("div"),f(EM.$$.fragment),$Fo=l(),nd=a("p"),IFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=a("code"),jFo=o("from_pretrained()"),NFo=o("class method or the "),Tre=a("code"),DFo=o("from_config()"),qFo=o(`class
method.`),GFo=l(),yM=a("p"),OFo=o("This class cannot be instantiated directly using "),Fre=a("code"),XFo=o("__init__()"),zFo=o(" (throws an error)."),VFo=l(),zr=a("div"),f(wM.$$.fragment),WFo=l(),Cre=a("p"),QFo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),HFo=l(),sd=a("p"),UFo=o(`Note:
Loading a model from its configuration file does `),Mre=a("strong"),JFo=o("not"),YFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=a("code"),KFo=o("from_pretrained()"),ZFo=o("to load the model weights."),e9o=l(),yre=a("p"),o9o=o("Examples:"),r9o=l(),f(AM.$$.fragment),t9o=l(),Ie=a("div"),f(LM.$$.fragment),a9o=l(),wre=a("p"),n9o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),s9o=l(),Va=a("p"),l9o=o("The model class to instantiate is selected based on the "),Are=a("code"),i9o=o("model_type"),d9o=o(` property of the config object (either
passed as an argument or loaded from `),Lre=a("code"),c9o=o("pretrained_model_name_or_path"),f9o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=a("code"),m9o=o("pretrained_model_name_or_path"),g9o=o(":"),h9o=l(),G=a("ul"),hb=a("li"),kre=a("strong"),p9o=o("albert"),_9o=o(" \u2014 "),H$=a("a"),u9o=o("AlbertForMultipleChoice"),b9o=o(" (ALBERT model)"),v9o=l(),pb=a("li"),xre=a("strong"),T9o=o("bert"),F9o=o(" \u2014 "),U$=a("a"),C9o=o("BertForMultipleChoice"),M9o=o(" (BERT model)"),E9o=l(),_b=a("li"),Rre=a("strong"),y9o=o("big_bird"),w9o=o(" \u2014 "),J$=a("a"),A9o=o("BigBirdForMultipleChoice"),L9o=o(" (BigBird model)"),B9o=l(),ub=a("li"),Sre=a("strong"),k9o=o("camembert"),x9o=o(" \u2014 "),Y$=a("a"),R9o=o("CamembertForMultipleChoice"),S9o=o(" (CamemBERT model)"),P9o=l(),bb=a("li"),Pre=a("strong"),$9o=o("canine"),I9o=o(" \u2014 "),K$=a("a"),j9o=o("CanineForMultipleChoice"),N9o=o(" (Canine model)"),D9o=l(),vb=a("li"),$re=a("strong"),q9o=o("convbert"),G9o=o(" \u2014 "),Z$=a("a"),O9o=o("ConvBertForMultipleChoice"),X9o=o(" (ConvBERT model)"),z9o=l(),Tb=a("li"),Ire=a("strong"),V9o=o("distilbert"),W9o=o(" \u2014 "),eI=a("a"),Q9o=o("DistilBertForMultipleChoice"),H9o=o(" (DistilBERT model)"),U9o=l(),Fb=a("li"),jre=a("strong"),J9o=o("electra"),Y9o=o(" \u2014 "),oI=a("a"),K9o=o("ElectraForMultipleChoice"),Z9o=o(" (ELECTRA model)"),eCo=l(),Cb=a("li"),Nre=a("strong"),oCo=o("flaubert"),rCo=o(" \u2014 "),rI=a("a"),tCo=o("FlaubertForMultipleChoice"),aCo=o(" (FlauBERT model)"),nCo=l(),Mb=a("li"),Dre=a("strong"),sCo=o("fnet"),lCo=o(" \u2014 "),tI=a("a"),iCo=o("FNetForMultipleChoice"),dCo=o(" (FNet model)"),cCo=l(),Eb=a("li"),qre=a("strong"),fCo=o("funnel"),mCo=o(" \u2014 "),aI=a("a"),gCo=o("FunnelForMultipleChoice"),hCo=o(" (Funnel Transformer model)"),pCo=l(),yb=a("li"),Gre=a("strong"),_Co=o("ibert"),uCo=o(" \u2014 "),nI=a("a"),bCo=o("IBertForMultipleChoice"),vCo=o(" (I-BERT model)"),TCo=l(),wb=a("li"),Ore=a("strong"),FCo=o("longformer"),CCo=o(" \u2014 "),sI=a("a"),MCo=o("LongformerForMultipleChoice"),ECo=o(" (Longformer model)"),yCo=l(),Ab=a("li"),Xre=a("strong"),wCo=o("megatron-bert"),ACo=o(" \u2014 "),lI=a("a"),LCo=o("MegatronBertForMultipleChoice"),BCo=o(" (MegatronBert model)"),kCo=l(),Lb=a("li"),zre=a("strong"),xCo=o("mobilebert"),RCo=o(" \u2014 "),iI=a("a"),SCo=o("MobileBertForMultipleChoice"),PCo=o(" (MobileBERT model)"),$Co=l(),Bb=a("li"),Vre=a("strong"),ICo=o("mpnet"),jCo=o(" \u2014 "),dI=a("a"),NCo=o("MPNetForMultipleChoice"),DCo=o(" (MPNet model)"),qCo=l(),kb=a("li"),Wre=a("strong"),GCo=o("nystromformer"),OCo=o(" \u2014 "),cI=a("a"),XCo=o("NystromformerForMultipleChoice"),zCo=o(" (Nystromformer model)"),VCo=l(),xb=a("li"),Qre=a("strong"),WCo=o("qdqbert"),QCo=o(" \u2014 "),fI=a("a"),HCo=o("QDQBertForMultipleChoice"),UCo=o(" (QDQBert model)"),JCo=l(),Rb=a("li"),Hre=a("strong"),YCo=o("rembert"),KCo=o(" \u2014 "),mI=a("a"),ZCo=o("RemBertForMultipleChoice"),e4o=o(" (RemBERT model)"),o4o=l(),Sb=a("li"),Ure=a("strong"),r4o=o("roberta"),t4o=o(" \u2014 "),gI=a("a"),a4o=o("RobertaForMultipleChoice"),n4o=o(" (RoBERTa model)"),s4o=l(),Pb=a("li"),Jre=a("strong"),l4o=o("roformer"),i4o=o(" \u2014 "),hI=a("a"),d4o=o("RoFormerForMultipleChoice"),c4o=o(" (RoFormer model)"),f4o=l(),$b=a("li"),Yre=a("strong"),m4o=o("squeezebert"),g4o=o(" \u2014 "),pI=a("a"),h4o=o("SqueezeBertForMultipleChoice"),p4o=o(" (SqueezeBERT model)"),_4o=l(),Ib=a("li"),Kre=a("strong"),u4o=o("xlm"),b4o=o(" \u2014 "),_I=a("a"),v4o=o("XLMForMultipleChoice"),T4o=o(" (XLM model)"),F4o=l(),jb=a("li"),Zre=a("strong"),C4o=o("xlm-roberta"),M4o=o(" \u2014 "),uI=a("a"),E4o=o("XLMRobertaForMultipleChoice"),y4o=o(" (XLM-RoBERTa model)"),w4o=l(),Nb=a("li"),ete=a("strong"),A4o=o("xlm-roberta-xl"),L4o=o(" \u2014 "),bI=a("a"),B4o=o("XLMRobertaXLForMultipleChoice"),k4o=o(" (XLM-RoBERTa-XL model)"),x4o=l(),Db=a("li"),ote=a("strong"),R4o=o("xlnet"),S4o=o(" \u2014 "),vI=a("a"),P4o=o("XLNetForMultipleChoice"),$4o=o(" (XLNet model)"),I4o=l(),qb=a("li"),rte=a("strong"),j4o=o("yoso"),N4o=o(" \u2014 "),TI=a("a"),D4o=o("YosoForMultipleChoice"),q4o=o(" (YOSO model)"),G4o=l(),Gb=a("p"),O4o=o("The model is set in evaluation mode by default using "),tte=a("code"),X4o=o("model.eval()"),z4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=a("code"),V4o=o("model.train()"),W4o=l(),nte=a("p"),Q4o=o("Examples:"),H4o=l(),f(BM.$$.fragment),kLe=l(),ld=a("h2"),Ob=a("a"),ste=a("span"),f(kM.$$.fragment),U4o=l(),lte=a("span"),J4o=o("AutoModelForNextSentencePrediction"),xLe=l(),Ko=a("div"),f(xM.$$.fragment),Y4o=l(),id=a("p"),K4o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=a("code"),Z4o=o("from_pretrained()"),eMo=o("class method or the "),dte=a("code"),oMo=o("from_config()"),rMo=o(`class
method.`),tMo=l(),RM=a("p"),aMo=o("This class cannot be instantiated directly using "),cte=a("code"),nMo=o("__init__()"),sMo=o(" (throws an error)."),lMo=l(),Vr=a("div"),f(SM.$$.fragment),iMo=l(),fte=a("p"),dMo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),cMo=l(),dd=a("p"),fMo=o(`Note:
Loading a model from its configuration file does `),mte=a("strong"),mMo=o("not"),gMo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=a("code"),hMo=o("from_pretrained()"),pMo=o("to load the model weights."),_Mo=l(),hte=a("p"),uMo=o("Examples:"),bMo=l(),f(PM.$$.fragment),vMo=l(),je=a("div"),f($M.$$.fragment),TMo=l(),pte=a("p"),FMo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),CMo=l(),Wa=a("p"),MMo=o("The model class to instantiate is selected based on the "),_te=a("code"),EMo=o("model_type"),yMo=o(` property of the config object (either
passed as an argument or loaded from `),ute=a("code"),wMo=o("pretrained_model_name_or_path"),AMo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=a("code"),LMo=o("pretrained_model_name_or_path"),BMo=o(":"),kMo=l(),na=a("ul"),Xb=a("li"),vte=a("strong"),xMo=o("bert"),RMo=o(" \u2014 "),FI=a("a"),SMo=o("BertForNextSentencePrediction"),PMo=o(" (BERT model)"),$Mo=l(),zb=a("li"),Tte=a("strong"),IMo=o("fnet"),jMo=o(" \u2014 "),CI=a("a"),NMo=o("FNetForNextSentencePrediction"),DMo=o(" (FNet model)"),qMo=l(),Vb=a("li"),Fte=a("strong"),GMo=o("megatron-bert"),OMo=o(" \u2014 "),MI=a("a"),XMo=o("MegatronBertForNextSentencePrediction"),zMo=o(" (MegatronBert model)"),VMo=l(),Wb=a("li"),Cte=a("strong"),WMo=o("mobilebert"),QMo=o(" \u2014 "),EI=a("a"),HMo=o("MobileBertForNextSentencePrediction"),UMo=o(" (MobileBERT model)"),JMo=l(),Qb=a("li"),Mte=a("strong"),YMo=o("qdqbert"),KMo=o(" \u2014 "),yI=a("a"),ZMo=o("QDQBertForNextSentencePrediction"),eEo=o(" (QDQBert model)"),oEo=l(),Hb=a("p"),rEo=o("The model is set in evaluation mode by default using "),Ete=a("code"),tEo=o("model.eval()"),aEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=a("code"),nEo=o("model.train()"),sEo=l(),wte=a("p"),lEo=o("Examples:"),iEo=l(),f(IM.$$.fragment),RLe=l(),cd=a("h2"),Ub=a("a"),Ate=a("span"),f(jM.$$.fragment),dEo=l(),Lte=a("span"),cEo=o("AutoModelForTokenClassification"),SLe=l(),Zo=a("div"),f(NM.$$.fragment),fEo=l(),fd=a("p"),mEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=a("code"),gEo=o("from_pretrained()"),hEo=o("class method or the "),kte=a("code"),pEo=o("from_config()"),_Eo=o(`class
method.`),uEo=l(),DM=a("p"),bEo=o("This class cannot be instantiated directly using "),xte=a("code"),vEo=o("__init__()"),TEo=o(" (throws an error)."),FEo=l(),Wr=a("div"),f(qM.$$.fragment),CEo=l(),Rte=a("p"),MEo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),EEo=l(),md=a("p"),yEo=o(`Note:
Loading a model from its configuration file does `),Ste=a("strong"),wEo=o("not"),AEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=a("code"),LEo=o("from_pretrained()"),BEo=o("to load the model weights."),kEo=l(),$te=a("p"),xEo=o("Examples:"),REo=l(),f(GM.$$.fragment),SEo=l(),Ne=a("div"),f(OM.$$.fragment),PEo=l(),Ite=a("p"),$Eo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),IEo=l(),Qa=a("p"),jEo=o("The model class to instantiate is selected based on the "),jte=a("code"),NEo=o("model_type"),DEo=o(` property of the config object (either
passed as an argument or loaded from `),Nte=a("code"),qEo=o("pretrained_model_name_or_path"),GEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=a("code"),OEo=o("pretrained_model_name_or_path"),XEo=o(":"),zEo=l(),D=a("ul"),Jb=a("li"),qte=a("strong"),VEo=o("albert"),WEo=o(" \u2014 "),wI=a("a"),QEo=o("AlbertForTokenClassification"),HEo=o(" (ALBERT model)"),UEo=l(),Yb=a("li"),Gte=a("strong"),JEo=o("bert"),YEo=o(" \u2014 "),AI=a("a"),KEo=o("BertForTokenClassification"),ZEo=o(" (BERT model)"),e3o=l(),Kb=a("li"),Ote=a("strong"),o3o=o("big_bird"),r3o=o(" \u2014 "),LI=a("a"),t3o=o("BigBirdForTokenClassification"),a3o=o(" (BigBird model)"),n3o=l(),Zb=a("li"),Xte=a("strong"),s3o=o("camembert"),l3o=o(" \u2014 "),BI=a("a"),i3o=o("CamembertForTokenClassification"),d3o=o(" (CamemBERT model)"),c3o=l(),e5=a("li"),zte=a("strong"),f3o=o("canine"),m3o=o(" \u2014 "),kI=a("a"),g3o=o("CanineForTokenClassification"),h3o=o(" (Canine model)"),p3o=l(),o5=a("li"),Vte=a("strong"),_3o=o("convbert"),u3o=o(" \u2014 "),xI=a("a"),b3o=o("ConvBertForTokenClassification"),v3o=o(" (ConvBERT model)"),T3o=l(),r5=a("li"),Wte=a("strong"),F3o=o("deberta"),C3o=o(" \u2014 "),RI=a("a"),M3o=o("DebertaForTokenClassification"),E3o=o(" (DeBERTa model)"),y3o=l(),t5=a("li"),Qte=a("strong"),w3o=o("deberta-v2"),A3o=o(" \u2014 "),SI=a("a"),L3o=o("DebertaV2ForTokenClassification"),B3o=o(" (DeBERTa-v2 model)"),k3o=l(),a5=a("li"),Hte=a("strong"),x3o=o("distilbert"),R3o=o(" \u2014 "),PI=a("a"),S3o=o("DistilBertForTokenClassification"),P3o=o(" (DistilBERT model)"),$3o=l(),n5=a("li"),Ute=a("strong"),I3o=o("electra"),j3o=o(" \u2014 "),$I=a("a"),N3o=o("ElectraForTokenClassification"),D3o=o(" (ELECTRA model)"),q3o=l(),s5=a("li"),Jte=a("strong"),G3o=o("flaubert"),O3o=o(" \u2014 "),II=a("a"),X3o=o("FlaubertForTokenClassification"),z3o=o(" (FlauBERT model)"),V3o=l(),l5=a("li"),Yte=a("strong"),W3o=o("fnet"),Q3o=o(" \u2014 "),jI=a("a"),H3o=o("FNetForTokenClassification"),U3o=o(" (FNet model)"),J3o=l(),i5=a("li"),Kte=a("strong"),Y3o=o("funnel"),K3o=o(" \u2014 "),NI=a("a"),Z3o=o("FunnelForTokenClassification"),eyo=o(" (Funnel Transformer model)"),oyo=l(),d5=a("li"),Zte=a("strong"),ryo=o("gpt2"),tyo=o(" \u2014 "),DI=a("a"),ayo=o("GPT2ForTokenClassification"),nyo=o(" (OpenAI GPT-2 model)"),syo=l(),c5=a("li"),eae=a("strong"),lyo=o("ibert"),iyo=o(" \u2014 "),qI=a("a"),dyo=o("IBertForTokenClassification"),cyo=o(" (I-BERT model)"),fyo=l(),f5=a("li"),oae=a("strong"),myo=o("layoutlm"),gyo=o(" \u2014 "),GI=a("a"),hyo=o("LayoutLMForTokenClassification"),pyo=o(" (LayoutLM model)"),_yo=l(),m5=a("li"),rae=a("strong"),uyo=o("layoutlmv2"),byo=o(" \u2014 "),OI=a("a"),vyo=o("LayoutLMv2ForTokenClassification"),Tyo=o(" (LayoutLMv2 model)"),Fyo=l(),g5=a("li"),tae=a("strong"),Cyo=o("longformer"),Myo=o(" \u2014 "),XI=a("a"),Eyo=o("LongformerForTokenClassification"),yyo=o(" (Longformer model)"),wyo=l(),h5=a("li"),aae=a("strong"),Ayo=o("megatron-bert"),Lyo=o(" \u2014 "),zI=a("a"),Byo=o("MegatronBertForTokenClassification"),kyo=o(" (MegatronBert model)"),xyo=l(),p5=a("li"),nae=a("strong"),Ryo=o("mobilebert"),Syo=o(" \u2014 "),VI=a("a"),Pyo=o("MobileBertForTokenClassification"),$yo=o(" (MobileBERT model)"),Iyo=l(),_5=a("li"),sae=a("strong"),jyo=o("mpnet"),Nyo=o(" \u2014 "),WI=a("a"),Dyo=o("MPNetForTokenClassification"),qyo=o(" (MPNet model)"),Gyo=l(),u5=a("li"),lae=a("strong"),Oyo=o("nystromformer"),Xyo=o(" \u2014 "),QI=a("a"),zyo=o("NystromformerForTokenClassification"),Vyo=o(" (Nystromformer model)"),Wyo=l(),b5=a("li"),iae=a("strong"),Qyo=o("qdqbert"),Hyo=o(" \u2014 "),HI=a("a"),Uyo=o("QDQBertForTokenClassification"),Jyo=o(" (QDQBert model)"),Yyo=l(),v5=a("li"),dae=a("strong"),Kyo=o("rembert"),Zyo=o(" \u2014 "),UI=a("a"),ewo=o("RemBertForTokenClassification"),owo=o(" (RemBERT model)"),rwo=l(),T5=a("li"),cae=a("strong"),two=o("roberta"),awo=o(" \u2014 "),JI=a("a"),nwo=o("RobertaForTokenClassification"),swo=o(" (RoBERTa model)"),lwo=l(),F5=a("li"),fae=a("strong"),iwo=o("roformer"),dwo=o(" \u2014 "),YI=a("a"),cwo=o("RoFormerForTokenClassification"),fwo=o(" (RoFormer model)"),mwo=l(),C5=a("li"),mae=a("strong"),gwo=o("squeezebert"),hwo=o(" \u2014 "),KI=a("a"),pwo=o("SqueezeBertForTokenClassification"),_wo=o(" (SqueezeBERT model)"),uwo=l(),M5=a("li"),gae=a("strong"),bwo=o("xlm"),vwo=o(" \u2014 "),ZI=a("a"),Two=o("XLMForTokenClassification"),Fwo=o(" (XLM model)"),Cwo=l(),E5=a("li"),hae=a("strong"),Mwo=o("xlm-roberta"),Ewo=o(" \u2014 "),ej=a("a"),ywo=o("XLMRobertaForTokenClassification"),wwo=o(" (XLM-RoBERTa model)"),Awo=l(),y5=a("li"),pae=a("strong"),Lwo=o("xlm-roberta-xl"),Bwo=o(" \u2014 "),oj=a("a"),kwo=o("XLMRobertaXLForTokenClassification"),xwo=o(" (XLM-RoBERTa-XL model)"),Rwo=l(),w5=a("li"),_ae=a("strong"),Swo=o("xlnet"),Pwo=o(" \u2014 "),rj=a("a"),$wo=o("XLNetForTokenClassification"),Iwo=o(" (XLNet model)"),jwo=l(),A5=a("li"),uae=a("strong"),Nwo=o("yoso"),Dwo=o(" \u2014 "),tj=a("a"),qwo=o("YosoForTokenClassification"),Gwo=o(" (YOSO model)"),Owo=l(),L5=a("p"),Xwo=o("The model is set in evaluation mode by default using "),bae=a("code"),zwo=o("model.eval()"),Vwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=a("code"),Wwo=o("model.train()"),Qwo=l(),Tae=a("p"),Hwo=o("Examples:"),Uwo=l(),f(XM.$$.fragment),PLe=l(),gd=a("h2"),B5=a("a"),Fae=a("span"),f(zM.$$.fragment),Jwo=l(),Cae=a("span"),Ywo=o("AutoModelForQuestionAnswering"),$Le=l(),er=a("div"),f(VM.$$.fragment),Kwo=l(),hd=a("p"),Zwo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=a("code"),eAo=o("from_pretrained()"),oAo=o("class method or the "),Eae=a("code"),rAo=o("from_config()"),tAo=o(`class
method.`),aAo=l(),WM=a("p"),nAo=o("This class cannot be instantiated directly using "),yae=a("code"),sAo=o("__init__()"),lAo=o(" (throws an error)."),iAo=l(),Qr=a("div"),f(QM.$$.fragment),dAo=l(),wae=a("p"),cAo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),fAo=l(),pd=a("p"),mAo=o(`Note:
Loading a model from its configuration file does `),Aae=a("strong"),gAo=o("not"),hAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=a("code"),pAo=o("from_pretrained()"),_Ao=o("to load the model weights."),uAo=l(),Bae=a("p"),bAo=o("Examples:"),vAo=l(),f(HM.$$.fragment),TAo=l(),De=a("div"),f(UM.$$.fragment),FAo=l(),kae=a("p"),CAo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),MAo=l(),Ha=a("p"),EAo=o("The model class to instantiate is selected based on the "),xae=a("code"),yAo=o("model_type"),wAo=o(` property of the config object (either
passed as an argument or loaded from `),Rae=a("code"),AAo=o("pretrained_model_name_or_path"),LAo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=a("code"),BAo=o("pretrained_model_name_or_path"),kAo=o(":"),xAo=l(),R=a("ul"),k5=a("li"),Pae=a("strong"),RAo=o("albert"),SAo=o(" \u2014 "),aj=a("a"),PAo=o("AlbertForQuestionAnswering"),$Ao=o(" (ALBERT model)"),IAo=l(),x5=a("li"),$ae=a("strong"),jAo=o("bart"),NAo=o(" \u2014 "),nj=a("a"),DAo=o("BartForQuestionAnswering"),qAo=o(" (BART model)"),GAo=l(),R5=a("li"),Iae=a("strong"),OAo=o("bert"),XAo=o(" \u2014 "),sj=a("a"),zAo=o("BertForQuestionAnswering"),VAo=o(" (BERT model)"),WAo=l(),S5=a("li"),jae=a("strong"),QAo=o("big_bird"),HAo=o(" \u2014 "),lj=a("a"),UAo=o("BigBirdForQuestionAnswering"),JAo=o(" (BigBird model)"),YAo=l(),P5=a("li"),Nae=a("strong"),KAo=o("bigbird_pegasus"),ZAo=o(" \u2014 "),ij=a("a"),e0o=o("BigBirdPegasusForQuestionAnswering"),o0o=o(" (BigBirdPegasus model)"),r0o=l(),$5=a("li"),Dae=a("strong"),t0o=o("camembert"),a0o=o(" \u2014 "),dj=a("a"),n0o=o("CamembertForQuestionAnswering"),s0o=o(" (CamemBERT model)"),l0o=l(),I5=a("li"),qae=a("strong"),i0o=o("canine"),d0o=o(" \u2014 "),cj=a("a"),c0o=o("CanineForQuestionAnswering"),f0o=o(" (Canine model)"),m0o=l(),j5=a("li"),Gae=a("strong"),g0o=o("convbert"),h0o=o(" \u2014 "),fj=a("a"),p0o=o("ConvBertForQuestionAnswering"),_0o=o(" (ConvBERT model)"),u0o=l(),N5=a("li"),Oae=a("strong"),b0o=o("deberta"),v0o=o(" \u2014 "),mj=a("a"),T0o=o("DebertaForQuestionAnswering"),F0o=o(" (DeBERTa model)"),C0o=l(),D5=a("li"),Xae=a("strong"),M0o=o("deberta-v2"),E0o=o(" \u2014 "),gj=a("a"),y0o=o("DebertaV2ForQuestionAnswering"),w0o=o(" (DeBERTa-v2 model)"),A0o=l(),q5=a("li"),zae=a("strong"),L0o=o("distilbert"),B0o=o(" \u2014 "),hj=a("a"),k0o=o("DistilBertForQuestionAnswering"),x0o=o(" (DistilBERT model)"),R0o=l(),G5=a("li"),Vae=a("strong"),S0o=o("electra"),P0o=o(" \u2014 "),pj=a("a"),$0o=o("ElectraForQuestionAnswering"),I0o=o(" (ELECTRA model)"),j0o=l(),O5=a("li"),Wae=a("strong"),N0o=o("flaubert"),D0o=o(" \u2014 "),_j=a("a"),q0o=o("FlaubertForQuestionAnsweringSimple"),G0o=o(" (FlauBERT model)"),O0o=l(),X5=a("li"),Qae=a("strong"),X0o=o("fnet"),z0o=o(" \u2014 "),uj=a("a"),V0o=o("FNetForQuestionAnswering"),W0o=o(" (FNet model)"),Q0o=l(),z5=a("li"),Hae=a("strong"),H0o=o("funnel"),U0o=o(" \u2014 "),bj=a("a"),J0o=o("FunnelForQuestionAnswering"),Y0o=o(" (Funnel Transformer model)"),K0o=l(),V5=a("li"),Uae=a("strong"),Z0o=o("gptj"),eLo=o(" \u2014 "),vj=a("a"),oLo=o("GPTJForQuestionAnswering"),rLo=o(" (GPT-J model)"),tLo=l(),W5=a("li"),Jae=a("strong"),aLo=o("ibert"),nLo=o(" \u2014 "),Tj=a("a"),sLo=o("IBertForQuestionAnswering"),lLo=o(" (I-BERT model)"),iLo=l(),Q5=a("li"),Yae=a("strong"),dLo=o("layoutlmv2"),cLo=o(" \u2014 "),Fj=a("a"),fLo=o("LayoutLMv2ForQuestionAnswering"),mLo=o(" (LayoutLMv2 model)"),gLo=l(),H5=a("li"),Kae=a("strong"),hLo=o("led"),pLo=o(" \u2014 "),Cj=a("a"),_Lo=o("LEDForQuestionAnswering"),uLo=o(" (LED model)"),bLo=l(),U5=a("li"),Zae=a("strong"),vLo=o("longformer"),TLo=o(" \u2014 "),Mj=a("a"),FLo=o("LongformerForQuestionAnswering"),CLo=o(" (Longformer model)"),MLo=l(),J5=a("li"),ene=a("strong"),ELo=o("lxmert"),yLo=o(" \u2014 "),Ej=a("a"),wLo=o("LxmertForQuestionAnswering"),ALo=o(" (LXMERT model)"),LLo=l(),Y5=a("li"),one=a("strong"),BLo=o("mbart"),kLo=o(" \u2014 "),yj=a("a"),xLo=o("MBartForQuestionAnswering"),RLo=o(" (mBART model)"),SLo=l(),K5=a("li"),rne=a("strong"),PLo=o("megatron-bert"),$Lo=o(" \u2014 "),wj=a("a"),ILo=o("MegatronBertForQuestionAnswering"),jLo=o(" (MegatronBert model)"),NLo=l(),Z5=a("li"),tne=a("strong"),DLo=o("mobilebert"),qLo=o(" \u2014 "),Aj=a("a"),GLo=o("MobileBertForQuestionAnswering"),OLo=o(" (MobileBERT model)"),XLo=l(),e2=a("li"),ane=a("strong"),zLo=o("mpnet"),VLo=o(" \u2014 "),Lj=a("a"),WLo=o("MPNetForQuestionAnswering"),QLo=o(" (MPNet model)"),HLo=l(),o2=a("li"),nne=a("strong"),ULo=o("nystromformer"),JLo=o(" \u2014 "),Bj=a("a"),YLo=o("NystromformerForQuestionAnswering"),KLo=o(" (Nystromformer model)"),ZLo=l(),r2=a("li"),sne=a("strong"),e8o=o("qdqbert"),o8o=o(" \u2014 "),kj=a("a"),r8o=o("QDQBertForQuestionAnswering"),t8o=o(" (QDQBert model)"),a8o=l(),t2=a("li"),lne=a("strong"),n8o=o("reformer"),s8o=o(" \u2014 "),xj=a("a"),l8o=o("ReformerForQuestionAnswering"),i8o=o(" (Reformer model)"),d8o=l(),a2=a("li"),ine=a("strong"),c8o=o("rembert"),f8o=o(" \u2014 "),Rj=a("a"),m8o=o("RemBertForQuestionAnswering"),g8o=o(" (RemBERT model)"),h8o=l(),n2=a("li"),dne=a("strong"),p8o=o("roberta"),_8o=o(" \u2014 "),Sj=a("a"),u8o=o("RobertaForQuestionAnswering"),b8o=o(" (RoBERTa model)"),v8o=l(),s2=a("li"),cne=a("strong"),T8o=o("roformer"),F8o=o(" \u2014 "),Pj=a("a"),C8o=o("RoFormerForQuestionAnswering"),M8o=o(" (RoFormer model)"),E8o=l(),l2=a("li"),fne=a("strong"),y8o=o("splinter"),w8o=o(" \u2014 "),$j=a("a"),A8o=o("SplinterForQuestionAnswering"),L8o=o(" (Splinter model)"),B8o=l(),i2=a("li"),mne=a("strong"),k8o=o("squeezebert"),x8o=o(" \u2014 "),Ij=a("a"),R8o=o("SqueezeBertForQuestionAnswering"),S8o=o(" (SqueezeBERT model)"),P8o=l(),d2=a("li"),gne=a("strong"),$8o=o("xlm"),I8o=o(" \u2014 "),jj=a("a"),j8o=o("XLMForQuestionAnsweringSimple"),N8o=o(" (XLM model)"),D8o=l(),c2=a("li"),hne=a("strong"),q8o=o("xlm-roberta"),G8o=o(" \u2014 "),Nj=a("a"),O8o=o("XLMRobertaForQuestionAnswering"),X8o=o(" (XLM-RoBERTa model)"),z8o=l(),f2=a("li"),pne=a("strong"),V8o=o("xlm-roberta-xl"),W8o=o(" \u2014 "),Dj=a("a"),Q8o=o("XLMRobertaXLForQuestionAnswering"),H8o=o(" (XLM-RoBERTa-XL model)"),U8o=l(),m2=a("li"),_ne=a("strong"),J8o=o("xlnet"),Y8o=o(" \u2014 "),qj=a("a"),K8o=o("XLNetForQuestionAnsweringSimple"),Z8o=o(" (XLNet model)"),eBo=l(),g2=a("li"),une=a("strong"),oBo=o("yoso"),rBo=o(" \u2014 "),Gj=a("a"),tBo=o("YosoForQuestionAnswering"),aBo=o(" (YOSO model)"),nBo=l(),h2=a("p"),sBo=o("The model is set in evaluation mode by default using "),bne=a("code"),lBo=o("model.eval()"),iBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=a("code"),dBo=o("model.train()"),cBo=l(),Tne=a("p"),fBo=o("Examples:"),mBo=l(),f(JM.$$.fragment),ILe=l(),_d=a("h2"),p2=a("a"),Fne=a("span"),f(YM.$$.fragment),gBo=l(),Cne=a("span"),hBo=o("AutoModelForTableQuestionAnswering"),jLe=l(),or=a("div"),f(KM.$$.fragment),pBo=l(),ud=a("p"),_Bo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=a("code"),uBo=o("from_pretrained()"),bBo=o("class method or the "),Ene=a("code"),vBo=o("from_config()"),TBo=o(`class
method.`),FBo=l(),ZM=a("p"),CBo=o("This class cannot be instantiated directly using "),yne=a("code"),MBo=o("__init__()"),EBo=o(" (throws an error)."),yBo=l(),Hr=a("div"),f(eE.$$.fragment),wBo=l(),wne=a("p"),ABo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),LBo=l(),bd=a("p"),BBo=o(`Note:
Loading a model from its configuration file does `),Ane=a("strong"),kBo=o("not"),xBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=a("code"),RBo=o("from_pretrained()"),SBo=o("to load the model weights."),PBo=l(),Bne=a("p"),$Bo=o("Examples:"),IBo=l(),f(oE.$$.fragment),jBo=l(),qe=a("div"),f(rE.$$.fragment),NBo=l(),kne=a("p"),DBo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),qBo=l(),Ua=a("p"),GBo=o("The model class to instantiate is selected based on the "),xne=a("code"),OBo=o("model_type"),XBo=o(` property of the config object (either
passed as an argument or loaded from `),Rne=a("code"),zBo=o("pretrained_model_name_or_path"),VBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=a("code"),WBo=o("pretrained_model_name_or_path"),QBo=o(":"),HBo=l(),Pne=a("ul"),_2=a("li"),$ne=a("strong"),UBo=o("tapas"),JBo=o(" \u2014 "),Oj=a("a"),YBo=o("TapasForQuestionAnswering"),KBo=o(" (TAPAS model)"),ZBo=l(),u2=a("p"),eko=o("The model is set in evaluation mode by default using "),Ine=a("code"),oko=o("model.eval()"),rko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=a("code"),tko=o("model.train()"),ako=l(),Nne=a("p"),nko=o("Examples:"),sko=l(),f(tE.$$.fragment),NLe=l(),vd=a("h2"),b2=a("a"),Dne=a("span"),f(aE.$$.fragment),lko=l(),qne=a("span"),iko=o("AutoModelForImageClassification"),DLe=l(),rr=a("div"),f(nE.$$.fragment),dko=l(),Td=a("p"),cko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=a("code"),fko=o("from_pretrained()"),mko=o("class method or the "),One=a("code"),gko=o("from_config()"),hko=o(`class
method.`),pko=l(),sE=a("p"),_ko=o("This class cannot be instantiated directly using "),Xne=a("code"),uko=o("__init__()"),bko=o(" (throws an error)."),vko=l(),Ur=a("div"),f(lE.$$.fragment),Tko=l(),zne=a("p"),Fko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Cko=l(),Fd=a("p"),Mko=o(`Note:
Loading a model from its configuration file does `),Vne=a("strong"),Eko=o("not"),yko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=a("code"),wko=o("from_pretrained()"),Ako=o("to load the model weights."),Lko=l(),Qne=a("p"),Bko=o("Examples:"),kko=l(),f(iE.$$.fragment),xko=l(),Ge=a("div"),f(dE.$$.fragment),Rko=l(),Hne=a("p"),Sko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Pko=l(),Ja=a("p"),$ko=o("The model class to instantiate is selected based on the "),Une=a("code"),Iko=o("model_type"),jko=o(` property of the config object (either
passed as an argument or loaded from `),Jne=a("code"),Nko=o("pretrained_model_name_or_path"),Dko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=a("code"),qko=o("pretrained_model_name_or_path"),Gko=o(":"),Oko=l(),be=a("ul"),v2=a("li"),Kne=a("strong"),Xko=o("beit"),zko=o(" \u2014 "),Xj=a("a"),Vko=o("BeitForImageClassification"),Wko=o(" (BEiT model)"),Qko=l(),T2=a("li"),Zne=a("strong"),Hko=o("convnext"),Uko=o(" \u2014 "),zj=a("a"),Jko=o("ConvNextForImageClassification"),Yko=o(" (ConvNext model)"),Kko=l(),Rs=a("li"),ese=a("strong"),Zko=o("deit"),exo=o(" \u2014 "),Vj=a("a"),oxo=o("DeiTForImageClassification"),rxo=o(" or "),Wj=a("a"),txo=o("DeiTForImageClassificationWithTeacher"),axo=o(" (DeiT model)"),nxo=l(),F2=a("li"),ose=a("strong"),sxo=o("imagegpt"),lxo=o(" \u2014 "),Qj=a("a"),ixo=o("ImageGPTForImageClassification"),dxo=o(" (ImageGPT model)"),cxo=l(),la=a("li"),rse=a("strong"),fxo=o("perceiver"),mxo=o(" \u2014 "),Hj=a("a"),gxo=o("PerceiverForImageClassificationLearned"),hxo=o(" or "),Uj=a("a"),pxo=o("PerceiverForImageClassificationFourier"),_xo=o(" or "),Jj=a("a"),uxo=o("PerceiverForImageClassificationConvProcessing"),bxo=o(" (Perceiver model)"),vxo=l(),C2=a("li"),tse=a("strong"),Txo=o("poolformer"),Fxo=o(" \u2014 "),Yj=a("a"),Cxo=o("PoolFormerForImageClassification"),Mxo=o(" (PoolFormer model)"),Exo=l(),M2=a("li"),ase=a("strong"),yxo=o("segformer"),wxo=o(" \u2014 "),Kj=a("a"),Axo=o("SegformerForImageClassification"),Lxo=o(" (SegFormer model)"),Bxo=l(),E2=a("li"),nse=a("strong"),kxo=o("swin"),xxo=o(" \u2014 "),Zj=a("a"),Rxo=o("SwinForImageClassification"),Sxo=o(" (Swin model)"),Pxo=l(),y2=a("li"),sse=a("strong"),$xo=o("vit"),Ixo=o(" \u2014 "),eN=a("a"),jxo=o("ViTForImageClassification"),Nxo=o(" (ViT model)"),Dxo=l(),w2=a("p"),qxo=o("The model is set in evaluation mode by default using "),lse=a("code"),Gxo=o("model.eval()"),Oxo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=a("code"),Xxo=o("model.train()"),zxo=l(),dse=a("p"),Vxo=o("Examples:"),Wxo=l(),f(cE.$$.fragment),qLe=l(),Cd=a("h2"),A2=a("a"),cse=a("span"),f(fE.$$.fragment),Qxo=l(),fse=a("span"),Hxo=o("AutoModelForVision2Seq"),GLe=l(),tr=a("div"),f(mE.$$.fragment),Uxo=l(),Md=a("p"),Jxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=a("code"),Yxo=o("from_pretrained()"),Kxo=o("class method or the "),gse=a("code"),Zxo=o("from_config()"),eRo=o(`class
method.`),oRo=l(),gE=a("p"),rRo=o("This class cannot be instantiated directly using "),hse=a("code"),tRo=o("__init__()"),aRo=o(" (throws an error)."),nRo=l(),Jr=a("div"),f(hE.$$.fragment),sRo=l(),pse=a("p"),lRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),iRo=l(),Ed=a("p"),dRo=o(`Note:
Loading a model from its configuration file does `),_se=a("strong"),cRo=o("not"),fRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),use=a("code"),mRo=o("from_pretrained()"),gRo=o("to load the model weights."),hRo=l(),bse=a("p"),pRo=o("Examples:"),_Ro=l(),f(pE.$$.fragment),uRo=l(),Oe=a("div"),f(_E.$$.fragment),bRo=l(),vse=a("p"),vRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),TRo=l(),Ya=a("p"),FRo=o("The model class to instantiate is selected based on the "),Tse=a("code"),CRo=o("model_type"),MRo=o(` property of the config object (either
passed as an argument or loaded from `),Fse=a("code"),ERo=o("pretrained_model_name_or_path"),yRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=a("code"),wRo=o("pretrained_model_name_or_path"),ARo=o(":"),LRo=l(),Mse=a("ul"),L2=a("li"),Ese=a("strong"),BRo=o("vision-encoder-decoder"),kRo=o(" \u2014 "),oN=a("a"),xRo=o("VisionEncoderDecoderModel"),RRo=o(" (Vision Encoder decoder model)"),SRo=l(),B2=a("p"),PRo=o("The model is set in evaluation mode by default using "),yse=a("code"),$Ro=o("model.eval()"),IRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=a("code"),jRo=o("model.train()"),NRo=l(),Ase=a("p"),DRo=o("Examples:"),qRo=l(),f(uE.$$.fragment),OLe=l(),yd=a("h2"),k2=a("a"),Lse=a("span"),f(bE.$$.fragment),GRo=l(),Bse=a("span"),ORo=o("AutoModelForAudioClassification"),XLe=l(),ar=a("div"),f(vE.$$.fragment),XRo=l(),wd=a("p"),zRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=a("code"),VRo=o("from_pretrained()"),WRo=o("class method or the "),xse=a("code"),QRo=o("from_config()"),HRo=o(`class
method.`),URo=l(),TE=a("p"),JRo=o("This class cannot be instantiated directly using "),Rse=a("code"),YRo=o("__init__()"),KRo=o(" (throws an error)."),ZRo=l(),Yr=a("div"),f(FE.$$.fragment),eSo=l(),Sse=a("p"),oSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),rSo=l(),Ad=a("p"),tSo=o(`Note:
Loading a model from its configuration file does `),Pse=a("strong"),aSo=o("not"),nSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=a("code"),sSo=o("from_pretrained()"),lSo=o("to load the model weights."),iSo=l(),Ise=a("p"),dSo=o("Examples:"),cSo=l(),f(CE.$$.fragment),fSo=l(),Xe=a("div"),f(ME.$$.fragment),mSo=l(),jse=a("p"),gSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),hSo=l(),Ka=a("p"),pSo=o("The model class to instantiate is selected based on the "),Nse=a("code"),_So=o("model_type"),uSo=o(` property of the config object (either
passed as an argument or loaded from `),Dse=a("code"),bSo=o("pretrained_model_name_or_path"),vSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=a("code"),TSo=o("pretrained_model_name_or_path"),FSo=o(":"),CSo=l(),ao=a("ul"),x2=a("li"),Gse=a("strong"),MSo=o("hubert"),ESo=o(" \u2014 "),rN=a("a"),ySo=o("HubertForSequenceClassification"),wSo=o(" (Hubert model)"),ASo=l(),R2=a("li"),Ose=a("strong"),LSo=o("sew"),BSo=o(" \u2014 "),tN=a("a"),kSo=o("SEWForSequenceClassification"),xSo=o(" (SEW model)"),RSo=l(),S2=a("li"),Xse=a("strong"),SSo=o("sew-d"),PSo=o(" \u2014 "),aN=a("a"),$So=o("SEWDForSequenceClassification"),ISo=o(" (SEW-D model)"),jSo=l(),P2=a("li"),zse=a("strong"),NSo=o("unispeech"),DSo=o(" \u2014 "),nN=a("a"),qSo=o("UniSpeechForSequenceClassification"),GSo=o(" (UniSpeech model)"),OSo=l(),$2=a("li"),Vse=a("strong"),XSo=o("unispeech-sat"),zSo=o(" \u2014 "),sN=a("a"),VSo=o("UniSpeechSatForSequenceClassification"),WSo=o(" (UniSpeechSat model)"),QSo=l(),I2=a("li"),Wse=a("strong"),HSo=o("wav2vec2"),USo=o(" \u2014 "),lN=a("a"),JSo=o("Wav2Vec2ForSequenceClassification"),YSo=o(" (Wav2Vec2 model)"),KSo=l(),j2=a("li"),Qse=a("strong"),ZSo=o("wavlm"),ePo=o(" \u2014 "),iN=a("a"),oPo=o("WavLMForSequenceClassification"),rPo=o(" (WavLM model)"),tPo=l(),N2=a("p"),aPo=o("The model is set in evaluation mode by default using "),Hse=a("code"),nPo=o("model.eval()"),sPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=a("code"),lPo=o("model.train()"),iPo=l(),Jse=a("p"),dPo=o("Examples:"),cPo=l(),f(EE.$$.fragment),zLe=l(),Ld=a("h2"),D2=a("a"),Yse=a("span"),f(yE.$$.fragment),fPo=l(),Kse=a("span"),mPo=o("AutoModelForAudioFrameClassification"),VLe=l(),nr=a("div"),f(wE.$$.fragment),gPo=l(),Bd=a("p"),hPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=a("code"),pPo=o("from_pretrained()"),_Po=o("class method or the "),ele=a("code"),uPo=o("from_config()"),bPo=o(`class
method.`),vPo=l(),AE=a("p"),TPo=o("This class cannot be instantiated directly using "),ole=a("code"),FPo=o("__init__()"),CPo=o(" (throws an error)."),MPo=l(),Kr=a("div"),f(LE.$$.fragment),EPo=l(),rle=a("p"),yPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wPo=l(),kd=a("p"),APo=o(`Note:
Loading a model from its configuration file does `),tle=a("strong"),LPo=o("not"),BPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),kPo=o("from_pretrained()"),xPo=o("to load the model weights."),RPo=l(),nle=a("p"),SPo=o("Examples:"),PPo=l(),f(BE.$$.fragment),$Po=l(),ze=a("div"),f(kE.$$.fragment),IPo=l(),sle=a("p"),jPo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),NPo=l(),Za=a("p"),DPo=o("The model class to instantiate is selected based on the "),lle=a("code"),qPo=o("model_type"),GPo=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),OPo=o("pretrained_model_name_or_path"),XPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),zPo=o("pretrained_model_name_or_path"),VPo=o(":"),WPo=l(),xd=a("ul"),q2=a("li"),cle=a("strong"),QPo=o("unispeech-sat"),HPo=o(" \u2014 "),dN=a("a"),UPo=o("UniSpeechSatForAudioFrameClassification"),JPo=o(" (UniSpeechSat model)"),YPo=l(),G2=a("li"),fle=a("strong"),KPo=o("wav2vec2"),ZPo=o(" \u2014 "),cN=a("a"),e$o=o("Wav2Vec2ForAudioFrameClassification"),o$o=o(" (Wav2Vec2 model)"),r$o=l(),O2=a("li"),mle=a("strong"),t$o=o("wavlm"),a$o=o(" \u2014 "),fN=a("a"),n$o=o("WavLMForAudioFrameClassification"),s$o=o(" (WavLM model)"),l$o=l(),X2=a("p"),i$o=o("The model is set in evaluation mode by default using "),gle=a("code"),d$o=o("model.eval()"),c$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=a("code"),f$o=o("model.train()"),m$o=l(),ple=a("p"),g$o=o("Examples:"),h$o=l(),f(xE.$$.fragment),WLe=l(),Rd=a("h2"),z2=a("a"),_le=a("span"),f(RE.$$.fragment),p$o=l(),ule=a("span"),_$o=o("AutoModelForCTC"),QLe=l(),sr=a("div"),f(SE.$$.fragment),u$o=l(),Sd=a("p"),b$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=a("code"),v$o=o("from_pretrained()"),T$o=o("class method or the "),vle=a("code"),F$o=o("from_config()"),C$o=o(`class
method.`),M$o=l(),PE=a("p"),E$o=o("This class cannot be instantiated directly using "),Tle=a("code"),y$o=o("__init__()"),w$o=o(" (throws an error)."),A$o=l(),Zr=a("div"),f($E.$$.fragment),L$o=l(),Fle=a("p"),B$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),k$o=l(),Pd=a("p"),x$o=o(`Note:
Loading a model from its configuration file does `),Cle=a("strong"),R$o=o("not"),S$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=a("code"),P$o=o("from_pretrained()"),$$o=o("to load the model weights."),I$o=l(),Ele=a("p"),j$o=o("Examples:"),N$o=l(),f(IE.$$.fragment),D$o=l(),Ve=a("div"),f(jE.$$.fragment),q$o=l(),yle=a("p"),G$o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),O$o=l(),en=a("p"),X$o=o("The model class to instantiate is selected based on the "),wle=a("code"),z$o=o("model_type"),V$o=o(` property of the config object (either
passed as an argument or loaded from `),Ale=a("code"),W$o=o("pretrained_model_name_or_path"),Q$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=a("code"),H$o=o("pretrained_model_name_or_path"),U$o=o(":"),J$o=l(),no=a("ul"),V2=a("li"),Ble=a("strong"),Y$o=o("hubert"),K$o=o(" \u2014 "),mN=a("a"),Z$o=o("HubertForCTC"),eIo=o(" (Hubert model)"),oIo=l(),W2=a("li"),kle=a("strong"),rIo=o("sew"),tIo=o(" \u2014 "),gN=a("a"),aIo=o("SEWForCTC"),nIo=o(" (SEW model)"),sIo=l(),Q2=a("li"),xle=a("strong"),lIo=o("sew-d"),iIo=o(" \u2014 "),hN=a("a"),dIo=o("SEWDForCTC"),cIo=o(" (SEW-D model)"),fIo=l(),H2=a("li"),Rle=a("strong"),mIo=o("unispeech"),gIo=o(" \u2014 "),pN=a("a"),hIo=o("UniSpeechForCTC"),pIo=o(" (UniSpeech model)"),_Io=l(),U2=a("li"),Sle=a("strong"),uIo=o("unispeech-sat"),bIo=o(" \u2014 "),_N=a("a"),vIo=o("UniSpeechSatForCTC"),TIo=o(" (UniSpeechSat model)"),FIo=l(),J2=a("li"),Ple=a("strong"),CIo=o("wav2vec2"),MIo=o(" \u2014 "),uN=a("a"),EIo=o("Wav2Vec2ForCTC"),yIo=o(" (Wav2Vec2 model)"),wIo=l(),Y2=a("li"),$le=a("strong"),AIo=o("wavlm"),LIo=o(" \u2014 "),bN=a("a"),BIo=o("WavLMForCTC"),kIo=o(" (WavLM model)"),xIo=l(),K2=a("p"),RIo=o("The model is set in evaluation mode by default using "),Ile=a("code"),SIo=o("model.eval()"),PIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=a("code"),$Io=o("model.train()"),IIo=l(),Nle=a("p"),jIo=o("Examples:"),NIo=l(),f(NE.$$.fragment),HLe=l(),$d=a("h2"),Z2=a("a"),Dle=a("span"),f(DE.$$.fragment),DIo=l(),qle=a("span"),qIo=o("AutoModelForSpeechSeq2Seq"),ULe=l(),lr=a("div"),f(qE.$$.fragment),GIo=l(),Id=a("p"),OIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=a("code"),XIo=o("from_pretrained()"),zIo=o("class method or the "),Ole=a("code"),VIo=o("from_config()"),WIo=o(`class
method.`),QIo=l(),GE=a("p"),HIo=o("This class cannot be instantiated directly using "),Xle=a("code"),UIo=o("__init__()"),JIo=o(" (throws an error)."),YIo=l(),et=a("div"),f(OE.$$.fragment),KIo=l(),zle=a("p"),ZIo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),ejo=l(),jd=a("p"),ojo=o(`Note:
Loading a model from its configuration file does `),Vle=a("strong"),rjo=o("not"),tjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=a("code"),ajo=o("from_pretrained()"),njo=o("to load the model weights."),sjo=l(),Qle=a("p"),ljo=o("Examples:"),ijo=l(),f(XE.$$.fragment),djo=l(),We=a("div"),f(zE.$$.fragment),cjo=l(),Hle=a("p"),fjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),mjo=l(),on=a("p"),gjo=o("The model class to instantiate is selected based on the "),Ule=a("code"),hjo=o("model_type"),pjo=o(` property of the config object (either
passed as an argument or loaded from `),Jle=a("code"),_jo=o("pretrained_model_name_or_path"),ujo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=a("code"),bjo=o("pretrained_model_name_or_path"),vjo=o(":"),Tjo=l(),VE=a("ul"),ev=a("li"),Kle=a("strong"),Fjo=o("speech-encoder-decoder"),Cjo=o(" \u2014 "),vN=a("a"),Mjo=o("SpeechEncoderDecoderModel"),Ejo=o(" (Speech Encoder decoder model)"),yjo=l(),ov=a("li"),Zle=a("strong"),wjo=o("speech_to_text"),Ajo=o(" \u2014 "),TN=a("a"),Ljo=o("Speech2TextForConditionalGeneration"),Bjo=o(" (Speech2Text model)"),kjo=l(),rv=a("p"),xjo=o("The model is set in evaluation mode by default using "),eie=a("code"),Rjo=o("model.eval()"),Sjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=a("code"),Pjo=o("model.train()"),$jo=l(),rie=a("p"),Ijo=o("Examples:"),jjo=l(),f(WE.$$.fragment),JLe=l(),Nd=a("h2"),tv=a("a"),tie=a("span"),f(QE.$$.fragment),Njo=l(),aie=a("span"),Djo=o("AutoModelForAudioXVector"),YLe=l(),ir=a("div"),f(HE.$$.fragment),qjo=l(),Dd=a("p"),Gjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=a("code"),Ojo=o("from_pretrained()"),Xjo=o("class method or the "),sie=a("code"),zjo=o("from_config()"),Vjo=o(`class
method.`),Wjo=l(),UE=a("p"),Qjo=o("This class cannot be instantiated directly using "),lie=a("code"),Hjo=o("__init__()"),Ujo=o(" (throws an error)."),Jjo=l(),ot=a("div"),f(JE.$$.fragment),Yjo=l(),iie=a("p"),Kjo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Zjo=l(),qd=a("p"),eNo=o(`Note:
Loading a model from its configuration file does `),die=a("strong"),oNo=o("not"),rNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=a("code"),tNo=o("from_pretrained()"),aNo=o("to load the model weights."),nNo=l(),fie=a("p"),sNo=o("Examples:"),lNo=l(),f(YE.$$.fragment),iNo=l(),Qe=a("div"),f(KE.$$.fragment),dNo=l(),mie=a("p"),cNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),fNo=l(),rn=a("p"),mNo=o("The model class to instantiate is selected based on the "),gie=a("code"),gNo=o("model_type"),hNo=o(` property of the config object (either
passed as an argument or loaded from `),hie=a("code"),pNo=o("pretrained_model_name_or_path"),_No=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=a("code"),uNo=o("pretrained_model_name_or_path"),bNo=o(":"),vNo=l(),Gd=a("ul"),av=a("li"),_ie=a("strong"),TNo=o("unispeech-sat"),FNo=o(" \u2014 "),FN=a("a"),CNo=o("UniSpeechSatForXVector"),MNo=o(" (UniSpeechSat model)"),ENo=l(),nv=a("li"),uie=a("strong"),yNo=o("wav2vec2"),wNo=o(" \u2014 "),CN=a("a"),ANo=o("Wav2Vec2ForXVector"),LNo=o(" (Wav2Vec2 model)"),BNo=l(),sv=a("li"),bie=a("strong"),kNo=o("wavlm"),xNo=o(" \u2014 "),MN=a("a"),RNo=o("WavLMForXVector"),SNo=o(" (WavLM model)"),PNo=l(),lv=a("p"),$No=o("The model is set in evaluation mode by default using "),vie=a("code"),INo=o("model.eval()"),jNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=a("code"),NNo=o("model.train()"),DNo=l(),Fie=a("p"),qNo=o("Examples:"),GNo=l(),f(ZE.$$.fragment),KLe=l(),Od=a("h2"),iv=a("a"),Cie=a("span"),f(e3.$$.fragment),ONo=l(),Mie=a("span"),XNo=o("AutoModelForMaskedImageModeling"),ZLe=l(),dr=a("div"),f(o3.$$.fragment),zNo=l(),Xd=a("p"),VNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=a("code"),WNo=o("from_pretrained()"),QNo=o("class method or the "),yie=a("code"),HNo=o("from_config()"),UNo=o(`class
method.`),JNo=l(),r3=a("p"),YNo=o("This class cannot be instantiated directly using "),wie=a("code"),KNo=o("__init__()"),ZNo=o(" (throws an error)."),eDo=l(),rt=a("div"),f(t3.$$.fragment),oDo=l(),Aie=a("p"),rDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),tDo=l(),zd=a("p"),aDo=o(`Note:
Loading a model from its configuration file does `),Lie=a("strong"),nDo=o("not"),sDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=a("code"),lDo=o("from_pretrained()"),iDo=o("to load the model weights."),dDo=l(),kie=a("p"),cDo=o("Examples:"),fDo=l(),f(a3.$$.fragment),mDo=l(),He=a("div"),f(n3.$$.fragment),gDo=l(),xie=a("p"),hDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),pDo=l(),tn=a("p"),_Do=o("The model class to instantiate is selected based on the "),Rie=a("code"),uDo=o("model_type"),bDo=o(` property of the config object (either
passed as an argument or loaded from `),Sie=a("code"),vDo=o("pretrained_model_name_or_path"),TDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=a("code"),FDo=o("pretrained_model_name_or_path"),CDo=o(":"),MDo=l(),Vd=a("ul"),dv=a("li"),$ie=a("strong"),EDo=o("deit"),yDo=o(" \u2014 "),EN=a("a"),wDo=o("DeiTForMaskedImageModeling"),ADo=o(" (DeiT model)"),LDo=l(),cv=a("li"),Iie=a("strong"),BDo=o("swin"),kDo=o(" \u2014 "),yN=a("a"),xDo=o("SwinForMaskedImageModeling"),RDo=o(" (Swin model)"),SDo=l(),fv=a("li"),jie=a("strong"),PDo=o("vit"),$Do=o(" \u2014 "),wN=a("a"),IDo=o("ViTForMaskedImageModeling"),jDo=o(" (ViT model)"),NDo=l(),mv=a("p"),DDo=o("The model is set in evaluation mode by default using "),Nie=a("code"),qDo=o("model.eval()"),GDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=a("code"),ODo=o("model.train()"),XDo=l(),qie=a("p"),zDo=o("Examples:"),VDo=l(),f(s3.$$.fragment),e8e=l(),Wd=a("h2"),gv=a("a"),Gie=a("span"),f(l3.$$.fragment),WDo=l(),Oie=a("span"),QDo=o("AutoModelForObjectDetection"),o8e=l(),cr=a("div"),f(i3.$$.fragment),HDo=l(),Qd=a("p"),UDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=a("code"),JDo=o("from_pretrained()"),YDo=o("class method or the "),zie=a("code"),KDo=o("from_config()"),ZDo=o(`class
method.`),eqo=l(),d3=a("p"),oqo=o("This class cannot be instantiated directly using "),Vie=a("code"),rqo=o("__init__()"),tqo=o(" (throws an error)."),aqo=l(),tt=a("div"),f(c3.$$.fragment),nqo=l(),Wie=a("p"),sqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),lqo=l(),Hd=a("p"),iqo=o(`Note:
Loading a model from its configuration file does `),Qie=a("strong"),dqo=o("not"),cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=a("code"),fqo=o("from_pretrained()"),mqo=o("to load the model weights."),gqo=l(),Uie=a("p"),hqo=o("Examples:"),pqo=l(),f(f3.$$.fragment),_qo=l(),Ue=a("div"),f(m3.$$.fragment),uqo=l(),Jie=a("p"),bqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),vqo=l(),an=a("p"),Tqo=o("The model class to instantiate is selected based on the "),Yie=a("code"),Fqo=o("model_type"),Cqo=o(` property of the config object (either
passed as an argument or loaded from `),Kie=a("code"),Mqo=o("pretrained_model_name_or_path"),Eqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=a("code"),yqo=o("pretrained_model_name_or_path"),wqo=o(":"),Aqo=l(),ede=a("ul"),hv=a("li"),ode=a("strong"),Lqo=o("detr"),Bqo=o(" \u2014 "),AN=a("a"),kqo=o("DetrForObjectDetection"),xqo=o(" (DETR model)"),Rqo=l(),pv=a("p"),Sqo=o("The model is set in evaluation mode by default using "),rde=a("code"),Pqo=o("model.eval()"),$qo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=a("code"),Iqo=o("model.train()"),jqo=l(),ade=a("p"),Nqo=o("Examples:"),Dqo=l(),f(g3.$$.fragment),r8e=l(),Ud=a("h2"),_v=a("a"),nde=a("span"),f(h3.$$.fragment),qqo=l(),sde=a("span"),Gqo=o("AutoModelForImageSegmentation"),t8e=l(),fr=a("div"),f(p3.$$.fragment),Oqo=l(),Jd=a("p"),Xqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=a("code"),zqo=o("from_pretrained()"),Vqo=o("class method or the "),ide=a("code"),Wqo=o("from_config()"),Qqo=o(`class
method.`),Hqo=l(),_3=a("p"),Uqo=o("This class cannot be instantiated directly using "),dde=a("code"),Jqo=o("__init__()"),Yqo=o(" (throws an error)."),Kqo=l(),at=a("div"),f(u3.$$.fragment),Zqo=l(),cde=a("p"),eGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),oGo=l(),Yd=a("p"),rGo=o(`Note:
Loading a model from its configuration file does `),fde=a("strong"),tGo=o("not"),aGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=a("code"),nGo=o("from_pretrained()"),sGo=o("to load the model weights."),lGo=l(),gde=a("p"),iGo=o("Examples:"),dGo=l(),f(b3.$$.fragment),cGo=l(),Je=a("div"),f(v3.$$.fragment),fGo=l(),hde=a("p"),mGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gGo=l(),nn=a("p"),hGo=o("The model class to instantiate is selected based on the "),pde=a("code"),pGo=o("model_type"),_Go=o(` property of the config object (either
passed as an argument or loaded from `),_de=a("code"),uGo=o("pretrained_model_name_or_path"),bGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=a("code"),vGo=o("pretrained_model_name_or_path"),TGo=o(":"),FGo=l(),bde=a("ul"),uv=a("li"),vde=a("strong"),CGo=o("detr"),MGo=o(" \u2014 "),LN=a("a"),EGo=o("DetrForSegmentation"),yGo=o(" (DETR model)"),wGo=l(),bv=a("p"),AGo=o("The model is set in evaluation mode by default using "),Tde=a("code"),LGo=o("model.eval()"),BGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=a("code"),kGo=o("model.train()"),xGo=l(),Cde=a("p"),RGo=o("Examples:"),SGo=l(),f(T3.$$.fragment),a8e=l(),Kd=a("h2"),vv=a("a"),Mde=a("span"),f(F3.$$.fragment),PGo=l(),Ede=a("span"),$Go=o("AutoModelForSemanticSegmentation"),n8e=l(),mr=a("div"),f(C3.$$.fragment),IGo=l(),Zd=a("p"),jGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=a("code"),NGo=o("from_pretrained()"),DGo=o("class method or the "),wde=a("code"),qGo=o("from_config()"),GGo=o(`class
method.`),OGo=l(),M3=a("p"),XGo=o("This class cannot be instantiated directly using "),Ade=a("code"),zGo=o("__init__()"),VGo=o(" (throws an error)."),WGo=l(),nt=a("div"),f(E3.$$.fragment),QGo=l(),Lde=a("p"),HGo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),UGo=l(),ec=a("p"),JGo=o(`Note:
Loading a model from its configuration file does `),Bde=a("strong"),YGo=o("not"),KGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=a("code"),ZGo=o("from_pretrained()"),eOo=o("to load the model weights."),oOo=l(),xde=a("p"),rOo=o("Examples:"),tOo=l(),f(y3.$$.fragment),aOo=l(),Ye=a("div"),f(w3.$$.fragment),nOo=l(),Rde=a("p"),sOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),lOo=l(),sn=a("p"),iOo=o("The model class to instantiate is selected based on the "),Sde=a("code"),dOo=o("model_type"),cOo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),fOo=o("pretrained_model_name_or_path"),mOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=a("code"),gOo=o("pretrained_model_name_or_path"),hOo=o(":"),pOo=l(),A3=a("ul"),Tv=a("li"),Ide=a("strong"),_Oo=o("beit"),uOo=o(" \u2014 "),BN=a("a"),bOo=o("BeitForSemanticSegmentation"),vOo=o(" (BEiT model)"),TOo=l(),Fv=a("li"),jde=a("strong"),FOo=o("segformer"),COo=o(" \u2014 "),kN=a("a"),MOo=o("SegformerForSemanticSegmentation"),EOo=o(" (SegFormer model)"),yOo=l(),Cv=a("p"),wOo=o("The model is set in evaluation mode by default using "),Nde=a("code"),AOo=o("model.eval()"),LOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=a("code"),BOo=o("model.train()"),kOo=l(),qde=a("p"),xOo=o("Examples:"),ROo=l(),f(L3.$$.fragment),s8e=l(),oc=a("h2"),Mv=a("a"),Gde=a("span"),f(B3.$$.fragment),SOo=l(),Ode=a("span"),POo=o("TFAutoModel"),l8e=l(),gr=a("div"),f(k3.$$.fragment),$Oo=l(),rc=a("p"),IOo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=a("code"),jOo=o("from_pretrained()"),NOo=o("class method or the "),zde=a("code"),DOo=o("from_config()"),qOo=o(`class
method.`),GOo=l(),x3=a("p"),OOo=o("This class cannot be instantiated directly using "),Vde=a("code"),XOo=o("__init__()"),zOo=o(" (throws an error)."),VOo=l(),st=a("div"),f(R3.$$.fragment),WOo=l(),Wde=a("p"),QOo=o("Instantiates one of the base model classes of the library from a configuration."),HOo=l(),tc=a("p"),UOo=o(`Note:
Loading a model from its configuration file does `),Qde=a("strong"),JOo=o("not"),YOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=a("code"),KOo=o("from_pretrained()"),ZOo=o("to load the model weights."),eXo=l(),Ude=a("p"),oXo=o("Examples:"),rXo=l(),f(S3.$$.fragment),tXo=l(),go=a("div"),f(P3.$$.fragment),aXo=l(),Jde=a("p"),nXo=o("Instantiate one of the base model classes of the library from a pretrained model."),sXo=l(),ln=a("p"),lXo=o("The model class to instantiate is selected based on the "),Yde=a("code"),iXo=o("model_type"),dXo=o(` property of the config object (either
passed as an argument or loaded from `),Kde=a("code"),cXo=o("pretrained_model_name_or_path"),fXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=a("code"),mXo=o("pretrained_model_name_or_path"),gXo=o(":"),hXo=l(),B=a("ul"),Ev=a("li"),ece=a("strong"),pXo=o("albert"),_Xo=o(" \u2014 "),xN=a("a"),uXo=o("TFAlbertModel"),bXo=o(" (ALBERT model)"),vXo=l(),yv=a("li"),oce=a("strong"),TXo=o("bart"),FXo=o(" \u2014 "),RN=a("a"),CXo=o("TFBartModel"),MXo=o(" (BART model)"),EXo=l(),wv=a("li"),rce=a("strong"),yXo=o("bert"),wXo=o(" \u2014 "),SN=a("a"),AXo=o("TFBertModel"),LXo=o(" (BERT model)"),BXo=l(),Av=a("li"),tce=a("strong"),kXo=o("blenderbot"),xXo=o(" \u2014 "),PN=a("a"),RXo=o("TFBlenderbotModel"),SXo=o(" (Blenderbot model)"),PXo=l(),Lv=a("li"),ace=a("strong"),$Xo=o("blenderbot-small"),IXo=o(" \u2014 "),$N=a("a"),jXo=o("TFBlenderbotSmallModel"),NXo=o(" (BlenderbotSmall model)"),DXo=l(),Bv=a("li"),nce=a("strong"),qXo=o("camembert"),GXo=o(" \u2014 "),IN=a("a"),OXo=o("TFCamembertModel"),XXo=o(" (CamemBERT model)"),zXo=l(),kv=a("li"),sce=a("strong"),VXo=o("clip"),WXo=o(" \u2014 "),jN=a("a"),QXo=o("TFCLIPModel"),HXo=o(" (CLIP model)"),UXo=l(),xv=a("li"),lce=a("strong"),JXo=o("convbert"),YXo=o(" \u2014 "),NN=a("a"),KXo=o("TFConvBertModel"),ZXo=o(" (ConvBERT model)"),ezo=l(),Rv=a("li"),ice=a("strong"),ozo=o("ctrl"),rzo=o(" \u2014 "),DN=a("a"),tzo=o("TFCTRLModel"),azo=o(" (CTRL model)"),nzo=l(),Sv=a("li"),dce=a("strong"),szo=o("deberta"),lzo=o(" \u2014 "),qN=a("a"),izo=o("TFDebertaModel"),dzo=o(" (DeBERTa model)"),czo=l(),Pv=a("li"),cce=a("strong"),fzo=o("deberta-v2"),mzo=o(" \u2014 "),GN=a("a"),gzo=o("TFDebertaV2Model"),hzo=o(" (DeBERTa-v2 model)"),pzo=l(),$v=a("li"),fce=a("strong"),_zo=o("distilbert"),uzo=o(" \u2014 "),ON=a("a"),bzo=o("TFDistilBertModel"),vzo=o(" (DistilBERT model)"),Tzo=l(),Iv=a("li"),mce=a("strong"),Fzo=o("dpr"),Czo=o(" \u2014 "),XN=a("a"),Mzo=o("TFDPRQuestionEncoder"),Ezo=o(" (DPR model)"),yzo=l(),jv=a("li"),gce=a("strong"),wzo=o("electra"),Azo=o(" \u2014 "),zN=a("a"),Lzo=o("TFElectraModel"),Bzo=o(" (ELECTRA model)"),kzo=l(),Nv=a("li"),hce=a("strong"),xzo=o("flaubert"),Rzo=o(" \u2014 "),VN=a("a"),Szo=o("TFFlaubertModel"),Pzo=o(" (FlauBERT model)"),$zo=l(),Ss=a("li"),pce=a("strong"),Izo=o("funnel"),jzo=o(" \u2014 "),WN=a("a"),Nzo=o("TFFunnelModel"),Dzo=o(" or "),QN=a("a"),qzo=o("TFFunnelBaseModel"),Gzo=o(" (Funnel Transformer model)"),Ozo=l(),Dv=a("li"),_ce=a("strong"),Xzo=o("gpt2"),zzo=o(" \u2014 "),HN=a("a"),Vzo=o("TFGPT2Model"),Wzo=o(" (OpenAI GPT-2 model)"),Qzo=l(),qv=a("li"),uce=a("strong"),Hzo=o("hubert"),Uzo=o(" \u2014 "),UN=a("a"),Jzo=o("TFHubertModel"),Yzo=o(" (Hubert model)"),Kzo=l(),Gv=a("li"),bce=a("strong"),Zzo=o("layoutlm"),eVo=o(" \u2014 "),JN=a("a"),oVo=o("TFLayoutLMModel"),rVo=o(" (LayoutLM model)"),tVo=l(),Ov=a("li"),vce=a("strong"),aVo=o("led"),nVo=o(" \u2014 "),YN=a("a"),sVo=o("TFLEDModel"),lVo=o(" (LED model)"),iVo=l(),Xv=a("li"),Tce=a("strong"),dVo=o("longformer"),cVo=o(" \u2014 "),KN=a("a"),fVo=o("TFLongformerModel"),mVo=o(" (Longformer model)"),gVo=l(),zv=a("li"),Fce=a("strong"),hVo=o("lxmert"),pVo=o(" \u2014 "),ZN=a("a"),_Vo=o("TFLxmertModel"),uVo=o(" (LXMERT model)"),bVo=l(),Vv=a("li"),Cce=a("strong"),vVo=o("marian"),TVo=o(" \u2014 "),eD=a("a"),FVo=o("TFMarianModel"),CVo=o(" (Marian model)"),MVo=l(),Wv=a("li"),Mce=a("strong"),EVo=o("mbart"),yVo=o(" \u2014 "),oD=a("a"),wVo=o("TFMBartModel"),AVo=o(" (mBART model)"),LVo=l(),Qv=a("li"),Ece=a("strong"),BVo=o("mobilebert"),kVo=o(" \u2014 "),rD=a("a"),xVo=o("TFMobileBertModel"),RVo=o(" (MobileBERT model)"),SVo=l(),Hv=a("li"),yce=a("strong"),PVo=o("mpnet"),$Vo=o(" \u2014 "),tD=a("a"),IVo=o("TFMPNetModel"),jVo=o(" (MPNet model)"),NVo=l(),Uv=a("li"),wce=a("strong"),DVo=o("mt5"),qVo=o(" \u2014 "),aD=a("a"),GVo=o("TFMT5Model"),OVo=o(" (mT5 model)"),XVo=l(),Jv=a("li"),Ace=a("strong"),zVo=o("openai-gpt"),VVo=o(" \u2014 "),nD=a("a"),WVo=o("TFOpenAIGPTModel"),QVo=o(" (OpenAI GPT model)"),HVo=l(),Yv=a("li"),Lce=a("strong"),UVo=o("pegasus"),JVo=o(" \u2014 "),sD=a("a"),YVo=o("TFPegasusModel"),KVo=o(" (Pegasus model)"),ZVo=l(),Kv=a("li"),Bce=a("strong"),eWo=o("rembert"),oWo=o(" \u2014 "),lD=a("a"),rWo=o("TFRemBertModel"),tWo=o(" (RemBERT model)"),aWo=l(),Zv=a("li"),kce=a("strong"),nWo=o("roberta"),sWo=o(" \u2014 "),iD=a("a"),lWo=o("TFRobertaModel"),iWo=o(" (RoBERTa model)"),dWo=l(),e6=a("li"),xce=a("strong"),cWo=o("roformer"),fWo=o(" \u2014 "),dD=a("a"),mWo=o("TFRoFormerModel"),gWo=o(" (RoFormer model)"),hWo=l(),o6=a("li"),Rce=a("strong"),pWo=o("speech_to_text"),_Wo=o(" \u2014 "),cD=a("a"),uWo=o("TFSpeech2TextModel"),bWo=o(" (Speech2Text model)"),vWo=l(),r6=a("li"),Sce=a("strong"),TWo=o("t5"),FWo=o(" \u2014 "),fD=a("a"),CWo=o("TFT5Model"),MWo=o(" (T5 model)"),EWo=l(),t6=a("li"),Pce=a("strong"),yWo=o("tapas"),wWo=o(" \u2014 "),mD=a("a"),AWo=o("TFTapasModel"),LWo=o(" (TAPAS model)"),BWo=l(),a6=a("li"),$ce=a("strong"),kWo=o("transfo-xl"),xWo=o(" \u2014 "),gD=a("a"),RWo=o("TFTransfoXLModel"),SWo=o(" (Transformer-XL model)"),PWo=l(),n6=a("li"),Ice=a("strong"),$Wo=o("vit"),IWo=o(" \u2014 "),hD=a("a"),jWo=o("TFViTModel"),NWo=o(" (ViT model)"),DWo=l(),s6=a("li"),jce=a("strong"),qWo=o("wav2vec2"),GWo=o(" \u2014 "),pD=a("a"),OWo=o("TFWav2Vec2Model"),XWo=o(" (Wav2Vec2 model)"),zWo=l(),l6=a("li"),Nce=a("strong"),VWo=o("xlm"),WWo=o(" \u2014 "),_D=a("a"),QWo=o("TFXLMModel"),HWo=o(" (XLM model)"),UWo=l(),i6=a("li"),Dce=a("strong"),JWo=o("xlm-roberta"),YWo=o(" \u2014 "),uD=a("a"),KWo=o("TFXLMRobertaModel"),ZWo=o(" (XLM-RoBERTa model)"),eQo=l(),d6=a("li"),qce=a("strong"),oQo=o("xlnet"),rQo=o(" \u2014 "),bD=a("a"),tQo=o("TFXLNetModel"),aQo=o(" (XLNet model)"),nQo=l(),Gce=a("p"),sQo=o("Examples:"),lQo=l(),f($3.$$.fragment),i8e=l(),ac=a("h2"),c6=a("a"),Oce=a("span"),f(I3.$$.fragment),iQo=l(),Xce=a("span"),dQo=o("TFAutoModelForPreTraining"),d8e=l(),hr=a("div"),f(j3.$$.fragment),cQo=l(),nc=a("p"),fQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=a("code"),mQo=o("from_pretrained()"),gQo=o("class method or the "),Vce=a("code"),hQo=o("from_config()"),pQo=o(`class
method.`),_Qo=l(),N3=a("p"),uQo=o("This class cannot be instantiated directly using "),Wce=a("code"),bQo=o("__init__()"),vQo=o(" (throws an error)."),TQo=l(),lt=a("div"),f(D3.$$.fragment),FQo=l(),Qce=a("p"),CQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),MQo=l(),sc=a("p"),EQo=o(`Note:
Loading a model from its configuration file does `),Hce=a("strong"),yQo=o("not"),wQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=a("code"),AQo=o("from_pretrained()"),LQo=o("to load the model weights."),BQo=l(),Jce=a("p"),kQo=o("Examples:"),xQo=l(),f(q3.$$.fragment),RQo=l(),ho=a("div"),f(G3.$$.fragment),SQo=l(),Yce=a("p"),PQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),$Qo=l(),dn=a("p"),IQo=o("The model class to instantiate is selected based on the "),Kce=a("code"),jQo=o("model_type"),NQo=o(` property of the config object (either
passed as an argument or loaded from `),Zce=a("code"),DQo=o("pretrained_model_name_or_path"),qQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=a("code"),GQo=o("pretrained_model_name_or_path"),OQo=o(":"),XQo=l(),H=a("ul"),f6=a("li"),ofe=a("strong"),zQo=o("albert"),VQo=o(" \u2014 "),vD=a("a"),WQo=o("TFAlbertForPreTraining"),QQo=o(" (ALBERT model)"),HQo=l(),m6=a("li"),rfe=a("strong"),UQo=o("bart"),JQo=o(" \u2014 "),TD=a("a"),YQo=o("TFBartForConditionalGeneration"),KQo=o(" (BART model)"),ZQo=l(),g6=a("li"),tfe=a("strong"),eHo=o("bert"),oHo=o(" \u2014 "),FD=a("a"),rHo=o("TFBertForPreTraining"),tHo=o(" (BERT model)"),aHo=l(),h6=a("li"),afe=a("strong"),nHo=o("camembert"),sHo=o(" \u2014 "),CD=a("a"),lHo=o("TFCamembertForMaskedLM"),iHo=o(" (CamemBERT model)"),dHo=l(),p6=a("li"),nfe=a("strong"),cHo=o("ctrl"),fHo=o(" \u2014 "),MD=a("a"),mHo=o("TFCTRLLMHeadModel"),gHo=o(" (CTRL model)"),hHo=l(),_6=a("li"),sfe=a("strong"),pHo=o("distilbert"),_Ho=o(" \u2014 "),ED=a("a"),uHo=o("TFDistilBertForMaskedLM"),bHo=o(" (DistilBERT model)"),vHo=l(),u6=a("li"),lfe=a("strong"),THo=o("electra"),FHo=o(" \u2014 "),yD=a("a"),CHo=o("TFElectraForPreTraining"),MHo=o(" (ELECTRA model)"),EHo=l(),b6=a("li"),ife=a("strong"),yHo=o("flaubert"),wHo=o(" \u2014 "),wD=a("a"),AHo=o("TFFlaubertWithLMHeadModel"),LHo=o(" (FlauBERT model)"),BHo=l(),v6=a("li"),dfe=a("strong"),kHo=o("funnel"),xHo=o(" \u2014 "),AD=a("a"),RHo=o("TFFunnelForPreTraining"),SHo=o(" (Funnel Transformer model)"),PHo=l(),T6=a("li"),cfe=a("strong"),$Ho=o("gpt2"),IHo=o(" \u2014 "),LD=a("a"),jHo=o("TFGPT2LMHeadModel"),NHo=o(" (OpenAI GPT-2 model)"),DHo=l(),F6=a("li"),ffe=a("strong"),qHo=o("layoutlm"),GHo=o(" \u2014 "),BD=a("a"),OHo=o("TFLayoutLMForMaskedLM"),XHo=o(" (LayoutLM model)"),zHo=l(),C6=a("li"),mfe=a("strong"),VHo=o("lxmert"),WHo=o(" \u2014 "),kD=a("a"),QHo=o("TFLxmertForPreTraining"),HHo=o(" (LXMERT model)"),UHo=l(),M6=a("li"),gfe=a("strong"),JHo=o("mobilebert"),YHo=o(" \u2014 "),xD=a("a"),KHo=o("TFMobileBertForPreTraining"),ZHo=o(" (MobileBERT model)"),eUo=l(),E6=a("li"),hfe=a("strong"),oUo=o("mpnet"),rUo=o(" \u2014 "),RD=a("a"),tUo=o("TFMPNetForMaskedLM"),aUo=o(" (MPNet model)"),nUo=l(),y6=a("li"),pfe=a("strong"),sUo=o("openai-gpt"),lUo=o(" \u2014 "),SD=a("a"),iUo=o("TFOpenAIGPTLMHeadModel"),dUo=o(" (OpenAI GPT model)"),cUo=l(),w6=a("li"),_fe=a("strong"),fUo=o("roberta"),mUo=o(" \u2014 "),PD=a("a"),gUo=o("TFRobertaForMaskedLM"),hUo=o(" (RoBERTa model)"),pUo=l(),A6=a("li"),ufe=a("strong"),_Uo=o("t5"),uUo=o(" \u2014 "),$D=a("a"),bUo=o("TFT5ForConditionalGeneration"),vUo=o(" (T5 model)"),TUo=l(),L6=a("li"),bfe=a("strong"),FUo=o("tapas"),CUo=o(" \u2014 "),ID=a("a"),MUo=o("TFTapasForMaskedLM"),EUo=o(" (TAPAS model)"),yUo=l(),B6=a("li"),vfe=a("strong"),wUo=o("transfo-xl"),AUo=o(" \u2014 "),jD=a("a"),LUo=o("TFTransfoXLLMHeadModel"),BUo=o(" (Transformer-XL model)"),kUo=l(),k6=a("li"),Tfe=a("strong"),xUo=o("xlm"),RUo=o(" \u2014 "),ND=a("a"),SUo=o("TFXLMWithLMHeadModel"),PUo=o(" (XLM model)"),$Uo=l(),x6=a("li"),Ffe=a("strong"),IUo=o("xlm-roberta"),jUo=o(" \u2014 "),DD=a("a"),NUo=o("TFXLMRobertaForMaskedLM"),DUo=o(" (XLM-RoBERTa model)"),qUo=l(),R6=a("li"),Cfe=a("strong"),GUo=o("xlnet"),OUo=o(" \u2014 "),qD=a("a"),XUo=o("TFXLNetLMHeadModel"),zUo=o(" (XLNet model)"),VUo=l(),Mfe=a("p"),WUo=o("Examples:"),QUo=l(),f(O3.$$.fragment),c8e=l(),lc=a("h2"),S6=a("a"),Efe=a("span"),f(X3.$$.fragment),HUo=l(),yfe=a("span"),UUo=o("TFAutoModelForCausalLM"),f8e=l(),pr=a("div"),f(z3.$$.fragment),JUo=l(),ic=a("p"),YUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=a("code"),KUo=o("from_pretrained()"),ZUo=o("class method or the "),Afe=a("code"),eJo=o("from_config()"),oJo=o(`class
method.`),rJo=l(),V3=a("p"),tJo=o("This class cannot be instantiated directly using "),Lfe=a("code"),aJo=o("__init__()"),nJo=o(" (throws an error)."),sJo=l(),it=a("div"),f(W3.$$.fragment),lJo=l(),Bfe=a("p"),iJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),dJo=l(),dc=a("p"),cJo=o(`Note:
Loading a model from its configuration file does `),kfe=a("strong"),fJo=o("not"),mJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=a("code"),gJo=o("from_pretrained()"),hJo=o("to load the model weights."),pJo=l(),Rfe=a("p"),_Jo=o("Examples:"),uJo=l(),f(Q3.$$.fragment),bJo=l(),po=a("div"),f(H3.$$.fragment),vJo=l(),Sfe=a("p"),TJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),FJo=l(),cn=a("p"),CJo=o("The model class to instantiate is selected based on the "),Pfe=a("code"),MJo=o("model_type"),EJo=o(` property of the config object (either
passed as an argument or loaded from `),$fe=a("code"),yJo=o("pretrained_model_name_or_path"),wJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=a("code"),AJo=o("pretrained_model_name_or_path"),LJo=o(":"),BJo=l(),he=a("ul"),P6=a("li"),jfe=a("strong"),kJo=o("bert"),xJo=o(" \u2014 "),GD=a("a"),RJo=o("TFBertLMHeadModel"),SJo=o(" (BERT model)"),PJo=l(),$6=a("li"),Nfe=a("strong"),$Jo=o("ctrl"),IJo=o(" \u2014 "),OD=a("a"),jJo=o("TFCTRLLMHeadModel"),NJo=o(" (CTRL model)"),DJo=l(),I6=a("li"),Dfe=a("strong"),qJo=o("gpt2"),GJo=o(" \u2014 "),XD=a("a"),OJo=o("TFGPT2LMHeadModel"),XJo=o(" (OpenAI GPT-2 model)"),zJo=l(),j6=a("li"),qfe=a("strong"),VJo=o("openai-gpt"),WJo=o(" \u2014 "),zD=a("a"),QJo=o("TFOpenAIGPTLMHeadModel"),HJo=o(" (OpenAI GPT model)"),UJo=l(),N6=a("li"),Gfe=a("strong"),JJo=o("rembert"),YJo=o(" \u2014 "),VD=a("a"),KJo=o("TFRemBertForCausalLM"),ZJo=o(" (RemBERT model)"),eYo=l(),D6=a("li"),Ofe=a("strong"),oYo=o("roberta"),rYo=o(" \u2014 "),WD=a("a"),tYo=o("TFRobertaForCausalLM"),aYo=o(" (RoBERTa model)"),nYo=l(),q6=a("li"),Xfe=a("strong"),sYo=o("roformer"),lYo=o(" \u2014 "),QD=a("a"),iYo=o("TFRoFormerForCausalLM"),dYo=o(" (RoFormer model)"),cYo=l(),G6=a("li"),zfe=a("strong"),fYo=o("transfo-xl"),mYo=o(" \u2014 "),HD=a("a"),gYo=o("TFTransfoXLLMHeadModel"),hYo=o(" (Transformer-XL model)"),pYo=l(),O6=a("li"),Vfe=a("strong"),_Yo=o("xlm"),uYo=o(" \u2014 "),UD=a("a"),bYo=o("TFXLMWithLMHeadModel"),vYo=o(" (XLM model)"),TYo=l(),X6=a("li"),Wfe=a("strong"),FYo=o("xlnet"),CYo=o(" \u2014 "),JD=a("a"),MYo=o("TFXLNetLMHeadModel"),EYo=o(" (XLNet model)"),yYo=l(),Qfe=a("p"),wYo=o("Examples:"),AYo=l(),f(U3.$$.fragment),m8e=l(),cc=a("h2"),z6=a("a"),Hfe=a("span"),f(J3.$$.fragment),LYo=l(),Ufe=a("span"),BYo=o("TFAutoModelForImageClassification"),g8e=l(),_r=a("div"),f(Y3.$$.fragment),kYo=l(),fc=a("p"),xYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=a("code"),RYo=o("from_pretrained()"),SYo=o("class method or the "),Yfe=a("code"),PYo=o("from_config()"),$Yo=o(`class
method.`),IYo=l(),K3=a("p"),jYo=o("This class cannot be instantiated directly using "),Kfe=a("code"),NYo=o("__init__()"),DYo=o(" (throws an error)."),qYo=l(),dt=a("div"),f(Z3.$$.fragment),GYo=l(),Zfe=a("p"),OYo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),XYo=l(),mc=a("p"),zYo=o(`Note:
Loading a model from its configuration file does `),eme=a("strong"),VYo=o("not"),WYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=a("code"),QYo=o("from_pretrained()"),HYo=o("to load the model weights."),UYo=l(),rme=a("p"),JYo=o("Examples:"),YYo=l(),f(ey.$$.fragment),KYo=l(),_o=a("div"),f(oy.$$.fragment),ZYo=l(),tme=a("p"),eKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),oKo=l(),fn=a("p"),rKo=o("The model class to instantiate is selected based on the "),ame=a("code"),tKo=o("model_type"),aKo=o(` property of the config object (either
passed as an argument or loaded from `),nme=a("code"),nKo=o("pretrained_model_name_or_path"),sKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=a("code"),lKo=o("pretrained_model_name_or_path"),iKo=o(":"),dKo=l(),lme=a("ul"),V6=a("li"),ime=a("strong"),cKo=o("vit"),fKo=o(" \u2014 "),YD=a("a"),mKo=o("TFViTForImageClassification"),gKo=o(" (ViT model)"),hKo=l(),dme=a("p"),pKo=o("Examples:"),_Ko=l(),f(ry.$$.fragment),h8e=l(),gc=a("h2"),W6=a("a"),cme=a("span"),f(ty.$$.fragment),uKo=l(),fme=a("span"),bKo=o("TFAutoModelForMaskedLM"),p8e=l(),ur=a("div"),f(ay.$$.fragment),vKo=l(),hc=a("p"),TKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=a("code"),FKo=o("from_pretrained()"),CKo=o("class method or the "),gme=a("code"),MKo=o("from_config()"),EKo=o(`class
method.`),yKo=l(),ny=a("p"),wKo=o("This class cannot be instantiated directly using "),hme=a("code"),AKo=o("__init__()"),LKo=o(" (throws an error)."),BKo=l(),ct=a("div"),f(sy.$$.fragment),kKo=l(),pme=a("p"),xKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),RKo=l(),pc=a("p"),SKo=o(`Note:
Loading a model from its configuration file does `),_me=a("strong"),PKo=o("not"),$Ko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=a("code"),IKo=o("from_pretrained()"),jKo=o("to load the model weights."),NKo=l(),bme=a("p"),DKo=o("Examples:"),qKo=l(),f(ly.$$.fragment),GKo=l(),uo=a("div"),f(iy.$$.fragment),OKo=l(),vme=a("p"),XKo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),zKo=l(),mn=a("p"),VKo=o("The model class to instantiate is selected based on the "),Tme=a("code"),WKo=o("model_type"),QKo=o(` property of the config object (either
passed as an argument or loaded from `),Fme=a("code"),HKo=o("pretrained_model_name_or_path"),UKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=a("code"),JKo=o("pretrained_model_name_or_path"),YKo=o(":"),KKo=l(),Y=a("ul"),Q6=a("li"),Mme=a("strong"),ZKo=o("albert"),eZo=o(" \u2014 "),KD=a("a"),oZo=o("TFAlbertForMaskedLM"),rZo=o(" (ALBERT model)"),tZo=l(),H6=a("li"),Eme=a("strong"),aZo=o("bert"),nZo=o(" \u2014 "),ZD=a("a"),sZo=o("TFBertForMaskedLM"),lZo=o(" (BERT model)"),iZo=l(),U6=a("li"),yme=a("strong"),dZo=o("camembert"),cZo=o(" \u2014 "),eq=a("a"),fZo=o("TFCamembertForMaskedLM"),mZo=o(" (CamemBERT model)"),gZo=l(),J6=a("li"),wme=a("strong"),hZo=o("convbert"),pZo=o(" \u2014 "),oq=a("a"),_Zo=o("TFConvBertForMaskedLM"),uZo=o(" (ConvBERT model)"),bZo=l(),Y6=a("li"),Ame=a("strong"),vZo=o("deberta"),TZo=o(" \u2014 "),rq=a("a"),FZo=o("TFDebertaForMaskedLM"),CZo=o(" (DeBERTa model)"),MZo=l(),K6=a("li"),Lme=a("strong"),EZo=o("deberta-v2"),yZo=o(" \u2014 "),tq=a("a"),wZo=o("TFDebertaV2ForMaskedLM"),AZo=o(" (DeBERTa-v2 model)"),LZo=l(),Z6=a("li"),Bme=a("strong"),BZo=o("distilbert"),kZo=o(" \u2014 "),aq=a("a"),xZo=o("TFDistilBertForMaskedLM"),RZo=o(" (DistilBERT model)"),SZo=l(),eT=a("li"),kme=a("strong"),PZo=o("electra"),$Zo=o(" \u2014 "),nq=a("a"),IZo=o("TFElectraForMaskedLM"),jZo=o(" (ELECTRA model)"),NZo=l(),oT=a("li"),xme=a("strong"),DZo=o("flaubert"),qZo=o(" \u2014 "),sq=a("a"),GZo=o("TFFlaubertWithLMHeadModel"),OZo=o(" (FlauBERT model)"),XZo=l(),rT=a("li"),Rme=a("strong"),zZo=o("funnel"),VZo=o(" \u2014 "),lq=a("a"),WZo=o("TFFunnelForMaskedLM"),QZo=o(" (Funnel Transformer model)"),HZo=l(),tT=a("li"),Sme=a("strong"),UZo=o("layoutlm"),JZo=o(" \u2014 "),iq=a("a"),YZo=o("TFLayoutLMForMaskedLM"),KZo=o(" (LayoutLM model)"),ZZo=l(),aT=a("li"),Pme=a("strong"),eer=o("longformer"),oer=o(" \u2014 "),dq=a("a"),rer=o("TFLongformerForMaskedLM"),ter=o(" (Longformer model)"),aer=l(),nT=a("li"),$me=a("strong"),ner=o("mobilebert"),ser=o(" \u2014 "),cq=a("a"),ler=o("TFMobileBertForMaskedLM"),ier=o(" (MobileBERT model)"),der=l(),sT=a("li"),Ime=a("strong"),cer=o("mpnet"),fer=o(" \u2014 "),fq=a("a"),mer=o("TFMPNetForMaskedLM"),ger=o(" (MPNet model)"),her=l(),lT=a("li"),jme=a("strong"),per=o("rembert"),_er=o(" \u2014 "),mq=a("a"),uer=o("TFRemBertForMaskedLM"),ber=o(" (RemBERT model)"),ver=l(),iT=a("li"),Nme=a("strong"),Ter=o("roberta"),Fer=o(" \u2014 "),gq=a("a"),Cer=o("TFRobertaForMaskedLM"),Mer=o(" (RoBERTa model)"),Eer=l(),dT=a("li"),Dme=a("strong"),yer=o("roformer"),wer=o(" \u2014 "),hq=a("a"),Aer=o("TFRoFormerForMaskedLM"),Ler=o(" (RoFormer model)"),Ber=l(),cT=a("li"),qme=a("strong"),ker=o("tapas"),xer=o(" \u2014 "),pq=a("a"),Rer=o("TFTapasForMaskedLM"),Ser=o(" (TAPAS model)"),Per=l(),fT=a("li"),Gme=a("strong"),$er=o("xlm"),Ier=o(" \u2014 "),_q=a("a"),jer=o("TFXLMWithLMHeadModel"),Ner=o(" (XLM model)"),Der=l(),mT=a("li"),Ome=a("strong"),qer=o("xlm-roberta"),Ger=o(" \u2014 "),uq=a("a"),Oer=o("TFXLMRobertaForMaskedLM"),Xer=o(" (XLM-RoBERTa model)"),zer=l(),Xme=a("p"),Ver=o("Examples:"),Wer=l(),f(dy.$$.fragment),_8e=l(),_c=a("h2"),gT=a("a"),zme=a("span"),f(cy.$$.fragment),Qer=l(),Vme=a("span"),Her=o("TFAutoModelForSeq2SeqLM"),u8e=l(),br=a("div"),f(fy.$$.fragment),Uer=l(),uc=a("p"),Jer=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=a("code"),Yer=o("from_pretrained()"),Ker=o("class method or the "),Qme=a("code"),Zer=o("from_config()"),eor=o(`class
method.`),oor=l(),my=a("p"),ror=o("This class cannot be instantiated directly using "),Hme=a("code"),tor=o("__init__()"),aor=o(" (throws an error)."),nor=l(),ft=a("div"),f(gy.$$.fragment),sor=l(),Ume=a("p"),lor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ior=l(),bc=a("p"),dor=o(`Note:
Loading a model from its configuration file does `),Jme=a("strong"),cor=o("not"),mor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=a("code"),gor=o("from_pretrained()"),hor=o("to load the model weights."),por=l(),Kme=a("p"),_or=o("Examples:"),uor=l(),f(hy.$$.fragment),bor=l(),bo=a("div"),f(py.$$.fragment),vor=l(),Zme=a("p"),Tor=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),For=l(),gn=a("p"),Cor=o("The model class to instantiate is selected based on the "),ege=a("code"),Mor=o("model_type"),Eor=o(` property of the config object (either
passed as an argument or loaded from `),oge=a("code"),yor=o("pretrained_model_name_or_path"),wor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=a("code"),Aor=o("pretrained_model_name_or_path"),Lor=o(":"),Bor=l(),pe=a("ul"),hT=a("li"),tge=a("strong"),kor=o("bart"),xor=o(" \u2014 "),bq=a("a"),Ror=o("TFBartForConditionalGeneration"),Sor=o(" (BART model)"),Por=l(),pT=a("li"),age=a("strong"),$or=o("blenderbot"),Ior=o(" \u2014 "),vq=a("a"),jor=o("TFBlenderbotForConditionalGeneration"),Nor=o(" (Blenderbot model)"),Dor=l(),_T=a("li"),nge=a("strong"),qor=o("blenderbot-small"),Gor=o(" \u2014 "),Tq=a("a"),Oor=o("TFBlenderbotSmallForConditionalGeneration"),Xor=o(" (BlenderbotSmall model)"),zor=l(),uT=a("li"),sge=a("strong"),Vor=o("encoder-decoder"),Wor=o(" \u2014 "),Fq=a("a"),Qor=o("TFEncoderDecoderModel"),Hor=o(" (Encoder decoder model)"),Uor=l(),bT=a("li"),lge=a("strong"),Jor=o("led"),Yor=o(" \u2014 "),Cq=a("a"),Kor=o("TFLEDForConditionalGeneration"),Zor=o(" (LED model)"),err=l(),vT=a("li"),ige=a("strong"),orr=o("marian"),rrr=o(" \u2014 "),Mq=a("a"),trr=o("TFMarianMTModel"),arr=o(" (Marian model)"),nrr=l(),TT=a("li"),dge=a("strong"),srr=o("mbart"),lrr=o(" \u2014 "),Eq=a("a"),irr=o("TFMBartForConditionalGeneration"),drr=o(" (mBART model)"),crr=l(),FT=a("li"),cge=a("strong"),frr=o("mt5"),mrr=o(" \u2014 "),yq=a("a"),grr=o("TFMT5ForConditionalGeneration"),hrr=o(" (mT5 model)"),prr=l(),CT=a("li"),fge=a("strong"),_rr=o("pegasus"),urr=o(" \u2014 "),wq=a("a"),brr=o("TFPegasusForConditionalGeneration"),vrr=o(" (Pegasus model)"),Trr=l(),MT=a("li"),mge=a("strong"),Frr=o("t5"),Crr=o(" \u2014 "),Aq=a("a"),Mrr=o("TFT5ForConditionalGeneration"),Err=o(" (T5 model)"),yrr=l(),gge=a("p"),wrr=o("Examples:"),Arr=l(),f(_y.$$.fragment),b8e=l(),vc=a("h2"),ET=a("a"),hge=a("span"),f(uy.$$.fragment),Lrr=l(),pge=a("span"),Brr=o("TFAutoModelForSequenceClassification"),v8e=l(),vr=a("div"),f(by.$$.fragment),krr=l(),Tc=a("p"),xrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=a("code"),Rrr=o("from_pretrained()"),Srr=o("class method or the "),uge=a("code"),Prr=o("from_config()"),$rr=o(`class
method.`),Irr=l(),vy=a("p"),jrr=o("This class cannot be instantiated directly using "),bge=a("code"),Nrr=o("__init__()"),Drr=o(" (throws an error)."),qrr=l(),mt=a("div"),f(Ty.$$.fragment),Grr=l(),vge=a("p"),Orr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xrr=l(),Fc=a("p"),zrr=o(`Note:
Loading a model from its configuration file does `),Tge=a("strong"),Vrr=o("not"),Wrr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=a("code"),Qrr=o("from_pretrained()"),Hrr=o("to load the model weights."),Urr=l(),Cge=a("p"),Jrr=o("Examples:"),Yrr=l(),f(Fy.$$.fragment),Krr=l(),vo=a("div"),f(Cy.$$.fragment),Zrr=l(),Mge=a("p"),etr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),otr=l(),hn=a("p"),rtr=o("The model class to instantiate is selected based on the "),Ege=a("code"),ttr=o("model_type"),atr=o(` property of the config object (either
passed as an argument or loaded from `),yge=a("code"),ntr=o("pretrained_model_name_or_path"),str=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=a("code"),ltr=o("pretrained_model_name_or_path"),itr=o(":"),dtr=l(),X=a("ul"),yT=a("li"),Age=a("strong"),ctr=o("albert"),ftr=o(" \u2014 "),Lq=a("a"),mtr=o("TFAlbertForSequenceClassification"),gtr=o(" (ALBERT model)"),htr=l(),wT=a("li"),Lge=a("strong"),ptr=o("bert"),_tr=o(" \u2014 "),Bq=a("a"),utr=o("TFBertForSequenceClassification"),btr=o(" (BERT model)"),vtr=l(),AT=a("li"),Bge=a("strong"),Ttr=o("camembert"),Ftr=o(" \u2014 "),kq=a("a"),Ctr=o("TFCamembertForSequenceClassification"),Mtr=o(" (CamemBERT model)"),Etr=l(),LT=a("li"),kge=a("strong"),ytr=o("convbert"),wtr=o(" \u2014 "),xq=a("a"),Atr=o("TFConvBertForSequenceClassification"),Ltr=o(" (ConvBERT model)"),Btr=l(),BT=a("li"),xge=a("strong"),ktr=o("ctrl"),xtr=o(" \u2014 "),Rq=a("a"),Rtr=o("TFCTRLForSequenceClassification"),Str=o(" (CTRL model)"),Ptr=l(),kT=a("li"),Rge=a("strong"),$tr=o("deberta"),Itr=o(" \u2014 "),Sq=a("a"),jtr=o("TFDebertaForSequenceClassification"),Ntr=o(" (DeBERTa model)"),Dtr=l(),xT=a("li"),Sge=a("strong"),qtr=o("deberta-v2"),Gtr=o(" \u2014 "),Pq=a("a"),Otr=o("TFDebertaV2ForSequenceClassification"),Xtr=o(" (DeBERTa-v2 model)"),ztr=l(),RT=a("li"),Pge=a("strong"),Vtr=o("distilbert"),Wtr=o(" \u2014 "),$q=a("a"),Qtr=o("TFDistilBertForSequenceClassification"),Htr=o(" (DistilBERT model)"),Utr=l(),ST=a("li"),$ge=a("strong"),Jtr=o("electra"),Ytr=o(" \u2014 "),Iq=a("a"),Ktr=o("TFElectraForSequenceClassification"),Ztr=o(" (ELECTRA model)"),ear=l(),PT=a("li"),Ige=a("strong"),oar=o("flaubert"),rar=o(" \u2014 "),jq=a("a"),tar=o("TFFlaubertForSequenceClassification"),aar=o(" (FlauBERT model)"),nar=l(),$T=a("li"),jge=a("strong"),sar=o("funnel"),lar=o(" \u2014 "),Nq=a("a"),iar=o("TFFunnelForSequenceClassification"),dar=o(" (Funnel Transformer model)"),car=l(),IT=a("li"),Nge=a("strong"),far=o("gpt2"),mar=o(" \u2014 "),Dq=a("a"),gar=o("TFGPT2ForSequenceClassification"),har=o(" (OpenAI GPT-2 model)"),par=l(),jT=a("li"),Dge=a("strong"),_ar=o("layoutlm"),uar=o(" \u2014 "),qq=a("a"),bar=o("TFLayoutLMForSequenceClassification"),Tar=o(" (LayoutLM model)"),Far=l(),NT=a("li"),qge=a("strong"),Car=o("longformer"),Mar=o(" \u2014 "),Gq=a("a"),Ear=o("TFLongformerForSequenceClassification"),yar=o(" (Longformer model)"),war=l(),DT=a("li"),Gge=a("strong"),Aar=o("mobilebert"),Lar=o(" \u2014 "),Oq=a("a"),Bar=o("TFMobileBertForSequenceClassification"),kar=o(" (MobileBERT model)"),xar=l(),qT=a("li"),Oge=a("strong"),Rar=o("mpnet"),Sar=o(" \u2014 "),Xq=a("a"),Par=o("TFMPNetForSequenceClassification"),$ar=o(" (MPNet model)"),Iar=l(),GT=a("li"),Xge=a("strong"),jar=o("openai-gpt"),Nar=o(" \u2014 "),zq=a("a"),Dar=o("TFOpenAIGPTForSequenceClassification"),qar=o(" (OpenAI GPT model)"),Gar=l(),OT=a("li"),zge=a("strong"),Oar=o("rembert"),Xar=o(" \u2014 "),Vq=a("a"),zar=o("TFRemBertForSequenceClassification"),Var=o(" (RemBERT model)"),War=l(),XT=a("li"),Vge=a("strong"),Qar=o("roberta"),Har=o(" \u2014 "),Wq=a("a"),Uar=o("TFRobertaForSequenceClassification"),Jar=o(" (RoBERTa model)"),Yar=l(),zT=a("li"),Wge=a("strong"),Kar=o("roformer"),Zar=o(" \u2014 "),Qq=a("a"),enr=o("TFRoFormerForSequenceClassification"),onr=o(" (RoFormer model)"),rnr=l(),VT=a("li"),Qge=a("strong"),tnr=o("tapas"),anr=o(" \u2014 "),Hq=a("a"),nnr=o("TFTapasForSequenceClassification"),snr=o(" (TAPAS model)"),lnr=l(),WT=a("li"),Hge=a("strong"),inr=o("transfo-xl"),dnr=o(" \u2014 "),Uq=a("a"),cnr=o("TFTransfoXLForSequenceClassification"),fnr=o(" (Transformer-XL model)"),mnr=l(),QT=a("li"),Uge=a("strong"),gnr=o("xlm"),hnr=o(" \u2014 "),Jq=a("a"),pnr=o("TFXLMForSequenceClassification"),_nr=o(" (XLM model)"),unr=l(),HT=a("li"),Jge=a("strong"),bnr=o("xlm-roberta"),vnr=o(" \u2014 "),Yq=a("a"),Tnr=o("TFXLMRobertaForSequenceClassification"),Fnr=o(" (XLM-RoBERTa model)"),Cnr=l(),UT=a("li"),Yge=a("strong"),Mnr=o("xlnet"),Enr=o(" \u2014 "),Kq=a("a"),ynr=o("TFXLNetForSequenceClassification"),wnr=o(" (XLNet model)"),Anr=l(),Kge=a("p"),Lnr=o("Examples:"),Bnr=l(),f(My.$$.fragment),T8e=l(),Cc=a("h2"),JT=a("a"),Zge=a("span"),f(Ey.$$.fragment),knr=l(),ehe=a("span"),xnr=o("TFAutoModelForMultipleChoice"),F8e=l(),Tr=a("div"),f(yy.$$.fragment),Rnr=l(),Mc=a("p"),Snr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=a("code"),Pnr=o("from_pretrained()"),$nr=o("class method or the "),rhe=a("code"),Inr=o("from_config()"),jnr=o(`class
method.`),Nnr=l(),wy=a("p"),Dnr=o("This class cannot be instantiated directly using "),the=a("code"),qnr=o("__init__()"),Gnr=o(" (throws an error)."),Onr=l(),gt=a("div"),f(Ay.$$.fragment),Xnr=l(),ahe=a("p"),znr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Vnr=l(),Ec=a("p"),Wnr=o(`Note:
Loading a model from its configuration file does `),nhe=a("strong"),Qnr=o("not"),Hnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),she=a("code"),Unr=o("from_pretrained()"),Jnr=o("to load the model weights."),Ynr=l(),lhe=a("p"),Knr=o("Examples:"),Znr=l(),f(Ly.$$.fragment),esr=l(),To=a("div"),f(By.$$.fragment),osr=l(),ihe=a("p"),rsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tsr=l(),pn=a("p"),asr=o("The model class to instantiate is selected based on the "),dhe=a("code"),nsr=o("model_type"),ssr=o(` property of the config object (either
passed as an argument or loaded from `),che=a("code"),lsr=o("pretrained_model_name_or_path"),isr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=a("code"),dsr=o("pretrained_model_name_or_path"),csr=o(":"),fsr=l(),te=a("ul"),YT=a("li"),mhe=a("strong"),msr=o("albert"),gsr=o(" \u2014 "),Zq=a("a"),hsr=o("TFAlbertForMultipleChoice"),psr=o(" (ALBERT model)"),_sr=l(),KT=a("li"),ghe=a("strong"),usr=o("bert"),bsr=o(" \u2014 "),eG=a("a"),vsr=o("TFBertForMultipleChoice"),Tsr=o(" (BERT model)"),Fsr=l(),ZT=a("li"),hhe=a("strong"),Csr=o("camembert"),Msr=o(" \u2014 "),oG=a("a"),Esr=o("TFCamembertForMultipleChoice"),ysr=o(" (CamemBERT model)"),wsr=l(),e7=a("li"),phe=a("strong"),Asr=o("convbert"),Lsr=o(" \u2014 "),rG=a("a"),Bsr=o("TFConvBertForMultipleChoice"),ksr=o(" (ConvBERT model)"),xsr=l(),o7=a("li"),_he=a("strong"),Rsr=o("distilbert"),Ssr=o(" \u2014 "),tG=a("a"),Psr=o("TFDistilBertForMultipleChoice"),$sr=o(" (DistilBERT model)"),Isr=l(),r7=a("li"),uhe=a("strong"),jsr=o("electra"),Nsr=o(" \u2014 "),aG=a("a"),Dsr=o("TFElectraForMultipleChoice"),qsr=o(" (ELECTRA model)"),Gsr=l(),t7=a("li"),bhe=a("strong"),Osr=o("flaubert"),Xsr=o(" \u2014 "),nG=a("a"),zsr=o("TFFlaubertForMultipleChoice"),Vsr=o(" (FlauBERT model)"),Wsr=l(),a7=a("li"),vhe=a("strong"),Qsr=o("funnel"),Hsr=o(" \u2014 "),sG=a("a"),Usr=o("TFFunnelForMultipleChoice"),Jsr=o(" (Funnel Transformer model)"),Ysr=l(),n7=a("li"),The=a("strong"),Ksr=o("longformer"),Zsr=o(" \u2014 "),lG=a("a"),elr=o("TFLongformerForMultipleChoice"),olr=o(" (Longformer model)"),rlr=l(),s7=a("li"),Fhe=a("strong"),tlr=o("mobilebert"),alr=o(" \u2014 "),iG=a("a"),nlr=o("TFMobileBertForMultipleChoice"),slr=o(" (MobileBERT model)"),llr=l(),l7=a("li"),Che=a("strong"),ilr=o("mpnet"),dlr=o(" \u2014 "),dG=a("a"),clr=o("TFMPNetForMultipleChoice"),flr=o(" (MPNet model)"),mlr=l(),i7=a("li"),Mhe=a("strong"),glr=o("rembert"),hlr=o(" \u2014 "),cG=a("a"),plr=o("TFRemBertForMultipleChoice"),_lr=o(" (RemBERT model)"),ulr=l(),d7=a("li"),Ehe=a("strong"),blr=o("roberta"),vlr=o(" \u2014 "),fG=a("a"),Tlr=o("TFRobertaForMultipleChoice"),Flr=o(" (RoBERTa model)"),Clr=l(),c7=a("li"),yhe=a("strong"),Mlr=o("roformer"),Elr=o(" \u2014 "),mG=a("a"),ylr=o("TFRoFormerForMultipleChoice"),wlr=o(" (RoFormer model)"),Alr=l(),f7=a("li"),whe=a("strong"),Llr=o("xlm"),Blr=o(" \u2014 "),gG=a("a"),klr=o("TFXLMForMultipleChoice"),xlr=o(" (XLM model)"),Rlr=l(),m7=a("li"),Ahe=a("strong"),Slr=o("xlm-roberta"),Plr=o(" \u2014 "),hG=a("a"),$lr=o("TFXLMRobertaForMultipleChoice"),Ilr=o(" (XLM-RoBERTa model)"),jlr=l(),g7=a("li"),Lhe=a("strong"),Nlr=o("xlnet"),Dlr=o(" \u2014 "),pG=a("a"),qlr=o("TFXLNetForMultipleChoice"),Glr=o(" (XLNet model)"),Olr=l(),Bhe=a("p"),Xlr=o("Examples:"),zlr=l(),f(ky.$$.fragment),C8e=l(),yc=a("h2"),h7=a("a"),khe=a("span"),f(xy.$$.fragment),Vlr=l(),xhe=a("span"),Wlr=o("TFAutoModelForTableQuestionAnswering"),M8e=l(),Fr=a("div"),f(Ry.$$.fragment),Qlr=l(),wc=a("p"),Hlr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=a("code"),Ulr=o("from_pretrained()"),Jlr=o("class method or the "),She=a("code"),Ylr=o("from_config()"),Klr=o(`class
method.`),Zlr=l(),Sy=a("p"),eir=o("This class cannot be instantiated directly using "),Phe=a("code"),oir=o("__init__()"),rir=o(" (throws an error)."),tir=l(),ht=a("div"),f(Py.$$.fragment),air=l(),$he=a("p"),nir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),sir=l(),Ac=a("p"),lir=o(`Note:
Loading a model from its configuration file does `),Ihe=a("strong"),iir=o("not"),dir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=a("code"),cir=o("from_pretrained()"),fir=o("to load the model weights."),mir=l(),Nhe=a("p"),gir=o("Examples:"),hir=l(),f($y.$$.fragment),pir=l(),Fo=a("div"),f(Iy.$$.fragment),_ir=l(),Dhe=a("p"),uir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),bir=l(),_n=a("p"),vir=o("The model class to instantiate is selected based on the "),qhe=a("code"),Tir=o("model_type"),Fir=o(` property of the config object (either
passed as an argument or loaded from `),Ghe=a("code"),Cir=o("pretrained_model_name_or_path"),Mir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=a("code"),Eir=o("pretrained_model_name_or_path"),yir=o(":"),wir=l(),Xhe=a("ul"),p7=a("li"),zhe=a("strong"),Air=o("tapas"),Lir=o(" \u2014 "),_G=a("a"),Bir=o("TFTapasForQuestionAnswering"),kir=o(" (TAPAS model)"),xir=l(),Vhe=a("p"),Rir=o("Examples:"),Sir=l(),f(jy.$$.fragment),E8e=l(),Lc=a("h2"),_7=a("a"),Whe=a("span"),f(Ny.$$.fragment),Pir=l(),Qhe=a("span"),$ir=o("TFAutoModelForTokenClassification"),y8e=l(),Cr=a("div"),f(Dy.$$.fragment),Iir=l(),Bc=a("p"),jir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=a("code"),Nir=o("from_pretrained()"),Dir=o("class method or the "),Uhe=a("code"),qir=o("from_config()"),Gir=o(`class
method.`),Oir=l(),qy=a("p"),Xir=o("This class cannot be instantiated directly using "),Jhe=a("code"),zir=o("__init__()"),Vir=o(" (throws an error)."),Wir=l(),pt=a("div"),f(Gy.$$.fragment),Qir=l(),Yhe=a("p"),Hir=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Uir=l(),kc=a("p"),Jir=o(`Note:
Loading a model from its configuration file does `),Khe=a("strong"),Yir=o("not"),Kir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=a("code"),Zir=o("from_pretrained()"),edr=o("to load the model weights."),odr=l(),epe=a("p"),rdr=o("Examples:"),tdr=l(),f(Oy.$$.fragment),adr=l(),Co=a("div"),f(Xy.$$.fragment),ndr=l(),ope=a("p"),sdr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),ldr=l(),un=a("p"),idr=o("The model class to instantiate is selected based on the "),rpe=a("code"),ddr=o("model_type"),cdr=o(` property of the config object (either
passed as an argument or loaded from `),tpe=a("code"),fdr=o("pretrained_model_name_or_path"),mdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=a("code"),gdr=o("pretrained_model_name_or_path"),hdr=o(":"),pdr=l(),K=a("ul"),u7=a("li"),npe=a("strong"),_dr=o("albert"),udr=o(" \u2014 "),uG=a("a"),bdr=o("TFAlbertForTokenClassification"),vdr=o(" (ALBERT model)"),Tdr=l(),b7=a("li"),spe=a("strong"),Fdr=o("bert"),Cdr=o(" \u2014 "),bG=a("a"),Mdr=o("TFBertForTokenClassification"),Edr=o(" (BERT model)"),ydr=l(),v7=a("li"),lpe=a("strong"),wdr=o("camembert"),Adr=o(" \u2014 "),vG=a("a"),Ldr=o("TFCamembertForTokenClassification"),Bdr=o(" (CamemBERT model)"),kdr=l(),T7=a("li"),ipe=a("strong"),xdr=o("convbert"),Rdr=o(" \u2014 "),TG=a("a"),Sdr=o("TFConvBertForTokenClassification"),Pdr=o(" (ConvBERT model)"),$dr=l(),F7=a("li"),dpe=a("strong"),Idr=o("deberta"),jdr=o(" \u2014 "),FG=a("a"),Ndr=o("TFDebertaForTokenClassification"),Ddr=o(" (DeBERTa model)"),qdr=l(),C7=a("li"),cpe=a("strong"),Gdr=o("deberta-v2"),Odr=o(" \u2014 "),CG=a("a"),Xdr=o("TFDebertaV2ForTokenClassification"),zdr=o(" (DeBERTa-v2 model)"),Vdr=l(),M7=a("li"),fpe=a("strong"),Wdr=o("distilbert"),Qdr=o(" \u2014 "),MG=a("a"),Hdr=o("TFDistilBertForTokenClassification"),Udr=o(" (DistilBERT model)"),Jdr=l(),E7=a("li"),mpe=a("strong"),Ydr=o("electra"),Kdr=o(" \u2014 "),EG=a("a"),Zdr=o("TFElectraForTokenClassification"),ecr=o(" (ELECTRA model)"),ocr=l(),y7=a("li"),gpe=a("strong"),rcr=o("flaubert"),tcr=o(" \u2014 "),yG=a("a"),acr=o("TFFlaubertForTokenClassification"),ncr=o(" (FlauBERT model)"),scr=l(),w7=a("li"),hpe=a("strong"),lcr=o("funnel"),icr=o(" \u2014 "),wG=a("a"),dcr=o("TFFunnelForTokenClassification"),ccr=o(" (Funnel Transformer model)"),fcr=l(),A7=a("li"),ppe=a("strong"),mcr=o("layoutlm"),gcr=o(" \u2014 "),AG=a("a"),hcr=o("TFLayoutLMForTokenClassification"),pcr=o(" (LayoutLM model)"),_cr=l(),L7=a("li"),_pe=a("strong"),ucr=o("longformer"),bcr=o(" \u2014 "),LG=a("a"),vcr=o("TFLongformerForTokenClassification"),Tcr=o(" (Longformer model)"),Fcr=l(),B7=a("li"),upe=a("strong"),Ccr=o("mobilebert"),Mcr=o(" \u2014 "),BG=a("a"),Ecr=o("TFMobileBertForTokenClassification"),ycr=o(" (MobileBERT model)"),wcr=l(),k7=a("li"),bpe=a("strong"),Acr=o("mpnet"),Lcr=o(" \u2014 "),kG=a("a"),Bcr=o("TFMPNetForTokenClassification"),kcr=o(" (MPNet model)"),xcr=l(),x7=a("li"),vpe=a("strong"),Rcr=o("rembert"),Scr=o(" \u2014 "),xG=a("a"),Pcr=o("TFRemBertForTokenClassification"),$cr=o(" (RemBERT model)"),Icr=l(),R7=a("li"),Tpe=a("strong"),jcr=o("roberta"),Ncr=o(" \u2014 "),RG=a("a"),Dcr=o("TFRobertaForTokenClassification"),qcr=o(" (RoBERTa model)"),Gcr=l(),S7=a("li"),Fpe=a("strong"),Ocr=o("roformer"),Xcr=o(" \u2014 "),SG=a("a"),zcr=o("TFRoFormerForTokenClassification"),Vcr=o(" (RoFormer model)"),Wcr=l(),P7=a("li"),Cpe=a("strong"),Qcr=o("xlm"),Hcr=o(" \u2014 "),PG=a("a"),Ucr=o("TFXLMForTokenClassification"),Jcr=o(" (XLM model)"),Ycr=l(),$7=a("li"),Mpe=a("strong"),Kcr=o("xlm-roberta"),Zcr=o(" \u2014 "),$G=a("a"),efr=o("TFXLMRobertaForTokenClassification"),ofr=o(" (XLM-RoBERTa model)"),rfr=l(),I7=a("li"),Epe=a("strong"),tfr=o("xlnet"),afr=o(" \u2014 "),IG=a("a"),nfr=o("TFXLNetForTokenClassification"),sfr=o(" (XLNet model)"),lfr=l(),ype=a("p"),ifr=o("Examples:"),dfr=l(),f(zy.$$.fragment),w8e=l(),xc=a("h2"),j7=a("a"),wpe=a("span"),f(Vy.$$.fragment),cfr=l(),Ape=a("span"),ffr=o("TFAutoModelForQuestionAnswering"),A8e=l(),Mr=a("div"),f(Wy.$$.fragment),mfr=l(),Rc=a("p"),gfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=a("code"),hfr=o("from_pretrained()"),pfr=o("class method or the "),Bpe=a("code"),_fr=o("from_config()"),ufr=o(`class
method.`),bfr=l(),Qy=a("p"),vfr=o("This class cannot be instantiated directly using "),kpe=a("code"),Tfr=o("__init__()"),Ffr=o(" (throws an error)."),Cfr=l(),_t=a("div"),f(Hy.$$.fragment),Mfr=l(),xpe=a("p"),Efr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),yfr=l(),Sc=a("p"),wfr=o(`Note:
Loading a model from its configuration file does `),Rpe=a("strong"),Afr=o("not"),Lfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=a("code"),Bfr=o("from_pretrained()"),kfr=o("to load the model weights."),xfr=l(),Ppe=a("p"),Rfr=o("Examples:"),Sfr=l(),f(Uy.$$.fragment),Pfr=l(),Mo=a("div"),f(Jy.$$.fragment),$fr=l(),$pe=a("p"),Ifr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),jfr=l(),bn=a("p"),Nfr=o("The model class to instantiate is selected based on the "),Ipe=a("code"),Dfr=o("model_type"),qfr=o(` property of the config object (either
passed as an argument or loaded from `),jpe=a("code"),Gfr=o("pretrained_model_name_or_path"),Ofr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=a("code"),Xfr=o("pretrained_model_name_or_path"),zfr=o(":"),Vfr=l(),Z=a("ul"),N7=a("li"),Dpe=a("strong"),Wfr=o("albert"),Qfr=o(" \u2014 "),jG=a("a"),Hfr=o("TFAlbertForQuestionAnswering"),Ufr=o(" (ALBERT model)"),Jfr=l(),D7=a("li"),qpe=a("strong"),Yfr=o("bert"),Kfr=o(" \u2014 "),NG=a("a"),Zfr=o("TFBertForQuestionAnswering"),emr=o(" (BERT model)"),omr=l(),q7=a("li"),Gpe=a("strong"),rmr=o("camembert"),tmr=o(" \u2014 "),DG=a("a"),amr=o("TFCamembertForQuestionAnswering"),nmr=o(" (CamemBERT model)"),smr=l(),G7=a("li"),Ope=a("strong"),lmr=o("convbert"),imr=o(" \u2014 "),qG=a("a"),dmr=o("TFConvBertForQuestionAnswering"),cmr=o(" (ConvBERT model)"),fmr=l(),O7=a("li"),Xpe=a("strong"),mmr=o("deberta"),gmr=o(" \u2014 "),GG=a("a"),hmr=o("TFDebertaForQuestionAnswering"),pmr=o(" (DeBERTa model)"),_mr=l(),X7=a("li"),zpe=a("strong"),umr=o("deberta-v2"),bmr=o(" \u2014 "),OG=a("a"),vmr=o("TFDebertaV2ForQuestionAnswering"),Tmr=o(" (DeBERTa-v2 model)"),Fmr=l(),z7=a("li"),Vpe=a("strong"),Cmr=o("distilbert"),Mmr=o(" \u2014 "),XG=a("a"),Emr=o("TFDistilBertForQuestionAnswering"),ymr=o(" (DistilBERT model)"),wmr=l(),V7=a("li"),Wpe=a("strong"),Amr=o("electra"),Lmr=o(" \u2014 "),zG=a("a"),Bmr=o("TFElectraForQuestionAnswering"),kmr=o(" (ELECTRA model)"),xmr=l(),W7=a("li"),Qpe=a("strong"),Rmr=o("flaubert"),Smr=o(" \u2014 "),VG=a("a"),Pmr=o("TFFlaubertForQuestionAnsweringSimple"),$mr=o(" (FlauBERT model)"),Imr=l(),Q7=a("li"),Hpe=a("strong"),jmr=o("funnel"),Nmr=o(" \u2014 "),WG=a("a"),Dmr=o("TFFunnelForQuestionAnswering"),qmr=o(" (Funnel Transformer model)"),Gmr=l(),H7=a("li"),Upe=a("strong"),Omr=o("longformer"),Xmr=o(" \u2014 "),QG=a("a"),zmr=o("TFLongformerForQuestionAnswering"),Vmr=o(" (Longformer model)"),Wmr=l(),U7=a("li"),Jpe=a("strong"),Qmr=o("mobilebert"),Hmr=o(" \u2014 "),HG=a("a"),Umr=o("TFMobileBertForQuestionAnswering"),Jmr=o(" (MobileBERT model)"),Ymr=l(),J7=a("li"),Ype=a("strong"),Kmr=o("mpnet"),Zmr=o(" \u2014 "),UG=a("a"),egr=o("TFMPNetForQuestionAnswering"),ogr=o(" (MPNet model)"),rgr=l(),Y7=a("li"),Kpe=a("strong"),tgr=o("rembert"),agr=o(" \u2014 "),JG=a("a"),ngr=o("TFRemBertForQuestionAnswering"),sgr=o(" (RemBERT model)"),lgr=l(),K7=a("li"),Zpe=a("strong"),igr=o("roberta"),dgr=o(" \u2014 "),YG=a("a"),cgr=o("TFRobertaForQuestionAnswering"),fgr=o(" (RoBERTa model)"),mgr=l(),Z7=a("li"),e_e=a("strong"),ggr=o("roformer"),hgr=o(" \u2014 "),KG=a("a"),pgr=o("TFRoFormerForQuestionAnswering"),_gr=o(" (RoFormer model)"),ugr=l(),eF=a("li"),o_e=a("strong"),bgr=o("xlm"),vgr=o(" \u2014 "),ZG=a("a"),Tgr=o("TFXLMForQuestionAnsweringSimple"),Fgr=o(" (XLM model)"),Cgr=l(),oF=a("li"),r_e=a("strong"),Mgr=o("xlm-roberta"),Egr=o(" \u2014 "),eO=a("a"),ygr=o("TFXLMRobertaForQuestionAnswering"),wgr=o(" (XLM-RoBERTa model)"),Agr=l(),rF=a("li"),t_e=a("strong"),Lgr=o("xlnet"),Bgr=o(" \u2014 "),oO=a("a"),kgr=o("TFXLNetForQuestionAnsweringSimple"),xgr=o(" (XLNet model)"),Rgr=l(),a_e=a("p"),Sgr=o("Examples:"),Pgr=l(),f(Yy.$$.fragment),L8e=l(),Pc=a("h2"),tF=a("a"),n_e=a("span"),f(Ky.$$.fragment),$gr=l(),s_e=a("span"),Igr=o("TFAutoModelForVision2Seq"),B8e=l(),Er=a("div"),f(Zy.$$.fragment),jgr=l(),$c=a("p"),Ngr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=a("code"),Dgr=o("from_pretrained()"),qgr=o("class method or the "),i_e=a("code"),Ggr=o("from_config()"),Ogr=o(`class
method.`),Xgr=l(),ew=a("p"),zgr=o("This class cannot be instantiated directly using "),d_e=a("code"),Vgr=o("__init__()"),Wgr=o(" (throws an error)."),Qgr=l(),ut=a("div"),f(ow.$$.fragment),Hgr=l(),c_e=a("p"),Ugr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Jgr=l(),Ic=a("p"),Ygr=o(`Note:
Loading a model from its configuration file does `),f_e=a("strong"),Kgr=o("not"),Zgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=a("code"),ehr=o("from_pretrained()"),ohr=o("to load the model weights."),rhr=l(),g_e=a("p"),thr=o("Examples:"),ahr=l(),f(rw.$$.fragment),nhr=l(),Eo=a("div"),f(tw.$$.fragment),shr=l(),h_e=a("p"),lhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),ihr=l(),vn=a("p"),dhr=o("The model class to instantiate is selected based on the "),p_e=a("code"),chr=o("model_type"),fhr=o(` property of the config object (either
passed as an argument or loaded from `),__e=a("code"),mhr=o("pretrained_model_name_or_path"),ghr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=a("code"),hhr=o("pretrained_model_name_or_path"),phr=o(":"),_hr=l(),b_e=a("ul"),aF=a("li"),v_e=a("strong"),uhr=o("vision-encoder-decoder"),bhr=o(" \u2014 "),rO=a("a"),vhr=o("TFVisionEncoderDecoderModel"),Thr=o(" (Vision Encoder decoder model)"),Fhr=l(),T_e=a("p"),Chr=o("Examples:"),Mhr=l(),f(aw.$$.fragment),k8e=l(),jc=a("h2"),nF=a("a"),F_e=a("span"),f(nw.$$.fragment),Ehr=l(),C_e=a("span"),yhr=o("TFAutoModelForSpeechSeq2Seq"),x8e=l(),yr=a("div"),f(sw.$$.fragment),whr=l(),Nc=a("p"),Ahr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=a("code"),Lhr=o("from_pretrained()"),Bhr=o("class method or the "),E_e=a("code"),khr=o("from_config()"),xhr=o(`class
method.`),Rhr=l(),lw=a("p"),Shr=o("This class cannot be instantiated directly using "),y_e=a("code"),Phr=o("__init__()"),$hr=o(" (throws an error)."),Ihr=l(),bt=a("div"),f(iw.$$.fragment),jhr=l(),w_e=a("p"),Nhr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Dhr=l(),Dc=a("p"),qhr=o(`Note:
Loading a model from its configuration file does `),A_e=a("strong"),Ghr=o("not"),Ohr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=a("code"),Xhr=o("from_pretrained()"),zhr=o("to load the model weights."),Vhr=l(),B_e=a("p"),Whr=o("Examples:"),Qhr=l(),f(dw.$$.fragment),Hhr=l(),yo=a("div"),f(cw.$$.fragment),Uhr=l(),k_e=a("p"),Jhr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Yhr=l(),Tn=a("p"),Khr=o("The model class to instantiate is selected based on the "),x_e=a("code"),Zhr=o("model_type"),epr=o(` property of the config object (either
passed as an argument or loaded from `),R_e=a("code"),opr=o("pretrained_model_name_or_path"),rpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=a("code"),tpr=o("pretrained_model_name_or_path"),apr=o(":"),npr=l(),P_e=a("ul"),sF=a("li"),$_e=a("strong"),spr=o("speech_to_text"),lpr=o(" \u2014 "),tO=a("a"),ipr=o("TFSpeech2TextForConditionalGeneration"),dpr=o(" (Speech2Text model)"),cpr=l(),I_e=a("p"),fpr=o("Examples:"),mpr=l(),f(fw.$$.fragment),R8e=l(),qc=a("h2"),lF=a("a"),j_e=a("span"),f(mw.$$.fragment),gpr=l(),N_e=a("span"),hpr=o("FlaxAutoModel"),S8e=l(),wr=a("div"),f(gw.$$.fragment),ppr=l(),Gc=a("p"),_pr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=a("code"),upr=o("from_pretrained()"),bpr=o("class method or the "),q_e=a("code"),vpr=o("from_config()"),Tpr=o(`class
method.`),Fpr=l(),hw=a("p"),Cpr=o("This class cannot be instantiated directly using "),G_e=a("code"),Mpr=o("__init__()"),Epr=o(" (throws an error)."),ypr=l(),vt=a("div"),f(pw.$$.fragment),wpr=l(),O_e=a("p"),Apr=o("Instantiates one of the base model classes of the library from a configuration."),Lpr=l(),Oc=a("p"),Bpr=o(`Note:
Loading a model from its configuration file does `),X_e=a("strong"),kpr=o("not"),xpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=a("code"),Rpr=o("from_pretrained()"),Spr=o("to load the model weights."),Ppr=l(),V_e=a("p"),$pr=o("Examples:"),Ipr=l(),f(_w.$$.fragment),jpr=l(),wo=a("div"),f(uw.$$.fragment),Npr=l(),W_e=a("p"),Dpr=o("Instantiate one of the base model classes of the library from a pretrained model."),qpr=l(),Fn=a("p"),Gpr=o("The model class to instantiate is selected based on the "),Q_e=a("code"),Opr=o("model_type"),Xpr=o(` property of the config object (either
passed as an argument or loaded from `),H_e=a("code"),zpr=o("pretrained_model_name_or_path"),Vpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=a("code"),Wpr=o("pretrained_model_name_or_path"),Qpr=o(":"),Hpr=l(),V=a("ul"),iF=a("li"),J_e=a("strong"),Upr=o("albert"),Jpr=o(" \u2014 "),aO=a("a"),Ypr=o("FlaxAlbertModel"),Kpr=o(" (ALBERT model)"),Zpr=l(),dF=a("li"),Y_e=a("strong"),e_r=o("bart"),o_r=o(" \u2014 "),nO=a("a"),r_r=o("FlaxBartModel"),t_r=o(" (BART model)"),a_r=l(),cF=a("li"),K_e=a("strong"),n_r=o("beit"),s_r=o(" \u2014 "),sO=a("a"),l_r=o("FlaxBeitModel"),i_r=o(" (BEiT model)"),d_r=l(),fF=a("li"),Z_e=a("strong"),c_r=o("bert"),f_r=o(" \u2014 "),lO=a("a"),m_r=o("FlaxBertModel"),g_r=o(" (BERT model)"),h_r=l(),mF=a("li"),eue=a("strong"),p_r=o("big_bird"),__r=o(" \u2014 "),iO=a("a"),u_r=o("FlaxBigBirdModel"),b_r=o(" (BigBird model)"),v_r=l(),gF=a("li"),oue=a("strong"),T_r=o("blenderbot"),F_r=o(" \u2014 "),dO=a("a"),C_r=o("FlaxBlenderbotModel"),M_r=o(" (Blenderbot model)"),E_r=l(),hF=a("li"),rue=a("strong"),y_r=o("blenderbot-small"),w_r=o(" \u2014 "),cO=a("a"),A_r=o("FlaxBlenderbotSmallModel"),L_r=o(" (BlenderbotSmall model)"),B_r=l(),pF=a("li"),tue=a("strong"),k_r=o("clip"),x_r=o(" \u2014 "),fO=a("a"),R_r=o("FlaxCLIPModel"),S_r=o(" (CLIP model)"),P_r=l(),_F=a("li"),aue=a("strong"),$_r=o("distilbert"),I_r=o(" \u2014 "),mO=a("a"),j_r=o("FlaxDistilBertModel"),N_r=o(" (DistilBERT model)"),D_r=l(),uF=a("li"),nue=a("strong"),q_r=o("electra"),G_r=o(" \u2014 "),gO=a("a"),O_r=o("FlaxElectraModel"),X_r=o(" (ELECTRA model)"),z_r=l(),bF=a("li"),sue=a("strong"),V_r=o("gpt2"),W_r=o(" \u2014 "),hO=a("a"),Q_r=o("FlaxGPT2Model"),H_r=o(" (OpenAI GPT-2 model)"),U_r=l(),vF=a("li"),lue=a("strong"),J_r=o("gpt_neo"),Y_r=o(" \u2014 "),pO=a("a"),K_r=o("FlaxGPTNeoModel"),Z_r=o(" (GPT Neo model)"),eur=l(),TF=a("li"),iue=a("strong"),our=o("gptj"),rur=o(" \u2014 "),_O=a("a"),tur=o("FlaxGPTJModel"),aur=o(" (GPT-J model)"),nur=l(),FF=a("li"),due=a("strong"),sur=o("marian"),lur=o(" \u2014 "),uO=a("a"),iur=o("FlaxMarianModel"),dur=o(" (Marian model)"),cur=l(),CF=a("li"),cue=a("strong"),fur=o("mbart"),mur=o(" \u2014 "),bO=a("a"),gur=o("FlaxMBartModel"),hur=o(" (mBART model)"),pur=l(),MF=a("li"),fue=a("strong"),_ur=o("mt5"),uur=o(" \u2014 "),vO=a("a"),bur=o("FlaxMT5Model"),vur=o(" (mT5 model)"),Tur=l(),EF=a("li"),mue=a("strong"),Fur=o("pegasus"),Cur=o(" \u2014 "),TO=a("a"),Mur=o("FlaxPegasusModel"),Eur=o(" (Pegasus model)"),yur=l(),yF=a("li"),gue=a("strong"),wur=o("roberta"),Aur=o(" \u2014 "),FO=a("a"),Lur=o("FlaxRobertaModel"),Bur=o(" (RoBERTa model)"),kur=l(),wF=a("li"),hue=a("strong"),xur=o("roformer"),Rur=o(" \u2014 "),CO=a("a"),Sur=o("FlaxRoFormerModel"),Pur=o(" (RoFormer model)"),$ur=l(),AF=a("li"),pue=a("strong"),Iur=o("t5"),jur=o(" \u2014 "),MO=a("a"),Nur=o("FlaxT5Model"),Dur=o(" (T5 model)"),qur=l(),LF=a("li"),_ue=a("strong"),Gur=o("vision-text-dual-encoder"),Our=o(" \u2014 "),EO=a("a"),Xur=o("FlaxVisionTextDualEncoderModel"),zur=o(" (VisionTextDualEncoder model)"),Vur=l(),BF=a("li"),uue=a("strong"),Wur=o("vit"),Qur=o(" \u2014 "),yO=a("a"),Hur=o("FlaxViTModel"),Uur=o(" (ViT model)"),Jur=l(),kF=a("li"),bue=a("strong"),Yur=o("wav2vec2"),Kur=o(" \u2014 "),wO=a("a"),Zur=o("FlaxWav2Vec2Model"),e1r=o(" (Wav2Vec2 model)"),o1r=l(),xF=a("li"),vue=a("strong"),r1r=o("xglm"),t1r=o(" \u2014 "),AO=a("a"),a1r=o("FlaxXGLMModel"),n1r=o(" (XGLM model)"),s1r=l(),Tue=a("p"),l1r=o("Examples:"),i1r=l(),f(bw.$$.fragment),P8e=l(),Xc=a("h2"),RF=a("a"),Fue=a("span"),f(vw.$$.fragment),d1r=l(),Cue=a("span"),c1r=o("FlaxAutoModelForCausalLM"),$8e=l(),Ar=a("div"),f(Tw.$$.fragment),f1r=l(),zc=a("p"),m1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=a("code"),g1r=o("from_pretrained()"),h1r=o("class method or the "),Eue=a("code"),p1r=o("from_config()"),_1r=o(`class
method.`),u1r=l(),Fw=a("p"),b1r=o("This class cannot be instantiated directly using "),yue=a("code"),v1r=o("__init__()"),T1r=o(" (throws an error)."),F1r=l(),Tt=a("div"),f(Cw.$$.fragment),C1r=l(),wue=a("p"),M1r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),E1r=l(),Vc=a("p"),y1r=o(`Note:
Loading a model from its configuration file does `),Aue=a("strong"),w1r=o("not"),A1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=a("code"),L1r=o("from_pretrained()"),B1r=o("to load the model weights."),k1r=l(),Bue=a("p"),x1r=o("Examples:"),R1r=l(),f(Mw.$$.fragment),S1r=l(),Ao=a("div"),f(Ew.$$.fragment),P1r=l(),kue=a("p"),$1r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),I1r=l(),Cn=a("p"),j1r=o("The model class to instantiate is selected based on the "),xue=a("code"),N1r=o("model_type"),D1r=o(` property of the config object (either
passed as an argument or loaded from `),Rue=a("code"),q1r=o("pretrained_model_name_or_path"),G1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=a("code"),O1r=o("pretrained_model_name_or_path"),X1r=o(":"),z1r=l(),Mn=a("ul"),SF=a("li"),Pue=a("strong"),V1r=o("gpt2"),W1r=o(" \u2014 "),LO=a("a"),Q1r=o("FlaxGPT2LMHeadModel"),H1r=o(" (OpenAI GPT-2 model)"),U1r=l(),PF=a("li"),$ue=a("strong"),J1r=o("gpt_neo"),Y1r=o(" \u2014 "),BO=a("a"),K1r=o("FlaxGPTNeoForCausalLM"),Z1r=o(" (GPT Neo model)"),ebr=l(),$F=a("li"),Iue=a("strong"),obr=o("gptj"),rbr=o(" \u2014 "),kO=a("a"),tbr=o("FlaxGPTJForCausalLM"),abr=o(" (GPT-J model)"),nbr=l(),IF=a("li"),jue=a("strong"),sbr=o("xglm"),lbr=o(" \u2014 "),xO=a("a"),ibr=o("FlaxXGLMForCausalLM"),dbr=o(" (XGLM model)"),cbr=l(),Nue=a("p"),fbr=o("Examples:"),mbr=l(),f(yw.$$.fragment),I8e=l(),Wc=a("h2"),jF=a("a"),Due=a("span"),f(ww.$$.fragment),gbr=l(),que=a("span"),hbr=o("FlaxAutoModelForPreTraining"),j8e=l(),Lr=a("div"),f(Aw.$$.fragment),pbr=l(),Qc=a("p"),_br=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=a("code"),ubr=o("from_pretrained()"),bbr=o("class method or the "),Oue=a("code"),vbr=o("from_config()"),Tbr=o(`class
method.`),Fbr=l(),Lw=a("p"),Cbr=o("This class cannot be instantiated directly using "),Xue=a("code"),Mbr=o("__init__()"),Ebr=o(" (throws an error)."),ybr=l(),Ft=a("div"),f(Bw.$$.fragment),wbr=l(),zue=a("p"),Abr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Lbr=l(),Hc=a("p"),Bbr=o(`Note:
Loading a model from its configuration file does `),Vue=a("strong"),kbr=o("not"),xbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=a("code"),Rbr=o("from_pretrained()"),Sbr=o("to load the model weights."),Pbr=l(),Que=a("p"),$br=o("Examples:"),Ibr=l(),f(kw.$$.fragment),jbr=l(),Lo=a("div"),f(xw.$$.fragment),Nbr=l(),Hue=a("p"),Dbr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),qbr=l(),En=a("p"),Gbr=o("The model class to instantiate is selected based on the "),Uue=a("code"),Obr=o("model_type"),Xbr=o(` property of the config object (either
passed as an argument or loaded from `),Jue=a("code"),zbr=o("pretrained_model_name_or_path"),Vbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=a("code"),Wbr=o("pretrained_model_name_or_path"),Qbr=o(":"),Hbr=l(),fe=a("ul"),NF=a("li"),Kue=a("strong"),Ubr=o("albert"),Jbr=o(" \u2014 "),RO=a("a"),Ybr=o("FlaxAlbertForPreTraining"),Kbr=o(" (ALBERT model)"),Zbr=l(),DF=a("li"),Zue=a("strong"),e5r=o("bart"),o5r=o(" \u2014 "),SO=a("a"),r5r=o("FlaxBartForConditionalGeneration"),t5r=o(" (BART model)"),a5r=l(),qF=a("li"),e1e=a("strong"),n5r=o("bert"),s5r=o(" \u2014 "),PO=a("a"),l5r=o("FlaxBertForPreTraining"),i5r=o(" (BERT model)"),d5r=l(),GF=a("li"),o1e=a("strong"),c5r=o("big_bird"),f5r=o(" \u2014 "),$O=a("a"),m5r=o("FlaxBigBirdForPreTraining"),g5r=o(" (BigBird model)"),h5r=l(),OF=a("li"),r1e=a("strong"),p5r=o("electra"),_5r=o(" \u2014 "),IO=a("a"),u5r=o("FlaxElectraForPreTraining"),b5r=o(" (ELECTRA model)"),v5r=l(),XF=a("li"),t1e=a("strong"),T5r=o("mbart"),F5r=o(" \u2014 "),jO=a("a"),C5r=o("FlaxMBartForConditionalGeneration"),M5r=o(" (mBART model)"),E5r=l(),zF=a("li"),a1e=a("strong"),y5r=o("mt5"),w5r=o(" \u2014 "),NO=a("a"),A5r=o("FlaxMT5ForConditionalGeneration"),L5r=o(" (mT5 model)"),B5r=l(),VF=a("li"),n1e=a("strong"),k5r=o("roberta"),x5r=o(" \u2014 "),DO=a("a"),R5r=o("FlaxRobertaForMaskedLM"),S5r=o(" (RoBERTa model)"),P5r=l(),WF=a("li"),s1e=a("strong"),$5r=o("roformer"),I5r=o(" \u2014 "),qO=a("a"),j5r=o("FlaxRoFormerForMaskedLM"),N5r=o(" (RoFormer model)"),D5r=l(),QF=a("li"),l1e=a("strong"),q5r=o("t5"),G5r=o(" \u2014 "),GO=a("a"),O5r=o("FlaxT5ForConditionalGeneration"),X5r=o(" (T5 model)"),z5r=l(),HF=a("li"),i1e=a("strong"),V5r=o("wav2vec2"),W5r=o(" \u2014 "),OO=a("a"),Q5r=o("FlaxWav2Vec2ForPreTraining"),H5r=o(" (Wav2Vec2 model)"),U5r=l(),d1e=a("p"),J5r=o("Examples:"),Y5r=l(),f(Rw.$$.fragment),N8e=l(),Uc=a("h2"),UF=a("a"),c1e=a("span"),f(Sw.$$.fragment),K5r=l(),f1e=a("span"),Z5r=o("FlaxAutoModelForMaskedLM"),D8e=l(),Br=a("div"),f(Pw.$$.fragment),e2r=l(),Jc=a("p"),o2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m1e=a("code"),r2r=o("from_pretrained()"),t2r=o("class method or the "),g1e=a("code"),a2r=o("from_config()"),n2r=o(`class
method.`),s2r=l(),$w=a("p"),l2r=o("This class cannot be instantiated directly using "),h1e=a("code"),i2r=o("__init__()"),d2r=o(" (throws an error)."),c2r=l(),Ct=a("div"),f(Iw.$$.fragment),f2r=l(),p1e=a("p"),m2r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),g2r=l(),Yc=a("p"),h2r=o(`Note:
Loading a model from its configuration file does `),_1e=a("strong"),p2r=o("not"),_2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u1e=a("code"),u2r=o("from_pretrained()"),b2r=o("to load the model weights."),v2r=l(),b1e=a("p"),T2r=o("Examples:"),F2r=l(),f(jw.$$.fragment),C2r=l(),Bo=a("div"),f(Nw.$$.fragment),M2r=l(),v1e=a("p"),E2r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),y2r=l(),yn=a("p"),w2r=o("The model class to instantiate is selected based on the "),T1e=a("code"),A2r=o("model_type"),L2r=o(` property of the config object (either
passed as an argument or loaded from `),F1e=a("code"),B2r=o("pretrained_model_name_or_path"),k2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C1e=a("code"),x2r=o("pretrained_model_name_or_path"),R2r=o(":"),S2r=l(),ve=a("ul"),JF=a("li"),M1e=a("strong"),P2r=o("albert"),$2r=o(" \u2014 "),XO=a("a"),I2r=o("FlaxAlbertForMaskedLM"),j2r=o(" (ALBERT model)"),N2r=l(),YF=a("li"),E1e=a("strong"),D2r=o("bart"),q2r=o(" \u2014 "),zO=a("a"),G2r=o("FlaxBartForConditionalGeneration"),O2r=o(" (BART model)"),X2r=l(),KF=a("li"),y1e=a("strong"),z2r=o("bert"),V2r=o(" \u2014 "),VO=a("a"),W2r=o("FlaxBertForMaskedLM"),Q2r=o(" (BERT model)"),H2r=l(),ZF=a("li"),w1e=a("strong"),U2r=o("big_bird"),J2r=o(" \u2014 "),WO=a("a"),Y2r=o("FlaxBigBirdForMaskedLM"),K2r=o(" (BigBird model)"),Z2r=l(),e9=a("li"),A1e=a("strong"),evr=o("distilbert"),ovr=o(" \u2014 "),QO=a("a"),rvr=o("FlaxDistilBertForMaskedLM"),tvr=o(" (DistilBERT model)"),avr=l(),o9=a("li"),L1e=a("strong"),nvr=o("electra"),svr=o(" \u2014 "),HO=a("a"),lvr=o("FlaxElectraForMaskedLM"),ivr=o(" (ELECTRA model)"),dvr=l(),r9=a("li"),B1e=a("strong"),cvr=o("mbart"),fvr=o(" \u2014 "),UO=a("a"),mvr=o("FlaxMBartForConditionalGeneration"),gvr=o(" (mBART model)"),hvr=l(),t9=a("li"),k1e=a("strong"),pvr=o("roberta"),_vr=o(" \u2014 "),JO=a("a"),uvr=o("FlaxRobertaForMaskedLM"),bvr=o(" (RoBERTa model)"),vvr=l(),a9=a("li"),x1e=a("strong"),Tvr=o("roformer"),Fvr=o(" \u2014 "),YO=a("a"),Cvr=o("FlaxRoFormerForMaskedLM"),Mvr=o(" (RoFormer model)"),Evr=l(),R1e=a("p"),yvr=o("Examples:"),wvr=l(),f(Dw.$$.fragment),q8e=l(),Kc=a("h2"),n9=a("a"),S1e=a("span"),f(qw.$$.fragment),Avr=l(),P1e=a("span"),Lvr=o("FlaxAutoModelForSeq2SeqLM"),G8e=l(),kr=a("div"),f(Gw.$$.fragment),Bvr=l(),Zc=a("p"),kvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$1e=a("code"),xvr=o("from_pretrained()"),Rvr=o("class method or the "),I1e=a("code"),Svr=o("from_config()"),Pvr=o(`class
method.`),$vr=l(),Ow=a("p"),Ivr=o("This class cannot be instantiated directly using "),j1e=a("code"),jvr=o("__init__()"),Nvr=o(" (throws an error)."),Dvr=l(),Mt=a("div"),f(Xw.$$.fragment),qvr=l(),N1e=a("p"),Gvr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Ovr=l(),ef=a("p"),Xvr=o(`Note:
Loading a model from its configuration file does `),D1e=a("strong"),zvr=o("not"),Vvr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q1e=a("code"),Wvr=o("from_pretrained()"),Qvr=o("to load the model weights."),Hvr=l(),G1e=a("p"),Uvr=o("Examples:"),Jvr=l(),f(zw.$$.fragment),Yvr=l(),ko=a("div"),f(Vw.$$.fragment),Kvr=l(),O1e=a("p"),Zvr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),e6r=l(),wn=a("p"),o6r=o("The model class to instantiate is selected based on the "),X1e=a("code"),r6r=o("model_type"),t6r=o(` property of the config object (either
passed as an argument or loaded from `),z1e=a("code"),a6r=o("pretrained_model_name_or_path"),n6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V1e=a("code"),s6r=o("pretrained_model_name_or_path"),l6r=o(":"),i6r=l(),Te=a("ul"),s9=a("li"),W1e=a("strong"),d6r=o("bart"),c6r=o(" \u2014 "),KO=a("a"),f6r=o("FlaxBartForConditionalGeneration"),m6r=o(" (BART model)"),g6r=l(),l9=a("li"),Q1e=a("strong"),h6r=o("blenderbot"),p6r=o(" \u2014 "),ZO=a("a"),_6r=o("FlaxBlenderbotForConditionalGeneration"),u6r=o(" (Blenderbot model)"),b6r=l(),i9=a("li"),H1e=a("strong"),v6r=o("blenderbot-small"),T6r=o(" \u2014 "),eX=a("a"),F6r=o("FlaxBlenderbotSmallForConditionalGeneration"),C6r=o(" (BlenderbotSmall model)"),M6r=l(),d9=a("li"),U1e=a("strong"),E6r=o("encoder-decoder"),y6r=o(" \u2014 "),oX=a("a"),w6r=o("FlaxEncoderDecoderModel"),A6r=o(" (Encoder decoder model)"),L6r=l(),c9=a("li"),J1e=a("strong"),B6r=o("marian"),k6r=o(" \u2014 "),rX=a("a"),x6r=o("FlaxMarianMTModel"),R6r=o(" (Marian model)"),S6r=l(),f9=a("li"),Y1e=a("strong"),P6r=o("mbart"),$6r=o(" \u2014 "),tX=a("a"),I6r=o("FlaxMBartForConditionalGeneration"),j6r=o(" (mBART model)"),N6r=l(),m9=a("li"),K1e=a("strong"),D6r=o("mt5"),q6r=o(" \u2014 "),aX=a("a"),G6r=o("FlaxMT5ForConditionalGeneration"),O6r=o(" (mT5 model)"),X6r=l(),g9=a("li"),Z1e=a("strong"),z6r=o("pegasus"),V6r=o(" \u2014 "),nX=a("a"),W6r=o("FlaxPegasusForConditionalGeneration"),Q6r=o(" (Pegasus model)"),H6r=l(),h9=a("li"),ebe=a("strong"),U6r=o("t5"),J6r=o(" \u2014 "),sX=a("a"),Y6r=o("FlaxT5ForConditionalGeneration"),K6r=o(" (T5 model)"),Z6r=l(),obe=a("p"),eTr=o("Examples:"),oTr=l(),f(Ww.$$.fragment),O8e=l(),of=a("h2"),p9=a("a"),rbe=a("span"),f(Qw.$$.fragment),rTr=l(),tbe=a("span"),tTr=o("FlaxAutoModelForSequenceClassification"),X8e=l(),xr=a("div"),f(Hw.$$.fragment),aTr=l(),rf=a("p"),nTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),abe=a("code"),sTr=o("from_pretrained()"),lTr=o("class method or the "),nbe=a("code"),iTr=o("from_config()"),dTr=o(`class
method.`),cTr=l(),Uw=a("p"),fTr=o("This class cannot be instantiated directly using "),sbe=a("code"),mTr=o("__init__()"),gTr=o(" (throws an error)."),hTr=l(),Et=a("div"),f(Jw.$$.fragment),pTr=l(),lbe=a("p"),_Tr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),uTr=l(),tf=a("p"),bTr=o(`Note:
Loading a model from its configuration file does `),ibe=a("strong"),vTr=o("not"),TTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dbe=a("code"),FTr=o("from_pretrained()"),CTr=o("to load the model weights."),MTr=l(),cbe=a("p"),ETr=o("Examples:"),yTr=l(),f(Yw.$$.fragment),wTr=l(),xo=a("div"),f(Kw.$$.fragment),ATr=l(),fbe=a("p"),LTr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),BTr=l(),An=a("p"),kTr=o("The model class to instantiate is selected based on the "),mbe=a("code"),xTr=o("model_type"),RTr=o(` property of the config object (either
passed as an argument or loaded from `),gbe=a("code"),STr=o("pretrained_model_name_or_path"),PTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hbe=a("code"),$Tr=o("pretrained_model_name_or_path"),ITr=o(":"),jTr=l(),Fe=a("ul"),_9=a("li"),pbe=a("strong"),NTr=o("albert"),DTr=o(" \u2014 "),lX=a("a"),qTr=o("FlaxAlbertForSequenceClassification"),GTr=o(" (ALBERT model)"),OTr=l(),u9=a("li"),_be=a("strong"),XTr=o("bart"),zTr=o(" \u2014 "),iX=a("a"),VTr=o("FlaxBartForSequenceClassification"),WTr=o(" (BART model)"),QTr=l(),b9=a("li"),ube=a("strong"),HTr=o("bert"),UTr=o(" \u2014 "),dX=a("a"),JTr=o("FlaxBertForSequenceClassification"),YTr=o(" (BERT model)"),KTr=l(),v9=a("li"),bbe=a("strong"),ZTr=o("big_bird"),e7r=o(" \u2014 "),cX=a("a"),o7r=o("FlaxBigBirdForSequenceClassification"),r7r=o(" (BigBird model)"),t7r=l(),T9=a("li"),vbe=a("strong"),a7r=o("distilbert"),n7r=o(" \u2014 "),fX=a("a"),s7r=o("FlaxDistilBertForSequenceClassification"),l7r=o(" (DistilBERT model)"),i7r=l(),F9=a("li"),Tbe=a("strong"),d7r=o("electra"),c7r=o(" \u2014 "),mX=a("a"),f7r=o("FlaxElectraForSequenceClassification"),m7r=o(" (ELECTRA model)"),g7r=l(),C9=a("li"),Fbe=a("strong"),h7r=o("mbart"),p7r=o(" \u2014 "),gX=a("a"),_7r=o("FlaxMBartForSequenceClassification"),u7r=o(" (mBART model)"),b7r=l(),M9=a("li"),Cbe=a("strong"),v7r=o("roberta"),T7r=o(" \u2014 "),hX=a("a"),F7r=o("FlaxRobertaForSequenceClassification"),C7r=o(" (RoBERTa model)"),M7r=l(),E9=a("li"),Mbe=a("strong"),E7r=o("roformer"),y7r=o(" \u2014 "),pX=a("a"),w7r=o("FlaxRoFormerForSequenceClassification"),A7r=o(" (RoFormer model)"),L7r=l(),Ebe=a("p"),B7r=o("Examples:"),k7r=l(),f(Zw.$$.fragment),z8e=l(),af=a("h2"),y9=a("a"),ybe=a("span"),f(eA.$$.fragment),x7r=l(),wbe=a("span"),R7r=o("FlaxAutoModelForQuestionAnswering"),V8e=l(),Rr=a("div"),f(oA.$$.fragment),S7r=l(),nf=a("p"),P7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Abe=a("code"),$7r=o("from_pretrained()"),I7r=o("class method or the "),Lbe=a("code"),j7r=o("from_config()"),N7r=o(`class
method.`),D7r=l(),rA=a("p"),q7r=o("This class cannot be instantiated directly using "),Bbe=a("code"),G7r=o("__init__()"),O7r=o(" (throws an error)."),X7r=l(),yt=a("div"),f(tA.$$.fragment),z7r=l(),kbe=a("p"),V7r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),W7r=l(),sf=a("p"),Q7r=o(`Note:
Loading a model from its configuration file does `),xbe=a("strong"),H7r=o("not"),U7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Rbe=a("code"),J7r=o("from_pretrained()"),Y7r=o("to load the model weights."),K7r=l(),Sbe=a("p"),Z7r=o("Examples:"),eFr=l(),f(aA.$$.fragment),oFr=l(),Ro=a("div"),f(nA.$$.fragment),rFr=l(),Pbe=a("p"),tFr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),aFr=l(),Ln=a("p"),nFr=o("The model class to instantiate is selected based on the "),$be=a("code"),sFr=o("model_type"),lFr=o(` property of the config object (either
passed as an argument or loaded from `),Ibe=a("code"),iFr=o("pretrained_model_name_or_path"),dFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jbe=a("code"),cFr=o("pretrained_model_name_or_path"),fFr=o(":"),mFr=l(),Ce=a("ul"),w9=a("li"),Nbe=a("strong"),gFr=o("albert"),hFr=o(" \u2014 "),_X=a("a"),pFr=o("FlaxAlbertForQuestionAnswering"),_Fr=o(" (ALBERT model)"),uFr=l(),A9=a("li"),Dbe=a("strong"),bFr=o("bart"),vFr=o(" \u2014 "),uX=a("a"),TFr=o("FlaxBartForQuestionAnswering"),FFr=o(" (BART model)"),CFr=l(),L9=a("li"),qbe=a("strong"),MFr=o("bert"),EFr=o(" \u2014 "),bX=a("a"),yFr=o("FlaxBertForQuestionAnswering"),wFr=o(" (BERT model)"),AFr=l(),B9=a("li"),Gbe=a("strong"),LFr=o("big_bird"),BFr=o(" \u2014 "),vX=a("a"),kFr=o("FlaxBigBirdForQuestionAnswering"),xFr=o(" (BigBird model)"),RFr=l(),k9=a("li"),Obe=a("strong"),SFr=o("distilbert"),PFr=o(" \u2014 "),TX=a("a"),$Fr=o("FlaxDistilBertForQuestionAnswering"),IFr=o(" (DistilBERT model)"),jFr=l(),x9=a("li"),Xbe=a("strong"),NFr=o("electra"),DFr=o(" \u2014 "),FX=a("a"),qFr=o("FlaxElectraForQuestionAnswering"),GFr=o(" (ELECTRA model)"),OFr=l(),R9=a("li"),zbe=a("strong"),XFr=o("mbart"),zFr=o(" \u2014 "),CX=a("a"),VFr=o("FlaxMBartForQuestionAnswering"),WFr=o(" (mBART model)"),QFr=l(),S9=a("li"),Vbe=a("strong"),HFr=o("roberta"),UFr=o(" \u2014 "),MX=a("a"),JFr=o("FlaxRobertaForQuestionAnswering"),YFr=o(" (RoBERTa model)"),KFr=l(),P9=a("li"),Wbe=a("strong"),ZFr=o("roformer"),e9r=o(" \u2014 "),EX=a("a"),o9r=o("FlaxRoFormerForQuestionAnswering"),r9r=o(" (RoFormer model)"),t9r=l(),Qbe=a("p"),a9r=o("Examples:"),n9r=l(),f(sA.$$.fragment),W8e=l(),lf=a("h2"),$9=a("a"),Hbe=a("span"),f(lA.$$.fragment),s9r=l(),Ube=a("span"),l9r=o("FlaxAutoModelForTokenClassification"),Q8e=l(),Sr=a("div"),f(iA.$$.fragment),i9r=l(),df=a("p"),d9r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Jbe=a("code"),c9r=o("from_pretrained()"),f9r=o("class method or the "),Ybe=a("code"),m9r=o("from_config()"),g9r=o(`class
method.`),h9r=l(),dA=a("p"),p9r=o("This class cannot be instantiated directly using "),Kbe=a("code"),_9r=o("__init__()"),u9r=o(" (throws an error)."),b9r=l(),wt=a("div"),f(cA.$$.fragment),v9r=l(),Zbe=a("p"),T9r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),F9r=l(),cf=a("p"),C9r=o(`Note:
Loading a model from its configuration file does `),e5e=a("strong"),M9r=o("not"),E9r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=a("code"),y9r=o("from_pretrained()"),w9r=o("to load the model weights."),A9r=l(),r5e=a("p"),L9r=o("Examples:"),B9r=l(),f(fA.$$.fragment),k9r=l(),So=a("div"),f(mA.$$.fragment),x9r=l(),t5e=a("p"),R9r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),S9r=l(),Bn=a("p"),P9r=o("The model class to instantiate is selected based on the "),a5e=a("code"),$9r=o("model_type"),I9r=o(` property of the config object (either
passed as an argument or loaded from `),n5e=a("code"),j9r=o("pretrained_model_name_or_path"),N9r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=a("code"),D9r=o("pretrained_model_name_or_path"),q9r=o(":"),G9r=l(),so=a("ul"),I9=a("li"),l5e=a("strong"),O9r=o("albert"),X9r=o(" \u2014 "),yX=a("a"),z9r=o("FlaxAlbertForTokenClassification"),V9r=o(" (ALBERT model)"),W9r=l(),j9=a("li"),i5e=a("strong"),Q9r=o("bert"),H9r=o(" \u2014 "),wX=a("a"),U9r=o("FlaxBertForTokenClassification"),J9r=o(" (BERT model)"),Y9r=l(),N9=a("li"),d5e=a("strong"),K9r=o("big_bird"),Z9r=o(" \u2014 "),AX=a("a"),eCr=o("FlaxBigBirdForTokenClassification"),oCr=o(" (BigBird model)"),rCr=l(),D9=a("li"),c5e=a("strong"),tCr=o("distilbert"),aCr=o(" \u2014 "),LX=a("a"),nCr=o("FlaxDistilBertForTokenClassification"),sCr=o(" (DistilBERT model)"),lCr=l(),q9=a("li"),f5e=a("strong"),iCr=o("electra"),dCr=o(" \u2014 "),BX=a("a"),cCr=o("FlaxElectraForTokenClassification"),fCr=o(" (ELECTRA model)"),mCr=l(),G9=a("li"),m5e=a("strong"),gCr=o("roberta"),hCr=o(" \u2014 "),kX=a("a"),pCr=o("FlaxRobertaForTokenClassification"),_Cr=o(" (RoBERTa model)"),uCr=l(),O9=a("li"),g5e=a("strong"),bCr=o("roformer"),vCr=o(" \u2014 "),xX=a("a"),TCr=o("FlaxRoFormerForTokenClassification"),FCr=o(" (RoFormer model)"),CCr=l(),h5e=a("p"),MCr=o("Examples:"),ECr=l(),f(gA.$$.fragment),H8e=l(),ff=a("h2"),X9=a("a"),p5e=a("span"),f(hA.$$.fragment),yCr=l(),_5e=a("span"),wCr=o("FlaxAutoModelForMultipleChoice"),U8e=l(),Pr=a("div"),f(pA.$$.fragment),ACr=l(),mf=a("p"),LCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),u5e=a("code"),BCr=o("from_pretrained()"),kCr=o("class method or the "),b5e=a("code"),xCr=o("from_config()"),RCr=o(`class
method.`),SCr=l(),_A=a("p"),PCr=o("This class cannot be instantiated directly using "),v5e=a("code"),$Cr=o("__init__()"),ICr=o(" (throws an error)."),jCr=l(),At=a("div"),f(uA.$$.fragment),NCr=l(),T5e=a("p"),DCr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),qCr=l(),gf=a("p"),GCr=o(`Note:
Loading a model from its configuration file does `),F5e=a("strong"),OCr=o("not"),XCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),C5e=a("code"),zCr=o("from_pretrained()"),VCr=o("to load the model weights."),WCr=l(),M5e=a("p"),QCr=o("Examples:"),HCr=l(),f(bA.$$.fragment),UCr=l(),Po=a("div"),f(vA.$$.fragment),JCr=l(),E5e=a("p"),YCr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),KCr=l(),kn=a("p"),ZCr=o("The model class to instantiate is selected based on the "),y5e=a("code"),e4r=o("model_type"),o4r=o(` property of the config object (either
passed as an argument or loaded from `),w5e=a("code"),r4r=o("pretrained_model_name_or_path"),t4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A5e=a("code"),a4r=o("pretrained_model_name_or_path"),n4r=o(":"),s4r=l(),lo=a("ul"),z9=a("li"),L5e=a("strong"),l4r=o("albert"),i4r=o(" \u2014 "),RX=a("a"),d4r=o("FlaxAlbertForMultipleChoice"),c4r=o(" (ALBERT model)"),f4r=l(),V9=a("li"),B5e=a("strong"),m4r=o("bert"),g4r=o(" \u2014 "),SX=a("a"),h4r=o("FlaxBertForMultipleChoice"),p4r=o(" (BERT model)"),_4r=l(),W9=a("li"),k5e=a("strong"),u4r=o("big_bird"),b4r=o(" \u2014 "),PX=a("a"),v4r=o("FlaxBigBirdForMultipleChoice"),T4r=o(" (BigBird model)"),F4r=l(),Q9=a("li"),x5e=a("strong"),C4r=o("distilbert"),M4r=o(" \u2014 "),$X=a("a"),E4r=o("FlaxDistilBertForMultipleChoice"),y4r=o(" (DistilBERT model)"),w4r=l(),H9=a("li"),R5e=a("strong"),A4r=o("electra"),L4r=o(" \u2014 "),IX=a("a"),B4r=o("FlaxElectraForMultipleChoice"),k4r=o(" (ELECTRA model)"),x4r=l(),U9=a("li"),S5e=a("strong"),R4r=o("roberta"),S4r=o(" \u2014 "),jX=a("a"),P4r=o("FlaxRobertaForMultipleChoice"),$4r=o(" (RoBERTa model)"),I4r=l(),J9=a("li"),P5e=a("strong"),j4r=o("roformer"),N4r=o(" \u2014 "),NX=a("a"),D4r=o("FlaxRoFormerForMultipleChoice"),q4r=o(" (RoFormer model)"),G4r=l(),$5e=a("p"),O4r=o("Examples:"),X4r=l(),f(TA.$$.fragment),J8e=l(),hf=a("h2"),Y9=a("a"),I5e=a("span"),f(FA.$$.fragment),z4r=l(),j5e=a("span"),V4r=o("FlaxAutoModelForNextSentencePrediction"),Y8e=l(),$r=a("div"),f(CA.$$.fragment),W4r=l(),pf=a("p"),Q4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),N5e=a("code"),H4r=o("from_pretrained()"),U4r=o("class method or the "),D5e=a("code"),J4r=o("from_config()"),Y4r=o(`class
method.`),K4r=l(),MA=a("p"),Z4r=o("This class cannot be instantiated directly using "),q5e=a("code"),eMr=o("__init__()"),oMr=o(" (throws an error)."),rMr=l(),Lt=a("div"),f(EA.$$.fragment),tMr=l(),G5e=a("p"),aMr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),nMr=l(),_f=a("p"),sMr=o(`Note:
Loading a model from its configuration file does `),O5e=a("strong"),lMr=o("not"),iMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),X5e=a("code"),dMr=o("from_pretrained()"),cMr=o("to load the model weights."),fMr=l(),z5e=a("p"),mMr=o("Examples:"),gMr=l(),f(yA.$$.fragment),hMr=l(),$o=a("div"),f(wA.$$.fragment),pMr=l(),V5e=a("p"),_Mr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),uMr=l(),xn=a("p"),bMr=o("The model class to instantiate is selected based on the "),W5e=a("code"),vMr=o("model_type"),TMr=o(` property of the config object (either
passed as an argument or loaded from `),Q5e=a("code"),FMr=o("pretrained_model_name_or_path"),CMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H5e=a("code"),MMr=o("pretrained_model_name_or_path"),EMr=o(":"),yMr=l(),U5e=a("ul"),K9=a("li"),J5e=a("strong"),wMr=o("bert"),AMr=o(" \u2014 "),DX=a("a"),LMr=o("FlaxBertForNextSentencePrediction"),BMr=o(" (BERT model)"),kMr=l(),Y5e=a("p"),xMr=o("Examples:"),RMr=l(),f(AA.$$.fragment),K8e=l(),uf=a("h2"),Z9=a("a"),K5e=a("span"),f(LA.$$.fragment),SMr=l(),Z5e=a("span"),PMr=o("FlaxAutoModelForImageClassification"),Z8e=l(),Ir=a("div"),f(BA.$$.fragment),$Mr=l(),bf=a("p"),IMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),e2e=a("code"),jMr=o("from_pretrained()"),NMr=o("class method or the "),o2e=a("code"),DMr=o("from_config()"),qMr=o(`class
method.`),GMr=l(),kA=a("p"),OMr=o("This class cannot be instantiated directly using "),r2e=a("code"),XMr=o("__init__()"),zMr=o(" (throws an error)."),VMr=l(),Bt=a("div"),f(xA.$$.fragment),WMr=l(),t2e=a("p"),QMr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),HMr=l(),vf=a("p"),UMr=o(`Note:
Loading a model from its configuration file does `),a2e=a("strong"),JMr=o("not"),YMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),n2e=a("code"),KMr=o("from_pretrained()"),ZMr=o("to load the model weights."),eEr=l(),s2e=a("p"),oEr=o("Examples:"),rEr=l(),f(RA.$$.fragment),tEr=l(),Io=a("div"),f(SA.$$.fragment),aEr=l(),l2e=a("p"),nEr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),sEr=l(),Rn=a("p"),lEr=o("The model class to instantiate is selected based on the "),i2e=a("code"),iEr=o("model_type"),dEr=o(` property of the config object (either
passed as an argument or loaded from `),d2e=a("code"),cEr=o("pretrained_model_name_or_path"),fEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c2e=a("code"),mEr=o("pretrained_model_name_or_path"),gEr=o(":"),hEr=l(),PA=a("ul"),eC=a("li"),f2e=a("strong"),pEr=o("beit"),_Er=o(" \u2014 "),qX=a("a"),uEr=o("FlaxBeitForImageClassification"),bEr=o(" (BEiT model)"),vEr=l(),oC=a("li"),m2e=a("strong"),TEr=o("vit"),FEr=o(" \u2014 "),GX=a("a"),CEr=o("FlaxViTForImageClassification"),MEr=o(" (ViT model)"),EEr=l(),g2e=a("p"),yEr=o("Examples:"),wEr=l(),f($A.$$.fragment),eBe=l(),Tf=a("h2"),rC=a("a"),h2e=a("span"),f(IA.$$.fragment),AEr=l(),p2e=a("span"),LEr=o("FlaxAutoModelForVision2Seq"),oBe=l(),jr=a("div"),f(jA.$$.fragment),BEr=l(),Ff=a("p"),kEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_2e=a("code"),xEr=o("from_pretrained()"),REr=o("class method or the "),u2e=a("code"),SEr=o("from_config()"),PEr=o(`class
method.`),$Er=l(),NA=a("p"),IEr=o("This class cannot be instantiated directly using "),b2e=a("code"),jEr=o("__init__()"),NEr=o(" (throws an error)."),DEr=l(),kt=a("div"),f(DA.$$.fragment),qEr=l(),v2e=a("p"),GEr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),OEr=l(),Cf=a("p"),XEr=o(`Note:
Loading a model from its configuration file does `),T2e=a("strong"),zEr=o("not"),VEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F2e=a("code"),WEr=o("from_pretrained()"),QEr=o("to load the model weights."),HEr=l(),C2e=a("p"),UEr=o("Examples:"),JEr=l(),f(qA.$$.fragment),YEr=l(),jo=a("div"),f(GA.$$.fragment),KEr=l(),M2e=a("p"),ZEr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),e3r=l(),Sn=a("p"),o3r=o("The model class to instantiate is selected based on the "),E2e=a("code"),r3r=o("model_type"),t3r=o(` property of the config object (either
passed as an argument or loaded from `),y2e=a("code"),a3r=o("pretrained_model_name_or_path"),n3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w2e=a("code"),s3r=o("pretrained_model_name_or_path"),l3r=o(":"),i3r=l(),A2e=a("ul"),tC=a("li"),L2e=a("strong"),d3r=o("vision-encoder-decoder"),c3r=o(" \u2014 "),OX=a("a"),f3r=o("FlaxVisionEncoderDecoderModel"),m3r=o(" (Vision Encoder decoder model)"),g3r=l(),B2e=a("p"),h3r=o("Examples:"),p3r=l(),f(OA.$$.fragment),this.h()},l(d){const u=T_t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var XA=s(ie);me=n(XA,"A",{id:!0,class:!0,href:!0});var k2e=s(me);to=n(k2e,"SPAN",{});var x2e=s(to);m(ce.$$.fragment,x2e),x2e.forEach(t),k2e.forEach(t),ue=i(XA),Do=n(XA,"SPAN",{});var u3r=s(Do);wi=r(u3r,"Auto Classes"),u3r.forEach(t),XA.forEach(t),Ef=i(d),sa=n(d,"P",{});var tBe=s(sa);Ai=r(tBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(tBe,"CODE",{});var b3r=s(Li);o4=r(b3r,"from_pretrained()"),b3r.forEach(t),yf=r(tBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),tBe.forEach(t),ye=i(d),io=n(d,"P",{});var aC=s(io);Bi=r(aC,"Instantiating one of "),Pn=n(aC,"A",{href:!0});var v3r=s(Pn);r4=r(v3r,"AutoConfig"),v3r.forEach(t),$n=r(aC,", "),In=n(aC,"A",{href:!0});var T3r=s(In);t4=r(T3r,"AutoModel"),T3r.forEach(t),ki=r(aC,`, and
`),jn=n(aC,"A",{href:!0});var F3r=s(jn);a4=r(F3r,"AutoTokenizer"),F3r.forEach(t),xi=r(aC," will directly create a class of the relevant architecture. For instance"),aC.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var aBe=s(ge);D0=r(aBe,"will create a model that is an instance of "),Ri=n(aBe,"A",{href:!0});var C3r=s(Ri);q0=r(C3r,"BertModel"),C3r.forEach(t),G0=r(aBe,"."),aBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var nBe=s(Ia);O0=r(nBe,"There is one class of "),Af=n(nBe,"CODE",{});var M3r=s(Af);X0=r(M3r,"AutoModel"),M3r.forEach(t),mxe=r(nBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),nBe.forEach(t),tLe=i(d),Si=n(d,"H2",{class:!0});var sBe=s(Si);Lf=n(sBe,"A",{id:!0,class:!0,href:!0});var E3r=s(Lf);$V=n(E3r,"SPAN",{});var y3r=s($V);m(n4.$$.fragment,y3r),y3r.forEach(t),E3r.forEach(t),gxe=i(sBe),IV=n(sBe,"SPAN",{});var w3r=s(IV);hxe=r(w3r,"Extending the Auto Classes"),w3r.forEach(t),sBe.forEach(t),aLe=i(d),Nn=n(d,"P",{});var XX=s(Nn);pxe=r(XX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=n(XX,"CODE",{});var A3r=s(jV);_xe=r(A3r,"NewModel"),A3r.forEach(t),uxe=r(XX,", make sure you have a "),NV=n(XX,"CODE",{});var L3r=s(NV);bxe=r(L3r,"NewModelConfig"),L3r.forEach(t),vxe=r(XX,` then you can add those to the auto
classes like this:`),XX.forEach(t),nLe=i(d),m(s4.$$.fragment,d),sLe=i(d),z0=n(d,"P",{});var B3r=s(z0);Txe=r(B3r,"You will then be able to use the auto classes like you would usually do!"),B3r.forEach(t),lLe=i(d),m(Bf.$$.fragment,d),iLe=i(d),Pi=n(d,"H2",{class:!0});var lBe=s(Pi);kf=n(lBe,"A",{id:!0,class:!0,href:!0});var k3r=s(kf);DV=n(k3r,"SPAN",{});var x3r=s(DV);m(l4.$$.fragment,x3r),x3r.forEach(t),k3r.forEach(t),Fxe=i(lBe),qV=n(lBe,"SPAN",{});var R3r=s(qV);Cxe=r(R3r,"AutoConfig"),R3r.forEach(t),lBe.forEach(t),dLe=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(i4.$$.fragment,Ps),Mxe=i(Ps),d4=n(Ps,"P",{});var iBe=s(d4);Exe=r(iBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V0=n(iBe,"A",{href:!0});var S3r=s(V0);yxe=r(S3r,"from_pretrained()"),S3r.forEach(t),wxe=r(iBe," class method."),iBe.forEach(t),Axe=i(Ps),c4=n(Ps,"P",{});var dBe=s(c4);Lxe=r(dBe,"This class cannot be instantiated directly using "),GV=n(dBe,"CODE",{});var P3r=s(GV);Bxe=r(P3r,"__init__()"),P3r.forEach(t),kxe=r(dBe," (throws an error)."),dBe.forEach(t),xxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(f4.$$.fragment,ia),Rxe=i(ia),OV=n(ia,"P",{});var $3r=s(OV);Sxe=r($3r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),$3r.forEach(t),Pxe=i(ia),$i=n(ia,"P",{});var zX=s($i);$xe=r(zX,"The configuration class to instantiate is selected based on the "),XV=n(zX,"CODE",{});var I3r=s(XV);Ixe=r(I3r,"model_type"),I3r.forEach(t),jxe=r(zX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=n(zX,"CODE",{});var j3r=s(zV);Nxe=r(j3r,"pretrained_model_name_or_path"),j3r.forEach(t),Dxe=r(zX,":"),zX.forEach(t),qxe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var R2e=s(xf);VV=n(R2e,"STRONG",{});var N3r=s(VV);Gxe=r(N3r,"albert"),N3r.forEach(t),Oxe=r(R2e," \u2014 "),W0=n(R2e,"A",{href:!0});var D3r=s(W0);Xxe=r(D3r,"AlbertConfig"),D3r.forEach(t),zxe=r(R2e," (ALBERT model)"),R2e.forEach(t),Vxe=i(T),Rf=n(T,"LI",{});var S2e=s(Rf);WV=n(S2e,"STRONG",{});var q3r=s(WV);Wxe=r(q3r,"bart"),q3r.forEach(t),Qxe=r(S2e," \u2014 "),Q0=n(S2e,"A",{href:!0});var G3r=s(Q0);Hxe=r(G3r,"BartConfig"),G3r.forEach(t),Uxe=r(S2e," (BART model)"),S2e.forEach(t),Jxe=i(T),Sf=n(T,"LI",{});var P2e=s(Sf);QV=n(P2e,"STRONG",{});var O3r=s(QV);Yxe=r(O3r,"beit"),O3r.forEach(t),Kxe=r(P2e," \u2014 "),H0=n(P2e,"A",{href:!0});var X3r=s(H0);Zxe=r(X3r,"BeitConfig"),X3r.forEach(t),eRe=r(P2e," (BEiT model)"),P2e.forEach(t),oRe=i(T),Pf=n(T,"LI",{});var $2e=s(Pf);HV=n($2e,"STRONG",{});var z3r=s(HV);rRe=r(z3r,"bert"),z3r.forEach(t),tRe=r($2e," \u2014 "),U0=n($2e,"A",{href:!0});var V3r=s(U0);aRe=r(V3r,"BertConfig"),V3r.forEach(t),nRe=r($2e," (BERT model)"),$2e.forEach(t),sRe=i(T),$f=n(T,"LI",{});var I2e=s($f);UV=n(I2e,"STRONG",{});var W3r=s(UV);lRe=r(W3r,"bert-generation"),W3r.forEach(t),iRe=r(I2e," \u2014 "),J0=n(I2e,"A",{href:!0});var Q3r=s(J0);dRe=r(Q3r,"BertGenerationConfig"),Q3r.forEach(t),cRe=r(I2e," (Bert Generation model)"),I2e.forEach(t),fRe=i(T),If=n(T,"LI",{});var j2e=s(If);JV=n(j2e,"STRONG",{});var H3r=s(JV);mRe=r(H3r,"big_bird"),H3r.forEach(t),gRe=r(j2e," \u2014 "),Y0=n(j2e,"A",{href:!0});var U3r=s(Y0);hRe=r(U3r,"BigBirdConfig"),U3r.forEach(t),pRe=r(j2e," (BigBird model)"),j2e.forEach(t),_Re=i(T),jf=n(T,"LI",{});var N2e=s(jf);YV=n(N2e,"STRONG",{});var J3r=s(YV);uRe=r(J3r,"bigbird_pegasus"),J3r.forEach(t),bRe=r(N2e," \u2014 "),K0=n(N2e,"A",{href:!0});var Y3r=s(K0);vRe=r(Y3r,"BigBirdPegasusConfig"),Y3r.forEach(t),TRe=r(N2e," (BigBirdPegasus model)"),N2e.forEach(t),FRe=i(T),Nf=n(T,"LI",{});var D2e=s(Nf);KV=n(D2e,"STRONG",{});var K3r=s(KV);CRe=r(K3r,"blenderbot"),K3r.forEach(t),MRe=r(D2e," \u2014 "),Z0=n(D2e,"A",{href:!0});var Z3r=s(Z0);ERe=r(Z3r,"BlenderbotConfig"),Z3r.forEach(t),yRe=r(D2e," (Blenderbot model)"),D2e.forEach(t),wRe=i(T),Df=n(T,"LI",{});var q2e=s(Df);ZV=n(q2e,"STRONG",{});var eyr=s(ZV);ARe=r(eyr,"blenderbot-small"),eyr.forEach(t),LRe=r(q2e," \u2014 "),eL=n(q2e,"A",{href:!0});var oyr=s(eL);BRe=r(oyr,"BlenderbotSmallConfig"),oyr.forEach(t),kRe=r(q2e," (BlenderbotSmall model)"),q2e.forEach(t),xRe=i(T),qf=n(T,"LI",{});var G2e=s(qf);eW=n(G2e,"STRONG",{});var ryr=s(eW);RRe=r(ryr,"camembert"),ryr.forEach(t),SRe=r(G2e," \u2014 "),oL=n(G2e,"A",{href:!0});var tyr=s(oL);PRe=r(tyr,"CamembertConfig"),tyr.forEach(t),$Re=r(G2e," (CamemBERT model)"),G2e.forEach(t),IRe=i(T),Gf=n(T,"LI",{});var O2e=s(Gf);oW=n(O2e,"STRONG",{});var ayr=s(oW);jRe=r(ayr,"canine"),ayr.forEach(t),NRe=r(O2e," \u2014 "),rL=n(O2e,"A",{href:!0});var nyr=s(rL);DRe=r(nyr,"CanineConfig"),nyr.forEach(t),qRe=r(O2e," (Canine model)"),O2e.forEach(t),GRe=i(T),Of=n(T,"LI",{});var X2e=s(Of);rW=n(X2e,"STRONG",{});var syr=s(rW);ORe=r(syr,"clip"),syr.forEach(t),XRe=r(X2e," \u2014 "),tL=n(X2e,"A",{href:!0});var lyr=s(tL);zRe=r(lyr,"CLIPConfig"),lyr.forEach(t),VRe=r(X2e," (CLIP model)"),X2e.forEach(t),WRe=i(T),Xf=n(T,"LI",{});var z2e=s(Xf);tW=n(z2e,"STRONG",{});var iyr=s(tW);QRe=r(iyr,"convbert"),iyr.forEach(t),HRe=r(z2e," \u2014 "),aL=n(z2e,"A",{href:!0});var dyr=s(aL);URe=r(dyr,"ConvBertConfig"),dyr.forEach(t),JRe=r(z2e," (ConvBERT model)"),z2e.forEach(t),YRe=i(T),zf=n(T,"LI",{});var V2e=s(zf);aW=n(V2e,"STRONG",{});var cyr=s(aW);KRe=r(cyr,"convnext"),cyr.forEach(t),ZRe=r(V2e," \u2014 "),nL=n(V2e,"A",{href:!0});var fyr=s(nL);eSe=r(fyr,"ConvNextConfig"),fyr.forEach(t),oSe=r(V2e," (ConvNext model)"),V2e.forEach(t),rSe=i(T),Vf=n(T,"LI",{});var W2e=s(Vf);nW=n(W2e,"STRONG",{});var myr=s(nW);tSe=r(myr,"ctrl"),myr.forEach(t),aSe=r(W2e," \u2014 "),sL=n(W2e,"A",{href:!0});var gyr=s(sL);nSe=r(gyr,"CTRLConfig"),gyr.forEach(t),sSe=r(W2e," (CTRL model)"),W2e.forEach(t),lSe=i(T),Wf=n(T,"LI",{});var Q2e=s(Wf);sW=n(Q2e,"STRONG",{});var hyr=s(sW);iSe=r(hyr,"deberta"),hyr.forEach(t),dSe=r(Q2e," \u2014 "),lL=n(Q2e,"A",{href:!0});var pyr=s(lL);cSe=r(pyr,"DebertaConfig"),pyr.forEach(t),fSe=r(Q2e," (DeBERTa model)"),Q2e.forEach(t),mSe=i(T),Qf=n(T,"LI",{});var H2e=s(Qf);lW=n(H2e,"STRONG",{});var _yr=s(lW);gSe=r(_yr,"deberta-v2"),_yr.forEach(t),hSe=r(H2e," \u2014 "),iL=n(H2e,"A",{href:!0});var uyr=s(iL);pSe=r(uyr,"DebertaV2Config"),uyr.forEach(t),_Se=r(H2e," (DeBERTa-v2 model)"),H2e.forEach(t),uSe=i(T),Hf=n(T,"LI",{});var U2e=s(Hf);iW=n(U2e,"STRONG",{});var byr=s(iW);bSe=r(byr,"deit"),byr.forEach(t),vSe=r(U2e," \u2014 "),dL=n(U2e,"A",{href:!0});var vyr=s(dL);TSe=r(vyr,"DeiTConfig"),vyr.forEach(t),FSe=r(U2e," (DeiT model)"),U2e.forEach(t),CSe=i(T),Uf=n(T,"LI",{});var J2e=s(Uf);dW=n(J2e,"STRONG",{});var Tyr=s(dW);MSe=r(Tyr,"detr"),Tyr.forEach(t),ESe=r(J2e," \u2014 "),cL=n(J2e,"A",{href:!0});var Fyr=s(cL);ySe=r(Fyr,"DetrConfig"),Fyr.forEach(t),wSe=r(J2e," (DETR model)"),J2e.forEach(t),ASe=i(T),Jf=n(T,"LI",{});var Y2e=s(Jf);cW=n(Y2e,"STRONG",{});var Cyr=s(cW);LSe=r(Cyr,"distilbert"),Cyr.forEach(t),BSe=r(Y2e," \u2014 "),fL=n(Y2e,"A",{href:!0});var Myr=s(fL);kSe=r(Myr,"DistilBertConfig"),Myr.forEach(t),xSe=r(Y2e," (DistilBERT model)"),Y2e.forEach(t),RSe=i(T),Yf=n(T,"LI",{});var K2e=s(Yf);fW=n(K2e,"STRONG",{});var Eyr=s(fW);SSe=r(Eyr,"dpr"),Eyr.forEach(t),PSe=r(K2e," \u2014 "),mL=n(K2e,"A",{href:!0});var yyr=s(mL);$Se=r(yyr,"DPRConfig"),yyr.forEach(t),ISe=r(K2e," (DPR model)"),K2e.forEach(t),jSe=i(T),Kf=n(T,"LI",{});var Z2e=s(Kf);mW=n(Z2e,"STRONG",{});var wyr=s(mW);NSe=r(wyr,"electra"),wyr.forEach(t),DSe=r(Z2e," \u2014 "),gL=n(Z2e,"A",{href:!0});var Ayr=s(gL);qSe=r(Ayr,"ElectraConfig"),Ayr.forEach(t),GSe=r(Z2e," (ELECTRA model)"),Z2e.forEach(t),OSe=i(T),Zf=n(T,"LI",{});var eve=s(Zf);gW=n(eve,"STRONG",{});var Lyr=s(gW);XSe=r(Lyr,"encoder-decoder"),Lyr.forEach(t),zSe=r(eve," \u2014 "),hL=n(eve,"A",{href:!0});var Byr=s(hL);VSe=r(Byr,"EncoderDecoderConfig"),Byr.forEach(t),WSe=r(eve," (Encoder decoder model)"),eve.forEach(t),QSe=i(T),em=n(T,"LI",{});var ove=s(em);hW=n(ove,"STRONG",{});var kyr=s(hW);HSe=r(kyr,"flaubert"),kyr.forEach(t),USe=r(ove," \u2014 "),pL=n(ove,"A",{href:!0});var xyr=s(pL);JSe=r(xyr,"FlaubertConfig"),xyr.forEach(t),YSe=r(ove," (FlauBERT model)"),ove.forEach(t),KSe=i(T),om=n(T,"LI",{});var rve=s(om);pW=n(rve,"STRONG",{});var Ryr=s(pW);ZSe=r(Ryr,"fnet"),Ryr.forEach(t),ePe=r(rve," \u2014 "),_L=n(rve,"A",{href:!0});var Syr=s(_L);oPe=r(Syr,"FNetConfig"),Syr.forEach(t),rPe=r(rve," (FNet model)"),rve.forEach(t),tPe=i(T),rm=n(T,"LI",{});var tve=s(rm);_W=n(tve,"STRONG",{});var Pyr=s(_W);aPe=r(Pyr,"fsmt"),Pyr.forEach(t),nPe=r(tve," \u2014 "),uL=n(tve,"A",{href:!0});var $yr=s(uL);sPe=r($yr,"FSMTConfig"),$yr.forEach(t),lPe=r(tve," (FairSeq Machine-Translation model)"),tve.forEach(t),iPe=i(T),tm=n(T,"LI",{});var ave=s(tm);uW=n(ave,"STRONG",{});var Iyr=s(uW);dPe=r(Iyr,"funnel"),Iyr.forEach(t),cPe=r(ave," \u2014 "),bL=n(ave,"A",{href:!0});var jyr=s(bL);fPe=r(jyr,"FunnelConfig"),jyr.forEach(t),mPe=r(ave," (Funnel Transformer model)"),ave.forEach(t),gPe=i(T),am=n(T,"LI",{});var nve=s(am);bW=n(nve,"STRONG",{});var Nyr=s(bW);hPe=r(Nyr,"gpt2"),Nyr.forEach(t),pPe=r(nve," \u2014 "),vL=n(nve,"A",{href:!0});var Dyr=s(vL);_Pe=r(Dyr,"GPT2Config"),Dyr.forEach(t),uPe=r(nve," (OpenAI GPT-2 model)"),nve.forEach(t),bPe=i(T),nm=n(T,"LI",{});var sve=s(nm);vW=n(sve,"STRONG",{});var qyr=s(vW);vPe=r(qyr,"gpt_neo"),qyr.forEach(t),TPe=r(sve," \u2014 "),TL=n(sve,"A",{href:!0});var Gyr=s(TL);FPe=r(Gyr,"GPTNeoConfig"),Gyr.forEach(t),CPe=r(sve," (GPT Neo model)"),sve.forEach(t),MPe=i(T),sm=n(T,"LI",{});var lve=s(sm);TW=n(lve,"STRONG",{});var Oyr=s(TW);EPe=r(Oyr,"gptj"),Oyr.forEach(t),yPe=r(lve," \u2014 "),FL=n(lve,"A",{href:!0});var Xyr=s(FL);wPe=r(Xyr,"GPTJConfig"),Xyr.forEach(t),APe=r(lve," (GPT-J model)"),lve.forEach(t),LPe=i(T),lm=n(T,"LI",{});var ive=s(lm);FW=n(ive,"STRONG",{});var zyr=s(FW);BPe=r(zyr,"hubert"),zyr.forEach(t),kPe=r(ive," \u2014 "),CL=n(ive,"A",{href:!0});var Vyr=s(CL);xPe=r(Vyr,"HubertConfig"),Vyr.forEach(t),RPe=r(ive," (Hubert model)"),ive.forEach(t),SPe=i(T),im=n(T,"LI",{});var dve=s(im);CW=n(dve,"STRONG",{});var Wyr=s(CW);PPe=r(Wyr,"ibert"),Wyr.forEach(t),$Pe=r(dve," \u2014 "),ML=n(dve,"A",{href:!0});var Qyr=s(ML);IPe=r(Qyr,"IBertConfig"),Qyr.forEach(t),jPe=r(dve," (I-BERT model)"),dve.forEach(t),NPe=i(T),dm=n(T,"LI",{});var cve=s(dm);MW=n(cve,"STRONG",{});var Hyr=s(MW);DPe=r(Hyr,"imagegpt"),Hyr.forEach(t),qPe=r(cve," \u2014 "),EL=n(cve,"A",{href:!0});var Uyr=s(EL);GPe=r(Uyr,"ImageGPTConfig"),Uyr.forEach(t),OPe=r(cve," (ImageGPT model)"),cve.forEach(t),XPe=i(T),cm=n(T,"LI",{});var fve=s(cm);EW=n(fve,"STRONG",{});var Jyr=s(EW);zPe=r(Jyr,"layoutlm"),Jyr.forEach(t),VPe=r(fve," \u2014 "),yL=n(fve,"A",{href:!0});var Yyr=s(yL);WPe=r(Yyr,"LayoutLMConfig"),Yyr.forEach(t),QPe=r(fve," (LayoutLM model)"),fve.forEach(t),HPe=i(T),fm=n(T,"LI",{});var mve=s(fm);yW=n(mve,"STRONG",{});var Kyr=s(yW);UPe=r(Kyr,"layoutlmv2"),Kyr.forEach(t),JPe=r(mve," \u2014 "),wL=n(mve,"A",{href:!0});var Zyr=s(wL);YPe=r(Zyr,"LayoutLMv2Config"),Zyr.forEach(t),KPe=r(mve," (LayoutLMv2 model)"),mve.forEach(t),ZPe=i(T),mm=n(T,"LI",{});var gve=s(mm);wW=n(gve,"STRONG",{});var ewr=s(wW);e$e=r(ewr,"led"),ewr.forEach(t),o$e=r(gve," \u2014 "),AL=n(gve,"A",{href:!0});var owr=s(AL);r$e=r(owr,"LEDConfig"),owr.forEach(t),t$e=r(gve," (LED model)"),gve.forEach(t),a$e=i(T),gm=n(T,"LI",{});var hve=s(gm);AW=n(hve,"STRONG",{});var rwr=s(AW);n$e=r(rwr,"longformer"),rwr.forEach(t),s$e=r(hve," \u2014 "),LL=n(hve,"A",{href:!0});var twr=s(LL);l$e=r(twr,"LongformerConfig"),twr.forEach(t),i$e=r(hve," (Longformer model)"),hve.forEach(t),d$e=i(T),hm=n(T,"LI",{});var pve=s(hm);LW=n(pve,"STRONG",{});var awr=s(LW);c$e=r(awr,"luke"),awr.forEach(t),f$e=r(pve," \u2014 "),BL=n(pve,"A",{href:!0});var nwr=s(BL);m$e=r(nwr,"LukeConfig"),nwr.forEach(t),g$e=r(pve," (LUKE model)"),pve.forEach(t),h$e=i(T),pm=n(T,"LI",{});var _ve=s(pm);BW=n(_ve,"STRONG",{});var swr=s(BW);p$e=r(swr,"lxmert"),swr.forEach(t),_$e=r(_ve," \u2014 "),kL=n(_ve,"A",{href:!0});var lwr=s(kL);u$e=r(lwr,"LxmertConfig"),lwr.forEach(t),b$e=r(_ve," (LXMERT model)"),_ve.forEach(t),v$e=i(T),_m=n(T,"LI",{});var uve=s(_m);kW=n(uve,"STRONG",{});var iwr=s(kW);T$e=r(iwr,"m2m_100"),iwr.forEach(t),F$e=r(uve," \u2014 "),xL=n(uve,"A",{href:!0});var dwr=s(xL);C$e=r(dwr,"M2M100Config"),dwr.forEach(t),M$e=r(uve," (M2M100 model)"),uve.forEach(t),E$e=i(T),um=n(T,"LI",{});var bve=s(um);xW=n(bve,"STRONG",{});var cwr=s(xW);y$e=r(cwr,"marian"),cwr.forEach(t),w$e=r(bve," \u2014 "),RL=n(bve,"A",{href:!0});var fwr=s(RL);A$e=r(fwr,"MarianConfig"),fwr.forEach(t),L$e=r(bve," (Marian model)"),bve.forEach(t),B$e=i(T),bm=n(T,"LI",{});var vve=s(bm);RW=n(vve,"STRONG",{});var mwr=s(RW);k$e=r(mwr,"mbart"),mwr.forEach(t),x$e=r(vve," \u2014 "),SL=n(vve,"A",{href:!0});var gwr=s(SL);R$e=r(gwr,"MBartConfig"),gwr.forEach(t),S$e=r(vve," (mBART model)"),vve.forEach(t),P$e=i(T),vm=n(T,"LI",{});var Tve=s(vm);SW=n(Tve,"STRONG",{});var hwr=s(SW);$$e=r(hwr,"megatron-bert"),hwr.forEach(t),I$e=r(Tve," \u2014 "),PL=n(Tve,"A",{href:!0});var pwr=s(PL);j$e=r(pwr,"MegatronBertConfig"),pwr.forEach(t),N$e=r(Tve," (MegatronBert model)"),Tve.forEach(t),D$e=i(T),Tm=n(T,"LI",{});var Fve=s(Tm);PW=n(Fve,"STRONG",{});var _wr=s(PW);q$e=r(_wr,"mobilebert"),_wr.forEach(t),G$e=r(Fve," \u2014 "),$L=n(Fve,"A",{href:!0});var uwr=s($L);O$e=r(uwr,"MobileBertConfig"),uwr.forEach(t),X$e=r(Fve," (MobileBERT model)"),Fve.forEach(t),z$e=i(T),Fm=n(T,"LI",{});var Cve=s(Fm);$W=n(Cve,"STRONG",{});var bwr=s($W);V$e=r(bwr,"mpnet"),bwr.forEach(t),W$e=r(Cve," \u2014 "),IL=n(Cve,"A",{href:!0});var vwr=s(IL);Q$e=r(vwr,"MPNetConfig"),vwr.forEach(t),H$e=r(Cve," (MPNet model)"),Cve.forEach(t),U$e=i(T),Cm=n(T,"LI",{});var Mve=s(Cm);IW=n(Mve,"STRONG",{});var Twr=s(IW);J$e=r(Twr,"mt5"),Twr.forEach(t),Y$e=r(Mve," \u2014 "),jL=n(Mve,"A",{href:!0});var Fwr=s(jL);K$e=r(Fwr,"MT5Config"),Fwr.forEach(t),Z$e=r(Mve," (mT5 model)"),Mve.forEach(t),eIe=i(T),Mm=n(T,"LI",{});var Eve=s(Mm);jW=n(Eve,"STRONG",{});var Cwr=s(jW);oIe=r(Cwr,"nystromformer"),Cwr.forEach(t),rIe=r(Eve," \u2014 "),NL=n(Eve,"A",{href:!0});var Mwr=s(NL);tIe=r(Mwr,"NystromformerConfig"),Mwr.forEach(t),aIe=r(Eve," (Nystromformer model)"),Eve.forEach(t),nIe=i(T),Em=n(T,"LI",{});var yve=s(Em);NW=n(yve,"STRONG",{});var Ewr=s(NW);sIe=r(Ewr,"openai-gpt"),Ewr.forEach(t),lIe=r(yve," \u2014 "),DL=n(yve,"A",{href:!0});var ywr=s(DL);iIe=r(ywr,"OpenAIGPTConfig"),ywr.forEach(t),dIe=r(yve," (OpenAI GPT model)"),yve.forEach(t),cIe=i(T),ym=n(T,"LI",{});var wve=s(ym);DW=n(wve,"STRONG",{});var wwr=s(DW);fIe=r(wwr,"pegasus"),wwr.forEach(t),mIe=r(wve," \u2014 "),qL=n(wve,"A",{href:!0});var Awr=s(qL);gIe=r(Awr,"PegasusConfig"),Awr.forEach(t),hIe=r(wve," (Pegasus model)"),wve.forEach(t),pIe=i(T),wm=n(T,"LI",{});var Ave=s(wm);qW=n(Ave,"STRONG",{});var Lwr=s(qW);_Ie=r(Lwr,"perceiver"),Lwr.forEach(t),uIe=r(Ave," \u2014 "),GL=n(Ave,"A",{href:!0});var Bwr=s(GL);bIe=r(Bwr,"PerceiverConfig"),Bwr.forEach(t),vIe=r(Ave," (Perceiver model)"),Ave.forEach(t),TIe=i(T),Am=n(T,"LI",{});var Lve=s(Am);GW=n(Lve,"STRONG",{});var kwr=s(GW);FIe=r(kwr,"plbart"),kwr.forEach(t),CIe=r(Lve," \u2014 "),OL=n(Lve,"A",{href:!0});var xwr=s(OL);MIe=r(xwr,"PLBartConfig"),xwr.forEach(t),EIe=r(Lve," (PLBart model)"),Lve.forEach(t),yIe=i(T),Lm=n(T,"LI",{});var Bve=s(Lm);OW=n(Bve,"STRONG",{});var Rwr=s(OW);wIe=r(Rwr,"poolformer"),Rwr.forEach(t),AIe=r(Bve," \u2014 "),XL=n(Bve,"A",{href:!0});var Swr=s(XL);LIe=r(Swr,"PoolFormerConfig"),Swr.forEach(t),BIe=r(Bve," (PoolFormer model)"),Bve.forEach(t),kIe=i(T),Bm=n(T,"LI",{});var kve=s(Bm);XW=n(kve,"STRONG",{});var Pwr=s(XW);xIe=r(Pwr,"prophetnet"),Pwr.forEach(t),RIe=r(kve," \u2014 "),zL=n(kve,"A",{href:!0});var $wr=s(zL);SIe=r($wr,"ProphetNetConfig"),$wr.forEach(t),PIe=r(kve," (ProphetNet model)"),kve.forEach(t),$Ie=i(T),km=n(T,"LI",{});var xve=s(km);zW=n(xve,"STRONG",{});var Iwr=s(zW);IIe=r(Iwr,"qdqbert"),Iwr.forEach(t),jIe=r(xve," \u2014 "),VL=n(xve,"A",{href:!0});var jwr=s(VL);NIe=r(jwr,"QDQBertConfig"),jwr.forEach(t),DIe=r(xve," (QDQBert model)"),xve.forEach(t),qIe=i(T),xm=n(T,"LI",{});var Rve=s(xm);VW=n(Rve,"STRONG",{});var Nwr=s(VW);GIe=r(Nwr,"rag"),Nwr.forEach(t),OIe=r(Rve," \u2014 "),WL=n(Rve,"A",{href:!0});var Dwr=s(WL);XIe=r(Dwr,"RagConfig"),Dwr.forEach(t),zIe=r(Rve," (RAG model)"),Rve.forEach(t),VIe=i(T),Rm=n(T,"LI",{});var Sve=s(Rm);WW=n(Sve,"STRONG",{});var qwr=s(WW);WIe=r(qwr,"realm"),qwr.forEach(t),QIe=r(Sve," \u2014 "),QL=n(Sve,"A",{href:!0});var Gwr=s(QL);HIe=r(Gwr,"RealmConfig"),Gwr.forEach(t),UIe=r(Sve," (Realm model)"),Sve.forEach(t),JIe=i(T),Sm=n(T,"LI",{});var Pve=s(Sm);QW=n(Pve,"STRONG",{});var Owr=s(QW);YIe=r(Owr,"reformer"),Owr.forEach(t),KIe=r(Pve," \u2014 "),HL=n(Pve,"A",{href:!0});var Xwr=s(HL);ZIe=r(Xwr,"ReformerConfig"),Xwr.forEach(t),eje=r(Pve," (Reformer model)"),Pve.forEach(t),oje=i(T),Pm=n(T,"LI",{});var $ve=s(Pm);HW=n($ve,"STRONG",{});var zwr=s(HW);rje=r(zwr,"rembert"),zwr.forEach(t),tje=r($ve," \u2014 "),UL=n($ve,"A",{href:!0});var Vwr=s(UL);aje=r(Vwr,"RemBertConfig"),Vwr.forEach(t),nje=r($ve," (RemBERT model)"),$ve.forEach(t),sje=i(T),$m=n(T,"LI",{});var Ive=s($m);UW=n(Ive,"STRONG",{});var Wwr=s(UW);lje=r(Wwr,"retribert"),Wwr.forEach(t),ije=r(Ive," \u2014 "),JL=n(Ive,"A",{href:!0});var Qwr=s(JL);dje=r(Qwr,"RetriBertConfig"),Qwr.forEach(t),cje=r(Ive," (RetriBERT model)"),Ive.forEach(t),fje=i(T),Im=n(T,"LI",{});var jve=s(Im);JW=n(jve,"STRONG",{});var Hwr=s(JW);mje=r(Hwr,"roberta"),Hwr.forEach(t),gje=r(jve," \u2014 "),YL=n(jve,"A",{href:!0});var Uwr=s(YL);hje=r(Uwr,"RobertaConfig"),Uwr.forEach(t),pje=r(jve," (RoBERTa model)"),jve.forEach(t),_je=i(T),jm=n(T,"LI",{});var Nve=s(jm);YW=n(Nve,"STRONG",{});var Jwr=s(YW);uje=r(Jwr,"roformer"),Jwr.forEach(t),bje=r(Nve," \u2014 "),KL=n(Nve,"A",{href:!0});var Ywr=s(KL);vje=r(Ywr,"RoFormerConfig"),Ywr.forEach(t),Tje=r(Nve," (RoFormer model)"),Nve.forEach(t),Fje=i(T),Nm=n(T,"LI",{});var Dve=s(Nm);KW=n(Dve,"STRONG",{});var Kwr=s(KW);Cje=r(Kwr,"segformer"),Kwr.forEach(t),Mje=r(Dve," \u2014 "),ZL=n(Dve,"A",{href:!0});var Zwr=s(ZL);Eje=r(Zwr,"SegformerConfig"),Zwr.forEach(t),yje=r(Dve," (SegFormer model)"),Dve.forEach(t),wje=i(T),Dm=n(T,"LI",{});var qve=s(Dm);ZW=n(qve,"STRONG",{});var eAr=s(ZW);Aje=r(eAr,"sew"),eAr.forEach(t),Lje=r(qve," \u2014 "),e8=n(qve,"A",{href:!0});var oAr=s(e8);Bje=r(oAr,"SEWConfig"),oAr.forEach(t),kje=r(qve," (SEW model)"),qve.forEach(t),xje=i(T),qm=n(T,"LI",{});var Gve=s(qm);eQ=n(Gve,"STRONG",{});var rAr=s(eQ);Rje=r(rAr,"sew-d"),rAr.forEach(t),Sje=r(Gve," \u2014 "),o8=n(Gve,"A",{href:!0});var tAr=s(o8);Pje=r(tAr,"SEWDConfig"),tAr.forEach(t),$je=r(Gve," (SEW-D model)"),Gve.forEach(t),Ije=i(T),Gm=n(T,"LI",{});var Ove=s(Gm);oQ=n(Ove,"STRONG",{});var aAr=s(oQ);jje=r(aAr,"speech-encoder-decoder"),aAr.forEach(t),Nje=r(Ove," \u2014 "),r8=n(Ove,"A",{href:!0});var nAr=s(r8);Dje=r(nAr,"SpeechEncoderDecoderConfig"),nAr.forEach(t),qje=r(Ove," (Speech Encoder decoder model)"),Ove.forEach(t),Gje=i(T),Om=n(T,"LI",{});var Xve=s(Om);rQ=n(Xve,"STRONG",{});var sAr=s(rQ);Oje=r(sAr,"speech_to_text"),sAr.forEach(t),Xje=r(Xve," \u2014 "),t8=n(Xve,"A",{href:!0});var lAr=s(t8);zje=r(lAr,"Speech2TextConfig"),lAr.forEach(t),Vje=r(Xve," (Speech2Text model)"),Xve.forEach(t),Wje=i(T),Xm=n(T,"LI",{});var zve=s(Xm);tQ=n(zve,"STRONG",{});var iAr=s(tQ);Qje=r(iAr,"speech_to_text_2"),iAr.forEach(t),Hje=r(zve," \u2014 "),a8=n(zve,"A",{href:!0});var dAr=s(a8);Uje=r(dAr,"Speech2Text2Config"),dAr.forEach(t),Jje=r(zve," (Speech2Text2 model)"),zve.forEach(t),Yje=i(T),zm=n(T,"LI",{});var Vve=s(zm);aQ=n(Vve,"STRONG",{});var cAr=s(aQ);Kje=r(cAr,"splinter"),cAr.forEach(t),Zje=r(Vve," \u2014 "),n8=n(Vve,"A",{href:!0});var fAr=s(n8);eNe=r(fAr,"SplinterConfig"),fAr.forEach(t),oNe=r(Vve," (Splinter model)"),Vve.forEach(t),rNe=i(T),Vm=n(T,"LI",{});var Wve=s(Vm);nQ=n(Wve,"STRONG",{});var mAr=s(nQ);tNe=r(mAr,"squeezebert"),mAr.forEach(t),aNe=r(Wve," \u2014 "),s8=n(Wve,"A",{href:!0});var gAr=s(s8);nNe=r(gAr,"SqueezeBertConfig"),gAr.forEach(t),sNe=r(Wve," (SqueezeBERT model)"),Wve.forEach(t),lNe=i(T),Wm=n(T,"LI",{});var Qve=s(Wm);sQ=n(Qve,"STRONG",{});var hAr=s(sQ);iNe=r(hAr,"swin"),hAr.forEach(t),dNe=r(Qve," \u2014 "),l8=n(Qve,"A",{href:!0});var pAr=s(l8);cNe=r(pAr,"SwinConfig"),pAr.forEach(t),fNe=r(Qve," (Swin model)"),Qve.forEach(t),mNe=i(T),Qm=n(T,"LI",{});var Hve=s(Qm);lQ=n(Hve,"STRONG",{});var _Ar=s(lQ);gNe=r(_Ar,"t5"),_Ar.forEach(t),hNe=r(Hve," \u2014 "),i8=n(Hve,"A",{href:!0});var uAr=s(i8);pNe=r(uAr,"T5Config"),uAr.forEach(t),_Ne=r(Hve," (T5 model)"),Hve.forEach(t),uNe=i(T),Hm=n(T,"LI",{});var Uve=s(Hm);iQ=n(Uve,"STRONG",{});var bAr=s(iQ);bNe=r(bAr,"tapas"),bAr.forEach(t),vNe=r(Uve," \u2014 "),d8=n(Uve,"A",{href:!0});var vAr=s(d8);TNe=r(vAr,"TapasConfig"),vAr.forEach(t),FNe=r(Uve," (TAPAS model)"),Uve.forEach(t),CNe=i(T),Um=n(T,"LI",{});var Jve=s(Um);dQ=n(Jve,"STRONG",{});var TAr=s(dQ);MNe=r(TAr,"transfo-xl"),TAr.forEach(t),ENe=r(Jve," \u2014 "),c8=n(Jve,"A",{href:!0});var FAr=s(c8);yNe=r(FAr,"TransfoXLConfig"),FAr.forEach(t),wNe=r(Jve," (Transformer-XL model)"),Jve.forEach(t),ANe=i(T),Jm=n(T,"LI",{});var Yve=s(Jm);cQ=n(Yve,"STRONG",{});var CAr=s(cQ);LNe=r(CAr,"trocr"),CAr.forEach(t),BNe=r(Yve," \u2014 "),f8=n(Yve,"A",{href:!0});var MAr=s(f8);kNe=r(MAr,"TrOCRConfig"),MAr.forEach(t),xNe=r(Yve," (TrOCR model)"),Yve.forEach(t),RNe=i(T),Ym=n(T,"LI",{});var Kve=s(Ym);fQ=n(Kve,"STRONG",{});var EAr=s(fQ);SNe=r(EAr,"unispeech"),EAr.forEach(t),PNe=r(Kve," \u2014 "),m8=n(Kve,"A",{href:!0});var yAr=s(m8);$Ne=r(yAr,"UniSpeechConfig"),yAr.forEach(t),INe=r(Kve," (UniSpeech model)"),Kve.forEach(t),jNe=i(T),Km=n(T,"LI",{});var Zve=s(Km);mQ=n(Zve,"STRONG",{});var wAr=s(mQ);NNe=r(wAr,"unispeech-sat"),wAr.forEach(t),DNe=r(Zve," \u2014 "),g8=n(Zve,"A",{href:!0});var AAr=s(g8);qNe=r(AAr,"UniSpeechSatConfig"),AAr.forEach(t),GNe=r(Zve," (UniSpeechSat model)"),Zve.forEach(t),ONe=i(T),Zm=n(T,"LI",{});var e6e=s(Zm);gQ=n(e6e,"STRONG",{});var LAr=s(gQ);XNe=r(LAr,"vilt"),LAr.forEach(t),zNe=r(e6e," \u2014 "),h8=n(e6e,"A",{href:!0});var BAr=s(h8);VNe=r(BAr,"ViltConfig"),BAr.forEach(t),WNe=r(e6e," (ViLT model)"),e6e.forEach(t),QNe=i(T),eg=n(T,"LI",{});var o6e=s(eg);hQ=n(o6e,"STRONG",{});var kAr=s(hQ);HNe=r(kAr,"vision-encoder-decoder"),kAr.forEach(t),UNe=r(o6e," \u2014 "),p8=n(o6e,"A",{href:!0});var xAr=s(p8);JNe=r(xAr,"VisionEncoderDecoderConfig"),xAr.forEach(t),YNe=r(o6e," (Vision Encoder decoder model)"),o6e.forEach(t),KNe=i(T),og=n(T,"LI",{});var r6e=s(og);pQ=n(r6e,"STRONG",{});var RAr=s(pQ);ZNe=r(RAr,"vision-text-dual-encoder"),RAr.forEach(t),eDe=r(r6e," \u2014 "),_8=n(r6e,"A",{href:!0});var SAr=s(_8);oDe=r(SAr,"VisionTextDualEncoderConfig"),SAr.forEach(t),rDe=r(r6e," (VisionTextDualEncoder model)"),r6e.forEach(t),tDe=i(T),rg=n(T,"LI",{});var t6e=s(rg);_Q=n(t6e,"STRONG",{});var PAr=s(_Q);aDe=r(PAr,"visual_bert"),PAr.forEach(t),nDe=r(t6e," \u2014 "),u8=n(t6e,"A",{href:!0});var $Ar=s(u8);sDe=r($Ar,"VisualBertConfig"),$Ar.forEach(t),lDe=r(t6e," (VisualBert model)"),t6e.forEach(t),iDe=i(T),tg=n(T,"LI",{});var a6e=s(tg);uQ=n(a6e,"STRONG",{});var IAr=s(uQ);dDe=r(IAr,"vit"),IAr.forEach(t),cDe=r(a6e," \u2014 "),b8=n(a6e,"A",{href:!0});var jAr=s(b8);fDe=r(jAr,"ViTConfig"),jAr.forEach(t),mDe=r(a6e," (ViT model)"),a6e.forEach(t),gDe=i(T),ag=n(T,"LI",{});var n6e=s(ag);bQ=n(n6e,"STRONG",{});var NAr=s(bQ);hDe=r(NAr,"vit_mae"),NAr.forEach(t),pDe=r(n6e," \u2014 "),v8=n(n6e,"A",{href:!0});var DAr=s(v8);_De=r(DAr,"ViTMAEConfig"),DAr.forEach(t),uDe=r(n6e," (ViTMAE model)"),n6e.forEach(t),bDe=i(T),ng=n(T,"LI",{});var s6e=s(ng);vQ=n(s6e,"STRONG",{});var qAr=s(vQ);vDe=r(qAr,"wav2vec2"),qAr.forEach(t),TDe=r(s6e," \u2014 "),T8=n(s6e,"A",{href:!0});var GAr=s(T8);FDe=r(GAr,"Wav2Vec2Config"),GAr.forEach(t),CDe=r(s6e," (Wav2Vec2 model)"),s6e.forEach(t),MDe=i(T),sg=n(T,"LI",{});var l6e=s(sg);TQ=n(l6e,"STRONG",{});var OAr=s(TQ);EDe=r(OAr,"wavlm"),OAr.forEach(t),yDe=r(l6e," \u2014 "),F8=n(l6e,"A",{href:!0});var XAr=s(F8);wDe=r(XAr,"WavLMConfig"),XAr.forEach(t),ADe=r(l6e," (WavLM model)"),l6e.forEach(t),LDe=i(T),lg=n(T,"LI",{});var i6e=s(lg);FQ=n(i6e,"STRONG",{});var zAr=s(FQ);BDe=r(zAr,"xglm"),zAr.forEach(t),kDe=r(i6e," \u2014 "),C8=n(i6e,"A",{href:!0});var VAr=s(C8);xDe=r(VAr,"XGLMConfig"),VAr.forEach(t),RDe=r(i6e," (XGLM model)"),i6e.forEach(t),SDe=i(T),ig=n(T,"LI",{});var d6e=s(ig);CQ=n(d6e,"STRONG",{});var WAr=s(CQ);PDe=r(WAr,"xlm"),WAr.forEach(t),$De=r(d6e," \u2014 "),M8=n(d6e,"A",{href:!0});var QAr=s(M8);IDe=r(QAr,"XLMConfig"),QAr.forEach(t),jDe=r(d6e," (XLM model)"),d6e.forEach(t),NDe=i(T),dg=n(T,"LI",{});var c6e=s(dg);MQ=n(c6e,"STRONG",{});var HAr=s(MQ);DDe=r(HAr,"xlm-prophetnet"),HAr.forEach(t),qDe=r(c6e," \u2014 "),E8=n(c6e,"A",{href:!0});var UAr=s(E8);GDe=r(UAr,"XLMProphetNetConfig"),UAr.forEach(t),ODe=r(c6e," (XLMProphetNet model)"),c6e.forEach(t),XDe=i(T),cg=n(T,"LI",{});var f6e=s(cg);EQ=n(f6e,"STRONG",{});var JAr=s(EQ);zDe=r(JAr,"xlm-roberta"),JAr.forEach(t),VDe=r(f6e," \u2014 "),y8=n(f6e,"A",{href:!0});var YAr=s(y8);WDe=r(YAr,"XLMRobertaConfig"),YAr.forEach(t),QDe=r(f6e," (XLM-RoBERTa model)"),f6e.forEach(t),HDe=i(T),fg=n(T,"LI",{});var m6e=s(fg);yQ=n(m6e,"STRONG",{});var KAr=s(yQ);UDe=r(KAr,"xlm-roberta-xl"),KAr.forEach(t),JDe=r(m6e," \u2014 "),w8=n(m6e,"A",{href:!0});var ZAr=s(w8);YDe=r(ZAr,"XLMRobertaXLConfig"),ZAr.forEach(t),KDe=r(m6e," (XLM-RoBERTa-XL model)"),m6e.forEach(t),ZDe=i(T),mg=n(T,"LI",{});var g6e=s(mg);wQ=n(g6e,"STRONG",{});var e0r=s(wQ);eqe=r(e0r,"xlnet"),e0r.forEach(t),oqe=r(g6e," \u2014 "),A8=n(g6e,"A",{href:!0});var o0r=s(A8);rqe=r(o0r,"XLNetConfig"),o0r.forEach(t),tqe=r(g6e," (XLNet model)"),g6e.forEach(t),aqe=i(T),gg=n(T,"LI",{});var h6e=s(gg);AQ=n(h6e,"STRONG",{});var r0r=s(AQ);nqe=r(r0r,"yoso"),r0r.forEach(t),sqe=r(h6e," \u2014 "),L8=n(h6e,"A",{href:!0});var t0r=s(L8);lqe=r(t0r,"YosoConfig"),t0r.forEach(t),iqe=r(h6e," (YOSO model)"),h6e.forEach(t),T.forEach(t),dqe=i(ia),LQ=n(ia,"P",{});var a0r=s(LQ);cqe=r(a0r,"Examples:"),a0r.forEach(t),fqe=i(ia),m(m4.$$.fragment,ia),ia.forEach(t),mqe=i(Ps),hg=n(Ps,"DIV",{class:!0});var cBe=s(hg);m(g4.$$.fragment,cBe),gqe=i(cBe),BQ=n(cBe,"P",{});var n0r=s(BQ);hqe=r(n0r,"Register a new configuration for this class."),n0r.forEach(t),cBe.forEach(t),Ps.forEach(t),cLe=i(d),Ii=n(d,"H2",{class:!0});var fBe=s(Ii);pg=n(fBe,"A",{id:!0,class:!0,href:!0});var s0r=s(pg);kQ=n(s0r,"SPAN",{});var l0r=s(kQ);m(h4.$$.fragment,l0r),l0r.forEach(t),s0r.forEach(t),pqe=i(fBe),xQ=n(fBe,"SPAN",{});var i0r=s(xQ);_qe=r(i0r,"AutoTokenizer"),i0r.forEach(t),fBe.forEach(t),fLe=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(p4.$$.fragment,$s),uqe=i($s),_4=n($s,"P",{});var mBe=s(_4);bqe=r(mBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B8=n(mBe,"A",{href:!0});var d0r=s(B8);vqe=r(d0r,"AutoTokenizer.from_pretrained()"),d0r.forEach(t),Tqe=r(mBe," class method."),mBe.forEach(t),Fqe=i($s),u4=n($s,"P",{});var gBe=s(u4);Cqe=r(gBe,"This class cannot be instantiated directly using "),RQ=n(gBe,"CODE",{});var c0r=s(RQ);Mqe=r(c0r,"__init__()"),c0r.forEach(t),Eqe=r(gBe," (throws an error)."),gBe.forEach(t),yqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(b4.$$.fragment,da),wqe=i(da),SQ=n(da,"P",{});var f0r=s(SQ);Aqe=r(f0r,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),f0r.forEach(t),Lqe=i(da),ja=n(da,"P",{});var nC=s(ja);Bqe=r(nC,"The tokenizer class to instantiate is selected based on the "),PQ=n(nC,"CODE",{});var m0r=s(PQ);kqe=r(m0r,"model_type"),m0r.forEach(t),xqe=r(nC,` property of the config object (either
passed as an argument or loaded from `),$Q=n(nC,"CODE",{});var g0r=s($Q);Rqe=r(g0r,"pretrained_model_name_or_path"),g0r.forEach(t),Sqe=r(nC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=n(nC,"CODE",{});var h0r=s(IQ);Pqe=r(h0r,"pretrained_model_name_or_path"),h0r.forEach(t),$qe=r(nC,":"),nC.forEach(t),Iqe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var zA=s(Dn);jQ=n(zA,"STRONG",{});var p0r=s(jQ);jqe=r(p0r,"albert"),p0r.forEach(t),Nqe=r(zA," \u2014 "),k8=n(zA,"A",{href:!0});var _0r=s(k8);Dqe=r(_0r,"AlbertTokenizer"),_0r.forEach(t),qqe=r(zA," or "),x8=n(zA,"A",{href:!0});var u0r=s(x8);Gqe=r(u0r,"AlbertTokenizerFast"),u0r.forEach(t),Oqe=r(zA," (ALBERT model)"),zA.forEach(t),Xqe=i(y),qn=n(y,"LI",{});var VA=s(qn);NQ=n(VA,"STRONG",{});var b0r=s(NQ);zqe=r(b0r,"bart"),b0r.forEach(t),Vqe=r(VA," \u2014 "),R8=n(VA,"A",{href:!0});var v0r=s(R8);Wqe=r(v0r,"BartTokenizer"),v0r.forEach(t),Qqe=r(VA," or "),S8=n(VA,"A",{href:!0});var T0r=s(S8);Hqe=r(T0r,"BartTokenizerFast"),T0r.forEach(t),Uqe=r(VA," (BART model)"),VA.forEach(t),Jqe=i(y),Gn=n(y,"LI",{});var WA=s(Gn);DQ=n(WA,"STRONG",{});var F0r=s(DQ);Yqe=r(F0r,"barthez"),F0r.forEach(t),Kqe=r(WA," \u2014 "),P8=n(WA,"A",{href:!0});var C0r=s(P8);Zqe=r(C0r,"BarthezTokenizer"),C0r.forEach(t),eGe=r(WA," or "),$8=n(WA,"A",{href:!0});var M0r=s($8);oGe=r(M0r,"BarthezTokenizerFast"),M0r.forEach(t),rGe=r(WA," (BARThez model)"),WA.forEach(t),tGe=i(y),_g=n(y,"LI",{});var p6e=s(_g);qQ=n(p6e,"STRONG",{});var E0r=s(qQ);aGe=r(E0r,"bartpho"),E0r.forEach(t),nGe=r(p6e," \u2014 "),I8=n(p6e,"A",{href:!0});var y0r=s(I8);sGe=r(y0r,"BartphoTokenizer"),y0r.forEach(t),lGe=r(p6e," (BARTpho model)"),p6e.forEach(t),iGe=i(y),On=n(y,"LI",{});var QA=s(On);GQ=n(QA,"STRONG",{});var w0r=s(GQ);dGe=r(w0r,"bert"),w0r.forEach(t),cGe=r(QA," \u2014 "),j8=n(QA,"A",{href:!0});var A0r=s(j8);fGe=r(A0r,"BertTokenizer"),A0r.forEach(t),mGe=r(QA," or "),N8=n(QA,"A",{href:!0});var L0r=s(N8);gGe=r(L0r,"BertTokenizerFast"),L0r.forEach(t),hGe=r(QA," (BERT model)"),QA.forEach(t),pGe=i(y),ug=n(y,"LI",{});var _6e=s(ug);OQ=n(_6e,"STRONG",{});var B0r=s(OQ);_Ge=r(B0r,"bert-generation"),B0r.forEach(t),uGe=r(_6e," \u2014 "),D8=n(_6e,"A",{href:!0});var k0r=s(D8);bGe=r(k0r,"BertGenerationTokenizer"),k0r.forEach(t),vGe=r(_6e," (Bert Generation model)"),_6e.forEach(t),TGe=i(y),bg=n(y,"LI",{});var u6e=s(bg);XQ=n(u6e,"STRONG",{});var x0r=s(XQ);FGe=r(x0r,"bert-japanese"),x0r.forEach(t),CGe=r(u6e," \u2014 "),q8=n(u6e,"A",{href:!0});var R0r=s(q8);MGe=r(R0r,"BertJapaneseTokenizer"),R0r.forEach(t),EGe=r(u6e," (BertJapanese model)"),u6e.forEach(t),yGe=i(y),vg=n(y,"LI",{});var b6e=s(vg);zQ=n(b6e,"STRONG",{});var S0r=s(zQ);wGe=r(S0r,"bertweet"),S0r.forEach(t),AGe=r(b6e," \u2014 "),G8=n(b6e,"A",{href:!0});var P0r=s(G8);LGe=r(P0r,"BertweetTokenizer"),P0r.forEach(t),BGe=r(b6e," (Bertweet model)"),b6e.forEach(t),kGe=i(y),Xn=n(y,"LI",{});var HA=s(Xn);VQ=n(HA,"STRONG",{});var $0r=s(VQ);xGe=r($0r,"big_bird"),$0r.forEach(t),RGe=r(HA," \u2014 "),O8=n(HA,"A",{href:!0});var I0r=s(O8);SGe=r(I0r,"BigBirdTokenizer"),I0r.forEach(t),PGe=r(HA," or "),X8=n(HA,"A",{href:!0});var j0r=s(X8);$Ge=r(j0r,"BigBirdTokenizerFast"),j0r.forEach(t),IGe=r(HA," (BigBird model)"),HA.forEach(t),jGe=i(y),zn=n(y,"LI",{});var UA=s(zn);WQ=n(UA,"STRONG",{});var N0r=s(WQ);NGe=r(N0r,"bigbird_pegasus"),N0r.forEach(t),DGe=r(UA," \u2014 "),z8=n(UA,"A",{href:!0});var D0r=s(z8);qGe=r(D0r,"PegasusTokenizer"),D0r.forEach(t),GGe=r(UA," or "),V8=n(UA,"A",{href:!0});var q0r=s(V8);OGe=r(q0r,"PegasusTokenizerFast"),q0r.forEach(t),XGe=r(UA," (BigBirdPegasus model)"),UA.forEach(t),zGe=i(y),Vn=n(y,"LI",{});var JA=s(Vn);QQ=n(JA,"STRONG",{});var G0r=s(QQ);VGe=r(G0r,"blenderbot"),G0r.forEach(t),WGe=r(JA," \u2014 "),W8=n(JA,"A",{href:!0});var O0r=s(W8);QGe=r(O0r,"BlenderbotTokenizer"),O0r.forEach(t),HGe=r(JA," or "),Q8=n(JA,"A",{href:!0});var X0r=s(Q8);UGe=r(X0r,"BlenderbotTokenizerFast"),X0r.forEach(t),JGe=r(JA," (Blenderbot model)"),JA.forEach(t),YGe=i(y),Tg=n(y,"LI",{});var v6e=s(Tg);HQ=n(v6e,"STRONG",{});var z0r=s(HQ);KGe=r(z0r,"blenderbot-small"),z0r.forEach(t),ZGe=r(v6e," \u2014 "),H8=n(v6e,"A",{href:!0});var V0r=s(H8);eOe=r(V0r,"BlenderbotSmallTokenizer"),V0r.forEach(t),oOe=r(v6e," (BlenderbotSmall model)"),v6e.forEach(t),rOe=i(y),Fg=n(y,"LI",{});var T6e=s(Fg);UQ=n(T6e,"STRONG",{});var W0r=s(UQ);tOe=r(W0r,"byt5"),W0r.forEach(t),aOe=r(T6e," \u2014 "),U8=n(T6e,"A",{href:!0});var Q0r=s(U8);nOe=r(Q0r,"ByT5Tokenizer"),Q0r.forEach(t),sOe=r(T6e," (ByT5 model)"),T6e.forEach(t),lOe=i(y),Wn=n(y,"LI",{});var YA=s(Wn);JQ=n(YA,"STRONG",{});var H0r=s(JQ);iOe=r(H0r,"camembert"),H0r.forEach(t),dOe=r(YA," \u2014 "),J8=n(YA,"A",{href:!0});var U0r=s(J8);cOe=r(U0r,"CamembertTokenizer"),U0r.forEach(t),fOe=r(YA," or "),Y8=n(YA,"A",{href:!0});var J0r=s(Y8);mOe=r(J0r,"CamembertTokenizerFast"),J0r.forEach(t),gOe=r(YA," (CamemBERT model)"),YA.forEach(t),hOe=i(y),Cg=n(y,"LI",{});var F6e=s(Cg);YQ=n(F6e,"STRONG",{});var Y0r=s(YQ);pOe=r(Y0r,"canine"),Y0r.forEach(t),_Oe=r(F6e," \u2014 "),K8=n(F6e,"A",{href:!0});var K0r=s(K8);uOe=r(K0r,"CanineTokenizer"),K0r.forEach(t),bOe=r(F6e," (Canine model)"),F6e.forEach(t),vOe=i(y),Qn=n(y,"LI",{});var KA=s(Qn);KQ=n(KA,"STRONG",{});var Z0r=s(KQ);TOe=r(Z0r,"clip"),Z0r.forEach(t),FOe=r(KA," \u2014 "),Z8=n(KA,"A",{href:!0});var eLr=s(Z8);COe=r(eLr,"CLIPTokenizer"),eLr.forEach(t),MOe=r(KA," or "),eB=n(KA,"A",{href:!0});var oLr=s(eB);EOe=r(oLr,"CLIPTokenizerFast"),oLr.forEach(t),yOe=r(KA," (CLIP model)"),KA.forEach(t),wOe=i(y),Hn=n(y,"LI",{});var ZA=s(Hn);ZQ=n(ZA,"STRONG",{});var rLr=s(ZQ);AOe=r(rLr,"convbert"),rLr.forEach(t),LOe=r(ZA," \u2014 "),oB=n(ZA,"A",{href:!0});var tLr=s(oB);BOe=r(tLr,"ConvBertTokenizer"),tLr.forEach(t),kOe=r(ZA," or "),rB=n(ZA,"A",{href:!0});var aLr=s(rB);xOe=r(aLr,"ConvBertTokenizerFast"),aLr.forEach(t),ROe=r(ZA," (ConvBERT model)"),ZA.forEach(t),SOe=i(y),Un=n(y,"LI",{});var e0=s(Un);eH=n(e0,"STRONG",{});var nLr=s(eH);POe=r(nLr,"cpm"),nLr.forEach(t),$Oe=r(e0," \u2014 "),tB=n(e0,"A",{href:!0});var sLr=s(tB);IOe=r(sLr,"CpmTokenizer"),sLr.forEach(t),jOe=r(e0," or "),oH=n(e0,"CODE",{});var lLr=s(oH);NOe=r(lLr,"CpmTokenizerFast"),lLr.forEach(t),DOe=r(e0," (CPM model)"),e0.forEach(t),qOe=i(y),Mg=n(y,"LI",{});var C6e=s(Mg);rH=n(C6e,"STRONG",{});var iLr=s(rH);GOe=r(iLr,"ctrl"),iLr.forEach(t),OOe=r(C6e," \u2014 "),aB=n(C6e,"A",{href:!0});var dLr=s(aB);XOe=r(dLr,"CTRLTokenizer"),dLr.forEach(t),zOe=r(C6e," (CTRL model)"),C6e.forEach(t),VOe=i(y),Jn=n(y,"LI",{});var o0=s(Jn);tH=n(o0,"STRONG",{});var cLr=s(tH);WOe=r(cLr,"deberta"),cLr.forEach(t),QOe=r(o0," \u2014 "),nB=n(o0,"A",{href:!0});var fLr=s(nB);HOe=r(fLr,"DebertaTokenizer"),fLr.forEach(t),UOe=r(o0," or "),sB=n(o0,"A",{href:!0});var mLr=s(sB);JOe=r(mLr,"DebertaTokenizerFast"),mLr.forEach(t),YOe=r(o0," (DeBERTa model)"),o0.forEach(t),KOe=i(y),Eg=n(y,"LI",{});var M6e=s(Eg);aH=n(M6e,"STRONG",{});var gLr=s(aH);ZOe=r(gLr,"deberta-v2"),gLr.forEach(t),eXe=r(M6e," \u2014 "),lB=n(M6e,"A",{href:!0});var hLr=s(lB);oXe=r(hLr,"DebertaV2Tokenizer"),hLr.forEach(t),rXe=r(M6e," (DeBERTa-v2 model)"),M6e.forEach(t),tXe=i(y),Yn=n(y,"LI",{});var r0=s(Yn);nH=n(r0,"STRONG",{});var pLr=s(nH);aXe=r(pLr,"distilbert"),pLr.forEach(t),nXe=r(r0," \u2014 "),iB=n(r0,"A",{href:!0});var _Lr=s(iB);sXe=r(_Lr,"DistilBertTokenizer"),_Lr.forEach(t),lXe=r(r0," or "),dB=n(r0,"A",{href:!0});var uLr=s(dB);iXe=r(uLr,"DistilBertTokenizerFast"),uLr.forEach(t),dXe=r(r0," (DistilBERT model)"),r0.forEach(t),cXe=i(y),Kn=n(y,"LI",{});var t0=s(Kn);sH=n(t0,"STRONG",{});var bLr=s(sH);fXe=r(bLr,"dpr"),bLr.forEach(t),mXe=r(t0," \u2014 "),cB=n(t0,"A",{href:!0});var vLr=s(cB);gXe=r(vLr,"DPRQuestionEncoderTokenizer"),vLr.forEach(t),hXe=r(t0," or "),fB=n(t0,"A",{href:!0});var TLr=s(fB);pXe=r(TLr,"DPRQuestionEncoderTokenizerFast"),TLr.forEach(t),_Xe=r(t0," (DPR model)"),t0.forEach(t),uXe=i(y),Zn=n(y,"LI",{});var a0=s(Zn);lH=n(a0,"STRONG",{});var FLr=s(lH);bXe=r(FLr,"electra"),FLr.forEach(t),vXe=r(a0," \u2014 "),mB=n(a0,"A",{href:!0});var CLr=s(mB);TXe=r(CLr,"ElectraTokenizer"),CLr.forEach(t),FXe=r(a0," or "),gB=n(a0,"A",{href:!0});var MLr=s(gB);CXe=r(MLr,"ElectraTokenizerFast"),MLr.forEach(t),MXe=r(a0," (ELECTRA model)"),a0.forEach(t),EXe=i(y),yg=n(y,"LI",{});var E6e=s(yg);iH=n(E6e,"STRONG",{});var ELr=s(iH);yXe=r(ELr,"flaubert"),ELr.forEach(t),wXe=r(E6e," \u2014 "),hB=n(E6e,"A",{href:!0});var yLr=s(hB);AXe=r(yLr,"FlaubertTokenizer"),yLr.forEach(t),LXe=r(E6e," (FlauBERT model)"),E6e.forEach(t),BXe=i(y),es=n(y,"LI",{});var n0=s(es);dH=n(n0,"STRONG",{});var wLr=s(dH);kXe=r(wLr,"fnet"),wLr.forEach(t),xXe=r(n0," \u2014 "),pB=n(n0,"A",{href:!0});var ALr=s(pB);RXe=r(ALr,"FNetTokenizer"),ALr.forEach(t),SXe=r(n0," or "),_B=n(n0,"A",{href:!0});var LLr=s(_B);PXe=r(LLr,"FNetTokenizerFast"),LLr.forEach(t),$Xe=r(n0," (FNet model)"),n0.forEach(t),IXe=i(y),wg=n(y,"LI",{});var y6e=s(wg);cH=n(y6e,"STRONG",{});var BLr=s(cH);jXe=r(BLr,"fsmt"),BLr.forEach(t),NXe=r(y6e," \u2014 "),uB=n(y6e,"A",{href:!0});var kLr=s(uB);DXe=r(kLr,"FSMTTokenizer"),kLr.forEach(t),qXe=r(y6e," (FairSeq Machine-Translation model)"),y6e.forEach(t),GXe=i(y),os=n(y,"LI",{});var s0=s(os);fH=n(s0,"STRONG",{});var xLr=s(fH);OXe=r(xLr,"funnel"),xLr.forEach(t),XXe=r(s0," \u2014 "),bB=n(s0,"A",{href:!0});var RLr=s(bB);zXe=r(RLr,"FunnelTokenizer"),RLr.forEach(t),VXe=r(s0," or "),vB=n(s0,"A",{href:!0});var SLr=s(vB);WXe=r(SLr,"FunnelTokenizerFast"),SLr.forEach(t),QXe=r(s0," (Funnel Transformer model)"),s0.forEach(t),HXe=i(y),rs=n(y,"LI",{});var l0=s(rs);mH=n(l0,"STRONG",{});var PLr=s(mH);UXe=r(PLr,"gpt2"),PLr.forEach(t),JXe=r(l0," \u2014 "),TB=n(l0,"A",{href:!0});var $Lr=s(TB);YXe=r($Lr,"GPT2Tokenizer"),$Lr.forEach(t),KXe=r(l0," or "),FB=n(l0,"A",{href:!0});var ILr=s(FB);ZXe=r(ILr,"GPT2TokenizerFast"),ILr.forEach(t),eze=r(l0," (OpenAI GPT-2 model)"),l0.forEach(t),oze=i(y),ts=n(y,"LI",{});var i0=s(ts);gH=n(i0,"STRONG",{});var jLr=s(gH);rze=r(jLr,"gpt_neo"),jLr.forEach(t),tze=r(i0," \u2014 "),CB=n(i0,"A",{href:!0});var NLr=s(CB);aze=r(NLr,"GPT2Tokenizer"),NLr.forEach(t),nze=r(i0," or "),MB=n(i0,"A",{href:!0});var DLr=s(MB);sze=r(DLr,"GPT2TokenizerFast"),DLr.forEach(t),lze=r(i0," (GPT Neo model)"),i0.forEach(t),ize=i(y),as=n(y,"LI",{});var d0=s(as);hH=n(d0,"STRONG",{});var qLr=s(hH);dze=r(qLr,"herbert"),qLr.forEach(t),cze=r(d0," \u2014 "),EB=n(d0,"A",{href:!0});var GLr=s(EB);fze=r(GLr,"HerbertTokenizer"),GLr.forEach(t),mze=r(d0," or "),yB=n(d0,"A",{href:!0});var OLr=s(yB);gze=r(OLr,"HerbertTokenizerFast"),OLr.forEach(t),hze=r(d0," (HerBERT model)"),d0.forEach(t),pze=i(y),Ag=n(y,"LI",{});var w6e=s(Ag);pH=n(w6e,"STRONG",{});var XLr=s(pH);_ze=r(XLr,"hubert"),XLr.forEach(t),uze=r(w6e," \u2014 "),wB=n(w6e,"A",{href:!0});var zLr=s(wB);bze=r(zLr,"Wav2Vec2CTCTokenizer"),zLr.forEach(t),vze=r(w6e," (Hubert model)"),w6e.forEach(t),Tze=i(y),ns=n(y,"LI",{});var c0=s(ns);_H=n(c0,"STRONG",{});var VLr=s(_H);Fze=r(VLr,"ibert"),VLr.forEach(t),Cze=r(c0," \u2014 "),AB=n(c0,"A",{href:!0});var WLr=s(AB);Mze=r(WLr,"RobertaTokenizer"),WLr.forEach(t),Eze=r(c0," or "),LB=n(c0,"A",{href:!0});var QLr=s(LB);yze=r(QLr,"RobertaTokenizerFast"),QLr.forEach(t),wze=r(c0," (I-BERT model)"),c0.forEach(t),Aze=i(y),ss=n(y,"LI",{});var f0=s(ss);uH=n(f0,"STRONG",{});var HLr=s(uH);Lze=r(HLr,"layoutlm"),HLr.forEach(t),Bze=r(f0," \u2014 "),BB=n(f0,"A",{href:!0});var ULr=s(BB);kze=r(ULr,"LayoutLMTokenizer"),ULr.forEach(t),xze=r(f0," or "),kB=n(f0,"A",{href:!0});var JLr=s(kB);Rze=r(JLr,"LayoutLMTokenizerFast"),JLr.forEach(t),Sze=r(f0," (LayoutLM model)"),f0.forEach(t),Pze=i(y),ls=n(y,"LI",{});var m0=s(ls);bH=n(m0,"STRONG",{});var YLr=s(bH);$ze=r(YLr,"layoutlmv2"),YLr.forEach(t),Ize=r(m0," \u2014 "),xB=n(m0,"A",{href:!0});var KLr=s(xB);jze=r(KLr,"LayoutLMv2Tokenizer"),KLr.forEach(t),Nze=r(m0," or "),RB=n(m0,"A",{href:!0});var ZLr=s(RB);Dze=r(ZLr,"LayoutLMv2TokenizerFast"),ZLr.forEach(t),qze=r(m0," (LayoutLMv2 model)"),m0.forEach(t),Gze=i(y),is=n(y,"LI",{});var g0=s(is);vH=n(g0,"STRONG",{});var e8r=s(vH);Oze=r(e8r,"layoutxlm"),e8r.forEach(t),Xze=r(g0," \u2014 "),SB=n(g0,"A",{href:!0});var o8r=s(SB);zze=r(o8r,"LayoutXLMTokenizer"),o8r.forEach(t),Vze=r(g0," or "),PB=n(g0,"A",{href:!0});var r8r=s(PB);Wze=r(r8r,"LayoutXLMTokenizerFast"),r8r.forEach(t),Qze=r(g0," (LayoutXLM model)"),g0.forEach(t),Hze=i(y),ds=n(y,"LI",{});var h0=s(ds);TH=n(h0,"STRONG",{});var t8r=s(TH);Uze=r(t8r,"led"),t8r.forEach(t),Jze=r(h0," \u2014 "),$B=n(h0,"A",{href:!0});var a8r=s($B);Yze=r(a8r,"LEDTokenizer"),a8r.forEach(t),Kze=r(h0," or "),IB=n(h0,"A",{href:!0});var n8r=s(IB);Zze=r(n8r,"LEDTokenizerFast"),n8r.forEach(t),eVe=r(h0," (LED model)"),h0.forEach(t),oVe=i(y),cs=n(y,"LI",{});var p0=s(cs);FH=n(p0,"STRONG",{});var s8r=s(FH);rVe=r(s8r,"longformer"),s8r.forEach(t),tVe=r(p0," \u2014 "),jB=n(p0,"A",{href:!0});var l8r=s(jB);aVe=r(l8r,"LongformerTokenizer"),l8r.forEach(t),nVe=r(p0," or "),NB=n(p0,"A",{href:!0});var i8r=s(NB);sVe=r(i8r,"LongformerTokenizerFast"),i8r.forEach(t),lVe=r(p0," (Longformer model)"),p0.forEach(t),iVe=i(y),Lg=n(y,"LI",{});var A6e=s(Lg);CH=n(A6e,"STRONG",{});var d8r=s(CH);dVe=r(d8r,"luke"),d8r.forEach(t),cVe=r(A6e," \u2014 "),DB=n(A6e,"A",{href:!0});var c8r=s(DB);fVe=r(c8r,"LukeTokenizer"),c8r.forEach(t),mVe=r(A6e," (LUKE model)"),A6e.forEach(t),gVe=i(y),fs=n(y,"LI",{});var _0=s(fs);MH=n(_0,"STRONG",{});var f8r=s(MH);hVe=r(f8r,"lxmert"),f8r.forEach(t),pVe=r(_0," \u2014 "),qB=n(_0,"A",{href:!0});var m8r=s(qB);_Ve=r(m8r,"LxmertTokenizer"),m8r.forEach(t),uVe=r(_0," or "),GB=n(_0,"A",{href:!0});var g8r=s(GB);bVe=r(g8r,"LxmertTokenizerFast"),g8r.forEach(t),vVe=r(_0," (LXMERT model)"),_0.forEach(t),TVe=i(y),Bg=n(y,"LI",{});var L6e=s(Bg);EH=n(L6e,"STRONG",{});var h8r=s(EH);FVe=r(h8r,"m2m_100"),h8r.forEach(t),CVe=r(L6e," \u2014 "),OB=n(L6e,"A",{href:!0});var p8r=s(OB);MVe=r(p8r,"M2M100Tokenizer"),p8r.forEach(t),EVe=r(L6e," (M2M100 model)"),L6e.forEach(t),yVe=i(y),kg=n(y,"LI",{});var B6e=s(kg);yH=n(B6e,"STRONG",{});var _8r=s(yH);wVe=r(_8r,"marian"),_8r.forEach(t),AVe=r(B6e," \u2014 "),XB=n(B6e,"A",{href:!0});var u8r=s(XB);LVe=r(u8r,"MarianTokenizer"),u8r.forEach(t),BVe=r(B6e," (Marian model)"),B6e.forEach(t),kVe=i(y),ms=n(y,"LI",{});var u0=s(ms);wH=n(u0,"STRONG",{});var b8r=s(wH);xVe=r(b8r,"mbart"),b8r.forEach(t),RVe=r(u0," \u2014 "),zB=n(u0,"A",{href:!0});var v8r=s(zB);SVe=r(v8r,"MBartTokenizer"),v8r.forEach(t),PVe=r(u0," or "),VB=n(u0,"A",{href:!0});var T8r=s(VB);$Ve=r(T8r,"MBartTokenizerFast"),T8r.forEach(t),IVe=r(u0," (mBART model)"),u0.forEach(t),jVe=i(y),gs=n(y,"LI",{});var b0=s(gs);AH=n(b0,"STRONG",{});var F8r=s(AH);NVe=r(F8r,"mbart50"),F8r.forEach(t),DVe=r(b0," \u2014 "),WB=n(b0,"A",{href:!0});var C8r=s(WB);qVe=r(C8r,"MBart50Tokenizer"),C8r.forEach(t),GVe=r(b0," or "),QB=n(b0,"A",{href:!0});var M8r=s(QB);OVe=r(M8r,"MBart50TokenizerFast"),M8r.forEach(t),XVe=r(b0," (mBART-50 model)"),b0.forEach(t),zVe=i(y),xg=n(y,"LI",{});var k6e=s(xg);LH=n(k6e,"STRONG",{});var E8r=s(LH);VVe=r(E8r,"mluke"),E8r.forEach(t),WVe=r(k6e," \u2014 "),HB=n(k6e,"A",{href:!0});var y8r=s(HB);QVe=r(y8r,"MLukeTokenizer"),y8r.forEach(t),HVe=r(k6e," (mLUKE model)"),k6e.forEach(t),UVe=i(y),hs=n(y,"LI",{});var v0=s(hs);BH=n(v0,"STRONG",{});var w8r=s(BH);JVe=r(w8r,"mobilebert"),w8r.forEach(t),YVe=r(v0," \u2014 "),UB=n(v0,"A",{href:!0});var A8r=s(UB);KVe=r(A8r,"MobileBertTokenizer"),A8r.forEach(t),ZVe=r(v0," or "),JB=n(v0,"A",{href:!0});var L8r=s(JB);eWe=r(L8r,"MobileBertTokenizerFast"),L8r.forEach(t),oWe=r(v0," (MobileBERT model)"),v0.forEach(t),rWe=i(y),ps=n(y,"LI",{});var T0=s(ps);kH=n(T0,"STRONG",{});var B8r=s(kH);tWe=r(B8r,"mpnet"),B8r.forEach(t),aWe=r(T0," \u2014 "),YB=n(T0,"A",{href:!0});var k8r=s(YB);nWe=r(k8r,"MPNetTokenizer"),k8r.forEach(t),sWe=r(T0," or "),KB=n(T0,"A",{href:!0});var x8r=s(KB);lWe=r(x8r,"MPNetTokenizerFast"),x8r.forEach(t),iWe=r(T0," (MPNet model)"),T0.forEach(t),dWe=i(y),_s=n(y,"LI",{});var F0=s(_s);xH=n(F0,"STRONG",{});var R8r=s(xH);cWe=r(R8r,"mt5"),R8r.forEach(t),fWe=r(F0," \u2014 "),ZB=n(F0,"A",{href:!0});var S8r=s(ZB);mWe=r(S8r,"MT5Tokenizer"),S8r.forEach(t),gWe=r(F0," or "),ek=n(F0,"A",{href:!0});var P8r=s(ek);hWe=r(P8r,"MT5TokenizerFast"),P8r.forEach(t),pWe=r(F0," (mT5 model)"),F0.forEach(t),_We=i(y),us=n(y,"LI",{});var C0=s(us);RH=n(C0,"STRONG",{});var $8r=s(RH);uWe=r($8r,"openai-gpt"),$8r.forEach(t),bWe=r(C0," \u2014 "),ok=n(C0,"A",{href:!0});var I8r=s(ok);vWe=r(I8r,"OpenAIGPTTokenizer"),I8r.forEach(t),TWe=r(C0," or "),rk=n(C0,"A",{href:!0});var j8r=s(rk);FWe=r(j8r,"OpenAIGPTTokenizerFast"),j8r.forEach(t),CWe=r(C0," (OpenAI GPT model)"),C0.forEach(t),MWe=i(y),bs=n(y,"LI",{});var M0=s(bs);SH=n(M0,"STRONG",{});var N8r=s(SH);EWe=r(N8r,"pegasus"),N8r.forEach(t),yWe=r(M0," \u2014 "),tk=n(M0,"A",{href:!0});var D8r=s(tk);wWe=r(D8r,"PegasusTokenizer"),D8r.forEach(t),AWe=r(M0," or "),ak=n(M0,"A",{href:!0});var q8r=s(ak);LWe=r(q8r,"PegasusTokenizerFast"),q8r.forEach(t),BWe=r(M0," (Pegasus model)"),M0.forEach(t),kWe=i(y),Rg=n(y,"LI",{});var x6e=s(Rg);PH=n(x6e,"STRONG",{});var G8r=s(PH);xWe=r(G8r,"perceiver"),G8r.forEach(t),RWe=r(x6e," \u2014 "),nk=n(x6e,"A",{href:!0});var O8r=s(nk);SWe=r(O8r,"PerceiverTokenizer"),O8r.forEach(t),PWe=r(x6e," (Perceiver model)"),x6e.forEach(t),$We=i(y),Sg=n(y,"LI",{});var R6e=s(Sg);$H=n(R6e,"STRONG",{});var X8r=s($H);IWe=r(X8r,"phobert"),X8r.forEach(t),jWe=r(R6e," \u2014 "),sk=n(R6e,"A",{href:!0});var z8r=s(sk);NWe=r(z8r,"PhobertTokenizer"),z8r.forEach(t),DWe=r(R6e," (PhoBERT model)"),R6e.forEach(t),qWe=i(y),Pg=n(y,"LI",{});var S6e=s(Pg);IH=n(S6e,"STRONG",{});var V8r=s(IH);GWe=r(V8r,"plbart"),V8r.forEach(t),OWe=r(S6e," \u2014 "),lk=n(S6e,"A",{href:!0});var W8r=s(lk);XWe=r(W8r,"PLBartTokenizer"),W8r.forEach(t),zWe=r(S6e," (PLBart model)"),S6e.forEach(t),VWe=i(y),$g=n(y,"LI",{});var P6e=s($g);jH=n(P6e,"STRONG",{});var Q8r=s(jH);WWe=r(Q8r,"prophetnet"),Q8r.forEach(t),QWe=r(P6e," \u2014 "),ik=n(P6e,"A",{href:!0});var H8r=s(ik);HWe=r(H8r,"ProphetNetTokenizer"),H8r.forEach(t),UWe=r(P6e," (ProphetNet model)"),P6e.forEach(t),JWe=i(y),vs=n(y,"LI",{});var E0=s(vs);NH=n(E0,"STRONG",{});var U8r=s(NH);YWe=r(U8r,"qdqbert"),U8r.forEach(t),KWe=r(E0," \u2014 "),dk=n(E0,"A",{href:!0});var J8r=s(dk);ZWe=r(J8r,"BertTokenizer"),J8r.forEach(t),eQe=r(E0," or "),ck=n(E0,"A",{href:!0});var Y8r=s(ck);oQe=r(Y8r,"BertTokenizerFast"),Y8r.forEach(t),rQe=r(E0," (QDQBert model)"),E0.forEach(t),tQe=i(y),Ig=n(y,"LI",{});var $6e=s(Ig);DH=n($6e,"STRONG",{});var K8r=s(DH);aQe=r(K8r,"rag"),K8r.forEach(t),nQe=r($6e," \u2014 "),fk=n($6e,"A",{href:!0});var Z8r=s(fk);sQe=r(Z8r,"RagTokenizer"),Z8r.forEach(t),lQe=r($6e," (RAG model)"),$6e.forEach(t),iQe=i(y),Ts=n(y,"LI",{});var y0=s(Ts);qH=n(y0,"STRONG",{});var eBr=s(qH);dQe=r(eBr,"reformer"),eBr.forEach(t),cQe=r(y0," \u2014 "),mk=n(y0,"A",{href:!0});var oBr=s(mk);fQe=r(oBr,"ReformerTokenizer"),oBr.forEach(t),mQe=r(y0," or "),gk=n(y0,"A",{href:!0});var rBr=s(gk);gQe=r(rBr,"ReformerTokenizerFast"),rBr.forEach(t),hQe=r(y0," (Reformer model)"),y0.forEach(t),pQe=i(y),Fs=n(y,"LI",{});var w0=s(Fs);GH=n(w0,"STRONG",{});var tBr=s(GH);_Qe=r(tBr,"rembert"),tBr.forEach(t),uQe=r(w0," \u2014 "),hk=n(w0,"A",{href:!0});var aBr=s(hk);bQe=r(aBr,"RemBertTokenizer"),aBr.forEach(t),vQe=r(w0," or "),pk=n(w0,"A",{href:!0});var nBr=s(pk);TQe=r(nBr,"RemBertTokenizerFast"),nBr.forEach(t),FQe=r(w0," (RemBERT model)"),w0.forEach(t),CQe=i(y),Cs=n(y,"LI",{});var A0=s(Cs);OH=n(A0,"STRONG",{});var sBr=s(OH);MQe=r(sBr,"retribert"),sBr.forEach(t),EQe=r(A0," \u2014 "),_k=n(A0,"A",{href:!0});var lBr=s(_k);yQe=r(lBr,"RetriBertTokenizer"),lBr.forEach(t),wQe=r(A0," or "),uk=n(A0,"A",{href:!0});var iBr=s(uk);AQe=r(iBr,"RetriBertTokenizerFast"),iBr.forEach(t),LQe=r(A0," (RetriBERT model)"),A0.forEach(t),BQe=i(y),Ms=n(y,"LI",{});var L0=s(Ms);XH=n(L0,"STRONG",{});var dBr=s(XH);kQe=r(dBr,"roberta"),dBr.forEach(t),xQe=r(L0," \u2014 "),bk=n(L0,"A",{href:!0});var cBr=s(bk);RQe=r(cBr,"RobertaTokenizer"),cBr.forEach(t),SQe=r(L0," or "),vk=n(L0,"A",{href:!0});var fBr=s(vk);PQe=r(fBr,"RobertaTokenizerFast"),fBr.forEach(t),$Qe=r(L0," (RoBERTa model)"),L0.forEach(t),IQe=i(y),Es=n(y,"LI",{});var B0=s(Es);zH=n(B0,"STRONG",{});var mBr=s(zH);jQe=r(mBr,"roformer"),mBr.forEach(t),NQe=r(B0," \u2014 "),Tk=n(B0,"A",{href:!0});var gBr=s(Tk);DQe=r(gBr,"RoFormerTokenizer"),gBr.forEach(t),qQe=r(B0," or "),Fk=n(B0,"A",{href:!0});var hBr=s(Fk);GQe=r(hBr,"RoFormerTokenizerFast"),hBr.forEach(t),OQe=r(B0," (RoFormer model)"),B0.forEach(t),XQe=i(y),jg=n(y,"LI",{});var I6e=s(jg);VH=n(I6e,"STRONG",{});var pBr=s(VH);zQe=r(pBr,"speech_to_text"),pBr.forEach(t),VQe=r(I6e," \u2014 "),Ck=n(I6e,"A",{href:!0});var _Br=s(Ck);WQe=r(_Br,"Speech2TextTokenizer"),_Br.forEach(t),QQe=r(I6e," (Speech2Text model)"),I6e.forEach(t),HQe=i(y),Ng=n(y,"LI",{});var j6e=s(Ng);WH=n(j6e,"STRONG",{});var uBr=s(WH);UQe=r(uBr,"speech_to_text_2"),uBr.forEach(t),JQe=r(j6e," \u2014 "),Mk=n(j6e,"A",{href:!0});var bBr=s(Mk);YQe=r(bBr,"Speech2Text2Tokenizer"),bBr.forEach(t),KQe=r(j6e," (Speech2Text2 model)"),j6e.forEach(t),ZQe=i(y),ys=n(y,"LI",{});var k0=s(ys);QH=n(k0,"STRONG",{});var vBr=s(QH);eHe=r(vBr,"splinter"),vBr.forEach(t),oHe=r(k0," \u2014 "),Ek=n(k0,"A",{href:!0});var TBr=s(Ek);rHe=r(TBr,"SplinterTokenizer"),TBr.forEach(t),tHe=r(k0," or "),yk=n(k0,"A",{href:!0});var FBr=s(yk);aHe=r(FBr,"SplinterTokenizerFast"),FBr.forEach(t),nHe=r(k0," (Splinter model)"),k0.forEach(t),sHe=i(y),ws=n(y,"LI",{});var x0=s(ws);HH=n(x0,"STRONG",{});var CBr=s(HH);lHe=r(CBr,"squeezebert"),CBr.forEach(t),iHe=r(x0," \u2014 "),wk=n(x0,"A",{href:!0});var MBr=s(wk);dHe=r(MBr,"SqueezeBertTokenizer"),MBr.forEach(t),cHe=r(x0," or "),Ak=n(x0,"A",{href:!0});var EBr=s(Ak);fHe=r(EBr,"SqueezeBertTokenizerFast"),EBr.forEach(t),mHe=r(x0," (SqueezeBERT model)"),x0.forEach(t),gHe=i(y),As=n(y,"LI",{});var R0=s(As);UH=n(R0,"STRONG",{});var yBr=s(UH);hHe=r(yBr,"t5"),yBr.forEach(t),pHe=r(R0," \u2014 "),Lk=n(R0,"A",{href:!0});var wBr=s(Lk);_He=r(wBr,"T5Tokenizer"),wBr.forEach(t),uHe=r(R0," or "),Bk=n(R0,"A",{href:!0});var ABr=s(Bk);bHe=r(ABr,"T5TokenizerFast"),ABr.forEach(t),vHe=r(R0," (T5 model)"),R0.forEach(t),THe=i(y),Dg=n(y,"LI",{});var N6e=s(Dg);JH=n(N6e,"STRONG",{});var LBr=s(JH);FHe=r(LBr,"tapas"),LBr.forEach(t),CHe=r(N6e," \u2014 "),kk=n(N6e,"A",{href:!0});var BBr=s(kk);MHe=r(BBr,"TapasTokenizer"),BBr.forEach(t),EHe=r(N6e," (TAPAS model)"),N6e.forEach(t),yHe=i(y),qg=n(y,"LI",{});var D6e=s(qg);YH=n(D6e,"STRONG",{});var kBr=s(YH);wHe=r(kBr,"transfo-xl"),kBr.forEach(t),AHe=r(D6e," \u2014 "),xk=n(D6e,"A",{href:!0});var xBr=s(xk);LHe=r(xBr,"TransfoXLTokenizer"),xBr.forEach(t),BHe=r(D6e," (Transformer-XL model)"),D6e.forEach(t),kHe=i(y),Gg=n(y,"LI",{});var q6e=s(Gg);KH=n(q6e,"STRONG",{});var RBr=s(KH);xHe=r(RBr,"wav2vec2"),RBr.forEach(t),RHe=r(q6e," \u2014 "),Rk=n(q6e,"A",{href:!0});var SBr=s(Rk);SHe=r(SBr,"Wav2Vec2CTCTokenizer"),SBr.forEach(t),PHe=r(q6e," (Wav2Vec2 model)"),q6e.forEach(t),$He=i(y),Og=n(y,"LI",{});var G6e=s(Og);ZH=n(G6e,"STRONG",{});var PBr=s(ZH);IHe=r(PBr,"wav2vec2_phoneme"),PBr.forEach(t),jHe=r(G6e," \u2014 "),Sk=n(G6e,"A",{href:!0});var $Br=s(Sk);NHe=r($Br,"Wav2Vec2PhonemeCTCTokenizer"),$Br.forEach(t),DHe=r(G6e," (Wav2Vec2Phoneme model)"),G6e.forEach(t),qHe=i(y),Ls=n(y,"LI",{});var S0=s(Ls);eU=n(S0,"STRONG",{});var IBr=s(eU);GHe=r(IBr,"xglm"),IBr.forEach(t),OHe=r(S0," \u2014 "),Pk=n(S0,"A",{href:!0});var jBr=s(Pk);XHe=r(jBr,"XGLMTokenizer"),jBr.forEach(t),zHe=r(S0," or "),$k=n(S0,"A",{href:!0});var NBr=s($k);VHe=r(NBr,"XGLMTokenizerFast"),NBr.forEach(t),WHe=r(S0," (XGLM model)"),S0.forEach(t),QHe=i(y),Xg=n(y,"LI",{});var O6e=s(Xg);oU=n(O6e,"STRONG",{});var DBr=s(oU);HHe=r(DBr,"xlm"),DBr.forEach(t),UHe=r(O6e," \u2014 "),Ik=n(O6e,"A",{href:!0});var qBr=s(Ik);JHe=r(qBr,"XLMTokenizer"),qBr.forEach(t),YHe=r(O6e," (XLM model)"),O6e.forEach(t),KHe=i(y),zg=n(y,"LI",{});var X6e=s(zg);rU=n(X6e,"STRONG",{});var GBr=s(rU);ZHe=r(GBr,"xlm-prophetnet"),GBr.forEach(t),eUe=r(X6e," \u2014 "),jk=n(X6e,"A",{href:!0});var OBr=s(jk);oUe=r(OBr,"XLMProphetNetTokenizer"),OBr.forEach(t),rUe=r(X6e," (XLMProphetNet model)"),X6e.forEach(t),tUe=i(y),Bs=n(y,"LI",{});var P0=s(Bs);tU=n(P0,"STRONG",{});var XBr=s(tU);aUe=r(XBr,"xlm-roberta"),XBr.forEach(t),nUe=r(P0," \u2014 "),Nk=n(P0,"A",{href:!0});var zBr=s(Nk);sUe=r(zBr,"XLMRobertaTokenizer"),zBr.forEach(t),lUe=r(P0," or "),Dk=n(P0,"A",{href:!0});var VBr=s(Dk);iUe=r(VBr,"XLMRobertaTokenizerFast"),VBr.forEach(t),dUe=r(P0," (XLM-RoBERTa model)"),P0.forEach(t),cUe=i(y),ks=n(y,"LI",{});var $0=s(ks);aU=n($0,"STRONG",{});var WBr=s(aU);fUe=r(WBr,"xlnet"),WBr.forEach(t),mUe=r($0," \u2014 "),qk=n($0,"A",{href:!0});var QBr=s(qk);gUe=r(QBr,"XLNetTokenizer"),QBr.forEach(t),hUe=r($0," or "),Gk=n($0,"A",{href:!0});var HBr=s(Gk);pUe=r(HBr,"XLNetTokenizerFast"),HBr.forEach(t),_Ue=r($0," (XLNet model)"),$0.forEach(t),y.forEach(t),uUe=i(da),nU=n(da,"P",{});var UBr=s(nU);bUe=r(UBr,"Examples:"),UBr.forEach(t),vUe=i(da),m(v4.$$.fragment,da),da.forEach(t),TUe=i($s),Vg=n($s,"DIV",{class:!0});var hBe=s(Vg);m(T4.$$.fragment,hBe),FUe=i(hBe),sU=n(hBe,"P",{});var JBr=s(sU);CUe=r(JBr,"Register a new tokenizer in this mapping."),JBr.forEach(t),hBe.forEach(t),$s.forEach(t),mLe=i(d),ji=n(d,"H2",{class:!0});var pBe=s(ji);Wg=n(pBe,"A",{id:!0,class:!0,href:!0});var YBr=s(Wg);lU=n(YBr,"SPAN",{});var KBr=s(lU);m(F4.$$.fragment,KBr),KBr.forEach(t),YBr.forEach(t),MUe=i(pBe),iU=n(pBe,"SPAN",{});var ZBr=s(iU);EUe=r(ZBr,"AutoFeatureExtractor"),ZBr.forEach(t),pBe.forEach(t),gLe=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(C4.$$.fragment,Is),yUe=i(Is),M4=n(Is,"P",{});var _Be=s(M4);wUe=r(_Be,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=n(_Be,"A",{href:!0});var ekr=s(Ok);AUe=r(ekr,"AutoFeatureExtractor.from_pretrained()"),ekr.forEach(t),LUe=r(_Be," class method."),_Be.forEach(t),BUe=i(Is),E4=n(Is,"P",{});var uBe=s(E4);kUe=r(uBe,"This class cannot be instantiated directly using "),dU=n(uBe,"CODE",{});var okr=s(dU);xUe=r(okr,"__init__()"),okr.forEach(t),RUe=r(uBe," (throws an error)."),uBe.forEach(t),SUe=i(Is),Le=n(Is,"DIV",{class:!0});var xt=s(Le);m(y4.$$.fragment,xt),PUe=i(xt),cU=n(xt,"P",{});var rkr=s(cU);$Ue=r(rkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),rkr.forEach(t),IUe=i(xt),Na=n(xt,"P",{});var sC=s(Na);jUe=r(sC,"The feature extractor class to instantiate is selected based on the "),fU=n(sC,"CODE",{});var tkr=s(fU);NUe=r(tkr,"model_type"),tkr.forEach(t),DUe=r(sC,` property of the config object
(either passed as an argument or loaded from `),mU=n(sC,"CODE",{});var akr=s(mU);qUe=r(akr,"pretrained_model_name_or_path"),akr.forEach(t),GUe=r(sC,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=n(sC,"CODE",{});var nkr=s(gU);OUe=r(nkr,"pretrained_model_name_or_path"),nkr.forEach(t),XUe=r(sC,":"),sC.forEach(t),zUe=i(xt),se=n(xt,"UL",{});var de=s(se);Qg=n(de,"LI",{});var z6e=s(Qg);hU=n(z6e,"STRONG",{});var skr=s(hU);VUe=r(skr,"beit"),skr.forEach(t),WUe=r(z6e," \u2014 "),Xk=n(z6e,"A",{href:!0});var lkr=s(Xk);QUe=r(lkr,"BeitFeatureExtractor"),lkr.forEach(t),HUe=r(z6e," (BEiT model)"),z6e.forEach(t),UUe=i(de),Hg=n(de,"LI",{});var V6e=s(Hg);pU=n(V6e,"STRONG",{});var ikr=s(pU);JUe=r(ikr,"clip"),ikr.forEach(t),YUe=r(V6e," \u2014 "),zk=n(V6e,"A",{href:!0});var dkr=s(zk);KUe=r(dkr,"CLIPFeatureExtractor"),dkr.forEach(t),ZUe=r(V6e," (CLIP model)"),V6e.forEach(t),eJe=i(de),Ug=n(de,"LI",{});var W6e=s(Ug);_U=n(W6e,"STRONG",{});var ckr=s(_U);oJe=r(ckr,"convnext"),ckr.forEach(t),rJe=r(W6e," \u2014 "),Vk=n(W6e,"A",{href:!0});var fkr=s(Vk);tJe=r(fkr,"ConvNextFeatureExtractor"),fkr.forEach(t),aJe=r(W6e," (ConvNext model)"),W6e.forEach(t),nJe=i(de),Jg=n(de,"LI",{});var Q6e=s(Jg);uU=n(Q6e,"STRONG",{});var mkr=s(uU);sJe=r(mkr,"deit"),mkr.forEach(t),lJe=r(Q6e," \u2014 "),Wk=n(Q6e,"A",{href:!0});var gkr=s(Wk);iJe=r(gkr,"DeiTFeatureExtractor"),gkr.forEach(t),dJe=r(Q6e," (DeiT model)"),Q6e.forEach(t),cJe=i(de),Yg=n(de,"LI",{});var H6e=s(Yg);bU=n(H6e,"STRONG",{});var hkr=s(bU);fJe=r(hkr,"detr"),hkr.forEach(t),mJe=r(H6e," \u2014 "),Qk=n(H6e,"A",{href:!0});var pkr=s(Qk);gJe=r(pkr,"DetrFeatureExtractor"),pkr.forEach(t),hJe=r(H6e," (DETR model)"),H6e.forEach(t),pJe=i(de),Kg=n(de,"LI",{});var U6e=s(Kg);vU=n(U6e,"STRONG",{});var _kr=s(vU);_Je=r(_kr,"hubert"),_kr.forEach(t),uJe=r(U6e," \u2014 "),Hk=n(U6e,"A",{href:!0});var ukr=s(Hk);bJe=r(ukr,"Wav2Vec2FeatureExtractor"),ukr.forEach(t),vJe=r(U6e," (Hubert model)"),U6e.forEach(t),TJe=i(de),Zg=n(de,"LI",{});var J6e=s(Zg);TU=n(J6e,"STRONG",{});var bkr=s(TU);FJe=r(bkr,"layoutlmv2"),bkr.forEach(t),CJe=r(J6e," \u2014 "),Uk=n(J6e,"A",{href:!0});var vkr=s(Uk);MJe=r(vkr,"LayoutLMv2FeatureExtractor"),vkr.forEach(t),EJe=r(J6e," (LayoutLMv2 model)"),J6e.forEach(t),yJe=i(de),eh=n(de,"LI",{});var Y6e=s(eh);FU=n(Y6e,"STRONG",{});var Tkr=s(FU);wJe=r(Tkr,"perceiver"),Tkr.forEach(t),AJe=r(Y6e," \u2014 "),Jk=n(Y6e,"A",{href:!0});var Fkr=s(Jk);LJe=r(Fkr,"PerceiverFeatureExtractor"),Fkr.forEach(t),BJe=r(Y6e," (Perceiver model)"),Y6e.forEach(t),kJe=i(de),oh=n(de,"LI",{});var K6e=s(oh);CU=n(K6e,"STRONG",{});var Ckr=s(CU);xJe=r(Ckr,"poolformer"),Ckr.forEach(t),RJe=r(K6e," \u2014 "),Yk=n(K6e,"A",{href:!0});var Mkr=s(Yk);SJe=r(Mkr,"PoolFormerFeatureExtractor"),Mkr.forEach(t),PJe=r(K6e," (PoolFormer model)"),K6e.forEach(t),$Je=i(de),rh=n(de,"LI",{});var Z6e=s(rh);MU=n(Z6e,"STRONG",{});var Ekr=s(MU);IJe=r(Ekr,"segformer"),Ekr.forEach(t),jJe=r(Z6e," \u2014 "),Kk=n(Z6e,"A",{href:!0});var ykr=s(Kk);NJe=r(ykr,"SegformerFeatureExtractor"),ykr.forEach(t),DJe=r(Z6e," (SegFormer model)"),Z6e.forEach(t),qJe=i(de),th=n(de,"LI",{});var eTe=s(th);EU=n(eTe,"STRONG",{});var wkr=s(EU);GJe=r(wkr,"speech_to_text"),wkr.forEach(t),OJe=r(eTe," \u2014 "),Zk=n(eTe,"A",{href:!0});var Akr=s(Zk);XJe=r(Akr,"Speech2TextFeatureExtractor"),Akr.forEach(t),zJe=r(eTe," (Speech2Text model)"),eTe.forEach(t),VJe=i(de),ah=n(de,"LI",{});var oTe=s(ah);yU=n(oTe,"STRONG",{});var Lkr=s(yU);WJe=r(Lkr,"swin"),Lkr.forEach(t),QJe=r(oTe," \u2014 "),ex=n(oTe,"A",{href:!0});var Bkr=s(ex);HJe=r(Bkr,"ViTFeatureExtractor"),Bkr.forEach(t),UJe=r(oTe," (Swin model)"),oTe.forEach(t),JJe=i(de),nh=n(de,"LI",{});var rTe=s(nh);wU=n(rTe,"STRONG",{});var kkr=s(wU);YJe=r(kkr,"vit"),kkr.forEach(t),KJe=r(rTe," \u2014 "),ox=n(rTe,"A",{href:!0});var xkr=s(ox);ZJe=r(xkr,"ViTFeatureExtractor"),xkr.forEach(t),eYe=r(rTe," (ViT model)"),rTe.forEach(t),oYe=i(de),sh=n(de,"LI",{});var tTe=s(sh);AU=n(tTe,"STRONG",{});var Rkr=s(AU);rYe=r(Rkr,"vit_mae"),Rkr.forEach(t),tYe=r(tTe," \u2014 "),rx=n(tTe,"A",{href:!0});var Skr=s(rx);aYe=r(Skr,"ViTFeatureExtractor"),Skr.forEach(t),nYe=r(tTe," (ViTMAE model)"),tTe.forEach(t),sYe=i(de),lh=n(de,"LI",{});var aTe=s(lh);LU=n(aTe,"STRONG",{});var Pkr=s(LU);lYe=r(Pkr,"wav2vec2"),Pkr.forEach(t),iYe=r(aTe," \u2014 "),tx=n(aTe,"A",{href:!0});var $kr=s(tx);dYe=r($kr,"Wav2Vec2FeatureExtractor"),$kr.forEach(t),cYe=r(aTe," (Wav2Vec2 model)"),aTe.forEach(t),de.forEach(t),fYe=i(xt),m(ih.$$.fragment,xt),mYe=i(xt),BU=n(xt,"P",{});var Ikr=s(BU);gYe=r(Ikr,"Examples:"),Ikr.forEach(t),hYe=i(xt),m(w4.$$.fragment,xt),xt.forEach(t),pYe=i(Is),dh=n(Is,"DIV",{class:!0});var bBe=s(dh);m(A4.$$.fragment,bBe),_Ye=i(bBe),kU=n(bBe,"P",{});var jkr=s(kU);uYe=r(jkr,"Register a new feature extractor for this class."),jkr.forEach(t),bBe.forEach(t),Is.forEach(t),hLe=i(d),Ni=n(d,"H2",{class:!0});var vBe=s(Ni);ch=n(vBe,"A",{id:!0,class:!0,href:!0});var Nkr=s(ch);xU=n(Nkr,"SPAN",{});var Dkr=s(xU);m(L4.$$.fragment,Dkr),Dkr.forEach(t),Nkr.forEach(t),bYe=i(vBe),RU=n(vBe,"SPAN",{});var qkr=s(RU);vYe=r(qkr,"AutoProcessor"),qkr.forEach(t),vBe.forEach(t),pLe=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(B4.$$.fragment,js),TYe=i(js),k4=n(js,"P",{});var TBe=s(k4);FYe=r(TBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=n(TBe,"A",{href:!0});var Gkr=s(ax);CYe=r(Gkr,"AutoProcessor.from_pretrained()"),Gkr.forEach(t),MYe=r(TBe," class method."),TBe.forEach(t),EYe=i(js),x4=n(js,"P",{});var FBe=s(x4);yYe=r(FBe,"This class cannot be instantiated directly using "),SU=n(FBe,"CODE",{});var Okr=s(SU);wYe=r(Okr,"__init__()"),Okr.forEach(t),AYe=r(FBe," (throws an error)."),FBe.forEach(t),LYe=i(js),Be=n(js,"DIV",{class:!0});var Rt=s(Be);m(R4.$$.fragment,Rt),BYe=i(Rt),PU=n(Rt,"P",{});var Xkr=s(PU);kYe=r(Xkr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Xkr.forEach(t),xYe=i(Rt),Di=n(Rt,"P",{});var VX=s(Di);RYe=r(VX,"The processor class to instantiate is selected based on the "),$U=n(VX,"CODE",{});var zkr=s($U);SYe=r(zkr,"model_type"),zkr.forEach(t),PYe=r(VX,` property of the config object (either
passed as an argument or loaded from `),IU=n(VX,"CODE",{});var Vkr=s(IU);$Ye=r(Vkr,"pretrained_model_name_or_path"),Vkr.forEach(t),IYe=r(VX," if possible):"),VX.forEach(t),jYe=i(Rt),we=n(Rt,"UL",{});var No=s(we);fh=n(No,"LI",{});var nTe=s(fh);jU=n(nTe,"STRONG",{});var Wkr=s(jU);NYe=r(Wkr,"clip"),Wkr.forEach(t),DYe=r(nTe," \u2014 "),nx=n(nTe,"A",{href:!0});var Qkr=s(nx);qYe=r(Qkr,"CLIPProcessor"),Qkr.forEach(t),GYe=r(nTe," (CLIP model)"),nTe.forEach(t),OYe=i(No),mh=n(No,"LI",{});var sTe=s(mh);NU=n(sTe,"STRONG",{});var Hkr=s(NU);XYe=r(Hkr,"layoutlmv2"),Hkr.forEach(t),zYe=r(sTe," \u2014 "),sx=n(sTe,"A",{href:!0});var Ukr=s(sx);VYe=r(Ukr,"LayoutLMv2Processor"),Ukr.forEach(t),WYe=r(sTe," (LayoutLMv2 model)"),sTe.forEach(t),QYe=i(No),gh=n(No,"LI",{});var lTe=s(gh);DU=n(lTe,"STRONG",{});var Jkr=s(DU);HYe=r(Jkr,"layoutxlm"),Jkr.forEach(t),UYe=r(lTe," \u2014 "),lx=n(lTe,"A",{href:!0});var Ykr=s(lx);JYe=r(Ykr,"LayoutXLMProcessor"),Ykr.forEach(t),YYe=r(lTe," (LayoutXLM model)"),lTe.forEach(t),KYe=i(No),hh=n(No,"LI",{});var iTe=s(hh);qU=n(iTe,"STRONG",{});var Kkr=s(qU);ZYe=r(Kkr,"speech_to_text"),Kkr.forEach(t),eKe=r(iTe," \u2014 "),ix=n(iTe,"A",{href:!0});var Zkr=s(ix);oKe=r(Zkr,"Speech2TextProcessor"),Zkr.forEach(t),rKe=r(iTe," (Speech2Text model)"),iTe.forEach(t),tKe=i(No),ph=n(No,"LI",{});var dTe=s(ph);GU=n(dTe,"STRONG",{});var exr=s(GU);aKe=r(exr,"speech_to_text_2"),exr.forEach(t),nKe=r(dTe," \u2014 "),dx=n(dTe,"A",{href:!0});var oxr=s(dx);sKe=r(oxr,"Speech2Text2Processor"),oxr.forEach(t),lKe=r(dTe," (Speech2Text2 model)"),dTe.forEach(t),iKe=i(No),_h=n(No,"LI",{});var cTe=s(_h);OU=n(cTe,"STRONG",{});var rxr=s(OU);dKe=r(rxr,"trocr"),rxr.forEach(t),cKe=r(cTe," \u2014 "),cx=n(cTe,"A",{href:!0});var txr=s(cx);fKe=r(txr,"TrOCRProcessor"),txr.forEach(t),mKe=r(cTe," (TrOCR model)"),cTe.forEach(t),gKe=i(No),uh=n(No,"LI",{});var fTe=s(uh);XU=n(fTe,"STRONG",{});var axr=s(XU);hKe=r(axr,"vision-text-dual-encoder"),axr.forEach(t),pKe=r(fTe," \u2014 "),fx=n(fTe,"A",{href:!0});var nxr=s(fx);_Ke=r(nxr,"VisionTextDualEncoderProcessor"),nxr.forEach(t),uKe=r(fTe," (VisionTextDualEncoder model)"),fTe.forEach(t),bKe=i(No),bh=n(No,"LI",{});var mTe=s(bh);zU=n(mTe,"STRONG",{});var sxr=s(zU);vKe=r(sxr,"wav2vec2"),sxr.forEach(t),TKe=r(mTe," \u2014 "),mx=n(mTe,"A",{href:!0});var lxr=s(mx);FKe=r(lxr,"Wav2Vec2Processor"),lxr.forEach(t),CKe=r(mTe," (Wav2Vec2 model)"),mTe.forEach(t),No.forEach(t),MKe=i(Rt),m(vh.$$.fragment,Rt),EKe=i(Rt),VU=n(Rt,"P",{});var ixr=s(VU);yKe=r(ixr,"Examples:"),ixr.forEach(t),wKe=i(Rt),m(S4.$$.fragment,Rt),Rt.forEach(t),AKe=i(js),Th=n(js,"DIV",{class:!0});var CBe=s(Th);m(P4.$$.fragment,CBe),LKe=i(CBe),WU=n(CBe,"P",{});var dxr=s(WU);BKe=r(dxr,"Register a new processor for this class."),dxr.forEach(t),CBe.forEach(t),js.forEach(t),_Le=i(d),qi=n(d,"H2",{class:!0});var MBe=s(qi);Fh=n(MBe,"A",{id:!0,class:!0,href:!0});var cxr=s(Fh);QU=n(cxr,"SPAN",{});var fxr=s(QU);m($4.$$.fragment,fxr),fxr.forEach(t),cxr.forEach(t),kKe=i(MBe),HU=n(MBe,"SPAN",{});var mxr=s(HU);xKe=r(mxr,"AutoModel"),mxr.forEach(t),MBe.forEach(t),uLe=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(I4.$$.fragment,Ns),RKe=i(Ns),Gi=n(Ns,"P",{});var WX=s(Gi);SKe=r(WX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=n(WX,"CODE",{});var gxr=s(UU);PKe=r(gxr,"from_pretrained()"),gxr.forEach(t),$Ke=r(WX,"class method or the "),JU=n(WX,"CODE",{});var hxr=s(JU);IKe=r(hxr,"from_config()"),hxr.forEach(t),jKe=r(WX,`class
method.`),WX.forEach(t),NKe=i(Ns),j4=n(Ns,"P",{});var EBe=s(j4);DKe=r(EBe,"This class cannot be instantiated directly using "),YU=n(EBe,"CODE",{});var pxr=s(YU);qKe=r(pxr,"__init__()"),pxr.forEach(t),GKe=r(EBe," (throws an error)."),EBe.forEach(t),OKe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(N4.$$.fragment,Ds),XKe=i(Ds),KU=n(Ds,"P",{});var _xr=s(KU);zKe=r(_xr,"Instantiates one of the base model classes of the library from a configuration."),_xr.forEach(t),VKe=i(Ds),Oi=n(Ds,"P",{});var QX=s(Oi);WKe=r(QX,`Note:
Loading a model from its configuration file does `),ZU=n(QX,"STRONG",{});var uxr=s(ZU);QKe=r(uxr,"not"),uxr.forEach(t),HKe=r(QX,` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=n(QX,"CODE",{});var bxr=s(eJ);UKe=r(bxr,"from_pretrained()"),bxr.forEach(t),JKe=r(QX,"to load the model weights."),QX.forEach(t),YKe=i(Ds),oJ=n(Ds,"P",{});var vxr=s(oJ);KKe=r(vxr,"Examples:"),vxr.forEach(t),ZKe=i(Ds),m(D4.$$.fragment,Ds),Ds.forEach(t),eZe=i(Ns),ke=n(Ns,"DIV",{class:!0});var St=s(ke);m(q4.$$.fragment,St),oZe=i(St),rJ=n(St,"P",{});var Txr=s(rJ);rZe=r(Txr,"Instantiate one of the base model classes of the library from a pretrained model."),Txr.forEach(t),tZe=i(St),Da=n(St,"P",{});var lC=s(Da);aZe=r(lC,"The model class to instantiate is selected based on the "),tJ=n(lC,"CODE",{});var Fxr=s(tJ);nZe=r(Fxr,"model_type"),Fxr.forEach(t),sZe=r(lC,` property of the config object (either
passed as an argument or loaded from `),aJ=n(lC,"CODE",{});var Cxr=s(aJ);lZe=r(Cxr,"pretrained_model_name_or_path"),Cxr.forEach(t),iZe=r(lC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=n(lC,"CODE",{});var Mxr=s(nJ);dZe=r(Mxr,"pretrained_model_name_or_path"),Mxr.forEach(t),cZe=r(lC,":"),lC.forEach(t),fZe=i(St),F=n(St,"UL",{});var C=s(F);Ch=n(C,"LI",{});var gTe=s(Ch);sJ=n(gTe,"STRONG",{});var Exr=s(sJ);mZe=r(Exr,"albert"),Exr.forEach(t),gZe=r(gTe," \u2014 "),gx=n(gTe,"A",{href:!0});var yxr=s(gx);hZe=r(yxr,"AlbertModel"),yxr.forEach(t),pZe=r(gTe," (ALBERT model)"),gTe.forEach(t),_Ze=i(C),Mh=n(C,"LI",{});var hTe=s(Mh);lJ=n(hTe,"STRONG",{});var wxr=s(lJ);uZe=r(wxr,"bart"),wxr.forEach(t),bZe=r(hTe," \u2014 "),hx=n(hTe,"A",{href:!0});var Axr=s(hx);vZe=r(Axr,"BartModel"),Axr.forEach(t),TZe=r(hTe," (BART model)"),hTe.forEach(t),FZe=i(C),Eh=n(C,"LI",{});var pTe=s(Eh);iJ=n(pTe,"STRONG",{});var Lxr=s(iJ);CZe=r(Lxr,"beit"),Lxr.forEach(t),MZe=r(pTe," \u2014 "),px=n(pTe,"A",{href:!0});var Bxr=s(px);EZe=r(Bxr,"BeitModel"),Bxr.forEach(t),yZe=r(pTe," (BEiT model)"),pTe.forEach(t),wZe=i(C),yh=n(C,"LI",{});var _Te=s(yh);dJ=n(_Te,"STRONG",{});var kxr=s(dJ);AZe=r(kxr,"bert"),kxr.forEach(t),LZe=r(_Te," \u2014 "),_x=n(_Te,"A",{href:!0});var xxr=s(_x);BZe=r(xxr,"BertModel"),xxr.forEach(t),kZe=r(_Te," (BERT model)"),_Te.forEach(t),xZe=i(C),wh=n(C,"LI",{});var uTe=s(wh);cJ=n(uTe,"STRONG",{});var Rxr=s(cJ);RZe=r(Rxr,"bert-generation"),Rxr.forEach(t),SZe=r(uTe," \u2014 "),ux=n(uTe,"A",{href:!0});var Sxr=s(ux);PZe=r(Sxr,"BertGenerationEncoder"),Sxr.forEach(t),$Ze=r(uTe," (Bert Generation model)"),uTe.forEach(t),IZe=i(C),Ah=n(C,"LI",{});var bTe=s(Ah);fJ=n(bTe,"STRONG",{});var Pxr=s(fJ);jZe=r(Pxr,"big_bird"),Pxr.forEach(t),NZe=r(bTe," \u2014 "),bx=n(bTe,"A",{href:!0});var $xr=s(bx);DZe=r($xr,"BigBirdModel"),$xr.forEach(t),qZe=r(bTe," (BigBird model)"),bTe.forEach(t),GZe=i(C),Lh=n(C,"LI",{});var vTe=s(Lh);mJ=n(vTe,"STRONG",{});var Ixr=s(mJ);OZe=r(Ixr,"bigbird_pegasus"),Ixr.forEach(t),XZe=r(vTe," \u2014 "),vx=n(vTe,"A",{href:!0});var jxr=s(vx);zZe=r(jxr,"BigBirdPegasusModel"),jxr.forEach(t),VZe=r(vTe," (BigBirdPegasus model)"),vTe.forEach(t),WZe=i(C),Bh=n(C,"LI",{});var TTe=s(Bh);gJ=n(TTe,"STRONG",{});var Nxr=s(gJ);QZe=r(Nxr,"blenderbot"),Nxr.forEach(t),HZe=r(TTe," \u2014 "),Tx=n(TTe,"A",{href:!0});var Dxr=s(Tx);UZe=r(Dxr,"BlenderbotModel"),Dxr.forEach(t),JZe=r(TTe," (Blenderbot model)"),TTe.forEach(t),YZe=i(C),kh=n(C,"LI",{});var FTe=s(kh);hJ=n(FTe,"STRONG",{});var qxr=s(hJ);KZe=r(qxr,"blenderbot-small"),qxr.forEach(t),ZZe=r(FTe," \u2014 "),Fx=n(FTe,"A",{href:!0});var Gxr=s(Fx);eeo=r(Gxr,"BlenderbotSmallModel"),Gxr.forEach(t),oeo=r(FTe," (BlenderbotSmall model)"),FTe.forEach(t),reo=i(C),xh=n(C,"LI",{});var CTe=s(xh);pJ=n(CTe,"STRONG",{});var Oxr=s(pJ);teo=r(Oxr,"camembert"),Oxr.forEach(t),aeo=r(CTe," \u2014 "),Cx=n(CTe,"A",{href:!0});var Xxr=s(Cx);neo=r(Xxr,"CamembertModel"),Xxr.forEach(t),seo=r(CTe," (CamemBERT model)"),CTe.forEach(t),leo=i(C),Rh=n(C,"LI",{});var MTe=s(Rh);_J=n(MTe,"STRONG",{});var zxr=s(_J);ieo=r(zxr,"canine"),zxr.forEach(t),deo=r(MTe," \u2014 "),Mx=n(MTe,"A",{href:!0});var Vxr=s(Mx);ceo=r(Vxr,"CanineModel"),Vxr.forEach(t),feo=r(MTe," (Canine model)"),MTe.forEach(t),meo=i(C),Sh=n(C,"LI",{});var ETe=s(Sh);uJ=n(ETe,"STRONG",{});var Wxr=s(uJ);geo=r(Wxr,"clip"),Wxr.forEach(t),heo=r(ETe," \u2014 "),Ex=n(ETe,"A",{href:!0});var Qxr=s(Ex);peo=r(Qxr,"CLIPModel"),Qxr.forEach(t),_eo=r(ETe," (CLIP model)"),ETe.forEach(t),ueo=i(C),Ph=n(C,"LI",{});var yTe=s(Ph);bJ=n(yTe,"STRONG",{});var Hxr=s(bJ);beo=r(Hxr,"convbert"),Hxr.forEach(t),veo=r(yTe," \u2014 "),yx=n(yTe,"A",{href:!0});var Uxr=s(yx);Teo=r(Uxr,"ConvBertModel"),Uxr.forEach(t),Feo=r(yTe," (ConvBERT model)"),yTe.forEach(t),Ceo=i(C),$h=n(C,"LI",{});var wTe=s($h);vJ=n(wTe,"STRONG",{});var Jxr=s(vJ);Meo=r(Jxr,"convnext"),Jxr.forEach(t),Eeo=r(wTe," \u2014 "),wx=n(wTe,"A",{href:!0});var Yxr=s(wx);yeo=r(Yxr,"ConvNextModel"),Yxr.forEach(t),weo=r(wTe," (ConvNext model)"),wTe.forEach(t),Aeo=i(C),Ih=n(C,"LI",{});var ATe=s(Ih);TJ=n(ATe,"STRONG",{});var Kxr=s(TJ);Leo=r(Kxr,"ctrl"),Kxr.forEach(t),Beo=r(ATe," \u2014 "),Ax=n(ATe,"A",{href:!0});var Zxr=s(Ax);keo=r(Zxr,"CTRLModel"),Zxr.forEach(t),xeo=r(ATe," (CTRL model)"),ATe.forEach(t),Reo=i(C),jh=n(C,"LI",{});var LTe=s(jh);FJ=n(LTe,"STRONG",{});var eRr=s(FJ);Seo=r(eRr,"deberta"),eRr.forEach(t),Peo=r(LTe," \u2014 "),Lx=n(LTe,"A",{href:!0});var oRr=s(Lx);$eo=r(oRr,"DebertaModel"),oRr.forEach(t),Ieo=r(LTe," (DeBERTa model)"),LTe.forEach(t),jeo=i(C),Nh=n(C,"LI",{});var BTe=s(Nh);CJ=n(BTe,"STRONG",{});var rRr=s(CJ);Neo=r(rRr,"deberta-v2"),rRr.forEach(t),Deo=r(BTe," \u2014 "),Bx=n(BTe,"A",{href:!0});var tRr=s(Bx);qeo=r(tRr,"DebertaV2Model"),tRr.forEach(t),Geo=r(BTe," (DeBERTa-v2 model)"),BTe.forEach(t),Oeo=i(C),Dh=n(C,"LI",{});var kTe=s(Dh);MJ=n(kTe,"STRONG",{});var aRr=s(MJ);Xeo=r(aRr,"deit"),aRr.forEach(t),zeo=r(kTe," \u2014 "),kx=n(kTe,"A",{href:!0});var nRr=s(kx);Veo=r(nRr,"DeiTModel"),nRr.forEach(t),Weo=r(kTe," (DeiT model)"),kTe.forEach(t),Qeo=i(C),qh=n(C,"LI",{});var xTe=s(qh);EJ=n(xTe,"STRONG",{});var sRr=s(EJ);Heo=r(sRr,"detr"),sRr.forEach(t),Ueo=r(xTe," \u2014 "),xx=n(xTe,"A",{href:!0});var lRr=s(xx);Jeo=r(lRr,"DetrModel"),lRr.forEach(t),Yeo=r(xTe," (DETR model)"),xTe.forEach(t),Keo=i(C),Gh=n(C,"LI",{});var RTe=s(Gh);yJ=n(RTe,"STRONG",{});var iRr=s(yJ);Zeo=r(iRr,"distilbert"),iRr.forEach(t),eoo=r(RTe," \u2014 "),Rx=n(RTe,"A",{href:!0});var dRr=s(Rx);ooo=r(dRr,"DistilBertModel"),dRr.forEach(t),roo=r(RTe," (DistilBERT model)"),RTe.forEach(t),too=i(C),Oh=n(C,"LI",{});var STe=s(Oh);wJ=n(STe,"STRONG",{});var cRr=s(wJ);aoo=r(cRr,"dpr"),cRr.forEach(t),noo=r(STe," \u2014 "),Sx=n(STe,"A",{href:!0});var fRr=s(Sx);soo=r(fRr,"DPRQuestionEncoder"),fRr.forEach(t),loo=r(STe," (DPR model)"),STe.forEach(t),ioo=i(C),Xh=n(C,"LI",{});var PTe=s(Xh);AJ=n(PTe,"STRONG",{});var mRr=s(AJ);doo=r(mRr,"electra"),mRr.forEach(t),coo=r(PTe," \u2014 "),Px=n(PTe,"A",{href:!0});var gRr=s(Px);foo=r(gRr,"ElectraModel"),gRr.forEach(t),moo=r(PTe," (ELECTRA model)"),PTe.forEach(t),goo=i(C),zh=n(C,"LI",{});var $Te=s(zh);LJ=n($Te,"STRONG",{});var hRr=s(LJ);hoo=r(hRr,"flaubert"),hRr.forEach(t),poo=r($Te," \u2014 "),$x=n($Te,"A",{href:!0});var pRr=s($x);_oo=r(pRr,"FlaubertModel"),pRr.forEach(t),uoo=r($Te," (FlauBERT model)"),$Te.forEach(t),boo=i(C),Vh=n(C,"LI",{});var ITe=s(Vh);BJ=n(ITe,"STRONG",{});var _Rr=s(BJ);voo=r(_Rr,"fnet"),_Rr.forEach(t),Too=r(ITe," \u2014 "),Ix=n(ITe,"A",{href:!0});var uRr=s(Ix);Foo=r(uRr,"FNetModel"),uRr.forEach(t),Coo=r(ITe," (FNet model)"),ITe.forEach(t),Moo=i(C),Wh=n(C,"LI",{});var jTe=s(Wh);kJ=n(jTe,"STRONG",{});var bRr=s(kJ);Eoo=r(bRr,"fsmt"),bRr.forEach(t),yoo=r(jTe," \u2014 "),jx=n(jTe,"A",{href:!0});var vRr=s(jx);woo=r(vRr,"FSMTModel"),vRr.forEach(t),Aoo=r(jTe," (FairSeq Machine-Translation model)"),jTe.forEach(t),Loo=i(C),xs=n(C,"LI",{});var I0=s(xs);xJ=n(I0,"STRONG",{});var TRr=s(xJ);Boo=r(TRr,"funnel"),TRr.forEach(t),koo=r(I0," \u2014 "),Nx=n(I0,"A",{href:!0});var FRr=s(Nx);xoo=r(FRr,"FunnelModel"),FRr.forEach(t),Roo=r(I0," or "),Dx=n(I0,"A",{href:!0});var CRr=s(Dx);Soo=r(CRr,"FunnelBaseModel"),CRr.forEach(t),Poo=r(I0," (Funnel Transformer model)"),I0.forEach(t),$oo=i(C),Qh=n(C,"LI",{});var NTe=s(Qh);RJ=n(NTe,"STRONG",{});var MRr=s(RJ);Ioo=r(MRr,"gpt2"),MRr.forEach(t),joo=r(NTe," \u2014 "),qx=n(NTe,"A",{href:!0});var ERr=s(qx);Noo=r(ERr,"GPT2Model"),ERr.forEach(t),Doo=r(NTe," (OpenAI GPT-2 model)"),NTe.forEach(t),qoo=i(C),Hh=n(C,"LI",{});var DTe=s(Hh);SJ=n(DTe,"STRONG",{});var yRr=s(SJ);Goo=r(yRr,"gpt_neo"),yRr.forEach(t),Ooo=r(DTe," \u2014 "),Gx=n(DTe,"A",{href:!0});var wRr=s(Gx);Xoo=r(wRr,"GPTNeoModel"),wRr.forEach(t),zoo=r(DTe," (GPT Neo model)"),DTe.forEach(t),Voo=i(C),Uh=n(C,"LI",{});var qTe=s(Uh);PJ=n(qTe,"STRONG",{});var ARr=s(PJ);Woo=r(ARr,"gptj"),ARr.forEach(t),Qoo=r(qTe," \u2014 "),Ox=n(qTe,"A",{href:!0});var LRr=s(Ox);Hoo=r(LRr,"GPTJModel"),LRr.forEach(t),Uoo=r(qTe," (GPT-J model)"),qTe.forEach(t),Joo=i(C),Jh=n(C,"LI",{});var GTe=s(Jh);$J=n(GTe,"STRONG",{});var BRr=s($J);Yoo=r(BRr,"hubert"),BRr.forEach(t),Koo=r(GTe," \u2014 "),Xx=n(GTe,"A",{href:!0});var kRr=s(Xx);Zoo=r(kRr,"HubertModel"),kRr.forEach(t),ero=r(GTe," (Hubert model)"),GTe.forEach(t),oro=i(C),Yh=n(C,"LI",{});var OTe=s(Yh);IJ=n(OTe,"STRONG",{});var xRr=s(IJ);rro=r(xRr,"ibert"),xRr.forEach(t),tro=r(OTe," \u2014 "),zx=n(OTe,"A",{href:!0});var RRr=s(zx);aro=r(RRr,"IBertModel"),RRr.forEach(t),nro=r(OTe," (I-BERT model)"),OTe.forEach(t),sro=i(C),Kh=n(C,"LI",{});var XTe=s(Kh);jJ=n(XTe,"STRONG",{});var SRr=s(jJ);lro=r(SRr,"imagegpt"),SRr.forEach(t),iro=r(XTe," \u2014 "),Vx=n(XTe,"A",{href:!0});var PRr=s(Vx);dro=r(PRr,"ImageGPTModel"),PRr.forEach(t),cro=r(XTe," (ImageGPT model)"),XTe.forEach(t),fro=i(C),Zh=n(C,"LI",{});var zTe=s(Zh);NJ=n(zTe,"STRONG",{});var $Rr=s(NJ);mro=r($Rr,"layoutlm"),$Rr.forEach(t),gro=r(zTe," \u2014 "),Wx=n(zTe,"A",{href:!0});var IRr=s(Wx);hro=r(IRr,"LayoutLMModel"),IRr.forEach(t),pro=r(zTe," (LayoutLM model)"),zTe.forEach(t),_ro=i(C),ep=n(C,"LI",{});var VTe=s(ep);DJ=n(VTe,"STRONG",{});var jRr=s(DJ);uro=r(jRr,"layoutlmv2"),jRr.forEach(t),bro=r(VTe," \u2014 "),Qx=n(VTe,"A",{href:!0});var NRr=s(Qx);vro=r(NRr,"LayoutLMv2Model"),NRr.forEach(t),Tro=r(VTe," (LayoutLMv2 model)"),VTe.forEach(t),Fro=i(C),op=n(C,"LI",{});var WTe=s(op);qJ=n(WTe,"STRONG",{});var DRr=s(qJ);Cro=r(DRr,"led"),DRr.forEach(t),Mro=r(WTe," \u2014 "),Hx=n(WTe,"A",{href:!0});var qRr=s(Hx);Ero=r(qRr,"LEDModel"),qRr.forEach(t),yro=r(WTe," (LED model)"),WTe.forEach(t),wro=i(C),rp=n(C,"LI",{});var QTe=s(rp);GJ=n(QTe,"STRONG",{});var GRr=s(GJ);Aro=r(GRr,"longformer"),GRr.forEach(t),Lro=r(QTe," \u2014 "),Ux=n(QTe,"A",{href:!0});var ORr=s(Ux);Bro=r(ORr,"LongformerModel"),ORr.forEach(t),kro=r(QTe," (Longformer model)"),QTe.forEach(t),xro=i(C),tp=n(C,"LI",{});var HTe=s(tp);OJ=n(HTe,"STRONG",{});var XRr=s(OJ);Rro=r(XRr,"luke"),XRr.forEach(t),Sro=r(HTe," \u2014 "),Jx=n(HTe,"A",{href:!0});var zRr=s(Jx);Pro=r(zRr,"LukeModel"),zRr.forEach(t),$ro=r(HTe," (LUKE model)"),HTe.forEach(t),Iro=i(C),ap=n(C,"LI",{});var UTe=s(ap);XJ=n(UTe,"STRONG",{});var VRr=s(XJ);jro=r(VRr,"lxmert"),VRr.forEach(t),Nro=r(UTe," \u2014 "),Yx=n(UTe,"A",{href:!0});var WRr=s(Yx);Dro=r(WRr,"LxmertModel"),WRr.forEach(t),qro=r(UTe," (LXMERT model)"),UTe.forEach(t),Gro=i(C),np=n(C,"LI",{});var JTe=s(np);zJ=n(JTe,"STRONG",{});var QRr=s(zJ);Oro=r(QRr,"m2m_100"),QRr.forEach(t),Xro=r(JTe," \u2014 "),Kx=n(JTe,"A",{href:!0});var HRr=s(Kx);zro=r(HRr,"M2M100Model"),HRr.forEach(t),Vro=r(JTe," (M2M100 model)"),JTe.forEach(t),Wro=i(C),sp=n(C,"LI",{});var YTe=s(sp);VJ=n(YTe,"STRONG",{});var URr=s(VJ);Qro=r(URr,"marian"),URr.forEach(t),Hro=r(YTe," \u2014 "),Zx=n(YTe,"A",{href:!0});var JRr=s(Zx);Uro=r(JRr,"MarianModel"),JRr.forEach(t),Jro=r(YTe," (Marian model)"),YTe.forEach(t),Yro=i(C),lp=n(C,"LI",{});var KTe=s(lp);WJ=n(KTe,"STRONG",{});var YRr=s(WJ);Kro=r(YRr,"mbart"),YRr.forEach(t),Zro=r(KTe," \u2014 "),eR=n(KTe,"A",{href:!0});var KRr=s(eR);eto=r(KRr,"MBartModel"),KRr.forEach(t),oto=r(KTe," (mBART model)"),KTe.forEach(t),rto=i(C),ip=n(C,"LI",{});var ZTe=s(ip);QJ=n(ZTe,"STRONG",{});var ZRr=s(QJ);tto=r(ZRr,"megatron-bert"),ZRr.forEach(t),ato=r(ZTe," \u2014 "),oR=n(ZTe,"A",{href:!0});var eSr=s(oR);nto=r(eSr,"MegatronBertModel"),eSr.forEach(t),sto=r(ZTe," (MegatronBert model)"),ZTe.forEach(t),lto=i(C),dp=n(C,"LI",{});var e7e=s(dp);HJ=n(e7e,"STRONG",{});var oSr=s(HJ);ito=r(oSr,"mobilebert"),oSr.forEach(t),dto=r(e7e," \u2014 "),rR=n(e7e,"A",{href:!0});var rSr=s(rR);cto=r(rSr,"MobileBertModel"),rSr.forEach(t),fto=r(e7e," (MobileBERT model)"),e7e.forEach(t),mto=i(C),cp=n(C,"LI",{});var o7e=s(cp);UJ=n(o7e,"STRONG",{});var tSr=s(UJ);gto=r(tSr,"mpnet"),tSr.forEach(t),hto=r(o7e," \u2014 "),tR=n(o7e,"A",{href:!0});var aSr=s(tR);pto=r(aSr,"MPNetModel"),aSr.forEach(t),_to=r(o7e," (MPNet model)"),o7e.forEach(t),uto=i(C),fp=n(C,"LI",{});var r7e=s(fp);JJ=n(r7e,"STRONG",{});var nSr=s(JJ);bto=r(nSr,"mt5"),nSr.forEach(t),vto=r(r7e," \u2014 "),aR=n(r7e,"A",{href:!0});var sSr=s(aR);Tto=r(sSr,"MT5Model"),sSr.forEach(t),Fto=r(r7e," (mT5 model)"),r7e.forEach(t),Cto=i(C),mp=n(C,"LI",{});var t7e=s(mp);YJ=n(t7e,"STRONG",{});var lSr=s(YJ);Mto=r(lSr,"nystromformer"),lSr.forEach(t),Eto=r(t7e," \u2014 "),nR=n(t7e,"A",{href:!0});var iSr=s(nR);yto=r(iSr,"NystromformerModel"),iSr.forEach(t),wto=r(t7e," (Nystromformer model)"),t7e.forEach(t),Ato=i(C),gp=n(C,"LI",{});var a7e=s(gp);KJ=n(a7e,"STRONG",{});var dSr=s(KJ);Lto=r(dSr,"openai-gpt"),dSr.forEach(t),Bto=r(a7e," \u2014 "),sR=n(a7e,"A",{href:!0});var cSr=s(sR);kto=r(cSr,"OpenAIGPTModel"),cSr.forEach(t),xto=r(a7e," (OpenAI GPT model)"),a7e.forEach(t),Rto=i(C),hp=n(C,"LI",{});var n7e=s(hp);ZJ=n(n7e,"STRONG",{});var fSr=s(ZJ);Sto=r(fSr,"pegasus"),fSr.forEach(t),Pto=r(n7e," \u2014 "),lR=n(n7e,"A",{href:!0});var mSr=s(lR);$to=r(mSr,"PegasusModel"),mSr.forEach(t),Ito=r(n7e," (Pegasus model)"),n7e.forEach(t),jto=i(C),pp=n(C,"LI",{});var s7e=s(pp);eY=n(s7e,"STRONG",{});var gSr=s(eY);Nto=r(gSr,"perceiver"),gSr.forEach(t),Dto=r(s7e," \u2014 "),iR=n(s7e,"A",{href:!0});var hSr=s(iR);qto=r(hSr,"PerceiverModel"),hSr.forEach(t),Gto=r(s7e," (Perceiver model)"),s7e.forEach(t),Oto=i(C),_p=n(C,"LI",{});var l7e=s(_p);oY=n(l7e,"STRONG",{});var pSr=s(oY);Xto=r(pSr,"plbart"),pSr.forEach(t),zto=r(l7e," \u2014 "),dR=n(l7e,"A",{href:!0});var _Sr=s(dR);Vto=r(_Sr,"PLBartModel"),_Sr.forEach(t),Wto=r(l7e," (PLBart model)"),l7e.forEach(t),Qto=i(C),up=n(C,"LI",{});var i7e=s(up);rY=n(i7e,"STRONG",{});var uSr=s(rY);Hto=r(uSr,"poolformer"),uSr.forEach(t),Uto=r(i7e," \u2014 "),cR=n(i7e,"A",{href:!0});var bSr=s(cR);Jto=r(bSr,"PoolFormerModel"),bSr.forEach(t),Yto=r(i7e," (PoolFormer model)"),i7e.forEach(t),Kto=i(C),bp=n(C,"LI",{});var d7e=s(bp);tY=n(d7e,"STRONG",{});var vSr=s(tY);Zto=r(vSr,"prophetnet"),vSr.forEach(t),eao=r(d7e," \u2014 "),fR=n(d7e,"A",{href:!0});var TSr=s(fR);oao=r(TSr,"ProphetNetModel"),TSr.forEach(t),rao=r(d7e," (ProphetNet model)"),d7e.forEach(t),tao=i(C),vp=n(C,"LI",{});var c7e=s(vp);aY=n(c7e,"STRONG",{});var FSr=s(aY);aao=r(FSr,"qdqbert"),FSr.forEach(t),nao=r(c7e," \u2014 "),mR=n(c7e,"A",{href:!0});var CSr=s(mR);sao=r(CSr,"QDQBertModel"),CSr.forEach(t),lao=r(c7e," (QDQBert model)"),c7e.forEach(t),iao=i(C),Tp=n(C,"LI",{});var f7e=s(Tp);nY=n(f7e,"STRONG",{});var MSr=s(nY);dao=r(MSr,"reformer"),MSr.forEach(t),cao=r(f7e," \u2014 "),gR=n(f7e,"A",{href:!0});var ESr=s(gR);fao=r(ESr,"ReformerModel"),ESr.forEach(t),mao=r(f7e," (Reformer model)"),f7e.forEach(t),gao=i(C),Fp=n(C,"LI",{});var m7e=s(Fp);sY=n(m7e,"STRONG",{});var ySr=s(sY);hao=r(ySr,"rembert"),ySr.forEach(t),pao=r(m7e," \u2014 "),hR=n(m7e,"A",{href:!0});var wSr=s(hR);_ao=r(wSr,"RemBertModel"),wSr.forEach(t),uao=r(m7e," (RemBERT model)"),m7e.forEach(t),bao=i(C),Cp=n(C,"LI",{});var g7e=s(Cp);lY=n(g7e,"STRONG",{});var ASr=s(lY);vao=r(ASr,"retribert"),ASr.forEach(t),Tao=r(g7e," \u2014 "),pR=n(g7e,"A",{href:!0});var LSr=s(pR);Fao=r(LSr,"RetriBertModel"),LSr.forEach(t),Cao=r(g7e," (RetriBERT model)"),g7e.forEach(t),Mao=i(C),Mp=n(C,"LI",{});var h7e=s(Mp);iY=n(h7e,"STRONG",{});var BSr=s(iY);Eao=r(BSr,"roberta"),BSr.forEach(t),yao=r(h7e," \u2014 "),_R=n(h7e,"A",{href:!0});var kSr=s(_R);wao=r(kSr,"RobertaModel"),kSr.forEach(t),Aao=r(h7e," (RoBERTa model)"),h7e.forEach(t),Lao=i(C),Ep=n(C,"LI",{});var p7e=s(Ep);dY=n(p7e,"STRONG",{});var xSr=s(dY);Bao=r(xSr,"roformer"),xSr.forEach(t),kao=r(p7e," \u2014 "),uR=n(p7e,"A",{href:!0});var RSr=s(uR);xao=r(RSr,"RoFormerModel"),RSr.forEach(t),Rao=r(p7e," (RoFormer model)"),p7e.forEach(t),Sao=i(C),yp=n(C,"LI",{});var _7e=s(yp);cY=n(_7e,"STRONG",{});var SSr=s(cY);Pao=r(SSr,"segformer"),SSr.forEach(t),$ao=r(_7e," \u2014 "),bR=n(_7e,"A",{href:!0});var PSr=s(bR);Iao=r(PSr,"SegformerModel"),PSr.forEach(t),jao=r(_7e," (SegFormer model)"),_7e.forEach(t),Nao=i(C),wp=n(C,"LI",{});var u7e=s(wp);fY=n(u7e,"STRONG",{});var $Sr=s(fY);Dao=r($Sr,"sew"),$Sr.forEach(t),qao=r(u7e," \u2014 "),vR=n(u7e,"A",{href:!0});var ISr=s(vR);Gao=r(ISr,"SEWModel"),ISr.forEach(t),Oao=r(u7e," (SEW model)"),u7e.forEach(t),Xao=i(C),Ap=n(C,"LI",{});var b7e=s(Ap);mY=n(b7e,"STRONG",{});var jSr=s(mY);zao=r(jSr,"sew-d"),jSr.forEach(t),Vao=r(b7e," \u2014 "),TR=n(b7e,"A",{href:!0});var NSr=s(TR);Wao=r(NSr,"SEWDModel"),NSr.forEach(t),Qao=r(b7e," (SEW-D model)"),b7e.forEach(t),Hao=i(C),Lp=n(C,"LI",{});var v7e=s(Lp);gY=n(v7e,"STRONG",{});var DSr=s(gY);Uao=r(DSr,"speech_to_text"),DSr.forEach(t),Jao=r(v7e," \u2014 "),FR=n(v7e,"A",{href:!0});var qSr=s(FR);Yao=r(qSr,"Speech2TextModel"),qSr.forEach(t),Kao=r(v7e," (Speech2Text model)"),v7e.forEach(t),Zao=i(C),Bp=n(C,"LI",{});var T7e=s(Bp);hY=n(T7e,"STRONG",{});var GSr=s(hY);eno=r(GSr,"splinter"),GSr.forEach(t),ono=r(T7e," \u2014 "),CR=n(T7e,"A",{href:!0});var OSr=s(CR);rno=r(OSr,"SplinterModel"),OSr.forEach(t),tno=r(T7e," (Splinter model)"),T7e.forEach(t),ano=i(C),kp=n(C,"LI",{});var F7e=s(kp);pY=n(F7e,"STRONG",{});var XSr=s(pY);nno=r(XSr,"squeezebert"),XSr.forEach(t),sno=r(F7e," \u2014 "),MR=n(F7e,"A",{href:!0});var zSr=s(MR);lno=r(zSr,"SqueezeBertModel"),zSr.forEach(t),ino=r(F7e," (SqueezeBERT model)"),F7e.forEach(t),dno=i(C),xp=n(C,"LI",{});var C7e=s(xp);_Y=n(C7e,"STRONG",{});var VSr=s(_Y);cno=r(VSr,"swin"),VSr.forEach(t),fno=r(C7e," \u2014 "),ER=n(C7e,"A",{href:!0});var WSr=s(ER);mno=r(WSr,"SwinModel"),WSr.forEach(t),gno=r(C7e," (Swin model)"),C7e.forEach(t),hno=i(C),Rp=n(C,"LI",{});var M7e=s(Rp);uY=n(M7e,"STRONG",{});var QSr=s(uY);pno=r(QSr,"t5"),QSr.forEach(t),_no=r(M7e," \u2014 "),yR=n(M7e,"A",{href:!0});var HSr=s(yR);uno=r(HSr,"T5Model"),HSr.forEach(t),bno=r(M7e," (T5 model)"),M7e.forEach(t),vno=i(C),Sp=n(C,"LI",{});var E7e=s(Sp);bY=n(E7e,"STRONG",{});var USr=s(bY);Tno=r(USr,"tapas"),USr.forEach(t),Fno=r(E7e," \u2014 "),wR=n(E7e,"A",{href:!0});var JSr=s(wR);Cno=r(JSr,"TapasModel"),JSr.forEach(t),Mno=r(E7e," (TAPAS model)"),E7e.forEach(t),Eno=i(C),Pp=n(C,"LI",{});var y7e=s(Pp);vY=n(y7e,"STRONG",{});var YSr=s(vY);yno=r(YSr,"transfo-xl"),YSr.forEach(t),wno=r(y7e," \u2014 "),AR=n(y7e,"A",{href:!0});var KSr=s(AR);Ano=r(KSr,"TransfoXLModel"),KSr.forEach(t),Lno=r(y7e," (Transformer-XL model)"),y7e.forEach(t),Bno=i(C),$p=n(C,"LI",{});var w7e=s($p);TY=n(w7e,"STRONG",{});var ZSr=s(TY);kno=r(ZSr,"unispeech"),ZSr.forEach(t),xno=r(w7e," \u2014 "),LR=n(w7e,"A",{href:!0});var ePr=s(LR);Rno=r(ePr,"UniSpeechModel"),ePr.forEach(t),Sno=r(w7e," (UniSpeech model)"),w7e.forEach(t),Pno=i(C),Ip=n(C,"LI",{});var A7e=s(Ip);FY=n(A7e,"STRONG",{});var oPr=s(FY);$no=r(oPr,"unispeech-sat"),oPr.forEach(t),Ino=r(A7e," \u2014 "),BR=n(A7e,"A",{href:!0});var rPr=s(BR);jno=r(rPr,"UniSpeechSatModel"),rPr.forEach(t),Nno=r(A7e," (UniSpeechSat model)"),A7e.forEach(t),Dno=i(C),jp=n(C,"LI",{});var L7e=s(jp);CY=n(L7e,"STRONG",{});var tPr=s(CY);qno=r(tPr,"vilt"),tPr.forEach(t),Gno=r(L7e," \u2014 "),kR=n(L7e,"A",{href:!0});var aPr=s(kR);Ono=r(aPr,"ViltModel"),aPr.forEach(t),Xno=r(L7e," (ViLT model)"),L7e.forEach(t),zno=i(C),Np=n(C,"LI",{});var B7e=s(Np);MY=n(B7e,"STRONG",{});var nPr=s(MY);Vno=r(nPr,"vision-text-dual-encoder"),nPr.forEach(t),Wno=r(B7e," \u2014 "),xR=n(B7e,"A",{href:!0});var sPr=s(xR);Qno=r(sPr,"VisionTextDualEncoderModel"),sPr.forEach(t),Hno=r(B7e," (VisionTextDualEncoder model)"),B7e.forEach(t),Uno=i(C),Dp=n(C,"LI",{});var k7e=s(Dp);EY=n(k7e,"STRONG",{});var lPr=s(EY);Jno=r(lPr,"visual_bert"),lPr.forEach(t),Yno=r(k7e," \u2014 "),RR=n(k7e,"A",{href:!0});var iPr=s(RR);Kno=r(iPr,"VisualBertModel"),iPr.forEach(t),Zno=r(k7e," (VisualBert model)"),k7e.forEach(t),eso=i(C),qp=n(C,"LI",{});var x7e=s(qp);yY=n(x7e,"STRONG",{});var dPr=s(yY);oso=r(dPr,"vit"),dPr.forEach(t),rso=r(x7e," \u2014 "),SR=n(x7e,"A",{href:!0});var cPr=s(SR);tso=r(cPr,"ViTModel"),cPr.forEach(t),aso=r(x7e," (ViT model)"),x7e.forEach(t),nso=i(C),Gp=n(C,"LI",{});var R7e=s(Gp);wY=n(R7e,"STRONG",{});var fPr=s(wY);sso=r(fPr,"vit_mae"),fPr.forEach(t),lso=r(R7e," \u2014 "),PR=n(R7e,"A",{href:!0});var mPr=s(PR);iso=r(mPr,"ViTMAEModel"),mPr.forEach(t),dso=r(R7e," (ViTMAE model)"),R7e.forEach(t),cso=i(C),Op=n(C,"LI",{});var S7e=s(Op);AY=n(S7e,"STRONG",{});var gPr=s(AY);fso=r(gPr,"wav2vec2"),gPr.forEach(t),mso=r(S7e," \u2014 "),$R=n(S7e,"A",{href:!0});var hPr=s($R);gso=r(hPr,"Wav2Vec2Model"),hPr.forEach(t),hso=r(S7e," (Wav2Vec2 model)"),S7e.forEach(t),pso=i(C),Xp=n(C,"LI",{});var P7e=s(Xp);LY=n(P7e,"STRONG",{});var pPr=s(LY);_so=r(pPr,"wavlm"),pPr.forEach(t),uso=r(P7e," \u2014 "),IR=n(P7e,"A",{href:!0});var _Pr=s(IR);bso=r(_Pr,"WavLMModel"),_Pr.forEach(t),vso=r(P7e," (WavLM model)"),P7e.forEach(t),Tso=i(C),zp=n(C,"LI",{});var $7e=s(zp);BY=n($7e,"STRONG",{});var uPr=s(BY);Fso=r(uPr,"xglm"),uPr.forEach(t),Cso=r($7e," \u2014 "),jR=n($7e,"A",{href:!0});var bPr=s(jR);Mso=r(bPr,"XGLMModel"),bPr.forEach(t),Eso=r($7e," (XGLM model)"),$7e.forEach(t),yso=i(C),Vp=n(C,"LI",{});var I7e=s(Vp);kY=n(I7e,"STRONG",{});var vPr=s(kY);wso=r(vPr,"xlm"),vPr.forEach(t),Aso=r(I7e," \u2014 "),NR=n(I7e,"A",{href:!0});var TPr=s(NR);Lso=r(TPr,"XLMModel"),TPr.forEach(t),Bso=r(I7e," (XLM model)"),I7e.forEach(t),kso=i(C),Wp=n(C,"LI",{});var j7e=s(Wp);xY=n(j7e,"STRONG",{});var FPr=s(xY);xso=r(FPr,"xlm-prophetnet"),FPr.forEach(t),Rso=r(j7e," \u2014 "),DR=n(j7e,"A",{href:!0});var CPr=s(DR);Sso=r(CPr,"XLMProphetNetModel"),CPr.forEach(t),Pso=r(j7e," (XLMProphetNet model)"),j7e.forEach(t),$so=i(C),Qp=n(C,"LI",{});var N7e=s(Qp);RY=n(N7e,"STRONG",{});var MPr=s(RY);Iso=r(MPr,"xlm-roberta"),MPr.forEach(t),jso=r(N7e," \u2014 "),qR=n(N7e,"A",{href:!0});var EPr=s(qR);Nso=r(EPr,"XLMRobertaModel"),EPr.forEach(t),Dso=r(N7e," (XLM-RoBERTa model)"),N7e.forEach(t),qso=i(C),Hp=n(C,"LI",{});var D7e=s(Hp);SY=n(D7e,"STRONG",{});var yPr=s(SY);Gso=r(yPr,"xlm-roberta-xl"),yPr.forEach(t),Oso=r(D7e," \u2014 "),GR=n(D7e,"A",{href:!0});var wPr=s(GR);Xso=r(wPr,"XLMRobertaXLModel"),wPr.forEach(t),zso=r(D7e," (XLM-RoBERTa-XL model)"),D7e.forEach(t),Vso=i(C),Up=n(C,"LI",{});var q7e=s(Up);PY=n(q7e,"STRONG",{});var APr=s(PY);Wso=r(APr,"xlnet"),APr.forEach(t),Qso=r(q7e," \u2014 "),OR=n(q7e,"A",{href:!0});var LPr=s(OR);Hso=r(LPr,"XLNetModel"),LPr.forEach(t),Uso=r(q7e," (XLNet model)"),q7e.forEach(t),Jso=i(C),Jp=n(C,"LI",{});var G7e=s(Jp);$Y=n(G7e,"STRONG",{});var BPr=s($Y);Yso=r(BPr,"yoso"),BPr.forEach(t),Kso=r(G7e," \u2014 "),XR=n(G7e,"A",{href:!0});var kPr=s(XR);Zso=r(kPr,"YosoModel"),kPr.forEach(t),elo=r(G7e," (YOSO model)"),G7e.forEach(t),C.forEach(t),olo=i(St),Yp=n(St,"P",{});var O7e=s(Yp);rlo=r(O7e,"The model is set in evaluation mode by default using "),IY=n(O7e,"CODE",{});var xPr=s(IY);tlo=r(xPr,"model.eval()"),xPr.forEach(t),alo=r(O7e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=n(O7e,"CODE",{});var RPr=s(jY);nlo=r(RPr,"model.train()"),RPr.forEach(t),O7e.forEach(t),slo=i(St),NY=n(St,"P",{});var SPr=s(NY);llo=r(SPr,"Examples:"),SPr.forEach(t),ilo=i(St),m(G4.$$.fragment,St),St.forEach(t),Ns.forEach(t),bLe=i(d),Xi=n(d,"H2",{class:!0});var yBe=s(Xi);Kp=n(yBe,"A",{id:!0,class:!0,href:!0});var PPr=s(Kp);DY=n(PPr,"SPAN",{});var $Pr=s(DY);m(O4.$$.fragment,$Pr),$Pr.forEach(t),PPr.forEach(t),dlo=i(yBe),qY=n(yBe,"SPAN",{});var IPr=s(qY);clo=r(IPr,"AutoModelForPreTraining"),IPr.forEach(t),yBe.forEach(t),vLe=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(X4.$$.fragment,qs),flo=i(qs),zi=n(qs,"P",{});var HX=s(zi);mlo=r(HX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=n(HX,"CODE",{});var jPr=s(GY);glo=r(jPr,"from_pretrained()"),jPr.forEach(t),hlo=r(HX,"class method or the "),OY=n(HX,"CODE",{});var NPr=s(OY);plo=r(NPr,"from_config()"),NPr.forEach(t),_lo=r(HX,`class
method.`),HX.forEach(t),ulo=i(qs),z4=n(qs,"P",{});var wBe=s(z4);blo=r(wBe,"This class cannot be instantiated directly using "),XY=n(wBe,"CODE",{});var DPr=s(XY);vlo=r(DPr,"__init__()"),DPr.forEach(t),Tlo=r(wBe," (throws an error)."),wBe.forEach(t),Flo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(V4.$$.fragment,Gs),Clo=i(Gs),zY=n(Gs,"P",{});var qPr=s(zY);Mlo=r(qPr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qPr.forEach(t),Elo=i(Gs),Vi=n(Gs,"P",{});var UX=s(Vi);ylo=r(UX,`Note:
Loading a model from its configuration file does `),VY=n(UX,"STRONG",{});var GPr=s(VY);wlo=r(GPr,"not"),GPr.forEach(t),Alo=r(UX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=n(UX,"CODE",{});var OPr=s(WY);Llo=r(OPr,"from_pretrained()"),OPr.forEach(t),Blo=r(UX,"to load the model weights."),UX.forEach(t),klo=i(Gs),QY=n(Gs,"P",{});var XPr=s(QY);xlo=r(XPr,"Examples:"),XPr.forEach(t),Rlo=i(Gs),m(W4.$$.fragment,Gs),Gs.forEach(t),Slo=i(qs),xe=n(qs,"DIV",{class:!0});var Pt=s(xe);m(Q4.$$.fragment,Pt),Plo=i(Pt),HY=n(Pt,"P",{});var zPr=s(HY);$lo=r(zPr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),zPr.forEach(t),Ilo=i(Pt),qa=n(Pt,"P",{});var iC=s(qa);jlo=r(iC,"The model class to instantiate is selected based on the "),UY=n(iC,"CODE",{});var VPr=s(UY);Nlo=r(VPr,"model_type"),VPr.forEach(t),Dlo=r(iC,` property of the config object (either
passed as an argument or loaded from `),JY=n(iC,"CODE",{});var WPr=s(JY);qlo=r(WPr,"pretrained_model_name_or_path"),WPr.forEach(t),Glo=r(iC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=n(iC,"CODE",{});var QPr=s(YY);Olo=r(QPr,"pretrained_model_name_or_path"),QPr.forEach(t),Xlo=r(iC,":"),iC.forEach(t),zlo=i(Pt),x=n(Pt,"UL",{});var S=s(x);Zp=n(S,"LI",{});var X7e=s(Zp);KY=n(X7e,"STRONG",{});var HPr=s(KY);Vlo=r(HPr,"albert"),HPr.forEach(t),Wlo=r(X7e," \u2014 "),zR=n(X7e,"A",{href:!0});var UPr=s(zR);Qlo=r(UPr,"AlbertForPreTraining"),UPr.forEach(t),Hlo=r(X7e," (ALBERT model)"),X7e.forEach(t),Ulo=i(S),e_=n(S,"LI",{});var z7e=s(e_);ZY=n(z7e,"STRONG",{});var JPr=s(ZY);Jlo=r(JPr,"bart"),JPr.forEach(t),Ylo=r(z7e," \u2014 "),VR=n(z7e,"A",{href:!0});var YPr=s(VR);Klo=r(YPr,"BartForConditionalGeneration"),YPr.forEach(t),Zlo=r(z7e," (BART model)"),z7e.forEach(t),eio=i(S),o_=n(S,"LI",{});var V7e=s(o_);eK=n(V7e,"STRONG",{});var KPr=s(eK);oio=r(KPr,"bert"),KPr.forEach(t),rio=r(V7e," \u2014 "),WR=n(V7e,"A",{href:!0});var ZPr=s(WR);tio=r(ZPr,"BertForPreTraining"),ZPr.forEach(t),aio=r(V7e," (BERT model)"),V7e.forEach(t),nio=i(S),r_=n(S,"LI",{});var W7e=s(r_);oK=n(W7e,"STRONG",{});var e$r=s(oK);sio=r(e$r,"big_bird"),e$r.forEach(t),lio=r(W7e," \u2014 "),QR=n(W7e,"A",{href:!0});var o$r=s(QR);iio=r(o$r,"BigBirdForPreTraining"),o$r.forEach(t),dio=r(W7e," (BigBird model)"),W7e.forEach(t),cio=i(S),t_=n(S,"LI",{});var Q7e=s(t_);rK=n(Q7e,"STRONG",{});var r$r=s(rK);fio=r(r$r,"camembert"),r$r.forEach(t),mio=r(Q7e," \u2014 "),HR=n(Q7e,"A",{href:!0});var t$r=s(HR);gio=r(t$r,"CamembertForMaskedLM"),t$r.forEach(t),hio=r(Q7e," (CamemBERT model)"),Q7e.forEach(t),pio=i(S),a_=n(S,"LI",{});var H7e=s(a_);tK=n(H7e,"STRONG",{});var a$r=s(tK);_io=r(a$r,"ctrl"),a$r.forEach(t),uio=r(H7e," \u2014 "),UR=n(H7e,"A",{href:!0});var n$r=s(UR);bio=r(n$r,"CTRLLMHeadModel"),n$r.forEach(t),vio=r(H7e," (CTRL model)"),H7e.forEach(t),Tio=i(S),n_=n(S,"LI",{});var U7e=s(n_);aK=n(U7e,"STRONG",{});var s$r=s(aK);Fio=r(s$r,"deberta"),s$r.forEach(t),Cio=r(U7e," \u2014 "),JR=n(U7e,"A",{href:!0});var l$r=s(JR);Mio=r(l$r,"DebertaForMaskedLM"),l$r.forEach(t),Eio=r(U7e," (DeBERTa model)"),U7e.forEach(t),yio=i(S),s_=n(S,"LI",{});var J7e=s(s_);nK=n(J7e,"STRONG",{});var i$r=s(nK);wio=r(i$r,"deberta-v2"),i$r.forEach(t),Aio=r(J7e," \u2014 "),YR=n(J7e,"A",{href:!0});var d$r=s(YR);Lio=r(d$r,"DebertaV2ForMaskedLM"),d$r.forEach(t),Bio=r(J7e," (DeBERTa-v2 model)"),J7e.forEach(t),kio=i(S),l_=n(S,"LI",{});var Y7e=s(l_);sK=n(Y7e,"STRONG",{});var c$r=s(sK);xio=r(c$r,"distilbert"),c$r.forEach(t),Rio=r(Y7e," \u2014 "),KR=n(Y7e,"A",{href:!0});var f$r=s(KR);Sio=r(f$r,"DistilBertForMaskedLM"),f$r.forEach(t),Pio=r(Y7e," (DistilBERT model)"),Y7e.forEach(t),$io=i(S),i_=n(S,"LI",{});var K7e=s(i_);lK=n(K7e,"STRONG",{});var m$r=s(lK);Iio=r(m$r,"electra"),m$r.forEach(t),jio=r(K7e," \u2014 "),ZR=n(K7e,"A",{href:!0});var g$r=s(ZR);Nio=r(g$r,"ElectraForPreTraining"),g$r.forEach(t),Dio=r(K7e," (ELECTRA model)"),K7e.forEach(t),qio=i(S),d_=n(S,"LI",{});var Z7e=s(d_);iK=n(Z7e,"STRONG",{});var h$r=s(iK);Gio=r(h$r,"flaubert"),h$r.forEach(t),Oio=r(Z7e," \u2014 "),eS=n(Z7e,"A",{href:!0});var p$r=s(eS);Xio=r(p$r,"FlaubertWithLMHeadModel"),p$r.forEach(t),zio=r(Z7e," (FlauBERT model)"),Z7e.forEach(t),Vio=i(S),c_=n(S,"LI",{});var eFe=s(c_);dK=n(eFe,"STRONG",{});var _$r=s(dK);Wio=r(_$r,"fnet"),_$r.forEach(t),Qio=r(eFe," \u2014 "),oS=n(eFe,"A",{href:!0});var u$r=s(oS);Hio=r(u$r,"FNetForPreTraining"),u$r.forEach(t),Uio=r(eFe," (FNet model)"),eFe.forEach(t),Jio=i(S),f_=n(S,"LI",{});var oFe=s(f_);cK=n(oFe,"STRONG",{});var b$r=s(cK);Yio=r(b$r,"fsmt"),b$r.forEach(t),Kio=r(oFe," \u2014 "),rS=n(oFe,"A",{href:!0});var v$r=s(rS);Zio=r(v$r,"FSMTForConditionalGeneration"),v$r.forEach(t),edo=r(oFe," (FairSeq Machine-Translation model)"),oFe.forEach(t),odo=i(S),m_=n(S,"LI",{});var rFe=s(m_);fK=n(rFe,"STRONG",{});var T$r=s(fK);rdo=r(T$r,"funnel"),T$r.forEach(t),tdo=r(rFe," \u2014 "),tS=n(rFe,"A",{href:!0});var F$r=s(tS);ado=r(F$r,"FunnelForPreTraining"),F$r.forEach(t),ndo=r(rFe," (Funnel Transformer model)"),rFe.forEach(t),sdo=i(S),g_=n(S,"LI",{});var tFe=s(g_);mK=n(tFe,"STRONG",{});var C$r=s(mK);ldo=r(C$r,"gpt2"),C$r.forEach(t),ido=r(tFe," \u2014 "),aS=n(tFe,"A",{href:!0});var M$r=s(aS);ddo=r(M$r,"GPT2LMHeadModel"),M$r.forEach(t),cdo=r(tFe," (OpenAI GPT-2 model)"),tFe.forEach(t),fdo=i(S),h_=n(S,"LI",{});var aFe=s(h_);gK=n(aFe,"STRONG",{});var E$r=s(gK);mdo=r(E$r,"ibert"),E$r.forEach(t),gdo=r(aFe," \u2014 "),nS=n(aFe,"A",{href:!0});var y$r=s(nS);hdo=r(y$r,"IBertForMaskedLM"),y$r.forEach(t),pdo=r(aFe," (I-BERT model)"),aFe.forEach(t),_do=i(S),p_=n(S,"LI",{});var nFe=s(p_);hK=n(nFe,"STRONG",{});var w$r=s(hK);udo=r(w$r,"layoutlm"),w$r.forEach(t),bdo=r(nFe," \u2014 "),sS=n(nFe,"A",{href:!0});var A$r=s(sS);vdo=r(A$r,"LayoutLMForMaskedLM"),A$r.forEach(t),Tdo=r(nFe," (LayoutLM model)"),nFe.forEach(t),Fdo=i(S),__=n(S,"LI",{});var sFe=s(__);pK=n(sFe,"STRONG",{});var L$r=s(pK);Cdo=r(L$r,"longformer"),L$r.forEach(t),Mdo=r(sFe," \u2014 "),lS=n(sFe,"A",{href:!0});var B$r=s(lS);Edo=r(B$r,"LongformerForMaskedLM"),B$r.forEach(t),ydo=r(sFe," (Longformer model)"),sFe.forEach(t),wdo=i(S),u_=n(S,"LI",{});var lFe=s(u_);_K=n(lFe,"STRONG",{});var k$r=s(_K);Ado=r(k$r,"lxmert"),k$r.forEach(t),Ldo=r(lFe," \u2014 "),iS=n(lFe,"A",{href:!0});var x$r=s(iS);Bdo=r(x$r,"LxmertForPreTraining"),x$r.forEach(t),kdo=r(lFe," (LXMERT model)"),lFe.forEach(t),xdo=i(S),b_=n(S,"LI",{});var iFe=s(b_);uK=n(iFe,"STRONG",{});var R$r=s(uK);Rdo=r(R$r,"megatron-bert"),R$r.forEach(t),Sdo=r(iFe," \u2014 "),dS=n(iFe,"A",{href:!0});var S$r=s(dS);Pdo=r(S$r,"MegatronBertForPreTraining"),S$r.forEach(t),$do=r(iFe," (MegatronBert model)"),iFe.forEach(t),Ido=i(S),v_=n(S,"LI",{});var dFe=s(v_);bK=n(dFe,"STRONG",{});var P$r=s(bK);jdo=r(P$r,"mobilebert"),P$r.forEach(t),Ndo=r(dFe," \u2014 "),cS=n(dFe,"A",{href:!0});var $$r=s(cS);Ddo=r($$r,"MobileBertForPreTraining"),$$r.forEach(t),qdo=r(dFe," (MobileBERT model)"),dFe.forEach(t),Gdo=i(S),T_=n(S,"LI",{});var cFe=s(T_);vK=n(cFe,"STRONG",{});var I$r=s(vK);Odo=r(I$r,"mpnet"),I$r.forEach(t),Xdo=r(cFe," \u2014 "),fS=n(cFe,"A",{href:!0});var j$r=s(fS);zdo=r(j$r,"MPNetForMaskedLM"),j$r.forEach(t),Vdo=r(cFe," (MPNet model)"),cFe.forEach(t),Wdo=i(S),F_=n(S,"LI",{});var fFe=s(F_);TK=n(fFe,"STRONG",{});var N$r=s(TK);Qdo=r(N$r,"openai-gpt"),N$r.forEach(t),Hdo=r(fFe," \u2014 "),mS=n(fFe,"A",{href:!0});var D$r=s(mS);Udo=r(D$r,"OpenAIGPTLMHeadModel"),D$r.forEach(t),Jdo=r(fFe," (OpenAI GPT model)"),fFe.forEach(t),Ydo=i(S),C_=n(S,"LI",{});var mFe=s(C_);FK=n(mFe,"STRONG",{});var q$r=s(FK);Kdo=r(q$r,"retribert"),q$r.forEach(t),Zdo=r(mFe," \u2014 "),gS=n(mFe,"A",{href:!0});var G$r=s(gS);eco=r(G$r,"RetriBertModel"),G$r.forEach(t),oco=r(mFe," (RetriBERT model)"),mFe.forEach(t),rco=i(S),M_=n(S,"LI",{});var gFe=s(M_);CK=n(gFe,"STRONG",{});var O$r=s(CK);tco=r(O$r,"roberta"),O$r.forEach(t),aco=r(gFe," \u2014 "),hS=n(gFe,"A",{href:!0});var X$r=s(hS);nco=r(X$r,"RobertaForMaskedLM"),X$r.forEach(t),sco=r(gFe," (RoBERTa model)"),gFe.forEach(t),lco=i(S),E_=n(S,"LI",{});var hFe=s(E_);MK=n(hFe,"STRONG",{});var z$r=s(MK);ico=r(z$r,"squeezebert"),z$r.forEach(t),dco=r(hFe," \u2014 "),pS=n(hFe,"A",{href:!0});var V$r=s(pS);cco=r(V$r,"SqueezeBertForMaskedLM"),V$r.forEach(t),fco=r(hFe," (SqueezeBERT model)"),hFe.forEach(t),mco=i(S),y_=n(S,"LI",{});var pFe=s(y_);EK=n(pFe,"STRONG",{});var W$r=s(EK);gco=r(W$r,"t5"),W$r.forEach(t),hco=r(pFe," \u2014 "),_S=n(pFe,"A",{href:!0});var Q$r=s(_S);pco=r(Q$r,"T5ForConditionalGeneration"),Q$r.forEach(t),_co=r(pFe," (T5 model)"),pFe.forEach(t),uco=i(S),w_=n(S,"LI",{});var _Fe=s(w_);yK=n(_Fe,"STRONG",{});var H$r=s(yK);bco=r(H$r,"tapas"),H$r.forEach(t),vco=r(_Fe," \u2014 "),uS=n(_Fe,"A",{href:!0});var U$r=s(uS);Tco=r(U$r,"TapasForMaskedLM"),U$r.forEach(t),Fco=r(_Fe," (TAPAS model)"),_Fe.forEach(t),Cco=i(S),A_=n(S,"LI",{});var uFe=s(A_);wK=n(uFe,"STRONG",{});var J$r=s(wK);Mco=r(J$r,"transfo-xl"),J$r.forEach(t),Eco=r(uFe," \u2014 "),bS=n(uFe,"A",{href:!0});var Y$r=s(bS);yco=r(Y$r,"TransfoXLLMHeadModel"),Y$r.forEach(t),wco=r(uFe," (Transformer-XL model)"),uFe.forEach(t),Aco=i(S),L_=n(S,"LI",{});var bFe=s(L_);AK=n(bFe,"STRONG",{});var K$r=s(AK);Lco=r(K$r,"unispeech"),K$r.forEach(t),Bco=r(bFe," \u2014 "),vS=n(bFe,"A",{href:!0});var Z$r=s(vS);kco=r(Z$r,"UniSpeechForPreTraining"),Z$r.forEach(t),xco=r(bFe," (UniSpeech model)"),bFe.forEach(t),Rco=i(S),B_=n(S,"LI",{});var vFe=s(B_);LK=n(vFe,"STRONG",{});var eIr=s(LK);Sco=r(eIr,"unispeech-sat"),eIr.forEach(t),Pco=r(vFe," \u2014 "),TS=n(vFe,"A",{href:!0});var oIr=s(TS);$co=r(oIr,"UniSpeechSatForPreTraining"),oIr.forEach(t),Ico=r(vFe," (UniSpeechSat model)"),vFe.forEach(t),jco=i(S),k_=n(S,"LI",{});var TFe=s(k_);BK=n(TFe,"STRONG",{});var rIr=s(BK);Nco=r(rIr,"visual_bert"),rIr.forEach(t),Dco=r(TFe," \u2014 "),FS=n(TFe,"A",{href:!0});var tIr=s(FS);qco=r(tIr,"VisualBertForPreTraining"),tIr.forEach(t),Gco=r(TFe," (VisualBert model)"),TFe.forEach(t),Oco=i(S),x_=n(S,"LI",{});var FFe=s(x_);kK=n(FFe,"STRONG",{});var aIr=s(kK);Xco=r(aIr,"vit_mae"),aIr.forEach(t),zco=r(FFe," \u2014 "),CS=n(FFe,"A",{href:!0});var nIr=s(CS);Vco=r(nIr,"ViTMAEForPreTraining"),nIr.forEach(t),Wco=r(FFe," (ViTMAE model)"),FFe.forEach(t),Qco=i(S),R_=n(S,"LI",{});var CFe=s(R_);xK=n(CFe,"STRONG",{});var sIr=s(xK);Hco=r(sIr,"wav2vec2"),sIr.forEach(t),Uco=r(CFe," \u2014 "),MS=n(CFe,"A",{href:!0});var lIr=s(MS);Jco=r(lIr,"Wav2Vec2ForPreTraining"),lIr.forEach(t),Yco=r(CFe," (Wav2Vec2 model)"),CFe.forEach(t),Kco=i(S),S_=n(S,"LI",{});var MFe=s(S_);RK=n(MFe,"STRONG",{});var iIr=s(RK);Zco=r(iIr,"xlm"),iIr.forEach(t),efo=r(MFe," \u2014 "),ES=n(MFe,"A",{href:!0});var dIr=s(ES);ofo=r(dIr,"XLMWithLMHeadModel"),dIr.forEach(t),rfo=r(MFe," (XLM model)"),MFe.forEach(t),tfo=i(S),P_=n(S,"LI",{});var EFe=s(P_);SK=n(EFe,"STRONG",{});var cIr=s(SK);afo=r(cIr,"xlm-roberta"),cIr.forEach(t),nfo=r(EFe," \u2014 "),yS=n(EFe,"A",{href:!0});var fIr=s(yS);sfo=r(fIr,"XLMRobertaForMaskedLM"),fIr.forEach(t),lfo=r(EFe," (XLM-RoBERTa model)"),EFe.forEach(t),ifo=i(S),$_=n(S,"LI",{});var yFe=s($_);PK=n(yFe,"STRONG",{});var mIr=s(PK);dfo=r(mIr,"xlm-roberta-xl"),mIr.forEach(t),cfo=r(yFe," \u2014 "),wS=n(yFe,"A",{href:!0});var gIr=s(wS);ffo=r(gIr,"XLMRobertaXLForMaskedLM"),gIr.forEach(t),mfo=r(yFe," (XLM-RoBERTa-XL model)"),yFe.forEach(t),gfo=i(S),I_=n(S,"LI",{});var wFe=s(I_);$K=n(wFe,"STRONG",{});var hIr=s($K);hfo=r(hIr,"xlnet"),hIr.forEach(t),pfo=r(wFe," \u2014 "),AS=n(wFe,"A",{href:!0});var pIr=s(AS);_fo=r(pIr,"XLNetLMHeadModel"),pIr.forEach(t),ufo=r(wFe," (XLNet model)"),wFe.forEach(t),S.forEach(t),bfo=i(Pt),j_=n(Pt,"P",{});var AFe=s(j_);vfo=r(AFe,"The model is set in evaluation mode by default using "),IK=n(AFe,"CODE",{});var _Ir=s(IK);Tfo=r(_Ir,"model.eval()"),_Ir.forEach(t),Ffo=r(AFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=n(AFe,"CODE",{});var uIr=s(jK);Cfo=r(uIr,"model.train()"),uIr.forEach(t),AFe.forEach(t),Mfo=i(Pt),NK=n(Pt,"P",{});var bIr=s(NK);Efo=r(bIr,"Examples:"),bIr.forEach(t),yfo=i(Pt),m(H4.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),TLe=i(d),Wi=n(d,"H2",{class:!0});var ABe=s(Wi);N_=n(ABe,"A",{id:!0,class:!0,href:!0});var vIr=s(N_);DK=n(vIr,"SPAN",{});var TIr=s(DK);m(U4.$$.fragment,TIr),TIr.forEach(t),vIr.forEach(t),wfo=i(ABe),qK=n(ABe,"SPAN",{});var FIr=s(qK);Afo=r(FIr,"AutoModelForCausalLM"),FIr.forEach(t),ABe.forEach(t),FLe=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(J4.$$.fragment,Os),Lfo=i(Os),Qi=n(Os,"P",{});var JX=s(Qi);Bfo=r(JX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=n(JX,"CODE",{});var CIr=s(GK);kfo=r(CIr,"from_pretrained()"),CIr.forEach(t),xfo=r(JX,"class method or the "),OK=n(JX,"CODE",{});var MIr=s(OK);Rfo=r(MIr,"from_config()"),MIr.forEach(t),Sfo=r(JX,`class
method.`),JX.forEach(t),Pfo=i(Os),Y4=n(Os,"P",{});var LBe=s(Y4);$fo=r(LBe,"This class cannot be instantiated directly using "),XK=n(LBe,"CODE",{});var EIr=s(XK);Ifo=r(EIr,"__init__()"),EIr.forEach(t),jfo=r(LBe," (throws an error)."),LBe.forEach(t),Nfo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(K4.$$.fragment,Xs),Dfo=i(Xs),zK=n(Xs,"P",{});var yIr=s(zK);qfo=r(yIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),yIr.forEach(t),Gfo=i(Xs),Hi=n(Xs,"P",{});var YX=s(Hi);Ofo=r(YX,`Note:
Loading a model from its configuration file does `),VK=n(YX,"STRONG",{});var wIr=s(VK);Xfo=r(wIr,"not"),wIr.forEach(t),zfo=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=n(YX,"CODE",{});var AIr=s(WK);Vfo=r(AIr,"from_pretrained()"),AIr.forEach(t),Wfo=r(YX,"to load the model weights."),YX.forEach(t),Qfo=i(Xs),QK=n(Xs,"P",{});var LIr=s(QK);Hfo=r(LIr,"Examples:"),LIr.forEach(t),Ufo=i(Xs),m(Z4.$$.fragment,Xs),Xs.forEach(t),Jfo=i(Os),Re=n(Os,"DIV",{class:!0});var $t=s(Re);m(eM.$$.fragment,$t),Yfo=i($t),HK=n($t,"P",{});var BIr=s(HK);Kfo=r(BIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),BIr.forEach(t),Zfo=i($t),Ga=n($t,"P",{});var dC=s(Ga);emo=r(dC,"The model class to instantiate is selected based on the "),UK=n(dC,"CODE",{});var kIr=s(UK);omo=r(kIr,"model_type"),kIr.forEach(t),rmo=r(dC,` property of the config object (either
passed as an argument or loaded from `),JK=n(dC,"CODE",{});var xIr=s(JK);tmo=r(xIr,"pretrained_model_name_or_path"),xIr.forEach(t),amo=r(dC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=n(dC,"CODE",{});var RIr=s(YK);nmo=r(RIr,"pretrained_model_name_or_path"),RIr.forEach(t),smo=r(dC,":"),dC.forEach(t),lmo=i($t),$=n($t,"UL",{});var j=s($);D_=n(j,"LI",{});var LFe=s(D_);KK=n(LFe,"STRONG",{});var SIr=s(KK);imo=r(SIr,"bart"),SIr.forEach(t),dmo=r(LFe," \u2014 "),LS=n(LFe,"A",{href:!0});var PIr=s(LS);cmo=r(PIr,"BartForCausalLM"),PIr.forEach(t),fmo=r(LFe," (BART model)"),LFe.forEach(t),mmo=i(j),q_=n(j,"LI",{});var BFe=s(q_);ZK=n(BFe,"STRONG",{});var $Ir=s(ZK);gmo=r($Ir,"bert"),$Ir.forEach(t),hmo=r(BFe," \u2014 "),BS=n(BFe,"A",{href:!0});var IIr=s(BS);pmo=r(IIr,"BertLMHeadModel"),IIr.forEach(t),_mo=r(BFe," (BERT model)"),BFe.forEach(t),umo=i(j),G_=n(j,"LI",{});var kFe=s(G_);eZ=n(kFe,"STRONG",{});var jIr=s(eZ);bmo=r(jIr,"bert-generation"),jIr.forEach(t),vmo=r(kFe," \u2014 "),kS=n(kFe,"A",{href:!0});var NIr=s(kS);Tmo=r(NIr,"BertGenerationDecoder"),NIr.forEach(t),Fmo=r(kFe," (Bert Generation model)"),kFe.forEach(t),Cmo=i(j),O_=n(j,"LI",{});var xFe=s(O_);oZ=n(xFe,"STRONG",{});var DIr=s(oZ);Mmo=r(DIr,"big_bird"),DIr.forEach(t),Emo=r(xFe," \u2014 "),xS=n(xFe,"A",{href:!0});var qIr=s(xS);ymo=r(qIr,"BigBirdForCausalLM"),qIr.forEach(t),wmo=r(xFe," (BigBird model)"),xFe.forEach(t),Amo=i(j),X_=n(j,"LI",{});var RFe=s(X_);rZ=n(RFe,"STRONG",{});var GIr=s(rZ);Lmo=r(GIr,"bigbird_pegasus"),GIr.forEach(t),Bmo=r(RFe," \u2014 "),RS=n(RFe,"A",{href:!0});var OIr=s(RS);kmo=r(OIr,"BigBirdPegasusForCausalLM"),OIr.forEach(t),xmo=r(RFe," (BigBirdPegasus model)"),RFe.forEach(t),Rmo=i(j),z_=n(j,"LI",{});var SFe=s(z_);tZ=n(SFe,"STRONG",{});var XIr=s(tZ);Smo=r(XIr,"blenderbot"),XIr.forEach(t),Pmo=r(SFe," \u2014 "),SS=n(SFe,"A",{href:!0});var zIr=s(SS);$mo=r(zIr,"BlenderbotForCausalLM"),zIr.forEach(t),Imo=r(SFe," (Blenderbot model)"),SFe.forEach(t),jmo=i(j),V_=n(j,"LI",{});var PFe=s(V_);aZ=n(PFe,"STRONG",{});var VIr=s(aZ);Nmo=r(VIr,"blenderbot-small"),VIr.forEach(t),Dmo=r(PFe," \u2014 "),PS=n(PFe,"A",{href:!0});var WIr=s(PS);qmo=r(WIr,"BlenderbotSmallForCausalLM"),WIr.forEach(t),Gmo=r(PFe," (BlenderbotSmall model)"),PFe.forEach(t),Omo=i(j),W_=n(j,"LI",{});var $Fe=s(W_);nZ=n($Fe,"STRONG",{});var QIr=s(nZ);Xmo=r(QIr,"camembert"),QIr.forEach(t),zmo=r($Fe," \u2014 "),$S=n($Fe,"A",{href:!0});var HIr=s($S);Vmo=r(HIr,"CamembertForCausalLM"),HIr.forEach(t),Wmo=r($Fe," (CamemBERT model)"),$Fe.forEach(t),Qmo=i(j),Q_=n(j,"LI",{});var IFe=s(Q_);sZ=n(IFe,"STRONG",{});var UIr=s(sZ);Hmo=r(UIr,"ctrl"),UIr.forEach(t),Umo=r(IFe," \u2014 "),IS=n(IFe,"A",{href:!0});var JIr=s(IS);Jmo=r(JIr,"CTRLLMHeadModel"),JIr.forEach(t),Ymo=r(IFe," (CTRL model)"),IFe.forEach(t),Kmo=i(j),H_=n(j,"LI",{});var jFe=s(H_);lZ=n(jFe,"STRONG",{});var YIr=s(lZ);Zmo=r(YIr,"electra"),YIr.forEach(t),ego=r(jFe," \u2014 "),jS=n(jFe,"A",{href:!0});var KIr=s(jS);ogo=r(KIr,"ElectraForCausalLM"),KIr.forEach(t),rgo=r(jFe," (ELECTRA model)"),jFe.forEach(t),tgo=i(j),U_=n(j,"LI",{});var NFe=s(U_);iZ=n(NFe,"STRONG",{});var ZIr=s(iZ);ago=r(ZIr,"gpt2"),ZIr.forEach(t),ngo=r(NFe," \u2014 "),NS=n(NFe,"A",{href:!0});var ejr=s(NS);sgo=r(ejr,"GPT2LMHeadModel"),ejr.forEach(t),lgo=r(NFe," (OpenAI GPT-2 model)"),NFe.forEach(t),igo=i(j),J_=n(j,"LI",{});var DFe=s(J_);dZ=n(DFe,"STRONG",{});var ojr=s(dZ);dgo=r(ojr,"gpt_neo"),ojr.forEach(t),cgo=r(DFe," \u2014 "),DS=n(DFe,"A",{href:!0});var rjr=s(DS);fgo=r(rjr,"GPTNeoForCausalLM"),rjr.forEach(t),mgo=r(DFe," (GPT Neo model)"),DFe.forEach(t),ggo=i(j),Y_=n(j,"LI",{});var qFe=s(Y_);cZ=n(qFe,"STRONG",{});var tjr=s(cZ);hgo=r(tjr,"gptj"),tjr.forEach(t),pgo=r(qFe," \u2014 "),qS=n(qFe,"A",{href:!0});var ajr=s(qS);_go=r(ajr,"GPTJForCausalLM"),ajr.forEach(t),ugo=r(qFe," (GPT-J model)"),qFe.forEach(t),bgo=i(j),K_=n(j,"LI",{});var GFe=s(K_);fZ=n(GFe,"STRONG",{});var njr=s(fZ);vgo=r(njr,"marian"),njr.forEach(t),Tgo=r(GFe," \u2014 "),GS=n(GFe,"A",{href:!0});var sjr=s(GS);Fgo=r(sjr,"MarianForCausalLM"),sjr.forEach(t),Cgo=r(GFe," (Marian model)"),GFe.forEach(t),Mgo=i(j),Z_=n(j,"LI",{});var OFe=s(Z_);mZ=n(OFe,"STRONG",{});var ljr=s(mZ);Ego=r(ljr,"mbart"),ljr.forEach(t),ygo=r(OFe," \u2014 "),OS=n(OFe,"A",{href:!0});var ijr=s(OS);wgo=r(ijr,"MBartForCausalLM"),ijr.forEach(t),Ago=r(OFe," (mBART model)"),OFe.forEach(t),Lgo=i(j),eu=n(j,"LI",{});var XFe=s(eu);gZ=n(XFe,"STRONG",{});var djr=s(gZ);Bgo=r(djr,"megatron-bert"),djr.forEach(t),kgo=r(XFe," \u2014 "),XS=n(XFe,"A",{href:!0});var cjr=s(XS);xgo=r(cjr,"MegatronBertForCausalLM"),cjr.forEach(t),Rgo=r(XFe," (MegatronBert model)"),XFe.forEach(t),Sgo=i(j),ou=n(j,"LI",{});var zFe=s(ou);hZ=n(zFe,"STRONG",{});var fjr=s(hZ);Pgo=r(fjr,"openai-gpt"),fjr.forEach(t),$go=r(zFe," \u2014 "),zS=n(zFe,"A",{href:!0});var mjr=s(zS);Igo=r(mjr,"OpenAIGPTLMHeadModel"),mjr.forEach(t),jgo=r(zFe," (OpenAI GPT model)"),zFe.forEach(t),Ngo=i(j),ru=n(j,"LI",{});var VFe=s(ru);pZ=n(VFe,"STRONG",{});var gjr=s(pZ);Dgo=r(gjr,"pegasus"),gjr.forEach(t),qgo=r(VFe," \u2014 "),VS=n(VFe,"A",{href:!0});var hjr=s(VS);Ggo=r(hjr,"PegasusForCausalLM"),hjr.forEach(t),Ogo=r(VFe," (Pegasus model)"),VFe.forEach(t),Xgo=i(j),tu=n(j,"LI",{});var WFe=s(tu);_Z=n(WFe,"STRONG",{});var pjr=s(_Z);zgo=r(pjr,"plbart"),pjr.forEach(t),Vgo=r(WFe," \u2014 "),WS=n(WFe,"A",{href:!0});var _jr=s(WS);Wgo=r(_jr,"PLBartForCausalLM"),_jr.forEach(t),Qgo=r(WFe," (PLBart model)"),WFe.forEach(t),Hgo=i(j),au=n(j,"LI",{});var QFe=s(au);uZ=n(QFe,"STRONG",{});var ujr=s(uZ);Ugo=r(ujr,"prophetnet"),ujr.forEach(t),Jgo=r(QFe," \u2014 "),QS=n(QFe,"A",{href:!0});var bjr=s(QS);Ygo=r(bjr,"ProphetNetForCausalLM"),bjr.forEach(t),Kgo=r(QFe," (ProphetNet model)"),QFe.forEach(t),Zgo=i(j),nu=n(j,"LI",{});var HFe=s(nu);bZ=n(HFe,"STRONG",{});var vjr=s(bZ);eho=r(vjr,"qdqbert"),vjr.forEach(t),oho=r(HFe," \u2014 "),HS=n(HFe,"A",{href:!0});var Tjr=s(HS);rho=r(Tjr,"QDQBertLMHeadModel"),Tjr.forEach(t),tho=r(HFe," (QDQBert model)"),HFe.forEach(t),aho=i(j),su=n(j,"LI",{});var UFe=s(su);vZ=n(UFe,"STRONG",{});var Fjr=s(vZ);nho=r(Fjr,"reformer"),Fjr.forEach(t),sho=r(UFe," \u2014 "),US=n(UFe,"A",{href:!0});var Cjr=s(US);lho=r(Cjr,"ReformerModelWithLMHead"),Cjr.forEach(t),iho=r(UFe," (Reformer model)"),UFe.forEach(t),dho=i(j),lu=n(j,"LI",{});var JFe=s(lu);TZ=n(JFe,"STRONG",{});var Mjr=s(TZ);cho=r(Mjr,"rembert"),Mjr.forEach(t),fho=r(JFe," \u2014 "),JS=n(JFe,"A",{href:!0});var Ejr=s(JS);mho=r(Ejr,"RemBertForCausalLM"),Ejr.forEach(t),gho=r(JFe," (RemBERT model)"),JFe.forEach(t),hho=i(j),iu=n(j,"LI",{});var YFe=s(iu);FZ=n(YFe,"STRONG",{});var yjr=s(FZ);pho=r(yjr,"roberta"),yjr.forEach(t),_ho=r(YFe," \u2014 "),YS=n(YFe,"A",{href:!0});var wjr=s(YS);uho=r(wjr,"RobertaForCausalLM"),wjr.forEach(t),bho=r(YFe," (RoBERTa model)"),YFe.forEach(t),vho=i(j),du=n(j,"LI",{});var KFe=s(du);CZ=n(KFe,"STRONG",{});var Ajr=s(CZ);Tho=r(Ajr,"roformer"),Ajr.forEach(t),Fho=r(KFe," \u2014 "),KS=n(KFe,"A",{href:!0});var Ljr=s(KS);Cho=r(Ljr,"RoFormerForCausalLM"),Ljr.forEach(t),Mho=r(KFe," (RoFormer model)"),KFe.forEach(t),Eho=i(j),cu=n(j,"LI",{});var ZFe=s(cu);MZ=n(ZFe,"STRONG",{});var Bjr=s(MZ);yho=r(Bjr,"speech_to_text_2"),Bjr.forEach(t),who=r(ZFe," \u2014 "),ZS=n(ZFe,"A",{href:!0});var kjr=s(ZS);Aho=r(kjr,"Speech2Text2ForCausalLM"),kjr.forEach(t),Lho=r(ZFe," (Speech2Text2 model)"),ZFe.forEach(t),Bho=i(j),fu=n(j,"LI",{});var e9e=s(fu);EZ=n(e9e,"STRONG",{});var xjr=s(EZ);kho=r(xjr,"transfo-xl"),xjr.forEach(t),xho=r(e9e," \u2014 "),eP=n(e9e,"A",{href:!0});var Rjr=s(eP);Rho=r(Rjr,"TransfoXLLMHeadModel"),Rjr.forEach(t),Sho=r(e9e," (Transformer-XL model)"),e9e.forEach(t),Pho=i(j),mu=n(j,"LI",{});var o9e=s(mu);yZ=n(o9e,"STRONG",{});var Sjr=s(yZ);$ho=r(Sjr,"trocr"),Sjr.forEach(t),Iho=r(o9e," \u2014 "),oP=n(o9e,"A",{href:!0});var Pjr=s(oP);jho=r(Pjr,"TrOCRForCausalLM"),Pjr.forEach(t),Nho=r(o9e," (TrOCR model)"),o9e.forEach(t),Dho=i(j),gu=n(j,"LI",{});var r9e=s(gu);wZ=n(r9e,"STRONG",{});var $jr=s(wZ);qho=r($jr,"xglm"),$jr.forEach(t),Gho=r(r9e," \u2014 "),rP=n(r9e,"A",{href:!0});var Ijr=s(rP);Oho=r(Ijr,"XGLMForCausalLM"),Ijr.forEach(t),Xho=r(r9e," (XGLM model)"),r9e.forEach(t),zho=i(j),hu=n(j,"LI",{});var t9e=s(hu);AZ=n(t9e,"STRONG",{});var jjr=s(AZ);Vho=r(jjr,"xlm"),jjr.forEach(t),Who=r(t9e," \u2014 "),tP=n(t9e,"A",{href:!0});var Njr=s(tP);Qho=r(Njr,"XLMWithLMHeadModel"),Njr.forEach(t),Hho=r(t9e," (XLM model)"),t9e.forEach(t),Uho=i(j),pu=n(j,"LI",{});var a9e=s(pu);LZ=n(a9e,"STRONG",{});var Djr=s(LZ);Jho=r(Djr,"xlm-prophetnet"),Djr.forEach(t),Yho=r(a9e," \u2014 "),aP=n(a9e,"A",{href:!0});var qjr=s(aP);Kho=r(qjr,"XLMProphetNetForCausalLM"),qjr.forEach(t),Zho=r(a9e," (XLMProphetNet model)"),a9e.forEach(t),epo=i(j),_u=n(j,"LI",{});var n9e=s(_u);BZ=n(n9e,"STRONG",{});var Gjr=s(BZ);opo=r(Gjr,"xlm-roberta"),Gjr.forEach(t),rpo=r(n9e," \u2014 "),nP=n(n9e,"A",{href:!0});var Ojr=s(nP);tpo=r(Ojr,"XLMRobertaForCausalLM"),Ojr.forEach(t),apo=r(n9e," (XLM-RoBERTa model)"),n9e.forEach(t),npo=i(j),uu=n(j,"LI",{});var s9e=s(uu);kZ=n(s9e,"STRONG",{});var Xjr=s(kZ);spo=r(Xjr,"xlm-roberta-xl"),Xjr.forEach(t),lpo=r(s9e," \u2014 "),sP=n(s9e,"A",{href:!0});var zjr=s(sP);ipo=r(zjr,"XLMRobertaXLForCausalLM"),zjr.forEach(t),dpo=r(s9e," (XLM-RoBERTa-XL model)"),s9e.forEach(t),cpo=i(j),bu=n(j,"LI",{});var l9e=s(bu);xZ=n(l9e,"STRONG",{});var Vjr=s(xZ);fpo=r(Vjr,"xlnet"),Vjr.forEach(t),mpo=r(l9e," \u2014 "),lP=n(l9e,"A",{href:!0});var Wjr=s(lP);gpo=r(Wjr,"XLNetLMHeadModel"),Wjr.forEach(t),hpo=r(l9e," (XLNet model)"),l9e.forEach(t),j.forEach(t),ppo=i($t),vu=n($t,"P",{});var i9e=s(vu);_po=r(i9e,"The model is set in evaluation mode by default using "),RZ=n(i9e,"CODE",{});var Qjr=s(RZ);upo=r(Qjr,"model.eval()"),Qjr.forEach(t),bpo=r(i9e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=n(i9e,"CODE",{});var Hjr=s(SZ);vpo=r(Hjr,"model.train()"),Hjr.forEach(t),i9e.forEach(t),Tpo=i($t),PZ=n($t,"P",{});var Ujr=s(PZ);Fpo=r(Ujr,"Examples:"),Ujr.forEach(t),Cpo=i($t),m(oM.$$.fragment,$t),$t.forEach(t),Os.forEach(t),CLe=i(d),Ui=n(d,"H2",{class:!0});var BBe=s(Ui);Tu=n(BBe,"A",{id:!0,class:!0,href:!0});var Jjr=s(Tu);$Z=n(Jjr,"SPAN",{});var Yjr=s($Z);m(rM.$$.fragment,Yjr),Yjr.forEach(t),Jjr.forEach(t),Mpo=i(BBe),IZ=n(BBe,"SPAN",{});var Kjr=s(IZ);Epo=r(Kjr,"AutoModelForMaskedLM"),Kjr.forEach(t),BBe.forEach(t),MLe=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(tM.$$.fragment,zs),ypo=i(zs),Ji=n(zs,"P",{});var KX=s(Ji);wpo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=n(KX,"CODE",{});var Zjr=s(jZ);Apo=r(Zjr,"from_pretrained()"),Zjr.forEach(t),Lpo=r(KX,"class method or the "),NZ=n(KX,"CODE",{});var eNr=s(NZ);Bpo=r(eNr,"from_config()"),eNr.forEach(t),kpo=r(KX,`class
method.`),KX.forEach(t),xpo=i(zs),aM=n(zs,"P",{});var kBe=s(aM);Rpo=r(kBe,"This class cannot be instantiated directly using "),DZ=n(kBe,"CODE",{});var oNr=s(DZ);Spo=r(oNr,"__init__()"),oNr.forEach(t),Ppo=r(kBe," (throws an error)."),kBe.forEach(t),$po=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(nM.$$.fragment,Vs),Ipo=i(Vs),qZ=n(Vs,"P",{});var rNr=s(qZ);jpo=r(rNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),rNr.forEach(t),Npo=i(Vs),Yi=n(Vs,"P",{});var ZX=s(Yi);Dpo=r(ZX,`Note:
Loading a model from its configuration file does `),GZ=n(ZX,"STRONG",{});var tNr=s(GZ);qpo=r(tNr,"not"),tNr.forEach(t),Gpo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=n(ZX,"CODE",{});var aNr=s(OZ);Opo=r(aNr,"from_pretrained()"),aNr.forEach(t),Xpo=r(ZX,"to load the model weights."),ZX.forEach(t),zpo=i(Vs),XZ=n(Vs,"P",{});var nNr=s(XZ);Vpo=r(nNr,"Examples:"),nNr.forEach(t),Wpo=i(Vs),m(sM.$$.fragment,Vs),Vs.forEach(t),Qpo=i(zs),Se=n(zs,"DIV",{class:!0});var It=s(Se);m(lM.$$.fragment,It),Hpo=i(It),zZ=n(It,"P",{});var sNr=s(zZ);Upo=r(sNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),sNr.forEach(t),Jpo=i(It),Oa=n(It,"P",{});var cC=s(Oa);Ypo=r(cC,"The model class to instantiate is selected based on the "),VZ=n(cC,"CODE",{});var lNr=s(VZ);Kpo=r(lNr,"model_type"),lNr.forEach(t),Zpo=r(cC,` property of the config object (either
passed as an argument or loaded from `),WZ=n(cC,"CODE",{});var iNr=s(WZ);e_o=r(iNr,"pretrained_model_name_or_path"),iNr.forEach(t),o_o=r(cC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=n(cC,"CODE",{});var dNr=s(QZ);r_o=r(dNr,"pretrained_model_name_or_path"),dNr.forEach(t),t_o=r(cC,":"),cC.forEach(t),a_o=i(It),I=n(It,"UL",{});var N=s(I);Fu=n(N,"LI",{});var d9e=s(Fu);HZ=n(d9e,"STRONG",{});var cNr=s(HZ);n_o=r(cNr,"albert"),cNr.forEach(t),s_o=r(d9e," \u2014 "),iP=n(d9e,"A",{href:!0});var fNr=s(iP);l_o=r(fNr,"AlbertForMaskedLM"),fNr.forEach(t),i_o=r(d9e," (ALBERT model)"),d9e.forEach(t),d_o=i(N),Cu=n(N,"LI",{});var c9e=s(Cu);UZ=n(c9e,"STRONG",{});var mNr=s(UZ);c_o=r(mNr,"bart"),mNr.forEach(t),f_o=r(c9e," \u2014 "),dP=n(c9e,"A",{href:!0});var gNr=s(dP);m_o=r(gNr,"BartForConditionalGeneration"),gNr.forEach(t),g_o=r(c9e," (BART model)"),c9e.forEach(t),h_o=i(N),Mu=n(N,"LI",{});var f9e=s(Mu);JZ=n(f9e,"STRONG",{});var hNr=s(JZ);p_o=r(hNr,"bert"),hNr.forEach(t),__o=r(f9e," \u2014 "),cP=n(f9e,"A",{href:!0});var pNr=s(cP);u_o=r(pNr,"BertForMaskedLM"),pNr.forEach(t),b_o=r(f9e," (BERT model)"),f9e.forEach(t),v_o=i(N),Eu=n(N,"LI",{});var m9e=s(Eu);YZ=n(m9e,"STRONG",{});var _Nr=s(YZ);T_o=r(_Nr,"big_bird"),_Nr.forEach(t),F_o=r(m9e," \u2014 "),fP=n(m9e,"A",{href:!0});var uNr=s(fP);C_o=r(uNr,"BigBirdForMaskedLM"),uNr.forEach(t),M_o=r(m9e," (BigBird model)"),m9e.forEach(t),E_o=i(N),yu=n(N,"LI",{});var g9e=s(yu);KZ=n(g9e,"STRONG",{});var bNr=s(KZ);y_o=r(bNr,"camembert"),bNr.forEach(t),w_o=r(g9e," \u2014 "),mP=n(g9e,"A",{href:!0});var vNr=s(mP);A_o=r(vNr,"CamembertForMaskedLM"),vNr.forEach(t),L_o=r(g9e," (CamemBERT model)"),g9e.forEach(t),B_o=i(N),wu=n(N,"LI",{});var h9e=s(wu);ZZ=n(h9e,"STRONG",{});var TNr=s(ZZ);k_o=r(TNr,"convbert"),TNr.forEach(t),x_o=r(h9e," \u2014 "),gP=n(h9e,"A",{href:!0});var FNr=s(gP);R_o=r(FNr,"ConvBertForMaskedLM"),FNr.forEach(t),S_o=r(h9e," (ConvBERT model)"),h9e.forEach(t),P_o=i(N),Au=n(N,"LI",{});var p9e=s(Au);eee=n(p9e,"STRONG",{});var CNr=s(eee);$_o=r(CNr,"deberta"),CNr.forEach(t),I_o=r(p9e," \u2014 "),hP=n(p9e,"A",{href:!0});var MNr=s(hP);j_o=r(MNr,"DebertaForMaskedLM"),MNr.forEach(t),N_o=r(p9e," (DeBERTa model)"),p9e.forEach(t),D_o=i(N),Lu=n(N,"LI",{});var _9e=s(Lu);oee=n(_9e,"STRONG",{});var ENr=s(oee);q_o=r(ENr,"deberta-v2"),ENr.forEach(t),G_o=r(_9e," \u2014 "),pP=n(_9e,"A",{href:!0});var yNr=s(pP);O_o=r(yNr,"DebertaV2ForMaskedLM"),yNr.forEach(t),X_o=r(_9e," (DeBERTa-v2 model)"),_9e.forEach(t),z_o=i(N),Bu=n(N,"LI",{});var u9e=s(Bu);ree=n(u9e,"STRONG",{});var wNr=s(ree);V_o=r(wNr,"distilbert"),wNr.forEach(t),W_o=r(u9e," \u2014 "),_P=n(u9e,"A",{href:!0});var ANr=s(_P);Q_o=r(ANr,"DistilBertForMaskedLM"),ANr.forEach(t),H_o=r(u9e," (DistilBERT model)"),u9e.forEach(t),U_o=i(N),ku=n(N,"LI",{});var b9e=s(ku);tee=n(b9e,"STRONG",{});var LNr=s(tee);J_o=r(LNr,"electra"),LNr.forEach(t),Y_o=r(b9e," \u2014 "),uP=n(b9e,"A",{href:!0});var BNr=s(uP);K_o=r(BNr,"ElectraForMaskedLM"),BNr.forEach(t),Z_o=r(b9e," (ELECTRA model)"),b9e.forEach(t),euo=i(N),xu=n(N,"LI",{});var v9e=s(xu);aee=n(v9e,"STRONG",{});var kNr=s(aee);ouo=r(kNr,"flaubert"),kNr.forEach(t),ruo=r(v9e," \u2014 "),bP=n(v9e,"A",{href:!0});var xNr=s(bP);tuo=r(xNr,"FlaubertWithLMHeadModel"),xNr.forEach(t),auo=r(v9e," (FlauBERT model)"),v9e.forEach(t),nuo=i(N),Ru=n(N,"LI",{});var T9e=s(Ru);nee=n(T9e,"STRONG",{});var RNr=s(nee);suo=r(RNr,"fnet"),RNr.forEach(t),luo=r(T9e," \u2014 "),vP=n(T9e,"A",{href:!0});var SNr=s(vP);iuo=r(SNr,"FNetForMaskedLM"),SNr.forEach(t),duo=r(T9e," (FNet model)"),T9e.forEach(t),cuo=i(N),Su=n(N,"LI",{});var F9e=s(Su);see=n(F9e,"STRONG",{});var PNr=s(see);fuo=r(PNr,"funnel"),PNr.forEach(t),muo=r(F9e," \u2014 "),TP=n(F9e,"A",{href:!0});var $Nr=s(TP);guo=r($Nr,"FunnelForMaskedLM"),$Nr.forEach(t),huo=r(F9e," (Funnel Transformer model)"),F9e.forEach(t),puo=i(N),Pu=n(N,"LI",{});var C9e=s(Pu);lee=n(C9e,"STRONG",{});var INr=s(lee);_uo=r(INr,"ibert"),INr.forEach(t),uuo=r(C9e," \u2014 "),FP=n(C9e,"A",{href:!0});var jNr=s(FP);buo=r(jNr,"IBertForMaskedLM"),jNr.forEach(t),vuo=r(C9e," (I-BERT model)"),C9e.forEach(t),Tuo=i(N),$u=n(N,"LI",{});var M9e=s($u);iee=n(M9e,"STRONG",{});var NNr=s(iee);Fuo=r(NNr,"layoutlm"),NNr.forEach(t),Cuo=r(M9e," \u2014 "),CP=n(M9e,"A",{href:!0});var DNr=s(CP);Muo=r(DNr,"LayoutLMForMaskedLM"),DNr.forEach(t),Euo=r(M9e," (LayoutLM model)"),M9e.forEach(t),yuo=i(N),Iu=n(N,"LI",{});var E9e=s(Iu);dee=n(E9e,"STRONG",{});var qNr=s(dee);wuo=r(qNr,"longformer"),qNr.forEach(t),Auo=r(E9e," \u2014 "),MP=n(E9e,"A",{href:!0});var GNr=s(MP);Luo=r(GNr,"LongformerForMaskedLM"),GNr.forEach(t),Buo=r(E9e," (Longformer model)"),E9e.forEach(t),kuo=i(N),ju=n(N,"LI",{});var y9e=s(ju);cee=n(y9e,"STRONG",{});var ONr=s(cee);xuo=r(ONr,"mbart"),ONr.forEach(t),Ruo=r(y9e," \u2014 "),EP=n(y9e,"A",{href:!0});var XNr=s(EP);Suo=r(XNr,"MBartForConditionalGeneration"),XNr.forEach(t),Puo=r(y9e," (mBART model)"),y9e.forEach(t),$uo=i(N),Nu=n(N,"LI",{});var w9e=s(Nu);fee=n(w9e,"STRONG",{});var zNr=s(fee);Iuo=r(zNr,"megatron-bert"),zNr.forEach(t),juo=r(w9e," \u2014 "),yP=n(w9e,"A",{href:!0});var VNr=s(yP);Nuo=r(VNr,"MegatronBertForMaskedLM"),VNr.forEach(t),Duo=r(w9e," (MegatronBert model)"),w9e.forEach(t),quo=i(N),Du=n(N,"LI",{});var A9e=s(Du);mee=n(A9e,"STRONG",{});var WNr=s(mee);Guo=r(WNr,"mobilebert"),WNr.forEach(t),Ouo=r(A9e," \u2014 "),wP=n(A9e,"A",{href:!0});var QNr=s(wP);Xuo=r(QNr,"MobileBertForMaskedLM"),QNr.forEach(t),zuo=r(A9e," (MobileBERT model)"),A9e.forEach(t),Vuo=i(N),qu=n(N,"LI",{});var L9e=s(qu);gee=n(L9e,"STRONG",{});var HNr=s(gee);Wuo=r(HNr,"mpnet"),HNr.forEach(t),Quo=r(L9e," \u2014 "),AP=n(L9e,"A",{href:!0});var UNr=s(AP);Huo=r(UNr,"MPNetForMaskedLM"),UNr.forEach(t),Uuo=r(L9e," (MPNet model)"),L9e.forEach(t),Juo=i(N),Gu=n(N,"LI",{});var B9e=s(Gu);hee=n(B9e,"STRONG",{});var JNr=s(hee);Yuo=r(JNr,"nystromformer"),JNr.forEach(t),Kuo=r(B9e," \u2014 "),LP=n(B9e,"A",{href:!0});var YNr=s(LP);Zuo=r(YNr,"NystromformerForMaskedLM"),YNr.forEach(t),e1o=r(B9e," (Nystromformer model)"),B9e.forEach(t),o1o=i(N),Ou=n(N,"LI",{});var k9e=s(Ou);pee=n(k9e,"STRONG",{});var KNr=s(pee);r1o=r(KNr,"perceiver"),KNr.forEach(t),t1o=r(k9e," \u2014 "),BP=n(k9e,"A",{href:!0});var ZNr=s(BP);a1o=r(ZNr,"PerceiverForMaskedLM"),ZNr.forEach(t),n1o=r(k9e," (Perceiver model)"),k9e.forEach(t),s1o=i(N),Xu=n(N,"LI",{});var x9e=s(Xu);_ee=n(x9e,"STRONG",{});var eDr=s(_ee);l1o=r(eDr,"qdqbert"),eDr.forEach(t),i1o=r(x9e," \u2014 "),kP=n(x9e,"A",{href:!0});var oDr=s(kP);d1o=r(oDr,"QDQBertForMaskedLM"),oDr.forEach(t),c1o=r(x9e," (QDQBert model)"),x9e.forEach(t),f1o=i(N),zu=n(N,"LI",{});var R9e=s(zu);uee=n(R9e,"STRONG",{});var rDr=s(uee);m1o=r(rDr,"reformer"),rDr.forEach(t),g1o=r(R9e," \u2014 "),xP=n(R9e,"A",{href:!0});var tDr=s(xP);h1o=r(tDr,"ReformerForMaskedLM"),tDr.forEach(t),p1o=r(R9e," (Reformer model)"),R9e.forEach(t),_1o=i(N),Vu=n(N,"LI",{});var S9e=s(Vu);bee=n(S9e,"STRONG",{});var aDr=s(bee);u1o=r(aDr,"rembert"),aDr.forEach(t),b1o=r(S9e," \u2014 "),RP=n(S9e,"A",{href:!0});var nDr=s(RP);v1o=r(nDr,"RemBertForMaskedLM"),nDr.forEach(t),T1o=r(S9e," (RemBERT model)"),S9e.forEach(t),F1o=i(N),Wu=n(N,"LI",{});var P9e=s(Wu);vee=n(P9e,"STRONG",{});var sDr=s(vee);C1o=r(sDr,"roberta"),sDr.forEach(t),M1o=r(P9e," \u2014 "),SP=n(P9e,"A",{href:!0});var lDr=s(SP);E1o=r(lDr,"RobertaForMaskedLM"),lDr.forEach(t),y1o=r(P9e," (RoBERTa model)"),P9e.forEach(t),w1o=i(N),Qu=n(N,"LI",{});var $9e=s(Qu);Tee=n($9e,"STRONG",{});var iDr=s(Tee);A1o=r(iDr,"roformer"),iDr.forEach(t),L1o=r($9e," \u2014 "),PP=n($9e,"A",{href:!0});var dDr=s(PP);B1o=r(dDr,"RoFormerForMaskedLM"),dDr.forEach(t),k1o=r($9e," (RoFormer model)"),$9e.forEach(t),x1o=i(N),Hu=n(N,"LI",{});var I9e=s(Hu);Fee=n(I9e,"STRONG",{});var cDr=s(Fee);R1o=r(cDr,"squeezebert"),cDr.forEach(t),S1o=r(I9e," \u2014 "),$P=n(I9e,"A",{href:!0});var fDr=s($P);P1o=r(fDr,"SqueezeBertForMaskedLM"),fDr.forEach(t),$1o=r(I9e," (SqueezeBERT model)"),I9e.forEach(t),I1o=i(N),Uu=n(N,"LI",{});var j9e=s(Uu);Cee=n(j9e,"STRONG",{});var mDr=s(Cee);j1o=r(mDr,"tapas"),mDr.forEach(t),N1o=r(j9e," \u2014 "),IP=n(j9e,"A",{href:!0});var gDr=s(IP);D1o=r(gDr,"TapasForMaskedLM"),gDr.forEach(t),q1o=r(j9e," (TAPAS model)"),j9e.forEach(t),G1o=i(N),Ju=n(N,"LI",{});var N9e=s(Ju);Mee=n(N9e,"STRONG",{});var hDr=s(Mee);O1o=r(hDr,"wav2vec2"),hDr.forEach(t),X1o=r(N9e," \u2014 "),Eee=n(N9e,"CODE",{});var pDr=s(Eee);z1o=r(pDr,"Wav2Vec2ForMaskedLM"),pDr.forEach(t),V1o=r(N9e,"(Wav2Vec2 model)"),N9e.forEach(t),W1o=i(N),Yu=n(N,"LI",{});var D9e=s(Yu);yee=n(D9e,"STRONG",{});var _Dr=s(yee);Q1o=r(_Dr,"xlm"),_Dr.forEach(t),H1o=r(D9e," \u2014 "),jP=n(D9e,"A",{href:!0});var uDr=s(jP);U1o=r(uDr,"XLMWithLMHeadModel"),uDr.forEach(t),J1o=r(D9e," (XLM model)"),D9e.forEach(t),Y1o=i(N),Ku=n(N,"LI",{});var q9e=s(Ku);wee=n(q9e,"STRONG",{});var bDr=s(wee);K1o=r(bDr,"xlm-roberta"),bDr.forEach(t),Z1o=r(q9e," \u2014 "),NP=n(q9e,"A",{href:!0});var vDr=s(NP);ebo=r(vDr,"XLMRobertaForMaskedLM"),vDr.forEach(t),obo=r(q9e," (XLM-RoBERTa model)"),q9e.forEach(t),rbo=i(N),Zu=n(N,"LI",{});var G9e=s(Zu);Aee=n(G9e,"STRONG",{});var TDr=s(Aee);tbo=r(TDr,"xlm-roberta-xl"),TDr.forEach(t),abo=r(G9e," \u2014 "),DP=n(G9e,"A",{href:!0});var FDr=s(DP);nbo=r(FDr,"XLMRobertaXLForMaskedLM"),FDr.forEach(t),sbo=r(G9e," (XLM-RoBERTa-XL model)"),G9e.forEach(t),lbo=i(N),e1=n(N,"LI",{});var O9e=s(e1);Lee=n(O9e,"STRONG",{});var CDr=s(Lee);ibo=r(CDr,"yoso"),CDr.forEach(t),dbo=r(O9e," \u2014 "),qP=n(O9e,"A",{href:!0});var MDr=s(qP);cbo=r(MDr,"YosoForMaskedLM"),MDr.forEach(t),fbo=r(O9e," (YOSO model)"),O9e.forEach(t),N.forEach(t),mbo=i(It),o1=n(It,"P",{});var X9e=s(o1);gbo=r(X9e,"The model is set in evaluation mode by default using "),Bee=n(X9e,"CODE",{});var EDr=s(Bee);hbo=r(EDr,"model.eval()"),EDr.forEach(t),pbo=r(X9e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=n(X9e,"CODE",{});var yDr=s(kee);_bo=r(yDr,"model.train()"),yDr.forEach(t),X9e.forEach(t),ubo=i(It),xee=n(It,"P",{});var wDr=s(xee);bbo=r(wDr,"Examples:"),wDr.forEach(t),vbo=i(It),m(iM.$$.fragment,It),It.forEach(t),zs.forEach(t),ELe=i(d),Ki=n(d,"H2",{class:!0});var xBe=s(Ki);r1=n(xBe,"A",{id:!0,class:!0,href:!0});var ADr=s(r1);Ree=n(ADr,"SPAN",{});var LDr=s(Ree);m(dM.$$.fragment,LDr),LDr.forEach(t),ADr.forEach(t),Tbo=i(xBe),See=n(xBe,"SPAN",{});var BDr=s(See);Fbo=r(BDr,"AutoModelForSeq2SeqLM"),BDr.forEach(t),xBe.forEach(t),yLe=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(cM.$$.fragment,Ws),Cbo=i(Ws),Zi=n(Ws,"P",{});var ez=s(Zi);Mbo=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=n(ez,"CODE",{});var kDr=s(Pee);Ebo=r(kDr,"from_pretrained()"),kDr.forEach(t),ybo=r(ez,"class method or the "),$ee=n(ez,"CODE",{});var xDr=s($ee);wbo=r(xDr,"from_config()"),xDr.forEach(t),Abo=r(ez,`class
method.`),ez.forEach(t),Lbo=i(Ws),fM=n(Ws,"P",{});var RBe=s(fM);Bbo=r(RBe,"This class cannot be instantiated directly using "),Iee=n(RBe,"CODE",{});var RDr=s(Iee);kbo=r(RDr,"__init__()"),RDr.forEach(t),xbo=r(RBe," (throws an error)."),RBe.forEach(t),Rbo=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(mM.$$.fragment,Qs),Sbo=i(Qs),jee=n(Qs,"P",{});var SDr=s(jee);Pbo=r(SDr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),SDr.forEach(t),$bo=i(Qs),ed=n(Qs,"P",{});var oz=s(ed);Ibo=r(oz,`Note:
Loading a model from its configuration file does `),Nee=n(oz,"STRONG",{});var PDr=s(Nee);jbo=r(PDr,"not"),PDr.forEach(t),Nbo=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=n(oz,"CODE",{});var $Dr=s(Dee);Dbo=r($Dr,"from_pretrained()"),$Dr.forEach(t),qbo=r(oz,"to load the model weights."),oz.forEach(t),Gbo=i(Qs),qee=n(Qs,"P",{});var IDr=s(qee);Obo=r(IDr,"Examples:"),IDr.forEach(t),Xbo=i(Qs),m(gM.$$.fragment,Qs),Qs.forEach(t),zbo=i(Ws),Pe=n(Ws,"DIV",{class:!0});var jt=s(Pe);m(hM.$$.fragment,jt),Vbo=i(jt),Gee=n(jt,"P",{});var jDr=s(Gee);Wbo=r(jDr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jDr.forEach(t),Qbo=i(jt),Xa=n(jt,"P",{});var fC=s(Xa);Hbo=r(fC,"The model class to instantiate is selected based on the "),Oee=n(fC,"CODE",{});var NDr=s(Oee);Ubo=r(NDr,"model_type"),NDr.forEach(t),Jbo=r(fC,` property of the config object (either
passed as an argument or loaded from `),Xee=n(fC,"CODE",{});var DDr=s(Xee);Ybo=r(DDr,"pretrained_model_name_or_path"),DDr.forEach(t),Kbo=r(fC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=n(fC,"CODE",{});var qDr=s(zee);Zbo=r(qDr,"pretrained_model_name_or_path"),qDr.forEach(t),e5o=r(fC,":"),fC.forEach(t),o5o=i(jt),ae=n(jt,"UL",{});var le=s(ae);t1=n(le,"LI",{});var z9e=s(t1);Vee=n(z9e,"STRONG",{});var GDr=s(Vee);r5o=r(GDr,"bart"),GDr.forEach(t),t5o=r(z9e," \u2014 "),GP=n(z9e,"A",{href:!0});var ODr=s(GP);a5o=r(ODr,"BartForConditionalGeneration"),ODr.forEach(t),n5o=r(z9e," (BART model)"),z9e.forEach(t),s5o=i(le),a1=n(le,"LI",{});var V9e=s(a1);Wee=n(V9e,"STRONG",{});var XDr=s(Wee);l5o=r(XDr,"bigbird_pegasus"),XDr.forEach(t),i5o=r(V9e," \u2014 "),OP=n(V9e,"A",{href:!0});var zDr=s(OP);d5o=r(zDr,"BigBirdPegasusForConditionalGeneration"),zDr.forEach(t),c5o=r(V9e," (BigBirdPegasus model)"),V9e.forEach(t),f5o=i(le),n1=n(le,"LI",{});var W9e=s(n1);Qee=n(W9e,"STRONG",{});var VDr=s(Qee);m5o=r(VDr,"blenderbot"),VDr.forEach(t),g5o=r(W9e," \u2014 "),XP=n(W9e,"A",{href:!0});var WDr=s(XP);h5o=r(WDr,"BlenderbotForConditionalGeneration"),WDr.forEach(t),p5o=r(W9e," (Blenderbot model)"),W9e.forEach(t),_5o=i(le),s1=n(le,"LI",{});var Q9e=s(s1);Hee=n(Q9e,"STRONG",{});var QDr=s(Hee);u5o=r(QDr,"blenderbot-small"),QDr.forEach(t),b5o=r(Q9e," \u2014 "),zP=n(Q9e,"A",{href:!0});var HDr=s(zP);v5o=r(HDr,"BlenderbotSmallForConditionalGeneration"),HDr.forEach(t),T5o=r(Q9e," (BlenderbotSmall model)"),Q9e.forEach(t),F5o=i(le),l1=n(le,"LI",{});var H9e=s(l1);Uee=n(H9e,"STRONG",{});var UDr=s(Uee);C5o=r(UDr,"encoder-decoder"),UDr.forEach(t),M5o=r(H9e," \u2014 "),VP=n(H9e,"A",{href:!0});var JDr=s(VP);E5o=r(JDr,"EncoderDecoderModel"),JDr.forEach(t),y5o=r(H9e," (Encoder decoder model)"),H9e.forEach(t),w5o=i(le),i1=n(le,"LI",{});var U9e=s(i1);Jee=n(U9e,"STRONG",{});var YDr=s(Jee);A5o=r(YDr,"fsmt"),YDr.forEach(t),L5o=r(U9e," \u2014 "),WP=n(U9e,"A",{href:!0});var KDr=s(WP);B5o=r(KDr,"FSMTForConditionalGeneration"),KDr.forEach(t),k5o=r(U9e," (FairSeq Machine-Translation model)"),U9e.forEach(t),x5o=i(le),d1=n(le,"LI",{});var J9e=s(d1);Yee=n(J9e,"STRONG",{});var ZDr=s(Yee);R5o=r(ZDr,"led"),ZDr.forEach(t),S5o=r(J9e," \u2014 "),QP=n(J9e,"A",{href:!0});var eqr=s(QP);P5o=r(eqr,"LEDForConditionalGeneration"),eqr.forEach(t),$5o=r(J9e," (LED model)"),J9e.forEach(t),I5o=i(le),c1=n(le,"LI",{});var Y9e=s(c1);Kee=n(Y9e,"STRONG",{});var oqr=s(Kee);j5o=r(oqr,"m2m_100"),oqr.forEach(t),N5o=r(Y9e," \u2014 "),HP=n(Y9e,"A",{href:!0});var rqr=s(HP);D5o=r(rqr,"M2M100ForConditionalGeneration"),rqr.forEach(t),q5o=r(Y9e," (M2M100 model)"),Y9e.forEach(t),G5o=i(le),f1=n(le,"LI",{});var K9e=s(f1);Zee=n(K9e,"STRONG",{});var tqr=s(Zee);O5o=r(tqr,"marian"),tqr.forEach(t),X5o=r(K9e," \u2014 "),UP=n(K9e,"A",{href:!0});var aqr=s(UP);z5o=r(aqr,"MarianMTModel"),aqr.forEach(t),V5o=r(K9e," (Marian model)"),K9e.forEach(t),W5o=i(le),m1=n(le,"LI",{});var Z9e=s(m1);eoe=n(Z9e,"STRONG",{});var nqr=s(eoe);Q5o=r(nqr,"mbart"),nqr.forEach(t),H5o=r(Z9e," \u2014 "),JP=n(Z9e,"A",{href:!0});var sqr=s(JP);U5o=r(sqr,"MBartForConditionalGeneration"),sqr.forEach(t),J5o=r(Z9e," (mBART model)"),Z9e.forEach(t),Y5o=i(le),g1=n(le,"LI",{});var eCe=s(g1);ooe=n(eCe,"STRONG",{});var lqr=s(ooe);K5o=r(lqr,"mt5"),lqr.forEach(t),Z5o=r(eCe," \u2014 "),YP=n(eCe,"A",{href:!0});var iqr=s(YP);e2o=r(iqr,"MT5ForConditionalGeneration"),iqr.forEach(t),o2o=r(eCe," (mT5 model)"),eCe.forEach(t),r2o=i(le),h1=n(le,"LI",{});var oCe=s(h1);roe=n(oCe,"STRONG",{});var dqr=s(roe);t2o=r(dqr,"pegasus"),dqr.forEach(t),a2o=r(oCe," \u2014 "),KP=n(oCe,"A",{href:!0});var cqr=s(KP);n2o=r(cqr,"PegasusForConditionalGeneration"),cqr.forEach(t),s2o=r(oCe," (Pegasus model)"),oCe.forEach(t),l2o=i(le),p1=n(le,"LI",{});var rCe=s(p1);toe=n(rCe,"STRONG",{});var fqr=s(toe);i2o=r(fqr,"plbart"),fqr.forEach(t),d2o=r(rCe," \u2014 "),ZP=n(rCe,"A",{href:!0});var mqr=s(ZP);c2o=r(mqr,"PLBartForConditionalGeneration"),mqr.forEach(t),f2o=r(rCe," (PLBart model)"),rCe.forEach(t),m2o=i(le),_1=n(le,"LI",{});var tCe=s(_1);aoe=n(tCe,"STRONG",{});var gqr=s(aoe);g2o=r(gqr,"prophetnet"),gqr.forEach(t),h2o=r(tCe," \u2014 "),e$=n(tCe,"A",{href:!0});var hqr=s(e$);p2o=r(hqr,"ProphetNetForConditionalGeneration"),hqr.forEach(t),_2o=r(tCe," (ProphetNet model)"),tCe.forEach(t),u2o=i(le),u1=n(le,"LI",{});var aCe=s(u1);noe=n(aCe,"STRONG",{});var pqr=s(noe);b2o=r(pqr,"t5"),pqr.forEach(t),v2o=r(aCe," \u2014 "),o$=n(aCe,"A",{href:!0});var _qr=s(o$);T2o=r(_qr,"T5ForConditionalGeneration"),_qr.forEach(t),F2o=r(aCe," (T5 model)"),aCe.forEach(t),C2o=i(le),b1=n(le,"LI",{});var nCe=s(b1);soe=n(nCe,"STRONG",{});var uqr=s(soe);M2o=r(uqr,"xlm-prophetnet"),uqr.forEach(t),E2o=r(nCe," \u2014 "),r$=n(nCe,"A",{href:!0});var bqr=s(r$);y2o=r(bqr,"XLMProphetNetForConditionalGeneration"),bqr.forEach(t),w2o=r(nCe," (XLMProphetNet model)"),nCe.forEach(t),le.forEach(t),A2o=i(jt),v1=n(jt,"P",{});var sCe=s(v1);L2o=r(sCe,"The model is set in evaluation mode by default using "),loe=n(sCe,"CODE",{});var vqr=s(loe);B2o=r(vqr,"model.eval()"),vqr.forEach(t),k2o=r(sCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=n(sCe,"CODE",{});var Tqr=s(ioe);x2o=r(Tqr,"model.train()"),Tqr.forEach(t),sCe.forEach(t),R2o=i(jt),doe=n(jt,"P",{});var Fqr=s(doe);S2o=r(Fqr,"Examples:"),Fqr.forEach(t),P2o=i(jt),m(pM.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),wLe=i(d),od=n(d,"H2",{class:!0});var SBe=s(od);T1=n(SBe,"A",{id:!0,class:!0,href:!0});var Cqr=s(T1);coe=n(Cqr,"SPAN",{});var Mqr=s(coe);m(_M.$$.fragment,Mqr),Mqr.forEach(t),Cqr.forEach(t),$2o=i(SBe),foe=n(SBe,"SPAN",{});var Eqr=s(foe);I2o=r(Eqr,"AutoModelForSequenceClassification"),Eqr.forEach(t),SBe.forEach(t),ALe=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(uM.$$.fragment,Hs),j2o=i(Hs),rd=n(Hs,"P",{});var rz=s(rd);N2o=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=n(rz,"CODE",{});var yqr=s(moe);D2o=r(yqr,"from_pretrained()"),yqr.forEach(t),q2o=r(rz,"class method or the "),goe=n(rz,"CODE",{});var wqr=s(goe);G2o=r(wqr,"from_config()"),wqr.forEach(t),O2o=r(rz,`class
method.`),rz.forEach(t),X2o=i(Hs),bM=n(Hs,"P",{});var PBe=s(bM);z2o=r(PBe,"This class cannot be instantiated directly using "),hoe=n(PBe,"CODE",{});var Aqr=s(hoe);V2o=r(Aqr,"__init__()"),Aqr.forEach(t),W2o=r(PBe," (throws an error)."),PBe.forEach(t),Q2o=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(vM.$$.fragment,Us),H2o=i(Us),poe=n(Us,"P",{});var Lqr=s(poe);U2o=r(Lqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lqr.forEach(t),J2o=i(Us),td=n(Us,"P",{});var tz=s(td);Y2o=r(tz,`Note:
Loading a model from its configuration file does `),_oe=n(tz,"STRONG",{});var Bqr=s(_oe);K2o=r(Bqr,"not"),Bqr.forEach(t),Z2o=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=n(tz,"CODE",{});var kqr=s(uoe);evo=r(kqr,"from_pretrained()"),kqr.forEach(t),ovo=r(tz,"to load the model weights."),tz.forEach(t),rvo=i(Us),boe=n(Us,"P",{});var xqr=s(boe);tvo=r(xqr,"Examples:"),xqr.forEach(t),avo=i(Us),m(TM.$$.fragment,Us),Us.forEach(t),nvo=i(Hs),$e=n(Hs,"DIV",{class:!0});var Nt=s($e);m(FM.$$.fragment,Nt),svo=i(Nt),voe=n(Nt,"P",{});var Rqr=s(voe);lvo=r(Rqr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rqr.forEach(t),ivo=i(Nt),za=n(Nt,"P",{});var mC=s(za);dvo=r(mC,"The model class to instantiate is selected based on the "),Toe=n(mC,"CODE",{});var Sqr=s(Toe);cvo=r(Sqr,"model_type"),Sqr.forEach(t),fvo=r(mC,` property of the config object (either
passed as an argument or loaded from `),Foe=n(mC,"CODE",{});var Pqr=s(Foe);mvo=r(Pqr,"pretrained_model_name_or_path"),Pqr.forEach(t),gvo=r(mC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=n(mC,"CODE",{});var $qr=s(Coe);hvo=r($qr,"pretrained_model_name_or_path"),$qr.forEach(t),pvo=r(mC,":"),mC.forEach(t),_vo=i(Nt),A=n(Nt,"UL",{});var L=s(A);F1=n(L,"LI",{});var lCe=s(F1);Moe=n(lCe,"STRONG",{});var Iqr=s(Moe);uvo=r(Iqr,"albert"),Iqr.forEach(t),bvo=r(lCe," \u2014 "),t$=n(lCe,"A",{href:!0});var jqr=s(t$);vvo=r(jqr,"AlbertForSequenceClassification"),jqr.forEach(t),Tvo=r(lCe," (ALBERT model)"),lCe.forEach(t),Fvo=i(L),C1=n(L,"LI",{});var iCe=s(C1);Eoe=n(iCe,"STRONG",{});var Nqr=s(Eoe);Cvo=r(Nqr,"bart"),Nqr.forEach(t),Mvo=r(iCe," \u2014 "),a$=n(iCe,"A",{href:!0});var Dqr=s(a$);Evo=r(Dqr,"BartForSequenceClassification"),Dqr.forEach(t),yvo=r(iCe," (BART model)"),iCe.forEach(t),wvo=i(L),M1=n(L,"LI",{});var dCe=s(M1);yoe=n(dCe,"STRONG",{});var qqr=s(yoe);Avo=r(qqr,"bert"),qqr.forEach(t),Lvo=r(dCe," \u2014 "),n$=n(dCe,"A",{href:!0});var Gqr=s(n$);Bvo=r(Gqr,"BertForSequenceClassification"),Gqr.forEach(t),kvo=r(dCe," (BERT model)"),dCe.forEach(t),xvo=i(L),E1=n(L,"LI",{});var cCe=s(E1);woe=n(cCe,"STRONG",{});var Oqr=s(woe);Rvo=r(Oqr,"big_bird"),Oqr.forEach(t),Svo=r(cCe," \u2014 "),s$=n(cCe,"A",{href:!0});var Xqr=s(s$);Pvo=r(Xqr,"BigBirdForSequenceClassification"),Xqr.forEach(t),$vo=r(cCe," (BigBird model)"),cCe.forEach(t),Ivo=i(L),y1=n(L,"LI",{});var fCe=s(y1);Aoe=n(fCe,"STRONG",{});var zqr=s(Aoe);jvo=r(zqr,"bigbird_pegasus"),zqr.forEach(t),Nvo=r(fCe," \u2014 "),l$=n(fCe,"A",{href:!0});var Vqr=s(l$);Dvo=r(Vqr,"BigBirdPegasusForSequenceClassification"),Vqr.forEach(t),qvo=r(fCe," (BigBirdPegasus model)"),fCe.forEach(t),Gvo=i(L),w1=n(L,"LI",{});var mCe=s(w1);Loe=n(mCe,"STRONG",{});var Wqr=s(Loe);Ovo=r(Wqr,"camembert"),Wqr.forEach(t),Xvo=r(mCe," \u2014 "),i$=n(mCe,"A",{href:!0});var Qqr=s(i$);zvo=r(Qqr,"CamembertForSequenceClassification"),Qqr.forEach(t),Vvo=r(mCe," (CamemBERT model)"),mCe.forEach(t),Wvo=i(L),A1=n(L,"LI",{});var gCe=s(A1);Boe=n(gCe,"STRONG",{});var Hqr=s(Boe);Qvo=r(Hqr,"canine"),Hqr.forEach(t),Hvo=r(gCe," \u2014 "),d$=n(gCe,"A",{href:!0});var Uqr=s(d$);Uvo=r(Uqr,"CanineForSequenceClassification"),Uqr.forEach(t),Jvo=r(gCe," (Canine model)"),gCe.forEach(t),Yvo=i(L),L1=n(L,"LI",{});var hCe=s(L1);koe=n(hCe,"STRONG",{});var Jqr=s(koe);Kvo=r(Jqr,"convbert"),Jqr.forEach(t),Zvo=r(hCe," \u2014 "),c$=n(hCe,"A",{href:!0});var Yqr=s(c$);e6o=r(Yqr,"ConvBertForSequenceClassification"),Yqr.forEach(t),o6o=r(hCe," (ConvBERT model)"),hCe.forEach(t),r6o=i(L),B1=n(L,"LI",{});var pCe=s(B1);xoe=n(pCe,"STRONG",{});var Kqr=s(xoe);t6o=r(Kqr,"ctrl"),Kqr.forEach(t),a6o=r(pCe," \u2014 "),f$=n(pCe,"A",{href:!0});var Zqr=s(f$);n6o=r(Zqr,"CTRLForSequenceClassification"),Zqr.forEach(t),s6o=r(pCe," (CTRL model)"),pCe.forEach(t),l6o=i(L),k1=n(L,"LI",{});var _Ce=s(k1);Roe=n(_Ce,"STRONG",{});var eGr=s(Roe);i6o=r(eGr,"deberta"),eGr.forEach(t),d6o=r(_Ce," \u2014 "),m$=n(_Ce,"A",{href:!0});var oGr=s(m$);c6o=r(oGr,"DebertaForSequenceClassification"),oGr.forEach(t),f6o=r(_Ce," (DeBERTa model)"),_Ce.forEach(t),m6o=i(L),x1=n(L,"LI",{});var uCe=s(x1);Soe=n(uCe,"STRONG",{});var rGr=s(Soe);g6o=r(rGr,"deberta-v2"),rGr.forEach(t),h6o=r(uCe," \u2014 "),g$=n(uCe,"A",{href:!0});var tGr=s(g$);p6o=r(tGr,"DebertaV2ForSequenceClassification"),tGr.forEach(t),_6o=r(uCe," (DeBERTa-v2 model)"),uCe.forEach(t),u6o=i(L),R1=n(L,"LI",{});var bCe=s(R1);Poe=n(bCe,"STRONG",{});var aGr=s(Poe);b6o=r(aGr,"distilbert"),aGr.forEach(t),v6o=r(bCe," \u2014 "),h$=n(bCe,"A",{href:!0});var nGr=s(h$);T6o=r(nGr,"DistilBertForSequenceClassification"),nGr.forEach(t),F6o=r(bCe," (DistilBERT model)"),bCe.forEach(t),C6o=i(L),S1=n(L,"LI",{});var vCe=s(S1);$oe=n(vCe,"STRONG",{});var sGr=s($oe);M6o=r(sGr,"electra"),sGr.forEach(t),E6o=r(vCe," \u2014 "),p$=n(vCe,"A",{href:!0});var lGr=s(p$);y6o=r(lGr,"ElectraForSequenceClassification"),lGr.forEach(t),w6o=r(vCe," (ELECTRA model)"),vCe.forEach(t),A6o=i(L),P1=n(L,"LI",{});var TCe=s(P1);Ioe=n(TCe,"STRONG",{});var iGr=s(Ioe);L6o=r(iGr,"flaubert"),iGr.forEach(t),B6o=r(TCe," \u2014 "),_$=n(TCe,"A",{href:!0});var dGr=s(_$);k6o=r(dGr,"FlaubertForSequenceClassification"),dGr.forEach(t),x6o=r(TCe," (FlauBERT model)"),TCe.forEach(t),R6o=i(L),$1=n(L,"LI",{});var FCe=s($1);joe=n(FCe,"STRONG",{});var cGr=s(joe);S6o=r(cGr,"fnet"),cGr.forEach(t),P6o=r(FCe," \u2014 "),u$=n(FCe,"A",{href:!0});var fGr=s(u$);$6o=r(fGr,"FNetForSequenceClassification"),fGr.forEach(t),I6o=r(FCe," (FNet model)"),FCe.forEach(t),j6o=i(L),I1=n(L,"LI",{});var CCe=s(I1);Noe=n(CCe,"STRONG",{});var mGr=s(Noe);N6o=r(mGr,"funnel"),mGr.forEach(t),D6o=r(CCe," \u2014 "),b$=n(CCe,"A",{href:!0});var gGr=s(b$);q6o=r(gGr,"FunnelForSequenceClassification"),gGr.forEach(t),G6o=r(CCe," (Funnel Transformer model)"),CCe.forEach(t),O6o=i(L),j1=n(L,"LI",{});var MCe=s(j1);Doe=n(MCe,"STRONG",{});var hGr=s(Doe);X6o=r(hGr,"gpt2"),hGr.forEach(t),z6o=r(MCe," \u2014 "),v$=n(MCe,"A",{href:!0});var pGr=s(v$);V6o=r(pGr,"GPT2ForSequenceClassification"),pGr.forEach(t),W6o=r(MCe," (OpenAI GPT-2 model)"),MCe.forEach(t),Q6o=i(L),N1=n(L,"LI",{});var ECe=s(N1);qoe=n(ECe,"STRONG",{});var _Gr=s(qoe);H6o=r(_Gr,"gpt_neo"),_Gr.forEach(t),U6o=r(ECe," \u2014 "),T$=n(ECe,"A",{href:!0});var uGr=s(T$);J6o=r(uGr,"GPTNeoForSequenceClassification"),uGr.forEach(t),Y6o=r(ECe," (GPT Neo model)"),ECe.forEach(t),K6o=i(L),D1=n(L,"LI",{});var yCe=s(D1);Goe=n(yCe,"STRONG",{});var bGr=s(Goe);Z6o=r(bGr,"gptj"),bGr.forEach(t),eTo=r(yCe," \u2014 "),F$=n(yCe,"A",{href:!0});var vGr=s(F$);oTo=r(vGr,"GPTJForSequenceClassification"),vGr.forEach(t),rTo=r(yCe," (GPT-J model)"),yCe.forEach(t),tTo=i(L),q1=n(L,"LI",{});var wCe=s(q1);Ooe=n(wCe,"STRONG",{});var TGr=s(Ooe);aTo=r(TGr,"ibert"),TGr.forEach(t),nTo=r(wCe," \u2014 "),C$=n(wCe,"A",{href:!0});var FGr=s(C$);sTo=r(FGr,"IBertForSequenceClassification"),FGr.forEach(t),lTo=r(wCe," (I-BERT model)"),wCe.forEach(t),iTo=i(L),G1=n(L,"LI",{});var ACe=s(G1);Xoe=n(ACe,"STRONG",{});var CGr=s(Xoe);dTo=r(CGr,"layoutlm"),CGr.forEach(t),cTo=r(ACe," \u2014 "),M$=n(ACe,"A",{href:!0});var MGr=s(M$);fTo=r(MGr,"LayoutLMForSequenceClassification"),MGr.forEach(t),mTo=r(ACe," (LayoutLM model)"),ACe.forEach(t),gTo=i(L),O1=n(L,"LI",{});var LCe=s(O1);zoe=n(LCe,"STRONG",{});var EGr=s(zoe);hTo=r(EGr,"layoutlmv2"),EGr.forEach(t),pTo=r(LCe," \u2014 "),E$=n(LCe,"A",{href:!0});var yGr=s(E$);_To=r(yGr,"LayoutLMv2ForSequenceClassification"),yGr.forEach(t),uTo=r(LCe," (LayoutLMv2 model)"),LCe.forEach(t),bTo=i(L),X1=n(L,"LI",{});var BCe=s(X1);Voe=n(BCe,"STRONG",{});var wGr=s(Voe);vTo=r(wGr,"led"),wGr.forEach(t),TTo=r(BCe," \u2014 "),y$=n(BCe,"A",{href:!0});var AGr=s(y$);FTo=r(AGr,"LEDForSequenceClassification"),AGr.forEach(t),CTo=r(BCe," (LED model)"),BCe.forEach(t),MTo=i(L),z1=n(L,"LI",{});var kCe=s(z1);Woe=n(kCe,"STRONG",{});var LGr=s(Woe);ETo=r(LGr,"longformer"),LGr.forEach(t),yTo=r(kCe," \u2014 "),w$=n(kCe,"A",{href:!0});var BGr=s(w$);wTo=r(BGr,"LongformerForSequenceClassification"),BGr.forEach(t),ATo=r(kCe," (Longformer model)"),kCe.forEach(t),LTo=i(L),V1=n(L,"LI",{});var xCe=s(V1);Qoe=n(xCe,"STRONG",{});var kGr=s(Qoe);BTo=r(kGr,"mbart"),kGr.forEach(t),kTo=r(xCe," \u2014 "),A$=n(xCe,"A",{href:!0});var xGr=s(A$);xTo=r(xGr,"MBartForSequenceClassification"),xGr.forEach(t),RTo=r(xCe," (mBART model)"),xCe.forEach(t),STo=i(L),W1=n(L,"LI",{});var RCe=s(W1);Hoe=n(RCe,"STRONG",{});var RGr=s(Hoe);PTo=r(RGr,"megatron-bert"),RGr.forEach(t),$To=r(RCe," \u2014 "),L$=n(RCe,"A",{href:!0});var SGr=s(L$);ITo=r(SGr,"MegatronBertForSequenceClassification"),SGr.forEach(t),jTo=r(RCe," (MegatronBert model)"),RCe.forEach(t),NTo=i(L),Q1=n(L,"LI",{});var SCe=s(Q1);Uoe=n(SCe,"STRONG",{});var PGr=s(Uoe);DTo=r(PGr,"mobilebert"),PGr.forEach(t),qTo=r(SCe," \u2014 "),B$=n(SCe,"A",{href:!0});var $Gr=s(B$);GTo=r($Gr,"MobileBertForSequenceClassification"),$Gr.forEach(t),OTo=r(SCe," (MobileBERT model)"),SCe.forEach(t),XTo=i(L),H1=n(L,"LI",{});var PCe=s(H1);Joe=n(PCe,"STRONG",{});var IGr=s(Joe);zTo=r(IGr,"mpnet"),IGr.forEach(t),VTo=r(PCe," \u2014 "),k$=n(PCe,"A",{href:!0});var jGr=s(k$);WTo=r(jGr,"MPNetForSequenceClassification"),jGr.forEach(t),QTo=r(PCe," (MPNet model)"),PCe.forEach(t),HTo=i(L),U1=n(L,"LI",{});var $Ce=s(U1);Yoe=n($Ce,"STRONG",{});var NGr=s(Yoe);UTo=r(NGr,"nystromformer"),NGr.forEach(t),JTo=r($Ce," \u2014 "),x$=n($Ce,"A",{href:!0});var DGr=s(x$);YTo=r(DGr,"NystromformerForSequenceClassification"),DGr.forEach(t),KTo=r($Ce," (Nystromformer model)"),$Ce.forEach(t),ZTo=i(L),J1=n(L,"LI",{});var ICe=s(J1);Koe=n(ICe,"STRONG",{});var qGr=s(Koe);e7o=r(qGr,"openai-gpt"),qGr.forEach(t),o7o=r(ICe," \u2014 "),R$=n(ICe,"A",{href:!0});var GGr=s(R$);r7o=r(GGr,"OpenAIGPTForSequenceClassification"),GGr.forEach(t),t7o=r(ICe," (OpenAI GPT model)"),ICe.forEach(t),a7o=i(L),Y1=n(L,"LI",{});var jCe=s(Y1);Zoe=n(jCe,"STRONG",{});var OGr=s(Zoe);n7o=r(OGr,"perceiver"),OGr.forEach(t),s7o=r(jCe," \u2014 "),S$=n(jCe,"A",{href:!0});var XGr=s(S$);l7o=r(XGr,"PerceiverForSequenceClassification"),XGr.forEach(t),i7o=r(jCe," (Perceiver model)"),jCe.forEach(t),d7o=i(L),K1=n(L,"LI",{});var NCe=s(K1);ere=n(NCe,"STRONG",{});var zGr=s(ere);c7o=r(zGr,"plbart"),zGr.forEach(t),f7o=r(NCe," \u2014 "),P$=n(NCe,"A",{href:!0});var VGr=s(P$);m7o=r(VGr,"PLBartForSequenceClassification"),VGr.forEach(t),g7o=r(NCe," (PLBart model)"),NCe.forEach(t),h7o=i(L),Z1=n(L,"LI",{});var DCe=s(Z1);ore=n(DCe,"STRONG",{});var WGr=s(ore);p7o=r(WGr,"qdqbert"),WGr.forEach(t),_7o=r(DCe," \u2014 "),$$=n(DCe,"A",{href:!0});var QGr=s($$);u7o=r(QGr,"QDQBertForSequenceClassification"),QGr.forEach(t),b7o=r(DCe," (QDQBert model)"),DCe.forEach(t),v7o=i(L),eb=n(L,"LI",{});var qCe=s(eb);rre=n(qCe,"STRONG",{});var HGr=s(rre);T7o=r(HGr,"reformer"),HGr.forEach(t),F7o=r(qCe," \u2014 "),I$=n(qCe,"A",{href:!0});var UGr=s(I$);C7o=r(UGr,"ReformerForSequenceClassification"),UGr.forEach(t),M7o=r(qCe," (Reformer model)"),qCe.forEach(t),E7o=i(L),ob=n(L,"LI",{});var GCe=s(ob);tre=n(GCe,"STRONG",{});var JGr=s(tre);y7o=r(JGr,"rembert"),JGr.forEach(t),w7o=r(GCe," \u2014 "),j$=n(GCe,"A",{href:!0});var YGr=s(j$);A7o=r(YGr,"RemBertForSequenceClassification"),YGr.forEach(t),L7o=r(GCe," (RemBERT model)"),GCe.forEach(t),B7o=i(L),rb=n(L,"LI",{});var OCe=s(rb);are=n(OCe,"STRONG",{});var KGr=s(are);k7o=r(KGr,"roberta"),KGr.forEach(t),x7o=r(OCe," \u2014 "),N$=n(OCe,"A",{href:!0});var ZGr=s(N$);R7o=r(ZGr,"RobertaForSequenceClassification"),ZGr.forEach(t),S7o=r(OCe," (RoBERTa model)"),OCe.forEach(t),P7o=i(L),tb=n(L,"LI",{});var XCe=s(tb);nre=n(XCe,"STRONG",{});var eOr=s(nre);$7o=r(eOr,"roformer"),eOr.forEach(t),I7o=r(XCe," \u2014 "),D$=n(XCe,"A",{href:!0});var oOr=s(D$);j7o=r(oOr,"RoFormerForSequenceClassification"),oOr.forEach(t),N7o=r(XCe," (RoFormer model)"),XCe.forEach(t),D7o=i(L),ab=n(L,"LI",{});var zCe=s(ab);sre=n(zCe,"STRONG",{});var rOr=s(sre);q7o=r(rOr,"squeezebert"),rOr.forEach(t),G7o=r(zCe," \u2014 "),q$=n(zCe,"A",{href:!0});var tOr=s(q$);O7o=r(tOr,"SqueezeBertForSequenceClassification"),tOr.forEach(t),X7o=r(zCe," (SqueezeBERT model)"),zCe.forEach(t),z7o=i(L),nb=n(L,"LI",{});var VCe=s(nb);lre=n(VCe,"STRONG",{});var aOr=s(lre);V7o=r(aOr,"tapas"),aOr.forEach(t),W7o=r(VCe," \u2014 "),G$=n(VCe,"A",{href:!0});var nOr=s(G$);Q7o=r(nOr,"TapasForSequenceClassification"),nOr.forEach(t),H7o=r(VCe," (TAPAS model)"),VCe.forEach(t),U7o=i(L),sb=n(L,"LI",{});var WCe=s(sb);ire=n(WCe,"STRONG",{});var sOr=s(ire);J7o=r(sOr,"transfo-xl"),sOr.forEach(t),Y7o=r(WCe," \u2014 "),O$=n(WCe,"A",{href:!0});var lOr=s(O$);K7o=r(lOr,"TransfoXLForSequenceClassification"),lOr.forEach(t),Z7o=r(WCe," (Transformer-XL model)"),WCe.forEach(t),eFo=i(L),lb=n(L,"LI",{});var QCe=s(lb);dre=n(QCe,"STRONG",{});var iOr=s(dre);oFo=r(iOr,"xlm"),iOr.forEach(t),rFo=r(QCe," \u2014 "),X$=n(QCe,"A",{href:!0});var dOr=s(X$);tFo=r(dOr,"XLMForSequenceClassification"),dOr.forEach(t),aFo=r(QCe," (XLM model)"),QCe.forEach(t),nFo=i(L),ib=n(L,"LI",{});var HCe=s(ib);cre=n(HCe,"STRONG",{});var cOr=s(cre);sFo=r(cOr,"xlm-roberta"),cOr.forEach(t),lFo=r(HCe," \u2014 "),z$=n(HCe,"A",{href:!0});var fOr=s(z$);iFo=r(fOr,"XLMRobertaForSequenceClassification"),fOr.forEach(t),dFo=r(HCe," (XLM-RoBERTa model)"),HCe.forEach(t),cFo=i(L),db=n(L,"LI",{});var UCe=s(db);fre=n(UCe,"STRONG",{});var mOr=s(fre);fFo=r(mOr,"xlm-roberta-xl"),mOr.forEach(t),mFo=r(UCe," \u2014 "),V$=n(UCe,"A",{href:!0});var gOr=s(V$);gFo=r(gOr,"XLMRobertaXLForSequenceClassification"),gOr.forEach(t),hFo=r(UCe," (XLM-RoBERTa-XL model)"),UCe.forEach(t),pFo=i(L),cb=n(L,"LI",{});var JCe=s(cb);mre=n(JCe,"STRONG",{});var hOr=s(mre);_Fo=r(hOr,"xlnet"),hOr.forEach(t),uFo=r(JCe," \u2014 "),W$=n(JCe,"A",{href:!0});var pOr=s(W$);bFo=r(pOr,"XLNetForSequenceClassification"),pOr.forEach(t),vFo=r(JCe," (XLNet model)"),JCe.forEach(t),TFo=i(L),fb=n(L,"LI",{});var YCe=s(fb);gre=n(YCe,"STRONG",{});var _Or=s(gre);FFo=r(_Or,"yoso"),_Or.forEach(t),CFo=r(YCe," \u2014 "),Q$=n(YCe,"A",{href:!0});var uOr=s(Q$);MFo=r(uOr,"YosoForSequenceClassification"),uOr.forEach(t),EFo=r(YCe," (YOSO model)"),YCe.forEach(t),L.forEach(t),yFo=i(Nt),mb=n(Nt,"P",{});var KCe=s(mb);wFo=r(KCe,"The model is set in evaluation mode by default using "),hre=n(KCe,"CODE",{});var bOr=s(hre);AFo=r(bOr,"model.eval()"),bOr.forEach(t),LFo=r(KCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=n(KCe,"CODE",{});var vOr=s(pre);BFo=r(vOr,"model.train()"),vOr.forEach(t),KCe.forEach(t),kFo=i(Nt),_re=n(Nt,"P",{});var TOr=s(_re);xFo=r(TOr,"Examples:"),TOr.forEach(t),RFo=i(Nt),m(CM.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),LLe=i(d),ad=n(d,"H2",{class:!0});var $Be=s(ad);gb=n($Be,"A",{id:!0,class:!0,href:!0});var FOr=s(gb);ure=n(FOr,"SPAN",{});var COr=s(ure);m(MM.$$.fragment,COr),COr.forEach(t),FOr.forEach(t),SFo=i($Be),bre=n($Be,"SPAN",{});var MOr=s(bre);PFo=r(MOr,"AutoModelForMultipleChoice"),MOr.forEach(t),$Be.forEach(t),BLe=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(EM.$$.fragment,Js),$Fo=i(Js),nd=n(Js,"P",{});var az=s(nd);IFo=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=n(az,"CODE",{});var EOr=s(vre);jFo=r(EOr,"from_pretrained()"),EOr.forEach(t),NFo=r(az,"class method or the "),Tre=n(az,"CODE",{});var yOr=s(Tre);DFo=r(yOr,"from_config()"),yOr.forEach(t),qFo=r(az,`class
method.`),az.forEach(t),GFo=i(Js),yM=n(Js,"P",{});var IBe=s(yM);OFo=r(IBe,"This class cannot be instantiated directly using "),Fre=n(IBe,"CODE",{});var wOr=s(Fre);XFo=r(wOr,"__init__()"),wOr.forEach(t),zFo=r(IBe," (throws an error)."),IBe.forEach(t),VFo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(wM.$$.fragment,Ys),WFo=i(Ys),Cre=n(Ys,"P",{});var AOr=s(Cre);QFo=r(AOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AOr.forEach(t),HFo=i(Ys),sd=n(Ys,"P",{});var nz=s(sd);UFo=r(nz,`Note:
Loading a model from its configuration file does `),Mre=n(nz,"STRONG",{});var LOr=s(Mre);JFo=r(LOr,"not"),LOr.forEach(t),YFo=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=n(nz,"CODE",{});var BOr=s(Ere);KFo=r(BOr,"from_pretrained()"),BOr.forEach(t),ZFo=r(nz,"to load the model weights."),nz.forEach(t),e9o=i(Ys),yre=n(Ys,"P",{});var kOr=s(yre);o9o=r(kOr,"Examples:"),kOr.forEach(t),r9o=i(Ys),m(AM.$$.fragment,Ys),Ys.forEach(t),t9o=i(Js),Ie=n(Js,"DIV",{class:!0});var Dt=s(Ie);m(LM.$$.fragment,Dt),a9o=i(Dt),wre=n(Dt,"P",{});var xOr=s(wre);n9o=r(xOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),xOr.forEach(t),s9o=i(Dt),Va=n(Dt,"P",{});var gC=s(Va);l9o=r(gC,"The model class to instantiate is selected based on the "),Are=n(gC,"CODE",{});var ROr=s(Are);i9o=r(ROr,"model_type"),ROr.forEach(t),d9o=r(gC,` property of the config object (either
passed as an argument or loaded from `),Lre=n(gC,"CODE",{});var SOr=s(Lre);c9o=r(SOr,"pretrained_model_name_or_path"),SOr.forEach(t),f9o=r(gC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=n(gC,"CODE",{});var POr=s(Bre);m9o=r(POr,"pretrained_model_name_or_path"),POr.forEach(t),g9o=r(gC,":"),gC.forEach(t),h9o=i(Dt),G=n(Dt,"UL",{});var O=s(G);hb=n(O,"LI",{});var ZCe=s(hb);kre=n(ZCe,"STRONG",{});var $Or=s(kre);p9o=r($Or,"albert"),$Or.forEach(t),_9o=r(ZCe," \u2014 "),H$=n(ZCe,"A",{href:!0});var IOr=s(H$);u9o=r(IOr,"AlbertForMultipleChoice"),IOr.forEach(t),b9o=r(ZCe," (ALBERT model)"),ZCe.forEach(t),v9o=i(O),pb=n(O,"LI",{});var e4e=s(pb);xre=n(e4e,"STRONG",{});var jOr=s(xre);T9o=r(jOr,"bert"),jOr.forEach(t),F9o=r(e4e," \u2014 "),U$=n(e4e,"A",{href:!0});var NOr=s(U$);C9o=r(NOr,"BertForMultipleChoice"),NOr.forEach(t),M9o=r(e4e," (BERT model)"),e4e.forEach(t),E9o=i(O),_b=n(O,"LI",{});var o4e=s(_b);Rre=n(o4e,"STRONG",{});var DOr=s(Rre);y9o=r(DOr,"big_bird"),DOr.forEach(t),w9o=r(o4e," \u2014 "),J$=n(o4e,"A",{href:!0});var qOr=s(J$);A9o=r(qOr,"BigBirdForMultipleChoice"),qOr.forEach(t),L9o=r(o4e," (BigBird model)"),o4e.forEach(t),B9o=i(O),ub=n(O,"LI",{});var r4e=s(ub);Sre=n(r4e,"STRONG",{});var GOr=s(Sre);k9o=r(GOr,"camembert"),GOr.forEach(t),x9o=r(r4e," \u2014 "),Y$=n(r4e,"A",{href:!0});var OOr=s(Y$);R9o=r(OOr,"CamembertForMultipleChoice"),OOr.forEach(t),S9o=r(r4e," (CamemBERT model)"),r4e.forEach(t),P9o=i(O),bb=n(O,"LI",{});var t4e=s(bb);Pre=n(t4e,"STRONG",{});var XOr=s(Pre);$9o=r(XOr,"canine"),XOr.forEach(t),I9o=r(t4e," \u2014 "),K$=n(t4e,"A",{href:!0});var zOr=s(K$);j9o=r(zOr,"CanineForMultipleChoice"),zOr.forEach(t),N9o=r(t4e," (Canine model)"),t4e.forEach(t),D9o=i(O),vb=n(O,"LI",{});var a4e=s(vb);$re=n(a4e,"STRONG",{});var VOr=s($re);q9o=r(VOr,"convbert"),VOr.forEach(t),G9o=r(a4e," \u2014 "),Z$=n(a4e,"A",{href:!0});var WOr=s(Z$);O9o=r(WOr,"ConvBertForMultipleChoice"),WOr.forEach(t),X9o=r(a4e," (ConvBERT model)"),a4e.forEach(t),z9o=i(O),Tb=n(O,"LI",{});var n4e=s(Tb);Ire=n(n4e,"STRONG",{});var QOr=s(Ire);V9o=r(QOr,"distilbert"),QOr.forEach(t),W9o=r(n4e," \u2014 "),eI=n(n4e,"A",{href:!0});var HOr=s(eI);Q9o=r(HOr,"DistilBertForMultipleChoice"),HOr.forEach(t),H9o=r(n4e," (DistilBERT model)"),n4e.forEach(t),U9o=i(O),Fb=n(O,"LI",{});var s4e=s(Fb);jre=n(s4e,"STRONG",{});var UOr=s(jre);J9o=r(UOr,"electra"),UOr.forEach(t),Y9o=r(s4e," \u2014 "),oI=n(s4e,"A",{href:!0});var JOr=s(oI);K9o=r(JOr,"ElectraForMultipleChoice"),JOr.forEach(t),Z9o=r(s4e," (ELECTRA model)"),s4e.forEach(t),eCo=i(O),Cb=n(O,"LI",{});var l4e=s(Cb);Nre=n(l4e,"STRONG",{});var YOr=s(Nre);oCo=r(YOr,"flaubert"),YOr.forEach(t),rCo=r(l4e," \u2014 "),rI=n(l4e,"A",{href:!0});var KOr=s(rI);tCo=r(KOr,"FlaubertForMultipleChoice"),KOr.forEach(t),aCo=r(l4e," (FlauBERT model)"),l4e.forEach(t),nCo=i(O),Mb=n(O,"LI",{});var i4e=s(Mb);Dre=n(i4e,"STRONG",{});var ZOr=s(Dre);sCo=r(ZOr,"fnet"),ZOr.forEach(t),lCo=r(i4e," \u2014 "),tI=n(i4e,"A",{href:!0});var eXr=s(tI);iCo=r(eXr,"FNetForMultipleChoice"),eXr.forEach(t),dCo=r(i4e," (FNet model)"),i4e.forEach(t),cCo=i(O),Eb=n(O,"LI",{});var d4e=s(Eb);qre=n(d4e,"STRONG",{});var oXr=s(qre);fCo=r(oXr,"funnel"),oXr.forEach(t),mCo=r(d4e," \u2014 "),aI=n(d4e,"A",{href:!0});var rXr=s(aI);gCo=r(rXr,"FunnelForMultipleChoice"),rXr.forEach(t),hCo=r(d4e," (Funnel Transformer model)"),d4e.forEach(t),pCo=i(O),yb=n(O,"LI",{});var c4e=s(yb);Gre=n(c4e,"STRONG",{});var tXr=s(Gre);_Co=r(tXr,"ibert"),tXr.forEach(t),uCo=r(c4e," \u2014 "),nI=n(c4e,"A",{href:!0});var aXr=s(nI);bCo=r(aXr,"IBertForMultipleChoice"),aXr.forEach(t),vCo=r(c4e," (I-BERT model)"),c4e.forEach(t),TCo=i(O),wb=n(O,"LI",{});var f4e=s(wb);Ore=n(f4e,"STRONG",{});var nXr=s(Ore);FCo=r(nXr,"longformer"),nXr.forEach(t),CCo=r(f4e," \u2014 "),sI=n(f4e,"A",{href:!0});var sXr=s(sI);MCo=r(sXr,"LongformerForMultipleChoice"),sXr.forEach(t),ECo=r(f4e," (Longformer model)"),f4e.forEach(t),yCo=i(O),Ab=n(O,"LI",{});var m4e=s(Ab);Xre=n(m4e,"STRONG",{});var lXr=s(Xre);wCo=r(lXr,"megatron-bert"),lXr.forEach(t),ACo=r(m4e," \u2014 "),lI=n(m4e,"A",{href:!0});var iXr=s(lI);LCo=r(iXr,"MegatronBertForMultipleChoice"),iXr.forEach(t),BCo=r(m4e," (MegatronBert model)"),m4e.forEach(t),kCo=i(O),Lb=n(O,"LI",{});var g4e=s(Lb);zre=n(g4e,"STRONG",{});var dXr=s(zre);xCo=r(dXr,"mobilebert"),dXr.forEach(t),RCo=r(g4e," \u2014 "),iI=n(g4e,"A",{href:!0});var cXr=s(iI);SCo=r(cXr,"MobileBertForMultipleChoice"),cXr.forEach(t),PCo=r(g4e," (MobileBERT model)"),g4e.forEach(t),$Co=i(O),Bb=n(O,"LI",{});var h4e=s(Bb);Vre=n(h4e,"STRONG",{});var fXr=s(Vre);ICo=r(fXr,"mpnet"),fXr.forEach(t),jCo=r(h4e," \u2014 "),dI=n(h4e,"A",{href:!0});var mXr=s(dI);NCo=r(mXr,"MPNetForMultipleChoice"),mXr.forEach(t),DCo=r(h4e," (MPNet model)"),h4e.forEach(t),qCo=i(O),kb=n(O,"LI",{});var p4e=s(kb);Wre=n(p4e,"STRONG",{});var gXr=s(Wre);GCo=r(gXr,"nystromformer"),gXr.forEach(t),OCo=r(p4e," \u2014 "),cI=n(p4e,"A",{href:!0});var hXr=s(cI);XCo=r(hXr,"NystromformerForMultipleChoice"),hXr.forEach(t),zCo=r(p4e," (Nystromformer model)"),p4e.forEach(t),VCo=i(O),xb=n(O,"LI",{});var _4e=s(xb);Qre=n(_4e,"STRONG",{});var pXr=s(Qre);WCo=r(pXr,"qdqbert"),pXr.forEach(t),QCo=r(_4e," \u2014 "),fI=n(_4e,"A",{href:!0});var _Xr=s(fI);HCo=r(_Xr,"QDQBertForMultipleChoice"),_Xr.forEach(t),UCo=r(_4e," (QDQBert model)"),_4e.forEach(t),JCo=i(O),Rb=n(O,"LI",{});var u4e=s(Rb);Hre=n(u4e,"STRONG",{});var uXr=s(Hre);YCo=r(uXr,"rembert"),uXr.forEach(t),KCo=r(u4e," \u2014 "),mI=n(u4e,"A",{href:!0});var bXr=s(mI);ZCo=r(bXr,"RemBertForMultipleChoice"),bXr.forEach(t),e4o=r(u4e," (RemBERT model)"),u4e.forEach(t),o4o=i(O),Sb=n(O,"LI",{});var b4e=s(Sb);Ure=n(b4e,"STRONG",{});var vXr=s(Ure);r4o=r(vXr,"roberta"),vXr.forEach(t),t4o=r(b4e," \u2014 "),gI=n(b4e,"A",{href:!0});var TXr=s(gI);a4o=r(TXr,"RobertaForMultipleChoice"),TXr.forEach(t),n4o=r(b4e," (RoBERTa model)"),b4e.forEach(t),s4o=i(O),Pb=n(O,"LI",{});var v4e=s(Pb);Jre=n(v4e,"STRONG",{});var FXr=s(Jre);l4o=r(FXr,"roformer"),FXr.forEach(t),i4o=r(v4e," \u2014 "),hI=n(v4e,"A",{href:!0});var CXr=s(hI);d4o=r(CXr,"RoFormerForMultipleChoice"),CXr.forEach(t),c4o=r(v4e," (RoFormer model)"),v4e.forEach(t),f4o=i(O),$b=n(O,"LI",{});var T4e=s($b);Yre=n(T4e,"STRONG",{});var MXr=s(Yre);m4o=r(MXr,"squeezebert"),MXr.forEach(t),g4o=r(T4e," \u2014 "),pI=n(T4e,"A",{href:!0});var EXr=s(pI);h4o=r(EXr,"SqueezeBertForMultipleChoice"),EXr.forEach(t),p4o=r(T4e," (SqueezeBERT model)"),T4e.forEach(t),_4o=i(O),Ib=n(O,"LI",{});var F4e=s(Ib);Kre=n(F4e,"STRONG",{});var yXr=s(Kre);u4o=r(yXr,"xlm"),yXr.forEach(t),b4o=r(F4e," \u2014 "),_I=n(F4e,"A",{href:!0});var wXr=s(_I);v4o=r(wXr,"XLMForMultipleChoice"),wXr.forEach(t),T4o=r(F4e," (XLM model)"),F4e.forEach(t),F4o=i(O),jb=n(O,"LI",{});var C4e=s(jb);Zre=n(C4e,"STRONG",{});var AXr=s(Zre);C4o=r(AXr,"xlm-roberta"),AXr.forEach(t),M4o=r(C4e," \u2014 "),uI=n(C4e,"A",{href:!0});var LXr=s(uI);E4o=r(LXr,"XLMRobertaForMultipleChoice"),LXr.forEach(t),y4o=r(C4e," (XLM-RoBERTa model)"),C4e.forEach(t),w4o=i(O),Nb=n(O,"LI",{});var M4e=s(Nb);ete=n(M4e,"STRONG",{});var BXr=s(ete);A4o=r(BXr,"xlm-roberta-xl"),BXr.forEach(t),L4o=r(M4e," \u2014 "),bI=n(M4e,"A",{href:!0});var kXr=s(bI);B4o=r(kXr,"XLMRobertaXLForMultipleChoice"),kXr.forEach(t),k4o=r(M4e," (XLM-RoBERTa-XL model)"),M4e.forEach(t),x4o=i(O),Db=n(O,"LI",{});var E4e=s(Db);ote=n(E4e,"STRONG",{});var xXr=s(ote);R4o=r(xXr,"xlnet"),xXr.forEach(t),S4o=r(E4e," \u2014 "),vI=n(E4e,"A",{href:!0});var RXr=s(vI);P4o=r(RXr,"XLNetForMultipleChoice"),RXr.forEach(t),$4o=r(E4e," (XLNet model)"),E4e.forEach(t),I4o=i(O),qb=n(O,"LI",{});var y4e=s(qb);rte=n(y4e,"STRONG",{});var SXr=s(rte);j4o=r(SXr,"yoso"),SXr.forEach(t),N4o=r(y4e," \u2014 "),TI=n(y4e,"A",{href:!0});var PXr=s(TI);D4o=r(PXr,"YosoForMultipleChoice"),PXr.forEach(t),q4o=r(y4e," (YOSO model)"),y4e.forEach(t),O.forEach(t),G4o=i(Dt),Gb=n(Dt,"P",{});var w4e=s(Gb);O4o=r(w4e,"The model is set in evaluation mode by default using "),tte=n(w4e,"CODE",{});var $Xr=s(tte);X4o=r($Xr,"model.eval()"),$Xr.forEach(t),z4o=r(w4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=n(w4e,"CODE",{});var IXr=s(ate);V4o=r(IXr,"model.train()"),IXr.forEach(t),w4e.forEach(t),W4o=i(Dt),nte=n(Dt,"P",{});var jXr=s(nte);Q4o=r(jXr,"Examples:"),jXr.forEach(t),H4o=i(Dt),m(BM.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),kLe=i(d),ld=n(d,"H2",{class:!0});var jBe=s(ld);Ob=n(jBe,"A",{id:!0,class:!0,href:!0});var NXr=s(Ob);ste=n(NXr,"SPAN",{});var DXr=s(ste);m(kM.$$.fragment,DXr),DXr.forEach(t),NXr.forEach(t),U4o=i(jBe),lte=n(jBe,"SPAN",{});var qXr=s(lte);J4o=r(qXr,"AutoModelForNextSentencePrediction"),qXr.forEach(t),jBe.forEach(t),xLe=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m(xM.$$.fragment,Ks),Y4o=i(Ks),id=n(Ks,"P",{});var sz=s(id);K4o=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=n(sz,"CODE",{});var GXr=s(ite);Z4o=r(GXr,"from_pretrained()"),GXr.forEach(t),eMo=r(sz,"class method or the "),dte=n(sz,"CODE",{});var OXr=s(dte);oMo=r(OXr,"from_config()"),OXr.forEach(t),rMo=r(sz,`class
method.`),sz.forEach(t),tMo=i(Ks),RM=n(Ks,"P",{});var NBe=s(RM);aMo=r(NBe,"This class cannot be instantiated directly using "),cte=n(NBe,"CODE",{});var XXr=s(cte);nMo=r(XXr,"__init__()"),XXr.forEach(t),sMo=r(NBe," (throws an error)."),NBe.forEach(t),lMo=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(SM.$$.fragment,Zs),iMo=i(Zs),fte=n(Zs,"P",{});var zXr=s(fte);dMo=r(zXr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zXr.forEach(t),cMo=i(Zs),dd=n(Zs,"P",{});var lz=s(dd);fMo=r(lz,`Note:
Loading a model from its configuration file does `),mte=n(lz,"STRONG",{});var VXr=s(mte);mMo=r(VXr,"not"),VXr.forEach(t),gMo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=n(lz,"CODE",{});var WXr=s(gte);hMo=r(WXr,"from_pretrained()"),WXr.forEach(t),pMo=r(lz,"to load the model weights."),lz.forEach(t),_Mo=i(Zs),hte=n(Zs,"P",{});var QXr=s(hte);uMo=r(QXr,"Examples:"),QXr.forEach(t),bMo=i(Zs),m(PM.$$.fragment,Zs),Zs.forEach(t),vMo=i(Ks),je=n(Ks,"DIV",{class:!0});var qt=s(je);m($M.$$.fragment,qt),TMo=i(qt),pte=n(qt,"P",{});var HXr=s(pte);FMo=r(HXr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),HXr.forEach(t),CMo=i(qt),Wa=n(qt,"P",{});var hC=s(Wa);MMo=r(hC,"The model class to instantiate is selected based on the "),_te=n(hC,"CODE",{});var UXr=s(_te);EMo=r(UXr,"model_type"),UXr.forEach(t),yMo=r(hC,` property of the config object (either
passed as an argument or loaded from `),ute=n(hC,"CODE",{});var JXr=s(ute);wMo=r(JXr,"pretrained_model_name_or_path"),JXr.forEach(t),AMo=r(hC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=n(hC,"CODE",{});var YXr=s(bte);LMo=r(YXr,"pretrained_model_name_or_path"),YXr.forEach(t),BMo=r(hC,":"),hC.forEach(t),kMo=i(qt),na=n(qt,"UL",{});var el=s(na);Xb=n(el,"LI",{});var A4e=s(Xb);vte=n(A4e,"STRONG",{});var KXr=s(vte);xMo=r(KXr,"bert"),KXr.forEach(t),RMo=r(A4e," \u2014 "),FI=n(A4e,"A",{href:!0});var ZXr=s(FI);SMo=r(ZXr,"BertForNextSentencePrediction"),ZXr.forEach(t),PMo=r(A4e," (BERT model)"),A4e.forEach(t),$Mo=i(el),zb=n(el,"LI",{});var L4e=s(zb);Tte=n(L4e,"STRONG",{});var ezr=s(Tte);IMo=r(ezr,"fnet"),ezr.forEach(t),jMo=r(L4e," \u2014 "),CI=n(L4e,"A",{href:!0});var ozr=s(CI);NMo=r(ozr,"FNetForNextSentencePrediction"),ozr.forEach(t),DMo=r(L4e," (FNet model)"),L4e.forEach(t),qMo=i(el),Vb=n(el,"LI",{});var B4e=s(Vb);Fte=n(B4e,"STRONG",{});var rzr=s(Fte);GMo=r(rzr,"megatron-bert"),rzr.forEach(t),OMo=r(B4e," \u2014 "),MI=n(B4e,"A",{href:!0});var tzr=s(MI);XMo=r(tzr,"MegatronBertForNextSentencePrediction"),tzr.forEach(t),zMo=r(B4e," (MegatronBert model)"),B4e.forEach(t),VMo=i(el),Wb=n(el,"LI",{});var k4e=s(Wb);Cte=n(k4e,"STRONG",{});var azr=s(Cte);WMo=r(azr,"mobilebert"),azr.forEach(t),QMo=r(k4e," \u2014 "),EI=n(k4e,"A",{href:!0});var nzr=s(EI);HMo=r(nzr,"MobileBertForNextSentencePrediction"),nzr.forEach(t),UMo=r(k4e," (MobileBERT model)"),k4e.forEach(t),JMo=i(el),Qb=n(el,"LI",{});var x4e=s(Qb);Mte=n(x4e,"STRONG",{});var szr=s(Mte);YMo=r(szr,"qdqbert"),szr.forEach(t),KMo=r(x4e," \u2014 "),yI=n(x4e,"A",{href:!0});var lzr=s(yI);ZMo=r(lzr,"QDQBertForNextSentencePrediction"),lzr.forEach(t),eEo=r(x4e," (QDQBert model)"),x4e.forEach(t),el.forEach(t),oEo=i(qt),Hb=n(qt,"P",{});var R4e=s(Hb);rEo=r(R4e,"The model is set in evaluation mode by default using "),Ete=n(R4e,"CODE",{});var izr=s(Ete);tEo=r(izr,"model.eval()"),izr.forEach(t),aEo=r(R4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=n(R4e,"CODE",{});var dzr=s(yte);nEo=r(dzr,"model.train()"),dzr.forEach(t),R4e.forEach(t),sEo=i(qt),wte=n(qt,"P",{});var czr=s(wte);lEo=r(czr,"Examples:"),czr.forEach(t),iEo=i(qt),m(IM.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),RLe=i(d),cd=n(d,"H2",{class:!0});var DBe=s(cd);Ub=n(DBe,"A",{id:!0,class:!0,href:!0});var fzr=s(Ub);Ate=n(fzr,"SPAN",{});var mzr=s(Ate);m(jM.$$.fragment,mzr),mzr.forEach(t),fzr.forEach(t),dEo=i(DBe),Lte=n(DBe,"SPAN",{});var gzr=s(Lte);cEo=r(gzr,"AutoModelForTokenClassification"),gzr.forEach(t),DBe.forEach(t),SLe=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(NM.$$.fragment,ol),fEo=i(ol),fd=n(ol,"P",{});var iz=s(fd);mEo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=n(iz,"CODE",{});var hzr=s(Bte);gEo=r(hzr,"from_pretrained()"),hzr.forEach(t),hEo=r(iz,"class method or the "),kte=n(iz,"CODE",{});var pzr=s(kte);pEo=r(pzr,"from_config()"),pzr.forEach(t),_Eo=r(iz,`class
method.`),iz.forEach(t),uEo=i(ol),DM=n(ol,"P",{});var qBe=s(DM);bEo=r(qBe,"This class cannot be instantiated directly using "),xte=n(qBe,"CODE",{});var _zr=s(xte);vEo=r(_zr,"__init__()"),_zr.forEach(t),TEo=r(qBe," (throws an error)."),qBe.forEach(t),FEo=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(qM.$$.fragment,rl),CEo=i(rl),Rte=n(rl,"P",{});var uzr=s(Rte);MEo=r(uzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),uzr.forEach(t),EEo=i(rl),md=n(rl,"P",{});var dz=s(md);yEo=r(dz,`Note:
Loading a model from its configuration file does `),Ste=n(dz,"STRONG",{});var bzr=s(Ste);wEo=r(bzr,"not"),bzr.forEach(t),AEo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=n(dz,"CODE",{});var vzr=s(Pte);LEo=r(vzr,"from_pretrained()"),vzr.forEach(t),BEo=r(dz,"to load the model weights."),dz.forEach(t),kEo=i(rl),$te=n(rl,"P",{});var Tzr=s($te);xEo=r(Tzr,"Examples:"),Tzr.forEach(t),REo=i(rl),m(GM.$$.fragment,rl),rl.forEach(t),SEo=i(ol),Ne=n(ol,"DIV",{class:!0});var Gt=s(Ne);m(OM.$$.fragment,Gt),PEo=i(Gt),Ite=n(Gt,"P",{});var Fzr=s(Ite);$Eo=r(Fzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Fzr.forEach(t),IEo=i(Gt),Qa=n(Gt,"P",{});var pC=s(Qa);jEo=r(pC,"The model class to instantiate is selected based on the "),jte=n(pC,"CODE",{});var Czr=s(jte);NEo=r(Czr,"model_type"),Czr.forEach(t),DEo=r(pC,` property of the config object (either
passed as an argument or loaded from `),Nte=n(pC,"CODE",{});var Mzr=s(Nte);qEo=r(Mzr,"pretrained_model_name_or_path"),Mzr.forEach(t),GEo=r(pC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=n(pC,"CODE",{});var Ezr=s(Dte);OEo=r(Ezr,"pretrained_model_name_or_path"),Ezr.forEach(t),XEo=r(pC,":"),pC.forEach(t),zEo=i(Gt),D=n(Gt,"UL",{});var q=s(D);Jb=n(q,"LI",{});var S4e=s(Jb);qte=n(S4e,"STRONG",{});var yzr=s(qte);VEo=r(yzr,"albert"),yzr.forEach(t),WEo=r(S4e," \u2014 "),wI=n(S4e,"A",{href:!0});var wzr=s(wI);QEo=r(wzr,"AlbertForTokenClassification"),wzr.forEach(t),HEo=r(S4e," (ALBERT model)"),S4e.forEach(t),UEo=i(q),Yb=n(q,"LI",{});var P4e=s(Yb);Gte=n(P4e,"STRONG",{});var Azr=s(Gte);JEo=r(Azr,"bert"),Azr.forEach(t),YEo=r(P4e," \u2014 "),AI=n(P4e,"A",{href:!0});var Lzr=s(AI);KEo=r(Lzr,"BertForTokenClassification"),Lzr.forEach(t),ZEo=r(P4e," (BERT model)"),P4e.forEach(t),e3o=i(q),Kb=n(q,"LI",{});var $4e=s(Kb);Ote=n($4e,"STRONG",{});var Bzr=s(Ote);o3o=r(Bzr,"big_bird"),Bzr.forEach(t),r3o=r($4e," \u2014 "),LI=n($4e,"A",{href:!0});var kzr=s(LI);t3o=r(kzr,"BigBirdForTokenClassification"),kzr.forEach(t),a3o=r($4e," (BigBird model)"),$4e.forEach(t),n3o=i(q),Zb=n(q,"LI",{});var I4e=s(Zb);Xte=n(I4e,"STRONG",{});var xzr=s(Xte);s3o=r(xzr,"camembert"),xzr.forEach(t),l3o=r(I4e," \u2014 "),BI=n(I4e,"A",{href:!0});var Rzr=s(BI);i3o=r(Rzr,"CamembertForTokenClassification"),Rzr.forEach(t),d3o=r(I4e," (CamemBERT model)"),I4e.forEach(t),c3o=i(q),e5=n(q,"LI",{});var j4e=s(e5);zte=n(j4e,"STRONG",{});var Szr=s(zte);f3o=r(Szr,"canine"),Szr.forEach(t),m3o=r(j4e," \u2014 "),kI=n(j4e,"A",{href:!0});var Pzr=s(kI);g3o=r(Pzr,"CanineForTokenClassification"),Pzr.forEach(t),h3o=r(j4e," (Canine model)"),j4e.forEach(t),p3o=i(q),o5=n(q,"LI",{});var N4e=s(o5);Vte=n(N4e,"STRONG",{});var $zr=s(Vte);_3o=r($zr,"convbert"),$zr.forEach(t),u3o=r(N4e," \u2014 "),xI=n(N4e,"A",{href:!0});var Izr=s(xI);b3o=r(Izr,"ConvBertForTokenClassification"),Izr.forEach(t),v3o=r(N4e," (ConvBERT model)"),N4e.forEach(t),T3o=i(q),r5=n(q,"LI",{});var D4e=s(r5);Wte=n(D4e,"STRONG",{});var jzr=s(Wte);F3o=r(jzr,"deberta"),jzr.forEach(t),C3o=r(D4e," \u2014 "),RI=n(D4e,"A",{href:!0});var Nzr=s(RI);M3o=r(Nzr,"DebertaForTokenClassification"),Nzr.forEach(t),E3o=r(D4e," (DeBERTa model)"),D4e.forEach(t),y3o=i(q),t5=n(q,"LI",{});var q4e=s(t5);Qte=n(q4e,"STRONG",{});var Dzr=s(Qte);w3o=r(Dzr,"deberta-v2"),Dzr.forEach(t),A3o=r(q4e," \u2014 "),SI=n(q4e,"A",{href:!0});var qzr=s(SI);L3o=r(qzr,"DebertaV2ForTokenClassification"),qzr.forEach(t),B3o=r(q4e," (DeBERTa-v2 model)"),q4e.forEach(t),k3o=i(q),a5=n(q,"LI",{});var G4e=s(a5);Hte=n(G4e,"STRONG",{});var Gzr=s(Hte);x3o=r(Gzr,"distilbert"),Gzr.forEach(t),R3o=r(G4e," \u2014 "),PI=n(G4e,"A",{href:!0});var Ozr=s(PI);S3o=r(Ozr,"DistilBertForTokenClassification"),Ozr.forEach(t),P3o=r(G4e," (DistilBERT model)"),G4e.forEach(t),$3o=i(q),n5=n(q,"LI",{});var O4e=s(n5);Ute=n(O4e,"STRONG",{});var Xzr=s(Ute);I3o=r(Xzr,"electra"),Xzr.forEach(t),j3o=r(O4e," \u2014 "),$I=n(O4e,"A",{href:!0});var zzr=s($I);N3o=r(zzr,"ElectraForTokenClassification"),zzr.forEach(t),D3o=r(O4e," (ELECTRA model)"),O4e.forEach(t),q3o=i(q),s5=n(q,"LI",{});var X4e=s(s5);Jte=n(X4e,"STRONG",{});var Vzr=s(Jte);G3o=r(Vzr,"flaubert"),Vzr.forEach(t),O3o=r(X4e," \u2014 "),II=n(X4e,"A",{href:!0});var Wzr=s(II);X3o=r(Wzr,"FlaubertForTokenClassification"),Wzr.forEach(t),z3o=r(X4e," (FlauBERT model)"),X4e.forEach(t),V3o=i(q),l5=n(q,"LI",{});var z4e=s(l5);Yte=n(z4e,"STRONG",{});var Qzr=s(Yte);W3o=r(Qzr,"fnet"),Qzr.forEach(t),Q3o=r(z4e," \u2014 "),jI=n(z4e,"A",{href:!0});var Hzr=s(jI);H3o=r(Hzr,"FNetForTokenClassification"),Hzr.forEach(t),U3o=r(z4e," (FNet model)"),z4e.forEach(t),J3o=i(q),i5=n(q,"LI",{});var V4e=s(i5);Kte=n(V4e,"STRONG",{});var Uzr=s(Kte);Y3o=r(Uzr,"funnel"),Uzr.forEach(t),K3o=r(V4e," \u2014 "),NI=n(V4e,"A",{href:!0});var Jzr=s(NI);Z3o=r(Jzr,"FunnelForTokenClassification"),Jzr.forEach(t),eyo=r(V4e," (Funnel Transformer model)"),V4e.forEach(t),oyo=i(q),d5=n(q,"LI",{});var W4e=s(d5);Zte=n(W4e,"STRONG",{});var Yzr=s(Zte);ryo=r(Yzr,"gpt2"),Yzr.forEach(t),tyo=r(W4e," \u2014 "),DI=n(W4e,"A",{href:!0});var Kzr=s(DI);ayo=r(Kzr,"GPT2ForTokenClassification"),Kzr.forEach(t),nyo=r(W4e," (OpenAI GPT-2 model)"),W4e.forEach(t),syo=i(q),c5=n(q,"LI",{});var Q4e=s(c5);eae=n(Q4e,"STRONG",{});var Zzr=s(eae);lyo=r(Zzr,"ibert"),Zzr.forEach(t),iyo=r(Q4e," \u2014 "),qI=n(Q4e,"A",{href:!0});var eVr=s(qI);dyo=r(eVr,"IBertForTokenClassification"),eVr.forEach(t),cyo=r(Q4e," (I-BERT model)"),Q4e.forEach(t),fyo=i(q),f5=n(q,"LI",{});var H4e=s(f5);oae=n(H4e,"STRONG",{});var oVr=s(oae);myo=r(oVr,"layoutlm"),oVr.forEach(t),gyo=r(H4e," \u2014 "),GI=n(H4e,"A",{href:!0});var rVr=s(GI);hyo=r(rVr,"LayoutLMForTokenClassification"),rVr.forEach(t),pyo=r(H4e," (LayoutLM model)"),H4e.forEach(t),_yo=i(q),m5=n(q,"LI",{});var U4e=s(m5);rae=n(U4e,"STRONG",{});var tVr=s(rae);uyo=r(tVr,"layoutlmv2"),tVr.forEach(t),byo=r(U4e," \u2014 "),OI=n(U4e,"A",{href:!0});var aVr=s(OI);vyo=r(aVr,"LayoutLMv2ForTokenClassification"),aVr.forEach(t),Tyo=r(U4e," (LayoutLMv2 model)"),U4e.forEach(t),Fyo=i(q),g5=n(q,"LI",{});var J4e=s(g5);tae=n(J4e,"STRONG",{});var nVr=s(tae);Cyo=r(nVr,"longformer"),nVr.forEach(t),Myo=r(J4e," \u2014 "),XI=n(J4e,"A",{href:!0});var sVr=s(XI);Eyo=r(sVr,"LongformerForTokenClassification"),sVr.forEach(t),yyo=r(J4e," (Longformer model)"),J4e.forEach(t),wyo=i(q),h5=n(q,"LI",{});var Y4e=s(h5);aae=n(Y4e,"STRONG",{});var lVr=s(aae);Ayo=r(lVr,"megatron-bert"),lVr.forEach(t),Lyo=r(Y4e," \u2014 "),zI=n(Y4e,"A",{href:!0});var iVr=s(zI);Byo=r(iVr,"MegatronBertForTokenClassification"),iVr.forEach(t),kyo=r(Y4e," (MegatronBert model)"),Y4e.forEach(t),xyo=i(q),p5=n(q,"LI",{});var K4e=s(p5);nae=n(K4e,"STRONG",{});var dVr=s(nae);Ryo=r(dVr,"mobilebert"),dVr.forEach(t),Syo=r(K4e," \u2014 "),VI=n(K4e,"A",{href:!0});var cVr=s(VI);Pyo=r(cVr,"MobileBertForTokenClassification"),cVr.forEach(t),$yo=r(K4e," (MobileBERT model)"),K4e.forEach(t),Iyo=i(q),_5=n(q,"LI",{});var Z4e=s(_5);sae=n(Z4e,"STRONG",{});var fVr=s(sae);jyo=r(fVr,"mpnet"),fVr.forEach(t),Nyo=r(Z4e," \u2014 "),WI=n(Z4e,"A",{href:!0});var mVr=s(WI);Dyo=r(mVr,"MPNetForTokenClassification"),mVr.forEach(t),qyo=r(Z4e," (MPNet model)"),Z4e.forEach(t),Gyo=i(q),u5=n(q,"LI",{});var eMe=s(u5);lae=n(eMe,"STRONG",{});var gVr=s(lae);Oyo=r(gVr,"nystromformer"),gVr.forEach(t),Xyo=r(eMe," \u2014 "),QI=n(eMe,"A",{href:!0});var hVr=s(QI);zyo=r(hVr,"NystromformerForTokenClassification"),hVr.forEach(t),Vyo=r(eMe," (Nystromformer model)"),eMe.forEach(t),Wyo=i(q),b5=n(q,"LI",{});var oMe=s(b5);iae=n(oMe,"STRONG",{});var pVr=s(iae);Qyo=r(pVr,"qdqbert"),pVr.forEach(t),Hyo=r(oMe," \u2014 "),HI=n(oMe,"A",{href:!0});var _Vr=s(HI);Uyo=r(_Vr,"QDQBertForTokenClassification"),_Vr.forEach(t),Jyo=r(oMe," (QDQBert model)"),oMe.forEach(t),Yyo=i(q),v5=n(q,"LI",{});var rMe=s(v5);dae=n(rMe,"STRONG",{});var uVr=s(dae);Kyo=r(uVr,"rembert"),uVr.forEach(t),Zyo=r(rMe," \u2014 "),UI=n(rMe,"A",{href:!0});var bVr=s(UI);ewo=r(bVr,"RemBertForTokenClassification"),bVr.forEach(t),owo=r(rMe," (RemBERT model)"),rMe.forEach(t),rwo=i(q),T5=n(q,"LI",{});var tMe=s(T5);cae=n(tMe,"STRONG",{});var vVr=s(cae);two=r(vVr,"roberta"),vVr.forEach(t),awo=r(tMe," \u2014 "),JI=n(tMe,"A",{href:!0});var TVr=s(JI);nwo=r(TVr,"RobertaForTokenClassification"),TVr.forEach(t),swo=r(tMe," (RoBERTa model)"),tMe.forEach(t),lwo=i(q),F5=n(q,"LI",{});var aMe=s(F5);fae=n(aMe,"STRONG",{});var FVr=s(fae);iwo=r(FVr,"roformer"),FVr.forEach(t),dwo=r(aMe," \u2014 "),YI=n(aMe,"A",{href:!0});var CVr=s(YI);cwo=r(CVr,"RoFormerForTokenClassification"),CVr.forEach(t),fwo=r(aMe," (RoFormer model)"),aMe.forEach(t),mwo=i(q),C5=n(q,"LI",{});var nMe=s(C5);mae=n(nMe,"STRONG",{});var MVr=s(mae);gwo=r(MVr,"squeezebert"),MVr.forEach(t),hwo=r(nMe," \u2014 "),KI=n(nMe,"A",{href:!0});var EVr=s(KI);pwo=r(EVr,"SqueezeBertForTokenClassification"),EVr.forEach(t),_wo=r(nMe," (SqueezeBERT model)"),nMe.forEach(t),uwo=i(q),M5=n(q,"LI",{});var sMe=s(M5);gae=n(sMe,"STRONG",{});var yVr=s(gae);bwo=r(yVr,"xlm"),yVr.forEach(t),vwo=r(sMe," \u2014 "),ZI=n(sMe,"A",{href:!0});var wVr=s(ZI);Two=r(wVr,"XLMForTokenClassification"),wVr.forEach(t),Fwo=r(sMe," (XLM model)"),sMe.forEach(t),Cwo=i(q),E5=n(q,"LI",{});var lMe=s(E5);hae=n(lMe,"STRONG",{});var AVr=s(hae);Mwo=r(AVr,"xlm-roberta"),AVr.forEach(t),Ewo=r(lMe," \u2014 "),ej=n(lMe,"A",{href:!0});var LVr=s(ej);ywo=r(LVr,"XLMRobertaForTokenClassification"),LVr.forEach(t),wwo=r(lMe," (XLM-RoBERTa model)"),lMe.forEach(t),Awo=i(q),y5=n(q,"LI",{});var iMe=s(y5);pae=n(iMe,"STRONG",{});var BVr=s(pae);Lwo=r(BVr,"xlm-roberta-xl"),BVr.forEach(t),Bwo=r(iMe," \u2014 "),oj=n(iMe,"A",{href:!0});var kVr=s(oj);kwo=r(kVr,"XLMRobertaXLForTokenClassification"),kVr.forEach(t),xwo=r(iMe," (XLM-RoBERTa-XL model)"),iMe.forEach(t),Rwo=i(q),w5=n(q,"LI",{});var dMe=s(w5);_ae=n(dMe,"STRONG",{});var xVr=s(_ae);Swo=r(xVr,"xlnet"),xVr.forEach(t),Pwo=r(dMe," \u2014 "),rj=n(dMe,"A",{href:!0});var RVr=s(rj);$wo=r(RVr,"XLNetForTokenClassification"),RVr.forEach(t),Iwo=r(dMe," (XLNet model)"),dMe.forEach(t),jwo=i(q),A5=n(q,"LI",{});var cMe=s(A5);uae=n(cMe,"STRONG",{});var SVr=s(uae);Nwo=r(SVr,"yoso"),SVr.forEach(t),Dwo=r(cMe," \u2014 "),tj=n(cMe,"A",{href:!0});var PVr=s(tj);qwo=r(PVr,"YosoForTokenClassification"),PVr.forEach(t),Gwo=r(cMe," (YOSO model)"),cMe.forEach(t),q.forEach(t),Owo=i(Gt),L5=n(Gt,"P",{});var fMe=s(L5);Xwo=r(fMe,"The model is set in evaluation mode by default using "),bae=n(fMe,"CODE",{});var $Vr=s(bae);zwo=r($Vr,"model.eval()"),$Vr.forEach(t),Vwo=r(fMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=n(fMe,"CODE",{});var IVr=s(vae);Wwo=r(IVr,"model.train()"),IVr.forEach(t),fMe.forEach(t),Qwo=i(Gt),Tae=n(Gt,"P",{});var jVr=s(Tae);Hwo=r(jVr,"Examples:"),jVr.forEach(t),Uwo=i(Gt),m(XM.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),PLe=i(d),gd=n(d,"H2",{class:!0});var GBe=s(gd);B5=n(GBe,"A",{id:!0,class:!0,href:!0});var NVr=s(B5);Fae=n(NVr,"SPAN",{});var DVr=s(Fae);m(zM.$$.fragment,DVr),DVr.forEach(t),NVr.forEach(t),Jwo=i(GBe),Cae=n(GBe,"SPAN",{});var qVr=s(Cae);Ywo=r(qVr,"AutoModelForQuestionAnswering"),qVr.forEach(t),GBe.forEach(t),$Le=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(VM.$$.fragment,tl),Kwo=i(tl),hd=n(tl,"P",{});var cz=s(hd);Zwo=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=n(cz,"CODE",{});var GVr=s(Mae);eAo=r(GVr,"from_pretrained()"),GVr.forEach(t),oAo=r(cz,"class method or the "),Eae=n(cz,"CODE",{});var OVr=s(Eae);rAo=r(OVr,"from_config()"),OVr.forEach(t),tAo=r(cz,`class
method.`),cz.forEach(t),aAo=i(tl),WM=n(tl,"P",{});var OBe=s(WM);nAo=r(OBe,"This class cannot be instantiated directly using "),yae=n(OBe,"CODE",{});var XVr=s(yae);sAo=r(XVr,"__init__()"),XVr.forEach(t),lAo=r(OBe," (throws an error)."),OBe.forEach(t),iAo=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(QM.$$.fragment,al),dAo=i(al),wae=n(al,"P",{});var zVr=s(wae);cAo=r(zVr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),zVr.forEach(t),fAo=i(al),pd=n(al,"P",{});var fz=s(pd);mAo=r(fz,`Note:
Loading a model from its configuration file does `),Aae=n(fz,"STRONG",{});var VVr=s(Aae);gAo=r(VVr,"not"),VVr.forEach(t),hAo=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=n(fz,"CODE",{});var WVr=s(Lae);pAo=r(WVr,"from_pretrained()"),WVr.forEach(t),_Ao=r(fz,"to load the model weights."),fz.forEach(t),uAo=i(al),Bae=n(al,"P",{});var QVr=s(Bae);bAo=r(QVr,"Examples:"),QVr.forEach(t),vAo=i(al),m(HM.$$.fragment,al),al.forEach(t),TAo=i(tl),De=n(tl,"DIV",{class:!0});var Ot=s(De);m(UM.$$.fragment,Ot),FAo=i(Ot),kae=n(Ot,"P",{});var HVr=s(kae);CAo=r(HVr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),HVr.forEach(t),MAo=i(Ot),Ha=n(Ot,"P",{});var _C=s(Ha);EAo=r(_C,"The model class to instantiate is selected based on the "),xae=n(_C,"CODE",{});var UVr=s(xae);yAo=r(UVr,"model_type"),UVr.forEach(t),wAo=r(_C,` property of the config object (either
passed as an argument or loaded from `),Rae=n(_C,"CODE",{});var JVr=s(Rae);AAo=r(JVr,"pretrained_model_name_or_path"),JVr.forEach(t),LAo=r(_C,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=n(_C,"CODE",{});var YVr=s(Sae);BAo=r(YVr,"pretrained_model_name_or_path"),YVr.forEach(t),kAo=r(_C,":"),_C.forEach(t),xAo=i(Ot),R=n(Ot,"UL",{});var P=s(R);k5=n(P,"LI",{});var mMe=s(k5);Pae=n(mMe,"STRONG",{});var KVr=s(Pae);RAo=r(KVr,"albert"),KVr.forEach(t),SAo=r(mMe," \u2014 "),aj=n(mMe,"A",{href:!0});var ZVr=s(aj);PAo=r(ZVr,"AlbertForQuestionAnswering"),ZVr.forEach(t),$Ao=r(mMe," (ALBERT model)"),mMe.forEach(t),IAo=i(P),x5=n(P,"LI",{});var gMe=s(x5);$ae=n(gMe,"STRONG",{});var eWr=s($ae);jAo=r(eWr,"bart"),eWr.forEach(t),NAo=r(gMe," \u2014 "),nj=n(gMe,"A",{href:!0});var oWr=s(nj);DAo=r(oWr,"BartForQuestionAnswering"),oWr.forEach(t),qAo=r(gMe," (BART model)"),gMe.forEach(t),GAo=i(P),R5=n(P,"LI",{});var hMe=s(R5);Iae=n(hMe,"STRONG",{});var rWr=s(Iae);OAo=r(rWr,"bert"),rWr.forEach(t),XAo=r(hMe," \u2014 "),sj=n(hMe,"A",{href:!0});var tWr=s(sj);zAo=r(tWr,"BertForQuestionAnswering"),tWr.forEach(t),VAo=r(hMe," (BERT model)"),hMe.forEach(t),WAo=i(P),S5=n(P,"LI",{});var pMe=s(S5);jae=n(pMe,"STRONG",{});var aWr=s(jae);QAo=r(aWr,"big_bird"),aWr.forEach(t),HAo=r(pMe," \u2014 "),lj=n(pMe,"A",{href:!0});var nWr=s(lj);UAo=r(nWr,"BigBirdForQuestionAnswering"),nWr.forEach(t),JAo=r(pMe," (BigBird model)"),pMe.forEach(t),YAo=i(P),P5=n(P,"LI",{});var _Me=s(P5);Nae=n(_Me,"STRONG",{});var sWr=s(Nae);KAo=r(sWr,"bigbird_pegasus"),sWr.forEach(t),ZAo=r(_Me," \u2014 "),ij=n(_Me,"A",{href:!0});var lWr=s(ij);e0o=r(lWr,"BigBirdPegasusForQuestionAnswering"),lWr.forEach(t),o0o=r(_Me," (BigBirdPegasus model)"),_Me.forEach(t),r0o=i(P),$5=n(P,"LI",{});var uMe=s($5);Dae=n(uMe,"STRONG",{});var iWr=s(Dae);t0o=r(iWr,"camembert"),iWr.forEach(t),a0o=r(uMe," \u2014 "),dj=n(uMe,"A",{href:!0});var dWr=s(dj);n0o=r(dWr,"CamembertForQuestionAnswering"),dWr.forEach(t),s0o=r(uMe," (CamemBERT model)"),uMe.forEach(t),l0o=i(P),I5=n(P,"LI",{});var bMe=s(I5);qae=n(bMe,"STRONG",{});var cWr=s(qae);i0o=r(cWr,"canine"),cWr.forEach(t),d0o=r(bMe," \u2014 "),cj=n(bMe,"A",{href:!0});var fWr=s(cj);c0o=r(fWr,"CanineForQuestionAnswering"),fWr.forEach(t),f0o=r(bMe," (Canine model)"),bMe.forEach(t),m0o=i(P),j5=n(P,"LI",{});var vMe=s(j5);Gae=n(vMe,"STRONG",{});var mWr=s(Gae);g0o=r(mWr,"convbert"),mWr.forEach(t),h0o=r(vMe," \u2014 "),fj=n(vMe,"A",{href:!0});var gWr=s(fj);p0o=r(gWr,"ConvBertForQuestionAnswering"),gWr.forEach(t),_0o=r(vMe," (ConvBERT model)"),vMe.forEach(t),u0o=i(P),N5=n(P,"LI",{});var TMe=s(N5);Oae=n(TMe,"STRONG",{});var hWr=s(Oae);b0o=r(hWr,"deberta"),hWr.forEach(t),v0o=r(TMe," \u2014 "),mj=n(TMe,"A",{href:!0});var pWr=s(mj);T0o=r(pWr,"DebertaForQuestionAnswering"),pWr.forEach(t),F0o=r(TMe," (DeBERTa model)"),TMe.forEach(t),C0o=i(P),D5=n(P,"LI",{});var FMe=s(D5);Xae=n(FMe,"STRONG",{});var _Wr=s(Xae);M0o=r(_Wr,"deberta-v2"),_Wr.forEach(t),E0o=r(FMe," \u2014 "),gj=n(FMe,"A",{href:!0});var uWr=s(gj);y0o=r(uWr,"DebertaV2ForQuestionAnswering"),uWr.forEach(t),w0o=r(FMe," (DeBERTa-v2 model)"),FMe.forEach(t),A0o=i(P),q5=n(P,"LI",{});var CMe=s(q5);zae=n(CMe,"STRONG",{});var bWr=s(zae);L0o=r(bWr,"distilbert"),bWr.forEach(t),B0o=r(CMe," \u2014 "),hj=n(CMe,"A",{href:!0});var vWr=s(hj);k0o=r(vWr,"DistilBertForQuestionAnswering"),vWr.forEach(t),x0o=r(CMe," (DistilBERT model)"),CMe.forEach(t),R0o=i(P),G5=n(P,"LI",{});var MMe=s(G5);Vae=n(MMe,"STRONG",{});var TWr=s(Vae);S0o=r(TWr,"electra"),TWr.forEach(t),P0o=r(MMe," \u2014 "),pj=n(MMe,"A",{href:!0});var FWr=s(pj);$0o=r(FWr,"ElectraForQuestionAnswering"),FWr.forEach(t),I0o=r(MMe," (ELECTRA model)"),MMe.forEach(t),j0o=i(P),O5=n(P,"LI",{});var EMe=s(O5);Wae=n(EMe,"STRONG",{});var CWr=s(Wae);N0o=r(CWr,"flaubert"),CWr.forEach(t),D0o=r(EMe," \u2014 "),_j=n(EMe,"A",{href:!0});var MWr=s(_j);q0o=r(MWr,"FlaubertForQuestionAnsweringSimple"),MWr.forEach(t),G0o=r(EMe," (FlauBERT model)"),EMe.forEach(t),O0o=i(P),X5=n(P,"LI",{});var yMe=s(X5);Qae=n(yMe,"STRONG",{});var EWr=s(Qae);X0o=r(EWr,"fnet"),EWr.forEach(t),z0o=r(yMe," \u2014 "),uj=n(yMe,"A",{href:!0});var yWr=s(uj);V0o=r(yWr,"FNetForQuestionAnswering"),yWr.forEach(t),W0o=r(yMe," (FNet model)"),yMe.forEach(t),Q0o=i(P),z5=n(P,"LI",{});var wMe=s(z5);Hae=n(wMe,"STRONG",{});var wWr=s(Hae);H0o=r(wWr,"funnel"),wWr.forEach(t),U0o=r(wMe," \u2014 "),bj=n(wMe,"A",{href:!0});var AWr=s(bj);J0o=r(AWr,"FunnelForQuestionAnswering"),AWr.forEach(t),Y0o=r(wMe," (Funnel Transformer model)"),wMe.forEach(t),K0o=i(P),V5=n(P,"LI",{});var AMe=s(V5);Uae=n(AMe,"STRONG",{});var LWr=s(Uae);Z0o=r(LWr,"gptj"),LWr.forEach(t),eLo=r(AMe," \u2014 "),vj=n(AMe,"A",{href:!0});var BWr=s(vj);oLo=r(BWr,"GPTJForQuestionAnswering"),BWr.forEach(t),rLo=r(AMe," (GPT-J model)"),AMe.forEach(t),tLo=i(P),W5=n(P,"LI",{});var LMe=s(W5);Jae=n(LMe,"STRONG",{});var kWr=s(Jae);aLo=r(kWr,"ibert"),kWr.forEach(t),nLo=r(LMe," \u2014 "),Tj=n(LMe,"A",{href:!0});var xWr=s(Tj);sLo=r(xWr,"IBertForQuestionAnswering"),xWr.forEach(t),lLo=r(LMe," (I-BERT model)"),LMe.forEach(t),iLo=i(P),Q5=n(P,"LI",{});var BMe=s(Q5);Yae=n(BMe,"STRONG",{});var RWr=s(Yae);dLo=r(RWr,"layoutlmv2"),RWr.forEach(t),cLo=r(BMe," \u2014 "),Fj=n(BMe,"A",{href:!0});var SWr=s(Fj);fLo=r(SWr,"LayoutLMv2ForQuestionAnswering"),SWr.forEach(t),mLo=r(BMe," (LayoutLMv2 model)"),BMe.forEach(t),gLo=i(P),H5=n(P,"LI",{});var kMe=s(H5);Kae=n(kMe,"STRONG",{});var PWr=s(Kae);hLo=r(PWr,"led"),PWr.forEach(t),pLo=r(kMe," \u2014 "),Cj=n(kMe,"A",{href:!0});var $Wr=s(Cj);_Lo=r($Wr,"LEDForQuestionAnswering"),$Wr.forEach(t),uLo=r(kMe," (LED model)"),kMe.forEach(t),bLo=i(P),U5=n(P,"LI",{});var xMe=s(U5);Zae=n(xMe,"STRONG",{});var IWr=s(Zae);vLo=r(IWr,"longformer"),IWr.forEach(t),TLo=r(xMe," \u2014 "),Mj=n(xMe,"A",{href:!0});var jWr=s(Mj);FLo=r(jWr,"LongformerForQuestionAnswering"),jWr.forEach(t),CLo=r(xMe," (Longformer model)"),xMe.forEach(t),MLo=i(P),J5=n(P,"LI",{});var RMe=s(J5);ene=n(RMe,"STRONG",{});var NWr=s(ene);ELo=r(NWr,"lxmert"),NWr.forEach(t),yLo=r(RMe," \u2014 "),Ej=n(RMe,"A",{href:!0});var DWr=s(Ej);wLo=r(DWr,"LxmertForQuestionAnswering"),DWr.forEach(t),ALo=r(RMe," (LXMERT model)"),RMe.forEach(t),LLo=i(P),Y5=n(P,"LI",{});var SMe=s(Y5);one=n(SMe,"STRONG",{});var qWr=s(one);BLo=r(qWr,"mbart"),qWr.forEach(t),kLo=r(SMe," \u2014 "),yj=n(SMe,"A",{href:!0});var GWr=s(yj);xLo=r(GWr,"MBartForQuestionAnswering"),GWr.forEach(t),RLo=r(SMe," (mBART model)"),SMe.forEach(t),SLo=i(P),K5=n(P,"LI",{});var PMe=s(K5);rne=n(PMe,"STRONG",{});var OWr=s(rne);PLo=r(OWr,"megatron-bert"),OWr.forEach(t),$Lo=r(PMe," \u2014 "),wj=n(PMe,"A",{href:!0});var XWr=s(wj);ILo=r(XWr,"MegatronBertForQuestionAnswering"),XWr.forEach(t),jLo=r(PMe," (MegatronBert model)"),PMe.forEach(t),NLo=i(P),Z5=n(P,"LI",{});var $Me=s(Z5);tne=n($Me,"STRONG",{});var zWr=s(tne);DLo=r(zWr,"mobilebert"),zWr.forEach(t),qLo=r($Me," \u2014 "),Aj=n($Me,"A",{href:!0});var VWr=s(Aj);GLo=r(VWr,"MobileBertForQuestionAnswering"),VWr.forEach(t),OLo=r($Me," (MobileBERT model)"),$Me.forEach(t),XLo=i(P),e2=n(P,"LI",{});var IMe=s(e2);ane=n(IMe,"STRONG",{});var WWr=s(ane);zLo=r(WWr,"mpnet"),WWr.forEach(t),VLo=r(IMe," \u2014 "),Lj=n(IMe,"A",{href:!0});var QWr=s(Lj);WLo=r(QWr,"MPNetForQuestionAnswering"),QWr.forEach(t),QLo=r(IMe," (MPNet model)"),IMe.forEach(t),HLo=i(P),o2=n(P,"LI",{});var jMe=s(o2);nne=n(jMe,"STRONG",{});var HWr=s(nne);ULo=r(HWr,"nystromformer"),HWr.forEach(t),JLo=r(jMe," \u2014 "),Bj=n(jMe,"A",{href:!0});var UWr=s(Bj);YLo=r(UWr,"NystromformerForQuestionAnswering"),UWr.forEach(t),KLo=r(jMe," (Nystromformer model)"),jMe.forEach(t),ZLo=i(P),r2=n(P,"LI",{});var NMe=s(r2);sne=n(NMe,"STRONG",{});var JWr=s(sne);e8o=r(JWr,"qdqbert"),JWr.forEach(t),o8o=r(NMe," \u2014 "),kj=n(NMe,"A",{href:!0});var YWr=s(kj);r8o=r(YWr,"QDQBertForQuestionAnswering"),YWr.forEach(t),t8o=r(NMe," (QDQBert model)"),NMe.forEach(t),a8o=i(P),t2=n(P,"LI",{});var DMe=s(t2);lne=n(DMe,"STRONG",{});var KWr=s(lne);n8o=r(KWr,"reformer"),KWr.forEach(t),s8o=r(DMe," \u2014 "),xj=n(DMe,"A",{href:!0});var ZWr=s(xj);l8o=r(ZWr,"ReformerForQuestionAnswering"),ZWr.forEach(t),i8o=r(DMe," (Reformer model)"),DMe.forEach(t),d8o=i(P),a2=n(P,"LI",{});var qMe=s(a2);ine=n(qMe,"STRONG",{});var eQr=s(ine);c8o=r(eQr,"rembert"),eQr.forEach(t),f8o=r(qMe," \u2014 "),Rj=n(qMe,"A",{href:!0});var oQr=s(Rj);m8o=r(oQr,"RemBertForQuestionAnswering"),oQr.forEach(t),g8o=r(qMe," (RemBERT model)"),qMe.forEach(t),h8o=i(P),n2=n(P,"LI",{});var GMe=s(n2);dne=n(GMe,"STRONG",{});var rQr=s(dne);p8o=r(rQr,"roberta"),rQr.forEach(t),_8o=r(GMe," \u2014 "),Sj=n(GMe,"A",{href:!0});var tQr=s(Sj);u8o=r(tQr,"RobertaForQuestionAnswering"),tQr.forEach(t),b8o=r(GMe," (RoBERTa model)"),GMe.forEach(t),v8o=i(P),s2=n(P,"LI",{});var OMe=s(s2);cne=n(OMe,"STRONG",{});var aQr=s(cne);T8o=r(aQr,"roformer"),aQr.forEach(t),F8o=r(OMe," \u2014 "),Pj=n(OMe,"A",{href:!0});var nQr=s(Pj);C8o=r(nQr,"RoFormerForQuestionAnswering"),nQr.forEach(t),M8o=r(OMe," (RoFormer model)"),OMe.forEach(t),E8o=i(P),l2=n(P,"LI",{});var XMe=s(l2);fne=n(XMe,"STRONG",{});var sQr=s(fne);y8o=r(sQr,"splinter"),sQr.forEach(t),w8o=r(XMe," \u2014 "),$j=n(XMe,"A",{href:!0});var lQr=s($j);A8o=r(lQr,"SplinterForQuestionAnswering"),lQr.forEach(t),L8o=r(XMe," (Splinter model)"),XMe.forEach(t),B8o=i(P),i2=n(P,"LI",{});var zMe=s(i2);mne=n(zMe,"STRONG",{});var iQr=s(mne);k8o=r(iQr,"squeezebert"),iQr.forEach(t),x8o=r(zMe," \u2014 "),Ij=n(zMe,"A",{href:!0});var dQr=s(Ij);R8o=r(dQr,"SqueezeBertForQuestionAnswering"),dQr.forEach(t),S8o=r(zMe," (SqueezeBERT model)"),zMe.forEach(t),P8o=i(P),d2=n(P,"LI",{});var VMe=s(d2);gne=n(VMe,"STRONG",{});var cQr=s(gne);$8o=r(cQr,"xlm"),cQr.forEach(t),I8o=r(VMe," \u2014 "),jj=n(VMe,"A",{href:!0});var fQr=s(jj);j8o=r(fQr,"XLMForQuestionAnsweringSimple"),fQr.forEach(t),N8o=r(VMe," (XLM model)"),VMe.forEach(t),D8o=i(P),c2=n(P,"LI",{});var WMe=s(c2);hne=n(WMe,"STRONG",{});var mQr=s(hne);q8o=r(mQr,"xlm-roberta"),mQr.forEach(t),G8o=r(WMe," \u2014 "),Nj=n(WMe,"A",{href:!0});var gQr=s(Nj);O8o=r(gQr,"XLMRobertaForQuestionAnswering"),gQr.forEach(t),X8o=r(WMe," (XLM-RoBERTa model)"),WMe.forEach(t),z8o=i(P),f2=n(P,"LI",{});var QMe=s(f2);pne=n(QMe,"STRONG",{});var hQr=s(pne);V8o=r(hQr,"xlm-roberta-xl"),hQr.forEach(t),W8o=r(QMe," \u2014 "),Dj=n(QMe,"A",{href:!0});var pQr=s(Dj);Q8o=r(pQr,"XLMRobertaXLForQuestionAnswering"),pQr.forEach(t),H8o=r(QMe," (XLM-RoBERTa-XL model)"),QMe.forEach(t),U8o=i(P),m2=n(P,"LI",{});var HMe=s(m2);_ne=n(HMe,"STRONG",{});var _Qr=s(_ne);J8o=r(_Qr,"xlnet"),_Qr.forEach(t),Y8o=r(HMe," \u2014 "),qj=n(HMe,"A",{href:!0});var uQr=s(qj);K8o=r(uQr,"XLNetForQuestionAnsweringSimple"),uQr.forEach(t),Z8o=r(HMe," (XLNet model)"),HMe.forEach(t),eBo=i(P),g2=n(P,"LI",{});var UMe=s(g2);une=n(UMe,"STRONG",{});var bQr=s(une);oBo=r(bQr,"yoso"),bQr.forEach(t),rBo=r(UMe," \u2014 "),Gj=n(UMe,"A",{href:!0});var vQr=s(Gj);tBo=r(vQr,"YosoForQuestionAnswering"),vQr.forEach(t),aBo=r(UMe," (YOSO model)"),UMe.forEach(t),P.forEach(t),nBo=i(Ot),h2=n(Ot,"P",{});var JMe=s(h2);sBo=r(JMe,"The model is set in evaluation mode by default using "),bne=n(JMe,"CODE",{});var TQr=s(bne);lBo=r(TQr,"model.eval()"),TQr.forEach(t),iBo=r(JMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=n(JMe,"CODE",{});var FQr=s(vne);dBo=r(FQr,"model.train()"),FQr.forEach(t),JMe.forEach(t),cBo=i(Ot),Tne=n(Ot,"P",{});var CQr=s(Tne);fBo=r(CQr,"Examples:"),CQr.forEach(t),mBo=i(Ot),m(JM.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),ILe=i(d),_d=n(d,"H2",{class:!0});var XBe=s(_d);p2=n(XBe,"A",{id:!0,class:!0,href:!0});var MQr=s(p2);Fne=n(MQr,"SPAN",{});var EQr=s(Fne);m(YM.$$.fragment,EQr),EQr.forEach(t),MQr.forEach(t),gBo=i(XBe),Cne=n(XBe,"SPAN",{});var yQr=s(Cne);hBo=r(yQr,"AutoModelForTableQuestionAnswering"),yQr.forEach(t),XBe.forEach(t),jLe=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(KM.$$.fragment,nl),pBo=i(nl),ud=n(nl,"P",{});var mz=s(ud);_Bo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=n(mz,"CODE",{});var wQr=s(Mne);uBo=r(wQr,"from_pretrained()"),wQr.forEach(t),bBo=r(mz,"class method or the "),Ene=n(mz,"CODE",{});var AQr=s(Ene);vBo=r(AQr,"from_config()"),AQr.forEach(t),TBo=r(mz,`class
method.`),mz.forEach(t),FBo=i(nl),ZM=n(nl,"P",{});var zBe=s(ZM);CBo=r(zBe,"This class cannot be instantiated directly using "),yne=n(zBe,"CODE",{});var LQr=s(yne);MBo=r(LQr,"__init__()"),LQr.forEach(t),EBo=r(zBe," (throws an error)."),zBe.forEach(t),yBo=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(eE.$$.fragment,sl),wBo=i(sl),wne=n(sl,"P",{});var BQr=s(wne);ABo=r(BQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),BQr.forEach(t),LBo=i(sl),bd=n(sl,"P",{});var gz=s(bd);BBo=r(gz,`Note:
Loading a model from its configuration file does `),Ane=n(gz,"STRONG",{});var kQr=s(Ane);kBo=r(kQr,"not"),kQr.forEach(t),xBo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=n(gz,"CODE",{});var xQr=s(Lne);RBo=r(xQr,"from_pretrained()"),xQr.forEach(t),SBo=r(gz,"to load the model weights."),gz.forEach(t),PBo=i(sl),Bne=n(sl,"P",{});var RQr=s(Bne);$Bo=r(RQr,"Examples:"),RQr.forEach(t),IBo=i(sl),m(oE.$$.fragment,sl),sl.forEach(t),jBo=i(nl),qe=n(nl,"DIV",{class:!0});var Xt=s(qe);m(rE.$$.fragment,Xt),NBo=i(Xt),kne=n(Xt,"P",{});var SQr=s(kne);DBo=r(SQr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),SQr.forEach(t),qBo=i(Xt),Ua=n(Xt,"P",{});var uC=s(Ua);GBo=r(uC,"The model class to instantiate is selected based on the "),xne=n(uC,"CODE",{});var PQr=s(xne);OBo=r(PQr,"model_type"),PQr.forEach(t),XBo=r(uC,` property of the config object (either
passed as an argument or loaded from `),Rne=n(uC,"CODE",{});var $Qr=s(Rne);zBo=r($Qr,"pretrained_model_name_or_path"),$Qr.forEach(t),VBo=r(uC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=n(uC,"CODE",{});var IQr=s(Sne);WBo=r(IQr,"pretrained_model_name_or_path"),IQr.forEach(t),QBo=r(uC,":"),uC.forEach(t),HBo=i(Xt),Pne=n(Xt,"UL",{});var jQr=s(Pne);_2=n(jQr,"LI",{});var YMe=s(_2);$ne=n(YMe,"STRONG",{});var NQr=s($ne);UBo=r(NQr,"tapas"),NQr.forEach(t),JBo=r(YMe," \u2014 "),Oj=n(YMe,"A",{href:!0});var DQr=s(Oj);YBo=r(DQr,"TapasForQuestionAnswering"),DQr.forEach(t),KBo=r(YMe," (TAPAS model)"),YMe.forEach(t),jQr.forEach(t),ZBo=i(Xt),u2=n(Xt,"P",{});var KMe=s(u2);eko=r(KMe,"The model is set in evaluation mode by default using "),Ine=n(KMe,"CODE",{});var qQr=s(Ine);oko=r(qQr,"model.eval()"),qQr.forEach(t),rko=r(KMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=n(KMe,"CODE",{});var GQr=s(jne);tko=r(GQr,"model.train()"),GQr.forEach(t),KMe.forEach(t),ako=i(Xt),Nne=n(Xt,"P",{});var OQr=s(Nne);nko=r(OQr,"Examples:"),OQr.forEach(t),sko=i(Xt),m(tE.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),NLe=i(d),vd=n(d,"H2",{class:!0});var VBe=s(vd);b2=n(VBe,"A",{id:!0,class:!0,href:!0});var XQr=s(b2);Dne=n(XQr,"SPAN",{});var zQr=s(Dne);m(aE.$$.fragment,zQr),zQr.forEach(t),XQr.forEach(t),lko=i(VBe),qne=n(VBe,"SPAN",{});var VQr=s(qne);iko=r(VQr,"AutoModelForImageClassification"),VQr.forEach(t),VBe.forEach(t),DLe=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(nE.$$.fragment,ll),dko=i(ll),Td=n(ll,"P",{});var hz=s(Td);cko=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=n(hz,"CODE",{});var WQr=s(Gne);fko=r(WQr,"from_pretrained()"),WQr.forEach(t),mko=r(hz,"class method or the "),One=n(hz,"CODE",{});var QQr=s(One);gko=r(QQr,"from_config()"),QQr.forEach(t),hko=r(hz,`class
method.`),hz.forEach(t),pko=i(ll),sE=n(ll,"P",{});var WBe=s(sE);_ko=r(WBe,"This class cannot be instantiated directly using "),Xne=n(WBe,"CODE",{});var HQr=s(Xne);uko=r(HQr,"__init__()"),HQr.forEach(t),bko=r(WBe," (throws an error)."),WBe.forEach(t),vko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(lE.$$.fragment,il),Tko=i(il),zne=n(il,"P",{});var UQr=s(zne);Fko=r(UQr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),UQr.forEach(t),Cko=i(il),Fd=n(il,"P",{});var pz=s(Fd);Mko=r(pz,`Note:
Loading a model from its configuration file does `),Vne=n(pz,"STRONG",{});var JQr=s(Vne);Eko=r(JQr,"not"),JQr.forEach(t),yko=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=n(pz,"CODE",{});var YQr=s(Wne);wko=r(YQr,"from_pretrained()"),YQr.forEach(t),Ako=r(pz,"to load the model weights."),pz.forEach(t),Lko=i(il),Qne=n(il,"P",{});var KQr=s(Qne);Bko=r(KQr,"Examples:"),KQr.forEach(t),kko=i(il),m(iE.$$.fragment,il),il.forEach(t),xko=i(ll),Ge=n(ll,"DIV",{class:!0});var zt=s(Ge);m(dE.$$.fragment,zt),Rko=i(zt),Hne=n(zt,"P",{});var ZQr=s(Hne);Sko=r(ZQr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ZQr.forEach(t),Pko=i(zt),Ja=n(zt,"P",{});var bC=s(Ja);$ko=r(bC,"The model class to instantiate is selected based on the "),Une=n(bC,"CODE",{});var eHr=s(Une);Iko=r(eHr,"model_type"),eHr.forEach(t),jko=r(bC,` property of the config object (either
passed as an argument or loaded from `),Jne=n(bC,"CODE",{});var oHr=s(Jne);Nko=r(oHr,"pretrained_model_name_or_path"),oHr.forEach(t),Dko=r(bC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=n(bC,"CODE",{});var rHr=s(Yne);qko=r(rHr,"pretrained_model_name_or_path"),rHr.forEach(t),Gko=r(bC,":"),bC.forEach(t),Oko=i(zt),be=n(zt,"UL",{});var Ke=s(be);v2=n(Ke,"LI",{});var ZMe=s(v2);Kne=n(ZMe,"STRONG",{});var tHr=s(Kne);Xko=r(tHr,"beit"),tHr.forEach(t),zko=r(ZMe," \u2014 "),Xj=n(ZMe,"A",{href:!0});var aHr=s(Xj);Vko=r(aHr,"BeitForImageClassification"),aHr.forEach(t),Wko=r(ZMe," (BEiT model)"),ZMe.forEach(t),Qko=i(Ke),T2=n(Ke,"LI",{});var eEe=s(T2);Zne=n(eEe,"STRONG",{});var nHr=s(Zne);Hko=r(nHr,"convnext"),nHr.forEach(t),Uko=r(eEe," \u2014 "),zj=n(eEe,"A",{href:!0});var sHr=s(zj);Jko=r(sHr,"ConvNextForImageClassification"),sHr.forEach(t),Yko=r(eEe," (ConvNext model)"),eEe.forEach(t),Kko=i(Ke),Rs=n(Ke,"LI",{});var j0=s(Rs);ese=n(j0,"STRONG",{});var lHr=s(ese);Zko=r(lHr,"deit"),lHr.forEach(t),exo=r(j0," \u2014 "),Vj=n(j0,"A",{href:!0});var iHr=s(Vj);oxo=r(iHr,"DeiTForImageClassification"),iHr.forEach(t),rxo=r(j0," or "),Wj=n(j0,"A",{href:!0});var dHr=s(Wj);txo=r(dHr,"DeiTForImageClassificationWithTeacher"),dHr.forEach(t),axo=r(j0," (DeiT model)"),j0.forEach(t),nxo=i(Ke),F2=n(Ke,"LI",{});var oEe=s(F2);ose=n(oEe,"STRONG",{});var cHr=s(ose);sxo=r(cHr,"imagegpt"),cHr.forEach(t),lxo=r(oEe," \u2014 "),Qj=n(oEe,"A",{href:!0});var fHr=s(Qj);ixo=r(fHr,"ImageGPTForImageClassification"),fHr.forEach(t),dxo=r(oEe," (ImageGPT model)"),oEe.forEach(t),cxo=i(Ke),la=n(Ke,"LI",{});var Mf=s(la);rse=n(Mf,"STRONG",{});var mHr=s(rse);fxo=r(mHr,"perceiver"),mHr.forEach(t),mxo=r(Mf," \u2014 "),Hj=n(Mf,"A",{href:!0});var gHr=s(Hj);gxo=r(gHr,"PerceiverForImageClassificationLearned"),gHr.forEach(t),hxo=r(Mf," or "),Uj=n(Mf,"A",{href:!0});var hHr=s(Uj);pxo=r(hHr,"PerceiverForImageClassificationFourier"),hHr.forEach(t),_xo=r(Mf," or "),Jj=n(Mf,"A",{href:!0});var pHr=s(Jj);uxo=r(pHr,"PerceiverForImageClassificationConvProcessing"),pHr.forEach(t),bxo=r(Mf," (Perceiver model)"),Mf.forEach(t),vxo=i(Ke),C2=n(Ke,"LI",{});var rEe=s(C2);tse=n(rEe,"STRONG",{});var _Hr=s(tse);Txo=r(_Hr,"poolformer"),_Hr.forEach(t),Fxo=r(rEe," \u2014 "),Yj=n(rEe,"A",{href:!0});var uHr=s(Yj);Cxo=r(uHr,"PoolFormerForImageClassification"),uHr.forEach(t),Mxo=r(rEe," (PoolFormer model)"),rEe.forEach(t),Exo=i(Ke),M2=n(Ke,"LI",{});var tEe=s(M2);ase=n(tEe,"STRONG",{});var bHr=s(ase);yxo=r(bHr,"segformer"),bHr.forEach(t),wxo=r(tEe," \u2014 "),Kj=n(tEe,"A",{href:!0});var vHr=s(Kj);Axo=r(vHr,"SegformerForImageClassification"),vHr.forEach(t),Lxo=r(tEe," (SegFormer model)"),tEe.forEach(t),Bxo=i(Ke),E2=n(Ke,"LI",{});var aEe=s(E2);nse=n(aEe,"STRONG",{});var THr=s(nse);kxo=r(THr,"swin"),THr.forEach(t),xxo=r(aEe," \u2014 "),Zj=n(aEe,"A",{href:!0});var FHr=s(Zj);Rxo=r(FHr,"SwinForImageClassification"),FHr.forEach(t),Sxo=r(aEe," (Swin model)"),aEe.forEach(t),Pxo=i(Ke),y2=n(Ke,"LI",{});var nEe=s(y2);sse=n(nEe,"STRONG",{});var CHr=s(sse);$xo=r(CHr,"vit"),CHr.forEach(t),Ixo=r(nEe," \u2014 "),eN=n(nEe,"A",{href:!0});var MHr=s(eN);jxo=r(MHr,"ViTForImageClassification"),MHr.forEach(t),Nxo=r(nEe," (ViT model)"),nEe.forEach(t),Ke.forEach(t),Dxo=i(zt),w2=n(zt,"P",{});var sEe=s(w2);qxo=r(sEe,"The model is set in evaluation mode by default using "),lse=n(sEe,"CODE",{});var EHr=s(lse);Gxo=r(EHr,"model.eval()"),EHr.forEach(t),Oxo=r(sEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=n(sEe,"CODE",{});var yHr=s(ise);Xxo=r(yHr,"model.train()"),yHr.forEach(t),sEe.forEach(t),zxo=i(zt),dse=n(zt,"P",{});var wHr=s(dse);Vxo=r(wHr,"Examples:"),wHr.forEach(t),Wxo=i(zt),m(cE.$$.fragment,zt),zt.forEach(t),ll.forEach(t),qLe=i(d),Cd=n(d,"H2",{class:!0});var QBe=s(Cd);A2=n(QBe,"A",{id:!0,class:!0,href:!0});var AHr=s(A2);cse=n(AHr,"SPAN",{});var LHr=s(cse);m(fE.$$.fragment,LHr),LHr.forEach(t),AHr.forEach(t),Qxo=i(QBe),fse=n(QBe,"SPAN",{});var BHr=s(fse);Hxo=r(BHr,"AutoModelForVision2Seq"),BHr.forEach(t),QBe.forEach(t),GLe=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(mE.$$.fragment,dl),Uxo=i(dl),Md=n(dl,"P",{});var _z=s(Md);Jxo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=n(_z,"CODE",{});var kHr=s(mse);Yxo=r(kHr,"from_pretrained()"),kHr.forEach(t),Kxo=r(_z,"class method or the "),gse=n(_z,"CODE",{});var xHr=s(gse);Zxo=r(xHr,"from_config()"),xHr.forEach(t),eRo=r(_z,`class
method.`),_z.forEach(t),oRo=i(dl),gE=n(dl,"P",{});var HBe=s(gE);rRo=r(HBe,"This class cannot be instantiated directly using "),hse=n(HBe,"CODE",{});var RHr=s(hse);tRo=r(RHr,"__init__()"),RHr.forEach(t),aRo=r(HBe," (throws an error)."),HBe.forEach(t),nRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(hE.$$.fragment,cl),sRo=i(cl),pse=n(cl,"P",{});var SHr=s(pse);lRo=r(SHr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),SHr.forEach(t),iRo=i(cl),Ed=n(cl,"P",{});var uz=s(Ed);dRo=r(uz,`Note:
Loading a model from its configuration file does `),_se=n(uz,"STRONG",{});var PHr=s(_se);cRo=r(PHr,"not"),PHr.forEach(t),fRo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),use=n(uz,"CODE",{});var $Hr=s(use);mRo=r($Hr,"from_pretrained()"),$Hr.forEach(t),gRo=r(uz,"to load the model weights."),uz.forEach(t),hRo=i(cl),bse=n(cl,"P",{});var IHr=s(bse);pRo=r(IHr,"Examples:"),IHr.forEach(t),_Ro=i(cl),m(pE.$$.fragment,cl),cl.forEach(t),uRo=i(dl),Oe=n(dl,"DIV",{class:!0});var Vt=s(Oe);m(_E.$$.fragment,Vt),bRo=i(Vt),vse=n(Vt,"P",{});var jHr=s(vse);vRo=r(jHr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jHr.forEach(t),TRo=i(Vt),Ya=n(Vt,"P",{});var vC=s(Ya);FRo=r(vC,"The model class to instantiate is selected based on the "),Tse=n(vC,"CODE",{});var NHr=s(Tse);CRo=r(NHr,"model_type"),NHr.forEach(t),MRo=r(vC,` property of the config object (either
passed as an argument or loaded from `),Fse=n(vC,"CODE",{});var DHr=s(Fse);ERo=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),yRo=r(vC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=n(vC,"CODE",{});var qHr=s(Cse);wRo=r(qHr,"pretrained_model_name_or_path"),qHr.forEach(t),ARo=r(vC,":"),vC.forEach(t),LRo=i(Vt),Mse=n(Vt,"UL",{});var GHr=s(Mse);L2=n(GHr,"LI",{});var lEe=s(L2);Ese=n(lEe,"STRONG",{});var OHr=s(Ese);BRo=r(OHr,"vision-encoder-decoder"),OHr.forEach(t),kRo=r(lEe," \u2014 "),oN=n(lEe,"A",{href:!0});var XHr=s(oN);xRo=r(XHr,"VisionEncoderDecoderModel"),XHr.forEach(t),RRo=r(lEe," (Vision Encoder decoder model)"),lEe.forEach(t),GHr.forEach(t),SRo=i(Vt),B2=n(Vt,"P",{});var iEe=s(B2);PRo=r(iEe,"The model is set in evaluation mode by default using "),yse=n(iEe,"CODE",{});var zHr=s(yse);$Ro=r(zHr,"model.eval()"),zHr.forEach(t),IRo=r(iEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=n(iEe,"CODE",{});var VHr=s(wse);jRo=r(VHr,"model.train()"),VHr.forEach(t),iEe.forEach(t),NRo=i(Vt),Ase=n(Vt,"P",{});var WHr=s(Ase);DRo=r(WHr,"Examples:"),WHr.forEach(t),qRo=i(Vt),m(uE.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),OLe=i(d),yd=n(d,"H2",{class:!0});var UBe=s(yd);k2=n(UBe,"A",{id:!0,class:!0,href:!0});var QHr=s(k2);Lse=n(QHr,"SPAN",{});var HHr=s(Lse);m(bE.$$.fragment,HHr),HHr.forEach(t),QHr.forEach(t),GRo=i(UBe),Bse=n(UBe,"SPAN",{});var UHr=s(Bse);ORo=r(UHr,"AutoModelForAudioClassification"),UHr.forEach(t),UBe.forEach(t),XLe=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(vE.$$.fragment,fl),XRo=i(fl),wd=n(fl,"P",{});var bz=s(wd);zRo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=n(bz,"CODE",{});var JHr=s(kse);VRo=r(JHr,"from_pretrained()"),JHr.forEach(t),WRo=r(bz,"class method or the "),xse=n(bz,"CODE",{});var YHr=s(xse);QRo=r(YHr,"from_config()"),YHr.forEach(t),HRo=r(bz,`class
method.`),bz.forEach(t),URo=i(fl),TE=n(fl,"P",{});var JBe=s(TE);JRo=r(JBe,"This class cannot be instantiated directly using "),Rse=n(JBe,"CODE",{});var KHr=s(Rse);YRo=r(KHr,"__init__()"),KHr.forEach(t),KRo=r(JBe," (throws an error)."),JBe.forEach(t),ZRo=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(FE.$$.fragment,ml),eSo=i(ml),Sse=n(ml,"P",{});var ZHr=s(Sse);oSo=r(ZHr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),ZHr.forEach(t),rSo=i(ml),Ad=n(ml,"P",{});var vz=s(Ad);tSo=r(vz,`Note:
Loading a model from its configuration file does `),Pse=n(vz,"STRONG",{});var eUr=s(Pse);aSo=r(eUr,"not"),eUr.forEach(t),nSo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=n(vz,"CODE",{});var oUr=s($se);sSo=r(oUr,"from_pretrained()"),oUr.forEach(t),lSo=r(vz,"to load the model weights."),vz.forEach(t),iSo=i(ml),Ise=n(ml,"P",{});var rUr=s(Ise);dSo=r(rUr,"Examples:"),rUr.forEach(t),cSo=i(ml),m(CE.$$.fragment,ml),ml.forEach(t),fSo=i(fl),Xe=n(fl,"DIV",{class:!0});var Wt=s(Xe);m(ME.$$.fragment,Wt),mSo=i(Wt),jse=n(Wt,"P",{});var tUr=s(jse);gSo=r(tUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),tUr.forEach(t),hSo=i(Wt),Ka=n(Wt,"P",{});var TC=s(Ka);pSo=r(TC,"The model class to instantiate is selected based on the "),Nse=n(TC,"CODE",{});var aUr=s(Nse);_So=r(aUr,"model_type"),aUr.forEach(t),uSo=r(TC,` property of the config object (either
passed as an argument or loaded from `),Dse=n(TC,"CODE",{});var nUr=s(Dse);bSo=r(nUr,"pretrained_model_name_or_path"),nUr.forEach(t),vSo=r(TC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=n(TC,"CODE",{});var sUr=s(qse);TSo=r(sUr,"pretrained_model_name_or_path"),sUr.forEach(t),FSo=r(TC,":"),TC.forEach(t),CSo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);x2=n(Qt,"LI",{});var dEe=s(x2);Gse=n(dEe,"STRONG",{});var lUr=s(Gse);MSo=r(lUr,"hubert"),lUr.forEach(t),ESo=r(dEe," \u2014 "),rN=n(dEe,"A",{href:!0});var iUr=s(rN);ySo=r(iUr,"HubertForSequenceClassification"),iUr.forEach(t),wSo=r(dEe," (Hubert model)"),dEe.forEach(t),ASo=i(Qt),R2=n(Qt,"LI",{});var cEe=s(R2);Ose=n(cEe,"STRONG",{});var dUr=s(Ose);LSo=r(dUr,"sew"),dUr.forEach(t),BSo=r(cEe," \u2014 "),tN=n(cEe,"A",{href:!0});var cUr=s(tN);kSo=r(cUr,"SEWForSequenceClassification"),cUr.forEach(t),xSo=r(cEe," (SEW model)"),cEe.forEach(t),RSo=i(Qt),S2=n(Qt,"LI",{});var fEe=s(S2);Xse=n(fEe,"STRONG",{});var fUr=s(Xse);SSo=r(fUr,"sew-d"),fUr.forEach(t),PSo=r(fEe," \u2014 "),aN=n(fEe,"A",{href:!0});var mUr=s(aN);$So=r(mUr,"SEWDForSequenceClassification"),mUr.forEach(t),ISo=r(fEe," (SEW-D model)"),fEe.forEach(t),jSo=i(Qt),P2=n(Qt,"LI",{});var mEe=s(P2);zse=n(mEe,"STRONG",{});var gUr=s(zse);NSo=r(gUr,"unispeech"),gUr.forEach(t),DSo=r(mEe," \u2014 "),nN=n(mEe,"A",{href:!0});var hUr=s(nN);qSo=r(hUr,"UniSpeechForSequenceClassification"),hUr.forEach(t),GSo=r(mEe," (UniSpeech model)"),mEe.forEach(t),OSo=i(Qt),$2=n(Qt,"LI",{});var gEe=s($2);Vse=n(gEe,"STRONG",{});var pUr=s(Vse);XSo=r(pUr,"unispeech-sat"),pUr.forEach(t),zSo=r(gEe," \u2014 "),sN=n(gEe,"A",{href:!0});var _Ur=s(sN);VSo=r(_Ur,"UniSpeechSatForSequenceClassification"),_Ur.forEach(t),WSo=r(gEe," (UniSpeechSat model)"),gEe.forEach(t),QSo=i(Qt),I2=n(Qt,"LI",{});var hEe=s(I2);Wse=n(hEe,"STRONG",{});var uUr=s(Wse);HSo=r(uUr,"wav2vec2"),uUr.forEach(t),USo=r(hEe," \u2014 "),lN=n(hEe,"A",{href:!0});var bUr=s(lN);JSo=r(bUr,"Wav2Vec2ForSequenceClassification"),bUr.forEach(t),YSo=r(hEe," (Wav2Vec2 model)"),hEe.forEach(t),KSo=i(Qt),j2=n(Qt,"LI",{});var pEe=s(j2);Qse=n(pEe,"STRONG",{});var vUr=s(Qse);ZSo=r(vUr,"wavlm"),vUr.forEach(t),ePo=r(pEe," \u2014 "),iN=n(pEe,"A",{href:!0});var TUr=s(iN);oPo=r(TUr,"WavLMForSequenceClassification"),TUr.forEach(t),rPo=r(pEe," (WavLM model)"),pEe.forEach(t),Qt.forEach(t),tPo=i(Wt),N2=n(Wt,"P",{});var _Ee=s(N2);aPo=r(_Ee,"The model is set in evaluation mode by default using "),Hse=n(_Ee,"CODE",{});var FUr=s(Hse);nPo=r(FUr,"model.eval()"),FUr.forEach(t),sPo=r(_Ee,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=n(_Ee,"CODE",{});var CUr=s(Use);lPo=r(CUr,"model.train()"),CUr.forEach(t),_Ee.forEach(t),iPo=i(Wt),Jse=n(Wt,"P",{});var MUr=s(Jse);dPo=r(MUr,"Examples:"),MUr.forEach(t),cPo=i(Wt),m(EE.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),zLe=i(d),Ld=n(d,"H2",{class:!0});var YBe=s(Ld);D2=n(YBe,"A",{id:!0,class:!0,href:!0});var EUr=s(D2);Yse=n(EUr,"SPAN",{});var yUr=s(Yse);m(yE.$$.fragment,yUr),yUr.forEach(t),EUr.forEach(t),fPo=i(YBe),Kse=n(YBe,"SPAN",{});var wUr=s(Kse);mPo=r(wUr,"AutoModelForAudioFrameClassification"),wUr.forEach(t),YBe.forEach(t),VLe=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(wE.$$.fragment,gl),gPo=i(gl),Bd=n(gl,"P",{});var Tz=s(Bd);hPo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=n(Tz,"CODE",{});var AUr=s(Zse);pPo=r(AUr,"from_pretrained()"),AUr.forEach(t),_Po=r(Tz,"class method or the "),ele=n(Tz,"CODE",{});var LUr=s(ele);uPo=r(LUr,"from_config()"),LUr.forEach(t),bPo=r(Tz,`class
method.`),Tz.forEach(t),vPo=i(gl),AE=n(gl,"P",{});var KBe=s(AE);TPo=r(KBe,"This class cannot be instantiated directly using "),ole=n(KBe,"CODE",{});var BUr=s(ole);FPo=r(BUr,"__init__()"),BUr.forEach(t),CPo=r(KBe," (throws an error)."),KBe.forEach(t),MPo=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(LE.$$.fragment,hl),EPo=i(hl),rle=n(hl,"P",{});var kUr=s(rle);yPo=r(kUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),kUr.forEach(t),wPo=i(hl),kd=n(hl,"P",{});var Fz=s(kd);APo=r(Fz,`Note:
Loading a model from its configuration file does `),tle=n(Fz,"STRONG",{});var xUr=s(tle);LPo=r(xUr,"not"),xUr.forEach(t),BPo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=n(Fz,"CODE",{});var RUr=s(ale);kPo=r(RUr,"from_pretrained()"),RUr.forEach(t),xPo=r(Fz,"to load the model weights."),Fz.forEach(t),RPo=i(hl),nle=n(hl,"P",{});var SUr=s(nle);SPo=r(SUr,"Examples:"),SUr.forEach(t),PPo=i(hl),m(BE.$$.fragment,hl),hl.forEach(t),$Po=i(gl),ze=n(gl,"DIV",{class:!0});var Ht=s(ze);m(kE.$$.fragment,Ht),IPo=i(Ht),sle=n(Ht,"P",{});var PUr=s(sle);jPo=r(PUr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),PUr.forEach(t),NPo=i(Ht),Za=n(Ht,"P",{});var FC=s(Za);DPo=r(FC,"The model class to instantiate is selected based on the "),lle=n(FC,"CODE",{});var $Ur=s(lle);qPo=r($Ur,"model_type"),$Ur.forEach(t),GPo=r(FC,` property of the config object (either
passed as an argument or loaded from `),ile=n(FC,"CODE",{});var IUr=s(ile);OPo=r(IUr,"pretrained_model_name_or_path"),IUr.forEach(t),XPo=r(FC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=n(FC,"CODE",{});var jUr=s(dle);zPo=r(jUr,"pretrained_model_name_or_path"),jUr.forEach(t),VPo=r(FC,":"),FC.forEach(t),WPo=i(Ht),xd=n(Ht,"UL",{});var Cz=s(xd);q2=n(Cz,"LI",{});var uEe=s(q2);cle=n(uEe,"STRONG",{});var NUr=s(cle);QPo=r(NUr,"unispeech-sat"),NUr.forEach(t),HPo=r(uEe," \u2014 "),dN=n(uEe,"A",{href:!0});var DUr=s(dN);UPo=r(DUr,"UniSpeechSatForAudioFrameClassification"),DUr.forEach(t),JPo=r(uEe," (UniSpeechSat model)"),uEe.forEach(t),YPo=i(Cz),G2=n(Cz,"LI",{});var bEe=s(G2);fle=n(bEe,"STRONG",{});var qUr=s(fle);KPo=r(qUr,"wav2vec2"),qUr.forEach(t),ZPo=r(bEe," \u2014 "),cN=n(bEe,"A",{href:!0});var GUr=s(cN);e$o=r(GUr,"Wav2Vec2ForAudioFrameClassification"),GUr.forEach(t),o$o=r(bEe," (Wav2Vec2 model)"),bEe.forEach(t),r$o=i(Cz),O2=n(Cz,"LI",{});var vEe=s(O2);mle=n(vEe,"STRONG",{});var OUr=s(mle);t$o=r(OUr,"wavlm"),OUr.forEach(t),a$o=r(vEe," \u2014 "),fN=n(vEe,"A",{href:!0});var XUr=s(fN);n$o=r(XUr,"WavLMForAudioFrameClassification"),XUr.forEach(t),s$o=r(vEe," (WavLM model)"),vEe.forEach(t),Cz.forEach(t),l$o=i(Ht),X2=n(Ht,"P",{});var TEe=s(X2);i$o=r(TEe,"The model is set in evaluation mode by default using "),gle=n(TEe,"CODE",{});var zUr=s(gle);d$o=r(zUr,"model.eval()"),zUr.forEach(t),c$o=r(TEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=n(TEe,"CODE",{});var VUr=s(hle);f$o=r(VUr,"model.train()"),VUr.forEach(t),TEe.forEach(t),m$o=i(Ht),ple=n(Ht,"P",{});var WUr=s(ple);g$o=r(WUr,"Examples:"),WUr.forEach(t),h$o=i(Ht),m(xE.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),WLe=i(d),Rd=n(d,"H2",{class:!0});var ZBe=s(Rd);z2=n(ZBe,"A",{id:!0,class:!0,href:!0});var QUr=s(z2);_le=n(QUr,"SPAN",{});var HUr=s(_le);m(RE.$$.fragment,HUr),HUr.forEach(t),QUr.forEach(t),p$o=i(ZBe),ule=n(ZBe,"SPAN",{});var UUr=s(ule);_$o=r(UUr,"AutoModelForCTC"),UUr.forEach(t),ZBe.forEach(t),QLe=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m(SE.$$.fragment,pl),u$o=i(pl),Sd=n(pl,"P",{});var Mz=s(Sd);b$o=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=n(Mz,"CODE",{});var JUr=s(ble);v$o=r(JUr,"from_pretrained()"),JUr.forEach(t),T$o=r(Mz,"class method or the "),vle=n(Mz,"CODE",{});var YUr=s(vle);F$o=r(YUr,"from_config()"),YUr.forEach(t),C$o=r(Mz,`class
method.`),Mz.forEach(t),M$o=i(pl),PE=n(pl,"P",{});var eke=s(PE);E$o=r(eke,"This class cannot be instantiated directly using "),Tle=n(eke,"CODE",{});var KUr=s(Tle);y$o=r(KUr,"__init__()"),KUr.forEach(t),w$o=r(eke," (throws an error)."),eke.forEach(t),A$o=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m($E.$$.fragment,_l),L$o=i(_l),Fle=n(_l,"P",{});var ZUr=s(Fle);B$o=r(ZUr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),ZUr.forEach(t),k$o=i(_l),Pd=n(_l,"P",{});var Ez=s(Pd);x$o=r(Ez,`Note:
Loading a model from its configuration file does `),Cle=n(Ez,"STRONG",{});var eJr=s(Cle);R$o=r(eJr,"not"),eJr.forEach(t),S$o=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=n(Ez,"CODE",{});var oJr=s(Mle);P$o=r(oJr,"from_pretrained()"),oJr.forEach(t),$$o=r(Ez,"to load the model weights."),Ez.forEach(t),I$o=i(_l),Ele=n(_l,"P",{});var rJr=s(Ele);j$o=r(rJr,"Examples:"),rJr.forEach(t),N$o=i(_l),m(IE.$$.fragment,_l),_l.forEach(t),D$o=i(pl),Ve=n(pl,"DIV",{class:!0});var Ut=s(Ve);m(jE.$$.fragment,Ut),q$o=i(Ut),yle=n(Ut,"P",{});var tJr=s(yle);G$o=r(tJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),tJr.forEach(t),O$o=i(Ut),en=n(Ut,"P",{});var CC=s(en);X$o=r(CC,"The model class to instantiate is selected based on the "),wle=n(CC,"CODE",{});var aJr=s(wle);z$o=r(aJr,"model_type"),aJr.forEach(t),V$o=r(CC,` property of the config object (either
passed as an argument or loaded from `),Ale=n(CC,"CODE",{});var nJr=s(Ale);W$o=r(nJr,"pretrained_model_name_or_path"),nJr.forEach(t),Q$o=r(CC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=n(CC,"CODE",{});var sJr=s(Lle);H$o=r(sJr,"pretrained_model_name_or_path"),sJr.forEach(t),U$o=r(CC,":"),CC.forEach(t),J$o=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);V2=n(Jt,"LI",{});var FEe=s(V2);Ble=n(FEe,"STRONG",{});var lJr=s(Ble);Y$o=r(lJr,"hubert"),lJr.forEach(t),K$o=r(FEe," \u2014 "),mN=n(FEe,"A",{href:!0});var iJr=s(mN);Z$o=r(iJr,"HubertForCTC"),iJr.forEach(t),eIo=r(FEe," (Hubert model)"),FEe.forEach(t),oIo=i(Jt),W2=n(Jt,"LI",{});var CEe=s(W2);kle=n(CEe,"STRONG",{});var dJr=s(kle);rIo=r(dJr,"sew"),dJr.forEach(t),tIo=r(CEe," \u2014 "),gN=n(CEe,"A",{href:!0});var cJr=s(gN);aIo=r(cJr,"SEWForCTC"),cJr.forEach(t),nIo=r(CEe," (SEW model)"),CEe.forEach(t),sIo=i(Jt),Q2=n(Jt,"LI",{});var MEe=s(Q2);xle=n(MEe,"STRONG",{});var fJr=s(xle);lIo=r(fJr,"sew-d"),fJr.forEach(t),iIo=r(MEe," \u2014 "),hN=n(MEe,"A",{href:!0});var mJr=s(hN);dIo=r(mJr,"SEWDForCTC"),mJr.forEach(t),cIo=r(MEe," (SEW-D model)"),MEe.forEach(t),fIo=i(Jt),H2=n(Jt,"LI",{});var EEe=s(H2);Rle=n(EEe,"STRONG",{});var gJr=s(Rle);mIo=r(gJr,"unispeech"),gJr.forEach(t),gIo=r(EEe," \u2014 "),pN=n(EEe,"A",{href:!0});var hJr=s(pN);hIo=r(hJr,"UniSpeechForCTC"),hJr.forEach(t),pIo=r(EEe," (UniSpeech model)"),EEe.forEach(t),_Io=i(Jt),U2=n(Jt,"LI",{});var yEe=s(U2);Sle=n(yEe,"STRONG",{});var pJr=s(Sle);uIo=r(pJr,"unispeech-sat"),pJr.forEach(t),bIo=r(yEe," \u2014 "),_N=n(yEe,"A",{href:!0});var _Jr=s(_N);vIo=r(_Jr,"UniSpeechSatForCTC"),_Jr.forEach(t),TIo=r(yEe," (UniSpeechSat model)"),yEe.forEach(t),FIo=i(Jt),J2=n(Jt,"LI",{});var wEe=s(J2);Ple=n(wEe,"STRONG",{});var uJr=s(Ple);CIo=r(uJr,"wav2vec2"),uJr.forEach(t),MIo=r(wEe," \u2014 "),uN=n(wEe,"A",{href:!0});var bJr=s(uN);EIo=r(bJr,"Wav2Vec2ForCTC"),bJr.forEach(t),yIo=r(wEe," (Wav2Vec2 model)"),wEe.forEach(t),wIo=i(Jt),Y2=n(Jt,"LI",{});var AEe=s(Y2);$le=n(AEe,"STRONG",{});var vJr=s($le);AIo=r(vJr,"wavlm"),vJr.forEach(t),LIo=r(AEe," \u2014 "),bN=n(AEe,"A",{href:!0});var TJr=s(bN);BIo=r(TJr,"WavLMForCTC"),TJr.forEach(t),kIo=r(AEe," (WavLM model)"),AEe.forEach(t),Jt.forEach(t),xIo=i(Ut),K2=n(Ut,"P",{});var LEe=s(K2);RIo=r(LEe,"The model is set in evaluation mode by default using "),Ile=n(LEe,"CODE",{});var FJr=s(Ile);SIo=r(FJr,"model.eval()"),FJr.forEach(t),PIo=r(LEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=n(LEe,"CODE",{});var CJr=s(jle);$Io=r(CJr,"model.train()"),CJr.forEach(t),LEe.forEach(t),IIo=i(Ut),Nle=n(Ut,"P",{});var MJr=s(Nle);jIo=r(MJr,"Examples:"),MJr.forEach(t),NIo=i(Ut),m(NE.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),HLe=i(d),$d=n(d,"H2",{class:!0});var oke=s($d);Z2=n(oke,"A",{id:!0,class:!0,href:!0});var EJr=s(Z2);Dle=n(EJr,"SPAN",{});var yJr=s(Dle);m(DE.$$.fragment,yJr),yJr.forEach(t),EJr.forEach(t),DIo=i(oke),qle=n(oke,"SPAN",{});var wJr=s(qle);qIo=r(wJr,"AutoModelForSpeechSeq2Seq"),wJr.forEach(t),oke.forEach(t),ULe=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(qE.$$.fragment,ul),GIo=i(ul),Id=n(ul,"P",{});var yz=s(Id);OIo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=n(yz,"CODE",{});var AJr=s(Gle);XIo=r(AJr,"from_pretrained()"),AJr.forEach(t),zIo=r(yz,"class method or the "),Ole=n(yz,"CODE",{});var LJr=s(Ole);VIo=r(LJr,"from_config()"),LJr.forEach(t),WIo=r(yz,`class
method.`),yz.forEach(t),QIo=i(ul),GE=n(ul,"P",{});var rke=s(GE);HIo=r(rke,"This class cannot be instantiated directly using "),Xle=n(rke,"CODE",{});var BJr=s(Xle);UIo=r(BJr,"__init__()"),BJr.forEach(t),JIo=r(rke," (throws an error)."),rke.forEach(t),YIo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(OE.$$.fragment,bl),KIo=i(bl),zle=n(bl,"P",{});var kJr=s(zle);ZIo=r(kJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kJr.forEach(t),ejo=i(bl),jd=n(bl,"P",{});var wz=s(jd);ojo=r(wz,`Note:
Loading a model from its configuration file does `),Vle=n(wz,"STRONG",{});var xJr=s(Vle);rjo=r(xJr,"not"),xJr.forEach(t),tjo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=n(wz,"CODE",{});var RJr=s(Wle);ajo=r(RJr,"from_pretrained()"),RJr.forEach(t),njo=r(wz,"to load the model weights."),wz.forEach(t),sjo=i(bl),Qle=n(bl,"P",{});var SJr=s(Qle);ljo=r(SJr,"Examples:"),SJr.forEach(t),ijo=i(bl),m(XE.$$.fragment,bl),bl.forEach(t),djo=i(ul),We=n(ul,"DIV",{class:!0});var Yt=s(We);m(zE.$$.fragment,Yt),cjo=i(Yt),Hle=n(Yt,"P",{});var PJr=s(Hle);fjo=r(PJr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),PJr.forEach(t),mjo=i(Yt),on=n(Yt,"P",{});var MC=s(on);gjo=r(MC,"The model class to instantiate is selected based on the "),Ule=n(MC,"CODE",{});var $Jr=s(Ule);hjo=r($Jr,"model_type"),$Jr.forEach(t),pjo=r(MC,` property of the config object (either
passed as an argument or loaded from `),Jle=n(MC,"CODE",{});var IJr=s(Jle);_jo=r(IJr,"pretrained_model_name_or_path"),IJr.forEach(t),ujo=r(MC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=n(MC,"CODE",{});var jJr=s(Yle);bjo=r(jJr,"pretrained_model_name_or_path"),jJr.forEach(t),vjo=r(MC,":"),MC.forEach(t),Tjo=i(Yt),VE=n(Yt,"UL",{});var tke=s(VE);ev=n(tke,"LI",{});var BEe=s(ev);Kle=n(BEe,"STRONG",{});var NJr=s(Kle);Fjo=r(NJr,"speech-encoder-decoder"),NJr.forEach(t),Cjo=r(BEe," \u2014 "),vN=n(BEe,"A",{href:!0});var DJr=s(vN);Mjo=r(DJr,"SpeechEncoderDecoderModel"),DJr.forEach(t),Ejo=r(BEe," (Speech Encoder decoder model)"),BEe.forEach(t),yjo=i(tke),ov=n(tke,"LI",{});var kEe=s(ov);Zle=n(kEe,"STRONG",{});var qJr=s(Zle);wjo=r(qJr,"speech_to_text"),qJr.forEach(t),Ajo=r(kEe," \u2014 "),TN=n(kEe,"A",{href:!0});var GJr=s(TN);Ljo=r(GJr,"Speech2TextForConditionalGeneration"),GJr.forEach(t),Bjo=r(kEe," (Speech2Text model)"),kEe.forEach(t),tke.forEach(t),kjo=i(Yt),rv=n(Yt,"P",{});var xEe=s(rv);xjo=r(xEe,"The model is set in evaluation mode by default using "),eie=n(xEe,"CODE",{});var OJr=s(eie);Rjo=r(OJr,"model.eval()"),OJr.forEach(t),Sjo=r(xEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=n(xEe,"CODE",{});var XJr=s(oie);Pjo=r(XJr,"model.train()"),XJr.forEach(t),xEe.forEach(t),$jo=i(Yt),rie=n(Yt,"P",{});var zJr=s(rie);Ijo=r(zJr,"Examples:"),zJr.forEach(t),jjo=i(Yt),m(WE.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),JLe=i(d),Nd=n(d,"H2",{class:!0});var ake=s(Nd);tv=n(ake,"A",{id:!0,class:!0,href:!0});var VJr=s(tv);tie=n(VJr,"SPAN",{});var WJr=s(tie);m(QE.$$.fragment,WJr),WJr.forEach(t),VJr.forEach(t),Njo=i(ake),aie=n(ake,"SPAN",{});var QJr=s(aie);Djo=r(QJr,"AutoModelForAudioXVector"),QJr.forEach(t),ake.forEach(t),YLe=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(HE.$$.fragment,vl),qjo=i(vl),Dd=n(vl,"P",{});var Az=s(Dd);Gjo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=n(Az,"CODE",{});var HJr=s(nie);Ojo=r(HJr,"from_pretrained()"),HJr.forEach(t),Xjo=r(Az,"class method or the "),sie=n(Az,"CODE",{});var UJr=s(sie);zjo=r(UJr,"from_config()"),UJr.forEach(t),Vjo=r(Az,`class
method.`),Az.forEach(t),Wjo=i(vl),UE=n(vl,"P",{});var nke=s(UE);Qjo=r(nke,"This class cannot be instantiated directly using "),lie=n(nke,"CODE",{});var JJr=s(lie);Hjo=r(JJr,"__init__()"),JJr.forEach(t),Ujo=r(nke," (throws an error)."),nke.forEach(t),Jjo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(JE.$$.fragment,Tl),Yjo=i(Tl),iie=n(Tl,"P",{});var YJr=s(iie);Kjo=r(YJr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),YJr.forEach(t),Zjo=i(Tl),qd=n(Tl,"P",{});var Lz=s(qd);eNo=r(Lz,`Note:
Loading a model from its configuration file does `),die=n(Lz,"STRONG",{});var KJr=s(die);oNo=r(KJr,"not"),KJr.forEach(t),rNo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=n(Lz,"CODE",{});var ZJr=s(cie);tNo=r(ZJr,"from_pretrained()"),ZJr.forEach(t),aNo=r(Lz,"to load the model weights."),Lz.forEach(t),nNo=i(Tl),fie=n(Tl,"P",{});var eYr=s(fie);sNo=r(eYr,"Examples:"),eYr.forEach(t),lNo=i(Tl),m(YE.$$.fragment,Tl),Tl.forEach(t),iNo=i(vl),Qe=n(vl,"DIV",{class:!0});var Kt=s(Qe);m(KE.$$.fragment,Kt),dNo=i(Kt),mie=n(Kt,"P",{});var oYr=s(mie);cNo=r(oYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),oYr.forEach(t),fNo=i(Kt),rn=n(Kt,"P",{});var EC=s(rn);mNo=r(EC,"The model class to instantiate is selected based on the "),gie=n(EC,"CODE",{});var rYr=s(gie);gNo=r(rYr,"model_type"),rYr.forEach(t),hNo=r(EC,` property of the config object (either
passed as an argument or loaded from `),hie=n(EC,"CODE",{});var tYr=s(hie);pNo=r(tYr,"pretrained_model_name_or_path"),tYr.forEach(t),_No=r(EC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=n(EC,"CODE",{});var aYr=s(pie);uNo=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),bNo=r(EC,":"),EC.forEach(t),vNo=i(Kt),Gd=n(Kt,"UL",{});var Bz=s(Gd);av=n(Bz,"LI",{});var REe=s(av);_ie=n(REe,"STRONG",{});var nYr=s(_ie);TNo=r(nYr,"unispeech-sat"),nYr.forEach(t),FNo=r(REe," \u2014 "),FN=n(REe,"A",{href:!0});var sYr=s(FN);CNo=r(sYr,"UniSpeechSatForXVector"),sYr.forEach(t),MNo=r(REe," (UniSpeechSat model)"),REe.forEach(t),ENo=i(Bz),nv=n(Bz,"LI",{});var SEe=s(nv);uie=n(SEe,"STRONG",{});var lYr=s(uie);yNo=r(lYr,"wav2vec2"),lYr.forEach(t),wNo=r(SEe," \u2014 "),CN=n(SEe,"A",{href:!0});var iYr=s(CN);ANo=r(iYr,"Wav2Vec2ForXVector"),iYr.forEach(t),LNo=r(SEe," (Wav2Vec2 model)"),SEe.forEach(t),BNo=i(Bz),sv=n(Bz,"LI",{});var PEe=s(sv);bie=n(PEe,"STRONG",{});var dYr=s(bie);kNo=r(dYr,"wavlm"),dYr.forEach(t),xNo=r(PEe," \u2014 "),MN=n(PEe,"A",{href:!0});var cYr=s(MN);RNo=r(cYr,"WavLMForXVector"),cYr.forEach(t),SNo=r(PEe," (WavLM model)"),PEe.forEach(t),Bz.forEach(t),PNo=i(Kt),lv=n(Kt,"P",{});var $Ee=s(lv);$No=r($Ee,"The model is set in evaluation mode by default using "),vie=n($Ee,"CODE",{});var fYr=s(vie);INo=r(fYr,"model.eval()"),fYr.forEach(t),jNo=r($Ee,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=n($Ee,"CODE",{});var mYr=s(Tie);NNo=r(mYr,"model.train()"),mYr.forEach(t),$Ee.forEach(t),DNo=i(Kt),Fie=n(Kt,"P",{});var gYr=s(Fie);qNo=r(gYr,"Examples:"),gYr.forEach(t),GNo=i(Kt),m(ZE.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),KLe=i(d),Od=n(d,"H2",{class:!0});var ske=s(Od);iv=n(ske,"A",{id:!0,class:!0,href:!0});var hYr=s(iv);Cie=n(hYr,"SPAN",{});var pYr=s(Cie);m(e3.$$.fragment,pYr),pYr.forEach(t),hYr.forEach(t),ONo=i(ske),Mie=n(ske,"SPAN",{});var _Yr=s(Mie);XNo=r(_Yr,"AutoModelForMaskedImageModeling"),_Yr.forEach(t),ske.forEach(t),ZLe=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(o3.$$.fragment,Fl),zNo=i(Fl),Xd=n(Fl,"P",{});var kz=s(Xd);VNo=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=n(kz,"CODE",{});var uYr=s(Eie);WNo=r(uYr,"from_pretrained()"),uYr.forEach(t),QNo=r(kz,"class method or the "),yie=n(kz,"CODE",{});var bYr=s(yie);HNo=r(bYr,"from_config()"),bYr.forEach(t),UNo=r(kz,`class
method.`),kz.forEach(t),JNo=i(Fl),r3=n(Fl,"P",{});var lke=s(r3);YNo=r(lke,"This class cannot be instantiated directly using "),wie=n(lke,"CODE",{});var vYr=s(wie);KNo=r(vYr,"__init__()"),vYr.forEach(t),ZNo=r(lke," (throws an error)."),lke.forEach(t),eDo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(t3.$$.fragment,Cl),oDo=i(Cl),Aie=n(Cl,"P",{});var TYr=s(Aie);rDo=r(TYr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),TYr.forEach(t),tDo=i(Cl),zd=n(Cl,"P",{});var xz=s(zd);aDo=r(xz,`Note:
Loading a model from its configuration file does `),Lie=n(xz,"STRONG",{});var FYr=s(Lie);nDo=r(FYr,"not"),FYr.forEach(t),sDo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=n(xz,"CODE",{});var CYr=s(Bie);lDo=r(CYr,"from_pretrained()"),CYr.forEach(t),iDo=r(xz,"to load the model weights."),xz.forEach(t),dDo=i(Cl),kie=n(Cl,"P",{});var MYr=s(kie);cDo=r(MYr,"Examples:"),MYr.forEach(t),fDo=i(Cl),m(a3.$$.fragment,Cl),Cl.forEach(t),mDo=i(Fl),He=n(Fl,"DIV",{class:!0});var Zt=s(He);m(n3.$$.fragment,Zt),gDo=i(Zt),xie=n(Zt,"P",{});var EYr=s(xie);hDo=r(EYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),EYr.forEach(t),pDo=i(Zt),tn=n(Zt,"P",{});var yC=s(tn);_Do=r(yC,"The model class to instantiate is selected based on the "),Rie=n(yC,"CODE",{});var yYr=s(Rie);uDo=r(yYr,"model_type"),yYr.forEach(t),bDo=r(yC,` property of the config object (either
passed as an argument or loaded from `),Sie=n(yC,"CODE",{});var wYr=s(Sie);vDo=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),TDo=r(yC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=n(yC,"CODE",{});var AYr=s(Pie);FDo=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),CDo=r(yC,":"),yC.forEach(t),MDo=i(Zt),Vd=n(Zt,"UL",{});var Rz=s(Vd);dv=n(Rz,"LI",{});var IEe=s(dv);$ie=n(IEe,"STRONG",{});var LYr=s($ie);EDo=r(LYr,"deit"),LYr.forEach(t),yDo=r(IEe," \u2014 "),EN=n(IEe,"A",{href:!0});var BYr=s(EN);wDo=r(BYr,"DeiTForMaskedImageModeling"),BYr.forEach(t),ADo=r(IEe," (DeiT model)"),IEe.forEach(t),LDo=i(Rz),cv=n(Rz,"LI",{});var jEe=s(cv);Iie=n(jEe,"STRONG",{});var kYr=s(Iie);BDo=r(kYr,"swin"),kYr.forEach(t),kDo=r(jEe," \u2014 "),yN=n(jEe,"A",{href:!0});var xYr=s(yN);xDo=r(xYr,"SwinForMaskedImageModeling"),xYr.forEach(t),RDo=r(jEe," (Swin model)"),jEe.forEach(t),SDo=i(Rz),fv=n(Rz,"LI",{});var NEe=s(fv);jie=n(NEe,"STRONG",{});var RYr=s(jie);PDo=r(RYr,"vit"),RYr.forEach(t),$Do=r(NEe," \u2014 "),wN=n(NEe,"A",{href:!0});var SYr=s(wN);IDo=r(SYr,"ViTForMaskedImageModeling"),SYr.forEach(t),jDo=r(NEe," (ViT model)"),NEe.forEach(t),Rz.forEach(t),NDo=i(Zt),mv=n(Zt,"P",{});var DEe=s(mv);DDo=r(DEe,"The model is set in evaluation mode by default using "),Nie=n(DEe,"CODE",{});var PYr=s(Nie);qDo=r(PYr,"model.eval()"),PYr.forEach(t),GDo=r(DEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=n(DEe,"CODE",{});var $Yr=s(Die);ODo=r($Yr,"model.train()"),$Yr.forEach(t),DEe.forEach(t),XDo=i(Zt),qie=n(Zt,"P",{});var IYr=s(qie);zDo=r(IYr,"Examples:"),IYr.forEach(t),VDo=i(Zt),m(s3.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),e8e=i(d),Wd=n(d,"H2",{class:!0});var ike=s(Wd);gv=n(ike,"A",{id:!0,class:!0,href:!0});var jYr=s(gv);Gie=n(jYr,"SPAN",{});var NYr=s(Gie);m(l3.$$.fragment,NYr),NYr.forEach(t),jYr.forEach(t),WDo=i(ike),Oie=n(ike,"SPAN",{});var DYr=s(Oie);QDo=r(DYr,"AutoModelForObjectDetection"),DYr.forEach(t),ike.forEach(t),o8e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(i3.$$.fragment,Ml),HDo=i(Ml),Qd=n(Ml,"P",{});var Sz=s(Qd);UDo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=n(Sz,"CODE",{});var qYr=s(Xie);JDo=r(qYr,"from_pretrained()"),qYr.forEach(t),YDo=r(Sz,"class method or the "),zie=n(Sz,"CODE",{});var GYr=s(zie);KDo=r(GYr,"from_config()"),GYr.forEach(t),ZDo=r(Sz,`class
method.`),Sz.forEach(t),eqo=i(Ml),d3=n(Ml,"P",{});var dke=s(d3);oqo=r(dke,"This class cannot be instantiated directly using "),Vie=n(dke,"CODE",{});var OYr=s(Vie);rqo=r(OYr,"__init__()"),OYr.forEach(t),tqo=r(dke," (throws an error)."),dke.forEach(t),aqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(c3.$$.fragment,El),nqo=i(El),Wie=n(El,"P",{});var XYr=s(Wie);sqo=r(XYr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),XYr.forEach(t),lqo=i(El),Hd=n(El,"P",{});var Pz=s(Hd);iqo=r(Pz,`Note:
Loading a model from its configuration file does `),Qie=n(Pz,"STRONG",{});var zYr=s(Qie);dqo=r(zYr,"not"),zYr.forEach(t),cqo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=n(Pz,"CODE",{});var VYr=s(Hie);fqo=r(VYr,"from_pretrained()"),VYr.forEach(t),mqo=r(Pz,"to load the model weights."),Pz.forEach(t),gqo=i(El),Uie=n(El,"P",{});var WYr=s(Uie);hqo=r(WYr,"Examples:"),WYr.forEach(t),pqo=i(El),m(f3.$$.fragment,El),El.forEach(t),_qo=i(Ml),Ue=n(Ml,"DIV",{class:!0});var ea=s(Ue);m(m3.$$.fragment,ea),uqo=i(ea),Jie=n(ea,"P",{});var QYr=s(Jie);bqo=r(QYr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),QYr.forEach(t),vqo=i(ea),an=n(ea,"P",{});var wC=s(an);Tqo=r(wC,"The model class to instantiate is selected based on the "),Yie=n(wC,"CODE",{});var HYr=s(Yie);Fqo=r(HYr,"model_type"),HYr.forEach(t),Cqo=r(wC,` property of the config object (either
passed as an argument or loaded from `),Kie=n(wC,"CODE",{});var UYr=s(Kie);Mqo=r(UYr,"pretrained_model_name_or_path"),UYr.forEach(t),Eqo=r(wC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=n(wC,"CODE",{});var JYr=s(Zie);yqo=r(JYr,"pretrained_model_name_or_path"),JYr.forEach(t),wqo=r(wC,":"),wC.forEach(t),Aqo=i(ea),ede=n(ea,"UL",{});var YYr=s(ede);hv=n(YYr,"LI",{});var qEe=s(hv);ode=n(qEe,"STRONG",{});var KYr=s(ode);Lqo=r(KYr,"detr"),KYr.forEach(t),Bqo=r(qEe," \u2014 "),AN=n(qEe,"A",{href:!0});var ZYr=s(AN);kqo=r(ZYr,"DetrForObjectDetection"),ZYr.forEach(t),xqo=r(qEe," (DETR model)"),qEe.forEach(t),YYr.forEach(t),Rqo=i(ea),pv=n(ea,"P",{});var GEe=s(pv);Sqo=r(GEe,"The model is set in evaluation mode by default using "),rde=n(GEe,"CODE",{});var eKr=s(rde);Pqo=r(eKr,"model.eval()"),eKr.forEach(t),$qo=r(GEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=n(GEe,"CODE",{});var oKr=s(tde);Iqo=r(oKr,"model.train()"),oKr.forEach(t),GEe.forEach(t),jqo=i(ea),ade=n(ea,"P",{});var rKr=s(ade);Nqo=r(rKr,"Examples:"),rKr.forEach(t),Dqo=i(ea),m(g3.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),r8e=i(d),Ud=n(d,"H2",{class:!0});var cke=s(Ud);_v=n(cke,"A",{id:!0,class:!0,href:!0});var tKr=s(_v);nde=n(tKr,"SPAN",{});var aKr=s(nde);m(h3.$$.fragment,aKr),aKr.forEach(t),tKr.forEach(t),qqo=i(cke),sde=n(cke,"SPAN",{});var nKr=s(sde);Gqo=r(nKr,"AutoModelForImageSegmentation"),nKr.forEach(t),cke.forEach(t),t8e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(p3.$$.fragment,yl),Oqo=i(yl),Jd=n(yl,"P",{});var $z=s(Jd);Xqo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=n($z,"CODE",{});var sKr=s(lde);zqo=r(sKr,"from_pretrained()"),sKr.forEach(t),Vqo=r($z,"class method or the "),ide=n($z,"CODE",{});var lKr=s(ide);Wqo=r(lKr,"from_config()"),lKr.forEach(t),Qqo=r($z,`class
method.`),$z.forEach(t),Hqo=i(yl),_3=n(yl,"P",{});var fke=s(_3);Uqo=r(fke,"This class cannot be instantiated directly using "),dde=n(fke,"CODE",{});var iKr=s(dde);Jqo=r(iKr,"__init__()"),iKr.forEach(t),Yqo=r(fke," (throws an error)."),fke.forEach(t),Kqo=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(u3.$$.fragment,wl),Zqo=i(wl),cde=n(wl,"P",{});var dKr=s(cde);eGo=r(dKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),dKr.forEach(t),oGo=i(wl),Yd=n(wl,"P",{});var Iz=s(Yd);rGo=r(Iz,`Note:
Loading a model from its configuration file does `),fde=n(Iz,"STRONG",{});var cKr=s(fde);tGo=r(cKr,"not"),cKr.forEach(t),aGo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=n(Iz,"CODE",{});var fKr=s(mde);nGo=r(fKr,"from_pretrained()"),fKr.forEach(t),sGo=r(Iz,"to load the model weights."),Iz.forEach(t),lGo=i(wl),gde=n(wl,"P",{});var mKr=s(gde);iGo=r(mKr,"Examples:"),mKr.forEach(t),dGo=i(wl),m(b3.$$.fragment,wl),wl.forEach(t),cGo=i(yl),Je=n(yl,"DIV",{class:!0});var oa=s(Je);m(v3.$$.fragment,oa),fGo=i(oa),hde=n(oa,"P",{});var gKr=s(hde);mGo=r(gKr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gKr.forEach(t),gGo=i(oa),nn=n(oa,"P",{});var AC=s(nn);hGo=r(AC,"The model class to instantiate is selected based on the "),pde=n(AC,"CODE",{});var hKr=s(pde);pGo=r(hKr,"model_type"),hKr.forEach(t),_Go=r(AC,` property of the config object (either
passed as an argument or loaded from `),_de=n(AC,"CODE",{});var pKr=s(_de);uGo=r(pKr,"pretrained_model_name_or_path"),pKr.forEach(t),bGo=r(AC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=n(AC,"CODE",{});var _Kr=s(ude);vGo=r(_Kr,"pretrained_model_name_or_path"),_Kr.forEach(t),TGo=r(AC,":"),AC.forEach(t),FGo=i(oa),bde=n(oa,"UL",{});var uKr=s(bde);uv=n(uKr,"LI",{});var OEe=s(uv);vde=n(OEe,"STRONG",{});var bKr=s(vde);CGo=r(bKr,"detr"),bKr.forEach(t),MGo=r(OEe," \u2014 "),LN=n(OEe,"A",{href:!0});var vKr=s(LN);EGo=r(vKr,"DetrForSegmentation"),vKr.forEach(t),yGo=r(OEe," (DETR model)"),OEe.forEach(t),uKr.forEach(t),wGo=i(oa),bv=n(oa,"P",{});var XEe=s(bv);AGo=r(XEe,"The model is set in evaluation mode by default using "),Tde=n(XEe,"CODE",{});var TKr=s(Tde);LGo=r(TKr,"model.eval()"),TKr.forEach(t),BGo=r(XEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=n(XEe,"CODE",{});var FKr=s(Fde);kGo=r(FKr,"model.train()"),FKr.forEach(t),XEe.forEach(t),xGo=i(oa),Cde=n(oa,"P",{});var CKr=s(Cde);RGo=r(CKr,"Examples:"),CKr.forEach(t),SGo=i(oa),m(T3.$$.fragment,oa),oa.forEach(t),yl.forEach(t),a8e=i(d),Kd=n(d,"H2",{class:!0});var mke=s(Kd);vv=n(mke,"A",{id:!0,class:!0,href:!0});var MKr=s(vv);Mde=n(MKr,"SPAN",{});var EKr=s(Mde);m(F3.$$.fragment,EKr),EKr.forEach(t),MKr.forEach(t),PGo=i(mke),Ede=n(mke,"SPAN",{});var yKr=s(Ede);$Go=r(yKr,"AutoModelForSemanticSegmentation"),yKr.forEach(t),mke.forEach(t),n8e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(C3.$$.fragment,Al),IGo=i(Al),Zd=n(Al,"P",{});var jz=s(Zd);jGo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=n(jz,"CODE",{});var wKr=s(yde);NGo=r(wKr,"from_pretrained()"),wKr.forEach(t),DGo=r(jz,"class method or the "),wde=n(jz,"CODE",{});var AKr=s(wde);qGo=r(AKr,"from_config()"),AKr.forEach(t),GGo=r(jz,`class
method.`),jz.forEach(t),OGo=i(Al),M3=n(Al,"P",{});var gke=s(M3);XGo=r(gke,"This class cannot be instantiated directly using "),Ade=n(gke,"CODE",{});var LKr=s(Ade);zGo=r(LKr,"__init__()"),LKr.forEach(t),VGo=r(gke," (throws an error)."),gke.forEach(t),WGo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(E3.$$.fragment,Ll),QGo=i(Ll),Lde=n(Ll,"P",{});var BKr=s(Lde);HGo=r(BKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),BKr.forEach(t),UGo=i(Ll),ec=n(Ll,"P",{});var Nz=s(ec);JGo=r(Nz,`Note:
Loading a model from its configuration file does `),Bde=n(Nz,"STRONG",{});var kKr=s(Bde);YGo=r(kKr,"not"),kKr.forEach(t),KGo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=n(Nz,"CODE",{});var xKr=s(kde);ZGo=r(xKr,"from_pretrained()"),xKr.forEach(t),eOo=r(Nz,"to load the model weights."),Nz.forEach(t),oOo=i(Ll),xde=n(Ll,"P",{});var RKr=s(xde);rOo=r(RKr,"Examples:"),RKr.forEach(t),tOo=i(Ll),m(y3.$$.fragment,Ll),Ll.forEach(t),aOo=i(Al),Ye=n(Al,"DIV",{class:!0});var ra=s(Ye);m(w3.$$.fragment,ra),nOo=i(ra),Rde=n(ra,"P",{});var SKr=s(Rde);sOo=r(SKr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),SKr.forEach(t),lOo=i(ra),sn=n(ra,"P",{});var LC=s(sn);iOo=r(LC,"The model class to instantiate is selected based on the "),Sde=n(LC,"CODE",{});var PKr=s(Sde);dOo=r(PKr,"model_type"),PKr.forEach(t),cOo=r(LC,` property of the config object (either
passed as an argument or loaded from `),Pde=n(LC,"CODE",{});var $Kr=s(Pde);fOo=r($Kr,"pretrained_model_name_or_path"),$Kr.forEach(t),mOo=r(LC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=n(LC,"CODE",{});var IKr=s($de);gOo=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),hOo=r(LC,":"),LC.forEach(t),pOo=i(ra),A3=n(ra,"UL",{});var hke=s(A3);Tv=n(hke,"LI",{});var zEe=s(Tv);Ide=n(zEe,"STRONG",{});var jKr=s(Ide);_Oo=r(jKr,"beit"),jKr.forEach(t),uOo=r(zEe," \u2014 "),BN=n(zEe,"A",{href:!0});var NKr=s(BN);bOo=r(NKr,"BeitForSemanticSegmentation"),NKr.forEach(t),vOo=r(zEe," (BEiT model)"),zEe.forEach(t),TOo=i(hke),Fv=n(hke,"LI",{});var VEe=s(Fv);jde=n(VEe,"STRONG",{});var DKr=s(jde);FOo=r(DKr,"segformer"),DKr.forEach(t),COo=r(VEe," \u2014 "),kN=n(VEe,"A",{href:!0});var qKr=s(kN);MOo=r(qKr,"SegformerForSemanticSegmentation"),qKr.forEach(t),EOo=r(VEe," (SegFormer model)"),VEe.forEach(t),hke.forEach(t),yOo=i(ra),Cv=n(ra,"P",{});var WEe=s(Cv);wOo=r(WEe,"The model is set in evaluation mode by default using "),Nde=n(WEe,"CODE",{});var GKr=s(Nde);AOo=r(GKr,"model.eval()"),GKr.forEach(t),LOo=r(WEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=n(WEe,"CODE",{});var OKr=s(Dde);BOo=r(OKr,"model.train()"),OKr.forEach(t),WEe.forEach(t),kOo=i(ra),qde=n(ra,"P",{});var XKr=s(qde);xOo=r(XKr,"Examples:"),XKr.forEach(t),ROo=i(ra),m(L3.$$.fragment,ra),ra.forEach(t),Al.forEach(t),s8e=i(d),oc=n(d,"H2",{class:!0});var pke=s(oc);Mv=n(pke,"A",{id:!0,class:!0,href:!0});var zKr=s(Mv);Gde=n(zKr,"SPAN",{});var VKr=s(Gde);m(B3.$$.fragment,VKr),VKr.forEach(t),zKr.forEach(t),SOo=i(pke),Ode=n(pke,"SPAN",{});var WKr=s(Ode);POo=r(WKr,"TFAutoModel"),WKr.forEach(t),pke.forEach(t),l8e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(k3.$$.fragment,Bl),$Oo=i(Bl),rc=n(Bl,"P",{});var Dz=s(rc);IOo=r(Dz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=n(Dz,"CODE",{});var QKr=s(Xde);jOo=r(QKr,"from_pretrained()"),QKr.forEach(t),NOo=r(Dz,"class method or the "),zde=n(Dz,"CODE",{});var HKr=s(zde);DOo=r(HKr,"from_config()"),HKr.forEach(t),qOo=r(Dz,`class
method.`),Dz.forEach(t),GOo=i(Bl),x3=n(Bl,"P",{});var _ke=s(x3);OOo=r(_ke,"This class cannot be instantiated directly using "),Vde=n(_ke,"CODE",{});var UKr=s(Vde);XOo=r(UKr,"__init__()"),UKr.forEach(t),zOo=r(_ke," (throws an error)."),_ke.forEach(t),VOo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(R3.$$.fragment,kl),WOo=i(kl),Wde=n(kl,"P",{});var JKr=s(Wde);QOo=r(JKr,"Instantiates one of the base model classes of the library from a configuration."),JKr.forEach(t),HOo=i(kl),tc=n(kl,"P",{});var qz=s(tc);UOo=r(qz,`Note:
Loading a model from its configuration file does `),Qde=n(qz,"STRONG",{});var YKr=s(Qde);JOo=r(YKr,"not"),YKr.forEach(t),YOo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=n(qz,"CODE",{});var KKr=s(Hde);KOo=r(KKr,"from_pretrained()"),KKr.forEach(t),ZOo=r(qz,"to load the model weights."),qz.forEach(t),eXo=i(kl),Ude=n(kl,"P",{});var ZKr=s(Ude);oXo=r(ZKr,"Examples:"),ZKr.forEach(t),rXo=i(kl),m(S3.$$.fragment,kl),kl.forEach(t),tXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(P3.$$.fragment,ca),aXo=i(ca),Jde=n(ca,"P",{});var eZr=s(Jde);nXo=r(eZr,"Instantiate one of the base model classes of the library from a pretrained model."),eZr.forEach(t),sXo=i(ca),ln=n(ca,"P",{});var BC=s(ln);lXo=r(BC,"The model class to instantiate is selected based on the "),Yde=n(BC,"CODE",{});var oZr=s(Yde);iXo=r(oZr,"model_type"),oZr.forEach(t),dXo=r(BC,` property of the config object (either
passed as an argument or loaded from `),Kde=n(BC,"CODE",{});var rZr=s(Kde);cXo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),fXo=r(BC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=n(BC,"CODE",{});var tZr=s(Zde);mXo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),gXo=r(BC,":"),BC.forEach(t),hXo=i(ca),B=n(ca,"UL",{});var k=s(B);Ev=n(k,"LI",{});var QEe=s(Ev);ece=n(QEe,"STRONG",{});var aZr=s(ece);pXo=r(aZr,"albert"),aZr.forEach(t),_Xo=r(QEe," \u2014 "),xN=n(QEe,"A",{href:!0});var nZr=s(xN);uXo=r(nZr,"TFAlbertModel"),nZr.forEach(t),bXo=r(QEe," (ALBERT model)"),QEe.forEach(t),vXo=i(k),yv=n(k,"LI",{});var HEe=s(yv);oce=n(HEe,"STRONG",{});var sZr=s(oce);TXo=r(sZr,"bart"),sZr.forEach(t),FXo=r(HEe," \u2014 "),RN=n(HEe,"A",{href:!0});var lZr=s(RN);CXo=r(lZr,"TFBartModel"),lZr.forEach(t),MXo=r(HEe," (BART model)"),HEe.forEach(t),EXo=i(k),wv=n(k,"LI",{});var UEe=s(wv);rce=n(UEe,"STRONG",{});var iZr=s(rce);yXo=r(iZr,"bert"),iZr.forEach(t),wXo=r(UEe," \u2014 "),SN=n(UEe,"A",{href:!0});var dZr=s(SN);AXo=r(dZr,"TFBertModel"),dZr.forEach(t),LXo=r(UEe," (BERT model)"),UEe.forEach(t),BXo=i(k),Av=n(k,"LI",{});var JEe=s(Av);tce=n(JEe,"STRONG",{});var cZr=s(tce);kXo=r(cZr,"blenderbot"),cZr.forEach(t),xXo=r(JEe," \u2014 "),PN=n(JEe,"A",{href:!0});var fZr=s(PN);RXo=r(fZr,"TFBlenderbotModel"),fZr.forEach(t),SXo=r(JEe," (Blenderbot model)"),JEe.forEach(t),PXo=i(k),Lv=n(k,"LI",{});var YEe=s(Lv);ace=n(YEe,"STRONG",{});var mZr=s(ace);$Xo=r(mZr,"blenderbot-small"),mZr.forEach(t),IXo=r(YEe," \u2014 "),$N=n(YEe,"A",{href:!0});var gZr=s($N);jXo=r(gZr,"TFBlenderbotSmallModel"),gZr.forEach(t),NXo=r(YEe," (BlenderbotSmall model)"),YEe.forEach(t),DXo=i(k),Bv=n(k,"LI",{});var KEe=s(Bv);nce=n(KEe,"STRONG",{});var hZr=s(nce);qXo=r(hZr,"camembert"),hZr.forEach(t),GXo=r(KEe," \u2014 "),IN=n(KEe,"A",{href:!0});var pZr=s(IN);OXo=r(pZr,"TFCamembertModel"),pZr.forEach(t),XXo=r(KEe," (CamemBERT model)"),KEe.forEach(t),zXo=i(k),kv=n(k,"LI",{});var ZEe=s(kv);sce=n(ZEe,"STRONG",{});var _Zr=s(sce);VXo=r(_Zr,"clip"),_Zr.forEach(t),WXo=r(ZEe," \u2014 "),jN=n(ZEe,"A",{href:!0});var uZr=s(jN);QXo=r(uZr,"TFCLIPModel"),uZr.forEach(t),HXo=r(ZEe," (CLIP model)"),ZEe.forEach(t),UXo=i(k),xv=n(k,"LI",{});var e3e=s(xv);lce=n(e3e,"STRONG",{});var bZr=s(lce);JXo=r(bZr,"convbert"),bZr.forEach(t),YXo=r(e3e," \u2014 "),NN=n(e3e,"A",{href:!0});var vZr=s(NN);KXo=r(vZr,"TFConvBertModel"),vZr.forEach(t),ZXo=r(e3e," (ConvBERT model)"),e3e.forEach(t),ezo=i(k),Rv=n(k,"LI",{});var o3e=s(Rv);ice=n(o3e,"STRONG",{});var TZr=s(ice);ozo=r(TZr,"ctrl"),TZr.forEach(t),rzo=r(o3e," \u2014 "),DN=n(o3e,"A",{href:!0});var FZr=s(DN);tzo=r(FZr,"TFCTRLModel"),FZr.forEach(t),azo=r(o3e," (CTRL model)"),o3e.forEach(t),nzo=i(k),Sv=n(k,"LI",{});var r3e=s(Sv);dce=n(r3e,"STRONG",{});var CZr=s(dce);szo=r(CZr,"deberta"),CZr.forEach(t),lzo=r(r3e," \u2014 "),qN=n(r3e,"A",{href:!0});var MZr=s(qN);izo=r(MZr,"TFDebertaModel"),MZr.forEach(t),dzo=r(r3e," (DeBERTa model)"),r3e.forEach(t),czo=i(k),Pv=n(k,"LI",{});var t3e=s(Pv);cce=n(t3e,"STRONG",{});var EZr=s(cce);fzo=r(EZr,"deberta-v2"),EZr.forEach(t),mzo=r(t3e," \u2014 "),GN=n(t3e,"A",{href:!0});var yZr=s(GN);gzo=r(yZr,"TFDebertaV2Model"),yZr.forEach(t),hzo=r(t3e," (DeBERTa-v2 model)"),t3e.forEach(t),pzo=i(k),$v=n(k,"LI",{});var a3e=s($v);fce=n(a3e,"STRONG",{});var wZr=s(fce);_zo=r(wZr,"distilbert"),wZr.forEach(t),uzo=r(a3e," \u2014 "),ON=n(a3e,"A",{href:!0});var AZr=s(ON);bzo=r(AZr,"TFDistilBertModel"),AZr.forEach(t),vzo=r(a3e," (DistilBERT model)"),a3e.forEach(t),Tzo=i(k),Iv=n(k,"LI",{});var n3e=s(Iv);mce=n(n3e,"STRONG",{});var LZr=s(mce);Fzo=r(LZr,"dpr"),LZr.forEach(t),Czo=r(n3e," \u2014 "),XN=n(n3e,"A",{href:!0});var BZr=s(XN);Mzo=r(BZr,"TFDPRQuestionEncoder"),BZr.forEach(t),Ezo=r(n3e," (DPR model)"),n3e.forEach(t),yzo=i(k),jv=n(k,"LI",{});var s3e=s(jv);gce=n(s3e,"STRONG",{});var kZr=s(gce);wzo=r(kZr,"electra"),kZr.forEach(t),Azo=r(s3e," \u2014 "),zN=n(s3e,"A",{href:!0});var xZr=s(zN);Lzo=r(xZr,"TFElectraModel"),xZr.forEach(t),Bzo=r(s3e," (ELECTRA model)"),s3e.forEach(t),kzo=i(k),Nv=n(k,"LI",{});var l3e=s(Nv);hce=n(l3e,"STRONG",{});var RZr=s(hce);xzo=r(RZr,"flaubert"),RZr.forEach(t),Rzo=r(l3e," \u2014 "),VN=n(l3e,"A",{href:!0});var SZr=s(VN);Szo=r(SZr,"TFFlaubertModel"),SZr.forEach(t),Pzo=r(l3e," (FlauBERT model)"),l3e.forEach(t),$zo=i(k),Ss=n(k,"LI",{});var N0=s(Ss);pce=n(N0,"STRONG",{});var PZr=s(pce);Izo=r(PZr,"funnel"),PZr.forEach(t),jzo=r(N0," \u2014 "),WN=n(N0,"A",{href:!0});var $Zr=s(WN);Nzo=r($Zr,"TFFunnelModel"),$Zr.forEach(t),Dzo=r(N0," or "),QN=n(N0,"A",{href:!0});var IZr=s(QN);qzo=r(IZr,"TFFunnelBaseModel"),IZr.forEach(t),Gzo=r(N0," (Funnel Transformer model)"),N0.forEach(t),Ozo=i(k),Dv=n(k,"LI",{});var i3e=s(Dv);_ce=n(i3e,"STRONG",{});var jZr=s(_ce);Xzo=r(jZr,"gpt2"),jZr.forEach(t),zzo=r(i3e," \u2014 "),HN=n(i3e,"A",{href:!0});var NZr=s(HN);Vzo=r(NZr,"TFGPT2Model"),NZr.forEach(t),Wzo=r(i3e," (OpenAI GPT-2 model)"),i3e.forEach(t),Qzo=i(k),qv=n(k,"LI",{});var d3e=s(qv);uce=n(d3e,"STRONG",{});var DZr=s(uce);Hzo=r(DZr,"hubert"),DZr.forEach(t),Uzo=r(d3e," \u2014 "),UN=n(d3e,"A",{href:!0});var qZr=s(UN);Jzo=r(qZr,"TFHubertModel"),qZr.forEach(t),Yzo=r(d3e," (Hubert model)"),d3e.forEach(t),Kzo=i(k),Gv=n(k,"LI",{});var c3e=s(Gv);bce=n(c3e,"STRONG",{});var GZr=s(bce);Zzo=r(GZr,"layoutlm"),GZr.forEach(t),eVo=r(c3e," \u2014 "),JN=n(c3e,"A",{href:!0});var OZr=s(JN);oVo=r(OZr,"TFLayoutLMModel"),OZr.forEach(t),rVo=r(c3e," (LayoutLM model)"),c3e.forEach(t),tVo=i(k),Ov=n(k,"LI",{});var f3e=s(Ov);vce=n(f3e,"STRONG",{});var XZr=s(vce);aVo=r(XZr,"led"),XZr.forEach(t),nVo=r(f3e," \u2014 "),YN=n(f3e,"A",{href:!0});var zZr=s(YN);sVo=r(zZr,"TFLEDModel"),zZr.forEach(t),lVo=r(f3e," (LED model)"),f3e.forEach(t),iVo=i(k),Xv=n(k,"LI",{});var m3e=s(Xv);Tce=n(m3e,"STRONG",{});var VZr=s(Tce);dVo=r(VZr,"longformer"),VZr.forEach(t),cVo=r(m3e," \u2014 "),KN=n(m3e,"A",{href:!0});var WZr=s(KN);fVo=r(WZr,"TFLongformerModel"),WZr.forEach(t),mVo=r(m3e," (Longformer model)"),m3e.forEach(t),gVo=i(k),zv=n(k,"LI",{});var g3e=s(zv);Fce=n(g3e,"STRONG",{});var QZr=s(Fce);hVo=r(QZr,"lxmert"),QZr.forEach(t),pVo=r(g3e," \u2014 "),ZN=n(g3e,"A",{href:!0});var HZr=s(ZN);_Vo=r(HZr,"TFLxmertModel"),HZr.forEach(t),uVo=r(g3e," (LXMERT model)"),g3e.forEach(t),bVo=i(k),Vv=n(k,"LI",{});var h3e=s(Vv);Cce=n(h3e,"STRONG",{});var UZr=s(Cce);vVo=r(UZr,"marian"),UZr.forEach(t),TVo=r(h3e," \u2014 "),eD=n(h3e,"A",{href:!0});var JZr=s(eD);FVo=r(JZr,"TFMarianModel"),JZr.forEach(t),CVo=r(h3e," (Marian model)"),h3e.forEach(t),MVo=i(k),Wv=n(k,"LI",{});var p3e=s(Wv);Mce=n(p3e,"STRONG",{});var YZr=s(Mce);EVo=r(YZr,"mbart"),YZr.forEach(t),yVo=r(p3e," \u2014 "),oD=n(p3e,"A",{href:!0});var KZr=s(oD);wVo=r(KZr,"TFMBartModel"),KZr.forEach(t),AVo=r(p3e," (mBART model)"),p3e.forEach(t),LVo=i(k),Qv=n(k,"LI",{});var _3e=s(Qv);Ece=n(_3e,"STRONG",{});var ZZr=s(Ece);BVo=r(ZZr,"mobilebert"),ZZr.forEach(t),kVo=r(_3e," \u2014 "),rD=n(_3e,"A",{href:!0});var eet=s(rD);xVo=r(eet,"TFMobileBertModel"),eet.forEach(t),RVo=r(_3e," (MobileBERT model)"),_3e.forEach(t),SVo=i(k),Hv=n(k,"LI",{});var u3e=s(Hv);yce=n(u3e,"STRONG",{});var oet=s(yce);PVo=r(oet,"mpnet"),oet.forEach(t),$Vo=r(u3e," \u2014 "),tD=n(u3e,"A",{href:!0});var ret=s(tD);IVo=r(ret,"TFMPNetModel"),ret.forEach(t),jVo=r(u3e," (MPNet model)"),u3e.forEach(t),NVo=i(k),Uv=n(k,"LI",{});var b3e=s(Uv);wce=n(b3e,"STRONG",{});var tet=s(wce);DVo=r(tet,"mt5"),tet.forEach(t),qVo=r(b3e," \u2014 "),aD=n(b3e,"A",{href:!0});var aet=s(aD);GVo=r(aet,"TFMT5Model"),aet.forEach(t),OVo=r(b3e," (mT5 model)"),b3e.forEach(t),XVo=i(k),Jv=n(k,"LI",{});var v3e=s(Jv);Ace=n(v3e,"STRONG",{});var net=s(Ace);zVo=r(net,"openai-gpt"),net.forEach(t),VVo=r(v3e," \u2014 "),nD=n(v3e,"A",{href:!0});var set=s(nD);WVo=r(set,"TFOpenAIGPTModel"),set.forEach(t),QVo=r(v3e," (OpenAI GPT model)"),v3e.forEach(t),HVo=i(k),Yv=n(k,"LI",{});var T3e=s(Yv);Lce=n(T3e,"STRONG",{});var iet=s(Lce);UVo=r(iet,"pegasus"),iet.forEach(t),JVo=r(T3e," \u2014 "),sD=n(T3e,"A",{href:!0});var det=s(sD);YVo=r(det,"TFPegasusModel"),det.forEach(t),KVo=r(T3e," (Pegasus model)"),T3e.forEach(t),ZVo=i(k),Kv=n(k,"LI",{});var F3e=s(Kv);Bce=n(F3e,"STRONG",{});var cet=s(Bce);eWo=r(cet,"rembert"),cet.forEach(t),oWo=r(F3e," \u2014 "),lD=n(F3e,"A",{href:!0});var fet=s(lD);rWo=r(fet,"TFRemBertModel"),fet.forEach(t),tWo=r(F3e," (RemBERT model)"),F3e.forEach(t),aWo=i(k),Zv=n(k,"LI",{});var C3e=s(Zv);kce=n(C3e,"STRONG",{});var met=s(kce);nWo=r(met,"roberta"),met.forEach(t),sWo=r(C3e," \u2014 "),iD=n(C3e,"A",{href:!0});var get=s(iD);lWo=r(get,"TFRobertaModel"),get.forEach(t),iWo=r(C3e," (RoBERTa model)"),C3e.forEach(t),dWo=i(k),e6=n(k,"LI",{});var M3e=s(e6);xce=n(M3e,"STRONG",{});var het=s(xce);cWo=r(het,"roformer"),het.forEach(t),fWo=r(M3e," \u2014 "),dD=n(M3e,"A",{href:!0});var pet=s(dD);mWo=r(pet,"TFRoFormerModel"),pet.forEach(t),gWo=r(M3e," (RoFormer model)"),M3e.forEach(t),hWo=i(k),o6=n(k,"LI",{});var E3e=s(o6);Rce=n(E3e,"STRONG",{});var _et=s(Rce);pWo=r(_et,"speech_to_text"),_et.forEach(t),_Wo=r(E3e," \u2014 "),cD=n(E3e,"A",{href:!0});var uet=s(cD);uWo=r(uet,"TFSpeech2TextModel"),uet.forEach(t),bWo=r(E3e," (Speech2Text model)"),E3e.forEach(t),vWo=i(k),r6=n(k,"LI",{});var y3e=s(r6);Sce=n(y3e,"STRONG",{});var bet=s(Sce);TWo=r(bet,"t5"),bet.forEach(t),FWo=r(y3e," \u2014 "),fD=n(y3e,"A",{href:!0});var vet=s(fD);CWo=r(vet,"TFT5Model"),vet.forEach(t),MWo=r(y3e," (T5 model)"),y3e.forEach(t),EWo=i(k),t6=n(k,"LI",{});var w3e=s(t6);Pce=n(w3e,"STRONG",{});var Tet=s(Pce);yWo=r(Tet,"tapas"),Tet.forEach(t),wWo=r(w3e," \u2014 "),mD=n(w3e,"A",{href:!0});var Fet=s(mD);AWo=r(Fet,"TFTapasModel"),Fet.forEach(t),LWo=r(w3e," (TAPAS model)"),w3e.forEach(t),BWo=i(k),a6=n(k,"LI",{});var A3e=s(a6);$ce=n(A3e,"STRONG",{});var Cet=s($ce);kWo=r(Cet,"transfo-xl"),Cet.forEach(t),xWo=r(A3e," \u2014 "),gD=n(A3e,"A",{href:!0});var Met=s(gD);RWo=r(Met,"TFTransfoXLModel"),Met.forEach(t),SWo=r(A3e," (Transformer-XL model)"),A3e.forEach(t),PWo=i(k),n6=n(k,"LI",{});var L3e=s(n6);Ice=n(L3e,"STRONG",{});var Eet=s(Ice);$Wo=r(Eet,"vit"),Eet.forEach(t),IWo=r(L3e," \u2014 "),hD=n(L3e,"A",{href:!0});var yet=s(hD);jWo=r(yet,"TFViTModel"),yet.forEach(t),NWo=r(L3e," (ViT model)"),L3e.forEach(t),DWo=i(k),s6=n(k,"LI",{});var B3e=s(s6);jce=n(B3e,"STRONG",{});var wet=s(jce);qWo=r(wet,"wav2vec2"),wet.forEach(t),GWo=r(B3e," \u2014 "),pD=n(B3e,"A",{href:!0});var Aet=s(pD);OWo=r(Aet,"TFWav2Vec2Model"),Aet.forEach(t),XWo=r(B3e," (Wav2Vec2 model)"),B3e.forEach(t),zWo=i(k),l6=n(k,"LI",{});var k3e=s(l6);Nce=n(k3e,"STRONG",{});var Let=s(Nce);VWo=r(Let,"xlm"),Let.forEach(t),WWo=r(k3e," \u2014 "),_D=n(k3e,"A",{href:!0});var Bet=s(_D);QWo=r(Bet,"TFXLMModel"),Bet.forEach(t),HWo=r(k3e," (XLM model)"),k3e.forEach(t),UWo=i(k),i6=n(k,"LI",{});var x3e=s(i6);Dce=n(x3e,"STRONG",{});var ket=s(Dce);JWo=r(ket,"xlm-roberta"),ket.forEach(t),YWo=r(x3e," \u2014 "),uD=n(x3e,"A",{href:!0});var xet=s(uD);KWo=r(xet,"TFXLMRobertaModel"),xet.forEach(t),ZWo=r(x3e," (XLM-RoBERTa model)"),x3e.forEach(t),eQo=i(k),d6=n(k,"LI",{});var R3e=s(d6);qce=n(R3e,"STRONG",{});var Ret=s(qce);oQo=r(Ret,"xlnet"),Ret.forEach(t),rQo=r(R3e," \u2014 "),bD=n(R3e,"A",{href:!0});var Set=s(bD);tQo=r(Set,"TFXLNetModel"),Set.forEach(t),aQo=r(R3e," (XLNet model)"),R3e.forEach(t),k.forEach(t),nQo=i(ca),Gce=n(ca,"P",{});var Pet=s(Gce);sQo=r(Pet,"Examples:"),Pet.forEach(t),lQo=i(ca),m($3.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),i8e=i(d),ac=n(d,"H2",{class:!0});var uke=s(ac);c6=n(uke,"A",{id:!0,class:!0,href:!0});var $et=s(c6);Oce=n($et,"SPAN",{});var Iet=s(Oce);m(I3.$$.fragment,Iet),Iet.forEach(t),$et.forEach(t),iQo=i(uke),Xce=n(uke,"SPAN",{});var jet=s(Xce);dQo=r(jet,"TFAutoModelForPreTraining"),jet.forEach(t),uke.forEach(t),d8e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(j3.$$.fragment,xl),cQo=i(xl),nc=n(xl,"P",{});var Gz=s(nc);fQo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=n(Gz,"CODE",{});var Net=s(zce);mQo=r(Net,"from_pretrained()"),Net.forEach(t),gQo=r(Gz,"class method or the "),Vce=n(Gz,"CODE",{});var Det=s(Vce);hQo=r(Det,"from_config()"),Det.forEach(t),pQo=r(Gz,`class
method.`),Gz.forEach(t),_Qo=i(xl),N3=n(xl,"P",{});var bke=s(N3);uQo=r(bke,"This class cannot be instantiated directly using "),Wce=n(bke,"CODE",{});var qet=s(Wce);bQo=r(qet,"__init__()"),qet.forEach(t),vQo=r(bke," (throws an error)."),bke.forEach(t),TQo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(D3.$$.fragment,Rl),FQo=i(Rl),Qce=n(Rl,"P",{});var Get=s(Qce);CQo=r(Get,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Get.forEach(t),MQo=i(Rl),sc=n(Rl,"P",{});var Oz=s(sc);EQo=r(Oz,`Note:
Loading a model from its configuration file does `),Hce=n(Oz,"STRONG",{});var Oet=s(Hce);yQo=r(Oet,"not"),Oet.forEach(t),wQo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=n(Oz,"CODE",{});var Xet=s(Uce);AQo=r(Xet,"from_pretrained()"),Xet.forEach(t),LQo=r(Oz,"to load the model weights."),Oz.forEach(t),BQo=i(Rl),Jce=n(Rl,"P",{});var zet=s(Jce);kQo=r(zet,"Examples:"),zet.forEach(t),xQo=i(Rl),m(q3.$$.fragment,Rl),Rl.forEach(t),RQo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(G3.$$.fragment,fa),SQo=i(fa),Yce=n(fa,"P",{});var Vet=s(Yce);PQo=r(Vet,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Vet.forEach(t),$Qo=i(fa),dn=n(fa,"P",{});var kC=s(dn);IQo=r(kC,"The model class to instantiate is selected based on the "),Kce=n(kC,"CODE",{});var Wet=s(Kce);jQo=r(Wet,"model_type"),Wet.forEach(t),NQo=r(kC,` property of the config object (either
passed as an argument or loaded from `),Zce=n(kC,"CODE",{});var Qet=s(Zce);DQo=r(Qet,"pretrained_model_name_or_path"),Qet.forEach(t),qQo=r(kC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=n(kC,"CODE",{});var Het=s(efe);GQo=r(Het,"pretrained_model_name_or_path"),Het.forEach(t),OQo=r(kC,":"),kC.forEach(t),XQo=i(fa),H=n(fa,"UL",{});var U=s(H);f6=n(U,"LI",{});var S3e=s(f6);ofe=n(S3e,"STRONG",{});var Uet=s(ofe);zQo=r(Uet,"albert"),Uet.forEach(t),VQo=r(S3e," \u2014 "),vD=n(S3e,"A",{href:!0});var Jet=s(vD);WQo=r(Jet,"TFAlbertForPreTraining"),Jet.forEach(t),QQo=r(S3e," (ALBERT model)"),S3e.forEach(t),HQo=i(U),m6=n(U,"LI",{});var P3e=s(m6);rfe=n(P3e,"STRONG",{});var Yet=s(rfe);UQo=r(Yet,"bart"),Yet.forEach(t),JQo=r(P3e," \u2014 "),TD=n(P3e,"A",{href:!0});var Ket=s(TD);YQo=r(Ket,"TFBartForConditionalGeneration"),Ket.forEach(t),KQo=r(P3e," (BART model)"),P3e.forEach(t),ZQo=i(U),g6=n(U,"LI",{});var $3e=s(g6);tfe=n($3e,"STRONG",{});var Zet=s(tfe);eHo=r(Zet,"bert"),Zet.forEach(t),oHo=r($3e," \u2014 "),FD=n($3e,"A",{href:!0});var eot=s(FD);rHo=r(eot,"TFBertForPreTraining"),eot.forEach(t),tHo=r($3e," (BERT model)"),$3e.forEach(t),aHo=i(U),h6=n(U,"LI",{});var I3e=s(h6);afe=n(I3e,"STRONG",{});var oot=s(afe);nHo=r(oot,"camembert"),oot.forEach(t),sHo=r(I3e," \u2014 "),CD=n(I3e,"A",{href:!0});var rot=s(CD);lHo=r(rot,"TFCamembertForMaskedLM"),rot.forEach(t),iHo=r(I3e," (CamemBERT model)"),I3e.forEach(t),dHo=i(U),p6=n(U,"LI",{});var j3e=s(p6);nfe=n(j3e,"STRONG",{});var tot=s(nfe);cHo=r(tot,"ctrl"),tot.forEach(t),fHo=r(j3e," \u2014 "),MD=n(j3e,"A",{href:!0});var aot=s(MD);mHo=r(aot,"TFCTRLLMHeadModel"),aot.forEach(t),gHo=r(j3e," (CTRL model)"),j3e.forEach(t),hHo=i(U),_6=n(U,"LI",{});var N3e=s(_6);sfe=n(N3e,"STRONG",{});var not=s(sfe);pHo=r(not,"distilbert"),not.forEach(t),_Ho=r(N3e," \u2014 "),ED=n(N3e,"A",{href:!0});var sot=s(ED);uHo=r(sot,"TFDistilBertForMaskedLM"),sot.forEach(t),bHo=r(N3e," (DistilBERT model)"),N3e.forEach(t),vHo=i(U),u6=n(U,"LI",{});var D3e=s(u6);lfe=n(D3e,"STRONG",{});var lot=s(lfe);THo=r(lot,"electra"),lot.forEach(t),FHo=r(D3e," \u2014 "),yD=n(D3e,"A",{href:!0});var iot=s(yD);CHo=r(iot,"TFElectraForPreTraining"),iot.forEach(t),MHo=r(D3e," (ELECTRA model)"),D3e.forEach(t),EHo=i(U),b6=n(U,"LI",{});var q3e=s(b6);ife=n(q3e,"STRONG",{});var dot=s(ife);yHo=r(dot,"flaubert"),dot.forEach(t),wHo=r(q3e," \u2014 "),wD=n(q3e,"A",{href:!0});var cot=s(wD);AHo=r(cot,"TFFlaubertWithLMHeadModel"),cot.forEach(t),LHo=r(q3e," (FlauBERT model)"),q3e.forEach(t),BHo=i(U),v6=n(U,"LI",{});var G3e=s(v6);dfe=n(G3e,"STRONG",{});var fot=s(dfe);kHo=r(fot,"funnel"),fot.forEach(t),xHo=r(G3e," \u2014 "),AD=n(G3e,"A",{href:!0});var mot=s(AD);RHo=r(mot,"TFFunnelForPreTraining"),mot.forEach(t),SHo=r(G3e," (Funnel Transformer model)"),G3e.forEach(t),PHo=i(U),T6=n(U,"LI",{});var O3e=s(T6);cfe=n(O3e,"STRONG",{});var got=s(cfe);$Ho=r(got,"gpt2"),got.forEach(t),IHo=r(O3e," \u2014 "),LD=n(O3e,"A",{href:!0});var hot=s(LD);jHo=r(hot,"TFGPT2LMHeadModel"),hot.forEach(t),NHo=r(O3e," (OpenAI GPT-2 model)"),O3e.forEach(t),DHo=i(U),F6=n(U,"LI",{});var X3e=s(F6);ffe=n(X3e,"STRONG",{});var pot=s(ffe);qHo=r(pot,"layoutlm"),pot.forEach(t),GHo=r(X3e," \u2014 "),BD=n(X3e,"A",{href:!0});var _ot=s(BD);OHo=r(_ot,"TFLayoutLMForMaskedLM"),_ot.forEach(t),XHo=r(X3e," (LayoutLM model)"),X3e.forEach(t),zHo=i(U),C6=n(U,"LI",{});var z3e=s(C6);mfe=n(z3e,"STRONG",{});var uot=s(mfe);VHo=r(uot,"lxmert"),uot.forEach(t),WHo=r(z3e," \u2014 "),kD=n(z3e,"A",{href:!0});var bot=s(kD);QHo=r(bot,"TFLxmertForPreTraining"),bot.forEach(t),HHo=r(z3e," (LXMERT model)"),z3e.forEach(t),UHo=i(U),M6=n(U,"LI",{});var V3e=s(M6);gfe=n(V3e,"STRONG",{});var vot=s(gfe);JHo=r(vot,"mobilebert"),vot.forEach(t),YHo=r(V3e," \u2014 "),xD=n(V3e,"A",{href:!0});var Tot=s(xD);KHo=r(Tot,"TFMobileBertForPreTraining"),Tot.forEach(t),ZHo=r(V3e," (MobileBERT model)"),V3e.forEach(t),eUo=i(U),E6=n(U,"LI",{});var W3e=s(E6);hfe=n(W3e,"STRONG",{});var Fot=s(hfe);oUo=r(Fot,"mpnet"),Fot.forEach(t),rUo=r(W3e," \u2014 "),RD=n(W3e,"A",{href:!0});var Cot=s(RD);tUo=r(Cot,"TFMPNetForMaskedLM"),Cot.forEach(t),aUo=r(W3e," (MPNet model)"),W3e.forEach(t),nUo=i(U),y6=n(U,"LI",{});var Q3e=s(y6);pfe=n(Q3e,"STRONG",{});var Mot=s(pfe);sUo=r(Mot,"openai-gpt"),Mot.forEach(t),lUo=r(Q3e," \u2014 "),SD=n(Q3e,"A",{href:!0});var Eot=s(SD);iUo=r(Eot,"TFOpenAIGPTLMHeadModel"),Eot.forEach(t),dUo=r(Q3e," (OpenAI GPT model)"),Q3e.forEach(t),cUo=i(U),w6=n(U,"LI",{});var H3e=s(w6);_fe=n(H3e,"STRONG",{});var yot=s(_fe);fUo=r(yot,"roberta"),yot.forEach(t),mUo=r(H3e," \u2014 "),PD=n(H3e,"A",{href:!0});var wot=s(PD);gUo=r(wot,"TFRobertaForMaskedLM"),wot.forEach(t),hUo=r(H3e," (RoBERTa model)"),H3e.forEach(t),pUo=i(U),A6=n(U,"LI",{});var U3e=s(A6);ufe=n(U3e,"STRONG",{});var Aot=s(ufe);_Uo=r(Aot,"t5"),Aot.forEach(t),uUo=r(U3e," \u2014 "),$D=n(U3e,"A",{href:!0});var Lot=s($D);bUo=r(Lot,"TFT5ForConditionalGeneration"),Lot.forEach(t),vUo=r(U3e," (T5 model)"),U3e.forEach(t),TUo=i(U),L6=n(U,"LI",{});var J3e=s(L6);bfe=n(J3e,"STRONG",{});var Bot=s(bfe);FUo=r(Bot,"tapas"),Bot.forEach(t),CUo=r(J3e," \u2014 "),ID=n(J3e,"A",{href:!0});var kot=s(ID);MUo=r(kot,"TFTapasForMaskedLM"),kot.forEach(t),EUo=r(J3e," (TAPAS model)"),J3e.forEach(t),yUo=i(U),B6=n(U,"LI",{});var Y3e=s(B6);vfe=n(Y3e,"STRONG",{});var xot=s(vfe);wUo=r(xot,"transfo-xl"),xot.forEach(t),AUo=r(Y3e," \u2014 "),jD=n(Y3e,"A",{href:!0});var Rot=s(jD);LUo=r(Rot,"TFTransfoXLLMHeadModel"),Rot.forEach(t),BUo=r(Y3e," (Transformer-XL model)"),Y3e.forEach(t),kUo=i(U),k6=n(U,"LI",{});var K3e=s(k6);Tfe=n(K3e,"STRONG",{});var Sot=s(Tfe);xUo=r(Sot,"xlm"),Sot.forEach(t),RUo=r(K3e," \u2014 "),ND=n(K3e,"A",{href:!0});var Pot=s(ND);SUo=r(Pot,"TFXLMWithLMHeadModel"),Pot.forEach(t),PUo=r(K3e," (XLM model)"),K3e.forEach(t),$Uo=i(U),x6=n(U,"LI",{});var Z3e=s(x6);Ffe=n(Z3e,"STRONG",{});var $ot=s(Ffe);IUo=r($ot,"xlm-roberta"),$ot.forEach(t),jUo=r(Z3e," \u2014 "),DD=n(Z3e,"A",{href:!0});var Iot=s(DD);NUo=r(Iot,"TFXLMRobertaForMaskedLM"),Iot.forEach(t),DUo=r(Z3e," (XLM-RoBERTa model)"),Z3e.forEach(t),qUo=i(U),R6=n(U,"LI",{});var eye=s(R6);Cfe=n(eye,"STRONG",{});var jot=s(Cfe);GUo=r(jot,"xlnet"),jot.forEach(t),OUo=r(eye," \u2014 "),qD=n(eye,"A",{href:!0});var Not=s(qD);XUo=r(Not,"TFXLNetLMHeadModel"),Not.forEach(t),zUo=r(eye," (XLNet model)"),eye.forEach(t),U.forEach(t),VUo=i(fa),Mfe=n(fa,"P",{});var Dot=s(Mfe);WUo=r(Dot,"Examples:"),Dot.forEach(t),QUo=i(fa),m(O3.$$.fragment,fa),fa.forEach(t),xl.forEach(t),c8e=i(d),lc=n(d,"H2",{class:!0});var vke=s(lc);S6=n(vke,"A",{id:!0,class:!0,href:!0});var qot=s(S6);Efe=n(qot,"SPAN",{});var Got=s(Efe);m(X3.$$.fragment,Got),Got.forEach(t),qot.forEach(t),HUo=i(vke),yfe=n(vke,"SPAN",{});var Oot=s(yfe);UUo=r(Oot,"TFAutoModelForCausalLM"),Oot.forEach(t),vke.forEach(t),f8e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(z3.$$.fragment,Sl),JUo=i(Sl),ic=n(Sl,"P",{});var Xz=s(ic);YUo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=n(Xz,"CODE",{});var Xot=s(wfe);KUo=r(Xot,"from_pretrained()"),Xot.forEach(t),ZUo=r(Xz,"class method or the "),Afe=n(Xz,"CODE",{});var zot=s(Afe);eJo=r(zot,"from_config()"),zot.forEach(t),oJo=r(Xz,`class
method.`),Xz.forEach(t),rJo=i(Sl),V3=n(Sl,"P",{});var Tke=s(V3);tJo=r(Tke,"This class cannot be instantiated directly using "),Lfe=n(Tke,"CODE",{});var Vot=s(Lfe);aJo=r(Vot,"__init__()"),Vot.forEach(t),nJo=r(Tke," (throws an error)."),Tke.forEach(t),sJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(W3.$$.fragment,Pl),lJo=i(Pl),Bfe=n(Pl,"P",{});var Wot=s(Bfe);iJo=r(Wot,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Wot.forEach(t),dJo=i(Pl),dc=n(Pl,"P",{});var zz=s(dc);cJo=r(zz,`Note:
Loading a model from its configuration file does `),kfe=n(zz,"STRONG",{});var Qot=s(kfe);fJo=r(Qot,"not"),Qot.forEach(t),mJo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=n(zz,"CODE",{});var Hot=s(xfe);gJo=r(Hot,"from_pretrained()"),Hot.forEach(t),hJo=r(zz,"to load the model weights."),zz.forEach(t),pJo=i(Pl),Rfe=n(Pl,"P",{});var Uot=s(Rfe);_Jo=r(Uot,"Examples:"),Uot.forEach(t),uJo=i(Pl),m(Q3.$$.fragment,Pl),Pl.forEach(t),bJo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(H3.$$.fragment,ma),vJo=i(ma),Sfe=n(ma,"P",{});var Jot=s(Sfe);TJo=r(Jot,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Jot.forEach(t),FJo=i(ma),cn=n(ma,"P",{});var xC=s(cn);CJo=r(xC,"The model class to instantiate is selected based on the "),Pfe=n(xC,"CODE",{});var Yot=s(Pfe);MJo=r(Yot,"model_type"),Yot.forEach(t),EJo=r(xC,` property of the config object (either
passed as an argument or loaded from `),$fe=n(xC,"CODE",{});var Kot=s($fe);yJo=r(Kot,"pretrained_model_name_or_path"),Kot.forEach(t),wJo=r(xC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=n(xC,"CODE",{});var Zot=s(Ife);AJo=r(Zot,"pretrained_model_name_or_path"),Zot.forEach(t),LJo=r(xC,":"),xC.forEach(t),BJo=i(ma),he=n(ma,"UL",{});var Me=s(he);P6=n(Me,"LI",{});var oye=s(P6);jfe=n(oye,"STRONG",{});var ert=s(jfe);kJo=r(ert,"bert"),ert.forEach(t),xJo=r(oye," \u2014 "),GD=n(oye,"A",{href:!0});var ort=s(GD);RJo=r(ort,"TFBertLMHeadModel"),ort.forEach(t),SJo=r(oye," (BERT model)"),oye.forEach(t),PJo=i(Me),$6=n(Me,"LI",{});var rye=s($6);Nfe=n(rye,"STRONG",{});var rrt=s(Nfe);$Jo=r(rrt,"ctrl"),rrt.forEach(t),IJo=r(rye," \u2014 "),OD=n(rye,"A",{href:!0});var trt=s(OD);jJo=r(trt,"TFCTRLLMHeadModel"),trt.forEach(t),NJo=r(rye," (CTRL model)"),rye.forEach(t),DJo=i(Me),I6=n(Me,"LI",{});var tye=s(I6);Dfe=n(tye,"STRONG",{});var art=s(Dfe);qJo=r(art,"gpt2"),art.forEach(t),GJo=r(tye," \u2014 "),XD=n(tye,"A",{href:!0});var nrt=s(XD);OJo=r(nrt,"TFGPT2LMHeadModel"),nrt.forEach(t),XJo=r(tye," (OpenAI GPT-2 model)"),tye.forEach(t),zJo=i(Me),j6=n(Me,"LI",{});var aye=s(j6);qfe=n(aye,"STRONG",{});var srt=s(qfe);VJo=r(srt,"openai-gpt"),srt.forEach(t),WJo=r(aye," \u2014 "),zD=n(aye,"A",{href:!0});var lrt=s(zD);QJo=r(lrt,"TFOpenAIGPTLMHeadModel"),lrt.forEach(t),HJo=r(aye," (OpenAI GPT model)"),aye.forEach(t),UJo=i(Me),N6=n(Me,"LI",{});var nye=s(N6);Gfe=n(nye,"STRONG",{});var irt=s(Gfe);JJo=r(irt,"rembert"),irt.forEach(t),YJo=r(nye," \u2014 "),VD=n(nye,"A",{href:!0});var drt=s(VD);KJo=r(drt,"TFRemBertForCausalLM"),drt.forEach(t),ZJo=r(nye," (RemBERT model)"),nye.forEach(t),eYo=i(Me),D6=n(Me,"LI",{});var sye=s(D6);Ofe=n(sye,"STRONG",{});var crt=s(Ofe);oYo=r(crt,"roberta"),crt.forEach(t),rYo=r(sye," \u2014 "),WD=n(sye,"A",{href:!0});var frt=s(WD);tYo=r(frt,"TFRobertaForCausalLM"),frt.forEach(t),aYo=r(sye," (RoBERTa model)"),sye.forEach(t),nYo=i(Me),q6=n(Me,"LI",{});var lye=s(q6);Xfe=n(lye,"STRONG",{});var mrt=s(Xfe);sYo=r(mrt,"roformer"),mrt.forEach(t),lYo=r(lye," \u2014 "),QD=n(lye,"A",{href:!0});var grt=s(QD);iYo=r(grt,"TFRoFormerForCausalLM"),grt.forEach(t),dYo=r(lye," (RoFormer model)"),lye.forEach(t),cYo=i(Me),G6=n(Me,"LI",{});var iye=s(G6);zfe=n(iye,"STRONG",{});var hrt=s(zfe);fYo=r(hrt,"transfo-xl"),hrt.forEach(t),mYo=r(iye," \u2014 "),HD=n(iye,"A",{href:!0});var prt=s(HD);gYo=r(prt,"TFTransfoXLLMHeadModel"),prt.forEach(t),hYo=r(iye," (Transformer-XL model)"),iye.forEach(t),pYo=i(Me),O6=n(Me,"LI",{});var dye=s(O6);Vfe=n(dye,"STRONG",{});var _rt=s(Vfe);_Yo=r(_rt,"xlm"),_rt.forEach(t),uYo=r(dye," \u2014 "),UD=n(dye,"A",{href:!0});var urt=s(UD);bYo=r(urt,"TFXLMWithLMHeadModel"),urt.forEach(t),vYo=r(dye," (XLM model)"),dye.forEach(t),TYo=i(Me),X6=n(Me,"LI",{});var cye=s(X6);Wfe=n(cye,"STRONG",{});var brt=s(Wfe);FYo=r(brt,"xlnet"),brt.forEach(t),CYo=r(cye," \u2014 "),JD=n(cye,"A",{href:!0});var vrt=s(JD);MYo=r(vrt,"TFXLNetLMHeadModel"),vrt.forEach(t),EYo=r(cye," (XLNet model)"),cye.forEach(t),Me.forEach(t),yYo=i(ma),Qfe=n(ma,"P",{});var Trt=s(Qfe);wYo=r(Trt,"Examples:"),Trt.forEach(t),AYo=i(ma),m(U3.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),m8e=i(d),cc=n(d,"H2",{class:!0});var Fke=s(cc);z6=n(Fke,"A",{id:!0,class:!0,href:!0});var Frt=s(z6);Hfe=n(Frt,"SPAN",{});var Crt=s(Hfe);m(J3.$$.fragment,Crt),Crt.forEach(t),Frt.forEach(t),LYo=i(Fke),Ufe=n(Fke,"SPAN",{});var Mrt=s(Ufe);BYo=r(Mrt,"TFAutoModelForImageClassification"),Mrt.forEach(t),Fke.forEach(t),g8e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(Y3.$$.fragment,$l),kYo=i($l),fc=n($l,"P",{});var Vz=s(fc);xYo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=n(Vz,"CODE",{});var Ert=s(Jfe);RYo=r(Ert,"from_pretrained()"),Ert.forEach(t),SYo=r(Vz,"class method or the "),Yfe=n(Vz,"CODE",{});var yrt=s(Yfe);PYo=r(yrt,"from_config()"),yrt.forEach(t),$Yo=r(Vz,`class
method.`),Vz.forEach(t),IYo=i($l),K3=n($l,"P",{});var Cke=s(K3);jYo=r(Cke,"This class cannot be instantiated directly using "),Kfe=n(Cke,"CODE",{});var wrt=s(Kfe);NYo=r(wrt,"__init__()"),wrt.forEach(t),DYo=r(Cke," (throws an error)."),Cke.forEach(t),qYo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(Z3.$$.fragment,Il),GYo=i(Il),Zfe=n(Il,"P",{});var Art=s(Zfe);OYo=r(Art,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Art.forEach(t),XYo=i(Il),mc=n(Il,"P",{});var Wz=s(mc);zYo=r(Wz,`Note:
Loading a model from its configuration file does `),eme=n(Wz,"STRONG",{});var Lrt=s(eme);VYo=r(Lrt,"not"),Lrt.forEach(t),WYo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=n(Wz,"CODE",{});var Brt=s(ome);QYo=r(Brt,"from_pretrained()"),Brt.forEach(t),HYo=r(Wz,"to load the model weights."),Wz.forEach(t),UYo=i(Il),rme=n(Il,"P",{});var krt=s(rme);JYo=r(krt,"Examples:"),krt.forEach(t),YYo=i(Il),m(ey.$$.fragment,Il),Il.forEach(t),KYo=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(oy.$$.fragment,ga),ZYo=i(ga),tme=n(ga,"P",{});var xrt=s(tme);eKo=r(xrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),xrt.forEach(t),oKo=i(ga),fn=n(ga,"P",{});var RC=s(fn);rKo=r(RC,"The model class to instantiate is selected based on the "),ame=n(RC,"CODE",{});var Rrt=s(ame);tKo=r(Rrt,"model_type"),Rrt.forEach(t),aKo=r(RC,` property of the config object (either
passed as an argument or loaded from `),nme=n(RC,"CODE",{});var Srt=s(nme);nKo=r(Srt,"pretrained_model_name_or_path"),Srt.forEach(t),sKo=r(RC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=n(RC,"CODE",{});var Prt=s(sme);lKo=r(Prt,"pretrained_model_name_or_path"),Prt.forEach(t),iKo=r(RC,":"),RC.forEach(t),dKo=i(ga),lme=n(ga,"UL",{});var $rt=s(lme);V6=n($rt,"LI",{});var fye=s(V6);ime=n(fye,"STRONG",{});var Irt=s(ime);cKo=r(Irt,"vit"),Irt.forEach(t),fKo=r(fye," \u2014 "),YD=n(fye,"A",{href:!0});var jrt=s(YD);mKo=r(jrt,"TFViTForImageClassification"),jrt.forEach(t),gKo=r(fye," (ViT model)"),fye.forEach(t),$rt.forEach(t),hKo=i(ga),dme=n(ga,"P",{});var Nrt=s(dme);pKo=r(Nrt,"Examples:"),Nrt.forEach(t),_Ko=i(ga),m(ry.$$.fragment,ga),ga.forEach(t),$l.forEach(t),h8e=i(d),gc=n(d,"H2",{class:!0});var Mke=s(gc);W6=n(Mke,"A",{id:!0,class:!0,href:!0});var Drt=s(W6);cme=n(Drt,"SPAN",{});var qrt=s(cme);m(ty.$$.fragment,qrt),qrt.forEach(t),Drt.forEach(t),uKo=i(Mke),fme=n(Mke,"SPAN",{});var Grt=s(fme);bKo=r(Grt,"TFAutoModelForMaskedLM"),Grt.forEach(t),Mke.forEach(t),p8e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(ay.$$.fragment,jl),vKo=i(jl),hc=n(jl,"P",{});var Qz=s(hc);TKo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=n(Qz,"CODE",{});var Ort=s(mme);FKo=r(Ort,"from_pretrained()"),Ort.forEach(t),CKo=r(Qz,"class method or the "),gme=n(Qz,"CODE",{});var Xrt=s(gme);MKo=r(Xrt,"from_config()"),Xrt.forEach(t),EKo=r(Qz,`class
method.`),Qz.forEach(t),yKo=i(jl),ny=n(jl,"P",{});var Eke=s(ny);wKo=r(Eke,"This class cannot be instantiated directly using "),hme=n(Eke,"CODE",{});var zrt=s(hme);AKo=r(zrt,"__init__()"),zrt.forEach(t),LKo=r(Eke," (throws an error)."),Eke.forEach(t),BKo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(sy.$$.fragment,Nl),kKo=i(Nl),pme=n(Nl,"P",{});var Vrt=s(pme);xKo=r(Vrt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Vrt.forEach(t),RKo=i(Nl),pc=n(Nl,"P",{});var Hz=s(pc);SKo=r(Hz,`Note:
Loading a model from its configuration file does `),_me=n(Hz,"STRONG",{});var Wrt=s(_me);PKo=r(Wrt,"not"),Wrt.forEach(t),$Ko=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=n(Hz,"CODE",{});var Qrt=s(ume);IKo=r(Qrt,"from_pretrained()"),Qrt.forEach(t),jKo=r(Hz,"to load the model weights."),Hz.forEach(t),NKo=i(Nl),bme=n(Nl,"P",{});var Hrt=s(bme);DKo=r(Hrt,"Examples:"),Hrt.forEach(t),qKo=i(Nl),m(ly.$$.fragment,Nl),Nl.forEach(t),GKo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(iy.$$.fragment,ha),OKo=i(ha),vme=n(ha,"P",{});var Urt=s(vme);XKo=r(Urt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Urt.forEach(t),zKo=i(ha),mn=n(ha,"P",{});var SC=s(mn);VKo=r(SC,"The model class to instantiate is selected based on the "),Tme=n(SC,"CODE",{});var Jrt=s(Tme);WKo=r(Jrt,"model_type"),Jrt.forEach(t),QKo=r(SC,` property of the config object (either
passed as an argument or loaded from `),Fme=n(SC,"CODE",{});var Yrt=s(Fme);HKo=r(Yrt,"pretrained_model_name_or_path"),Yrt.forEach(t),UKo=r(SC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=n(SC,"CODE",{});var Krt=s(Cme);JKo=r(Krt,"pretrained_model_name_or_path"),Krt.forEach(t),YKo=r(SC,":"),SC.forEach(t),KKo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);Q6=n(ee,"LI",{});var mye=s(Q6);Mme=n(mye,"STRONG",{});var Zrt=s(Mme);ZKo=r(Zrt,"albert"),Zrt.forEach(t),eZo=r(mye," \u2014 "),KD=n(mye,"A",{href:!0});var ett=s(KD);oZo=r(ett,"TFAlbertForMaskedLM"),ett.forEach(t),rZo=r(mye," (ALBERT model)"),mye.forEach(t),tZo=i(ee),H6=n(ee,"LI",{});var gye=s(H6);Eme=n(gye,"STRONG",{});var ott=s(Eme);aZo=r(ott,"bert"),ott.forEach(t),nZo=r(gye," \u2014 "),ZD=n(gye,"A",{href:!0});var rtt=s(ZD);sZo=r(rtt,"TFBertForMaskedLM"),rtt.forEach(t),lZo=r(gye," (BERT model)"),gye.forEach(t),iZo=i(ee),U6=n(ee,"LI",{});var hye=s(U6);yme=n(hye,"STRONG",{});var ttt=s(yme);dZo=r(ttt,"camembert"),ttt.forEach(t),cZo=r(hye," \u2014 "),eq=n(hye,"A",{href:!0});var att=s(eq);fZo=r(att,"TFCamembertForMaskedLM"),att.forEach(t),mZo=r(hye," (CamemBERT model)"),hye.forEach(t),gZo=i(ee),J6=n(ee,"LI",{});var pye=s(J6);wme=n(pye,"STRONG",{});var ntt=s(wme);hZo=r(ntt,"convbert"),ntt.forEach(t),pZo=r(pye," \u2014 "),oq=n(pye,"A",{href:!0});var stt=s(oq);_Zo=r(stt,"TFConvBertForMaskedLM"),stt.forEach(t),uZo=r(pye," (ConvBERT model)"),pye.forEach(t),bZo=i(ee),Y6=n(ee,"LI",{});var _ye=s(Y6);Ame=n(_ye,"STRONG",{});var ltt=s(Ame);vZo=r(ltt,"deberta"),ltt.forEach(t),TZo=r(_ye," \u2014 "),rq=n(_ye,"A",{href:!0});var itt=s(rq);FZo=r(itt,"TFDebertaForMaskedLM"),itt.forEach(t),CZo=r(_ye," (DeBERTa model)"),_ye.forEach(t),MZo=i(ee),K6=n(ee,"LI",{});var uye=s(K6);Lme=n(uye,"STRONG",{});var dtt=s(Lme);EZo=r(dtt,"deberta-v2"),dtt.forEach(t),yZo=r(uye," \u2014 "),tq=n(uye,"A",{href:!0});var ctt=s(tq);wZo=r(ctt,"TFDebertaV2ForMaskedLM"),ctt.forEach(t),AZo=r(uye," (DeBERTa-v2 model)"),uye.forEach(t),LZo=i(ee),Z6=n(ee,"LI",{});var bye=s(Z6);Bme=n(bye,"STRONG",{});var ftt=s(Bme);BZo=r(ftt,"distilbert"),ftt.forEach(t),kZo=r(bye," \u2014 "),aq=n(bye,"A",{href:!0});var mtt=s(aq);xZo=r(mtt,"TFDistilBertForMaskedLM"),mtt.forEach(t),RZo=r(bye," (DistilBERT model)"),bye.forEach(t),SZo=i(ee),eT=n(ee,"LI",{});var vye=s(eT);kme=n(vye,"STRONG",{});var gtt=s(kme);PZo=r(gtt,"electra"),gtt.forEach(t),$Zo=r(vye," \u2014 "),nq=n(vye,"A",{href:!0});var htt=s(nq);IZo=r(htt,"TFElectraForMaskedLM"),htt.forEach(t),jZo=r(vye," (ELECTRA model)"),vye.forEach(t),NZo=i(ee),oT=n(ee,"LI",{});var Tye=s(oT);xme=n(Tye,"STRONG",{});var ptt=s(xme);DZo=r(ptt,"flaubert"),ptt.forEach(t),qZo=r(Tye," \u2014 "),sq=n(Tye,"A",{href:!0});var _tt=s(sq);GZo=r(_tt,"TFFlaubertWithLMHeadModel"),_tt.forEach(t),OZo=r(Tye," (FlauBERT model)"),Tye.forEach(t),XZo=i(ee),rT=n(ee,"LI",{});var Fye=s(rT);Rme=n(Fye,"STRONG",{});var utt=s(Rme);zZo=r(utt,"funnel"),utt.forEach(t),VZo=r(Fye," \u2014 "),lq=n(Fye,"A",{href:!0});var btt=s(lq);WZo=r(btt,"TFFunnelForMaskedLM"),btt.forEach(t),QZo=r(Fye," (Funnel Transformer model)"),Fye.forEach(t),HZo=i(ee),tT=n(ee,"LI",{});var Cye=s(tT);Sme=n(Cye,"STRONG",{});var vtt=s(Sme);UZo=r(vtt,"layoutlm"),vtt.forEach(t),JZo=r(Cye," \u2014 "),iq=n(Cye,"A",{href:!0});var Ttt=s(iq);YZo=r(Ttt,"TFLayoutLMForMaskedLM"),Ttt.forEach(t),KZo=r(Cye," (LayoutLM model)"),Cye.forEach(t),ZZo=i(ee),aT=n(ee,"LI",{});var Mye=s(aT);Pme=n(Mye,"STRONG",{});var Ftt=s(Pme);eer=r(Ftt,"longformer"),Ftt.forEach(t),oer=r(Mye," \u2014 "),dq=n(Mye,"A",{href:!0});var Ctt=s(dq);rer=r(Ctt,"TFLongformerForMaskedLM"),Ctt.forEach(t),ter=r(Mye," (Longformer model)"),Mye.forEach(t),aer=i(ee),nT=n(ee,"LI",{});var Eye=s(nT);$me=n(Eye,"STRONG",{});var Mtt=s($me);ner=r(Mtt,"mobilebert"),Mtt.forEach(t),ser=r(Eye," \u2014 "),cq=n(Eye,"A",{href:!0});var Ett=s(cq);ler=r(Ett,"TFMobileBertForMaskedLM"),Ett.forEach(t),ier=r(Eye," (MobileBERT model)"),Eye.forEach(t),der=i(ee),sT=n(ee,"LI",{});var yye=s(sT);Ime=n(yye,"STRONG",{});var ytt=s(Ime);cer=r(ytt,"mpnet"),ytt.forEach(t),fer=r(yye," \u2014 "),fq=n(yye,"A",{href:!0});var wtt=s(fq);mer=r(wtt,"TFMPNetForMaskedLM"),wtt.forEach(t),ger=r(yye," (MPNet model)"),yye.forEach(t),her=i(ee),lT=n(ee,"LI",{});var wye=s(lT);jme=n(wye,"STRONG",{});var Att=s(jme);per=r(Att,"rembert"),Att.forEach(t),_er=r(wye," \u2014 "),mq=n(wye,"A",{href:!0});var Ltt=s(mq);uer=r(Ltt,"TFRemBertForMaskedLM"),Ltt.forEach(t),ber=r(wye," (RemBERT model)"),wye.forEach(t),ver=i(ee),iT=n(ee,"LI",{});var Aye=s(iT);Nme=n(Aye,"STRONG",{});var Btt=s(Nme);Ter=r(Btt,"roberta"),Btt.forEach(t),Fer=r(Aye," \u2014 "),gq=n(Aye,"A",{href:!0});var ktt=s(gq);Cer=r(ktt,"TFRobertaForMaskedLM"),ktt.forEach(t),Mer=r(Aye," (RoBERTa model)"),Aye.forEach(t),Eer=i(ee),dT=n(ee,"LI",{});var Lye=s(dT);Dme=n(Lye,"STRONG",{});var xtt=s(Dme);yer=r(xtt,"roformer"),xtt.forEach(t),wer=r(Lye," \u2014 "),hq=n(Lye,"A",{href:!0});var Rtt=s(hq);Aer=r(Rtt,"TFRoFormerForMaskedLM"),Rtt.forEach(t),Ler=r(Lye," (RoFormer model)"),Lye.forEach(t),Ber=i(ee),cT=n(ee,"LI",{});var Bye=s(cT);qme=n(Bye,"STRONG",{});var Stt=s(qme);ker=r(Stt,"tapas"),Stt.forEach(t),xer=r(Bye," \u2014 "),pq=n(Bye,"A",{href:!0});var Ptt=s(pq);Rer=r(Ptt,"TFTapasForMaskedLM"),Ptt.forEach(t),Ser=r(Bye," (TAPAS model)"),Bye.forEach(t),Per=i(ee),fT=n(ee,"LI",{});var kye=s(fT);Gme=n(kye,"STRONG",{});var $tt=s(Gme);$er=r($tt,"xlm"),$tt.forEach(t),Ier=r(kye," \u2014 "),_q=n(kye,"A",{href:!0});var Itt=s(_q);jer=r(Itt,"TFXLMWithLMHeadModel"),Itt.forEach(t),Ner=r(kye," (XLM model)"),kye.forEach(t),Der=i(ee),mT=n(ee,"LI",{});var xye=s(mT);Ome=n(xye,"STRONG",{});var jtt=s(Ome);qer=r(jtt,"xlm-roberta"),jtt.forEach(t),Ger=r(xye," \u2014 "),uq=n(xye,"A",{href:!0});var Ntt=s(uq);Oer=r(Ntt,"TFXLMRobertaForMaskedLM"),Ntt.forEach(t),Xer=r(xye," (XLM-RoBERTa model)"),xye.forEach(t),ee.forEach(t),zer=i(ha),Xme=n(ha,"P",{});var Dtt=s(Xme);Ver=r(Dtt,"Examples:"),Dtt.forEach(t),Wer=i(ha),m(dy.$$.fragment,ha),ha.forEach(t),jl.forEach(t),_8e=i(d),_c=n(d,"H2",{class:!0});var yke=s(_c);gT=n(yke,"A",{id:!0,class:!0,href:!0});var qtt=s(gT);zme=n(qtt,"SPAN",{});var Gtt=s(zme);m(cy.$$.fragment,Gtt),Gtt.forEach(t),qtt.forEach(t),Qer=i(yke),Vme=n(yke,"SPAN",{});var Ott=s(Vme);Her=r(Ott,"TFAutoModelForSeq2SeqLM"),Ott.forEach(t),yke.forEach(t),u8e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(fy.$$.fragment,Dl),Uer=i(Dl),uc=n(Dl,"P",{});var Uz=s(uc);Jer=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=n(Uz,"CODE",{});var Xtt=s(Wme);Yer=r(Xtt,"from_pretrained()"),Xtt.forEach(t),Ker=r(Uz,"class method or the "),Qme=n(Uz,"CODE",{});var ztt=s(Qme);Zer=r(ztt,"from_config()"),ztt.forEach(t),eor=r(Uz,`class
method.`),Uz.forEach(t),oor=i(Dl),my=n(Dl,"P",{});var wke=s(my);ror=r(wke,"This class cannot be instantiated directly using "),Hme=n(wke,"CODE",{});var Vtt=s(Hme);tor=r(Vtt,"__init__()"),Vtt.forEach(t),aor=r(wke," (throws an error)."),wke.forEach(t),nor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(gy.$$.fragment,ql),sor=i(ql),Ume=n(ql,"P",{});var Wtt=s(Ume);lor=r(Wtt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Wtt.forEach(t),ior=i(ql),bc=n(ql,"P",{});var Jz=s(bc);dor=r(Jz,`Note:
Loading a model from its configuration file does `),Jme=n(Jz,"STRONG",{});var Qtt=s(Jme);cor=r(Qtt,"not"),Qtt.forEach(t),mor=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=n(Jz,"CODE",{});var Htt=s(Yme);gor=r(Htt,"from_pretrained()"),Htt.forEach(t),hor=r(Jz,"to load the model weights."),Jz.forEach(t),por=i(ql),Kme=n(ql,"P",{});var Utt=s(Kme);_or=r(Utt,"Examples:"),Utt.forEach(t),uor=i(ql),m(hy.$$.fragment,ql),ql.forEach(t),bor=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(py.$$.fragment,pa),vor=i(pa),Zme=n(pa,"P",{});var Jtt=s(Zme);Tor=r(Jtt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Jtt.forEach(t),For=i(pa),gn=n(pa,"P",{});var PC=s(gn);Cor=r(PC,"The model class to instantiate is selected based on the "),ege=n(PC,"CODE",{});var Ytt=s(ege);Mor=r(Ytt,"model_type"),Ytt.forEach(t),Eor=r(PC,` property of the config object (either
passed as an argument or loaded from `),oge=n(PC,"CODE",{});var Ktt=s(oge);yor=r(Ktt,"pretrained_model_name_or_path"),Ktt.forEach(t),wor=r(PC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=n(PC,"CODE",{});var Ztt=s(rge);Aor=r(Ztt,"pretrained_model_name_or_path"),Ztt.forEach(t),Lor=r(PC,":"),PC.forEach(t),Bor=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);hT=n(Ee,"LI",{});var Rye=s(hT);tge=n(Rye,"STRONG",{});var eat=s(tge);kor=r(eat,"bart"),eat.forEach(t),xor=r(Rye," \u2014 "),bq=n(Rye,"A",{href:!0});var oat=s(bq);Ror=r(oat,"TFBartForConditionalGeneration"),oat.forEach(t),Sor=r(Rye," (BART model)"),Rye.forEach(t),Por=i(Ee),pT=n(Ee,"LI",{});var Sye=s(pT);age=n(Sye,"STRONG",{});var rat=s(age);$or=r(rat,"blenderbot"),rat.forEach(t),Ior=r(Sye," \u2014 "),vq=n(Sye,"A",{href:!0});var tat=s(vq);jor=r(tat,"TFBlenderbotForConditionalGeneration"),tat.forEach(t),Nor=r(Sye," (Blenderbot model)"),Sye.forEach(t),Dor=i(Ee),_T=n(Ee,"LI",{});var Pye=s(_T);nge=n(Pye,"STRONG",{});var aat=s(nge);qor=r(aat,"blenderbot-small"),aat.forEach(t),Gor=r(Pye," \u2014 "),Tq=n(Pye,"A",{href:!0});var nat=s(Tq);Oor=r(nat,"TFBlenderbotSmallForConditionalGeneration"),nat.forEach(t),Xor=r(Pye," (BlenderbotSmall model)"),Pye.forEach(t),zor=i(Ee),uT=n(Ee,"LI",{});var $ye=s(uT);sge=n($ye,"STRONG",{});var sat=s(sge);Vor=r(sat,"encoder-decoder"),sat.forEach(t),Wor=r($ye," \u2014 "),Fq=n($ye,"A",{href:!0});var lat=s(Fq);Qor=r(lat,"TFEncoderDecoderModel"),lat.forEach(t),Hor=r($ye," (Encoder decoder model)"),$ye.forEach(t),Uor=i(Ee),bT=n(Ee,"LI",{});var Iye=s(bT);lge=n(Iye,"STRONG",{});var iat=s(lge);Jor=r(iat,"led"),iat.forEach(t),Yor=r(Iye," \u2014 "),Cq=n(Iye,"A",{href:!0});var dat=s(Cq);Kor=r(dat,"TFLEDForConditionalGeneration"),dat.forEach(t),Zor=r(Iye," (LED model)"),Iye.forEach(t),err=i(Ee),vT=n(Ee,"LI",{});var jye=s(vT);ige=n(jye,"STRONG",{});var cat=s(ige);orr=r(cat,"marian"),cat.forEach(t),rrr=r(jye," \u2014 "),Mq=n(jye,"A",{href:!0});var fat=s(Mq);trr=r(fat,"TFMarianMTModel"),fat.forEach(t),arr=r(jye," (Marian model)"),jye.forEach(t),nrr=i(Ee),TT=n(Ee,"LI",{});var Nye=s(TT);dge=n(Nye,"STRONG",{});var mat=s(dge);srr=r(mat,"mbart"),mat.forEach(t),lrr=r(Nye," \u2014 "),Eq=n(Nye,"A",{href:!0});var gat=s(Eq);irr=r(gat,"TFMBartForConditionalGeneration"),gat.forEach(t),drr=r(Nye," (mBART model)"),Nye.forEach(t),crr=i(Ee),FT=n(Ee,"LI",{});var Dye=s(FT);cge=n(Dye,"STRONG",{});var hat=s(cge);frr=r(hat,"mt5"),hat.forEach(t),mrr=r(Dye," \u2014 "),yq=n(Dye,"A",{href:!0});var pat=s(yq);grr=r(pat,"TFMT5ForConditionalGeneration"),pat.forEach(t),hrr=r(Dye," (mT5 model)"),Dye.forEach(t),prr=i(Ee),CT=n(Ee,"LI",{});var qye=s(CT);fge=n(qye,"STRONG",{});var _at=s(fge);_rr=r(_at,"pegasus"),_at.forEach(t),urr=r(qye," \u2014 "),wq=n(qye,"A",{href:!0});var uat=s(wq);brr=r(uat,"TFPegasusForConditionalGeneration"),uat.forEach(t),vrr=r(qye," (Pegasus model)"),qye.forEach(t),Trr=i(Ee),MT=n(Ee,"LI",{});var Gye=s(MT);mge=n(Gye,"STRONG",{});var bat=s(mge);Frr=r(bat,"t5"),bat.forEach(t),Crr=r(Gye," \u2014 "),Aq=n(Gye,"A",{href:!0});var vat=s(Aq);Mrr=r(vat,"TFT5ForConditionalGeneration"),vat.forEach(t),Err=r(Gye," (T5 model)"),Gye.forEach(t),Ee.forEach(t),yrr=i(pa),gge=n(pa,"P",{});var Tat=s(gge);wrr=r(Tat,"Examples:"),Tat.forEach(t),Arr=i(pa),m(_y.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),b8e=i(d),vc=n(d,"H2",{class:!0});var Ake=s(vc);ET=n(Ake,"A",{id:!0,class:!0,href:!0});var Fat=s(ET);hge=n(Fat,"SPAN",{});var Cat=s(hge);m(uy.$$.fragment,Cat),Cat.forEach(t),Fat.forEach(t),Lrr=i(Ake),pge=n(Ake,"SPAN",{});var Mat=s(pge);Brr=r(Mat,"TFAutoModelForSequenceClassification"),Mat.forEach(t),Ake.forEach(t),v8e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(by.$$.fragment,Gl),krr=i(Gl),Tc=n(Gl,"P",{});var Yz=s(Tc);xrr=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=n(Yz,"CODE",{});var Eat=s(_ge);Rrr=r(Eat,"from_pretrained()"),Eat.forEach(t),Srr=r(Yz,"class method or the "),uge=n(Yz,"CODE",{});var yat=s(uge);Prr=r(yat,"from_config()"),yat.forEach(t),$rr=r(Yz,`class
method.`),Yz.forEach(t),Irr=i(Gl),vy=n(Gl,"P",{});var Lke=s(vy);jrr=r(Lke,"This class cannot be instantiated directly using "),bge=n(Lke,"CODE",{});var wat=s(bge);Nrr=r(wat,"__init__()"),wat.forEach(t),Drr=r(Lke," (throws an error)."),Lke.forEach(t),qrr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(Ty.$$.fragment,Ol),Grr=i(Ol),vge=n(Ol,"P",{});var Aat=s(vge);Orr=r(Aat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Aat.forEach(t),Xrr=i(Ol),Fc=n(Ol,"P",{});var Kz=s(Fc);zrr=r(Kz,`Note:
Loading a model from its configuration file does `),Tge=n(Kz,"STRONG",{});var Lat=s(Tge);Vrr=r(Lat,"not"),Lat.forEach(t),Wrr=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=n(Kz,"CODE",{});var Bat=s(Fge);Qrr=r(Bat,"from_pretrained()"),Bat.forEach(t),Hrr=r(Kz,"to load the model weights."),Kz.forEach(t),Urr=i(Ol),Cge=n(Ol,"P",{});var kat=s(Cge);Jrr=r(kat,"Examples:"),kat.forEach(t),Yrr=i(Ol),m(Fy.$$.fragment,Ol),Ol.forEach(t),Krr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(Cy.$$.fragment,_a),Zrr=i(_a),Mge=n(_a,"P",{});var xat=s(Mge);etr=r(xat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),xat.forEach(t),otr=i(_a),hn=n(_a,"P",{});var $C=s(hn);rtr=r($C,"The model class to instantiate is selected based on the "),Ege=n($C,"CODE",{});var Rat=s(Ege);ttr=r(Rat,"model_type"),Rat.forEach(t),atr=r($C,` property of the config object (either
passed as an argument or loaded from `),yge=n($C,"CODE",{});var Sat=s(yge);ntr=r(Sat,"pretrained_model_name_or_path"),Sat.forEach(t),str=r($C,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=n($C,"CODE",{});var Pat=s(wge);ltr=r(Pat,"pretrained_model_name_or_path"),Pat.forEach(t),itr=r($C,":"),$C.forEach(t),dtr=i(_a),X=n(_a,"UL",{});var W=s(X);yT=n(W,"LI",{});var Oye=s(yT);Age=n(Oye,"STRONG",{});var $at=s(Age);ctr=r($at,"albert"),$at.forEach(t),ftr=r(Oye," \u2014 "),Lq=n(Oye,"A",{href:!0});var Iat=s(Lq);mtr=r(Iat,"TFAlbertForSequenceClassification"),Iat.forEach(t),gtr=r(Oye," (ALBERT model)"),Oye.forEach(t),htr=i(W),wT=n(W,"LI",{});var Xye=s(wT);Lge=n(Xye,"STRONG",{});var jat=s(Lge);ptr=r(jat,"bert"),jat.forEach(t),_tr=r(Xye," \u2014 "),Bq=n(Xye,"A",{href:!0});var Nat=s(Bq);utr=r(Nat,"TFBertForSequenceClassification"),Nat.forEach(t),btr=r(Xye," (BERT model)"),Xye.forEach(t),vtr=i(W),AT=n(W,"LI",{});var zye=s(AT);Bge=n(zye,"STRONG",{});var Dat=s(Bge);Ttr=r(Dat,"camembert"),Dat.forEach(t),Ftr=r(zye," \u2014 "),kq=n(zye,"A",{href:!0});var qat=s(kq);Ctr=r(qat,"TFCamembertForSequenceClassification"),qat.forEach(t),Mtr=r(zye," (CamemBERT model)"),zye.forEach(t),Etr=i(W),LT=n(W,"LI",{});var Vye=s(LT);kge=n(Vye,"STRONG",{});var Gat=s(kge);ytr=r(Gat,"convbert"),Gat.forEach(t),wtr=r(Vye," \u2014 "),xq=n(Vye,"A",{href:!0});var Oat=s(xq);Atr=r(Oat,"TFConvBertForSequenceClassification"),Oat.forEach(t),Ltr=r(Vye," (ConvBERT model)"),Vye.forEach(t),Btr=i(W),BT=n(W,"LI",{});var Wye=s(BT);xge=n(Wye,"STRONG",{});var Xat=s(xge);ktr=r(Xat,"ctrl"),Xat.forEach(t),xtr=r(Wye," \u2014 "),Rq=n(Wye,"A",{href:!0});var zat=s(Rq);Rtr=r(zat,"TFCTRLForSequenceClassification"),zat.forEach(t),Str=r(Wye," (CTRL model)"),Wye.forEach(t),Ptr=i(W),kT=n(W,"LI",{});var Qye=s(kT);Rge=n(Qye,"STRONG",{});var Vat=s(Rge);$tr=r(Vat,"deberta"),Vat.forEach(t),Itr=r(Qye," \u2014 "),Sq=n(Qye,"A",{href:!0});var Wat=s(Sq);jtr=r(Wat,"TFDebertaForSequenceClassification"),Wat.forEach(t),Ntr=r(Qye," (DeBERTa model)"),Qye.forEach(t),Dtr=i(W),xT=n(W,"LI",{});var Hye=s(xT);Sge=n(Hye,"STRONG",{});var Qat=s(Sge);qtr=r(Qat,"deberta-v2"),Qat.forEach(t),Gtr=r(Hye," \u2014 "),Pq=n(Hye,"A",{href:!0});var Hat=s(Pq);Otr=r(Hat,"TFDebertaV2ForSequenceClassification"),Hat.forEach(t),Xtr=r(Hye," (DeBERTa-v2 model)"),Hye.forEach(t),ztr=i(W),RT=n(W,"LI",{});var Uye=s(RT);Pge=n(Uye,"STRONG",{});var Uat=s(Pge);Vtr=r(Uat,"distilbert"),Uat.forEach(t),Wtr=r(Uye," \u2014 "),$q=n(Uye,"A",{href:!0});var Jat=s($q);Qtr=r(Jat,"TFDistilBertForSequenceClassification"),Jat.forEach(t),Htr=r(Uye," (DistilBERT model)"),Uye.forEach(t),Utr=i(W),ST=n(W,"LI",{});var Jye=s(ST);$ge=n(Jye,"STRONG",{});var Yat=s($ge);Jtr=r(Yat,"electra"),Yat.forEach(t),Ytr=r(Jye," \u2014 "),Iq=n(Jye,"A",{href:!0});var Kat=s(Iq);Ktr=r(Kat,"TFElectraForSequenceClassification"),Kat.forEach(t),Ztr=r(Jye," (ELECTRA model)"),Jye.forEach(t),ear=i(W),PT=n(W,"LI",{});var Yye=s(PT);Ige=n(Yye,"STRONG",{});var Zat=s(Ige);oar=r(Zat,"flaubert"),Zat.forEach(t),rar=r(Yye," \u2014 "),jq=n(Yye,"A",{href:!0});var ent=s(jq);tar=r(ent,"TFFlaubertForSequenceClassification"),ent.forEach(t),aar=r(Yye," (FlauBERT model)"),Yye.forEach(t),nar=i(W),$T=n(W,"LI",{});var Kye=s($T);jge=n(Kye,"STRONG",{});var ont=s(jge);sar=r(ont,"funnel"),ont.forEach(t),lar=r(Kye," \u2014 "),Nq=n(Kye,"A",{href:!0});var rnt=s(Nq);iar=r(rnt,"TFFunnelForSequenceClassification"),rnt.forEach(t),dar=r(Kye," (Funnel Transformer model)"),Kye.forEach(t),car=i(W),IT=n(W,"LI",{});var Zye=s(IT);Nge=n(Zye,"STRONG",{});var tnt=s(Nge);far=r(tnt,"gpt2"),tnt.forEach(t),mar=r(Zye," \u2014 "),Dq=n(Zye,"A",{href:!0});var ant=s(Dq);gar=r(ant,"TFGPT2ForSequenceClassification"),ant.forEach(t),har=r(Zye," (OpenAI GPT-2 model)"),Zye.forEach(t),par=i(W),jT=n(W,"LI",{});var ewe=s(jT);Dge=n(ewe,"STRONG",{});var nnt=s(Dge);_ar=r(nnt,"layoutlm"),nnt.forEach(t),uar=r(ewe," \u2014 "),qq=n(ewe,"A",{href:!0});var snt=s(qq);bar=r(snt,"TFLayoutLMForSequenceClassification"),snt.forEach(t),Tar=r(ewe," (LayoutLM model)"),ewe.forEach(t),Far=i(W),NT=n(W,"LI",{});var owe=s(NT);qge=n(owe,"STRONG",{});var lnt=s(qge);Car=r(lnt,"longformer"),lnt.forEach(t),Mar=r(owe," \u2014 "),Gq=n(owe,"A",{href:!0});var int=s(Gq);Ear=r(int,"TFLongformerForSequenceClassification"),int.forEach(t),yar=r(owe," (Longformer model)"),owe.forEach(t),war=i(W),DT=n(W,"LI",{});var rwe=s(DT);Gge=n(rwe,"STRONG",{});var dnt=s(Gge);Aar=r(dnt,"mobilebert"),dnt.forEach(t),Lar=r(rwe," \u2014 "),Oq=n(rwe,"A",{href:!0});var cnt=s(Oq);Bar=r(cnt,"TFMobileBertForSequenceClassification"),cnt.forEach(t),kar=r(rwe," (MobileBERT model)"),rwe.forEach(t),xar=i(W),qT=n(W,"LI",{});var twe=s(qT);Oge=n(twe,"STRONG",{});var fnt=s(Oge);Rar=r(fnt,"mpnet"),fnt.forEach(t),Sar=r(twe," \u2014 "),Xq=n(twe,"A",{href:!0});var mnt=s(Xq);Par=r(mnt,"TFMPNetForSequenceClassification"),mnt.forEach(t),$ar=r(twe," (MPNet model)"),twe.forEach(t),Iar=i(W),GT=n(W,"LI",{});var awe=s(GT);Xge=n(awe,"STRONG",{});var gnt=s(Xge);jar=r(gnt,"openai-gpt"),gnt.forEach(t),Nar=r(awe," \u2014 "),zq=n(awe,"A",{href:!0});var hnt=s(zq);Dar=r(hnt,"TFOpenAIGPTForSequenceClassification"),hnt.forEach(t),qar=r(awe," (OpenAI GPT model)"),awe.forEach(t),Gar=i(W),OT=n(W,"LI",{});var nwe=s(OT);zge=n(nwe,"STRONG",{});var pnt=s(zge);Oar=r(pnt,"rembert"),pnt.forEach(t),Xar=r(nwe," \u2014 "),Vq=n(nwe,"A",{href:!0});var _nt=s(Vq);zar=r(_nt,"TFRemBertForSequenceClassification"),_nt.forEach(t),Var=r(nwe," (RemBERT model)"),nwe.forEach(t),War=i(W),XT=n(W,"LI",{});var swe=s(XT);Vge=n(swe,"STRONG",{});var unt=s(Vge);Qar=r(unt,"roberta"),unt.forEach(t),Har=r(swe," \u2014 "),Wq=n(swe,"A",{href:!0});var bnt=s(Wq);Uar=r(bnt,"TFRobertaForSequenceClassification"),bnt.forEach(t),Jar=r(swe," (RoBERTa model)"),swe.forEach(t),Yar=i(W),zT=n(W,"LI",{});var lwe=s(zT);Wge=n(lwe,"STRONG",{});var vnt=s(Wge);Kar=r(vnt,"roformer"),vnt.forEach(t),Zar=r(lwe," \u2014 "),Qq=n(lwe,"A",{href:!0});var Tnt=s(Qq);enr=r(Tnt,"TFRoFormerForSequenceClassification"),Tnt.forEach(t),onr=r(lwe," (RoFormer model)"),lwe.forEach(t),rnr=i(W),VT=n(W,"LI",{});var iwe=s(VT);Qge=n(iwe,"STRONG",{});var Fnt=s(Qge);tnr=r(Fnt,"tapas"),Fnt.forEach(t),anr=r(iwe," \u2014 "),Hq=n(iwe,"A",{href:!0});var Cnt=s(Hq);nnr=r(Cnt,"TFTapasForSequenceClassification"),Cnt.forEach(t),snr=r(iwe," (TAPAS model)"),iwe.forEach(t),lnr=i(W),WT=n(W,"LI",{});var dwe=s(WT);Hge=n(dwe,"STRONG",{});var Mnt=s(Hge);inr=r(Mnt,"transfo-xl"),Mnt.forEach(t),dnr=r(dwe," \u2014 "),Uq=n(dwe,"A",{href:!0});var Ent=s(Uq);cnr=r(Ent,"TFTransfoXLForSequenceClassification"),Ent.forEach(t),fnr=r(dwe," (Transformer-XL model)"),dwe.forEach(t),mnr=i(W),QT=n(W,"LI",{});var cwe=s(QT);Uge=n(cwe,"STRONG",{});var ynt=s(Uge);gnr=r(ynt,"xlm"),ynt.forEach(t),hnr=r(cwe," \u2014 "),Jq=n(cwe,"A",{href:!0});var wnt=s(Jq);pnr=r(wnt,"TFXLMForSequenceClassification"),wnt.forEach(t),_nr=r(cwe," (XLM model)"),cwe.forEach(t),unr=i(W),HT=n(W,"LI",{});var fwe=s(HT);Jge=n(fwe,"STRONG",{});var Ant=s(Jge);bnr=r(Ant,"xlm-roberta"),Ant.forEach(t),vnr=r(fwe," \u2014 "),Yq=n(fwe,"A",{href:!0});var Lnt=s(Yq);Tnr=r(Lnt,"TFXLMRobertaForSequenceClassification"),Lnt.forEach(t),Fnr=r(fwe," (XLM-RoBERTa model)"),fwe.forEach(t),Cnr=i(W),UT=n(W,"LI",{});var mwe=s(UT);Yge=n(mwe,"STRONG",{});var Bnt=s(Yge);Mnr=r(Bnt,"xlnet"),Bnt.forEach(t),Enr=r(mwe," \u2014 "),Kq=n(mwe,"A",{href:!0});var knt=s(Kq);ynr=r(knt,"TFXLNetForSequenceClassification"),knt.forEach(t),wnr=r(mwe," (XLNet model)"),mwe.forEach(t),W.forEach(t),Anr=i(_a),Kge=n(_a,"P",{});var xnt=s(Kge);Lnr=r(xnt,"Examples:"),xnt.forEach(t),Bnr=i(_a),m(My.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),T8e=i(d),Cc=n(d,"H2",{class:!0});var Bke=s(Cc);JT=n(Bke,"A",{id:!0,class:!0,href:!0});var Rnt=s(JT);Zge=n(Rnt,"SPAN",{});var Snt=s(Zge);m(Ey.$$.fragment,Snt),Snt.forEach(t),Rnt.forEach(t),knr=i(Bke),ehe=n(Bke,"SPAN",{});var Pnt=s(ehe);xnr=r(Pnt,"TFAutoModelForMultipleChoice"),Pnt.forEach(t),Bke.forEach(t),F8e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(yy.$$.fragment,Xl),Rnr=i(Xl),Mc=n(Xl,"P",{});var Zz=s(Mc);Snr=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=n(Zz,"CODE",{});var $nt=s(ohe);Pnr=r($nt,"from_pretrained()"),$nt.forEach(t),$nr=r(Zz,"class method or the "),rhe=n(Zz,"CODE",{});var Int=s(rhe);Inr=r(Int,"from_config()"),Int.forEach(t),jnr=r(Zz,`class
method.`),Zz.forEach(t),Nnr=i(Xl),wy=n(Xl,"P",{});var kke=s(wy);Dnr=r(kke,"This class cannot be instantiated directly using "),the=n(kke,"CODE",{});var jnt=s(the);qnr=r(jnt,"__init__()"),jnt.forEach(t),Gnr=r(kke," (throws an error)."),kke.forEach(t),Onr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(Ay.$$.fragment,zl),Xnr=i(zl),ahe=n(zl,"P",{});var Nnt=s(ahe);znr=r(Nnt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Nnt.forEach(t),Vnr=i(zl),Ec=n(zl,"P",{});var eV=s(Ec);Wnr=r(eV,`Note:
Loading a model from its configuration file does `),nhe=n(eV,"STRONG",{});var Dnt=s(nhe);Qnr=r(Dnt,"not"),Dnt.forEach(t),Hnr=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),she=n(eV,"CODE",{});var qnt=s(she);Unr=r(qnt,"from_pretrained()"),qnt.forEach(t),Jnr=r(eV,"to load the model weights."),eV.forEach(t),Ynr=i(zl),lhe=n(zl,"P",{});var Gnt=s(lhe);Knr=r(Gnt,"Examples:"),Gnt.forEach(t),Znr=i(zl),m(Ly.$$.fragment,zl),zl.forEach(t),esr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(By.$$.fragment,ua),osr=i(ua),ihe=n(ua,"P",{});var Ont=s(ihe);rsr=r(Ont,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ont.forEach(t),tsr=i(ua),pn=n(ua,"P",{});var IC=s(pn);asr=r(IC,"The model class to instantiate is selected based on the "),dhe=n(IC,"CODE",{});var Xnt=s(dhe);nsr=r(Xnt,"model_type"),Xnt.forEach(t),ssr=r(IC,` property of the config object (either
passed as an argument or loaded from `),che=n(IC,"CODE",{});var znt=s(che);lsr=r(znt,"pretrained_model_name_or_path"),znt.forEach(t),isr=r(IC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=n(IC,"CODE",{});var Vnt=s(fhe);dsr=r(Vnt,"pretrained_model_name_or_path"),Vnt.forEach(t),csr=r(IC,":"),IC.forEach(t),fsr=i(ua),te=n(ua,"UL",{});var ne=s(te);YT=n(ne,"LI",{});var gwe=s(YT);mhe=n(gwe,"STRONG",{});var Wnt=s(mhe);msr=r(Wnt,"albert"),Wnt.forEach(t),gsr=r(gwe," \u2014 "),Zq=n(gwe,"A",{href:!0});var Qnt=s(Zq);hsr=r(Qnt,"TFAlbertForMultipleChoice"),Qnt.forEach(t),psr=r(gwe," (ALBERT model)"),gwe.forEach(t),_sr=i(ne),KT=n(ne,"LI",{});var hwe=s(KT);ghe=n(hwe,"STRONG",{});var Hnt=s(ghe);usr=r(Hnt,"bert"),Hnt.forEach(t),bsr=r(hwe," \u2014 "),eG=n(hwe,"A",{href:!0});var Unt=s(eG);vsr=r(Unt,"TFBertForMultipleChoice"),Unt.forEach(t),Tsr=r(hwe," (BERT model)"),hwe.forEach(t),Fsr=i(ne),ZT=n(ne,"LI",{});var pwe=s(ZT);hhe=n(pwe,"STRONG",{});var Jnt=s(hhe);Csr=r(Jnt,"camembert"),Jnt.forEach(t),Msr=r(pwe," \u2014 "),oG=n(pwe,"A",{href:!0});var Ynt=s(oG);Esr=r(Ynt,"TFCamembertForMultipleChoice"),Ynt.forEach(t),ysr=r(pwe," (CamemBERT model)"),pwe.forEach(t),wsr=i(ne),e7=n(ne,"LI",{});var _we=s(e7);phe=n(_we,"STRONG",{});var Knt=s(phe);Asr=r(Knt,"convbert"),Knt.forEach(t),Lsr=r(_we," \u2014 "),rG=n(_we,"A",{href:!0});var Znt=s(rG);Bsr=r(Znt,"TFConvBertForMultipleChoice"),Znt.forEach(t),ksr=r(_we," (ConvBERT model)"),_we.forEach(t),xsr=i(ne),o7=n(ne,"LI",{});var uwe=s(o7);_he=n(uwe,"STRONG",{});var est=s(_he);Rsr=r(est,"distilbert"),est.forEach(t),Ssr=r(uwe," \u2014 "),tG=n(uwe,"A",{href:!0});var ost=s(tG);Psr=r(ost,"TFDistilBertForMultipleChoice"),ost.forEach(t),$sr=r(uwe," (DistilBERT model)"),uwe.forEach(t),Isr=i(ne),r7=n(ne,"LI",{});var bwe=s(r7);uhe=n(bwe,"STRONG",{});var rst=s(uhe);jsr=r(rst,"electra"),rst.forEach(t),Nsr=r(bwe," \u2014 "),aG=n(bwe,"A",{href:!0});var tst=s(aG);Dsr=r(tst,"TFElectraForMultipleChoice"),tst.forEach(t),qsr=r(bwe," (ELECTRA model)"),bwe.forEach(t),Gsr=i(ne),t7=n(ne,"LI",{});var vwe=s(t7);bhe=n(vwe,"STRONG",{});var ast=s(bhe);Osr=r(ast,"flaubert"),ast.forEach(t),Xsr=r(vwe," \u2014 "),nG=n(vwe,"A",{href:!0});var nst=s(nG);zsr=r(nst,"TFFlaubertForMultipleChoice"),nst.forEach(t),Vsr=r(vwe," (FlauBERT model)"),vwe.forEach(t),Wsr=i(ne),a7=n(ne,"LI",{});var Twe=s(a7);vhe=n(Twe,"STRONG",{});var sst=s(vhe);Qsr=r(sst,"funnel"),sst.forEach(t),Hsr=r(Twe," \u2014 "),sG=n(Twe,"A",{href:!0});var lst=s(sG);Usr=r(lst,"TFFunnelForMultipleChoice"),lst.forEach(t),Jsr=r(Twe," (Funnel Transformer model)"),Twe.forEach(t),Ysr=i(ne),n7=n(ne,"LI",{});var Fwe=s(n7);The=n(Fwe,"STRONG",{});var ist=s(The);Ksr=r(ist,"longformer"),ist.forEach(t),Zsr=r(Fwe," \u2014 "),lG=n(Fwe,"A",{href:!0});var dst=s(lG);elr=r(dst,"TFLongformerForMultipleChoice"),dst.forEach(t),olr=r(Fwe," (Longformer model)"),Fwe.forEach(t),rlr=i(ne),s7=n(ne,"LI",{});var Cwe=s(s7);Fhe=n(Cwe,"STRONG",{});var cst=s(Fhe);tlr=r(cst,"mobilebert"),cst.forEach(t),alr=r(Cwe," \u2014 "),iG=n(Cwe,"A",{href:!0});var fst=s(iG);nlr=r(fst,"TFMobileBertForMultipleChoice"),fst.forEach(t),slr=r(Cwe," (MobileBERT model)"),Cwe.forEach(t),llr=i(ne),l7=n(ne,"LI",{});var Mwe=s(l7);Che=n(Mwe,"STRONG",{});var mst=s(Che);ilr=r(mst,"mpnet"),mst.forEach(t),dlr=r(Mwe," \u2014 "),dG=n(Mwe,"A",{href:!0});var gst=s(dG);clr=r(gst,"TFMPNetForMultipleChoice"),gst.forEach(t),flr=r(Mwe," (MPNet model)"),Mwe.forEach(t),mlr=i(ne),i7=n(ne,"LI",{});var Ewe=s(i7);Mhe=n(Ewe,"STRONG",{});var hst=s(Mhe);glr=r(hst,"rembert"),hst.forEach(t),hlr=r(Ewe," \u2014 "),cG=n(Ewe,"A",{href:!0});var pst=s(cG);plr=r(pst,"TFRemBertForMultipleChoice"),pst.forEach(t),_lr=r(Ewe," (RemBERT model)"),Ewe.forEach(t),ulr=i(ne),d7=n(ne,"LI",{});var ywe=s(d7);Ehe=n(ywe,"STRONG",{});var _st=s(Ehe);blr=r(_st,"roberta"),_st.forEach(t),vlr=r(ywe," \u2014 "),fG=n(ywe,"A",{href:!0});var ust=s(fG);Tlr=r(ust,"TFRobertaForMultipleChoice"),ust.forEach(t),Flr=r(ywe," (RoBERTa model)"),ywe.forEach(t),Clr=i(ne),c7=n(ne,"LI",{});var wwe=s(c7);yhe=n(wwe,"STRONG",{});var bst=s(yhe);Mlr=r(bst,"roformer"),bst.forEach(t),Elr=r(wwe," \u2014 "),mG=n(wwe,"A",{href:!0});var vst=s(mG);ylr=r(vst,"TFRoFormerForMultipleChoice"),vst.forEach(t),wlr=r(wwe," (RoFormer model)"),wwe.forEach(t),Alr=i(ne),f7=n(ne,"LI",{});var Awe=s(f7);whe=n(Awe,"STRONG",{});var Tst=s(whe);Llr=r(Tst,"xlm"),Tst.forEach(t),Blr=r(Awe," \u2014 "),gG=n(Awe,"A",{href:!0});var Fst=s(gG);klr=r(Fst,"TFXLMForMultipleChoice"),Fst.forEach(t),xlr=r(Awe," (XLM model)"),Awe.forEach(t),Rlr=i(ne),m7=n(ne,"LI",{});var Lwe=s(m7);Ahe=n(Lwe,"STRONG",{});var Cst=s(Ahe);Slr=r(Cst,"xlm-roberta"),Cst.forEach(t),Plr=r(Lwe," \u2014 "),hG=n(Lwe,"A",{href:!0});var Mst=s(hG);$lr=r(Mst,"TFXLMRobertaForMultipleChoice"),Mst.forEach(t),Ilr=r(Lwe," (XLM-RoBERTa model)"),Lwe.forEach(t),jlr=i(ne),g7=n(ne,"LI",{});var Bwe=s(g7);Lhe=n(Bwe,"STRONG",{});var Est=s(Lhe);Nlr=r(Est,"xlnet"),Est.forEach(t),Dlr=r(Bwe," \u2014 "),pG=n(Bwe,"A",{href:!0});var yst=s(pG);qlr=r(yst,"TFXLNetForMultipleChoice"),yst.forEach(t),Glr=r(Bwe," (XLNet model)"),Bwe.forEach(t),ne.forEach(t),Olr=i(ua),Bhe=n(ua,"P",{});var wst=s(Bhe);Xlr=r(wst,"Examples:"),wst.forEach(t),zlr=i(ua),m(ky.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),C8e=i(d),yc=n(d,"H2",{class:!0});var xke=s(yc);h7=n(xke,"A",{id:!0,class:!0,href:!0});var Ast=s(h7);khe=n(Ast,"SPAN",{});var Lst=s(khe);m(xy.$$.fragment,Lst),Lst.forEach(t),Ast.forEach(t),Vlr=i(xke),xhe=n(xke,"SPAN",{});var Bst=s(xhe);Wlr=r(Bst,"TFAutoModelForTableQuestionAnswering"),Bst.forEach(t),xke.forEach(t),M8e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(Ry.$$.fragment,Vl),Qlr=i(Vl),wc=n(Vl,"P",{});var oV=s(wc);Hlr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=n(oV,"CODE",{});var kst=s(Rhe);Ulr=r(kst,"from_pretrained()"),kst.forEach(t),Jlr=r(oV,"class method or the "),She=n(oV,"CODE",{});var xst=s(She);Ylr=r(xst,"from_config()"),xst.forEach(t),Klr=r(oV,`class
method.`),oV.forEach(t),Zlr=i(Vl),Sy=n(Vl,"P",{});var Rke=s(Sy);eir=r(Rke,"This class cannot be instantiated directly using "),Phe=n(Rke,"CODE",{});var Rst=s(Phe);oir=r(Rst,"__init__()"),Rst.forEach(t),rir=r(Rke," (throws an error)."),Rke.forEach(t),tir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(Py.$$.fragment,Wl),air=i(Wl),$he=n(Wl,"P",{});var Sst=s($he);nir=r(Sst,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Sst.forEach(t),sir=i(Wl),Ac=n(Wl,"P",{});var rV=s(Ac);lir=r(rV,`Note:
Loading a model from its configuration file does `),Ihe=n(rV,"STRONG",{});var Pst=s(Ihe);iir=r(Pst,"not"),Pst.forEach(t),dir=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=n(rV,"CODE",{});var $st=s(jhe);cir=r($st,"from_pretrained()"),$st.forEach(t),fir=r(rV,"to load the model weights."),rV.forEach(t),mir=i(Wl),Nhe=n(Wl,"P",{});var Ist=s(Nhe);gir=r(Ist,"Examples:"),Ist.forEach(t),hir=i(Wl),m($y.$$.fragment,Wl),Wl.forEach(t),pir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(Iy.$$.fragment,ba),_ir=i(ba),Dhe=n(ba,"P",{});var jst=s(Dhe);uir=r(jst,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),jst.forEach(t),bir=i(ba),_n=n(ba,"P",{});var jC=s(_n);vir=r(jC,"The model class to instantiate is selected based on the "),qhe=n(jC,"CODE",{});var Nst=s(qhe);Tir=r(Nst,"model_type"),Nst.forEach(t),Fir=r(jC,` property of the config object (either
passed as an argument or loaded from `),Ghe=n(jC,"CODE",{});var Dst=s(Ghe);Cir=r(Dst,"pretrained_model_name_or_path"),Dst.forEach(t),Mir=r(jC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=n(jC,"CODE",{});var qst=s(Ohe);Eir=r(qst,"pretrained_model_name_or_path"),qst.forEach(t),yir=r(jC,":"),jC.forEach(t),wir=i(ba),Xhe=n(ba,"UL",{});var Gst=s(Xhe);p7=n(Gst,"LI",{});var kwe=s(p7);zhe=n(kwe,"STRONG",{});var Ost=s(zhe);Air=r(Ost,"tapas"),Ost.forEach(t),Lir=r(kwe," \u2014 "),_G=n(kwe,"A",{href:!0});var Xst=s(_G);Bir=r(Xst,"TFTapasForQuestionAnswering"),Xst.forEach(t),kir=r(kwe," (TAPAS model)"),kwe.forEach(t),Gst.forEach(t),xir=i(ba),Vhe=n(ba,"P",{});var zst=s(Vhe);Rir=r(zst,"Examples:"),zst.forEach(t),Sir=i(ba),m(jy.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),E8e=i(d),Lc=n(d,"H2",{class:!0});var Ske=s(Lc);_7=n(Ske,"A",{id:!0,class:!0,href:!0});var Vst=s(_7);Whe=n(Vst,"SPAN",{});var Wst=s(Whe);m(Ny.$$.fragment,Wst),Wst.forEach(t),Vst.forEach(t),Pir=i(Ske),Qhe=n(Ske,"SPAN",{});var Qst=s(Qhe);$ir=r(Qst,"TFAutoModelForTokenClassification"),Qst.forEach(t),Ske.forEach(t),y8e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(Dy.$$.fragment,Ql),Iir=i(Ql),Bc=n(Ql,"P",{});var tV=s(Bc);jir=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=n(tV,"CODE",{});var Hst=s(Hhe);Nir=r(Hst,"from_pretrained()"),Hst.forEach(t),Dir=r(tV,"class method or the "),Uhe=n(tV,"CODE",{});var Ust=s(Uhe);qir=r(Ust,"from_config()"),Ust.forEach(t),Gir=r(tV,`class
method.`),tV.forEach(t),Oir=i(Ql),qy=n(Ql,"P",{});var Pke=s(qy);Xir=r(Pke,"This class cannot be instantiated directly using "),Jhe=n(Pke,"CODE",{});var Jst=s(Jhe);zir=r(Jst,"__init__()"),Jst.forEach(t),Vir=r(Pke," (throws an error)."),Pke.forEach(t),Wir=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(Gy.$$.fragment,Hl),Qir=i(Hl),Yhe=n(Hl,"P",{});var Yst=s(Yhe);Hir=r(Yst,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Yst.forEach(t),Uir=i(Hl),kc=n(Hl,"P",{});var aV=s(kc);Jir=r(aV,`Note:
Loading a model from its configuration file does `),Khe=n(aV,"STRONG",{});var Kst=s(Khe);Yir=r(Kst,"not"),Kst.forEach(t),Kir=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=n(aV,"CODE",{});var Zst=s(Zhe);Zir=r(Zst,"from_pretrained()"),Zst.forEach(t),edr=r(aV,"to load the model weights."),aV.forEach(t),odr=i(Hl),epe=n(Hl,"P",{});var elt=s(epe);rdr=r(elt,"Examples:"),elt.forEach(t),tdr=i(Hl),m(Oy.$$.fragment,Hl),Hl.forEach(t),adr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(Xy.$$.fragment,va),ndr=i(va),ope=n(va,"P",{});var olt=s(ope);sdr=r(olt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),olt.forEach(t),ldr=i(va),un=n(va,"P",{});var NC=s(un);idr=r(NC,"The model class to instantiate is selected based on the "),rpe=n(NC,"CODE",{});var rlt=s(rpe);ddr=r(rlt,"model_type"),rlt.forEach(t),cdr=r(NC,` property of the config object (either
passed as an argument or loaded from `),tpe=n(NC,"CODE",{});var tlt=s(tpe);fdr=r(tlt,"pretrained_model_name_or_path"),tlt.forEach(t),mdr=r(NC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=n(NC,"CODE",{});var alt=s(ape);gdr=r(alt,"pretrained_model_name_or_path"),alt.forEach(t),hdr=r(NC,":"),NC.forEach(t),pdr=i(va),K=n(va,"UL",{});var oe=s(K);u7=n(oe,"LI",{});var xwe=s(u7);npe=n(xwe,"STRONG",{});var nlt=s(npe);_dr=r(nlt,"albert"),nlt.forEach(t),udr=r(xwe," \u2014 "),uG=n(xwe,"A",{href:!0});var slt=s(uG);bdr=r(slt,"TFAlbertForTokenClassification"),slt.forEach(t),vdr=r(xwe," (ALBERT model)"),xwe.forEach(t),Tdr=i(oe),b7=n(oe,"LI",{});var Rwe=s(b7);spe=n(Rwe,"STRONG",{});var llt=s(spe);Fdr=r(llt,"bert"),llt.forEach(t),Cdr=r(Rwe," \u2014 "),bG=n(Rwe,"A",{href:!0});var ilt=s(bG);Mdr=r(ilt,"TFBertForTokenClassification"),ilt.forEach(t),Edr=r(Rwe," (BERT model)"),Rwe.forEach(t),ydr=i(oe),v7=n(oe,"LI",{});var Swe=s(v7);lpe=n(Swe,"STRONG",{});var dlt=s(lpe);wdr=r(dlt,"camembert"),dlt.forEach(t),Adr=r(Swe," \u2014 "),vG=n(Swe,"A",{href:!0});var clt=s(vG);Ldr=r(clt,"TFCamembertForTokenClassification"),clt.forEach(t),Bdr=r(Swe," (CamemBERT model)"),Swe.forEach(t),kdr=i(oe),T7=n(oe,"LI",{});var Pwe=s(T7);ipe=n(Pwe,"STRONG",{});var flt=s(ipe);xdr=r(flt,"convbert"),flt.forEach(t),Rdr=r(Pwe," \u2014 "),TG=n(Pwe,"A",{href:!0});var mlt=s(TG);Sdr=r(mlt,"TFConvBertForTokenClassification"),mlt.forEach(t),Pdr=r(Pwe," (ConvBERT model)"),Pwe.forEach(t),$dr=i(oe),F7=n(oe,"LI",{});var $we=s(F7);dpe=n($we,"STRONG",{});var glt=s(dpe);Idr=r(glt,"deberta"),glt.forEach(t),jdr=r($we," \u2014 "),FG=n($we,"A",{href:!0});var hlt=s(FG);Ndr=r(hlt,"TFDebertaForTokenClassification"),hlt.forEach(t),Ddr=r($we," (DeBERTa model)"),$we.forEach(t),qdr=i(oe),C7=n(oe,"LI",{});var Iwe=s(C7);cpe=n(Iwe,"STRONG",{});var plt=s(cpe);Gdr=r(plt,"deberta-v2"),plt.forEach(t),Odr=r(Iwe," \u2014 "),CG=n(Iwe,"A",{href:!0});var _lt=s(CG);Xdr=r(_lt,"TFDebertaV2ForTokenClassification"),_lt.forEach(t),zdr=r(Iwe," (DeBERTa-v2 model)"),Iwe.forEach(t),Vdr=i(oe),M7=n(oe,"LI",{});var jwe=s(M7);fpe=n(jwe,"STRONG",{});var ult=s(fpe);Wdr=r(ult,"distilbert"),ult.forEach(t),Qdr=r(jwe," \u2014 "),MG=n(jwe,"A",{href:!0});var blt=s(MG);Hdr=r(blt,"TFDistilBertForTokenClassification"),blt.forEach(t),Udr=r(jwe," (DistilBERT model)"),jwe.forEach(t),Jdr=i(oe),E7=n(oe,"LI",{});var Nwe=s(E7);mpe=n(Nwe,"STRONG",{});var vlt=s(mpe);Ydr=r(vlt,"electra"),vlt.forEach(t),Kdr=r(Nwe," \u2014 "),EG=n(Nwe,"A",{href:!0});var Tlt=s(EG);Zdr=r(Tlt,"TFElectraForTokenClassification"),Tlt.forEach(t),ecr=r(Nwe," (ELECTRA model)"),Nwe.forEach(t),ocr=i(oe),y7=n(oe,"LI",{});var Dwe=s(y7);gpe=n(Dwe,"STRONG",{});var Flt=s(gpe);rcr=r(Flt,"flaubert"),Flt.forEach(t),tcr=r(Dwe," \u2014 "),yG=n(Dwe,"A",{href:!0});var Clt=s(yG);acr=r(Clt,"TFFlaubertForTokenClassification"),Clt.forEach(t),ncr=r(Dwe," (FlauBERT model)"),Dwe.forEach(t),scr=i(oe),w7=n(oe,"LI",{});var qwe=s(w7);hpe=n(qwe,"STRONG",{});var Mlt=s(hpe);lcr=r(Mlt,"funnel"),Mlt.forEach(t),icr=r(qwe," \u2014 "),wG=n(qwe,"A",{href:!0});var Elt=s(wG);dcr=r(Elt,"TFFunnelForTokenClassification"),Elt.forEach(t),ccr=r(qwe," (Funnel Transformer model)"),qwe.forEach(t),fcr=i(oe),A7=n(oe,"LI",{});var Gwe=s(A7);ppe=n(Gwe,"STRONG",{});var ylt=s(ppe);mcr=r(ylt,"layoutlm"),ylt.forEach(t),gcr=r(Gwe," \u2014 "),AG=n(Gwe,"A",{href:!0});var wlt=s(AG);hcr=r(wlt,"TFLayoutLMForTokenClassification"),wlt.forEach(t),pcr=r(Gwe," (LayoutLM model)"),Gwe.forEach(t),_cr=i(oe),L7=n(oe,"LI",{});var Owe=s(L7);_pe=n(Owe,"STRONG",{});var Alt=s(_pe);ucr=r(Alt,"longformer"),Alt.forEach(t),bcr=r(Owe," \u2014 "),LG=n(Owe,"A",{href:!0});var Llt=s(LG);vcr=r(Llt,"TFLongformerForTokenClassification"),Llt.forEach(t),Tcr=r(Owe," (Longformer model)"),Owe.forEach(t),Fcr=i(oe),B7=n(oe,"LI",{});var Xwe=s(B7);upe=n(Xwe,"STRONG",{});var Blt=s(upe);Ccr=r(Blt,"mobilebert"),Blt.forEach(t),Mcr=r(Xwe," \u2014 "),BG=n(Xwe,"A",{href:!0});var klt=s(BG);Ecr=r(klt,"TFMobileBertForTokenClassification"),klt.forEach(t),ycr=r(Xwe," (MobileBERT model)"),Xwe.forEach(t),wcr=i(oe),k7=n(oe,"LI",{});var zwe=s(k7);bpe=n(zwe,"STRONG",{});var xlt=s(bpe);Acr=r(xlt,"mpnet"),xlt.forEach(t),Lcr=r(zwe," \u2014 "),kG=n(zwe,"A",{href:!0});var Rlt=s(kG);Bcr=r(Rlt,"TFMPNetForTokenClassification"),Rlt.forEach(t),kcr=r(zwe," (MPNet model)"),zwe.forEach(t),xcr=i(oe),x7=n(oe,"LI",{});var Vwe=s(x7);vpe=n(Vwe,"STRONG",{});var Slt=s(vpe);Rcr=r(Slt,"rembert"),Slt.forEach(t),Scr=r(Vwe," \u2014 "),xG=n(Vwe,"A",{href:!0});var Plt=s(xG);Pcr=r(Plt,"TFRemBertForTokenClassification"),Plt.forEach(t),$cr=r(Vwe," (RemBERT model)"),Vwe.forEach(t),Icr=i(oe),R7=n(oe,"LI",{});var Wwe=s(R7);Tpe=n(Wwe,"STRONG",{});var $lt=s(Tpe);jcr=r($lt,"roberta"),$lt.forEach(t),Ncr=r(Wwe," \u2014 "),RG=n(Wwe,"A",{href:!0});var Ilt=s(RG);Dcr=r(Ilt,"TFRobertaForTokenClassification"),Ilt.forEach(t),qcr=r(Wwe," (RoBERTa model)"),Wwe.forEach(t),Gcr=i(oe),S7=n(oe,"LI",{});var Qwe=s(S7);Fpe=n(Qwe,"STRONG",{});var jlt=s(Fpe);Ocr=r(jlt,"roformer"),jlt.forEach(t),Xcr=r(Qwe," \u2014 "),SG=n(Qwe,"A",{href:!0});var Nlt=s(SG);zcr=r(Nlt,"TFRoFormerForTokenClassification"),Nlt.forEach(t),Vcr=r(Qwe," (RoFormer model)"),Qwe.forEach(t),Wcr=i(oe),P7=n(oe,"LI",{});var Hwe=s(P7);Cpe=n(Hwe,"STRONG",{});var Dlt=s(Cpe);Qcr=r(Dlt,"xlm"),Dlt.forEach(t),Hcr=r(Hwe," \u2014 "),PG=n(Hwe,"A",{href:!0});var qlt=s(PG);Ucr=r(qlt,"TFXLMForTokenClassification"),qlt.forEach(t),Jcr=r(Hwe," (XLM model)"),Hwe.forEach(t),Ycr=i(oe),$7=n(oe,"LI",{});var Uwe=s($7);Mpe=n(Uwe,"STRONG",{});var Glt=s(Mpe);Kcr=r(Glt,"xlm-roberta"),Glt.forEach(t),Zcr=r(Uwe," \u2014 "),$G=n(Uwe,"A",{href:!0});var Olt=s($G);efr=r(Olt,"TFXLMRobertaForTokenClassification"),Olt.forEach(t),ofr=r(Uwe," (XLM-RoBERTa model)"),Uwe.forEach(t),rfr=i(oe),I7=n(oe,"LI",{});var Jwe=s(I7);Epe=n(Jwe,"STRONG",{});var Xlt=s(Epe);tfr=r(Xlt,"xlnet"),Xlt.forEach(t),afr=r(Jwe," \u2014 "),IG=n(Jwe,"A",{href:!0});var zlt=s(IG);nfr=r(zlt,"TFXLNetForTokenClassification"),zlt.forEach(t),sfr=r(Jwe," (XLNet model)"),Jwe.forEach(t),oe.forEach(t),lfr=i(va),ype=n(va,"P",{});var Vlt=s(ype);ifr=r(Vlt,"Examples:"),Vlt.forEach(t),dfr=i(va),m(zy.$$.fragment,va),va.forEach(t),Ql.forEach(t),w8e=i(d),xc=n(d,"H2",{class:!0});var $ke=s(xc);j7=n($ke,"A",{id:!0,class:!0,href:!0});var Wlt=s(j7);wpe=n(Wlt,"SPAN",{});var Qlt=s(wpe);m(Vy.$$.fragment,Qlt),Qlt.forEach(t),Wlt.forEach(t),cfr=i($ke),Ape=n($ke,"SPAN",{});var Hlt=s(Ape);ffr=r(Hlt,"TFAutoModelForQuestionAnswering"),Hlt.forEach(t),$ke.forEach(t),A8e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(Wy.$$.fragment,Ul),mfr=i(Ul),Rc=n(Ul,"P",{});var nV=s(Rc);gfr=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=n(nV,"CODE",{});var Ult=s(Lpe);hfr=r(Ult,"from_pretrained()"),Ult.forEach(t),pfr=r(nV,"class method or the "),Bpe=n(nV,"CODE",{});var Jlt=s(Bpe);_fr=r(Jlt,"from_config()"),Jlt.forEach(t),ufr=r(nV,`class
method.`),nV.forEach(t),bfr=i(Ul),Qy=n(Ul,"P",{});var Ike=s(Qy);vfr=r(Ike,"This class cannot be instantiated directly using "),kpe=n(Ike,"CODE",{});var Ylt=s(kpe);Tfr=r(Ylt,"__init__()"),Ylt.forEach(t),Ffr=r(Ike," (throws an error)."),Ike.forEach(t),Cfr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(Hy.$$.fragment,Jl),Mfr=i(Jl),xpe=n(Jl,"P",{});var Klt=s(xpe);Efr=r(Klt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Klt.forEach(t),yfr=i(Jl),Sc=n(Jl,"P",{});var sV=s(Sc);wfr=r(sV,`Note:
Loading a model from its configuration file does `),Rpe=n(sV,"STRONG",{});var Zlt=s(Rpe);Afr=r(Zlt,"not"),Zlt.forEach(t),Lfr=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=n(sV,"CODE",{});var eit=s(Spe);Bfr=r(eit,"from_pretrained()"),eit.forEach(t),kfr=r(sV,"to load the model weights."),sV.forEach(t),xfr=i(Jl),Ppe=n(Jl,"P",{});var oit=s(Ppe);Rfr=r(oit,"Examples:"),oit.forEach(t),Sfr=i(Jl),m(Uy.$$.fragment,Jl),Jl.forEach(t),Pfr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(Jy.$$.fragment,Ta),$fr=i(Ta),$pe=n(Ta,"P",{});var rit=s($pe);Ifr=r(rit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),rit.forEach(t),jfr=i(Ta),bn=n(Ta,"P",{});var DC=s(bn);Nfr=r(DC,"The model class to instantiate is selected based on the "),Ipe=n(DC,"CODE",{});var tit=s(Ipe);Dfr=r(tit,"model_type"),tit.forEach(t),qfr=r(DC,` property of the config object (either
passed as an argument or loaded from `),jpe=n(DC,"CODE",{});var ait=s(jpe);Gfr=r(ait,"pretrained_model_name_or_path"),ait.forEach(t),Ofr=r(DC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=n(DC,"CODE",{});var nit=s(Npe);Xfr=r(nit,"pretrained_model_name_or_path"),nit.forEach(t),zfr=r(DC,":"),DC.forEach(t),Vfr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);N7=n(re,"LI",{});var Ywe=s(N7);Dpe=n(Ywe,"STRONG",{});var sit=s(Dpe);Wfr=r(sit,"albert"),sit.forEach(t),Qfr=r(Ywe," \u2014 "),jG=n(Ywe,"A",{href:!0});var lit=s(jG);Hfr=r(lit,"TFAlbertForQuestionAnswering"),lit.forEach(t),Ufr=r(Ywe," (ALBERT model)"),Ywe.forEach(t),Jfr=i(re),D7=n(re,"LI",{});var Kwe=s(D7);qpe=n(Kwe,"STRONG",{});var iit=s(qpe);Yfr=r(iit,"bert"),iit.forEach(t),Kfr=r(Kwe," \u2014 "),NG=n(Kwe,"A",{href:!0});var dit=s(NG);Zfr=r(dit,"TFBertForQuestionAnswering"),dit.forEach(t),emr=r(Kwe," (BERT model)"),Kwe.forEach(t),omr=i(re),q7=n(re,"LI",{});var Zwe=s(q7);Gpe=n(Zwe,"STRONG",{});var cit=s(Gpe);rmr=r(cit,"camembert"),cit.forEach(t),tmr=r(Zwe," \u2014 "),DG=n(Zwe,"A",{href:!0});var fit=s(DG);amr=r(fit,"TFCamembertForQuestionAnswering"),fit.forEach(t),nmr=r(Zwe," (CamemBERT model)"),Zwe.forEach(t),smr=i(re),G7=n(re,"LI",{});var eAe=s(G7);Ope=n(eAe,"STRONG",{});var mit=s(Ope);lmr=r(mit,"convbert"),mit.forEach(t),imr=r(eAe," \u2014 "),qG=n(eAe,"A",{href:!0});var git=s(qG);dmr=r(git,"TFConvBertForQuestionAnswering"),git.forEach(t),cmr=r(eAe," (ConvBERT model)"),eAe.forEach(t),fmr=i(re),O7=n(re,"LI",{});var oAe=s(O7);Xpe=n(oAe,"STRONG",{});var hit=s(Xpe);mmr=r(hit,"deberta"),hit.forEach(t),gmr=r(oAe," \u2014 "),GG=n(oAe,"A",{href:!0});var pit=s(GG);hmr=r(pit,"TFDebertaForQuestionAnswering"),pit.forEach(t),pmr=r(oAe," (DeBERTa model)"),oAe.forEach(t),_mr=i(re),X7=n(re,"LI",{});var rAe=s(X7);zpe=n(rAe,"STRONG",{});var _it=s(zpe);umr=r(_it,"deberta-v2"),_it.forEach(t),bmr=r(rAe," \u2014 "),OG=n(rAe,"A",{href:!0});var uit=s(OG);vmr=r(uit,"TFDebertaV2ForQuestionAnswering"),uit.forEach(t),Tmr=r(rAe," (DeBERTa-v2 model)"),rAe.forEach(t),Fmr=i(re),z7=n(re,"LI",{});var tAe=s(z7);Vpe=n(tAe,"STRONG",{});var bit=s(Vpe);Cmr=r(bit,"distilbert"),bit.forEach(t),Mmr=r(tAe," \u2014 "),XG=n(tAe,"A",{href:!0});var vit=s(XG);Emr=r(vit,"TFDistilBertForQuestionAnswering"),vit.forEach(t),ymr=r(tAe," (DistilBERT model)"),tAe.forEach(t),wmr=i(re),V7=n(re,"LI",{});var aAe=s(V7);Wpe=n(aAe,"STRONG",{});var Tit=s(Wpe);Amr=r(Tit,"electra"),Tit.forEach(t),Lmr=r(aAe," \u2014 "),zG=n(aAe,"A",{href:!0});var Fit=s(zG);Bmr=r(Fit,"TFElectraForQuestionAnswering"),Fit.forEach(t),kmr=r(aAe," (ELECTRA model)"),aAe.forEach(t),xmr=i(re),W7=n(re,"LI",{});var nAe=s(W7);Qpe=n(nAe,"STRONG",{});var Cit=s(Qpe);Rmr=r(Cit,"flaubert"),Cit.forEach(t),Smr=r(nAe," \u2014 "),VG=n(nAe,"A",{href:!0});var Mit=s(VG);Pmr=r(Mit,"TFFlaubertForQuestionAnsweringSimple"),Mit.forEach(t),$mr=r(nAe," (FlauBERT model)"),nAe.forEach(t),Imr=i(re),Q7=n(re,"LI",{});var sAe=s(Q7);Hpe=n(sAe,"STRONG",{});var Eit=s(Hpe);jmr=r(Eit,"funnel"),Eit.forEach(t),Nmr=r(sAe," \u2014 "),WG=n(sAe,"A",{href:!0});var yit=s(WG);Dmr=r(yit,"TFFunnelForQuestionAnswering"),yit.forEach(t),qmr=r(sAe," (Funnel Transformer model)"),sAe.forEach(t),Gmr=i(re),H7=n(re,"LI",{});var lAe=s(H7);Upe=n(lAe,"STRONG",{});var wit=s(Upe);Omr=r(wit,"longformer"),wit.forEach(t),Xmr=r(lAe," \u2014 "),QG=n(lAe,"A",{href:!0});var Ait=s(QG);zmr=r(Ait,"TFLongformerForQuestionAnswering"),Ait.forEach(t),Vmr=r(lAe," (Longformer model)"),lAe.forEach(t),Wmr=i(re),U7=n(re,"LI",{});var iAe=s(U7);Jpe=n(iAe,"STRONG",{});var Lit=s(Jpe);Qmr=r(Lit,"mobilebert"),Lit.forEach(t),Hmr=r(iAe," \u2014 "),HG=n(iAe,"A",{href:!0});var Bit=s(HG);Umr=r(Bit,"TFMobileBertForQuestionAnswering"),Bit.forEach(t),Jmr=r(iAe," (MobileBERT model)"),iAe.forEach(t),Ymr=i(re),J7=n(re,"LI",{});var dAe=s(J7);Ype=n(dAe,"STRONG",{});var kit=s(Ype);Kmr=r(kit,"mpnet"),kit.forEach(t),Zmr=r(dAe," \u2014 "),UG=n(dAe,"A",{href:!0});var xit=s(UG);egr=r(xit,"TFMPNetForQuestionAnswering"),xit.forEach(t),ogr=r(dAe," (MPNet model)"),dAe.forEach(t),rgr=i(re),Y7=n(re,"LI",{});var cAe=s(Y7);Kpe=n(cAe,"STRONG",{});var Rit=s(Kpe);tgr=r(Rit,"rembert"),Rit.forEach(t),agr=r(cAe," \u2014 "),JG=n(cAe,"A",{href:!0});var Sit=s(JG);ngr=r(Sit,"TFRemBertForQuestionAnswering"),Sit.forEach(t),sgr=r(cAe," (RemBERT model)"),cAe.forEach(t),lgr=i(re),K7=n(re,"LI",{});var fAe=s(K7);Zpe=n(fAe,"STRONG",{});var Pit=s(Zpe);igr=r(Pit,"roberta"),Pit.forEach(t),dgr=r(fAe," \u2014 "),YG=n(fAe,"A",{href:!0});var $it=s(YG);cgr=r($it,"TFRobertaForQuestionAnswering"),$it.forEach(t),fgr=r(fAe," (RoBERTa model)"),fAe.forEach(t),mgr=i(re),Z7=n(re,"LI",{});var mAe=s(Z7);e_e=n(mAe,"STRONG",{});var Iit=s(e_e);ggr=r(Iit,"roformer"),Iit.forEach(t),hgr=r(mAe," \u2014 "),KG=n(mAe,"A",{href:!0});var jit=s(KG);pgr=r(jit,"TFRoFormerForQuestionAnswering"),jit.forEach(t),_gr=r(mAe," (RoFormer model)"),mAe.forEach(t),ugr=i(re),eF=n(re,"LI",{});var gAe=s(eF);o_e=n(gAe,"STRONG",{});var Nit=s(o_e);bgr=r(Nit,"xlm"),Nit.forEach(t),vgr=r(gAe," \u2014 "),ZG=n(gAe,"A",{href:!0});var Dit=s(ZG);Tgr=r(Dit,"TFXLMForQuestionAnsweringSimple"),Dit.forEach(t),Fgr=r(gAe," (XLM model)"),gAe.forEach(t),Cgr=i(re),oF=n(re,"LI",{});var hAe=s(oF);r_e=n(hAe,"STRONG",{});var qit=s(r_e);Mgr=r(qit,"xlm-roberta"),qit.forEach(t),Egr=r(hAe," \u2014 "),eO=n(hAe,"A",{href:!0});var Git=s(eO);ygr=r(Git,"TFXLMRobertaForQuestionAnswering"),Git.forEach(t),wgr=r(hAe," (XLM-RoBERTa model)"),hAe.forEach(t),Agr=i(re),rF=n(re,"LI",{});var pAe=s(rF);t_e=n(pAe,"STRONG",{});var Oit=s(t_e);Lgr=r(Oit,"xlnet"),Oit.forEach(t),Bgr=r(pAe," \u2014 "),oO=n(pAe,"A",{href:!0});var Xit=s(oO);kgr=r(Xit,"TFXLNetForQuestionAnsweringSimple"),Xit.forEach(t),xgr=r(pAe," (XLNet model)"),pAe.forEach(t),re.forEach(t),Rgr=i(Ta),a_e=n(Ta,"P",{});var zit=s(a_e);Sgr=r(zit,"Examples:"),zit.forEach(t),Pgr=i(Ta),m(Yy.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),L8e=i(d),Pc=n(d,"H2",{class:!0});var jke=s(Pc);tF=n(jke,"A",{id:!0,class:!0,href:!0});var Vit=s(tF);n_e=n(Vit,"SPAN",{});var Wit=s(n_e);m(Ky.$$.fragment,Wit),Wit.forEach(t),Vit.forEach(t),$gr=i(jke),s_e=n(jke,"SPAN",{});var Qit=s(s_e);Igr=r(Qit,"TFAutoModelForVision2Seq"),Qit.forEach(t),jke.forEach(t),B8e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(Zy.$$.fragment,Yl),jgr=i(Yl),$c=n(Yl,"P",{});var lV=s($c);Ngr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=n(lV,"CODE",{});var Hit=s(l_e);Dgr=r(Hit,"from_pretrained()"),Hit.forEach(t),qgr=r(lV,"class method or the "),i_e=n(lV,"CODE",{});var Uit=s(i_e);Ggr=r(Uit,"from_config()"),Uit.forEach(t),Ogr=r(lV,`class
method.`),lV.forEach(t),Xgr=i(Yl),ew=n(Yl,"P",{});var Nke=s(ew);zgr=r(Nke,"This class cannot be instantiated directly using "),d_e=n(Nke,"CODE",{});var Jit=s(d_e);Vgr=r(Jit,"__init__()"),Jit.forEach(t),Wgr=r(Nke," (throws an error)."),Nke.forEach(t),Qgr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(ow.$$.fragment,Kl),Hgr=i(Kl),c_e=n(Kl,"P",{});var Yit=s(c_e);Ugr=r(Yit,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Yit.forEach(t),Jgr=i(Kl),Ic=n(Kl,"P",{});var iV=s(Ic);Ygr=r(iV,`Note:
Loading a model from its configuration file does `),f_e=n(iV,"STRONG",{});var Kit=s(f_e);Kgr=r(Kit,"not"),Kit.forEach(t),Zgr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=n(iV,"CODE",{});var Zit=s(m_e);ehr=r(Zit,"from_pretrained()"),Zit.forEach(t),ohr=r(iV,"to load the model weights."),iV.forEach(t),rhr=i(Kl),g_e=n(Kl,"P",{});var edt=s(g_e);thr=r(edt,"Examples:"),edt.forEach(t),ahr=i(Kl),m(rw.$$.fragment,Kl),Kl.forEach(t),nhr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(tw.$$.fragment,Fa),shr=i(Fa),h_e=n(Fa,"P",{});var odt=s(h_e);lhr=r(odt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),odt.forEach(t),ihr=i(Fa),vn=n(Fa,"P",{});var qC=s(vn);dhr=r(qC,"The model class to instantiate is selected based on the "),p_e=n(qC,"CODE",{});var rdt=s(p_e);chr=r(rdt,"model_type"),rdt.forEach(t),fhr=r(qC,` property of the config object (either
passed as an argument or loaded from `),__e=n(qC,"CODE",{});var tdt=s(__e);mhr=r(tdt,"pretrained_model_name_or_path"),tdt.forEach(t),ghr=r(qC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=n(qC,"CODE",{});var adt=s(u_e);hhr=r(adt,"pretrained_model_name_or_path"),adt.forEach(t),phr=r(qC,":"),qC.forEach(t),_hr=i(Fa),b_e=n(Fa,"UL",{});var ndt=s(b_e);aF=n(ndt,"LI",{});var _Ae=s(aF);v_e=n(_Ae,"STRONG",{});var sdt=s(v_e);uhr=r(sdt,"vision-encoder-decoder"),sdt.forEach(t),bhr=r(_Ae," \u2014 "),rO=n(_Ae,"A",{href:!0});var ldt=s(rO);vhr=r(ldt,"TFVisionEncoderDecoderModel"),ldt.forEach(t),Thr=r(_Ae," (Vision Encoder decoder model)"),_Ae.forEach(t),ndt.forEach(t),Fhr=i(Fa),T_e=n(Fa,"P",{});var idt=s(T_e);Chr=r(idt,"Examples:"),idt.forEach(t),Mhr=i(Fa),m(aw.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),k8e=i(d),jc=n(d,"H2",{class:!0});var Dke=s(jc);nF=n(Dke,"A",{id:!0,class:!0,href:!0});var ddt=s(nF);F_e=n(ddt,"SPAN",{});var cdt=s(F_e);m(nw.$$.fragment,cdt),cdt.forEach(t),ddt.forEach(t),Ehr=i(Dke),C_e=n(Dke,"SPAN",{});var fdt=s(C_e);yhr=r(fdt,"TFAutoModelForSpeechSeq2Seq"),fdt.forEach(t),Dke.forEach(t),x8e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(sw.$$.fragment,Zl),whr=i(Zl),Nc=n(Zl,"P",{});var dV=s(Nc);Ahr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=n(dV,"CODE",{});var mdt=s(M_e);Lhr=r(mdt,"from_pretrained()"),mdt.forEach(t),Bhr=r(dV,"class method or the "),E_e=n(dV,"CODE",{});var gdt=s(E_e);khr=r(gdt,"from_config()"),gdt.forEach(t),xhr=r(dV,`class
method.`),dV.forEach(t),Rhr=i(Zl),lw=n(Zl,"P",{});var qke=s(lw);Shr=r(qke,"This class cannot be instantiated directly using "),y_e=n(qke,"CODE",{});var hdt=s(y_e);Phr=r(hdt,"__init__()"),hdt.forEach(t),$hr=r(qke," (throws an error)."),qke.forEach(t),Ihr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(iw.$$.fragment,ei),jhr=i(ei),w_e=n(ei,"P",{});var pdt=s(w_e);Nhr=r(pdt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),pdt.forEach(t),Dhr=i(ei),Dc=n(ei,"P",{});var cV=s(Dc);qhr=r(cV,`Note:
Loading a model from its configuration file does `),A_e=n(cV,"STRONG",{});var _dt=s(A_e);Ghr=r(_dt,"not"),_dt.forEach(t),Ohr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=n(cV,"CODE",{});var udt=s(L_e);Xhr=r(udt,"from_pretrained()"),udt.forEach(t),zhr=r(cV,"to load the model weights."),cV.forEach(t),Vhr=i(ei),B_e=n(ei,"P",{});var bdt=s(B_e);Whr=r(bdt,"Examples:"),bdt.forEach(t),Qhr=i(ei),m(dw.$$.fragment,ei),ei.forEach(t),Hhr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(cw.$$.fragment,Ca),Uhr=i(Ca),k_e=n(Ca,"P",{});var vdt=s(k_e);Jhr=r(vdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vdt.forEach(t),Yhr=i(Ca),Tn=n(Ca,"P",{});var GC=s(Tn);Khr=r(GC,"The model class to instantiate is selected based on the "),x_e=n(GC,"CODE",{});var Tdt=s(x_e);Zhr=r(Tdt,"model_type"),Tdt.forEach(t),epr=r(GC,` property of the config object (either
passed as an argument or loaded from `),R_e=n(GC,"CODE",{});var Fdt=s(R_e);opr=r(Fdt,"pretrained_model_name_or_path"),Fdt.forEach(t),rpr=r(GC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=n(GC,"CODE",{});var Cdt=s(S_e);tpr=r(Cdt,"pretrained_model_name_or_path"),Cdt.forEach(t),apr=r(GC,":"),GC.forEach(t),npr=i(Ca),P_e=n(Ca,"UL",{});var Mdt=s(P_e);sF=n(Mdt,"LI",{});var uAe=s(sF);$_e=n(uAe,"STRONG",{});var Edt=s($_e);spr=r(Edt,"speech_to_text"),Edt.forEach(t),lpr=r(uAe," \u2014 "),tO=n(uAe,"A",{href:!0});var ydt=s(tO);ipr=r(ydt,"TFSpeech2TextForConditionalGeneration"),ydt.forEach(t),dpr=r(uAe," (Speech2Text model)"),uAe.forEach(t),Mdt.forEach(t),cpr=i(Ca),I_e=n(Ca,"P",{});var wdt=s(I_e);fpr=r(wdt,"Examples:"),wdt.forEach(t),mpr=i(Ca),m(fw.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),R8e=i(d),qc=n(d,"H2",{class:!0});var Gke=s(qc);lF=n(Gke,"A",{id:!0,class:!0,href:!0});var Adt=s(lF);j_e=n(Adt,"SPAN",{});var Ldt=s(j_e);m(mw.$$.fragment,Ldt),Ldt.forEach(t),Adt.forEach(t),gpr=i(Gke),N_e=n(Gke,"SPAN",{});var Bdt=s(N_e);hpr=r(Bdt,"FlaxAutoModel"),Bdt.forEach(t),Gke.forEach(t),S8e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(gw.$$.fragment,oi),ppr=i(oi),Gc=n(oi,"P",{});var fV=s(Gc);_pr=r(fV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=n(fV,"CODE",{});var kdt=s(D_e);upr=r(kdt,"from_pretrained()"),kdt.forEach(t),bpr=r(fV,"class method or the "),q_e=n(fV,"CODE",{});var xdt=s(q_e);vpr=r(xdt,"from_config()"),xdt.forEach(t),Tpr=r(fV,`class
method.`),fV.forEach(t),Fpr=i(oi),hw=n(oi,"P",{});var Oke=s(hw);Cpr=r(Oke,"This class cannot be instantiated directly using "),G_e=n(Oke,"CODE",{});var Rdt=s(G_e);Mpr=r(Rdt,"__init__()"),Rdt.forEach(t),Epr=r(Oke," (throws an error)."),Oke.forEach(t),ypr=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(pw.$$.fragment,ri),wpr=i(ri),O_e=n(ri,"P",{});var Sdt=s(O_e);Apr=r(Sdt,"Instantiates one of the base model classes of the library from a configuration."),Sdt.forEach(t),Lpr=i(ri),Oc=n(ri,"P",{});var mV=s(Oc);Bpr=r(mV,`Note:
Loading a model from its configuration file does `),X_e=n(mV,"STRONG",{});var Pdt=s(X_e);kpr=r(Pdt,"not"),Pdt.forEach(t),xpr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=n(mV,"CODE",{});var $dt=s(z_e);Rpr=r($dt,"from_pretrained()"),$dt.forEach(t),Spr=r(mV,"to load the model weights."),mV.forEach(t),Ppr=i(ri),V_e=n(ri,"P",{});var Idt=s(V_e);$pr=r(Idt,"Examples:"),Idt.forEach(t),Ipr=i(ri),m(_w.$$.fragment,ri),ri.forEach(t),jpr=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(uw.$$.fragment,Ma),Npr=i(Ma),W_e=n(Ma,"P",{});var jdt=s(W_e);Dpr=r(jdt,"Instantiate one of the base model classes of the library from a pretrained model."),jdt.forEach(t),qpr=i(Ma),Fn=n(Ma,"P",{});var OC=s(Fn);Gpr=r(OC,"The model class to instantiate is selected based on the "),Q_e=n(OC,"CODE",{});var Ndt=s(Q_e);Opr=r(Ndt,"model_type"),Ndt.forEach(t),Xpr=r(OC,` property of the config object (either
passed as an argument or loaded from `),H_e=n(OC,"CODE",{});var Ddt=s(H_e);zpr=r(Ddt,"pretrained_model_name_or_path"),Ddt.forEach(t),Vpr=r(OC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=n(OC,"CODE",{});var qdt=s(U_e);Wpr=r(qdt,"pretrained_model_name_or_path"),qdt.forEach(t),Qpr=r(OC,":"),OC.forEach(t),Hpr=i(Ma),V=n(Ma,"UL",{});var Q=s(V);iF=n(Q,"LI",{});var bAe=s(iF);J_e=n(bAe,"STRONG",{});var Gdt=s(J_e);Upr=r(Gdt,"albert"),Gdt.forEach(t),Jpr=r(bAe," \u2014 "),aO=n(bAe,"A",{href:!0});var Odt=s(aO);Ypr=r(Odt,"FlaxAlbertModel"),Odt.forEach(t),Kpr=r(bAe," (ALBERT model)"),bAe.forEach(t),Zpr=i(Q),dF=n(Q,"LI",{});var vAe=s(dF);Y_e=n(vAe,"STRONG",{});var Xdt=s(Y_e);e_r=r(Xdt,"bart"),Xdt.forEach(t),o_r=r(vAe," \u2014 "),nO=n(vAe,"A",{href:!0});var zdt=s(nO);r_r=r(zdt,"FlaxBartModel"),zdt.forEach(t),t_r=r(vAe," (BART model)"),vAe.forEach(t),a_r=i(Q),cF=n(Q,"LI",{});var TAe=s(cF);K_e=n(TAe,"STRONG",{});var Vdt=s(K_e);n_r=r(Vdt,"beit"),Vdt.forEach(t),s_r=r(TAe," \u2014 "),sO=n(TAe,"A",{href:!0});var Wdt=s(sO);l_r=r(Wdt,"FlaxBeitModel"),Wdt.forEach(t),i_r=r(TAe," (BEiT model)"),TAe.forEach(t),d_r=i(Q),fF=n(Q,"LI",{});var FAe=s(fF);Z_e=n(FAe,"STRONG",{});var Qdt=s(Z_e);c_r=r(Qdt,"bert"),Qdt.forEach(t),f_r=r(FAe," \u2014 "),lO=n(FAe,"A",{href:!0});var Hdt=s(lO);m_r=r(Hdt,"FlaxBertModel"),Hdt.forEach(t),g_r=r(FAe," (BERT model)"),FAe.forEach(t),h_r=i(Q),mF=n(Q,"LI",{});var CAe=s(mF);eue=n(CAe,"STRONG",{});var Udt=s(eue);p_r=r(Udt,"big_bird"),Udt.forEach(t),__r=r(CAe," \u2014 "),iO=n(CAe,"A",{href:!0});var Jdt=s(iO);u_r=r(Jdt,"FlaxBigBirdModel"),Jdt.forEach(t),b_r=r(CAe," (BigBird model)"),CAe.forEach(t),v_r=i(Q),gF=n(Q,"LI",{});var MAe=s(gF);oue=n(MAe,"STRONG",{});var Ydt=s(oue);T_r=r(Ydt,"blenderbot"),Ydt.forEach(t),F_r=r(MAe," \u2014 "),dO=n(MAe,"A",{href:!0});var Kdt=s(dO);C_r=r(Kdt,"FlaxBlenderbotModel"),Kdt.forEach(t),M_r=r(MAe," (Blenderbot model)"),MAe.forEach(t),E_r=i(Q),hF=n(Q,"LI",{});var EAe=s(hF);rue=n(EAe,"STRONG",{});var Zdt=s(rue);y_r=r(Zdt,"blenderbot-small"),Zdt.forEach(t),w_r=r(EAe," \u2014 "),cO=n(EAe,"A",{href:!0});var ect=s(cO);A_r=r(ect,"FlaxBlenderbotSmallModel"),ect.forEach(t),L_r=r(EAe," (BlenderbotSmall model)"),EAe.forEach(t),B_r=i(Q),pF=n(Q,"LI",{});var yAe=s(pF);tue=n(yAe,"STRONG",{});var oct=s(tue);k_r=r(oct,"clip"),oct.forEach(t),x_r=r(yAe," \u2014 "),fO=n(yAe,"A",{href:!0});var rct=s(fO);R_r=r(rct,"FlaxCLIPModel"),rct.forEach(t),S_r=r(yAe," (CLIP model)"),yAe.forEach(t),P_r=i(Q),_F=n(Q,"LI",{});var wAe=s(_F);aue=n(wAe,"STRONG",{});var tct=s(aue);$_r=r(tct,"distilbert"),tct.forEach(t),I_r=r(wAe," \u2014 "),mO=n(wAe,"A",{href:!0});var act=s(mO);j_r=r(act,"FlaxDistilBertModel"),act.forEach(t),N_r=r(wAe," (DistilBERT model)"),wAe.forEach(t),D_r=i(Q),uF=n(Q,"LI",{});var AAe=s(uF);nue=n(AAe,"STRONG",{});var nct=s(nue);q_r=r(nct,"electra"),nct.forEach(t),G_r=r(AAe," \u2014 "),gO=n(AAe,"A",{href:!0});var sct=s(gO);O_r=r(sct,"FlaxElectraModel"),sct.forEach(t),X_r=r(AAe," (ELECTRA model)"),AAe.forEach(t),z_r=i(Q),bF=n(Q,"LI",{});var LAe=s(bF);sue=n(LAe,"STRONG",{});var lct=s(sue);V_r=r(lct,"gpt2"),lct.forEach(t),W_r=r(LAe," \u2014 "),hO=n(LAe,"A",{href:!0});var ict=s(hO);Q_r=r(ict,"FlaxGPT2Model"),ict.forEach(t),H_r=r(LAe," (OpenAI GPT-2 model)"),LAe.forEach(t),U_r=i(Q),vF=n(Q,"LI",{});var BAe=s(vF);lue=n(BAe,"STRONG",{});var dct=s(lue);J_r=r(dct,"gpt_neo"),dct.forEach(t),Y_r=r(BAe," \u2014 "),pO=n(BAe,"A",{href:!0});var cct=s(pO);K_r=r(cct,"FlaxGPTNeoModel"),cct.forEach(t),Z_r=r(BAe," (GPT Neo model)"),BAe.forEach(t),eur=i(Q),TF=n(Q,"LI",{});var kAe=s(TF);iue=n(kAe,"STRONG",{});var fct=s(iue);our=r(fct,"gptj"),fct.forEach(t),rur=r(kAe," \u2014 "),_O=n(kAe,"A",{href:!0});var mct=s(_O);tur=r(mct,"FlaxGPTJModel"),mct.forEach(t),aur=r(kAe," (GPT-J model)"),kAe.forEach(t),nur=i(Q),FF=n(Q,"LI",{});var xAe=s(FF);due=n(xAe,"STRONG",{});var gct=s(due);sur=r(gct,"marian"),gct.forEach(t),lur=r(xAe," \u2014 "),uO=n(xAe,"A",{href:!0});var hct=s(uO);iur=r(hct,"FlaxMarianModel"),hct.forEach(t),dur=r(xAe," (Marian model)"),xAe.forEach(t),cur=i(Q),CF=n(Q,"LI",{});var RAe=s(CF);cue=n(RAe,"STRONG",{});var pct=s(cue);fur=r(pct,"mbart"),pct.forEach(t),mur=r(RAe," \u2014 "),bO=n(RAe,"A",{href:!0});var _ct=s(bO);gur=r(_ct,"FlaxMBartModel"),_ct.forEach(t),hur=r(RAe," (mBART model)"),RAe.forEach(t),pur=i(Q),MF=n(Q,"LI",{});var SAe=s(MF);fue=n(SAe,"STRONG",{});var uct=s(fue);_ur=r(uct,"mt5"),uct.forEach(t),uur=r(SAe," \u2014 "),vO=n(SAe,"A",{href:!0});var bct=s(vO);bur=r(bct,"FlaxMT5Model"),bct.forEach(t),vur=r(SAe," (mT5 model)"),SAe.forEach(t),Tur=i(Q),EF=n(Q,"LI",{});var PAe=s(EF);mue=n(PAe,"STRONG",{});var vct=s(mue);Fur=r(vct,"pegasus"),vct.forEach(t),Cur=r(PAe," \u2014 "),TO=n(PAe,"A",{href:!0});var Tct=s(TO);Mur=r(Tct,"FlaxPegasusModel"),Tct.forEach(t),Eur=r(PAe," (Pegasus model)"),PAe.forEach(t),yur=i(Q),yF=n(Q,"LI",{});var $Ae=s(yF);gue=n($Ae,"STRONG",{});var Fct=s(gue);wur=r(Fct,"roberta"),Fct.forEach(t),Aur=r($Ae," \u2014 "),FO=n($Ae,"A",{href:!0});var Cct=s(FO);Lur=r(Cct,"FlaxRobertaModel"),Cct.forEach(t),Bur=r($Ae," (RoBERTa model)"),$Ae.forEach(t),kur=i(Q),wF=n(Q,"LI",{});var IAe=s(wF);hue=n(IAe,"STRONG",{});var Mct=s(hue);xur=r(Mct,"roformer"),Mct.forEach(t),Rur=r(IAe," \u2014 "),CO=n(IAe,"A",{href:!0});var Ect=s(CO);Sur=r(Ect,"FlaxRoFormerModel"),Ect.forEach(t),Pur=r(IAe," (RoFormer model)"),IAe.forEach(t),$ur=i(Q),AF=n(Q,"LI",{});var jAe=s(AF);pue=n(jAe,"STRONG",{});var yct=s(pue);Iur=r(yct,"t5"),yct.forEach(t),jur=r(jAe," \u2014 "),MO=n(jAe,"A",{href:!0});var wct=s(MO);Nur=r(wct,"FlaxT5Model"),wct.forEach(t),Dur=r(jAe," (T5 model)"),jAe.forEach(t),qur=i(Q),LF=n(Q,"LI",{});var NAe=s(LF);_ue=n(NAe,"STRONG",{});var Act=s(_ue);Gur=r(Act,"vision-text-dual-encoder"),Act.forEach(t),Our=r(NAe," \u2014 "),EO=n(NAe,"A",{href:!0});var Lct=s(EO);Xur=r(Lct,"FlaxVisionTextDualEncoderModel"),Lct.forEach(t),zur=r(NAe," (VisionTextDualEncoder model)"),NAe.forEach(t),Vur=i(Q),BF=n(Q,"LI",{});var DAe=s(BF);uue=n(DAe,"STRONG",{});var Bct=s(uue);Wur=r(Bct,"vit"),Bct.forEach(t),Qur=r(DAe," \u2014 "),yO=n(DAe,"A",{href:!0});var kct=s(yO);Hur=r(kct,"FlaxViTModel"),kct.forEach(t),Uur=r(DAe," (ViT model)"),DAe.forEach(t),Jur=i(Q),kF=n(Q,"LI",{});var qAe=s(kF);bue=n(qAe,"STRONG",{});var xct=s(bue);Yur=r(xct,"wav2vec2"),xct.forEach(t),Kur=r(qAe," \u2014 "),wO=n(qAe,"A",{href:!0});var Rct=s(wO);Zur=r(Rct,"FlaxWav2Vec2Model"),Rct.forEach(t),e1r=r(qAe," (Wav2Vec2 model)"),qAe.forEach(t),o1r=i(Q),xF=n(Q,"LI",{});var GAe=s(xF);vue=n(GAe,"STRONG",{});var Sct=s(vue);r1r=r(Sct,"xglm"),Sct.forEach(t),t1r=r(GAe," \u2014 "),AO=n(GAe,"A",{href:!0});var Pct=s(AO);a1r=r(Pct,"FlaxXGLMModel"),Pct.forEach(t),n1r=r(GAe," (XGLM model)"),GAe.forEach(t),Q.forEach(t),s1r=i(Ma),Tue=n(Ma,"P",{});var $ct=s(Tue);l1r=r($ct,"Examples:"),$ct.forEach(t),i1r=i(Ma),m(bw.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),P8e=i(d),Xc=n(d,"H2",{class:!0});var Xke=s(Xc);RF=n(Xke,"A",{id:!0,class:!0,href:!0});var Ict=s(RF);Fue=n(Ict,"SPAN",{});var jct=s(Fue);m(vw.$$.fragment,jct),jct.forEach(t),Ict.forEach(t),d1r=i(Xke),Cue=n(Xke,"SPAN",{});var Nct=s(Cue);c1r=r(Nct,"FlaxAutoModelForCausalLM"),Nct.forEach(t),Xke.forEach(t),$8e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(Tw.$$.fragment,ti),f1r=i(ti),zc=n(ti,"P",{});var gV=s(zc);m1r=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=n(gV,"CODE",{});var Dct=s(Mue);g1r=r(Dct,"from_pretrained()"),Dct.forEach(t),h1r=r(gV,"class method or the "),Eue=n(gV,"CODE",{});var qct=s(Eue);p1r=r(qct,"from_config()"),qct.forEach(t),_1r=r(gV,`class
method.`),gV.forEach(t),u1r=i(ti),Fw=n(ti,"P",{});var zke=s(Fw);b1r=r(zke,"This class cannot be instantiated directly using "),yue=n(zke,"CODE",{});var Gct=s(yue);v1r=r(Gct,"__init__()"),Gct.forEach(t),T1r=r(zke," (throws an error)."),zke.forEach(t),F1r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(Cw.$$.fragment,ai),C1r=i(ai),wue=n(ai,"P",{});var Oct=s(wue);M1r=r(Oct,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Oct.forEach(t),E1r=i(ai),Vc=n(ai,"P",{});var hV=s(Vc);y1r=r(hV,`Note:
Loading a model from its configuration file does `),Aue=n(hV,"STRONG",{});var Xct=s(Aue);w1r=r(Xct,"not"),Xct.forEach(t),A1r=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=n(hV,"CODE",{});var zct=s(Lue);L1r=r(zct,"from_pretrained()"),zct.forEach(t),B1r=r(hV,"to load the model weights."),hV.forEach(t),k1r=i(ai),Bue=n(ai,"P",{});var Vct=s(Bue);x1r=r(Vct,"Examples:"),Vct.forEach(t),R1r=i(ai),m(Mw.$$.fragment,ai),ai.forEach(t),S1r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(Ew.$$.fragment,Ea),P1r=i(Ea),kue=n(Ea,"P",{});var Wct=s(kue);$1r=r(Wct,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Wct.forEach(t),I1r=i(Ea),Cn=n(Ea,"P",{});var XC=s(Cn);j1r=r(XC,"The model class to instantiate is selected based on the "),xue=n(XC,"CODE",{});var Qct=s(xue);N1r=r(Qct,"model_type"),Qct.forEach(t),D1r=r(XC,` property of the config object (either
passed as an argument or loaded from `),Rue=n(XC,"CODE",{});var Hct=s(Rue);q1r=r(Hct,"pretrained_model_name_or_path"),Hct.forEach(t),G1r=r(XC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=n(XC,"CODE",{});var Uct=s(Sue);O1r=r(Uct,"pretrained_model_name_or_path"),Uct.forEach(t),X1r=r(XC,":"),XC.forEach(t),z1r=i(Ea),Mn=n(Ea,"UL",{});var zC=s(Mn);SF=n(zC,"LI",{});var OAe=s(SF);Pue=n(OAe,"STRONG",{});var Jct=s(Pue);V1r=r(Jct,"gpt2"),Jct.forEach(t),W1r=r(OAe," \u2014 "),LO=n(OAe,"A",{href:!0});var Yct=s(LO);Q1r=r(Yct,"FlaxGPT2LMHeadModel"),Yct.forEach(t),H1r=r(OAe," (OpenAI GPT-2 model)"),OAe.forEach(t),U1r=i(zC),PF=n(zC,"LI",{});var XAe=s(PF);$ue=n(XAe,"STRONG",{});var Kct=s($ue);J1r=r(Kct,"gpt_neo"),Kct.forEach(t),Y1r=r(XAe," \u2014 "),BO=n(XAe,"A",{href:!0});var Zct=s(BO);K1r=r(Zct,"FlaxGPTNeoForCausalLM"),Zct.forEach(t),Z1r=r(XAe," (GPT Neo model)"),XAe.forEach(t),ebr=i(zC),$F=n(zC,"LI",{});var zAe=s($F);Iue=n(zAe,"STRONG",{});var eft=s(Iue);obr=r(eft,"gptj"),eft.forEach(t),rbr=r(zAe," \u2014 "),kO=n(zAe,"A",{href:!0});var oft=s(kO);tbr=r(oft,"FlaxGPTJForCausalLM"),oft.forEach(t),abr=r(zAe," (GPT-J model)"),zAe.forEach(t),nbr=i(zC),IF=n(zC,"LI",{});var VAe=s(IF);jue=n(VAe,"STRONG",{});var rft=s(jue);sbr=r(rft,"xglm"),rft.forEach(t),lbr=r(VAe," \u2014 "),xO=n(VAe,"A",{href:!0});var tft=s(xO);ibr=r(tft,"FlaxXGLMForCausalLM"),tft.forEach(t),dbr=r(VAe," (XGLM model)"),VAe.forEach(t),zC.forEach(t),cbr=i(Ea),Nue=n(Ea,"P",{});var aft=s(Nue);fbr=r(aft,"Examples:"),aft.forEach(t),mbr=i(Ea),m(yw.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),I8e=i(d),Wc=n(d,"H2",{class:!0});var Vke=s(Wc);jF=n(Vke,"A",{id:!0,class:!0,href:!0});var nft=s(jF);Due=n(nft,"SPAN",{});var sft=s(Due);m(ww.$$.fragment,sft),sft.forEach(t),nft.forEach(t),gbr=i(Vke),que=n(Vke,"SPAN",{});var lft=s(que);hbr=r(lft,"FlaxAutoModelForPreTraining"),lft.forEach(t),Vke.forEach(t),j8e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(Aw.$$.fragment,ni),pbr=i(ni),Qc=n(ni,"P",{});var pV=s(Qc);_br=r(pV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=n(pV,"CODE",{});var ift=s(Gue);ubr=r(ift,"from_pretrained()"),ift.forEach(t),bbr=r(pV,"class method or the "),Oue=n(pV,"CODE",{});var dft=s(Oue);vbr=r(dft,"from_config()"),dft.forEach(t),Tbr=r(pV,`class
method.`),pV.forEach(t),Fbr=i(ni),Lw=n(ni,"P",{});var Wke=s(Lw);Cbr=r(Wke,"This class cannot be instantiated directly using "),Xue=n(Wke,"CODE",{});var cft=s(Xue);Mbr=r(cft,"__init__()"),cft.forEach(t),Ebr=r(Wke," (throws an error)."),Wke.forEach(t),ybr=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(Bw.$$.fragment,si),wbr=i(si),zue=n(si,"P",{});var fft=s(zue);Abr=r(fft,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),fft.forEach(t),Lbr=i(si),Hc=n(si,"P",{});var _V=s(Hc);Bbr=r(_V,`Note:
Loading a model from its configuration file does `),Vue=n(_V,"STRONG",{});var mft=s(Vue);kbr=r(mft,"not"),mft.forEach(t),xbr=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=n(_V,"CODE",{});var gft=s(Wue);Rbr=r(gft,"from_pretrained()"),gft.forEach(t),Sbr=r(_V,"to load the model weights."),_V.forEach(t),Pbr=i(si),Que=n(si,"P",{});var hft=s(Que);$br=r(hft,"Examples:"),hft.forEach(t),Ibr=i(si),m(kw.$$.fragment,si),si.forEach(t),jbr=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m(xw.$$.fragment,ya),Nbr=i(ya),Hue=n(ya,"P",{});var pft=s(Hue);Dbr=r(pft,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),pft.forEach(t),qbr=i(ya),En=n(ya,"P",{});var VC=s(En);Gbr=r(VC,"The model class to instantiate is selected based on the "),Uue=n(VC,"CODE",{});var _ft=s(Uue);Obr=r(_ft,"model_type"),_ft.forEach(t),Xbr=r(VC,` property of the config object (either
passed as an argument or loaded from `),Jue=n(VC,"CODE",{});var uft=s(Jue);zbr=r(uft,"pretrained_model_name_or_path"),uft.forEach(t),Vbr=r(VC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=n(VC,"CODE",{});var bft=s(Yue);Wbr=r(bft,"pretrained_model_name_or_path"),bft.forEach(t),Qbr=r(VC,":"),VC.forEach(t),Hbr=i(ya),fe=n(ya,"UL",{});var _e=s(fe);NF=n(_e,"LI",{});var WAe=s(NF);Kue=n(WAe,"STRONG",{});var vft=s(Kue);Ubr=r(vft,"albert"),vft.forEach(t),Jbr=r(WAe," \u2014 "),RO=n(WAe,"A",{href:!0});var Tft=s(RO);Ybr=r(Tft,"FlaxAlbertForPreTraining"),Tft.forEach(t),Kbr=r(WAe," (ALBERT model)"),WAe.forEach(t),Zbr=i(_e),DF=n(_e,"LI",{});var QAe=s(DF);Zue=n(QAe,"STRONG",{});var Fft=s(Zue);e5r=r(Fft,"bart"),Fft.forEach(t),o5r=r(QAe," \u2014 "),SO=n(QAe,"A",{href:!0});var Cft=s(SO);r5r=r(Cft,"FlaxBartForConditionalGeneration"),Cft.forEach(t),t5r=r(QAe," (BART model)"),QAe.forEach(t),a5r=i(_e),qF=n(_e,"LI",{});var HAe=s(qF);e1e=n(HAe,"STRONG",{});var Mft=s(e1e);n5r=r(Mft,"bert"),Mft.forEach(t),s5r=r(HAe," \u2014 "),PO=n(HAe,"A",{href:!0});var Eft=s(PO);l5r=r(Eft,"FlaxBertForPreTraining"),Eft.forEach(t),i5r=r(HAe," (BERT model)"),HAe.forEach(t),d5r=i(_e),GF=n(_e,"LI",{});var UAe=s(GF);o1e=n(UAe,"STRONG",{});var yft=s(o1e);c5r=r(yft,"big_bird"),yft.forEach(t),f5r=r(UAe," \u2014 "),$O=n(UAe,"A",{href:!0});var wft=s($O);m5r=r(wft,"FlaxBigBirdForPreTraining"),wft.forEach(t),g5r=r(UAe," (BigBird model)"),UAe.forEach(t),h5r=i(_e),OF=n(_e,"LI",{});var JAe=s(OF);r1e=n(JAe,"STRONG",{});var Aft=s(r1e);p5r=r(Aft,"electra"),Aft.forEach(t),_5r=r(JAe," \u2014 "),IO=n(JAe,"A",{href:!0});var Lft=s(IO);u5r=r(Lft,"FlaxElectraForPreTraining"),Lft.forEach(t),b5r=r(JAe," (ELECTRA model)"),JAe.forEach(t),v5r=i(_e),XF=n(_e,"LI",{});var YAe=s(XF);t1e=n(YAe,"STRONG",{});var Bft=s(t1e);T5r=r(Bft,"mbart"),Bft.forEach(t),F5r=r(YAe," \u2014 "),jO=n(YAe,"A",{href:!0});var kft=s(jO);C5r=r(kft,"FlaxMBartForConditionalGeneration"),kft.forEach(t),M5r=r(YAe," (mBART model)"),YAe.forEach(t),E5r=i(_e),zF=n(_e,"LI",{});var KAe=s(zF);a1e=n(KAe,"STRONG",{});var xft=s(a1e);y5r=r(xft,"mt5"),xft.forEach(t),w5r=r(KAe," \u2014 "),NO=n(KAe,"A",{href:!0});var Rft=s(NO);A5r=r(Rft,"FlaxMT5ForConditionalGeneration"),Rft.forEach(t),L5r=r(KAe," (mT5 model)"),KAe.forEach(t),B5r=i(_e),VF=n(_e,"LI",{});var ZAe=s(VF);n1e=n(ZAe,"STRONG",{});var Sft=s(n1e);k5r=r(Sft,"roberta"),Sft.forEach(t),x5r=r(ZAe," \u2014 "),DO=n(ZAe,"A",{href:!0});var Pft=s(DO);R5r=r(Pft,"FlaxRobertaForMaskedLM"),Pft.forEach(t),S5r=r(ZAe," (RoBERTa model)"),ZAe.forEach(t),P5r=i(_e),WF=n(_e,"LI",{});var e0e=s(WF);s1e=n(e0e,"STRONG",{});var $ft=s(s1e);$5r=r($ft,"roformer"),$ft.forEach(t),I5r=r(e0e," \u2014 "),qO=n(e0e,"A",{href:!0});var Ift=s(qO);j5r=r(Ift,"FlaxRoFormerForMaskedLM"),Ift.forEach(t),N5r=r(e0e," (RoFormer model)"),e0e.forEach(t),D5r=i(_e),QF=n(_e,"LI",{});var o0e=s(QF);l1e=n(o0e,"STRONG",{});var jft=s(l1e);q5r=r(jft,"t5"),jft.forEach(t),G5r=r(o0e," \u2014 "),GO=n(o0e,"A",{href:!0});var Nft=s(GO);O5r=r(Nft,"FlaxT5ForConditionalGeneration"),Nft.forEach(t),X5r=r(o0e," (T5 model)"),o0e.forEach(t),z5r=i(_e),HF=n(_e,"LI",{});var r0e=s(HF);i1e=n(r0e,"STRONG",{});var Dft=s(i1e);V5r=r(Dft,"wav2vec2"),Dft.forEach(t),W5r=r(r0e," \u2014 "),OO=n(r0e,"A",{href:!0});var qft=s(OO);Q5r=r(qft,"FlaxWav2Vec2ForPreTraining"),qft.forEach(t),H5r=r(r0e," (Wav2Vec2 model)"),r0e.forEach(t),_e.forEach(t),U5r=i(ya),d1e=n(ya,"P",{});var Gft=s(d1e);J5r=r(Gft,"Examples:"),Gft.forEach(t),Y5r=i(ya),m(Rw.$$.fragment,ya),ya.forEach(t),ni.forEach(t),N8e=i(d),Uc=n(d,"H2",{class:!0});var Qke=s(Uc);UF=n(Qke,"A",{id:!0,class:!0,href:!0});var Oft=s(UF);c1e=n(Oft,"SPAN",{});var Xft=s(c1e);m(Sw.$$.fragment,Xft),Xft.forEach(t),Oft.forEach(t),K5r=i(Qke),f1e=n(Qke,"SPAN",{});var zft=s(f1e);Z5r=r(zft,"FlaxAutoModelForMaskedLM"),zft.forEach(t),Qke.forEach(t),D8e=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(Pw.$$.fragment,li),e2r=i(li),Jc=n(li,"P",{});var uV=s(Jc);o2r=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m1e=n(uV,"CODE",{});var Vft=s(m1e);r2r=r(Vft,"from_pretrained()"),Vft.forEach(t),t2r=r(uV,"class method or the "),g1e=n(uV,"CODE",{});var Wft=s(g1e);a2r=r(Wft,"from_config()"),Wft.forEach(t),n2r=r(uV,`class
method.`),uV.forEach(t),s2r=i(li),$w=n(li,"P",{});var Hke=s($w);l2r=r(Hke,"This class cannot be instantiated directly using "),h1e=n(Hke,"CODE",{});var Qft=s(h1e);i2r=r(Qft,"__init__()"),Qft.forEach(t),d2r=r(Hke," (throws an error)."),Hke.forEach(t),c2r=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(Iw.$$.fragment,ii),f2r=i(ii),p1e=n(ii,"P",{});var Hft=s(p1e);m2r=r(Hft,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Hft.forEach(t),g2r=i(ii),Yc=n(ii,"P",{});var bV=s(Yc);h2r=r(bV,`Note:
Loading a model from its configuration file does `),_1e=n(bV,"STRONG",{});var Uft=s(_1e);p2r=r(Uft,"not"),Uft.forEach(t),_2r=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u1e=n(bV,"CODE",{});var Jft=s(u1e);u2r=r(Jft,"from_pretrained()"),Jft.forEach(t),b2r=r(bV,"to load the model weights."),bV.forEach(t),v2r=i(ii),b1e=n(ii,"P",{});var Yft=s(b1e);T2r=r(Yft,"Examples:"),Yft.forEach(t),F2r=i(ii),m(jw.$$.fragment,ii),ii.forEach(t),C2r=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(Nw.$$.fragment,wa),M2r=i(wa),v1e=n(wa,"P",{});var Kft=s(v1e);E2r=r(Kft,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Kft.forEach(t),y2r=i(wa),yn=n(wa,"P",{});var WC=s(yn);w2r=r(WC,"The model class to instantiate is selected based on the "),T1e=n(WC,"CODE",{});var Zft=s(T1e);A2r=r(Zft,"model_type"),Zft.forEach(t),L2r=r(WC,` property of the config object (either
passed as an argument or loaded from `),F1e=n(WC,"CODE",{});var emt=s(F1e);B2r=r(emt,"pretrained_model_name_or_path"),emt.forEach(t),k2r=r(WC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C1e=n(WC,"CODE",{});var omt=s(C1e);x2r=r(omt,"pretrained_model_name_or_path"),omt.forEach(t),R2r=r(WC,":"),WC.forEach(t),S2r=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);JF=n(Ze,"LI",{});var t0e=s(JF);M1e=n(t0e,"STRONG",{});var rmt=s(M1e);P2r=r(rmt,"albert"),rmt.forEach(t),$2r=r(t0e," \u2014 "),XO=n(t0e,"A",{href:!0});var tmt=s(XO);I2r=r(tmt,"FlaxAlbertForMaskedLM"),tmt.forEach(t),j2r=r(t0e," (ALBERT model)"),t0e.forEach(t),N2r=i(Ze),YF=n(Ze,"LI",{});var a0e=s(YF);E1e=n(a0e,"STRONG",{});var amt=s(E1e);D2r=r(amt,"bart"),amt.forEach(t),q2r=r(a0e," \u2014 "),zO=n(a0e,"A",{href:!0});var nmt=s(zO);G2r=r(nmt,"FlaxBartForConditionalGeneration"),nmt.forEach(t),O2r=r(a0e," (BART model)"),a0e.forEach(t),X2r=i(Ze),KF=n(Ze,"LI",{});var n0e=s(KF);y1e=n(n0e,"STRONG",{});var smt=s(y1e);z2r=r(smt,"bert"),smt.forEach(t),V2r=r(n0e," \u2014 "),VO=n(n0e,"A",{href:!0});var lmt=s(VO);W2r=r(lmt,"FlaxBertForMaskedLM"),lmt.forEach(t),Q2r=r(n0e," (BERT model)"),n0e.forEach(t),H2r=i(Ze),ZF=n(Ze,"LI",{});var s0e=s(ZF);w1e=n(s0e,"STRONG",{});var imt=s(w1e);U2r=r(imt,"big_bird"),imt.forEach(t),J2r=r(s0e," \u2014 "),WO=n(s0e,"A",{href:!0});var dmt=s(WO);Y2r=r(dmt,"FlaxBigBirdForMaskedLM"),dmt.forEach(t),K2r=r(s0e," (BigBird model)"),s0e.forEach(t),Z2r=i(Ze),e9=n(Ze,"LI",{});var l0e=s(e9);A1e=n(l0e,"STRONG",{});var cmt=s(A1e);evr=r(cmt,"distilbert"),cmt.forEach(t),ovr=r(l0e," \u2014 "),QO=n(l0e,"A",{href:!0});var fmt=s(QO);rvr=r(fmt,"FlaxDistilBertForMaskedLM"),fmt.forEach(t),tvr=r(l0e," (DistilBERT model)"),l0e.forEach(t),avr=i(Ze),o9=n(Ze,"LI",{});var i0e=s(o9);L1e=n(i0e,"STRONG",{});var mmt=s(L1e);nvr=r(mmt,"electra"),mmt.forEach(t),svr=r(i0e," \u2014 "),HO=n(i0e,"A",{href:!0});var gmt=s(HO);lvr=r(gmt,"FlaxElectraForMaskedLM"),gmt.forEach(t),ivr=r(i0e," (ELECTRA model)"),i0e.forEach(t),dvr=i(Ze),r9=n(Ze,"LI",{});var d0e=s(r9);B1e=n(d0e,"STRONG",{});var hmt=s(B1e);cvr=r(hmt,"mbart"),hmt.forEach(t),fvr=r(d0e," \u2014 "),UO=n(d0e,"A",{href:!0});var pmt=s(UO);mvr=r(pmt,"FlaxMBartForConditionalGeneration"),pmt.forEach(t),gvr=r(d0e," (mBART model)"),d0e.forEach(t),hvr=i(Ze),t9=n(Ze,"LI",{});var c0e=s(t9);k1e=n(c0e,"STRONG",{});var _mt=s(k1e);pvr=r(_mt,"roberta"),_mt.forEach(t),_vr=r(c0e," \u2014 "),JO=n(c0e,"A",{href:!0});var umt=s(JO);uvr=r(umt,"FlaxRobertaForMaskedLM"),umt.forEach(t),bvr=r(c0e," (RoBERTa model)"),c0e.forEach(t),vvr=i(Ze),a9=n(Ze,"LI",{});var f0e=s(a9);x1e=n(f0e,"STRONG",{});var bmt=s(x1e);Tvr=r(bmt,"roformer"),bmt.forEach(t),Fvr=r(f0e," \u2014 "),YO=n(f0e,"A",{href:!0});var vmt=s(YO);Cvr=r(vmt,"FlaxRoFormerForMaskedLM"),vmt.forEach(t),Mvr=r(f0e," (RoFormer model)"),f0e.forEach(t),Ze.forEach(t),Evr=i(wa),R1e=n(wa,"P",{});var Tmt=s(R1e);yvr=r(Tmt,"Examples:"),Tmt.forEach(t),wvr=i(wa),m(Dw.$$.fragment,wa),wa.forEach(t),li.forEach(t),q8e=i(d),Kc=n(d,"H2",{class:!0});var Uke=s(Kc);n9=n(Uke,"A",{id:!0,class:!0,href:!0});var Fmt=s(n9);S1e=n(Fmt,"SPAN",{});var Cmt=s(S1e);m(qw.$$.fragment,Cmt),Cmt.forEach(t),Fmt.forEach(t),Avr=i(Uke),P1e=n(Uke,"SPAN",{});var Mmt=s(P1e);Lvr=r(Mmt,"FlaxAutoModelForSeq2SeqLM"),Mmt.forEach(t),Uke.forEach(t),G8e=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(Gw.$$.fragment,di),Bvr=i(di),Zc=n(di,"P",{});var vV=s(Zc);kvr=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$1e=n(vV,"CODE",{});var Emt=s($1e);xvr=r(Emt,"from_pretrained()"),Emt.forEach(t),Rvr=r(vV,"class method or the "),I1e=n(vV,"CODE",{});var ymt=s(I1e);Svr=r(ymt,"from_config()"),ymt.forEach(t),Pvr=r(vV,`class
method.`),vV.forEach(t),$vr=i(di),Ow=n(di,"P",{});var Jke=s(Ow);Ivr=r(Jke,"This class cannot be instantiated directly using "),j1e=n(Jke,"CODE",{});var wmt=s(j1e);jvr=r(wmt,"__init__()"),wmt.forEach(t),Nvr=r(Jke," (throws an error)."),Jke.forEach(t),Dvr=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(Xw.$$.fragment,ci),qvr=i(ci),N1e=n(ci,"P",{});var Amt=s(N1e);Gvr=r(Amt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Amt.forEach(t),Ovr=i(ci),ef=n(ci,"P",{});var TV=s(ef);Xvr=r(TV,`Note:
Loading a model from its configuration file does `),D1e=n(TV,"STRONG",{});var Lmt=s(D1e);zvr=r(Lmt,"not"),Lmt.forEach(t),Vvr=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),q1e=n(TV,"CODE",{});var Bmt=s(q1e);Wvr=r(Bmt,"from_pretrained()"),Bmt.forEach(t),Qvr=r(TV,"to load the model weights."),TV.forEach(t),Hvr=i(ci),G1e=n(ci,"P",{});var kmt=s(G1e);Uvr=r(kmt,"Examples:"),kmt.forEach(t),Jvr=i(ci),m(zw.$$.fragment,ci),ci.forEach(t),Yvr=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(Vw.$$.fragment,Aa),Kvr=i(Aa),O1e=n(Aa,"P",{});var xmt=s(O1e);Zvr=r(xmt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),xmt.forEach(t),e6r=i(Aa),wn=n(Aa,"P",{});var QC=s(wn);o6r=r(QC,"The model class to instantiate is selected based on the "),X1e=n(QC,"CODE",{});var Rmt=s(X1e);r6r=r(Rmt,"model_type"),Rmt.forEach(t),t6r=r(QC,` property of the config object (either
passed as an argument or loaded from `),z1e=n(QC,"CODE",{});var Smt=s(z1e);a6r=r(Smt,"pretrained_model_name_or_path"),Smt.forEach(t),n6r=r(QC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V1e=n(QC,"CODE",{});var Pmt=s(V1e);s6r=r(Pmt,"pretrained_model_name_or_path"),Pmt.forEach(t),l6r=r(QC,":"),QC.forEach(t),i6r=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);s9=n(eo,"LI",{});var m0e=s(s9);W1e=n(m0e,"STRONG",{});var $mt=s(W1e);d6r=r($mt,"bart"),$mt.forEach(t),c6r=r(m0e," \u2014 "),KO=n(m0e,"A",{href:!0});var Imt=s(KO);f6r=r(Imt,"FlaxBartForConditionalGeneration"),Imt.forEach(t),m6r=r(m0e," (BART model)"),m0e.forEach(t),g6r=i(eo),l9=n(eo,"LI",{});var g0e=s(l9);Q1e=n(g0e,"STRONG",{});var jmt=s(Q1e);h6r=r(jmt,"blenderbot"),jmt.forEach(t),p6r=r(g0e," \u2014 "),ZO=n(g0e,"A",{href:!0});var Nmt=s(ZO);_6r=r(Nmt,"FlaxBlenderbotForConditionalGeneration"),Nmt.forEach(t),u6r=r(g0e," (Blenderbot model)"),g0e.forEach(t),b6r=i(eo),i9=n(eo,"LI",{});var h0e=s(i9);H1e=n(h0e,"STRONG",{});var Dmt=s(H1e);v6r=r(Dmt,"blenderbot-small"),Dmt.forEach(t),T6r=r(h0e," \u2014 "),eX=n(h0e,"A",{href:!0});var qmt=s(eX);F6r=r(qmt,"FlaxBlenderbotSmallForConditionalGeneration"),qmt.forEach(t),C6r=r(h0e," (BlenderbotSmall model)"),h0e.forEach(t),M6r=i(eo),d9=n(eo,"LI",{});var p0e=s(d9);U1e=n(p0e,"STRONG",{});var Gmt=s(U1e);E6r=r(Gmt,"encoder-decoder"),Gmt.forEach(t),y6r=r(p0e," \u2014 "),oX=n(p0e,"A",{href:!0});var Omt=s(oX);w6r=r(Omt,"FlaxEncoderDecoderModel"),Omt.forEach(t),A6r=r(p0e," (Encoder decoder model)"),p0e.forEach(t),L6r=i(eo),c9=n(eo,"LI",{});var _0e=s(c9);J1e=n(_0e,"STRONG",{});var Xmt=s(J1e);B6r=r(Xmt,"marian"),Xmt.forEach(t),k6r=r(_0e," \u2014 "),rX=n(_0e,"A",{href:!0});var zmt=s(rX);x6r=r(zmt,"FlaxMarianMTModel"),zmt.forEach(t),R6r=r(_0e," (Marian model)"),_0e.forEach(t),S6r=i(eo),f9=n(eo,"LI",{});var u0e=s(f9);Y1e=n(u0e,"STRONG",{});var Vmt=s(Y1e);P6r=r(Vmt,"mbart"),Vmt.forEach(t),$6r=r(u0e," \u2014 "),tX=n(u0e,"A",{href:!0});var Wmt=s(tX);I6r=r(Wmt,"FlaxMBartForConditionalGeneration"),Wmt.forEach(t),j6r=r(u0e," (mBART model)"),u0e.forEach(t),N6r=i(eo),m9=n(eo,"LI",{});var b0e=s(m9);K1e=n(b0e,"STRONG",{});var Qmt=s(K1e);D6r=r(Qmt,"mt5"),Qmt.forEach(t),q6r=r(b0e," \u2014 "),aX=n(b0e,"A",{href:!0});var Hmt=s(aX);G6r=r(Hmt,"FlaxMT5ForConditionalGeneration"),Hmt.forEach(t),O6r=r(b0e," (mT5 model)"),b0e.forEach(t),X6r=i(eo),g9=n(eo,"LI",{});var v0e=s(g9);Z1e=n(v0e,"STRONG",{});var Umt=s(Z1e);z6r=r(Umt,"pegasus"),Umt.forEach(t),V6r=r(v0e," \u2014 "),nX=n(v0e,"A",{href:!0});var Jmt=s(nX);W6r=r(Jmt,"FlaxPegasusForConditionalGeneration"),Jmt.forEach(t),Q6r=r(v0e," (Pegasus model)"),v0e.forEach(t),H6r=i(eo),h9=n(eo,"LI",{});var T0e=s(h9);ebe=n(T0e,"STRONG",{});var Ymt=s(ebe);U6r=r(Ymt,"t5"),Ymt.forEach(t),J6r=r(T0e," \u2014 "),sX=n(T0e,"A",{href:!0});var Kmt=s(sX);Y6r=r(Kmt,"FlaxT5ForConditionalGeneration"),Kmt.forEach(t),K6r=r(T0e," (T5 model)"),T0e.forEach(t),eo.forEach(t),Z6r=i(Aa),obe=n(Aa,"P",{});var Zmt=s(obe);eTr=r(Zmt,"Examples:"),Zmt.forEach(t),oTr=i(Aa),m(Ww.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),O8e=i(d),of=n(d,"H2",{class:!0});var Yke=s(of);p9=n(Yke,"A",{id:!0,class:!0,href:!0});var egt=s(p9);rbe=n(egt,"SPAN",{});var ogt=s(rbe);m(Qw.$$.fragment,ogt),ogt.forEach(t),egt.forEach(t),rTr=i(Yke),tbe=n(Yke,"SPAN",{});var rgt=s(tbe);tTr=r(rgt,"FlaxAutoModelForSequenceClassification"),rgt.forEach(t),Yke.forEach(t),X8e=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(Hw.$$.fragment,fi),aTr=i(fi),rf=n(fi,"P",{});var FV=s(rf);nTr=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),abe=n(FV,"CODE",{});var tgt=s(abe);sTr=r(tgt,"from_pretrained()"),tgt.forEach(t),lTr=r(FV,"class method or the "),nbe=n(FV,"CODE",{});var agt=s(nbe);iTr=r(agt,"from_config()"),agt.forEach(t),dTr=r(FV,`class
method.`),FV.forEach(t),cTr=i(fi),Uw=n(fi,"P",{});var Kke=s(Uw);fTr=r(Kke,"This class cannot be instantiated directly using "),sbe=n(Kke,"CODE",{});var ngt=s(sbe);mTr=r(ngt,"__init__()"),ngt.forEach(t),gTr=r(Kke," (throws an error)."),Kke.forEach(t),hTr=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(Jw.$$.fragment,mi),pTr=i(mi),lbe=n(mi,"P",{});var sgt=s(lbe);_Tr=r(sgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),sgt.forEach(t),uTr=i(mi),tf=n(mi,"P",{});var CV=s(tf);bTr=r(CV,`Note:
Loading a model from its configuration file does `),ibe=n(CV,"STRONG",{});var lgt=s(ibe);vTr=r(lgt,"not"),lgt.forEach(t),TTr=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),dbe=n(CV,"CODE",{});var igt=s(dbe);FTr=r(igt,"from_pretrained()"),igt.forEach(t),CTr=r(CV,"to load the model weights."),CV.forEach(t),MTr=i(mi),cbe=n(mi,"P",{});var dgt=s(cbe);ETr=r(dgt,"Examples:"),dgt.forEach(t),yTr=i(mi),m(Yw.$$.fragment,mi),mi.forEach(t),wTr=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(Kw.$$.fragment,La),ATr=i(La),fbe=n(La,"P",{});var cgt=s(fbe);LTr=r(cgt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),cgt.forEach(t),BTr=i(La),An=n(La,"P",{});var HC=s(An);kTr=r(HC,"The model class to instantiate is selected based on the "),mbe=n(HC,"CODE",{});var fgt=s(mbe);xTr=r(fgt,"model_type"),fgt.forEach(t),RTr=r(HC,` property of the config object (either
passed as an argument or loaded from `),gbe=n(HC,"CODE",{});var mgt=s(gbe);STr=r(mgt,"pretrained_model_name_or_path"),mgt.forEach(t),PTr=r(HC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hbe=n(HC,"CODE",{});var ggt=s(hbe);$Tr=r(ggt,"pretrained_model_name_or_path"),ggt.forEach(t),ITr=r(HC,":"),HC.forEach(t),jTr=i(La),Fe=n(La,"UL",{});var oo=s(Fe);_9=n(oo,"LI",{});var F0e=s(_9);pbe=n(F0e,"STRONG",{});var hgt=s(pbe);NTr=r(hgt,"albert"),hgt.forEach(t),DTr=r(F0e," \u2014 "),lX=n(F0e,"A",{href:!0});var pgt=s(lX);qTr=r(pgt,"FlaxAlbertForSequenceClassification"),pgt.forEach(t),GTr=r(F0e," (ALBERT model)"),F0e.forEach(t),OTr=i(oo),u9=n(oo,"LI",{});var C0e=s(u9);_be=n(C0e,"STRONG",{});var _gt=s(_be);XTr=r(_gt,"bart"),_gt.forEach(t),zTr=r(C0e," \u2014 "),iX=n(C0e,"A",{href:!0});var ugt=s(iX);VTr=r(ugt,"FlaxBartForSequenceClassification"),ugt.forEach(t),WTr=r(C0e," (BART model)"),C0e.forEach(t),QTr=i(oo),b9=n(oo,"LI",{});var M0e=s(b9);ube=n(M0e,"STRONG",{});var bgt=s(ube);HTr=r(bgt,"bert"),bgt.forEach(t),UTr=r(M0e," \u2014 "),dX=n(M0e,"A",{href:!0});var vgt=s(dX);JTr=r(vgt,"FlaxBertForSequenceClassification"),vgt.forEach(t),YTr=r(M0e," (BERT model)"),M0e.forEach(t),KTr=i(oo),v9=n(oo,"LI",{});var E0e=s(v9);bbe=n(E0e,"STRONG",{});var Tgt=s(bbe);ZTr=r(Tgt,"big_bird"),Tgt.forEach(t),e7r=r(E0e," \u2014 "),cX=n(E0e,"A",{href:!0});var Fgt=s(cX);o7r=r(Fgt,"FlaxBigBirdForSequenceClassification"),Fgt.forEach(t),r7r=r(E0e," (BigBird model)"),E0e.forEach(t),t7r=i(oo),T9=n(oo,"LI",{});var y0e=s(T9);vbe=n(y0e,"STRONG",{});var Cgt=s(vbe);a7r=r(Cgt,"distilbert"),Cgt.forEach(t),n7r=r(y0e," \u2014 "),fX=n(y0e,"A",{href:!0});var Mgt=s(fX);s7r=r(Mgt,"FlaxDistilBertForSequenceClassification"),Mgt.forEach(t),l7r=r(y0e," (DistilBERT model)"),y0e.forEach(t),i7r=i(oo),F9=n(oo,"LI",{});var w0e=s(F9);Tbe=n(w0e,"STRONG",{});var Egt=s(Tbe);d7r=r(Egt,"electra"),Egt.forEach(t),c7r=r(w0e," \u2014 "),mX=n(w0e,"A",{href:!0});var ygt=s(mX);f7r=r(ygt,"FlaxElectraForSequenceClassification"),ygt.forEach(t),m7r=r(w0e," (ELECTRA model)"),w0e.forEach(t),g7r=i(oo),C9=n(oo,"LI",{});var A0e=s(C9);Fbe=n(A0e,"STRONG",{});var wgt=s(Fbe);h7r=r(wgt,"mbart"),wgt.forEach(t),p7r=r(A0e," \u2014 "),gX=n(A0e,"A",{href:!0});var Agt=s(gX);_7r=r(Agt,"FlaxMBartForSequenceClassification"),Agt.forEach(t),u7r=r(A0e," (mBART model)"),A0e.forEach(t),b7r=i(oo),M9=n(oo,"LI",{});var L0e=s(M9);Cbe=n(L0e,"STRONG",{});var Lgt=s(Cbe);v7r=r(Lgt,"roberta"),Lgt.forEach(t),T7r=r(L0e," \u2014 "),hX=n(L0e,"A",{href:!0});var Bgt=s(hX);F7r=r(Bgt,"FlaxRobertaForSequenceClassification"),Bgt.forEach(t),C7r=r(L0e," (RoBERTa model)"),L0e.forEach(t),M7r=i(oo),E9=n(oo,"LI",{});var B0e=s(E9);Mbe=n(B0e,"STRONG",{});var kgt=s(Mbe);E7r=r(kgt,"roformer"),kgt.forEach(t),y7r=r(B0e," \u2014 "),pX=n(B0e,"A",{href:!0});var xgt=s(pX);w7r=r(xgt,"FlaxRoFormerForSequenceClassification"),xgt.forEach(t),A7r=r(B0e," (RoFormer model)"),B0e.forEach(t),oo.forEach(t),L7r=i(La),Ebe=n(La,"P",{});var Rgt=s(Ebe);B7r=r(Rgt,"Examples:"),Rgt.forEach(t),k7r=i(La),m(Zw.$$.fragment,La),La.forEach(t),fi.forEach(t),z8e=i(d),af=n(d,"H2",{class:!0});var Zke=s(af);y9=n(Zke,"A",{id:!0,class:!0,href:!0});var Sgt=s(y9);ybe=n(Sgt,"SPAN",{});var Pgt=s(ybe);m(eA.$$.fragment,Pgt),Pgt.forEach(t),Sgt.forEach(t),x7r=i(Zke),wbe=n(Zke,"SPAN",{});var $gt=s(wbe);R7r=r($gt,"FlaxAutoModelForQuestionAnswering"),$gt.forEach(t),Zke.forEach(t),V8e=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(oA.$$.fragment,gi),S7r=i(gi),nf=n(gi,"P",{});var MV=s(nf);P7r=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Abe=n(MV,"CODE",{});var Igt=s(Abe);$7r=r(Igt,"from_pretrained()"),Igt.forEach(t),I7r=r(MV,"class method or the "),Lbe=n(MV,"CODE",{});var jgt=s(Lbe);j7r=r(jgt,"from_config()"),jgt.forEach(t),N7r=r(MV,`class
method.`),MV.forEach(t),D7r=i(gi),rA=n(gi,"P",{});var exe=s(rA);q7r=r(exe,"This class cannot be instantiated directly using "),Bbe=n(exe,"CODE",{});var Ngt=s(Bbe);G7r=r(Ngt,"__init__()"),Ngt.forEach(t),O7r=r(exe," (throws an error)."),exe.forEach(t),X7r=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(tA.$$.fragment,hi),z7r=i(hi),kbe=n(hi,"P",{});var Dgt=s(kbe);V7r=r(Dgt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Dgt.forEach(t),W7r=i(hi),sf=n(hi,"P",{});var EV=s(sf);Q7r=r(EV,`Note:
Loading a model from its configuration file does `),xbe=n(EV,"STRONG",{});var qgt=s(xbe);H7r=r(qgt,"not"),qgt.forEach(t),U7r=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Rbe=n(EV,"CODE",{});var Ggt=s(Rbe);J7r=r(Ggt,"from_pretrained()"),Ggt.forEach(t),Y7r=r(EV,"to load the model weights."),EV.forEach(t),K7r=i(hi),Sbe=n(hi,"P",{});var Ogt=s(Sbe);Z7r=r(Ogt,"Examples:"),Ogt.forEach(t),eFr=i(hi),m(aA.$$.fragment,hi),hi.forEach(t),oFr=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(nA.$$.fragment,Ba),rFr=i(Ba),Pbe=n(Ba,"P",{});var Xgt=s(Pbe);tFr=r(Xgt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Xgt.forEach(t),aFr=i(Ba),Ln=n(Ba,"P",{});var UC=s(Ln);nFr=r(UC,"The model class to instantiate is selected based on the "),$be=n(UC,"CODE",{});var zgt=s($be);sFr=r(zgt,"model_type"),zgt.forEach(t),lFr=r(UC,` property of the config object (either
passed as an argument or loaded from `),Ibe=n(UC,"CODE",{});var Vgt=s(Ibe);iFr=r(Vgt,"pretrained_model_name_or_path"),Vgt.forEach(t),dFr=r(UC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jbe=n(UC,"CODE",{});var Wgt=s(jbe);cFr=r(Wgt,"pretrained_model_name_or_path"),Wgt.forEach(t),fFr=r(UC,":"),UC.forEach(t),mFr=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);w9=n(ro,"LI",{});var k0e=s(w9);Nbe=n(k0e,"STRONG",{});var Qgt=s(Nbe);gFr=r(Qgt,"albert"),Qgt.forEach(t),hFr=r(k0e," \u2014 "),_X=n(k0e,"A",{href:!0});var Hgt=s(_X);pFr=r(Hgt,"FlaxAlbertForQuestionAnswering"),Hgt.forEach(t),_Fr=r(k0e," (ALBERT model)"),k0e.forEach(t),uFr=i(ro),A9=n(ro,"LI",{});var x0e=s(A9);Dbe=n(x0e,"STRONG",{});var Ugt=s(Dbe);bFr=r(Ugt,"bart"),Ugt.forEach(t),vFr=r(x0e," \u2014 "),uX=n(x0e,"A",{href:!0});var Jgt=s(uX);TFr=r(Jgt,"FlaxBartForQuestionAnswering"),Jgt.forEach(t),FFr=r(x0e," (BART model)"),x0e.forEach(t),CFr=i(ro),L9=n(ro,"LI",{});var R0e=s(L9);qbe=n(R0e,"STRONG",{});var Ygt=s(qbe);MFr=r(Ygt,"bert"),Ygt.forEach(t),EFr=r(R0e," \u2014 "),bX=n(R0e,"A",{href:!0});var Kgt=s(bX);yFr=r(Kgt,"FlaxBertForQuestionAnswering"),Kgt.forEach(t),wFr=r(R0e," (BERT model)"),R0e.forEach(t),AFr=i(ro),B9=n(ro,"LI",{});var S0e=s(B9);Gbe=n(S0e,"STRONG",{});var Zgt=s(Gbe);LFr=r(Zgt,"big_bird"),Zgt.forEach(t),BFr=r(S0e," \u2014 "),vX=n(S0e,"A",{href:!0});var eht=s(vX);kFr=r(eht,"FlaxBigBirdForQuestionAnswering"),eht.forEach(t),xFr=r(S0e," (BigBird model)"),S0e.forEach(t),RFr=i(ro),k9=n(ro,"LI",{});var P0e=s(k9);Obe=n(P0e,"STRONG",{});var oht=s(Obe);SFr=r(oht,"distilbert"),oht.forEach(t),PFr=r(P0e," \u2014 "),TX=n(P0e,"A",{href:!0});var rht=s(TX);$Fr=r(rht,"FlaxDistilBertForQuestionAnswering"),rht.forEach(t),IFr=r(P0e," (DistilBERT model)"),P0e.forEach(t),jFr=i(ro),x9=n(ro,"LI",{});var $0e=s(x9);Xbe=n($0e,"STRONG",{});var tht=s(Xbe);NFr=r(tht,"electra"),tht.forEach(t),DFr=r($0e," \u2014 "),FX=n($0e,"A",{href:!0});var aht=s(FX);qFr=r(aht,"FlaxElectraForQuestionAnswering"),aht.forEach(t),GFr=r($0e," (ELECTRA model)"),$0e.forEach(t),OFr=i(ro),R9=n(ro,"LI",{});var I0e=s(R9);zbe=n(I0e,"STRONG",{});var nht=s(zbe);XFr=r(nht,"mbart"),nht.forEach(t),zFr=r(I0e," \u2014 "),CX=n(I0e,"A",{href:!0});var sht=s(CX);VFr=r(sht,"FlaxMBartForQuestionAnswering"),sht.forEach(t),WFr=r(I0e," (mBART model)"),I0e.forEach(t),QFr=i(ro),S9=n(ro,"LI",{});var j0e=s(S9);Vbe=n(j0e,"STRONG",{});var lht=s(Vbe);HFr=r(lht,"roberta"),lht.forEach(t),UFr=r(j0e," \u2014 "),MX=n(j0e,"A",{href:!0});var iht=s(MX);JFr=r(iht,"FlaxRobertaForQuestionAnswering"),iht.forEach(t),YFr=r(j0e," (RoBERTa model)"),j0e.forEach(t),KFr=i(ro),P9=n(ro,"LI",{});var N0e=s(P9);Wbe=n(N0e,"STRONG",{});var dht=s(Wbe);ZFr=r(dht,"roformer"),dht.forEach(t),e9r=r(N0e," \u2014 "),EX=n(N0e,"A",{href:!0});var cht=s(EX);o9r=r(cht,"FlaxRoFormerForQuestionAnswering"),cht.forEach(t),r9r=r(N0e," (RoFormer model)"),N0e.forEach(t),ro.forEach(t),t9r=i(Ba),Qbe=n(Ba,"P",{});var fht=s(Qbe);a9r=r(fht,"Examples:"),fht.forEach(t),n9r=i(Ba),m(sA.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),W8e=i(d),lf=n(d,"H2",{class:!0});var oxe=s(lf);$9=n(oxe,"A",{id:!0,class:!0,href:!0});var mht=s($9);Hbe=n(mht,"SPAN",{});var ght=s(Hbe);m(lA.$$.fragment,ght),ght.forEach(t),mht.forEach(t),s9r=i(oxe),Ube=n(oxe,"SPAN",{});var hht=s(Ube);l9r=r(hht,"FlaxAutoModelForTokenClassification"),hht.forEach(t),oxe.forEach(t),Q8e=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(iA.$$.fragment,pi),i9r=i(pi),df=n(pi,"P",{});var yV=s(df);d9r=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Jbe=n(yV,"CODE",{});var pht=s(Jbe);c9r=r(pht,"from_pretrained()"),pht.forEach(t),f9r=r(yV,"class method or the "),Ybe=n(yV,"CODE",{});var _ht=s(Ybe);m9r=r(_ht,"from_config()"),_ht.forEach(t),g9r=r(yV,`class
method.`),yV.forEach(t),h9r=i(pi),dA=n(pi,"P",{});var rxe=s(dA);p9r=r(rxe,"This class cannot be instantiated directly using "),Kbe=n(rxe,"CODE",{});var uht=s(Kbe);_9r=r(uht,"__init__()"),uht.forEach(t),u9r=r(rxe," (throws an error)."),rxe.forEach(t),b9r=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(cA.$$.fragment,_i),v9r=i(_i),Zbe=n(_i,"P",{});var bht=s(Zbe);T9r=r(bht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),bht.forEach(t),F9r=i(_i),cf=n(_i,"P",{});var wV=s(cf);C9r=r(wV,`Note:
Loading a model from its configuration file does `),e5e=n(wV,"STRONG",{});var vht=s(e5e);M9r=r(vht,"not"),vht.forEach(t),E9r=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=n(wV,"CODE",{});var Tht=s(o5e);y9r=r(Tht,"from_pretrained()"),Tht.forEach(t),w9r=r(wV,"to load the model weights."),wV.forEach(t),A9r=i(_i),r5e=n(_i,"P",{});var Fht=s(r5e);L9r=r(Fht,"Examples:"),Fht.forEach(t),B9r=i(_i),m(fA.$$.fragment,_i),_i.forEach(t),k9r=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(mA.$$.fragment,ka),x9r=i(ka),t5e=n(ka,"P",{});var Cht=s(t5e);R9r=r(Cht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Cht.forEach(t),S9r=i(ka),Bn=n(ka,"P",{});var JC=s(Bn);P9r=r(JC,"The model class to instantiate is selected based on the "),a5e=n(JC,"CODE",{});var Mht=s(a5e);$9r=r(Mht,"model_type"),Mht.forEach(t),I9r=r(JC,` property of the config object (either
passed as an argument or loaded from `),n5e=n(JC,"CODE",{});var Eht=s(n5e);j9r=r(Eht,"pretrained_model_name_or_path"),Eht.forEach(t),N9r=r(JC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=n(JC,"CODE",{});var yht=s(s5e);D9r=r(yht,"pretrained_model_name_or_path"),yht.forEach(t),q9r=r(JC,":"),JC.forEach(t),G9r=i(ka),so=n(ka,"UL",{});var ta=s(so);I9=n(ta,"LI",{});var D0e=s(I9);l5e=n(D0e,"STRONG",{});var wht=s(l5e);O9r=r(wht,"albert"),wht.forEach(t),X9r=r(D0e," \u2014 "),yX=n(D0e,"A",{href:!0});var Aht=s(yX);z9r=r(Aht,"FlaxAlbertForTokenClassification"),Aht.forEach(t),V9r=r(D0e," (ALBERT model)"),D0e.forEach(t),W9r=i(ta),j9=n(ta,"LI",{});var q0e=s(j9);i5e=n(q0e,"STRONG",{});var Lht=s(i5e);Q9r=r(Lht,"bert"),Lht.forEach(t),H9r=r(q0e," \u2014 "),wX=n(q0e,"A",{href:!0});var Bht=s(wX);U9r=r(Bht,"FlaxBertForTokenClassification"),Bht.forEach(t),J9r=r(q0e," (BERT model)"),q0e.forEach(t),Y9r=i(ta),N9=n(ta,"LI",{});var G0e=s(N9);d5e=n(G0e,"STRONG",{});var kht=s(d5e);K9r=r(kht,"big_bird"),kht.forEach(t),Z9r=r(G0e," \u2014 "),AX=n(G0e,"A",{href:!0});var xht=s(AX);eCr=r(xht,"FlaxBigBirdForTokenClassification"),xht.forEach(t),oCr=r(G0e," (BigBird model)"),G0e.forEach(t),rCr=i(ta),D9=n(ta,"LI",{});var O0e=s(D9);c5e=n(O0e,"STRONG",{});var Rht=s(c5e);tCr=r(Rht,"distilbert"),Rht.forEach(t),aCr=r(O0e," \u2014 "),LX=n(O0e,"A",{href:!0});var Sht=s(LX);nCr=r(Sht,"FlaxDistilBertForTokenClassification"),Sht.forEach(t),sCr=r(O0e," (DistilBERT model)"),O0e.forEach(t),lCr=i(ta),q9=n(ta,"LI",{});var X0e=s(q9);f5e=n(X0e,"STRONG",{});var Pht=s(f5e);iCr=r(Pht,"electra"),Pht.forEach(t),dCr=r(X0e," \u2014 "),BX=n(X0e,"A",{href:!0});var $ht=s(BX);cCr=r($ht,"FlaxElectraForTokenClassification"),$ht.forEach(t),fCr=r(X0e," (ELECTRA model)"),X0e.forEach(t),mCr=i(ta),G9=n(ta,"LI",{});var z0e=s(G9);m5e=n(z0e,"STRONG",{});var Iht=s(m5e);gCr=r(Iht,"roberta"),Iht.forEach(t),hCr=r(z0e," \u2014 "),kX=n(z0e,"A",{href:!0});var jht=s(kX);pCr=r(jht,"FlaxRobertaForTokenClassification"),jht.forEach(t),_Cr=r(z0e," (RoBERTa model)"),z0e.forEach(t),uCr=i(ta),O9=n(ta,"LI",{});var V0e=s(O9);g5e=n(V0e,"STRONG",{});var Nht=s(g5e);bCr=r(Nht,"roformer"),Nht.forEach(t),vCr=r(V0e," \u2014 "),xX=n(V0e,"A",{href:!0});var Dht=s(xX);TCr=r(Dht,"FlaxRoFormerForTokenClassification"),Dht.forEach(t),FCr=r(V0e," (RoFormer model)"),V0e.forEach(t),ta.forEach(t),CCr=i(ka),h5e=n(ka,"P",{});var qht=s(h5e);MCr=r(qht,"Examples:"),qht.forEach(t),ECr=i(ka),m(gA.$$.fragment,ka),ka.forEach(t),pi.forEach(t),H8e=i(d),ff=n(d,"H2",{class:!0});var txe=s(ff);X9=n(txe,"A",{id:!0,class:!0,href:!0});var Ght=s(X9);p5e=n(Ght,"SPAN",{});var Oht=s(p5e);m(hA.$$.fragment,Oht),Oht.forEach(t),Ght.forEach(t),yCr=i(txe),_5e=n(txe,"SPAN",{});var Xht=s(_5e);wCr=r(Xht,"FlaxAutoModelForMultipleChoice"),Xht.forEach(t),txe.forEach(t),U8e=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(pA.$$.fragment,ui),ACr=i(ui),mf=n(ui,"P",{});var AV=s(mf);LCr=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),u5e=n(AV,"CODE",{});var zht=s(u5e);BCr=r(zht,"from_pretrained()"),zht.forEach(t),kCr=r(AV,"class method or the "),b5e=n(AV,"CODE",{});var Vht=s(b5e);xCr=r(Vht,"from_config()"),Vht.forEach(t),RCr=r(AV,`class
method.`),AV.forEach(t),SCr=i(ui),_A=n(ui,"P",{});var axe=s(_A);PCr=r(axe,"This class cannot be instantiated directly using "),v5e=n(axe,"CODE",{});var Wht=s(v5e);$Cr=r(Wht,"__init__()"),Wht.forEach(t),ICr=r(axe," (throws an error)."),axe.forEach(t),jCr=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(uA.$$.fragment,bi),NCr=i(bi),T5e=n(bi,"P",{});var Qht=s(T5e);DCr=r(Qht,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Qht.forEach(t),qCr=i(bi),gf=n(bi,"P",{});var LV=s(gf);GCr=r(LV,`Note:
Loading a model from its configuration file does `),F5e=n(LV,"STRONG",{});var Hht=s(F5e);OCr=r(Hht,"not"),Hht.forEach(t),XCr=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),C5e=n(LV,"CODE",{});var Uht=s(C5e);zCr=r(Uht,"from_pretrained()"),Uht.forEach(t),VCr=r(LV,"to load the model weights."),LV.forEach(t),WCr=i(bi),M5e=n(bi,"P",{});var Jht=s(M5e);QCr=r(Jht,"Examples:"),Jht.forEach(t),HCr=i(bi),m(bA.$$.fragment,bi),bi.forEach(t),UCr=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(vA.$$.fragment,xa),JCr=i(xa),E5e=n(xa,"P",{});var Yht=s(E5e);YCr=r(Yht,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Yht.forEach(t),KCr=i(xa),kn=n(xa,"P",{});var YC=s(kn);ZCr=r(YC,"The model class to instantiate is selected based on the "),y5e=n(YC,"CODE",{});var Kht=s(y5e);e4r=r(Kht,"model_type"),Kht.forEach(t),o4r=r(YC,` property of the config object (either
passed as an argument or loaded from `),w5e=n(YC,"CODE",{});var Zht=s(w5e);r4r=r(Zht,"pretrained_model_name_or_path"),Zht.forEach(t),t4r=r(YC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A5e=n(YC,"CODE",{});var ept=s(A5e);a4r=r(ept,"pretrained_model_name_or_path"),ept.forEach(t),n4r=r(YC,":"),YC.forEach(t),s4r=i(xa),lo=n(xa,"UL",{});var aa=s(lo);z9=n(aa,"LI",{});var W0e=s(z9);L5e=n(W0e,"STRONG",{});var opt=s(L5e);l4r=r(opt,"albert"),opt.forEach(t),i4r=r(W0e," \u2014 "),RX=n(W0e,"A",{href:!0});var rpt=s(RX);d4r=r(rpt,"FlaxAlbertForMultipleChoice"),rpt.forEach(t),c4r=r(W0e," (ALBERT model)"),W0e.forEach(t),f4r=i(aa),V9=n(aa,"LI",{});var Q0e=s(V9);B5e=n(Q0e,"STRONG",{});var tpt=s(B5e);m4r=r(tpt,"bert"),tpt.forEach(t),g4r=r(Q0e," \u2014 "),SX=n(Q0e,"A",{href:!0});var apt=s(SX);h4r=r(apt,"FlaxBertForMultipleChoice"),apt.forEach(t),p4r=r(Q0e," (BERT model)"),Q0e.forEach(t),_4r=i(aa),W9=n(aa,"LI",{});var H0e=s(W9);k5e=n(H0e,"STRONG",{});var npt=s(k5e);u4r=r(npt,"big_bird"),npt.forEach(t),b4r=r(H0e," \u2014 "),PX=n(H0e,"A",{href:!0});var spt=s(PX);v4r=r(spt,"FlaxBigBirdForMultipleChoice"),spt.forEach(t),T4r=r(H0e," (BigBird model)"),H0e.forEach(t),F4r=i(aa),Q9=n(aa,"LI",{});var U0e=s(Q9);x5e=n(U0e,"STRONG",{});var lpt=s(x5e);C4r=r(lpt,"distilbert"),lpt.forEach(t),M4r=r(U0e," \u2014 "),$X=n(U0e,"A",{href:!0});var ipt=s($X);E4r=r(ipt,"FlaxDistilBertForMultipleChoice"),ipt.forEach(t),y4r=r(U0e," (DistilBERT model)"),U0e.forEach(t),w4r=i(aa),H9=n(aa,"LI",{});var J0e=s(H9);R5e=n(J0e,"STRONG",{});var dpt=s(R5e);A4r=r(dpt,"electra"),dpt.forEach(t),L4r=r(J0e," \u2014 "),IX=n(J0e,"A",{href:!0});var cpt=s(IX);B4r=r(cpt,"FlaxElectraForMultipleChoice"),cpt.forEach(t),k4r=r(J0e," (ELECTRA model)"),J0e.forEach(t),x4r=i(aa),U9=n(aa,"LI",{});var Y0e=s(U9);S5e=n(Y0e,"STRONG",{});var fpt=s(S5e);R4r=r(fpt,"roberta"),fpt.forEach(t),S4r=r(Y0e," \u2014 "),jX=n(Y0e,"A",{href:!0});var mpt=s(jX);P4r=r(mpt,"FlaxRobertaForMultipleChoice"),mpt.forEach(t),$4r=r(Y0e," (RoBERTa model)"),Y0e.forEach(t),I4r=i(aa),J9=n(aa,"LI",{});var K0e=s(J9);P5e=n(K0e,"STRONG",{});var gpt=s(P5e);j4r=r(gpt,"roformer"),gpt.forEach(t),N4r=r(K0e," \u2014 "),NX=n(K0e,"A",{href:!0});var hpt=s(NX);D4r=r(hpt,"FlaxRoFormerForMultipleChoice"),hpt.forEach(t),q4r=r(K0e," (RoFormer model)"),K0e.forEach(t),aa.forEach(t),G4r=i(xa),$5e=n(xa,"P",{});var ppt=s($5e);O4r=r(ppt,"Examples:"),ppt.forEach(t),X4r=i(xa),m(TA.$$.fragment,xa),xa.forEach(t),ui.forEach(t),J8e=i(d),hf=n(d,"H2",{class:!0});var nxe=s(hf);Y9=n(nxe,"A",{id:!0,class:!0,href:!0});var _pt=s(Y9);I5e=n(_pt,"SPAN",{});var upt=s(I5e);m(FA.$$.fragment,upt),upt.forEach(t),_pt.forEach(t),z4r=i(nxe),j5e=n(nxe,"SPAN",{});var bpt=s(j5e);V4r=r(bpt,"FlaxAutoModelForNextSentencePrediction"),bpt.forEach(t),nxe.forEach(t),Y8e=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(CA.$$.fragment,vi),W4r=i(vi),pf=n(vi,"P",{});var BV=s(pf);Q4r=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),N5e=n(BV,"CODE",{});var vpt=s(N5e);H4r=r(vpt,"from_pretrained()"),vpt.forEach(t),U4r=r(BV,"class method or the "),D5e=n(BV,"CODE",{});var Tpt=s(D5e);J4r=r(Tpt,"from_config()"),Tpt.forEach(t),Y4r=r(BV,`class
method.`),BV.forEach(t),K4r=i(vi),MA=n(vi,"P",{});var sxe=s(MA);Z4r=r(sxe,"This class cannot be instantiated directly using "),q5e=n(sxe,"CODE",{});var Fpt=s(q5e);eMr=r(Fpt,"__init__()"),Fpt.forEach(t),oMr=r(sxe," (throws an error)."),sxe.forEach(t),rMr=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(EA.$$.fragment,Ti),tMr=i(Ti),G5e=n(Ti,"P",{});var Cpt=s(G5e);aMr=r(Cpt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Cpt.forEach(t),nMr=i(Ti),_f=n(Ti,"P",{});var kV=s(_f);sMr=r(kV,`Note:
Loading a model from its configuration file does `),O5e=n(kV,"STRONG",{});var Mpt=s(O5e);lMr=r(Mpt,"not"),Mpt.forEach(t),iMr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),X5e=n(kV,"CODE",{});var Ept=s(X5e);dMr=r(Ept,"from_pretrained()"),Ept.forEach(t),cMr=r(kV,"to load the model weights."),kV.forEach(t),fMr=i(Ti),z5e=n(Ti,"P",{});var ypt=s(z5e);mMr=r(ypt,"Examples:"),ypt.forEach(t),gMr=i(Ti),m(yA.$$.fragment,Ti),Ti.forEach(t),hMr=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(wA.$$.fragment,Ra),pMr=i(Ra),V5e=n(Ra,"P",{});var wpt=s(V5e);_Mr=r(wpt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),wpt.forEach(t),uMr=i(Ra),xn=n(Ra,"P",{});var KC=s(xn);bMr=r(KC,"The model class to instantiate is selected based on the "),W5e=n(KC,"CODE",{});var Apt=s(W5e);vMr=r(Apt,"model_type"),Apt.forEach(t),TMr=r(KC,` property of the config object (either
passed as an argument or loaded from `),Q5e=n(KC,"CODE",{});var Lpt=s(Q5e);FMr=r(Lpt,"pretrained_model_name_or_path"),Lpt.forEach(t),CMr=r(KC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H5e=n(KC,"CODE",{});var Bpt=s(H5e);MMr=r(Bpt,"pretrained_model_name_or_path"),Bpt.forEach(t),EMr=r(KC,":"),KC.forEach(t),yMr=i(Ra),U5e=n(Ra,"UL",{});var kpt=s(U5e);K9=n(kpt,"LI",{});var Z0e=s(K9);J5e=n(Z0e,"STRONG",{});var xpt=s(J5e);wMr=r(xpt,"bert"),xpt.forEach(t),AMr=r(Z0e," \u2014 "),DX=n(Z0e,"A",{href:!0});var Rpt=s(DX);LMr=r(Rpt,"FlaxBertForNextSentencePrediction"),Rpt.forEach(t),BMr=r(Z0e," (BERT model)"),Z0e.forEach(t),kpt.forEach(t),kMr=i(Ra),Y5e=n(Ra,"P",{});var Spt=s(Y5e);xMr=r(Spt,"Examples:"),Spt.forEach(t),RMr=i(Ra),m(AA.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),K8e=i(d),uf=n(d,"H2",{class:!0});var lxe=s(uf);Z9=n(lxe,"A",{id:!0,class:!0,href:!0});var Ppt=s(Z9);K5e=n(Ppt,"SPAN",{});var $pt=s(K5e);m(LA.$$.fragment,$pt),$pt.forEach(t),Ppt.forEach(t),SMr=i(lxe),Z5e=n(lxe,"SPAN",{});var Ipt=s(Z5e);PMr=r(Ipt,"FlaxAutoModelForImageClassification"),Ipt.forEach(t),lxe.forEach(t),Z8e=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(BA.$$.fragment,Fi),$Mr=i(Fi),bf=n(Fi,"P",{});var xV=s(bf);IMr=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),e2e=n(xV,"CODE",{});var jpt=s(e2e);jMr=r(jpt,"from_pretrained()"),jpt.forEach(t),NMr=r(xV,"class method or the "),o2e=n(xV,"CODE",{});var Npt=s(o2e);DMr=r(Npt,"from_config()"),Npt.forEach(t),qMr=r(xV,`class
method.`),xV.forEach(t),GMr=i(Fi),kA=n(Fi,"P",{});var ixe=s(kA);OMr=r(ixe,"This class cannot be instantiated directly using "),r2e=n(ixe,"CODE",{});var Dpt=s(r2e);XMr=r(Dpt,"__init__()"),Dpt.forEach(t),zMr=r(ixe," (throws an error)."),ixe.forEach(t),VMr=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m(xA.$$.fragment,Ci),WMr=i(Ci),t2e=n(Ci,"P",{});var qpt=s(t2e);QMr=r(qpt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qpt.forEach(t),HMr=i(Ci),vf=n(Ci,"P",{});var RV=s(vf);UMr=r(RV,`Note:
Loading a model from its configuration file does `),a2e=n(RV,"STRONG",{});var Gpt=s(a2e);JMr=r(Gpt,"not"),Gpt.forEach(t),YMr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),n2e=n(RV,"CODE",{});var Opt=s(n2e);KMr=r(Opt,"from_pretrained()"),Opt.forEach(t),ZMr=r(RV,"to load the model weights."),RV.forEach(t),eEr=i(Ci),s2e=n(Ci,"P",{});var Xpt=s(s2e);oEr=r(Xpt,"Examples:"),Xpt.forEach(t),rEr=i(Ci),m(RA.$$.fragment,Ci),Ci.forEach(t),tEr=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m(SA.$$.fragment,Sa),aEr=i(Sa),l2e=n(Sa,"P",{});var zpt=s(l2e);nEr=r(zpt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),zpt.forEach(t),sEr=i(Sa),Rn=n(Sa,"P",{});var ZC=s(Rn);lEr=r(ZC,"The model class to instantiate is selected based on the "),i2e=n(ZC,"CODE",{});var Vpt=s(i2e);iEr=r(Vpt,"model_type"),Vpt.forEach(t),dEr=r(ZC,` property of the config object (either
passed as an argument or loaded from `),d2e=n(ZC,"CODE",{});var Wpt=s(d2e);cEr=r(Wpt,"pretrained_model_name_or_path"),Wpt.forEach(t),fEr=r(ZC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c2e=n(ZC,"CODE",{});var Qpt=s(c2e);mEr=r(Qpt,"pretrained_model_name_or_path"),Qpt.forEach(t),gEr=r(ZC,":"),ZC.forEach(t),hEr=i(Sa),PA=n(Sa,"UL",{});var dxe=s(PA);eC=n(dxe,"LI",{});var eLe=s(eC);f2e=n(eLe,"STRONG",{});var Hpt=s(f2e);pEr=r(Hpt,"beit"),Hpt.forEach(t),_Er=r(eLe," \u2014 "),qX=n(eLe,"A",{href:!0});var Upt=s(qX);uEr=r(Upt,"FlaxBeitForImageClassification"),Upt.forEach(t),bEr=r(eLe," (BEiT model)"),eLe.forEach(t),vEr=i(dxe),oC=n(dxe,"LI",{});var oLe=s(oC);m2e=n(oLe,"STRONG",{});var Jpt=s(m2e);TEr=r(Jpt,"vit"),Jpt.forEach(t),FEr=r(oLe," \u2014 "),GX=n(oLe,"A",{href:!0});var Ypt=s(GX);CEr=r(Ypt,"FlaxViTForImageClassification"),Ypt.forEach(t),MEr=r(oLe," (ViT model)"),oLe.forEach(t),dxe.forEach(t),EEr=i(Sa),g2e=n(Sa,"P",{});var Kpt=s(g2e);yEr=r(Kpt,"Examples:"),Kpt.forEach(t),wEr=i(Sa),m($A.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),eBe=i(d),Tf=n(d,"H2",{class:!0});var cxe=s(Tf);rC=n(cxe,"A",{id:!0,class:!0,href:!0});var Zpt=s(rC);h2e=n(Zpt,"SPAN",{});var e_t=s(h2e);m(IA.$$.fragment,e_t),e_t.forEach(t),Zpt.forEach(t),AEr=i(cxe),p2e=n(cxe,"SPAN",{});var o_t=s(p2e);LEr=r(o_t,"FlaxAutoModelForVision2Seq"),o_t.forEach(t),cxe.forEach(t),oBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(jA.$$.fragment,Mi),BEr=i(Mi),Ff=n(Mi,"P",{});var SV=s(Ff);kEr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_2e=n(SV,"CODE",{});var r_t=s(_2e);xEr=r(r_t,"from_pretrained()"),r_t.forEach(t),REr=r(SV,"class method or the "),u2e=n(SV,"CODE",{});var t_t=s(u2e);SEr=r(t_t,"from_config()"),t_t.forEach(t),PEr=r(SV,`class
method.`),SV.forEach(t),$Er=i(Mi),NA=n(Mi,"P",{});var fxe=s(NA);IEr=r(fxe,"This class cannot be instantiated directly using "),b2e=n(fxe,"CODE",{});var a_t=s(b2e);jEr=r(a_t,"__init__()"),a_t.forEach(t),NEr=r(fxe," (throws an error)."),fxe.forEach(t),DEr=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(DA.$$.fragment,Ei),qEr=i(Ei),v2e=n(Ei,"P",{});var n_t=s(v2e);GEr=r(n_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),n_t.forEach(t),OEr=i(Ei),Cf=n(Ei,"P",{});var PV=s(Cf);XEr=r(PV,`Note:
Loading a model from its configuration file does `),T2e=n(PV,"STRONG",{});var s_t=s(T2e);zEr=r(s_t,"not"),s_t.forEach(t),VEr=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),F2e=n(PV,"CODE",{});var l_t=s(F2e);WEr=r(l_t,"from_pretrained()"),l_t.forEach(t),QEr=r(PV,"to load the model weights."),PV.forEach(t),HEr=i(Ei),C2e=n(Ei,"P",{});var i_t=s(C2e);UEr=r(i_t,"Examples:"),i_t.forEach(t),JEr=i(Ei),m(qA.$$.fragment,Ei),Ei.forEach(t),YEr=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(GA.$$.fragment,Pa),KEr=i(Pa),M2e=n(Pa,"P",{});var d_t=s(M2e);ZEr=r(d_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),d_t.forEach(t),e3r=i(Pa),Sn=n(Pa,"P",{});var e4=s(Sn);o3r=r(e4,"The model class to instantiate is selected based on the "),E2e=n(e4,"CODE",{});var c_t=s(E2e);r3r=r(c_t,"model_type"),c_t.forEach(t),t3r=r(e4,` property of the config object (either
passed as an argument or loaded from `),y2e=n(e4,"CODE",{});var f_t=s(y2e);a3r=r(f_t,"pretrained_model_name_or_path"),f_t.forEach(t),n3r=r(e4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w2e=n(e4,"CODE",{});var m_t=s(w2e);s3r=r(m_t,"pretrained_model_name_or_path"),m_t.forEach(t),l3r=r(e4,":"),e4.forEach(t),i3r=i(Pa),A2e=n(Pa,"UL",{});var g_t=s(A2e);tC=n(g_t,"LI",{});var rLe=s(tC);L2e=n(rLe,"STRONG",{});var h_t=s(L2e);d3r=r(h_t,"vision-encoder-decoder"),h_t.forEach(t),c3r=r(rLe," \u2014 "),OX=n(rLe,"A",{href:!0});var p_t=s(OX);f3r=r(p_t,"FlaxVisionEncoderDecoderModel"),p_t.forEach(t),m3r=r(rLe," (Vision Encoder decoder model)"),rLe.forEach(t),g_t.forEach(t),g3r=i(Pa),B2e=n(Pa,"P",{});var __t=s(B2e);h3r=r(__t,"Examples:"),__t.forEach(t),p3r=i(Pa),m(OA.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(y_t)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(V0,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(W0,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertConfig"),c(Q0,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartConfig"),c(H0,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitConfig"),c(U0,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertConfig"),c(J0,"href","/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(Y0,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdConfig"),c(K0,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(Z0,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(eL,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(oL,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertConfig"),c(rL,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineConfig"),c(tL,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPConfig"),c(aL,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertConfig"),c(nL,"href","/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextConfig"),c(sL,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLConfig"),c(lL,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaConfig"),c(iL,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(dL,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTConfig"),c(cL,"href","/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrConfig"),c(fL,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertConfig"),c(mL,"href","/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRConfig"),c(gL,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraConfig"),c(hL,"href","/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(pL,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertConfig"),c(_L,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetConfig"),c(uL,"href","/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTConfig"),c(bL,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelConfig"),c(vL,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Config"),c(TL,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(FL,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJConfig"),c(CL,"href","/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertConfig"),c(ML,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertConfig"),c(EL,"href","/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(yL,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(wL,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(AL,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDConfig"),c(LL,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerConfig"),c(BL,"href","/docs/transformers/pr_15796/en/model_doc/luke#transformers.LukeConfig"),c(kL,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertConfig"),c(xL,"href","/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Config"),c(RL,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianConfig"),c(SL,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartConfig"),c(PL,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c($L,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(IL,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetConfig"),c(jL,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Config"),c(NL,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(DL,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(qL,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusConfig"),c(GL,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverConfig"),c(OL,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartConfig"),c(XL,"href","/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(zL,"href","/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(VL,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(WL,"href","/docs/transformers/pr_15796/en/model_doc/rag#transformers.RagConfig"),c(QL,"href","/docs/transformers/pr_15796/en/model_doc/realm#transformers.RealmConfig"),c(HL,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerConfig"),c(UL,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertConfig"),c(JL,"href","/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertConfig"),c(YL,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaConfig"),c(KL,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerConfig"),c(ZL,"href","/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerConfig"),c(e8,"href","/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWConfig"),c(o8,"href","/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDConfig"),c(r8,"href","/docs/transformers/pr_15796/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(t8,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(a8,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(n8,"href","/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterConfig"),c(s8,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(l8,"href","/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinConfig"),c(i8,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Config"),c(d8,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasConfig"),c(c8,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(f8,"href","/docs/transformers/pr_15796/en/model_doc/trocr#transformers.TrOCRConfig"),c(m8,"href","/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(g8,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(h8,"href","/docs/transformers/pr_15796/en/model_doc/vilt#transformers.ViltConfig"),c(p8,"href","/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(_8,"href","/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(u8,"href","/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(b8,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTConfig"),c(v8,"href","/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(T8,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(F8,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMConfig"),c(C8,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMConfig"),c(M8,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMConfig"),c(E8,"href","/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(y8,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(w8,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(A8,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetConfig"),c(L8,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(hg,"class","docstring"),c(Go,"class","docstring"),c(pg,"id","transformers.AutoTokenizer"),c(pg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pg,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(B8,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(k8,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertTokenizer"),c(x8,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(R8,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartTokenizer"),c(S8,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartTokenizerFast"),c(P8,"href","/docs/transformers/pr_15796/en/model_doc/barthez#transformers.BarthezTokenizer"),c($8,"href","/docs/transformers/pr_15796/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(I8,"href","/docs/transformers/pr_15796/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(j8,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertTokenizer"),c(N8,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertTokenizerFast"),c(D8,"href","/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(q8,"href","/docs/transformers/pr_15796/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(G8,"href","/docs/transformers/pr_15796/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(O8,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(X8,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(z8,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(V8,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(W8,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(Q8,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(H8,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(U8,"href","/docs/transformers/pr_15796/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(J8,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertTokenizer"),c(Y8,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(K8,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineTokenizer"),c(Z8,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPTokenizer"),c(eB,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(oB,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(rB,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(tB,"href","/docs/transformers/pr_15796/en/model_doc/cpm#transformers.CpmTokenizer"),c(aB,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(nB,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaTokenizer"),c(sB,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(lB,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(iB,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(dB,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(cB,"href","/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(fB,"href","/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(mB,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraTokenizer"),c(gB,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(hB,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(pB,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetTokenizer"),c(_B,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(uB,"href","/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(bB,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelTokenizer"),c(vB,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(TB,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(FB,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(CB,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(MB,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(EB,"href","/docs/transformers/pr_15796/en/model_doc/herbert#transformers.HerbertTokenizer"),c(yB,"href","/docs/transformers/pr_15796/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(wB,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(AB,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaTokenizer"),c(LB,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(BB,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(kB,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(xB,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(RB,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(SB,"href","/docs/transformers/pr_15796/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(PB,"href","/docs/transformers/pr_15796/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c($B,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDTokenizer"),c(IB,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDTokenizerFast"),c(jB,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerTokenizer"),c(NB,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(DB,"href","/docs/transformers/pr_15796/en/model_doc/luke#transformers.LukeTokenizer"),c(qB,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(GB,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(OB,"href","/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(XB,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianTokenizer"),c(zB,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartTokenizer"),c(VB,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(WB,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(QB,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(HB,"href","/docs/transformers/pr_15796/en/model_doc/mluke#transformers.MLukeTokenizer"),c(UB,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(JB,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(YB,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(KB,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ZB,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.T5Tokenizer"),c(ek,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.T5TokenizerFast"),c(ok,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(rk,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(tk,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(ak,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(nk,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(sk,"href","/docs/transformers/pr_15796/en/model_doc/phobert#transformers.PhobertTokenizer"),c(lk,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartTokenizer"),c(ik,"href","/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(dk,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertTokenizer"),c(ck,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertTokenizerFast"),c(fk,"href","/docs/transformers/pr_15796/en/model_doc/rag#transformers.RagTokenizer"),c(mk,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerTokenizer"),c(gk,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(hk,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertTokenizer"),c(pk,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(_k,"href","/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(uk,"href","/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(bk,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaTokenizer"),c(vk,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Tk,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Fk,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Ck,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Mk,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ek,"href","/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterTokenizer"),c(yk,"href","/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(wk,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Ak,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Lk,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.T5Tokenizer"),c(Bk,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.T5TokenizerFast"),c(kk,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasTokenizer"),c(xk,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Rk,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Sk,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Pk,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMTokenizer"),c($k,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Ik,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMTokenizer"),c(jk,"href","/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Nk,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Dk,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(qk,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Gk,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Vg,"class","docstring"),c(Oo,"class","docstring"),c(Wg,"id","transformers.AutoFeatureExtractor"),c(Wg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Ok,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Xk,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(zk,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Vk,"href","/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Wk,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Qk,"href","/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Hk,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(ex,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ox,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(dh,"class","docstring"),c(Xo,"class","docstring"),c(ch,"id","transformers.AutoProcessor"),c(ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ch,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(ax,"href","/docs/transformers/pr_15796/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(nx,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPProcessor"),c(sx,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(lx,"href","/docs/transformers/pr_15796/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(ix,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(dx,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(cx,"href","/docs/transformers/pr_15796/en/model_doc/trocr#transformers.TrOCRProcessor"),c(fx,"href","/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(mx,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Th,"class","docstring"),c(zo,"class","docstring"),c(Fh,"id","transformers.AutoModel"),c(Fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fh,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(gx,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertModel"),c(hx,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartModel"),c(px,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitModel"),c(_x,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertModel"),c(ux,"href","/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(bx,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdModel"),c(vx,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Tx,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Fx,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Cx,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertModel"),c(Mx,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineModel"),c(Ex,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.CLIPModel"),c(yx,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertModel"),c(wx,"href","/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextModel"),c(Ax,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLModel"),c(Lx,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaModel"),c(Bx,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(kx,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTModel"),c(xx,"href","/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrModel"),c(Rx,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertModel"),c(Sx,"href","/docs/transformers/pr_15796/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Px,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraModel"),c($x,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ix,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetModel"),c(jx,"href","/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTModel"),c(Nx,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelModel"),c(Dx,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelBaseModel"),c(qx,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2Model"),c(Gx,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Ox,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJModel"),c(Xx,"href","/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertModel"),c(zx,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertModel"),c(Vx,"href","/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Wx,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Qx,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Hx,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDModel"),c(Ux,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerModel"),c(Jx,"href","/docs/transformers/pr_15796/en/model_doc/luke#transformers.LukeModel"),c(Yx,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertModel"),c(Kx,"href","/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100Model"),c(Zx,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianModel"),c(eR,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartModel"),c(oR,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(rR,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertModel"),c(tR,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetModel"),c(aR,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5Model"),c(nR,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerModel"),c(sR,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(lR,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusModel"),c(iR,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverModel"),c(dR,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartModel"),c(cR,"href","/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerModel"),c(fR,"href","/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(mR,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertModel"),c(gR,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerModel"),c(hR,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertModel"),c(pR,"href","/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertModel"),c(_R,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaModel"),c(uR,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerModel"),c(bR,"href","/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerModel"),c(vR,"href","/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWModel"),c(TR,"href","/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDModel"),c(FR,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(CR,"href","/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterModel"),c(MR,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(ER,"href","/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinModel"),c(yR,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5Model"),c(wR,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasModel"),c(AR,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(LR,"href","/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechModel"),c(BR,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(kR,"href","/docs/transformers/pr_15796/en/model_doc/vilt#transformers.ViltModel"),c(xR,"href","/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(RR,"href","/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertModel"),c(SR,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTModel"),c(PR,"href","/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEModel"),c($R,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(IR,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMModel"),c(jR,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMModel"),c(NR,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMModel"),c(DR,"href","/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(qR,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(GR,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(OR,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetModel"),c(XR,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(Kp,"id","transformers.AutoModelForPreTraining"),c(Kp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kp,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(zR,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForPreTraining"),c(VR,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WR,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForPreTraining"),c(QR,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(HR,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(UR,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(JR,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(YR,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(KR,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(ZR,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForPreTraining"),c(eS,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(oS,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForPreTraining"),c(rS,"href","/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(tS,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(aS,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(nS,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(sS,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(lS,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(iS,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(dS,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(cS,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(fS,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(mS,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(gS,"href","/docs/transformers/pr_15796/en/model_doc/retribert#transformers.RetriBertModel"),c(hS,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(pS,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(_S,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(uS,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(bS,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(vS,"href","/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(TS,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(FS,"href","/docs/transformers/pr_15796/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(CS,"href","/docs/transformers/pr_15796/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(MS,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(ES,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(yS,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(wS,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(AS,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(N_,"id","transformers.AutoModelForCausalLM"),c(N_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c(LS,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForCausalLM"),c(BS,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertLMHeadModel"),c(kS,"href","/docs/transformers/pr_15796/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(xS,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(RS,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(SS,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(PS,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c($S,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(IS,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(jS,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForCausalLM"),c(NS,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(DS,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(qS,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(GS,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianForCausalLM"),c(OS,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForCausalLM"),c(XS,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(zS,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(VS,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(WS,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(QS,"href","/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(HS,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(US,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(JS,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(YS,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(KS,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(ZS,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(eP,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(oP,"href","/docs/transformers/pr_15796/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(rP,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(tP,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(aP,"href","/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(nP,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(sP,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(lP,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Tu,"id","transformers.AutoModelForMaskedLM"),c(Tu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(iP,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(dP,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(cP,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForMaskedLM"),c(fP,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(mP,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(gP,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(hP,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(pP,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(_P,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(uP,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(bP,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(vP,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(TP,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(FP,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(CP,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(MP,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(EP,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(yP,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(wP,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(AP,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(LP,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(BP,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(kP,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(xP,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(RP,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(SP,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(PP,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c($P,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(IP,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(jP,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(NP,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(DP,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(qP,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(r1,"id","transformers.AutoModelForSeq2SeqLM"),c(r1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r1,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(GP,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(OP,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(XP,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(zP,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(VP,"href","/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(WP,"href","/docs/transformers/pr_15796/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(QP,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(HP,"href","/docs/transformers/pr_15796/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.MarianMTModel"),c(JP,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(ZP,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(e$,"href","/docs/transformers/pr_15796/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15796/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(T1,"id","transformers.AutoModelForSequenceClassification"),c(T1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T1,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(t$,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(a$,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForSequenceClassification"),c(n$,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForSequenceClassification"),c(s$,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(l$,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(i$,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(d$,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(c$,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15796/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c($$,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(gb,"id","transformers.AutoModelForMultipleChoice"),c(gb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gb,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(H$,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(U$,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForMultipleChoice"),c(J$,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(Y$,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(K$,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(Z$,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(eI,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(oI,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(Ob,"id","transformers.AutoModelForNextSentencePrediction"),c(Ob,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ob,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(FI,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(CI,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(MI,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(EI,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(yI,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(Ub,"id","transformers.AutoModelForTokenClassification"),c(Ub,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ub,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(wI,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(AI,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForTokenClassification"),c(LI,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(BI,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(kI,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForTokenClassification"),c(xI,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(RI,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(SI,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(PI,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c($I,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(II,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(jI,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(NI,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(DI,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(qI,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(GI,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(OI,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(XI,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(zI,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(VI,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(WI,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(QI,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(HI,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(UI,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(JI,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(YI,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(KI,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(ej,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(oj,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(rj,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(tj,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(B5,"id","transformers.AutoModelForQuestionAnswering"),c(B5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B5,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(aj,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(nj,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(sj,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(lj,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(ij,"href","/docs/transformers/pr_15796/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(dj,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(cj,"href","/docs/transformers/pr_15796/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(fj,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(uj,"href","/docs/transformers/pr_15796/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15796/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Fj,"href","/docs/transformers/pr_15796/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Ej,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15796/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15796/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15796/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15796/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15796/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15796/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Nj,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Gj,"href","/docs/transformers/pr_15796/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(p2,"id","transformers.AutoModelForTableQuestionAnswering"),c(p2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p2,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Oj,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(b2,"id","transformers.AutoModelForImageClassification"),c(b2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b2,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Xj,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitForImageClassification"),c(zj,"href","/docs/transformers/pr_15796/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Vj,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Wj,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Qj,"href","/docs/transformers/pr_15796/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Hj,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Uj,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(Jj,"href","/docs/transformers/pr_15796/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(Yj,"href","/docs/transformers/pr_15796/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(Kj,"href","/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(Zj,"href","/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinForImageClassification"),c(eN,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(A2,"id","transformers.AutoModelForVision2Seq"),c(A2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A2,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(oN,"href","/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(k2,"id","transformers.AutoModelForAudioClassification"),c(k2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k2,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(rN,"href","/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(tN,"href","/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(aN,"href","/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(nN,"href","/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(sN,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(lN,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(iN,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(D2,"id","transformers.AutoModelForAudioFrameClassification"),c(D2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D2,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(dN,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(cN,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(fN,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(z2,"id","transformers.AutoModelForCTC"),c(z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z2,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(mN,"href","/docs/transformers/pr_15796/en/model_doc/hubert#transformers.HubertForCTC"),c(gN,"href","/docs/transformers/pr_15796/en/model_doc/sew#transformers.SEWForCTC"),c(hN,"href","/docs/transformers/pr_15796/en/model_doc/sew-d#transformers.SEWDForCTC"),c(pN,"href","/docs/transformers/pr_15796/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(_N,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(uN,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(bN,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(Z2,"id","transformers.AutoModelForSpeechSeq2Seq"),c(Z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z2,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(vN,"href","/docs/transformers/pr_15796/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(TN,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(tv,"id","transformers.AutoModelForAudioXVector"),c(tv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tv,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(FN,"href","/docs/transformers/pr_15796/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(CN,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(MN,"href","/docs/transformers/pr_15796/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(iv,"id","transformers.AutoModelForMaskedImageModeling"),c(iv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(iv,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(EN,"href","/docs/transformers/pr_15796/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(yN,"href","/docs/transformers/pr_15796/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(wN,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(gv,"id","transformers.AutoModelForObjectDetection"),c(gv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gv,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(AN,"href","/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(_v,"id","transformers.AutoModelForImageSegmentation"),c(_v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_v,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(LN,"href","/docs/transformers/pr_15796/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(vv,"id","transformers.AutoModelForSemanticSegmentation"),c(vv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vv,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(BN,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(kN,"href","/docs/transformers/pr_15796/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(Mv,"id","transformers.TFAutoModel"),c(Mv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mv,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c(xN,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertModel"),c(RN,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartModel"),c(SN,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertModel"),c(PN,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c($N,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(IN,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertModel"),c(jN,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.TFCLIPModel"),c(NN,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertModel"),c(DN,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLModel"),c(qN,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaModel"),c(GN,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(ON,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(XN,"href","/docs/transformers/pr_15796/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(zN,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraModel"),c(VN,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(WN,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelModel"),c(QN,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(HN,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2Model"),c(UN,"href","/docs/transformers/pr_15796/en/model_doc/hubert#transformers.TFHubertModel"),c(JN,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(YN,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.TFLEDModel"),c(KN,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerModel"),c(ZN,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.TFLxmertModel"),c(eD,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.TFMarianModel"),c(oD,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.TFMBartModel"),c(rD,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(tD,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetModel"),c(aD,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.TFMT5Model"),c(nD,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(sD,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.TFPegasusModel"),c(lD,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertModel"),c(iD,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaModel"),c(dD,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerModel"),c(cD,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(fD,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5Model"),c(mD,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasModel"),c(gD,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(hD,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.TFViTModel"),c(pD,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(_D,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMModel"),c(uD,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(bD,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(c6,"id","transformers.TFAutoModelForPreTraining"),c(c6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(c6,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(vD,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(TD,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(FD,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForPreTraining"),c(CD,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(MD,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(ED,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(yD,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(wD,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(AD,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(LD,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(BD,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(kD,"href","/docs/transformers/pr_15796/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(xD,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(RD,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(SD,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(PD,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c($D,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(ID,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(jD,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ND,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(DD,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(qD,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(S6,"id","transformers.TFAutoModelForCausalLM"),c(S6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S6,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(GD,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(OD,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(XD,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(zD,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(VD,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(WD,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(QD,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(HD,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(UD,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(JD,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(z6,"id","transformers.TFAutoModelForImageClassification"),c(z6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z6,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(YD,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(W6,"id","transformers.TFAutoModelForMaskedLM"),c(W6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W6,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(KD,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(ZD,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(eq,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(oq,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(rq,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(tq,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(aq,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(nq,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(sq,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(lq,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(iq,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(dq,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(cq,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(fq,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(mq,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(gq,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(hq,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(pq,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(_q,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(uq,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(gT,"id","transformers.TFAutoModelForSeq2SeqLM"),c(gT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gT,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(bq,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(vq,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Tq,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(Fq,"href","/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(Cq,"href","/docs/transformers/pr_15796/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Mq,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.TFMarianMTModel"),c(Eq,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(yq,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(wq,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(ET,"id","transformers.TFAutoModelForSequenceClassification"),c(ET,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ET,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(Lq,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Bq,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(kq,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(xq,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Rq,"href","/docs/transformers/pr_15796/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Sq,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Pq,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c($q,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Iq,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15796/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15796/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(JT,"id","transformers.TFAutoModelForMultipleChoice"),c(JT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(JT,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(Zq,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(eG,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(oG,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(rG,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(tG,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(aG,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(nG,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(sG,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(lG,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(h7,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(h7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(h7,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(_G,"href","/docs/transformers/pr_15796/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(_7,"id","transformers.TFAutoModelForTokenClassification"),c(_7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_7,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(uG,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(bG,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(vG,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(TG,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(FG,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(CG,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(MG,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(EG,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(yG,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(wG,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(AG,"href","/docs/transformers/pr_15796/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(LG,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(BG,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(xG,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(RG,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(SG,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(PG,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c($G,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(IG,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(j7,"id","transformers.TFAutoModelForQuestionAnswering"),c(j7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j7,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(jG,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(NG,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(DG,"href","/docs/transformers/pr_15796/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(qG,"href","/docs/transformers/pr_15796/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(GG,"href","/docs/transformers/pr_15796/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(OG,"href","/docs/transformers/pr_15796/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(XG,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(zG,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(VG,"href","/docs/transformers/pr_15796/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(WG,"href","/docs/transformers/pr_15796/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15796/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15796/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15796/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(JG,"href","/docs/transformers/pr_15796/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15796/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(eO,"href","/docs/transformers/pr_15796/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15796/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(tF,"id","transformers.TFAutoModelForVision2Seq"),c(tF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tF,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(rO,"href","/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(nF,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(nF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nF,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(tO,"href","/docs/transformers/pr_15796/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(lF,"id","transformers.FlaxAutoModel"),c(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lF,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(aO,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertModel"),c(nO,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartModel"),c(sO,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.FlaxBeitModel"),c(lO,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertModel"),c(iO,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(dO,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(cO,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(fO,"href","/docs/transformers/pr_15796/en/model_doc/clip#transformers.FlaxCLIPModel"),c(mO,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(gO,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraModel"),c(hO,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(pO,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(_O,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(uO,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.FlaxMarianModel"),c(bO,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartModel"),c(vO,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5Model"),c(TO,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(FO,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(CO,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(MO,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5Model"),c(EO,"href","/docs/transformers/pr_15796/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(yO,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.FlaxViTModel"),c(wO,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(AO,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(RF,"id","transformers.FlaxAutoModelForCausalLM"),c(RF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(RF,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(LO,"href","/docs/transformers/pr_15796/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(BO,"href","/docs/transformers/pr_15796/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(kO,"href","/docs/transformers/pr_15796/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(xO,"href","/docs/transformers/pr_15796/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(jF,"id","transformers.FlaxAutoModelForPreTraining"),c(jF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jF,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(RO,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(SO,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(PO,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c($O,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(IO,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(jO,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(NO,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(DO,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(qO,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(GO,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15796/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(UF,"id","transformers.FlaxAutoModelForMaskedLM"),c(UF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(UF,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(XO,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(zO,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(VO,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(WO,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(QO,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(HO,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(UO,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(JO,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(YO,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(n9,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(n9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(n9,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(KO,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(ZO,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15796/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(oX,"href","/docs/transformers/pr_15796/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(rX,"href","/docs/transformers/pr_15796/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(tX,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15796/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15796/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(sX,"href","/docs/transformers/pr_15796/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(p9,"id","transformers.FlaxAutoModelForSequenceClassification"),c(p9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p9,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(lX,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(iX,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(dX,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(cX,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(fX,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(mX,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(gX,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(hX,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(pX,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(y9,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(y9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(y9,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(_X,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(uX,"href","/docs/transformers/pr_15796/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(bX,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(vX,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(TX,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(FX,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(CX,"href","/docs/transformers/pr_15796/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(MX,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(EX,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c($9,"id","transformers.FlaxAutoModelForTokenClassification"),c($9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($9,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(yX,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(wX,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(AX,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(LX,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(BX,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(kX,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(xX,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(X9,"id","transformers.FlaxAutoModelForMultipleChoice"),c(X9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X9,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(RX,"href","/docs/transformers/pr_15796/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(SX,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(PX,"href","/docs/transformers/pr_15796/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c($X,"href","/docs/transformers/pr_15796/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(IX,"href","/docs/transformers/pr_15796/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(jX,"href","/docs/transformers/pr_15796/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(NX,"href","/docs/transformers/pr_15796/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(Y9,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(Y9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y9,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(DX,"href","/docs/transformers/pr_15796/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(Z9,"id","transformers.FlaxAutoModelForImageClassification"),c(Z9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z9,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(qX,"href","/docs/transformers/pr_15796/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(GX,"href","/docs/transformers/pr_15796/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(rC,"id","transformers.FlaxAutoModelForVision2Seq"),c(rC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rC,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(OX,"href","/docs/transformers/pr_15796/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,o4),e(sa,yf),b(d,ye,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,r4),e(io,$n),e(io,In),e(In,t4),e(io,ki),e(io,jn),e(jn,a4),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,D0),e(ge,Ri),e(Ri,q0),e(ge,G0),b(d,qo,u),b(d,Ia,u),e(Ia,O0),e(Ia,Af),e(Af,X0),e(Ia,mxe),b(d,tLe,u),b(d,Si,u),e(Si,Lf),e(Lf,$V),g(n4,$V,null),e(Si,gxe),e(Si,IV),e(IV,hxe),b(d,aLe,u),b(d,Nn,u),e(Nn,pxe),e(Nn,jV),e(jV,_xe),e(Nn,uxe),e(Nn,NV),e(NV,bxe),e(Nn,vxe),b(d,nLe,u),g(s4,d,u),b(d,sLe,u),b(d,z0,u),e(z0,Txe),b(d,lLe,u),g(Bf,d,u),b(d,iLe,u),b(d,Pi,u),e(Pi,kf),e(kf,DV),g(l4,DV,null),e(Pi,Fxe),e(Pi,qV),e(qV,Cxe),b(d,dLe,u),b(d,Go,u),g(i4,Go,null),e(Go,Mxe),e(Go,d4),e(d4,Exe),e(d4,V0),e(V0,yxe),e(d4,wxe),e(Go,Axe),e(Go,c4),e(c4,Lxe),e(c4,GV),e(GV,Bxe),e(c4,kxe),e(Go,xxe),e(Go,fo),g(f4,fo,null),e(fo,Rxe),e(fo,OV),e(OV,Sxe),e(fo,Pxe),e(fo,$i),e($i,$xe),e($i,XV),e(XV,Ixe),e($i,jxe),e($i,zV),e(zV,Nxe),e($i,Dxe),e(fo,qxe),e(fo,v),e(v,xf),e(xf,VV),e(VV,Gxe),e(xf,Oxe),e(xf,W0),e(W0,Xxe),e(xf,zxe),e(v,Vxe),e(v,Rf),e(Rf,WV),e(WV,Wxe),e(Rf,Qxe),e(Rf,Q0),e(Q0,Hxe),e(Rf,Uxe),e(v,Jxe),e(v,Sf),e(Sf,QV),e(QV,Yxe),e(Sf,Kxe),e(Sf,H0),e(H0,Zxe),e(Sf,eRe),e(v,oRe),e(v,Pf),e(Pf,HV),e(HV,rRe),e(Pf,tRe),e(Pf,U0),e(U0,aRe),e(Pf,nRe),e(v,sRe),e(v,$f),e($f,UV),e(UV,lRe),e($f,iRe),e($f,J0),e(J0,dRe),e($f,cRe),e(v,fRe),e(v,If),e(If,JV),e(JV,mRe),e(If,gRe),e(If,Y0),e(Y0,hRe),e(If,pRe),e(v,_Re),e(v,jf),e(jf,YV),e(YV,uRe),e(jf,bRe),e(jf,K0),e(K0,vRe),e(jf,TRe),e(v,FRe),e(v,Nf),e(Nf,KV),e(KV,CRe),e(Nf,MRe),e(Nf,Z0),e(Z0,ERe),e(Nf,yRe),e(v,wRe),e(v,Df),e(Df,ZV),e(ZV,ARe),e(Df,LRe),e(Df,eL),e(eL,BRe),e(Df,kRe),e(v,xRe),e(v,qf),e(qf,eW),e(eW,RRe),e(qf,SRe),e(qf,oL),e(oL,PRe),e(qf,$Re),e(v,IRe),e(v,Gf),e(Gf,oW),e(oW,jRe),e(Gf,NRe),e(Gf,rL),e(rL,DRe),e(Gf,qRe),e(v,GRe),e(v,Of),e(Of,rW),e(rW,ORe),e(Of,XRe),e(Of,tL),e(tL,zRe),e(Of,VRe),e(v,WRe),e(v,Xf),e(Xf,tW),e(tW,QRe),e(Xf,HRe),e(Xf,aL),e(aL,URe),e(Xf,JRe),e(v,YRe),e(v,zf),e(zf,aW),e(aW,KRe),e(zf,ZRe),e(zf,nL),e(nL,eSe),e(zf,oSe),e(v,rSe),e(v,Vf),e(Vf,nW),e(nW,tSe),e(Vf,aSe),e(Vf,sL),e(sL,nSe),e(Vf,sSe),e(v,lSe),e(v,Wf),e(Wf,sW),e(sW,iSe),e(Wf,dSe),e(Wf,lL),e(lL,cSe),e(Wf,fSe),e(v,mSe),e(v,Qf),e(Qf,lW),e(lW,gSe),e(Qf,hSe),e(Qf,iL),e(iL,pSe),e(Qf,_Se),e(v,uSe),e(v,Hf),e(Hf,iW),e(iW,bSe),e(Hf,vSe),e(Hf,dL),e(dL,TSe),e(Hf,FSe),e(v,CSe),e(v,Uf),e(Uf,dW),e(dW,MSe),e(Uf,ESe),e(Uf,cL),e(cL,ySe),e(Uf,wSe),e(v,ASe),e(v,Jf),e(Jf,cW),e(cW,LSe),e(Jf,BSe),e(Jf,fL),e(fL,kSe),e(Jf,xSe),e(v,RSe),e(v,Yf),e(Yf,fW),e(fW,SSe),e(Yf,PSe),e(Yf,mL),e(mL,$Se),e(Yf,ISe),e(v,jSe),e(v,Kf),e(Kf,mW),e(mW,NSe),e(Kf,DSe),e(Kf,gL),e(gL,qSe),e(Kf,GSe),e(v,OSe),e(v,Zf),e(Zf,gW),e(gW,XSe),e(Zf,zSe),e(Zf,hL),e(hL,VSe),e(Zf,WSe),e(v,QSe),e(v,em),e(em,hW),e(hW,HSe),e(em,USe),e(em,pL),e(pL,JSe),e(em,YSe),e(v,KSe),e(v,om),e(om,pW),e(pW,ZSe),e(om,ePe),e(om,_L),e(_L,oPe),e(om,rPe),e(v,tPe),e(v,rm),e(rm,_W),e(_W,aPe),e(rm,nPe),e(rm,uL),e(uL,sPe),e(rm,lPe),e(v,iPe),e(v,tm),e(tm,uW),e(uW,dPe),e(tm,cPe),e(tm,bL),e(bL,fPe),e(tm,mPe),e(v,gPe),e(v,am),e(am,bW),e(bW,hPe),e(am,pPe),e(am,vL),e(vL,_Pe),e(am,uPe),e(v,bPe),e(v,nm),e(nm,vW),e(vW,vPe),e(nm,TPe),e(nm,TL),e(TL,FPe),e(nm,CPe),e(v,MPe),e(v,sm),e(sm,TW),e(TW,EPe),e(sm,yPe),e(sm,FL),e(FL,wPe),e(sm,APe),e(v,LPe),e(v,lm),e(lm,FW),e(FW,BPe),e(lm,kPe),e(lm,CL),e(CL,xPe),e(lm,RPe),e(v,SPe),e(v,im),e(im,CW),e(CW,PPe),e(im,$Pe),e(im,ML),e(ML,IPe),e(im,jPe),e(v,NPe),e(v,dm),e(dm,MW),e(MW,DPe),e(dm,qPe),e(dm,EL),e(EL,GPe),e(dm,OPe),e(v,XPe),e(v,cm),e(cm,EW),e(EW,zPe),e(cm,VPe),e(cm,yL),e(yL,WPe),e(cm,QPe),e(v,HPe),e(v,fm),e(fm,yW),e(yW,UPe),e(fm,JPe),e(fm,wL),e(wL,YPe),e(fm,KPe),e(v,ZPe),e(v,mm),e(mm,wW),e(wW,e$e),e(mm,o$e),e(mm,AL),e(AL,r$e),e(mm,t$e),e(v,a$e),e(v,gm),e(gm,AW),e(AW,n$e),e(gm,s$e),e(gm,LL),e(LL,l$e),e(gm,i$e),e(v,d$e),e(v,hm),e(hm,LW),e(LW,c$e),e(hm,f$e),e(hm,BL),e(BL,m$e),e(hm,g$e),e(v,h$e),e(v,pm),e(pm,BW),e(BW,p$e),e(pm,_$e),e(pm,kL),e(kL,u$e),e(pm,b$e),e(v,v$e),e(v,_m),e(_m,kW),e(kW,T$e),e(_m,F$e),e(_m,xL),e(xL,C$e),e(_m,M$e),e(v,E$e),e(v,um),e(um,xW),e(xW,y$e),e(um,w$e),e(um,RL),e(RL,A$e),e(um,L$e),e(v,B$e),e(v,bm),e(bm,RW),e(RW,k$e),e(bm,x$e),e(bm,SL),e(SL,R$e),e(bm,S$e),e(v,P$e),e(v,vm),e(vm,SW),e(SW,$$e),e(vm,I$e),e(vm,PL),e(PL,j$e),e(vm,N$e),e(v,D$e),e(v,Tm),e(Tm,PW),e(PW,q$e),e(Tm,G$e),e(Tm,$L),e($L,O$e),e(Tm,X$e),e(v,z$e),e(v,Fm),e(Fm,$W),e($W,V$e),e(Fm,W$e),e(Fm,IL),e(IL,Q$e),e(Fm,H$e),e(v,U$e),e(v,Cm),e(Cm,IW),e(IW,J$e),e(Cm,Y$e),e(Cm,jL),e(jL,K$e),e(Cm,Z$e),e(v,eIe),e(v,Mm),e(Mm,jW),e(jW,oIe),e(Mm,rIe),e(Mm,NL),e(NL,tIe),e(Mm,aIe),e(v,nIe),e(v,Em),e(Em,NW),e(NW,sIe),e(Em,lIe),e(Em,DL),e(DL,iIe),e(Em,dIe),e(v,cIe),e(v,ym),e(ym,DW),e(DW,fIe),e(ym,mIe),e(ym,qL),e(qL,gIe),e(ym,hIe),e(v,pIe),e(v,wm),e(wm,qW),e(qW,_Ie),e(wm,uIe),e(wm,GL),e(GL,bIe),e(wm,vIe),e(v,TIe),e(v,Am),e(Am,GW),e(GW,FIe),e(Am,CIe),e(Am,OL),e(OL,MIe),e(Am,EIe),e(v,yIe),e(v,Lm),e(Lm,OW),e(OW,wIe),e(Lm,AIe),e(Lm,XL),e(XL,LIe),e(Lm,BIe),e(v,kIe),e(v,Bm),e(Bm,XW),e(XW,xIe),e(Bm,RIe),e(Bm,zL),e(zL,SIe),e(Bm,PIe),e(v,$Ie),e(v,km),e(km,zW),e(zW,IIe),e(km,jIe),e(km,VL),e(VL,NIe),e(km,DIe),e(v,qIe),e(v,xm),e(xm,VW),e(VW,GIe),e(xm,OIe),e(xm,WL),e(WL,XIe),e(xm,zIe),e(v,VIe),e(v,Rm),e(Rm,WW),e(WW,WIe),e(Rm,QIe),e(Rm,QL),e(QL,HIe),e(Rm,UIe),e(v,JIe),e(v,Sm),e(Sm,QW),e(QW,YIe),e(Sm,KIe),e(Sm,HL),e(HL,ZIe),e(Sm,eje),e(v,oje),e(v,Pm),e(Pm,HW),e(HW,rje),e(Pm,tje),e(Pm,UL),e(UL,aje),e(Pm,nje),e(v,sje),e(v,$m),e($m,UW),e(UW,lje),e($m,ije),e($m,JL),e(JL,dje),e($m,cje),e(v,fje),e(v,Im),e(Im,JW),e(JW,mje),e(Im,gje),e(Im,YL),e(YL,hje),e(Im,pje),e(v,_je),e(v,jm),e(jm,YW),e(YW,uje),e(jm,bje),e(jm,KL),e(KL,vje),e(jm,Tje),e(v,Fje),e(v,Nm),e(Nm,KW),e(KW,Cje),e(Nm,Mje),e(Nm,ZL),e(ZL,Eje),e(Nm,yje),e(v,wje),e(v,Dm),e(Dm,ZW),e(ZW,Aje),e(Dm,Lje),e(Dm,e8),e(e8,Bje),e(Dm,kje),e(v,xje),e(v,qm),e(qm,eQ),e(eQ,Rje),e(qm,Sje),e(qm,o8),e(o8,Pje),e(qm,$je),e(v,Ije),e(v,Gm),e(Gm,oQ),e(oQ,jje),e(Gm,Nje),e(Gm,r8),e(r8,Dje),e(Gm,qje),e(v,Gje),e(v,Om),e(Om,rQ),e(rQ,Oje),e(Om,Xje),e(Om,t8),e(t8,zje),e(Om,Vje),e(v,Wje),e(v,Xm),e(Xm,tQ),e(tQ,Qje),e(Xm,Hje),e(Xm,a8),e(a8,Uje),e(Xm,Jje),e(v,Yje),e(v,zm),e(zm,aQ),e(aQ,Kje),e(zm,Zje),e(zm,n8),e(n8,eNe),e(zm,oNe),e(v,rNe),e(v,Vm),e(Vm,nQ),e(nQ,tNe),e(Vm,aNe),e(Vm,s8),e(s8,nNe),e(Vm,sNe),e(v,lNe),e(v,Wm),e(Wm,sQ),e(sQ,iNe),e(Wm,dNe),e(Wm,l8),e(l8,cNe),e(Wm,fNe),e(v,mNe),e(v,Qm),e(Qm,lQ),e(lQ,gNe),e(Qm,hNe),e(Qm,i8),e(i8,pNe),e(Qm,_Ne),e(v,uNe),e(v,Hm),e(Hm,iQ),e(iQ,bNe),e(Hm,vNe),e(Hm,d8),e(d8,TNe),e(Hm,FNe),e(v,CNe),e(v,Um),e(Um,dQ),e(dQ,MNe),e(Um,ENe),e(Um,c8),e(c8,yNe),e(Um,wNe),e(v,ANe),e(v,Jm),e(Jm,cQ),e(cQ,LNe),e(Jm,BNe),e(Jm,f8),e(f8,kNe),e(Jm,xNe),e(v,RNe),e(v,Ym),e(Ym,fQ),e(fQ,SNe),e(Ym,PNe),e(Ym,m8),e(m8,$Ne),e(Ym,INe),e(v,jNe),e(v,Km),e(Km,mQ),e(mQ,NNe),e(Km,DNe),e(Km,g8),e(g8,qNe),e(Km,GNe),e(v,ONe),e(v,Zm),e(Zm,gQ),e(gQ,XNe),e(Zm,zNe),e(Zm,h8),e(h8,VNe),e(Zm,WNe),e(v,QNe),e(v,eg),e(eg,hQ),e(hQ,HNe),e(eg,UNe),e(eg,p8),e(p8,JNe),e(eg,YNe),e(v,KNe),e(v,og),e(og,pQ),e(pQ,ZNe),e(og,eDe),e(og,_8),e(_8,oDe),e(og,rDe),e(v,tDe),e(v,rg),e(rg,_Q),e(_Q,aDe),e(rg,nDe),e(rg,u8),e(u8,sDe),e(rg,lDe),e(v,iDe),e(v,tg),e(tg,uQ),e(uQ,dDe),e(tg,cDe),e(tg,b8),e(b8,fDe),e(tg,mDe),e(v,gDe),e(v,ag),e(ag,bQ),e(bQ,hDe),e(ag,pDe),e(ag,v8),e(v8,_De),e(ag,uDe),e(v,bDe),e(v,ng),e(ng,vQ),e(vQ,vDe),e(ng,TDe),e(ng,T8),e(T8,FDe),e(ng,CDe),e(v,MDe),e(v,sg),e(sg,TQ),e(TQ,EDe),e(sg,yDe),e(sg,F8),e(F8,wDe),e(sg,ADe),e(v,LDe),e(v,lg),e(lg,FQ),e(FQ,BDe),e(lg,kDe),e(lg,C8),e(C8,xDe),e(lg,RDe),e(v,SDe),e(v,ig),e(ig,CQ),e(CQ,PDe),e(ig,$De),e(ig,M8),e(M8,IDe),e(ig,jDe),e(v,NDe),e(v,dg),e(dg,MQ),e(MQ,DDe),e(dg,qDe),e(dg,E8),e(E8,GDe),e(dg,ODe),e(v,XDe),e(v,cg),e(cg,EQ),e(EQ,zDe),e(cg,VDe),e(cg,y8),e(y8,WDe),e(cg,QDe),e(v,HDe),e(v,fg),e(fg,yQ),e(yQ,UDe),e(fg,JDe),e(fg,w8),e(w8,YDe),e(fg,KDe),e(v,ZDe),e(v,mg),e(mg,wQ),e(wQ,eqe),e(mg,oqe),e(mg,A8),e(A8,rqe),e(mg,tqe),e(v,aqe),e(v,gg),e(gg,AQ),e(AQ,nqe),e(gg,sqe),e(gg,L8),e(L8,lqe),e(gg,iqe),e(fo,dqe),e(fo,LQ),e(LQ,cqe),e(fo,fqe),g(m4,fo,null),e(Go,mqe),e(Go,hg),g(g4,hg,null),e(hg,gqe),e(hg,BQ),e(BQ,hqe),b(d,cLe,u),b(d,Ii,u),e(Ii,pg),e(pg,kQ),g(h4,kQ,null),e(Ii,pqe),e(Ii,xQ),e(xQ,_qe),b(d,fLe,u),b(d,Oo,u),g(p4,Oo,null),e(Oo,uqe),e(Oo,_4),e(_4,bqe),e(_4,B8),e(B8,vqe),e(_4,Tqe),e(Oo,Fqe),e(Oo,u4),e(u4,Cqe),e(u4,RQ),e(RQ,Mqe),e(u4,Eqe),e(Oo,yqe),e(Oo,mo),g(b4,mo,null),e(mo,wqe),e(mo,SQ),e(SQ,Aqe),e(mo,Lqe),e(mo,ja),e(ja,Bqe),e(ja,PQ),e(PQ,kqe),e(ja,xqe),e(ja,$Q),e($Q,Rqe),e(ja,Sqe),e(ja,IQ),e(IQ,Pqe),e(ja,$qe),e(mo,Iqe),e(mo,M),e(M,Dn),e(Dn,jQ),e(jQ,jqe),e(Dn,Nqe),e(Dn,k8),e(k8,Dqe),e(Dn,qqe),e(Dn,x8),e(x8,Gqe),e(Dn,Oqe),e(M,Xqe),e(M,qn),e(qn,NQ),e(NQ,zqe),e(qn,Vqe),e(qn,R8),e(R8,Wqe),e(qn,Qqe),e(qn,S8),e(S8,Hqe),e(qn,Uqe),e(M,Jqe),e(M,Gn),e(Gn,DQ),e(DQ,Yqe),e(Gn,Kqe),e(Gn,P8),e(P8,Zqe),e(Gn,eGe),e(Gn,$8),e($8,oGe),e(Gn,rGe),e(M,tGe),e(M,_g),e(_g,qQ),e(qQ,aGe),e(_g,nGe),e(_g,I8),e(I8,sGe),e(_g,lGe),e(M,iGe),e(M,On),e(On,GQ),e(GQ,dGe),e(On,cGe),e(On,j8),e(j8,fGe),e(On,mGe),e(On,N8),e(N8,gGe),e(On,hGe),e(M,pGe),e(M,ug),e(ug,OQ),e(OQ,_Ge),e(ug,uGe),e(ug,D8),e(D8,bGe),e(ug,vGe),e(M,TGe),e(M,bg),e(bg,XQ),e(XQ,FGe),e(bg,CGe),e(bg,q8),e(q8,MGe),e(bg,EGe),e(M,yGe),e(M,vg),e(vg,zQ),e(zQ,wGe),e(vg,AGe),e(vg,G8),e(G8,LGe),e(vg,BGe),e(M,kGe),e(M,Xn),e(Xn,VQ),e(VQ,xGe),e(Xn,RGe),e(Xn,O8),e(O8,SGe),e(Xn,PGe),e(Xn,X8),e(X8,$Ge),e(Xn,IGe),e(M,jGe),e(M,zn),e(zn,WQ),e(WQ,NGe),e(zn,DGe),e(zn,z8),e(z8,qGe),e(zn,GGe),e(zn,V8),e(V8,OGe),e(zn,XGe),e(M,zGe),e(M,Vn),e(Vn,QQ),e(QQ,VGe),e(Vn,WGe),e(Vn,W8),e(W8,QGe),e(Vn,HGe),e(Vn,Q8),e(Q8,UGe),e(Vn,JGe),e(M,YGe),e(M,Tg),e(Tg,HQ),e(HQ,KGe),e(Tg,ZGe),e(Tg,H8),e(H8,eOe),e(Tg,oOe),e(M,rOe),e(M,Fg),e(Fg,UQ),e(UQ,tOe),e(Fg,aOe),e(Fg,U8),e(U8,nOe),e(Fg,sOe),e(M,lOe),e(M,Wn),e(Wn,JQ),e(JQ,iOe),e(Wn,dOe),e(Wn,J8),e(J8,cOe),e(Wn,fOe),e(Wn,Y8),e(Y8,mOe),e(Wn,gOe),e(M,hOe),e(M,Cg),e(Cg,YQ),e(YQ,pOe),e(Cg,_Oe),e(Cg,K8),e(K8,uOe),e(Cg,bOe),e(M,vOe),e(M,Qn),e(Qn,KQ),e(KQ,TOe),e(Qn,FOe),e(Qn,Z8),e(Z8,COe),e(Qn,MOe),e(Qn,eB),e(eB,EOe),e(Qn,yOe),e(M,wOe),e(M,Hn),e(Hn,ZQ),e(ZQ,AOe),e(Hn,LOe),e(Hn,oB),e(oB,BOe),e(Hn,kOe),e(Hn,rB),e(rB,xOe),e(Hn,ROe),e(M,SOe),e(M,Un),e(Un,eH),e(eH,POe),e(Un,$Oe),e(Un,tB),e(tB,IOe),e(Un,jOe),e(Un,oH),e(oH,NOe),e(Un,DOe),e(M,qOe),e(M,Mg),e(Mg,rH),e(rH,GOe),e(Mg,OOe),e(Mg,aB),e(aB,XOe),e(Mg,zOe),e(M,VOe),e(M,Jn),e(Jn,tH),e(tH,WOe),e(Jn,QOe),e(Jn,nB),e(nB,HOe),e(Jn,UOe),e(Jn,sB),e(sB,JOe),e(Jn,YOe),e(M,KOe),e(M,Eg),e(Eg,aH),e(aH,ZOe),e(Eg,eXe),e(Eg,lB),e(lB,oXe),e(Eg,rXe),e(M,tXe),e(M,Yn),e(Yn,nH),e(nH,aXe),e(Yn,nXe),e(Yn,iB),e(iB,sXe),e(Yn,lXe),e(Yn,dB),e(dB,iXe),e(Yn,dXe),e(M,cXe),e(M,Kn),e(Kn,sH),e(sH,fXe),e(Kn,mXe),e(Kn,cB),e(cB,gXe),e(Kn,hXe),e(Kn,fB),e(fB,pXe),e(Kn,_Xe),e(M,uXe),e(M,Zn),e(Zn,lH),e(lH,bXe),e(Zn,vXe),e(Zn,mB),e(mB,TXe),e(Zn,FXe),e(Zn,gB),e(gB,CXe),e(Zn,MXe),e(M,EXe),e(M,yg),e(yg,iH),e(iH,yXe),e(yg,wXe),e(yg,hB),e(hB,AXe),e(yg,LXe),e(M,BXe),e(M,es),e(es,dH),e(dH,kXe),e(es,xXe),e(es,pB),e(pB,RXe),e(es,SXe),e(es,_B),e(_B,PXe),e(es,$Xe),e(M,IXe),e(M,wg),e(wg,cH),e(cH,jXe),e(wg,NXe),e(wg,uB),e(uB,DXe),e(wg,qXe),e(M,GXe),e(M,os),e(os,fH),e(fH,OXe),e(os,XXe),e(os,bB),e(bB,zXe),e(os,VXe),e(os,vB),e(vB,WXe),e(os,QXe),e(M,HXe),e(M,rs),e(rs,mH),e(mH,UXe),e(rs,JXe),e(rs,TB),e(TB,YXe),e(rs,KXe),e(rs,FB),e(FB,ZXe),e(rs,eze),e(M,oze),e(M,ts),e(ts,gH),e(gH,rze),e(ts,tze),e(ts,CB),e(CB,aze),e(ts,nze),e(ts,MB),e(MB,sze),e(ts,lze),e(M,ize),e(M,as),e(as,hH),e(hH,dze),e(as,cze),e(as,EB),e(EB,fze),e(as,mze),e(as,yB),e(yB,gze),e(as,hze),e(M,pze),e(M,Ag),e(Ag,pH),e(pH,_ze),e(Ag,uze),e(Ag,wB),e(wB,bze),e(Ag,vze),e(M,Tze),e(M,ns),e(ns,_H),e(_H,Fze),e(ns,Cze),e(ns,AB),e(AB,Mze),e(ns,Eze),e(ns,LB),e(LB,yze),e(ns,wze),e(M,Aze),e(M,ss),e(ss,uH),e(uH,Lze),e(ss,Bze),e(ss,BB),e(BB,kze),e(ss,xze),e(ss,kB),e(kB,Rze),e(ss,Sze),e(M,Pze),e(M,ls),e(ls,bH),e(bH,$ze),e(ls,Ize),e(ls,xB),e(xB,jze),e(ls,Nze),e(ls,RB),e(RB,Dze),e(ls,qze),e(M,Gze),e(M,is),e(is,vH),e(vH,Oze),e(is,Xze),e(is,SB),e(SB,zze),e(is,Vze),e(is,PB),e(PB,Wze),e(is,Qze),e(M,Hze),e(M,ds),e(ds,TH),e(TH,Uze),e(ds,Jze),e(ds,$B),e($B,Yze),e(ds,Kze),e(ds,IB),e(IB,Zze),e(ds,eVe),e(M,oVe),e(M,cs),e(cs,FH),e(FH,rVe),e(cs,tVe),e(cs,jB),e(jB,aVe),e(cs,nVe),e(cs,NB),e(NB,sVe),e(cs,lVe),e(M,iVe),e(M,Lg),e(Lg,CH),e(CH,dVe),e(Lg,cVe),e(Lg,DB),e(DB,fVe),e(Lg,mVe),e(M,gVe),e(M,fs),e(fs,MH),e(MH,hVe),e(fs,pVe),e(fs,qB),e(qB,_Ve),e(fs,uVe),e(fs,GB),e(GB,bVe),e(fs,vVe),e(M,TVe),e(M,Bg),e(Bg,EH),e(EH,FVe),e(Bg,CVe),e(Bg,OB),e(OB,MVe),e(Bg,EVe),e(M,yVe),e(M,kg),e(kg,yH),e(yH,wVe),e(kg,AVe),e(kg,XB),e(XB,LVe),e(kg,BVe),e(M,kVe),e(M,ms),e(ms,wH),e(wH,xVe),e(ms,RVe),e(ms,zB),e(zB,SVe),e(ms,PVe),e(ms,VB),e(VB,$Ve),e(ms,IVe),e(M,jVe),e(M,gs),e(gs,AH),e(AH,NVe),e(gs,DVe),e(gs,WB),e(WB,qVe),e(gs,GVe),e(gs,QB),e(QB,OVe),e(gs,XVe),e(M,zVe),e(M,xg),e(xg,LH),e(LH,VVe),e(xg,WVe),e(xg,HB),e(HB,QVe),e(xg,HVe),e(M,UVe),e(M,hs),e(hs,BH),e(BH,JVe),e(hs,YVe),e(hs,UB),e(UB,KVe),e(hs,ZVe),e(hs,JB),e(JB,eWe),e(hs,oWe),e(M,rWe),e(M,ps),e(ps,kH),e(kH,tWe),e(ps,aWe),e(ps,YB),e(YB,nWe),e(ps,sWe),e(ps,KB),e(KB,lWe),e(ps,iWe),e(M,dWe),e(M,_s),e(_s,xH),e(xH,cWe),e(_s,fWe),e(_s,ZB),e(ZB,mWe),e(_s,gWe),e(_s,ek),e(ek,hWe),e(_s,pWe),e(M,_We),e(M,us),e(us,RH),e(RH,uWe),e(us,bWe),e(us,ok),e(ok,vWe),e(us,TWe),e(us,rk),e(rk,FWe),e(us,CWe),e(M,MWe),e(M,bs),e(bs,SH),e(SH,EWe),e(bs,yWe),e(bs,tk),e(tk,wWe),e(bs,AWe),e(bs,ak),e(ak,LWe),e(bs,BWe),e(M,kWe),e(M,Rg),e(Rg,PH),e(PH,xWe),e(Rg,RWe),e(Rg,nk),e(nk,SWe),e(Rg,PWe),e(M,$We),e(M,Sg),e(Sg,$H),e($H,IWe),e(Sg,jWe),e(Sg,sk),e(sk,NWe),e(Sg,DWe),e(M,qWe),e(M,Pg),e(Pg,IH),e(IH,GWe),e(Pg,OWe),e(Pg,lk),e(lk,XWe),e(Pg,zWe),e(M,VWe),e(M,$g),e($g,jH),e(jH,WWe),e($g,QWe),e($g,ik),e(ik,HWe),e($g,UWe),e(M,JWe),e(M,vs),e(vs,NH),e(NH,YWe),e(vs,KWe),e(vs,dk),e(dk,ZWe),e(vs,eQe),e(vs,ck),e(ck,oQe),e(vs,rQe),e(M,tQe),e(M,Ig),e(Ig,DH),e(DH,aQe),e(Ig,nQe),e(Ig,fk),e(fk,sQe),e(Ig,lQe),e(M,iQe),e(M,Ts),e(Ts,qH),e(qH,dQe),e(Ts,cQe),e(Ts,mk),e(mk,fQe),e(Ts,mQe),e(Ts,gk),e(gk,gQe),e(Ts,hQe),e(M,pQe),e(M,Fs),e(Fs,GH),e(GH,_Qe),e(Fs,uQe),e(Fs,hk),e(hk,bQe),e(Fs,vQe),e(Fs,pk),e(pk,TQe),e(Fs,FQe),e(M,CQe),e(M,Cs),e(Cs,OH),e(OH,MQe),e(Cs,EQe),e(Cs,_k),e(_k,yQe),e(Cs,wQe),e(Cs,uk),e(uk,AQe),e(Cs,LQe),e(M,BQe),e(M,Ms),e(Ms,XH),e(XH,kQe),e(Ms,xQe),e(Ms,bk),e(bk,RQe),e(Ms,SQe),e(Ms,vk),e(vk,PQe),e(Ms,$Qe),e(M,IQe),e(M,Es),e(Es,zH),e(zH,jQe),e(Es,NQe),e(Es,Tk),e(Tk,DQe),e(Es,qQe),e(Es,Fk),e(Fk,GQe),e(Es,OQe),e(M,XQe),e(M,jg),e(jg,VH),e(VH,zQe),e(jg,VQe),e(jg,Ck),e(Ck,WQe),e(jg,QQe),e(M,HQe),e(M,Ng),e(Ng,WH),e(WH,UQe),e(Ng,JQe),e(Ng,Mk),e(Mk,YQe),e(Ng,KQe),e(M,ZQe),e(M,ys),e(ys,QH),e(QH,eHe),e(ys,oHe),e(ys,Ek),e(Ek,rHe),e(ys,tHe),e(ys,yk),e(yk,aHe),e(ys,nHe),e(M,sHe),e(M,ws),e(ws,HH),e(HH,lHe),e(ws,iHe),e(ws,wk),e(wk,dHe),e(ws,cHe),e(ws,Ak),e(Ak,fHe),e(ws,mHe),e(M,gHe),e(M,As),e(As,UH),e(UH,hHe),e(As,pHe),e(As,Lk),e(Lk,_He),e(As,uHe),e(As,Bk),e(Bk,bHe),e(As,vHe),e(M,THe),e(M,Dg),e(Dg,JH),e(JH,FHe),e(Dg,CHe),e(Dg,kk),e(kk,MHe),e(Dg,EHe),e(M,yHe),e(M,qg),e(qg,YH),e(YH,wHe),e(qg,AHe),e(qg,xk),e(xk,LHe),e(qg,BHe),e(M,kHe),e(M,Gg),e(Gg,KH),e(KH,xHe),e(Gg,RHe),e(Gg,Rk),e(Rk,SHe),e(Gg,PHe),e(M,$He),e(M,Og),e(Og,ZH),e(ZH,IHe),e(Og,jHe),e(Og,Sk),e(Sk,NHe),e(Og,DHe),e(M,qHe),e(M,Ls),e(Ls,eU),e(eU,GHe),e(Ls,OHe),e(Ls,Pk),e(Pk,XHe),e(Ls,zHe),e(Ls,$k),e($k,VHe),e(Ls,WHe),e(M,QHe),e(M,Xg),e(Xg,oU),e(oU,HHe),e(Xg,UHe),e(Xg,Ik),e(Ik,JHe),e(Xg,YHe),e(M,KHe),e(M,zg),e(zg,rU),e(rU,ZHe),e(zg,eUe),e(zg,jk),e(jk,oUe),e(zg,rUe),e(M,tUe),e(M,Bs),e(Bs,tU),e(tU,aUe),e(Bs,nUe),e(Bs,Nk),e(Nk,sUe),e(Bs,lUe),e(Bs,Dk),e(Dk,iUe),e(Bs,dUe),e(M,cUe),e(M,ks),e(ks,aU),e(aU,fUe),e(ks,mUe),e(ks,qk),e(qk,gUe),e(ks,hUe),e(ks,Gk),e(Gk,pUe),e(ks,_Ue),e(mo,uUe),e(mo,nU),e(nU,bUe),e(mo,vUe),g(v4,mo,null),e(Oo,TUe),e(Oo,Vg),g(T4,Vg,null),e(Vg,FUe),e(Vg,sU),e(sU,CUe),b(d,mLe,u),b(d,ji,u),e(ji,Wg),e(Wg,lU),g(F4,lU,null),e(ji,MUe),e(ji,iU),e(iU,EUe),b(d,gLe,u),b(d,Xo,u),g(C4,Xo,null),e(Xo,yUe),e(Xo,M4),e(M4,wUe),e(M4,Ok),e(Ok,AUe),e(M4,LUe),e(Xo,BUe),e(Xo,E4),e(E4,kUe),e(E4,dU),e(dU,xUe),e(E4,RUe),e(Xo,SUe),e(Xo,Le),g(y4,Le,null),e(Le,PUe),e(Le,cU),e(cU,$Ue),e(Le,IUe),e(Le,Na),e(Na,jUe),e(Na,fU),e(fU,NUe),e(Na,DUe),e(Na,mU),e(mU,qUe),e(Na,GUe),e(Na,gU),e(gU,OUe),e(Na,XUe),e(Le,zUe),e(Le,se),e(se,Qg),e(Qg,hU),e(hU,VUe),e(Qg,WUe),e(Qg,Xk),e(Xk,QUe),e(Qg,HUe),e(se,UUe),e(se,Hg),e(Hg,pU),e(pU,JUe),e(Hg,YUe),e(Hg,zk),e(zk,KUe),e(Hg,ZUe),e(se,eJe),e(se,Ug),e(Ug,_U),e(_U,oJe),e(Ug,rJe),e(Ug,Vk),e(Vk,tJe),e(Ug,aJe),e(se,nJe),e(se,Jg),e(Jg,uU),e(uU,sJe),e(Jg,lJe),e(Jg,Wk),e(Wk,iJe),e(Jg,dJe),e(se,cJe),e(se,Yg),e(Yg,bU),e(bU,fJe),e(Yg,mJe),e(Yg,Qk),e(Qk,gJe),e(Yg,hJe),e(se,pJe),e(se,Kg),e(Kg,vU),e(vU,_Je),e(Kg,uJe),e(Kg,Hk),e(Hk,bJe),e(Kg,vJe),e(se,TJe),e(se,Zg),e(Zg,TU),e(TU,FJe),e(Zg,CJe),e(Zg,Uk),e(Uk,MJe),e(Zg,EJe),e(se,yJe),e(se,eh),e(eh,FU),e(FU,wJe),e(eh,AJe),e(eh,Jk),e(Jk,LJe),e(eh,BJe),e(se,kJe),e(se,oh),e(oh,CU),e(CU,xJe),e(oh,RJe),e(oh,Yk),e(Yk,SJe),e(oh,PJe),e(se,$Je),e(se,rh),e(rh,MU),e(MU,IJe),e(rh,jJe),e(rh,Kk),e(Kk,NJe),e(rh,DJe),e(se,qJe),e(se,th),e(th,EU),e(EU,GJe),e(th,OJe),e(th,Zk),e(Zk,XJe),e(th,zJe),e(se,VJe),e(se,ah),e(ah,yU),e(yU,WJe),e(ah,QJe),e(ah,ex),e(ex,HJe),e(ah,UJe),e(se,JJe),e(se,nh),e(nh,wU),e(wU,YJe),e(nh,KJe),e(nh,ox),e(ox,ZJe),e(nh,eYe),e(se,oYe),e(se,sh),e(sh,AU),e(AU,rYe),e(sh,tYe),e(sh,rx),e(rx,aYe),e(sh,nYe),e(se,sYe),e(se,lh),e(lh,LU),e(LU,lYe),e(lh,iYe),e(lh,tx),e(tx,dYe),e(lh,cYe),e(Le,fYe),g(ih,Le,null),e(Le,mYe),e(Le,BU),e(BU,gYe),e(Le,hYe),g(w4,Le,null),e(Xo,pYe),e(Xo,dh),g(A4,dh,null),e(dh,_Ye),e(dh,kU),e(kU,uYe),b(d,hLe,u),b(d,Ni,u),e(Ni,ch),e(ch,xU),g(L4,xU,null),e(Ni,bYe),e(Ni,RU),e(RU,vYe),b(d,pLe,u),b(d,zo,u),g(B4,zo,null),e(zo,TYe),e(zo,k4),e(k4,FYe),e(k4,ax),e(ax,CYe),e(k4,MYe),e(zo,EYe),e(zo,x4),e(x4,yYe),e(x4,SU),e(SU,wYe),e(x4,AYe),e(zo,LYe),e(zo,Be),g(R4,Be,null),e(Be,BYe),e(Be,PU),e(PU,kYe),e(Be,xYe),e(Be,Di),e(Di,RYe),e(Di,$U),e($U,SYe),e(Di,PYe),e(Di,IU),e(IU,$Ye),e(Di,IYe),e(Be,jYe),e(Be,we),e(we,fh),e(fh,jU),e(jU,NYe),e(fh,DYe),e(fh,nx),e(nx,qYe),e(fh,GYe),e(we,OYe),e(we,mh),e(mh,NU),e(NU,XYe),e(mh,zYe),e(mh,sx),e(sx,VYe),e(mh,WYe),e(we,QYe),e(we,gh),e(gh,DU),e(DU,HYe),e(gh,UYe),e(gh,lx),e(lx,JYe),e(gh,YYe),e(we,KYe),e(we,hh),e(hh,qU),e(qU,ZYe),e(hh,eKe),e(hh,ix),e(ix,oKe),e(hh,rKe),e(we,tKe),e(we,ph),e(ph,GU),e(GU,aKe),e(ph,nKe),e(ph,dx),e(dx,sKe),e(ph,lKe),e(we,iKe),e(we,_h),e(_h,OU),e(OU,dKe),e(_h,cKe),e(_h,cx),e(cx,fKe),e(_h,mKe),e(we,gKe),e(we,uh),e(uh,XU),e(XU,hKe),e(uh,pKe),e(uh,fx),e(fx,_Ke),e(uh,uKe),e(we,bKe),e(we,bh),e(bh,zU),e(zU,vKe),e(bh,TKe),e(bh,mx),e(mx,FKe),e(bh,CKe),e(Be,MKe),g(vh,Be,null),e(Be,EKe),e(Be,VU),e(VU,yKe),e(Be,wKe),g(S4,Be,null),e(zo,AKe),e(zo,Th),g(P4,Th,null),e(Th,LKe),e(Th,WU),e(WU,BKe),b(d,_Le,u),b(d,qi,u),e(qi,Fh),e(Fh,QU),g($4,QU,null),e(qi,kKe),e(qi,HU),e(HU,xKe),b(d,uLe,u),b(d,Vo,u),g(I4,Vo,null),e(Vo,RKe),e(Vo,Gi),e(Gi,SKe),e(Gi,UU),e(UU,PKe),e(Gi,$Ke),e(Gi,JU),e(JU,IKe),e(Gi,jKe),e(Vo,NKe),e(Vo,j4),e(j4,DKe),e(j4,YU),e(YU,qKe),e(j4,GKe),e(Vo,OKe),e(Vo,Nr),g(N4,Nr,null),e(Nr,XKe),e(Nr,KU),e(KU,zKe),e(Nr,VKe),e(Nr,Oi),e(Oi,WKe),e(Oi,ZU),e(ZU,QKe),e(Oi,HKe),e(Oi,eJ),e(eJ,UKe),e(Oi,JKe),e(Nr,YKe),e(Nr,oJ),e(oJ,KKe),e(Nr,ZKe),g(D4,Nr,null),e(Vo,eZe),e(Vo,ke),g(q4,ke,null),e(ke,oZe),e(ke,rJ),e(rJ,rZe),e(ke,tZe),e(ke,Da),e(Da,aZe),e(Da,tJ),e(tJ,nZe),e(Da,sZe),e(Da,aJ),e(aJ,lZe),e(Da,iZe),e(Da,nJ),e(nJ,dZe),e(Da,cZe),e(ke,fZe),e(ke,F),e(F,Ch),e(Ch,sJ),e(sJ,mZe),e(Ch,gZe),e(Ch,gx),e(gx,hZe),e(Ch,pZe),e(F,_Ze),e(F,Mh),e(Mh,lJ),e(lJ,uZe),e(Mh,bZe),e(Mh,hx),e(hx,vZe),e(Mh,TZe),e(F,FZe),e(F,Eh),e(Eh,iJ),e(iJ,CZe),e(Eh,MZe),e(Eh,px),e(px,EZe),e(Eh,yZe),e(F,wZe),e(F,yh),e(yh,dJ),e(dJ,AZe),e(yh,LZe),e(yh,_x),e(_x,BZe),e(yh,kZe),e(F,xZe),e(F,wh),e(wh,cJ),e(cJ,RZe),e(wh,SZe),e(wh,ux),e(ux,PZe),e(wh,$Ze),e(F,IZe),e(F,Ah),e(Ah,fJ),e(fJ,jZe),e(Ah,NZe),e(Ah,bx),e(bx,DZe),e(Ah,qZe),e(F,GZe),e(F,Lh),e(Lh,mJ),e(mJ,OZe),e(Lh,XZe),e(Lh,vx),e(vx,zZe),e(Lh,VZe),e(F,WZe),e(F,Bh),e(Bh,gJ),e(gJ,QZe),e(Bh,HZe),e(Bh,Tx),e(Tx,UZe),e(Bh,JZe),e(F,YZe),e(F,kh),e(kh,hJ),e(hJ,KZe),e(kh,ZZe),e(kh,Fx),e(Fx,eeo),e(kh,oeo),e(F,reo),e(F,xh),e(xh,pJ),e(pJ,teo),e(xh,aeo),e(xh,Cx),e(Cx,neo),e(xh,seo),e(F,leo),e(F,Rh),e(Rh,_J),e(_J,ieo),e(Rh,deo),e(Rh,Mx),e(Mx,ceo),e(Rh,feo),e(F,meo),e(F,Sh),e(Sh,uJ),e(uJ,geo),e(Sh,heo),e(Sh,Ex),e(Ex,peo),e(Sh,_eo),e(F,ueo),e(F,Ph),e(Ph,bJ),e(bJ,beo),e(Ph,veo),e(Ph,yx),e(yx,Teo),e(Ph,Feo),e(F,Ceo),e(F,$h),e($h,vJ),e(vJ,Meo),e($h,Eeo),e($h,wx),e(wx,yeo),e($h,weo),e(F,Aeo),e(F,Ih),e(Ih,TJ),e(TJ,Leo),e(Ih,Beo),e(Ih,Ax),e(Ax,keo),e(Ih,xeo),e(F,Reo),e(F,jh),e(jh,FJ),e(FJ,Seo),e(jh,Peo),e(jh,Lx),e(Lx,$eo),e(jh,Ieo),e(F,jeo),e(F,Nh),e(Nh,CJ),e(CJ,Neo),e(Nh,Deo),e(Nh,Bx),e(Bx,qeo),e(Nh,Geo),e(F,Oeo),e(F,Dh),e(Dh,MJ),e(MJ,Xeo),e(Dh,zeo),e(Dh,kx),e(kx,Veo),e(Dh,Weo),e(F,Qeo),e(F,qh),e(qh,EJ),e(EJ,Heo),e(qh,Ueo),e(qh,xx),e(xx,Jeo),e(qh,Yeo),e(F,Keo),e(F,Gh),e(Gh,yJ),e(yJ,Zeo),e(Gh,eoo),e(Gh,Rx),e(Rx,ooo),e(Gh,roo),e(F,too),e(F,Oh),e(Oh,wJ),e(wJ,aoo),e(Oh,noo),e(Oh,Sx),e(Sx,soo),e(Oh,loo),e(F,ioo),e(F,Xh),e(Xh,AJ),e(AJ,doo),e(Xh,coo),e(Xh,Px),e(Px,foo),e(Xh,moo),e(F,goo),e(F,zh),e(zh,LJ),e(LJ,hoo),e(zh,poo),e(zh,$x),e($x,_oo),e(zh,uoo),e(F,boo),e(F,Vh),e(Vh,BJ),e(BJ,voo),e(Vh,Too),e(Vh,Ix),e(Ix,Foo),e(Vh,Coo),e(F,Moo),e(F,Wh),e(Wh,kJ),e(kJ,Eoo),e(Wh,yoo),e(Wh,jx),e(jx,woo),e(Wh,Aoo),e(F,Loo),e(F,xs),e(xs,xJ),e(xJ,Boo),e(xs,koo),e(xs,Nx),e(Nx,xoo),e(xs,Roo),e(xs,Dx),e(Dx,Soo),e(xs,Poo),e(F,$oo),e(F,Qh),e(Qh,RJ),e(RJ,Ioo),e(Qh,joo),e(Qh,qx),e(qx,Noo),e(Qh,Doo),e(F,qoo),e(F,Hh),e(Hh,SJ),e(SJ,Goo),e(Hh,Ooo),e(Hh,Gx),e(Gx,Xoo),e(Hh,zoo),e(F,Voo),e(F,Uh),e(Uh,PJ),e(PJ,Woo),e(Uh,Qoo),e(Uh,Ox),e(Ox,Hoo),e(Uh,Uoo),e(F,Joo),e(F,Jh),e(Jh,$J),e($J,Yoo),e(Jh,Koo),e(Jh,Xx),e(Xx,Zoo),e(Jh,ero),e(F,oro),e(F,Yh),e(Yh,IJ),e(IJ,rro),e(Yh,tro),e(Yh,zx),e(zx,aro),e(Yh,nro),e(F,sro),e(F,Kh),e(Kh,jJ),e(jJ,lro),e(Kh,iro),e(Kh,Vx),e(Vx,dro),e(Kh,cro),e(F,fro),e(F,Zh),e(Zh,NJ),e(NJ,mro),e(Zh,gro),e(Zh,Wx),e(Wx,hro),e(Zh,pro),e(F,_ro),e(F,ep),e(ep,DJ),e(DJ,uro),e(ep,bro),e(ep,Qx),e(Qx,vro),e(ep,Tro),e(F,Fro),e(F,op),e(op,qJ),e(qJ,Cro),e(op,Mro),e(op,Hx),e(Hx,Ero),e(op,yro),e(F,wro),e(F,rp),e(rp,GJ),e(GJ,Aro),e(rp,Lro),e(rp,Ux),e(Ux,Bro),e(rp,kro),e(F,xro),e(F,tp),e(tp,OJ),e(OJ,Rro),e(tp,Sro),e(tp,Jx),e(Jx,Pro),e(tp,$ro),e(F,Iro),e(F,ap),e(ap,XJ),e(XJ,jro),e(ap,Nro),e(ap,Yx),e(Yx,Dro),e(ap,qro),e(F,Gro),e(F,np),e(np,zJ),e(zJ,Oro),e(np,Xro),e(np,Kx),e(Kx,zro),e(np,Vro),e(F,Wro),e(F,sp),e(sp,VJ),e(VJ,Qro),e(sp,Hro),e(sp,Zx),e(Zx,Uro),e(sp,Jro),e(F,Yro),e(F,lp),e(lp,WJ),e(WJ,Kro),e(lp,Zro),e(lp,eR),e(eR,eto),e(lp,oto),e(F,rto),e(F,ip),e(ip,QJ),e(QJ,tto),e(ip,ato),e(ip,oR),e(oR,nto),e(ip,sto),e(F,lto),e(F,dp),e(dp,HJ),e(HJ,ito),e(dp,dto),e(dp,rR),e(rR,cto),e(dp,fto),e(F,mto),e(F,cp),e(cp,UJ),e(UJ,gto),e(cp,hto),e(cp,tR),e(tR,pto),e(cp,_to),e(F,uto),e(F,fp),e(fp,JJ),e(JJ,bto),e(fp,vto),e(fp,aR),e(aR,Tto),e(fp,Fto),e(F,Cto),e(F,mp),e(mp,YJ),e(YJ,Mto),e(mp,Eto),e(mp,nR),e(nR,yto),e(mp,wto),e(F,Ato),e(F,gp),e(gp,KJ),e(KJ,Lto),e(gp,Bto),e(gp,sR),e(sR,kto),e(gp,xto),e(F,Rto),e(F,hp),e(hp,ZJ),e(ZJ,Sto),e(hp,Pto),e(hp,lR),e(lR,$to),e(hp,Ito),e(F,jto),e(F,pp),e(pp,eY),e(eY,Nto),e(pp,Dto),e(pp,iR),e(iR,qto),e(pp,Gto),e(F,Oto),e(F,_p),e(_p,oY),e(oY,Xto),e(_p,zto),e(_p,dR),e(dR,Vto),e(_p,Wto),e(F,Qto),e(F,up),e(up,rY),e(rY,Hto),e(up,Uto),e(up,cR),e(cR,Jto),e(up,Yto),e(F,Kto),e(F,bp),e(bp,tY),e(tY,Zto),e(bp,eao),e(bp,fR),e(fR,oao),e(bp,rao),e(F,tao),e(F,vp),e(vp,aY),e(aY,aao),e(vp,nao),e(vp,mR),e(mR,sao),e(vp,lao),e(F,iao),e(F,Tp),e(Tp,nY),e(nY,dao),e(Tp,cao),e(Tp,gR),e(gR,fao),e(Tp,mao),e(F,gao),e(F,Fp),e(Fp,sY),e(sY,hao),e(Fp,pao),e(Fp,hR),e(hR,_ao),e(Fp,uao),e(F,bao),e(F,Cp),e(Cp,lY),e(lY,vao),e(Cp,Tao),e(Cp,pR),e(pR,Fao),e(Cp,Cao),e(F,Mao),e(F,Mp),e(Mp,iY),e(iY,Eao),e(Mp,yao),e(Mp,_R),e(_R,wao),e(Mp,Aao),e(F,Lao),e(F,Ep),e(Ep,dY),e(dY,Bao),e(Ep,kao),e(Ep,uR),e(uR,xao),e(Ep,Rao),e(F,Sao),e(F,yp),e(yp,cY),e(cY,Pao),e(yp,$ao),e(yp,bR),e(bR,Iao),e(yp,jao),e(F,Nao),e(F,wp),e(wp,fY),e(fY,Dao),e(wp,qao),e(wp,vR),e(vR,Gao),e(wp,Oao),e(F,Xao),e(F,Ap),e(Ap,mY),e(mY,zao),e(Ap,Vao),e(Ap,TR),e(TR,Wao),e(Ap,Qao),e(F,Hao),e(F,Lp),e(Lp,gY),e(gY,Uao),e(Lp,Jao),e(Lp,FR),e(FR,Yao),e(Lp,Kao),e(F,Zao),e(F,Bp),e(Bp,hY),e(hY,eno),e(Bp,ono),e(Bp,CR),e(CR,rno),e(Bp,tno),e(F,ano),e(F,kp),e(kp,pY),e(pY,nno),e(kp,sno),e(kp,MR),e(MR,lno),e(kp,ino),e(F,dno),e(F,xp),e(xp,_Y),e(_Y,cno),e(xp,fno),e(xp,ER),e(ER,mno),e(xp,gno),e(F,hno),e(F,Rp),e(Rp,uY),e(uY,pno),e(Rp,_no),e(Rp,yR),e(yR,uno),e(Rp,bno),e(F,vno),e(F,Sp),e(Sp,bY),e(bY,Tno),e(Sp,Fno),e(Sp,wR),e(wR,Cno),e(Sp,Mno),e(F,Eno),e(F,Pp),e(Pp,vY),e(vY,yno),e(Pp,wno),e(Pp,AR),e(AR,Ano),e(Pp,Lno),e(F,Bno),e(F,$p),e($p,TY),e(TY,kno),e($p,xno),e($p,LR),e(LR,Rno),e($p,Sno),e(F,Pno),e(F,Ip),e(Ip,FY),e(FY,$no),e(Ip,Ino),e(Ip,BR),e(BR,jno),e(Ip,Nno),e(F,Dno),e(F,jp),e(jp,CY),e(CY,qno),e(jp,Gno),e(jp,kR),e(kR,Ono),e(jp,Xno),e(F,zno),e(F,Np),e(Np,MY),e(MY,Vno),e(Np,Wno),e(Np,xR),e(xR,Qno),e(Np,Hno),e(F,Uno),e(F,Dp),e(Dp,EY),e(EY,Jno),e(Dp,Yno),e(Dp,RR),e(RR,Kno),e(Dp,Zno),e(F,eso),e(F,qp),e(qp,yY),e(yY,oso),e(qp,rso),e(qp,SR),e(SR,tso),e(qp,aso),e(F,nso),e(F,Gp),e(Gp,wY),e(wY,sso),e(Gp,lso),e(Gp,PR),e(PR,iso),e(Gp,dso),e(F,cso),e(F,Op),e(Op,AY),e(AY,fso),e(Op,mso),e(Op,$R),e($R,gso),e(Op,hso),e(F,pso),e(F,Xp),e(Xp,LY),e(LY,_so),e(Xp,uso),e(Xp,IR),e(IR,bso),e(Xp,vso),e(F,Tso),e(F,zp),e(zp,BY),e(BY,Fso),e(zp,Cso),e(zp,jR),e(jR,Mso),e(zp,Eso),e(F,yso),e(F,Vp),e(Vp,kY),e(kY,wso),e(Vp,Aso),e(Vp,NR),e(NR,Lso),e(Vp,Bso),e(F,kso),e(F,Wp),e(Wp,xY),e(xY,xso),e(Wp,Rso),e(Wp,DR),e(DR,Sso),e(Wp,Pso),e(F,$so),e(F,Qp),e(Qp,RY),e(RY,Iso),e(Qp,jso),e(Qp,qR),e(qR,Nso),e(Qp,Dso),e(F,qso),e(F,Hp),e(Hp,SY),e(SY,Gso),e(Hp,Oso),e(Hp,GR),e(GR,Xso),e(Hp,zso),e(F,Vso),e(F,Up),e(Up,PY),e(PY,Wso),e(Up,Qso),e(Up,OR),e(OR,Hso),e(Up,Uso),e(F,Jso),e(F,Jp),e(Jp,$Y),e($Y,Yso),e(Jp,Kso),e(Jp,XR),e(XR,Zso),e(Jp,elo),e(ke,olo),e(ke,Yp),e(Yp,rlo),e(Yp,IY),e(IY,tlo),e(Yp,alo),e(Yp,jY),e(jY,nlo),e(ke,slo),e(ke,NY),e(NY,llo),e(ke,ilo),g(G4,ke,null),b(d,bLe,u),b(d,Xi,u),e(Xi,Kp),e(Kp,DY),g(O4,DY,null),e(Xi,dlo),e(Xi,qY),e(qY,clo),b(d,vLe,u),b(d,Wo,u),g(X4,Wo,null),e(Wo,flo),e(Wo,zi),e(zi,mlo),e(zi,GY),e(GY,glo),e(zi,hlo),e(zi,OY),e(OY,plo),e(zi,_lo),e(Wo,ulo),e(Wo,z4),e(z4,blo),e(z4,XY),e(XY,vlo),e(z4,Tlo),e(Wo,Flo),e(Wo,Dr),g(V4,Dr,null),e(Dr,Clo),e(Dr,zY),e(zY,Mlo),e(Dr,Elo),e(Dr,Vi),e(Vi,ylo),e(Vi,VY),e(VY,wlo),e(Vi,Alo),e(Vi,WY),e(WY,Llo),e(Vi,Blo),e(Dr,klo),e(Dr,QY),e(QY,xlo),e(Dr,Rlo),g(W4,Dr,null),e(Wo,Slo),e(Wo,xe),g(Q4,xe,null),e(xe,Plo),e(xe,HY),e(HY,$lo),e(xe,Ilo),e(xe,qa),e(qa,jlo),e(qa,UY),e(UY,Nlo),e(qa,Dlo),e(qa,JY),e(JY,qlo),e(qa,Glo),e(qa,YY),e(YY,Olo),e(qa,Xlo),e(xe,zlo),e(xe,x),e(x,Zp),e(Zp,KY),e(KY,Vlo),e(Zp,Wlo),e(Zp,zR),e(zR,Qlo),e(Zp,Hlo),e(x,Ulo),e(x,e_),e(e_,ZY),e(ZY,Jlo),e(e_,Ylo),e(e_,VR),e(VR,Klo),e(e_,Zlo),e(x,eio),e(x,o_),e(o_,eK),e(eK,oio),e(o_,rio),e(o_,WR),e(WR,tio),e(o_,aio),e(x,nio),e(x,r_),e(r_,oK),e(oK,sio),e(r_,lio),e(r_,QR),e(QR,iio),e(r_,dio),e(x,cio),e(x,t_),e(t_,rK),e(rK,fio),e(t_,mio),e(t_,HR),e(HR,gio),e(t_,hio),e(x,pio),e(x,a_),e(a_,tK),e(tK,_io),e(a_,uio),e(a_,UR),e(UR,bio),e(a_,vio),e(x,Tio),e(x,n_),e(n_,aK),e(aK,Fio),e(n_,Cio),e(n_,JR),e(JR,Mio),e(n_,Eio),e(x,yio),e(x,s_),e(s_,nK),e(nK,wio),e(s_,Aio),e(s_,YR),e(YR,Lio),e(s_,Bio),e(x,kio),e(x,l_),e(l_,sK),e(sK,xio),e(l_,Rio),e(l_,KR),e(KR,Sio),e(l_,Pio),e(x,$io),e(x,i_),e(i_,lK),e(lK,Iio),e(i_,jio),e(i_,ZR),e(ZR,Nio),e(i_,Dio),e(x,qio),e(x,d_),e(d_,iK),e(iK,Gio),e(d_,Oio),e(d_,eS),e(eS,Xio),e(d_,zio),e(x,Vio),e(x,c_),e(c_,dK),e(dK,Wio),e(c_,Qio),e(c_,oS),e(oS,Hio),e(c_,Uio),e(x,Jio),e(x,f_),e(f_,cK),e(cK,Yio),e(f_,Kio),e(f_,rS),e(rS,Zio),e(f_,edo),e(x,odo),e(x,m_),e(m_,fK),e(fK,rdo),e(m_,tdo),e(m_,tS),e(tS,ado),e(m_,ndo),e(x,sdo),e(x,g_),e(g_,mK),e(mK,ldo),e(g_,ido),e(g_,aS),e(aS,ddo),e(g_,cdo),e(x,fdo),e(x,h_),e(h_,gK),e(gK,mdo),e(h_,gdo),e(h_,nS),e(nS,hdo),e(h_,pdo),e(x,_do),e(x,p_),e(p_,hK),e(hK,udo),e(p_,bdo),e(p_,sS),e(sS,vdo),e(p_,Tdo),e(x,Fdo),e(x,__),e(__,pK),e(pK,Cdo),e(__,Mdo),e(__,lS),e(lS,Edo),e(__,ydo),e(x,wdo),e(x,u_),e(u_,_K),e(_K,Ado),e(u_,Ldo),e(u_,iS),e(iS,Bdo),e(u_,kdo),e(x,xdo),e(x,b_),e(b_,uK),e(uK,Rdo),e(b_,Sdo),e(b_,dS),e(dS,Pdo),e(b_,$do),e(x,Ido),e(x,v_),e(v_,bK),e(bK,jdo),e(v_,Ndo),e(v_,cS),e(cS,Ddo),e(v_,qdo),e(x,Gdo),e(x,T_),e(T_,vK),e(vK,Odo),e(T_,Xdo),e(T_,fS),e(fS,zdo),e(T_,Vdo),e(x,Wdo),e(x,F_),e(F_,TK),e(TK,Qdo),e(F_,Hdo),e(F_,mS),e(mS,Udo),e(F_,Jdo),e(x,Ydo),e(x,C_),e(C_,FK),e(FK,Kdo),e(C_,Zdo),e(C_,gS),e(gS,eco),e(C_,oco),e(x,rco),e(x,M_),e(M_,CK),e(CK,tco),e(M_,aco),e(M_,hS),e(hS,nco),e(M_,sco),e(x,lco),e(x,E_),e(E_,MK),e(MK,ico),e(E_,dco),e(E_,pS),e(pS,cco),e(E_,fco),e(x,mco),e(x,y_),e(y_,EK),e(EK,gco),e(y_,hco),e(y_,_S),e(_S,pco),e(y_,_co),e(x,uco),e(x,w_),e(w_,yK),e(yK,bco),e(w_,vco),e(w_,uS),e(uS,Tco),e(w_,Fco),e(x,Cco),e(x,A_),e(A_,wK),e(wK,Mco),e(A_,Eco),e(A_,bS),e(bS,yco),e(A_,wco),e(x,Aco),e(x,L_),e(L_,AK),e(AK,Lco),e(L_,Bco),e(L_,vS),e(vS,kco),e(L_,xco),e(x,Rco),e(x,B_),e(B_,LK),e(LK,Sco),e(B_,Pco),e(B_,TS),e(TS,$co),e(B_,Ico),e(x,jco),e(x,k_),e(k_,BK),e(BK,Nco),e(k_,Dco),e(k_,FS),e(FS,qco),e(k_,Gco),e(x,Oco),e(x,x_),e(x_,kK),e(kK,Xco),e(x_,zco),e(x_,CS),e(CS,Vco),e(x_,Wco),e(x,Qco),e(x,R_),e(R_,xK),e(xK,Hco),e(R_,Uco),e(R_,MS),e(MS,Jco),e(R_,Yco),e(x,Kco),e(x,S_),e(S_,RK),e(RK,Zco),e(S_,efo),e(S_,ES),e(ES,ofo),e(S_,rfo),e(x,tfo),e(x,P_),e(P_,SK),e(SK,afo),e(P_,nfo),e(P_,yS),e(yS,sfo),e(P_,lfo),e(x,ifo),e(x,$_),e($_,PK),e(PK,dfo),e($_,cfo),e($_,wS),e(wS,ffo),e($_,mfo),e(x,gfo),e(x,I_),e(I_,$K),e($K,hfo),e(I_,pfo),e(I_,AS),e(AS,_fo),e(I_,ufo),e(xe,bfo),e(xe,j_),e(j_,vfo),e(j_,IK),e(IK,Tfo),e(j_,Ffo),e(j_,jK),e(jK,Cfo),e(xe,Mfo),e(xe,NK),e(NK,Efo),e(xe,yfo),g(H4,xe,null),b(d,TLe,u),b(d,Wi,u),e(Wi,N_),e(N_,DK),g(U4,DK,null),e(Wi,wfo),e(Wi,qK),e(qK,Afo),b(d,FLe,u),b(d,Qo,u),g(J4,Qo,null),e(Qo,Lfo),e(Qo,Qi),e(Qi,Bfo),e(Qi,GK),e(GK,kfo),e(Qi,xfo),e(Qi,OK),e(OK,Rfo),e(Qi,Sfo),e(Qo,Pfo),e(Qo,Y4),e(Y4,$fo),e(Y4,XK),e(XK,Ifo),e(Y4,jfo),e(Qo,Nfo),e(Qo,qr),g(K4,qr,null),e(qr,Dfo),e(qr,zK),e(zK,qfo),e(qr,Gfo),e(qr,Hi),e(Hi,Ofo),e(Hi,VK),e(VK,Xfo),e(Hi,zfo),e(Hi,WK),e(WK,Vfo),e(Hi,Wfo),e(qr,Qfo),e(qr,QK),e(QK,Hfo),e(qr,Ufo),g(Z4,qr,null),e(Qo,Jfo),e(Qo,Re),g(eM,Re,null),e(Re,Yfo),e(Re,HK),e(HK,Kfo),e(Re,Zfo),e(Re,Ga),e(Ga,emo),e(Ga,UK),e(UK,omo),e(Ga,rmo),e(Ga,JK),e(JK,tmo),e(Ga,amo),e(Ga,YK),e(YK,nmo),e(Ga,smo),e(Re,lmo),e(Re,$),e($,D_),e(D_,KK),e(KK,imo),e(D_,dmo),e(D_,LS),e(LS,cmo),e(D_,fmo),e($,mmo),e($,q_),e(q_,ZK),e(ZK,gmo),e(q_,hmo),e(q_,BS),e(BS,pmo),e(q_,_mo),e($,umo),e($,G_),e(G_,eZ),e(eZ,bmo),e(G_,vmo),e(G_,kS),e(kS,Tmo),e(G_,Fmo),e($,Cmo),e($,O_),e(O_,oZ),e(oZ,Mmo),e(O_,Emo),e(O_,xS),e(xS,ymo),e(O_,wmo),e($,Amo),e($,X_),e(X_,rZ),e(rZ,Lmo),e(X_,Bmo),e(X_,RS),e(RS,kmo),e(X_,xmo),e($,Rmo),e($,z_),e(z_,tZ),e(tZ,Smo),e(z_,Pmo),e(z_,SS),e(SS,$mo),e(z_,Imo),e($,jmo),e($,V_),e(V_,aZ),e(aZ,Nmo),e(V_,Dmo),e(V_,PS),e(PS,qmo),e(V_,Gmo),e($,Omo),e($,W_),e(W_,nZ),e(nZ,Xmo),e(W_,zmo),e(W_,$S),e($S,Vmo),e(W_,Wmo),e($,Qmo),e($,Q_),e(Q_,sZ),e(sZ,Hmo),e(Q_,Umo),e(Q_,IS),e(IS,Jmo),e(Q_,Ymo),e($,Kmo),e($,H_),e(H_,lZ),e(lZ,Zmo),e(H_,ego),e(H_,jS),e(jS,ogo),e(H_,rgo),e($,tgo),e($,U_),e(U_,iZ),e(iZ,ago),e(U_,ngo),e(U_,NS),e(NS,sgo),e(U_,lgo),e($,igo),e($,J_),e(J_,dZ),e(dZ,dgo),e(J_,cgo),e(J_,DS),e(DS,fgo),e(J_,mgo),e($,ggo),e($,Y_),e(Y_,cZ),e(cZ,hgo),e(Y_,pgo),e(Y_,qS),e(qS,_go),e(Y_,ugo),e($,bgo),e($,K_),e(K_,fZ),e(fZ,vgo),e(K_,Tgo),e(K_,GS),e(GS,Fgo),e(K_,Cgo),e($,Mgo),e($,Z_),e(Z_,mZ),e(mZ,Ego),e(Z_,ygo),e(Z_,OS),e(OS,wgo),e(Z_,Ago),e($,Lgo),e($,eu),e(eu,gZ),e(gZ,Bgo),e(eu,kgo),e(eu,XS),e(XS,xgo),e(eu,Rgo),e($,Sgo),e($,ou),e(ou,hZ),e(hZ,Pgo),e(ou,$go),e(ou,zS),e(zS,Igo),e(ou,jgo),e($,Ngo),e($,ru),e(ru,pZ),e(pZ,Dgo),e(ru,qgo),e(ru,VS),e(VS,Ggo),e(ru,Ogo),e($,Xgo),e($,tu),e(tu,_Z),e(_Z,zgo),e(tu,Vgo),e(tu,WS),e(WS,Wgo),e(tu,Qgo),e($,Hgo),e($,au),e(au,uZ),e(uZ,Ugo),e(au,Jgo),e(au,QS),e(QS,Ygo),e(au,Kgo),e($,Zgo),e($,nu),e(nu,bZ),e(bZ,eho),e(nu,oho),e(nu,HS),e(HS,rho),e(nu,tho),e($,aho),e($,su),e(su,vZ),e(vZ,nho),e(su,sho),e(su,US),e(US,lho),e(su,iho),e($,dho),e($,lu),e(lu,TZ),e(TZ,cho),e(lu,fho),e(lu,JS),e(JS,mho),e(lu,gho),e($,hho),e($,iu),e(iu,FZ),e(FZ,pho),e(iu,_ho),e(iu,YS),e(YS,uho),e(iu,bho),e($,vho),e($,du),e(du,CZ),e(CZ,Tho),e(du,Fho),e(du,KS),e(KS,Cho),e(du,Mho),e($,Eho),e($,cu),e(cu,MZ),e(MZ,yho),e(cu,who),e(cu,ZS),e(ZS,Aho),e(cu,Lho),e($,Bho),e($,fu),e(fu,EZ),e(EZ,kho),e(fu,xho),e(fu,eP),e(eP,Rho),e(fu,Sho),e($,Pho),e($,mu),e(mu,yZ),e(yZ,$ho),e(mu,Iho),e(mu,oP),e(oP,jho),e(mu,Nho),e($,Dho),e($,gu),e(gu,wZ),e(wZ,qho),e(gu,Gho),e(gu,rP),e(rP,Oho),e(gu,Xho),e($,zho),e($,hu),e(hu,AZ),e(AZ,Vho),e(hu,Who),e(hu,tP),e(tP,Qho),e(hu,Hho),e($,Uho),e($,pu),e(pu,LZ),e(LZ,Jho),e(pu,Yho),e(pu,aP),e(aP,Kho),e(pu,Zho),e($,epo),e($,_u),e(_u,BZ),e(BZ,opo),e(_u,rpo),e(_u,nP),e(nP,tpo),e(_u,apo),e($,npo),e($,uu),e(uu,kZ),e(kZ,spo),e(uu,lpo),e(uu,sP),e(sP,ipo),e(uu,dpo),e($,cpo),e($,bu),e(bu,xZ),e(xZ,fpo),e(bu,mpo),e(bu,lP),e(lP,gpo),e(bu,hpo),e(Re,ppo),e(Re,vu),e(vu,_po),e(vu,RZ),e(RZ,upo),e(vu,bpo),e(vu,SZ),e(SZ,vpo),e(Re,Tpo),e(Re,PZ),e(PZ,Fpo),e(Re,Cpo),g(oM,Re,null),b(d,CLe,u),b(d,Ui,u),e(Ui,Tu),e(Tu,$Z),g(rM,$Z,null),e(Ui,Mpo),e(Ui,IZ),e(IZ,Epo),b(d,MLe,u),b(d,Ho,u),g(tM,Ho,null),e(Ho,ypo),e(Ho,Ji),e(Ji,wpo),e(Ji,jZ),e(jZ,Apo),e(Ji,Lpo),e(Ji,NZ),e(NZ,Bpo),e(Ji,kpo),e(Ho,xpo),e(Ho,aM),e(aM,Rpo),e(aM,DZ),e(DZ,Spo),e(aM,Ppo),e(Ho,$po),e(Ho,Gr),g(nM,Gr,null),e(Gr,Ipo),e(Gr,qZ),e(qZ,jpo),e(Gr,Npo),e(Gr,Yi),e(Yi,Dpo),e(Yi,GZ),e(GZ,qpo),e(Yi,Gpo),e(Yi,OZ),e(OZ,Opo),e(Yi,Xpo),e(Gr,zpo),e(Gr,XZ),e(XZ,Vpo),e(Gr,Wpo),g(sM,Gr,null),e(Ho,Qpo),e(Ho,Se),g(lM,Se,null),e(Se,Hpo),e(Se,zZ),e(zZ,Upo),e(Se,Jpo),e(Se,Oa),e(Oa,Ypo),e(Oa,VZ),e(VZ,Kpo),e(Oa,Zpo),e(Oa,WZ),e(WZ,e_o),e(Oa,o_o),e(Oa,QZ),e(QZ,r_o),e(Oa,t_o),e(Se,a_o),e(Se,I),e(I,Fu),e(Fu,HZ),e(HZ,n_o),e(Fu,s_o),e(Fu,iP),e(iP,l_o),e(Fu,i_o),e(I,d_o),e(I,Cu),e(Cu,UZ),e(UZ,c_o),e(Cu,f_o),e(Cu,dP),e(dP,m_o),e(Cu,g_o),e(I,h_o),e(I,Mu),e(Mu,JZ),e(JZ,p_o),e(Mu,__o),e(Mu,cP),e(cP,u_o),e(Mu,b_o),e(I,v_o),e(I,Eu),e(Eu,YZ),e(YZ,T_o),e(Eu,F_o),e(Eu,fP),e(fP,C_o),e(Eu,M_o),e(I,E_o),e(I,yu),e(yu,KZ),e(KZ,y_o),e(yu,w_o),e(yu,mP),e(mP,A_o),e(yu,L_o),e(I,B_o),e(I,wu),e(wu,ZZ),e(ZZ,k_o),e(wu,x_o),e(wu,gP),e(gP,R_o),e(wu,S_o),e(I,P_o),e(I,Au),e(Au,eee),e(eee,$_o),e(Au,I_o),e(Au,hP),e(hP,j_o),e(Au,N_o),e(I,D_o),e(I,Lu),e(Lu,oee),e(oee,q_o),e(Lu,G_o),e(Lu,pP),e(pP,O_o),e(Lu,X_o),e(I,z_o),e(I,Bu),e(Bu,ree),e(ree,V_o),e(Bu,W_o),e(Bu,_P),e(_P,Q_o),e(Bu,H_o),e(I,U_o),e(I,ku),e(ku,tee),e(tee,J_o),e(ku,Y_o),e(ku,uP),e(uP,K_o),e(ku,Z_o),e(I,euo),e(I,xu),e(xu,aee),e(aee,ouo),e(xu,ruo),e(xu,bP),e(bP,tuo),e(xu,auo),e(I,nuo),e(I,Ru),e(Ru,nee),e(nee,suo),e(Ru,luo),e(Ru,vP),e(vP,iuo),e(Ru,duo),e(I,cuo),e(I,Su),e(Su,see),e(see,fuo),e(Su,muo),e(Su,TP),e(TP,guo),e(Su,huo),e(I,puo),e(I,Pu),e(Pu,lee),e(lee,_uo),e(Pu,uuo),e(Pu,FP),e(FP,buo),e(Pu,vuo),e(I,Tuo),e(I,$u),e($u,iee),e(iee,Fuo),e($u,Cuo),e($u,CP),e(CP,Muo),e($u,Euo),e(I,yuo),e(I,Iu),e(Iu,dee),e(dee,wuo),e(Iu,Auo),e(Iu,MP),e(MP,Luo),e(Iu,Buo),e(I,kuo),e(I,ju),e(ju,cee),e(cee,xuo),e(ju,Ruo),e(ju,EP),e(EP,Suo),e(ju,Puo),e(I,$uo),e(I,Nu),e(Nu,fee),e(fee,Iuo),e(Nu,juo),e(Nu,yP),e(yP,Nuo),e(Nu,Duo),e(I,quo),e(I,Du),e(Du,mee),e(mee,Guo),e(Du,Ouo),e(Du,wP),e(wP,Xuo),e(Du,zuo),e(I,Vuo),e(I,qu),e(qu,gee),e(gee,Wuo),e(qu,Quo),e(qu,AP),e(AP,Huo),e(qu,Uuo),e(I,Juo),e(I,Gu),e(Gu,hee),e(hee,Yuo),e(Gu,Kuo),e(Gu,LP),e(LP,Zuo),e(Gu,e1o),e(I,o1o),e(I,Ou),e(Ou,pee),e(pee,r1o),e(Ou,t1o),e(Ou,BP),e(BP,a1o),e(Ou,n1o),e(I,s1o),e(I,Xu),e(Xu,_ee),e(_ee,l1o),e(Xu,i1o),e(Xu,kP),e(kP,d1o),e(Xu,c1o),e(I,f1o),e(I,zu),e(zu,uee),e(uee,m1o),e(zu,g1o),e(zu,xP),e(xP,h1o),e(zu,p1o),e(I,_1o),e(I,Vu),e(Vu,bee),e(bee,u1o),e(Vu,b1o),e(Vu,RP),e(RP,v1o),e(Vu,T1o),e(I,F1o),e(I,Wu),e(Wu,vee),e(vee,C1o),e(Wu,M1o),e(Wu,SP),e(SP,E1o),e(Wu,y1o),e(I,w1o),e(I,Qu),e(Qu,Tee),e(Tee,A1o),e(Qu,L1o),e(Qu,PP),e(PP,B1o),e(Qu,k1o),e(I,x1o),e(I,Hu),e(Hu,Fee),e(Fee,R1o),e(Hu,S1o),e(Hu,$P),e($P,P1o),e(Hu,$1o),e(I,I1o),e(I,Uu),e(Uu,Cee),e(Cee,j1o),e(Uu,N1o),e(Uu,IP),e(IP,D1o),e(Uu,q1o),e(I,G1o),e(I,Ju),e(Ju,Mee),e(Mee,O1o),e(Ju,X1o),e(Ju,Eee),e(Eee,z1o),e(Ju,V1o),e(I,W1o),e(I,Yu),e(Yu,yee),e(yee,Q1o),e(Yu,H1o),e(Yu,jP),e(jP,U1o),e(Yu,J1o),e(I,Y1o),e(I,Ku),e(Ku,wee),e(wee,K1o),e(Ku,Z1o),e(Ku,NP),e(NP,ebo),e(Ku,obo),e(I,rbo),e(I,Zu),e(Zu,Aee),e(Aee,tbo),e(Zu,abo),e(Zu,DP),e(DP,nbo),e(Zu,sbo),e(I,lbo),e(I,e1),e(e1,Lee),e(Lee,ibo),e(e1,dbo),e(e1,qP),e(qP,cbo),e(e1,fbo),e(Se,mbo),e(Se,o1),e(o1,gbo),e(o1,Bee),e(Bee,hbo),e(o1,pbo),e(o1,kee),e(kee,_bo),e(Se,ubo),e(Se,xee),e(xee,bbo),e(Se,vbo),g(iM,Se,null),b(d,ELe,u),b(d,Ki,u),e(Ki,r1),e(r1,Ree),g(dM,Ree,null),e(Ki,Tbo),e(Ki,See),e(See,Fbo),b(d,yLe,u),b(d,Uo,u),g(cM,Uo,null),e(Uo,Cbo),e(Uo,Zi),e(Zi,Mbo),e(Zi,Pee),e(Pee,Ebo),e(Zi,ybo),e(Zi,$ee),e($ee,wbo),e(Zi,Abo),e(Uo,Lbo),e(Uo,fM),e(fM,Bbo),e(fM,Iee),e(Iee,kbo),e(fM,xbo),e(Uo,Rbo),e(Uo,Or),g(mM,Or,null),e(Or,Sbo),e(Or,jee),e(jee,Pbo),e(Or,$bo),e(Or,ed),e(ed,Ibo),e(ed,Nee),e(Nee,jbo),e(ed,Nbo),e(ed,Dee),e(Dee,Dbo),e(ed,qbo),e(Or,Gbo),e(Or,qee),e(qee,Obo),e(Or,Xbo),g(gM,Or,null),e(Uo,zbo),e(Uo,Pe),g(hM,Pe,null),e(Pe,Vbo),e(Pe,Gee),e(Gee,Wbo),e(Pe,Qbo),e(Pe,Xa),e(Xa,Hbo),e(Xa,Oee),e(Oee,Ubo),e(Xa,Jbo),e(Xa,Xee),e(Xee,Ybo),e(Xa,Kbo),e(Xa,zee),e(zee,Zbo),e(Xa,e5o),e(Pe,o5o),e(Pe,ae),e(ae,t1),e(t1,Vee),e(Vee,r5o),e(t1,t5o),e(t1,GP),e(GP,a5o),e(t1,n5o),e(ae,s5o),e(ae,a1),e(a1,Wee),e(Wee,l5o),e(a1,i5o),e(a1,OP),e(OP,d5o),e(a1,c5o),e(ae,f5o),e(ae,n1),e(n1,Qee),e(Qee,m5o),e(n1,g5o),e(n1,XP),e(XP,h5o),e(n1,p5o),e(ae,_5o),e(ae,s1),e(s1,Hee),e(Hee,u5o),e(s1,b5o),e(s1,zP),e(zP,v5o),e(s1,T5o),e(ae,F5o),e(ae,l1),e(l1,Uee),e(Uee,C5o),e(l1,M5o),e(l1,VP),e(VP,E5o),e(l1,y5o),e(ae,w5o),e(ae,i1),e(i1,Jee),e(Jee,A5o),e(i1,L5o),e(i1,WP),e(WP,B5o),e(i1,k5o),e(ae,x5o),e(ae,d1),e(d1,Yee),e(Yee,R5o),e(d1,S5o),e(d1,QP),e(QP,P5o),e(d1,$5o),e(ae,I5o),e(ae,c1),e(c1,Kee),e(Kee,j5o),e(c1,N5o),e(c1,HP),e(HP,D5o),e(c1,q5o),e(ae,G5o),e(ae,f1),e(f1,Zee),e(Zee,O5o),e(f1,X5o),e(f1,UP),e(UP,z5o),e(f1,V5o),e(ae,W5o),e(ae,m1),e(m1,eoe),e(eoe,Q5o),e(m1,H5o),e(m1,JP),e(JP,U5o),e(m1,J5o),e(ae,Y5o),e(ae,g1),e(g1,ooe),e(ooe,K5o),e(g1,Z5o),e(g1,YP),e(YP,e2o),e(g1,o2o),e(ae,r2o),e(ae,h1),e(h1,roe),e(roe,t2o),e(h1,a2o),e(h1,KP),e(KP,n2o),e(h1,s2o),e(ae,l2o),e(ae,p1),e(p1,toe),e(toe,i2o),e(p1,d2o),e(p1,ZP),e(ZP,c2o),e(p1,f2o),e(ae,m2o),e(ae,_1),e(_1,aoe),e(aoe,g2o),e(_1,h2o),e(_1,e$),e(e$,p2o),e(_1,_2o),e(ae,u2o),e(ae,u1),e(u1,noe),e(noe,b2o),e(u1,v2o),e(u1,o$),e(o$,T2o),e(u1,F2o),e(ae,C2o),e(ae,b1),e(b1,soe),e(soe,M2o),e(b1,E2o),e(b1,r$),e(r$,y2o),e(b1,w2o),e(Pe,A2o),e(Pe,v1),e(v1,L2o),e(v1,loe),e(loe,B2o),e(v1,k2o),e(v1,ioe),e(ioe,x2o),e(Pe,R2o),e(Pe,doe),e(doe,S2o),e(Pe,P2o),g(pM,Pe,null),b(d,wLe,u),b(d,od,u),e(od,T1),e(T1,coe),g(_M,coe,null),e(od,$2o),e(od,foe),e(foe,I2o),b(d,ALe,u),b(d,Jo,u),g(uM,Jo,null),e(Jo,j2o),e(Jo,rd),e(rd,N2o),e(rd,moe),e(moe,D2o),e(rd,q2o),e(rd,goe),e(goe,G2o),e(rd,O2o),e(Jo,X2o),e(Jo,bM),e(bM,z2o),e(bM,hoe),e(hoe,V2o),e(bM,W2o),e(Jo,Q2o),e(Jo,Xr),g(vM,Xr,null),e(Xr,H2o),e(Xr,poe),e(poe,U2o),e(Xr,J2o),e(Xr,td),e(td,Y2o),e(td,_oe),e(_oe,K2o),e(td,Z2o),e(td,uoe),e(uoe,evo),e(td,ovo),e(Xr,rvo),e(Xr,boe),e(boe,tvo),e(Xr,avo),g(TM,Xr,null),e(Jo,nvo),e(Jo,$e),g(FM,$e,null),e($e,svo),e($e,voe),e(voe,lvo),e($e,ivo),e($e,za),e(za,dvo),e(za,Toe),e(Toe,cvo),e(za,fvo),e(za,Foe),e(Foe,mvo),e(za,gvo),e(za,Coe),e(Coe,hvo),e(za,pvo),e($e,_vo),e($e,A),e(A,F1),e(F1,Moe),e(Moe,uvo),e(F1,bvo),e(F1,t$),e(t$,vvo),e(F1,Tvo),e(A,Fvo),e(A,C1),e(C1,Eoe),e(Eoe,Cvo),e(C1,Mvo),e(C1,a$),e(a$,Evo),e(C1,yvo),e(A,wvo),e(A,M1),e(M1,yoe),e(yoe,Avo),e(M1,Lvo),e(M1,n$),e(n$,Bvo),e(M1,kvo),e(A,xvo),e(A,E1),e(E1,woe),e(woe,Rvo),e(E1,Svo),e(E1,s$),e(s$,Pvo),e(E1,$vo),e(A,Ivo),e(A,y1),e(y1,Aoe),e(Aoe,jvo),e(y1,Nvo),e(y1,l$),e(l$,Dvo),e(y1,qvo),e(A,Gvo),e(A,w1),e(w1,Loe),e(Loe,Ovo),e(w1,Xvo),e(w1,i$),e(i$,zvo),e(w1,Vvo),e(A,Wvo),e(A,A1),e(A1,Boe),e(Boe,Qvo),e(A1,Hvo),e(A1,d$),e(d$,Uvo),e(A1,Jvo),e(A,Yvo),e(A,L1),e(L1,koe),e(koe,Kvo),e(L1,Zvo),e(L1,c$),e(c$,e6o),e(L1,o6o),e(A,r6o),e(A,B1),e(B1,xoe),e(xoe,t6o),e(B1,a6o),e(B1,f$),e(f$,n6o),e(B1,s6o),e(A,l6o),e(A,k1),e(k1,Roe),e(Roe,i6o),e(k1,d6o),e(k1,m$),e(m$,c6o),e(k1,f6o),e(A,m6o),e(A,x1),e(x1,Soe),e(Soe,g6o),e(x1,h6o),e(x1,g$),e(g$,p6o),e(x1,_6o),e(A,u6o),e(A,R1),e(R1,Poe),e(Poe,b6o),e(R1,v6o),e(R1,h$),e(h$,T6o),e(R1,F6o),e(A,C6o),e(A,S1),e(S1,$oe),e($oe,M6o),e(S1,E6o),e(S1,p$),e(p$,y6o),e(S1,w6o),e(A,A6o),e(A,P1),e(P1,Ioe),e(Ioe,L6o),e(P1,B6o),e(P1,_$),e(_$,k6o),e(P1,x6o),e(A,R6o),e(A,$1),e($1,joe),e(joe,S6o),e($1,P6o),e($1,u$),e(u$,$6o),e($1,I6o),e(A,j6o),e(A,I1),e(I1,Noe),e(Noe,N6o),e(I1,D6o),e(I1,b$),e(b$,q6o),e(I1,G6o),e(A,O6o),e(A,j1),e(j1,Doe),e(Doe,X6o),e(j1,z6o),e(j1,v$),e(v$,V6o),e(j1,W6o),e(A,Q6o),e(A,N1),e(N1,qoe),e(qoe,H6o),e(N1,U6o),e(N1,T$),e(T$,J6o),e(N1,Y6o),e(A,K6o),e(A,D1),e(D1,Goe),e(Goe,Z6o),e(D1,eTo),e(D1,F$),e(F$,oTo),e(D1,rTo),e(A,tTo),e(A,q1),e(q1,Ooe),e(Ooe,aTo),e(q1,nTo),e(q1,C$),e(C$,sTo),e(q1,lTo),e(A,iTo),e(A,G1),e(G1,Xoe),e(Xoe,dTo),e(G1,cTo),e(G1,M$),e(M$,fTo),e(G1,mTo),e(A,gTo),e(A,O1),e(O1,zoe),e(zoe,hTo),e(O1,pTo),e(O1,E$),e(E$,_To),e(O1,uTo),e(A,bTo),e(A,X1),e(X1,Voe),e(Voe,vTo),e(X1,TTo),e(X1,y$),e(y$,FTo),e(X1,CTo),e(A,MTo),e(A,z1),e(z1,Woe),e(Woe,ETo),e(z1,yTo),e(z1,w$),e(w$,wTo),e(z1,ATo),e(A,LTo),e(A,V1),e(V1,Qoe),e(Qoe,BTo),e(V1,kTo),e(V1,A$),e(A$,xTo),e(V1,RTo),e(A,STo),e(A,W1),e(W1,Hoe),e(Hoe,PTo),e(W1,$To),e(W1,L$),e(L$,ITo),e(W1,jTo),e(A,NTo),e(A,Q1),e(Q1,Uoe),e(Uoe,DTo),e(Q1,qTo),e(Q1,B$),e(B$,GTo),e(Q1,OTo),e(A,XTo),e(A,H1),e(H1,Joe),e(Joe,zTo),e(H1,VTo),e(H1,k$),e(k$,WTo),e(H1,QTo),e(A,HTo),e(A,U1),e(U1,Yoe),e(Yoe,UTo),e(U1,JTo),e(U1,x$),e(x$,YTo),e(U1,KTo),e(A,ZTo),e(A,J1),e(J1,Koe),e(Koe,e7o),e(J1,o7o),e(J1,R$),e(R$,r7o),e(J1,t7o),e(A,a7o),e(A,Y1),e(Y1,Zoe),e(Zoe,n7o),e(Y1,s7o),e(Y1,S$),e(S$,l7o),e(Y1,i7o),e(A,d7o),e(A,K1),e(K1,ere),e(ere,c7o),e(K1,f7o),e(K1,P$),e(P$,m7o),e(K1,g7o),e(A,h7o),e(A,Z1),e(Z1,ore),e(ore,p7o),e(Z1,_7o),e(Z1,$$),e($$,u7o),e(Z1,b7o),e(A,v7o),e(A,eb),e(eb,rre),e(rre,T7o),e(eb,F7o),e(eb,I$),e(I$,C7o),e(eb,M7o),e(A,E7o),e(A,ob),e(ob,tre),e(tre,y7o),e(ob,w7o),e(ob,j$),e(j$,A7o),e(ob,L7o),e(A,B7o),e(A,rb),e(rb,are),e(are,k7o),e(rb,x7o),e(rb,N$),e(N$,R7o),e(rb,S7o),e(A,P7o),e(A,tb),e(tb,nre),e(nre,$7o),e(tb,I7o),e(tb,D$),e(D$,j7o),e(tb,N7o),e(A,D7o),e(A,ab),e(ab,sre),e(sre,q7o),e(ab,G7o),e(ab,q$),e(q$,O7o),e(ab,X7o),e(A,z7o),e(A,nb),e(nb,lre),e(lre,V7o),e(nb,W7o),e(nb,G$),e(G$,Q7o),e(nb,H7o),e(A,U7o),e(A,sb),e(sb,ire),e(ire,J7o),e(sb,Y7o),e(sb,O$),e(O$,K7o),e(sb,Z7o),e(A,eFo),e(A,lb),e(lb,dre),e(dre,oFo),e(lb,rFo),e(lb,X$),e(X$,tFo),e(lb,aFo),e(A,nFo),e(A,ib),e(ib,cre),e(cre,sFo),e(ib,lFo),e(ib,z$),e(z$,iFo),e(ib,dFo),e(A,cFo),e(A,db),e(db,fre),e(fre,fFo),e(db,mFo),e(db,V$),e(V$,gFo),e(db,hFo),e(A,pFo),e(A,cb),e(cb,mre),e(mre,_Fo),e(cb,uFo),e(cb,W$),e(W$,bFo),e(cb,vFo),e(A,TFo),e(A,fb),e(fb,gre),e(gre,FFo),e(fb,CFo),e(fb,Q$),e(Q$,MFo),e(fb,EFo),e($e,yFo),e($e,mb),e(mb,wFo),e(mb,hre),e(hre,AFo),e(mb,LFo),e(mb,pre),e(pre,BFo),e($e,kFo),e($e,_re),e(_re,xFo),e($e,RFo),g(CM,$e,null),b(d,LLe,u),b(d,ad,u),e(ad,gb),e(gb,ure),g(MM,ure,null),e(ad,SFo),e(ad,bre),e(bre,PFo),b(d,BLe,u),b(d,Yo,u),g(EM,Yo,null),e(Yo,$Fo),e(Yo,nd),e(nd,IFo),e(nd,vre),e(vre,jFo),e(nd,NFo),e(nd,Tre),e(Tre,DFo),e(nd,qFo),e(Yo,GFo),e(Yo,yM),e(yM,OFo),e(yM,Fre),e(Fre,XFo),e(yM,zFo),e(Yo,VFo),e(Yo,zr),g(wM,zr,null),e(zr,WFo),e(zr,Cre),e(Cre,QFo),e(zr,HFo),e(zr,sd),e(sd,UFo),e(sd,Mre),e(Mre,JFo),e(sd,YFo),e(sd,Ere),e(Ere,KFo),e(sd,ZFo),e(zr,e9o),e(zr,yre),e(yre,o9o),e(zr,r9o),g(AM,zr,null),e(Yo,t9o),e(Yo,Ie),g(LM,Ie,null),e(Ie,a9o),e(Ie,wre),e(wre,n9o),e(Ie,s9o),e(Ie,Va),e(Va,l9o),e(Va,Are),e(Are,i9o),e(Va,d9o),e(Va,Lre),e(Lre,c9o),e(Va,f9o),e(Va,Bre),e(Bre,m9o),e(Va,g9o),e(Ie,h9o),e(Ie,G),e(G,hb),e(hb,kre),e(kre,p9o),e(hb,_9o),e(hb,H$),e(H$,u9o),e(hb,b9o),e(G,v9o),e(G,pb),e(pb,xre),e(xre,T9o),e(pb,F9o),e(pb,U$),e(U$,C9o),e(pb,M9o),e(G,E9o),e(G,_b),e(_b,Rre),e(Rre,y9o),e(_b,w9o),e(_b,J$),e(J$,A9o),e(_b,L9o),e(G,B9o),e(G,ub),e(ub,Sre),e(Sre,k9o),e(ub,x9o),e(ub,Y$),e(Y$,R9o),e(ub,S9o),e(G,P9o),e(G,bb),e(bb,Pre),e(Pre,$9o),e(bb,I9o),e(bb,K$),e(K$,j9o),e(bb,N9o),e(G,D9o),e(G,vb),e(vb,$re),e($re,q9o),e(vb,G9o),e(vb,Z$),e(Z$,O9o),e(vb,X9o),e(G,z9o),e(G,Tb),e(Tb,Ire),e(Ire,V9o),e(Tb,W9o),e(Tb,eI),e(eI,Q9o),e(Tb,H9o),e(G,U9o),e(G,Fb),e(Fb,jre),e(jre,J9o),e(Fb,Y9o),e(Fb,oI),e(oI,K9o),e(Fb,Z9o),e(G,eCo),e(G,Cb),e(Cb,Nre),e(Nre,oCo),e(Cb,rCo),e(Cb,rI),e(rI,tCo),e(Cb,aCo),e(G,nCo),e(G,Mb),e(Mb,Dre),e(Dre,sCo),e(Mb,lCo),e(Mb,tI),e(tI,iCo),e(Mb,dCo),e(G,cCo),e(G,Eb),e(Eb,qre),e(qre,fCo),e(Eb,mCo),e(Eb,aI),e(aI,gCo),e(Eb,hCo),e(G,pCo),e(G,yb),e(yb,Gre),e(Gre,_Co),e(yb,uCo),e(yb,nI),e(nI,bCo),e(yb,vCo),e(G,TCo),e(G,wb),e(wb,Ore),e(Ore,FCo),e(wb,CCo),e(wb,sI),e(sI,MCo),e(wb,ECo),e(G,yCo),e(G,Ab),e(Ab,Xre),e(Xre,wCo),e(Ab,ACo),e(Ab,lI),e(lI,LCo),e(Ab,BCo),e(G,kCo),e(G,Lb),e(Lb,zre),e(zre,xCo),e(Lb,RCo),e(Lb,iI),e(iI,SCo),e(Lb,PCo),e(G,$Co),e(G,Bb),e(Bb,Vre),e(Vre,ICo),e(Bb,jCo),e(Bb,dI),e(dI,NCo),e(Bb,DCo),e(G,qCo),e(G,kb),e(kb,Wre),e(Wre,GCo),e(kb,OCo),e(kb,cI),e(cI,XCo),e(kb,zCo),e(G,VCo),e(G,xb),e(xb,Qre),e(Qre,WCo),e(xb,QCo),e(xb,fI),e(fI,HCo),e(xb,UCo),e(G,JCo),e(G,Rb),e(Rb,Hre),e(Hre,YCo),e(Rb,KCo),e(Rb,mI),e(mI,ZCo),e(Rb,e4o),e(G,o4o),e(G,Sb),e(Sb,Ure),e(Ure,r4o),e(Sb,t4o),e(Sb,gI),e(gI,a4o),e(Sb,n4o),e(G,s4o),e(G,Pb),e(Pb,Jre),e(Jre,l4o),e(Pb,i4o),e(Pb,hI),e(hI,d4o),e(Pb,c4o),e(G,f4o),e(G,$b),e($b,Yre),e(Yre,m4o),e($b,g4o),e($b,pI),e(pI,h4o),e($b,p4o),e(G,_4o),e(G,Ib),e(Ib,Kre),e(Kre,u4o),e(Ib,b4o),e(Ib,_I),e(_I,v4o),e(Ib,T4o),e(G,F4o),e(G,jb),e(jb,Zre),e(Zre,C4o),e(jb,M4o),e(jb,uI),e(uI,E4o),e(jb,y4o),e(G,w4o),e(G,Nb),e(Nb,ete),e(ete,A4o),e(Nb,L4o),e(Nb,bI),e(bI,B4o),e(Nb,k4o),e(G,x4o),e(G,Db),e(Db,ote),e(ote,R4o),e(Db,S4o),e(Db,vI),e(vI,P4o),e(Db,$4o),e(G,I4o),e(G,qb),e(qb,rte),e(rte,j4o),e(qb,N4o),e(qb,TI),e(TI,D4o),e(qb,q4o),e(Ie,G4o),e(Ie,Gb),e(Gb,O4o),e(Gb,tte),e(tte,X4o),e(Gb,z4o),e(Gb,ate),e(ate,V4o),e(Ie,W4o),e(Ie,nte),e(nte,Q4o),e(Ie,H4o),g(BM,Ie,null),b(d,kLe,u),b(d,ld,u),e(ld,Ob),e(Ob,ste),g(kM,ste,null),e(ld,U4o),e(ld,lte),e(lte,J4o),b(d,xLe,u),b(d,Ko,u),g(xM,Ko,null),e(Ko,Y4o),e(Ko,id),e(id,K4o),e(id,ite),e(ite,Z4o),e(id,eMo),e(id,dte),e(dte,oMo),e(id,rMo),e(Ko,tMo),e(Ko,RM),e(RM,aMo),e(RM,cte),e(cte,nMo),e(RM,sMo),e(Ko,lMo),e(Ko,Vr),g(SM,Vr,null),e(Vr,iMo),e(Vr,fte),e(fte,dMo),e(Vr,cMo),e(Vr,dd),e(dd,fMo),e(dd,mte),e(mte,mMo),e(dd,gMo),e(dd,gte),e(gte,hMo),e(dd,pMo),e(Vr,_Mo),e(Vr,hte),e(hte,uMo),e(Vr,bMo),g(PM,Vr,null),e(Ko,vMo),e(Ko,je),g($M,je,null),e(je,TMo),e(je,pte),e(pte,FMo),e(je,CMo),e(je,Wa),e(Wa,MMo),e(Wa,_te),e(_te,EMo),e(Wa,yMo),e(Wa,ute),e(ute,wMo),e(Wa,AMo),e(Wa,bte),e(bte,LMo),e(Wa,BMo),e(je,kMo),e(je,na),e(na,Xb),e(Xb,vte),e(vte,xMo),e(Xb,RMo),e(Xb,FI),e(FI,SMo),e(Xb,PMo),e(na,$Mo),e(na,zb),e(zb,Tte),e(Tte,IMo),e(zb,jMo),e(zb,CI),e(CI,NMo),e(zb,DMo),e(na,qMo),e(na,Vb),e(Vb,Fte),e(Fte,GMo),e(Vb,OMo),e(Vb,MI),e(MI,XMo),e(Vb,zMo),e(na,VMo),e(na,Wb),e(Wb,Cte),e(Cte,WMo),e(Wb,QMo),e(Wb,EI),e(EI,HMo),e(Wb,UMo),e(na,JMo),e(na,Qb),e(Qb,Mte),e(Mte,YMo),e(Qb,KMo),e(Qb,yI),e(yI,ZMo),e(Qb,eEo),e(je,oEo),e(je,Hb),e(Hb,rEo),e(Hb,Ete),e(Ete,tEo),e(Hb,aEo),e(Hb,yte),e(yte,nEo),e(je,sEo),e(je,wte),e(wte,lEo),e(je,iEo),g(IM,je,null),b(d,RLe,u),b(d,cd,u),e(cd,Ub),e(Ub,Ate),g(jM,Ate,null),e(cd,dEo),e(cd,Lte),e(Lte,cEo),b(d,SLe,u),b(d,Zo,u),g(NM,Zo,null),e(Zo,fEo),e(Zo,fd),e(fd,mEo),e(fd,Bte),e(Bte,gEo),e(fd,hEo),e(fd,kte),e(kte,pEo),e(fd,_Eo),e(Zo,uEo),e(Zo,DM),e(DM,bEo),e(DM,xte),e(xte,vEo),e(DM,TEo),e(Zo,FEo),e(Zo,Wr),g(qM,Wr,null),e(Wr,CEo),e(Wr,Rte),e(Rte,MEo),e(Wr,EEo),e(Wr,md),e(md,yEo),e(md,Ste),e(Ste,wEo),e(md,AEo),e(md,Pte),e(Pte,LEo),e(md,BEo),e(Wr,kEo),e(Wr,$te),e($te,xEo),e(Wr,REo),g(GM,Wr,null),e(Zo,SEo),e(Zo,Ne),g(OM,Ne,null),e(Ne,PEo),e(Ne,Ite),e(Ite,$Eo),e(Ne,IEo),e(Ne,Qa),e(Qa,jEo),e(Qa,jte),e(jte,NEo),e(Qa,DEo),e(Qa,Nte),e(Nte,qEo),e(Qa,GEo),e(Qa,Dte),e(Dte,OEo),e(Qa,XEo),e(Ne,zEo),e(Ne,D),e(D,Jb),e(Jb,qte),e(qte,VEo),e(Jb,WEo),e(Jb,wI),e(wI,QEo),e(Jb,HEo),e(D,UEo),e(D,Yb),e(Yb,Gte),e(Gte,JEo),e(Yb,YEo),e(Yb,AI),e(AI,KEo),e(Yb,ZEo),e(D,e3o),e(D,Kb),e(Kb,Ote),e(Ote,o3o),e(Kb,r3o),e(Kb,LI),e(LI,t3o),e(Kb,a3o),e(D,n3o),e(D,Zb),e(Zb,Xte),e(Xte,s3o),e(Zb,l3o),e(Zb,BI),e(BI,i3o),e(Zb,d3o),e(D,c3o),e(D,e5),e(e5,zte),e(zte,f3o),e(e5,m3o),e(e5,kI),e(kI,g3o),e(e5,h3o),e(D,p3o),e(D,o5),e(o5,Vte),e(Vte,_3o),e(o5,u3o),e(o5,xI),e(xI,b3o),e(o5,v3o),e(D,T3o),e(D,r5),e(r5,Wte),e(Wte,F3o),e(r5,C3o),e(r5,RI),e(RI,M3o),e(r5,E3o),e(D,y3o),e(D,t5),e(t5,Qte),e(Qte,w3o),e(t5,A3o),e(t5,SI),e(SI,L3o),e(t5,B3o),e(D,k3o),e(D,a5),e(a5,Hte),e(Hte,x3o),e(a5,R3o),e(a5,PI),e(PI,S3o),e(a5,P3o),e(D,$3o),e(D,n5),e(n5,Ute),e(Ute,I3o),e(n5,j3o),e(n5,$I),e($I,N3o),e(n5,D3o),e(D,q3o),e(D,s5),e(s5,Jte),e(Jte,G3o),e(s5,O3o),e(s5,II),e(II,X3o),e(s5,z3o),e(D,V3o),e(D,l5),e(l5,Yte),e(Yte,W3o),e(l5,Q3o),e(l5,jI),e(jI,H3o),e(l5,U3o),e(D,J3o),e(D,i5),e(i5,Kte),e(Kte,Y3o),e(i5,K3o),e(i5,NI),e(NI,Z3o),e(i5,eyo),e(D,oyo),e(D,d5),e(d5,Zte),e(Zte,ryo),e(d5,tyo),e(d5,DI),e(DI,ayo),e(d5,nyo),e(D,syo),e(D,c5),e(c5,eae),e(eae,lyo),e(c5,iyo),e(c5,qI),e(qI,dyo),e(c5,cyo),e(D,fyo),e(D,f5),e(f5,oae),e(oae,myo),e(f5,gyo),e(f5,GI),e(GI,hyo),e(f5,pyo),e(D,_yo),e(D,m5),e(m5,rae),e(rae,uyo),e(m5,byo),e(m5,OI),e(OI,vyo),e(m5,Tyo),e(D,Fyo),e(D,g5),e(g5,tae),e(tae,Cyo),e(g5,Myo),e(g5,XI),e(XI,Eyo),e(g5,yyo),e(D,wyo),e(D,h5),e(h5,aae),e(aae,Ayo),e(h5,Lyo),e(h5,zI),e(zI,Byo),e(h5,kyo),e(D,xyo),e(D,p5),e(p5,nae),e(nae,Ryo),e(p5,Syo),e(p5,VI),e(VI,Pyo),e(p5,$yo),e(D,Iyo),e(D,_5),e(_5,sae),e(sae,jyo),e(_5,Nyo),e(_5,WI),e(WI,Dyo),e(_5,qyo),e(D,Gyo),e(D,u5),e(u5,lae),e(lae,Oyo),e(u5,Xyo),e(u5,QI),e(QI,zyo),e(u5,Vyo),e(D,Wyo),e(D,b5),e(b5,iae),e(iae,Qyo),e(b5,Hyo),e(b5,HI),e(HI,Uyo),e(b5,Jyo),e(D,Yyo),e(D,v5),e(v5,dae),e(dae,Kyo),e(v5,Zyo),e(v5,UI),e(UI,ewo),e(v5,owo),e(D,rwo),e(D,T5),e(T5,cae),e(cae,two),e(T5,awo),e(T5,JI),e(JI,nwo),e(T5,swo),e(D,lwo),e(D,F5),e(F5,fae),e(fae,iwo),e(F5,dwo),e(F5,YI),e(YI,cwo),e(F5,fwo),e(D,mwo),e(D,C5),e(C5,mae),e(mae,gwo),e(C5,hwo),e(C5,KI),e(KI,pwo),e(C5,_wo),e(D,uwo),e(D,M5),e(M5,gae),e(gae,bwo),e(M5,vwo),e(M5,ZI),e(ZI,Two),e(M5,Fwo),e(D,Cwo),e(D,E5),e(E5,hae),e(hae,Mwo),e(E5,Ewo),e(E5,ej),e(ej,ywo),e(E5,wwo),e(D,Awo),e(D,y5),e(y5,pae),e(pae,Lwo),e(y5,Bwo),e(y5,oj),e(oj,kwo),e(y5,xwo),e(D,Rwo),e(D,w5),e(w5,_ae),e(_ae,Swo),e(w5,Pwo),e(w5,rj),e(rj,$wo),e(w5,Iwo),e(D,jwo),e(D,A5),e(A5,uae),e(uae,Nwo),e(A5,Dwo),e(A5,tj),e(tj,qwo),e(A5,Gwo),e(Ne,Owo),e(Ne,L5),e(L5,Xwo),e(L5,bae),e(bae,zwo),e(L5,Vwo),e(L5,vae),e(vae,Wwo),e(Ne,Qwo),e(Ne,Tae),e(Tae,Hwo),e(Ne,Uwo),g(XM,Ne,null),b(d,PLe,u),b(d,gd,u),e(gd,B5),e(B5,Fae),g(zM,Fae,null),e(gd,Jwo),e(gd,Cae),e(Cae,Ywo),b(d,$Le,u),b(d,er,u),g(VM,er,null),e(er,Kwo),e(er,hd),e(hd,Zwo),e(hd,Mae),e(Mae,eAo),e(hd,oAo),e(hd,Eae),e(Eae,rAo),e(hd,tAo),e(er,aAo),e(er,WM),e(WM,nAo),e(WM,yae),e(yae,sAo),e(WM,lAo),e(er,iAo),e(er,Qr),g(QM,Qr,null),e(Qr,dAo),e(Qr,wae),e(wae,cAo),e(Qr,fAo),e(Qr,pd),e(pd,mAo),e(pd,Aae),e(Aae,gAo),e(pd,hAo),e(pd,Lae),e(Lae,pAo),e(pd,_Ao),e(Qr,uAo),e(Qr,Bae),e(Bae,bAo),e(Qr,vAo),g(HM,Qr,null),e(er,TAo),e(er,De),g(UM,De,null),e(De,FAo),e(De,kae),e(kae,CAo),e(De,MAo),e(De,Ha),e(Ha,EAo),e(Ha,xae),e(xae,yAo),e(Ha,wAo),e(Ha,Rae),e(Rae,AAo),e(Ha,LAo),e(Ha,Sae),e(Sae,BAo),e(Ha,kAo),e(De,xAo),e(De,R),e(R,k5),e(k5,Pae),e(Pae,RAo),e(k5,SAo),e(k5,aj),e(aj,PAo),e(k5,$Ao),e(R,IAo),e(R,x5),e(x5,$ae),e($ae,jAo),e(x5,NAo),e(x5,nj),e(nj,DAo),e(x5,qAo),e(R,GAo),e(R,R5),e(R5,Iae),e(Iae,OAo),e(R5,XAo),e(R5,sj),e(sj,zAo),e(R5,VAo),e(R,WAo),e(R,S5),e(S5,jae),e(jae,QAo),e(S5,HAo),e(S5,lj),e(lj,UAo),e(S5,JAo),e(R,YAo),e(R,P5),e(P5,Nae),e(Nae,KAo),e(P5,ZAo),e(P5,ij),e(ij,e0o),e(P5,o0o),e(R,r0o),e(R,$5),e($5,Dae),e(Dae,t0o),e($5,a0o),e($5,dj),e(dj,n0o),e($5,s0o),e(R,l0o),e(R,I5),e(I5,qae),e(qae,i0o),e(I5,d0o),e(I5,cj),e(cj,c0o),e(I5,f0o),e(R,m0o),e(R,j5),e(j5,Gae),e(Gae,g0o),e(j5,h0o),e(j5,fj),e(fj,p0o),e(j5,_0o),e(R,u0o),e(R,N5),e(N5,Oae),e(Oae,b0o),e(N5,v0o),e(N5,mj),e(mj,T0o),e(N5,F0o),e(R,C0o),e(R,D5),e(D5,Xae),e(Xae,M0o),e(D5,E0o),e(D5,gj),e(gj,y0o),e(D5,w0o),e(R,A0o),e(R,q5),e(q5,zae),e(zae,L0o),e(q5,B0o),e(q5,hj),e(hj,k0o),e(q5,x0o),e(R,R0o),e(R,G5),e(G5,Vae),e(Vae,S0o),e(G5,P0o),e(G5,pj),e(pj,$0o),e(G5,I0o),e(R,j0o),e(R,O5),e(O5,Wae),e(Wae,N0o),e(O5,D0o),e(O5,_j),e(_j,q0o),e(O5,G0o),e(R,O0o),e(R,X5),e(X5,Qae),e(Qae,X0o),e(X5,z0o),e(X5,uj),e(uj,V0o),e(X5,W0o),e(R,Q0o),e(R,z5),e(z5,Hae),e(Hae,H0o),e(z5,U0o),e(z5,bj),e(bj,J0o),e(z5,Y0o),e(R,K0o),e(R,V5),e(V5,Uae),e(Uae,Z0o),e(V5,eLo),e(V5,vj),e(vj,oLo),e(V5,rLo),e(R,tLo),e(R,W5),e(W5,Jae),e(Jae,aLo),e(W5,nLo),e(W5,Tj),e(Tj,sLo),e(W5,lLo),e(R,iLo),e(R,Q5),e(Q5,Yae),e(Yae,dLo),e(Q5,cLo),e(Q5,Fj),e(Fj,fLo),e(Q5,mLo),e(R,gLo),e(R,H5),e(H5,Kae),e(Kae,hLo),e(H5,pLo),e(H5,Cj),e(Cj,_Lo),e(H5,uLo),e(R,bLo),e(R,U5),e(U5,Zae),e(Zae,vLo),e(U5,TLo),e(U5,Mj),e(Mj,FLo),e(U5,CLo),e(R,MLo),e(R,J5),e(J5,ene),e(ene,ELo),e(J5,yLo),e(J5,Ej),e(Ej,wLo),e(J5,ALo),e(R,LLo),e(R,Y5),e(Y5,one),e(one,BLo),e(Y5,kLo),e(Y5,yj),e(yj,xLo),e(Y5,RLo),e(R,SLo),e(R,K5),e(K5,rne),e(rne,PLo),e(K5,$Lo),e(K5,wj),e(wj,ILo),e(K5,jLo),e(R,NLo),e(R,Z5),e(Z5,tne),e(tne,DLo),e(Z5,qLo),e(Z5,Aj),e(Aj,GLo),e(Z5,OLo),e(R,XLo),e(R,e2),e(e2,ane),e(ane,zLo),e(e2,VLo),e(e2,Lj),e(Lj,WLo),e(e2,QLo),e(R,HLo),e(R,o2),e(o2,nne),e(nne,ULo),e(o2,JLo),e(o2,Bj),e(Bj,YLo),e(o2,KLo),e(R,ZLo),e(R,r2),e(r2,sne),e(sne,e8o),e(r2,o8o),e(r2,kj),e(kj,r8o),e(r2,t8o),e(R,a8o),e(R,t2),e(t2,lne),e(lne,n8o),e(t2,s8o),e(t2,xj),e(xj,l8o),e(t2,i8o),e(R,d8o),e(R,a2),e(a2,ine),e(ine,c8o),e(a2,f8o),e(a2,Rj),e(Rj,m8o),e(a2,g8o),e(R,h8o),e(R,n2),e(n2,dne),e(dne,p8o),e(n2,_8o),e(n2,Sj),e(Sj,u8o),e(n2,b8o),e(R,v8o),e(R,s2),e(s2,cne),e(cne,T8o),e(s2,F8o),e(s2,Pj),e(Pj,C8o),e(s2,M8o),e(R,E8o),e(R,l2),e(l2,fne),e(fne,y8o),e(l2,w8o),e(l2,$j),e($j,A8o),e(l2,L8o),e(R,B8o),e(R,i2),e(i2,mne),e(mne,k8o),e(i2,x8o),e(i2,Ij),e(Ij,R8o),e(i2,S8o),e(R,P8o),e(R,d2),e(d2,gne),e(gne,$8o),e(d2,I8o),e(d2,jj),e(jj,j8o),e(d2,N8o),e(R,D8o),e(R,c2),e(c2,hne),e(hne,q8o),e(c2,G8o),e(c2,Nj),e(Nj,O8o),e(c2,X8o),e(R,z8o),e(R,f2),e(f2,pne),e(pne,V8o),e(f2,W8o),e(f2,Dj),e(Dj,Q8o),e(f2,H8o),e(R,U8o),e(R,m2),e(m2,_ne),e(_ne,J8o),e(m2,Y8o),e(m2,qj),e(qj,K8o),e(m2,Z8o),e(R,eBo),e(R,g2),e(g2,une),e(une,oBo),e(g2,rBo),e(g2,Gj),e(Gj,tBo),e(g2,aBo),e(De,nBo),e(De,h2),e(h2,sBo),e(h2,bne),e(bne,lBo),e(h2,iBo),e(h2,vne),e(vne,dBo),e(De,cBo),e(De,Tne),e(Tne,fBo),e(De,mBo),g(JM,De,null),b(d,ILe,u),b(d,_d,u),e(_d,p2),e(p2,Fne),g(YM,Fne,null),e(_d,gBo),e(_d,Cne),e(Cne,hBo),b(d,jLe,u),b(d,or,u),g(KM,or,null),e(or,pBo),e(or,ud),e(ud,_Bo),e(ud,Mne),e(Mne,uBo),e(ud,bBo),e(ud,Ene),e(Ene,vBo),e(ud,TBo),e(or,FBo),e(or,ZM),e(ZM,CBo),e(ZM,yne),e(yne,MBo),e(ZM,EBo),e(or,yBo),e(or,Hr),g(eE,Hr,null),e(Hr,wBo),e(Hr,wne),e(wne,ABo),e(Hr,LBo),e(Hr,bd),e(bd,BBo),e(bd,Ane),e(Ane,kBo),e(bd,xBo),e(bd,Lne),e(Lne,RBo),e(bd,SBo),e(Hr,PBo),e(Hr,Bne),e(Bne,$Bo),e(Hr,IBo),g(oE,Hr,null),e(or,jBo),e(or,qe),g(rE,qe,null),e(qe,NBo),e(qe,kne),e(kne,DBo),e(qe,qBo),e(qe,Ua),e(Ua,GBo),e(Ua,xne),e(xne,OBo),e(Ua,XBo),e(Ua,Rne),e(Rne,zBo),e(Ua,VBo),e(Ua,Sne),e(Sne,WBo),e(Ua,QBo),e(qe,HBo),e(qe,Pne),e(Pne,_2),e(_2,$ne),e($ne,UBo),e(_2,JBo),e(_2,Oj),e(Oj,YBo),e(_2,KBo),e(qe,ZBo),e(qe,u2),e(u2,eko),e(u2,Ine),e(Ine,oko),e(u2,rko),e(u2,jne),e(jne,tko),e(qe,ako),e(qe,Nne),e(Nne,nko),e(qe,sko),g(tE,qe,null),b(d,NLe,u),b(d,vd,u),e(vd,b2),e(b2,Dne),g(aE,Dne,null),e(vd,lko),e(vd,qne),e(qne,iko),b(d,DLe,u),b(d,rr,u),g(nE,rr,null),e(rr,dko),e(rr,Td),e(Td,cko),e(Td,Gne),e(Gne,fko),e(Td,mko),e(Td,One),e(One,gko),e(Td,hko),e(rr,pko),e(rr,sE),e(sE,_ko),e(sE,Xne),e(Xne,uko),e(sE,bko),e(rr,vko),e(rr,Ur),g(lE,Ur,null),e(Ur,Tko),e(Ur,zne),e(zne,Fko),e(Ur,Cko),e(Ur,Fd),e(Fd,Mko),e(Fd,Vne),e(Vne,Eko),e(Fd,yko),e(Fd,Wne),e(Wne,wko),e(Fd,Ako),e(Ur,Lko),e(Ur,Qne),e(Qne,Bko),e(Ur,kko),g(iE,Ur,null),e(rr,xko),e(rr,Ge),g(dE,Ge,null),e(Ge,Rko),e(Ge,Hne),e(Hne,Sko),e(Ge,Pko),e(Ge,Ja),e(Ja,$ko),e(Ja,Une),e(Une,Iko),e(Ja,jko),e(Ja,Jne),e(Jne,Nko),e(Ja,Dko),e(Ja,Yne),e(Yne,qko),e(Ja,Gko),e(Ge,Oko),e(Ge,be),e(be,v2),e(v2,Kne),e(Kne,Xko),e(v2,zko),e(v2,Xj),e(Xj,Vko),e(v2,Wko),e(be,Qko),e(be,T2),e(T2,Zne),e(Zne,Hko),e(T2,Uko),e(T2,zj),e(zj,Jko),e(T2,Yko),e(be,Kko),e(be,Rs),e(Rs,ese),e(ese,Zko),e(Rs,exo),e(Rs,Vj),e(Vj,oxo),e(Rs,rxo),e(Rs,Wj),e(Wj,txo),e(Rs,axo),e(be,nxo),e(be,F2),e(F2,ose),e(ose,sxo),e(F2,lxo),e(F2,Qj),e(Qj,ixo),e(F2,dxo),e(be,cxo),e(be,la),e(la,rse),e(rse,fxo),e(la,mxo),e(la,Hj),e(Hj,gxo),e(la,hxo),e(la,Uj),e(Uj,pxo),e(la,_xo),e(la,Jj),e(Jj,uxo),e(la,bxo),e(be,vxo),e(be,C2),e(C2,tse),e(tse,Txo),e(C2,Fxo),e(C2,Yj),e(Yj,Cxo),e(C2,Mxo),e(be,Exo),e(be,M2),e(M2,ase),e(ase,yxo),e(M2,wxo),e(M2,Kj),e(Kj,Axo),e(M2,Lxo),e(be,Bxo),e(be,E2),e(E2,nse),e(nse,kxo),e(E2,xxo),e(E2,Zj),e(Zj,Rxo),e(E2,Sxo),e(be,Pxo),e(be,y2),e(y2,sse),e(sse,$xo),e(y2,Ixo),e(y2,eN),e(eN,jxo),e(y2,Nxo),e(Ge,Dxo),e(Ge,w2),e(w2,qxo),e(w2,lse),e(lse,Gxo),e(w2,Oxo),e(w2,ise),e(ise,Xxo),e(Ge,zxo),e(Ge,dse),e(dse,Vxo),e(Ge,Wxo),g(cE,Ge,null),b(d,qLe,u),b(d,Cd,u),e(Cd,A2),e(A2,cse),g(fE,cse,null),e(Cd,Qxo),e(Cd,fse),e(fse,Hxo),b(d,GLe,u),b(d,tr,u),g(mE,tr,null),e(tr,Uxo),e(tr,Md),e(Md,Jxo),e(Md,mse),e(mse,Yxo),e(Md,Kxo),e(Md,gse),e(gse,Zxo),e(Md,eRo),e(tr,oRo),e(tr,gE),e(gE,rRo),e(gE,hse),e(hse,tRo),e(gE,aRo),e(tr,nRo),e(tr,Jr),g(hE,Jr,null),e(Jr,sRo),e(Jr,pse),e(pse,lRo),e(Jr,iRo),e(Jr,Ed),e(Ed,dRo),e(Ed,_se),e(_se,cRo),e(Ed,fRo),e(Ed,use),e(use,mRo),e(Ed,gRo),e(Jr,hRo),e(Jr,bse),e(bse,pRo),e(Jr,_Ro),g(pE,Jr,null),e(tr,uRo),e(tr,Oe),g(_E,Oe,null),e(Oe,bRo),e(Oe,vse),e(vse,vRo),e(Oe,TRo),e(Oe,Ya),e(Ya,FRo),e(Ya,Tse),e(Tse,CRo),e(Ya,MRo),e(Ya,Fse),e(Fse,ERo),e(Ya,yRo),e(Ya,Cse),e(Cse,wRo),e(Ya,ARo),e(Oe,LRo),e(Oe,Mse),e(Mse,L2),e(L2,Ese),e(Ese,BRo),e(L2,kRo),e(L2,oN),e(oN,xRo),e(L2,RRo),e(Oe,SRo),e(Oe,B2),e(B2,PRo),e(B2,yse),e(yse,$Ro),e(B2,IRo),e(B2,wse),e(wse,jRo),e(Oe,NRo),e(Oe,Ase),e(Ase,DRo),e(Oe,qRo),g(uE,Oe,null),b(d,OLe,u),b(d,yd,u),e(yd,k2),e(k2,Lse),g(bE,Lse,null),e(yd,GRo),e(yd,Bse),e(Bse,ORo),b(d,XLe,u),b(d,ar,u),g(vE,ar,null),e(ar,XRo),e(ar,wd),e(wd,zRo),e(wd,kse),e(kse,VRo),e(wd,WRo),e(wd,xse),e(xse,QRo),e(wd,HRo),e(ar,URo),e(ar,TE),e(TE,JRo),e(TE,Rse),e(Rse,YRo),e(TE,KRo),e(ar,ZRo),e(ar,Yr),g(FE,Yr,null),e(Yr,eSo),e(Yr,Sse),e(Sse,oSo),e(Yr,rSo),e(Yr,Ad),e(Ad,tSo),e(Ad,Pse),e(Pse,aSo),e(Ad,nSo),e(Ad,$se),e($se,sSo),e(Ad,lSo),e(Yr,iSo),e(Yr,Ise),e(Ise,dSo),e(Yr,cSo),g(CE,Yr,null),e(ar,fSo),e(ar,Xe),g(ME,Xe,null),e(Xe,mSo),e(Xe,jse),e(jse,gSo),e(Xe,hSo),e(Xe,Ka),e(Ka,pSo),e(Ka,Nse),e(Nse,_So),e(Ka,uSo),e(Ka,Dse),e(Dse,bSo),e(Ka,vSo),e(Ka,qse),e(qse,TSo),e(Ka,FSo),e(Xe,CSo),e(Xe,ao),e(ao,x2),e(x2,Gse),e(Gse,MSo),e(x2,ESo),e(x2,rN),e(rN,ySo),e(x2,wSo),e(ao,ASo),e(ao,R2),e(R2,Ose),e(Ose,LSo),e(R2,BSo),e(R2,tN),e(tN,kSo),e(R2,xSo),e(ao,RSo),e(ao,S2),e(S2,Xse),e(Xse,SSo),e(S2,PSo),e(S2,aN),e(aN,$So),e(S2,ISo),e(ao,jSo),e(ao,P2),e(P2,zse),e(zse,NSo),e(P2,DSo),e(P2,nN),e(nN,qSo),e(P2,GSo),e(ao,OSo),e(ao,$2),e($2,Vse),e(Vse,XSo),e($2,zSo),e($2,sN),e(sN,VSo),e($2,WSo),e(ao,QSo),e(ao,I2),e(I2,Wse),e(Wse,HSo),e(I2,USo),e(I2,lN),e(lN,JSo),e(I2,YSo),e(ao,KSo),e(ao,j2),e(j2,Qse),e(Qse,ZSo),e(j2,ePo),e(j2,iN),e(iN,oPo),e(j2,rPo),e(Xe,tPo),e(Xe,N2),e(N2,aPo),e(N2,Hse),e(Hse,nPo),e(N2,sPo),e(N2,Use),e(Use,lPo),e(Xe,iPo),e(Xe,Jse),e(Jse,dPo),e(Xe,cPo),g(EE,Xe,null),b(d,zLe,u),b(d,Ld,u),e(Ld,D2),e(D2,Yse),g(yE,Yse,null),e(Ld,fPo),e(Ld,Kse),e(Kse,mPo),b(d,VLe,u),b(d,nr,u),g(wE,nr,null),e(nr,gPo),e(nr,Bd),e(Bd,hPo),e(Bd,Zse),e(Zse,pPo),e(Bd,_Po),e(Bd,ele),e(ele,uPo),e(Bd,bPo),e(nr,vPo),e(nr,AE),e(AE,TPo),e(AE,ole),e(ole,FPo),e(AE,CPo),e(nr,MPo),e(nr,Kr),g(LE,Kr,null),e(Kr,EPo),e(Kr,rle),e(rle,yPo),e(Kr,wPo),e(Kr,kd),e(kd,APo),e(kd,tle),e(tle,LPo),e(kd,BPo),e(kd,ale),e(ale,kPo),e(kd,xPo),e(Kr,RPo),e(Kr,nle),e(nle,SPo),e(Kr,PPo),g(BE,Kr,null),e(nr,$Po),e(nr,ze),g(kE,ze,null),e(ze,IPo),e(ze,sle),e(sle,jPo),e(ze,NPo),e(ze,Za),e(Za,DPo),e(Za,lle),e(lle,qPo),e(Za,GPo),e(Za,ile),e(ile,OPo),e(Za,XPo),e(Za,dle),e(dle,zPo),e(Za,VPo),e(ze,WPo),e(ze,xd),e(xd,q2),e(q2,cle),e(cle,QPo),e(q2,HPo),e(q2,dN),e(dN,UPo),e(q2,JPo),e(xd,YPo),e(xd,G2),e(G2,fle),e(fle,KPo),e(G2,ZPo),e(G2,cN),e(cN,e$o),e(G2,o$o),e(xd,r$o),e(xd,O2),e(O2,mle),e(mle,t$o),e(O2,a$o),e(O2,fN),e(fN,n$o),e(O2,s$o),e(ze,l$o),e(ze,X2),e(X2,i$o),e(X2,gle),e(gle,d$o),e(X2,c$o),e(X2,hle),e(hle,f$o),e(ze,m$o),e(ze,ple),e(ple,g$o),e(ze,h$o),g(xE,ze,null),b(d,WLe,u),b(d,Rd,u),e(Rd,z2),e(z2,_le),g(RE,_le,null),e(Rd,p$o),e(Rd,ule),e(ule,_$o),b(d,QLe,u),b(d,sr,u),g(SE,sr,null),e(sr,u$o),e(sr,Sd),e(Sd,b$o),e(Sd,ble),e(ble,v$o),e(Sd,T$o),e(Sd,vle),e(vle,F$o),e(Sd,C$o),e(sr,M$o),e(sr,PE),e(PE,E$o),e(PE,Tle),e(Tle,y$o),e(PE,w$o),e(sr,A$o),e(sr,Zr),g($E,Zr,null),e(Zr,L$o),e(Zr,Fle),e(Fle,B$o),e(Zr,k$o),e(Zr,Pd),e(Pd,x$o),e(Pd,Cle),e(Cle,R$o),e(Pd,S$o),e(Pd,Mle),e(Mle,P$o),e(Pd,$$o),e(Zr,I$o),e(Zr,Ele),e(Ele,j$o),e(Zr,N$o),g(IE,Zr,null),e(sr,D$o),e(sr,Ve),g(jE,Ve,null),e(Ve,q$o),e(Ve,yle),e(yle,G$o),e(Ve,O$o),e(Ve,en),e(en,X$o),e(en,wle),e(wle,z$o),e(en,V$o),e(en,Ale),e(Ale,W$o),e(en,Q$o),e(en,Lle),e(Lle,H$o),e(en,U$o),e(Ve,J$o),e(Ve,no),e(no,V2),e(V2,Ble),e(Ble,Y$o),e(V2,K$o),e(V2,mN),e(mN,Z$o),e(V2,eIo),e(no,oIo),e(no,W2),e(W2,kle),e(kle,rIo),e(W2,tIo),e(W2,gN),e(gN,aIo),e(W2,nIo),e(no,sIo),e(no,Q2),e(Q2,xle),e(xle,lIo),e(Q2,iIo),e(Q2,hN),e(hN,dIo),e(Q2,cIo),e(no,fIo),e(no,H2),e(H2,Rle),e(Rle,mIo),e(H2,gIo),e(H2,pN),e(pN,hIo),e(H2,pIo),e(no,_Io),e(no,U2),e(U2,Sle),e(Sle,uIo),e(U2,bIo),e(U2,_N),e(_N,vIo),e(U2,TIo),e(no,FIo),e(no,J2),e(J2,Ple),e(Ple,CIo),e(J2,MIo),e(J2,uN),e(uN,EIo),e(J2,yIo),e(no,wIo),e(no,Y2),e(Y2,$le),e($le,AIo),e(Y2,LIo),e(Y2,bN),e(bN,BIo),e(Y2,kIo),e(Ve,xIo),e(Ve,K2),e(K2,RIo),e(K2,Ile),e(Ile,SIo),e(K2,PIo),e(K2,jle),e(jle,$Io),e(Ve,IIo),e(Ve,Nle),e(Nle,jIo),e(Ve,NIo),g(NE,Ve,null),b(d,HLe,u),b(d,$d,u),e($d,Z2),e(Z2,Dle),g(DE,Dle,null),e($d,DIo),e($d,qle),e(qle,qIo),b(d,ULe,u),b(d,lr,u),g(qE,lr,null),e(lr,GIo),e(lr,Id),e(Id,OIo),e(Id,Gle),e(Gle,XIo),e(Id,zIo),e(Id,Ole),e(Ole,VIo),e(Id,WIo),e(lr,QIo),e(lr,GE),e(GE,HIo),e(GE,Xle),e(Xle,UIo),e(GE,JIo),e(lr,YIo),e(lr,et),g(OE,et,null),e(et,KIo),e(et,zle),e(zle,ZIo),e(et,ejo),e(et,jd),e(jd,ojo),e(jd,Vle),e(Vle,rjo),e(jd,tjo),e(jd,Wle),e(Wle,ajo),e(jd,njo),e(et,sjo),e(et,Qle),e(Qle,ljo),e(et,ijo),g(XE,et,null),e(lr,djo),e(lr,We),g(zE,We,null),e(We,cjo),e(We,Hle),e(Hle,fjo),e(We,mjo),e(We,on),e(on,gjo),e(on,Ule),e(Ule,hjo),e(on,pjo),e(on,Jle),e(Jle,_jo),e(on,ujo),e(on,Yle),e(Yle,bjo),e(on,vjo),e(We,Tjo),e(We,VE),e(VE,ev),e(ev,Kle),e(Kle,Fjo),e(ev,Cjo),e(ev,vN),e(vN,Mjo),e(ev,Ejo),e(VE,yjo),e(VE,ov),e(ov,Zle),e(Zle,wjo),e(ov,Ajo),e(ov,TN),e(TN,Ljo),e(ov,Bjo),e(We,kjo),e(We,rv),e(rv,xjo),e(rv,eie),e(eie,Rjo),e(rv,Sjo),e(rv,oie),e(oie,Pjo),e(We,$jo),e(We,rie),e(rie,Ijo),e(We,jjo),g(WE,We,null),b(d,JLe,u),b(d,Nd,u),e(Nd,tv),e(tv,tie),g(QE,tie,null),e(Nd,Njo),e(Nd,aie),e(aie,Djo),b(d,YLe,u),b(d,ir,u),g(HE,ir,null),e(ir,qjo),e(ir,Dd),e(Dd,Gjo),e(Dd,nie),e(nie,Ojo),e(Dd,Xjo),e(Dd,sie),e(sie,zjo),e(Dd,Vjo),e(ir,Wjo),e(ir,UE),e(UE,Qjo),e(UE,lie),e(lie,Hjo),e(UE,Ujo),e(ir,Jjo),e(ir,ot),g(JE,ot,null),e(ot,Yjo),e(ot,iie),e(iie,Kjo),e(ot,Zjo),e(ot,qd),e(qd,eNo),e(qd,die),e(die,oNo),e(qd,rNo),e(qd,cie),e(cie,tNo),e(qd,aNo),e(ot,nNo),e(ot,fie),e(fie,sNo),e(ot,lNo),g(YE,ot,null),e(ir,iNo),e(ir,Qe),g(KE,Qe,null),e(Qe,dNo),e(Qe,mie),e(mie,cNo),e(Qe,fNo),e(Qe,rn),e(rn,mNo),e(rn,gie),e(gie,gNo),e(rn,hNo),e(rn,hie),e(hie,pNo),e(rn,_No),e(rn,pie),e(pie,uNo),e(rn,bNo),e(Qe,vNo),e(Qe,Gd),e(Gd,av),e(av,_ie),e(_ie,TNo),e(av,FNo),e(av,FN),e(FN,CNo),e(av,MNo),e(Gd,ENo),e(Gd,nv),e(nv,uie),e(uie,yNo),e(nv,wNo),e(nv,CN),e(CN,ANo),e(nv,LNo),e(Gd,BNo),e(Gd,sv),e(sv,bie),e(bie,kNo),e(sv,xNo),e(sv,MN),e(MN,RNo),e(sv,SNo),e(Qe,PNo),e(Qe,lv),e(lv,$No),e(lv,vie),e(vie,INo),e(lv,jNo),e(lv,Tie),e(Tie,NNo),e(Qe,DNo),e(Qe,Fie),e(Fie,qNo),e(Qe,GNo),g(ZE,Qe,null),b(d,KLe,u),b(d,Od,u),e(Od,iv),e(iv,Cie),g(e3,Cie,null),e(Od,ONo),e(Od,Mie),e(Mie,XNo),b(d,ZLe,u),b(d,dr,u),g(o3,dr,null),e(dr,zNo),e(dr,Xd),e(Xd,VNo),e(Xd,Eie),e(Eie,WNo),e(Xd,QNo),e(Xd,yie),e(yie,HNo),e(Xd,UNo),e(dr,JNo),e(dr,r3),e(r3,YNo),e(r3,wie),e(wie,KNo),e(r3,ZNo),e(dr,eDo),e(dr,rt),g(t3,rt,null),e(rt,oDo),e(rt,Aie),e(Aie,rDo),e(rt,tDo),e(rt,zd),e(zd,aDo),e(zd,Lie),e(Lie,nDo),e(zd,sDo),e(zd,Bie),e(Bie,lDo),e(zd,iDo),e(rt,dDo),e(rt,kie),e(kie,cDo),e(rt,fDo),g(a3,rt,null),e(dr,mDo),e(dr,He),g(n3,He,null),e(He,gDo),e(He,xie),e(xie,hDo),e(He,pDo),e(He,tn),e(tn,_Do),e(tn,Rie),e(Rie,uDo),e(tn,bDo),e(tn,Sie),e(Sie,vDo),e(tn,TDo),e(tn,Pie),e(Pie,FDo),e(tn,CDo),e(He,MDo),e(He,Vd),e(Vd,dv),e(dv,$ie),e($ie,EDo),e(dv,yDo),e(dv,EN),e(EN,wDo),e(dv,ADo),e(Vd,LDo),e(Vd,cv),e(cv,Iie),e(Iie,BDo),e(cv,kDo),e(cv,yN),e(yN,xDo),e(cv,RDo),e(Vd,SDo),e(Vd,fv),e(fv,jie),e(jie,PDo),e(fv,$Do),e(fv,wN),e(wN,IDo),e(fv,jDo),e(He,NDo),e(He,mv),e(mv,DDo),e(mv,Nie),e(Nie,qDo),e(mv,GDo),e(mv,Die),e(Die,ODo),e(He,XDo),e(He,qie),e(qie,zDo),e(He,VDo),g(s3,He,null),b(d,e8e,u),b(d,Wd,u),e(Wd,gv),e(gv,Gie),g(l3,Gie,null),e(Wd,WDo),e(Wd,Oie),e(Oie,QDo),b(d,o8e,u),b(d,cr,u),g(i3,cr,null),e(cr,HDo),e(cr,Qd),e(Qd,UDo),e(Qd,Xie),e(Xie,JDo),e(Qd,YDo),e(Qd,zie),e(zie,KDo),e(Qd,ZDo),e(cr,eqo),e(cr,d3),e(d3,oqo),e(d3,Vie),e(Vie,rqo),e(d3,tqo),e(cr,aqo),e(cr,tt),g(c3,tt,null),e(tt,nqo),e(tt,Wie),e(Wie,sqo),e(tt,lqo),e(tt,Hd),e(Hd,iqo),e(Hd,Qie),e(Qie,dqo),e(Hd,cqo),e(Hd,Hie),e(Hie,fqo),e(Hd,mqo),e(tt,gqo),e(tt,Uie),e(Uie,hqo),e(tt,pqo),g(f3,tt,null),e(cr,_qo),e(cr,Ue),g(m3,Ue,null),e(Ue,uqo),e(Ue,Jie),e(Jie,bqo),e(Ue,vqo),e(Ue,an),e(an,Tqo),e(an,Yie),e(Yie,Fqo),e(an,Cqo),e(an,Kie),e(Kie,Mqo),e(an,Eqo),e(an,Zie),e(Zie,yqo),e(an,wqo),e(Ue,Aqo),e(Ue,ede),e(ede,hv),e(hv,ode),e(ode,Lqo),e(hv,Bqo),e(hv,AN),e(AN,kqo),e(hv,xqo),e(Ue,Rqo),e(Ue,pv),e(pv,Sqo),e(pv,rde),e(rde,Pqo),e(pv,$qo),e(pv,tde),e(tde,Iqo),e(Ue,jqo),e(Ue,ade),e(ade,Nqo),e(Ue,Dqo),g(g3,Ue,null),b(d,r8e,u),b(d,Ud,u),e(Ud,_v),e(_v,nde),g(h3,nde,null),e(Ud,qqo),e(Ud,sde),e(sde,Gqo),b(d,t8e,u),b(d,fr,u),g(p3,fr,null),e(fr,Oqo),e(fr,Jd),e(Jd,Xqo),e(Jd,lde),e(lde,zqo),e(Jd,Vqo),e(Jd,ide),e(ide,Wqo),e(Jd,Qqo),e(fr,Hqo),e(fr,_3),e(_3,Uqo),e(_3,dde),e(dde,Jqo),e(_3,Yqo),e(fr,Kqo),e(fr,at),g(u3,at,null),e(at,Zqo),e(at,cde),e(cde,eGo),e(at,oGo),e(at,Yd),e(Yd,rGo),e(Yd,fde),e(fde,tGo),e(Yd,aGo),e(Yd,mde),e(mde,nGo),e(Yd,sGo),e(at,lGo),e(at,gde),e(gde,iGo),e(at,dGo),g(b3,at,null),e(fr,cGo),e(fr,Je),g(v3,Je,null),e(Je,fGo),e(Je,hde),e(hde,mGo),e(Je,gGo),e(Je,nn),e(nn,hGo),e(nn,pde),e(pde,pGo),e(nn,_Go),e(nn,_de),e(_de,uGo),e(nn,bGo),e(nn,ude),e(ude,vGo),e(nn,TGo),e(Je,FGo),e(Je,bde),e(bde,uv),e(uv,vde),e(vde,CGo),e(uv,MGo),e(uv,LN),e(LN,EGo),e(uv,yGo),e(Je,wGo),e(Je,bv),e(bv,AGo),e(bv,Tde),e(Tde,LGo),e(bv,BGo),e(bv,Fde),e(Fde,kGo),e(Je,xGo),e(Je,Cde),e(Cde,RGo),e(Je,SGo),g(T3,Je,null),b(d,a8e,u),b(d,Kd,u),e(Kd,vv),e(vv,Mde),g(F3,Mde,null),e(Kd,PGo),e(Kd,Ede),e(Ede,$Go),b(d,n8e,u),b(d,mr,u),g(C3,mr,null),e(mr,IGo),e(mr,Zd),e(Zd,jGo),e(Zd,yde),e(yde,NGo),e(Zd,DGo),e(Zd,wde),e(wde,qGo),e(Zd,GGo),e(mr,OGo),e(mr,M3),e(M3,XGo),e(M3,Ade),e(Ade,zGo),e(M3,VGo),e(mr,WGo),e(mr,nt),g(E3,nt,null),e(nt,QGo),e(nt,Lde),e(Lde,HGo),e(nt,UGo),e(nt,ec),e(ec,JGo),e(ec,Bde),e(Bde,YGo),e(ec,KGo),e(ec,kde),e(kde,ZGo),e(ec,eOo),e(nt,oOo),e(nt,xde),e(xde,rOo),e(nt,tOo),g(y3,nt,null),e(mr,aOo),e(mr,Ye),g(w3,Ye,null),e(Ye,nOo),e(Ye,Rde),e(Rde,sOo),e(Ye,lOo),e(Ye,sn),e(sn,iOo),e(sn,Sde),e(Sde,dOo),e(sn,cOo),e(sn,Pde),e(Pde,fOo),e(sn,mOo),e(sn,$de),e($de,gOo),e(sn,hOo),e(Ye,pOo),e(Ye,A3),e(A3,Tv),e(Tv,Ide),e(Ide,_Oo),e(Tv,uOo),e(Tv,BN),e(BN,bOo),e(Tv,vOo),e(A3,TOo),e(A3,Fv),e(Fv,jde),e(jde,FOo),e(Fv,COo),e(Fv,kN),e(kN,MOo),e(Fv,EOo),e(Ye,yOo),e(Ye,Cv),e(Cv,wOo),e(Cv,Nde),e(Nde,AOo),e(Cv,LOo),e(Cv,Dde),e(Dde,BOo),e(Ye,kOo),e(Ye,qde),e(qde,xOo),e(Ye,ROo),g(L3,Ye,null),b(d,s8e,u),b(d,oc,u),e(oc,Mv),e(Mv,Gde),g(B3,Gde,null),e(oc,SOo),e(oc,Ode),e(Ode,POo),b(d,l8e,u),b(d,gr,u),g(k3,gr,null),e(gr,$Oo),e(gr,rc),e(rc,IOo),e(rc,Xde),e(Xde,jOo),e(rc,NOo),e(rc,zde),e(zde,DOo),e(rc,qOo),e(gr,GOo),e(gr,x3),e(x3,OOo),e(x3,Vde),e(Vde,XOo),e(x3,zOo),e(gr,VOo),e(gr,st),g(R3,st,null),e(st,WOo),e(st,Wde),e(Wde,QOo),e(st,HOo),e(st,tc),e(tc,UOo),e(tc,Qde),e(Qde,JOo),e(tc,YOo),e(tc,Hde),e(Hde,KOo),e(tc,ZOo),e(st,eXo),e(st,Ude),e(Ude,oXo),e(st,rXo),g(S3,st,null),e(gr,tXo),e(gr,go),g(P3,go,null),e(go,aXo),e(go,Jde),e(Jde,nXo),e(go,sXo),e(go,ln),e(ln,lXo),e(ln,Yde),e(Yde,iXo),e(ln,dXo),e(ln,Kde),e(Kde,cXo),e(ln,fXo),e(ln,Zde),e(Zde,mXo),e(ln,gXo),e(go,hXo),e(go,B),e(B,Ev),e(Ev,ece),e(ece,pXo),e(Ev,_Xo),e(Ev,xN),e(xN,uXo),e(Ev,bXo),e(B,vXo),e(B,yv),e(yv,oce),e(oce,TXo),e(yv,FXo),e(yv,RN),e(RN,CXo),e(yv,MXo),e(B,EXo),e(B,wv),e(wv,rce),e(rce,yXo),e(wv,wXo),e(wv,SN),e(SN,AXo),e(wv,LXo),e(B,BXo),e(B,Av),e(Av,tce),e(tce,kXo),e(Av,xXo),e(Av,PN),e(PN,RXo),e(Av,SXo),e(B,PXo),e(B,Lv),e(Lv,ace),e(ace,$Xo),e(Lv,IXo),e(Lv,$N),e($N,jXo),e(Lv,NXo),e(B,DXo),e(B,Bv),e(Bv,nce),e(nce,qXo),e(Bv,GXo),e(Bv,IN),e(IN,OXo),e(Bv,XXo),e(B,zXo),e(B,kv),e(kv,sce),e(sce,VXo),e(kv,WXo),e(kv,jN),e(jN,QXo),e(kv,HXo),e(B,UXo),e(B,xv),e(xv,lce),e(lce,JXo),e(xv,YXo),e(xv,NN),e(NN,KXo),e(xv,ZXo),e(B,ezo),e(B,Rv),e(Rv,ice),e(ice,ozo),e(Rv,rzo),e(Rv,DN),e(DN,tzo),e(Rv,azo),e(B,nzo),e(B,Sv),e(Sv,dce),e(dce,szo),e(Sv,lzo),e(Sv,qN),e(qN,izo),e(Sv,dzo),e(B,czo),e(B,Pv),e(Pv,cce),e(cce,fzo),e(Pv,mzo),e(Pv,GN),e(GN,gzo),e(Pv,hzo),e(B,pzo),e(B,$v),e($v,fce),e(fce,_zo),e($v,uzo),e($v,ON),e(ON,bzo),e($v,vzo),e(B,Tzo),e(B,Iv),e(Iv,mce),e(mce,Fzo),e(Iv,Czo),e(Iv,XN),e(XN,Mzo),e(Iv,Ezo),e(B,yzo),e(B,jv),e(jv,gce),e(gce,wzo),e(jv,Azo),e(jv,zN),e(zN,Lzo),e(jv,Bzo),e(B,kzo),e(B,Nv),e(Nv,hce),e(hce,xzo),e(Nv,Rzo),e(Nv,VN),e(VN,Szo),e(Nv,Pzo),e(B,$zo),e(B,Ss),e(Ss,pce),e(pce,Izo),e(Ss,jzo),e(Ss,WN),e(WN,Nzo),e(Ss,Dzo),e(Ss,QN),e(QN,qzo),e(Ss,Gzo),e(B,Ozo),e(B,Dv),e(Dv,_ce),e(_ce,Xzo),e(Dv,zzo),e(Dv,HN),e(HN,Vzo),e(Dv,Wzo),e(B,Qzo),e(B,qv),e(qv,uce),e(uce,Hzo),e(qv,Uzo),e(qv,UN),e(UN,Jzo),e(qv,Yzo),e(B,Kzo),e(B,Gv),e(Gv,bce),e(bce,Zzo),e(Gv,eVo),e(Gv,JN),e(JN,oVo),e(Gv,rVo),e(B,tVo),e(B,Ov),e(Ov,vce),e(vce,aVo),e(Ov,nVo),e(Ov,YN),e(YN,sVo),e(Ov,lVo),e(B,iVo),e(B,Xv),e(Xv,Tce),e(Tce,dVo),e(Xv,cVo),e(Xv,KN),e(KN,fVo),e(Xv,mVo),e(B,gVo),e(B,zv),e(zv,Fce),e(Fce,hVo),e(zv,pVo),e(zv,ZN),e(ZN,_Vo),e(zv,uVo),e(B,bVo),e(B,Vv),e(Vv,Cce),e(Cce,vVo),e(Vv,TVo),e(Vv,eD),e(eD,FVo),e(Vv,CVo),e(B,MVo),e(B,Wv),e(Wv,Mce),e(Mce,EVo),e(Wv,yVo),e(Wv,oD),e(oD,wVo),e(Wv,AVo),e(B,LVo),e(B,Qv),e(Qv,Ece),e(Ece,BVo),e(Qv,kVo),e(Qv,rD),e(rD,xVo),e(Qv,RVo),e(B,SVo),e(B,Hv),e(Hv,yce),e(yce,PVo),e(Hv,$Vo),e(Hv,tD),e(tD,IVo),e(Hv,jVo),e(B,NVo),e(B,Uv),e(Uv,wce),e(wce,DVo),e(Uv,qVo),e(Uv,aD),e(aD,GVo),e(Uv,OVo),e(B,XVo),e(B,Jv),e(Jv,Ace),e(Ace,zVo),e(Jv,VVo),e(Jv,nD),e(nD,WVo),e(Jv,QVo),e(B,HVo),e(B,Yv),e(Yv,Lce),e(Lce,UVo),e(Yv,JVo),e(Yv,sD),e(sD,YVo),e(Yv,KVo),e(B,ZVo),e(B,Kv),e(Kv,Bce),e(Bce,eWo),e(Kv,oWo),e(Kv,lD),e(lD,rWo),e(Kv,tWo),e(B,aWo),e(B,Zv),e(Zv,kce),e(kce,nWo),e(Zv,sWo),e(Zv,iD),e(iD,lWo),e(Zv,iWo),e(B,dWo),e(B,e6),e(e6,xce),e(xce,cWo),e(e6,fWo),e(e6,dD),e(dD,mWo),e(e6,gWo),e(B,hWo),e(B,o6),e(o6,Rce),e(Rce,pWo),e(o6,_Wo),e(o6,cD),e(cD,uWo),e(o6,bWo),e(B,vWo),e(B,r6),e(r6,Sce),e(Sce,TWo),e(r6,FWo),e(r6,fD),e(fD,CWo),e(r6,MWo),e(B,EWo),e(B,t6),e(t6,Pce),e(Pce,yWo),e(t6,wWo),e(t6,mD),e(mD,AWo),e(t6,LWo),e(B,BWo),e(B,a6),e(a6,$ce),e($ce,kWo),e(a6,xWo),e(a6,gD),e(gD,RWo),e(a6,SWo),e(B,PWo),e(B,n6),e(n6,Ice),e(Ice,$Wo),e(n6,IWo),e(n6,hD),e(hD,jWo),e(n6,NWo),e(B,DWo),e(B,s6),e(s6,jce),e(jce,qWo),e(s6,GWo),e(s6,pD),e(pD,OWo),e(s6,XWo),e(B,zWo),e(B,l6),e(l6,Nce),e(Nce,VWo),e(l6,WWo),e(l6,_D),e(_D,QWo),e(l6,HWo),e(B,UWo),e(B,i6),e(i6,Dce),e(Dce,JWo),e(i6,YWo),e(i6,uD),e(uD,KWo),e(i6,ZWo),e(B,eQo),e(B,d6),e(d6,qce),e(qce,oQo),e(d6,rQo),e(d6,bD),e(bD,tQo),e(d6,aQo),e(go,nQo),e(go,Gce),e(Gce,sQo),e(go,lQo),g($3,go,null),b(d,i8e,u),b(d,ac,u),e(ac,c6),e(c6,Oce),g(I3,Oce,null),e(ac,iQo),e(ac,Xce),e(Xce,dQo),b(d,d8e,u),b(d,hr,u),g(j3,hr,null),e(hr,cQo),e(hr,nc),e(nc,fQo),e(nc,zce),e(zce,mQo),e(nc,gQo),e(nc,Vce),e(Vce,hQo),e(nc,pQo),e(hr,_Qo),e(hr,N3),e(N3,uQo),e(N3,Wce),e(Wce,bQo),e(N3,vQo),e(hr,TQo),e(hr,lt),g(D3,lt,null),e(lt,FQo),e(lt,Qce),e(Qce,CQo),e(lt,MQo),e(lt,sc),e(sc,EQo),e(sc,Hce),e(Hce,yQo),e(sc,wQo),e(sc,Uce),e(Uce,AQo),e(sc,LQo),e(lt,BQo),e(lt,Jce),e(Jce,kQo),e(lt,xQo),g(q3,lt,null),e(hr,RQo),e(hr,ho),g(G3,ho,null),e(ho,SQo),e(ho,Yce),e(Yce,PQo),e(ho,$Qo),e(ho,dn),e(dn,IQo),e(dn,Kce),e(Kce,jQo),e(dn,NQo),e(dn,Zce),e(Zce,DQo),e(dn,qQo),e(dn,efe),e(efe,GQo),e(dn,OQo),e(ho,XQo),e(ho,H),e(H,f6),e(f6,ofe),e(ofe,zQo),e(f6,VQo),e(f6,vD),e(vD,WQo),e(f6,QQo),e(H,HQo),e(H,m6),e(m6,rfe),e(rfe,UQo),e(m6,JQo),e(m6,TD),e(TD,YQo),e(m6,KQo),e(H,ZQo),e(H,g6),e(g6,tfe),e(tfe,eHo),e(g6,oHo),e(g6,FD),e(FD,rHo),e(g6,tHo),e(H,aHo),e(H,h6),e(h6,afe),e(afe,nHo),e(h6,sHo),e(h6,CD),e(CD,lHo),e(h6,iHo),e(H,dHo),e(H,p6),e(p6,nfe),e(nfe,cHo),e(p6,fHo),e(p6,MD),e(MD,mHo),e(p6,gHo),e(H,hHo),e(H,_6),e(_6,sfe),e(sfe,pHo),e(_6,_Ho),e(_6,ED),e(ED,uHo),e(_6,bHo),e(H,vHo),e(H,u6),e(u6,lfe),e(lfe,THo),e(u6,FHo),e(u6,yD),e(yD,CHo),e(u6,MHo),e(H,EHo),e(H,b6),e(b6,ife),e(ife,yHo),e(b6,wHo),e(b6,wD),e(wD,AHo),e(b6,LHo),e(H,BHo),e(H,v6),e(v6,dfe),e(dfe,kHo),e(v6,xHo),e(v6,AD),e(AD,RHo),e(v6,SHo),e(H,PHo),e(H,T6),e(T6,cfe),e(cfe,$Ho),e(T6,IHo),e(T6,LD),e(LD,jHo),e(T6,NHo),e(H,DHo),e(H,F6),e(F6,ffe),e(ffe,qHo),e(F6,GHo),e(F6,BD),e(BD,OHo),e(F6,XHo),e(H,zHo),e(H,C6),e(C6,mfe),e(mfe,VHo),e(C6,WHo),e(C6,kD),e(kD,QHo),e(C6,HHo),e(H,UHo),e(H,M6),e(M6,gfe),e(gfe,JHo),e(M6,YHo),e(M6,xD),e(xD,KHo),e(M6,ZHo),e(H,eUo),e(H,E6),e(E6,hfe),e(hfe,oUo),e(E6,rUo),e(E6,RD),e(RD,tUo),e(E6,aUo),e(H,nUo),e(H,y6),e(y6,pfe),e(pfe,sUo),e(y6,lUo),e(y6,SD),e(SD,iUo),e(y6,dUo),e(H,cUo),e(H,w6),e(w6,_fe),e(_fe,fUo),e(w6,mUo),e(w6,PD),e(PD,gUo),e(w6,hUo),e(H,pUo),e(H,A6),e(A6,ufe),e(ufe,_Uo),e(A6,uUo),e(A6,$D),e($D,bUo),e(A6,vUo),e(H,TUo),e(H,L6),e(L6,bfe),e(bfe,FUo),e(L6,CUo),e(L6,ID),e(ID,MUo),e(L6,EUo),e(H,yUo),e(H,B6),e(B6,vfe),e(vfe,wUo),e(B6,AUo),e(B6,jD),e(jD,LUo),e(B6,BUo),e(H,kUo),e(H,k6),e(k6,Tfe),e(Tfe,xUo),e(k6,RUo),e(k6,ND),e(ND,SUo),e(k6,PUo),e(H,$Uo),e(H,x6),e(x6,Ffe),e(Ffe,IUo),e(x6,jUo),e(x6,DD),e(DD,NUo),e(x6,DUo),e(H,qUo),e(H,R6),e(R6,Cfe),e(Cfe,GUo),e(R6,OUo),e(R6,qD),e(qD,XUo),e(R6,zUo),e(ho,VUo),e(ho,Mfe),e(Mfe,WUo),e(ho,QUo),g(O3,ho,null),b(d,c8e,u),b(d,lc,u),e(lc,S6),e(S6,Efe),g(X3,Efe,null),e(lc,HUo),e(lc,yfe),e(yfe,UUo),b(d,f8e,u),b(d,pr,u),g(z3,pr,null),e(pr,JUo),e(pr,ic),e(ic,YUo),e(ic,wfe),e(wfe,KUo),e(ic,ZUo),e(ic,Afe),e(Afe,eJo),e(ic,oJo),e(pr,rJo),e(pr,V3),e(V3,tJo),e(V3,Lfe),e(Lfe,aJo),e(V3,nJo),e(pr,sJo),e(pr,it),g(W3,it,null),e(it,lJo),e(it,Bfe),e(Bfe,iJo),e(it,dJo),e(it,dc),e(dc,cJo),e(dc,kfe),e(kfe,fJo),e(dc,mJo),e(dc,xfe),e(xfe,gJo),e(dc,hJo),e(it,pJo),e(it,Rfe),e(Rfe,_Jo),e(it,uJo),g(Q3,it,null),e(pr,bJo),e(pr,po),g(H3,po,null),e(po,vJo),e(po,Sfe),e(Sfe,TJo),e(po,FJo),e(po,cn),e(cn,CJo),e(cn,Pfe),e(Pfe,MJo),e(cn,EJo),e(cn,$fe),e($fe,yJo),e(cn,wJo),e(cn,Ife),e(Ife,AJo),e(cn,LJo),e(po,BJo),e(po,he),e(he,P6),e(P6,jfe),e(jfe,kJo),e(P6,xJo),e(P6,GD),e(GD,RJo),e(P6,SJo),e(he,PJo),e(he,$6),e($6,Nfe),e(Nfe,$Jo),e($6,IJo),e($6,OD),e(OD,jJo),e($6,NJo),e(he,DJo),e(he,I6),e(I6,Dfe),e(Dfe,qJo),e(I6,GJo),e(I6,XD),e(XD,OJo),e(I6,XJo),e(he,zJo),e(he,j6),e(j6,qfe),e(qfe,VJo),e(j6,WJo),e(j6,zD),e(zD,QJo),e(j6,HJo),e(he,UJo),e(he,N6),e(N6,Gfe),e(Gfe,JJo),e(N6,YJo),e(N6,VD),e(VD,KJo),e(N6,ZJo),e(he,eYo),e(he,D6),e(D6,Ofe),e(Ofe,oYo),e(D6,rYo),e(D6,WD),e(WD,tYo),e(D6,aYo),e(he,nYo),e(he,q6),e(q6,Xfe),e(Xfe,sYo),e(q6,lYo),e(q6,QD),e(QD,iYo),e(q6,dYo),e(he,cYo),e(he,G6),e(G6,zfe),e(zfe,fYo),e(G6,mYo),e(G6,HD),e(HD,gYo),e(G6,hYo),e(he,pYo),e(he,O6),e(O6,Vfe),e(Vfe,_Yo),e(O6,uYo),e(O6,UD),e(UD,bYo),e(O6,vYo),e(he,TYo),e(he,X6),e(X6,Wfe),e(Wfe,FYo),e(X6,CYo),e(X6,JD),e(JD,MYo),e(X6,EYo),e(po,yYo),e(po,Qfe),e(Qfe,wYo),e(po,AYo),g(U3,po,null),b(d,m8e,u),b(d,cc,u),e(cc,z6),e(z6,Hfe),g(J3,Hfe,null),e(cc,LYo),e(cc,Ufe),e(Ufe,BYo),b(d,g8e,u),b(d,_r,u),g(Y3,_r,null),e(_r,kYo),e(_r,fc),e(fc,xYo),e(fc,Jfe),e(Jfe,RYo),e(fc,SYo),e(fc,Yfe),e(Yfe,PYo),e(fc,$Yo),e(_r,IYo),e(_r,K3),e(K3,jYo),e(K3,Kfe),e(Kfe,NYo),e(K3,DYo),e(_r,qYo),e(_r,dt),g(Z3,dt,null),e(dt,GYo),e(dt,Zfe),e(Zfe,OYo),e(dt,XYo),e(dt,mc),e(mc,zYo),e(mc,eme),e(eme,VYo),e(mc,WYo),e(mc,ome),e(ome,QYo),e(mc,HYo),e(dt,UYo),e(dt,rme),e(rme,JYo),e(dt,YYo),g(ey,dt,null),e(_r,KYo),e(_r,_o),g(oy,_o,null),e(_o,ZYo),e(_o,tme),e(tme,eKo),e(_o,oKo),e(_o,fn),e(fn,rKo),e(fn,ame),e(ame,tKo),e(fn,aKo),e(fn,nme),e(nme,nKo),e(fn,sKo),e(fn,sme),e(sme,lKo),e(fn,iKo),e(_o,dKo),e(_o,lme),e(lme,V6),e(V6,ime),e(ime,cKo),e(V6,fKo),e(V6,YD),e(YD,mKo),e(V6,gKo),e(_o,hKo),e(_o,dme),e(dme,pKo),e(_o,_Ko),g(ry,_o,null),b(d,h8e,u),b(d,gc,u),e(gc,W6),e(W6,cme),g(ty,cme,null),e(gc,uKo),e(gc,fme),e(fme,bKo),b(d,p8e,u),b(d,ur,u),g(ay,ur,null),e(ur,vKo),e(ur,hc),e(hc,TKo),e(hc,mme),e(mme,FKo),e(hc,CKo),e(hc,gme),e(gme,MKo),e(hc,EKo),e(ur,yKo),e(ur,ny),e(ny,wKo),e(ny,hme),e(hme,AKo),e(ny,LKo),e(ur,BKo),e(ur,ct),g(sy,ct,null),e(ct,kKo),e(ct,pme),e(pme,xKo),e(ct,RKo),e(ct,pc),e(pc,SKo),e(pc,_me),e(_me,PKo),e(pc,$Ko),e(pc,ume),e(ume,IKo),e(pc,jKo),e(ct,NKo),e(ct,bme),e(bme,DKo),e(ct,qKo),g(ly,ct,null),e(ur,GKo),e(ur,uo),g(iy,uo,null),e(uo,OKo),e(uo,vme),e(vme,XKo),e(uo,zKo),e(uo,mn),e(mn,VKo),e(mn,Tme),e(Tme,WKo),e(mn,QKo),e(mn,Fme),e(Fme,HKo),e(mn,UKo),e(mn,Cme),e(Cme,JKo),e(mn,YKo),e(uo,KKo),e(uo,Y),e(Y,Q6),e(Q6,Mme),e(Mme,ZKo),e(Q6,eZo),e(Q6,KD),e(KD,oZo),e(Q6,rZo),e(Y,tZo),e(Y,H6),e(H6,Eme),e(Eme,aZo),e(H6,nZo),e(H6,ZD),e(ZD,sZo),e(H6,lZo),e(Y,iZo),e(Y,U6),e(U6,yme),e(yme,dZo),e(U6,cZo),e(U6,eq),e(eq,fZo),e(U6,mZo),e(Y,gZo),e(Y,J6),e(J6,wme),e(wme,hZo),e(J6,pZo),e(J6,oq),e(oq,_Zo),e(J6,uZo),e(Y,bZo),e(Y,Y6),e(Y6,Ame),e(Ame,vZo),e(Y6,TZo),e(Y6,rq),e(rq,FZo),e(Y6,CZo),e(Y,MZo),e(Y,K6),e(K6,Lme),e(Lme,EZo),e(K6,yZo),e(K6,tq),e(tq,wZo),e(K6,AZo),e(Y,LZo),e(Y,Z6),e(Z6,Bme),e(Bme,BZo),e(Z6,kZo),e(Z6,aq),e(aq,xZo),e(Z6,RZo),e(Y,SZo),e(Y,eT),e(eT,kme),e(kme,PZo),e(eT,$Zo),e(eT,nq),e(nq,IZo),e(eT,jZo),e(Y,NZo),e(Y,oT),e(oT,xme),e(xme,DZo),e(oT,qZo),e(oT,sq),e(sq,GZo),e(oT,OZo),e(Y,XZo),e(Y,rT),e(rT,Rme),e(Rme,zZo),e(rT,VZo),e(rT,lq),e(lq,WZo),e(rT,QZo),e(Y,HZo),e(Y,tT),e(tT,Sme),e(Sme,UZo),e(tT,JZo),e(tT,iq),e(iq,YZo),e(tT,KZo),e(Y,ZZo),e(Y,aT),e(aT,Pme),e(Pme,eer),e(aT,oer),e(aT,dq),e(dq,rer),e(aT,ter),e(Y,aer),e(Y,nT),e(nT,$me),e($me,ner),e(nT,ser),e(nT,cq),e(cq,ler),e(nT,ier),e(Y,der),e(Y,sT),e(sT,Ime),e(Ime,cer),e(sT,fer),e(sT,fq),e(fq,mer),e(sT,ger),e(Y,her),e(Y,lT),e(lT,jme),e(jme,per),e(lT,_er),e(lT,mq),e(mq,uer),e(lT,ber),e(Y,ver),e(Y,iT),e(iT,Nme),e(Nme,Ter),e(iT,Fer),e(iT,gq),e(gq,Cer),e(iT,Mer),e(Y,Eer),e(Y,dT),e(dT,Dme),e(Dme,yer),e(dT,wer),e(dT,hq),e(hq,Aer),e(dT,Ler),e(Y,Ber),e(Y,cT),e(cT,qme),e(qme,ker),e(cT,xer),e(cT,pq),e(pq,Rer),e(cT,Ser),e(Y,Per),e(Y,fT),e(fT,Gme),e(Gme,$er),e(fT,Ier),e(fT,_q),e(_q,jer),e(fT,Ner),e(Y,Der),e(Y,mT),e(mT,Ome),e(Ome,qer),e(mT,Ger),e(mT,uq),e(uq,Oer),e(mT,Xer),e(uo,zer),e(uo,Xme),e(Xme,Ver),e(uo,Wer),g(dy,uo,null),b(d,_8e,u),b(d,_c,u),e(_c,gT),e(gT,zme),g(cy,zme,null),e(_c,Qer),e(_c,Vme),e(Vme,Her),b(d,u8e,u),b(d,br,u),g(fy,br,null),e(br,Uer),e(br,uc),e(uc,Jer),e(uc,Wme),e(Wme,Yer),e(uc,Ker),e(uc,Qme),e(Qme,Zer),e(uc,eor),e(br,oor),e(br,my),e(my,ror),e(my,Hme),e(Hme,tor),e(my,aor),e(br,nor),e(br,ft),g(gy,ft,null),e(ft,sor),e(ft,Ume),e(Ume,lor),e(ft,ior),e(ft,bc),e(bc,dor),e(bc,Jme),e(Jme,cor),e(bc,mor),e(bc,Yme),e(Yme,gor),e(bc,hor),e(ft,por),e(ft,Kme),e(Kme,_or),e(ft,uor),g(hy,ft,null),e(br,bor),e(br,bo),g(py,bo,null),e(bo,vor),e(bo,Zme),e(Zme,Tor),e(bo,For),e(bo,gn),e(gn,Cor),e(gn,ege),e(ege,Mor),e(gn,Eor),e(gn,oge),e(oge,yor),e(gn,wor),e(gn,rge),e(rge,Aor),e(gn,Lor),e(bo,Bor),e(bo,pe),e(pe,hT),e(hT,tge),e(tge,kor),e(hT,xor),e(hT,bq),e(bq,Ror),e(hT,Sor),e(pe,Por),e(pe,pT),e(pT,age),e(age,$or),e(pT,Ior),e(pT,vq),e(vq,jor),e(pT,Nor),e(pe,Dor),e(pe,_T),e(_T,nge),e(nge,qor),e(_T,Gor),e(_T,Tq),e(Tq,Oor),e(_T,Xor),e(pe,zor),e(pe,uT),e(uT,sge),e(sge,Vor),e(uT,Wor),e(uT,Fq),e(Fq,Qor),e(uT,Hor),e(pe,Uor),e(pe,bT),e(bT,lge),e(lge,Jor),e(bT,Yor),e(bT,Cq),e(Cq,Kor),e(bT,Zor),e(pe,err),e(pe,vT),e(vT,ige),e(ige,orr),e(vT,rrr),e(vT,Mq),e(Mq,trr),e(vT,arr),e(pe,nrr),e(pe,TT),e(TT,dge),e(dge,srr),e(TT,lrr),e(TT,Eq),e(Eq,irr),e(TT,drr),e(pe,crr),e(pe,FT),e(FT,cge),e(cge,frr),e(FT,mrr),e(FT,yq),e(yq,grr),e(FT,hrr),e(pe,prr),e(pe,CT),e(CT,fge),e(fge,_rr),e(CT,urr),e(CT,wq),e(wq,brr),e(CT,vrr),e(pe,Trr),e(pe,MT),e(MT,mge),e(mge,Frr),e(MT,Crr),e(MT,Aq),e(Aq,Mrr),e(MT,Err),e(bo,yrr),e(bo,gge),e(gge,wrr),e(bo,Arr),g(_y,bo,null),b(d,b8e,u),b(d,vc,u),e(vc,ET),e(ET,hge),g(uy,hge,null),e(vc,Lrr),e(vc,pge),e(pge,Brr),b(d,v8e,u),b(d,vr,u),g(by,vr,null),e(vr,krr),e(vr,Tc),e(Tc,xrr),e(Tc,_ge),e(_ge,Rrr),e(Tc,Srr),e(Tc,uge),e(uge,Prr),e(Tc,$rr),e(vr,Irr),e(vr,vy),e(vy,jrr),e(vy,bge),e(bge,Nrr),e(vy,Drr),e(vr,qrr),e(vr,mt),g(Ty,mt,null),e(mt,Grr),e(mt,vge),e(vge,Orr),e(mt,Xrr),e(mt,Fc),e(Fc,zrr),e(Fc,Tge),e(Tge,Vrr),e(Fc,Wrr),e(Fc,Fge),e(Fge,Qrr),e(Fc,Hrr),e(mt,Urr),e(mt,Cge),e(Cge,Jrr),e(mt,Yrr),g(Fy,mt,null),e(vr,Krr),e(vr,vo),g(Cy,vo,null),e(vo,Zrr),e(vo,Mge),e(Mge,etr),e(vo,otr),e(vo,hn),e(hn,rtr),e(hn,Ege),e(Ege,ttr),e(hn,atr),e(hn,yge),e(yge,ntr),e(hn,str),e(hn,wge),e(wge,ltr),e(hn,itr),e(vo,dtr),e(vo,X),e(X,yT),e(yT,Age),e(Age,ctr),e(yT,ftr),e(yT,Lq),e(Lq,mtr),e(yT,gtr),e(X,htr),e(X,wT),e(wT,Lge),e(Lge,ptr),e(wT,_tr),e(wT,Bq),e(Bq,utr),e(wT,btr),e(X,vtr),e(X,AT),e(AT,Bge),e(Bge,Ttr),e(AT,Ftr),e(AT,kq),e(kq,Ctr),e(AT,Mtr),e(X,Etr),e(X,LT),e(LT,kge),e(kge,ytr),e(LT,wtr),e(LT,xq),e(xq,Atr),e(LT,Ltr),e(X,Btr),e(X,BT),e(BT,xge),e(xge,ktr),e(BT,xtr),e(BT,Rq),e(Rq,Rtr),e(BT,Str),e(X,Ptr),e(X,kT),e(kT,Rge),e(Rge,$tr),e(kT,Itr),e(kT,Sq),e(Sq,jtr),e(kT,Ntr),e(X,Dtr),e(X,xT),e(xT,Sge),e(Sge,qtr),e(xT,Gtr),e(xT,Pq),e(Pq,Otr),e(xT,Xtr),e(X,ztr),e(X,RT),e(RT,Pge),e(Pge,Vtr),e(RT,Wtr),e(RT,$q),e($q,Qtr),e(RT,Htr),e(X,Utr),e(X,ST),e(ST,$ge),e($ge,Jtr),e(ST,Ytr),e(ST,Iq),e(Iq,Ktr),e(ST,Ztr),e(X,ear),e(X,PT),e(PT,Ige),e(Ige,oar),e(PT,rar),e(PT,jq),e(jq,tar),e(PT,aar),e(X,nar),e(X,$T),e($T,jge),e(jge,sar),e($T,lar),e($T,Nq),e(Nq,iar),e($T,dar),e(X,car),e(X,IT),e(IT,Nge),e(Nge,far),e(IT,mar),e(IT,Dq),e(Dq,gar),e(IT,har),e(X,par),e(X,jT),e(jT,Dge),e(Dge,_ar),e(jT,uar),e(jT,qq),e(qq,bar),e(jT,Tar),e(X,Far),e(X,NT),e(NT,qge),e(qge,Car),e(NT,Mar),e(NT,Gq),e(Gq,Ear),e(NT,yar),e(X,war),e(X,DT),e(DT,Gge),e(Gge,Aar),e(DT,Lar),e(DT,Oq),e(Oq,Bar),e(DT,kar),e(X,xar),e(X,qT),e(qT,Oge),e(Oge,Rar),e(qT,Sar),e(qT,Xq),e(Xq,Par),e(qT,$ar),e(X,Iar),e(X,GT),e(GT,Xge),e(Xge,jar),e(GT,Nar),e(GT,zq),e(zq,Dar),e(GT,qar),e(X,Gar),e(X,OT),e(OT,zge),e(zge,Oar),e(OT,Xar),e(OT,Vq),e(Vq,zar),e(OT,Var),e(X,War),e(X,XT),e(XT,Vge),e(Vge,Qar),e(XT,Har),e(XT,Wq),e(Wq,Uar),e(XT,Jar),e(X,Yar),e(X,zT),e(zT,Wge),e(Wge,Kar),e(zT,Zar),e(zT,Qq),e(Qq,enr),e(zT,onr),e(X,rnr),e(X,VT),e(VT,Qge),e(Qge,tnr),e(VT,anr),e(VT,Hq),e(Hq,nnr),e(VT,snr),e(X,lnr),e(X,WT),e(WT,Hge),e(Hge,inr),e(WT,dnr),e(WT,Uq),e(Uq,cnr),e(WT,fnr),e(X,mnr),e(X,QT),e(QT,Uge),e(Uge,gnr),e(QT,hnr),e(QT,Jq),e(Jq,pnr),e(QT,_nr),e(X,unr),e(X,HT),e(HT,Jge),e(Jge,bnr),e(HT,vnr),e(HT,Yq),e(Yq,Tnr),e(HT,Fnr),e(X,Cnr),e(X,UT),e(UT,Yge),e(Yge,Mnr),e(UT,Enr),e(UT,Kq),e(Kq,ynr),e(UT,wnr),e(vo,Anr),e(vo,Kge),e(Kge,Lnr),e(vo,Bnr),g(My,vo,null),b(d,T8e,u),b(d,Cc,u),e(Cc,JT),e(JT,Zge),g(Ey,Zge,null),e(Cc,knr),e(Cc,ehe),e(ehe,xnr),b(d,F8e,u),b(d,Tr,u),g(yy,Tr,null),e(Tr,Rnr),e(Tr,Mc),e(Mc,Snr),e(Mc,ohe),e(ohe,Pnr),e(Mc,$nr),e(Mc,rhe),e(rhe,Inr),e(Mc,jnr),e(Tr,Nnr),e(Tr,wy),e(wy,Dnr),e(wy,the),e(the,qnr),e(wy,Gnr),e(Tr,Onr),e(Tr,gt),g(Ay,gt,null),e(gt,Xnr),e(gt,ahe),e(ahe,znr),e(gt,Vnr),e(gt,Ec),e(Ec,Wnr),e(Ec,nhe),e(nhe,Qnr),e(Ec,Hnr),e(Ec,she),e(she,Unr),e(Ec,Jnr),e(gt,Ynr),e(gt,lhe),e(lhe,Knr),e(gt,Znr),g(Ly,gt,null),e(Tr,esr),e(Tr,To),g(By,To,null),e(To,osr),e(To,ihe),e(ihe,rsr),e(To,tsr),e(To,pn),e(pn,asr),e(pn,dhe),e(dhe,nsr),e(pn,ssr),e(pn,che),e(che,lsr),e(pn,isr),e(pn,fhe),e(fhe,dsr),e(pn,csr),e(To,fsr),e(To,te),e(te,YT),e(YT,mhe),e(mhe,msr),e(YT,gsr),e(YT,Zq),e(Zq,hsr),e(YT,psr),e(te,_sr),e(te,KT),e(KT,ghe),e(ghe,usr),e(KT,bsr),e(KT,eG),e(eG,vsr),e(KT,Tsr),e(te,Fsr),e(te,ZT),e(ZT,hhe),e(hhe,Csr),e(ZT,Msr),e(ZT,oG),e(oG,Esr),e(ZT,ysr),e(te,wsr),e(te,e7),e(e7,phe),e(phe,Asr),e(e7,Lsr),e(e7,rG),e(rG,Bsr),e(e7,ksr),e(te,xsr),e(te,o7),e(o7,_he),e(_he,Rsr),e(o7,Ssr),e(o7,tG),e(tG,Psr),e(o7,$sr),e(te,Isr),e(te,r7),e(r7,uhe),e(uhe,jsr),e(r7,Nsr),e(r7,aG),e(aG,Dsr),e(r7,qsr),e(te,Gsr),e(te,t7),e(t7,bhe),e(bhe,Osr),e(t7,Xsr),e(t7,nG),e(nG,zsr),e(t7,Vsr),e(te,Wsr),e(te,a7),e(a7,vhe),e(vhe,Qsr),e(a7,Hsr),e(a7,sG),e(sG,Usr),e(a7,Jsr),e(te,Ysr),e(te,n7),e(n7,The),e(The,Ksr),e(n7,Zsr),e(n7,lG),e(lG,elr),e(n7,olr),e(te,rlr),e(te,s7),e(s7,Fhe),e(Fhe,tlr),e(s7,alr),e(s7,iG),e(iG,nlr),e(s7,slr),e(te,llr),e(te,l7),e(l7,Che),e(Che,ilr),e(l7,dlr),e(l7,dG),e(dG,clr),e(l7,flr),e(te,mlr),e(te,i7),e(i7,Mhe),e(Mhe,glr),e(i7,hlr),e(i7,cG),e(cG,plr),e(i7,_lr),e(te,ulr),e(te,d7),e(d7,Ehe),e(Ehe,blr),e(d7,vlr),e(d7,fG),e(fG,Tlr),e(d7,Flr),e(te,Clr),e(te,c7),e(c7,yhe),e(yhe,Mlr),e(c7,Elr),e(c7,mG),e(mG,ylr),e(c7,wlr),e(te,Alr),e(te,f7),e(f7,whe),e(whe,Llr),e(f7,Blr),e(f7,gG),e(gG,klr),e(f7,xlr),e(te,Rlr),e(te,m7),e(m7,Ahe),e(Ahe,Slr),e(m7,Plr),e(m7,hG),e(hG,$lr),e(m7,Ilr),e(te,jlr),e(te,g7),e(g7,Lhe),e(Lhe,Nlr),e(g7,Dlr),e(g7,pG),e(pG,qlr),e(g7,Glr),e(To,Olr),e(To,Bhe),e(Bhe,Xlr),e(To,zlr),g(ky,To,null),b(d,C8e,u),b(d,yc,u),e(yc,h7),e(h7,khe),g(xy,khe,null),e(yc,Vlr),e(yc,xhe),e(xhe,Wlr),b(d,M8e,u),b(d,Fr,u),g(Ry,Fr,null),e(Fr,Qlr),e(Fr,wc),e(wc,Hlr),e(wc,Rhe),e(Rhe,Ulr),e(wc,Jlr),e(wc,She),e(She,Ylr),e(wc,Klr),e(Fr,Zlr),e(Fr,Sy),e(Sy,eir),e(Sy,Phe),e(Phe,oir),e(Sy,rir),e(Fr,tir),e(Fr,ht),g(Py,ht,null),e(ht,air),e(ht,$he),e($he,nir),e(ht,sir),e(ht,Ac),e(Ac,lir),e(Ac,Ihe),e(Ihe,iir),e(Ac,dir),e(Ac,jhe),e(jhe,cir),e(Ac,fir),e(ht,mir),e(ht,Nhe),e(Nhe,gir),e(ht,hir),g($y,ht,null),e(Fr,pir),e(Fr,Fo),g(Iy,Fo,null),e(Fo,_ir),e(Fo,Dhe),e(Dhe,uir),e(Fo,bir),e(Fo,_n),e(_n,vir),e(_n,qhe),e(qhe,Tir),e(_n,Fir),e(_n,Ghe),e(Ghe,Cir),e(_n,Mir),e(_n,Ohe),e(Ohe,Eir),e(_n,yir),e(Fo,wir),e(Fo,Xhe),e(Xhe,p7),e(p7,zhe),e(zhe,Air),e(p7,Lir),e(p7,_G),e(_G,Bir),e(p7,kir),e(Fo,xir),e(Fo,Vhe),e(Vhe,Rir),e(Fo,Sir),g(jy,Fo,null),b(d,E8e,u),b(d,Lc,u),e(Lc,_7),e(_7,Whe),g(Ny,Whe,null),e(Lc,Pir),e(Lc,Qhe),e(Qhe,$ir),b(d,y8e,u),b(d,Cr,u),g(Dy,Cr,null),e(Cr,Iir),e(Cr,Bc),e(Bc,jir),e(Bc,Hhe),e(Hhe,Nir),e(Bc,Dir),e(Bc,Uhe),e(Uhe,qir),e(Bc,Gir),e(Cr,Oir),e(Cr,qy),e(qy,Xir),e(qy,Jhe),e(Jhe,zir),e(qy,Vir),e(Cr,Wir),e(Cr,pt),g(Gy,pt,null),e(pt,Qir),e(pt,Yhe),e(Yhe,Hir),e(pt,Uir),e(pt,kc),e(kc,Jir),e(kc,Khe),e(Khe,Yir),e(kc,Kir),e(kc,Zhe),e(Zhe,Zir),e(kc,edr),e(pt,odr),e(pt,epe),e(epe,rdr),e(pt,tdr),g(Oy,pt,null),e(Cr,adr),e(Cr,Co),g(Xy,Co,null),e(Co,ndr),e(Co,ope),e(ope,sdr),e(Co,ldr),e(Co,un),e(un,idr),e(un,rpe),e(rpe,ddr),e(un,cdr),e(un,tpe),e(tpe,fdr),e(un,mdr),e(un,ape),e(ape,gdr),e(un,hdr),e(Co,pdr),e(Co,K),e(K,u7),e(u7,npe),e(npe,_dr),e(u7,udr),e(u7,uG),e(uG,bdr),e(u7,vdr),e(K,Tdr),e(K,b7),e(b7,spe),e(spe,Fdr),e(b7,Cdr),e(b7,bG),e(bG,Mdr),e(b7,Edr),e(K,ydr),e(K,v7),e(v7,lpe),e(lpe,wdr),e(v7,Adr),e(v7,vG),e(vG,Ldr),e(v7,Bdr),e(K,kdr),e(K,T7),e(T7,ipe),e(ipe,xdr),e(T7,Rdr),e(T7,TG),e(TG,Sdr),e(T7,Pdr),e(K,$dr),e(K,F7),e(F7,dpe),e(dpe,Idr),e(F7,jdr),e(F7,FG),e(FG,Ndr),e(F7,Ddr),e(K,qdr),e(K,C7),e(C7,cpe),e(cpe,Gdr),e(C7,Odr),e(C7,CG),e(CG,Xdr),e(C7,zdr),e(K,Vdr),e(K,M7),e(M7,fpe),e(fpe,Wdr),e(M7,Qdr),e(M7,MG),e(MG,Hdr),e(M7,Udr),e(K,Jdr),e(K,E7),e(E7,mpe),e(mpe,Ydr),e(E7,Kdr),e(E7,EG),e(EG,Zdr),e(E7,ecr),e(K,ocr),e(K,y7),e(y7,gpe),e(gpe,rcr),e(y7,tcr),e(y7,yG),e(yG,acr),e(y7,ncr),e(K,scr),e(K,w7),e(w7,hpe),e(hpe,lcr),e(w7,icr),e(w7,wG),e(wG,dcr),e(w7,ccr),e(K,fcr),e(K,A7),e(A7,ppe),e(ppe,mcr),e(A7,gcr),e(A7,AG),e(AG,hcr),e(A7,pcr),e(K,_cr),e(K,L7),e(L7,_pe),e(_pe,ucr),e(L7,bcr),e(L7,LG),e(LG,vcr),e(L7,Tcr),e(K,Fcr),e(K,B7),e(B7,upe),e(upe,Ccr),e(B7,Mcr),e(B7,BG),e(BG,Ecr),e(B7,ycr),e(K,wcr),e(K,k7),e(k7,bpe),e(bpe,Acr),e(k7,Lcr),e(k7,kG),e(kG,Bcr),e(k7,kcr),e(K,xcr),e(K,x7),e(x7,vpe),e(vpe,Rcr),e(x7,Scr),e(x7,xG),e(xG,Pcr),e(x7,$cr),e(K,Icr),e(K,R7),e(R7,Tpe),e(Tpe,jcr),e(R7,Ncr),e(R7,RG),e(RG,Dcr),e(R7,qcr),e(K,Gcr),e(K,S7),e(S7,Fpe),e(Fpe,Ocr),e(S7,Xcr),e(S7,SG),e(SG,zcr),e(S7,Vcr),e(K,Wcr),e(K,P7),e(P7,Cpe),e(Cpe,Qcr),e(P7,Hcr),e(P7,PG),e(PG,Ucr),e(P7,Jcr),e(K,Ycr),e(K,$7),e($7,Mpe),e(Mpe,Kcr),e($7,Zcr),e($7,$G),e($G,efr),e($7,ofr),e(K,rfr),e(K,I7),e(I7,Epe),e(Epe,tfr),e(I7,afr),e(I7,IG),e(IG,nfr),e(I7,sfr),e(Co,lfr),e(Co,ype),e(ype,ifr),e(Co,dfr),g(zy,Co,null),b(d,w8e,u),b(d,xc,u),e(xc,j7),e(j7,wpe),g(Vy,wpe,null),e(xc,cfr),e(xc,Ape),e(Ape,ffr),b(d,A8e,u),b(d,Mr,u),g(Wy,Mr,null),e(Mr,mfr),e(Mr,Rc),e(Rc,gfr),e(Rc,Lpe),e(Lpe,hfr),e(Rc,pfr),e(Rc,Bpe),e(Bpe,_fr),e(Rc,ufr),e(Mr,bfr),e(Mr,Qy),e(Qy,vfr),e(Qy,kpe),e(kpe,Tfr),e(Qy,Ffr),e(Mr,Cfr),e(Mr,_t),g(Hy,_t,null),e(_t,Mfr),e(_t,xpe),e(xpe,Efr),e(_t,yfr),e(_t,Sc),e(Sc,wfr),e(Sc,Rpe),e(Rpe,Afr),e(Sc,Lfr),e(Sc,Spe),e(Spe,Bfr),e(Sc,kfr),e(_t,xfr),e(_t,Ppe),e(Ppe,Rfr),e(_t,Sfr),g(Uy,_t,null),e(Mr,Pfr),e(Mr,Mo),g(Jy,Mo,null),e(Mo,$fr),e(Mo,$pe),e($pe,Ifr),e(Mo,jfr),e(Mo,bn),e(bn,Nfr),e(bn,Ipe),e(Ipe,Dfr),e(bn,qfr),e(bn,jpe),e(jpe,Gfr),e(bn,Ofr),e(bn,Npe),e(Npe,Xfr),e(bn,zfr),e(Mo,Vfr),e(Mo,Z),e(Z,N7),e(N7,Dpe),e(Dpe,Wfr),e(N7,Qfr),e(N7,jG),e(jG,Hfr),e(N7,Ufr),e(Z,Jfr),e(Z,D7),e(D7,qpe),e(qpe,Yfr),e(D7,Kfr),e(D7,NG),e(NG,Zfr),e(D7,emr),e(Z,omr),e(Z,q7),e(q7,Gpe),e(Gpe,rmr),e(q7,tmr),e(q7,DG),e(DG,amr),e(q7,nmr),e(Z,smr),e(Z,G7),e(G7,Ope),e(Ope,lmr),e(G7,imr),e(G7,qG),e(qG,dmr),e(G7,cmr),e(Z,fmr),e(Z,O7),e(O7,Xpe),e(Xpe,mmr),e(O7,gmr),e(O7,GG),e(GG,hmr),e(O7,pmr),e(Z,_mr),e(Z,X7),e(X7,zpe),e(zpe,umr),e(X7,bmr),e(X7,OG),e(OG,vmr),e(X7,Tmr),e(Z,Fmr),e(Z,z7),e(z7,Vpe),e(Vpe,Cmr),e(z7,Mmr),e(z7,XG),e(XG,Emr),e(z7,ymr),e(Z,wmr),e(Z,V7),e(V7,Wpe),e(Wpe,Amr),e(V7,Lmr),e(V7,zG),e(zG,Bmr),e(V7,kmr),e(Z,xmr),e(Z,W7),e(W7,Qpe),e(Qpe,Rmr),e(W7,Smr),e(W7,VG),e(VG,Pmr),e(W7,$mr),e(Z,Imr),e(Z,Q7),e(Q7,Hpe),e(Hpe,jmr),e(Q7,Nmr),e(Q7,WG),e(WG,Dmr),e(Q7,qmr),e(Z,Gmr),e(Z,H7),e(H7,Upe),e(Upe,Omr),e(H7,Xmr),e(H7,QG),e(QG,zmr),e(H7,Vmr),e(Z,Wmr),e(Z,U7),e(U7,Jpe),e(Jpe,Qmr),e(U7,Hmr),e(U7,HG),e(HG,Umr),e(U7,Jmr),e(Z,Ymr),e(Z,J7),e(J7,Ype),e(Ype,Kmr),e(J7,Zmr),e(J7,UG),e(UG,egr),e(J7,ogr),e(Z,rgr),e(Z,Y7),e(Y7,Kpe),e(Kpe,tgr),e(Y7,agr),e(Y7,JG),e(JG,ngr),e(Y7,sgr),e(Z,lgr),e(Z,K7),e(K7,Zpe),e(Zpe,igr),e(K7,dgr),e(K7,YG),e(YG,cgr),e(K7,fgr),e(Z,mgr),e(Z,Z7),e(Z7,e_e),e(e_e,ggr),e(Z7,hgr),e(Z7,KG),e(KG,pgr),e(Z7,_gr),e(Z,ugr),e(Z,eF),e(eF,o_e),e(o_e,bgr),e(eF,vgr),e(eF,ZG),e(ZG,Tgr),e(eF,Fgr),e(Z,Cgr),e(Z,oF),e(oF,r_e),e(r_e,Mgr),e(oF,Egr),e(oF,eO),e(eO,ygr),e(oF,wgr),e(Z,Agr),e(Z,rF),e(rF,t_e),e(t_e,Lgr),e(rF,Bgr),e(rF,oO),e(oO,kgr),e(rF,xgr),e(Mo,Rgr),e(Mo,a_e),e(a_e,Sgr),e(Mo,Pgr),g(Yy,Mo,null),b(d,L8e,u),b(d,Pc,u),e(Pc,tF),e(tF,n_e),g(Ky,n_e,null),e(Pc,$gr),e(Pc,s_e),e(s_e,Igr),b(d,B8e,u),b(d,Er,u),g(Zy,Er,null),e(Er,jgr),e(Er,$c),e($c,Ngr),e($c,l_e),e(l_e,Dgr),e($c,qgr),e($c,i_e),e(i_e,Ggr),e($c,Ogr),e(Er,Xgr),e(Er,ew),e(ew,zgr),e(ew,d_e),e(d_e,Vgr),e(ew,Wgr),e(Er,Qgr),e(Er,ut),g(ow,ut,null),e(ut,Hgr),e(ut,c_e),e(c_e,Ugr),e(ut,Jgr),e(ut,Ic),e(Ic,Ygr),e(Ic,f_e),e(f_e,Kgr),e(Ic,Zgr),e(Ic,m_e),e(m_e,ehr),e(Ic,ohr),e(ut,rhr),e(ut,g_e),e(g_e,thr),e(ut,ahr),g(rw,ut,null),e(Er,nhr),e(Er,Eo),g(tw,Eo,null),e(Eo,shr),e(Eo,h_e),e(h_e,lhr),e(Eo,ihr),e(Eo,vn),e(vn,dhr),e(vn,p_e),e(p_e,chr),e(vn,fhr),e(vn,__e),e(__e,mhr),e(vn,ghr),e(vn,u_e),e(u_e,hhr),e(vn,phr),e(Eo,_hr),e(Eo,b_e),e(b_e,aF),e(aF,v_e),e(v_e,uhr),e(aF,bhr),e(aF,rO),e(rO,vhr),e(aF,Thr),e(Eo,Fhr),e(Eo,T_e),e(T_e,Chr),e(Eo,Mhr),g(aw,Eo,null),b(d,k8e,u),b(d,jc,u),e(jc,nF),e(nF,F_e),g(nw,F_e,null),e(jc,Ehr),e(jc,C_e),e(C_e,yhr),b(d,x8e,u),b(d,yr,u),g(sw,yr,null),e(yr,whr),e(yr,Nc),e(Nc,Ahr),e(Nc,M_e),e(M_e,Lhr),e(Nc,Bhr),e(Nc,E_e),e(E_e,khr),e(Nc,xhr),e(yr,Rhr),e(yr,lw),e(lw,Shr),e(lw,y_e),e(y_e,Phr),e(lw,$hr),e(yr,Ihr),e(yr,bt),g(iw,bt,null),e(bt,jhr),e(bt,w_e),e(w_e,Nhr),e(bt,Dhr),e(bt,Dc),e(Dc,qhr),e(Dc,A_e),e(A_e,Ghr),e(Dc,Ohr),e(Dc,L_e),e(L_e,Xhr),e(Dc,zhr),e(bt,Vhr),e(bt,B_e),e(B_e,Whr),e(bt,Qhr),g(dw,bt,null),e(yr,Hhr),e(yr,yo),g(cw,yo,null),e(yo,Uhr),e(yo,k_e),e(k_e,Jhr),e(yo,Yhr),e(yo,Tn),e(Tn,Khr),e(Tn,x_e),e(x_e,Zhr),e(Tn,epr),e(Tn,R_e),e(R_e,opr),e(Tn,rpr),e(Tn,S_e),e(S_e,tpr),e(Tn,apr),e(yo,npr),e(yo,P_e),e(P_e,sF),e(sF,$_e),e($_e,spr),e(sF,lpr),e(sF,tO),e(tO,ipr),e(sF,dpr),e(yo,cpr),e(yo,I_e),e(I_e,fpr),e(yo,mpr),g(fw,yo,null),b(d,R8e,u),b(d,qc,u),e(qc,lF),e(lF,j_e),g(mw,j_e,null),e(qc,gpr),e(qc,N_e),e(N_e,hpr),b(d,S8e,u),b(d,wr,u),g(gw,wr,null),e(wr,ppr),e(wr,Gc),e(Gc,_pr),e(Gc,D_e),e(D_e,upr),e(Gc,bpr),e(Gc,q_e),e(q_e,vpr),e(Gc,Tpr),e(wr,Fpr),e(wr,hw),e(hw,Cpr),e(hw,G_e),e(G_e,Mpr),e(hw,Epr),e(wr,ypr),e(wr,vt),g(pw,vt,null),e(vt,wpr),e(vt,O_e),e(O_e,Apr),e(vt,Lpr),e(vt,Oc),e(Oc,Bpr),e(Oc,X_e),e(X_e,kpr),e(Oc,xpr),e(Oc,z_e),e(z_e,Rpr),e(Oc,Spr),e(vt,Ppr),e(vt,V_e),e(V_e,$pr),e(vt,Ipr),g(_w,vt,null),e(wr,jpr),e(wr,wo),g(uw,wo,null),e(wo,Npr),e(wo,W_e),e(W_e,Dpr),e(wo,qpr),e(wo,Fn),e(Fn,Gpr),e(Fn,Q_e),e(Q_e,Opr),e(Fn,Xpr),e(Fn,H_e),e(H_e,zpr),e(Fn,Vpr),e(Fn,U_e),e(U_e,Wpr),e(Fn,Qpr),e(wo,Hpr),e(wo,V),e(V,iF),e(iF,J_e),e(J_e,Upr),e(iF,Jpr),e(iF,aO),e(aO,Ypr),e(iF,Kpr),e(V,Zpr),e(V,dF),e(dF,Y_e),e(Y_e,e_r),e(dF,o_r),e(dF,nO),e(nO,r_r),e(dF,t_r),e(V,a_r),e(V,cF),e(cF,K_e),e(K_e,n_r),e(cF,s_r),e(cF,sO),e(sO,l_r),e(cF,i_r),e(V,d_r),e(V,fF),e(fF,Z_e),e(Z_e,c_r),e(fF,f_r),e(fF,lO),e(lO,m_r),e(fF,g_r),e(V,h_r),e(V,mF),e(mF,eue),e(eue,p_r),e(mF,__r),e(mF,iO),e(iO,u_r),e(mF,b_r),e(V,v_r),e(V,gF),e(gF,oue),e(oue,T_r),e(gF,F_r),e(gF,dO),e(dO,C_r),e(gF,M_r),e(V,E_r),e(V,hF),e(hF,rue),e(rue,y_r),e(hF,w_r),e(hF,cO),e(cO,A_r),e(hF,L_r),e(V,B_r),e(V,pF),e(pF,tue),e(tue,k_r),e(pF,x_r),e(pF,fO),e(fO,R_r),e(pF,S_r),e(V,P_r),e(V,_F),e(_F,aue),e(aue,$_r),e(_F,I_r),e(_F,mO),e(mO,j_r),e(_F,N_r),e(V,D_r),e(V,uF),e(uF,nue),e(nue,q_r),e(uF,G_r),e(uF,gO),e(gO,O_r),e(uF,X_r),e(V,z_r),e(V,bF),e(bF,sue),e(sue,V_r),e(bF,W_r),e(bF,hO),e(hO,Q_r),e(bF,H_r),e(V,U_r),e(V,vF),e(vF,lue),e(lue,J_r),e(vF,Y_r),e(vF,pO),e(pO,K_r),e(vF,Z_r),e(V,eur),e(V,TF),e(TF,iue),e(iue,our),e(TF,rur),e(TF,_O),e(_O,tur),e(TF,aur),e(V,nur),e(V,FF),e(FF,due),e(due,sur),e(FF,lur),e(FF,uO),e(uO,iur),e(FF,dur),e(V,cur),e(V,CF),e(CF,cue),e(cue,fur),e(CF,mur),e(CF,bO),e(bO,gur),e(CF,hur),e(V,pur),e(V,MF),e(MF,fue),e(fue,_ur),e(MF,uur),e(MF,vO),e(vO,bur),e(MF,vur),e(V,Tur),e(V,EF),e(EF,mue),e(mue,Fur),e(EF,Cur),e(EF,TO),e(TO,Mur),e(EF,Eur),e(V,yur),e(V,yF),e(yF,gue),e(gue,wur),e(yF,Aur),e(yF,FO),e(FO,Lur),e(yF,Bur),e(V,kur),e(V,wF),e(wF,hue),e(hue,xur),e(wF,Rur),e(wF,CO),e(CO,Sur),e(wF,Pur),e(V,$ur),e(V,AF),e(AF,pue),e(pue,Iur),e(AF,jur),e(AF,MO),e(MO,Nur),e(AF,Dur),e(V,qur),e(V,LF),e(LF,_ue),e(_ue,Gur),e(LF,Our),e(LF,EO),e(EO,Xur),e(LF,zur),e(V,Vur),e(V,BF),e(BF,uue),e(uue,Wur),e(BF,Qur),e(BF,yO),e(yO,Hur),e(BF,Uur),e(V,Jur),e(V,kF),e(kF,bue),e(bue,Yur),e(kF,Kur),e(kF,wO),e(wO,Zur),e(kF,e1r),e(V,o1r),e(V,xF),e(xF,vue),e(vue,r1r),e(xF,t1r),e(xF,AO),e(AO,a1r),e(xF,n1r),e(wo,s1r),e(wo,Tue),e(Tue,l1r),e(wo,i1r),g(bw,wo,null),b(d,P8e,u),b(d,Xc,u),e(Xc,RF),e(RF,Fue),g(vw,Fue,null),e(Xc,d1r),e(Xc,Cue),e(Cue,c1r),b(d,$8e,u),b(d,Ar,u),g(Tw,Ar,null),e(Ar,f1r),e(Ar,zc),e(zc,m1r),e(zc,Mue),e(Mue,g1r),e(zc,h1r),e(zc,Eue),e(Eue,p1r),e(zc,_1r),e(Ar,u1r),e(Ar,Fw),e(Fw,b1r),e(Fw,yue),e(yue,v1r),e(Fw,T1r),e(Ar,F1r),e(Ar,Tt),g(Cw,Tt,null),e(Tt,C1r),e(Tt,wue),e(wue,M1r),e(Tt,E1r),e(Tt,Vc),e(Vc,y1r),e(Vc,Aue),e(Aue,w1r),e(Vc,A1r),e(Vc,Lue),e(Lue,L1r),e(Vc,B1r),e(Tt,k1r),e(Tt,Bue),e(Bue,x1r),e(Tt,R1r),g(Mw,Tt,null),e(Ar,S1r),e(Ar,Ao),g(Ew,Ao,null),e(Ao,P1r),e(Ao,kue),e(kue,$1r),e(Ao,I1r),e(Ao,Cn),e(Cn,j1r),e(Cn,xue),e(xue,N1r),e(Cn,D1r),e(Cn,Rue),e(Rue,q1r),e(Cn,G1r),e(Cn,Sue),e(Sue,O1r),e(Cn,X1r),e(Ao,z1r),e(Ao,Mn),e(Mn,SF),e(SF,Pue),e(Pue,V1r),e(SF,W1r),e(SF,LO),e(LO,Q1r),e(SF,H1r),e(Mn,U1r),e(Mn,PF),e(PF,$ue),e($ue,J1r),e(PF,Y1r),e(PF,BO),e(BO,K1r),e(PF,Z1r),e(Mn,ebr),e(Mn,$F),e($F,Iue),e(Iue,obr),e($F,rbr),e($F,kO),e(kO,tbr),e($F,abr),e(Mn,nbr),e(Mn,IF),e(IF,jue),e(jue,sbr),e(IF,lbr),e(IF,xO),e(xO,ibr),e(IF,dbr),e(Ao,cbr),e(Ao,Nue),e(Nue,fbr),e(Ao,mbr),g(yw,Ao,null),b(d,I8e,u),b(d,Wc,u),e(Wc,jF),e(jF,Due),g(ww,Due,null),e(Wc,gbr),e(Wc,que),e(que,hbr),b(d,j8e,u),b(d,Lr,u),g(Aw,Lr,null),e(Lr,pbr),e(Lr,Qc),e(Qc,_br),e(Qc,Gue),e(Gue,ubr),e(Qc,bbr),e(Qc,Oue),e(Oue,vbr),e(Qc,Tbr),e(Lr,Fbr),e(Lr,Lw),e(Lw,Cbr),e(Lw,Xue),e(Xue,Mbr),e(Lw,Ebr),e(Lr,ybr),e(Lr,Ft),g(Bw,Ft,null),e(Ft,wbr),e(Ft,zue),e(zue,Abr),e(Ft,Lbr),e(Ft,Hc),e(Hc,Bbr),e(Hc,Vue),e(Vue,kbr),e(Hc,xbr),e(Hc,Wue),e(Wue,Rbr),e(Hc,Sbr),e(Ft,Pbr),e(Ft,Que),e(Que,$br),e(Ft,Ibr),g(kw,Ft,null),e(Lr,jbr),e(Lr,Lo),g(xw,Lo,null),e(Lo,Nbr),e(Lo,Hue),e(Hue,Dbr),e(Lo,qbr),e(Lo,En),e(En,Gbr),e(En,Uue),e(Uue,Obr),e(En,Xbr),e(En,Jue),e(Jue,zbr),e(En,Vbr),e(En,Yue),e(Yue,Wbr),e(En,Qbr),e(Lo,Hbr),e(Lo,fe),e(fe,NF),e(NF,Kue),e(Kue,Ubr),e(NF,Jbr),e(NF,RO),e(RO,Ybr),e(NF,Kbr),e(fe,Zbr),e(fe,DF),e(DF,Zue),e(Zue,e5r),e(DF,o5r),e(DF,SO),e(SO,r5r),e(DF,t5r),e(fe,a5r),e(fe,qF),e(qF,e1e),e(e1e,n5r),e(qF,s5r),e(qF,PO),e(PO,l5r),e(qF,i5r),e(fe,d5r),e(fe,GF),e(GF,o1e),e(o1e,c5r),e(GF,f5r),e(GF,$O),e($O,m5r),e(GF,g5r),e(fe,h5r),e(fe,OF),e(OF,r1e),e(r1e,p5r),e(OF,_5r),e(OF,IO),e(IO,u5r),e(OF,b5r),e(fe,v5r),e(fe,XF),e(XF,t1e),e(t1e,T5r),e(XF,F5r),e(XF,jO),e(jO,C5r),e(XF,M5r),e(fe,E5r),e(fe,zF),e(zF,a1e),e(a1e,y5r),e(zF,w5r),e(zF,NO),e(NO,A5r),e(zF,L5r),e(fe,B5r),e(fe,VF),e(VF,n1e),e(n1e,k5r),e(VF,x5r),e(VF,DO),e(DO,R5r),e(VF,S5r),e(fe,P5r),e(fe,WF),e(WF,s1e),e(s1e,$5r),e(WF,I5r),e(WF,qO),e(qO,j5r),e(WF,N5r),e(fe,D5r),e(fe,QF),e(QF,l1e),e(l1e,q5r),e(QF,G5r),e(QF,GO),e(GO,O5r),e(QF,X5r),e(fe,z5r),e(fe,HF),e(HF,i1e),e(i1e,V5r),e(HF,W5r),e(HF,OO),e(OO,Q5r),e(HF,H5r),e(Lo,U5r),e(Lo,d1e),e(d1e,J5r),e(Lo,Y5r),g(Rw,Lo,null),b(d,N8e,u),b(d,Uc,u),e(Uc,UF),e(UF,c1e),g(Sw,c1e,null),e(Uc,K5r),e(Uc,f1e),e(f1e,Z5r),b(d,D8e,u),b(d,Br,u),g(Pw,Br,null),e(Br,e2r),e(Br,Jc),e(Jc,o2r),e(Jc,m1e),e(m1e,r2r),e(Jc,t2r),e(Jc,g1e),e(g1e,a2r),e(Jc,n2r),e(Br,s2r),e(Br,$w),e($w,l2r),e($w,h1e),e(h1e,i2r),e($w,d2r),e(Br,c2r),e(Br,Ct),g(Iw,Ct,null),e(Ct,f2r),e(Ct,p1e),e(p1e,m2r),e(Ct,g2r),e(Ct,Yc),e(Yc,h2r),e(Yc,_1e),e(_1e,p2r),e(Yc,_2r),e(Yc,u1e),e(u1e,u2r),e(Yc,b2r),e(Ct,v2r),e(Ct,b1e),e(b1e,T2r),e(Ct,F2r),g(jw,Ct,null),e(Br,C2r),e(Br,Bo),g(Nw,Bo,null),e(Bo,M2r),e(Bo,v1e),e(v1e,E2r),e(Bo,y2r),e(Bo,yn),e(yn,w2r),e(yn,T1e),e(T1e,A2r),e(yn,L2r),e(yn,F1e),e(F1e,B2r),e(yn,k2r),e(yn,C1e),e(C1e,x2r),e(yn,R2r),e(Bo,S2r),e(Bo,ve),e(ve,JF),e(JF,M1e),e(M1e,P2r),e(JF,$2r),e(JF,XO),e(XO,I2r),e(JF,j2r),e(ve,N2r),e(ve,YF),e(YF,E1e),e(E1e,D2r),e(YF,q2r),e(YF,zO),e(zO,G2r),e(YF,O2r),e(ve,X2r),e(ve,KF),e(KF,y1e),e(y1e,z2r),e(KF,V2r),e(KF,VO),e(VO,W2r),e(KF,Q2r),e(ve,H2r),e(ve,ZF),e(ZF,w1e),e(w1e,U2r),e(ZF,J2r),e(ZF,WO),e(WO,Y2r),e(ZF,K2r),e(ve,Z2r),e(ve,e9),e(e9,A1e),e(A1e,evr),e(e9,ovr),e(e9,QO),e(QO,rvr),e(e9,tvr),e(ve,avr),e(ve,o9),e(o9,L1e),e(L1e,nvr),e(o9,svr),e(o9,HO),e(HO,lvr),e(o9,ivr),e(ve,dvr),e(ve,r9),e(r9,B1e),e(B1e,cvr),e(r9,fvr),e(r9,UO),e(UO,mvr),e(r9,gvr),e(ve,hvr),e(ve,t9),e(t9,k1e),e(k1e,pvr),e(t9,_vr),e(t9,JO),e(JO,uvr),e(t9,bvr),e(ve,vvr),e(ve,a9),e(a9,x1e),e(x1e,Tvr),e(a9,Fvr),e(a9,YO),e(YO,Cvr),e(a9,Mvr),e(Bo,Evr),e(Bo,R1e),e(R1e,yvr),e(Bo,wvr),g(Dw,Bo,null),b(d,q8e,u),b(d,Kc,u),e(Kc,n9),e(n9,S1e),g(qw,S1e,null),e(Kc,Avr),e(Kc,P1e),e(P1e,Lvr),b(d,G8e,u),b(d,kr,u),g(Gw,kr,null),e(kr,Bvr),e(kr,Zc),e(Zc,kvr),e(Zc,$1e),e($1e,xvr),e(Zc,Rvr),e(Zc,I1e),e(I1e,Svr),e(Zc,Pvr),e(kr,$vr),e(kr,Ow),e(Ow,Ivr),e(Ow,j1e),e(j1e,jvr),e(Ow,Nvr),e(kr,Dvr),e(kr,Mt),g(Xw,Mt,null),e(Mt,qvr),e(Mt,N1e),e(N1e,Gvr),e(Mt,Ovr),e(Mt,ef),e(ef,Xvr),e(ef,D1e),e(D1e,zvr),e(ef,Vvr),e(ef,q1e),e(q1e,Wvr),e(ef,Qvr),e(Mt,Hvr),e(Mt,G1e),e(G1e,Uvr),e(Mt,Jvr),g(zw,Mt,null),e(kr,Yvr),e(kr,ko),g(Vw,ko,null),e(ko,Kvr),e(ko,O1e),e(O1e,Zvr),e(ko,e6r),e(ko,wn),e(wn,o6r),e(wn,X1e),e(X1e,r6r),e(wn,t6r),e(wn,z1e),e(z1e,a6r),e(wn,n6r),e(wn,V1e),e(V1e,s6r),e(wn,l6r),e(ko,i6r),e(ko,Te),e(Te,s9),e(s9,W1e),e(W1e,d6r),e(s9,c6r),e(s9,KO),e(KO,f6r),e(s9,m6r),e(Te,g6r),e(Te,l9),e(l9,Q1e),e(Q1e,h6r),e(l9,p6r),e(l9,ZO),e(ZO,_6r),e(l9,u6r),e(Te,b6r),e(Te,i9),e(i9,H1e),e(H1e,v6r),e(i9,T6r),e(i9,eX),e(eX,F6r),e(i9,C6r),e(Te,M6r),e(Te,d9),e(d9,U1e),e(U1e,E6r),e(d9,y6r),e(d9,oX),e(oX,w6r),e(d9,A6r),e(Te,L6r),e(Te,c9),e(c9,J1e),e(J1e,B6r),e(c9,k6r),e(c9,rX),e(rX,x6r),e(c9,R6r),e(Te,S6r),e(Te,f9),e(f9,Y1e),e(Y1e,P6r),e(f9,$6r),e(f9,tX),e(tX,I6r),e(f9,j6r),e(Te,N6r),e(Te,m9),e(m9,K1e),e(K1e,D6r),e(m9,q6r),e(m9,aX),e(aX,G6r),e(m9,O6r),e(Te,X6r),e(Te,g9),e(g9,Z1e),e(Z1e,z6r),e(g9,V6r),e(g9,nX),e(nX,W6r),e(g9,Q6r),e(Te,H6r),e(Te,h9),e(h9,ebe),e(ebe,U6r),e(h9,J6r),e(h9,sX),e(sX,Y6r),e(h9,K6r),e(ko,Z6r),e(ko,obe),e(obe,eTr),e(ko,oTr),g(Ww,ko,null),b(d,O8e,u),b(d,of,u),e(of,p9),e(p9,rbe),g(Qw,rbe,null),e(of,rTr),e(of,tbe),e(tbe,tTr),b(d,X8e,u),b(d,xr,u),g(Hw,xr,null),e(xr,aTr),e(xr,rf),e(rf,nTr),e(rf,abe),e(abe,sTr),e(rf,lTr),e(rf,nbe),e(nbe,iTr),e(rf,dTr),e(xr,cTr),e(xr,Uw),e(Uw,fTr),e(Uw,sbe),e(sbe,mTr),e(Uw,gTr),e(xr,hTr),e(xr,Et),g(Jw,Et,null),e(Et,pTr),e(Et,lbe),e(lbe,_Tr),e(Et,uTr),e(Et,tf),e(tf,bTr),e(tf,ibe),e(ibe,vTr),e(tf,TTr),e(tf,dbe),e(dbe,FTr),e(tf,CTr),e(Et,MTr),e(Et,cbe),e(cbe,ETr),e(Et,yTr),g(Yw,Et,null),e(xr,wTr),e(xr,xo),g(Kw,xo,null),e(xo,ATr),e(xo,fbe),e(fbe,LTr),e(xo,BTr),e(xo,An),e(An,kTr),e(An,mbe),e(mbe,xTr),e(An,RTr),e(An,gbe),e(gbe,STr),e(An,PTr),e(An,hbe),e(hbe,$Tr),e(An,ITr),e(xo,jTr),e(xo,Fe),e(Fe,_9),e(_9,pbe),e(pbe,NTr),e(_9,DTr),e(_9,lX),e(lX,qTr),e(_9,GTr),e(Fe,OTr),e(Fe,u9),e(u9,_be),e(_be,XTr),e(u9,zTr),e(u9,iX),e(iX,VTr),e(u9,WTr),e(Fe,QTr),e(Fe,b9),e(b9,ube),e(ube,HTr),e(b9,UTr),e(b9,dX),e(dX,JTr),e(b9,YTr),e(Fe,KTr),e(Fe,v9),e(v9,bbe),e(bbe,ZTr),e(v9,e7r),e(v9,cX),e(cX,o7r),e(v9,r7r),e(Fe,t7r),e(Fe,T9),e(T9,vbe),e(vbe,a7r),e(T9,n7r),e(T9,fX),e(fX,s7r),e(T9,l7r),e(Fe,i7r),e(Fe,F9),e(F9,Tbe),e(Tbe,d7r),e(F9,c7r),e(F9,mX),e(mX,f7r),e(F9,m7r),e(Fe,g7r),e(Fe,C9),e(C9,Fbe),e(Fbe,h7r),e(C9,p7r),e(C9,gX),e(gX,_7r),e(C9,u7r),e(Fe,b7r),e(Fe,M9),e(M9,Cbe),e(Cbe,v7r),e(M9,T7r),e(M9,hX),e(hX,F7r),e(M9,C7r),e(Fe,M7r),e(Fe,E9),e(E9,Mbe),e(Mbe,E7r),e(E9,y7r),e(E9,pX),e(pX,w7r),e(E9,A7r),e(xo,L7r),e(xo,Ebe),e(Ebe,B7r),e(xo,k7r),g(Zw,xo,null),b(d,z8e,u),b(d,af,u),e(af,y9),e(y9,ybe),g(eA,ybe,null),e(af,x7r),e(af,wbe),e(wbe,R7r),b(d,V8e,u),b(d,Rr,u),g(oA,Rr,null),e(Rr,S7r),e(Rr,nf),e(nf,P7r),e(nf,Abe),e(Abe,$7r),e(nf,I7r),e(nf,Lbe),e(Lbe,j7r),e(nf,N7r),e(Rr,D7r),e(Rr,rA),e(rA,q7r),e(rA,Bbe),e(Bbe,G7r),e(rA,O7r),e(Rr,X7r),e(Rr,yt),g(tA,yt,null),e(yt,z7r),e(yt,kbe),e(kbe,V7r),e(yt,W7r),e(yt,sf),e(sf,Q7r),e(sf,xbe),e(xbe,H7r),e(sf,U7r),e(sf,Rbe),e(Rbe,J7r),e(sf,Y7r),e(yt,K7r),e(yt,Sbe),e(Sbe,Z7r),e(yt,eFr),g(aA,yt,null),e(Rr,oFr),e(Rr,Ro),g(nA,Ro,null),e(Ro,rFr),e(Ro,Pbe),e(Pbe,tFr),e(Ro,aFr),e(Ro,Ln),e(Ln,nFr),e(Ln,$be),e($be,sFr),e(Ln,lFr),e(Ln,Ibe),e(Ibe,iFr),e(Ln,dFr),e(Ln,jbe),e(jbe,cFr),e(Ln,fFr),e(Ro,mFr),e(Ro,Ce),e(Ce,w9),e(w9,Nbe),e(Nbe,gFr),e(w9,hFr),e(w9,_X),e(_X,pFr),e(w9,_Fr),e(Ce,uFr),e(Ce,A9),e(A9,Dbe),e(Dbe,bFr),e(A9,vFr),e(A9,uX),e(uX,TFr),e(A9,FFr),e(Ce,CFr),e(Ce,L9),e(L9,qbe),e(qbe,MFr),e(L9,EFr),e(L9,bX),e(bX,yFr),e(L9,wFr),e(Ce,AFr),e(Ce,B9),e(B9,Gbe),e(Gbe,LFr),e(B9,BFr),e(B9,vX),e(vX,kFr),e(B9,xFr),e(Ce,RFr),e(Ce,k9),e(k9,Obe),e(Obe,SFr),e(k9,PFr),e(k9,TX),e(TX,$Fr),e(k9,IFr),e(Ce,jFr),e(Ce,x9),e(x9,Xbe),e(Xbe,NFr),e(x9,DFr),e(x9,FX),e(FX,qFr),e(x9,GFr),e(Ce,OFr),e(Ce,R9),e(R9,zbe),e(zbe,XFr),e(R9,zFr),e(R9,CX),e(CX,VFr),e(R9,WFr),e(Ce,QFr),e(Ce,S9),e(S9,Vbe),e(Vbe,HFr),e(S9,UFr),e(S9,MX),e(MX,JFr),e(S9,YFr),e(Ce,KFr),e(Ce,P9),e(P9,Wbe),e(Wbe,ZFr),e(P9,e9r),e(P9,EX),e(EX,o9r),e(P9,r9r),e(Ro,t9r),e(Ro,Qbe),e(Qbe,a9r),e(Ro,n9r),g(sA,Ro,null),b(d,W8e,u),b(d,lf,u),e(lf,$9),e($9,Hbe),g(lA,Hbe,null),e(lf,s9r),e(lf,Ube),e(Ube,l9r),b(d,Q8e,u),b(d,Sr,u),g(iA,Sr,null),e(Sr,i9r),e(Sr,df),e(df,d9r),e(df,Jbe),e(Jbe,c9r),e(df,f9r),e(df,Ybe),e(Ybe,m9r),e(df,g9r),e(Sr,h9r),e(Sr,dA),e(dA,p9r),e(dA,Kbe),e(Kbe,_9r),e(dA,u9r),e(Sr,b9r),e(Sr,wt),g(cA,wt,null),e(wt,v9r),e(wt,Zbe),e(Zbe,T9r),e(wt,F9r),e(wt,cf),e(cf,C9r),e(cf,e5e),e(e5e,M9r),e(cf,E9r),e(cf,o5e),e(o5e,y9r),e(cf,w9r),e(wt,A9r),e(wt,r5e),e(r5e,L9r),e(wt,B9r),g(fA,wt,null),e(Sr,k9r),e(Sr,So),g(mA,So,null),e(So,x9r),e(So,t5e),e(t5e,R9r),e(So,S9r),e(So,Bn),e(Bn,P9r),e(Bn,a5e),e(a5e,$9r),e(Bn,I9r),e(Bn,n5e),e(n5e,j9r),e(Bn,N9r),e(Bn,s5e),e(s5e,D9r),e(Bn,q9r),e(So,G9r),e(So,so),e(so,I9),e(I9,l5e),e(l5e,O9r),e(I9,X9r),e(I9,yX),e(yX,z9r),e(I9,V9r),e(so,W9r),e(so,j9),e(j9,i5e),e(i5e,Q9r),e(j9,H9r),e(j9,wX),e(wX,U9r),e(j9,J9r),e(so,Y9r),e(so,N9),e(N9,d5e),e(d5e,K9r),e(N9,Z9r),e(N9,AX),e(AX,eCr),e(N9,oCr),e(so,rCr),e(so,D9),e(D9,c5e),e(c5e,tCr),e(D9,aCr),e(D9,LX),e(LX,nCr),e(D9,sCr),e(so,lCr),e(so,q9),e(q9,f5e),e(f5e,iCr),e(q9,dCr),e(q9,BX),e(BX,cCr),e(q9,fCr),e(so,mCr),e(so,G9),e(G9,m5e),e(m5e,gCr),e(G9,hCr),e(G9,kX),e(kX,pCr),e(G9,_Cr),e(so,uCr),e(so,O9),e(O9,g5e),e(g5e,bCr),e(O9,vCr),e(O9,xX),e(xX,TCr),e(O9,FCr),e(So,CCr),e(So,h5e),e(h5e,MCr),e(So,ECr),g(gA,So,null),b(d,H8e,u),b(d,ff,u),e(ff,X9),e(X9,p5e),g(hA,p5e,null),e(ff,yCr),e(ff,_5e),e(_5e,wCr),b(d,U8e,u),b(d,Pr,u),g(pA,Pr,null),e(Pr,ACr),e(Pr,mf),e(mf,LCr),e(mf,u5e),e(u5e,BCr),e(mf,kCr),e(mf,b5e),e(b5e,xCr),e(mf,RCr),e(Pr,SCr),e(Pr,_A),e(_A,PCr),e(_A,v5e),e(v5e,$Cr),e(_A,ICr),e(Pr,jCr),e(Pr,At),g(uA,At,null),e(At,NCr),e(At,T5e),e(T5e,DCr),e(At,qCr),e(At,gf),e(gf,GCr),e(gf,F5e),e(F5e,OCr),e(gf,XCr),e(gf,C5e),e(C5e,zCr),e(gf,VCr),e(At,WCr),e(At,M5e),e(M5e,QCr),e(At,HCr),g(bA,At,null),e(Pr,UCr),e(Pr,Po),g(vA,Po,null),e(Po,JCr),e(Po,E5e),e(E5e,YCr),e(Po,KCr),e(Po,kn),e(kn,ZCr),e(kn,y5e),e(y5e,e4r),e(kn,o4r),e(kn,w5e),e(w5e,r4r),e(kn,t4r),e(kn,A5e),e(A5e,a4r),e(kn,n4r),e(Po,s4r),e(Po,lo),e(lo,z9),e(z9,L5e),e(L5e,l4r),e(z9,i4r),e(z9,RX),e(RX,d4r),e(z9,c4r),e(lo,f4r),e(lo,V9),e(V9,B5e),e(B5e,m4r),e(V9,g4r),e(V9,SX),e(SX,h4r),e(V9,p4r),e(lo,_4r),e(lo,W9),e(W9,k5e),e(k5e,u4r),e(W9,b4r),e(W9,PX),e(PX,v4r),e(W9,T4r),e(lo,F4r),e(lo,Q9),e(Q9,x5e),e(x5e,C4r),e(Q9,M4r),e(Q9,$X),e($X,E4r),e(Q9,y4r),e(lo,w4r),e(lo,H9),e(H9,R5e),e(R5e,A4r),e(H9,L4r),e(H9,IX),e(IX,B4r),e(H9,k4r),e(lo,x4r),e(lo,U9),e(U9,S5e),e(S5e,R4r),e(U9,S4r),e(U9,jX),e(jX,P4r),e(U9,$4r),e(lo,I4r),e(lo,J9),e(J9,P5e),e(P5e,j4r),e(J9,N4r),e(J9,NX),e(NX,D4r),e(J9,q4r),e(Po,G4r),e(Po,$5e),e($5e,O4r),e(Po,X4r),g(TA,Po,null),b(d,J8e,u),b(d,hf,u),e(hf,Y9),e(Y9,I5e),g(FA,I5e,null),e(hf,z4r),e(hf,j5e),e(j5e,V4r),b(d,Y8e,u),b(d,$r,u),g(CA,$r,null),e($r,W4r),e($r,pf),e(pf,Q4r),e(pf,N5e),e(N5e,H4r),e(pf,U4r),e(pf,D5e),e(D5e,J4r),e(pf,Y4r),e($r,K4r),e($r,MA),e(MA,Z4r),e(MA,q5e),e(q5e,eMr),e(MA,oMr),e($r,rMr),e($r,Lt),g(EA,Lt,null),e(Lt,tMr),e(Lt,G5e),e(G5e,aMr),e(Lt,nMr),e(Lt,_f),e(_f,sMr),e(_f,O5e),e(O5e,lMr),e(_f,iMr),e(_f,X5e),e(X5e,dMr),e(_f,cMr),e(Lt,fMr),e(Lt,z5e),e(z5e,mMr),e(Lt,gMr),g(yA,Lt,null),e($r,hMr),e($r,$o),g(wA,$o,null),e($o,pMr),e($o,V5e),e(V5e,_Mr),e($o,uMr),e($o,xn),e(xn,bMr),e(xn,W5e),e(W5e,vMr),e(xn,TMr),e(xn,Q5e),e(Q5e,FMr),e(xn,CMr),e(xn,H5e),e(H5e,MMr),e(xn,EMr),e($o,yMr),e($o,U5e),e(U5e,K9),e(K9,J5e),e(J5e,wMr),e(K9,AMr),e(K9,DX),e(DX,LMr),e(K9,BMr),e($o,kMr),e($o,Y5e),e(Y5e,xMr),e($o,RMr),g(AA,$o,null),b(d,K8e,u),b(d,uf,u),e(uf,Z9),e(Z9,K5e),g(LA,K5e,null),e(uf,SMr),e(uf,Z5e),e(Z5e,PMr),b(d,Z8e,u),b(d,Ir,u),g(BA,Ir,null),e(Ir,$Mr),e(Ir,bf),e(bf,IMr),e(bf,e2e),e(e2e,jMr),e(bf,NMr),e(bf,o2e),e(o2e,DMr),e(bf,qMr),e(Ir,GMr),e(Ir,kA),e(kA,OMr),e(kA,r2e),e(r2e,XMr),e(kA,zMr),e(Ir,VMr),e(Ir,Bt),g(xA,Bt,null),e(Bt,WMr),e(Bt,t2e),e(t2e,QMr),e(Bt,HMr),e(Bt,vf),e(vf,UMr),e(vf,a2e),e(a2e,JMr),e(vf,YMr),e(vf,n2e),e(n2e,KMr),e(vf,ZMr),e(Bt,eEr),e(Bt,s2e),e(s2e,oEr),e(Bt,rEr),g(RA,Bt,null),e(Ir,tEr),e(Ir,Io),g(SA,Io,null),e(Io,aEr),e(Io,l2e),e(l2e,nEr),e(Io,sEr),e(Io,Rn),e(Rn,lEr),e(Rn,i2e),e(i2e,iEr),e(Rn,dEr),e(Rn,d2e),e(d2e,cEr),e(Rn,fEr),e(Rn,c2e),e(c2e,mEr),e(Rn,gEr),e(Io,hEr),e(Io,PA),e(PA,eC),e(eC,f2e),e(f2e,pEr),e(eC,_Er),e(eC,qX),e(qX,uEr),e(eC,bEr),e(PA,vEr),e(PA,oC),e(oC,m2e),e(m2e,TEr),e(oC,FEr),e(oC,GX),e(GX,CEr),e(oC,MEr),e(Io,EEr),e(Io,g2e),e(g2e,yEr),e(Io,wEr),g($A,Io,null),b(d,eBe,u),b(d,Tf,u),e(Tf,rC),e(rC,h2e),g(IA,h2e,null),e(Tf,AEr),e(Tf,p2e),e(p2e,LEr),b(d,oBe,u),b(d,jr,u),g(jA,jr,null),e(jr,BEr),e(jr,Ff),e(Ff,kEr),e(Ff,_2e),e(_2e,xEr),e(Ff,REr),e(Ff,u2e),e(u2e,SEr),e(Ff,PEr),e(jr,$Er),e(jr,NA),e(NA,IEr),e(NA,b2e),e(b2e,jEr),e(NA,NEr),e(jr,DEr),e(jr,kt),g(DA,kt,null),e(kt,qEr),e(kt,v2e),e(v2e,GEr),e(kt,OEr),e(kt,Cf),e(Cf,XEr),e(Cf,T2e),e(T2e,zEr),e(Cf,VEr),e(Cf,F2e),e(F2e,WEr),e(Cf,QEr),e(kt,HEr),e(kt,C2e),e(C2e,UEr),e(kt,JEr),g(qA,kt,null),e(jr,YEr),e(jr,jo),g(GA,jo,null),e(jo,KEr),e(jo,M2e),e(M2e,ZEr),e(jo,e3r),e(jo,Sn),e(Sn,o3r),e(Sn,E2e),e(E2e,r3r),e(Sn,t3r),e(Sn,y2e),e(y2e,a3r),e(Sn,n3r),e(Sn,w2e),e(w2e,s3r),e(Sn,l3r),e(jo,i3r),e(jo,A2e),e(A2e,tC),e(tC,L2e),e(L2e,d3r),e(tC,c3r),e(tC,OX),e(OX,f3r),e(tC,m3r),e(jo,g3r),e(jo,B2e),e(B2e,h3r),e(jo,p3r),g(OA,jo,null),rBe=!0},p(d,[u]){const XA={};u&2&&(XA.$$scope={dirty:u,ctx:d}),Bf.$set(XA);const k2e={};u&2&&(k2e.$$scope={dirty:u,ctx:d}),ih.$set(k2e);const x2e={};u&2&&(x2e.$$scope={dirty:u,ctx:d}),vh.$set(x2e)},i(d){rBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(n4.$$.fragment,d),h(s4.$$.fragment,d),h(Bf.$$.fragment,d),h(l4.$$.fragment,d),h(i4.$$.fragment,d),h(f4.$$.fragment,d),h(m4.$$.fragment,d),h(g4.$$.fragment,d),h(h4.$$.fragment,d),h(p4.$$.fragment,d),h(b4.$$.fragment,d),h(v4.$$.fragment,d),h(T4.$$.fragment,d),h(F4.$$.fragment,d),h(C4.$$.fragment,d),h(y4.$$.fragment,d),h(ih.$$.fragment,d),h(w4.$$.fragment,d),h(A4.$$.fragment,d),h(L4.$$.fragment,d),h(B4.$$.fragment,d),h(R4.$$.fragment,d),h(vh.$$.fragment,d),h(S4.$$.fragment,d),h(P4.$$.fragment,d),h($4.$$.fragment,d),h(I4.$$.fragment,d),h(N4.$$.fragment,d),h(D4.$$.fragment,d),h(q4.$$.fragment,d),h(G4.$$.fragment,d),h(O4.$$.fragment,d),h(X4.$$.fragment,d),h(V4.$$.fragment,d),h(W4.$$.fragment,d),h(Q4.$$.fragment,d),h(H4.$$.fragment,d),h(U4.$$.fragment,d),h(J4.$$.fragment,d),h(K4.$$.fragment,d),h(Z4.$$.fragment,d),h(eM.$$.fragment,d),h(oM.$$.fragment,d),h(rM.$$.fragment,d),h(tM.$$.fragment,d),h(nM.$$.fragment,d),h(sM.$$.fragment,d),h(lM.$$.fragment,d),h(iM.$$.fragment,d),h(dM.$$.fragment,d),h(cM.$$.fragment,d),h(mM.$$.fragment,d),h(gM.$$.fragment,d),h(hM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(uM.$$.fragment,d),h(vM.$$.fragment,d),h(TM.$$.fragment,d),h(FM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(EM.$$.fragment,d),h(wM.$$.fragment,d),h(AM.$$.fragment,d),h(LM.$$.fragment,d),h(BM.$$.fragment,d),h(kM.$$.fragment,d),h(xM.$$.fragment,d),h(SM.$$.fragment,d),h(PM.$$.fragment,d),h($M.$$.fragment,d),h(IM.$$.fragment,d),h(jM.$$.fragment,d),h(NM.$$.fragment,d),h(qM.$$.fragment,d),h(GM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(VM.$$.fragment,d),h(QM.$$.fragment,d),h(HM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(KM.$$.fragment,d),h(eE.$$.fragment,d),h(oE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(nE.$$.fragment,d),h(lE.$$.fragment,d),h(iE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(hE.$$.fragment,d),h(pE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(bE.$$.fragment,d),h(vE.$$.fragment,d),h(FE.$$.fragment,d),h(CE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(wE.$$.fragment,d),h(LE.$$.fragment,d),h(BE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(RE.$$.fragment,d),h(SE.$$.fragment,d),h($E.$$.fragment,d),h(IE.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(OE.$$.fragment,d),h(XE.$$.fragment,d),h(zE.$$.fragment,d),h(WE.$$.fragment,d),h(QE.$$.fragment,d),h(HE.$$.fragment,d),h(JE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(t3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(c3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(u3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(E3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(W3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(U3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(Z3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(sy.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(Ay.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(xy.$$.fragment,d),h(Ry.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(sw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(nA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(mA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(pA.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),rBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(n4.$$.fragment,d),p(s4.$$.fragment,d),p(Bf.$$.fragment,d),p(l4.$$.fragment,d),p(i4.$$.fragment,d),p(f4.$$.fragment,d),p(m4.$$.fragment,d),p(g4.$$.fragment,d),p(h4.$$.fragment,d),p(p4.$$.fragment,d),p(b4.$$.fragment,d),p(v4.$$.fragment,d),p(T4.$$.fragment,d),p(F4.$$.fragment,d),p(C4.$$.fragment,d),p(y4.$$.fragment,d),p(ih.$$.fragment,d),p(w4.$$.fragment,d),p(A4.$$.fragment,d),p(L4.$$.fragment,d),p(B4.$$.fragment,d),p(R4.$$.fragment,d),p(vh.$$.fragment,d),p(S4.$$.fragment,d),p(P4.$$.fragment,d),p($4.$$.fragment,d),p(I4.$$.fragment,d),p(N4.$$.fragment,d),p(D4.$$.fragment,d),p(q4.$$.fragment,d),p(G4.$$.fragment,d),p(O4.$$.fragment,d),p(X4.$$.fragment,d),p(V4.$$.fragment,d),p(W4.$$.fragment,d),p(Q4.$$.fragment,d),p(H4.$$.fragment,d),p(U4.$$.fragment,d),p(J4.$$.fragment,d),p(K4.$$.fragment,d),p(Z4.$$.fragment,d),p(eM.$$.fragment,d),p(oM.$$.fragment,d),p(rM.$$.fragment,d),p(tM.$$.fragment,d),p(nM.$$.fragment,d),p(sM.$$.fragment,d),p(lM.$$.fragment,d),p(iM.$$.fragment,d),p(dM.$$.fragment,d),p(cM.$$.fragment,d),p(mM.$$.fragment,d),p(gM.$$.fragment,d),p(hM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(uM.$$.fragment,d),p(vM.$$.fragment,d),p(TM.$$.fragment,d),p(FM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(EM.$$.fragment,d),p(wM.$$.fragment,d),p(AM.$$.fragment,d),p(LM.$$.fragment,d),p(BM.$$.fragment,d),p(kM.$$.fragment,d),p(xM.$$.fragment,d),p(SM.$$.fragment,d),p(PM.$$.fragment,d),p($M.$$.fragment,d),p(IM.$$.fragment,d),p(jM.$$.fragment,d),p(NM.$$.fragment,d),p(qM.$$.fragment,d),p(GM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(VM.$$.fragment,d),p(QM.$$.fragment,d),p(HM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(KM.$$.fragment,d),p(eE.$$.fragment,d),p(oE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(nE.$$.fragment,d),p(lE.$$.fragment,d),p(iE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(hE.$$.fragment,d),p(pE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(bE.$$.fragment,d),p(vE.$$.fragment,d),p(FE.$$.fragment,d),p(CE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(wE.$$.fragment,d),p(LE.$$.fragment,d),p(BE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(RE.$$.fragment,d),p(SE.$$.fragment,d),p($E.$$.fragment,d),p(IE.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(OE.$$.fragment,d),p(XE.$$.fragment,d),p(zE.$$.fragment,d),p(WE.$$.fragment,d),p(QE.$$.fragment,d),p(HE.$$.fragment,d),p(JE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(t3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(c3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(u3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(E3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(W3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(U3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(Z3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(sy.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(gy.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(Ty.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(Ay.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(xy.$$.fragment,d),p(Ry.$$.fragment,d),p(Py.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(Gy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Wy.$$.fragment,d),p(Hy.$$.fragment,d),p(Uy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ow.$$.fragment,d),p(rw.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(sw.$$.fragment,d),p(iw.$$.fragment,d),p(dw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(pw.$$.fragment,d),p(_w.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Cw.$$.fragment,d),p(Mw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Aw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(xw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(eA.$$.fragment,d),p(oA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(nA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(mA.$$.fragment,d),p(gA.$$.fragment,d),p(hA.$$.fragment,d),p(pA.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(vA.$$.fragment,d),p(TA.$$.fragment,d),p(FA.$$.fragment,d),p(CA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(wA.$$.fragment,d),p(AA.$$.fragment,d),p(LA.$$.fragment,d),p(BA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(SA.$$.fragment,d),p($A.$$.fragment,d),p(IA.$$.fragment,d),p(jA.$$.fragment,d),p(DA.$$.fragment,d),p(qA.$$.fragment,d),p(GA.$$.fragment,d),p(OA.$$.fragment,d),rBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(Ef),d&&t(sa),d&&t(ye),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(tLe),d&&t(Si),_(n4),d&&t(aLe),d&&t(Nn),d&&t(nLe),_(s4,d),d&&t(sLe),d&&t(z0),d&&t(lLe),_(Bf,d),d&&t(iLe),d&&t(Pi),_(l4),d&&t(dLe),d&&t(Go),_(i4),_(f4),_(m4),_(g4),d&&t(cLe),d&&t(Ii),_(h4),d&&t(fLe),d&&t(Oo),_(p4),_(b4),_(v4),_(T4),d&&t(mLe),d&&t(ji),_(F4),d&&t(gLe),d&&t(Xo),_(C4),_(y4),_(ih),_(w4),_(A4),d&&t(hLe),d&&t(Ni),_(L4),d&&t(pLe),d&&t(zo),_(B4),_(R4),_(vh),_(S4),_(P4),d&&t(_Le),d&&t(qi),_($4),d&&t(uLe),d&&t(Vo),_(I4),_(N4),_(D4),_(q4),_(G4),d&&t(bLe),d&&t(Xi),_(O4),d&&t(vLe),d&&t(Wo),_(X4),_(V4),_(W4),_(Q4),_(H4),d&&t(TLe),d&&t(Wi),_(U4),d&&t(FLe),d&&t(Qo),_(J4),_(K4),_(Z4),_(eM),_(oM),d&&t(CLe),d&&t(Ui),_(rM),d&&t(MLe),d&&t(Ho),_(tM),_(nM),_(sM),_(lM),_(iM),d&&t(ELe),d&&t(Ki),_(dM),d&&t(yLe),d&&t(Uo),_(cM),_(mM),_(gM),_(hM),_(pM),d&&t(wLe),d&&t(od),_(_M),d&&t(ALe),d&&t(Jo),_(uM),_(vM),_(TM),_(FM),_(CM),d&&t(LLe),d&&t(ad),_(MM),d&&t(BLe),d&&t(Yo),_(EM),_(wM),_(AM),_(LM),_(BM),d&&t(kLe),d&&t(ld),_(kM),d&&t(xLe),d&&t(Ko),_(xM),_(SM),_(PM),_($M),_(IM),d&&t(RLe),d&&t(cd),_(jM),d&&t(SLe),d&&t(Zo),_(NM),_(qM),_(GM),_(OM),_(XM),d&&t(PLe),d&&t(gd),_(zM),d&&t($Le),d&&t(er),_(VM),_(QM),_(HM),_(UM),_(JM),d&&t(ILe),d&&t(_d),_(YM),d&&t(jLe),d&&t(or),_(KM),_(eE),_(oE),_(rE),_(tE),d&&t(NLe),d&&t(vd),_(aE),d&&t(DLe),d&&t(rr),_(nE),_(lE),_(iE),_(dE),_(cE),d&&t(qLe),d&&t(Cd),_(fE),d&&t(GLe),d&&t(tr),_(mE),_(hE),_(pE),_(_E),_(uE),d&&t(OLe),d&&t(yd),_(bE),d&&t(XLe),d&&t(ar),_(vE),_(FE),_(CE),_(ME),_(EE),d&&t(zLe),d&&t(Ld),_(yE),d&&t(VLe),d&&t(nr),_(wE),_(LE),_(BE),_(kE),_(xE),d&&t(WLe),d&&t(Rd),_(RE),d&&t(QLe),d&&t(sr),_(SE),_($E),_(IE),_(jE),_(NE),d&&t(HLe),d&&t($d),_(DE),d&&t(ULe),d&&t(lr),_(qE),_(OE),_(XE),_(zE),_(WE),d&&t(JLe),d&&t(Nd),_(QE),d&&t(YLe),d&&t(ir),_(HE),_(JE),_(YE),_(KE),_(ZE),d&&t(KLe),d&&t(Od),_(e3),d&&t(ZLe),d&&t(dr),_(o3),_(t3),_(a3),_(n3),_(s3),d&&t(e8e),d&&t(Wd),_(l3),d&&t(o8e),d&&t(cr),_(i3),_(c3),_(f3),_(m3),_(g3),d&&t(r8e),d&&t(Ud),_(h3),d&&t(t8e),d&&t(fr),_(p3),_(u3),_(b3),_(v3),_(T3),d&&t(a8e),d&&t(Kd),_(F3),d&&t(n8e),d&&t(mr),_(C3),_(E3),_(y3),_(w3),_(L3),d&&t(s8e),d&&t(oc),_(B3),d&&t(l8e),d&&t(gr),_(k3),_(R3),_(S3),_(P3),_($3),d&&t(i8e),d&&t(ac),_(I3),d&&t(d8e),d&&t(hr),_(j3),_(D3),_(q3),_(G3),_(O3),d&&t(c8e),d&&t(lc),_(X3),d&&t(f8e),d&&t(pr),_(z3),_(W3),_(Q3),_(H3),_(U3),d&&t(m8e),d&&t(cc),_(J3),d&&t(g8e),d&&t(_r),_(Y3),_(Z3),_(ey),_(oy),_(ry),d&&t(h8e),d&&t(gc),_(ty),d&&t(p8e),d&&t(ur),_(ay),_(sy),_(ly),_(iy),_(dy),d&&t(_8e),d&&t(_c),_(cy),d&&t(u8e),d&&t(br),_(fy),_(gy),_(hy),_(py),_(_y),d&&t(b8e),d&&t(vc),_(uy),d&&t(v8e),d&&t(vr),_(by),_(Ty),_(Fy),_(Cy),_(My),d&&t(T8e),d&&t(Cc),_(Ey),d&&t(F8e),d&&t(Tr),_(yy),_(Ay),_(Ly),_(By),_(ky),d&&t(C8e),d&&t(yc),_(xy),d&&t(M8e),d&&t(Fr),_(Ry),_(Py),_($y),_(Iy),_(jy),d&&t(E8e),d&&t(Lc),_(Ny),d&&t(y8e),d&&t(Cr),_(Dy),_(Gy),_(Oy),_(Xy),_(zy),d&&t(w8e),d&&t(xc),_(Vy),d&&t(A8e),d&&t(Mr),_(Wy),_(Hy),_(Uy),_(Jy),_(Yy),d&&t(L8e),d&&t(Pc),_(Ky),d&&t(B8e),d&&t(Er),_(Zy),_(ow),_(rw),_(tw),_(aw),d&&t(k8e),d&&t(jc),_(nw),d&&t(x8e),d&&t(yr),_(sw),_(iw),_(dw),_(cw),_(fw),d&&t(R8e),d&&t(qc),_(mw),d&&t(S8e),d&&t(wr),_(gw),_(pw),_(_w),_(uw),_(bw),d&&t(P8e),d&&t(Xc),_(vw),d&&t($8e),d&&t(Ar),_(Tw),_(Cw),_(Mw),_(Ew),_(yw),d&&t(I8e),d&&t(Wc),_(ww),d&&t(j8e),d&&t(Lr),_(Aw),_(Bw),_(kw),_(xw),_(Rw),d&&t(N8e),d&&t(Uc),_(Sw),d&&t(D8e),d&&t(Br),_(Pw),_(Iw),_(jw),_(Nw),_(Dw),d&&t(q8e),d&&t(Kc),_(qw),d&&t(G8e),d&&t(kr),_(Gw),_(Xw),_(zw),_(Vw),_(Ww),d&&t(O8e),d&&t(of),_(Qw),d&&t(X8e),d&&t(xr),_(Hw),_(Jw),_(Yw),_(Kw),_(Zw),d&&t(z8e),d&&t(af),_(eA),d&&t(V8e),d&&t(Rr),_(oA),_(tA),_(aA),_(nA),_(sA),d&&t(W8e),d&&t(lf),_(lA),d&&t(Q8e),d&&t(Sr),_(iA),_(cA),_(fA),_(mA),_(gA),d&&t(H8e),d&&t(ff),_(hA),d&&t(U8e),d&&t(Pr),_(pA),_(uA),_(bA),_(vA),_(TA),d&&t(J8e),d&&t(hf),_(FA),d&&t(Y8e),d&&t($r),_(CA),_(EA),_(yA),_(wA),_(AA),d&&t(K8e),d&&t(uf),_(LA),d&&t(Z8e),d&&t(Ir),_(BA),_(xA),_(RA),_(SA),_($A),d&&t(eBe),d&&t(Tf),_(IA),d&&t(oBe),d&&t(jr),_(jA),_(DA),_(qA),_(GA),_(OA)}}}const y_t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function w_t(yi,J,Ae){let{fw:ie}=J;return yi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class S_t extends u_t{constructor(J){super();b_t(this,J,w_t,E_t,v_t,{fw:0})}}export{S_t as default,y_t as metadata};
