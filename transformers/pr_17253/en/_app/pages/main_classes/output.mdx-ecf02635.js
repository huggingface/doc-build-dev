import{S as C1,i as z1,s as E1,e as n,k as d,w as h,t as p,M as N1,c as s,d as t,m as i,a,x as f,h as c,b as r,F as o,g as l,y as m,q as _,o as g,B as v,v as P1}from"../../chunks/vendor-6b77c823.js";import{T as B1}from"../../chunks/Tip-39098574.js";import{D as y}from"../../chunks/Docstring-1088f2fb.js";import{C as A1}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as T}from"../../chunks/IconCopyLink-7a11ce68.js";function L1(Fu){let x,Bt,$,A,K,q,nn,X;return{c(){x=n("p"),Bt=p("You can\u2019t unpack a "),$=n("code"),A=p("ModelOutput"),K=p(" directly. Use the "),q=n("a"),nn=p("to_tuple()"),X=p(` method to convert it to a tuple
before.`),this.h()},l(Z){x=s(Z,"P",{});var M=a(x);Bt=c(M,"You can\u2019t unpack a "),$=s(M,"CODE",{});var C=a($);A=c(C,"ModelOutput"),C.forEach(t),K=c(M," directly. Use the "),q=s(M,"A",{href:!0});var xa=a(q);nn=c(xa,"to_tuple()"),xa.forEach(t),X=c(M,` method to convert it to a tuple
before.`),M.forEach(t),this.h()},h(){r(q,"href","/docs/transformers/pr_17253/en/main_classes/output#transformers.utils.ModelOutput.to_tuple")},m(Z,M){l(Z,x,M),o(x,Bt),o(x,$),o($,A),o(x,K),o(x,q),o(q,nn),o(x,X)},d(Z){Z&&t(x)}}}function j1(Fu){let x,Bt,$,A,K,q,nn,X,Z,M,C,xa,$a,Lh,jh,Mu,qa,Wh,ku,sn,Au,w,Dh,Ia,Hh,Qh,Oa,Ih,Vh,Va,Rh,Uh,Ra,Yh,Jh,Ua,Gh,Kh,Ya,Xh,Zh,Ja,ef,tf,Ga,of,nf,Ka,sf,af,Xa,rf,df,Za,uf,lf,er,pf,cf,Cu,O,hf,tr,ff,mf,or,_f,gf,nr,vf,yf,sr,Tf,wf,zu,S,bf,ar,xf,$f,rr,qf,Of,dr,Sf,Ff,ir,Mf,kf,Eu,an,Nu,Lt,Af,ur,Cf,zf,Pu,F,Ef,lr,Nf,Pf,pr,Bf,Lf,cr,jf,Wf,hr,Df,Hf,Bu,Sa,Qf,Lu,ee,jt,fr,rn,If,mr,Vf,ju,k,dn,Rf,te,Uf,_r,Yf,Jf,gr,Gf,Kf,Xf,Wt,Zf,Dt,un,em,ln,tm,vr,om,nm,Wu,oe,Ht,yr,pn,sm,Tr,am,Du,ne,cn,rm,wr,dm,Hu,se,Qt,br,hn,im,xr,um,Qu,ae,fn,lm,$r,pm,Iu,re,It,qr,mn,cm,Or,hm,Vu,de,_n,fm,Sr,mm,Ru,ie,Vt,Fr,gn,_m,Mr,gm,Uu,ue,vn,vm,kr,ym,Yu,le,Rt,Ar,yn,Tm,Cr,wm,Ju,pe,Tn,bm,zr,xm,Gu,ce,Ut,Er,wn,$m,Nr,qm,Ku,he,bn,Om,Pr,Sm,Xu,fe,Yt,Br,xn,Fm,Lr,Mm,Zu,me,$n,km,jr,Am,el,_e,Jt,Wr,qn,Cm,Dr,zm,tl,ge,On,Em,Hr,Nm,ol,ve,Gt,Qr,Sn,Pm,Ir,Bm,nl,ye,Fn,Lm,Vr,jm,sl,Te,Kt,Rr,Mn,Wm,Ur,Dm,al,we,kn,Hm,Yr,Qm,rl,be,Xt,Jr,An,Im,Gr,Vm,dl,xe,Cn,Rm,Kr,Um,il,$e,Zt,Xr,zn,Ym,Zr,Jm,ul,qe,En,Gm,ed,Km,ll,Oe,eo,td,Nn,Xm,od,Zm,pl,Se,Pn,e_,nd,t_,cl,Fe,to,sd,Bn,o_,ad,n_,hl,Me,Ln,s_,rd,a_,fl,ke,oo,dd,jn,r_,id,d_,ml,Ae,Wn,i_,ud,u_,_l,Ce,no,ld,Dn,l_,pd,p_,gl,ze,Hn,c_,cd,h_,vl,Ee,so,hd,Qn,f_,fd,m_,yl,Ne,In,__,md,g_,Tl,Pe,ao,_d,Vn,v_,gd,y_,wl,Be,Rn,T_,vd,w_,bl,Le,ro,yd,Un,b_,Td,x_,xl,je,Yn,$_,wd,q_,$l,We,io,bd,Jn,O_,xd,S_,ql,De,Gn,F_,$d,M_,Ol,He,uo,qd,Kn,k_,Od,A_,Sl,Qe,Xn,C_,Sd,z_,Fl,Ie,lo,Fd,Zn,E_,Md,N_,Ml,Ve,es,P_,kd,B_,kl,Re,po,Ad,ts,L_,Cd,j_,Al,Ue,os,W_,zd,D_,Cl,Ye,co,Ed,ns,H_,Nd,Q_,zl,Je,ss,I_,Pd,V_,El,Ge,ho,Bd,as,R_,Ld,U_,Nl,Ke,rs,Y_,jd,J_,Pl,Xe,fo,Wd,ds,G_,Dd,K_,Bl,Ze,is,X_,Hd,Z_,Ll,et,mo,Qd,us,eg,Id,tg,jl,tt,ls,og,Vd,ng,Wl,ot,_o,Rd,ps,sg,Ud,ag,Dl,nt,cs,rg,Yd,dg,Hl,st,go,Jd,hs,ig,Gd,ug,Ql,at,fs,lg,Kd,pg,Il,rt,vo,Xd,ms,cg,Zd,hg,Vl,dt,_s,fg,ei,mg,Rl,it,yo,ti,gs,_g,oi,gg,Ul,ut,vs,vg,ni,yg,Yl,lt,To,si,ys,Tg,ai,wg,Jl,pt,Ts,bg,ri,xg,Gl,ct,wo,di,ws,$g,ii,qg,Kl,ht,bs,Og,ui,Sg,Xl,ft,bo,li,xs,Fg,pi,Mg,Zl,mt,$s,kg,ci,Ag,ep,_t,xo,hi,qs,Cg,fi,zg,tp,gt,Os,Eg,mi,Ng,op,vt,$o,_i,Ss,Pg,gi,Bg,np,yt,Fs,Lg,vi,jg,sp,Tt,qo,yi,Ms,Wg,Ti,Dg,ap,wt,ks,Hg,wi,Qg,rp,bt,Oo,bi,As,Ig,xi,Vg,dp,z,Cs,Rg,$i,Ug,Yg,So,zs,Jg,qi,Gg,ip,xt,Fo,Oi,Es,Kg,Si,Xg,up,E,Ns,Zg,Fi,ev,tv,Mo,Ps,ov,Mi,nv,lp,$t,ko,ki,Bs,sv,Ai,av,pp,N,Ls,rv,Ci,dv,iv,Ao,js,uv,zi,lv,cp,qt,Co,Ei,Ws,pv,Ni,cv,hp,P,Ds,hv,Pi,fv,mv,zo,Hs,_v,Bi,gv,fp,Ot,Eo,Li,Qs,vv,ji,yv,mp,B,Is,Tv,Wi,wv,bv,No,Vs,xv,Di,$v,_p,St,Po,Hi,Rs,qv,Qi,Ov,gp,L,Us,Sv,Ii,Fv,Mv,Bo,Ys,kv,Vi,Av,vp,Ft,Lo,Ri,Js,Cv,Ui,zv,yp,j,Gs,Ev,Yi,Nv,Pv,jo,Ks,Bv,Ji,Lv,Tp,Mt,Wo,Gi,Xs,jv,Ki,Wv,wp,W,Zs,Dv,Xi,Hv,Qv,Do,ea,Iv,Zi,Vv,bp,kt,Ho,eu,ta,Rv,tu,Uv,xp,D,oa,Yv,ou,Jv,Gv,Qo,na,Kv,nu,Xv,$p,At,Io,su,sa,Zv,au,ey,qp,H,aa,ty,ru,oy,ny,Vo,ra,sy,du,ay,Op,Ct,Ro,iu,da,ry,uu,dy,Sp,Q,ia,iy,lu,uy,ly,Uo,ua,py,pu,cy,Fp,zt,Yo,cu,la,hy,hu,fy,Mp,I,pa,my,fu,_y,gy,Jo,ca,vy,mu,yy,kp,Et,Go,_u,ha,Ty,gu,wy,Ap,V,fa,by,vu,xy,$y,Ko,ma,qy,yu,Oy,Cp,Nt,Xo,Tu,_a,Sy,wu,Fy,zp,R,ga,My,bu,ky,Ay,Zo,va,Cy,xu,zy,Ep,Pt,en,$u,ya,Ey,qu,Ny,Np,U,Ta,Py,Ou,By,Ly,tn,wa,jy,Su,Wy,Pp;return q=new T({}),sn=new A1({props:{code:`from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertForSequenceClassification
<span class="hljs-keyword">import</span> torch

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
outputs = model(**inputs, labels=labels)`}}),an=new A1({props:{code:"outputs[:2]",highlighted:'outputs[:<span class="hljs-number">2</span>]'}}),rn=new T({}),dn=new y({props:{name:"class transformers.utils.ModelOutput",anchor:"transformers.utils.ModelOutput",parameters:"",source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/utils/generic.py#L147"}}),Wt=new B1({props:{warning:!0,$$slots:{default:[L1]},$$scope:{ctx:Fu}}}),un=new y({props:{name:"to_tuple",anchor:"transformers.utils.ModelOutput.to_tuple",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/utils/generic.py#L236"}}),pn=new T({}),cn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutput",anchor:"transformers.modeling_outputs.BaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L24"}}),hn=new T({}),fn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPooling",anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L69"}}),mn=new T({}),_n=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L162"}}),gn=new T({}),vn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L195"}}),yn=new T({}),Tn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPast",anchor:"transformers.modeling_outputs.BaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L123"}}),wn=new T({}),bn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L244"}}),xn=new T({}),$n=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L290"}}),qn=new T({}),On=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutput",anchor:"transformers.modeling_outputs.CausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L351"}}),Sn=new T({}),Fn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L416"}}),Mn=new T({}),kn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithPast",anchor:"transformers.modeling_outputs.CausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L380"}}),An=new T({}),Cn=new y({props:{name:"class transformers.modeling_outputs.MaskedLMOutput",anchor:"transformers.modeling_outputs.MaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MaskedLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L496"}}),zn=new T({}),En=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqLMOutput",anchor:"transformers.modeling_outputs.Seq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L525"}}),Nn=new T({}),Pn=new y({props:{name:"class transformers.modeling_outputs.NextSentencePredictorOutput",anchor:"transformers.modeling_outputs.NextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sequence prediction (classification) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L585"}}),Bn=new T({}),Ln=new y({props:{name:"class transformers.modeling_outputs.SequenceClassifierOutput",anchor:"transformers.modeling_outputs.SequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L615"}}),jn=new T({}),Wn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L644"}}),Dn=new T({}),Hn=new y({props:{name:"class transformers.modeling_outputs.MultipleChoiceModelOutput",anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L704"}}),Qn=new T({}),In=new y({props:{name:"class transformers.modeling_outputs.TokenClassifierOutput",anchor:"transformers.modeling_outputs.TokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.TokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L735"}}),Vn=new T({}),Rn=new y({props:{name:"class transformers.modeling_outputs.QuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L764"}}),Un=new T({}),Yn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_outputs.py#L796"}}),Jn=new T({}),Gn=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutput",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L24"}}),Kn=new T({}),Xn=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L50"}}),Zn=new T({}),es=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L84"}}),ts=new T({}),os=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L132"}}),ns=new T({}),ss=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L201"}}),as=new T({}),rs=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L244"}}),ds=new T({}),is=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutput",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L304"}}),us=new T({}),ls=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L369"}}),ps=new T({}),cs=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L333"}}),hs=new T({}),fs=new y({props:{name:"class transformers.modeling_tf_outputs.TFMaskedLMOutput",anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L412"}}),ms=new T({}),_s=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L441"}}),gs=new T({}),vs=new y({props:{name:"class transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sentence prediction loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L500"}}),ys=new T({}),Ts=new y({props:{name:"class transformers.modeling_tf_outputs.TFSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L530"}}),ws=new T({}),bs=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L559"}}),xs=new T({}),$s=new y({props:{name:"class transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L611"}}),qs=new T({}),Os=new y({props:{name:"class transformers.modeling_tf_outputs.TFTokenClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L642"}}),Ss=new T({}),Fs=new y({props:{name:"class transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L671"}}),Ms=new T({}),ks=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_tf_outputs.py#L703"}}),As=new T({}),Cs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L23"}}),zs=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Es=new T({}),Ns=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, jnp.ndarray]</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L49"}}),Ps=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Bs=new T({}),Ls=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"pooler_output",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L79"}}),js=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Ws=new T({}),Ds=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L159"}}),Hs=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Qs=new T({}),Is=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L205"}}),Vs=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Rs=new T({}),Us=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key, value
states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.
Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L266"}}),Ys=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Js=new T({}),Gs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMaskedLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L307"}}),Ks=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),Xs=new T({}),Zs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L336"}}),ea=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),ta=new T({}),oa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L393"}}),na=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),sa=new T({}),aa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L420"}}),ra=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),da=new T({}),ia=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L446"}}),ua=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),la=new T({}),pa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L503"}}),ca=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),ha=new T({}),fa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L531"}}),ma=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),_a=new T({}),ga=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L557"}}),va=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),ya=new T({}),Ta=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/transformers/modeling_flax_outputs.py#L586"}}),wa=new y({props:{name:"replace",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17253/src/flax/struct.py#L120"}}),{c(){x=n("meta"),Bt=d(),$=n("h1"),A=n("a"),K=n("span"),h(q.$$.fragment),nn=d(),X=n("span"),Z=p("Model outputs"),M=d(),C=n("p"),xa=p("All models have outputs that are instances of subclasses of "),$a=n("a"),Lh=p("ModelOutput"),jh=p(`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Mu=d(),qa=n("p"),Wh=p("Let\u2019s see of this looks on an example:"),ku=d(),h(sn.$$.fragment),Au=d(),w=n("p"),Dh=p("The "),Ia=n("code"),Hh=p("outputs"),Qh=p(" object is a "),Oa=n("a"),Ih=p("SequenceClassifierOutput"),Vh=p(`, as we can see in the
documentation of that class below, it means it has an optional `),Va=n("code"),Rh=p("loss"),Uh=p(", a "),Ra=n("code"),Yh=p("logits"),Jh=p(" an optional "),Ua=n("code"),Gh=p("hidden_states"),Kh=p(` and
an optional `),Ya=n("code"),Xh=p("attentions"),Zh=p(" attribute. Here we have the "),Ja=n("code"),ef=p("loss"),tf=p(" since we passed along "),Ga=n("code"),of=p("labels"),nf=p(`, but we don\u2019t have
`),Ka=n("code"),sf=p("hidden_states"),af=p(" and "),Xa=n("code"),rf=p("attentions"),df=p(" because we didn\u2019t pass "),Za=n("code"),uf=p("output_hidden_states=True"),lf=p(` or
`),er=n("code"),pf=p("output_attentions=True"),cf=p("."),Cu=d(),O=n("p"),hf=p(`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),tr=n("code"),ff=p("None"),mf=p(". Here for instance "),or=n("code"),_f=p("outputs.loss"),gf=p(" is the loss computed by the model, and "),nr=n("code"),vf=p("outputs.attentions"),yf=p(` is
`),sr=n("code"),Tf=p("None"),wf=p("."),zu=d(),S=n("p"),bf=p("When considering our "),ar=n("code"),xf=p("outputs"),$f=p(" object as tuple, it only considers the attributes that don\u2019t have "),rr=n("code"),qf=p("None"),Of=p(` values.
Here for instance, it has two elements, `),dr=n("code"),Sf=p("loss"),Ff=p(" then "),ir=n("code"),Mf=p("logits"),kf=p(", so"),Eu=d(),h(an.$$.fragment),Nu=d(),Lt=n("p"),Af=p("will return the tuple "),ur=n("code"),Cf=p("(outputs.loss, outputs.logits)"),zf=p(" for instance."),Pu=d(),F=n("p"),Ef=p("When considering our "),lr=n("code"),Nf=p("outputs"),Pf=p(" object as dictionary, it only considers the attributes that don\u2019t have "),pr=n("code"),Bf=p("None"),Lf=p(`
values. Here for instance, it has two keys that are `),cr=n("code"),jf=p("loss"),Wf=p(" and "),hr=n("code"),Df=p("logits"),Hf=p("."),Bu=d(),Sa=n("p"),Qf=p(`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),Lu=d(),ee=n("h2"),jt=n("a"),fr=n("span"),h(rn.$$.fragment),If=d(),mr=n("span"),Vf=p("ModelOutput"),ju=d(),k=n("div"),h(dn.$$.fragment),Rf=d(),te=n("p"),Uf=p("Base class for all model outputs as dataclass. Has a "),_r=n("code"),Yf=p("__getitem__"),Jf=p(` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),gr=n("code"),Gf=p("None"),Kf=p(` attributes. Otherwise behaves like a regular
python dictionary.`),Xf=d(),h(Wt.$$.fragment),Zf=d(),Dt=n("div"),h(un.$$.fragment),em=d(),ln=n("p"),tm=p("Convert self to a tuple containing all the attributes/keys that are not "),vr=n("code"),om=p("None"),nm=p("."),Wu=d(),oe=n("h2"),Ht=n("a"),yr=n("span"),h(pn.$$.fragment),sm=d(),Tr=n("span"),am=p("BaseModelOutput"),Du=d(),ne=n("div"),h(cn.$$.fragment),rm=d(),wr=n("p"),dm=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Hu=d(),se=n("h2"),Qt=n("a"),br=n("span"),h(hn.$$.fragment),im=d(),xr=n("span"),um=p("BaseModelOutputWithPooling"),Qu=d(),ae=n("div"),h(fn.$$.fragment),lm=d(),$r=n("p"),pm=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Iu=d(),re=n("h2"),It=n("a"),qr=n("span"),h(mn.$$.fragment),cm=d(),Or=n("span"),hm=p("BaseModelOutputWithCrossAttentions"),Vu=d(),de=n("div"),h(_n.$$.fragment),fm=d(),Sr=n("p"),mm=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Ru=d(),ie=n("h2"),Vt=n("a"),Fr=n("span"),h(gn.$$.fragment),_m=d(),Mr=n("span"),gm=p("BaseModelOutputWithPoolingAndCrossAttentions"),Uu=d(),ue=n("div"),h(vn.$$.fragment),vm=d(),kr=n("p"),ym=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Yu=d(),le=n("h2"),Rt=n("a"),Ar=n("span"),h(yn.$$.fragment),Tm=d(),Cr=n("span"),wm=p("BaseModelOutputWithPast"),Ju=d(),pe=n("div"),h(Tn.$$.fragment),bm=d(),zr=n("p"),xm=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Gu=d(),ce=n("h2"),Ut=n("a"),Er=n("span"),h(wn.$$.fragment),$m=d(),Nr=n("span"),qm=p("BaseModelOutputWithPastAndCrossAttentions"),Ku=d(),he=n("div"),h(bn.$$.fragment),Om=d(),Pr=n("p"),Sm=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Xu=d(),fe=n("h2"),Yt=n("a"),Br=n("span"),h(xn.$$.fragment),Fm=d(),Lr=n("span"),Mm=p("Seq2SeqModelOutput"),Zu=d(),me=n("div"),h($n.$$.fragment),km=d(),jr=n("p"),Am=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),el=d(),_e=n("h2"),Jt=n("a"),Wr=n("span"),h(qn.$$.fragment),Cm=d(),Dr=n("span"),zm=p("CausalLMOutput"),tl=d(),ge=n("div"),h(On.$$.fragment),Em=d(),Hr=n("p"),Nm=p("Base class for causal language model (or autoregressive) outputs."),ol=d(),ve=n("h2"),Gt=n("a"),Qr=n("span"),h(Sn.$$.fragment),Pm=d(),Ir=n("span"),Bm=p("CausalLMOutputWithCrossAttentions"),nl=d(),ye=n("div"),h(Fn.$$.fragment),Lm=d(),Vr=n("p"),jm=p("Base class for causal language model (or autoregressive) outputs."),sl=d(),Te=n("h2"),Kt=n("a"),Rr=n("span"),h(Mn.$$.fragment),Wm=d(),Ur=n("span"),Dm=p("CausalLMOutputWithPast"),al=d(),we=n("div"),h(kn.$$.fragment),Hm=d(),Yr=n("p"),Qm=p("Base class for causal language model (or autoregressive) outputs."),rl=d(),be=n("h2"),Xt=n("a"),Jr=n("span"),h(An.$$.fragment),Im=d(),Gr=n("span"),Vm=p("MaskedLMOutput"),dl=d(),xe=n("div"),h(Cn.$$.fragment),Rm=d(),Kr=n("p"),Um=p("Base class for masked language models outputs."),il=d(),$e=n("h2"),Zt=n("a"),Xr=n("span"),h(zn.$$.fragment),Ym=d(),Zr=n("span"),Jm=p("Seq2SeqLMOutput"),ul=d(),qe=n("div"),h(En.$$.fragment),Gm=d(),ed=n("p"),Km=p("Base class for sequence-to-sequence language models outputs."),ll=d(),Oe=n("h2"),eo=n("a"),td=n("span"),h(Nn.$$.fragment),Xm=d(),od=n("span"),Zm=p("NextSentencePredictorOutput"),pl=d(),Se=n("div"),h(Pn.$$.fragment),e_=d(),nd=n("p"),t_=p("Base class for outputs of models predicting if two sentences are consecutive or not."),cl=d(),Fe=n("h2"),to=n("a"),sd=n("span"),h(Bn.$$.fragment),o_=d(),ad=n("span"),n_=p("SequenceClassifierOutput"),hl=d(),Me=n("div"),h(Ln.$$.fragment),s_=d(),rd=n("p"),a_=p("Base class for outputs of sentence classification models."),fl=d(),ke=n("h2"),oo=n("a"),dd=n("span"),h(jn.$$.fragment),r_=d(),id=n("span"),d_=p("Seq2SeqSequenceClassifierOutput"),ml=d(),Ae=n("div"),h(Wn.$$.fragment),i_=d(),ud=n("p"),u_=p("Base class for outputs of sequence-to-sequence sentence classification models."),_l=d(),Ce=n("h2"),no=n("a"),ld=n("span"),h(Dn.$$.fragment),l_=d(),pd=n("span"),p_=p("MultipleChoiceModelOutput"),gl=d(),ze=n("div"),h(Hn.$$.fragment),c_=d(),cd=n("p"),h_=p("Base class for outputs of multiple choice models."),vl=d(),Ee=n("h2"),so=n("a"),hd=n("span"),h(Qn.$$.fragment),f_=d(),fd=n("span"),m_=p("TokenClassifierOutput"),yl=d(),Ne=n("div"),h(In.$$.fragment),__=d(),md=n("p"),g_=p("Base class for outputs of token classification models."),Tl=d(),Pe=n("h2"),ao=n("a"),_d=n("span"),h(Vn.$$.fragment),v_=d(),gd=n("span"),y_=p("QuestionAnsweringModelOutput"),wl=d(),Be=n("div"),h(Rn.$$.fragment),T_=d(),vd=n("p"),w_=p("Base class for outputs of question answering models."),bl=d(),Le=n("h2"),ro=n("a"),yd=n("span"),h(Un.$$.fragment),b_=d(),Td=n("span"),x_=p("Seq2SeqQuestionAnsweringModelOutput"),xl=d(),je=n("div"),h(Yn.$$.fragment),$_=d(),wd=n("p"),q_=p("Base class for outputs of sequence-to-sequence question answering models."),$l=d(),We=n("h2"),io=n("a"),bd=n("span"),h(Jn.$$.fragment),O_=d(),xd=n("span"),S_=p("TFBaseModelOutput"),ql=d(),De=n("div"),h(Gn.$$.fragment),F_=d(),$d=n("p"),M_=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Ol=d(),He=n("h2"),uo=n("a"),qd=n("span"),h(Kn.$$.fragment),k_=d(),Od=n("span"),A_=p("TFBaseModelOutputWithPooling"),Sl=d(),Qe=n("div"),h(Xn.$$.fragment),C_=d(),Sd=n("p"),z_=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Fl=d(),Ie=n("h2"),lo=n("a"),Fd=n("span"),h(Zn.$$.fragment),E_=d(),Md=n("span"),N_=p("TFBaseModelOutputWithPoolingAndCrossAttentions"),Ml=d(),Ve=n("div"),h(es.$$.fragment),P_=d(),kd=n("p"),B_=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),kl=d(),Re=n("h2"),po=n("a"),Ad=n("span"),h(ts.$$.fragment),L_=d(),Cd=n("span"),j_=p("TFBaseModelOutputWithPast"),Al=d(),Ue=n("div"),h(os.$$.fragment),W_=d(),zd=n("p"),D_=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Cl=d(),Ye=n("h2"),co=n("a"),Ed=n("span"),h(ns.$$.fragment),H_=d(),Nd=n("span"),Q_=p("TFBaseModelOutputWithPastAndCrossAttentions"),zl=d(),Je=n("div"),h(ss.$$.fragment),I_=d(),Pd=n("p"),V_=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),El=d(),Ge=n("h2"),ho=n("a"),Bd=n("span"),h(as.$$.fragment),R_=d(),Ld=n("span"),U_=p("TFSeq2SeqModelOutput"),Nl=d(),Ke=n("div"),h(rs.$$.fragment),Y_=d(),jd=n("p"),J_=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Pl=d(),Xe=n("h2"),fo=n("a"),Wd=n("span"),h(ds.$$.fragment),G_=d(),Dd=n("span"),K_=p("TFCausalLMOutput"),Bl=d(),Ze=n("div"),h(is.$$.fragment),X_=d(),Hd=n("p"),Z_=p("Base class for causal language model (or autoregressive) outputs."),Ll=d(),et=n("h2"),mo=n("a"),Qd=n("span"),h(us.$$.fragment),eg=d(),Id=n("span"),tg=p("TFCausalLMOutputWithCrossAttentions"),jl=d(),tt=n("div"),h(ls.$$.fragment),og=d(),Vd=n("p"),ng=p("Base class for causal language model (or autoregressive) outputs."),Wl=d(),ot=n("h2"),_o=n("a"),Rd=n("span"),h(ps.$$.fragment),sg=d(),Ud=n("span"),ag=p("TFCausalLMOutputWithPast"),Dl=d(),nt=n("div"),h(cs.$$.fragment),rg=d(),Yd=n("p"),dg=p("Base class for causal language model (or autoregressive) outputs."),Hl=d(),st=n("h2"),go=n("a"),Jd=n("span"),h(hs.$$.fragment),ig=d(),Gd=n("span"),ug=p("TFMaskedLMOutput"),Ql=d(),at=n("div"),h(fs.$$.fragment),lg=d(),Kd=n("p"),pg=p("Base class for masked language models outputs."),Il=d(),rt=n("h2"),vo=n("a"),Xd=n("span"),h(ms.$$.fragment),cg=d(),Zd=n("span"),hg=p("TFSeq2SeqLMOutput"),Vl=d(),dt=n("div"),h(_s.$$.fragment),fg=d(),ei=n("p"),mg=p("Base class for sequence-to-sequence language models outputs."),Rl=d(),it=n("h2"),yo=n("a"),ti=n("span"),h(gs.$$.fragment),_g=d(),oi=n("span"),gg=p("TFNextSentencePredictorOutput"),Ul=d(),ut=n("div"),h(vs.$$.fragment),vg=d(),ni=n("p"),yg=p("Base class for outputs of models predicting if two sentences are consecutive or not."),Yl=d(),lt=n("h2"),To=n("a"),si=n("span"),h(ys.$$.fragment),Tg=d(),ai=n("span"),wg=p("TFSequenceClassifierOutput"),Jl=d(),pt=n("div"),h(Ts.$$.fragment),bg=d(),ri=n("p"),xg=p("Base class for outputs of sentence classification models."),Gl=d(),ct=n("h2"),wo=n("a"),di=n("span"),h(ws.$$.fragment),$g=d(),ii=n("span"),qg=p("TFSeq2SeqSequenceClassifierOutput"),Kl=d(),ht=n("div"),h(bs.$$.fragment),Og=d(),ui=n("p"),Sg=p("Base class for outputs of sequence-to-sequence sentence classification models."),Xl=d(),ft=n("h2"),bo=n("a"),li=n("span"),h(xs.$$.fragment),Fg=d(),pi=n("span"),Mg=p("TFMultipleChoiceModelOutput"),Zl=d(),mt=n("div"),h($s.$$.fragment),kg=d(),ci=n("p"),Ag=p("Base class for outputs of multiple choice models."),ep=d(),_t=n("h2"),xo=n("a"),hi=n("span"),h(qs.$$.fragment),Cg=d(),fi=n("span"),zg=p("TFTokenClassifierOutput"),tp=d(),gt=n("div"),h(Os.$$.fragment),Eg=d(),mi=n("p"),Ng=p("Base class for outputs of token classification models."),op=d(),vt=n("h2"),$o=n("a"),_i=n("span"),h(Ss.$$.fragment),Pg=d(),gi=n("span"),Bg=p("TFQuestionAnsweringModelOutput"),np=d(),yt=n("div"),h(Fs.$$.fragment),Lg=d(),vi=n("p"),jg=p("Base class for outputs of question answering models."),sp=d(),Tt=n("h2"),qo=n("a"),yi=n("span"),h(Ms.$$.fragment),Wg=d(),Ti=n("span"),Dg=p("TFSeq2SeqQuestionAnsweringModelOutput"),ap=d(),wt=n("div"),h(ks.$$.fragment),Hg=d(),wi=n("p"),Qg=p("Base class for outputs of sequence-to-sequence question answering models."),rp=d(),bt=n("h2"),Oo=n("a"),bi=n("span"),h(As.$$.fragment),Ig=d(),xi=n("span"),Vg=p("FlaxBaseModelOutput"),dp=d(),z=n("div"),h(Cs.$$.fragment),Rg=d(),$i=n("p"),Ug=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),Yg=d(),So=n("div"),h(zs.$$.fragment),Jg=d(),qi=n("p"),Gg=p("\u201CReturns a new object replacing the specified fields with new values."),ip=d(),xt=n("h2"),Fo=n("a"),Oi=n("span"),h(Es.$$.fragment),Kg=d(),Si=n("span"),Xg=p("FlaxBaseModelOutputWithPast"),up=d(),E=n("div"),h(Ns.$$.fragment),Zg=d(),Fi=n("p"),ev=p("Base class for model\u2019s outputs, with potential hidden states and attentions."),tv=d(),Mo=n("div"),h(Ps.$$.fragment),ov=d(),Mi=n("p"),nv=p("\u201CReturns a new object replacing the specified fields with new values."),lp=d(),$t=n("h2"),ko=n("a"),ki=n("span"),h(Bs.$$.fragment),sv=d(),Ai=n("span"),av=p("FlaxBaseModelOutputWithPooling"),pp=d(),N=n("div"),h(Ls.$$.fragment),rv=d(),Ci=n("p"),dv=p("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),iv=d(),Ao=n("div"),h(js.$$.fragment),uv=d(),zi=n("p"),lv=p("\u201CReturns a new object replacing the specified fields with new values."),cp=d(),qt=n("h2"),Co=n("a"),Ei=n("span"),h(Ws.$$.fragment),pv=d(),Ni=n("span"),cv=p("FlaxBaseModelOutputWithPastAndCrossAttentions"),hp=d(),P=n("div"),h(Ds.$$.fragment),hv=d(),Pi=n("p"),fv=p("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),mv=d(),zo=n("div"),h(Hs.$$.fragment),_v=d(),Bi=n("p"),gv=p("\u201CReturns a new object replacing the specified fields with new values."),fp=d(),Ot=n("h2"),Eo=n("a"),Li=n("span"),h(Qs.$$.fragment),vv=d(),ji=n("span"),yv=p("FlaxSeq2SeqModelOutput"),mp=d(),B=n("div"),h(Is.$$.fragment),Tv=d(),Wi=n("p"),wv=p(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),bv=d(),No=n("div"),h(Vs.$$.fragment),xv=d(),Di=n("p"),$v=p("\u201CReturns a new object replacing the specified fields with new values."),_p=d(),St=n("h2"),Po=n("a"),Hi=n("span"),h(Rs.$$.fragment),qv=d(),Qi=n("span"),Ov=p("FlaxCausalLMOutputWithCrossAttentions"),gp=d(),L=n("div"),h(Us.$$.fragment),Sv=d(),Ii=n("p"),Fv=p("Base class for causal language model (or autoregressive) outputs."),Mv=d(),Bo=n("div"),h(Ys.$$.fragment),kv=d(),Vi=n("p"),Av=p("\u201CReturns a new object replacing the specified fields with new values."),vp=d(),Ft=n("h2"),Lo=n("a"),Ri=n("span"),h(Js.$$.fragment),Cv=d(),Ui=n("span"),zv=p("FlaxMaskedLMOutput"),yp=d(),j=n("div"),h(Gs.$$.fragment),Ev=d(),Yi=n("p"),Nv=p("Base class for masked language models outputs."),Pv=d(),jo=n("div"),h(Ks.$$.fragment),Bv=d(),Ji=n("p"),Lv=p("\u201CReturns a new object replacing the specified fields with new values."),Tp=d(),Mt=n("h2"),Wo=n("a"),Gi=n("span"),h(Xs.$$.fragment),jv=d(),Ki=n("span"),Wv=p("FlaxSeq2SeqLMOutput"),wp=d(),W=n("div"),h(Zs.$$.fragment),Dv=d(),Xi=n("p"),Hv=p("Base class for sequence-to-sequence language models outputs."),Qv=d(),Do=n("div"),h(ea.$$.fragment),Iv=d(),Zi=n("p"),Vv=p("\u201CReturns a new object replacing the specified fields with new values."),bp=d(),kt=n("h2"),Ho=n("a"),eu=n("span"),h(ta.$$.fragment),Rv=d(),tu=n("span"),Uv=p("FlaxNextSentencePredictorOutput"),xp=d(),D=n("div"),h(oa.$$.fragment),Yv=d(),ou=n("p"),Jv=p("Base class for outputs of models predicting if two sentences are consecutive or not."),Gv=d(),Qo=n("div"),h(na.$$.fragment),Kv=d(),nu=n("p"),Xv=p("\u201CReturns a new object replacing the specified fields with new values."),$p=d(),At=n("h2"),Io=n("a"),su=n("span"),h(sa.$$.fragment),Zv=d(),au=n("span"),ey=p("FlaxSequenceClassifierOutput"),qp=d(),H=n("div"),h(aa.$$.fragment),ty=d(),ru=n("p"),oy=p("Base class for outputs of sentence classification models."),ny=d(),Vo=n("div"),h(ra.$$.fragment),sy=d(),du=n("p"),ay=p("\u201CReturns a new object replacing the specified fields with new values."),Op=d(),Ct=n("h2"),Ro=n("a"),iu=n("span"),h(da.$$.fragment),ry=d(),uu=n("span"),dy=p("FlaxSeq2SeqSequenceClassifierOutput"),Sp=d(),Q=n("div"),h(ia.$$.fragment),iy=d(),lu=n("p"),uy=p("Base class for outputs of sequence-to-sequence sentence classification models."),ly=d(),Uo=n("div"),h(ua.$$.fragment),py=d(),pu=n("p"),cy=p("\u201CReturns a new object replacing the specified fields with new values."),Fp=d(),zt=n("h2"),Yo=n("a"),cu=n("span"),h(la.$$.fragment),hy=d(),hu=n("span"),fy=p("FlaxMultipleChoiceModelOutput"),Mp=d(),I=n("div"),h(pa.$$.fragment),my=d(),fu=n("p"),_y=p("Base class for outputs of multiple choice models."),gy=d(),Jo=n("div"),h(ca.$$.fragment),vy=d(),mu=n("p"),yy=p("\u201CReturns a new object replacing the specified fields with new values."),kp=d(),Et=n("h2"),Go=n("a"),_u=n("span"),h(ha.$$.fragment),Ty=d(),gu=n("span"),wy=p("FlaxTokenClassifierOutput"),Ap=d(),V=n("div"),h(fa.$$.fragment),by=d(),vu=n("p"),xy=p("Base class for outputs of token classification models."),$y=d(),Ko=n("div"),h(ma.$$.fragment),qy=d(),yu=n("p"),Oy=p("\u201CReturns a new object replacing the specified fields with new values."),Cp=d(),Nt=n("h2"),Xo=n("a"),Tu=n("span"),h(_a.$$.fragment),Sy=d(),wu=n("span"),Fy=p("FlaxQuestionAnsweringModelOutput"),zp=d(),R=n("div"),h(ga.$$.fragment),My=d(),bu=n("p"),ky=p("Base class for outputs of question answering models."),Ay=d(),Zo=n("div"),h(va.$$.fragment),Cy=d(),xu=n("p"),zy=p("\u201CReturns a new object replacing the specified fields with new values."),Ep=d(),Pt=n("h2"),en=n("a"),$u=n("span"),h(ya.$$.fragment),Ey=d(),qu=n("span"),Ny=p("FlaxSeq2SeqQuestionAnsweringModelOutput"),Np=d(),U=n("div"),h(Ta.$$.fragment),Py=d(),Ou=n("p"),By=p("Base class for outputs of sequence-to-sequence question answering models."),Ly=d(),tn=n("div"),h(wa.$$.fragment),jy=d(),Su=n("p"),Wy=p("\u201CReturns a new object replacing the specified fields with new values."),this.h()},l(e){const u=N1('[data-svelte="svelte-1phssyn"]',document.head);x=s(u,"META",{name:!0,content:!0}),u.forEach(t),Bt=i(e),$=s(e,"H1",{class:!0});var ba=a($);A=s(ba,"A",{id:!0,class:!0,href:!0});var Dy=a(A);K=s(Dy,"SPAN",{});var Hy=a(K);f(q.$$.fragment,Hy),Hy.forEach(t),Dy.forEach(t),nn=i(ba),X=s(ba,"SPAN",{});var Qy=a(X);Z=c(Qy,"Model outputs"),Qy.forEach(t),ba.forEach(t),M=i(e),C=s(e,"P",{});var Bp=a(C);xa=c(Bp,"All models have outputs that are instances of subclasses of "),$a=s(Bp,"A",{href:!0});var Iy=a($a);Lh=c(Iy,"ModelOutput"),Iy.forEach(t),jh=c(Bp,`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Bp.forEach(t),Mu=i(e),qa=s(e,"P",{});var Vy=a(qa);Wh=c(Vy,"Let\u2019s see of this looks on an example:"),Vy.forEach(t),ku=i(e),f(sn.$$.fragment,e),Au=i(e),w=s(e,"P",{});var b=a(w);Dh=c(b,"The "),Ia=s(b,"CODE",{});var Ry=a(Ia);Hh=c(Ry,"outputs"),Ry.forEach(t),Qh=c(b," object is a "),Oa=s(b,"A",{href:!0});var Uy=a(Oa);Ih=c(Uy,"SequenceClassifierOutput"),Uy.forEach(t),Vh=c(b,`, as we can see in the
documentation of that class below, it means it has an optional `),Va=s(b,"CODE",{});var Yy=a(Va);Rh=c(Yy,"loss"),Yy.forEach(t),Uh=c(b,", a "),Ra=s(b,"CODE",{});var Jy=a(Ra);Yh=c(Jy,"logits"),Jy.forEach(t),Jh=c(b," an optional "),Ua=s(b,"CODE",{});var Gy=a(Ua);Gh=c(Gy,"hidden_states"),Gy.forEach(t),Kh=c(b,` and
an optional `),Ya=s(b,"CODE",{});var Ky=a(Ya);Xh=c(Ky,"attentions"),Ky.forEach(t),Zh=c(b," attribute. Here we have the "),Ja=s(b,"CODE",{});var Xy=a(Ja);ef=c(Xy,"loss"),Xy.forEach(t),tf=c(b," since we passed along "),Ga=s(b,"CODE",{});var Zy=a(Ga);of=c(Zy,"labels"),Zy.forEach(t),nf=c(b,`, but we don\u2019t have
`),Ka=s(b,"CODE",{});var eT=a(Ka);sf=c(eT,"hidden_states"),eT.forEach(t),af=c(b," and "),Xa=s(b,"CODE",{});var tT=a(Xa);rf=c(tT,"attentions"),tT.forEach(t),df=c(b," because we didn\u2019t pass "),Za=s(b,"CODE",{});var oT=a(Za);uf=c(oT,"output_hidden_states=True"),oT.forEach(t),lf=c(b,` or
`),er=s(b,"CODE",{});var nT=a(er);pf=c(nT,"output_attentions=True"),nT.forEach(t),cf=c(b,"."),b.forEach(t),Cu=i(e),O=s(e,"P",{});var Y=a(O);hf=c(Y,`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),tr=s(Y,"CODE",{});var sT=a(tr);ff=c(sT,"None"),sT.forEach(t),mf=c(Y,". Here for instance "),or=s(Y,"CODE",{});var aT=a(or);_f=c(aT,"outputs.loss"),aT.forEach(t),gf=c(Y," is the loss computed by the model, and "),nr=s(Y,"CODE",{});var rT=a(nr);vf=c(rT,"outputs.attentions"),rT.forEach(t),yf=c(Y,` is
`),sr=s(Y,"CODE",{});var dT=a(sr);Tf=c(dT,"None"),dT.forEach(t),wf=c(Y,"."),Y.forEach(t),zu=i(e),S=s(e,"P",{});var J=a(S);bf=c(J,"When considering our "),ar=s(J,"CODE",{});var iT=a(ar);xf=c(iT,"outputs"),iT.forEach(t),$f=c(J," object as tuple, it only considers the attributes that don\u2019t have "),rr=s(J,"CODE",{});var uT=a(rr);qf=c(uT,"None"),uT.forEach(t),Of=c(J,` values.
Here for instance, it has two elements, `),dr=s(J,"CODE",{});var lT=a(dr);Sf=c(lT,"loss"),lT.forEach(t),Ff=c(J," then "),ir=s(J,"CODE",{});var pT=a(ir);Mf=c(pT,"logits"),pT.forEach(t),kf=c(J,", so"),J.forEach(t),Eu=i(e),f(an.$$.fragment,e),Nu=i(e),Lt=s(e,"P",{});var Lp=a(Lt);Af=c(Lp,"will return the tuple "),ur=s(Lp,"CODE",{});var cT=a(ur);Cf=c(cT,"(outputs.loss, outputs.logits)"),cT.forEach(t),zf=c(Lp," for instance."),Lp.forEach(t),Pu=i(e),F=s(e,"P",{});var G=a(F);Ef=c(G,"When considering our "),lr=s(G,"CODE",{});var hT=a(lr);Nf=c(hT,"outputs"),hT.forEach(t),Pf=c(G," object as dictionary, it only considers the attributes that don\u2019t have "),pr=s(G,"CODE",{});var fT=a(pr);Bf=c(fT,"None"),fT.forEach(t),Lf=c(G,`
values. Here for instance, it has two keys that are `),cr=s(G,"CODE",{});var mT=a(cr);jf=c(mT,"loss"),mT.forEach(t),Wf=c(G," and "),hr=s(G,"CODE",{});var _T=a(hr);Df=c(_T,"logits"),_T.forEach(t),Hf=c(G,"."),G.forEach(t),Bu=i(e),Sa=s(e,"P",{});var gT=a(Sa);Qf=c(gT,`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),gT.forEach(t),Lu=i(e),ee=s(e,"H2",{class:!0});var jp=a(ee);jt=s(jp,"A",{id:!0,class:!0,href:!0});var vT=a(jt);fr=s(vT,"SPAN",{});var yT=a(fr);f(rn.$$.fragment,yT),yT.forEach(t),vT.forEach(t),If=i(jp),mr=s(jp,"SPAN",{});var TT=a(mr);Vf=c(TT,"ModelOutput"),TT.forEach(t),jp.forEach(t),ju=i(e),k=s(e,"DIV",{class:!0});var on=a(k);f(dn.$$.fragment,on),Rf=i(on),te=s(on,"P",{});var Fa=a(te);Uf=c(Fa,"Base class for all model outputs as dataclass. Has a "),_r=s(Fa,"CODE",{});var wT=a(_r);Yf=c(wT,"__getitem__"),wT.forEach(t),Jf=c(Fa,` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),gr=s(Fa,"CODE",{});var bT=a(gr);Gf=c(bT,"None"),bT.forEach(t),Kf=c(Fa,` attributes. Otherwise behaves like a regular
python dictionary.`),Fa.forEach(t),Xf=i(on),f(Wt.$$.fragment,on),Zf=i(on),Dt=s(on,"DIV",{class:!0});var Wp=a(Dt);f(un.$$.fragment,Wp),em=i(Wp),ln=s(Wp,"P",{});var Dp=a(ln);tm=c(Dp,"Convert self to a tuple containing all the attributes/keys that are not "),vr=s(Dp,"CODE",{});var xT=a(vr);om=c(xT,"None"),xT.forEach(t),nm=c(Dp,"."),Dp.forEach(t),Wp.forEach(t),on.forEach(t),Wu=i(e),oe=s(e,"H2",{class:!0});var Hp=a(oe);Ht=s(Hp,"A",{id:!0,class:!0,href:!0});var $T=a(Ht);yr=s($T,"SPAN",{});var qT=a(yr);f(pn.$$.fragment,qT),qT.forEach(t),$T.forEach(t),sm=i(Hp),Tr=s(Hp,"SPAN",{});var OT=a(Tr);am=c(OT,"BaseModelOutput"),OT.forEach(t),Hp.forEach(t),Du=i(e),ne=s(e,"DIV",{class:!0});var Qp=a(ne);f(cn.$$.fragment,Qp),rm=i(Qp),wr=s(Qp,"P",{});var ST=a(wr);dm=c(ST,"Base class for model\u2019s outputs, with potential hidden states and attentions."),ST.forEach(t),Qp.forEach(t),Hu=i(e),se=s(e,"H2",{class:!0});var Ip=a(se);Qt=s(Ip,"A",{id:!0,class:!0,href:!0});var FT=a(Qt);br=s(FT,"SPAN",{});var MT=a(br);f(hn.$$.fragment,MT),MT.forEach(t),FT.forEach(t),im=i(Ip),xr=s(Ip,"SPAN",{});var kT=a(xr);um=c(kT,"BaseModelOutputWithPooling"),kT.forEach(t),Ip.forEach(t),Qu=i(e),ae=s(e,"DIV",{class:!0});var Vp=a(ae);f(fn.$$.fragment,Vp),lm=i(Vp),$r=s(Vp,"P",{});var AT=a($r);pm=c(AT,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),AT.forEach(t),Vp.forEach(t),Iu=i(e),re=s(e,"H2",{class:!0});var Rp=a(re);It=s(Rp,"A",{id:!0,class:!0,href:!0});var CT=a(It);qr=s(CT,"SPAN",{});var zT=a(qr);f(mn.$$.fragment,zT),zT.forEach(t),CT.forEach(t),cm=i(Rp),Or=s(Rp,"SPAN",{});var ET=a(Or);hm=c(ET,"BaseModelOutputWithCrossAttentions"),ET.forEach(t),Rp.forEach(t),Vu=i(e),de=s(e,"DIV",{class:!0});var Up=a(de);f(_n.$$.fragment,Up),fm=i(Up),Sr=s(Up,"P",{});var NT=a(Sr);mm=c(NT,"Base class for model\u2019s outputs, with potential hidden states and attentions."),NT.forEach(t),Up.forEach(t),Ru=i(e),ie=s(e,"H2",{class:!0});var Yp=a(ie);Vt=s(Yp,"A",{id:!0,class:!0,href:!0});var PT=a(Vt);Fr=s(PT,"SPAN",{});var BT=a(Fr);f(gn.$$.fragment,BT),BT.forEach(t),PT.forEach(t),_m=i(Yp),Mr=s(Yp,"SPAN",{});var LT=a(Mr);gm=c(LT,"BaseModelOutputWithPoolingAndCrossAttentions"),LT.forEach(t),Yp.forEach(t),Uu=i(e),ue=s(e,"DIV",{class:!0});var Jp=a(ue);f(vn.$$.fragment,Jp),vm=i(Jp),kr=s(Jp,"P",{});var jT=a(kr);ym=c(jT,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),jT.forEach(t),Jp.forEach(t),Yu=i(e),le=s(e,"H2",{class:!0});var Gp=a(le);Rt=s(Gp,"A",{id:!0,class:!0,href:!0});var WT=a(Rt);Ar=s(WT,"SPAN",{});var DT=a(Ar);f(yn.$$.fragment,DT),DT.forEach(t),WT.forEach(t),Tm=i(Gp),Cr=s(Gp,"SPAN",{});var HT=a(Cr);wm=c(HT,"BaseModelOutputWithPast"),HT.forEach(t),Gp.forEach(t),Ju=i(e),pe=s(e,"DIV",{class:!0});var Kp=a(pe);f(Tn.$$.fragment,Kp),bm=i(Kp),zr=s(Kp,"P",{});var QT=a(zr);xm=c(QT,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),QT.forEach(t),Kp.forEach(t),Gu=i(e),ce=s(e,"H2",{class:!0});var Xp=a(ce);Ut=s(Xp,"A",{id:!0,class:!0,href:!0});var IT=a(Ut);Er=s(IT,"SPAN",{});var VT=a(Er);f(wn.$$.fragment,VT),VT.forEach(t),IT.forEach(t),$m=i(Xp),Nr=s(Xp,"SPAN",{});var RT=a(Nr);qm=c(RT,"BaseModelOutputWithPastAndCrossAttentions"),RT.forEach(t),Xp.forEach(t),Ku=i(e),he=s(e,"DIV",{class:!0});var Zp=a(he);f(bn.$$.fragment,Zp),Om=i(Zp),Pr=s(Zp,"P",{});var UT=a(Pr);Sm=c(UT,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),UT.forEach(t),Zp.forEach(t),Xu=i(e),fe=s(e,"H2",{class:!0});var ec=a(fe);Yt=s(ec,"A",{id:!0,class:!0,href:!0});var YT=a(Yt);Br=s(YT,"SPAN",{});var JT=a(Br);f(xn.$$.fragment,JT),JT.forEach(t),YT.forEach(t),Fm=i(ec),Lr=s(ec,"SPAN",{});var GT=a(Lr);Mm=c(GT,"Seq2SeqModelOutput"),GT.forEach(t),ec.forEach(t),Zu=i(e),me=s(e,"DIV",{class:!0});var tc=a(me);f($n.$$.fragment,tc),km=i(tc),jr=s(tc,"P",{});var KT=a(jr);Am=c(KT,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),KT.forEach(t),tc.forEach(t),el=i(e),_e=s(e,"H2",{class:!0});var oc=a(_e);Jt=s(oc,"A",{id:!0,class:!0,href:!0});var XT=a(Jt);Wr=s(XT,"SPAN",{});var ZT=a(Wr);f(qn.$$.fragment,ZT),ZT.forEach(t),XT.forEach(t),Cm=i(oc),Dr=s(oc,"SPAN",{});var e2=a(Dr);zm=c(e2,"CausalLMOutput"),e2.forEach(t),oc.forEach(t),tl=i(e),ge=s(e,"DIV",{class:!0});var nc=a(ge);f(On.$$.fragment,nc),Em=i(nc),Hr=s(nc,"P",{});var t2=a(Hr);Nm=c(t2,"Base class for causal language model (or autoregressive) outputs."),t2.forEach(t),nc.forEach(t),ol=i(e),ve=s(e,"H2",{class:!0});var sc=a(ve);Gt=s(sc,"A",{id:!0,class:!0,href:!0});var o2=a(Gt);Qr=s(o2,"SPAN",{});var n2=a(Qr);f(Sn.$$.fragment,n2),n2.forEach(t),o2.forEach(t),Pm=i(sc),Ir=s(sc,"SPAN",{});var s2=a(Ir);Bm=c(s2,"CausalLMOutputWithCrossAttentions"),s2.forEach(t),sc.forEach(t),nl=i(e),ye=s(e,"DIV",{class:!0});var ac=a(ye);f(Fn.$$.fragment,ac),Lm=i(ac),Vr=s(ac,"P",{});var a2=a(Vr);jm=c(a2,"Base class for causal language model (or autoregressive) outputs."),a2.forEach(t),ac.forEach(t),sl=i(e),Te=s(e,"H2",{class:!0});var rc=a(Te);Kt=s(rc,"A",{id:!0,class:!0,href:!0});var r2=a(Kt);Rr=s(r2,"SPAN",{});var d2=a(Rr);f(Mn.$$.fragment,d2),d2.forEach(t),r2.forEach(t),Wm=i(rc),Ur=s(rc,"SPAN",{});var i2=a(Ur);Dm=c(i2,"CausalLMOutputWithPast"),i2.forEach(t),rc.forEach(t),al=i(e),we=s(e,"DIV",{class:!0});var dc=a(we);f(kn.$$.fragment,dc),Hm=i(dc),Yr=s(dc,"P",{});var u2=a(Yr);Qm=c(u2,"Base class for causal language model (or autoregressive) outputs."),u2.forEach(t),dc.forEach(t),rl=i(e),be=s(e,"H2",{class:!0});var ic=a(be);Xt=s(ic,"A",{id:!0,class:!0,href:!0});var l2=a(Xt);Jr=s(l2,"SPAN",{});var p2=a(Jr);f(An.$$.fragment,p2),p2.forEach(t),l2.forEach(t),Im=i(ic),Gr=s(ic,"SPAN",{});var c2=a(Gr);Vm=c(c2,"MaskedLMOutput"),c2.forEach(t),ic.forEach(t),dl=i(e),xe=s(e,"DIV",{class:!0});var uc=a(xe);f(Cn.$$.fragment,uc),Rm=i(uc),Kr=s(uc,"P",{});var h2=a(Kr);Um=c(h2,"Base class for masked language models outputs."),h2.forEach(t),uc.forEach(t),il=i(e),$e=s(e,"H2",{class:!0});var lc=a($e);Zt=s(lc,"A",{id:!0,class:!0,href:!0});var f2=a(Zt);Xr=s(f2,"SPAN",{});var m2=a(Xr);f(zn.$$.fragment,m2),m2.forEach(t),f2.forEach(t),Ym=i(lc),Zr=s(lc,"SPAN",{});var _2=a(Zr);Jm=c(_2,"Seq2SeqLMOutput"),_2.forEach(t),lc.forEach(t),ul=i(e),qe=s(e,"DIV",{class:!0});var pc=a(qe);f(En.$$.fragment,pc),Gm=i(pc),ed=s(pc,"P",{});var g2=a(ed);Km=c(g2,"Base class for sequence-to-sequence language models outputs."),g2.forEach(t),pc.forEach(t),ll=i(e),Oe=s(e,"H2",{class:!0});var cc=a(Oe);eo=s(cc,"A",{id:!0,class:!0,href:!0});var v2=a(eo);td=s(v2,"SPAN",{});var y2=a(td);f(Nn.$$.fragment,y2),y2.forEach(t),v2.forEach(t),Xm=i(cc),od=s(cc,"SPAN",{});var T2=a(od);Zm=c(T2,"NextSentencePredictorOutput"),T2.forEach(t),cc.forEach(t),pl=i(e),Se=s(e,"DIV",{class:!0});var hc=a(Se);f(Pn.$$.fragment,hc),e_=i(hc),nd=s(hc,"P",{});var w2=a(nd);t_=c(w2,"Base class for outputs of models predicting if two sentences are consecutive or not."),w2.forEach(t),hc.forEach(t),cl=i(e),Fe=s(e,"H2",{class:!0});var fc=a(Fe);to=s(fc,"A",{id:!0,class:!0,href:!0});var b2=a(to);sd=s(b2,"SPAN",{});var x2=a(sd);f(Bn.$$.fragment,x2),x2.forEach(t),b2.forEach(t),o_=i(fc),ad=s(fc,"SPAN",{});var $2=a(ad);n_=c($2,"SequenceClassifierOutput"),$2.forEach(t),fc.forEach(t),hl=i(e),Me=s(e,"DIV",{class:!0});var mc=a(Me);f(Ln.$$.fragment,mc),s_=i(mc),rd=s(mc,"P",{});var q2=a(rd);a_=c(q2,"Base class for outputs of sentence classification models."),q2.forEach(t),mc.forEach(t),fl=i(e),ke=s(e,"H2",{class:!0});var _c=a(ke);oo=s(_c,"A",{id:!0,class:!0,href:!0});var O2=a(oo);dd=s(O2,"SPAN",{});var S2=a(dd);f(jn.$$.fragment,S2),S2.forEach(t),O2.forEach(t),r_=i(_c),id=s(_c,"SPAN",{});var F2=a(id);d_=c(F2,"Seq2SeqSequenceClassifierOutput"),F2.forEach(t),_c.forEach(t),ml=i(e),Ae=s(e,"DIV",{class:!0});var gc=a(Ae);f(Wn.$$.fragment,gc),i_=i(gc),ud=s(gc,"P",{});var M2=a(ud);u_=c(M2,"Base class for outputs of sequence-to-sequence sentence classification models."),M2.forEach(t),gc.forEach(t),_l=i(e),Ce=s(e,"H2",{class:!0});var vc=a(Ce);no=s(vc,"A",{id:!0,class:!0,href:!0});var k2=a(no);ld=s(k2,"SPAN",{});var A2=a(ld);f(Dn.$$.fragment,A2),A2.forEach(t),k2.forEach(t),l_=i(vc),pd=s(vc,"SPAN",{});var C2=a(pd);p_=c(C2,"MultipleChoiceModelOutput"),C2.forEach(t),vc.forEach(t),gl=i(e),ze=s(e,"DIV",{class:!0});var yc=a(ze);f(Hn.$$.fragment,yc),c_=i(yc),cd=s(yc,"P",{});var z2=a(cd);h_=c(z2,"Base class for outputs of multiple choice models."),z2.forEach(t),yc.forEach(t),vl=i(e),Ee=s(e,"H2",{class:!0});var Tc=a(Ee);so=s(Tc,"A",{id:!0,class:!0,href:!0});var E2=a(so);hd=s(E2,"SPAN",{});var N2=a(hd);f(Qn.$$.fragment,N2),N2.forEach(t),E2.forEach(t),f_=i(Tc),fd=s(Tc,"SPAN",{});var P2=a(fd);m_=c(P2,"TokenClassifierOutput"),P2.forEach(t),Tc.forEach(t),yl=i(e),Ne=s(e,"DIV",{class:!0});var wc=a(Ne);f(In.$$.fragment,wc),__=i(wc),md=s(wc,"P",{});var B2=a(md);g_=c(B2,"Base class for outputs of token classification models."),B2.forEach(t),wc.forEach(t),Tl=i(e),Pe=s(e,"H2",{class:!0});var bc=a(Pe);ao=s(bc,"A",{id:!0,class:!0,href:!0});var L2=a(ao);_d=s(L2,"SPAN",{});var j2=a(_d);f(Vn.$$.fragment,j2),j2.forEach(t),L2.forEach(t),v_=i(bc),gd=s(bc,"SPAN",{});var W2=a(gd);y_=c(W2,"QuestionAnsweringModelOutput"),W2.forEach(t),bc.forEach(t),wl=i(e),Be=s(e,"DIV",{class:!0});var xc=a(Be);f(Rn.$$.fragment,xc),T_=i(xc),vd=s(xc,"P",{});var D2=a(vd);w_=c(D2,"Base class for outputs of question answering models."),D2.forEach(t),xc.forEach(t),bl=i(e),Le=s(e,"H2",{class:!0});var $c=a(Le);ro=s($c,"A",{id:!0,class:!0,href:!0});var H2=a(ro);yd=s(H2,"SPAN",{});var Q2=a(yd);f(Un.$$.fragment,Q2),Q2.forEach(t),H2.forEach(t),b_=i($c),Td=s($c,"SPAN",{});var I2=a(Td);x_=c(I2,"Seq2SeqQuestionAnsweringModelOutput"),I2.forEach(t),$c.forEach(t),xl=i(e),je=s(e,"DIV",{class:!0});var qc=a(je);f(Yn.$$.fragment,qc),$_=i(qc),wd=s(qc,"P",{});var V2=a(wd);q_=c(V2,"Base class for outputs of sequence-to-sequence question answering models."),V2.forEach(t),qc.forEach(t),$l=i(e),We=s(e,"H2",{class:!0});var Oc=a(We);io=s(Oc,"A",{id:!0,class:!0,href:!0});var R2=a(io);bd=s(R2,"SPAN",{});var U2=a(bd);f(Jn.$$.fragment,U2),U2.forEach(t),R2.forEach(t),O_=i(Oc),xd=s(Oc,"SPAN",{});var Y2=a(xd);S_=c(Y2,"TFBaseModelOutput"),Y2.forEach(t),Oc.forEach(t),ql=i(e),De=s(e,"DIV",{class:!0});var Sc=a(De);f(Gn.$$.fragment,Sc),F_=i(Sc),$d=s(Sc,"P",{});var J2=a($d);M_=c(J2,"Base class for model\u2019s outputs, with potential hidden states and attentions."),J2.forEach(t),Sc.forEach(t),Ol=i(e),He=s(e,"H2",{class:!0});var Fc=a(He);uo=s(Fc,"A",{id:!0,class:!0,href:!0});var G2=a(uo);qd=s(G2,"SPAN",{});var K2=a(qd);f(Kn.$$.fragment,K2),K2.forEach(t),G2.forEach(t),k_=i(Fc),Od=s(Fc,"SPAN",{});var X2=a(Od);A_=c(X2,"TFBaseModelOutputWithPooling"),X2.forEach(t),Fc.forEach(t),Sl=i(e),Qe=s(e,"DIV",{class:!0});var Mc=a(Qe);f(Xn.$$.fragment,Mc),C_=i(Mc),Sd=s(Mc,"P",{});var Z2=a(Sd);z_=c(Z2,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Z2.forEach(t),Mc.forEach(t),Fl=i(e),Ie=s(e,"H2",{class:!0});var kc=a(Ie);lo=s(kc,"A",{id:!0,class:!0,href:!0});var ew=a(lo);Fd=s(ew,"SPAN",{});var tw=a(Fd);f(Zn.$$.fragment,tw),tw.forEach(t),ew.forEach(t),E_=i(kc),Md=s(kc,"SPAN",{});var ow=a(Md);N_=c(ow,"TFBaseModelOutputWithPoolingAndCrossAttentions"),ow.forEach(t),kc.forEach(t),Ml=i(e),Ve=s(e,"DIV",{class:!0});var Ac=a(Ve);f(es.$$.fragment,Ac),P_=i(Ac),kd=s(Ac,"P",{});var nw=a(kd);B_=c(nw,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),nw.forEach(t),Ac.forEach(t),kl=i(e),Re=s(e,"H2",{class:!0});var Cc=a(Re);po=s(Cc,"A",{id:!0,class:!0,href:!0});var sw=a(po);Ad=s(sw,"SPAN",{});var aw=a(Ad);f(ts.$$.fragment,aw),aw.forEach(t),sw.forEach(t),L_=i(Cc),Cd=s(Cc,"SPAN",{});var rw=a(Cd);j_=c(rw,"TFBaseModelOutputWithPast"),rw.forEach(t),Cc.forEach(t),Al=i(e),Ue=s(e,"DIV",{class:!0});var zc=a(Ue);f(os.$$.fragment,zc),W_=i(zc),zd=s(zc,"P",{});var dw=a(zd);D_=c(dw,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),dw.forEach(t),zc.forEach(t),Cl=i(e),Ye=s(e,"H2",{class:!0});var Ec=a(Ye);co=s(Ec,"A",{id:!0,class:!0,href:!0});var iw=a(co);Ed=s(iw,"SPAN",{});var uw=a(Ed);f(ns.$$.fragment,uw),uw.forEach(t),iw.forEach(t),H_=i(Ec),Nd=s(Ec,"SPAN",{});var lw=a(Nd);Q_=c(lw,"TFBaseModelOutputWithPastAndCrossAttentions"),lw.forEach(t),Ec.forEach(t),zl=i(e),Je=s(e,"DIV",{class:!0});var Nc=a(Je);f(ss.$$.fragment,Nc),I_=i(Nc),Pd=s(Nc,"P",{});var pw=a(Pd);V_=c(pw,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),pw.forEach(t),Nc.forEach(t),El=i(e),Ge=s(e,"H2",{class:!0});var Pc=a(Ge);ho=s(Pc,"A",{id:!0,class:!0,href:!0});var cw=a(ho);Bd=s(cw,"SPAN",{});var hw=a(Bd);f(as.$$.fragment,hw),hw.forEach(t),cw.forEach(t),R_=i(Pc),Ld=s(Pc,"SPAN",{});var fw=a(Ld);U_=c(fw,"TFSeq2SeqModelOutput"),fw.forEach(t),Pc.forEach(t),Nl=i(e),Ke=s(e,"DIV",{class:!0});var Bc=a(Ke);f(rs.$$.fragment,Bc),Y_=i(Bc),jd=s(Bc,"P",{});var mw=a(jd);J_=c(mw,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),mw.forEach(t),Bc.forEach(t),Pl=i(e),Xe=s(e,"H2",{class:!0});var Lc=a(Xe);fo=s(Lc,"A",{id:!0,class:!0,href:!0});var _w=a(fo);Wd=s(_w,"SPAN",{});var gw=a(Wd);f(ds.$$.fragment,gw),gw.forEach(t),_w.forEach(t),G_=i(Lc),Dd=s(Lc,"SPAN",{});var vw=a(Dd);K_=c(vw,"TFCausalLMOutput"),vw.forEach(t),Lc.forEach(t),Bl=i(e),Ze=s(e,"DIV",{class:!0});var jc=a(Ze);f(is.$$.fragment,jc),X_=i(jc),Hd=s(jc,"P",{});var yw=a(Hd);Z_=c(yw,"Base class for causal language model (or autoregressive) outputs."),yw.forEach(t),jc.forEach(t),Ll=i(e),et=s(e,"H2",{class:!0});var Wc=a(et);mo=s(Wc,"A",{id:!0,class:!0,href:!0});var Tw=a(mo);Qd=s(Tw,"SPAN",{});var ww=a(Qd);f(us.$$.fragment,ww),ww.forEach(t),Tw.forEach(t),eg=i(Wc),Id=s(Wc,"SPAN",{});var bw=a(Id);tg=c(bw,"TFCausalLMOutputWithCrossAttentions"),bw.forEach(t),Wc.forEach(t),jl=i(e),tt=s(e,"DIV",{class:!0});var Dc=a(tt);f(ls.$$.fragment,Dc),og=i(Dc),Vd=s(Dc,"P",{});var xw=a(Vd);ng=c(xw,"Base class for causal language model (or autoregressive) outputs."),xw.forEach(t),Dc.forEach(t),Wl=i(e),ot=s(e,"H2",{class:!0});var Hc=a(ot);_o=s(Hc,"A",{id:!0,class:!0,href:!0});var $w=a(_o);Rd=s($w,"SPAN",{});var qw=a(Rd);f(ps.$$.fragment,qw),qw.forEach(t),$w.forEach(t),sg=i(Hc),Ud=s(Hc,"SPAN",{});var Ow=a(Ud);ag=c(Ow,"TFCausalLMOutputWithPast"),Ow.forEach(t),Hc.forEach(t),Dl=i(e),nt=s(e,"DIV",{class:!0});var Qc=a(nt);f(cs.$$.fragment,Qc),rg=i(Qc),Yd=s(Qc,"P",{});var Sw=a(Yd);dg=c(Sw,"Base class for causal language model (or autoregressive) outputs."),Sw.forEach(t),Qc.forEach(t),Hl=i(e),st=s(e,"H2",{class:!0});var Ic=a(st);go=s(Ic,"A",{id:!0,class:!0,href:!0});var Fw=a(go);Jd=s(Fw,"SPAN",{});var Mw=a(Jd);f(hs.$$.fragment,Mw),Mw.forEach(t),Fw.forEach(t),ig=i(Ic),Gd=s(Ic,"SPAN",{});var kw=a(Gd);ug=c(kw,"TFMaskedLMOutput"),kw.forEach(t),Ic.forEach(t),Ql=i(e),at=s(e,"DIV",{class:!0});var Vc=a(at);f(fs.$$.fragment,Vc),lg=i(Vc),Kd=s(Vc,"P",{});var Aw=a(Kd);pg=c(Aw,"Base class for masked language models outputs."),Aw.forEach(t),Vc.forEach(t),Il=i(e),rt=s(e,"H2",{class:!0});var Rc=a(rt);vo=s(Rc,"A",{id:!0,class:!0,href:!0});var Cw=a(vo);Xd=s(Cw,"SPAN",{});var zw=a(Xd);f(ms.$$.fragment,zw),zw.forEach(t),Cw.forEach(t),cg=i(Rc),Zd=s(Rc,"SPAN",{});var Ew=a(Zd);hg=c(Ew,"TFSeq2SeqLMOutput"),Ew.forEach(t),Rc.forEach(t),Vl=i(e),dt=s(e,"DIV",{class:!0});var Uc=a(dt);f(_s.$$.fragment,Uc),fg=i(Uc),ei=s(Uc,"P",{});var Nw=a(ei);mg=c(Nw,"Base class for sequence-to-sequence language models outputs."),Nw.forEach(t),Uc.forEach(t),Rl=i(e),it=s(e,"H2",{class:!0});var Yc=a(it);yo=s(Yc,"A",{id:!0,class:!0,href:!0});var Pw=a(yo);ti=s(Pw,"SPAN",{});var Bw=a(ti);f(gs.$$.fragment,Bw),Bw.forEach(t),Pw.forEach(t),_g=i(Yc),oi=s(Yc,"SPAN",{});var Lw=a(oi);gg=c(Lw,"TFNextSentencePredictorOutput"),Lw.forEach(t),Yc.forEach(t),Ul=i(e),ut=s(e,"DIV",{class:!0});var Jc=a(ut);f(vs.$$.fragment,Jc),vg=i(Jc),ni=s(Jc,"P",{});var jw=a(ni);yg=c(jw,"Base class for outputs of models predicting if two sentences are consecutive or not."),jw.forEach(t),Jc.forEach(t),Yl=i(e),lt=s(e,"H2",{class:!0});var Gc=a(lt);To=s(Gc,"A",{id:!0,class:!0,href:!0});var Ww=a(To);si=s(Ww,"SPAN",{});var Dw=a(si);f(ys.$$.fragment,Dw),Dw.forEach(t),Ww.forEach(t),Tg=i(Gc),ai=s(Gc,"SPAN",{});var Hw=a(ai);wg=c(Hw,"TFSequenceClassifierOutput"),Hw.forEach(t),Gc.forEach(t),Jl=i(e),pt=s(e,"DIV",{class:!0});var Kc=a(pt);f(Ts.$$.fragment,Kc),bg=i(Kc),ri=s(Kc,"P",{});var Qw=a(ri);xg=c(Qw,"Base class for outputs of sentence classification models."),Qw.forEach(t),Kc.forEach(t),Gl=i(e),ct=s(e,"H2",{class:!0});var Xc=a(ct);wo=s(Xc,"A",{id:!0,class:!0,href:!0});var Iw=a(wo);di=s(Iw,"SPAN",{});var Vw=a(di);f(ws.$$.fragment,Vw),Vw.forEach(t),Iw.forEach(t),$g=i(Xc),ii=s(Xc,"SPAN",{});var Rw=a(ii);qg=c(Rw,"TFSeq2SeqSequenceClassifierOutput"),Rw.forEach(t),Xc.forEach(t),Kl=i(e),ht=s(e,"DIV",{class:!0});var Zc=a(ht);f(bs.$$.fragment,Zc),Og=i(Zc),ui=s(Zc,"P",{});var Uw=a(ui);Sg=c(Uw,"Base class for outputs of sequence-to-sequence sentence classification models."),Uw.forEach(t),Zc.forEach(t),Xl=i(e),ft=s(e,"H2",{class:!0});var eh=a(ft);bo=s(eh,"A",{id:!0,class:!0,href:!0});var Yw=a(bo);li=s(Yw,"SPAN",{});var Jw=a(li);f(xs.$$.fragment,Jw),Jw.forEach(t),Yw.forEach(t),Fg=i(eh),pi=s(eh,"SPAN",{});var Gw=a(pi);Mg=c(Gw,"TFMultipleChoiceModelOutput"),Gw.forEach(t),eh.forEach(t),Zl=i(e),mt=s(e,"DIV",{class:!0});var th=a(mt);f($s.$$.fragment,th),kg=i(th),ci=s(th,"P",{});var Kw=a(ci);Ag=c(Kw,"Base class for outputs of multiple choice models."),Kw.forEach(t),th.forEach(t),ep=i(e),_t=s(e,"H2",{class:!0});var oh=a(_t);xo=s(oh,"A",{id:!0,class:!0,href:!0});var Xw=a(xo);hi=s(Xw,"SPAN",{});var Zw=a(hi);f(qs.$$.fragment,Zw),Zw.forEach(t),Xw.forEach(t),Cg=i(oh),fi=s(oh,"SPAN",{});var eb=a(fi);zg=c(eb,"TFTokenClassifierOutput"),eb.forEach(t),oh.forEach(t),tp=i(e),gt=s(e,"DIV",{class:!0});var nh=a(gt);f(Os.$$.fragment,nh),Eg=i(nh),mi=s(nh,"P",{});var tb=a(mi);Ng=c(tb,"Base class for outputs of token classification models."),tb.forEach(t),nh.forEach(t),op=i(e),vt=s(e,"H2",{class:!0});var sh=a(vt);$o=s(sh,"A",{id:!0,class:!0,href:!0});var ob=a($o);_i=s(ob,"SPAN",{});var nb=a(_i);f(Ss.$$.fragment,nb),nb.forEach(t),ob.forEach(t),Pg=i(sh),gi=s(sh,"SPAN",{});var sb=a(gi);Bg=c(sb,"TFQuestionAnsweringModelOutput"),sb.forEach(t),sh.forEach(t),np=i(e),yt=s(e,"DIV",{class:!0});var ah=a(yt);f(Fs.$$.fragment,ah),Lg=i(ah),vi=s(ah,"P",{});var ab=a(vi);jg=c(ab,"Base class for outputs of question answering models."),ab.forEach(t),ah.forEach(t),sp=i(e),Tt=s(e,"H2",{class:!0});var rh=a(Tt);qo=s(rh,"A",{id:!0,class:!0,href:!0});var rb=a(qo);yi=s(rb,"SPAN",{});var db=a(yi);f(Ms.$$.fragment,db),db.forEach(t),rb.forEach(t),Wg=i(rh),Ti=s(rh,"SPAN",{});var ib=a(Ti);Dg=c(ib,"TFSeq2SeqQuestionAnsweringModelOutput"),ib.forEach(t),rh.forEach(t),ap=i(e),wt=s(e,"DIV",{class:!0});var dh=a(wt);f(ks.$$.fragment,dh),Hg=i(dh),wi=s(dh,"P",{});var ub=a(wi);Qg=c(ub,"Base class for outputs of sequence-to-sequence question answering models."),ub.forEach(t),dh.forEach(t),rp=i(e),bt=s(e,"H2",{class:!0});var ih=a(bt);Oo=s(ih,"A",{id:!0,class:!0,href:!0});var lb=a(Oo);bi=s(lb,"SPAN",{});var pb=a(bi);f(As.$$.fragment,pb),pb.forEach(t),lb.forEach(t),Ig=i(ih),xi=s(ih,"SPAN",{});var cb=a(xi);Vg=c(cb,"FlaxBaseModelOutput"),cb.forEach(t),ih.forEach(t),dp=i(e),z=s(e,"DIV",{class:!0});var Ma=a(z);f(Cs.$$.fragment,Ma),Rg=i(Ma),$i=s(Ma,"P",{});var hb=a($i);Ug=c(hb,"Base class for model\u2019s outputs, with potential hidden states and attentions."),hb.forEach(t),Yg=i(Ma),So=s(Ma,"DIV",{class:!0});var uh=a(So);f(zs.$$.fragment,uh),Jg=i(uh),qi=s(uh,"P",{});var fb=a(qi);Gg=c(fb,"\u201CReturns a new object replacing the specified fields with new values."),fb.forEach(t),uh.forEach(t),Ma.forEach(t),ip=i(e),xt=s(e,"H2",{class:!0});var lh=a(xt);Fo=s(lh,"A",{id:!0,class:!0,href:!0});var mb=a(Fo);Oi=s(mb,"SPAN",{});var _b=a(Oi);f(Es.$$.fragment,_b),_b.forEach(t),mb.forEach(t),Kg=i(lh),Si=s(lh,"SPAN",{});var gb=a(Si);Xg=c(gb,"FlaxBaseModelOutputWithPast"),gb.forEach(t),lh.forEach(t),up=i(e),E=s(e,"DIV",{class:!0});var ka=a(E);f(Ns.$$.fragment,ka),Zg=i(ka),Fi=s(ka,"P",{});var vb=a(Fi);ev=c(vb,"Base class for model\u2019s outputs, with potential hidden states and attentions."),vb.forEach(t),tv=i(ka),Mo=s(ka,"DIV",{class:!0});var ph=a(Mo);f(Ps.$$.fragment,ph),ov=i(ph),Mi=s(ph,"P",{});var yb=a(Mi);nv=c(yb,"\u201CReturns a new object replacing the specified fields with new values."),yb.forEach(t),ph.forEach(t),ka.forEach(t),lp=i(e),$t=s(e,"H2",{class:!0});var ch=a($t);ko=s(ch,"A",{id:!0,class:!0,href:!0});var Tb=a(ko);ki=s(Tb,"SPAN",{});var wb=a(ki);f(Bs.$$.fragment,wb),wb.forEach(t),Tb.forEach(t),sv=i(ch),Ai=s(ch,"SPAN",{});var bb=a(Ai);av=c(bb,"FlaxBaseModelOutputWithPooling"),bb.forEach(t),ch.forEach(t),pp=i(e),N=s(e,"DIV",{class:!0});var Aa=a(N);f(Ls.$$.fragment,Aa),rv=i(Aa),Ci=s(Aa,"P",{});var xb=a(Ci);dv=c(xb,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),xb.forEach(t),iv=i(Aa),Ao=s(Aa,"DIV",{class:!0});var hh=a(Ao);f(js.$$.fragment,hh),uv=i(hh),zi=s(hh,"P",{});var $b=a(zi);lv=c($b,"\u201CReturns a new object replacing the specified fields with new values."),$b.forEach(t),hh.forEach(t),Aa.forEach(t),cp=i(e),qt=s(e,"H2",{class:!0});var fh=a(qt);Co=s(fh,"A",{id:!0,class:!0,href:!0});var qb=a(Co);Ei=s(qb,"SPAN",{});var Ob=a(Ei);f(Ws.$$.fragment,Ob),Ob.forEach(t),qb.forEach(t),pv=i(fh),Ni=s(fh,"SPAN",{});var Sb=a(Ni);cv=c(Sb,"FlaxBaseModelOutputWithPastAndCrossAttentions"),Sb.forEach(t),fh.forEach(t),hp=i(e),P=s(e,"DIV",{class:!0});var Ca=a(P);f(Ds.$$.fragment,Ca),hv=i(Ca),Pi=s(Ca,"P",{});var Fb=a(Pi);fv=c(Fb,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Fb.forEach(t),mv=i(Ca),zo=s(Ca,"DIV",{class:!0});var mh=a(zo);f(Hs.$$.fragment,mh),_v=i(mh),Bi=s(mh,"P",{});var Mb=a(Bi);gv=c(Mb,"\u201CReturns a new object replacing the specified fields with new values."),Mb.forEach(t),mh.forEach(t),Ca.forEach(t),fp=i(e),Ot=s(e,"H2",{class:!0});var _h=a(Ot);Eo=s(_h,"A",{id:!0,class:!0,href:!0});var kb=a(Eo);Li=s(kb,"SPAN",{});var Ab=a(Li);f(Qs.$$.fragment,Ab),Ab.forEach(t),kb.forEach(t),vv=i(_h),ji=s(_h,"SPAN",{});var Cb=a(ji);yv=c(Cb,"FlaxSeq2SeqModelOutput"),Cb.forEach(t),_h.forEach(t),mp=i(e),B=s(e,"DIV",{class:!0});var za=a(B);f(Is.$$.fragment,za),Tv=i(za),Wi=s(za,"P",{});var zb=a(Wi);wv=c(zb,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),zb.forEach(t),bv=i(za),No=s(za,"DIV",{class:!0});var gh=a(No);f(Vs.$$.fragment,gh),xv=i(gh),Di=s(gh,"P",{});var Eb=a(Di);$v=c(Eb,"\u201CReturns a new object replacing the specified fields with new values."),Eb.forEach(t),gh.forEach(t),za.forEach(t),_p=i(e),St=s(e,"H2",{class:!0});var vh=a(St);Po=s(vh,"A",{id:!0,class:!0,href:!0});var Nb=a(Po);Hi=s(Nb,"SPAN",{});var Pb=a(Hi);f(Rs.$$.fragment,Pb),Pb.forEach(t),Nb.forEach(t),qv=i(vh),Qi=s(vh,"SPAN",{});var Bb=a(Qi);Ov=c(Bb,"FlaxCausalLMOutputWithCrossAttentions"),Bb.forEach(t),vh.forEach(t),gp=i(e),L=s(e,"DIV",{class:!0});var Ea=a(L);f(Us.$$.fragment,Ea),Sv=i(Ea),Ii=s(Ea,"P",{});var Lb=a(Ii);Fv=c(Lb,"Base class for causal language model (or autoregressive) outputs."),Lb.forEach(t),Mv=i(Ea),Bo=s(Ea,"DIV",{class:!0});var yh=a(Bo);f(Ys.$$.fragment,yh),kv=i(yh),Vi=s(yh,"P",{});var jb=a(Vi);Av=c(jb,"\u201CReturns a new object replacing the specified fields with new values."),jb.forEach(t),yh.forEach(t),Ea.forEach(t),vp=i(e),Ft=s(e,"H2",{class:!0});var Th=a(Ft);Lo=s(Th,"A",{id:!0,class:!0,href:!0});var Wb=a(Lo);Ri=s(Wb,"SPAN",{});var Db=a(Ri);f(Js.$$.fragment,Db),Db.forEach(t),Wb.forEach(t),Cv=i(Th),Ui=s(Th,"SPAN",{});var Hb=a(Ui);zv=c(Hb,"FlaxMaskedLMOutput"),Hb.forEach(t),Th.forEach(t),yp=i(e),j=s(e,"DIV",{class:!0});var Na=a(j);f(Gs.$$.fragment,Na),Ev=i(Na),Yi=s(Na,"P",{});var Qb=a(Yi);Nv=c(Qb,"Base class for masked language models outputs."),Qb.forEach(t),Pv=i(Na),jo=s(Na,"DIV",{class:!0});var wh=a(jo);f(Ks.$$.fragment,wh),Bv=i(wh),Ji=s(wh,"P",{});var Ib=a(Ji);Lv=c(Ib,"\u201CReturns a new object replacing the specified fields with new values."),Ib.forEach(t),wh.forEach(t),Na.forEach(t),Tp=i(e),Mt=s(e,"H2",{class:!0});var bh=a(Mt);Wo=s(bh,"A",{id:!0,class:!0,href:!0});var Vb=a(Wo);Gi=s(Vb,"SPAN",{});var Rb=a(Gi);f(Xs.$$.fragment,Rb),Rb.forEach(t),Vb.forEach(t),jv=i(bh),Ki=s(bh,"SPAN",{});var Ub=a(Ki);Wv=c(Ub,"FlaxSeq2SeqLMOutput"),Ub.forEach(t),bh.forEach(t),wp=i(e),W=s(e,"DIV",{class:!0});var Pa=a(W);f(Zs.$$.fragment,Pa),Dv=i(Pa),Xi=s(Pa,"P",{});var Yb=a(Xi);Hv=c(Yb,"Base class for sequence-to-sequence language models outputs."),Yb.forEach(t),Qv=i(Pa),Do=s(Pa,"DIV",{class:!0});var xh=a(Do);f(ea.$$.fragment,xh),Iv=i(xh),Zi=s(xh,"P",{});var Jb=a(Zi);Vv=c(Jb,"\u201CReturns a new object replacing the specified fields with new values."),Jb.forEach(t),xh.forEach(t),Pa.forEach(t),bp=i(e),kt=s(e,"H2",{class:!0});var $h=a(kt);Ho=s($h,"A",{id:!0,class:!0,href:!0});var Gb=a(Ho);eu=s(Gb,"SPAN",{});var Kb=a(eu);f(ta.$$.fragment,Kb),Kb.forEach(t),Gb.forEach(t),Rv=i($h),tu=s($h,"SPAN",{});var Xb=a(tu);Uv=c(Xb,"FlaxNextSentencePredictorOutput"),Xb.forEach(t),$h.forEach(t),xp=i(e),D=s(e,"DIV",{class:!0});var Ba=a(D);f(oa.$$.fragment,Ba),Yv=i(Ba),ou=s(Ba,"P",{});var Zb=a(ou);Jv=c(Zb,"Base class for outputs of models predicting if two sentences are consecutive or not."),Zb.forEach(t),Gv=i(Ba),Qo=s(Ba,"DIV",{class:!0});var qh=a(Qo);f(na.$$.fragment,qh),Kv=i(qh),nu=s(qh,"P",{});var e1=a(nu);Xv=c(e1,"\u201CReturns a new object replacing the specified fields with new values."),e1.forEach(t),qh.forEach(t),Ba.forEach(t),$p=i(e),At=s(e,"H2",{class:!0});var Oh=a(At);Io=s(Oh,"A",{id:!0,class:!0,href:!0});var t1=a(Io);su=s(t1,"SPAN",{});var o1=a(su);f(sa.$$.fragment,o1),o1.forEach(t),t1.forEach(t),Zv=i(Oh),au=s(Oh,"SPAN",{});var n1=a(au);ey=c(n1,"FlaxSequenceClassifierOutput"),n1.forEach(t),Oh.forEach(t),qp=i(e),H=s(e,"DIV",{class:!0});var La=a(H);f(aa.$$.fragment,La),ty=i(La),ru=s(La,"P",{});var s1=a(ru);oy=c(s1,"Base class for outputs of sentence classification models."),s1.forEach(t),ny=i(La),Vo=s(La,"DIV",{class:!0});var Sh=a(Vo);f(ra.$$.fragment,Sh),sy=i(Sh),du=s(Sh,"P",{});var a1=a(du);ay=c(a1,"\u201CReturns a new object replacing the specified fields with new values."),a1.forEach(t),Sh.forEach(t),La.forEach(t),Op=i(e),Ct=s(e,"H2",{class:!0});var Fh=a(Ct);Ro=s(Fh,"A",{id:!0,class:!0,href:!0});var r1=a(Ro);iu=s(r1,"SPAN",{});var d1=a(iu);f(da.$$.fragment,d1),d1.forEach(t),r1.forEach(t),ry=i(Fh),uu=s(Fh,"SPAN",{});var i1=a(uu);dy=c(i1,"FlaxSeq2SeqSequenceClassifierOutput"),i1.forEach(t),Fh.forEach(t),Sp=i(e),Q=s(e,"DIV",{class:!0});var ja=a(Q);f(ia.$$.fragment,ja),iy=i(ja),lu=s(ja,"P",{});var u1=a(lu);uy=c(u1,"Base class for outputs of sequence-to-sequence sentence classification models."),u1.forEach(t),ly=i(ja),Uo=s(ja,"DIV",{class:!0});var Mh=a(Uo);f(ua.$$.fragment,Mh),py=i(Mh),pu=s(Mh,"P",{});var l1=a(pu);cy=c(l1,"\u201CReturns a new object replacing the specified fields with new values."),l1.forEach(t),Mh.forEach(t),ja.forEach(t),Fp=i(e),zt=s(e,"H2",{class:!0});var kh=a(zt);Yo=s(kh,"A",{id:!0,class:!0,href:!0});var p1=a(Yo);cu=s(p1,"SPAN",{});var c1=a(cu);f(la.$$.fragment,c1),c1.forEach(t),p1.forEach(t),hy=i(kh),hu=s(kh,"SPAN",{});var h1=a(hu);fy=c(h1,"FlaxMultipleChoiceModelOutput"),h1.forEach(t),kh.forEach(t),Mp=i(e),I=s(e,"DIV",{class:!0});var Wa=a(I);f(pa.$$.fragment,Wa),my=i(Wa),fu=s(Wa,"P",{});var f1=a(fu);_y=c(f1,"Base class for outputs of multiple choice models."),f1.forEach(t),gy=i(Wa),Jo=s(Wa,"DIV",{class:!0});var Ah=a(Jo);f(ca.$$.fragment,Ah),vy=i(Ah),mu=s(Ah,"P",{});var m1=a(mu);yy=c(m1,"\u201CReturns a new object replacing the specified fields with new values."),m1.forEach(t),Ah.forEach(t),Wa.forEach(t),kp=i(e),Et=s(e,"H2",{class:!0});var Ch=a(Et);Go=s(Ch,"A",{id:!0,class:!0,href:!0});var _1=a(Go);_u=s(_1,"SPAN",{});var g1=a(_u);f(ha.$$.fragment,g1),g1.forEach(t),_1.forEach(t),Ty=i(Ch),gu=s(Ch,"SPAN",{});var v1=a(gu);wy=c(v1,"FlaxTokenClassifierOutput"),v1.forEach(t),Ch.forEach(t),Ap=i(e),V=s(e,"DIV",{class:!0});var Da=a(V);f(fa.$$.fragment,Da),by=i(Da),vu=s(Da,"P",{});var y1=a(vu);xy=c(y1,"Base class for outputs of token classification models."),y1.forEach(t),$y=i(Da),Ko=s(Da,"DIV",{class:!0});var zh=a(Ko);f(ma.$$.fragment,zh),qy=i(zh),yu=s(zh,"P",{});var T1=a(yu);Oy=c(T1,"\u201CReturns a new object replacing the specified fields with new values."),T1.forEach(t),zh.forEach(t),Da.forEach(t),Cp=i(e),Nt=s(e,"H2",{class:!0});var Eh=a(Nt);Xo=s(Eh,"A",{id:!0,class:!0,href:!0});var w1=a(Xo);Tu=s(w1,"SPAN",{});var b1=a(Tu);f(_a.$$.fragment,b1),b1.forEach(t),w1.forEach(t),Sy=i(Eh),wu=s(Eh,"SPAN",{});var x1=a(wu);Fy=c(x1,"FlaxQuestionAnsweringModelOutput"),x1.forEach(t),Eh.forEach(t),zp=i(e),R=s(e,"DIV",{class:!0});var Ha=a(R);f(ga.$$.fragment,Ha),My=i(Ha),bu=s(Ha,"P",{});var $1=a(bu);ky=c($1,"Base class for outputs of question answering models."),$1.forEach(t),Ay=i(Ha),Zo=s(Ha,"DIV",{class:!0});var Nh=a(Zo);f(va.$$.fragment,Nh),Cy=i(Nh),xu=s(Nh,"P",{});var q1=a(xu);zy=c(q1,"\u201CReturns a new object replacing the specified fields with new values."),q1.forEach(t),Nh.forEach(t),Ha.forEach(t),Ep=i(e),Pt=s(e,"H2",{class:!0});var Ph=a(Pt);en=s(Ph,"A",{id:!0,class:!0,href:!0});var O1=a(en);$u=s(O1,"SPAN",{});var S1=a($u);f(ya.$$.fragment,S1),S1.forEach(t),O1.forEach(t),Ey=i(Ph),qu=s(Ph,"SPAN",{});var F1=a(qu);Ny=c(F1,"FlaxSeq2SeqQuestionAnsweringModelOutput"),F1.forEach(t),Ph.forEach(t),Np=i(e),U=s(e,"DIV",{class:!0});var Qa=a(U);f(Ta.$$.fragment,Qa),Py=i(Qa),Ou=s(Qa,"P",{});var M1=a(Ou);By=c(M1,"Base class for outputs of sequence-to-sequence question answering models."),M1.forEach(t),Ly=i(Qa),tn=s(Qa,"DIV",{class:!0});var Bh=a(tn);f(wa.$$.fragment,Bh),jy=i(Bh),Su=s(Bh,"P",{});var k1=a(Su);Wy=c(k1,"\u201CReturns a new object replacing the specified fields with new values."),k1.forEach(t),Bh.forEach(t),Qa.forEach(t),this.h()},h(){r(x,"name","hf:doc:metadata"),r(x,"content",JSON.stringify(W1)),r(A,"id","model-outputs"),r(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(A,"href","#model-outputs"),r($,"class","relative group"),r($a,"href","/docs/transformers/pr_17253/en/main_classes/output#transformers.utils.ModelOutput"),r(Oa,"href","/docs/transformers/pr_17253/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"),r(jt,"id","transformers.utils.ModelOutput"),r(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(jt,"href","#transformers.utils.ModelOutput"),r(ee,"class","relative group"),r(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ht,"id","transformers.modeling_outputs.BaseModelOutput"),r(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ht,"href","#transformers.modeling_outputs.BaseModelOutput"),r(oe,"class","relative group"),r(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Qt,"id","transformers.modeling_outputs.BaseModelOutputWithPooling"),r(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Qt,"href","#transformers.modeling_outputs.BaseModelOutputWithPooling"),r(se,"class","relative group"),r(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(It,"id","transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(It,"href","#transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(re,"class","relative group"),r(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Vt,"id","transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Vt,"href","#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(ie,"class","relative group"),r(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Rt,"id","transformers.modeling_outputs.BaseModelOutputWithPast"),r(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Rt,"href","#transformers.modeling_outputs.BaseModelOutputWithPast"),r(le,"class","relative group"),r(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ut,"id","transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ut,"href","#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(ce,"class","relative group"),r(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Yt,"id","transformers.modeling_outputs.Seq2SeqModelOutput"),r(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Yt,"href","#transformers.modeling_outputs.Seq2SeqModelOutput"),r(fe,"class","relative group"),r(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Jt,"id","transformers.modeling_outputs.CausalLMOutput"),r(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Jt,"href","#transformers.modeling_outputs.CausalLMOutput"),r(_e,"class","relative group"),r(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Gt,"id","transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Gt,"href","#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(ve,"class","relative group"),r(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Kt,"id","transformers.modeling_outputs.CausalLMOutputWithPast"),r(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Kt,"href","#transformers.modeling_outputs.CausalLMOutputWithPast"),r(Te,"class","relative group"),r(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Xt,"id","transformers.modeling_outputs.MaskedLMOutput"),r(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Xt,"href","#transformers.modeling_outputs.MaskedLMOutput"),r(be,"class","relative group"),r(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Zt,"id","transformers.modeling_outputs.Seq2SeqLMOutput"),r(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Zt,"href","#transformers.modeling_outputs.Seq2SeqLMOutput"),r($e,"class","relative group"),r(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(eo,"id","transformers.modeling_outputs.NextSentencePredictorOutput"),r(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(eo,"href","#transformers.modeling_outputs.NextSentencePredictorOutput"),r(Oe,"class","relative group"),r(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(to,"id","transformers.modeling_outputs.SequenceClassifierOutput"),r(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(to,"href","#transformers.modeling_outputs.SequenceClassifierOutput"),r(Fe,"class","relative group"),r(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(oo,"id","transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(oo,"href","#transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(ke,"class","relative group"),r(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(no,"id","transformers.modeling_outputs.MultipleChoiceModelOutput"),r(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(no,"href","#transformers.modeling_outputs.MultipleChoiceModelOutput"),r(Ce,"class","relative group"),r(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(so,"id","transformers.modeling_outputs.TokenClassifierOutput"),r(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(so,"href","#transformers.modeling_outputs.TokenClassifierOutput"),r(Ee,"class","relative group"),r(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ao,"id","transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ao,"href","#transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(Pe,"class","relative group"),r(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ro,"id","transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ro,"href","#transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(Le,"class","relative group"),r(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(io,"id","transformers.modeling_tf_outputs.TFBaseModelOutput"),r(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(io,"href","#transformers.modeling_tf_outputs.TFBaseModelOutput"),r(We,"class","relative group"),r(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(uo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(uo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(He,"class","relative group"),r(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(lo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(lo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(Ie,"class","relative group"),r(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(po,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(po,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(Re,"class","relative group"),r(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(co,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(co,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(Ye,"class","relative group"),r(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ho,"id","transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ho,"href","#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(Ge,"class","relative group"),r(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(fo,"id","transformers.modeling_tf_outputs.TFCausalLMOutput"),r(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(fo,"href","#transformers.modeling_tf_outputs.TFCausalLMOutput"),r(Xe,"class","relative group"),r(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(mo,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(mo,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(et,"class","relative group"),r(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(_o,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(_o,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(ot,"class","relative group"),r(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(go,"id","transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(go,"href","#transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(st,"class","relative group"),r(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(vo,"id","transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(vo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(rt,"class","relative group"),r(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(yo,"id","transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(yo,"href","#transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(it,"class","relative group"),r(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(To,"id","transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(To,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(To,"href","#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(lt,"class","relative group"),r(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(wo,"id","transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(wo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r(ct,"class","relative group"),r(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(bo,"id","transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(bo,"href","#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(ft,"class","relative group"),r(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(xo,"id","transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(xo,"href","#transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r(_t,"class","relative group"),r(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r($o,"id","transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r($o,"href","#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r(vt,"class","relative group"),r(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(qo,"id","transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(qo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(Tt,"class","relative group"),r(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Oo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(Oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Oo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(bt,"class","relative group"),r(So,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Fo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Fo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r(xt,"class","relative group"),r(Mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ko,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r(ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ko,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r($t,"class","relative group"),r(Ao,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Co,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Co,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(qt,"class","relative group"),r(zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Eo,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Eo,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(Ot,"class","relative group"),r(No,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Po,"id","transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(Po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Po,"href","#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(St,"class","relative group"),r(Bo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Lo,"id","transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(Lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Lo,"href","#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(Ft,"class","relative group"),r(jo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Wo,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(Wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Wo,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(Mt,"class","relative group"),r(Do,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ho,"id","transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(Ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ho,"href","#transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(kt,"class","relative group"),r(Qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Io,"id","transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Io,"href","#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(At,"class","relative group"),r(Vo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Ro,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(Ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ro,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(Ct,"class","relative group"),r(Uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Yo,"id","transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(Yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Yo,"href","#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(zt,"class","relative group"),r(Jo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Go,"id","transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(Go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Go,"href","#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(Et,"class","relative group"),r(Ko,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Xo,"id","transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(Xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Xo,"href","#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(Nt,"class","relative group"),r(Zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(en,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(en,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(Pt,"class","relative group"),r(tn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,u){o(document.head,x),l(e,Bt,u),l(e,$,u),o($,A),o(A,K),m(q,K,null),o($,nn),o($,X),o(X,Z),l(e,M,u),l(e,C,u),o(C,xa),o(C,$a),o($a,Lh),o(C,jh),l(e,Mu,u),l(e,qa,u),o(qa,Wh),l(e,ku,u),m(sn,e,u),l(e,Au,u),l(e,w,u),o(w,Dh),o(w,Ia),o(Ia,Hh),o(w,Qh),o(w,Oa),o(Oa,Ih),o(w,Vh),o(w,Va),o(Va,Rh),o(w,Uh),o(w,Ra),o(Ra,Yh),o(w,Jh),o(w,Ua),o(Ua,Gh),o(w,Kh),o(w,Ya),o(Ya,Xh),o(w,Zh),o(w,Ja),o(Ja,ef),o(w,tf),o(w,Ga),o(Ga,of),o(w,nf),o(w,Ka),o(Ka,sf),o(w,af),o(w,Xa),o(Xa,rf),o(w,df),o(w,Za),o(Za,uf),o(w,lf),o(w,er),o(er,pf),o(w,cf),l(e,Cu,u),l(e,O,u),o(O,hf),o(O,tr),o(tr,ff),o(O,mf),o(O,or),o(or,_f),o(O,gf),o(O,nr),o(nr,vf),o(O,yf),o(O,sr),o(sr,Tf),o(O,wf),l(e,zu,u),l(e,S,u),o(S,bf),o(S,ar),o(ar,xf),o(S,$f),o(S,rr),o(rr,qf),o(S,Of),o(S,dr),o(dr,Sf),o(S,Ff),o(S,ir),o(ir,Mf),o(S,kf),l(e,Eu,u),m(an,e,u),l(e,Nu,u),l(e,Lt,u),o(Lt,Af),o(Lt,ur),o(ur,Cf),o(Lt,zf),l(e,Pu,u),l(e,F,u),o(F,Ef),o(F,lr),o(lr,Nf),o(F,Pf),o(F,pr),o(pr,Bf),o(F,Lf),o(F,cr),o(cr,jf),o(F,Wf),o(F,hr),o(hr,Df),o(F,Hf),l(e,Bu,u),l(e,Sa,u),o(Sa,Qf),l(e,Lu,u),l(e,ee,u),o(ee,jt),o(jt,fr),m(rn,fr,null),o(ee,If),o(ee,mr),o(mr,Vf),l(e,ju,u),l(e,k,u),m(dn,k,null),o(k,Rf),o(k,te),o(te,Uf),o(te,_r),o(_r,Yf),o(te,Jf),o(te,gr),o(gr,Gf),o(te,Kf),o(k,Xf),m(Wt,k,null),o(k,Zf),o(k,Dt),m(un,Dt,null),o(Dt,em),o(Dt,ln),o(ln,tm),o(ln,vr),o(vr,om),o(ln,nm),l(e,Wu,u),l(e,oe,u),o(oe,Ht),o(Ht,yr),m(pn,yr,null),o(oe,sm),o(oe,Tr),o(Tr,am),l(e,Du,u),l(e,ne,u),m(cn,ne,null),o(ne,rm),o(ne,wr),o(wr,dm),l(e,Hu,u),l(e,se,u),o(se,Qt),o(Qt,br),m(hn,br,null),o(se,im),o(se,xr),o(xr,um),l(e,Qu,u),l(e,ae,u),m(fn,ae,null),o(ae,lm),o(ae,$r),o($r,pm),l(e,Iu,u),l(e,re,u),o(re,It),o(It,qr),m(mn,qr,null),o(re,cm),o(re,Or),o(Or,hm),l(e,Vu,u),l(e,de,u),m(_n,de,null),o(de,fm),o(de,Sr),o(Sr,mm),l(e,Ru,u),l(e,ie,u),o(ie,Vt),o(Vt,Fr),m(gn,Fr,null),o(ie,_m),o(ie,Mr),o(Mr,gm),l(e,Uu,u),l(e,ue,u),m(vn,ue,null),o(ue,vm),o(ue,kr),o(kr,ym),l(e,Yu,u),l(e,le,u),o(le,Rt),o(Rt,Ar),m(yn,Ar,null),o(le,Tm),o(le,Cr),o(Cr,wm),l(e,Ju,u),l(e,pe,u),m(Tn,pe,null),o(pe,bm),o(pe,zr),o(zr,xm),l(e,Gu,u),l(e,ce,u),o(ce,Ut),o(Ut,Er),m(wn,Er,null),o(ce,$m),o(ce,Nr),o(Nr,qm),l(e,Ku,u),l(e,he,u),m(bn,he,null),o(he,Om),o(he,Pr),o(Pr,Sm),l(e,Xu,u),l(e,fe,u),o(fe,Yt),o(Yt,Br),m(xn,Br,null),o(fe,Fm),o(fe,Lr),o(Lr,Mm),l(e,Zu,u),l(e,me,u),m($n,me,null),o(me,km),o(me,jr),o(jr,Am),l(e,el,u),l(e,_e,u),o(_e,Jt),o(Jt,Wr),m(qn,Wr,null),o(_e,Cm),o(_e,Dr),o(Dr,zm),l(e,tl,u),l(e,ge,u),m(On,ge,null),o(ge,Em),o(ge,Hr),o(Hr,Nm),l(e,ol,u),l(e,ve,u),o(ve,Gt),o(Gt,Qr),m(Sn,Qr,null),o(ve,Pm),o(ve,Ir),o(Ir,Bm),l(e,nl,u),l(e,ye,u),m(Fn,ye,null),o(ye,Lm),o(ye,Vr),o(Vr,jm),l(e,sl,u),l(e,Te,u),o(Te,Kt),o(Kt,Rr),m(Mn,Rr,null),o(Te,Wm),o(Te,Ur),o(Ur,Dm),l(e,al,u),l(e,we,u),m(kn,we,null),o(we,Hm),o(we,Yr),o(Yr,Qm),l(e,rl,u),l(e,be,u),o(be,Xt),o(Xt,Jr),m(An,Jr,null),o(be,Im),o(be,Gr),o(Gr,Vm),l(e,dl,u),l(e,xe,u),m(Cn,xe,null),o(xe,Rm),o(xe,Kr),o(Kr,Um),l(e,il,u),l(e,$e,u),o($e,Zt),o(Zt,Xr),m(zn,Xr,null),o($e,Ym),o($e,Zr),o(Zr,Jm),l(e,ul,u),l(e,qe,u),m(En,qe,null),o(qe,Gm),o(qe,ed),o(ed,Km),l(e,ll,u),l(e,Oe,u),o(Oe,eo),o(eo,td),m(Nn,td,null),o(Oe,Xm),o(Oe,od),o(od,Zm),l(e,pl,u),l(e,Se,u),m(Pn,Se,null),o(Se,e_),o(Se,nd),o(nd,t_),l(e,cl,u),l(e,Fe,u),o(Fe,to),o(to,sd),m(Bn,sd,null),o(Fe,o_),o(Fe,ad),o(ad,n_),l(e,hl,u),l(e,Me,u),m(Ln,Me,null),o(Me,s_),o(Me,rd),o(rd,a_),l(e,fl,u),l(e,ke,u),o(ke,oo),o(oo,dd),m(jn,dd,null),o(ke,r_),o(ke,id),o(id,d_),l(e,ml,u),l(e,Ae,u),m(Wn,Ae,null),o(Ae,i_),o(Ae,ud),o(ud,u_),l(e,_l,u),l(e,Ce,u),o(Ce,no),o(no,ld),m(Dn,ld,null),o(Ce,l_),o(Ce,pd),o(pd,p_),l(e,gl,u),l(e,ze,u),m(Hn,ze,null),o(ze,c_),o(ze,cd),o(cd,h_),l(e,vl,u),l(e,Ee,u),o(Ee,so),o(so,hd),m(Qn,hd,null),o(Ee,f_),o(Ee,fd),o(fd,m_),l(e,yl,u),l(e,Ne,u),m(In,Ne,null),o(Ne,__),o(Ne,md),o(md,g_),l(e,Tl,u),l(e,Pe,u),o(Pe,ao),o(ao,_d),m(Vn,_d,null),o(Pe,v_),o(Pe,gd),o(gd,y_),l(e,wl,u),l(e,Be,u),m(Rn,Be,null),o(Be,T_),o(Be,vd),o(vd,w_),l(e,bl,u),l(e,Le,u),o(Le,ro),o(ro,yd),m(Un,yd,null),o(Le,b_),o(Le,Td),o(Td,x_),l(e,xl,u),l(e,je,u),m(Yn,je,null),o(je,$_),o(je,wd),o(wd,q_),l(e,$l,u),l(e,We,u),o(We,io),o(io,bd),m(Jn,bd,null),o(We,O_),o(We,xd),o(xd,S_),l(e,ql,u),l(e,De,u),m(Gn,De,null),o(De,F_),o(De,$d),o($d,M_),l(e,Ol,u),l(e,He,u),o(He,uo),o(uo,qd),m(Kn,qd,null),o(He,k_),o(He,Od),o(Od,A_),l(e,Sl,u),l(e,Qe,u),m(Xn,Qe,null),o(Qe,C_),o(Qe,Sd),o(Sd,z_),l(e,Fl,u),l(e,Ie,u),o(Ie,lo),o(lo,Fd),m(Zn,Fd,null),o(Ie,E_),o(Ie,Md),o(Md,N_),l(e,Ml,u),l(e,Ve,u),m(es,Ve,null),o(Ve,P_),o(Ve,kd),o(kd,B_),l(e,kl,u),l(e,Re,u),o(Re,po),o(po,Ad),m(ts,Ad,null),o(Re,L_),o(Re,Cd),o(Cd,j_),l(e,Al,u),l(e,Ue,u),m(os,Ue,null),o(Ue,W_),o(Ue,zd),o(zd,D_),l(e,Cl,u),l(e,Ye,u),o(Ye,co),o(co,Ed),m(ns,Ed,null),o(Ye,H_),o(Ye,Nd),o(Nd,Q_),l(e,zl,u),l(e,Je,u),m(ss,Je,null),o(Je,I_),o(Je,Pd),o(Pd,V_),l(e,El,u),l(e,Ge,u),o(Ge,ho),o(ho,Bd),m(as,Bd,null),o(Ge,R_),o(Ge,Ld),o(Ld,U_),l(e,Nl,u),l(e,Ke,u),m(rs,Ke,null),o(Ke,Y_),o(Ke,jd),o(jd,J_),l(e,Pl,u),l(e,Xe,u),o(Xe,fo),o(fo,Wd),m(ds,Wd,null),o(Xe,G_),o(Xe,Dd),o(Dd,K_),l(e,Bl,u),l(e,Ze,u),m(is,Ze,null),o(Ze,X_),o(Ze,Hd),o(Hd,Z_),l(e,Ll,u),l(e,et,u),o(et,mo),o(mo,Qd),m(us,Qd,null),o(et,eg),o(et,Id),o(Id,tg),l(e,jl,u),l(e,tt,u),m(ls,tt,null),o(tt,og),o(tt,Vd),o(Vd,ng),l(e,Wl,u),l(e,ot,u),o(ot,_o),o(_o,Rd),m(ps,Rd,null),o(ot,sg),o(ot,Ud),o(Ud,ag),l(e,Dl,u),l(e,nt,u),m(cs,nt,null),o(nt,rg),o(nt,Yd),o(Yd,dg),l(e,Hl,u),l(e,st,u),o(st,go),o(go,Jd),m(hs,Jd,null),o(st,ig),o(st,Gd),o(Gd,ug),l(e,Ql,u),l(e,at,u),m(fs,at,null),o(at,lg),o(at,Kd),o(Kd,pg),l(e,Il,u),l(e,rt,u),o(rt,vo),o(vo,Xd),m(ms,Xd,null),o(rt,cg),o(rt,Zd),o(Zd,hg),l(e,Vl,u),l(e,dt,u),m(_s,dt,null),o(dt,fg),o(dt,ei),o(ei,mg),l(e,Rl,u),l(e,it,u),o(it,yo),o(yo,ti),m(gs,ti,null),o(it,_g),o(it,oi),o(oi,gg),l(e,Ul,u),l(e,ut,u),m(vs,ut,null),o(ut,vg),o(ut,ni),o(ni,yg),l(e,Yl,u),l(e,lt,u),o(lt,To),o(To,si),m(ys,si,null),o(lt,Tg),o(lt,ai),o(ai,wg),l(e,Jl,u),l(e,pt,u),m(Ts,pt,null),o(pt,bg),o(pt,ri),o(ri,xg),l(e,Gl,u),l(e,ct,u),o(ct,wo),o(wo,di),m(ws,di,null),o(ct,$g),o(ct,ii),o(ii,qg),l(e,Kl,u),l(e,ht,u),m(bs,ht,null),o(ht,Og),o(ht,ui),o(ui,Sg),l(e,Xl,u),l(e,ft,u),o(ft,bo),o(bo,li),m(xs,li,null),o(ft,Fg),o(ft,pi),o(pi,Mg),l(e,Zl,u),l(e,mt,u),m($s,mt,null),o(mt,kg),o(mt,ci),o(ci,Ag),l(e,ep,u),l(e,_t,u),o(_t,xo),o(xo,hi),m(qs,hi,null),o(_t,Cg),o(_t,fi),o(fi,zg),l(e,tp,u),l(e,gt,u),m(Os,gt,null),o(gt,Eg),o(gt,mi),o(mi,Ng),l(e,op,u),l(e,vt,u),o(vt,$o),o($o,_i),m(Ss,_i,null),o(vt,Pg),o(vt,gi),o(gi,Bg),l(e,np,u),l(e,yt,u),m(Fs,yt,null),o(yt,Lg),o(yt,vi),o(vi,jg),l(e,sp,u),l(e,Tt,u),o(Tt,qo),o(qo,yi),m(Ms,yi,null),o(Tt,Wg),o(Tt,Ti),o(Ti,Dg),l(e,ap,u),l(e,wt,u),m(ks,wt,null),o(wt,Hg),o(wt,wi),o(wi,Qg),l(e,rp,u),l(e,bt,u),o(bt,Oo),o(Oo,bi),m(As,bi,null),o(bt,Ig),o(bt,xi),o(xi,Vg),l(e,dp,u),l(e,z,u),m(Cs,z,null),o(z,Rg),o(z,$i),o($i,Ug),o(z,Yg),o(z,So),m(zs,So,null),o(So,Jg),o(So,qi),o(qi,Gg),l(e,ip,u),l(e,xt,u),o(xt,Fo),o(Fo,Oi),m(Es,Oi,null),o(xt,Kg),o(xt,Si),o(Si,Xg),l(e,up,u),l(e,E,u),m(Ns,E,null),o(E,Zg),o(E,Fi),o(Fi,ev),o(E,tv),o(E,Mo),m(Ps,Mo,null),o(Mo,ov),o(Mo,Mi),o(Mi,nv),l(e,lp,u),l(e,$t,u),o($t,ko),o(ko,ki),m(Bs,ki,null),o($t,sv),o($t,Ai),o(Ai,av),l(e,pp,u),l(e,N,u),m(Ls,N,null),o(N,rv),o(N,Ci),o(Ci,dv),o(N,iv),o(N,Ao),m(js,Ao,null),o(Ao,uv),o(Ao,zi),o(zi,lv),l(e,cp,u),l(e,qt,u),o(qt,Co),o(Co,Ei),m(Ws,Ei,null),o(qt,pv),o(qt,Ni),o(Ni,cv),l(e,hp,u),l(e,P,u),m(Ds,P,null),o(P,hv),o(P,Pi),o(Pi,fv),o(P,mv),o(P,zo),m(Hs,zo,null),o(zo,_v),o(zo,Bi),o(Bi,gv),l(e,fp,u),l(e,Ot,u),o(Ot,Eo),o(Eo,Li),m(Qs,Li,null),o(Ot,vv),o(Ot,ji),o(ji,yv),l(e,mp,u),l(e,B,u),m(Is,B,null),o(B,Tv),o(B,Wi),o(Wi,wv),o(B,bv),o(B,No),m(Vs,No,null),o(No,xv),o(No,Di),o(Di,$v),l(e,_p,u),l(e,St,u),o(St,Po),o(Po,Hi),m(Rs,Hi,null),o(St,qv),o(St,Qi),o(Qi,Ov),l(e,gp,u),l(e,L,u),m(Us,L,null),o(L,Sv),o(L,Ii),o(Ii,Fv),o(L,Mv),o(L,Bo),m(Ys,Bo,null),o(Bo,kv),o(Bo,Vi),o(Vi,Av),l(e,vp,u),l(e,Ft,u),o(Ft,Lo),o(Lo,Ri),m(Js,Ri,null),o(Ft,Cv),o(Ft,Ui),o(Ui,zv),l(e,yp,u),l(e,j,u),m(Gs,j,null),o(j,Ev),o(j,Yi),o(Yi,Nv),o(j,Pv),o(j,jo),m(Ks,jo,null),o(jo,Bv),o(jo,Ji),o(Ji,Lv),l(e,Tp,u),l(e,Mt,u),o(Mt,Wo),o(Wo,Gi),m(Xs,Gi,null),o(Mt,jv),o(Mt,Ki),o(Ki,Wv),l(e,wp,u),l(e,W,u),m(Zs,W,null),o(W,Dv),o(W,Xi),o(Xi,Hv),o(W,Qv),o(W,Do),m(ea,Do,null),o(Do,Iv),o(Do,Zi),o(Zi,Vv),l(e,bp,u),l(e,kt,u),o(kt,Ho),o(Ho,eu),m(ta,eu,null),o(kt,Rv),o(kt,tu),o(tu,Uv),l(e,xp,u),l(e,D,u),m(oa,D,null),o(D,Yv),o(D,ou),o(ou,Jv),o(D,Gv),o(D,Qo),m(na,Qo,null),o(Qo,Kv),o(Qo,nu),o(nu,Xv),l(e,$p,u),l(e,At,u),o(At,Io),o(Io,su),m(sa,su,null),o(At,Zv),o(At,au),o(au,ey),l(e,qp,u),l(e,H,u),m(aa,H,null),o(H,ty),o(H,ru),o(ru,oy),o(H,ny),o(H,Vo),m(ra,Vo,null),o(Vo,sy),o(Vo,du),o(du,ay),l(e,Op,u),l(e,Ct,u),o(Ct,Ro),o(Ro,iu),m(da,iu,null),o(Ct,ry),o(Ct,uu),o(uu,dy),l(e,Sp,u),l(e,Q,u),m(ia,Q,null),o(Q,iy),o(Q,lu),o(lu,uy),o(Q,ly),o(Q,Uo),m(ua,Uo,null),o(Uo,py),o(Uo,pu),o(pu,cy),l(e,Fp,u),l(e,zt,u),o(zt,Yo),o(Yo,cu),m(la,cu,null),o(zt,hy),o(zt,hu),o(hu,fy),l(e,Mp,u),l(e,I,u),m(pa,I,null),o(I,my),o(I,fu),o(fu,_y),o(I,gy),o(I,Jo),m(ca,Jo,null),o(Jo,vy),o(Jo,mu),o(mu,yy),l(e,kp,u),l(e,Et,u),o(Et,Go),o(Go,_u),m(ha,_u,null),o(Et,Ty),o(Et,gu),o(gu,wy),l(e,Ap,u),l(e,V,u),m(fa,V,null),o(V,by),o(V,vu),o(vu,xy),o(V,$y),o(V,Ko),m(ma,Ko,null),o(Ko,qy),o(Ko,yu),o(yu,Oy),l(e,Cp,u),l(e,Nt,u),o(Nt,Xo),o(Xo,Tu),m(_a,Tu,null),o(Nt,Sy),o(Nt,wu),o(wu,Fy),l(e,zp,u),l(e,R,u),m(ga,R,null),o(R,My),o(R,bu),o(bu,ky),o(R,Ay),o(R,Zo),m(va,Zo,null),o(Zo,Cy),o(Zo,xu),o(xu,zy),l(e,Ep,u),l(e,Pt,u),o(Pt,en),o(en,$u),m(ya,$u,null),o(Pt,Ey),o(Pt,qu),o(qu,Ny),l(e,Np,u),l(e,U,u),m(Ta,U,null),o(U,Py),o(U,Ou),o(Ou,By),o(U,Ly),o(U,tn),m(wa,tn,null),o(tn,jy),o(tn,Su),o(Su,Wy),Pp=!0},p(e,[u]){const ba={};u&2&&(ba.$$scope={dirty:u,ctx:e}),Wt.$set(ba)},i(e){Pp||(_(q.$$.fragment,e),_(sn.$$.fragment,e),_(an.$$.fragment,e),_(rn.$$.fragment,e),_(dn.$$.fragment,e),_(Wt.$$.fragment,e),_(un.$$.fragment,e),_(pn.$$.fragment,e),_(cn.$$.fragment,e),_(hn.$$.fragment,e),_(fn.$$.fragment,e),_(mn.$$.fragment,e),_(_n.$$.fragment,e),_(gn.$$.fragment,e),_(vn.$$.fragment,e),_(yn.$$.fragment,e),_(Tn.$$.fragment,e),_(wn.$$.fragment,e),_(bn.$$.fragment,e),_(xn.$$.fragment,e),_($n.$$.fragment,e),_(qn.$$.fragment,e),_(On.$$.fragment,e),_(Sn.$$.fragment,e),_(Fn.$$.fragment,e),_(Mn.$$.fragment,e),_(kn.$$.fragment,e),_(An.$$.fragment,e),_(Cn.$$.fragment,e),_(zn.$$.fragment,e),_(En.$$.fragment,e),_(Nn.$$.fragment,e),_(Pn.$$.fragment,e),_(Bn.$$.fragment,e),_(Ln.$$.fragment,e),_(jn.$$.fragment,e),_(Wn.$$.fragment,e),_(Dn.$$.fragment,e),_(Hn.$$.fragment,e),_(Qn.$$.fragment,e),_(In.$$.fragment,e),_(Vn.$$.fragment,e),_(Rn.$$.fragment,e),_(Un.$$.fragment,e),_(Yn.$$.fragment,e),_(Jn.$$.fragment,e),_(Gn.$$.fragment,e),_(Kn.$$.fragment,e),_(Xn.$$.fragment,e),_(Zn.$$.fragment,e),_(es.$$.fragment,e),_(ts.$$.fragment,e),_(os.$$.fragment,e),_(ns.$$.fragment,e),_(ss.$$.fragment,e),_(as.$$.fragment,e),_(rs.$$.fragment,e),_(ds.$$.fragment,e),_(is.$$.fragment,e),_(us.$$.fragment,e),_(ls.$$.fragment,e),_(ps.$$.fragment,e),_(cs.$$.fragment,e),_(hs.$$.fragment,e),_(fs.$$.fragment,e),_(ms.$$.fragment,e),_(_s.$$.fragment,e),_(gs.$$.fragment,e),_(vs.$$.fragment,e),_(ys.$$.fragment,e),_(Ts.$$.fragment,e),_(ws.$$.fragment,e),_(bs.$$.fragment,e),_(xs.$$.fragment,e),_($s.$$.fragment,e),_(qs.$$.fragment,e),_(Os.$$.fragment,e),_(Ss.$$.fragment,e),_(Fs.$$.fragment,e),_(Ms.$$.fragment,e),_(ks.$$.fragment,e),_(As.$$.fragment,e),_(Cs.$$.fragment,e),_(zs.$$.fragment,e),_(Es.$$.fragment,e),_(Ns.$$.fragment,e),_(Ps.$$.fragment,e),_(Bs.$$.fragment,e),_(Ls.$$.fragment,e),_(js.$$.fragment,e),_(Ws.$$.fragment,e),_(Ds.$$.fragment,e),_(Hs.$$.fragment,e),_(Qs.$$.fragment,e),_(Is.$$.fragment,e),_(Vs.$$.fragment,e),_(Rs.$$.fragment,e),_(Us.$$.fragment,e),_(Ys.$$.fragment,e),_(Js.$$.fragment,e),_(Gs.$$.fragment,e),_(Ks.$$.fragment,e),_(Xs.$$.fragment,e),_(Zs.$$.fragment,e),_(ea.$$.fragment,e),_(ta.$$.fragment,e),_(oa.$$.fragment,e),_(na.$$.fragment,e),_(sa.$$.fragment,e),_(aa.$$.fragment,e),_(ra.$$.fragment,e),_(da.$$.fragment,e),_(ia.$$.fragment,e),_(ua.$$.fragment,e),_(la.$$.fragment,e),_(pa.$$.fragment,e),_(ca.$$.fragment,e),_(ha.$$.fragment,e),_(fa.$$.fragment,e),_(ma.$$.fragment,e),_(_a.$$.fragment,e),_(ga.$$.fragment,e),_(va.$$.fragment,e),_(ya.$$.fragment,e),_(Ta.$$.fragment,e),_(wa.$$.fragment,e),Pp=!0)},o(e){g(q.$$.fragment,e),g(sn.$$.fragment,e),g(an.$$.fragment,e),g(rn.$$.fragment,e),g(dn.$$.fragment,e),g(Wt.$$.fragment,e),g(un.$$.fragment,e),g(pn.$$.fragment,e),g(cn.$$.fragment,e),g(hn.$$.fragment,e),g(fn.$$.fragment,e),g(mn.$$.fragment,e),g(_n.$$.fragment,e),g(gn.$$.fragment,e),g(vn.$$.fragment,e),g(yn.$$.fragment,e),g(Tn.$$.fragment,e),g(wn.$$.fragment,e),g(bn.$$.fragment,e),g(xn.$$.fragment,e),g($n.$$.fragment,e),g(qn.$$.fragment,e),g(On.$$.fragment,e),g(Sn.$$.fragment,e),g(Fn.$$.fragment,e),g(Mn.$$.fragment,e),g(kn.$$.fragment,e),g(An.$$.fragment,e),g(Cn.$$.fragment,e),g(zn.$$.fragment,e),g(En.$$.fragment,e),g(Nn.$$.fragment,e),g(Pn.$$.fragment,e),g(Bn.$$.fragment,e),g(Ln.$$.fragment,e),g(jn.$$.fragment,e),g(Wn.$$.fragment,e),g(Dn.$$.fragment,e),g(Hn.$$.fragment,e),g(Qn.$$.fragment,e),g(In.$$.fragment,e),g(Vn.$$.fragment,e),g(Rn.$$.fragment,e),g(Un.$$.fragment,e),g(Yn.$$.fragment,e),g(Jn.$$.fragment,e),g(Gn.$$.fragment,e),g(Kn.$$.fragment,e),g(Xn.$$.fragment,e),g(Zn.$$.fragment,e),g(es.$$.fragment,e),g(ts.$$.fragment,e),g(os.$$.fragment,e),g(ns.$$.fragment,e),g(ss.$$.fragment,e),g(as.$$.fragment,e),g(rs.$$.fragment,e),g(ds.$$.fragment,e),g(is.$$.fragment,e),g(us.$$.fragment,e),g(ls.$$.fragment,e),g(ps.$$.fragment,e),g(cs.$$.fragment,e),g(hs.$$.fragment,e),g(fs.$$.fragment,e),g(ms.$$.fragment,e),g(_s.$$.fragment,e),g(gs.$$.fragment,e),g(vs.$$.fragment,e),g(ys.$$.fragment,e),g(Ts.$$.fragment,e),g(ws.$$.fragment,e),g(bs.$$.fragment,e),g(xs.$$.fragment,e),g($s.$$.fragment,e),g(qs.$$.fragment,e),g(Os.$$.fragment,e),g(Ss.$$.fragment,e),g(Fs.$$.fragment,e),g(Ms.$$.fragment,e),g(ks.$$.fragment,e),g(As.$$.fragment,e),g(Cs.$$.fragment,e),g(zs.$$.fragment,e),g(Es.$$.fragment,e),g(Ns.$$.fragment,e),g(Ps.$$.fragment,e),g(Bs.$$.fragment,e),g(Ls.$$.fragment,e),g(js.$$.fragment,e),g(Ws.$$.fragment,e),g(Ds.$$.fragment,e),g(Hs.$$.fragment,e),g(Qs.$$.fragment,e),g(Is.$$.fragment,e),g(Vs.$$.fragment,e),g(Rs.$$.fragment,e),g(Us.$$.fragment,e),g(Ys.$$.fragment,e),g(Js.$$.fragment,e),g(Gs.$$.fragment,e),g(Ks.$$.fragment,e),g(Xs.$$.fragment,e),g(Zs.$$.fragment,e),g(ea.$$.fragment,e),g(ta.$$.fragment,e),g(oa.$$.fragment,e),g(na.$$.fragment,e),g(sa.$$.fragment,e),g(aa.$$.fragment,e),g(ra.$$.fragment,e),g(da.$$.fragment,e),g(ia.$$.fragment,e),g(ua.$$.fragment,e),g(la.$$.fragment,e),g(pa.$$.fragment,e),g(ca.$$.fragment,e),g(ha.$$.fragment,e),g(fa.$$.fragment,e),g(ma.$$.fragment,e),g(_a.$$.fragment,e),g(ga.$$.fragment,e),g(va.$$.fragment,e),g(ya.$$.fragment,e),g(Ta.$$.fragment,e),g(wa.$$.fragment,e),Pp=!1},d(e){t(x),e&&t(Bt),e&&t($),v(q),e&&t(M),e&&t(C),e&&t(Mu),e&&t(qa),e&&t(ku),v(sn,e),e&&t(Au),e&&t(w),e&&t(Cu),e&&t(O),e&&t(zu),e&&t(S),e&&t(Eu),v(an,e),e&&t(Nu),e&&t(Lt),e&&t(Pu),e&&t(F),e&&t(Bu),e&&t(Sa),e&&t(Lu),e&&t(ee),v(rn),e&&t(ju),e&&t(k),v(dn),v(Wt),v(un),e&&t(Wu),e&&t(oe),v(pn),e&&t(Du),e&&t(ne),v(cn),e&&t(Hu),e&&t(se),v(hn),e&&t(Qu),e&&t(ae),v(fn),e&&t(Iu),e&&t(re),v(mn),e&&t(Vu),e&&t(de),v(_n),e&&t(Ru),e&&t(ie),v(gn),e&&t(Uu),e&&t(ue),v(vn),e&&t(Yu),e&&t(le),v(yn),e&&t(Ju),e&&t(pe),v(Tn),e&&t(Gu),e&&t(ce),v(wn),e&&t(Ku),e&&t(he),v(bn),e&&t(Xu),e&&t(fe),v(xn),e&&t(Zu),e&&t(me),v($n),e&&t(el),e&&t(_e),v(qn),e&&t(tl),e&&t(ge),v(On),e&&t(ol),e&&t(ve),v(Sn),e&&t(nl),e&&t(ye),v(Fn),e&&t(sl),e&&t(Te),v(Mn),e&&t(al),e&&t(we),v(kn),e&&t(rl),e&&t(be),v(An),e&&t(dl),e&&t(xe),v(Cn),e&&t(il),e&&t($e),v(zn),e&&t(ul),e&&t(qe),v(En),e&&t(ll),e&&t(Oe),v(Nn),e&&t(pl),e&&t(Se),v(Pn),e&&t(cl),e&&t(Fe),v(Bn),e&&t(hl),e&&t(Me),v(Ln),e&&t(fl),e&&t(ke),v(jn),e&&t(ml),e&&t(Ae),v(Wn),e&&t(_l),e&&t(Ce),v(Dn),e&&t(gl),e&&t(ze),v(Hn),e&&t(vl),e&&t(Ee),v(Qn),e&&t(yl),e&&t(Ne),v(In),e&&t(Tl),e&&t(Pe),v(Vn),e&&t(wl),e&&t(Be),v(Rn),e&&t(bl),e&&t(Le),v(Un),e&&t(xl),e&&t(je),v(Yn),e&&t($l),e&&t(We),v(Jn),e&&t(ql),e&&t(De),v(Gn),e&&t(Ol),e&&t(He),v(Kn),e&&t(Sl),e&&t(Qe),v(Xn),e&&t(Fl),e&&t(Ie),v(Zn),e&&t(Ml),e&&t(Ve),v(es),e&&t(kl),e&&t(Re),v(ts),e&&t(Al),e&&t(Ue),v(os),e&&t(Cl),e&&t(Ye),v(ns),e&&t(zl),e&&t(Je),v(ss),e&&t(El),e&&t(Ge),v(as),e&&t(Nl),e&&t(Ke),v(rs),e&&t(Pl),e&&t(Xe),v(ds),e&&t(Bl),e&&t(Ze),v(is),e&&t(Ll),e&&t(et),v(us),e&&t(jl),e&&t(tt),v(ls),e&&t(Wl),e&&t(ot),v(ps),e&&t(Dl),e&&t(nt),v(cs),e&&t(Hl),e&&t(st),v(hs),e&&t(Ql),e&&t(at),v(fs),e&&t(Il),e&&t(rt),v(ms),e&&t(Vl),e&&t(dt),v(_s),e&&t(Rl),e&&t(it),v(gs),e&&t(Ul),e&&t(ut),v(vs),e&&t(Yl),e&&t(lt),v(ys),e&&t(Jl),e&&t(pt),v(Ts),e&&t(Gl),e&&t(ct),v(ws),e&&t(Kl),e&&t(ht),v(bs),e&&t(Xl),e&&t(ft),v(xs),e&&t(Zl),e&&t(mt),v($s),e&&t(ep),e&&t(_t),v(qs),e&&t(tp),e&&t(gt),v(Os),e&&t(op),e&&t(vt),v(Ss),e&&t(np),e&&t(yt),v(Fs),e&&t(sp),e&&t(Tt),v(Ms),e&&t(ap),e&&t(wt),v(ks),e&&t(rp),e&&t(bt),v(As),e&&t(dp),e&&t(z),v(Cs),v(zs),e&&t(ip),e&&t(xt),v(Es),e&&t(up),e&&t(E),v(Ns),v(Ps),e&&t(lp),e&&t($t),v(Bs),e&&t(pp),e&&t(N),v(Ls),v(js),e&&t(cp),e&&t(qt),v(Ws),e&&t(hp),e&&t(P),v(Ds),v(Hs),e&&t(fp),e&&t(Ot),v(Qs),e&&t(mp),e&&t(B),v(Is),v(Vs),e&&t(_p),e&&t(St),v(Rs),e&&t(gp),e&&t(L),v(Us),v(Ys),e&&t(vp),e&&t(Ft),v(Js),e&&t(yp),e&&t(j),v(Gs),v(Ks),e&&t(Tp),e&&t(Mt),v(Xs),e&&t(wp),e&&t(W),v(Zs),v(ea),e&&t(bp),e&&t(kt),v(ta),e&&t(xp),e&&t(D),v(oa),v(na),e&&t($p),e&&t(At),v(sa),e&&t(qp),e&&t(H),v(aa),v(ra),e&&t(Op),e&&t(Ct),v(da),e&&t(Sp),e&&t(Q),v(ia),v(ua),e&&t(Fp),e&&t(zt),v(la),e&&t(Mp),e&&t(I),v(pa),v(ca),e&&t(kp),e&&t(Et),v(ha),e&&t(Ap),e&&t(V),v(fa),v(ma),e&&t(Cp),e&&t(Nt),v(_a),e&&t(zp),e&&t(R),v(ga),v(va),e&&t(Ep),e&&t(Pt),v(ya),e&&t(Np),e&&t(U),v(Ta),v(wa)}}}const W1={local:"model-outputs",sections:[{local:"transformers.utils.ModelOutput",title:"ModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutput",title:"BaseModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutputWithPooling",title:"BaseModelOutputWithPooling"},{local:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",title:"BaseModelOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",title:"BaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPast",title:"BaseModelOutputWithPast"},{local:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",title:"BaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_outputs.Seq2SeqModelOutput",title:"Seq2SeqModelOutput"},{local:"transformers.modeling_outputs.CausalLMOutput",title:"CausalLMOutput"},{local:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",title:"CausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.CausalLMOutputWithPast",title:"CausalLMOutputWithPast"},{local:"transformers.modeling_outputs.MaskedLMOutput",title:"MaskedLMOutput"},{local:"transformers.modeling_outputs.Seq2SeqLMOutput",title:"Seq2SeqLMOutput"},{local:"transformers.modeling_outputs.NextSentencePredictorOutput",title:"NextSentencePredictorOutput"},{local:"transformers.modeling_outputs.SequenceClassifierOutput",title:"SequenceClassifierOutput"},{local:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",title:"Seq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_outputs.MultipleChoiceModelOutput",title:"MultipleChoiceModelOutput"},{local:"transformers.modeling_outputs.TokenClassifierOutput",title:"TokenClassifierOutput"},{local:"transformers.modeling_outputs.QuestionAnsweringModelOutput",title:"QuestionAnsweringModelOutput"},{local:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",title:"Seq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutput",title:"TFBaseModelOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",title:"TFBaseModelOutputWithPooling"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",title:"TFBaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",title:"TFBaseModelOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",title:"TFBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",title:"TFSeq2SeqModelOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutput",title:"TFCausalLMOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",title:"TFCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",title:"TFCausalLMOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFMaskedLMOutput",title:"TFMaskedLMOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",title:"TFSeq2SeqLMOutput"},{local:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",title:"TFNextSentencePredictorOutput"},{local:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",title:"TFSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",title:"TFSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",title:"TFMultipleChoiceModelOutput"},{local:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",title:"TFTokenClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",title:"TFQuestionAnsweringModelOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",title:"TFSeq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",title:"FlaxBaseModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",title:"FlaxBaseModelOutputWithPast"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",title:"FlaxBaseModelOutputWithPooling"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",title:"FlaxBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",title:"FlaxSeq2SeqModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",title:"FlaxCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",title:"FlaxMaskedLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",title:"FlaxSeq2SeqLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",title:"FlaxNextSentencePredictorOutput"},{local:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",title:"FlaxSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",title:"FlaxSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",title:"FlaxMultipleChoiceModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",title:"FlaxTokenClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",title:"FlaxQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",title:"FlaxSeq2SeqQuestionAnsweringModelOutput"}],title:"Model outputs"};function D1(Fu){return P1(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class U1 extends C1{constructor(x){super();z1(this,x,D1,j1,E1,{})}}export{U1 as default,W1 as metadata};
