import{S as is,i as ls,s as ps,e as n,k as l,w as f,t as a,M as ds,c as s,d as o,m as p,a as r,x as g,h as i,b as d,G as e,g as h,y as _,L as cs,q as k,o as b,B as v,v as hs}from"../../chunks/vendor-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as as}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as It}from"../../chunks/IconCopyLink-hf-doc-builder.js";function ms(_n){let $,st,A,D,Fe,U,St,Ie,Mt,rt,E,j,Se,W,Nt,Me,Ct,at,F,Ot,H,Vt,Xt,it,_e,Ut,lt,ke,Ne,Wt,pt,be,Ht,dt,G,ct,ve,Gt,ht,Te,K,Kt,we,Qt,Jt,mt,Q,ut,ye,Ce,Yt,ft,q,Zt,J,eo,to,Y,oo,no,gt,x,I,Oe,Z,so,Ve,ro,_t,m,ee,ao,L,io,qe,lo,po,te,co,ho,mo,oe,uo,ze,fo,go,_o,z,ne,ko,Xe,bo,vo,se,Be,To,Ue,wo,yo,$e,qo,We,zo,Bo,S,re,$o,He,Ao,Eo,M,ae,xo,Ge,Lo,Ro,N,ie,Po,le,Do,Ke,jo,Fo,kt,R,C,Qe,pe,Io,Je,So,bt,u,de,Mo,y,No,Ye,Co,Oo,Ae,Vo,Xo,ce,Uo,Wo,Ho,he,Go,Ee,Ko,Qo,Jo,B,me,Yo,Ze,Zo,en,ue,xe,tn,et,on,nn,Le,sn,tt,rn,an,O,fe,ln,ot,pn,dn,V,ge,cn,nt,hn,vt;return U=new It({}),W=new It({}),G=new as({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

bartpho = AutoModel.from_pretrained("vinai/bartpho-syllable")

tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-syllable")

line = "Ch\xFAng t\xF4i l\xE0 nh\u1EEFng nghi\xEAn c\u1EE9u vi\xEAn."

input_ids = tokenizer(line, return_tensors="pt")

with torch.no_grad():
    features = bartpho(**input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
from transformers import TFAutoModel

bartpho = TFAutoModel.from_pretrained("vinai/bartpho-syllable")
input_ids = tokenizer(line, return_tensors="tf")
features = bartpho(**input_ids)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;Ch\xFAng t\xF4i l\xE0 nh\u1EEFng nghi\xEAn c\u1EE9u vi\xEAn.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bartpho(**input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(line, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>features = bartpho(**input_ids)`}}),Q=new as({props:{code:`from transformers import MBartForConditionalGeneration

bartpho = MBartForConditionalGeneration.from_pretrained("vinai/bartpho-syllable")
TXT = "Ch\xFAng t\xF4i l\xE0 <mask> nghi\xEAn c\u1EE9u vi\xEAn."
input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]
logits = bartpho(input_ids).logits
masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)
print(tokenizer.decode(predictions).split())`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MBartForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = MBartForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>TXT = <span class="hljs-string">&quot;Ch\xFAng t\xF4i l\xE0 &lt;mask&gt; nghi\xEAn c\u1EE9u vi\xEAn.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer([TXT], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = bartpho(input_ids).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>masked_index = (input_ids[<span class="hljs-number">0</span>] == tokenizer.mask_token_id).nonzero().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits[<span class="hljs-number">0</span>, masked_index].softmax(dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>values, predictions = probs.topk(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(predictions).split())`}}),Z=new It({}),ee=new P({props:{name:"class transformers.BartphoTokenizer",anchor:"transformers.BartphoTokenizer",parameters:[{name:"vocab_file",val:""},{name:"monolingual_vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file. This vocabulary is the pre-trained SentencePiece model available from the
multilingual XLM-RoBERTa, also used in mBART, consisting of 250K types.`,name:"vocab_file"},{anchor:"transformers.BartphoTokenizer.monolingual_vocab_file",description:`<strong>monolingual_vocab_file</strong> (<code>str</code>) &#x2014;
Path to the monolingual vocabulary file. This monolingual vocabulary consists of Vietnamese-specialized
types extracted from the multilingual vocabulary vocab_file of 250K types.`,name:"monolingual_vocab_file"},{anchor:"transformers.BartphoTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BartphoTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BartphoTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BartphoTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BartphoTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BartphoTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BartphoTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BartphoTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.BartphoTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.BartphoTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L46"}}),ne=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L193",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),re=new P({props:{name:"convert_tokens_to_string",anchor:"transformers.BartphoTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L295"}}),ae=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L247",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ie=new P({props:{name:"get_special_tokens_mask",anchor:"transformers.BartphoTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L219",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pe=new It({}),de=new P({props:{name:"class transformers.BartphoTokenizerFast",anchor:"transformers.BartphoTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"monolingual_vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BartphoTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BartphoTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BartphoTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BartphoTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BartphoTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BartphoTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BartphoTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BartphoTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L57"}}),me=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L256",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),fe=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L282",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ge=new P({props:{name:"get_added_vocab_hacking",anchor:"transformers.BartphoTokenizerFast.get_added_vocab_hacking",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L148",returnDescription:`
<p>The added tokens, and their original and new ids</p>
`,returnType:`
<p><code>Dict[str, int], Dict[int, int]</code></p>
`}}),{c(){$=n("meta"),st=l(),A=n("h1"),D=n("a"),Fe=n("span"),f(U.$$.fragment),St=l(),Ie=n("span"),Mt=a("BARTpho"),rt=l(),E=n("h2"),j=n("a"),Se=n("span"),f(W.$$.fragment),Nt=l(),Me=n("span"),Ct=a("Overview"),at=l(),F=n("p"),Ot=a("The BARTpho model was proposed in "),H=n("a"),Vt=a("BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"),Xt=a(" by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen."),it=l(),_e=n("p"),Ut=a("The abstract from the paper is the following:"),lt=l(),ke=n("p"),Ne=n("em"),Wt=a(`We present BARTpho with two versions \u2014 BARTpho_word and BARTpho_syllable \u2014 the first public large-scale monolingual
sequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \u201Clarge\u201D architecture and pre-training
scheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments
on a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho
outperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future
research and applications of generative Vietnamese NLP tasks.`),pt=l(),be=n("p"),Ht=a("Example of use:"),dt=l(),f(G.$$.fragment),ct=l(),ve=n("p"),Gt=a("Tips:"),ht=l(),Te=n("ul"),K=n("li"),Kt=a(`Following mBART, BARTpho uses the \u201Clarge\u201D architecture of BART with an additional layer-normalization layer on top of
both the encoder and decoder. Thus, usage examples in the `),we=n("a"),Qt=a("documentation of BART"),Jt=a(`, when adapting to use
with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.
For example:`),mt=l(),f(Q.$$.fragment),ut=l(),ye=n("ul"),Ce=n("li"),Yt=a(`This implementation is only for tokenization: \u201Cmonolingual_vocab_file\u201D consists of Vietnamese-specialized types
extracted from the pre-trained SentencePiece model \u201Cvocab_file\u201D that is available from the multilingual XLM-RoBERTa.
Other languages, if employing this pre-trained multilingual SentencePiece model \u201Cvocab_file\u201D for subword
segmentation, can reuse BartphoTokenizer with their own language-specialized \u201Cmonolingual_vocab_file\u201D.`),ft=l(),q=n("p"),Zt=a("This model was contributed by "),J=n("a"),eo=a("dqnguyen"),to=a(". The original code can be found "),Y=n("a"),oo=a("here"),no=a("."),gt=l(),x=n("h2"),I=n("a"),Oe=n("span"),f(Z.$$.fragment),so=l(),Ve=n("span"),ro=a("BartphoTokenizer"),_t=l(),m=n("div"),f(ee.$$.fragment),ao=l(),L=n("p"),io=a("Adapted from "),qe=n("a"),lo=a("XLMRobertaTokenizer"),po=a(". Based on "),te=n("a"),co=a("SentencePiece"),ho=a("."),mo=l(),oe=n("p"),uo=a("This tokenizer inherits from "),ze=n("a"),fo=a("PreTrainedTokenizer"),go=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),_o=l(),z=n("div"),f(ne.$$.fragment),ko=l(),Xe=n("p"),bo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An BARTPho sequence has the following format:`),vo=l(),se=n("ul"),Be=n("li"),To=a("single sequence: "),Ue=n("code"),wo=a("<s> X </s>"),yo=l(),$e=n("li"),qo=a("pair of sequences: "),We=n("code"),zo=a("<s> A </s></s> B </s>"),Bo=l(),S=n("div"),f(re.$$.fragment),$o=l(),He=n("p"),Ao=a("Converts a sequence of tokens (strings for sub-words) in a single string."),Eo=l(),M=n("div"),f(ae.$$.fragment),xo=l(),Ge=n("p"),Lo=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not
make use of token type ids, therefore a list of zeros is returned.`),Ro=l(),N=n("div"),f(ie.$$.fragment),Po=l(),le=n("p"),Do=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ke=n("code"),jo=a("prepare_for_model"),Fo=a(" method."),kt=l(),R=n("h2"),C=n("a"),Qe=n("span"),f(pe.$$.fragment),Io=l(),Je=n("span"),So=a("BartphoTokenizerFast"),bt=l(),u=n("div"),f(de.$$.fragment),Mo=l(),y=n("p"),No=a("Construct a \u201Cfast\u201D BARTpho tokenizer (backed by HuggingFace\u2019s "),Ye=n("em"),Co=a("tokenizers"),Oo=a(` library). Adapted from
`),Ae=n("a"),Vo=a("XLMRobertaTokenizerFast"),Xo=a(". Based on "),ce=n("a"),Uo=a("SentencePiece"),Wo=a("."),Ho=l(),he=n("p"),Go=a("This tokenizer inherits from "),Ee=n("a"),Ko=a("PreTrainedTokenizerFast"),Qo=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Jo=l(),B=n("div"),f(me.$$.fragment),Yo=l(),Ze=n("p"),Zo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARTpho sequence has the following format:`),en=l(),ue=n("ul"),xe=n("li"),tn=a("single sequence: "),et=n("code"),on=a("<s> X </s>"),nn=l(),Le=n("li"),sn=a("pair of sequences: "),tt=n("code"),rn=a("<s> A </s></s> B </s>"),an=l(),O=n("div"),f(fe.$$.fragment),ln=l(),ot=n("p"),pn=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTpho does not
make use of token type ids, therefore a list of zeros is returned.`),dn=l(),V=n("div"),f(ge.$$.fragment),cn=l(),nt=n("p"),hn=a("Returns the added tokens in the vocabulary as a dictionary of token to index."),this.h()},l(t){const c=ds('[data-svelte="svelte-1phssyn"]',document.head);$=s(c,"META",{name:!0,content:!0}),c.forEach(o),st=p(t),A=s(t,"H1",{class:!0});var Tt=r(A);D=s(Tt,"A",{id:!0,class:!0,href:!0});var kn=r(D);Fe=s(kn,"SPAN",{});var bn=r(Fe);g(U.$$.fragment,bn),bn.forEach(o),kn.forEach(o),St=p(Tt),Ie=s(Tt,"SPAN",{});var vn=r(Ie);Mt=i(vn,"BARTpho"),vn.forEach(o),Tt.forEach(o),rt=p(t),E=s(t,"H2",{class:!0});var wt=r(E);j=s(wt,"A",{id:!0,class:!0,href:!0});var Tn=r(j);Se=s(Tn,"SPAN",{});var wn=r(Se);g(W.$$.fragment,wn),wn.forEach(o),Tn.forEach(o),Nt=p(wt),Me=s(wt,"SPAN",{});var yn=r(Me);Ct=i(yn,"Overview"),yn.forEach(o),wt.forEach(o),at=p(t),F=s(t,"P",{});var yt=r(F);Ot=i(yt,"The BARTpho model was proposed in "),H=s(yt,"A",{href:!0,rel:!0});var qn=r(H);Vt=i(qn,"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"),qn.forEach(o),Xt=i(yt," by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen."),yt.forEach(o),it=p(t),_e=s(t,"P",{});var zn=r(_e);Ut=i(zn,"The abstract from the paper is the following:"),zn.forEach(o),lt=p(t),ke=s(t,"P",{});var Bn=r(ke);Ne=s(Bn,"EM",{});var $n=r(Ne);Wt=i($n,`We present BARTpho with two versions \u2014 BARTpho_word and BARTpho_syllable \u2014 the first public large-scale monolingual
sequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \u201Clarge\u201D architecture and pre-training
scheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments
on a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho
outperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future
research and applications of generative Vietnamese NLP tasks.`),$n.forEach(o),Bn.forEach(o),pt=p(t),be=s(t,"P",{});var An=r(be);Ht=i(An,"Example of use:"),An.forEach(o),dt=p(t),g(G.$$.fragment,t),ct=p(t),ve=s(t,"P",{});var En=r(ve);Gt=i(En,"Tips:"),En.forEach(o),ht=p(t),Te=s(t,"UL",{});var xn=r(Te);K=s(xn,"LI",{});var qt=r(K);Kt=i(qt,`Following mBART, BARTpho uses the \u201Clarge\u201D architecture of BART with an additional layer-normalization layer on top of
both the encoder and decoder. Thus, usage examples in the `),we=s(qt,"A",{href:!0});var Ln=r(we);Qt=i(Ln,"documentation of BART"),Ln.forEach(o),Jt=i(qt,`, when adapting to use
with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.
For example:`),qt.forEach(o),xn.forEach(o),mt=p(t),g(Q.$$.fragment,t),ut=p(t),ye=s(t,"UL",{});var Rn=r(ye);Ce=s(Rn,"LI",{});var Pn=r(Ce);Yt=i(Pn,`This implementation is only for tokenization: \u201Cmonolingual_vocab_file\u201D consists of Vietnamese-specialized types
extracted from the pre-trained SentencePiece model \u201Cvocab_file\u201D that is available from the multilingual XLM-RoBERTa.
Other languages, if employing this pre-trained multilingual SentencePiece model \u201Cvocab_file\u201D for subword
segmentation, can reuse BartphoTokenizer with their own language-specialized \u201Cmonolingual_vocab_file\u201D.`),Pn.forEach(o),Rn.forEach(o),ft=p(t),q=s(t,"P",{});var Re=r(q);Zt=i(Re,"This model was contributed by "),J=s(Re,"A",{href:!0,rel:!0});var Dn=r(J);eo=i(Dn,"dqnguyen"),Dn.forEach(o),to=i(Re,". The original code can be found "),Y=s(Re,"A",{href:!0,rel:!0});var jn=r(Y);oo=i(jn,"here"),jn.forEach(o),no=i(Re,"."),Re.forEach(o),gt=p(t),x=s(t,"H2",{class:!0});var zt=r(x);I=s(zt,"A",{id:!0,class:!0,href:!0});var Fn=r(I);Oe=s(Fn,"SPAN",{});var In=r(Oe);g(Z.$$.fragment,In),In.forEach(o),Fn.forEach(o),so=p(zt),Ve=s(zt,"SPAN",{});var Sn=r(Ve);ro=i(Sn,"BartphoTokenizer"),Sn.forEach(o),zt.forEach(o),_t=p(t),m=s(t,"DIV",{class:!0});var T=r(m);g(ee.$$.fragment,T),ao=p(T),L=s(T,"P",{});var Pe=r(L);io=i(Pe,"Adapted from "),qe=s(Pe,"A",{href:!0});var Mn=r(qe);lo=i(Mn,"XLMRobertaTokenizer"),Mn.forEach(o),po=i(Pe,". Based on "),te=s(Pe,"A",{href:!0,rel:!0});var Nn=r(te);co=i(Nn,"SentencePiece"),Nn.forEach(o),ho=i(Pe,"."),Pe.forEach(o),mo=p(T),oe=s(T,"P",{});var Bt=r(oe);uo=i(Bt,"This tokenizer inherits from "),ze=s(Bt,"A",{href:!0});var Cn=r(ze);fo=i(Cn,"PreTrainedTokenizer"),Cn.forEach(o),go=i(Bt,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Bt.forEach(o),_o=p(T),z=s(T,"DIV",{class:!0});var De=r(z);g(ne.$$.fragment,De),ko=p(De),Xe=s(De,"P",{});var On=r(Xe);bo=i(On,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An BARTPho sequence has the following format:`),On.forEach(o),vo=p(De),se=s(De,"UL",{});var $t=r(se);Be=s($t,"LI",{});var mn=r(Be);To=i(mn,"single sequence: "),Ue=s(mn,"CODE",{});var Vn=r(Ue);wo=i(Vn,"<s> X </s>"),Vn.forEach(o),mn.forEach(o),yo=p($t),$e=s($t,"LI",{});var un=r($e);qo=i(un,"pair of sequences: "),We=s(un,"CODE",{});var Xn=r(We);zo=i(Xn,"<s> A </s></s> B </s>"),Xn.forEach(o),un.forEach(o),$t.forEach(o),De.forEach(o),Bo=p(T),S=s(T,"DIV",{class:!0});var At=r(S);g(re.$$.fragment,At),$o=p(At),He=s(At,"P",{});var Un=r(He);Ao=i(Un,"Converts a sequence of tokens (strings for sub-words) in a single string."),Un.forEach(o),At.forEach(o),Eo=p(T),M=s(T,"DIV",{class:!0});var Et=r(M);g(ae.$$.fragment,Et),xo=p(Et),Ge=s(Et,"P",{});var Wn=r(Ge);Lo=i(Wn,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not
make use of token type ids, therefore a list of zeros is returned.`),Wn.forEach(o),Et.forEach(o),Ro=p(T),N=s(T,"DIV",{class:!0});var xt=r(N);g(ie.$$.fragment,xt),Po=p(xt),le=s(xt,"P",{});var Lt=r(le);Do=i(Lt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ke=s(Lt,"CODE",{});var Hn=r(Ke);jo=i(Hn,"prepare_for_model"),Hn.forEach(o),Fo=i(Lt," method."),Lt.forEach(o),xt.forEach(o),T.forEach(o),kt=p(t),R=s(t,"H2",{class:!0});var Rt=r(R);C=s(Rt,"A",{id:!0,class:!0,href:!0});var Gn=r(C);Qe=s(Gn,"SPAN",{});var Kn=r(Qe);g(pe.$$.fragment,Kn),Kn.forEach(o),Gn.forEach(o),Io=p(Rt),Je=s(Rt,"SPAN",{});var Qn=r(Je);So=i(Qn,"BartphoTokenizerFast"),Qn.forEach(o),Rt.forEach(o),bt=p(t),u=s(t,"DIV",{class:!0});var w=r(u);g(de.$$.fragment,w),Mo=p(w),y=s(w,"P",{});var X=r(y);No=i(X,"Construct a \u201Cfast\u201D BARTpho tokenizer (backed by HuggingFace\u2019s "),Ye=s(X,"EM",{});var Jn=r(Ye);Co=i(Jn,"tokenizers"),Jn.forEach(o),Oo=i(X,` library). Adapted from
`),Ae=s(X,"A",{href:!0});var Yn=r(Ae);Vo=i(Yn,"XLMRobertaTokenizerFast"),Yn.forEach(o),Xo=i(X,". Based on "),ce=s(X,"A",{href:!0,rel:!0});var Zn=r(ce);Uo=i(Zn,"SentencePiece"),Zn.forEach(o),Wo=i(X,"."),X.forEach(o),Ho=p(w),he=s(w,"P",{});var Pt=r(he);Go=i(Pt,"This tokenizer inherits from "),Ee=s(Pt,"A",{href:!0});var es=r(Ee);Ko=i(es,"PreTrainedTokenizerFast"),es.forEach(o),Qo=i(Pt,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Pt.forEach(o),Jo=p(w),B=s(w,"DIV",{class:!0});var je=r(B);g(me.$$.fragment,je),Yo=p(je),Ze=s(je,"P",{});var ts=r(Ze);Zo=i(ts,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARTpho sequence has the following format:`),ts.forEach(o),en=p(je),ue=s(je,"UL",{});var Dt=r(ue);xe=s(Dt,"LI",{});var fn=r(xe);tn=i(fn,"single sequence: "),et=s(fn,"CODE",{});var os=r(et);on=i(os,"<s> X </s>"),os.forEach(o),fn.forEach(o),nn=p(Dt),Le=s(Dt,"LI",{});var gn=r(Le);sn=i(gn,"pair of sequences: "),tt=s(gn,"CODE",{});var ns=r(tt);rn=i(ns,"<s> A </s></s> B </s>"),ns.forEach(o),gn.forEach(o),Dt.forEach(o),je.forEach(o),an=p(w),O=s(w,"DIV",{class:!0});var jt=r(O);g(fe.$$.fragment,jt),ln=p(jt),ot=s(jt,"P",{});var ss=r(ot);pn=i(ss,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTpho does not
make use of token type ids, therefore a list of zeros is returned.`),ss.forEach(o),jt.forEach(o),dn=p(w),V=s(w,"DIV",{class:!0});var Ft=r(V);g(ge.$$.fragment,Ft),cn=p(Ft),nt=s(Ft,"P",{});var rs=r(nt);hn=i(rs,"Returns the added tokens in the vocabulary as a dictionary of token to index."),rs.forEach(o),Ft.forEach(o),w.forEach(o),this.h()},h(){d($,"name","hf:doc:metadata"),d($,"content",JSON.stringify(us)),d(D,"id","bartpho"),d(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(D,"href","#bartpho"),d(A,"class","relative group"),d(j,"id","overview"),d(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(j,"href","#overview"),d(E,"class","relative group"),d(H,"href","https://arxiv.org/abs/2109.09701"),d(H,"rel","nofollow"),d(we,"href","bart"),d(J,"href","https://huggingface.co/dqnguyen"),d(J,"rel","nofollow"),d(Y,"href","https://github.com/VinAIResearch/BARTpho"),d(Y,"rel","nofollow"),d(I,"id","transformers.BartphoTokenizer"),d(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I,"href","#transformers.BartphoTokenizer"),d(x,"class","relative group"),d(qe,"href","/docs/transformers/pr_17254/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(te,"href","https://github.com/google/sentencepiece"),d(te,"rel","nofollow"),d(ze,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"id","transformers.BartphoTokenizerFast"),d(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(C,"href","#transformers.BartphoTokenizerFast"),d(R,"class","relative group"),d(Ae,"href","/docs/transformers/pr_17254/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(ce,"href","https://github.com/google/sentencepiece"),d(ce,"rel","nofollow"),d(Ee,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,c){e(document.head,$),h(t,st,c),h(t,A,c),e(A,D),e(D,Fe),_(U,Fe,null),e(A,St),e(A,Ie),e(Ie,Mt),h(t,rt,c),h(t,E,c),e(E,j),e(j,Se),_(W,Se,null),e(E,Nt),e(E,Me),e(Me,Ct),h(t,at,c),h(t,F,c),e(F,Ot),e(F,H),e(H,Vt),e(F,Xt),h(t,it,c),h(t,_e,c),e(_e,Ut),h(t,lt,c),h(t,ke,c),e(ke,Ne),e(Ne,Wt),h(t,pt,c),h(t,be,c),e(be,Ht),h(t,dt,c),_(G,t,c),h(t,ct,c),h(t,ve,c),e(ve,Gt),h(t,ht,c),h(t,Te,c),e(Te,K),e(K,Kt),e(K,we),e(we,Qt),e(K,Jt),h(t,mt,c),_(Q,t,c),h(t,ut,c),h(t,ye,c),e(ye,Ce),e(Ce,Yt),h(t,ft,c),h(t,q,c),e(q,Zt),e(q,J),e(J,eo),e(q,to),e(q,Y),e(Y,oo),e(q,no),h(t,gt,c),h(t,x,c),e(x,I),e(I,Oe),_(Z,Oe,null),e(x,so),e(x,Ve),e(Ve,ro),h(t,_t,c),h(t,m,c),_(ee,m,null),e(m,ao),e(m,L),e(L,io),e(L,qe),e(qe,lo),e(L,po),e(L,te),e(te,co),e(L,ho),e(m,mo),e(m,oe),e(oe,uo),e(oe,ze),e(ze,fo),e(oe,go),e(m,_o),e(m,z),_(ne,z,null),e(z,ko),e(z,Xe),e(Xe,bo),e(z,vo),e(z,se),e(se,Be),e(Be,To),e(Be,Ue),e(Ue,wo),e(se,yo),e(se,$e),e($e,qo),e($e,We),e(We,zo),e(m,Bo),e(m,S),_(re,S,null),e(S,$o),e(S,He),e(He,Ao),e(m,Eo),e(m,M),_(ae,M,null),e(M,xo),e(M,Ge),e(Ge,Lo),e(m,Ro),e(m,N),_(ie,N,null),e(N,Po),e(N,le),e(le,Do),e(le,Ke),e(Ke,jo),e(le,Fo),h(t,kt,c),h(t,R,c),e(R,C),e(C,Qe),_(pe,Qe,null),e(R,Io),e(R,Je),e(Je,So),h(t,bt,c),h(t,u,c),_(de,u,null),e(u,Mo),e(u,y),e(y,No),e(y,Ye),e(Ye,Co),e(y,Oo),e(y,Ae),e(Ae,Vo),e(y,Xo),e(y,ce),e(ce,Uo),e(y,Wo),e(u,Ho),e(u,he),e(he,Go),e(he,Ee),e(Ee,Ko),e(he,Qo),e(u,Jo),e(u,B),_(me,B,null),e(B,Yo),e(B,Ze),e(Ze,Zo),e(B,en),e(B,ue),e(ue,xe),e(xe,tn),e(xe,et),e(et,on),e(ue,nn),e(ue,Le),e(Le,sn),e(Le,tt),e(tt,rn),e(u,an),e(u,O),_(fe,O,null),e(O,ln),e(O,ot),e(ot,pn),e(u,dn),e(u,V),_(ge,V,null),e(V,cn),e(V,nt),e(nt,hn),vt=!0},p:cs,i(t){vt||(k(U.$$.fragment,t),k(W.$$.fragment,t),k(G.$$.fragment,t),k(Q.$$.fragment,t),k(Z.$$.fragment,t),k(ee.$$.fragment,t),k(ne.$$.fragment,t),k(re.$$.fragment,t),k(ae.$$.fragment,t),k(ie.$$.fragment,t),k(pe.$$.fragment,t),k(de.$$.fragment,t),k(me.$$.fragment,t),k(fe.$$.fragment,t),k(ge.$$.fragment,t),vt=!0)},o(t){b(U.$$.fragment,t),b(W.$$.fragment,t),b(G.$$.fragment,t),b(Q.$$.fragment,t),b(Z.$$.fragment,t),b(ee.$$.fragment,t),b(ne.$$.fragment,t),b(re.$$.fragment,t),b(ae.$$.fragment,t),b(ie.$$.fragment,t),b(pe.$$.fragment,t),b(de.$$.fragment,t),b(me.$$.fragment,t),b(fe.$$.fragment,t),b(ge.$$.fragment,t),vt=!1},d(t){o($),t&&o(st),t&&o(A),v(U),t&&o(rt),t&&o(E),v(W),t&&o(at),t&&o(F),t&&o(it),t&&o(_e),t&&o(lt),t&&o(ke),t&&o(pt),t&&o(be),t&&o(dt),v(G,t),t&&o(ct),t&&o(ve),t&&o(ht),t&&o(Te),t&&o(mt),v(Q,t),t&&o(ut),t&&o(ye),t&&o(ft),t&&o(q),t&&o(gt),t&&o(x),v(Z),t&&o(_t),t&&o(m),v(ee),v(ne),v(re),v(ae),v(ie),t&&o(kt),t&&o(R),v(pe),t&&o(bt),t&&o(u),v(de),v(me),v(fe),v(ge)}}}const us={local:"bartpho",sections:[{local:"overview",title:"Overview"},{local:"transformers.BartphoTokenizer",title:"BartphoTokenizer"},{local:"transformers.BartphoTokenizerFast",title:"BartphoTokenizerFast"}],title:"BARTpho"};function fs(_n){return hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vs extends is{constructor($){super();ls(this,$,fs,ms,ps,{})}}export{vs as default,us as metadata};
