import{S as Ss,i as Us,s as Ws,e as s,k as l,w as g,t as a,M as Xs,c as n,d as o,m as d,a as r,x as k,h as i,b as c,G as e,g as h,y as b,L as Hs,q as v,o as T,B as y,v as Gs}from"../../chunks/vendor-316217a5.js";import{D as $}from"../../chunks/Docstring-1fe4a33a.js";import{C as Qs}from"../../chunks/CodeBlock-8418a95e.js";import{I as Lt}from"../../chunks/IconCopyLink-40460835.js";function Ys(ns){let q,Ke,z,B,$e,S,xt,qe,Dt,Ze,L,A,ze,U,Bt,Le,At,et,R,Rt,W,It,Ft,tt,fe,Nt,ot,ue,xe,Ct,st,_e,jt,nt,X,rt,P,Ot,H,Mt,Vt,G,St,Ut,at,x,I,De,Q,Wt,Be,Xt,it,m,Y,Ht,Ae,Gt,Qt,J,Yt,ge,Jt,Kt,Zt,F,K,eo,Re,to,oo,w,Z,so,Ie,no,ro,ee,ke,ao,Fe,io,lo,be,co,Ne,po,ho,N,te,mo,Ce,fo,uo,C,oe,_o,je,go,ko,j,se,bo,ne,vo,Oe,To,yo,lt,D,O,Me,re,Po,Ve,wo,dt,f,ae,Eo,ie,$o,Se,qo,zo,Lo,Ue,xo,Do,We,Xe,Bo,Ao,le,Ro,ve,Io,Fo,No,E,de,Co,He,jo,Oo,ce,Te,Mo,Ge,Vo,So,ye,Uo,Qe,Wo,Xo,M,pe,Ho,Ye,Go,Qo,V,he,Yo,me,Jo,Je,Ko,Zo,ct;return S=new Lt({}),U=new Lt({}),X=new Qs({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

phobert = AutoModel.from_pretrained("vinai/phobert-base")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
line = "T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 ."

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = phobert(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# phobert = TFAutoModel.from_pretrained("vinai/phobert-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 .&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># phobert = TFAutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)</span>`}}),Q=new Lt({}),Y=new $({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.PhobertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.PhobertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.PhobertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.PhobertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.PhobertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PhobertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L68"}}),K=new $({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L343"}}),Z=new $({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L166",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),te=new $({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L313"}}),oe=new $({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L220",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),se=new $({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L192",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),re=new Lt({}),ae=new $({props:{name:"class transformers.PhobertTokenizerFast",anchor:"transformers.PhobertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L59"}}),de=new $({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L166",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pe=new $({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L220",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),he=new $({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L192",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){q=s("meta"),Ke=l(),z=s("h1"),B=s("a"),$e=s("span"),g(S.$$.fragment),xt=l(),qe=s("span"),Dt=a("PhoBERT"),Ze=l(),L=s("h2"),A=s("a"),ze=s("span"),g(U.$$.fragment),Bt=l(),Le=s("span"),At=a("Overview"),et=l(),R=s("p"),Rt=a("The PhoBERT model was proposed in "),W=s("a"),It=a("PhoBERT: Pre-trained language models for Vietnamese"),Ft=a(" by Dat Quoc Nguyen, Anh Tuan Nguyen."),tt=l(),fe=s("p"),Nt=a("The abstract from the paper is the following:"),ot=l(),ue=s("p"),xe=s("em"),Ct=a(`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),st=l(),_e=s("p"),jt=a("Example of use:"),nt=l(),g(X.$$.fragment),rt=l(),P=s("p"),Ot=a("This model was contributed by "),H=s("a"),Mt=a("dqnguyen"),Vt=a(". The original code can be found "),G=s("a"),St=a("here"),Ut=a("."),at=l(),x=s("h2"),I=s("a"),De=s("span"),g(Q.$$.fragment),Wt=l(),Be=s("span"),Xt=a("PhobertTokenizer"),it=l(),m=s("div"),g(Y.$$.fragment),Ht=l(),Ae=s("p"),Gt=a("Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),Qt=l(),J=s("p"),Yt=a("This tokenizer inherits from "),ge=s("a"),Jt=a("PreTrainedTokenizer"),Kt=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Zt=l(),F=s("div"),g(K.$$.fragment),eo=l(),Re=s("p"),to=a("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),oo=l(),w=s("div"),g(Z.$$.fragment),so=l(),Ie=s("p"),no=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),ro=l(),ee=s("ul"),ke=s("li"),ao=a("single sequence: "),Fe=s("code"),io=a("<s> X </s>"),lo=l(),be=s("li"),co=a("pair of sequences: "),Ne=s("code"),po=a("<s> A </s></s> B </s>"),ho=l(),N=s("div"),g(te.$$.fragment),mo=l(),Ce=s("p"),fo=a("Converts a sequence of tokens (string) in a single string."),uo=l(),C=s("div"),g(oe.$$.fragment),_o=l(),je=s("p"),go=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),ko=l(),j=s("div"),g(se.$$.fragment),bo=l(),ne=s("p"),vo=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Oe=s("code"),To=a("prepare_for_model"),yo=a(" method."),lt=l(),D=s("h2"),O=s("a"),Me=s("span"),g(re.$$.fragment),Po=l(),Ve=s("span"),wo=a("PhobertTokenizerFast"),dt=l(),f=s("div"),g(ae.$$.fragment),Eo=l(),ie=s("p"),$o=a("Construct a \u201CFast\u201D BPE tokenizer for PhoBERT (backed by HuggingFace\u2019s "),Se=s("em"),qo=a("tokenizers"),zo=a(" library)."),Lo=l(),Ue=s("p"),xo=a("Peculiarities:"),Do=l(),We=s("ul"),Xe=s("li"),Bo=a(`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Ao=l(),le=s("p"),Ro=a("This tokenizer inherits from "),ve=s("a"),Io=a("PreTrainedTokenizer"),Fo=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),No=l(),E=s("div"),g(de.$$.fragment),Co=l(),He=s("p"),jo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),Oo=l(),ce=s("ul"),Te=s("li"),Mo=a("single sequence: "),Ge=s("code"),Vo=a("<s> X </s>"),So=l(),ye=s("li"),Uo=a("pair of sequences: "),Qe=s("code"),Wo=a("<s> A </s></s> B </s>"),Xo=l(),M=s("div"),g(pe.$$.fragment),Ho=l(),Ye=s("p"),Go=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Qo=l(),V=s("div"),g(he.$$.fragment),Yo=l(),me=s("p"),Jo=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Je=s("code"),Ko=a("prepare_for_model"),Zo=a(" method."),this.h()},l(t){const p=Xs('[data-svelte="svelte-1phssyn"]',document.head);q=n(p,"META",{name:!0,content:!0}),p.forEach(o),Ke=d(t),z=n(t,"H1",{class:!0});var pt=r(z);B=n(pt,"A",{id:!0,class:!0,href:!0});var rs=r(B);$e=n(rs,"SPAN",{});var as=r($e);k(S.$$.fragment,as),as.forEach(o),rs.forEach(o),xt=d(pt),qe=n(pt,"SPAN",{});var is=r(qe);Dt=i(is,"PhoBERT"),is.forEach(o),pt.forEach(o),Ze=d(t),L=n(t,"H2",{class:!0});var ht=r(L);A=n(ht,"A",{id:!0,class:!0,href:!0});var ls=r(A);ze=n(ls,"SPAN",{});var ds=r(ze);k(U.$$.fragment,ds),ds.forEach(o),ls.forEach(o),Bt=d(ht),Le=n(ht,"SPAN",{});var cs=r(Le);At=i(cs,"Overview"),cs.forEach(o),ht.forEach(o),et=d(t),R=n(t,"P",{});var mt=r(R);Rt=i(mt,"The PhoBERT model was proposed in "),W=n(mt,"A",{href:!0,rel:!0});var ps=r(W);It=i(ps,"PhoBERT: Pre-trained language models for Vietnamese"),ps.forEach(o),Ft=i(mt," by Dat Quoc Nguyen, Anh Tuan Nguyen."),mt.forEach(o),tt=d(t),fe=n(t,"P",{});var hs=r(fe);Nt=i(hs,"The abstract from the paper is the following:"),hs.forEach(o),ot=d(t),ue=n(t,"P",{});var ms=r(ue);xe=n(ms,"EM",{});var fs=r(xe);Ct=i(fs,`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),fs.forEach(o),ms.forEach(o),st=d(t),_e=n(t,"P",{});var us=r(_e);jt=i(us,"Example of use:"),us.forEach(o),nt=d(t),k(X.$$.fragment,t),rt=d(t),P=n(t,"P",{});var Pe=r(P);Ot=i(Pe,"This model was contributed by "),H=n(Pe,"A",{href:!0,rel:!0});var _s=r(H);Mt=i(_s,"dqnguyen"),_s.forEach(o),Vt=i(Pe,". The original code can be found "),G=n(Pe,"A",{href:!0,rel:!0});var gs=r(G);St=i(gs,"here"),gs.forEach(o),Ut=i(Pe,"."),Pe.forEach(o),at=d(t),x=n(t,"H2",{class:!0});var ft=r(x);I=n(ft,"A",{id:!0,class:!0,href:!0});var ks=r(I);De=n(ks,"SPAN",{});var bs=r(De);k(Q.$$.fragment,bs),bs.forEach(o),ks.forEach(o),Wt=d(ft),Be=n(ft,"SPAN",{});var vs=r(Be);Xt=i(vs,"PhobertTokenizer"),vs.forEach(o),ft.forEach(o),it=d(t),m=n(t,"DIV",{class:!0});var u=r(m);k(Y.$$.fragment,u),Ht=d(u),Ae=n(u,"P",{});var Ts=r(Ae);Gt=i(Ts,"Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),Ts.forEach(o),Qt=d(u),J=n(u,"P",{});var ut=r(J);Yt=i(ut,"This tokenizer inherits from "),ge=n(ut,"A",{href:!0});var ys=r(ge);Jt=i(ys,"PreTrainedTokenizer"),ys.forEach(o),Kt=i(ut,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ut.forEach(o),Zt=d(u),F=n(u,"DIV",{class:!0});var _t=r(F);k(K.$$.fragment,_t),eo=d(_t),Re=n(_t,"P",{});var Ps=r(Re);to=i(Ps,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),Ps.forEach(o),_t.forEach(o),oo=d(u),w=n(u,"DIV",{class:!0});var we=r(w);k(Z.$$.fragment,we),so=d(we),Ie=n(we,"P",{});var ws=r(Ie);no=i(ws,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),ws.forEach(o),ro=d(we),ee=n(we,"UL",{});var gt=r(ee);ke=n(gt,"LI",{});var es=r(ke);ao=i(es,"single sequence: "),Fe=n(es,"CODE",{});var Es=r(Fe);io=i(Es,"<s> X </s>"),Es.forEach(o),es.forEach(o),lo=d(gt),be=n(gt,"LI",{});var ts=r(be);co=i(ts,"pair of sequences: "),Ne=n(ts,"CODE",{});var $s=r(Ne);po=i($s,"<s> A </s></s> B </s>"),$s.forEach(o),ts.forEach(o),gt.forEach(o),we.forEach(o),ho=d(u),N=n(u,"DIV",{class:!0});var kt=r(N);k(te.$$.fragment,kt),mo=d(kt),Ce=n(kt,"P",{});var qs=r(Ce);fo=i(qs,"Converts a sequence of tokens (string) in a single string."),qs.forEach(o),kt.forEach(o),uo=d(u),C=n(u,"DIV",{class:!0});var bt=r(C);k(oe.$$.fragment,bt),_o=d(bt),je=n(bt,"P",{});var zs=r(je);go=i(zs,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),zs.forEach(o),bt.forEach(o),ko=d(u),j=n(u,"DIV",{class:!0});var vt=r(j);k(se.$$.fragment,vt),bo=d(vt),ne=n(vt,"P",{});var Tt=r(ne);vo=i(Tt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Oe=n(Tt,"CODE",{});var Ls=r(Oe);To=i(Ls,"prepare_for_model"),Ls.forEach(o),yo=i(Tt," method."),Tt.forEach(o),vt.forEach(o),u.forEach(o),lt=d(t),D=n(t,"H2",{class:!0});var yt=r(D);O=n(yt,"A",{id:!0,class:!0,href:!0});var xs=r(O);Me=n(xs,"SPAN",{});var Ds=r(Me);k(re.$$.fragment,Ds),Ds.forEach(o),xs.forEach(o),Po=d(yt),Ve=n(yt,"SPAN",{});var Bs=r(Ve);wo=i(Bs,"PhobertTokenizerFast"),Bs.forEach(o),yt.forEach(o),dt=d(t),f=n(t,"DIV",{class:!0});var _=r(f);k(ae.$$.fragment,_),Eo=d(_),ie=n(_,"P",{});var Pt=r(ie);$o=i(Pt,"Construct a \u201CFast\u201D BPE tokenizer for PhoBERT (backed by HuggingFace\u2019s "),Se=n(Pt,"EM",{});var As=r(Se);qo=i(As,"tokenizers"),As.forEach(o),zo=i(Pt," library)."),Pt.forEach(o),Lo=d(_),Ue=n(_,"P",{});var Rs=r(Ue);xo=i(Rs,"Peculiarities:"),Rs.forEach(o),Do=d(_),We=n(_,"UL",{});var Is=r(We);Xe=n(Is,"LI",{});var Fs=r(Xe);Bo=i(Fs,`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Fs.forEach(o),Is.forEach(o),Ao=d(_),le=n(_,"P",{});var wt=r(le);Ro=i(wt,"This tokenizer inherits from "),ve=n(wt,"A",{href:!0});var Ns=r(ve);Io=i(Ns,"PreTrainedTokenizer"),Ns.forEach(o),Fo=i(wt,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),wt.forEach(o),No=d(_),E=n(_,"DIV",{class:!0});var Ee=r(E);k(de.$$.fragment,Ee),Co=d(Ee),He=n(Ee,"P",{});var Cs=r(He);jo=i(Cs,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),Cs.forEach(o),Oo=d(Ee),ce=n(Ee,"UL",{});var Et=r(ce);Te=n(Et,"LI",{});var os=r(Te);Mo=i(os,"single sequence: "),Ge=n(os,"CODE",{});var js=r(Ge);Vo=i(js,"<s> X </s>"),js.forEach(o),os.forEach(o),So=d(Et),ye=n(Et,"LI",{});var ss=r(ye);Uo=i(ss,"pair of sequences: "),Qe=n(ss,"CODE",{});var Os=r(Qe);Wo=i(Os,"<s> A </s></s> B </s>"),Os.forEach(o),ss.forEach(o),Et.forEach(o),Ee.forEach(o),Xo=d(_),M=n(_,"DIV",{class:!0});var $t=r(M);k(pe.$$.fragment,$t),Ho=d($t),Ye=n($t,"P",{});var Ms=r(Ye);Go=i(Ms,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Ms.forEach(o),$t.forEach(o),Qo=d(_),V=n(_,"DIV",{class:!0});var qt=r(V);k(he.$$.fragment,qt),Yo=d(qt),me=n(qt,"P",{});var zt=r(me);Jo=i(zt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Je=n(zt,"CODE",{});var Vs=r(Je);Ko=i(Vs,"prepare_for_model"),Vs.forEach(o),Zo=i(zt," method."),zt.forEach(o),qt.forEach(o),_.forEach(o),this.h()},h(){c(q,"name","hf:doc:metadata"),c(q,"content",JSON.stringify(Js)),c(B,"id","phobert"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#phobert"),c(z,"class","relative group"),c(A,"id","overview"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#overview"),c(L,"class","relative group"),c(W,"href","https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf"),c(W,"rel","nofollow"),c(H,"href","https://huggingface.co/dqnguyen"),c(H,"rel","nofollow"),c(G,"href","https://github.com/VinAIResearch/PhoBERT"),c(G,"rel","nofollow"),c(I,"id","transformers.PhobertTokenizer"),c(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I,"href","#transformers.PhobertTokenizer"),c(x,"class","relative group"),c(ge,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(O,"id","transformers.PhobertTokenizerFast"),c(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O,"href","#transformers.PhobertTokenizerFast"),c(D,"class","relative group"),c(ve,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,q),h(t,Ke,p),h(t,z,p),e(z,B),e(B,$e),b(S,$e,null),e(z,xt),e(z,qe),e(qe,Dt),h(t,Ze,p),h(t,L,p),e(L,A),e(A,ze),b(U,ze,null),e(L,Bt),e(L,Le),e(Le,At),h(t,et,p),h(t,R,p),e(R,Rt),e(R,W),e(W,It),e(R,Ft),h(t,tt,p),h(t,fe,p),e(fe,Nt),h(t,ot,p),h(t,ue,p),e(ue,xe),e(xe,Ct),h(t,st,p),h(t,_e,p),e(_e,jt),h(t,nt,p),b(X,t,p),h(t,rt,p),h(t,P,p),e(P,Ot),e(P,H),e(H,Mt),e(P,Vt),e(P,G),e(G,St),e(P,Ut),h(t,at,p),h(t,x,p),e(x,I),e(I,De),b(Q,De,null),e(x,Wt),e(x,Be),e(Be,Xt),h(t,it,p),h(t,m,p),b(Y,m,null),e(m,Ht),e(m,Ae),e(Ae,Gt),e(m,Qt),e(m,J),e(J,Yt),e(J,ge),e(ge,Jt),e(J,Kt),e(m,Zt),e(m,F),b(K,F,null),e(F,eo),e(F,Re),e(Re,to),e(m,oo),e(m,w),b(Z,w,null),e(w,so),e(w,Ie),e(Ie,no),e(w,ro),e(w,ee),e(ee,ke),e(ke,ao),e(ke,Fe),e(Fe,io),e(ee,lo),e(ee,be),e(be,co),e(be,Ne),e(Ne,po),e(m,ho),e(m,N),b(te,N,null),e(N,mo),e(N,Ce),e(Ce,fo),e(m,uo),e(m,C),b(oe,C,null),e(C,_o),e(C,je),e(je,go),e(m,ko),e(m,j),b(se,j,null),e(j,bo),e(j,ne),e(ne,vo),e(ne,Oe),e(Oe,To),e(ne,yo),h(t,lt,p),h(t,D,p),e(D,O),e(O,Me),b(re,Me,null),e(D,Po),e(D,Ve),e(Ve,wo),h(t,dt,p),h(t,f,p),b(ae,f,null),e(f,Eo),e(f,ie),e(ie,$o),e(ie,Se),e(Se,qo),e(ie,zo),e(f,Lo),e(f,Ue),e(Ue,xo),e(f,Do),e(f,We),e(We,Xe),e(Xe,Bo),e(f,Ao),e(f,le),e(le,Ro),e(le,ve),e(ve,Io),e(le,Fo),e(f,No),e(f,E),b(de,E,null),e(E,Co),e(E,He),e(He,jo),e(E,Oo),e(E,ce),e(ce,Te),e(Te,Mo),e(Te,Ge),e(Ge,Vo),e(ce,So),e(ce,ye),e(ye,Uo),e(ye,Qe),e(Qe,Wo),e(f,Xo),e(f,M),b(pe,M,null),e(M,Ho),e(M,Ye),e(Ye,Go),e(f,Qo),e(f,V),b(he,V,null),e(V,Yo),e(V,me),e(me,Jo),e(me,Je),e(Je,Ko),e(me,Zo),ct=!0},p:Hs,i(t){ct||(v(S.$$.fragment,t),v(U.$$.fragment,t),v(X.$$.fragment,t),v(Q.$$.fragment,t),v(Y.$$.fragment,t),v(K.$$.fragment,t),v(Z.$$.fragment,t),v(te.$$.fragment,t),v(oe.$$.fragment,t),v(se.$$.fragment,t),v(re.$$.fragment,t),v(ae.$$.fragment,t),v(de.$$.fragment,t),v(pe.$$.fragment,t),v(he.$$.fragment,t),ct=!0)},o(t){T(S.$$.fragment,t),T(U.$$.fragment,t),T(X.$$.fragment,t),T(Q.$$.fragment,t),T(Y.$$.fragment,t),T(K.$$.fragment,t),T(Z.$$.fragment,t),T(te.$$.fragment,t),T(oe.$$.fragment,t),T(se.$$.fragment,t),T(re.$$.fragment,t),T(ae.$$.fragment,t),T(de.$$.fragment,t),T(pe.$$.fragment,t),T(he.$$.fragment,t),ct=!1},d(t){o(q),t&&o(Ke),t&&o(z),y(S),t&&o(Ze),t&&o(L),y(U),t&&o(et),t&&o(R),t&&o(tt),t&&o(fe),t&&o(ot),t&&o(ue),t&&o(st),t&&o(_e),t&&o(nt),y(X,t),t&&o(rt),t&&o(P),t&&o(at),t&&o(x),y(Q),t&&o(it),t&&o(m),y(Y),y(K),y(Z),y(te),y(oe),y(se),t&&o(lt),t&&o(D),y(re),t&&o(dt),t&&o(f),y(ae),y(de),y(pe),y(he)}}}const Js={local:"phobert",sections:[{local:"overview",title:"Overview"},{local:"transformers.PhobertTokenizer",title:"PhobertTokenizer"},{local:"transformers.PhobertTokenizerFast",title:"PhobertTokenizerFast"}],title:"PhoBERT"};function Ks(ns){return Gs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sn extends Ss{constructor(q){super();Us(this,q,Ks,Ys,Ws,{})}}export{sn as default,Js as metadata};
