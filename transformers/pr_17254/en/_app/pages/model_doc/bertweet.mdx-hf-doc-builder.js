import{S as oo,i as no,s as ao,e as r,k as l,w as g,t as a,M as io,c as o,d as s,m as c,a as n,x as _,h as i,b as d,G as e,g as f,y as k,L as lo,q as w,o as v,B as b,v as co}from"../../chunks/vendor-hf-doc-builder.js";import{D as y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as po}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Nt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function mo(_r){let B,ot,q,P,xe,W,jt,De,Ct,nt,L,A,Pe,H,Ot,Ae,Mt,at,R,St,X,Vt,Ut,it,ke,Wt,lt,we,Re,Ht,ct,ve,Xt,dt,Q,pt,E,Qt,Y,Yt,Zt,Z,Gt,Jt,mt,x,I,Ie,G,Kt,Fe,es,ft,m,J,ts,Ne,ss,rs,K,os,be,ns,as,is,F,ee,ls,je,cs,ds,$,te,ps,Ce,ms,fs,se,Te,hs,Oe,us,gs,ye,_s,Me,ks,ws,N,re,vs,Se,bs,Ts,j,oe,ys,Ve,Es,$s,C,ne,zs,ae,Bs,Ue,qs,Ls,xs,O,ie,Ds,We,Ps,As,M,le,Rs,He,Is,ht,D,S,Xe,ce,Fs,Qe,Ns,ut,h,de,js,pe,Cs,Ye,Os,Ms,Ss,Ze,Vs,Us,Ge,Je,Ws,Hs,me,Xs,Ee,Qs,Ys,Zs,z,fe,Gs,Ke,Js,Ks,he,$e,er,et,tr,sr,ze,rr,tt,or,nr,V,ue,ar,st,ir,lr,U,ge,cr,_e,dr,rt,pr,mr,gt;return W=new Nt({}),H=new Nt({}),Q=new po({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

# For transformers v4.x+:
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

# For transformers v3.x:
# tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

# INPUT TWEET IS ALREADY NORMALIZED!
line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertweet = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v4.x+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>, use_fast=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v3.x:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TWEET IS ALREADY NORMALIZED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bertweet(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>`}}),G=new Nt({}),J=new y({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BertweetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BertweetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BertweetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BertweetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BertweetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BertweetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L68"}}),ee=new y({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L417"}}),te=new y({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L186",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),re=new y({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L387"}}),oe=new y({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L240",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ne=new y({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L212",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ie=new y({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L360"}}),le=new y({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L326"}}),ce=new Nt({}),de=new y({props:{name:"class transformers.BertweetTokenizerFast",anchor:"transformers.BertweetTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L55"}}),fe=new y({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L162",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ue=new y({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L216",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ge=new y({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){B=r("meta"),ot=l(),q=r("h1"),P=r("a"),xe=r("span"),g(W.$$.fragment),jt=l(),De=r("span"),Ct=a("BERTweet"),nt=l(),L=r("h2"),A=r("a"),Pe=r("span"),g(H.$$.fragment),Ot=l(),Ae=r("span"),Mt=a("Overview"),at=l(),R=r("p"),St=a("The BERTweet model was proposed in "),X=r("a"),Vt=a("BERTweet: A pre-trained language model for English Tweets"),Ut=a(" by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),it=l(),ke=r("p"),Wt=a("The abstract from the paper is the following:"),lt=l(),we=r("p"),Re=r("em"),Ht=a(`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),ct=l(),ve=r("p"),Xt=a("Example of use:"),dt=l(),g(Q.$$.fragment),pt=l(),E=r("p"),Qt=a("This model was contributed by "),Y=r("a"),Yt=a("dqnguyen"),Zt=a(". The original code can be found "),Z=r("a"),Gt=a("here"),Jt=a("."),mt=l(),x=r("h2"),I=r("a"),Ie=r("span"),g(G.$$.fragment),Kt=l(),Fe=r("span"),es=a("BertweetTokenizer"),ft=l(),m=r("div"),g(J.$$.fragment),ts=l(),Ne=r("p"),ss=a("Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),rs=l(),K=r("p"),os=a("This tokenizer inherits from "),be=r("a"),ns=a("PreTrainedTokenizer"),as=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),is=l(),F=r("div"),g(ee.$$.fragment),ls=l(),je=r("p"),cs=a("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),ds=l(),$=r("div"),g(te.$$.fragment),ps=l(),Ce=r("p"),ms=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),fs=l(),se=r("ul"),Te=r("li"),hs=a("single sequence: "),Oe=r("code"),us=a("<s> X </s>"),gs=l(),ye=r("li"),_s=a("pair of sequences: "),Me=r("code"),ks=a("<s> A </s></s> B </s>"),ws=l(),N=r("div"),g(re.$$.fragment),vs=l(),Se=r("p"),bs=a("Converts a sequence of tokens (string) in a single string."),Ts=l(),j=r("div"),g(oe.$$.fragment),ys=l(),Ve=r("p"),Es=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),$s=l(),C=r("div"),g(ne.$$.fragment),zs=l(),ae=r("p"),Bs=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ue=r("code"),qs=a("prepare_for_model"),Ls=a(" method."),xs=l(),O=r("div"),g(ie.$$.fragment),Ds=l(),We=r("p"),Ps=a("Normalize tokens in a Tweet"),As=l(),M=r("div"),g(le.$$.fragment),Rs=l(),He=r("p"),Is=a("Normalize a raw Tweet"),ht=l(),D=r("h2"),S=r("a"),Xe=r("span"),g(ce.$$.fragment),Fs=l(),Qe=r("span"),Ns=a("BertweetTokenizerFast"),ut=l(),h=r("div"),g(de.$$.fragment),js=l(),pe=r("p"),Cs=a("Construct a \u201CFast\u201D BPE tokenizer for BERTweet (backed by HuggingFace\u2019s "),Ye=r("em"),Os=a("tokenizers"),Ms=a(" library)."),Ss=l(),Ze=r("p"),Vs=a("Peculiarities:"),Us=l(),Ge=r("ul"),Je=r("li"),Ws=a(`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Hs=l(),me=r("p"),Xs=a("This tokenizer inherits from "),Ee=r("a"),Qs=a("PreTrainedTokenizer"),Ys=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),Zs=l(),z=r("div"),g(fe.$$.fragment),Gs=l(),Ke=r("p"),Js=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),Ks=l(),he=r("ul"),$e=r("li"),er=a("single sequence: "),et=r("code"),tr=a("<s> X </s>"),sr=l(),ze=r("li"),rr=a("pair of sequences: "),tt=r("code"),or=a("<s> A </s></s> B </s>"),nr=l(),V=r("div"),g(ue.$$.fragment),ar=l(),st=r("p"),ir=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),lr=l(),U=r("div"),g(ge.$$.fragment),cr=l(),_e=r("p"),dr=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),rt=r("code"),pr=a("prepare_for_model"),mr=a(" method."),this.h()},l(t){const p=io('[data-svelte="svelte-1phssyn"]',document.head);B=o(p,"META",{name:!0,content:!0}),p.forEach(s),ot=c(t),q=o(t,"H1",{class:!0});var _t=n(q);P=o(_t,"A",{id:!0,class:!0,href:!0});var kr=n(P);xe=o(kr,"SPAN",{});var wr=n(xe);_(W.$$.fragment,wr),wr.forEach(s),kr.forEach(s),jt=c(_t),De=o(_t,"SPAN",{});var vr=n(De);Ct=i(vr,"BERTweet"),vr.forEach(s),_t.forEach(s),nt=c(t),L=o(t,"H2",{class:!0});var kt=n(L);A=o(kt,"A",{id:!0,class:!0,href:!0});var br=n(A);Pe=o(br,"SPAN",{});var Tr=n(Pe);_(H.$$.fragment,Tr),Tr.forEach(s),br.forEach(s),Ot=c(kt),Ae=o(kt,"SPAN",{});var yr=n(Ae);Mt=i(yr,"Overview"),yr.forEach(s),kt.forEach(s),at=c(t),R=o(t,"P",{});var wt=n(R);St=i(wt,"The BERTweet model was proposed in "),X=o(wt,"A",{href:!0,rel:!0});var Er=n(X);Vt=i(Er,"BERTweet: A pre-trained language model for English Tweets"),Er.forEach(s),Ut=i(wt," by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),wt.forEach(s),it=c(t),ke=o(t,"P",{});var $r=n(ke);Wt=i($r,"The abstract from the paper is the following:"),$r.forEach(s),lt=c(t),we=o(t,"P",{});var zr=n(we);Re=o(zr,"EM",{});var Br=n(Re);Ht=i(Br,`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),Br.forEach(s),zr.forEach(s),ct=c(t),ve=o(t,"P",{});var qr=n(ve);Xt=i(qr,"Example of use:"),qr.forEach(s),dt=c(t),_(Q.$$.fragment,t),pt=c(t),E=o(t,"P",{});var Be=n(E);Qt=i(Be,"This model was contributed by "),Y=o(Be,"A",{href:!0,rel:!0});var Lr=n(Y);Yt=i(Lr,"dqnguyen"),Lr.forEach(s),Zt=i(Be,". The original code can be found "),Z=o(Be,"A",{href:!0,rel:!0});var xr=n(Z);Gt=i(xr,"here"),xr.forEach(s),Jt=i(Be,"."),Be.forEach(s),mt=c(t),x=o(t,"H2",{class:!0});var vt=n(x);I=o(vt,"A",{id:!0,class:!0,href:!0});var Dr=n(I);Ie=o(Dr,"SPAN",{});var Pr=n(Ie);_(G.$$.fragment,Pr),Pr.forEach(s),Dr.forEach(s),Kt=c(vt),Fe=o(vt,"SPAN",{});var Ar=n(Fe);es=i(Ar,"BertweetTokenizer"),Ar.forEach(s),vt.forEach(s),ft=c(t),m=o(t,"DIV",{class:!0});var u=n(m);_(J.$$.fragment,u),ts=c(u),Ne=o(u,"P",{});var Rr=n(Ne);ss=i(Rr,"Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),Rr.forEach(s),rs=c(u),K=o(u,"P",{});var bt=n(K);os=i(bt,"This tokenizer inherits from "),be=o(bt,"A",{href:!0});var Ir=n(be);ns=i(Ir,"PreTrainedTokenizer"),Ir.forEach(s),as=i(bt,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),bt.forEach(s),is=c(u),F=o(u,"DIV",{class:!0});var Tt=n(F);_(ee.$$.fragment,Tt),ls=c(Tt),je=o(Tt,"P",{});var Fr=n(je);cs=i(Fr,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),Fr.forEach(s),Tt.forEach(s),ds=c(u),$=o(u,"DIV",{class:!0});var qe=n($);_(te.$$.fragment,qe),ps=c(qe),Ce=o(qe,"P",{});var Nr=n(Ce);ms=i(Nr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),Nr.forEach(s),fs=c(qe),se=o(qe,"UL",{});var yt=n(se);Te=o(yt,"LI",{});var fr=n(Te);hs=i(fr,"single sequence: "),Oe=o(fr,"CODE",{});var jr=n(Oe);us=i(jr,"<s> X </s>"),jr.forEach(s),fr.forEach(s),gs=c(yt),ye=o(yt,"LI",{});var hr=n(ye);_s=i(hr,"pair of sequences: "),Me=o(hr,"CODE",{});var Cr=n(Me);ks=i(Cr,"<s> A </s></s> B </s>"),Cr.forEach(s),hr.forEach(s),yt.forEach(s),qe.forEach(s),ws=c(u),N=o(u,"DIV",{class:!0});var Et=n(N);_(re.$$.fragment,Et),vs=c(Et),Se=o(Et,"P",{});var Or=n(Se);bs=i(Or,"Converts a sequence of tokens (string) in a single string."),Or.forEach(s),Et.forEach(s),Ts=c(u),j=o(u,"DIV",{class:!0});var $t=n(j);_(oe.$$.fragment,$t),ys=c($t),Ve=o($t,"P",{});var Mr=n(Ve);Es=i(Mr,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),Mr.forEach(s),$t.forEach(s),$s=c(u),C=o(u,"DIV",{class:!0});var zt=n(C);_(ne.$$.fragment,zt),zs=c(zt),ae=o(zt,"P",{});var Bt=n(ae);Bs=i(Bt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ue=o(Bt,"CODE",{});var Sr=n(Ue);qs=i(Sr,"prepare_for_model"),Sr.forEach(s),Ls=i(Bt," method."),Bt.forEach(s),zt.forEach(s),xs=c(u),O=o(u,"DIV",{class:!0});var qt=n(O);_(ie.$$.fragment,qt),Ds=c(qt),We=o(qt,"P",{});var Vr=n(We);Ps=i(Vr,"Normalize tokens in a Tweet"),Vr.forEach(s),qt.forEach(s),As=c(u),M=o(u,"DIV",{class:!0});var Lt=n(M);_(le.$$.fragment,Lt),Rs=c(Lt),He=o(Lt,"P",{});var Ur=n(He);Is=i(Ur,"Normalize a raw Tweet"),Ur.forEach(s),Lt.forEach(s),u.forEach(s),ht=c(t),D=o(t,"H2",{class:!0});var xt=n(D);S=o(xt,"A",{id:!0,class:!0,href:!0});var Wr=n(S);Xe=o(Wr,"SPAN",{});var Hr=n(Xe);_(ce.$$.fragment,Hr),Hr.forEach(s),Wr.forEach(s),Fs=c(xt),Qe=o(xt,"SPAN",{});var Xr=n(Qe);Ns=i(Xr,"BertweetTokenizerFast"),Xr.forEach(s),xt.forEach(s),ut=c(t),h=o(t,"DIV",{class:!0});var T=n(h);_(de.$$.fragment,T),js=c(T),pe=o(T,"P",{});var Dt=n(pe);Cs=i(Dt,"Construct a \u201CFast\u201D BPE tokenizer for BERTweet (backed by HuggingFace\u2019s "),Ye=o(Dt,"EM",{});var Qr=n(Ye);Os=i(Qr,"tokenizers"),Qr.forEach(s),Ms=i(Dt," library)."),Dt.forEach(s),Ss=c(T),Ze=o(T,"P",{});var Yr=n(Ze);Vs=i(Yr,"Peculiarities:"),Yr.forEach(s),Us=c(T),Ge=o(T,"UL",{});var Zr=n(Ge);Je=o(Zr,"LI",{});var Gr=n(Je);Ws=i(Gr,`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Gr.forEach(s),Zr.forEach(s),Hs=c(T),me=o(T,"P",{});var Pt=n(me);Xs=i(Pt,"This tokenizer inherits from "),Ee=o(Pt,"A",{href:!0});var Jr=n(Ee);Qs=i(Jr,"PreTrainedTokenizer"),Jr.forEach(s),Ys=i(Pt,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),Pt.forEach(s),Zs=c(T),z=o(T,"DIV",{class:!0});var Le=n(z);_(fe.$$.fragment,Le),Gs=c(Le),Ke=o(Le,"P",{});var Kr=n(Ke);Js=i(Kr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),Kr.forEach(s),Ks=c(Le),he=o(Le,"UL",{});var At=n(he);$e=o(At,"LI",{});var ur=n($e);er=i(ur,"single sequence: "),et=o(ur,"CODE",{});var eo=n(et);tr=i(eo,"<s> X </s>"),eo.forEach(s),ur.forEach(s),sr=c(At),ze=o(At,"LI",{});var gr=n(ze);rr=i(gr,"pair of sequences: "),tt=o(gr,"CODE",{});var to=n(tt);or=i(to,"<s> A </s></s> B </s>"),to.forEach(s),gr.forEach(s),At.forEach(s),Le.forEach(s),nr=c(T),V=o(T,"DIV",{class:!0});var Rt=n(V);_(ue.$$.fragment,Rt),ar=c(Rt),st=o(Rt,"P",{});var so=n(st);ir=i(so,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),so.forEach(s),Rt.forEach(s),lr=c(T),U=o(T,"DIV",{class:!0});var It=n(U);_(ge.$$.fragment,It),cr=c(It),_e=o(It,"P",{});var Ft=n(_e);dr=i(Ft,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),rt=o(Ft,"CODE",{});var ro=n(rt);pr=i(ro,"prepare_for_model"),ro.forEach(s),mr=i(Ft," method."),Ft.forEach(s),It.forEach(s),T.forEach(s),this.h()},h(){d(B,"name","hf:doc:metadata"),d(B,"content",JSON.stringify(fo)),d(P,"id","bertweet"),d(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P,"href","#bertweet"),d(q,"class","relative group"),d(A,"id","overview"),d(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(A,"href","#overview"),d(L,"class","relative group"),d(X,"href","https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf"),d(X,"rel","nofollow"),d(Y,"href","https://huggingface.co/dqnguyen"),d(Y,"rel","nofollow"),d(Z,"href","https://github.com/VinAIResearch/BERTweet"),d(Z,"rel","nofollow"),d(I,"id","transformers.BertweetTokenizer"),d(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I,"href","#transformers.BertweetTokenizer"),d(x,"class","relative group"),d(be,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"id","transformers.BertweetTokenizerFast"),d(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(S,"href","#transformers.BertweetTokenizerFast"),d(D,"class","relative group"),d(Ee,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,B),f(t,ot,p),f(t,q,p),e(q,P),e(P,xe),k(W,xe,null),e(q,jt),e(q,De),e(De,Ct),f(t,nt,p),f(t,L,p),e(L,A),e(A,Pe),k(H,Pe,null),e(L,Ot),e(L,Ae),e(Ae,Mt),f(t,at,p),f(t,R,p),e(R,St),e(R,X),e(X,Vt),e(R,Ut),f(t,it,p),f(t,ke,p),e(ke,Wt),f(t,lt,p),f(t,we,p),e(we,Re),e(Re,Ht),f(t,ct,p),f(t,ve,p),e(ve,Xt),f(t,dt,p),k(Q,t,p),f(t,pt,p),f(t,E,p),e(E,Qt),e(E,Y),e(Y,Yt),e(E,Zt),e(E,Z),e(Z,Gt),e(E,Jt),f(t,mt,p),f(t,x,p),e(x,I),e(I,Ie),k(G,Ie,null),e(x,Kt),e(x,Fe),e(Fe,es),f(t,ft,p),f(t,m,p),k(J,m,null),e(m,ts),e(m,Ne),e(Ne,ss),e(m,rs),e(m,K),e(K,os),e(K,be),e(be,ns),e(K,as),e(m,is),e(m,F),k(ee,F,null),e(F,ls),e(F,je),e(je,cs),e(m,ds),e(m,$),k(te,$,null),e($,ps),e($,Ce),e(Ce,ms),e($,fs),e($,se),e(se,Te),e(Te,hs),e(Te,Oe),e(Oe,us),e(se,gs),e(se,ye),e(ye,_s),e(ye,Me),e(Me,ks),e(m,ws),e(m,N),k(re,N,null),e(N,vs),e(N,Se),e(Se,bs),e(m,Ts),e(m,j),k(oe,j,null),e(j,ys),e(j,Ve),e(Ve,Es),e(m,$s),e(m,C),k(ne,C,null),e(C,zs),e(C,ae),e(ae,Bs),e(ae,Ue),e(Ue,qs),e(ae,Ls),e(m,xs),e(m,O),k(ie,O,null),e(O,Ds),e(O,We),e(We,Ps),e(m,As),e(m,M),k(le,M,null),e(M,Rs),e(M,He),e(He,Is),f(t,ht,p),f(t,D,p),e(D,S),e(S,Xe),k(ce,Xe,null),e(D,Fs),e(D,Qe),e(Qe,Ns),f(t,ut,p),f(t,h,p),k(de,h,null),e(h,js),e(h,pe),e(pe,Cs),e(pe,Ye),e(Ye,Os),e(pe,Ms),e(h,Ss),e(h,Ze),e(Ze,Vs),e(h,Us),e(h,Ge),e(Ge,Je),e(Je,Ws),e(h,Hs),e(h,me),e(me,Xs),e(me,Ee),e(Ee,Qs),e(me,Ys),e(h,Zs),e(h,z),k(fe,z,null),e(z,Gs),e(z,Ke),e(Ke,Js),e(z,Ks),e(z,he),e(he,$e),e($e,er),e($e,et),e(et,tr),e(he,sr),e(he,ze),e(ze,rr),e(ze,tt),e(tt,or),e(h,nr),e(h,V),k(ue,V,null),e(V,ar),e(V,st),e(st,ir),e(h,lr),e(h,U),k(ge,U,null),e(U,cr),e(U,_e),e(_e,dr),e(_e,rt),e(rt,pr),e(_e,mr),gt=!0},p:lo,i(t){gt||(w(W.$$.fragment,t),w(H.$$.fragment,t),w(Q.$$.fragment,t),w(G.$$.fragment,t),w(J.$$.fragment,t),w(ee.$$.fragment,t),w(te.$$.fragment,t),w(re.$$.fragment,t),w(oe.$$.fragment,t),w(ne.$$.fragment,t),w(ie.$$.fragment,t),w(le.$$.fragment,t),w(ce.$$.fragment,t),w(de.$$.fragment,t),w(fe.$$.fragment,t),w(ue.$$.fragment,t),w(ge.$$.fragment,t),gt=!0)},o(t){v(W.$$.fragment,t),v(H.$$.fragment,t),v(Q.$$.fragment,t),v(G.$$.fragment,t),v(J.$$.fragment,t),v(ee.$$.fragment,t),v(te.$$.fragment,t),v(re.$$.fragment,t),v(oe.$$.fragment,t),v(ne.$$.fragment,t),v(ie.$$.fragment,t),v(le.$$.fragment,t),v(ce.$$.fragment,t),v(de.$$.fragment,t),v(fe.$$.fragment,t),v(ue.$$.fragment,t),v(ge.$$.fragment,t),gt=!1},d(t){s(B),t&&s(ot),t&&s(q),b(W),t&&s(nt),t&&s(L),b(H),t&&s(at),t&&s(R),t&&s(it),t&&s(ke),t&&s(lt),t&&s(we),t&&s(ct),t&&s(ve),t&&s(dt),b(Q,t),t&&s(pt),t&&s(E),t&&s(mt),t&&s(x),b(G),t&&s(ft),t&&s(m),b(J),b(ee),b(te),b(re),b(oe),b(ne),b(ie),b(le),t&&s(ht),t&&s(D),b(ce),t&&s(ut),t&&s(h),b(de),b(fe),b(ue),b(ge)}}}const fo={local:"bertweet",sections:[{local:"overview",title:"Overview"},{local:"transformers.BertweetTokenizer",title:"BertweetTokenizer"},{local:"transformers.BertweetTokenizerFast",title:"BertweetTokenizerFast"}],title:"BERTweet"};function ho(_r){return co(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wo extends oo{constructor(B){super();no(this,B,ho,mo,ao,{})}}export{wo as default,fo as metadata};
