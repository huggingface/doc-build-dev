import{S as Zn,i as es,s as ts,e as n,k as p,w as g,t as a,M as os,c as s,d as o,m as d,a as r,x as _,h as i,b as l,G as e,g as h,y as k,L as ns,q as b,o as v,B as T,v as ss}from"../../chunks/vendor-316217a5.js";import{D as V}from"../../chunks/Docstring-1fe4a33a.js";import{C as Yn}from"../../chunks/CodeBlock-8418a95e.js";import{I as Pt}from"../../chunks/IconCopyLink-40460835.js";function rs(dn){let $,tt,A,P,De,X,Dt,je,jt,ot,E,D,Fe,U,Ft,Ie,It,nt,j,St,W,Mt,Nt,st,fe,Ct,rt,ge,Se,Ot,at,_e,Vt,it,H,lt,ke,Xt,pt,be,G,Ut,ve,Wt,Ht,dt,K,ct,Te,Me,Gt,ht,y,Kt,Q,Qt,Jt,J,Yt,Zt,mt,x,F,Ne,Y,eo,Ce,to,ut,m,Z,oo,L,no,we,so,ro,ee,ao,io,lo,te,po,ye,co,ho,mo,q,oe,uo,Oe,fo,go,ne,qe,_o,Ve,ko,bo,ze,vo,Xe,To,wo,I,se,yo,Ue,qo,zo,S,re,Bo,We,$o,Ao,M,ae,Eo,ie,xo,He,Lo,Ro,ft,R,N,Ge,le,Po,Ke,Do,gt,u,pe,jo,w,Fo,Qe,Io,So,Be,Mo,No,de,Co,Oo,Vo,ce,Xo,$e,Uo,Wo,Ho,z,he,Go,Je,Ko,Qo,me,Ae,Jo,Ye,Yo,Zo,Ee,en,Ze,tn,on,C,ue,nn,et,sn,_t;return X=new Pt({}),U=new Pt({}),H=new Yn({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

bartpho = AutoModel.from_pretrained("vinai/bartpho-syllable")

tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-syllable")

line = "Ch\xFAng t\xF4i l\xE0 nh\u1EEFng nghi\xEAn c\u1EE9u vi\xEAn."

input_ids = tokenizer(line, return_tensors="pt")

with torch.no_grad():
    features = bartpho(**input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
from transformers import TFAutoModel

bartpho = TFAutoModel.from_pretrained("vinai/bartpho-syllable")
input_ids = tokenizer(line, return_tensors="tf")
features = bartpho(**input_ids)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;Ch\xFAng t\xF4i l\xE0 nh\u1EEFng nghi\xEAn c\u1EE9u vi\xEAn.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bartpho(**input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(line, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>features = bartpho(**input_ids)`}}),K=new Yn({props:{code:`from transformers import MBartForConditionalGeneration

bartpho = MBartForConditionalGeneration.from_pretrained("vinai/bartpho-syllable")
TXT = "Ch\xFAng t\xF4i l\xE0 <mask> nghi\xEAn c\u1EE9u vi\xEAn."
input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]
logits = bartpho(input_ids).logits
masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)
print(tokenizer.decode(predictions).split())`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MBartForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>bartpho = MBartForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;vinai/bartpho-syllable&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>TXT = <span class="hljs-string">&quot;Ch\xFAng t\xF4i l\xE0 &lt;mask&gt; nghi\xEAn c\u1EE9u vi\xEAn.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer([TXT], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = bartpho(input_ids).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>masked_index = (input_ids[<span class="hljs-number">0</span>] == tokenizer.mask_token_id).nonzero().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits[<span class="hljs-number">0</span>, masked_index].softmax(dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>values, predictions = probs.topk(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(predictions).split())`}}),Y=new Pt({}),Z=new V({props:{name:"class transformers.BartphoTokenizer",anchor:"transformers.BartphoTokenizer",parameters:[{name:"vocab_file",val:""},{name:"monolingual_vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file. This vocabulary is the pre-trained SentencePiece model available from the
multilingual XLM-RoBERTa, also used in mBART, consisting of 250K types.`,name:"vocab_file"},{anchor:"transformers.BartphoTokenizer.monolingual_vocab_file",description:`<strong>monolingual_vocab_file</strong> (<code>str</code>) &#x2014;
Path to the monolingual vocabulary file. This monolingual vocabulary consists of Vietnamese-specialized
types extracted from the multilingual vocabulary vocab_file of 250K types.`,name:"monolingual_vocab_file"},{anchor:"transformers.BartphoTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BartphoTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BartphoTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BartphoTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BartphoTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BartphoTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BartphoTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BartphoTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.BartphoTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.BartphoTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L46"}}),oe=new V({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L193",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),se=new V({props:{name:"convert_tokens_to_string",anchor:"transformers.BartphoTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L295"}}),re=new V({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L247",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ae=new V({props:{name:"get_special_tokens_mask",anchor:"transformers.BartphoTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BartphoTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho.py#L219",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),le=new Pt({}),pe=new V({props:{name:"class transformers.BartphoTokenizerFast",anchor:"transformers.BartphoTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BartphoTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BartphoTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BartphoTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BartphoTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BartphoTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BartphoTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BartphoTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BartphoTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L59"}}),he=new V({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L198",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ue=new V({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BartphoTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bartpho/tokenization_bartpho_fast.py#L224",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){$=n("meta"),tt=p(),A=n("h1"),P=n("a"),De=n("span"),g(X.$$.fragment),Dt=p(),je=n("span"),jt=a("BARTpho"),ot=p(),E=n("h2"),D=n("a"),Fe=n("span"),g(U.$$.fragment),Ft=p(),Ie=n("span"),It=a("Overview"),nt=p(),j=n("p"),St=a("The BARTpho model was proposed in "),W=n("a"),Mt=a("BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"),Nt=a(" by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen."),st=p(),fe=n("p"),Ct=a("The abstract from the paper is the following:"),rt=p(),ge=n("p"),Se=n("em"),Ot=a(`We present BARTpho with two versions \u2014 BARTpho_word and BARTpho_syllable \u2014 the first public large-scale monolingual
sequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \u201Clarge\u201D architecture and pre-training
scheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments
on a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho
outperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future
research and applications of generative Vietnamese NLP tasks.`),at=p(),_e=n("p"),Vt=a("Example of use:"),it=p(),g(H.$$.fragment),lt=p(),ke=n("p"),Xt=a("Tips:"),pt=p(),be=n("ul"),G=n("li"),Ut=a(`Following mBART, BARTpho uses the \u201Clarge\u201D architecture of BART with an additional layer-normalization layer on top of
both the encoder and decoder. Thus, usage examples in the `),ve=n("a"),Wt=a("documentation of BART"),Ht=a(`, when adapting to use
with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.
For example:`),dt=p(),g(K.$$.fragment),ct=p(),Te=n("ul"),Me=n("li"),Gt=a(`This implementation is only for tokenization: \u201Cmonolingual_vocab_file\u201D consists of Vietnamese-specialized types
extracted from the pre-trained SentencePiece model \u201Cvocab_file\u201D that is available from the multilingual XLM-RoBERTa.
Other languages, if employing this pre-trained multilingual SentencePiece model \u201Cvocab_file\u201D for subword
segmentation, can reuse BartphoTokenizer with their own language-specialized \u201Cmonolingual_vocab_file\u201D.`),ht=p(),y=n("p"),Kt=a("This model was contributed by "),Q=n("a"),Qt=a("dqnguyen"),Jt=a(". The original code can be found "),J=n("a"),Yt=a("here"),Zt=a("."),mt=p(),x=n("h2"),F=n("a"),Ne=n("span"),g(Y.$$.fragment),eo=p(),Ce=n("span"),to=a("BartphoTokenizer"),ut=p(),m=n("div"),g(Z.$$.fragment),oo=p(),L=n("p"),no=a("Adapted from "),we=n("a"),so=a("XLMRobertaTokenizer"),ro=a(". Based on "),ee=n("a"),ao=a("SentencePiece"),io=a("."),lo=p(),te=n("p"),po=a("This tokenizer inherits from "),ye=n("a"),co=a("PreTrainedTokenizer"),ho=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),mo=p(),q=n("div"),g(oe.$$.fragment),uo=p(),Oe=n("p"),fo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An BARTPho sequence has the following format:`),go=p(),ne=n("ul"),qe=n("li"),_o=a("single sequence: "),Ve=n("code"),ko=a("<s> X </s>"),bo=p(),ze=n("li"),vo=a("pair of sequences: "),Xe=n("code"),To=a("<s> A </s></s> B </s>"),wo=p(),I=n("div"),g(se.$$.fragment),yo=p(),Ue=n("p"),qo=a("Converts a sequence of tokens (strings for sub-words) in a single string."),zo=p(),S=n("div"),g(re.$$.fragment),Bo=p(),We=n("p"),$o=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not
make use of token type ids, therefore a list of zeros is returned.`),Ao=p(),M=n("div"),g(ae.$$.fragment),Eo=p(),ie=n("p"),xo=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),He=n("code"),Lo=a("prepare_for_model"),Ro=a(" method."),ft=p(),R=n("h2"),N=n("a"),Ge=n("span"),g(le.$$.fragment),Po=p(),Ke=n("span"),Do=a("BartphoTokenizerFast"),gt=p(),u=n("div"),g(pe.$$.fragment),jo=p(),w=n("p"),Fo=a("Construct a \u201Cfast\u201D BARTpho tokenizer (backed by HuggingFace\u2019s "),Qe=n("em"),Io=a("tokenizers"),So=a(` library). Adapted from
`),Be=n("a"),Mo=a("XLMRobertaTokenizerFast"),No=a(". Based on "),de=n("a"),Co=a("SentencePiece"),Oo=a("."),Vo=p(),ce=n("p"),Xo=a("This tokenizer inherits from "),$e=n("a"),Uo=a("PreTrainedTokenizerFast"),Wo=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Ho=p(),z=n("div"),g(he.$$.fragment),Go=p(),Je=n("p"),Ko=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARTpho sequence has the following format:`),Qo=p(),me=n("ul"),Ae=n("li"),Jo=a("single sequence: "),Ye=n("code"),Yo=a("<s> X </s>"),Zo=p(),Ee=n("li"),en=a("pair of sequences: "),Ze=n("code"),tn=a("<s> A </s></s> B </s>"),on=p(),C=n("div"),g(ue.$$.fragment),nn=p(),et=n("p"),sn=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTpho does not
make use of token type ids, therefore a list of zeros is returned.`),this.h()},l(t){const c=os('[data-svelte="svelte-1phssyn"]',document.head);$=s(c,"META",{name:!0,content:!0}),c.forEach(o),tt=d(t),A=s(t,"H1",{class:!0});var kt=r(A);P=s(kt,"A",{id:!0,class:!0,href:!0});var cn=r(P);De=s(cn,"SPAN",{});var hn=r(De);_(X.$$.fragment,hn),hn.forEach(o),cn.forEach(o),Dt=d(kt),je=s(kt,"SPAN",{});var mn=r(je);jt=i(mn,"BARTpho"),mn.forEach(o),kt.forEach(o),ot=d(t),E=s(t,"H2",{class:!0});var bt=r(E);D=s(bt,"A",{id:!0,class:!0,href:!0});var un=r(D);Fe=s(un,"SPAN",{});var fn=r(Fe);_(U.$$.fragment,fn),fn.forEach(o),un.forEach(o),Ft=d(bt),Ie=s(bt,"SPAN",{});var gn=r(Ie);It=i(gn,"Overview"),gn.forEach(o),bt.forEach(o),nt=d(t),j=s(t,"P",{});var vt=r(j);St=i(vt,"The BARTpho model was proposed in "),W=s(vt,"A",{href:!0,rel:!0});var _n=r(W);Mt=i(_n,"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"),_n.forEach(o),Nt=i(vt," by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen."),vt.forEach(o),st=d(t),fe=s(t,"P",{});var kn=r(fe);Ct=i(kn,"The abstract from the paper is the following:"),kn.forEach(o),rt=d(t),ge=s(t,"P",{});var bn=r(ge);Se=s(bn,"EM",{});var vn=r(Se);Ot=i(vn,`We present BARTpho with two versions \u2014 BARTpho_word and BARTpho_syllable \u2014 the first public large-scale monolingual
sequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \u201Clarge\u201D architecture and pre-training
scheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments
on a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho
outperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future
research and applications of generative Vietnamese NLP tasks.`),vn.forEach(o),bn.forEach(o),at=d(t),_e=s(t,"P",{});var Tn=r(_e);Vt=i(Tn,"Example of use:"),Tn.forEach(o),it=d(t),_(H.$$.fragment,t),lt=d(t),ke=s(t,"P",{});var wn=r(ke);Xt=i(wn,"Tips:"),wn.forEach(o),pt=d(t),be=s(t,"UL",{});var yn=r(be);G=s(yn,"LI",{});var Tt=r(G);Ut=i(Tt,`Following mBART, BARTpho uses the \u201Clarge\u201D architecture of BART with an additional layer-normalization layer on top of
both the encoder and decoder. Thus, usage examples in the `),ve=s(Tt,"A",{href:!0});var qn=r(ve);Wt=i(qn,"documentation of BART"),qn.forEach(o),Ht=i(Tt,`, when adapting to use
with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.
For example:`),Tt.forEach(o),yn.forEach(o),dt=d(t),_(K.$$.fragment,t),ct=d(t),Te=s(t,"UL",{});var zn=r(Te);Me=s(zn,"LI",{});var Bn=r(Me);Gt=i(Bn,`This implementation is only for tokenization: \u201Cmonolingual_vocab_file\u201D consists of Vietnamese-specialized types
extracted from the pre-trained SentencePiece model \u201Cvocab_file\u201D that is available from the multilingual XLM-RoBERTa.
Other languages, if employing this pre-trained multilingual SentencePiece model \u201Cvocab_file\u201D for subword
segmentation, can reuse BartphoTokenizer with their own language-specialized \u201Cmonolingual_vocab_file\u201D.`),Bn.forEach(o),zn.forEach(o),ht=d(t),y=s(t,"P",{});var xe=r(y);Kt=i(xe,"This model was contributed by "),Q=s(xe,"A",{href:!0,rel:!0});var $n=r(Q);Qt=i($n,"dqnguyen"),$n.forEach(o),Jt=i(xe,". The original code can be found "),J=s(xe,"A",{href:!0,rel:!0});var An=r(J);Yt=i(An,"here"),An.forEach(o),Zt=i(xe,"."),xe.forEach(o),mt=d(t),x=s(t,"H2",{class:!0});var wt=r(x);F=s(wt,"A",{id:!0,class:!0,href:!0});var En=r(F);Ne=s(En,"SPAN",{});var xn=r(Ne);_(Y.$$.fragment,xn),xn.forEach(o),En.forEach(o),eo=d(wt),Ce=s(wt,"SPAN",{});var Ln=r(Ce);to=i(Ln,"BartphoTokenizer"),Ln.forEach(o),wt.forEach(o),ut=d(t),m=s(t,"DIV",{class:!0});var f=r(m);_(Z.$$.fragment,f),oo=d(f),L=s(f,"P",{});var Le=r(L);no=i(Le,"Adapted from "),we=s(Le,"A",{href:!0});var Rn=r(we);so=i(Rn,"XLMRobertaTokenizer"),Rn.forEach(o),ro=i(Le,". Based on "),ee=s(Le,"A",{href:!0,rel:!0});var Pn=r(ee);ao=i(Pn,"SentencePiece"),Pn.forEach(o),io=i(Le,"."),Le.forEach(o),lo=d(f),te=s(f,"P",{});var yt=r(te);po=i(yt,"This tokenizer inherits from "),ye=s(yt,"A",{href:!0});var Dn=r(ye);co=i(Dn,"PreTrainedTokenizer"),Dn.forEach(o),ho=i(yt,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),yt.forEach(o),mo=d(f),q=s(f,"DIV",{class:!0});var Re=r(q);_(oe.$$.fragment,Re),uo=d(Re),Oe=s(Re,"P",{});var jn=r(Oe);fo=i(jn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An BARTPho sequence has the following format:`),jn.forEach(o),go=d(Re),ne=s(Re,"UL",{});var qt=r(ne);qe=s(qt,"LI",{});var rn=r(qe);_o=i(rn,"single sequence: "),Ve=s(rn,"CODE",{});var Fn=r(Ve);ko=i(Fn,"<s> X </s>"),Fn.forEach(o),rn.forEach(o),bo=d(qt),ze=s(qt,"LI",{});var an=r(ze);vo=i(an,"pair of sequences: "),Xe=s(an,"CODE",{});var In=r(Xe);To=i(In,"<s> A </s></s> B </s>"),In.forEach(o),an.forEach(o),qt.forEach(o),Re.forEach(o),wo=d(f),I=s(f,"DIV",{class:!0});var zt=r(I);_(se.$$.fragment,zt),yo=d(zt),Ue=s(zt,"P",{});var Sn=r(Ue);qo=i(Sn,"Converts a sequence of tokens (strings for sub-words) in a single string."),Sn.forEach(o),zt.forEach(o),zo=d(f),S=s(f,"DIV",{class:!0});var Bt=r(S);_(re.$$.fragment,Bt),Bo=d(Bt),We=s(Bt,"P",{});var Mn=r(We);$o=i(Mn,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not
make use of token type ids, therefore a list of zeros is returned.`),Mn.forEach(o),Bt.forEach(o),Ao=d(f),M=s(f,"DIV",{class:!0});var $t=r(M);_(ae.$$.fragment,$t),Eo=d($t),ie=s($t,"P",{});var At=r(ie);xo=i(At,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),He=s(At,"CODE",{});var Nn=r(He);Lo=i(Nn,"prepare_for_model"),Nn.forEach(o),Ro=i(At," method."),At.forEach(o),$t.forEach(o),f.forEach(o),ft=d(t),R=s(t,"H2",{class:!0});var Et=r(R);N=s(Et,"A",{id:!0,class:!0,href:!0});var Cn=r(N);Ge=s(Cn,"SPAN",{});var On=r(Ge);_(le.$$.fragment,On),On.forEach(o),Cn.forEach(o),Po=d(Et),Ke=s(Et,"SPAN",{});var Vn=r(Ke);Do=i(Vn,"BartphoTokenizerFast"),Vn.forEach(o),Et.forEach(o),gt=d(t),u=s(t,"DIV",{class:!0});var B=r(u);_(pe.$$.fragment,B),jo=d(B),w=s(B,"P",{});var O=r(w);Fo=i(O,"Construct a \u201Cfast\u201D BARTpho tokenizer (backed by HuggingFace\u2019s "),Qe=s(O,"EM",{});var Xn=r(Qe);Io=i(Xn,"tokenizers"),Xn.forEach(o),So=i(O,` library). Adapted from
`),Be=s(O,"A",{href:!0});var Un=r(Be);Mo=i(Un,"XLMRobertaTokenizerFast"),Un.forEach(o),No=i(O,". Based on "),de=s(O,"A",{href:!0,rel:!0});var Wn=r(de);Co=i(Wn,"SentencePiece"),Wn.forEach(o),Oo=i(O,"."),O.forEach(o),Vo=d(B),ce=s(B,"P",{});var xt=r(ce);Xo=i(xt,"This tokenizer inherits from "),$e=s(xt,"A",{href:!0});var Hn=r($e);Uo=i(Hn,"PreTrainedTokenizerFast"),Hn.forEach(o),Wo=i(xt,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),xt.forEach(o),Ho=d(B),z=s(B,"DIV",{class:!0});var Pe=r(z);_(he.$$.fragment,Pe),Go=d(Pe),Je=s(Pe,"P",{});var Gn=r(Je);Ko=i(Gn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARTpho sequence has the following format:`),Gn.forEach(o),Qo=d(Pe),me=s(Pe,"UL",{});var Lt=r(me);Ae=s(Lt,"LI",{});var ln=r(Ae);Jo=i(ln,"single sequence: "),Ye=s(ln,"CODE",{});var Kn=r(Ye);Yo=i(Kn,"<s> X </s>"),Kn.forEach(o),ln.forEach(o),Zo=d(Lt),Ee=s(Lt,"LI",{});var pn=r(Ee);en=i(pn,"pair of sequences: "),Ze=s(pn,"CODE",{});var Qn=r(Ze);tn=i(Qn,"<s> A </s></s> B </s>"),Qn.forEach(o),pn.forEach(o),Lt.forEach(o),Pe.forEach(o),on=d(B),C=s(B,"DIV",{class:!0});var Rt=r(C);_(ue.$$.fragment,Rt),nn=d(Rt),et=s(Rt,"P",{});var Jn=r(et);sn=i(Jn,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTpho does not
make use of token type ids, therefore a list of zeros is returned.`),Jn.forEach(o),Rt.forEach(o),B.forEach(o),this.h()},h(){l($,"name","hf:doc:metadata"),l($,"content",JSON.stringify(as)),l(P,"id","bartpho"),l(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(P,"href","#bartpho"),l(A,"class","relative group"),l(D,"id","overview"),l(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(D,"href","#overview"),l(E,"class","relative group"),l(W,"href","https://arxiv.org/abs/2109.09701"),l(W,"rel","nofollow"),l(ve,"href","bart"),l(Q,"href","https://huggingface.co/dqnguyen"),l(Q,"rel","nofollow"),l(J,"href","https://github.com/VinAIResearch/BARTpho"),l(J,"rel","nofollow"),l(F,"id","transformers.BartphoTokenizer"),l(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(F,"href","#transformers.BartphoTokenizer"),l(x,"class","relative group"),l(we,"href","/docs/transformers/pr_17254/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),l(ee,"href","https://github.com/google/sentencepiece"),l(ee,"rel","nofollow"),l(ye,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(N,"id","transformers.BartphoTokenizerFast"),l(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(N,"href","#transformers.BartphoTokenizerFast"),l(R,"class","relative group"),l(Be,"href","/docs/transformers/pr_17254/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),l(de,"href","https://github.com/google/sentencepiece"),l(de,"rel","nofollow"),l($e,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,c){e(document.head,$),h(t,tt,c),h(t,A,c),e(A,P),e(P,De),k(X,De,null),e(A,Dt),e(A,je),e(je,jt),h(t,ot,c),h(t,E,c),e(E,D),e(D,Fe),k(U,Fe,null),e(E,Ft),e(E,Ie),e(Ie,It),h(t,nt,c),h(t,j,c),e(j,St),e(j,W),e(W,Mt),e(j,Nt),h(t,st,c),h(t,fe,c),e(fe,Ct),h(t,rt,c),h(t,ge,c),e(ge,Se),e(Se,Ot),h(t,at,c),h(t,_e,c),e(_e,Vt),h(t,it,c),k(H,t,c),h(t,lt,c),h(t,ke,c),e(ke,Xt),h(t,pt,c),h(t,be,c),e(be,G),e(G,Ut),e(G,ve),e(ve,Wt),e(G,Ht),h(t,dt,c),k(K,t,c),h(t,ct,c),h(t,Te,c),e(Te,Me),e(Me,Gt),h(t,ht,c),h(t,y,c),e(y,Kt),e(y,Q),e(Q,Qt),e(y,Jt),e(y,J),e(J,Yt),e(y,Zt),h(t,mt,c),h(t,x,c),e(x,F),e(F,Ne),k(Y,Ne,null),e(x,eo),e(x,Ce),e(Ce,to),h(t,ut,c),h(t,m,c),k(Z,m,null),e(m,oo),e(m,L),e(L,no),e(L,we),e(we,so),e(L,ro),e(L,ee),e(ee,ao),e(L,io),e(m,lo),e(m,te),e(te,po),e(te,ye),e(ye,co),e(te,ho),e(m,mo),e(m,q),k(oe,q,null),e(q,uo),e(q,Oe),e(Oe,fo),e(q,go),e(q,ne),e(ne,qe),e(qe,_o),e(qe,Ve),e(Ve,ko),e(ne,bo),e(ne,ze),e(ze,vo),e(ze,Xe),e(Xe,To),e(m,wo),e(m,I),k(se,I,null),e(I,yo),e(I,Ue),e(Ue,qo),e(m,zo),e(m,S),k(re,S,null),e(S,Bo),e(S,We),e(We,$o),e(m,Ao),e(m,M),k(ae,M,null),e(M,Eo),e(M,ie),e(ie,xo),e(ie,He),e(He,Lo),e(ie,Ro),h(t,ft,c),h(t,R,c),e(R,N),e(N,Ge),k(le,Ge,null),e(R,Po),e(R,Ke),e(Ke,Do),h(t,gt,c),h(t,u,c),k(pe,u,null),e(u,jo),e(u,w),e(w,Fo),e(w,Qe),e(Qe,Io),e(w,So),e(w,Be),e(Be,Mo),e(w,No),e(w,de),e(de,Co),e(w,Oo),e(u,Vo),e(u,ce),e(ce,Xo),e(ce,$e),e($e,Uo),e(ce,Wo),e(u,Ho),e(u,z),k(he,z,null),e(z,Go),e(z,Je),e(Je,Ko),e(z,Qo),e(z,me),e(me,Ae),e(Ae,Jo),e(Ae,Ye),e(Ye,Yo),e(me,Zo),e(me,Ee),e(Ee,en),e(Ee,Ze),e(Ze,tn),e(u,on),e(u,C),k(ue,C,null),e(C,nn),e(C,et),e(et,sn),_t=!0},p:ns,i(t){_t||(b(X.$$.fragment,t),b(U.$$.fragment,t),b(H.$$.fragment,t),b(K.$$.fragment,t),b(Y.$$.fragment,t),b(Z.$$.fragment,t),b(oe.$$.fragment,t),b(se.$$.fragment,t),b(re.$$.fragment,t),b(ae.$$.fragment,t),b(le.$$.fragment,t),b(pe.$$.fragment,t),b(he.$$.fragment,t),b(ue.$$.fragment,t),_t=!0)},o(t){v(X.$$.fragment,t),v(U.$$.fragment,t),v(H.$$.fragment,t),v(K.$$.fragment,t),v(Y.$$.fragment,t),v(Z.$$.fragment,t),v(oe.$$.fragment,t),v(se.$$.fragment,t),v(re.$$.fragment,t),v(ae.$$.fragment,t),v(le.$$.fragment,t),v(pe.$$.fragment,t),v(he.$$.fragment,t),v(ue.$$.fragment,t),_t=!1},d(t){o($),t&&o(tt),t&&o(A),T(X),t&&o(ot),t&&o(E),T(U),t&&o(nt),t&&o(j),t&&o(st),t&&o(fe),t&&o(rt),t&&o(ge),t&&o(at),t&&o(_e),t&&o(it),T(H,t),t&&o(lt),t&&o(ke),t&&o(pt),t&&o(be),t&&o(dt),T(K,t),t&&o(ct),t&&o(Te),t&&o(ht),t&&o(y),t&&o(mt),t&&o(x),T(Y),t&&o(ut),t&&o(m),T(Z),T(oe),T(se),T(re),T(ae),t&&o(ft),t&&o(R),T(le),t&&o(gt),t&&o(u),T(pe),T(he),T(ue)}}}const as={local:"bartpho",sections:[{local:"overview",title:"Overview"},{local:"transformers.BartphoTokenizer",title:"BartphoTokenizer"},{local:"transformers.BartphoTokenizerFast",title:"BartphoTokenizerFast"}],title:"BARTpho"};function is(dn){return ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hs extends Zn{constructor($){super();es(this,$,is,rs,ts,{})}}export{hs as default,as as metadata};
