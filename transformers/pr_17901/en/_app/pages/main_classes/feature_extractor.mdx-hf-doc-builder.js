import{S as un,i as fn,s as hn,e as a,k as c,w as h,t as s,M as gn,c as o,d as r,m as d,a as n,x as g,h as i,b as l,G as e,g as E,y as _,q as v,o as x,B as y,v as _n,L as vn}from"../../chunks/vendor-hf-doc-builder.js";import{T as pn}from"../../chunks/Tip-hf-doc-builder.js";import{D as T}from"../../chunks/Docstring-hf-doc-builder.js";import{C as xn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Vt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as yn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function bn(K){let m,k,f,$,I;return{c(){m=a("p"),k=s("Passing "),f=a("code"),$=s("use_auth_token=True"),I=s(" is required when you want to use a private model.")},l(p){m=o(p,"P",{});var w=n(m);k=i(w,"Passing "),f=o(w,"CODE",{});var L=n(f);$=i(L,"use_auth_token=True"),L.forEach(r),I=i(w," is required when you want to use a private model."),w.forEach(r)},m(p,w){E(p,m,w),e(m,k),e(m,f),e(f,$),e(m,I)},d(p){p&&r(m)}}}function $n(K){let m,k,f,$,I;return $=new xn({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),k=s("Examples:"),f=c(),h($.$$.fragment)},l(p){m=o(p,"P",{});var w=n(m);k=i(w,"Examples:"),w.forEach(r),f=d(p),g($.$$.fragment,p)},m(p,w){E(p,m,w),e(m,k),E(p,f,w),_($,p,w),I=!0},p:vn,i(p){I||(v($.$$.fragment,p),I=!0)},o(p){x($.$$.fragment,p),I=!1},d(p){p&&r(m),p&&r(f),y($,p)}}}function En(K){let m,k,f,$,I,p,w,L;return{c(){m=a("p"),k=s("If the "),f=a("code"),$=s("processed_features"),I=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),p=a("code"),w=s("return_tensors"),L=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(V){m=o(V,"P",{});var z=n(m);k=i(z,"If the "),f=o(z,"CODE",{});var D=n(f);$=i(D,"processed_features"),D.forEach(r),I=i(z,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),p=o(z,"CODE",{});var Je=n(p);w=i(Je,"return_tensors"),Je.forEach(r),L=i(z,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),z.forEach(r)},m(V,z){E(V,m,z),e(m,k),e(m,f),e(f,$),e(m,I),e(m,p),e(p,w),e(m,L)},d(V){V&&r(m)}}}function wn(K){let m,k,f,$,I,p,w,L,V,z,D,Je,nt,xr,yr,st,br,$r,Ot,O,Q,it,xe,Er,ct,wr,Wt,M,ye,Fr,dt,Ir,Tr,N,be,kr,B,Pr,Ye,zr,Dr,lt,Mr,qr,Ke,Lr,Nr,Sr,X,Br,Z,Cr,ee,$e,Ar,W,jr,mt,Vr,Or,Qe,Wr,Rr,Rt,R,te,pt,Ee,Ur,ut,Hr,Ut,C,we,Gr,ft,Jr,Yr,S,Fe,Kr,ht,Qr,Xr,U,Zr,gt,ea,ta,_t,ra,aa,oa,re,Ht,H,ae,vt,Ie,na,xt,sa,Gt,P,Te,ia,G,ca,Xe,da,la,yt,ma,pa,ua,bt,fa,ha,oe,ke,ga,$t,_a,va,ne,Pe,xa,ze,ya,Et,ba,$a,Jt,J,se,wt,De,Ea,Ft,wa,Yt,b,Me,Fa,It,Ia,Ta,ie,qe,ka,Le,Pa,Tt,za,Da,Ma,ce,Ne,qa,Se,La,kt,Na,Sa,Ba,de,Be,Ca,Ce,Aa,Pt,ja,Va,Oa,le,Ae,Wa,Y,Ra,zt,Ua,Ha,Dt,Ga,Ja,Ya,me,je,Ka,q,Qa,Mt,Xa,Za,qt,eo,to,Lt,ro,ao,Nt,oo,no,so,pe,Ve,io,Oe,co,St,lo,mo,po,ue,We,uo,Re,fo,Bt,ho,go,_o,fe,Ue,vo,He,xo,Ct,yo,bo,Kt;return p=new Vt({}),xe=new Vt({}),ye=new T({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L204"}}),be=new T({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L228",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),X=new pn({props:{$$slots:{default:[bn]},$$scope:{ctx:K}}}),Z=new yn({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[$n]},$$scope:{ctx:K}}}),$e=new T({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your feature extractor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_17901/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L312"}}),Ee=new Vt({}),we=new T({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_sequence_utils.py#L30"}}),Fe=new T({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17901/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17901/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_sequence_utils.py#L53"}}),re=new pn({props:{$$slots:{default:[En]},$$scope:{ctx:K}}}),Ie=new Vt({}),Te=new T({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L62"}}),ke=new T({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/pr_17901/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/pr_17901/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L116"}}),Pe=new T({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/feature_extraction_utils.py#L181",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),De=new Vt({}),Me=new T({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L77"}}),qe=new T({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape (n_channels, height, width) or (height, width, n_channels)) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L287",returnDescription:`
<p>A center cropped <code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape: (n_channels,
height, width).</p>
`,returnType:`
<p>new_image</p>
`}}),Ne=new T({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L119"}}),Be=new T({props:{name:"expand_dims",anchor:"transformers.ImageFeatureExtractionMixin.expand_dims",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.expand_dims.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to expand.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L166"}}),Ae=new T({props:{name:"flip_channel_order",anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image whose color channels to flip. If <code>np.ndarray</code> or <code>torch.Tensor</code>, the channel dimension should
be first.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L362"}}),je=new T({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L186"}}),Ve=new T({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L222",returnDescription:`
<p>A resized <code>PIL.Image.Image</code>.</p>
`,returnType:`
<p>image</p>
`}}),We=new T({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L133"}}),Ue=new T({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/vr_17901/src/transformers/image_utils.py#L89"}}),{c(){m=a("meta"),k=c(),f=a("h1"),$=a("a"),I=a("span"),h(p.$$.fragment),w=c(),L=a("span"),V=s("Feature Extractor"),z=c(),D=a("p"),Je=s(`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),nt=a("em"),xr=s("e.g."),yr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),st=a("em"),br=s("e.g."),$r=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Ot=c(),O=a("h2"),Q=a("a"),it=a("span"),h(xe.$$.fragment),Er=c(),ct=a("span"),wr=s("FeatureExtractionMixin"),Wt=c(),M=a("div"),h(ye.$$.fragment),Fr=c(),dt=a("p"),Ir=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Tr=c(),N=a("div"),h(be.$$.fragment),kr=c(),B=a("p"),Pr=s("Instantiate a type of "),Ye=a("a"),zr=s("FeatureExtractionMixin"),Dr=s(" from a feature extractor, "),lt=a("em"),Mr=s("e.g."),qr=s(` a
derived class of `),Ke=a("a"),Lr=s("SequenceFeatureExtractor"),Nr=s("."),Sr=c(),h(X.$$.fragment),Br=c(),h(Z.$$.fragment),Cr=c(),ee=a("div"),h($e.$$.fragment),Ar=c(),W=a("p"),jr=s("Save a feature_extractor object to the directory "),mt=a("code"),Vr=s("save_directory"),Or=s(`, so that it can be re-loaded using the
`),Qe=a("a"),Wr=s("from_pretrained()"),Rr=s(" class method."),Rt=c(),R=a("h2"),te=a("a"),pt=a("span"),h(Ee.$$.fragment),Ur=c(),ut=a("span"),Hr=s("SequenceFeatureExtractor"),Ut=c(),C=a("div"),h(we.$$.fragment),Gr=c(),ft=a("p"),Jr=s("This is a general feature extraction class for speech recognition."),Yr=c(),S=a("div"),h(Fe.$$.fragment),Kr=c(),ht=a("p"),Qr=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),Xr=c(),U=a("p"),Zr=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),gt=a("code"),ea=s("self.padding_side"),ta=s(`,
`),_t=a("code"),ra=s("self.padding_value"),aa=s(")"),oa=c(),h(re.$$.fragment),Ht=c(),H=a("h2"),ae=a("a"),vt=a("span"),h(Ie.$$.fragment),na=c(),xt=a("span"),sa=s("BatchFeature"),Gt=c(),P=a("div"),h(Te.$$.fragment),ia=c(),G=a("p"),ca=s("Holds the output of the "),Xe=a("a"),da=s("pad()"),la=s(" and feature extractor specific "),yt=a("code"),ma=s("__call__"),pa=s(" methods."),ua=c(),bt=a("p"),fa=s("This class is derived from a python dictionary and can be used as a dictionary."),ha=c(),oe=a("div"),h(ke.$$.fragment),ga=c(),$t=a("p"),_a=s("Convert the inner content to tensors."),va=c(),ne=a("div"),h(Pe.$$.fragment),xa=c(),ze=a("p"),ya=s("Send all values to device by calling "),Et=a("code"),ba=s("v.to(device)"),$a=s(" (PyTorch only)."),Jt=c(),J=a("h2"),se=a("a"),wt=a("span"),h(De.$$.fragment),Ea=c(),Ft=a("span"),wa=s("ImageFeatureExtractionMixin"),Yt=c(),b=a("div"),h(Me.$$.fragment),Fa=c(),It=a("p"),Ia=s("Mixin that contain utilities for preparing image features."),Ta=c(),ie=a("div"),h(qe.$$.fragment),ka=c(),Le=a("p"),Pa=s("Crops "),Tt=a("code"),za=s("image"),Da=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),Ma=c(),ce=a("div"),h(Ne.$$.fragment),qa=c(),Se=a("p"),La=s("Converts "),kt=a("code"),Na=s("PIL.Image.Image"),Sa=s(" to RGB format."),Ba=c(),de=a("div"),h(Be.$$.fragment),Ca=c(),Ce=a("p"),Aa=s("Expands 2-dimensional "),Pt=a("code"),ja=s("image"),Va=s(" to 3 dimensions."),Oa=c(),le=a("div"),h(Ae.$$.fragment),Wa=c(),Y=a("p"),Ra=s("Flips the channel order of "),zt=a("code"),Ua=s("image"),Ha=s(` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),Dt=a("code"),Ga=s("image"),Ja=s(" to a NumPy array if it\u2019s a PIL Image."),Ya=c(),me=a("div"),h(je.$$.fragment),Ka=c(),q=a("p"),Qa=s("Normalizes "),Mt=a("code"),Xa=s("image"),Za=s(" with "),qt=a("code"),eo=s("mean"),to=s(" and "),Lt=a("code"),ro=s("std"),ao=s(". Note that this will trigger a conversion of "),Nt=a("code"),oo=s("image"),no=s(` to a NumPy array
if it\u2019s a PIL Image.`),so=c(),pe=a("div"),h(Ve.$$.fragment),io=c(),Oe=a("p"),co=s("Resizes "),St=a("code"),lo=s("image"),mo=s(". Enforces conversion of input to PIL.Image."),po=c(),ue=a("div"),h(We.$$.fragment),uo=c(),Re=a("p"),fo=s("Converts "),Bt=a("code"),ho=s("image"),go=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),_o=c(),fe=a("div"),h(Ue.$$.fragment),vo=c(),He=a("p"),xo=s("Converts "),Ct=a("code"),yo=s("image"),bo=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const u=gn('[data-svelte="svelte-1phssyn"]',document.head);m=o(u,"META",{name:!0,content:!0}),u.forEach(r),k=d(t),f=o(t,"H1",{class:!0});var Ge=n(f);$=o(Ge,"A",{id:!0,class:!0,href:!0});var At=n($);I=o(At,"SPAN",{});var jt=n(I);g(p.$$.fragment,jt),jt.forEach(r),At.forEach(r),w=d(Ge),L=o(Ge,"SPAN",{});var $o=n(L);V=i($o,"Feature Extractor"),$o.forEach(r),Ge.forEach(r),z=d(t),D=o(t,"P",{});var Ze=n(D);Je=i(Ze,`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),nt=o(Ze,"EM",{});var Eo=n(nt);xr=i(Eo,"e.g."),Eo.forEach(r),yr=i(Ze,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),st=o(Ze,"EM",{});var wo=n(st);br=i(wo,"e.g."),wo.forEach(r),$r=i(Ze,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Ze.forEach(r),Ot=d(t),O=o(t,"H2",{class:!0});var Qt=n(O);Q=o(Qt,"A",{id:!0,class:!0,href:!0});var Fo=n(Q);it=o(Fo,"SPAN",{});var Io=n(it);g(xe.$$.fragment,Io),Io.forEach(r),Fo.forEach(r),Er=d(Qt),ct=o(Qt,"SPAN",{});var To=n(ct);wr=i(To,"FeatureExtractionMixin"),To.forEach(r),Qt.forEach(r),Wt=d(t),M=o(t,"DIV",{class:!0});var he=n(M);g(ye.$$.fragment,he),Fr=d(he),dt=o(he,"P",{});var ko=n(dt);Ir=i(ko,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),ko.forEach(r),Tr=d(he),N=o(he,"DIV",{class:!0});var ge=n(N);g(be.$$.fragment,ge),kr=d(ge),B=o(ge,"P",{});var _e=n(B);Pr=i(_e,"Instantiate a type of "),Ye=o(_e,"A",{href:!0});var Po=n(Ye);zr=i(Po,"FeatureExtractionMixin"),Po.forEach(r),Dr=i(_e," from a feature extractor, "),lt=o(_e,"EM",{});var zo=n(lt);Mr=i(zo,"e.g."),zo.forEach(r),qr=i(_e,` a
derived class of `),Ke=o(_e,"A",{href:!0});var Do=n(Ke);Lr=i(Do,"SequenceFeatureExtractor"),Do.forEach(r),Nr=i(_e,"."),_e.forEach(r),Sr=d(ge),g(X.$$.fragment,ge),Br=d(ge),g(Z.$$.fragment,ge),ge.forEach(r),Cr=d(he),ee=o(he,"DIV",{class:!0});var Xt=n(ee);g($e.$$.fragment,Xt),Ar=d(Xt),W=o(Xt,"P",{});var et=n(W);jr=i(et,"Save a feature_extractor object to the directory "),mt=o(et,"CODE",{});var Mo=n(mt);Vr=i(Mo,"save_directory"),Mo.forEach(r),Or=i(et,`, so that it can be re-loaded using the
`),Qe=o(et,"A",{href:!0});var qo=n(Qe);Wr=i(qo,"from_pretrained()"),qo.forEach(r),Rr=i(et," class method."),et.forEach(r),Xt.forEach(r),he.forEach(r),Rt=d(t),R=o(t,"H2",{class:!0});var Zt=n(R);te=o(Zt,"A",{id:!0,class:!0,href:!0});var Lo=n(te);pt=o(Lo,"SPAN",{});var No=n(pt);g(Ee.$$.fragment,No),No.forEach(r),Lo.forEach(r),Ur=d(Zt),ut=o(Zt,"SPAN",{});var So=n(ut);Hr=i(So,"SequenceFeatureExtractor"),So.forEach(r),Zt.forEach(r),Ut=d(t),C=o(t,"DIV",{class:!0});var tt=n(C);g(we.$$.fragment,tt),Gr=d(tt),ft=o(tt,"P",{});var Bo=n(ft);Jr=i(Bo,"This is a general feature extraction class for speech recognition."),Bo.forEach(r),Yr=d(tt),S=o(tt,"DIV",{class:!0});var ve=n(S);g(Fe.$$.fragment,ve),Kr=d(ve),ht=o(ve,"P",{});var Co=n(ht);Qr=i(Co,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),Co.forEach(r),Xr=d(ve),U=o(ve,"P",{});var rt=n(U);Zr=i(rt,"Padding side (left/right) padding values are defined at the feature extractor level (with "),gt=o(rt,"CODE",{});var Ao=n(gt);ea=i(Ao,"self.padding_side"),Ao.forEach(r),ta=i(rt,`,
`),_t=o(rt,"CODE",{});var jo=n(_t);ra=i(jo,"self.padding_value"),jo.forEach(r),aa=i(rt,")"),rt.forEach(r),oa=d(ve),g(re.$$.fragment,ve),ve.forEach(r),tt.forEach(r),Ht=d(t),H=o(t,"H2",{class:!0});var er=n(H);ae=o(er,"A",{id:!0,class:!0,href:!0});var Vo=n(ae);vt=o(Vo,"SPAN",{});var Oo=n(vt);g(Ie.$$.fragment,Oo),Oo.forEach(r),Vo.forEach(r),na=d(er),xt=o(er,"SPAN",{});var Wo=n(xt);sa=i(Wo,"BatchFeature"),Wo.forEach(r),er.forEach(r),Gt=d(t),P=o(t,"DIV",{class:!0});var A=n(P);g(Te.$$.fragment,A),ia=d(A),G=o(A,"P",{});var at=n(G);ca=i(at,"Holds the output of the "),Xe=o(at,"A",{href:!0});var Ro=n(Xe);da=i(Ro,"pad()"),Ro.forEach(r),la=i(at," and feature extractor specific "),yt=o(at,"CODE",{});var Uo=n(yt);ma=i(Uo,"__call__"),Uo.forEach(r),pa=i(at," methods."),at.forEach(r),ua=d(A),bt=o(A,"P",{});var Ho=n(bt);fa=i(Ho,"This class is derived from a python dictionary and can be used as a dictionary."),Ho.forEach(r),ha=d(A),oe=o(A,"DIV",{class:!0});var tr=n(oe);g(ke.$$.fragment,tr),ga=d(tr),$t=o(tr,"P",{});var Go=n($t);_a=i(Go,"Convert the inner content to tensors."),Go.forEach(r),tr.forEach(r),va=d(A),ne=o(A,"DIV",{class:!0});var rr=n(ne);g(Pe.$$.fragment,rr),xa=d(rr),ze=o(rr,"P",{});var ar=n(ze);ya=i(ar,"Send all values to device by calling "),Et=o(ar,"CODE",{});var Jo=n(Et);ba=i(Jo,"v.to(device)"),Jo.forEach(r),$a=i(ar," (PyTorch only)."),ar.forEach(r),rr.forEach(r),A.forEach(r),Jt=d(t),J=o(t,"H2",{class:!0});var or=n(J);se=o(or,"A",{id:!0,class:!0,href:!0});var Yo=n(se);wt=o(Yo,"SPAN",{});var Ko=n(wt);g(De.$$.fragment,Ko),Ko.forEach(r),Yo.forEach(r),Ea=d(or),Ft=o(or,"SPAN",{});var Qo=n(Ft);wa=i(Qo,"ImageFeatureExtractionMixin"),Qo.forEach(r),or.forEach(r),Yt=d(t),b=o(t,"DIV",{class:!0});var F=n(b);g(Me.$$.fragment,F),Fa=d(F),It=o(F,"P",{});var Xo=n(It);Ia=i(Xo,"Mixin that contain utilities for preparing image features."),Xo.forEach(r),Ta=d(F),ie=o(F,"DIV",{class:!0});var nr=n(ie);g(qe.$$.fragment,nr),ka=d(nr),Le=o(nr,"P",{});var sr=n(Le);Pa=i(sr,"Crops "),Tt=o(sr,"CODE",{});var Zo=n(Tt);za=i(Zo,"image"),Zo.forEach(r),Da=i(sr,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),sr.forEach(r),nr.forEach(r),Ma=d(F),ce=o(F,"DIV",{class:!0});var ir=n(ce);g(Ne.$$.fragment,ir),qa=d(ir),Se=o(ir,"P",{});var cr=n(Se);La=i(cr,"Converts "),kt=o(cr,"CODE",{});var en=n(kt);Na=i(en,"PIL.Image.Image"),en.forEach(r),Sa=i(cr," to RGB format."),cr.forEach(r),ir.forEach(r),Ba=d(F),de=o(F,"DIV",{class:!0});var dr=n(de);g(Be.$$.fragment,dr),Ca=d(dr),Ce=o(dr,"P",{});var lr=n(Ce);Aa=i(lr,"Expands 2-dimensional "),Pt=o(lr,"CODE",{});var tn=n(Pt);ja=i(tn,"image"),tn.forEach(r),Va=i(lr," to 3 dimensions."),lr.forEach(r),dr.forEach(r),Oa=d(F),le=o(F,"DIV",{class:!0});var mr=n(le);g(Ae.$$.fragment,mr),Wa=d(mr),Y=o(mr,"P",{});var ot=n(Y);Ra=i(ot,"Flips the channel order of "),zt=o(ot,"CODE",{});var rn=n(zt);Ua=i(rn,"image"),rn.forEach(r),Ha=i(ot,` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),Dt=o(ot,"CODE",{});var an=n(Dt);Ga=i(an,"image"),an.forEach(r),Ja=i(ot," to a NumPy array if it\u2019s a PIL Image."),ot.forEach(r),mr.forEach(r),Ya=d(F),me=o(F,"DIV",{class:!0});var pr=n(me);g(je.$$.fragment,pr),Ka=d(pr),q=o(pr,"P",{});var j=n(q);Qa=i(j,"Normalizes "),Mt=o(j,"CODE",{});var on=n(Mt);Xa=i(on,"image"),on.forEach(r),Za=i(j," with "),qt=o(j,"CODE",{});var nn=n(qt);eo=i(nn,"mean"),nn.forEach(r),to=i(j," and "),Lt=o(j,"CODE",{});var sn=n(Lt);ro=i(sn,"std"),sn.forEach(r),ao=i(j,". Note that this will trigger a conversion of "),Nt=o(j,"CODE",{});var cn=n(Nt);oo=i(cn,"image"),cn.forEach(r),no=i(j,` to a NumPy array
if it\u2019s a PIL Image.`),j.forEach(r),pr.forEach(r),so=d(F),pe=o(F,"DIV",{class:!0});var ur=n(pe);g(Ve.$$.fragment,ur),io=d(ur),Oe=o(ur,"P",{});var fr=n(Oe);co=i(fr,"Resizes "),St=o(fr,"CODE",{});var dn=n(St);lo=i(dn,"image"),dn.forEach(r),mo=i(fr,". Enforces conversion of input to PIL.Image."),fr.forEach(r),ur.forEach(r),po=d(F),ue=o(F,"DIV",{class:!0});var hr=n(ue);g(We.$$.fragment,hr),uo=d(hr),Re=o(hr,"P",{});var gr=n(Re);fo=i(gr,"Converts "),Bt=o(gr,"CODE",{});var ln=n(Bt);ho=i(ln,"image"),ln.forEach(r),go=i(gr,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),gr.forEach(r),hr.forEach(r),_o=d(F),fe=o(F,"DIV",{class:!0});var _r=n(fe);g(Ue.$$.fragment,_r),vo=d(_r),He=o(_r,"P",{});var vr=n(He);xo=i(vr,"Converts "),Ct=o(vr,"CODE",{});var mn=n(Ct);yo=i(mn,"image"),mn.forEach(r),bo=i(vr,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),vr.forEach(r),_r.forEach(r),F.forEach(r),this.h()},h(){l(m,"name","hf:doc:metadata"),l(m,"content",JSON.stringify(Fn)),l($,"id","feature-extractor"),l($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($,"href","#feature-extractor"),l(f,"class","relative group"),l(Q,"id","transformers.FeatureExtractionMixin"),l(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Q,"href","#transformers.FeatureExtractionMixin"),l(O,"class","relative group"),l(Ye,"href","/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(Ke,"href","/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Qe,"href","/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained"),l(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(te,"id","transformers.SequenceFeatureExtractor"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#transformers.SequenceFeatureExtractor"),l(R,"class","relative group"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ae,"id","transformers.BatchFeature"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#transformers.BatchFeature"),l(H,"class","relative group"),l(Xe,"href","/docs/transformers/pr_17901/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),l(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(se,"id","transformers.ImageFeatureExtractionMixin"),l(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(se,"href","#transformers.ImageFeatureExtractionMixin"),l(J,"class","relative group"),l(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,m),E(t,k,u),E(t,f,u),e(f,$),e($,I),_(p,I,null),e(f,w),e(f,L),e(L,V),E(t,z,u),E(t,D,u),e(D,Je),e(D,nt),e(nt,xr),e(D,yr),e(D,st),e(st,br),e(D,$r),E(t,Ot,u),E(t,O,u),e(O,Q),e(Q,it),_(xe,it,null),e(O,Er),e(O,ct),e(ct,wr),E(t,Wt,u),E(t,M,u),_(ye,M,null),e(M,Fr),e(M,dt),e(dt,Ir),e(M,Tr),e(M,N),_(be,N,null),e(N,kr),e(N,B),e(B,Pr),e(B,Ye),e(Ye,zr),e(B,Dr),e(B,lt),e(lt,Mr),e(B,qr),e(B,Ke),e(Ke,Lr),e(B,Nr),e(N,Sr),_(X,N,null),e(N,Br),_(Z,N,null),e(M,Cr),e(M,ee),_($e,ee,null),e(ee,Ar),e(ee,W),e(W,jr),e(W,mt),e(mt,Vr),e(W,Or),e(W,Qe),e(Qe,Wr),e(W,Rr),E(t,Rt,u),E(t,R,u),e(R,te),e(te,pt),_(Ee,pt,null),e(R,Ur),e(R,ut),e(ut,Hr),E(t,Ut,u),E(t,C,u),_(we,C,null),e(C,Gr),e(C,ft),e(ft,Jr),e(C,Yr),e(C,S),_(Fe,S,null),e(S,Kr),e(S,ht),e(ht,Qr),e(S,Xr),e(S,U),e(U,Zr),e(U,gt),e(gt,ea),e(U,ta),e(U,_t),e(_t,ra),e(U,aa),e(S,oa),_(re,S,null),E(t,Ht,u),E(t,H,u),e(H,ae),e(ae,vt),_(Ie,vt,null),e(H,na),e(H,xt),e(xt,sa),E(t,Gt,u),E(t,P,u),_(Te,P,null),e(P,ia),e(P,G),e(G,ca),e(G,Xe),e(Xe,da),e(G,la),e(G,yt),e(yt,ma),e(G,pa),e(P,ua),e(P,bt),e(bt,fa),e(P,ha),e(P,oe),_(ke,oe,null),e(oe,ga),e(oe,$t),e($t,_a),e(P,va),e(P,ne),_(Pe,ne,null),e(ne,xa),e(ne,ze),e(ze,ya),e(ze,Et),e(Et,ba),e(ze,$a),E(t,Jt,u),E(t,J,u),e(J,se),e(se,wt),_(De,wt,null),e(J,Ea),e(J,Ft),e(Ft,wa),E(t,Yt,u),E(t,b,u),_(Me,b,null),e(b,Fa),e(b,It),e(It,Ia),e(b,Ta),e(b,ie),_(qe,ie,null),e(ie,ka),e(ie,Le),e(Le,Pa),e(Le,Tt),e(Tt,za),e(Le,Da),e(b,Ma),e(b,ce),_(Ne,ce,null),e(ce,qa),e(ce,Se),e(Se,La),e(Se,kt),e(kt,Na),e(Se,Sa),e(b,Ba),e(b,de),_(Be,de,null),e(de,Ca),e(de,Ce),e(Ce,Aa),e(Ce,Pt),e(Pt,ja),e(Ce,Va),e(b,Oa),e(b,le),_(Ae,le,null),e(le,Wa),e(le,Y),e(Y,Ra),e(Y,zt),e(zt,Ua),e(Y,Ha),e(Y,Dt),e(Dt,Ga),e(Y,Ja),e(b,Ya),e(b,me),_(je,me,null),e(me,Ka),e(me,q),e(q,Qa),e(q,Mt),e(Mt,Xa),e(q,Za),e(q,qt),e(qt,eo),e(q,to),e(q,Lt),e(Lt,ro),e(q,ao),e(q,Nt),e(Nt,oo),e(q,no),e(b,so),e(b,pe),_(Ve,pe,null),e(pe,io),e(pe,Oe),e(Oe,co),e(Oe,St),e(St,lo),e(Oe,mo),e(b,po),e(b,ue),_(We,ue,null),e(ue,uo),e(ue,Re),e(Re,fo),e(Re,Bt),e(Bt,ho),e(Re,go),e(b,_o),e(b,fe),_(Ue,fe,null),e(fe,vo),e(fe,He),e(He,xo),e(He,Ct),e(Ct,yo),e(He,bo),Kt=!0},p(t,[u]){const Ge={};u&2&&(Ge.$$scope={dirty:u,ctx:t}),X.$set(Ge);const At={};u&2&&(At.$$scope={dirty:u,ctx:t}),Z.$set(At);const jt={};u&2&&(jt.$$scope={dirty:u,ctx:t}),re.$set(jt)},i(t){Kt||(v(p.$$.fragment,t),v(xe.$$.fragment,t),v(ye.$$.fragment,t),v(be.$$.fragment,t),v(X.$$.fragment,t),v(Z.$$.fragment,t),v($e.$$.fragment,t),v(Ee.$$.fragment,t),v(we.$$.fragment,t),v(Fe.$$.fragment,t),v(re.$$.fragment,t),v(Ie.$$.fragment,t),v(Te.$$.fragment,t),v(ke.$$.fragment,t),v(Pe.$$.fragment,t),v(De.$$.fragment,t),v(Me.$$.fragment,t),v(qe.$$.fragment,t),v(Ne.$$.fragment,t),v(Be.$$.fragment,t),v(Ae.$$.fragment,t),v(je.$$.fragment,t),v(Ve.$$.fragment,t),v(We.$$.fragment,t),v(Ue.$$.fragment,t),Kt=!0)},o(t){x(p.$$.fragment,t),x(xe.$$.fragment,t),x(ye.$$.fragment,t),x(be.$$.fragment,t),x(X.$$.fragment,t),x(Z.$$.fragment,t),x($e.$$.fragment,t),x(Ee.$$.fragment,t),x(we.$$.fragment,t),x(Fe.$$.fragment,t),x(re.$$.fragment,t),x(Ie.$$.fragment,t),x(Te.$$.fragment,t),x(ke.$$.fragment,t),x(Pe.$$.fragment,t),x(De.$$.fragment,t),x(Me.$$.fragment,t),x(qe.$$.fragment,t),x(Ne.$$.fragment,t),x(Be.$$.fragment,t),x(Ae.$$.fragment,t),x(je.$$.fragment,t),x(Ve.$$.fragment,t),x(We.$$.fragment,t),x(Ue.$$.fragment,t),Kt=!1},d(t){r(m),t&&r(k),t&&r(f),y(p),t&&r(z),t&&r(D),t&&r(Ot),t&&r(O),y(xe),t&&r(Wt),t&&r(M),y(ye),y(be),y(X),y(Z),y($e),t&&r(Rt),t&&r(R),y(Ee),t&&r(Ut),t&&r(C),y(we),y(Fe),y(re),t&&r(Ht),t&&r(H),y(Ie),t&&r(Gt),t&&r(P),y(Te),y(ke),y(Pe),t&&r(Jt),t&&r(J),y(De),t&&r(Yt),t&&r(b),y(Me),y(qe),y(Ne),y(Be),y(Ae),y(je),y(Ve),y(We),y(Ue)}}}const Fn={local:"feature-extractor",sections:[{local:"transformers.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function In(K){return _n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qn extends un{constructor(m){super();fn(this,m,In,wn,hn,{})}}export{qn as default,Fn as metadata};
