import{S as Ji,i as Ki,s as Qi,e as a,k as l,w as u,t as s,M as Xi,c as n,d as r,m as c,a as o,x as f,h as i,b as m,N as ka,F as t,g as d,y as g,L as Yi,q as _,o as w,B as v,v as Zi}from"../../chunks/vendor-6b77c823.js";import{D as $}from"../../chunks/Docstring-abef54e3.js";import{C as Na}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Y}from"../../chunks/IconCopyLink-7a11ce68.js";function el(vs){let W,Dr,S,Z,yt,_e,Fa,bt,Ca,Lr,ee,Oa,$t,Ra,ja,Pr,P,At,qa,Ua,we,Ga,zt,Va,Ma,Ha,Et,Ba,Wr,I,te,xt,ve,Ja,Tt,Ka,Sr,E,ye,Qa,be,Xa,$e,Ya,Za,en,re,Ae,tn,Dt,rn,Ir,k,ae,Lt,ze,an,Pt,nn,kr,h,Ee,on,dt,sn,xe,ln,cn,b,mn,Wt,pn,dn,Te,hn,un,St,fn,gn,It,_n,wn,kt,vn,yn,Nt,bn,$n,Ft,An,zn,En,Ct,xn,Tn,De,Dn,Le,Ln,Pn,Wn,x,Pe,Ot,Sn,In,We,Rt,kn,Nn,Se,Fn,Ie,Cn,On,Rn,jt,qt,jn,qn,Ut,Gt,Un,Gn,Vt,Mt,Vn,Mn,Ht,Hn,Bn,ke,Jn,Bt,Kn,Qn,Ne,Xn,T,Yn,Jt,Zn,eo,ht,to,ro,Kt,ao,no,oo,Fe,so,Qt,io,lo,Ce,co,ne,Oe,mo,Xt,po,Nr,N,oe,Yt,Re,ho,Zt,uo,Fr,z,je,fo,F,go,er,_o,wo,qe,vo,yo,bo,tr,$o,Ao,se,Ue,zo,rr,Eo,Cr,C,Ge,xo,ar,To,Or,O,ie,nr,Ve,Do,or,Lo,Rr,R,le,sr,Me,Po,ir,Wo,jr,j,He,So,lr,Io,qr,q,Be,ko,cr,No,Ur,U,Je,Fo,mr,Co,Gr,G,Ke,Oo,pr,Ro,Vr,Qe,ys,Mr,V,Xe,jo,dr,qo,Hr,Ye,bs,Br,M,Ze,Uo,hr,Go,Jr,et,$s,Kr,H,tt,Vo,ur,Mo,Qr,rt,As,Xr,D,at,Ho,nt,Bo,fr,Jo,Ko,Qo,ce,Xo,gr,Yo,Zo,ot,es,Yr,B,me,_r,st,ts,wr,rs,Zr,J,it,as,vr,ns,ea,K,pe,yr,lt,os,br,ss,ta,Q,de,$r,ct,is,Ar,ls,ra,L,mt,cs,X,ms,zr,ps,ds,Er,hs,us,fs,he,pt,gs,xr,_s,aa;return _e=new Y({}),ve=new Y({}),ye=new $({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": typing.Iterable[torch.nn.parameter.Parameter]"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": typing.Tuple[float, float] = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"},{name:"no_deprecation_warning",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L273",parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to (0.9, 0.999)) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"},{anchor:"transformers.AdamW.no_deprecation_warning",description:`<strong>no_deprecation_warning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
A flag used to disable the deprecation warning (set to <code>True</code> to disable the warning).`,name:"no_deprecation_warning"}]}}),Ae=new $({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": typing.Callable = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L323",parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}]}}),ze=new Y({}),Ee=new $({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L385",parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to (1e-30, 1e-3)) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}]}}),ke=new Na({props:{code:"Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)'}}),Ne=new Na({props:{code:"Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)'}}),Fe=new Na({props:{code:`from transformers.optimization import Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`}}),Ce=new Na({props:{code:`# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)`,highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`}}),Oe=new $({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L531",parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}]}}),Re=new Y({}),je=new $({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": typing.Union[float, keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"exclude_from_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L152",parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and
Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to &#x2018;AdamWeightDecay&#x2019;) &#x2014;
Optional name for the operations created when applying gradients.
kwargs &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by
norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time
inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use
<code>learning_rate</code> instead.`,name:"name"}]}}),Ue=new $({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L209"}}),Ge=new $({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L82",parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}]}}),Ve=new Y({}),Me=new Y({}),He=new $({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/trainer_utils.py#L296"}}),Be=new $({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L233",parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"}]}}),Je=new $({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L34",parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ke=new $({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L50",parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Xe=new $({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L104",parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ze=new $({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L138",parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),tt=new $({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L75",parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),at=new $({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization.py#L173",parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),st=new Y({}),it=new $({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": typing.Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L24",parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}]}}),lt=new Y({}),ct=new Y({}),mt=new $({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L282"}}),pt=new $({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16162/src/transformers/optimization_tf.py#L344"}}),{c(){W=a("meta"),Dr=l(),S=a("h1"),Z=a("a"),yt=a("span"),u(_e.$$.fragment),Fa=l(),bt=a("span"),Ca=s("Optimization"),Lr=l(),ee=a("p"),Oa=s("The "),$t=a("code"),Ra=s(".optimization"),ja=s(" module provides:"),Pr=l(),P=a("ul"),At=a("li"),qa=s("an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ua=l(),we=a("li"),Ga=s("several schedules in the form of schedule objects that inherit from "),zt=a("code"),Va=s("_LRSchedule"),Ma=s(":"),Ha=l(),Et=a("li"),Ba=s("a gradient accumulation class to accumulate the gradients of multiple batches"),Wr=l(),I=a("h2"),te=a("a"),xt=a("span"),u(ve.$$.fragment),Ja=l(),Tt=a("span"),Ka=s("AdamW (PyTorch)"),Sr=l(),E=a("div"),u(ye.$$.fragment),Qa=l(),be=a("p"),Xa=s("Implements Adam algorithm with weight decay fix as introduced in "),$e=a("a"),Ya=s(`Decoupled Weight Decay
Regularization`),Za=s("."),en=l(),re=a("div"),u(Ae.$$.fragment),tn=l(),Dt=a("p"),rn=s("Performs a single optimization step."),Ir=l(),k=a("h2"),ae=a("a"),Lt=a("span"),u(ze.$$.fragment),an=l(),Pt=a("span"),nn=s("AdaFactor (PyTorch)"),kr=l(),h=a("div"),u(Ee.$$.fragment),on=l(),dt=a("p"),sn=s(`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=a("a"),ln=s("https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),cn=l(),b=a("p"),mn=s("Paper: "),Wt=a("em"),pn=s("Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),dn=l(),Te=a("a"),hn=s("https://arxiv.org/abs/1804.04235"),un=s(` Note that
this optimizer internally adjusts the learning rate depending on the `),St=a("code"),fn=s("scale_parameter"),gn=s(", "),It=a("code"),_n=s("relative_step"),wn=s(` and
`),kt=a("code"),vn=s("warmup_init"),yn=s(" options. To use a manual (external) learning rate schedule you should set "),Nt=a("code"),bn=s("scale_parameter=False"),$n=s(` and
`),Ft=a("code"),An=s("relative_step=False"),zn=s("."),En=l(),Ct=a("p"),xn=s("This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Tn=l(),De=a("p"),Dn=s("Recommended T5 finetuning settings ("),Le=a("a"),Ln=s("https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Pn=s("):"),Wn=l(),x=a("ul"),Pe=a("li"),Ot=a("p"),Sn=s("Training without LR warmup or clip_threshold is not recommended."),In=l(),We=a("ul"),Rt=a("li"),kn=s("use scheduled LR warm-up to fixed LR"),Nn=l(),Se=a("li"),Fn=s("use clip_threshold=1.0 ("),Ie=a("a"),Cn=s("https://arxiv.org/abs/1804.04235"),On=s(")"),Rn=l(),jt=a("li"),qt=a("p"),jn=s("Disable relative updates"),qn=l(),Ut=a("li"),Gt=a("p"),Un=s("Use scale_parameter=False"),Gn=l(),Vt=a("li"),Mt=a("p"),Vn=s("Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),Mn=l(),Ht=a("p"),Hn=s("Example:"),Bn=l(),u(ke.$$.fragment),Jn=l(),Bt=a("p"),Kn=s("Others reported the following combination to work well:"),Qn=l(),u(Ne.$$.fragment),Xn=l(),T=a("p"),Yn=s("When using "),Jt=a("code"),Zn=s("lr=None"),eo=s(" with "),ht=a("a"),to=s("Trainer"),ro=s(" you will most likely need to use "),Kt=a("code"),ao=s("AdafactorSchedule"),no=s("scheduler as following:"),oo=l(),u(Fe.$$.fragment),so=l(),Qt=a("p"),io=s("Usage:"),lo=l(),u(Ce.$$.fragment),co=l(),ne=a("div"),u(Oe.$$.fragment),mo=l(),Xt=a("p"),po=s("Performs a single optimization step"),Nr=l(),N=a("h2"),oe=a("a"),Yt=a("span"),u(Re.$$.fragment),ho=l(),Zt=a("span"),uo=s("AdamWeightDecay (TensorFlow)"),Fr=l(),z=a("div"),u(je.$$.fragment),fo=l(),F=a("p"),go=s(`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),er=a("em"),_o=s("not"),wo=s(` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=a("a"),vo=s(`Decoupled Weight Decay
Regularization`),yo=s("."),bo=l(),tr=a("p"),$o=s(`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),Ao=l(),se=a("div"),u(Ue.$$.fragment),zo=l(),rr=a("p"),Eo=s("Creates an optimizer from its config with WarmUp custom object."),Cr=l(),C=a("div"),u(Ge.$$.fragment),xo=l(),ar=a("p"),To=s("Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),Or=l(),O=a("h2"),ie=a("a"),nr=a("span"),u(Ve.$$.fragment),Do=l(),or=a("span"),Lo=s("Schedules"),Rr=l(),R=a("h3"),le=a("a"),sr=a("span"),u(Me.$$.fragment),Po=l(),ir=a("span"),Wo=s("Learning Rate Schedules (Pytorch)"),jr=l(),j=a("div"),u(He.$$.fragment),So=l(),lr=a("p"),Io=s("An enumeration."),qr=l(),q=a("div"),u(Be.$$.fragment),ko=l(),cr=a("p"),No=s("Unified API to get any scheduler from its name."),Ur=l(),U=a("div"),u(Je.$$.fragment),Fo=l(),mr=a("p"),Co=s("Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Gr=l(),G=a("div"),u(Ke.$$.fragment),Oo=l(),pr=a("p"),Ro=s(`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Vr=l(),Qe=a("img"),Mr=l(),V=a("div"),u(Xe.$$.fragment),jo=l(),dr=a("p"),qo=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Hr=l(),Ye=a("img"),Br=l(),M=a("div"),u(Ze.$$.fragment),Uo=l(),hr=a("p"),Go=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Jr=l(),et=a("img"),Kr=l(),H=a("div"),u(tt.$$.fragment),Vo=l(),ur=a("p"),Mo=s(`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Qr=l(),rt=a("img"),Xr=l(),D=a("div"),u(at.$$.fragment),Ho=l(),nt=a("p"),Bo=s(`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),fr=a("em"),Jo=s("lr_end"),Ko=s(`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Qo=l(),ce=a("p"),Xo=s("Note: "),gr=a("em"),Yo=s("power"),Zo=s(` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=a("a"),es=s("https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),Yr=l(),B=a("h3"),me=a("a"),_r=a("span"),u(st.$$.fragment),ts=l(),wr=a("span"),rs=s("Warmup (TensorFlow)"),Zr=l(),J=a("div"),u(it.$$.fragment),as=l(),vr=a("p"),ns=s("Applies a warmup schedule on a given learning rate decay schedule."),ea=l(),K=a("h2"),pe=a("a"),yr=a("span"),u(lt.$$.fragment),os=l(),br=a("span"),ss=s("Gradient Strategies"),ta=l(),Q=a("h3"),de=a("a"),$r=a("span"),u(ct.$$.fragment),is=l(),Ar=a("span"),ls=s("GradientAccumulator (TensorFlow)"),ra=l(),L=a("div"),u(mt.$$.fragment),cs=l(),X=a("p"),ms=s(`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),zr=a("code"),ps=s(".gradients"),ds=s(", scale the gradients if required, and pass the result to "),Er=a("code"),hs=s("apply_gradients"),us=s("."),fs=l(),he=a("div"),u(pt.$$.fragment),gs=l(),xr=a("p"),_s=s("Resets the accumulated gradients on the current replica."),this.h()},l(e){const p=Xi('[data-svelte="svelte-1phssyn"]',document.head);W=n(p,"META",{name:!0,content:!0}),p.forEach(r),Dr=c(e),S=n(e,"H1",{class:!0});var na=o(S);Z=n(na,"A",{id:!0,class:!0,href:!0});var zs=o(Z);yt=n(zs,"SPAN",{});var Es=o(yt);f(_e.$$.fragment,Es),Es.forEach(r),zs.forEach(r),Fa=c(na),bt=n(na,"SPAN",{});var xs=o(bt);Ca=i(xs,"Optimization"),xs.forEach(r),na.forEach(r),Lr=c(e),ee=n(e,"P",{});var oa=o(ee);Oa=i(oa,"The "),$t=n(oa,"CODE",{});var Ts=o($t);Ra=i(Ts,".optimization"),Ts.forEach(r),ja=i(oa," module provides:"),oa.forEach(r),Pr=c(e),P=n(e,"UL",{});var ut=o(P);At=n(ut,"LI",{});var Ds=o(At);qa=i(Ds,"an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ds.forEach(r),Ua=c(ut),we=n(ut,"LI",{});var sa=o(we);Ga=i(sa,"several schedules in the form of schedule objects that inherit from "),zt=n(sa,"CODE",{});var Ls=o(zt);Va=i(Ls,"_LRSchedule"),Ls.forEach(r),Ma=i(sa,":"),sa.forEach(r),Ha=c(ut),Et=n(ut,"LI",{});var Ps=o(Et);Ba=i(Ps,"a gradient accumulation class to accumulate the gradients of multiple batches"),Ps.forEach(r),ut.forEach(r),Wr=c(e),I=n(e,"H2",{class:!0});var ia=o(I);te=n(ia,"A",{id:!0,class:!0,href:!0});var Ws=o(te);xt=n(Ws,"SPAN",{});var Ss=o(xt);f(ve.$$.fragment,Ss),Ss.forEach(r),Ws.forEach(r),Ja=c(ia),Tt=n(ia,"SPAN",{});var Is=o(Tt);Ka=i(Is,"AdamW (PyTorch)"),Is.forEach(r),ia.forEach(r),Sr=c(e),E=n(e,"DIV",{class:!0});var ft=o(E);f(ye.$$.fragment,ft),Qa=c(ft),be=n(ft,"P",{});var la=o(be);Xa=i(la,"Implements Adam algorithm with weight decay fix as introduced in "),$e=n(la,"A",{href:!0,rel:!0});var ks=o($e);Ya=i(ks,`Decoupled Weight Decay
Regularization`),ks.forEach(r),Za=i(la,"."),la.forEach(r),en=c(ft),re=n(ft,"DIV",{class:!0});var ca=o(re);f(Ae.$$.fragment,ca),tn=c(ca),Dt=n(ca,"P",{});var Ns=o(Dt);rn=i(Ns,"Performs a single optimization step."),Ns.forEach(r),ca.forEach(r),ft.forEach(r),Ir=c(e),k=n(e,"H2",{class:!0});var ma=o(k);ae=n(ma,"A",{id:!0,class:!0,href:!0});var Fs=o(ae);Lt=n(Fs,"SPAN",{});var Cs=o(Lt);f(ze.$$.fragment,Cs),Cs.forEach(r),Fs.forEach(r),an=c(ma),Pt=n(ma,"SPAN",{});var Os=o(Pt);nn=i(Os,"AdaFactor (PyTorch)"),Os.forEach(r),ma.forEach(r),kr=c(e),h=n(e,"DIV",{class:!0});var y=o(h);f(Ee.$$.fragment,y),on=c(y),dt=n(y,"P",{});var ws=o(dt);sn=i(ws,`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=n(ws,"A",{href:!0,rel:!0});var Rs=o(xe);ln=i(Rs,"https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),Rs.forEach(r),ws.forEach(r),cn=c(y),b=n(y,"P",{});var A=o(b);mn=i(A,"Paper: "),Wt=n(A,"EM",{});var js=o(Wt);pn=i(js,"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),js.forEach(r),dn=c(A),Te=n(A,"A",{href:!0,rel:!0});var qs=o(Te);hn=i(qs,"https://arxiv.org/abs/1804.04235"),qs.forEach(r),un=i(A,` Note that
this optimizer internally adjusts the learning rate depending on the `),St=n(A,"CODE",{});var Us=o(St);fn=i(Us,"scale_parameter"),Us.forEach(r),gn=i(A,", "),It=n(A,"CODE",{});var Gs=o(It);_n=i(Gs,"relative_step"),Gs.forEach(r),wn=i(A,` and
`),kt=n(A,"CODE",{});var Vs=o(kt);vn=i(Vs,"warmup_init"),Vs.forEach(r),yn=i(A," options. To use a manual (external) learning rate schedule you should set "),Nt=n(A,"CODE",{});var Ms=o(Nt);bn=i(Ms,"scale_parameter=False"),Ms.forEach(r),$n=i(A,` and
`),Ft=n(A,"CODE",{});var Hs=o(Ft);An=i(Hs,"relative_step=False"),Hs.forEach(r),zn=i(A,"."),A.forEach(r),En=c(y),Ct=n(y,"P",{});var Bs=o(Ct);xn=i(Bs,"This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Bs.forEach(r),Tn=c(y),De=n(y,"P",{});var pa=o(De);Dn=i(pa,"Recommended T5 finetuning settings ("),Le=n(pa,"A",{href:!0,rel:!0});var Js=o(Le);Ln=i(Js,"https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Js.forEach(r),Pn=i(pa,"):"),pa.forEach(r),Wn=c(y),x=n(y,"UL",{});var ue=o(x);Pe=n(ue,"LI",{});var da=o(Pe);Ot=n(da,"P",{});var Ks=o(Ot);Sn=i(Ks,"Training without LR warmup or clip_threshold is not recommended."),Ks.forEach(r),In=c(da),We=n(da,"UL",{});var ha=o(We);Rt=n(ha,"LI",{});var Qs=o(Rt);kn=i(Qs,"use scheduled LR warm-up to fixed LR"),Qs.forEach(r),Nn=c(ha),Se=n(ha,"LI",{});var ua=o(Se);Fn=i(ua,"use clip_threshold=1.0 ("),Ie=n(ua,"A",{href:!0,rel:!0});var Xs=o(Ie);Cn=i(Xs,"https://arxiv.org/abs/1804.04235"),Xs.forEach(r),On=i(ua,")"),ua.forEach(r),ha.forEach(r),da.forEach(r),Rn=c(ue),jt=n(ue,"LI",{});var Ys=o(jt);qt=n(Ys,"P",{});var Zs=o(qt);jn=i(Zs,"Disable relative updates"),Zs.forEach(r),Ys.forEach(r),qn=c(ue),Ut=n(ue,"LI",{});var ei=o(Ut);Gt=n(ei,"P",{});var ti=o(Gt);Un=i(ti,"Use scale_parameter=False"),ti.forEach(r),ei.forEach(r),Gn=c(ue),Vt=n(ue,"LI",{});var ri=o(Vt);Mt=n(ri,"P",{});var ai=o(Mt);Vn=i(ai,"Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),ai.forEach(r),ri.forEach(r),ue.forEach(r),Mn=c(y),Ht=n(y,"P",{});var ni=o(Ht);Hn=i(ni,"Example:"),ni.forEach(r),Bn=c(y),f(ke.$$.fragment,y),Jn=c(y),Bt=n(y,"P",{});var oi=o(Bt);Kn=i(oi,"Others reported the following combination to work well:"),oi.forEach(r),Qn=c(y),f(Ne.$$.fragment,y),Xn=c(y),T=n(y,"P",{});var fe=o(T);Yn=i(fe,"When using "),Jt=n(fe,"CODE",{});var si=o(Jt);Zn=i(si,"lr=None"),si.forEach(r),eo=i(fe," with "),ht=n(fe,"A",{href:!0});var ii=o(ht);to=i(ii,"Trainer"),ii.forEach(r),ro=i(fe," you will most likely need to use "),Kt=n(fe,"CODE",{});var li=o(Kt);ao=i(li,"AdafactorSchedule"),li.forEach(r),no=i(fe,"scheduler as following:"),fe.forEach(r),oo=c(y),f(Fe.$$.fragment,y),so=c(y),Qt=n(y,"P",{});var ci=o(Qt);io=i(ci,"Usage:"),ci.forEach(r),lo=c(y),f(Ce.$$.fragment,y),co=c(y),ne=n(y,"DIV",{class:!0});var fa=o(ne);f(Oe.$$.fragment,fa),mo=c(fa),Xt=n(fa,"P",{});var mi=o(Xt);po=i(mi,"Performs a single optimization step"),mi.forEach(r),fa.forEach(r),y.forEach(r),Nr=c(e),N=n(e,"H2",{class:!0});var ga=o(N);oe=n(ga,"A",{id:!0,class:!0,href:!0});var pi=o(oe);Yt=n(pi,"SPAN",{});var di=o(Yt);f(Re.$$.fragment,di),di.forEach(r),pi.forEach(r),ho=c(ga),Zt=n(ga,"SPAN",{});var hi=o(Zt);uo=i(hi,"AdamWeightDecay (TensorFlow)"),hi.forEach(r),ga.forEach(r),Fr=c(e),z=n(e,"DIV",{class:!0});var ge=o(z);f(je.$$.fragment,ge),fo=c(ge),F=n(ge,"P",{});var gt=o(F);go=i(gt,`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),er=n(gt,"EM",{});var ui=o(er);_o=i(ui,"not"),ui.forEach(r),wo=i(gt,` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=n(gt,"A",{href:!0,rel:!0});var fi=o(qe);vo=i(fi,`Decoupled Weight Decay
Regularization`),fi.forEach(r),yo=i(gt,"."),gt.forEach(r),bo=c(ge),tr=n(ge,"P",{});var gi=o(tr);$o=i(gi,`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),gi.forEach(r),Ao=c(ge),se=n(ge,"DIV",{class:!0});var _a=o(se);f(Ue.$$.fragment,_a),zo=c(_a),rr=n(_a,"P",{});var _i=o(rr);Eo=i(_i,"Creates an optimizer from its config with WarmUp custom object."),_i.forEach(r),_a.forEach(r),ge.forEach(r),Cr=c(e),C=n(e,"DIV",{class:!0});var wa=o(C);f(Ge.$$.fragment,wa),xo=c(wa),ar=n(wa,"P",{});var wi=o(ar);To=i(wi,"Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),wi.forEach(r),wa.forEach(r),Or=c(e),O=n(e,"H2",{class:!0});var va=o(O);ie=n(va,"A",{id:!0,class:!0,href:!0});var vi=o(ie);nr=n(vi,"SPAN",{});var yi=o(nr);f(Ve.$$.fragment,yi),yi.forEach(r),vi.forEach(r),Do=c(va),or=n(va,"SPAN",{});var bi=o(or);Lo=i(bi,"Schedules"),bi.forEach(r),va.forEach(r),Rr=c(e),R=n(e,"H3",{class:!0});var ya=o(R);le=n(ya,"A",{id:!0,class:!0,href:!0});var $i=o(le);sr=n($i,"SPAN",{});var Ai=o(sr);f(Me.$$.fragment,Ai),Ai.forEach(r),$i.forEach(r),Po=c(ya),ir=n(ya,"SPAN",{});var zi=o(ir);Wo=i(zi,"Learning Rate Schedules (Pytorch)"),zi.forEach(r),ya.forEach(r),jr=c(e),j=n(e,"DIV",{class:!0});var ba=o(j);f(He.$$.fragment,ba),So=c(ba),lr=n(ba,"P",{});var Ei=o(lr);Io=i(Ei,"An enumeration."),Ei.forEach(r),ba.forEach(r),qr=c(e),q=n(e,"DIV",{class:!0});var $a=o(q);f(Be.$$.fragment,$a),ko=c($a),cr=n($a,"P",{});var xi=o(cr);No=i(xi,"Unified API to get any scheduler from its name."),xi.forEach(r),$a.forEach(r),Ur=c(e),U=n(e,"DIV",{class:!0});var Aa=o(U);f(Je.$$.fragment,Aa),Fo=c(Aa),mr=n(Aa,"P",{});var Ti=o(mr);Co=i(Ti,"Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ti.forEach(r),Aa.forEach(r),Gr=c(e),G=n(e,"DIV",{class:!0});var za=o(G);f(Ke.$$.fragment,za),Oo=c(za),pr=n(za,"P",{});var Di=o(pr);Ro=i(Di,`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Di.forEach(r),za.forEach(r),Vr=c(e),Qe=n(e,"IMG",{alt:!0,src:!0}),Mr=c(e),V=n(e,"DIV",{class:!0});var Ea=o(V);f(Xe.$$.fragment,Ea),jo=c(Ea),dr=n(Ea,"P",{});var Li=o(dr);qo=i(Li,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Li.forEach(r),Ea.forEach(r),Hr=c(e),Ye=n(e,"IMG",{alt:!0,src:!0}),Br=c(e),M=n(e,"DIV",{class:!0});var xa=o(M);f(Ze.$$.fragment,xa),Uo=c(xa),hr=n(xa,"P",{});var Pi=o(hr);Go=i(Pi,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Pi.forEach(r),xa.forEach(r),Jr=c(e),et=n(e,"IMG",{alt:!0,src:!0}),Kr=c(e),H=n(e,"DIV",{class:!0});var Ta=o(H);f(tt.$$.fragment,Ta),Vo=c(Ta),ur=n(Ta,"P",{});var Wi=o(ur);Mo=i(Wi,`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Wi.forEach(r),Ta.forEach(r),Qr=c(e),rt=n(e,"IMG",{alt:!0,src:!0}),Xr=c(e),D=n(e,"DIV",{class:!0});var _t=o(D);f(at.$$.fragment,_t),Ho=c(_t),nt=n(_t,"P",{});var Da=o(nt);Bo=i(Da,`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),fr=n(Da,"EM",{});var Si=o(fr);Jo=i(Si,"lr_end"),Si.forEach(r),Ko=i(Da,`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Da.forEach(r),Qo=c(_t),ce=n(_t,"P",{});var Tr=o(ce);Xo=i(Tr,"Note: "),gr=n(Tr,"EM",{});var Ii=o(gr);Yo=i(Ii,"power"),Ii.forEach(r),Zo=i(Tr,` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=n(Tr,"A",{href:!0,rel:!0});var ki=o(ot);es=i(ki,"https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),ki.forEach(r),Tr.forEach(r),_t.forEach(r),Yr=c(e),B=n(e,"H3",{class:!0});var La=o(B);me=n(La,"A",{id:!0,class:!0,href:!0});var Ni=o(me);_r=n(Ni,"SPAN",{});var Fi=o(_r);f(st.$$.fragment,Fi),Fi.forEach(r),Ni.forEach(r),ts=c(La),wr=n(La,"SPAN",{});var Ci=o(wr);rs=i(Ci,"Warmup (TensorFlow)"),Ci.forEach(r),La.forEach(r),Zr=c(e),J=n(e,"DIV",{class:!0});var Pa=o(J);f(it.$$.fragment,Pa),as=c(Pa),vr=n(Pa,"P",{});var Oi=o(vr);ns=i(Oi,"Applies a warmup schedule on a given learning rate decay schedule."),Oi.forEach(r),Pa.forEach(r),ea=c(e),K=n(e,"H2",{class:!0});var Wa=o(K);pe=n(Wa,"A",{id:!0,class:!0,href:!0});var Ri=o(pe);yr=n(Ri,"SPAN",{});var ji=o(yr);f(lt.$$.fragment,ji),ji.forEach(r),Ri.forEach(r),os=c(Wa),br=n(Wa,"SPAN",{});var qi=o(br);ss=i(qi,"Gradient Strategies"),qi.forEach(r),Wa.forEach(r),ta=c(e),Q=n(e,"H3",{class:!0});var Sa=o(Q);de=n(Sa,"A",{id:!0,class:!0,href:!0});var Ui=o(de);$r=n(Ui,"SPAN",{});var Gi=o($r);f(ct.$$.fragment,Gi),Gi.forEach(r),Ui.forEach(r),is=c(Sa),Ar=n(Sa,"SPAN",{});var Vi=o(Ar);ls=i(Vi,"GradientAccumulator (TensorFlow)"),Vi.forEach(r),Sa.forEach(r),ra=c(e),L=n(e,"DIV",{class:!0});var wt=o(L);f(mt.$$.fragment,wt),cs=c(wt),X=n(wt,"P",{});var vt=o(X);ms=i(vt,`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),zr=n(vt,"CODE",{});var Mi=o(zr);ps=i(Mi,".gradients"),Mi.forEach(r),ds=i(vt,", scale the gradients if required, and pass the result to "),Er=n(vt,"CODE",{});var Hi=o(Er);hs=i(Hi,"apply_gradients"),Hi.forEach(r),us=i(vt,"."),vt.forEach(r),fs=c(wt),he=n(wt,"DIV",{class:!0});var Ia=o(he);f(pt.$$.fragment,Ia),gs=c(Ia),xr=n(Ia,"P",{});var Bi=o(xr);_s=i(Bi,"Resets the accumulated gradients on the current replica."),Bi.forEach(r),Ia.forEach(r),wt.forEach(r),this.h()},h(){m(W,"name","hf:doc:metadata"),m(W,"content",JSON.stringify(tl)),m(Z,"id","optimization"),m(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Z,"href","#optimization"),m(S,"class","relative group"),m(te,"id","transformers.AdamW"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#transformers.AdamW"),m(I,"class","relative group"),m($e,"href","https://arxiv.org/abs/1711.05101"),m($e,"rel","nofollow"),m(re,"class","docstring"),m(E,"class","docstring"),m(ae,"id","transformers.Adafactor"),m(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ae,"href","#transformers.Adafactor"),m(k,"class","relative group"),m(xe,"href","https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),m(xe,"rel","nofollow"),m(Te,"href","https://arxiv.org/abs/1804.04235"),m(Te,"rel","nofollow"),m(Le,"href","https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),m(Le,"rel","nofollow"),m(Ie,"href","https://arxiv.org/abs/1804.04235"),m(Ie,"rel","nofollow"),m(ht,"href","/docs/transformers/pr_16162/en/main_classes/trainer#transformers.Trainer"),m(ne,"class","docstring"),m(h,"class","docstring"),m(oe,"id","transformers.AdamWeightDecay"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#transformers.AdamWeightDecay"),m(N,"class","relative group"),m(qe,"href","https://arxiv.org/abs/1711.05101"),m(qe,"rel","nofollow"),m(se,"class","docstring"),m(z,"class","docstring"),m(C,"class","docstring"),m(ie,"id","schedules"),m(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ie,"href","#schedules"),m(O,"class","relative group"),m(le,"id","transformers.SchedulerType"),m(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(le,"href","#transformers.SchedulerType"),m(R,"class","relative group"),m(j,"class","docstring"),m(q,"class","docstring"),m(U,"class","docstring"),m(G,"class","docstring"),m(Qe,"alt",""),ka(Qe.src,ys="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||m(Qe,"src",ys),m(V,"class","docstring"),m(Ye,"alt",""),ka(Ye.src,bs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||m(Ye,"src",bs),m(M,"class","docstring"),m(et,"alt",""),ka(et.src,$s="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||m(et,"src",$s),m(H,"class","docstring"),m(rt,"alt",""),ka(rt.src,As="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||m(rt,"src",As),m(ot,"href","https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),m(ot,"rel","nofollow"),m(D,"class","docstring"),m(me,"id","transformers.WarmUp"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#transformers.WarmUp"),m(B,"class","relative group"),m(J,"class","docstring"),m(pe,"id","gradient-strategies"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#gradient-strategies"),m(K,"class","relative group"),m(de,"id","transformers.GradientAccumulator"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#transformers.GradientAccumulator"),m(Q,"class","relative group"),m(he,"class","docstring"),m(L,"class","docstring")},m(e,p){t(document.head,W),d(e,Dr,p),d(e,S,p),t(S,Z),t(Z,yt),g(_e,yt,null),t(S,Fa),t(S,bt),t(bt,Ca),d(e,Lr,p),d(e,ee,p),t(ee,Oa),t(ee,$t),t($t,Ra),t(ee,ja),d(e,Pr,p),d(e,P,p),t(P,At),t(At,qa),t(P,Ua),t(P,we),t(we,Ga),t(we,zt),t(zt,Va),t(we,Ma),t(P,Ha),t(P,Et),t(Et,Ba),d(e,Wr,p),d(e,I,p),t(I,te),t(te,xt),g(ve,xt,null),t(I,Ja),t(I,Tt),t(Tt,Ka),d(e,Sr,p),d(e,E,p),g(ye,E,null),t(E,Qa),t(E,be),t(be,Xa),t(be,$e),t($e,Ya),t(be,Za),t(E,en),t(E,re),g(Ae,re,null),t(re,tn),t(re,Dt),t(Dt,rn),d(e,Ir,p),d(e,k,p),t(k,ae),t(ae,Lt),g(ze,Lt,null),t(k,an),t(k,Pt),t(Pt,nn),d(e,kr,p),d(e,h,p),g(Ee,h,null),t(h,on),t(h,dt),t(dt,sn),t(dt,xe),t(xe,ln),t(h,cn),t(h,b),t(b,mn),t(b,Wt),t(Wt,pn),t(b,dn),t(b,Te),t(Te,hn),t(b,un),t(b,St),t(St,fn),t(b,gn),t(b,It),t(It,_n),t(b,wn),t(b,kt),t(kt,vn),t(b,yn),t(b,Nt),t(Nt,bn),t(b,$n),t(b,Ft),t(Ft,An),t(b,zn),t(h,En),t(h,Ct),t(Ct,xn),t(h,Tn),t(h,De),t(De,Dn),t(De,Le),t(Le,Ln),t(De,Pn),t(h,Wn),t(h,x),t(x,Pe),t(Pe,Ot),t(Ot,Sn),t(Pe,In),t(Pe,We),t(We,Rt),t(Rt,kn),t(We,Nn),t(We,Se),t(Se,Fn),t(Se,Ie),t(Ie,Cn),t(Se,On),t(x,Rn),t(x,jt),t(jt,qt),t(qt,jn),t(x,qn),t(x,Ut),t(Ut,Gt),t(Gt,Un),t(x,Gn),t(x,Vt),t(Vt,Mt),t(Mt,Vn),t(h,Mn),t(h,Ht),t(Ht,Hn),t(h,Bn),g(ke,h,null),t(h,Jn),t(h,Bt),t(Bt,Kn),t(h,Qn),g(Ne,h,null),t(h,Xn),t(h,T),t(T,Yn),t(T,Jt),t(Jt,Zn),t(T,eo),t(T,ht),t(ht,to),t(T,ro),t(T,Kt),t(Kt,ao),t(T,no),t(h,oo),g(Fe,h,null),t(h,so),t(h,Qt),t(Qt,io),t(h,lo),g(Ce,h,null),t(h,co),t(h,ne),g(Oe,ne,null),t(ne,mo),t(ne,Xt),t(Xt,po),d(e,Nr,p),d(e,N,p),t(N,oe),t(oe,Yt),g(Re,Yt,null),t(N,ho),t(N,Zt),t(Zt,uo),d(e,Fr,p),d(e,z,p),g(je,z,null),t(z,fo),t(z,F),t(F,go),t(F,er),t(er,_o),t(F,wo),t(F,qe),t(qe,vo),t(F,yo),t(z,bo),t(z,tr),t(tr,$o),t(z,Ao),t(z,se),g(Ue,se,null),t(se,zo),t(se,rr),t(rr,Eo),d(e,Cr,p),d(e,C,p),g(Ge,C,null),t(C,xo),t(C,ar),t(ar,To),d(e,Or,p),d(e,O,p),t(O,ie),t(ie,nr),g(Ve,nr,null),t(O,Do),t(O,or),t(or,Lo),d(e,Rr,p),d(e,R,p),t(R,le),t(le,sr),g(Me,sr,null),t(R,Po),t(R,ir),t(ir,Wo),d(e,jr,p),d(e,j,p),g(He,j,null),t(j,So),t(j,lr),t(lr,Io),d(e,qr,p),d(e,q,p),g(Be,q,null),t(q,ko),t(q,cr),t(cr,No),d(e,Ur,p),d(e,U,p),g(Je,U,null),t(U,Fo),t(U,mr),t(mr,Co),d(e,Gr,p),d(e,G,p),g(Ke,G,null),t(G,Oo),t(G,pr),t(pr,Ro),d(e,Vr,p),d(e,Qe,p),d(e,Mr,p),d(e,V,p),g(Xe,V,null),t(V,jo),t(V,dr),t(dr,qo),d(e,Hr,p),d(e,Ye,p),d(e,Br,p),d(e,M,p),g(Ze,M,null),t(M,Uo),t(M,hr),t(hr,Go),d(e,Jr,p),d(e,et,p),d(e,Kr,p),d(e,H,p),g(tt,H,null),t(H,Vo),t(H,ur),t(ur,Mo),d(e,Qr,p),d(e,rt,p),d(e,Xr,p),d(e,D,p),g(at,D,null),t(D,Ho),t(D,nt),t(nt,Bo),t(nt,fr),t(fr,Jo),t(nt,Ko),t(D,Qo),t(D,ce),t(ce,Xo),t(ce,gr),t(gr,Yo),t(ce,Zo),t(ce,ot),t(ot,es),d(e,Yr,p),d(e,B,p),t(B,me),t(me,_r),g(st,_r,null),t(B,ts),t(B,wr),t(wr,rs),d(e,Zr,p),d(e,J,p),g(it,J,null),t(J,as),t(J,vr),t(vr,ns),d(e,ea,p),d(e,K,p),t(K,pe),t(pe,yr),g(lt,yr,null),t(K,os),t(K,br),t(br,ss),d(e,ta,p),d(e,Q,p),t(Q,de),t(de,$r),g(ct,$r,null),t(Q,is),t(Q,Ar),t(Ar,ls),d(e,ra,p),d(e,L,p),g(mt,L,null),t(L,cs),t(L,X),t(X,ms),t(X,zr),t(zr,ps),t(X,ds),t(X,Er),t(Er,hs),t(X,us),t(L,fs),t(L,he),g(pt,he,null),t(he,gs),t(he,xr),t(xr,_s),aa=!0},p:Yi,i(e){aa||(_(_e.$$.fragment,e),_(ve.$$.fragment,e),_(ye.$$.fragment,e),_(Ae.$$.fragment,e),_(ze.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ce.$$.fragment,e),_(Oe.$$.fragment,e),_(Re.$$.fragment,e),_(je.$$.fragment,e),_(Ue.$$.fragment,e),_(Ge.$$.fragment,e),_(Ve.$$.fragment,e),_(Me.$$.fragment,e),_(He.$$.fragment,e),_(Be.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(tt.$$.fragment,e),_(at.$$.fragment,e),_(st.$$.fragment,e),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(ct.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),aa=!0)},o(e){w(_e.$$.fragment,e),w(ve.$$.fragment,e),w(ye.$$.fragment,e),w(Ae.$$.fragment,e),w(ze.$$.fragment,e),w(Ee.$$.fragment,e),w(ke.$$.fragment,e),w(Ne.$$.fragment,e),w(Fe.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Re.$$.fragment,e),w(je.$$.fragment,e),w(Ue.$$.fragment,e),w(Ge.$$.fragment,e),w(Ve.$$.fragment,e),w(Me.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Ze.$$.fragment,e),w(tt.$$.fragment,e),w(at.$$.fragment,e),w(st.$$.fragment,e),w(it.$$.fragment,e),w(lt.$$.fragment,e),w(ct.$$.fragment,e),w(mt.$$.fragment,e),w(pt.$$.fragment,e),aa=!1},d(e){r(W),e&&r(Dr),e&&r(S),v(_e),e&&r(Lr),e&&r(ee),e&&r(Pr),e&&r(P),e&&r(Wr),e&&r(I),v(ve),e&&r(Sr),e&&r(E),v(ye),v(Ae),e&&r(Ir),e&&r(k),v(ze),e&&r(kr),e&&r(h),v(Ee),v(ke),v(Ne),v(Fe),v(Ce),v(Oe),e&&r(Nr),e&&r(N),v(Re),e&&r(Fr),e&&r(z),v(je),v(Ue),e&&r(Cr),e&&r(C),v(Ge),e&&r(Or),e&&r(O),v(Ve),e&&r(Rr),e&&r(R),v(Me),e&&r(jr),e&&r(j),v(He),e&&r(qr),e&&r(q),v(Be),e&&r(Ur),e&&r(U),v(Je),e&&r(Gr),e&&r(G),v(Ke),e&&r(Vr),e&&r(Qe),e&&r(Mr),e&&r(V),v(Xe),e&&r(Hr),e&&r(Ye),e&&r(Br),e&&r(M),v(Ze),e&&r(Jr),e&&r(et),e&&r(Kr),e&&r(H),v(tt),e&&r(Qr),e&&r(rt),e&&r(Xr),e&&r(D),v(at),e&&r(Yr),e&&r(B),v(st),e&&r(Zr),e&&r(J),v(it),e&&r(ea),e&&r(K),v(lt),e&&r(ta),e&&r(Q),v(ct),e&&r(ra),e&&r(L),v(mt),v(pt)}}}const tl={local:"optimization",sections:[{local:"transformers.AdamW",title:"AdamW (PyTorch)"},{local:"transformers.Adafactor",title:"AdaFactor (PyTorch)"},{local:"transformers.AdamWeightDecay",title:"AdamWeightDecay (TensorFlow)"},{local:"schedules",sections:[{local:"transformers.SchedulerType",title:"Learning Rate Schedules (Pytorch)"},{local:"transformers.WarmUp",title:"Warmup (TensorFlow)"}],title:"Schedules"},{local:"gradient-strategies",sections:[{local:"transformers.GradientAccumulator",title:"GradientAccumulator (TensorFlow)"}],title:"Gradient Strategies"}],title:"Optimization"};function rl(vs){return Zi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class il extends Ji{constructor(W){super();Ki(this,W,rl,el,Qi,{})}}export{il as default,tl as metadata};
