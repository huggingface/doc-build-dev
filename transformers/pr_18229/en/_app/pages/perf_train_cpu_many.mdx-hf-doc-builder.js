import{S as tt,i as ot,s as at,e as l,k as f,w as h,t as s,M as lt,c as i,d as n,m as c,a as r,x as m,h as p,b as _,G as t,g as a,y as u,L as it,q as d,o as x,B as b,v as rt}from"../chunks/vendor-hf-doc-builder.js";import{I as ze}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../chunks/CodeBlock-hf-doc-builder.js";function st(Dn){let v,he,w,y,te,O,Xe,oe,Je,me,z,Qe,ue,$,C,ae,L,Ve,le,Ye,de,X,Ze,xe,J,en,be,E,nn,S,tn,on,ve,g,T,ie,U,an,re,ln,we,Q,rn,$e,V,sn,ge,M,Pe,Y,pn,ye,q,Ce,Z,fn,Ee,j,Te,k,cn,H,_n,hn,ke,P,D,se,I,mn,pe,un,De,W,dn,B,xn,Re,R,bn,fe,vn,wn,Ae,F,Ne,A,$n,ce,gn,Pn,Oe,ee,yn,Le,G,Se,N,Cn,_e,En,Tn,Ue,K,Me;return O=new ze({}),L=new ze({}),U=new ze({}),M=new ne({props:{code:"pip install oneccl_bind_pt==1.10.0 -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install oneccl_bind_pt==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span> -f https://software.intel.com/ipex-whl-stable'}}),q=new ne({props:{code:"pip install oneccl_bind_pt==1.11.0 -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install oneccl_bind_pt==<span class="hljs-number">1</span>.<span class="hljs-number">11</span>.<span class="hljs-number">0</span> -f https://software.intel.com/ipex-whl-stable'}}),j=new ne({props:{code:"pip install oneccl_bind_pt==1.12.0 -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install oneccl_bind_pt==<span class="hljs-number">1</span>.<span class="hljs-number">12</span>.<span class="hljs-number">0</span> -f https://software.intel.com/ipex-whl-stable'}}),I=new ze({}),F=new ne({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),G=new ne({props:{code:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`,highlighted:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`}}),K=new ne({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),{c(){v=l("meta"),he=f(),w=l("h1"),y=l("a"),te=l("span"),h(O.$$.fragment),Xe=f(),oe=l("span"),Je=s("Efficient Training on Multiple CPUs"),me=f(),z=l("p"),Qe=s("When training on a single CPU is too slow, we will use multiple CPUs, This guide focuses on PyTorch-based DDP enabling and how to do it efficiently."),ue=f(),$=l("h2"),C=l("a"),ae=l("span"),h(L.$$.fragment),Ve=f(),le=l("span"),Ye=s("Intel\xAE oneCCL Bindings for PyTorch"),de=f(),X=l("p"),Ze=s("Intel\xAE oneCCL (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the oneCCL documentation and oneCCL specification."),xe=f(),J=l("p"),en=s("oneccl_bindings_for_pytorch module implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),be=f(),E=l("p"),nn=s("Check more detailed information for "),S=l("a"),tn=s("oneccl_bind_pt"),on=s("."),ve=f(),g=l("h3"),T=l("a"),ie=l("span"),h(U.$$.fragment),an=f(),re=l("span"),ln=s("Intel\xAE oneCCL Bindings for PyTorch installation:"),we=f(),Q=l("p"),rn=s("Wheel files are avaiable for the following Python versions:"),$e=f(),V=l("p"),sn=s("For PyTorch-1.10:"),ge=f(),h(M.$$.fragment),Pe=f(),Y=l("p"),pn=s("For PyTorch-1.11:"),ye=f(),h(q.$$.fragment),Ce=f(),Z=l("p"),fn=s("For PyTorch-1.12:"),Ee=f(),h(j.$$.fragment),Te=f(),k=l("p"),cn=s("Check more approaches for "),H=l("a"),_n=s("oneccl_bind_pt installation"),hn=s("."),ke=f(),P=l("h3"),D=l("a"),se=l("span"),h(I.$$.fragment),mn=f(),pe=l("span"),un=s("Usage in Trainer"),De=s(`

To enable DDP in Trainer with ccl backend, users should add **\`--xpu_backend ccl\`** in training command arguments.
`),W=l("p"),dn=s("Take an example of the use cases on "),B=l("a"),xn=s("Transformers question-answering"),Re=f(),R=l("p"),bn=s("following command enables "),fe=l("strong"),vn=s("2DDP"),wn=s(" in one Xeon node, with one process running per one socket, OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Ae=f(),h(F.$$.fragment),Ne=f(),A=l("p"),$n=s("following command enables "),ce=l("strong"),gn=s("4DDP"),Pn=s(" in two Xeons (node0 and node1, taking node0 as the master), ppn(processes per node) is set to 2, with one process running per one socket, OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Oe=f(),ee=l("p"),yn=s("in node0, you need to create a config file which contains ip of each node(for ex: hostfile) and pass to mpirun as a argument"),Le=f(),h(G.$$.fragment),Se=f(),N=l("p"),Cn=s("run the following command in node0 and "),_e=l("strong"),En=s("4DDP"),Tn=s(" will be enabled in node0 and node1"),Ue=f(),h(K.$$.fragment),this.h()},l(e){const o=lt('[data-svelte="svelte-1phssyn"]',document.head);v=i(o,"META",{name:!0,content:!0}),o.forEach(n),he=c(e),w=i(e,"H1",{class:!0});var qe=r(w);y=i(qe,"A",{id:!0,class:!0,href:!0});var Rn=r(y);te=i(Rn,"SPAN",{});var An=r(te);m(O.$$.fragment,An),An.forEach(n),Rn.forEach(n),Xe=c(qe),oe=i(qe,"SPAN",{});var Nn=r(oe);Je=p(Nn,"Efficient Training on Multiple CPUs"),Nn.forEach(n),qe.forEach(n),me=c(e),z=i(e,"P",{});var On=r(z);Qe=p(On,"When training on a single CPU is too slow, we will use multiple CPUs, This guide focuses on PyTorch-based DDP enabling and how to do it efficiently."),On.forEach(n),ue=c(e),$=i(e,"H2",{class:!0});var je=r($);C=i(je,"A",{id:!0,class:!0,href:!0});var Ln=r(C);ae=i(Ln,"SPAN",{});var Sn=r(ae);m(L.$$.fragment,Sn),Sn.forEach(n),Ln.forEach(n),Ve=c(je),le=i(je,"SPAN",{});var Un=r(le);Ye=p(Un,"Intel\xAE oneCCL Bindings for PyTorch"),Un.forEach(n),je.forEach(n),de=c(e),X=i(e,"P",{});var Mn=r(X);Ze=p(Mn,"Intel\xAE oneCCL (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the oneCCL documentation and oneCCL specification."),Mn.forEach(n),xe=c(e),J=i(e,"P",{});var qn=r(J);en=p(qn,"oneccl_bindings_for_pytorch module implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),qn.forEach(n),be=c(e),E=i(e,"P",{});var He=r(E);nn=p(He,"Check more detailed information for "),S=i(He,"A",{href:!0,rel:!0});var jn=r(S);tn=p(jn,"oneccl_bind_pt"),jn.forEach(n),on=p(He,"."),He.forEach(n),ve=c(e),g=i(e,"H3",{class:!0});var Ie=r(g);T=i(Ie,"A",{id:!0,class:!0,href:!0});var Hn=r(T);ie=i(Hn,"SPAN",{});var In=r(ie);m(U.$$.fragment,In),In.forEach(n),Hn.forEach(n),an=c(Ie),re=i(Ie,"SPAN",{});var Wn=r(re);ln=p(Wn,"Intel\xAE oneCCL Bindings for PyTorch installation:"),Wn.forEach(n),Ie.forEach(n),we=c(e),Q=i(e,"P",{});var Bn=r(Q);rn=p(Bn,"Wheel files are avaiable for the following Python versions:"),Bn.forEach(n),$e=c(e),V=i(e,"P",{});var Fn=r(V);sn=p(Fn,"For PyTorch-1.10:"),Fn.forEach(n),ge=c(e),m(M.$$.fragment,e),Pe=c(e),Y=i(e,"P",{});var Gn=r(Y);pn=p(Gn,"For PyTorch-1.11:"),Gn.forEach(n),ye=c(e),m(q.$$.fragment,e),Ce=c(e),Z=i(e,"P",{});var Kn=r(Z);fn=p(Kn,"For PyTorch-1.12:"),Kn.forEach(n),Ee=c(e),m(j.$$.fragment,e),Te=c(e),k=i(e,"P",{});var We=r(k);cn=p(We,"Check more approaches for "),H=i(We,"A",{href:!0,rel:!0});var zn=r(H);_n=p(zn,"oneccl_bind_pt installation"),zn.forEach(n),hn=p(We,"."),We.forEach(n),ke=c(e),P=i(e,"H3",{class:!0});var Be=r(P);D=i(Be,"A",{id:!0,class:!0,href:!0});var Xn=r(D);se=i(Xn,"SPAN",{});var Jn=r(se);m(I.$$.fragment,Jn),Jn.forEach(n),Xn.forEach(n),mn=c(Be),pe=i(Be,"SPAN",{});var Qn=r(pe);un=p(Qn,"Usage in Trainer"),Qn.forEach(n),Be.forEach(n),De=p(e,`

To enable DDP in Trainer with ccl backend, users should add **\`--xpu_backend ccl\`** in training command arguments.
`),W=i(e,"P",{});var kn=r(W);dn=p(kn,"Take an example of the use cases on "),B=i(kn,"A",{href:!0,rel:!0});var Vn=r(B);xn=p(Vn,"Transformers question-answering"),Vn.forEach(n),kn.forEach(n),Re=c(e),R=i(e,"P",{});var Fe=r(R);bn=p(Fe,"following command enables "),fe=i(Fe,"STRONG",{});var Yn=r(fe);vn=p(Yn,"2DDP"),Yn.forEach(n),wn=p(Fe," in one Xeon node, with one process running per one socket, OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Fe.forEach(n),Ae=c(e),m(F.$$.fragment,e),Ne=c(e),A=i(e,"P",{});var Ge=r(A);$n=p(Ge,"following command enables "),ce=i(Ge,"STRONG",{});var Zn=r(ce);gn=p(Zn,"4DDP"),Zn.forEach(n),Pn=p(Ge," in two Xeons (node0 and node1, taking node0 as the master), ppn(processes per node) is set to 2, with one process running per one socket, OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Ge.forEach(n),Oe=c(e),ee=i(e,"P",{});var et=r(ee);yn=p(et,"in node0, you need to create a config file which contains ip of each node(for ex: hostfile) and pass to mpirun as a argument"),et.forEach(n),Le=c(e),m(G.$$.fragment,e),Se=c(e),N=i(e,"P",{});var Ke=r(N);Cn=p(Ke,"run the following command in node0 and "),_e=i(Ke,"STRONG",{});var nt=r(_e);En=p(nt,"4DDP"),nt.forEach(n),Tn=p(Ke," will be enabled in node0 and node1"),Ke.forEach(n),Ue=c(e),m(K.$$.fragment,e),this.h()},h(){_(v,"name","hf:doc:metadata"),_(v,"content",JSON.stringify(pt)),_(y,"id","efficient-training-on-multiple-cpus"),_(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(y,"href","#efficient-training-on-multiple-cpus"),_(w,"class","relative group"),_(C,"id","intel-oneccl-bindings-for-pytorch"),_(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(C,"href","#intel-oneccl-bindings-for-pytorch"),_($,"class","relative group"),_(S,"href","https://github.com/intel/torch-ccl"),_(S,"rel","nofollow"),_(T,"id","intel-oneccl-bindings-for-pytorch-installation"),_(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(T,"href","#intel-oneccl-bindings-for-pytorch-installation"),_(g,"class","relative group"),_(H,"href","https://github.com/intel/torch-ccl"),_(H,"rel","nofollow"),_(D,"id","usage-in-trainer"),_(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(D,"href","#usage-in-trainer"),_(P,"class","relative group"),_(B,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),_(B,"rel","nofollow")},m(e,o){t(document.head,v),a(e,he,o),a(e,w,o),t(w,y),t(y,te),u(O,te,null),t(w,Xe),t(w,oe),t(oe,Je),a(e,me,o),a(e,z,o),t(z,Qe),a(e,ue,o),a(e,$,o),t($,C),t(C,ae),u(L,ae,null),t($,Ve),t($,le),t(le,Ye),a(e,de,o),a(e,X,o),t(X,Ze),a(e,xe,o),a(e,J,o),t(J,en),a(e,be,o),a(e,E,o),t(E,nn),t(E,S),t(S,tn),t(E,on),a(e,ve,o),a(e,g,o),t(g,T),t(T,ie),u(U,ie,null),t(g,an),t(g,re),t(re,ln),a(e,we,o),a(e,Q,o),t(Q,rn),a(e,$e,o),a(e,V,o),t(V,sn),a(e,ge,o),u(M,e,o),a(e,Pe,o),a(e,Y,o),t(Y,pn),a(e,ye,o),u(q,e,o),a(e,Ce,o),a(e,Z,o),t(Z,fn),a(e,Ee,o),u(j,e,o),a(e,Te,o),a(e,k,o),t(k,cn),t(k,H),t(H,_n),t(k,hn),a(e,ke,o),a(e,P,o),t(P,D),t(D,se),u(I,se,null),t(P,mn),t(P,pe),t(pe,un),a(e,De,o),a(e,W,o),t(W,dn),t(W,B),t(B,xn),a(e,Re,o),a(e,R,o),t(R,bn),t(R,fe),t(fe,vn),t(R,wn),a(e,Ae,o),u(F,e,o),a(e,Ne,o),a(e,A,o),t(A,$n),t(A,ce),t(ce,gn),t(A,Pn),a(e,Oe,o),a(e,ee,o),t(ee,yn),a(e,Le,o),u(G,e,o),a(e,Se,o),a(e,N,o),t(N,Cn),t(N,_e),t(_e,En),t(N,Tn),a(e,Ue,o),u(K,e,o),Me=!0},p:it,i(e){Me||(d(O.$$.fragment,e),d(L.$$.fragment,e),d(U.$$.fragment,e),d(M.$$.fragment,e),d(q.$$.fragment,e),d(j.$$.fragment,e),d(I.$$.fragment,e),d(F.$$.fragment,e),d(G.$$.fragment,e),d(K.$$.fragment,e),Me=!0)},o(e){x(O.$$.fragment,e),x(L.$$.fragment,e),x(U.$$.fragment,e),x(M.$$.fragment,e),x(q.$$.fragment,e),x(j.$$.fragment,e),x(I.$$.fragment,e),x(F.$$.fragment,e),x(G.$$.fragment,e),x(K.$$.fragment,e),Me=!1},d(e){n(v),e&&n(he),e&&n(w),b(O),e&&n(me),e&&n(z),e&&n(ue),e&&n($),b(L),e&&n(de),e&&n(X),e&&n(xe),e&&n(J),e&&n(be),e&&n(E),e&&n(ve),e&&n(g),b(U),e&&n(we),e&&n(Q),e&&n($e),e&&n(V),e&&n(ge),b(M,e),e&&n(Pe),e&&n(Y),e&&n(ye),b(q,e),e&&n(Ce),e&&n(Z),e&&n(Ee),b(j,e),e&&n(Te),e&&n(k),e&&n(ke),e&&n(P),b(I),e&&n(De),e&&n(W),e&&n(Re),e&&n(R),e&&n(Ae),b(F,e),e&&n(Ne),e&&n(A),e&&n(Oe),e&&n(ee),e&&n(Le),b(G,e),e&&n(Se),e&&n(N),e&&n(Ue),b(K,e)}}}const pt={local:"efficient-training-on-multiple-cpus",sections:[{local:"intel-oneccl-bindings-for-pytorch",sections:[{local:"intel-oneccl-bindings-for-pytorch-installation",title:"Intel\xAE oneCCL Bindings for PyTorch installation:"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Intel\xAE oneCCL Bindings for PyTorch"}],title:"Efficient Training on Multiple CPUs"};function ft(Dn){return rt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mt extends tt{constructor(v){super();ot(this,v,ft,st,at,{})}}export{mt as default,pt as metadata};
