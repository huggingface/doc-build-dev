import{S as zo,i as qo,s as Mo,e as a,k as c,w as _,t as s,M as Do,c as o,d as r,m as d,a as n,x as v,h as i,b as l,F as e,g,y as x,q as y,o as b,B as E,v as Lo,L as So}from"../../chunks/vendor-6b77c823.js";import{T as Po}from"../../chunks/Tip-39098574.js";import{D as P}from"../../chunks/Docstring-1088f2fb.js";import{C as No}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Pt}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Bo}from"../../chunks/ExampleCodeBlock-5212b321.js";function Ao(K){let m,T,f,h,F;return{c(){m=a("p"),T=s("Passing "),f=a("code"),h=s("use_auth_token=True"),F=s(" is required when you want to use a private model.")},l(u){m=o(u,"P",{});var $=n(m);T=i($,"Passing "),f=o($,"CODE",{});var L=n(f);h=i(L,"use_auth_token=True"),L.forEach(r),F=i($," is required when you want to use a private model."),$.forEach(r)},m(u,$){g(u,m,$),e(m,T),e(m,f),e(f,h),e(m,F)},d(u){u&&r(m)}}}function jo(K){let m,T,f,h,F;return h=new No({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),T=s("Examples:"),f=c(),_(h.$$.fragment)},l(u){m=o(u,"P",{});var $=n(m);T=i($,"Examples:"),$.forEach(r),f=d(u),v(h.$$.fragment,u)},m(u,$){g(u,m,$),e(m,T),g(u,f,$),x(h,u,$),F=!0},p:So,i(u){F||(y(h.$$.fragment,u),F=!0)},o(u){b(h.$$.fragment,u),F=!1},d(u){u&&r(m),u&&r(f),E(h,u)}}}function Co(K){let m,T,f,h,F,u,$,L;return{c(){m=a("p"),T=s("If the "),f=a("code"),h=s("processed_features"),F=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),u=a("code"),$=s("return_tensors"),L=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(V){m=o(V,"P",{});var z=n(m);T=i(z,"If the "),f=o(z,"CODE",{});var q=n(f);h=i(q,"processed_features"),q.forEach(r),F=i(z,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),u=o(z,"CODE",{});var je=n(u);$=i(je,"return_tensors"),je.forEach(r),L=i(z,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),z.forEach(r)},m(V,z){g(V,m,z),e(m,T),e(m,f),e(f,h),e(m,F),e(m,u),e(u,$),e(m,L)},d(V){V&&r(m)}}}function Vo(K){let m,T,f,h,F,u,$,L,V,z,q,je,Ke,tr,rr,Qe,ar,or,zt,O,Q,Xe,ge,nr,Ze,sr,qt,M,_e,ir,et,cr,dr,S,ve,lr,B,mr,Ce,ur,pr,tt,fr,hr,Ve,gr,_r,vr,X,xr,Z,yr,ee,xe,br,W,Er,rt,$r,wr,Oe,Fr,Tr,Mt,U,te,at,ye,kr,ot,Ir,Dt,A,be,Pr,nt,zr,qr,N,Ee,Mr,st,Dr,Lr,H,Sr,it,Nr,Br,ct,Ar,jr,Cr,re,Lt,R,ae,dt,$e,Vr,lt,Or,St,k,we,Wr,J,Ur,We,Hr,Rr,mt,Jr,Yr,Gr,ut,Kr,Qr,oe,Fe,Xr,pt,Zr,ea,ne,Te,ta,ke,ra,ft,aa,oa,Nt,Y,se,ht,Ie,na,gt,sa,Bt,w,Pe,ia,_t,ca,da,ie,ze,la,qe,ma,vt,ua,pa,fa,ce,Me,ha,D,ga,xt,_a,va,yt,xa,ya,bt,ba,Ea,Et,$a,wa,Fa,de,De,Ta,G,ka,$t,Ia,Pa,wt,za,qa,Ma,le,Le,Da,Se,La,Ft,Sa,Na,Ba,me,Ne,Aa,Be,ja,Tt,Ca,Va,At;return u=new Pt({}),ge=new Pt({}),_e=new P({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L204"}}),ve=new P({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L228",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),X=new Po({props:{$$slots:{default:[Ao]},$$scope:{ctx:K}}}),Z=new Bo({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[jo]},$$scope:{ctx:K}}}),xe=new P({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your feature extractor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_17196/en/main_classes/processors#transformers.ProcessorMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L312"}}),ye=new Pt({}),be=new P({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_sequence_utils.py#L30"}}),Ee=new P({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_sequence_utils.py#L53"}}),re=new Po({props:{$$slots:{default:[Co]},$$scope:{ctx:K}}}),$e=new Pt({}),we=new P({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L62"}}),Fe=new P({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L116"}}),Te=new P({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/feature_extraction_utils.py#L181",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ie=new Pt({}),Pe=new P({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L77"}}),ze=new P({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L250"}}),Me=new P({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L152"}}),De=new P({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L188"}}),Le=new P({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L119"}}),Ne=new P({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/image_utils.py#L89"}}),{c(){m=a("meta"),T=c(),f=a("h1"),h=a("a"),F=a("span"),_(u.$$.fragment),$=c(),L=a("span"),V=s("Feature Extractor"),z=c(),q=a("p"),je=s(`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),Ke=a("em"),tr=s("e.g."),rr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),Qe=a("em"),ar=s("e.g."),or=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),zt=c(),O=a("h2"),Q=a("a"),Xe=a("span"),_(ge.$$.fragment),nr=c(),Ze=a("span"),sr=s("FeatureExtractionMixin"),qt=c(),M=a("div"),_(_e.$$.fragment),ir=c(),et=a("p"),cr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),dr=c(),S=a("div"),_(ve.$$.fragment),lr=c(),B=a("p"),mr=s("Instantiate a type of "),Ce=a("a"),ur=s("FeatureExtractionMixin"),pr=s(" from a feature extractor, "),tt=a("em"),fr=s("e.g."),hr=s(` a
derived class of `),Ve=a("a"),gr=s("SequenceFeatureExtractor"),_r=s("."),vr=c(),_(X.$$.fragment),xr=c(),_(Z.$$.fragment),yr=c(),ee=a("div"),_(xe.$$.fragment),br=c(),W=a("p"),Er=s("Save a feature_extractor object to the directory "),rt=a("code"),$r=s("save_directory"),wr=s(`, so that it can be re-loaded using the
`),Oe=a("a"),Fr=s("from_pretrained()"),Tr=s(" class method."),Mt=c(),U=a("h2"),te=a("a"),at=a("span"),_(ye.$$.fragment),kr=c(),ot=a("span"),Ir=s("SequenceFeatureExtractor"),Dt=c(),A=a("div"),_(be.$$.fragment),Pr=c(),nt=a("p"),zr=s("This is a general feature extraction class for speech recognition."),qr=c(),N=a("div"),_(Ee.$$.fragment),Mr=c(),st=a("p"),Dr=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),Lr=c(),H=a("p"),Sr=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),it=a("code"),Nr=s("self.padding_side"),Br=s(`,
`),ct=a("code"),Ar=s("self.padding_value"),jr=s(")"),Cr=c(),_(re.$$.fragment),Lt=c(),R=a("h2"),ae=a("a"),dt=a("span"),_($e.$$.fragment),Vr=c(),lt=a("span"),Or=s("BatchFeature"),St=c(),k=a("div"),_(we.$$.fragment),Wr=c(),J=a("p"),Ur=s("Holds the output of the "),We=a("a"),Hr=s("pad()"),Rr=s(" and feature extractor specific "),mt=a("code"),Jr=s("__call__"),Yr=s(" methods."),Gr=c(),ut=a("p"),Kr=s("This class is derived from a python dictionary and can be used as a dictionary."),Qr=c(),oe=a("div"),_(Fe.$$.fragment),Xr=c(),pt=a("p"),Zr=s("Convert the inner content to tensors."),ea=c(),ne=a("div"),_(Te.$$.fragment),ta=c(),ke=a("p"),ra=s("Send all values to device by calling "),ft=a("code"),aa=s("v.to(device)"),oa=s(" (PyTorch only)."),Nt=c(),Y=a("h2"),se=a("a"),ht=a("span"),_(Ie.$$.fragment),na=c(),gt=a("span"),sa=s("ImageFeatureExtractionMixin"),Bt=c(),w=a("div"),_(Pe.$$.fragment),ia=c(),_t=a("p"),ca=s("Mixin that contain utilities for preparing image features."),da=c(),ie=a("div"),_(ze.$$.fragment),la=c(),qe=a("p"),ma=s("Crops "),vt=a("code"),ua=s("image"),pa=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),fa=c(),ce=a("div"),_(Me.$$.fragment),ha=c(),D=a("p"),ga=s("Normalizes "),xt=a("code"),_a=s("image"),va=s(" with "),yt=a("code"),xa=s("mean"),ya=s(" and "),bt=a("code"),ba=s("std"),Ea=s(". Note that this will trigger a conversion of "),Et=a("code"),$a=s("image"),wa=s(` to a NumPy array
if it\u2019s a PIL Image.`),Fa=c(),de=a("div"),_(De.$$.fragment),Ta=c(),G=a("p"),ka=s("Resizes "),$t=a("code"),Ia=s("image"),Pa=s(". Note that this will trigger a conversion of "),wt=a("code"),za=s("image"),qa=s(" to a PIL Image."),Ma=c(),le=a("div"),_(Le.$$.fragment),Da=c(),Se=a("p"),La=s("Converts "),Ft=a("code"),Sa=s("image"),Na=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Ba=c(),me=a("div"),_(Ne.$$.fragment),Aa=c(),Be=a("p"),ja=s("Converts "),Tt=a("code"),Ca=s("image"),Va=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const p=Do('[data-svelte="svelte-1phssyn"]',document.head);m=o(p,"META",{name:!0,content:!0}),p.forEach(r),T=d(t),f=o(t,"H1",{class:!0});var Ae=n(f);h=o(Ae,"A",{id:!0,class:!0,href:!0});var kt=n(h);F=o(kt,"SPAN",{});var It=n(F);v(u.$$.fragment,It),It.forEach(r),kt.forEach(r),$=d(Ae),L=o(Ae,"SPAN",{});var Oa=n(L);V=i(Oa,"Feature Extractor"),Oa.forEach(r),Ae.forEach(r),z=d(t),q=o(t,"P",{});var Ue=n(q);je=i(Ue,`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),Ke=o(Ue,"EM",{});var Wa=n(Ke);tr=i(Wa,"e.g."),Wa.forEach(r),rr=i(Ue,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),Qe=o(Ue,"EM",{});var Ua=n(Qe);ar=i(Ua,"e.g."),Ua.forEach(r),or=i(Ue,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Ue.forEach(r),zt=d(t),O=o(t,"H2",{class:!0});var jt=n(O);Q=o(jt,"A",{id:!0,class:!0,href:!0});var Ha=n(Q);Xe=o(Ha,"SPAN",{});var Ra=n(Xe);v(ge.$$.fragment,Ra),Ra.forEach(r),Ha.forEach(r),nr=d(jt),Ze=o(jt,"SPAN",{});var Ja=n(Ze);sr=i(Ja,"FeatureExtractionMixin"),Ja.forEach(r),jt.forEach(r),qt=d(t),M=o(t,"DIV",{class:!0});var ue=n(M);v(_e.$$.fragment,ue),ir=d(ue),et=o(ue,"P",{});var Ya=n(et);cr=i(Ya,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Ya.forEach(r),dr=d(ue),S=o(ue,"DIV",{class:!0});var pe=n(S);v(ve.$$.fragment,pe),lr=d(pe),B=o(pe,"P",{});var fe=n(B);mr=i(fe,"Instantiate a type of "),Ce=o(fe,"A",{href:!0});var Ga=n(Ce);ur=i(Ga,"FeatureExtractionMixin"),Ga.forEach(r),pr=i(fe," from a feature extractor, "),tt=o(fe,"EM",{});var Ka=n(tt);fr=i(Ka,"e.g."),Ka.forEach(r),hr=i(fe,` a
derived class of `),Ve=o(fe,"A",{href:!0});var Qa=n(Ve);gr=i(Qa,"SequenceFeatureExtractor"),Qa.forEach(r),_r=i(fe,"."),fe.forEach(r),vr=d(pe),v(X.$$.fragment,pe),xr=d(pe),v(Z.$$.fragment,pe),pe.forEach(r),yr=d(ue),ee=o(ue,"DIV",{class:!0});var Ct=n(ee);v(xe.$$.fragment,Ct),br=d(Ct),W=o(Ct,"P",{});var He=n(W);Er=i(He,"Save a feature_extractor object to the directory "),rt=o(He,"CODE",{});var Xa=n(rt);$r=i(Xa,"save_directory"),Xa.forEach(r),wr=i(He,`, so that it can be re-loaded using the
`),Oe=o(He,"A",{href:!0});var Za=n(Oe);Fr=i(Za,"from_pretrained()"),Za.forEach(r),Tr=i(He," class method."),He.forEach(r),Ct.forEach(r),ue.forEach(r),Mt=d(t),U=o(t,"H2",{class:!0});var Vt=n(U);te=o(Vt,"A",{id:!0,class:!0,href:!0});var eo=n(te);at=o(eo,"SPAN",{});var to=n(at);v(ye.$$.fragment,to),to.forEach(r),eo.forEach(r),kr=d(Vt),ot=o(Vt,"SPAN",{});var ro=n(ot);Ir=i(ro,"SequenceFeatureExtractor"),ro.forEach(r),Vt.forEach(r),Dt=d(t),A=o(t,"DIV",{class:!0});var Re=n(A);v(be.$$.fragment,Re),Pr=d(Re),nt=o(Re,"P",{});var ao=n(nt);zr=i(ao,"This is a general feature extraction class for speech recognition."),ao.forEach(r),qr=d(Re),N=o(Re,"DIV",{class:!0});var he=n(N);v(Ee.$$.fragment,he),Mr=d(he),st=o(he,"P",{});var oo=n(st);Dr=i(oo,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),oo.forEach(r),Lr=d(he),H=o(he,"P",{});var Je=n(H);Sr=i(Je,"Padding side (left/right) padding values are defined at the feature extractor level (with "),it=o(Je,"CODE",{});var no=n(it);Nr=i(no,"self.padding_side"),no.forEach(r),Br=i(Je,`,
`),ct=o(Je,"CODE",{});var so=n(ct);Ar=i(so,"self.padding_value"),so.forEach(r),jr=i(Je,")"),Je.forEach(r),Cr=d(he),v(re.$$.fragment,he),he.forEach(r),Re.forEach(r),Lt=d(t),R=o(t,"H2",{class:!0});var Ot=n(R);ae=o(Ot,"A",{id:!0,class:!0,href:!0});var io=n(ae);dt=o(io,"SPAN",{});var co=n(dt);v($e.$$.fragment,co),co.forEach(r),io.forEach(r),Vr=d(Ot),lt=o(Ot,"SPAN",{});var lo=n(lt);Or=i(lo,"BatchFeature"),lo.forEach(r),Ot.forEach(r),St=d(t),k=o(t,"DIV",{class:!0});var j=n(k);v(we.$$.fragment,j),Wr=d(j),J=o(j,"P",{});var Ye=n(J);Ur=i(Ye,"Holds the output of the "),We=o(Ye,"A",{href:!0});var mo=n(We);Hr=i(mo,"pad()"),mo.forEach(r),Rr=i(Ye," and feature extractor specific "),mt=o(Ye,"CODE",{});var uo=n(mt);Jr=i(uo,"__call__"),uo.forEach(r),Yr=i(Ye," methods."),Ye.forEach(r),Gr=d(j),ut=o(j,"P",{});var po=n(ut);Kr=i(po,"This class is derived from a python dictionary and can be used as a dictionary."),po.forEach(r),Qr=d(j),oe=o(j,"DIV",{class:!0});var Wt=n(oe);v(Fe.$$.fragment,Wt),Xr=d(Wt),pt=o(Wt,"P",{});var fo=n(pt);Zr=i(fo,"Convert the inner content to tensors."),fo.forEach(r),Wt.forEach(r),ea=d(j),ne=o(j,"DIV",{class:!0});var Ut=n(ne);v(Te.$$.fragment,Ut),ta=d(Ut),ke=o(Ut,"P",{});var Ht=n(ke);ra=i(Ht,"Send all values to device by calling "),ft=o(Ht,"CODE",{});var ho=n(ft);aa=i(ho,"v.to(device)"),ho.forEach(r),oa=i(Ht," (PyTorch only)."),Ht.forEach(r),Ut.forEach(r),j.forEach(r),Nt=d(t),Y=o(t,"H2",{class:!0});var Rt=n(Y);se=o(Rt,"A",{id:!0,class:!0,href:!0});var go=n(se);ht=o(go,"SPAN",{});var _o=n(ht);v(Ie.$$.fragment,_o),_o.forEach(r),go.forEach(r),na=d(Rt),gt=o(Rt,"SPAN",{});var vo=n(gt);sa=i(vo,"ImageFeatureExtractionMixin"),vo.forEach(r),Rt.forEach(r),Bt=d(t),w=o(t,"DIV",{class:!0});var I=n(w);v(Pe.$$.fragment,I),ia=d(I),_t=o(I,"P",{});var xo=n(_t);ca=i(xo,"Mixin that contain utilities for preparing image features."),xo.forEach(r),da=d(I),ie=o(I,"DIV",{class:!0});var Jt=n(ie);v(ze.$$.fragment,Jt),la=d(Jt),qe=o(Jt,"P",{});var Yt=n(qe);ma=i(Yt,"Crops "),vt=o(Yt,"CODE",{});var yo=n(vt);ua=i(yo,"image"),yo.forEach(r),pa=i(Yt,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),Yt.forEach(r),Jt.forEach(r),fa=d(I),ce=o(I,"DIV",{class:!0});var Gt=n(ce);v(Me.$$.fragment,Gt),ha=d(Gt),D=o(Gt,"P",{});var C=n(D);ga=i(C,"Normalizes "),xt=o(C,"CODE",{});var bo=n(xt);_a=i(bo,"image"),bo.forEach(r),va=i(C," with "),yt=o(C,"CODE",{});var Eo=n(yt);xa=i(Eo,"mean"),Eo.forEach(r),ya=i(C," and "),bt=o(C,"CODE",{});var $o=n(bt);ba=i($o,"std"),$o.forEach(r),Ea=i(C,". Note that this will trigger a conversion of "),Et=o(C,"CODE",{});var wo=n(Et);$a=i(wo,"image"),wo.forEach(r),wa=i(C,` to a NumPy array
if it\u2019s a PIL Image.`),C.forEach(r),Gt.forEach(r),Fa=d(I),de=o(I,"DIV",{class:!0});var Kt=n(de);v(De.$$.fragment,Kt),Ta=d(Kt),G=o(Kt,"P",{});var Ge=n(G);ka=i(Ge,"Resizes "),$t=o(Ge,"CODE",{});var Fo=n($t);Ia=i(Fo,"image"),Fo.forEach(r),Pa=i(Ge,". Note that this will trigger a conversion of "),wt=o(Ge,"CODE",{});var To=n(wt);za=i(To,"image"),To.forEach(r),qa=i(Ge," to a PIL Image."),Ge.forEach(r),Kt.forEach(r),Ma=d(I),le=o(I,"DIV",{class:!0});var Qt=n(le);v(Le.$$.fragment,Qt),Da=d(Qt),Se=o(Qt,"P",{});var Xt=n(Se);La=i(Xt,"Converts "),Ft=o(Xt,"CODE",{});var ko=n(Ft);Sa=i(ko,"image"),ko.forEach(r),Na=i(Xt,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Xt.forEach(r),Qt.forEach(r),Ba=d(I),me=o(I,"DIV",{class:!0});var Zt=n(me);v(Ne.$$.fragment,Zt),Aa=d(Zt),Be=o(Zt,"P",{});var er=n(Be);ja=i(er,"Converts "),Tt=o(er,"CODE",{});var Io=n(Tt);Ca=i(Io,"image"),Io.forEach(r),Va=i(er,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),er.forEach(r),Zt.forEach(r),I.forEach(r),this.h()},h(){l(m,"name","hf:doc:metadata"),l(m,"content",JSON.stringify(Oo)),l(h,"id","feature-extractor"),l(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(h,"href","#feature-extractor"),l(f,"class","relative group"),l(Q,"id","transformers.FeatureExtractionMixin"),l(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Q,"href","#transformers.FeatureExtractionMixin"),l(O,"class","relative group"),l(Ce,"href","/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(Ve,"href","/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Oe,"href","/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained"),l(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(te,"id","transformers.SequenceFeatureExtractor"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#transformers.SequenceFeatureExtractor"),l(U,"class","relative group"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ae,"id","transformers.BatchFeature"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#transformers.BatchFeature"),l(R,"class","relative group"),l(We,"href","/docs/transformers/pr_17196/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),l(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(se,"id","transformers.ImageFeatureExtractionMixin"),l(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(se,"href","#transformers.ImageFeatureExtractionMixin"),l(Y,"class","relative group"),l(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,m),g(t,T,p),g(t,f,p),e(f,h),e(h,F),x(u,F,null),e(f,$),e(f,L),e(L,V),g(t,z,p),g(t,q,p),e(q,je),e(q,Ke),e(Ke,tr),e(q,rr),e(q,Qe),e(Qe,ar),e(q,or),g(t,zt,p),g(t,O,p),e(O,Q),e(Q,Xe),x(ge,Xe,null),e(O,nr),e(O,Ze),e(Ze,sr),g(t,qt,p),g(t,M,p),x(_e,M,null),e(M,ir),e(M,et),e(et,cr),e(M,dr),e(M,S),x(ve,S,null),e(S,lr),e(S,B),e(B,mr),e(B,Ce),e(Ce,ur),e(B,pr),e(B,tt),e(tt,fr),e(B,hr),e(B,Ve),e(Ve,gr),e(B,_r),e(S,vr),x(X,S,null),e(S,xr),x(Z,S,null),e(M,yr),e(M,ee),x(xe,ee,null),e(ee,br),e(ee,W),e(W,Er),e(W,rt),e(rt,$r),e(W,wr),e(W,Oe),e(Oe,Fr),e(W,Tr),g(t,Mt,p),g(t,U,p),e(U,te),e(te,at),x(ye,at,null),e(U,kr),e(U,ot),e(ot,Ir),g(t,Dt,p),g(t,A,p),x(be,A,null),e(A,Pr),e(A,nt),e(nt,zr),e(A,qr),e(A,N),x(Ee,N,null),e(N,Mr),e(N,st),e(st,Dr),e(N,Lr),e(N,H),e(H,Sr),e(H,it),e(it,Nr),e(H,Br),e(H,ct),e(ct,Ar),e(H,jr),e(N,Cr),x(re,N,null),g(t,Lt,p),g(t,R,p),e(R,ae),e(ae,dt),x($e,dt,null),e(R,Vr),e(R,lt),e(lt,Or),g(t,St,p),g(t,k,p),x(we,k,null),e(k,Wr),e(k,J),e(J,Ur),e(J,We),e(We,Hr),e(J,Rr),e(J,mt),e(mt,Jr),e(J,Yr),e(k,Gr),e(k,ut),e(ut,Kr),e(k,Qr),e(k,oe),x(Fe,oe,null),e(oe,Xr),e(oe,pt),e(pt,Zr),e(k,ea),e(k,ne),x(Te,ne,null),e(ne,ta),e(ne,ke),e(ke,ra),e(ke,ft),e(ft,aa),e(ke,oa),g(t,Nt,p),g(t,Y,p),e(Y,se),e(se,ht),x(Ie,ht,null),e(Y,na),e(Y,gt),e(gt,sa),g(t,Bt,p),g(t,w,p),x(Pe,w,null),e(w,ia),e(w,_t),e(_t,ca),e(w,da),e(w,ie),x(ze,ie,null),e(ie,la),e(ie,qe),e(qe,ma),e(qe,vt),e(vt,ua),e(qe,pa),e(w,fa),e(w,ce),x(Me,ce,null),e(ce,ha),e(ce,D),e(D,ga),e(D,xt),e(xt,_a),e(D,va),e(D,yt),e(yt,xa),e(D,ya),e(D,bt),e(bt,ba),e(D,Ea),e(D,Et),e(Et,$a),e(D,wa),e(w,Fa),e(w,de),x(De,de,null),e(de,Ta),e(de,G),e(G,ka),e(G,$t),e($t,Ia),e(G,Pa),e(G,wt),e(wt,za),e(G,qa),e(w,Ma),e(w,le),x(Le,le,null),e(le,Da),e(le,Se),e(Se,La),e(Se,Ft),e(Ft,Sa),e(Se,Na),e(w,Ba),e(w,me),x(Ne,me,null),e(me,Aa),e(me,Be),e(Be,ja),e(Be,Tt),e(Tt,Ca),e(Be,Va),At=!0},p(t,[p]){const Ae={};p&2&&(Ae.$$scope={dirty:p,ctx:t}),X.$set(Ae);const kt={};p&2&&(kt.$$scope={dirty:p,ctx:t}),Z.$set(kt);const It={};p&2&&(It.$$scope={dirty:p,ctx:t}),re.$set(It)},i(t){At||(y(u.$$.fragment,t),y(ge.$$.fragment,t),y(_e.$$.fragment,t),y(ve.$$.fragment,t),y(X.$$.fragment,t),y(Z.$$.fragment,t),y(xe.$$.fragment,t),y(ye.$$.fragment,t),y(be.$$.fragment,t),y(Ee.$$.fragment,t),y(re.$$.fragment,t),y($e.$$.fragment,t),y(we.$$.fragment,t),y(Fe.$$.fragment,t),y(Te.$$.fragment,t),y(Ie.$$.fragment,t),y(Pe.$$.fragment,t),y(ze.$$.fragment,t),y(Me.$$.fragment,t),y(De.$$.fragment,t),y(Le.$$.fragment,t),y(Ne.$$.fragment,t),At=!0)},o(t){b(u.$$.fragment,t),b(ge.$$.fragment,t),b(_e.$$.fragment,t),b(ve.$$.fragment,t),b(X.$$.fragment,t),b(Z.$$.fragment,t),b(xe.$$.fragment,t),b(ye.$$.fragment,t),b(be.$$.fragment,t),b(Ee.$$.fragment,t),b(re.$$.fragment,t),b($e.$$.fragment,t),b(we.$$.fragment,t),b(Fe.$$.fragment,t),b(Te.$$.fragment,t),b(Ie.$$.fragment,t),b(Pe.$$.fragment,t),b(ze.$$.fragment,t),b(Me.$$.fragment,t),b(De.$$.fragment,t),b(Le.$$.fragment,t),b(Ne.$$.fragment,t),At=!1},d(t){r(m),t&&r(T),t&&r(f),E(u),t&&r(z),t&&r(q),t&&r(zt),t&&r(O),E(ge),t&&r(qt),t&&r(M),E(_e),E(ve),E(X),E(Z),E(xe),t&&r(Mt),t&&r(U),E(ye),t&&r(Dt),t&&r(A),E(be),E(Ee),E(re),t&&r(Lt),t&&r(R),E($e),t&&r(St),t&&r(k),E(we),E(Fe),E(Te),t&&r(Nt),t&&r(Y),E(Ie),t&&r(Bt),t&&r(w),E(Pe),E(ze),E(Me),E(De),E(Le),E(Ne)}}}const Oo={local:"feature-extractor",sections:[{local:"transformers.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function Wo(K){return Lo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ko extends zo{constructor(m){super();qo(this,m,Wo,Vo,Mo,{})}}export{Ko as default,Oo as metadata};
