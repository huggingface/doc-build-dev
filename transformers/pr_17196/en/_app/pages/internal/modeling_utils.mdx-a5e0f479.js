import{S as Qi,i as Bi,s as Gi,e as r,k as l,w as h,t as a,M as Ri,c as s,d as o,m as d,a as n,x as g,h as i,b as f,F as t,g as m,y as _,q as v,o as b,B as $,v as Ui,L as Ki}from"../../chunks/vendor-6b77c823.js";import{T as ro}from"../../chunks/Tip-39098574.js";import{D as x}from"../../chunks/Docstring-1088f2fb.js";import{C as Wi}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as so}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Xi}from"../../chunks/ExampleCodeBlock-5212b321.js";function Ji(P){let p,k,u,y,q,w,D,O,C,Q,F,z,B,A,S,G,L;return{c(){p=r("p"),k=a("One of "),u=r("code"),y=a("start_states"),q=a(" or "),w=r("code"),D=a("start_positions"),O=a(" should be not "),C=r("code"),Q=a("None"),F=a(". If both are set, "),z=r("code"),B=a("start_positions"),A=a(` overrides
`),S=r("code"),G=a("start_states"),L=a(".")},l(E){p=s(E,"P",{});var T=n(p);k=i(T,"One of "),u=s(T,"CODE",{});var I=n(u);y=i(I,"start_states"),I.forEach(o),q=i(T," or "),w=s(T,"CODE",{});var me=n(w);D=i(me,"start_positions"),me.forEach(o),O=i(T," should be not "),C=s(T,"CODE",{});var R=n(C);Q=i(R,"None"),R.forEach(o),F=i(T,". If both are set, "),z=s(T,"CODE",{});var fe=n(z);B=i(fe,"start_positions"),fe.forEach(o),A=i(T,` overrides
`),S=s(T,"CODE",{});var ie=n(S);G=i(ie,"start_states"),ie.forEach(o),L=i(T,"."),T.forEach(o)},m(E,T){m(E,p,T),t(p,k),t(p,u),t(u,y),t(p,q),t(p,w),t(w,D),t(p,O),t(p,C),t(C,Q),t(p,F),t(p,z),t(z,B),t(p,A),t(p,S),t(S,G),t(p,L)},d(E){E&&o(p)}}}function Yi(P){let p,k,u,y,q,w,D,O,C,Q,F,z,B,A,S,G,L;return{c(){p=r("p"),k=a("One of "),u=r("code"),y=a("start_states"),q=a(" or "),w=r("code"),D=a("start_positions"),O=a(" should be not "),C=r("code"),Q=a("None"),F=a(". If both are set, "),z=r("code"),B=a("start_positions"),A=a(` overrides
`),S=r("code"),G=a("start_states"),L=a(".")},l(E){p=s(E,"P",{});var T=n(p);k=i(T,"One of "),u=s(T,"CODE",{});var I=n(u);y=i(I,"start_states"),I.forEach(o),q=i(T," or "),w=s(T,"CODE",{});var me=n(w);D=i(me,"start_positions"),me.forEach(o),O=i(T," should be not "),C=s(T,"CODE",{});var R=n(C);Q=i(R,"None"),R.forEach(o),F=i(T,". If both are set, "),z=s(T,"CODE",{});var fe=n(z);B=i(fe,"start_positions"),fe.forEach(o),A=i(T,` overrides
`),S=s(T,"CODE",{});var ie=n(S);G=i(ie,"start_states"),ie.forEach(o),L=i(T,"."),T.forEach(o)},m(E,T){m(E,p,T),t(p,k),t(p,u),t(u,y),t(p,q),t(p,w),t(w,D),t(p,O),t(p,C),t(C,Q),t(p,F),t(p,z),t(z,B),t(p,A),t(p,S),t(S,G),t(p,L)},d(E){E&&o(p)}}}function Zi(P){let p,k,u,y,q;return y=new Wi({props:{code:`# rename the usual forward() fn to forward_chunk()
def forward_chunk(self, hidden_states):
    hidden_states = self.decoder(hidden_states)
    return hidden_states


# implement a chunked forward function
def forward(self, hidden_states):
    return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)`,highlighted:`<span class="hljs-comment"># rename the usual forward() fn to forward_chunk()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_chunk</span>(<span class="hljs-params">self, hidden_states</span>):
    hidden_states = self.decoder(hidden_states)
    <span class="hljs-keyword">return</span> hidden_states


<span class="hljs-comment"># implement a chunked forward function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
    <span class="hljs-keyword">return</span> apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)`}}),{c(){p=r("p"),k=a("Examples:"),u=l(),h(y.$$.fragment)},l(w){p=s(w,"P",{});var D=n(p);k=i(D,"Examples:"),D.forEach(o),u=d(w),g(y.$$.fragment,w)},m(w,D){m(w,p,D),t(p,k),m(w,u,D),_(y,w,D),q=!0},p:Ki,i(w){q||(v(y.$$.fragment,w),q=!0)},o(w){b(y.$$.fragment,w),q=!1},d(w){w&&o(p),w&&o(u),$(y,w)}}}function el(P){let p,k;return{c(){p=r("p"),k=a("This API is experimental and may have some slight breaking changes in the next releases.")},l(u){p=s(u,"P",{});var y=n(p);k=i(y,"This API is experimental and may have some slight breaking changes in the next releases."),y.forEach(o)},m(u,y){m(u,p,y),t(p,k)},d(u){u&&o(p)}}}function tl(P){let p,k;return{c(){p=r("p"),k=a("Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.")},l(u){p=s(u,"P",{});var y=n(p);k=i(y,"Any label of -100 will be ignored (along with the corresponding logits) in the loss computation."),y.forEach(o)},m(u,y){m(u,p,y),t(p,k)},d(u){u&&o(p)}}}function ol(P){let p,k;return{c(){p=r("p"),k=a("Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.")},l(u){p=s(u,"P",{});var y=n(p);k=i(y,"Any label of -100 will be ignored (along with the corresponding logits) in the loss computation."),y.forEach(o)},m(u,y){m(u,p,y),t(p,k)},d(u){u&&o(p)}}}function rl(P){let p,k;return{c(){p=r("p"),k=a("Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.")},l(u){p=s(u,"P",{});var y=n(p);k=i(y,"Any label of -100 will be ignored (along with the corresponding logits) in the loss computation."),y.forEach(o)},m(u,y){m(u,p,y),t(p,k)},d(u){u&&o(p)}}}function sl(P){let p,k,u,y,q,w,D,O,C,Q,F,z,B,A,S,G,L,E,T,I,me,R,fe,ie,U,Re,rs,no,ss,ns,ao,as,pr,K,Ue,is,io,ls,ds,Ft,Ke,mr,W,We,cs,lo,ps,ms,Le,Xe,fs,Pe,fr,X,Je,us,co,hs,gs,Ce,Ye,_s,ze,ur,ue,Ze,vs,et,bs,At,$s,ys,hr,J,tt,ws,po,Ts,ks,Ot,ot,gr,Y,rt,xs,mo,Es,qs,Se,st,Ds,fo,Ls,_r,he,Fe,uo,nt,Ps,ho,Cs,vr,N,at,zs,H,Ss,go,Fs,As,_o,Os,Is,vo,Ns,Hs,bo,Ms,Vs,js,M,Qs,$o,Bs,Gs,yo,Rs,Us,wo,Ks,Ws,To,Xs,Js,Ys,Ae,br,ge,it,Zs,lt,en,ko,tn,on,$r,Z,dt,rn,xo,sn,nn,Eo,an,yr,ee,ct,ln,qo,dn,cn,Do,pn,wr,te,pt,mn,Lo,fn,un,Po,hn,Tr,_e,Oe,Co,mt,gn,zo,_n,kr,oe,ft,vn,So,bn,$n,Fo,yn,xr,V,ut,wn,Ao,Tn,kn,Oo,xn,En,le,ht,qn,Io,Dn,Ln,gt,Pn,_t,Cn,zn,Er,re,vt,Sn,No,Fn,An,de,bt,On,Ho,In,Nn,Ie,qr,ve,Ne,Mo,$t,Hn,Vo,Mn,Dr,se,yt,Vn,jo,jn,Qn,He,Lr,ne,wt,Bn,Qo,Gn,Rn,Me,Pr,be,Tt,Un,Bo,Kn,Cr,$e,kt,Wn,Go,Xn,zr,ye,xt,Jn,Ro,Yn,Sr,ae,Et,Zn,Uo,ea,ta,Ve,Fr,we,je,Ko,qt,oa,Wo,ra,Ar,Te,Dt,sa,Lt,na,Xo,aa,ia,Or,j,Pt,la,Jo,da,ca,Yo,pa,ma,ke,xe,fa,Zo,ua,ha,er,ga,_a,va,Ee,ba,tr,$a,ya,or,wa,Ta,ka,qe,xa,rr,Ea,qa,sr,Da,La,Ir,De,Ct,Pa,nr,Ca,Nr;return w=new so({}),I=new so({}),Re=new x({props:{name:"class transformers.Conv1D",anchor:"transformers.Conv1D",parameters:[{name:"nf",val:""},{name:"nx",val:""}],parametersDescription:[{anchor:"transformers.Conv1D.nf",description:"<strong>nf</strong> (<code>int</code>) &#x2014; The number of output features.",name:"nf"},{anchor:"transformers.Conv1D.nx",description:"<strong>nx</strong> (<code>int</code>) &#x2014; The number of input features.",name:"nx"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L86"}}),Ue=new x({props:{name:"class transformers.modeling_utils.PoolerStartLogits",anchor:"transformers.modeling_utils.PoolerStartLogits",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerStartLogits.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model, will be used to grab the <code>hidden_size</code> of the model.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2478"}}),Ke=new x({props:{name:"forward",anchor:"transformers.modeling_utils.PoolerStartLogits.forward",parameters:[{name:"hidden_states",val:": FloatTensor"},{name:"p_mask",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerStartLogits.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>) &#x2014;
The final hidden states of the model.`,name:"hidden_states"},{anchor:"transformers.modeling_utils.PoolerStartLogits.forward.p_mask",description:`<strong>p_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len)</code>, <em>optional</em>) &#x2014;
Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.`,name:"p_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2491",returnDescription:`
<p>The start logits for SQuAD.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),We=new x({props:{name:"class transformers.modeling_utils.PoolerEndLogits",anchor:"transformers.modeling_utils.PoolerEndLogits",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerEndLogits.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model, will be used to grab the <code>hidden_size</code> of the model and the <code>layer_norm_eps</code>
to use.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2516"}}),Xe=new x({props:{name:"forward",anchor:"transformers.modeling_utils.PoolerEndLogits.forward",parameters:[{name:"hidden_states",val:": FloatTensor"},{name:"start_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"p_mask",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerEndLogits.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>) &#x2014;
The final hidden states of the model.`,name:"hidden_states"},{anchor:"transformers.modeling_utils.PoolerEndLogits.forward.start_states",description:`<strong>start_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>, <em>optional</em>) &#x2014;
The hidden states of the first tokens for the labeled span.`,name:"start_states"},{anchor:"transformers.modeling_utils.PoolerEndLogits.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
The position of the first token for the labeled span.`,name:"start_positions"},{anchor:"transformers.modeling_utils.PoolerEndLogits.forward.p_mask",description:`<strong>p_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len)</code>, <em>optional</em>) &#x2014;
Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.`,name:"p_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2533",returnDescription:`
<p>The end logits for SQuAD.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Pe=new ro({props:{$$slots:{default:[Ji]},$$scope:{ctx:P}}}),Je=new x({props:{name:"class transformers.modeling_utils.PoolerAnswerClass",anchor:"transformers.modeling_utils.PoolerAnswerClass",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerAnswerClass.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model, will be used to grab the <code>hidden_size</code> of the model.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2585"}}),Ye=new x({props:{name:"forward",anchor:"transformers.modeling_utils.PoolerAnswerClass.forward",parameters:[{name:"hidden_states",val:": FloatTensor"},{name:"start_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"cls_index",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_utils.PoolerAnswerClass.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>) &#x2014;
The final hidden states of the model.`,name:"hidden_states"},{anchor:"transformers.modeling_utils.PoolerAnswerClass.forward.start_states",description:`<strong>start_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>, <em>optional</em>) &#x2014;
The hidden states of the first tokens for the labeled span.`,name:"start_states"},{anchor:"transformers.modeling_utils.PoolerAnswerClass.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
The position of the first token for the labeled span.`,name:"start_positions"},{anchor:"transformers.modeling_utils.PoolerAnswerClass.forward.cls_index",description:`<strong>cls_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Position of the CLS token for each sentence in the batch. If <code>None</code>, takes the last token.`,name:"cls_index"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2600",returnDescription:`
<p>The SQuAD 2.0 answer class.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),ze=new ro({props:{$$slots:{default:[Yi]},$$scope:{ctx:P}}}),Ze=new x({props:{name:"class transformers.modeling_utils.SquadHeadOutput",anchor:"transformers.modeling_utils.SquadHeadOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_top_log_probs",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_top_index",val:": typing.Optional[torch.LongTensor] = None"},{name:"end_top_log_probs",val:": typing.Optional[torch.FloatTensor] = None"},{name:"end_top_index",val:": typing.Optional[torch.LongTensor] = None"},{name:"cls_logits",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_utils.SquadHeadOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned if both <code>start_positions</code> and <code>end_positions</code> are provided) &#x2014;
Classification loss as the sum of start token, end token (and is_impossible if provided) classification
losses.`,name:"loss"},{anchor:"transformers.modeling_utils.SquadHeadOutput.start_top_log_probs",description:`<strong>start_top_log_probs</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.start_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) &#x2014;
Log probabilities for the top config.start_n_top start token possibilities (beam-search).`,name:"start_top_log_probs"},{anchor:"transformers.modeling_utils.SquadHeadOutput.start_top_index",description:`<strong>start_top_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, config.start_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) &#x2014;
Indices for the top config.start_n_top start token possibilities (beam-search).`,name:"start_top_index"},{anchor:"transformers.modeling_utils.SquadHeadOutput.end_top_log_probs",description:`<strong>end_top_log_probs</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.start_n_top * config.end_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) &#x2014;
Log probabilities for the top <code>config.start_n_top * config.end_n_top</code> end token possibilities
(beam-search).`,name:"end_top_log_probs"},{anchor:"transformers.modeling_utils.SquadHeadOutput.end_top_index",description:`<strong>end_top_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, config.start_n_top * config.end_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) &#x2014;
Indices for the top <code>config.start_n_top * config.end_n_top</code> end token possibilities (beam-search).`,name:"end_top_index"},{anchor:"transformers.modeling_utils.SquadHeadOutput.cls_logits",description:`<strong>cls_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) &#x2014;
Log probabilities for the <code>is_impossible</code> label of the answers.`,name:"cls_logits"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2651"}}),tt=new x({props:{name:"class transformers.modeling_utils.SQuADHead",anchor:"transformers.modeling_utils.SQuADHead",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.modeling_utils.SQuADHead.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model, will be used to grab the <code>hidden_size</code> of the model and the <code>layer_norm_eps</code>
to use.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2681"}}),ot=new x({props:{name:"forward",anchor:"transformers.modeling_utils.SQuADHead.forward",parameters:[{name:"hidden_states",val:": FloatTensor"},{name:"start_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"end_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"cls_index",val:": typing.Optional[torch.LongTensor] = None"},{name:"is_impossible",val:": typing.Optional[torch.LongTensor] = None"},{name:"p_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": bool = False"}],parametersDescription:[{anchor:"transformers.modeling_utils.SQuADHead.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, hidden_size)</code>) &#x2014;
Final hidden states of the model on the sequence tokens.`,name:"hidden_states"},{anchor:"transformers.modeling_utils.SQuADHead.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Positions of the first token for the labeled span.`,name:"start_positions"},{anchor:"transformers.modeling_utils.SQuADHead.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Positions of the last token for the labeled span.`,name:"end_positions"},{anchor:"transformers.modeling_utils.SQuADHead.forward.cls_index",description:`<strong>cls_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Position of the CLS token for each sentence in the batch. If <code>None</code>, takes the last token.`,name:"cls_index"},{anchor:"transformers.modeling_utils.SQuADHead.forward.is_impossible",description:`<strong>is_impossible</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Whether the question has a possible answer in the paragraph or not.`,name:"is_impossible"},{anchor:"transformers.modeling_utils.SQuADHead.forward.p_mask",description:`<strong>p_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len)</code>, <em>optional</em>) &#x2014;
Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.`,name:"p_mask"},{anchor:"transformers.modeling_utils.SQuADHead.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2700",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput"
>transformers.modeling_utils.SquadHeadOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.configuration_utils.PretrainedConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned if both <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Classification loss as the sum of start token, end token (and is_impossible if provided) classification
losses.</li>
<li><strong>start_top_log_probs</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.start_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) \u2014 Log probabilities for the top config.start_n_top start token possibilities (beam-search).</li>
<li><strong>start_top_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, config.start_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) \u2014 Indices for the top config.start_n_top start token possibilities (beam-search).</li>
<li><strong>end_top_log_probs</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.start_n_top * config.end_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) \u2014 Log probabilities for the top <code>config.start_n_top * config.end_n_top</code> end token possibilities
(beam-search).</li>
<li><strong>end_top_index</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, config.start_n_top * config.end_n_top)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) \u2014 Indices for the top <code>config.start_n_top * config.end_n_top</code> end token possibilities (beam-search).</li>
<li><strong>cls_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>, returned if <code>start_positions</code> or <code>end_positions</code> is not provided) \u2014 Log probabilities for the <code>is_impossible</code> label of the answers.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.modeling_utils.SquadHeadOutput"
>transformers.modeling_utils.SquadHeadOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),rt=new x({props:{name:"class transformers.modeling_utils.SequenceSummary",anchor:"transformers.modeling_utils.SequenceSummary",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.modeling_utils.SequenceSummary.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model. Relevant arguments in the config class of the model are (refer to the actual
config class of your model for the default values it uses):</p>
<ul>
<li>
<p><strong>summary_type</strong> (<code>str</code>) &#x2014; The method to use to make this summary. Accepted values are:</p>
<ul>
<li><code>&quot;last&quot;</code> &#x2014; Take the last token hidden state (like XLNet)</li>
<li><code>&quot;first&quot;</code> &#x2014; Take the first token hidden state (like Bert)</li>
<li><code>&quot;mean&quot;</code> &#x2014; Take the mean of all tokens hidden states</li>
<li><code>&quot;cls_index&quot;</code> &#x2014; Supply a Tensor of classification token position (GPT/GPT-2)</li>
<li><code>&quot;attn&quot;</code> &#x2014; Not implemented now, use multi-head attention</li>
</ul>
</li>
<li>
<p><strong>summary_use_proj</strong> (<code>bool</code>) &#x2014; Add a projection after the vector extraction.</p>
</li>
<li>
<p><strong>summary_proj_to_labels</strong> (<code>bool</code>) &#x2014; If <code>True</code>, the projection outputs to <code>config.num_labels</code> classes
(otherwise to <code>config.hidden_size</code>).</p>
</li>
<li>
<p><strong>summary_activation</strong> (<code>Optional[str]</code>) &#x2014; Set to <code>&quot;tanh&quot;</code> to add a tanh activation to the output,
another string or <code>None</code> will add no activation.</p>
</li>
<li>
<p><strong>summary_first_dropout</strong> (<code>float</code>) &#x2014; Optional dropout probability before the projection and activation.</p>
</li>
<li>
<p><strong>summary_last_dropout</strong> (<code>float</code>)&#x2014; Optional dropout probability after the projection and activation.</p>
</li>
</ul>`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2798"}}),st=new x({props:{name:"forward",anchor:"transformers.modeling_utils.SequenceSummary.forward",parameters:[{name:"hidden_states",val:": FloatTensor"},{name:"cls_index",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_utils.SequenceSummary.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>[batch_size, seq_len, hidden_size]</code>) &#x2014;
The hidden states of the last layer.`,name:"hidden_states"},{anchor:"transformers.modeling_utils.SequenceSummary.forward.cls_index",description:`<strong>cls_index</strong> (<code>torch.LongTensor</code> of shape <code>[batch_size]</code> or <code>[batch_size, ...]</code> where &#x2026; are optional leading dimensions of <code>hidden_states</code>, <em>optional</em>) &#x2014;
Used if <code>summary_type == &quot;cls_index&quot;</code> and takes the last token of the sequence as classification token.`,name:"cls_index"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_utils.py#L2853",returnDescription:`
<p>The summary of the sequence hidden states.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),nt=new so({}),at=new x({props:{name:"transformers.apply_chunking_to_forward",anchor:"transformers.apply_chunking_to_forward",parameters:[{name:"forward_fn",val:": typing.Callable[..., torch.Tensor]"},{name:"chunk_size",val:": int"},{name:"chunk_dim",val:": int"},{name:"*input_tensors",val:""}],parametersDescription:[{anchor:"transformers.apply_chunking_to_forward.forward_fn",description:`<strong>forward_fn</strong> (<code>Callable[..., torch.Tensor]</code>) &#x2014;
The forward function of the model.`,name:"forward_fn"},{anchor:"transformers.apply_chunking_to_forward.chunk_size",description:`<strong>chunk_size</strong> (<code>int</code>) &#x2014;
The chunk size of a chunked tensor: <code>num_chunks = len(input_tensors[0]) / chunk_size</code>.`,name:"chunk_size"},{anchor:"transformers.apply_chunking_to_forward.chunk_dim",description:`<strong>chunk_dim</strong> (<code>int</code>) &#x2014;
The dimension over which the <code>input_tensors</code> should be chunked.`,name:"chunk_dim"},{anchor:"transformers.apply_chunking_to_forward.input_tensors",description:`<strong>input_tensors</strong> (<code>Tuple[torch.Tensor]</code>) &#x2014;
The input tensors of <code>forward_fn</code> which will be chunked`,name:"input_tensors"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L169",returnDescription:`
<p>A tensor with the same shape as the <code>forward_fn</code> would have given if applied\`.</p>
`,returnType:`
<p><code>torch.Tensor</code></p>
`}}),Ae=new Xi({props:{anchor:"transformers.apply_chunking_to_forward.example",$$slots:{default:[Zi]},$$scope:{ctx:P}}}),it=new x({props:{name:"transformers.pytorch_utils.find_pruneable_heads_and_indices",anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices",parameters:[{name:"heads",val:": typing.List[int]"},{name:"n_heads",val:": int"},{name:"head_size",val:": int"},{name:"already_pruned_heads",val:": typing.Set[int]"}],parametersDescription:[{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.heads",description:"<strong>heads</strong> (<code>List[int]</code>) &#x2014; List of the indices of heads to prune.",name:"heads"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.n_heads",description:"<strong>n_heads</strong> (<code>int</code>) &#x2014; The number of heads in the model.",name:"n_heads"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.head_size",description:"<strong>head_size</strong> (<code>int</code>) &#x2014; The size of each head.",name:"head_size"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.already_pruned_heads",description:"<strong>already_pruned_heads</strong> (<code>Set[int]</code>) &#x2014; A set of already pruned heads.",name:"already_pruned_heads"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L244",returnDescription:`
<p>A tuple with the remaining heads and their corresponding indices.</p>
`,returnType:`
<p><code>Tuple[Set[int], torch.LongTensor]</code></p>
`}}),dt=new x({props:{name:"transformers.prune_layer",anchor:"transformers.prune_layer",parameters:[{name:"layer",val:": typing.Union[torch.nn.modules.linear.Linear, transformers.pytorch_utils.Conv1D]"},{name:"index",val:": LongTensor"},{name:"dim",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.prune_layer.layer",description:"<strong>layer</strong> (<code>Union[torch.nn.Linear, Conv1D]</code>) &#x2014; The layer to prune.",name:"layer"},{anchor:"transformers.prune_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.prune_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L145",returnDescription:`
<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`
<p><code>torch.nn.Linear</code> or <a
  href="/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.Conv1D"
>Conv1D</a></p>
`}}),ct=new x({props:{name:"transformers.pytorch_utils.prune_conv1d_layer",anchor:"transformers.pytorch_utils.prune_conv1d_layer",parameters:[{name:"layer",val:": Conv1D"},{name:"index",val:": LongTensor"},{name:"dim",val:": int = 1"}],parametersDescription:[{anchor:"transformers.pytorch_utils.prune_conv1d_layer.layer",description:'<strong>layer</strong> (<a href="/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.Conv1D">Conv1D</a>) &#x2014; The layer to prune.',name:"layer"},{anchor:"transformers.pytorch_utils.prune_conv1d_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.pytorch_utils.prune_conv1d_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L112",returnDescription:`
<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.Conv1D"
>Conv1D</a></p>
`}}),pt=new x({props:{name:"transformers.pytorch_utils.prune_linear_layer",anchor:"transformers.pytorch_utils.prune_linear_layer",parameters:[{name:"layer",val:": Linear"},{name:"index",val:": LongTensor"},{name:"dim",val:": int = 0"}],parametersDescription:[{anchor:"transformers.pytorch_utils.prune_linear_layer.layer",description:"<strong>layer</strong> (<code>torch.nn.Linear</code>) &#x2014; The layer to prune.",name:"layer"},{anchor:"transformers.pytorch_utils.prune_linear_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.pytorch_utils.prune_linear_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/pytorch_utils.py#L52",returnDescription:`
<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`
<p><code>torch.nn.Linear</code></p>
`}}),mt=new so({}),ft=new x({props:{name:"class transformers.modeling_tf_utils.TFConv1D",anchor:"transformers.modeling_tf_utils.TFConv1D",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.modeling_tf_utils.TFConv1D.nf",description:`<strong>nf</strong> (<code>int</code>) &#x2014;
The number of output features.`,name:"nf"},{anchor:"transformers.modeling_tf_utils.TFConv1D.nx",description:`<strong>nx</strong> (<code>int</code>) &#x2014;
The number of input features.`,name:"nx"},{anchor:"transformers.modeling_tf_utils.TFConv1D.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation to use to initialize the weights.
kwargs &#x2014;
Additional keyword arguments passed along to the <code>__init__</code> of <code>tf.keras.layers.Layer</code>.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L1983"}}),ut=new x({props:{name:"class transformers.TFSharedEmbeddings",anchor:"transformers.TFSharedEmbeddings",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFSharedEmbeddings.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>) &#x2014;
The size of the vocabulary, e.g., the number of unique tokens.`,name:"vocab_size"},{anchor:"transformers.TFSharedEmbeddings.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>) &#x2014;
The size of the embedding vectors.`,name:"hidden_size"},{anchor:"transformers.TFSharedEmbeddings.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation to use when initializing the weights. If no value is provided, it will default to
{@html &quot;<span class="\\&quot;katex\\&quot;"><span class="\\&quot;katex-mathml\\&quot;"><math xmlns="\\&quot;http://www.w3.org/1998/Math/MathML\\&quot;"><semantics><mrow><mn>1</mn><mi mathvariant="\\&quot;normal\\&quot;">/</mi><msqrt><mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="\\&quot;normal\\&quot;">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msqrt></mrow><annotation encoding="\\&quot;application/x-tex\\&quot;">1/\\\\sqrt{hidden\\\\_size}</annotation></semantics></math></span><span class="\\&quot;katex-html\\&quot;" aria-hidden="\\&quot;true\\&quot;"><span class="\\&quot;base\\&quot;"><span class="\\&quot;strut\\&quot;" style="\\&quot;height:1.24em;vertical-align:-0.3628em;\\&quot;"></span><span class="\\&quot;mord\\&quot;">1/</span><span class="\\&quot;mord" sqrt\\"><span class="\\&quot;vlist-t" vlist-t2\\"><span class="\\&quot;vlist-r\\&quot;"><span class="\\&quot;vlist\\&quot;" style="\\&quot;height:0.8772em;\\&quot;"><span class="\\&quot;svg-align\\&quot;" style="\\&quot;top:-3.2em;\\&quot;"><span class="\\&quot;pstrut\\&quot;" style="\\&quot;height:3.2em;\\&quot;"></span><span class="\\&quot;mord\\&quot;" style="\\&quot;padding-left:1em;\\&quot;"><span class="\\&quot;mord" mathnormal\\">hi</span><span class="\\&quot;mord" mathnormal\\">dd</span><span class="\\&quot;mord" mathnormal\\">e</span><span class="\\&quot;mord" mathnormal\\">n</span><span class="\\&quot;mord\\&quot;" style="\\&quot;margin-right:0.02778em;\\&quot;">_</span><span class="\\&quot;mord" mathnormal\\">s</span><span class="\\&quot;mord" mathnormal\\">i</span><span class="\\&quot;mord" mathnormal\\">ze</span></span></span><span style="\\&quot;top:-2.8372em;\\&quot;"><span class="\\&quot;pstrut\\&quot;" style="\\&quot;height:3.2em;\\&quot;"></span><span class="\\&quot;hide-tail\\&quot;" style="\\&quot;min-width:1.02em;height:1.28em;\\&quot;"><svg xmlns="\\&quot;http://www.w3.org/2000/svg\\&quot;" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119\\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\\nc340,-704.7,510.7,-1060.3,512,-1067\\nl0 -0\\nc4.7,-7.3,11,-11,19,-11\\nH40000v40H1012.3\\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\\nM1001 80h400000v40h-400000z"/></svg></span></span></span><span class="\\&quot;vlist-s\\&quot;">&#x200B;</span></span><span class="\\&quot;vlist-r\\&quot;"><span class="\\&quot;vlist\\&quot;" style="\\&quot;height:0.3628em;\\&quot;"><span></span></span></span></span></span></span></span></span>&quot;}.
kwargs &#x2014;
Additional keyword arguments passed along to the <code>__init__</code> of <code>tf.keras.layers.Layer</code>.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L2023"}}),ht=new x({props:{name:"call",anchor:"transformers.TFSharedEmbeddings.call",parameters:[{name:"inputs",val:": Tensor"},{name:"mode",val:": str = 'embedding'"}],parametersDescription:[{anchor:"transformers.TFSharedEmbeddings.call.inputs",description:`<strong>inputs</strong> (<code>tf.Tensor</code>) &#x2014;
In embedding mode, should be an int64 tensor with shape <code>[batch_size, length]</code>.</p>
<p>In linear mode, should be a float tensor with shape <code>[batch_size, length, hidden_size]</code>.`,name:"inputs"},{anchor:"transformers.TFSharedEmbeddings.call.mode",description:`<strong>mode</strong> (<code>str</code>, defaults to <code>&quot;embedding&quot;</code>) &#x2014;
A valid value is either <code>&quot;embedding&quot;</code> or <code>&quot;linear&quot;</code>, the first one indicates that the layer should be
used as an embedding layer, the second one that the layer should be used as a linear decoder.`,name:"mode"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L2068",returnDescription:`
<p>In embedding mode, the output is a float32 embedding tensor, with shape <code>[batch_size, length, embedding_size]</code>.</p>
<p>In linear mode, the output is a float32 with shape <code>[batch_size, length, vocab_size]</code>.</p>
`,returnType:`
<p><code>tf.Tensor</code></p>
`}}),vt=new x({props:{name:"class transformers.TFSequenceSummary",anchor:"transformers.TFSequenceSummary",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFSequenceSummary.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The config used by the model. Relevant arguments in the config class of the model are (refer to the actual
config class of your model for the default values it uses):</p>
<ul>
<li>
<p><strong>summary_type</strong> (<code>str</code>) &#x2014; The method to use to make this summary. Accepted values are:</p>
<ul>
<li><code>&quot;last&quot;</code> &#x2014; Take the last token hidden state (like XLNet)</li>
<li><code>&quot;first&quot;</code> &#x2014; Take the first token hidden state (like Bert)</li>
<li><code>&quot;mean&quot;</code> &#x2014; Take the mean of all tokens hidden states</li>
<li><code>&quot;cls_index&quot;</code> &#x2014; Supply a Tensor of classification token position (GPT/GPT-2)</li>
<li><code>&quot;attn&quot;</code> &#x2014; Not implemented now, use multi-head attention</li>
</ul>
</li>
<li>
<p><strong>summary_use_proj</strong> (<code>bool</code>) &#x2014; Add a projection after the vector extraction.</p>
</li>
<li>
<p><strong>summary_proj_to_labels</strong> (<code>bool</code>) &#x2014; If <code>True</code>, the projection outputs to <code>config.num_labels</code> classes
(otherwise to <code>config.hidden_size</code>).</p>
</li>
<li>
<p><strong>summary_activation</strong> (<code>Optional[str]</code>) &#x2014; Set to <code>&quot;tanh&quot;</code> to add a tanh activation to the output,
another string or <code>None</code> will add no activation.</p>
</li>
<li>
<p><strong>summary_first_dropout</strong> (<code>float</code>) &#x2014; Optional dropout probability before the projection and activation.</p>
</li>
<li>
<p><strong>summary_last_dropout</strong> (<code>float</code>)&#x2014; Optional dropout probability after the projection and activation.</p>
</li>
</ul>`,name:"config"},{anchor:"transformers.TFSequenceSummary.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, defaults to 0.02) &#x2014; The standard deviation to use to initialize the weights.
kwargs &#x2014;
Additional keyword arguments passed along to the <code>__init__</code> of <code>tf.keras.layers.Layer</code>.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L2121"}}),bt=new x({props:{name:"register_for_auto_class",anchor:"transformers.TFSequenceSummary.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'TFAutoModel'"}],parametersDescription:[{anchor:"transformers.TFSequenceSummary.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;TFAutoModel&quot;</code>) &#x2014;
The auto class to register this new model with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L2236"}}),Ie=new ro({props:{warning:!0,$$slots:{default:[el]},$$scope:{ctx:P}}}),$t=new so({}),yt=new x({props:{name:"class transformers.modeling_tf_utils.TFCausalLanguageModelingLoss",anchor:"transformers.modeling_tf_utils.TFCausalLanguageModelingLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L171"}}),He=new ro({props:{$$slots:{default:[tl]},$$scope:{ctx:P}}}),wt=new x({props:{name:"class transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss",anchor:"transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L262"}}),Me=new ro({props:{$$slots:{default:[ol]},$$scope:{ctx:P}}}),Tt=new x({props:{name:"class transformers.modeling_tf_utils.TFMultipleChoiceLoss",anchor:"transformers.modeling_tf_utils.TFMultipleChoiceLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L252"}}),kt=new x({props:{name:"class transformers.modeling_tf_utils.TFQuestionAnsweringLoss",anchor:"transformers.modeling_tf_utils.TFQuestionAnsweringLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L193"}}),xt=new x({props:{name:"class transformers.modeling_tf_utils.TFSequenceClassificationLoss",anchor:"transformers.modeling_tf_utils.TFSequenceClassificationLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L236"}}),Et=new x({props:{name:"class transformers.modeling_tf_utils.TFTokenClassificationLoss",anchor:"transformers.modeling_tf_utils.TFTokenClassificationLoss",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L208"}}),Ve=new ro({props:{$$slots:{default:[rl]},$$scope:{ctx:P}}}),qt=new so({}),Dt=new x({props:{name:"transformers.modeling_tf_utils.get_initializer",anchor:"transformers.modeling_tf_utils.get_initializer",parameters:[{name:"initializer_range",val:": float = 0.02"}],parametersDescription:[{anchor:"transformers.modeling_tf_utils.get_initializer.initializer_range",description:"<strong>initializer_range</strong> (<em>float</em>, defaults to 0.02) &#x2014; Standard deviation of the initializer range.",name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L2263",returnDescription:`
<p>The truncated normal initializer.</p>
`,returnType:`
<p><code>tf.initializers.TruncatedNormal</code></p>
`}}),Pt=new x({props:{name:"transformers.modeling_tf_utils.keras_serializable",anchor:"transformers.modeling_tf_utils.keras_serializable",parameters:[],parametersDescription:[{anchor:"transformers.modeling_tf_utils.keras_serializable.cls",description:`<strong>cls</strong> (a <code>tf.keras.layers.Layers subclass</code>) &#x2014;
Typically a <code>TF.MainLayer</code> class in this project, in general must accept a <code>config</code> argument to its
initializer.`,name:"cls"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/modeling_tf_utils.py#L106",returnDescription:`
<p>The same class object, with modifications for Keras deserialization.</p>
`}}),Ct=new x({props:{name:"transformers.shape_list",anchor:"transformers.shape_list",parameters:[{name:"tensor",val:": typing.Union[tensorflow.python.framework.ops.Tensor, numpy.ndarray]"}],parametersDescription:[{anchor:"transformers.shape_list.tensor",description:"<strong>tensor</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code>) &#x2014; The tensor we want the shape of.",name:"tensor"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/tf_utils.py#L26",returnDescription:`
<p>The shape of the tensor as a list.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){p=r("meta"),k=l(),u=r("h1"),y=r("a"),q=r("span"),h(w.$$.fragment),D=l(),O=r("span"),C=a("Custom Layers and Utilities"),Q=l(),F=r("p"),z=a("This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling."),B=l(),A=r("p"),S=a("Most of those are only useful if you are studying the code of the models in the library."),G=l(),L=r("h2"),E=r("a"),T=r("span"),h(I.$$.fragment),me=l(),R=r("span"),fe=a("Pytorch custom modules"),ie=l(),U=r("div"),h(Re.$$.fragment),rs=l(),no=r("p"),ss=a("1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)."),ns=l(),ao=r("p"),as=a("Basically works like a linear layer but the weights are transposed."),pr=l(),K=r("div"),h(Ue.$$.fragment),is=l(),io=r("p"),ls=a("Compute SQuAD start logits from sequence hidden states."),ds=l(),Ft=r("div"),h(Ke.$$.fragment),mr=l(),W=r("div"),h(We.$$.fragment),cs=l(),lo=r("p"),ps=a("Compute SQuAD end logits from sequence hidden states."),ms=l(),Le=r("div"),h(Xe.$$.fragment),fs=l(),h(Pe.$$.fragment),fr=l(),X=r("div"),h(Je.$$.fragment),us=l(),co=r("p"),hs=a("Compute SQuAD 2.0 answer class from classification and start tokens hidden states."),gs=l(),Ce=r("div"),h(Ye.$$.fragment),_s=l(),h(ze.$$.fragment),ur=l(),ue=r("div"),h(Ze.$$.fragment),vs=l(),et=r("p"),bs=a("Base class for outputs of question answering models using a "),At=r("a"),$s=a("SQuADHead"),ys=a("."),hr=l(),J=r("div"),h(tt.$$.fragment),ws=l(),po=r("p"),Ts=a("A SQuAD head inspired by XLNet."),ks=l(),Ot=r("div"),h(ot.$$.fragment),gr=l(),Y=r("div"),h(rt.$$.fragment),xs=l(),mo=r("p"),Es=a("Compute a single vector summary of a sequence hidden states."),qs=l(),Se=r("div"),h(st.$$.fragment),Ds=l(),fo=r("p"),Ls=a("Compute a single vector summary of a sequence hidden states."),_r=l(),he=r("h2"),Fe=r("a"),uo=r("span"),h(nt.$$.fragment),Ps=l(),ho=r("span"),Cs=a("PyTorch Helper Functions"),vr=l(),N=r("div"),h(at.$$.fragment),zs=l(),H=r("p"),Ss=a("This function chunks the "),go=r("code"),Fs=a("input_tensors"),As=a(" into smaller input tensor parts of size "),_o=r("code"),Os=a("chunk_size"),Is=a(` over the dimension
`),vo=r("code"),Ns=a("chunk_dim"),Hs=a(". It then applies a layer "),bo=r("code"),Ms=a("forward_fn"),Vs=a(" to each chunk independently to save memory."),js=l(),M=r("p"),Qs=a("If the "),$o=r("code"),Bs=a("forward_fn"),Gs=a(" is independent across the "),yo=r("code"),Rs=a("chunk_dim"),Us=a(` this function will yield the same result as directly
applying `),wo=r("code"),Ks=a("forward_fn"),Ws=a(" to "),To=r("code"),Xs=a("input_tensors"),Js=a("."),Ys=l(),h(Ae.$$.fragment),br=l(),ge=r("div"),h(it.$$.fragment),Zs=l(),lt=r("p"),en=a("Finds the heads and their indices taking "),ko=r("code"),tn=a("already_pruned_heads"),on=a(" into account."),$r=l(),Z=r("div"),h(dt.$$.fragment),rn=l(),xo=r("p"),sn=a("Prune a Conv1D or linear layer to keep only entries in index."),nn=l(),Eo=r("p"),an=a("Used to remove heads."),yr=l(),ee=r("div"),h(ct.$$.fragment),ln=l(),qo=r("p"),dn=a(`Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights
are transposed.`),cn=l(),Do=r("p"),pn=a("Used to remove heads."),wr=l(),te=r("div"),h(pt.$$.fragment),mn=l(),Lo=r("p"),fn=a("Prune a linear layer to keep only entries in index."),un=l(),Po=r("p"),hn=a("Used to remove heads."),Tr=l(),_e=r("h2"),Oe=r("a"),Co=r("span"),h(mt.$$.fragment),gn=l(),zo=r("span"),_n=a("TensorFlow custom layers"),kr=l(),oe=r("div"),h(ft.$$.fragment),vn=l(),So=r("p"),bn=a("1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)."),$n=l(),Fo=r("p"),yn=a("Basically works like a linear layer but the weights are transposed."),xr=l(),V=r("div"),h(ut.$$.fragment),wn=l(),Ao=r("p"),Tn=a("Construct shared token embeddings."),kn=l(),Oo=r("p"),xn=a(`The weights of the embedding layer is usually shared with the weights of the linear decoder when doing language
modeling.`),En=l(),le=r("div"),h(ht.$$.fragment),qn=l(),Io=r("p"),Dn=a("Get token embeddings of inputs or decode final hidden state."),Ln=l(),gt=r("p"),Pn=a(`Shared weights logic is adapted from
`),_t=r("a"),Cn=a("here"),zn=a("."),Er=l(),re=r("div"),h(vt.$$.fragment),Sn=l(),No=r("p"),Fn=a("Compute a single vector summary of a sequence hidden states."),An=l(),de=r("div"),h(bt.$$.fragment),On=l(),Ho=r("p"),In=a(`Register this class with a given auto class. This should only be used for custom models as the ones in the
library are already mapped with an auto class.`),Nn=l(),h(Ie.$$.fragment),qr=l(),ve=r("h2"),Ne=r("a"),Mo=r("span"),h($t.$$.fragment),Hn=l(),Vo=r("span"),Mn=a("TensorFlow loss functions"),Dr=l(),se=r("div"),h(yt.$$.fragment),Vn=l(),jo=r("p"),jn=a("Loss function suitable for causal language modeling (CLM), that is, the task of guessing the next token."),Qn=l(),h(He.$$.fragment),Lr=l(),ne=r("div"),h(wt.$$.fragment),Bn=l(),Qo=r("p"),Gn=a("Loss function suitable for masked language modeling (MLM), that is, the task of guessing the masked tokens."),Rn=l(),h(Me.$$.fragment),Pr=l(),be=r("div"),h(Tt.$$.fragment),Un=l(),Bo=r("p"),Kn=a("Loss function suitable for multiple choice tasks."),Cr=l(),$e=r("div"),h(kt.$$.fragment),Wn=l(),Go=r("p"),Xn=a("Loss function suitable for question answering."),zr=l(),ye=r("div"),h(xt.$$.fragment),Jn=l(),Ro=r("p"),Yn=a("Loss function suitable for sequence classification."),Sr=l(),ae=r("div"),h(Et.$$.fragment),Zn=l(),Uo=r("p"),ea=a("Loss function suitable for token classification."),ta=l(),h(Ve.$$.fragment),Fr=l(),we=r("h2"),je=r("a"),Ko=r("span"),h(qt.$$.fragment),oa=l(),Wo=r("span"),ra=a("TensorFlow Helper Functions"),Ar=l(),Te=r("div"),h(Dt.$$.fragment),sa=l(),Lt=r("p"),na=a("Creates a "),Xo=r("code"),aa=a("tf.initializers.TruncatedNormal"),ia=a(" with the given range."),Or=l(),j=r("div"),h(Pt.$$.fragment),la=l(),Jo=r("p"),da=a("Decorate a Keras Layer class to support Keras serialization."),ca=l(),Yo=r("p"),pa=a("This is done by:"),ma=l(),ke=r("ol"),xe=r("li"),fa=a("Adding a "),Zo=r("code"),ua=a("transformers_config"),ha=a(" dict to the Keras config dictionary in "),er=r("code"),ga=a("get_config"),_a=a(` (called by Keras at
serialization time.`),va=l(),Ee=r("li"),ba=a("Wrapping "),tr=r("code"),$a=a("__init__"),ya=a(" to accept that "),or=r("code"),wa=a("transformers_config"),Ta=a(` dict (passed by Keras at deserialization time) and
convert it to a config object for the actual layer initializer.`),ka=l(),qe=r("li"),xa=a(`Registering the class as a custom object in Keras (if the Tensorflow version supports this), so that it does not
need to be supplied in `),rr=r("code"),Ea=a("custom_objects"),qa=a(" in the call to "),sr=r("code"),Da=a("tf.keras.models.load_model"),La=a("."),Ir=l(),De=r("div"),h(Ct.$$.fragment),Pa=l(),nr=r("p"),Ca=a("Deal with dynamic shape in tensorflow cleanly."),this.h()},l(e){const c=Ri('[data-svelte="svelte-1phssyn"]',document.head);p=s(c,"META",{name:!0,content:!0}),c.forEach(o),k=d(e),u=s(e,"H1",{class:!0});var zt=n(u);y=s(zt,"A",{id:!0,class:!0,href:!0});var ar=n(y);q=s(ar,"SPAN",{});var ir=n(q);g(w.$$.fragment,ir),ir.forEach(o),ar.forEach(o),D=d(zt),O=s(zt,"SPAN",{});var lr=n(O);C=i(lr,"Custom Layers and Utilities"),lr.forEach(o),zt.forEach(o),Q=d(e),F=s(e,"P",{});var dr=n(F);z=i(dr,"This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling."),dr.forEach(o),B=d(e),A=s(e,"P",{});var cr=n(A);S=i(cr,"Most of those are only useful if you are studying the code of the models in the library."),cr.forEach(o),G=d(e),L=s(e,"H2",{class:!0});var St=n(L);E=s(St,"A",{id:!0,class:!0,href:!0});var za=n(E);T=s(za,"SPAN",{});var Sa=n(T);g(I.$$.fragment,Sa),Sa.forEach(o),za.forEach(o),me=d(St),R=s(St,"SPAN",{});var Fa=n(R);fe=i(Fa,"Pytorch custom modules"),Fa.forEach(o),St.forEach(o),ie=d(e),U=s(e,"DIV",{class:!0});var It=n(U);g(Re.$$.fragment,It),rs=d(It),no=s(It,"P",{});var Aa=n(no);ss=i(Aa,"1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)."),Aa.forEach(o),ns=d(It),ao=s(It,"P",{});var Oa=n(ao);as=i(Oa,"Basically works like a linear layer but the weights are transposed."),Oa.forEach(o),It.forEach(o),pr=d(e),K=s(e,"DIV",{class:!0});var Nt=n(K);g(Ue.$$.fragment,Nt),is=d(Nt),io=s(Nt,"P",{});var Ia=n(io);ls=i(Ia,"Compute SQuAD start logits from sequence hidden states."),Ia.forEach(o),ds=d(Nt),Ft=s(Nt,"DIV",{class:!0});var Na=n(Ft);g(Ke.$$.fragment,Na),Na.forEach(o),Nt.forEach(o),mr=d(e),W=s(e,"DIV",{class:!0});var Ht=n(W);g(We.$$.fragment,Ht),cs=d(Ht),lo=s(Ht,"P",{});var Ha=n(lo);ps=i(Ha,"Compute SQuAD end logits from sequence hidden states."),Ha.forEach(o),ms=d(Ht),Le=s(Ht,"DIV",{class:!0});var Hr=n(Le);g(Xe.$$.fragment,Hr),fs=d(Hr),g(Pe.$$.fragment,Hr),Hr.forEach(o),Ht.forEach(o),fr=d(e),X=s(e,"DIV",{class:!0});var Mt=n(X);g(Je.$$.fragment,Mt),us=d(Mt),co=s(Mt,"P",{});var Ma=n(co);hs=i(Ma,"Compute SQuAD 2.0 answer class from classification and start tokens hidden states."),Ma.forEach(o),gs=d(Mt),Ce=s(Mt,"DIV",{class:!0});var Mr=n(Ce);g(Ye.$$.fragment,Mr),_s=d(Mr),g(ze.$$.fragment,Mr),Mr.forEach(o),Mt.forEach(o),ur=d(e),ue=s(e,"DIV",{class:!0});var Vr=n(ue);g(Ze.$$.fragment,Vr),vs=d(Vr),et=s(Vr,"P",{});var jr=n(et);bs=i(jr,"Base class for outputs of question answering models using a "),At=s(jr,"A",{href:!0});var Va=n(At);$s=i(Va,"SQuADHead"),Va.forEach(o),ys=i(jr,"."),jr.forEach(o),Vr.forEach(o),hr=d(e),J=s(e,"DIV",{class:!0});var Vt=n(J);g(tt.$$.fragment,Vt),ws=d(Vt),po=s(Vt,"P",{});var ja=n(po);Ts=i(ja,"A SQuAD head inspired by XLNet."),ja.forEach(o),ks=d(Vt),Ot=s(Vt,"DIV",{class:!0});var Qa=n(Ot);g(ot.$$.fragment,Qa),Qa.forEach(o),Vt.forEach(o),gr=d(e),Y=s(e,"DIV",{class:!0});var jt=n(Y);g(rt.$$.fragment,jt),xs=d(jt),mo=s(jt,"P",{});var Ba=n(mo);Es=i(Ba,"Compute a single vector summary of a sequence hidden states."),Ba.forEach(o),qs=d(jt),Se=s(jt,"DIV",{class:!0});var Qr=n(Se);g(st.$$.fragment,Qr),Ds=d(Qr),fo=s(Qr,"P",{});var Ga=n(fo);Ls=i(Ga,"Compute a single vector summary of a sequence hidden states."),Ga.forEach(o),Qr.forEach(o),jt.forEach(o),_r=d(e),he=s(e,"H2",{class:!0});var Br=n(he);Fe=s(Br,"A",{id:!0,class:!0,href:!0});var Ra=n(Fe);uo=s(Ra,"SPAN",{});var Ua=n(uo);g(nt.$$.fragment,Ua),Ua.forEach(o),Ra.forEach(o),Ps=d(Br),ho=s(Br,"SPAN",{});var Ka=n(ho);Cs=i(Ka,"PyTorch Helper Functions"),Ka.forEach(o),Br.forEach(o),vr=d(e),N=s(e,"DIV",{class:!0});var Qe=n(N);g(at.$$.fragment,Qe),zs=d(Qe),H=s(Qe,"P",{});var ce=n(H);Ss=i(ce,"This function chunks the "),go=s(ce,"CODE",{});var Wa=n(go);Fs=i(Wa,"input_tensors"),Wa.forEach(o),As=i(ce," into smaller input tensor parts of size "),_o=s(ce,"CODE",{});var Xa=n(_o);Os=i(Xa,"chunk_size"),Xa.forEach(o),Is=i(ce,` over the dimension
`),vo=s(ce,"CODE",{});var Ja=n(vo);Ns=i(Ja,"chunk_dim"),Ja.forEach(o),Hs=i(ce,". It then applies a layer "),bo=s(ce,"CODE",{});var Ya=n(bo);Ms=i(Ya,"forward_fn"),Ya.forEach(o),Vs=i(ce," to each chunk independently to save memory."),ce.forEach(o),js=d(Qe),M=s(Qe,"P",{});var pe=n(M);Qs=i(pe,"If the "),$o=s(pe,"CODE",{});var Za=n($o);Bs=i(Za,"forward_fn"),Za.forEach(o),Gs=i(pe," is independent across the "),yo=s(pe,"CODE",{});var ei=n(yo);Rs=i(ei,"chunk_dim"),ei.forEach(o),Us=i(pe,` this function will yield the same result as directly
applying `),wo=s(pe,"CODE",{});var ti=n(wo);Ks=i(ti,"forward_fn"),ti.forEach(o),Ws=i(pe," to "),To=s(pe,"CODE",{});var oi=n(To);Xs=i(oi,"input_tensors"),oi.forEach(o),Js=i(pe,"."),pe.forEach(o),Ys=d(Qe),g(Ae.$$.fragment,Qe),Qe.forEach(o),br=d(e),ge=s(e,"DIV",{class:!0});var Gr=n(ge);g(it.$$.fragment,Gr),Zs=d(Gr),lt=s(Gr,"P",{});var Rr=n(lt);en=i(Rr,"Finds the heads and their indices taking "),ko=s(Rr,"CODE",{});var ri=n(ko);tn=i(ri,"already_pruned_heads"),ri.forEach(o),on=i(Rr," into account."),Rr.forEach(o),Gr.forEach(o),$r=d(e),Z=s(e,"DIV",{class:!0});var Qt=n(Z);g(dt.$$.fragment,Qt),rn=d(Qt),xo=s(Qt,"P",{});var si=n(xo);sn=i(si,"Prune a Conv1D or linear layer to keep only entries in index."),si.forEach(o),nn=d(Qt),Eo=s(Qt,"P",{});var ni=n(Eo);an=i(ni,"Used to remove heads."),ni.forEach(o),Qt.forEach(o),yr=d(e),ee=s(e,"DIV",{class:!0});var Bt=n(ee);g(ct.$$.fragment,Bt),ln=d(Bt),qo=s(Bt,"P",{});var ai=n(qo);dn=i(ai,`Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights
are transposed.`),ai.forEach(o),cn=d(Bt),Do=s(Bt,"P",{});var ii=n(Do);pn=i(ii,"Used to remove heads."),ii.forEach(o),Bt.forEach(o),wr=d(e),te=s(e,"DIV",{class:!0});var Gt=n(te);g(pt.$$.fragment,Gt),mn=d(Gt),Lo=s(Gt,"P",{});var li=n(Lo);fn=i(li,"Prune a linear layer to keep only entries in index."),li.forEach(o),un=d(Gt),Po=s(Gt,"P",{});var di=n(Po);hn=i(di,"Used to remove heads."),di.forEach(o),Gt.forEach(o),Tr=d(e),_e=s(e,"H2",{class:!0});var Ur=n(_e);Oe=s(Ur,"A",{id:!0,class:!0,href:!0});var ci=n(Oe);Co=s(ci,"SPAN",{});var pi=n(Co);g(mt.$$.fragment,pi),pi.forEach(o),ci.forEach(o),gn=d(Ur),zo=s(Ur,"SPAN",{});var mi=n(zo);_n=i(mi,"TensorFlow custom layers"),mi.forEach(o),Ur.forEach(o),kr=d(e),oe=s(e,"DIV",{class:!0});var Rt=n(oe);g(ft.$$.fragment,Rt),vn=d(Rt),So=s(Rt,"P",{});var fi=n(So);bn=i(fi,"1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)."),fi.forEach(o),$n=d(Rt),Fo=s(Rt,"P",{});var ui=n(Fo);yn=i(ui,"Basically works like a linear layer but the weights are transposed."),ui.forEach(o),Rt.forEach(o),xr=d(e),V=s(e,"DIV",{class:!0});var Be=n(V);g(ut.$$.fragment,Be),wn=d(Be),Ao=s(Be,"P",{});var hi=n(Ao);Tn=i(hi,"Construct shared token embeddings."),hi.forEach(o),kn=d(Be),Oo=s(Be,"P",{});var gi=n(Oo);xn=i(gi,`The weights of the embedding layer is usually shared with the weights of the linear decoder when doing language
modeling.`),gi.forEach(o),En=d(Be),le=s(Be,"DIV",{class:!0});var Ut=n(le);g(ht.$$.fragment,Ut),qn=d(Ut),Io=s(Ut,"P",{});var _i=n(Io);Dn=i(_i,"Get token embeddings of inputs or decode final hidden state."),_i.forEach(o),Ln=d(Ut),gt=s(Ut,"P",{});var Kr=n(gt);Pn=i(Kr,`Shared weights logic is adapted from
`),_t=s(Kr,"A",{href:!0,rel:!0});var vi=n(_t);Cn=i(vi,"here"),vi.forEach(o),zn=i(Kr,"."),Kr.forEach(o),Ut.forEach(o),Be.forEach(o),Er=d(e),re=s(e,"DIV",{class:!0});var Kt=n(re);g(vt.$$.fragment,Kt),Sn=d(Kt),No=s(Kt,"P",{});var bi=n(No);Fn=i(bi,"Compute a single vector summary of a sequence hidden states."),bi.forEach(o),An=d(Kt),de=s(Kt,"DIV",{class:!0});var Wt=n(de);g(bt.$$.fragment,Wt),On=d(Wt),Ho=s(Wt,"P",{});var $i=n(Ho);In=i($i,`Register this class with a given auto class. This should only be used for custom models as the ones in the
library are already mapped with an auto class.`),$i.forEach(o),Nn=d(Wt),g(Ie.$$.fragment,Wt),Wt.forEach(o),Kt.forEach(o),qr=d(e),ve=s(e,"H2",{class:!0});var Wr=n(ve);Ne=s(Wr,"A",{id:!0,class:!0,href:!0});var yi=n(Ne);Mo=s(yi,"SPAN",{});var wi=n(Mo);g($t.$$.fragment,wi),wi.forEach(o),yi.forEach(o),Hn=d(Wr),Vo=s(Wr,"SPAN",{});var Ti=n(Vo);Mn=i(Ti,"TensorFlow loss functions"),Ti.forEach(o),Wr.forEach(o),Dr=d(e),se=s(e,"DIV",{class:!0});var Xt=n(se);g(yt.$$.fragment,Xt),Vn=d(Xt),jo=s(Xt,"P",{});var ki=n(jo);jn=i(ki,"Loss function suitable for causal language modeling (CLM), that is, the task of guessing the next token."),ki.forEach(o),Qn=d(Xt),g(He.$$.fragment,Xt),Xt.forEach(o),Lr=d(e),ne=s(e,"DIV",{class:!0});var Jt=n(ne);g(wt.$$.fragment,Jt),Bn=d(Jt),Qo=s(Jt,"P",{});var xi=n(Qo);Gn=i(xi,"Loss function suitable for masked language modeling (MLM), that is, the task of guessing the masked tokens."),xi.forEach(o),Rn=d(Jt),g(Me.$$.fragment,Jt),Jt.forEach(o),Pr=d(e),be=s(e,"DIV",{class:!0});var Xr=n(be);g(Tt.$$.fragment,Xr),Un=d(Xr),Bo=s(Xr,"P",{});var Ei=n(Bo);Kn=i(Ei,"Loss function suitable for multiple choice tasks."),Ei.forEach(o),Xr.forEach(o),Cr=d(e),$e=s(e,"DIV",{class:!0});var Jr=n($e);g(kt.$$.fragment,Jr),Wn=d(Jr),Go=s(Jr,"P",{});var qi=n(Go);Xn=i(qi,"Loss function suitable for question answering."),qi.forEach(o),Jr.forEach(o),zr=d(e),ye=s(e,"DIV",{class:!0});var Yr=n(ye);g(xt.$$.fragment,Yr),Jn=d(Yr),Ro=s(Yr,"P",{});var Di=n(Ro);Yn=i(Di,"Loss function suitable for sequence classification."),Di.forEach(o),Yr.forEach(o),Sr=d(e),ae=s(e,"DIV",{class:!0});var Yt=n(ae);g(Et.$$.fragment,Yt),Zn=d(Yt),Uo=s(Yt,"P",{});var Li=n(Uo);ea=i(Li,"Loss function suitable for token classification."),Li.forEach(o),ta=d(Yt),g(Ve.$$.fragment,Yt),Yt.forEach(o),Fr=d(e),we=s(e,"H2",{class:!0});var Zr=n(we);je=s(Zr,"A",{id:!0,class:!0,href:!0});var Pi=n(je);Ko=s(Pi,"SPAN",{});var Ci=n(Ko);g(qt.$$.fragment,Ci),Ci.forEach(o),Pi.forEach(o),oa=d(Zr),Wo=s(Zr,"SPAN",{});var zi=n(Wo);ra=i(zi,"TensorFlow Helper Functions"),zi.forEach(o),Zr.forEach(o),Ar=d(e),Te=s(e,"DIV",{class:!0});var es=n(Te);g(Dt.$$.fragment,es),sa=d(es),Lt=s(es,"P",{});var ts=n(Lt);na=i(ts,"Creates a "),Xo=s(ts,"CODE",{});var Si=n(Xo);aa=i(Si,"tf.initializers.TruncatedNormal"),Si.forEach(o),ia=i(ts," with the given range."),ts.forEach(o),es.forEach(o),Or=d(e),j=s(e,"DIV",{class:!0});var Ge=n(j);g(Pt.$$.fragment,Ge),la=d(Ge),Jo=s(Ge,"P",{});var Fi=n(Jo);da=i(Fi,"Decorate a Keras Layer class to support Keras serialization."),Fi.forEach(o),ca=d(Ge),Yo=s(Ge,"P",{});var Ai=n(Yo);pa=i(Ai,"This is done by:"),Ai.forEach(o),ma=d(Ge),ke=s(Ge,"OL",{});var Zt=n(ke);xe=s(Zt,"LI",{});var eo=n(xe);fa=i(eo,"Adding a "),Zo=s(eo,"CODE",{});var Oi=n(Zo);ua=i(Oi,"transformers_config"),Oi.forEach(o),ha=i(eo," dict to the Keras config dictionary in "),er=s(eo,"CODE",{});var Ii=n(er);ga=i(Ii,"get_config"),Ii.forEach(o),_a=i(eo,` (called by Keras at
serialization time.`),eo.forEach(o),va=d(Zt),Ee=s(Zt,"LI",{});var to=n(Ee);ba=i(to,"Wrapping "),tr=s(to,"CODE",{});var Ni=n(tr);$a=i(Ni,"__init__"),Ni.forEach(o),ya=i(to," to accept that "),or=s(to,"CODE",{});var Hi=n(or);wa=i(Hi,"transformers_config"),Hi.forEach(o),Ta=i(to,` dict (passed by Keras at deserialization time) and
convert it to a config object for the actual layer initializer.`),to.forEach(o),ka=d(Zt),qe=s(Zt,"LI",{});var oo=n(qe);xa=i(oo,`Registering the class as a custom object in Keras (if the Tensorflow version supports this), so that it does not
need to be supplied in `),rr=s(oo,"CODE",{});var Mi=n(rr);Ea=i(Mi,"custom_objects"),Mi.forEach(o),qa=i(oo," in the call to "),sr=s(oo,"CODE",{});var Vi=n(sr);Da=i(Vi,"tf.keras.models.load_model"),Vi.forEach(o),La=i(oo,"."),oo.forEach(o),Zt.forEach(o),Ge.forEach(o),Ir=d(e),De=s(e,"DIV",{class:!0});var os=n(De);g(Ct.$$.fragment,os),Pa=d(os),nr=s(os,"P",{});var ji=n(nr);Ca=i(ji,"Deal with dynamic shape in tensorflow cleanly."),ji.forEach(o),os.forEach(o),this.h()},h(){f(p,"name","hf:doc:metadata"),f(p,"content",JSON.stringify(nl)),f(y,"id","custom-layers-and-utilities"),f(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(y,"href","#custom-layers-and-utilities"),f(u,"class","relative group"),f(E,"id","transformers.Conv1D"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#transformers.Conv1D"),f(L,"class","relative group"),f(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(At,"href","/docs/transformers/pr_17196/en/internal/modeling_utils#transformers.modeling_utils.SQuADHead"),f(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Fe,"id","transformers.apply_chunking_to_forward"),f(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Fe,"href","#transformers.apply_chunking_to_forward"),f(he,"class","relative group"),f(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Oe,"id","transformers.modeling_tf_utils.TFConv1D"),f(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Oe,"href","#transformers.modeling_tf_utils.TFConv1D"),f(_e,"class","relative group"),f(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(_t,"href","https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24"),f(_t,"rel","nofollow"),f(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Ne,"id","transformers.modeling_tf_utils.TFCausalLanguageModelingLoss"),f(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ne,"href","#transformers.modeling_tf_utils.TFCausalLanguageModelingLoss"),f(ve,"class","relative group"),f(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(je,"id","transformers.modeling_tf_utils.get_initializer"),f(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(je,"href","#transformers.modeling_tf_utils.get_initializer"),f(we,"class","relative group"),f(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,p),m(e,k,c),m(e,u,c),t(u,y),t(y,q),_(w,q,null),t(u,D),t(u,O),t(O,C),m(e,Q,c),m(e,F,c),t(F,z),m(e,B,c),m(e,A,c),t(A,S),m(e,G,c),m(e,L,c),t(L,E),t(E,T),_(I,T,null),t(L,me),t(L,R),t(R,fe),m(e,ie,c),m(e,U,c),_(Re,U,null),t(U,rs),t(U,no),t(no,ss),t(U,ns),t(U,ao),t(ao,as),m(e,pr,c),m(e,K,c),_(Ue,K,null),t(K,is),t(K,io),t(io,ls),t(K,ds),t(K,Ft),_(Ke,Ft,null),m(e,mr,c),m(e,W,c),_(We,W,null),t(W,cs),t(W,lo),t(lo,ps),t(W,ms),t(W,Le),_(Xe,Le,null),t(Le,fs),_(Pe,Le,null),m(e,fr,c),m(e,X,c),_(Je,X,null),t(X,us),t(X,co),t(co,hs),t(X,gs),t(X,Ce),_(Ye,Ce,null),t(Ce,_s),_(ze,Ce,null),m(e,ur,c),m(e,ue,c),_(Ze,ue,null),t(ue,vs),t(ue,et),t(et,bs),t(et,At),t(At,$s),t(et,ys),m(e,hr,c),m(e,J,c),_(tt,J,null),t(J,ws),t(J,po),t(po,Ts),t(J,ks),t(J,Ot),_(ot,Ot,null),m(e,gr,c),m(e,Y,c),_(rt,Y,null),t(Y,xs),t(Y,mo),t(mo,Es),t(Y,qs),t(Y,Se),_(st,Se,null),t(Se,Ds),t(Se,fo),t(fo,Ls),m(e,_r,c),m(e,he,c),t(he,Fe),t(Fe,uo),_(nt,uo,null),t(he,Ps),t(he,ho),t(ho,Cs),m(e,vr,c),m(e,N,c),_(at,N,null),t(N,zs),t(N,H),t(H,Ss),t(H,go),t(go,Fs),t(H,As),t(H,_o),t(_o,Os),t(H,Is),t(H,vo),t(vo,Ns),t(H,Hs),t(H,bo),t(bo,Ms),t(H,Vs),t(N,js),t(N,M),t(M,Qs),t(M,$o),t($o,Bs),t(M,Gs),t(M,yo),t(yo,Rs),t(M,Us),t(M,wo),t(wo,Ks),t(M,Ws),t(M,To),t(To,Xs),t(M,Js),t(N,Ys),_(Ae,N,null),m(e,br,c),m(e,ge,c),_(it,ge,null),t(ge,Zs),t(ge,lt),t(lt,en),t(lt,ko),t(ko,tn),t(lt,on),m(e,$r,c),m(e,Z,c),_(dt,Z,null),t(Z,rn),t(Z,xo),t(xo,sn),t(Z,nn),t(Z,Eo),t(Eo,an),m(e,yr,c),m(e,ee,c),_(ct,ee,null),t(ee,ln),t(ee,qo),t(qo,dn),t(ee,cn),t(ee,Do),t(Do,pn),m(e,wr,c),m(e,te,c),_(pt,te,null),t(te,mn),t(te,Lo),t(Lo,fn),t(te,un),t(te,Po),t(Po,hn),m(e,Tr,c),m(e,_e,c),t(_e,Oe),t(Oe,Co),_(mt,Co,null),t(_e,gn),t(_e,zo),t(zo,_n),m(e,kr,c),m(e,oe,c),_(ft,oe,null),t(oe,vn),t(oe,So),t(So,bn),t(oe,$n),t(oe,Fo),t(Fo,yn),m(e,xr,c),m(e,V,c),_(ut,V,null),t(V,wn),t(V,Ao),t(Ao,Tn),t(V,kn),t(V,Oo),t(Oo,xn),t(V,En),t(V,le),_(ht,le,null),t(le,qn),t(le,Io),t(Io,Dn),t(le,Ln),t(le,gt),t(gt,Pn),t(gt,_t),t(_t,Cn),t(gt,zn),m(e,Er,c),m(e,re,c),_(vt,re,null),t(re,Sn),t(re,No),t(No,Fn),t(re,An),t(re,de),_(bt,de,null),t(de,On),t(de,Ho),t(Ho,In),t(de,Nn),_(Ie,de,null),m(e,qr,c),m(e,ve,c),t(ve,Ne),t(Ne,Mo),_($t,Mo,null),t(ve,Hn),t(ve,Vo),t(Vo,Mn),m(e,Dr,c),m(e,se,c),_(yt,se,null),t(se,Vn),t(se,jo),t(jo,jn),t(se,Qn),_(He,se,null),m(e,Lr,c),m(e,ne,c),_(wt,ne,null),t(ne,Bn),t(ne,Qo),t(Qo,Gn),t(ne,Rn),_(Me,ne,null),m(e,Pr,c),m(e,be,c),_(Tt,be,null),t(be,Un),t(be,Bo),t(Bo,Kn),m(e,Cr,c),m(e,$e,c),_(kt,$e,null),t($e,Wn),t($e,Go),t(Go,Xn),m(e,zr,c),m(e,ye,c),_(xt,ye,null),t(ye,Jn),t(ye,Ro),t(Ro,Yn),m(e,Sr,c),m(e,ae,c),_(Et,ae,null),t(ae,Zn),t(ae,Uo),t(Uo,ea),t(ae,ta),_(Ve,ae,null),m(e,Fr,c),m(e,we,c),t(we,je),t(je,Ko),_(qt,Ko,null),t(we,oa),t(we,Wo),t(Wo,ra),m(e,Ar,c),m(e,Te,c),_(Dt,Te,null),t(Te,sa),t(Te,Lt),t(Lt,na),t(Lt,Xo),t(Xo,aa),t(Lt,ia),m(e,Or,c),m(e,j,c),_(Pt,j,null),t(j,la),t(j,Jo),t(Jo,da),t(j,ca),t(j,Yo),t(Yo,pa),t(j,ma),t(j,ke),t(ke,xe),t(xe,fa),t(xe,Zo),t(Zo,ua),t(xe,ha),t(xe,er),t(er,ga),t(xe,_a),t(ke,va),t(ke,Ee),t(Ee,ba),t(Ee,tr),t(tr,$a),t(Ee,ya),t(Ee,or),t(or,wa),t(Ee,Ta),t(ke,ka),t(ke,qe),t(qe,xa),t(qe,rr),t(rr,Ea),t(qe,qa),t(qe,sr),t(sr,Da),t(qe,La),m(e,Ir,c),m(e,De,c),_(Ct,De,null),t(De,Pa),t(De,nr),t(nr,Ca),Nr=!0},p(e,[c]){const zt={};c&2&&(zt.$$scope={dirty:c,ctx:e}),Pe.$set(zt);const ar={};c&2&&(ar.$$scope={dirty:c,ctx:e}),ze.$set(ar);const ir={};c&2&&(ir.$$scope={dirty:c,ctx:e}),Ae.$set(ir);const lr={};c&2&&(lr.$$scope={dirty:c,ctx:e}),Ie.$set(lr);const dr={};c&2&&(dr.$$scope={dirty:c,ctx:e}),He.$set(dr);const cr={};c&2&&(cr.$$scope={dirty:c,ctx:e}),Me.$set(cr);const St={};c&2&&(St.$$scope={dirty:c,ctx:e}),Ve.$set(St)},i(e){Nr||(v(w.$$.fragment,e),v(I.$$.fragment,e),v(Re.$$.fragment,e),v(Ue.$$.fragment,e),v(Ke.$$.fragment,e),v(We.$$.fragment,e),v(Xe.$$.fragment,e),v(Pe.$$.fragment,e),v(Je.$$.fragment,e),v(Ye.$$.fragment,e),v(ze.$$.fragment,e),v(Ze.$$.fragment,e),v(tt.$$.fragment,e),v(ot.$$.fragment,e),v(rt.$$.fragment,e),v(st.$$.fragment,e),v(nt.$$.fragment,e),v(at.$$.fragment,e),v(Ae.$$.fragment,e),v(it.$$.fragment,e),v(dt.$$.fragment,e),v(ct.$$.fragment,e),v(pt.$$.fragment,e),v(mt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(ht.$$.fragment,e),v(vt.$$.fragment,e),v(bt.$$.fragment,e),v(Ie.$$.fragment,e),v($t.$$.fragment,e),v(yt.$$.fragment,e),v(He.$$.fragment,e),v(wt.$$.fragment,e),v(Me.$$.fragment,e),v(Tt.$$.fragment,e),v(kt.$$.fragment,e),v(xt.$$.fragment,e),v(Et.$$.fragment,e),v(Ve.$$.fragment,e),v(qt.$$.fragment,e),v(Dt.$$.fragment,e),v(Pt.$$.fragment,e),v(Ct.$$.fragment,e),Nr=!0)},o(e){b(w.$$.fragment,e),b(I.$$.fragment,e),b(Re.$$.fragment,e),b(Ue.$$.fragment,e),b(Ke.$$.fragment,e),b(We.$$.fragment,e),b(Xe.$$.fragment,e),b(Pe.$$.fragment,e),b(Je.$$.fragment,e),b(Ye.$$.fragment,e),b(ze.$$.fragment,e),b(Ze.$$.fragment,e),b(tt.$$.fragment,e),b(ot.$$.fragment,e),b(rt.$$.fragment,e),b(st.$$.fragment,e),b(nt.$$.fragment,e),b(at.$$.fragment,e),b(Ae.$$.fragment,e),b(it.$$.fragment,e),b(dt.$$.fragment,e),b(ct.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ft.$$.fragment,e),b(ut.$$.fragment,e),b(ht.$$.fragment,e),b(vt.$$.fragment,e),b(bt.$$.fragment,e),b(Ie.$$.fragment,e),b($t.$$.fragment,e),b(yt.$$.fragment,e),b(He.$$.fragment,e),b(wt.$$.fragment,e),b(Me.$$.fragment,e),b(Tt.$$.fragment,e),b(kt.$$.fragment,e),b(xt.$$.fragment,e),b(Et.$$.fragment,e),b(Ve.$$.fragment,e),b(qt.$$.fragment,e),b(Dt.$$.fragment,e),b(Pt.$$.fragment,e),b(Ct.$$.fragment,e),Nr=!1},d(e){o(p),e&&o(k),e&&o(u),$(w),e&&o(Q),e&&o(F),e&&o(B),e&&o(A),e&&o(G),e&&o(L),$(I),e&&o(ie),e&&o(U),$(Re),e&&o(pr),e&&o(K),$(Ue),$(Ke),e&&o(mr),e&&o(W),$(We),$(Xe),$(Pe),e&&o(fr),e&&o(X),$(Je),$(Ye),$(ze),e&&o(ur),e&&o(ue),$(Ze),e&&o(hr),e&&o(J),$(tt),$(ot),e&&o(gr),e&&o(Y),$(rt),$(st),e&&o(_r),e&&o(he),$(nt),e&&o(vr),e&&o(N),$(at),$(Ae),e&&o(br),e&&o(ge),$(it),e&&o($r),e&&o(Z),$(dt),e&&o(yr),e&&o(ee),$(ct),e&&o(wr),e&&o(te),$(pt),e&&o(Tr),e&&o(_e),$(mt),e&&o(kr),e&&o(oe),$(ft),e&&o(xr),e&&o(V),$(ut),$(ht),e&&o(Er),e&&o(re),$(vt),$(bt),$(Ie),e&&o(qr),e&&o(ve),$($t),e&&o(Dr),e&&o(se),$(yt),$(He),e&&o(Lr),e&&o(ne),$(wt),$(Me),e&&o(Pr),e&&o(be),$(Tt),e&&o(Cr),e&&o($e),$(kt),e&&o(zr),e&&o(ye),$(xt),e&&o(Sr),e&&o(ae),$(Et),$(Ve),e&&o(Fr),e&&o(we),$(qt),e&&o(Ar),e&&o(Te),$(Dt),e&&o(Or),e&&o(j),$(Pt),e&&o(Ir),e&&o(De),$(Ct)}}}const nl={local:"custom-layers-and-utilities",sections:[{local:"transformers.Conv1D",title:"Pytorch custom modules"},{local:"transformers.apply_chunking_to_forward",title:"PyTorch Helper Functions"},{local:"transformers.modeling_tf_utils.TFConv1D",title:"TensorFlow custom layers"},{local:"transformers.modeling_tf_utils.TFCausalLanguageModelingLoss",title:"TensorFlow loss functions"},{local:"transformers.modeling_tf_utils.get_initializer",title:"TensorFlow Helper Functions"}],title:"Custom Layers and Utilities"};function al(P){return Ui(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fl extends Qi{constructor(p){super();Bi(this,p,al,sl,Gi,{})}}export{fl as default,nl as metadata};
