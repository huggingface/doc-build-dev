import{S as qu,i as Eu,s as ju,e as o,k as h,w as y,t,Y as M,M as Lu,c as r,d as a,m as d,a as i,x as k,h as n,Z as R,b as c,F as s,g as u,y as w,q as x,o as z,B as $,v as Fu,L as ps}from"../../chunks/vendor-6b77c823.js";import{T as mo}from"../../chunks/Tip-39098574.js";import{D as X}from"../../chunks/Docstring-1088f2fb.js";import{C as rs}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as V}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as ls}from"../../chunks/ExampleCodeBlock-5212b321.js";function Au(T){let p,v,g,f,b;return f=new rs({props:{code:`from transformers import ReformerModel, ReformerConfig

# Initializing a Reformer configuration
configuration = ReformerConfig()

# Initializing a Reformer model
model = ReformerModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerModel, ReformerConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Reformer configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ReformerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Reformer model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=o("p"),v=t("Examples:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Examples:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Cu(T){let p,v,g,f,b;return{c(){p=o("p"),v=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),f=t("Module"),b=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var L=i(g);f=n(L,"Module"),L.forEach(a),b=n(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(l,_){u(l,p,_),s(p,v),s(p,g),s(g,f),s(p,b)},d(l){l&&a(p)}}}function Pu(T){let p,v,g,f,b;return f=new rs({props:{code:`from transformers import ReformerTokenizer, ReformerModel
import torch

tokenizer = ReformerTokenizer.from_pretrained("google/reformer-crime-and-punishment")
model = ReformerModel.from_pretrained("google/reformer-crime-and-punishment")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;google/reformer-crime-and-punishment&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerModel.from_pretrained(<span class="hljs-string">&quot;google/reformer-crime-and-punishment&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){p=o("p"),v=t("Example:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Su(T){let p,v,g,f,b;return{c(){p=o("p"),v=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),f=t("Module"),b=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var L=i(g);f=n(L,"Module"),L.forEach(a),b=n(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(l,_){u(l,p,_),s(p,v),s(p,g),s(g,f),s(p,b)},d(l){l&&a(p)}}}function Ou(T){let p,v,g,f,b;return f=new rs({props:{code:`import torch
from transformers import ReformerTokenizer, ReformerModelWithLMHead

tokenizer = ReformerTokenizer.from_pretrained("google/reformer-crime-and-punishment")
model = ReformerModelWithLMHead.from_pretrained("google/reformer-crime-and-punishment")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerModelWithLMHead

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;google/reformer-crime-and-punishment&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerModelWithLMHead.from_pretrained(<span class="hljs-string">&quot;google/reformer-crime-and-punishment&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){p=o("p"),v=t("Example:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Nu(T){let p,v,g,f,b;return{c(){p=o("p"),v=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),f=t("Module"),b=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var L=i(g);f=n(L,"Module"),L.forEach(a),b=n(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(l,_){u(l,p,_),s(p,v),s(p,g),s(g,f),s(p,b)},d(l){l&&a(p)}}}function Du(T){let p,v,g,f,b;return f=new rs({props:{code:`import torch
from transformers import ReformerTokenizer, ReformerForMaskedLM

tokenizer = ReformerTokenizer.from_pretrained("hf-internal-testing/tiny-random-reformer")
model = ReformerForMaskedLM.from_pretrained("hf-internal-testing/tiny-random-reformer")

# add mask_token
tokenizer.add_special_tokens({"mask_token": "[MASK]"})
inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForMaskedLM.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add mask_token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.add_special_tokens({<span class="hljs-string">&quot;mask_token&quot;</span>: <span class="hljs-string">&quot;[MASK]&quot;</span>})
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
<span class="hljs-string">&#x27;it&#x27;</span>`}}),{c(){p=o("p"),v=t("Example:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Hu(T){let p,v;return p=new rs({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-[MASK] tokens
labels = torch.where(
    inputs.input_ids == tokenizer.mask_token_id, labels[:, : inputs["input_ids"].shape[-1]], -100
)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(
<span class="hljs-meta">... </span>    inputs.input_ids == tokenizer.mask_token_id, labels[:, : inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape[-<span class="hljs-number">1</span>]], -<span class="hljs-number">100</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">7.09</span>`}}),{c(){y(p.$$.fragment)},l(g){k(p.$$.fragment,g)},m(g,f){w(p,g,f),v=!0},p:ps,i(g){v||(x(p.$$.fragment,g),v=!0)},o(g){z(p.$$.fragment,g),v=!1},d(g){$(p,g)}}}function Iu(T){let p,v,g,f,b;return{c(){p=o("p"),v=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),f=t("Module"),b=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var L=i(g);f=n(L,"Module"),L.forEach(a),b=n(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(l,_){u(l,p,_),s(p,v),s(p,g),s(g,f),s(p,b)},d(l){l&&a(p)}}}function Wu(T){let p,v,g,f,b;return f=new rs({props:{code:`import torch
from transformers import ReformerTokenizer, ReformerForSequenceClassification

tokenizer = ReformerTokenizer.from_pretrained("hf-internal-testing/tiny-random-reformer")
model = ReformerForSequenceClassification.from_pretrained("hf-internal-testing/tiny-random-reformer")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;LABEL_1&#x27;</span>`}}),{c(){p=o("p"),v=t("Example of single-label classification:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example of single-label classification:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Qu(T){let p,v;return p=new rs({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = ReformerForSequenceClassification.from_pretrained(
    "hf-internal-testing/tiny-random-reformer", num_labels=num_labels
)

labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>, num_labels=num_labels
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.69</span>`}}),{c(){y(p.$$.fragment)},l(g){k(p.$$.fragment,g)},m(g,f){w(p,g,f),v=!0},p:ps,i(g){v||(x(p.$$.fragment,g),v=!0)},o(g){z(p.$$.fragment,g),v=!1},d(g){$(p,g)}}}function Ku(T){let p,v,g,f,b;return f=new rs({props:{code:`import torch
from transformers import ReformerTokenizer, ReformerForSequenceClassification

tokenizer = ReformerTokenizer.from_pretrained("hf-internal-testing/tiny-random-reformer")
model = ReformerForSequenceClassification.from_pretrained(
    "hf-internal-testing/tiny-random-reformer", problem_type="multi_label_classification"
)

# add pad_token
tokenizer.add_special_tokens({"pad_token": "[PAD]"})
inputs = tokenizer("Hello, my dog is cute", max_length=100, padding="max_length", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add pad_token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.add_special_tokens({<span class="hljs-string">&quot;pad_token&quot;</span>: <span class="hljs-string">&quot;[PAD]&quot;</span>})
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, max_length=<span class="hljs-number">100</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;LABEL_1&#x27;</span>`}}),{c(){p=o("p"),v=t("Example of multi-label classification:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example of multi-label classification:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Uu(T){let p,v;return p=new rs({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = ReformerForSequenceClassification.from_pretrained(
    "hf-internal-testing/tiny-random-reformer", num_labels=num_labels
)
model.train()
num_labels = len(model.config.id2label)
labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>, num_labels=num_labels
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),{c(){y(p.$$.fragment)},l(g){k(p.$$.fragment,g)},m(g,f){w(p,g,f),v=!0},p:ps,i(g){v||(x(p.$$.fragment,g),v=!0)},o(g){z(p.$$.fragment,g),v=!1},d(g){$(p,g)}}}function Xu(T){let p,v,g,f,b;return{c(){p=o("p"),v=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),f=t("Module"),b=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var L=i(g);f=n(L,"Module"),L.forEach(a),b=n(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(l,_){u(l,p,_),s(p,v),s(p,g),s(g,f),s(p,b)},d(l){l&&a(p)}}}function Vu(T){let p,v,g,f,b;return f=new rs({props:{code:`from transformers import ReformerTokenizer, ReformerForQuestionAnswering
import torch

tokenizer = ReformerTokenizer.from_pretrained("hf-internal-testing/tiny-random-reformer")
model = ReformerForQuestionAnswering.from_pretrained("hf-internal-testing/tiny-random-reformer")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ReformerTokenizer, ReformerForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ReformerTokenizer.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ReformerForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-reformer&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
<span class="hljs-string">&#x27;&#x27;</span>`}}),{c(){p=o("p"),v=t("Example:"),g=h(),y(f.$$.fragment)},l(l){p=r(l,"P",{});var _=i(p);v=n(_,"Example:"),_.forEach(a),g=d(l),k(f.$$.fragment,l)},m(l,_){u(l,p,_),s(p,v),u(l,g,_),w(f,l,_),b=!0},p:ps,i(l){b||(x(f.$$.fragment,l),b=!0)},o(l){z(f.$$.fragment,l),b=!1},d(l){l&&a(p),l&&a(g),$(f,l)}}}function Bu(T){let p,v;return p=new rs({props:{code:`# target is "nice puppet"
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">3.28</span>`}}),{c(){y(p.$$.fragment)},l(g){k(p.$$.fragment,g)},m(g,f){w(p,g,f),v=!0},p:ps,i(g){v||(x(p.$$.fragment,g),v=!0)},o(g){z(p.$$.fragment,g),v=!1},d(g){$(p,g)}}}function Gu(T){let p,v,g,f,b,l,_,L,Ai,co,ws,St,Ci,Pi,Ae,Si,Oi,ho,xs,Vs,Ot,Ce,Ni,Nt,Di,go,Bs,Hi,Pe,Ii,Wi,uo,st,Qi,fo,et,Dt,Ki,_o,ms,Ui,Se,Xi,Vi,Oe,Bi,Gi,vo,Ne,Ht,Ji,Yi,bo,at,cs,Zi,It,sl,el,Wt,al,tl,De,nl,yo,zs,Gs,Qt,He,ol,Kt,rl,ko,q,il,Ie,ll,pl,wo,su='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>',xo,Ut,ml,cl,zo,eu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">i, \\ldots, n_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',$o,To,au='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">n_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Mo,Xt,hl,dl,Ro,tu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msup><mn>2</mn><mn>19</mn></msup><mo>\u2248</mo><mn>0.5</mn><mi>M</mi></mrow><annotation encoding="application/x-tex">n_s = 2^{19} \\approx 0.5M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2248</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">0.5</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>',qo,Vt,gl,ul,Eo,nu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup><mo>\u2248</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">d = 2^{10} \\approx 1000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2248</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1000</span></span></span></span>',jo,Lo,ou='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo><mtext>\xA0with\xA0</mtext><mi>i</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><mi>d</mi><mo fence="true">]</mo></mrow><mtext>\xA0and\xA0</mtext><mi>j</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msub><mi>n</mi><mi>s</mi></msub><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s\\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0with\xA0</span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose delimcenter" style="top:0em;">]</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0and\xA0</span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span></span>',Fo,$s,fl,Ao,ru='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',Co,Po,iu='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo><mtext>\xA0with\xA0</mtext><mi>i</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>1</mn></msup><mo fence="true">]</mo></mrow><mtext>\xA0and\xA0</mtext><mi>j</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{1}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^1\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s^1\\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2472em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0with\xA0</span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2141em;vertical-align:-0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0and\xA0</span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2141em;vertical-align:-0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span></span>',So,We,_l,Oo,lu='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo><mtext>\xA0with\xA0</mtext><mi>i</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo fence="true">]</mo></mrow><mtext>\xA0and\xA0</mtext><mi>j</mi><mo>\u2208</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X^{2}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^2\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s^2\\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2472em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0with\xA0</span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2141em;vertical-align:-0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">\xA0and\xA0</span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2141em;vertical-align:-0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span></span>',No,Qe,vl,Do,pu='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>d</mi><mo>=</mo><msup><mi>d</mi><mn>1</mn></msup><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mtext>\xA0and\xA0</mtext><msub><mi>n</mi><mi>s</mi></msub><mo>=</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>\xD7</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">d = d^1 + d^2 \\text{ and } n_s = n_s^1 \\times n_s^2 .</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9474em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0141em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord text"><span class="mord">\xA0and\xA0</span></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord">.</span></span></span></span></span>',Ho,Ke,bl,Io,mu=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>k</mi></mrow><mn>1</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if\xA0\xA0</mtext><mi>i</mi><mo>&lt;</mo><msup><mi>d</mi><mn>1</mn></msup><mtext>\xA0with\xA0</mtext><mi>k</mi><mo>=</mo><mi>j</mi><mspace></mspace><mspace width="0.6667em"/><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext>\u2009</mtext><mtext>\u2009</mtext><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mo>\u2212</mo><msup><mi>d</mi><mn>1</mn></msup><mo separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if\xA0</mtext><mi>i</mi><mo>\u2265</mo><msup><mi>d</mi><mn>1</mn></msup><mtext>\xA0with\xA0</mtext><mi>l</mi><mo>=</mo><mo stretchy="false">\u230A</mo><mfrac><mi>j</mi><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup></mfrac><mo stretchy="false">\u230B</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">X_{i,j} = \\begin{cases}
X^{1}_{i, k}, &amp; \\text{if }\\ i &lt; d^1 \\text{ with } k = j \\mod n_s^1 \\\\
X^{2}_{i - d^1, l}, &amp; \\text{if } i \\ge d^1 \\text{ with } l = \\lfloor\\frac{j}{n_s^1}\\rfloor
\\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7466em;"><span style="top:-3.7466em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span><span style="top:-2.3066em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.3806em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">\u2212</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.4555em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.2466em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7466em;"><span style="top:-3.7466em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if\xA0</span></span><span class="mspace">\xA0</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord text"><span class="mord">\xA0with\xA0</span></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace allowbreak"></span><span class="mspace" style="margin-right:0.6667em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">mod</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.3066em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if\xA0</span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2265</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord text"><span class="mord">\xA0with\xA0</span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">\u230A</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9078em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.214em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.5452em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">\u230B</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.2466em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>`,Wo,F,yl,Qo,cu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2208</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_j \\in \\mathbb{R}^{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8252em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2208</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>',Ko,Uo,hu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow><mn>1</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mrow><mi>l</mi><mo separator="true">,</mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">x^1_{k, l} + x^2_{l, k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2333em;vertical-align:-0.4192em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2333em;vertical-align:-0.4192em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span></span></span></span>',Xo,Bt,kl,wl,Vo,du='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',Bo,Go,gu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mtext>\xA0and\xA0</mtext><mi>l</mi></mrow><annotation encoding="application/x-tex">k \\text{ and } l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord text"><span class="mord">\xA0and\xA0</span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>',Jo,Yo,uu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',Zo,sr,hs,xl,er,fu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>1</mn></msup><mo>=</mo><msup><mn>2</mn><mn>5</mn></msup><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo>=</mo><msup><mn>2</mn><mn>5</mn></msup><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>9</mn></msup><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo>=</mo><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">d^1 = 2^5, d^2 = 2^5, n_s^1 = 2^9, n_s^2 = 2^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span>',ar,tr,_u='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>14</mn></msup><mo>+</mo><msup><mn>2</mn><mn>15</mn></msup><mo>\u2248</mo><mn>49000</mn></mrow><annotation encoding="application/x-tex">2^{14} + 2^{15} \\approx 49000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">\u2248</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">49000</span></span></span></span>',nr,or,E,zl,Gt,$l,Tl,rr,vu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>d</mi><mn>1</mn></msup><mo separator="true">,</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d^1, d^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',ir,Jt,Ml,Rl,Yt,ql,El,lr,bu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mi>s</mi><mn>1</mn></msubsup><mo separator="true">,</mo><msubsup><mi>n</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_s^1, n_s^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',pr,Zt,jl,Ll,sn,Fl,Al,en,Cl,Pl,mr,Ts,Js,an,Ue,Sl,tn,Ol,cr,J,Nl,Xe,Dl,Hl,nn,Il,Wl,on,Ql,Kl,hr,O,Ul,rn,Xl,Vl,ln,Bl,Gl,pn,Jl,Yl,mn,Zl,sp,cn,ep,ap,dr,ds,tp,Ve,np,op,Be,rp,ip,gr,I,lp,hn,pp,mp,ur,yu=`<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_{\\text{buckets}}^1,
n_{\\text{buckets}}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0972em;vertical-align:-0.2831em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>`,fr,_r,ku=`<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msub><mi>n</mi><mtext>buckets</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,\\ldots,
n_{\\text{buckets}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>`,vr,br,wu=`<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>\u2212</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>\u2212</mo><mn>1</mn><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><mn>1</mn><mo>\u2212</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>1</mn></msubsup><mo>\u2212</mo><msubsup><mi>n</mi><mtext>buckets</mtext><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-1,\\ldots, n_{\\text{buckets}}^1-1, \\ldots,
1-n_{\\text{buckets}}^2, \\ldots, n_{\\text{buckets}}^1-n_{\\text{buckets}}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2212</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0972em;vertical-align:-0.2831em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2212</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2212</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0972em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2212</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0972em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">buckets</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>`,yr,kr,gs,cp,dn,hp,dp,gn,gp,up,wr,Y,fp,xr,xu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>\xD7</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\mathcal{O}(n_s \\times n_s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',zr,$r,zu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>\xD7</mo><mi>log</mi><mo>\u2061</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\mathcal{O}(n_s \\times \\log(n_s))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span>',Tr,Mr,$u='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">n_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Rr,qr,Ms,Ys,un,Ge,_p,fn,vp,Er,Z,bp,_n,yp,kp,vn,wp,xp,bn,zp,$p,jr,ss,Tp,Lr,Tu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>\xD7</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\mathcal{O}(n_s \\times n_s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',Fr,Ar,Mu='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>\xD7</mo><mi>log</mi><mo>\u2061</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\mathcal{O}(n_s \\times \\log(n_s))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span>',Cr,Pr,Ru='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">n_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Sr,Or,Rs,Zs,yn,Je,Mp,kn,Rp,Nr,us,qp,wn,Ep,jp,xn,Lp,Fp,Dr,se,Ap,tt,Cp,Pp,Hr,Ye,Ir,qs,ee,zn,Ze,Sp,$n,Op,Wr,B,sa,Np,Es,Dp,nt,Hp,Ip,ea,Wp,Qp,Kp,js,Up,ot,Xp,Vp,rt,Bp,Gp,Jp,ae,Qr,Ls,te,Tn,aa,Yp,Mn,Zp,Kr,G,ta,sm,na,em,oa,am,tm,nm,ra,om,it,rm,im,lm,lt,ia,Ur,Fs,ne,Rn,la,pm,qn,mm,Xr,is,pa,cm,As,hm,En,dm,gm,ma,um,fm,_m,ca,vm,pt,bm,ym,Vr,Cs,oe,jn,ha,km,Ln,wm,Br,N,da,xm,ga,zm,ua,$m,Tm,Mm,fa,Rm,mt,qm,Em,jm,_a,Lm,va,Fm,Am,Cm,es,ba,Pm,Ps,Sm,ct,Om,Nm,Fn,Dm,Hm,Im,re,Wm,ie,Gr,Ss,le,An,ya,Qm,Cn,Km,Jr,D,ka,Um,Os,Xm,Pn,Vm,Bm,wa,Gm,Jm,Ym,xa,Zm,ht,sc,ec,ac,za,tc,$a,nc,oc,rc,as,Ta,ic,Ns,lc,dt,pc,mc,Sn,cc,hc,dc,pe,gc,me,Yr,Ds,ce,On,Ma,uc,Nn,fc,Zr,H,Ra,_c,Hs,vc,Dn,bc,yc,qa,kc,wc,xc,Ea,zc,gt,$c,Tc,Mc,ja,Rc,La,qc,Ec,jc,W,Fa,Lc,Is,Fc,ut,Ac,Cc,Hn,Pc,Sc,Oc,he,Nc,de,Dc,ge,si,Ws,ue,In,Aa,Hc,Wn,Ic,ei,P,Ca,Wc,Qn,Qc,Kc,Pa,Uc,Sa,Xc,Vc,Bc,Oa,Gc,ft,Jc,Yc,Zc,Na,sh,Da,eh,ah,th,A,Ha,nh,Qs,oh,_t,rh,ih,Kn,lh,ph,mh,fe,ch,_e,hh,ve,dh,be,gh,ye,ai,Ks,ke,Un,Ia,uh,Xn,fh,ti,S,Wa,_h,Us,vh,Vn,bh,yh,Bn,kh,wh,xh,Qa,zh,Ka,$h,Th,Mh,Ua,Rh,vt,qh,Eh,jh,Xa,Lh,Va,Fh,Ah,Ch,Q,Ba,Ph,Xs,Sh,bt,Oh,Nh,Gn,Dh,Hh,Ih,we,Wh,xe,Qh,ze,ni;return l=new V({}),Ce=new V({}),He=new V({}),Ue=new V({}),Ge=new V({}),Je=new V({}),Ye=new rs({props:{code:`input_ids = tokenizer.encode("This is a sentence from the training data", return_tensors="pt")
loss = model(input_ids, labels=input_ids)[0]`,highlighted:`input_ids = tokenizer.encode(<span class="hljs-string">&quot;This is a sentence from the training data&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
loss = model(input_ids, labels=input_ids)[<span class="hljs-number">0</span>]`}}),Ze=new V({}),sa=new X({props:{name:"class transformers.ReformerConfig",anchor:"transformers.ReformerConfig",parameters:[{name:"attention_head_size",val:" = 64"},{name:"attn_layers",val:" = ['local', 'lsh', 'local', 'lsh', 'local', 'lsh']"},{name:"axial_norm_std",val:" = 1.0"},{name:"axial_pos_embds",val:" = True"},{name:"axial_pos_shape",val:" = [64, 64]"},{name:"axial_pos_embds_dim",val:" = [64, 192]"},{name:"chunk_size_lm_head",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"feed_forward_size",val:" = 512"},{name:"hash_seed",val:" = None"},{name:"hidden_act",val:" = 'relu'"},{name:"hidden_dropout_prob",val:" = 0.05"},{name:"hidden_size",val:" = 256"},{name:"initializer_range",val:" = 0.02"},{name:"is_decoder",val:" = False"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"local_num_chunks_before",val:" = 1"},{name:"local_num_chunks_after",val:" = 0"},{name:"local_attention_probs_dropout_prob",val:" = 0.05"},{name:"local_attn_chunk_length",val:" = 64"},{name:"lsh_attn_chunk_length",val:" = 64"},{name:"lsh_attention_probs_dropout_prob",val:" = 0.0"},{name:"lsh_num_chunks_before",val:" = 1"},{name:"lsh_num_chunks_after",val:" = 0"},{name:"max_position_embeddings",val:" = 4096"},{name:"num_attention_heads",val:" = 12"},{name:"num_buckets",val:" = None"},{name:"num_hashes",val:" = 1"},{name:"pad_token_id",val:" = 0"},{name:"vocab_size",val:" = 320"},{name:"tie_word_embeddings",val:" = False"},{name:"use_cache",val:" = True"},{name:"classifier_dropout",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ReformerConfig.attention_head_size",description:`<strong>attention_head_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the projected key, query and value vectors`,name:"attention_head_size"},{anchor:"transformers.ReformerConfig.attn_layers",description:`<strong>attn_layers</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;]</code>) &#x2014;
List of attention layer types in ascending order. It can be chosen between a LSHSelfAttention layer
(<code>&quot;lsh&quot;</code>) and a LocalSelfAttention layer (<code>&quot;local&quot;</code>).</p>
<p>For more information on LSHSelfAttention layer, see <a href="reformer#lsh-self-attention">LSH Self Attention</a>. For
more information on LocalSelfAttention layer, see <a href="reformer#local-self-attention">Local Self Attention</a>.`,name:"attn_layers"},{anchor:"transformers.ReformerConfig.axial_pos_embds",description:`<strong>axial_pos_embds</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use axial position embeddings. For more information on how axial position embeddings
work, see <a href="reformer#axial-positional-encodings">Axial Position Encodings</a>.`,name:"axial_pos_embds"},{anchor:"transformers.ReformerConfig.axial_norm_std",description:`<strong>axial_norm_std</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The standard deviation of the normal_initializer for initializing the weight matrices of the axial
positional encodings.`,name:"axial_norm_std"},{anchor:"transformers.ReformerConfig.axial_pos_shape",description:`<strong>axial_pos_shape</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 64]</code>) &#x2014;
The position dims of the axial position encodings. During training, the product of the position dims has to
be equal to the sequence length.</p>
<p>For more information on how axial position embeddings work, see <a href="reformer#axial-positional-encodings">Axial Position
Encodings</a>.`,name:"axial_pos_shape"},{anchor:"transformers.ReformerConfig.axial_pos_embds_dim",description:`<strong>axial_pos_embds_dim</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 192]</code>) &#x2014;
The embedding dims of the axial position encodings. The sum of the embedding dims has to be equal to the
hidden size.</p>
<p>For more information on how axial position embeddings work, see <a href="reformer#axial-positional-encodings">Axial Position
Encodings</a>.`,name:"axial_pos_embds_dim"},{anchor:"transformers.ReformerConfig.chunk_size_lm_head",description:`<strong>chunk_size_lm_head</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The chunk size of the final language model feed forward head layer. A chunk size of 0 means that the feed
forward layer is not chunked. A chunk size of n means that the feed forward layer processes n &lt;
sequence_length embeddings at a time.</p>
<p>For more information on feed forward chunking, see <a href="../glossary#feed-forward-chunking">How does Feed Forward Chunking
work?</a>.`,name:"chunk_size_lm_head"},{anchor:"transformers.ReformerConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The token id for the end-of-sentence token.`,name:"eos_token_id"},{anchor:"transformers.ReformerConfig.feed_forward_size",description:`<strong>feed_forward_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the feed_forward layer in the residual attention block.`,name:"feed_forward_size"},{anchor:"transformers.ReformerConfig.hash_seed",description:`<strong>hash_seed</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Seed that can be used to make local sensitive hashing in <code>LSHSelfAttention</code> deterministic. This should only
be set for testing purposed. For evaluation and training purposes <code>hash_seed</code> should be left as <code>None</code> to
ensure fully random rotations in local sensitive hashing scheme.`,name:"hash_seed"},{anchor:"transformers.ReformerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the feed forward layer in the residual attention
block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ReformerConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ReformerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the output hidden states of the residual attention blocks.`,name:"hidden_size"},{anchor:"transformers.ReformerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ReformerConfig.is_decoder",description:`<strong>is_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use a causal mask in addition to the <code>attention_mask</code> passed to <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a>. When
using the Reformer for causal language modeling, this argument should be set to <code>True</code>.`,name:"is_decoder"},{anchor:"transformers.ReformerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ReformerConfig.local_chunk_length",description:`<strong>local_chunk_length</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Length of chunk which attends to itself in <code>LocalSelfAttention</code>. Chunking reduces memory complexity from
sequence length x sequence length (self attention) to chunk length x chunk length x sequence length / chunk
length (chunked self attention).`,name:"local_chunk_length"},{anchor:"transformers.ReformerConfig.local_num_chunks_before",description:`<strong>local_num_chunks_before</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of previous neighbouring chunks to attend to in <code>LocalSelfAttention</code> layer to itself.`,name:"local_num_chunks_before"},{anchor:"transformers.ReformerConfig.local_num_chunks_after",description:`<strong>local_num_chunks_after</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Number of following neighbouring chunks to attend to in <code>LocalSelfAttention</code> layer in addition to itself.`,name:"local_num_chunks_after"},{anchor:"transformers.ReformerConfig.local_attention_probs_dropout_prob",description:`<strong>local_attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities in <code>LocalSelfAttention</code>.`,name:"local_attention_probs_dropout_prob"},{anchor:"transformers.ReformerConfig.lsh_attn_chunk_length",description:`<strong>lsh_attn_chunk_length</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Length of chunk which attends to itself in <code>LSHSelfAttention</code>. Chunking reduces memory complexity from
sequence length x sequence length (self attention) to chunk length x chunk length x sequence length / chunk
length (chunked self attention).`,name:"lsh_attn_chunk_length"},{anchor:"transformers.ReformerConfig.lsh_num_chunks_before",description:`<strong>lsh_num_chunks_before</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of previous neighbouring chunks to attend to in <code>LSHSelfAttention</code> layer to itself.`,name:"lsh_num_chunks_before"},{anchor:"transformers.ReformerConfig.lsh_num_chunks_after",description:`<strong>lsh_num_chunks_after</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Number of following neighbouring chunks to attend to in <code>LSHSelfAttention</code> layer to itself.`,name:"lsh_num_chunks_after"},{anchor:"transformers.ReformerConfig.lsh_attention_probs_dropout_prob",description:`<strong>lsh_attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities in <code>LSHSelfAttention</code>.`,name:"lsh_attention_probs_dropout_prob"},{anchor:"transformers.ReformerConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ReformerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ReformerConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code> or <code>List[int]</code>, <em>optional</em>) &#x2014;
Number of buckets, the key query vectors can be &#x201C;hashed into&#x201D; using the locality sensitive hashing scheme.
Each query key vector is hashed into a hash in <code>1, ..., num_buckets</code>. The number of buckets can also be
factorized into a list for improved memory complexity. In this case, each query key vector is hashed into a
hash in <code>1-1, 1-2, ..., num_buckets[0]-1, ..., num_buckets[0]-num_buckets[1]</code> if <code>num_buckets</code> is
factorized into two factors. The number of buckets (or the product the factors) should approximately equal
sequence length / lsh_chunk_length. If <code>num_buckets</code> not set, a good value is calculated on the fly.`,name:"num_buckets"},{anchor:"transformers.ReformerConfig.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of hashing rounds (e.g., number of random rotations) in Local Sensitive Hashing scheme. The higher
<code>num_hashes</code>, the more accurate the <code>LSHSelfAttention</code> becomes, but also the more memory and time intensive
the hashing becomes.`,name:"num_hashes"},{anchor:"transformers.ReformerConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The token id for the padding token.`,name:"pad_token_id"},{anchor:"transformers.ReformerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 320) &#x2014;\\
Vocabulary size of the Reformer model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a>.`,name:"vocab_size"},{anchor:"transformers.ReformerConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie input and output embeddings.`,name:"tie_word_embeddings"},{anchor:"transformers.ReformerConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.ReformerConfig.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The dropout ratio for the classification head.`,name:"classifier_dropout"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/configuration_reformer.py#L32"}}),ae=new ls({props:{anchor:"transformers.ReformerConfig.example",$$slots:{default:[Au]},$$scope:{ctx:T}}}),aa=new V({}),ta=new X({props:{name:"class transformers.ReformerTokenizer",anchor:"transformers.ReformerTokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"additional_special_tokens",val:" = []"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ReformerTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.ReformerTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.ReformerTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ReformerTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ReformerTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.ReformerTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/tokenization_reformer.py#L48"}}),ia=new X({props:{name:"save_vocabulary",anchor:"transformers.ReformerTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/tokenization_reformer.py#L164"}}),la=new V({}),pa=new X({props:{name:"class transformers.ReformerTokenizerFast",anchor:"transformers.ReformerTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"additional_special_tokens",val:" = []"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ReformerTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.ReformerTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.ReformerTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ReformerTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ReformerTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/tokenization_reformer_fast.py#L57"}}),ha=new V({}),da=new X({props:{name:"class transformers.ReformerModel",anchor:"transformers.ReformerModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ReformerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L1982"}}),ba=new X({props:{name:"forward",anchor:"transformers.ReformerModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"num_hashes",val:": typing.Optional[int] = None"},{name:"past_buckets_states",val:": typing.Optional[typing.List[typing.Tuple[torch.Tensor]]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ReformerModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be
a multiple of the relevant model&#x2019;s chunk lengths (lsh&#x2019;s, local&#x2019;s or both). During evaluation, the indices
are automatically padded to be a multiple of the chunk length.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a>. See <a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ReformerModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ReformerModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ReformerModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ReformerModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ReformerModel.forward.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites
the default defined in <code>config.num_hashes</code>.</p>
<p>For more information, see <code>num_hashes</code> in <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>.`,name:"num_hashes"},{anchor:"transformers.ReformerModel.forward.past_buckets_states",description:`<strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>) &#x2014;
List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed
up sequential decoding.`,name:"past_buckets_states"},{anchor:"transformers.ReformerModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ReformerModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ReformerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ReformerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2010",returnDescription:`
<p>A <code>transformers.models.reformer.modeling_reformer.ReformerModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig"
>ReformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_predict, hidden_size)</code>) \u2014 Sequence of hidden-states at the last layer of the model.</p>
<p><code>num_predict</code> corresponds to <code>target_mapping.shape[1]</code>. If <code>target_mapping</code> is <code>None</code>, then <code>num_predict</code>
corresponds to <code>sequence_length</code>.</p>
</li>
<li>
<p><strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed buckets and hidden-states that can be used (see <code>past_buckets_states</code> input) to speed
up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings and one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.reformer.modeling_reformer.ReformerModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new mo({props:{$$slots:{default:[Cu]},$$scope:{ctx:T}}}),ie=new ls({props:{anchor:"transformers.ReformerModel.forward.example",$$slots:{default:[Pu]},$$scope:{ctx:T}}}),ya=new V({}),ka=new X({props:{name:"class transformers.ReformerModelWithLMHead",anchor:"transformers.ReformerModelWithLMHead",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ReformerModelWithLMHead.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2194"}}),Ta=new X({props:{name:"forward",anchor:"transformers.ReformerModelWithLMHead.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"num_hashes",val:": typing.Optional[int] = None"},{name:"past_buckets_states",val:": typing.Optional[typing.List[typing.Tuple[torch.Tensor]]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.ReformerModelWithLMHead.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be
a multiple of the relevant model&#x2019;s chunk lengths (lsh&#x2019;s, local&#x2019;s or both). During evaluation, the indices
are automatically padded to be a multiple of the chunk length.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a>. See <a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ReformerModelWithLMHead.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ReformerModelWithLMHead.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ReformerModelWithLMHead.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ReformerModelWithLMHead.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ReformerModelWithLMHead.forward.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites
the default defined in <code>config.num_hashes</code>.</p>
<p>For more information, see <code>num_hashes</code> in <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>.`,name:"num_hashes"},{anchor:"transformers.ReformerModelWithLMHead.forward.past_buckets_states",description:`<strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>) &#x2014;
List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed
up sequential decoding.`,name:"past_buckets_states"},{anchor:"transformers.ReformerModelWithLMHead.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ReformerModelWithLMHead.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ReformerModelWithLMHead.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ReformerModelWithLMHead.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ReformerModelWithLMHead.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2219",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig"
>ReformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new mo({props:{$$slots:{default:[Su]},$$scope:{ctx:T}}}),me=new ls({props:{anchor:"transformers.ReformerModelWithLMHead.forward.example",$$slots:{default:[Ou]},$$scope:{ctx:T}}}),Ma=new V({}),Ra=new X({props:{name:"class transformers.ReformerForMaskedLM",anchor:"transformers.ReformerForMaskedLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ReformerForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2317"}}),Fa=new X({props:{name:"forward",anchor:"transformers.ReformerForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"num_hashes",val:": typing.Optional[int] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ReformerForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be
a multiple of the relevant model&#x2019;s chunk lengths (lsh&#x2019;s, local&#x2019;s or both). During evaluation, the indices
are automatically padded to be a multiple of the chunk length.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a>. See <a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ReformerForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ReformerForMaskedLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ReformerForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ReformerForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ReformerForMaskedLM.forward.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites
the default defined in <code>config.num_hashes</code>.</p>
<p>For more information, see <code>num_hashes</code> in <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>.`,name:"num_hashes"},{anchor:"transformers.ReformerForMaskedLM.forward.past_buckets_states",description:`<strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>) &#x2014;
List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed
up sequential decoding.`,name:"past_buckets_states"},{anchor:"transformers.ReformerForMaskedLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ReformerForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ReformerForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ReformerForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ReformerForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked),
the loss is only computed for the tokens with labels`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2336",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig"
>ReformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new mo({props:{$$slots:{default:[Nu]},$$scope:{ctx:T}}}),de=new ls({props:{anchor:"transformers.ReformerForMaskedLM.forward.example",$$slots:{default:[Du]},$$scope:{ctx:T}}}),ge=new ls({props:{anchor:"transformers.ReformerForMaskedLM.forward.example-2",$$slots:{default:[Hu]},$$scope:{ctx:T}}}),Aa=new V({}),Ca=new X({props:{name:"class transformers.ReformerForSequenceClassification",anchor:"transformers.ReformerForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ReformerForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2437"}}),Ha=new X({props:{name:"forward",anchor:"transformers.ReformerForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"num_hashes",val:": typing.Optional[int] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ReformerForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be
a multiple of the relevant model&#x2019;s chunk lengths (lsh&#x2019;s, local&#x2019;s or both). During evaluation, the indices
are automatically padded to be a multiple of the chunk length.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a>. See <a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ReformerForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ReformerForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ReformerForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ReformerForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ReformerForSequenceClassification.forward.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites
the default defined in <code>config.num_hashes</code>.</p>
<p>For more information, see <code>num_hashes</code> in <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>.`,name:"num_hashes"},{anchor:"transformers.ReformerForSequenceClassification.forward.past_buckets_states",description:`<strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>) &#x2014;
List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed
up sequential decoding.`,name:"past_buckets_states"},{anchor:"transformers.ReformerForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ReformerForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ReformerForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ReformerForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ReformerForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2451",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig"
>ReformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new mo({props:{$$slots:{default:[Iu]},$$scope:{ctx:T}}}),_e=new ls({props:{anchor:"transformers.ReformerForSequenceClassification.forward.example",$$slots:{default:[Wu]},$$scope:{ctx:T}}}),ve=new ls({props:{anchor:"transformers.ReformerForSequenceClassification.forward.example-2",$$slots:{default:[Qu]},$$scope:{ctx:T}}}),be=new ls({props:{anchor:"transformers.ReformerForSequenceClassification.forward.example-3",$$slots:{default:[Ku]},$$scope:{ctx:T}}}),ye=new ls({props:{anchor:"transformers.ReformerForSequenceClassification.forward.example-4",$$slots:{default:[Uu]},$$scope:{ctx:T}}}),Ia=new V({}),Wa=new X({props:{name:"class transformers.ReformerForQuestionAnswering",anchor:"transformers.ReformerForQuestionAnswering",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ReformerForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2626"}}),Ba=new X({props:{name:"forward",anchor:"transformers.ReformerForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"num_hashes",val:": typing.Optional[int] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ReformerForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be
a multiple of the relevant model&#x2019;s chunk lengths (lsh&#x2019;s, local&#x2019;s or both). During evaluation, the indices
are automatically padded to be a multiple of the chunk length.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a>. See <a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ReformerForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ReformerForQuestionAnswering.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ReformerForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ReformerForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ReformerForQuestionAnswering.forward.num_hashes",description:`<strong>num_hashes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites
the default defined in <code>config.num_hashes</code>.</p>
<p>For more information, see <code>num_hashes</code> in <a href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a>.`,name:"num_hashes"},{anchor:"transformers.ReformerForQuestionAnswering.forward.past_buckets_states",description:`<strong>past_buckets_states</strong> (<code>List[Tuple(torch.LongTensor, torch.FloatTensor)]</code>, <em>optional</em>) &#x2014;
List of <code>Tuple(torch.LongTensor, torch.FloatTensor</code> of length <code>config.n_layers</code>, with the first element
being the previous <em>buckets</em> of shape <code>(batch_size, num_heads, num_hashes, sequence_length)</code>) and the
second being the previous <em>hidden_states</em> of shape <code>(batch_size, sequence_length, hidden_size)</code>).</p>
<p>Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed
up sequential decoding.`,name:"past_buckets_states"},{anchor:"transformers.ReformerForQuestionAnswering.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ReformerForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ReformerForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ReformerForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ReformerForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.ReformerForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/reformer/modeling_reformer.py#L2638",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerConfig"
>ReformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new mo({props:{$$slots:{default:[Xu]},$$scope:{ctx:T}}}),xe=new ls({props:{anchor:"transformers.ReformerForQuestionAnswering.forward.example",$$slots:{default:[Vu]},$$scope:{ctx:T}}}),ze=new ls({props:{anchor:"transformers.ReformerForQuestionAnswering.forward.example-2",$$slots:{default:[Bu]},$$scope:{ctx:T}}}),{c(){p=o("meta"),v=h(),g=o("h1"),f=o("a"),b=o("span"),y(l.$$.fragment),_=h(),L=o("span"),Ai=t("Reformer"),co=h(),ws=o("p"),St=o("strong"),Ci=t("DISCLAIMER:"),Pi=t(" This model is still a work in progress, if you see something strange, file a "),Ae=o("a"),Si=t("Github Issue"),Oi=t("."),ho=h(),xs=o("h2"),Vs=o("a"),Ot=o("span"),y(Ce.$$.fragment),Ni=h(),Nt=o("span"),Di=t("Overview"),go=h(),Bs=o("p"),Hi=t("The Reformer model was proposed in the paper "),Pe=o("a"),Ii=t("Reformer: The Efficient Transformer"),Wi=t(" by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya."),uo=h(),st=o("p"),Qi=t("The abstract from the paper is the following:"),fo=h(),et=o("p"),Dt=o("em"),Ki=t(`Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.`),_o=h(),ms=o("p"),Ui=t("This model was contributed by "),Se=o("a"),Xi=t("patrickvonplaten"),Vi=t(`. The Authors\u2019 code can be
found `),Oe=o("a"),Bi=t("here"),Gi=t("."),vo=h(),Ne=o("p"),Ht=o("strong"),Ji=t("Note"),Yi=t(":"),bo=h(),at=o("ul"),cs=o("li"),Zi=t("Reformer does "),It=o("strong"),sl=t("not"),el=t(" work with "),Wt=o("em"),al=t("torch.nn.DataParallel"),tl=t(" due to a bug in PyTorch, see "),De=o("a"),nl=t("issue #36035"),yo=h(),zs=o("h2"),Gs=o("a"),Qt=o("span"),y(He.$$.fragment),ol=h(),Kt=o("span"),rl=t("Axial Positional Encodings"),ko=h(),q=o("p"),il=t("Axial Positional Encodings were first implemented in Google\u2019s "),Ie=o("a"),ll=t("trax library"),pl=t(`
and developed by the authors of this model\u2019s paper. In models that are treating very long input sequences, the
conventional position id encodings store an embedings vector of size `),wo=new M,xo=t(" being the "),Ut=o("code"),ml=t("config.hidden_size"),cl=t(` for
every position `),zo=new M,$o=t(", with "),To=new M,Mo=t(" being "),Xt=o("code"),hl=t("config.max_embedding_size"),dl=t(`. This means that having
a sequence length of `),Ro=new M,qo=t(" and a "),Vt=o("code"),gl=t("config.hidden_size"),ul=t(" of "),Eo=new M,jo=t(`
would result in a position encoding matrix:
`),Lo=new M,Fo=h(),$s=o("p"),fl=t("which alone has over 500M parameters to store. Axial positional encodings factorize "),Ao=new M,Co=t(` into two matrices:
`),Po=new M,So=h(),We=o("p"),_l=t(`and
`),Oo=new M,No=h(),Qe=o("p"),vl=t(`with:
`),Do=new M,Ho=h(),Ke=o("p"),bl=t(`Therefore the following holds:
`),Io=new M,Wo=h(),F=o("p"),yl=t("Intuitively, this means that a position embedding vector "),Qo=new M,Ko=t(` is now the composition of two
factorized embedding vectors: `),Uo=new M,Xo=t(", where as the "),Bt=o("code"),kl=t("config.max_embedding_size"),wl=t(` dimension
`),Vo=new M,Bo=t(" is factorized into "),Go=new M,Jo=t(`. This design ensures that each position embedding vector
`),Yo=new M,Zo=t(" is unique."),sr=h(),hs=o("p"),xl=t("Using the above example again, axial position encoding with "),er=new M,ar=t(`
can drastically reduced the number of parameters to `),tr=new M,nr=t(" parameters."),or=h(),E=o("p"),zl=t("In practice, the parameter "),Gt=o("code"),$l=t("config.axial_pos_embds_dim"),Tl=t(" is set to a tuple "),rr=new M,ir=t(` which sum has to be
equal to `),Jt=o("code"),Ml=t("config.hidden_size"),Rl=t(" and "),Yt=o("code"),ql=t("config.axial_pos_shape"),El=t(" is set to a tuple "),lr=new M,pr=t(` which
product has to be equal to `),Zt=o("code"),jl=t("config.max_embedding_size"),Ll=t(", which during training has to be equal to the "),sn=o("em"),Fl=t(`sequence
length`),Al=t(" of the "),en=o("code"),Cl=t("input_ids"),Pl=t("."),mr=h(),Ts=o("h2"),Js=o("a"),an=o("span"),y(Ue.$$.fragment),Sl=h(),tn=o("span"),Ol=t("LSH Self Attention"),cr=h(),J=o("p"),Nl=t(`In Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key
query embedding vectors are also tied. LSH self attention uses the locality sensitive hashing mechanism proposed in
`),Xe=o("a"),Dl=t("Practical and Optimal LSH for Angular Distance"),Hl=t(` to assign each of the tied key
query embedding vectors to one of `),nn=o("code"),Il=t("config.num_buckets"),Wl=t(` possible buckets. The premise is that the more \u201Csimilar\u201D
key query embedding vectors (in terms of `),on=o("em"),Ql=t("cosine similarity"),Kl=t(`) are to each other, the more likely they are assigned to
the same bucket.`),hr=h(),O=o("p"),Ul=t("The accuracy of the LSH mechanism can be improved by increasing "),rn=o("code"),Xl=t("config.num_hashes"),Vl=t(` or directly the argument
`),ln=o("code"),Bl=t("num_hashes"),Gl=t(` of the forward function so that the output of the LSH self attention better approximates the output
of the \u201Cnormal\u201D full self attention. The buckets are then sorted and chunked into query key embedding vector chunks
each of length `),pn=o("code"),Jl=t("config.lsh_chunk_length"),Yl=t(`. For each chunk, the query embedding vectors attend to its key vectors
(which are tied to themselves) and to the key embedding vectors of `),mn=o("code"),Zl=t("config.lsh_num_chunks_before"),sp=t(` previous
neighboring chunks and `),cn=o("code"),ep=t("config.lsh_num_chunks_after"),ap=t(" following neighboring chunks."),dr=h(),ds=o("p"),tp=t("For more information, see the "),Ve=o("a"),np=t("original Paper"),op=t(" or this great "),Be=o("a"),rp=t("blog post"),ip=t("."),gr=h(),I=o("p"),lp=t("Note that "),hn=o("code"),pp=t("config.num_buckets"),mp=t(" can also be factorized into a list "),ur=new M,fr=t(". This way instead of assigning the query key embedding vectors to one of "),_r=new M,vr=t(" they are assigned to one of "),br=new M,yr=t(`. This is crucial for very long sequences to
save memory.`),kr=h(),gs=o("p"),cp=t("When training a model from scratch, it is recommended to leave "),dn=o("code"),hp=t("config.num_buckets=None"),dp=t(`, so that depending on the
sequence length a good value for `),gn=o("code"),gp=t("num_buckets"),up=t(` is calculated on the fly. This value will then automatically be
saved in the config and should be reused for inference.`),wr=h(),Y=o("p"),fp=t(`Using LSH self attention, the memory and time complexity of the query-key matmul operation can be reduced from
`),xr=new M,zr=t(" to "),$r=new M,Tr=t(`, which usually represents the memory
and time bottleneck in a transformer model, with `),Mr=new M,Rr=t(" being the sequence length."),qr=h(),Ms=o("h2"),Ys=o("a"),un=o("span"),y(Ge.$$.fragment),_p=h(),fn=o("span"),vp=t("Local Self Attention"),Er=h(),Z=o("p"),bp=t(`Local self attention is essentially a \u201Cnormal\u201D self attention layer with key, query and value projections, but is
chunked so that in each chunk of length `),_n=o("code"),yp=t("config.local_chunk_length"),kp=t(` the query embedding vectors only attends to
the key embedding vectors in its chunk and to the key embedding vectors of `),vn=o("code"),wp=t("config.local_num_chunks_before"),xp=t(`
previous neighboring chunks and `),bn=o("code"),zp=t("config.local_num_chunks_after"),$p=t(" following neighboring chunks."),jr=h(),ss=o("p"),Tp=t(`Using Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from
`),Lr=new M,Fr=t(" to "),Ar=new M,Cr=t(`, which usually represents the memory
and time bottleneck in a transformer model, with `),Pr=new M,Sr=t(" being the sequence length."),Or=h(),Rs=o("h2"),Zs=o("a"),yn=o("span"),y(Je.$$.fragment),Mp=h(),kn=o("span"),Rp=t("Training"),Nr=h(),us=o("p"),qp=t(`During training, we must ensure that the sequence length is set to a value that can be divided by the least common
multiple of `),wn=o("code"),Ep=t("config.lsh_chunk_length"),jp=t(" and "),xn=o("code"),Lp=t("config.local_chunk_length"),Fp=t(` and that the parameters of the Axial
Positional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can
easily be trained on sequences as long as 64000 tokens.`),Dr=h(),se=o("p"),Ap=t("For training, the "),tt=o("a"),Cp=t("ReformerModelWithLMHead"),Pp=t(" should be used as follows:"),Hr=h(),y(Ye.$$.fragment),Ir=h(),qs=o("h2"),ee=o("a"),zn=o("span"),y(Ze.$$.fragment),Sp=h(),$n=o("span"),Op=t("ReformerConfig"),Wr=h(),B=o("div"),y(sa.$$.fragment),Np=h(),Es=o("p"),Dp=t("This is the configuration class to store the configuration of a "),nt=o("a"),Hp=t("ReformerModel"),Ip=t(`. It is used to instantiate a
Reformer model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the ReFormer
`),ea=o("a"),Wp=t("google/reformer-crime-and-punishment"),Qp=t(" architecture."),Kp=h(),js=o("p"),Up=t("Configuration objects inherit from "),ot=o("a"),Xp=t("PretrainedConfig"),Vp=t(` and can be used to control the model outputs. Read the
documentation from `),rt=o("a"),Bp=t("PretrainedConfig"),Gp=t(" for more information."),Jp=h(),y(ae.$$.fragment),Qr=h(),Ls=o("h2"),te=o("a"),Tn=o("span"),y(aa.$$.fragment),Yp=h(),Mn=o("span"),Zp=t("ReformerTokenizer"),Kr=h(),G=o("div"),y(ta.$$.fragment),sm=h(),na=o("p"),em=t("Construct a Reformer tokenizer. Based on "),oa=o("a"),am=t("SentencePiece"),tm=t(" ."),nm=h(),ra=o("p"),om=t("This tokenizer inherits from "),it=o("a"),rm=t("PreTrainedTokenizer"),im=t(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),lm=h(),lt=o("div"),y(ia.$$.fragment),Ur=h(),Fs=o("h2"),ne=o("a"),Rn=o("span"),y(la.$$.fragment),pm=h(),qn=o("span"),mm=t("ReformerTokenizerFast"),Xr=h(),is=o("div"),y(pa.$$.fragment),cm=h(),As=o("p"),hm=t("Construct a \u201Cfast\u201D Reformer tokenizer (backed by HuggingFace\u2019s "),En=o("em"),dm=t("tokenizers"),gm=t(` library). Based on
`),ma=o("a"),um=t("Unigram"),fm=t("."),_m=h(),ca=o("p"),vm=t("This tokenizer inherits from "),pt=o("a"),bm=t("PreTrainedTokenizerFast"),ym=t(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Vr=h(),Cs=o("h2"),oe=o("a"),jn=o("span"),y(ha.$$.fragment),km=h(),Ln=o("span"),wm=t("ReformerModel"),Br=h(),N=o("div"),y(da.$$.fragment),xm=h(),ga=o("p"),zm=t(`The bare Reformer Model transformer outputting raw hidden-stateswithout any specific head on top.
Reformer was proposed in `),ua=o("a"),$m=t("Reformer: The Efficient Transformer"),Tm=t(` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Mm=h(),fa=o("p"),Rm=t("This model inherits from "),mt=o("a"),qm=t("PreTrainedModel"),Em=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jm=h(),_a=o("p"),Lm=t("This model is also a PyTorch "),va=o("a"),Fm=t("torch.nn.Module"),Am=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Cm=h(),es=o("div"),y(ba.$$.fragment),Pm=h(),Ps=o("p"),Sm=t("The "),ct=o("a"),Om=t("ReformerModel"),Nm=t(" forward method, overrides the "),Fn=o("code"),Dm=t("__call__"),Hm=t(" special method."),Im=h(),y(re.$$.fragment),Wm=h(),y(ie.$$.fragment),Gr=h(),Ss=o("h2"),le=o("a"),An=o("span"),y(ya.$$.fragment),Qm=h(),Cn=o("span"),Km=t("ReformerModelWithLMHead"),Jr=h(),D=o("div"),y(ka.$$.fragment),Um=h(),Os=o("p"),Xm=t("Reformer Model with a "),Pn=o("code"),Vm=t("language modeling"),Bm=t(` head on top.
Reformer was proposed in `),wa=o("a"),Gm=t("Reformer: The Efficient Transformer"),Jm=t(` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Ym=h(),xa=o("p"),Zm=t("This model inherits from "),ht=o("a"),sc=t("PreTrainedModel"),ec=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ac=h(),za=o("p"),tc=t("This model is also a PyTorch "),$a=o("a"),nc=t("torch.nn.Module"),oc=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rc=h(),as=o("div"),y(Ta.$$.fragment),ic=h(),Ns=o("p"),lc=t("The "),dt=o("a"),pc=t("ReformerModelWithLMHead"),mc=t(" forward method, overrides the "),Sn=o("code"),cc=t("__call__"),hc=t(" special method."),dc=h(),y(pe.$$.fragment),gc=h(),y(me.$$.fragment),Yr=h(),Ds=o("h2"),ce=o("a"),On=o("span"),y(Ma.$$.fragment),uc=h(),Nn=o("span"),fc=t("ReformerForMaskedLM"),Zr=h(),H=o("div"),y(Ra.$$.fragment),_c=h(),Hs=o("p"),vc=t("Reformer Model with a "),Dn=o("code"),bc=t("language modeling"),yc=t(` head on top.
Reformer was proposed in `),qa=o("a"),kc=t("Reformer: The Efficient Transformer"),wc=t(` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),xc=h(),Ea=o("p"),zc=t("This model inherits from "),gt=o("a"),$c=t("PreTrainedModel"),Tc=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Mc=h(),ja=o("p"),Rc=t("This model is also a PyTorch "),La=o("a"),qc=t("torch.nn.Module"),Ec=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jc=h(),W=o("div"),y(Fa.$$.fragment),Lc=h(),Is=o("p"),Fc=t("The "),ut=o("a"),Ac=t("ReformerForMaskedLM"),Cc=t(" forward method, overrides the "),Hn=o("code"),Pc=t("__call__"),Sc=t(" special method."),Oc=h(),y(he.$$.fragment),Nc=h(),y(de.$$.fragment),Dc=h(),y(ge.$$.fragment),si=h(),Ws=o("h2"),ue=o("a"),In=o("span"),y(Aa.$$.fragment),Hc=h(),Wn=o("span"),Ic=t("ReformerForSequenceClassification"),ei=h(),P=o("div"),y(Ca.$$.fragment),Wc=h(),Qn=o("p"),Qc=t(`Reformer Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Kc=h(),Pa=o("p"),Uc=t("Reformer was proposed in "),Sa=o("a"),Xc=t("Reformer: The Efficient Transformer"),Vc=t(` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Bc=h(),Oa=o("p"),Gc=t("This model inherits from "),ft=o("a"),Jc=t("PreTrainedModel"),Yc=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zc=h(),Na=o("p"),sh=t("This model is also a PyTorch "),Da=o("a"),eh=t("torch.nn.Module"),ah=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),th=h(),A=o("div"),y(Ha.$$.fragment),nh=h(),Qs=o("p"),oh=t("The "),_t=o("a"),rh=t("ReformerForSequenceClassification"),ih=t(" forward method, overrides the "),Kn=o("code"),lh=t("__call__"),ph=t(" special method."),mh=h(),y(fe.$$.fragment),ch=h(),y(_e.$$.fragment),hh=h(),y(ve.$$.fragment),dh=h(),y(be.$$.fragment),gh=h(),y(ye.$$.fragment),ai=h(),Ks=o("h2"),ke=o("a"),Un=o("span"),y(Ia.$$.fragment),uh=h(),Xn=o("span"),fh=t("ReformerForQuestionAnswering"),ti=h(),S=o("div"),y(Wa.$$.fragment),_h=h(),Us=o("p"),vh=t(`Reformer Model with a span classification head on top for extractive question-answering tasks like SQuAD / TriviaQA
( a linear layer on top of hidden-states output to compute `),Vn=o("code"),bh=t("span start logits"),yh=t(" and "),Bn=o("code"),kh=t("span end logits"),wh=t("."),xh=h(),Qa=o("p"),zh=t("Reformer was proposed in "),Ka=o("a"),$h=t("Reformer: The Efficient Transformer"),Th=t(` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Mh=h(),Ua=o("p"),Rh=t("This model inherits from "),vt=o("a"),qh=t("PreTrainedModel"),Eh=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jh=h(),Xa=o("p"),Lh=t("This model is also a PyTorch "),Va=o("a"),Fh=t("torch.nn.Module"),Ah=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ch=h(),Q=o("div"),y(Ba.$$.fragment),Ph=h(),Xs=o("p"),Sh=t("The "),bt=o("a"),Oh=t("ReformerForQuestionAnswering"),Nh=t(" forward method, overrides the "),Gn=o("code"),Dh=t("__call__"),Hh=t(" special method."),Ih=h(),y(we.$$.fragment),Wh=h(),y(xe.$$.fragment),Qh=h(),y(ze.$$.fragment),this.h()},l(e){const m=Lu('[data-svelte="svelte-1phssyn"]',document.head);p=r(m,"META",{name:!0,content:!0}),m.forEach(a),v=d(e),g=r(e,"H1",{class:!0});var Ga=i(g);f=r(Ga,"A",{id:!0,class:!0,href:!0});var Jn=i(f);b=r(Jn,"SPAN",{});var Yn=i(b);k(l.$$.fragment,Yn),Yn.forEach(a),Jn.forEach(a),_=d(Ga),L=r(Ga,"SPAN",{});var Zn=i(L);Ai=n(Zn,"Reformer"),Zn.forEach(a),Ga.forEach(a),co=d(e),ws=r(e,"P",{});var $e=i(ws);St=r($e,"STRONG",{});var so=i(St);Ci=n(so,"DISCLAIMER:"),so.forEach(a),Pi=n($e," This model is still a work in progress, if you see something strange, file a "),Ae=r($e,"A",{href:!0,rel:!0});var eo=i(Ae);Si=n(eo,"Github Issue"),eo.forEach(a),Oi=n($e,"."),$e.forEach(a),ho=d(e),xs=r(e,"H2",{class:!0});var Ja=i(xs);Vs=r(Ja,"A",{id:!0,class:!0,href:!0});var ao=i(Vs);Ot=r(ao,"SPAN",{});var to=i(Ot);k(Ce.$$.fragment,to),to.forEach(a),ao.forEach(a),Ni=d(Ja),Nt=r(Ja,"SPAN",{});var no=i(Nt);Di=n(no,"Overview"),no.forEach(a),Ja.forEach(a),go=d(e),Bs=r(e,"P",{});var Ya=i(Bs);Hi=n(Ya,"The Reformer model was proposed in the paper "),Pe=r(Ya,"A",{href:!0,rel:!0});var oo=i(Pe);Ii=n(oo,"Reformer: The Efficient Transformer"),oo.forEach(a),Wi=n(Ya," by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya."),Ya.forEach(a),uo=d(e),st=r(e,"P",{});var ro=i(st);Qi=n(ro,"The abstract from the paper is the following:"),ro.forEach(a),fo=d(e),et=r(e,"P",{});var io=i(et);Dt=r(io,"EM",{});var lo=i(Dt);Ki=n(lo,`Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.`),lo.forEach(a),io.forEach(a),_o=d(e),ms=r(e,"P",{});var yt=i(ms);Ui=n(yt,"This model was contributed by "),Se=r(yt,"A",{href:!0,rel:!0});var Bh=i(Se);Xi=n(Bh,"patrickvonplaten"),Bh.forEach(a),Vi=n(yt,`. The Authors\u2019 code can be
found `),Oe=r(yt,"A",{href:!0,rel:!0});var Gh=i(Oe);Bi=n(Gh,"here"),Gh.forEach(a),Gi=n(yt,"."),yt.forEach(a),vo=d(e),Ne=r(e,"P",{});var Kh=i(Ne);Ht=r(Kh,"STRONG",{});var Jh=i(Ht);Ji=n(Jh,"Note"),Jh.forEach(a),Yi=n(Kh,":"),Kh.forEach(a),bo=d(e),at=r(e,"UL",{});var Yh=i(at);cs=r(Yh,"LI",{});var Za=i(cs);Zi=n(Za,"Reformer does "),It=r(Za,"STRONG",{});var Zh=i(It);sl=n(Zh,"not"),Zh.forEach(a),el=n(Za," work with "),Wt=r(Za,"EM",{});var sd=i(Wt);al=n(sd,"torch.nn.DataParallel"),sd.forEach(a),tl=n(Za," due to a bug in PyTorch, see "),De=r(Za,"A",{href:!0,rel:!0});var ed=i(De);nl=n(ed,"issue #36035"),ed.forEach(a),Za.forEach(a),Yh.forEach(a),yo=d(e),zs=r(e,"H2",{class:!0});var oi=i(zs);Gs=r(oi,"A",{id:!0,class:!0,href:!0});var ad=i(Gs);Qt=r(ad,"SPAN",{});var td=i(Qt);k(He.$$.fragment,td),td.forEach(a),ad.forEach(a),ol=d(oi),Kt=r(oi,"SPAN",{});var nd=i(Kt);rl=n(nd,"Axial Positional Encodings"),nd.forEach(a),oi.forEach(a),ko=d(e),q=r(e,"P",{});var j=i(q);il=n(j,"Axial Positional Encodings were first implemented in Google\u2019s "),Ie=r(j,"A",{href:!0,rel:!0});var od=i(Ie);ll=n(od,"trax library"),od.forEach(a),pl=n(j,`
and developed by the authors of this model\u2019s paper. In models that are treating very long input sequences, the
conventional position id encodings store an embedings vector of size `),wo=R(j),xo=n(j," being the "),Ut=r(j,"CODE",{});var rd=i(Ut);ml=n(rd,"config.hidden_size"),rd.forEach(a),cl=n(j,` for
every position `),zo=R(j),$o=n(j,", with "),To=R(j),Mo=n(j," being "),Xt=r(j,"CODE",{});var id=i(Xt);hl=n(id,"config.max_embedding_size"),id.forEach(a),dl=n(j,`. This means that having
a sequence length of `),Ro=R(j),qo=n(j," and a "),Vt=r(j,"CODE",{});var ld=i(Vt);gl=n(ld,"config.hidden_size"),ld.forEach(a),ul=n(j," of "),Eo=R(j),jo=n(j,`
would result in a position encoding matrix:
`),Lo=R(j),j.forEach(a),Fo=d(e),$s=r(e,"P",{});var po=i($s);fl=n(po,"which alone has over 500M parameters to store. Axial positional encodings factorize "),Ao=R(po),Co=n(po,` into two matrices:
`),Po=R(po),po.forEach(a),So=d(e),We=r(e,"P",{});var Uh=i(We);_l=n(Uh,`and
`),Oo=R(Uh),Uh.forEach(a),No=d(e),Qe=r(e,"P",{});var Xh=i(Qe);vl=n(Xh,`with:
`),Do=R(Xh),Xh.forEach(a),Ho=d(e),Ke=r(e,"P",{});var Vh=i(Ke);bl=n(Vh,`Therefore the following holds:
`),Io=R(Vh),Vh.forEach(a),Wo=d(e),F=r(e,"P",{});var K=i(F);yl=n(K,"Intuitively, this means that a position embedding vector "),Qo=R(K),Ko=n(K,` is now the composition of two
factorized embedding vectors: `),Uo=R(K),Xo=n(K,", where as the "),Bt=r(K,"CODE",{});var pd=i(Bt);kl=n(pd,"config.max_embedding_size"),pd.forEach(a),wl=n(K,` dimension
`),Vo=R(K),Bo=n(K," is factorized into "),Go=R(K),Jo=n(K,`. This design ensures that each position embedding vector
`),Yo=R(K),Zo=n(K," is unique."),K.forEach(a),sr=d(e),hs=r(e,"P",{});var kt=i(hs);xl=n(kt,"Using the above example again, axial position encoding with "),er=R(kt),ar=n(kt,`
can drastically reduced the number of parameters to `),tr=R(kt),nr=n(kt," parameters."),kt.forEach(a),or=d(e),E=r(e,"P",{});var C=i(E);zl=n(C,"In practice, the parameter "),Gt=r(C,"CODE",{});var md=i(Gt);$l=n(md,"config.axial_pos_embds_dim"),md.forEach(a),Tl=n(C," is set to a tuple "),rr=R(C),ir=n(C,` which sum has to be
equal to `),Jt=r(C,"CODE",{});var cd=i(Jt);Ml=n(cd,"config.hidden_size"),cd.forEach(a),Rl=n(C," and "),Yt=r(C,"CODE",{});var hd=i(Yt);ql=n(hd,"config.axial_pos_shape"),hd.forEach(a),El=n(C," is set to a tuple "),lr=R(C),pr=n(C,` which
product has to be equal to `),Zt=r(C,"CODE",{});var dd=i(Zt);jl=n(dd,"config.max_embedding_size"),dd.forEach(a),Ll=n(C,", which during training has to be equal to the "),sn=r(C,"EM",{});var gd=i(sn);Fl=n(gd,`sequence
length`),gd.forEach(a),Al=n(C," of the "),en=r(C,"CODE",{});var ud=i(en);Cl=n(ud,"input_ids"),ud.forEach(a),Pl=n(C,"."),C.forEach(a),mr=d(e),Ts=r(e,"H2",{class:!0});var ri=i(Ts);Js=r(ri,"A",{id:!0,class:!0,href:!0});var fd=i(Js);an=r(fd,"SPAN",{});var _d=i(an);k(Ue.$$.fragment,_d),_d.forEach(a),fd.forEach(a),Sl=d(ri),tn=r(ri,"SPAN",{});var vd=i(tn);Ol=n(vd,"LSH Self Attention"),vd.forEach(a),ri.forEach(a),cr=d(e),J=r(e,"P",{});var Te=i(J);Nl=n(Te,`In Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key
query embedding vectors are also tied. LSH self attention uses the locality sensitive hashing mechanism proposed in
`),Xe=r(Te,"A",{href:!0,rel:!0});var bd=i(Xe);Dl=n(bd,"Practical and Optimal LSH for Angular Distance"),bd.forEach(a),Hl=n(Te,` to assign each of the tied key
query embedding vectors to one of `),nn=r(Te,"CODE",{});var yd=i(nn);Il=n(yd,"config.num_buckets"),yd.forEach(a),Wl=n(Te,` possible buckets. The premise is that the more \u201Csimilar\u201D
key query embedding vectors (in terms of `),on=r(Te,"EM",{});var kd=i(on);Ql=n(kd,"cosine similarity"),kd.forEach(a),Kl=n(Te,`) are to each other, the more likely they are assigned to
the same bucket.`),Te.forEach(a),hr=d(e),O=r(e,"P",{});var ts=i(O);Ul=n(ts,"The accuracy of the LSH mechanism can be improved by increasing "),rn=r(ts,"CODE",{});var wd=i(rn);Xl=n(wd,"config.num_hashes"),wd.forEach(a),Vl=n(ts,` or directly the argument
`),ln=r(ts,"CODE",{});var xd=i(ln);Bl=n(xd,"num_hashes"),xd.forEach(a),Gl=n(ts,` of the forward function so that the output of the LSH self attention better approximates the output
of the \u201Cnormal\u201D full self attention. The buckets are then sorted and chunked into query key embedding vector chunks
each of length `),pn=r(ts,"CODE",{});var zd=i(pn);Jl=n(zd,"config.lsh_chunk_length"),zd.forEach(a),Yl=n(ts,`. For each chunk, the query embedding vectors attend to its key vectors
(which are tied to themselves) and to the key embedding vectors of `),mn=r(ts,"CODE",{});var $d=i(mn);Zl=n($d,"config.lsh_num_chunks_before"),$d.forEach(a),sp=n(ts,` previous
neighboring chunks and `),cn=r(ts,"CODE",{});var Td=i(cn);ep=n(Td,"config.lsh_num_chunks_after"),Td.forEach(a),ap=n(ts," following neighboring chunks."),ts.forEach(a),dr=d(e),ds=r(e,"P",{});var wt=i(ds);tp=n(wt,"For more information, see the "),Ve=r(wt,"A",{href:!0,rel:!0});var Md=i(Ve);np=n(Md,"original Paper"),Md.forEach(a),op=n(wt," or this great "),Be=r(wt,"A",{href:!0,rel:!0});var Rd=i(Be);rp=n(Rd,"blog post"),Rd.forEach(a),ip=n(wt,"."),wt.forEach(a),gr=d(e),I=r(e,"P",{});var fs=i(I);lp=n(fs,"Note that "),hn=r(fs,"CODE",{});var qd=i(hn);pp=n(qd,"config.num_buckets"),qd.forEach(a),mp=n(fs," can also be factorized into a list "),ur=R(fs),fr=n(fs,". This way instead of assigning the query key embedding vectors to one of "),_r=R(fs),vr=n(fs," they are assigned to one of "),br=R(fs),yr=n(fs,`. This is crucial for very long sequences to
save memory.`),fs.forEach(a),kr=d(e),gs=r(e,"P",{});var xt=i(gs);cp=n(xt,"When training a model from scratch, it is recommended to leave "),dn=r(xt,"CODE",{});var Ed=i(dn);hp=n(Ed,"config.num_buckets=None"),Ed.forEach(a),dp=n(xt,`, so that depending on the
sequence length a good value for `),gn=r(xt,"CODE",{});var jd=i(gn);gp=n(jd,"num_buckets"),jd.forEach(a),up=n(xt,` is calculated on the fly. This value will then automatically be
saved in the config and should be reused for inference.`),xt.forEach(a),wr=d(e),Y=r(e,"P",{});var Me=i(Y);fp=n(Me,`Using LSH self attention, the memory and time complexity of the query-key matmul operation can be reduced from
`),xr=R(Me),zr=n(Me," to "),$r=R(Me),Tr=n(Me,`, which usually represents the memory
and time bottleneck in a transformer model, with `),Mr=R(Me),Rr=n(Me," being the sequence length."),Me.forEach(a),qr=d(e),Ms=r(e,"H2",{class:!0});var ii=i(Ms);Ys=r(ii,"A",{id:!0,class:!0,href:!0});var Ld=i(Ys);un=r(Ld,"SPAN",{});var Fd=i(un);k(Ge.$$.fragment,Fd),Fd.forEach(a),Ld.forEach(a),_p=d(ii),fn=r(ii,"SPAN",{});var Ad=i(fn);vp=n(Ad,"Local Self Attention"),Ad.forEach(a),ii.forEach(a),Er=d(e),Z=r(e,"P",{});var Re=i(Z);bp=n(Re,`Local self attention is essentially a \u201Cnormal\u201D self attention layer with key, query and value projections, but is
chunked so that in each chunk of length `),_n=r(Re,"CODE",{});var Cd=i(_n);yp=n(Cd,"config.local_chunk_length"),Cd.forEach(a),kp=n(Re,` the query embedding vectors only attends to
the key embedding vectors in its chunk and to the key embedding vectors of `),vn=r(Re,"CODE",{});var Pd=i(vn);wp=n(Pd,"config.local_num_chunks_before"),Pd.forEach(a),xp=n(Re,`
previous neighboring chunks and `),bn=r(Re,"CODE",{});var Sd=i(bn);zp=n(Sd,"config.local_num_chunks_after"),Sd.forEach(a),$p=n(Re," following neighboring chunks."),Re.forEach(a),jr=d(e),ss=r(e,"P",{});var qe=i(ss);Tp=n(qe,`Using Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from
`),Lr=R(qe),Fr=n(qe," to "),Ar=R(qe),Cr=n(qe,`, which usually represents the memory
and time bottleneck in a transformer model, with `),Pr=R(qe),Sr=n(qe," being the sequence length."),qe.forEach(a),Or=d(e),Rs=r(e,"H2",{class:!0});var li=i(Rs);Zs=r(li,"A",{id:!0,class:!0,href:!0});var Od=i(Zs);yn=r(Od,"SPAN",{});var Nd=i(yn);k(Je.$$.fragment,Nd),Nd.forEach(a),Od.forEach(a),Mp=d(li),kn=r(li,"SPAN",{});var Dd=i(kn);Rp=n(Dd,"Training"),Dd.forEach(a),li.forEach(a),Nr=d(e),us=r(e,"P",{});var zt=i(us);qp=n(zt,`During training, we must ensure that the sequence length is set to a value that can be divided by the least common
multiple of `),wn=r(zt,"CODE",{});var Hd=i(wn);Ep=n(Hd,"config.lsh_chunk_length"),Hd.forEach(a),jp=n(zt," and "),xn=r(zt,"CODE",{});var Id=i(xn);Lp=n(Id,"config.local_chunk_length"),Id.forEach(a),Fp=n(zt,` and that the parameters of the Axial
Positional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can
easily be trained on sequences as long as 64000 tokens.`),zt.forEach(a),Dr=d(e),se=r(e,"P",{});var pi=i(se);Ap=n(pi,"For training, the "),tt=r(pi,"A",{href:!0});var Wd=i(tt);Cp=n(Wd,"ReformerModelWithLMHead"),Wd.forEach(a),Pp=n(pi," should be used as follows:"),pi.forEach(a),Hr=d(e),k(Ye.$$.fragment,e),Ir=d(e),qs=r(e,"H2",{class:!0});var mi=i(qs);ee=r(mi,"A",{id:!0,class:!0,href:!0});var Qd=i(ee);zn=r(Qd,"SPAN",{});var Kd=i(zn);k(Ze.$$.fragment,Kd),Kd.forEach(a),Qd.forEach(a),Sp=d(mi),$n=r(mi,"SPAN",{});var Ud=i($n);Op=n(Ud,"ReformerConfig"),Ud.forEach(a),mi.forEach(a),Wr=d(e),B=r(e,"DIV",{class:!0});var Ee=i(B);k(sa.$$.fragment,Ee),Np=d(Ee),Es=r(Ee,"P",{});var $t=i(Es);Dp=n($t,"This is the configuration class to store the configuration of a "),nt=r($t,"A",{href:!0});var Xd=i(nt);Hp=n(Xd,"ReformerModel"),Xd.forEach(a),Ip=n($t,`. It is used to instantiate a
Reformer model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the ReFormer
`),ea=r($t,"A",{href:!0,rel:!0});var Vd=i(ea);Wp=n(Vd,"google/reformer-crime-and-punishment"),Vd.forEach(a),Qp=n($t," architecture."),$t.forEach(a),Kp=d(Ee),js=r(Ee,"P",{});var Tt=i(js);Up=n(Tt,"Configuration objects inherit from "),ot=r(Tt,"A",{href:!0});var Bd=i(ot);Xp=n(Bd,"PretrainedConfig"),Bd.forEach(a),Vp=n(Tt,` and can be used to control the model outputs. Read the
documentation from `),rt=r(Tt,"A",{href:!0});var Gd=i(rt);Bp=n(Gd,"PretrainedConfig"),Gd.forEach(a),Gp=n(Tt," for more information."),Tt.forEach(a),Jp=d(Ee),k(ae.$$.fragment,Ee),Ee.forEach(a),Qr=d(e),Ls=r(e,"H2",{class:!0});var ci=i(Ls);te=r(ci,"A",{id:!0,class:!0,href:!0});var Jd=i(te);Tn=r(Jd,"SPAN",{});var Yd=i(Tn);k(aa.$$.fragment,Yd),Yd.forEach(a),Jd.forEach(a),Yp=d(ci),Mn=r(ci,"SPAN",{});var Zd=i(Mn);Zp=n(Zd,"ReformerTokenizer"),Zd.forEach(a),ci.forEach(a),Kr=d(e),G=r(e,"DIV",{class:!0});var je=i(G);k(ta.$$.fragment,je),sm=d(je),na=r(je,"P",{});var hi=i(na);em=n(hi,"Construct a Reformer tokenizer. Based on "),oa=r(hi,"A",{href:!0,rel:!0});var sg=i(oa);am=n(sg,"SentencePiece"),sg.forEach(a),tm=n(hi," ."),hi.forEach(a),nm=d(je),ra=r(je,"P",{});var di=i(ra);om=n(di,"This tokenizer inherits from "),it=r(di,"A",{href:!0});var eg=i(it);rm=n(eg,"PreTrainedTokenizer"),eg.forEach(a),im=n(di,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),di.forEach(a),lm=d(je),lt=r(je,"DIV",{class:!0});var ag=i(lt);k(ia.$$.fragment,ag),ag.forEach(a),je.forEach(a),Ur=d(e),Fs=r(e,"H2",{class:!0});var gi=i(Fs);ne=r(gi,"A",{id:!0,class:!0,href:!0});var tg=i(ne);Rn=r(tg,"SPAN",{});var ng=i(Rn);k(la.$$.fragment,ng),ng.forEach(a),tg.forEach(a),pm=d(gi),qn=r(gi,"SPAN",{});var og=i(qn);mm=n(og,"ReformerTokenizerFast"),og.forEach(a),gi.forEach(a),Xr=d(e),is=r(e,"DIV",{class:!0});var Mt=i(is);k(pa.$$.fragment,Mt),cm=d(Mt),As=r(Mt,"P",{});var Rt=i(As);hm=n(Rt,"Construct a \u201Cfast\u201D Reformer tokenizer (backed by HuggingFace\u2019s "),En=r(Rt,"EM",{});var rg=i(En);dm=n(rg,"tokenizers"),rg.forEach(a),gm=n(Rt,` library). Based on
`),ma=r(Rt,"A",{href:!0,rel:!0});var ig=i(ma);um=n(ig,"Unigram"),ig.forEach(a),fm=n(Rt,"."),Rt.forEach(a),_m=d(Mt),ca=r(Mt,"P",{});var ui=i(ca);vm=n(ui,"This tokenizer inherits from "),pt=r(ui,"A",{href:!0});var lg=i(pt);bm=n(lg,"PreTrainedTokenizerFast"),lg.forEach(a),ym=n(ui,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),ui.forEach(a),Mt.forEach(a),Vr=d(e),Cs=r(e,"H2",{class:!0});var fi=i(Cs);oe=r(fi,"A",{id:!0,class:!0,href:!0});var pg=i(oe);jn=r(pg,"SPAN",{});var mg=i(jn);k(ha.$$.fragment,mg),mg.forEach(a),pg.forEach(a),km=d(fi),Ln=r(fi,"SPAN",{});var cg=i(Ln);wm=n(cg,"ReformerModel"),cg.forEach(a),fi.forEach(a),Br=d(e),N=r(e,"DIV",{class:!0});var _s=i(N);k(da.$$.fragment,_s),xm=d(_s),ga=r(_s,"P",{});var _i=i(ga);zm=n(_i,`The bare Reformer Model transformer outputting raw hidden-stateswithout any specific head on top.
Reformer was proposed in `),ua=r(_i,"A",{href:!0,rel:!0});var hg=i(ua);$m=n(hg,"Reformer: The Efficient Transformer"),hg.forEach(a),Tm=n(_i,` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),_i.forEach(a),Mm=d(_s),fa=r(_s,"P",{});var vi=i(fa);Rm=n(vi,"This model inherits from "),mt=r(vi,"A",{href:!0});var dg=i(mt);qm=n(dg,"PreTrainedModel"),dg.forEach(a),Em=n(vi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vi.forEach(a),jm=d(_s),_a=r(_s,"P",{});var bi=i(_a);Lm=n(bi,"This model is also a PyTorch "),va=r(bi,"A",{href:!0,rel:!0});var gg=i(va);Fm=n(gg,"torch.nn.Module"),gg.forEach(a),Am=n(bi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),bi.forEach(a),Cm=d(_s),es=r(_s,"DIV",{class:!0});var Le=i(es);k(ba.$$.fragment,Le),Pm=d(Le),Ps=r(Le,"P",{});var qt=i(Ps);Sm=n(qt,"The "),ct=r(qt,"A",{href:!0});var ug=i(ct);Om=n(ug,"ReformerModel"),ug.forEach(a),Nm=n(qt," forward method, overrides the "),Fn=r(qt,"CODE",{});var fg=i(Fn);Dm=n(fg,"__call__"),fg.forEach(a),Hm=n(qt," special method."),qt.forEach(a),Im=d(Le),k(re.$$.fragment,Le),Wm=d(Le),k(ie.$$.fragment,Le),Le.forEach(a),_s.forEach(a),Gr=d(e),Ss=r(e,"H2",{class:!0});var yi=i(Ss);le=r(yi,"A",{id:!0,class:!0,href:!0});var _g=i(le);An=r(_g,"SPAN",{});var vg=i(An);k(ya.$$.fragment,vg),vg.forEach(a),_g.forEach(a),Qm=d(yi),Cn=r(yi,"SPAN",{});var bg=i(Cn);Km=n(bg,"ReformerModelWithLMHead"),bg.forEach(a),yi.forEach(a),Jr=d(e),D=r(e,"DIV",{class:!0});var vs=i(D);k(ka.$$.fragment,vs),Um=d(vs),Os=r(vs,"P",{});var Et=i(Os);Xm=n(Et,"Reformer Model with a "),Pn=r(Et,"CODE",{});var yg=i(Pn);Vm=n(yg,"language modeling"),yg.forEach(a),Bm=n(Et,` head on top.
Reformer was proposed in `),wa=r(Et,"A",{href:!0,rel:!0});var kg=i(wa);Gm=n(kg,"Reformer: The Efficient Transformer"),kg.forEach(a),Jm=n(Et,` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Et.forEach(a),Ym=d(vs),xa=r(vs,"P",{});var ki=i(xa);Zm=n(ki,"This model inherits from "),ht=r(ki,"A",{href:!0});var wg=i(ht);sc=n(wg,"PreTrainedModel"),wg.forEach(a),ec=n(ki,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ki.forEach(a),ac=d(vs),za=r(vs,"P",{});var wi=i(za);tc=n(wi,"This model is also a PyTorch "),$a=r(wi,"A",{href:!0,rel:!0});var xg=i($a);nc=n(xg,"torch.nn.Module"),xg.forEach(a),oc=n(wi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),wi.forEach(a),rc=d(vs),as=r(vs,"DIV",{class:!0});var Fe=i(as);k(Ta.$$.fragment,Fe),ic=d(Fe),Ns=r(Fe,"P",{});var jt=i(Ns);lc=n(jt,"The "),dt=r(jt,"A",{href:!0});var zg=i(dt);pc=n(zg,"ReformerModelWithLMHead"),zg.forEach(a),mc=n(jt," forward method, overrides the "),Sn=r(jt,"CODE",{});var $g=i(Sn);cc=n($g,"__call__"),$g.forEach(a),hc=n(jt," special method."),jt.forEach(a),dc=d(Fe),k(pe.$$.fragment,Fe),gc=d(Fe),k(me.$$.fragment,Fe),Fe.forEach(a),vs.forEach(a),Yr=d(e),Ds=r(e,"H2",{class:!0});var xi=i(Ds);ce=r(xi,"A",{id:!0,class:!0,href:!0});var Tg=i(ce);On=r(Tg,"SPAN",{});var Mg=i(On);k(Ma.$$.fragment,Mg),Mg.forEach(a),Tg.forEach(a),uc=d(xi),Nn=r(xi,"SPAN",{});var Rg=i(Nn);fc=n(Rg,"ReformerForMaskedLM"),Rg.forEach(a),xi.forEach(a),Zr=d(e),H=r(e,"DIV",{class:!0});var bs=i(H);k(Ra.$$.fragment,bs),_c=d(bs),Hs=r(bs,"P",{});var Lt=i(Hs);vc=n(Lt,"Reformer Model with a "),Dn=r(Lt,"CODE",{});var qg=i(Dn);bc=n(qg,"language modeling"),qg.forEach(a),yc=n(Lt,` head on top.
Reformer was proposed in `),qa=r(Lt,"A",{href:!0,rel:!0});var Eg=i(qa);kc=n(Eg,"Reformer: The Efficient Transformer"),Eg.forEach(a),wc=n(Lt,` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Lt.forEach(a),xc=d(bs),Ea=r(bs,"P",{});var zi=i(Ea);zc=n(zi,"This model inherits from "),gt=r(zi,"A",{href:!0});var jg=i(gt);$c=n(jg,"PreTrainedModel"),jg.forEach(a),Tc=n(zi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zi.forEach(a),Mc=d(bs),ja=r(bs,"P",{});var $i=i(ja);Rc=n($i,"This model is also a PyTorch "),La=r($i,"A",{href:!0,rel:!0});var Lg=i(La);qc=n(Lg,"torch.nn.Module"),Lg.forEach(a),Ec=n($i,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$i.forEach(a),jc=d(bs),W=r(bs,"DIV",{class:!0});var ys=i(W);k(Fa.$$.fragment,ys),Lc=d(ys),Is=r(ys,"P",{});var Ft=i(Is);Fc=n(Ft,"The "),ut=r(Ft,"A",{href:!0});var Fg=i(ut);Ac=n(Fg,"ReformerForMaskedLM"),Fg.forEach(a),Cc=n(Ft," forward method, overrides the "),Hn=r(Ft,"CODE",{});var Ag=i(Hn);Pc=n(Ag,"__call__"),Ag.forEach(a),Sc=n(Ft," special method."),Ft.forEach(a),Oc=d(ys),k(he.$$.fragment,ys),Nc=d(ys),k(de.$$.fragment,ys),Dc=d(ys),k(ge.$$.fragment,ys),ys.forEach(a),bs.forEach(a),si=d(e),Ws=r(e,"H2",{class:!0});var Ti=i(Ws);ue=r(Ti,"A",{id:!0,class:!0,href:!0});var Cg=i(ue);In=r(Cg,"SPAN",{});var Pg=i(In);k(Aa.$$.fragment,Pg),Pg.forEach(a),Cg.forEach(a),Hc=d(Ti),Wn=r(Ti,"SPAN",{});var Sg=i(Wn);Ic=n(Sg,"ReformerForSequenceClassification"),Sg.forEach(a),Ti.forEach(a),ei=d(e),P=r(e,"DIV",{class:!0});var ns=i(P);k(Ca.$$.fragment,ns),Wc=d(ns),Qn=r(ns,"P",{});var Og=i(Qn);Qc=n(Og,`Reformer Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Og.forEach(a),Kc=d(ns),Pa=r(ns,"P",{});var Mi=i(Pa);Uc=n(Mi,"Reformer was proposed in "),Sa=r(Mi,"A",{href:!0,rel:!0});var Ng=i(Sa);Xc=n(Ng,"Reformer: The Efficient Transformer"),Ng.forEach(a),Vc=n(Mi,` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),Mi.forEach(a),Bc=d(ns),Oa=r(ns,"P",{});var Ri=i(Oa);Gc=n(Ri,"This model inherits from "),ft=r(Ri,"A",{href:!0});var Dg=i(ft);Jc=n(Dg,"PreTrainedModel"),Dg.forEach(a),Yc=n(Ri,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ri.forEach(a),Zc=d(ns),Na=r(ns,"P",{});var qi=i(Na);sh=n(qi,"This model is also a PyTorch "),Da=r(qi,"A",{href:!0,rel:!0});var Hg=i(Da);eh=n(Hg,"torch.nn.Module"),Hg.forEach(a),ah=n(qi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qi.forEach(a),th=d(ns),A=r(ns,"DIV",{class:!0});var U=i(A);k(Ha.$$.fragment,U),nh=d(U),Qs=r(U,"P",{});var At=i(Qs);oh=n(At,"The "),_t=r(At,"A",{href:!0});var Ig=i(_t);rh=n(Ig,"ReformerForSequenceClassification"),Ig.forEach(a),ih=n(At," forward method, overrides the "),Kn=r(At,"CODE",{});var Wg=i(Kn);lh=n(Wg,"__call__"),Wg.forEach(a),ph=n(At," special method."),At.forEach(a),mh=d(U),k(fe.$$.fragment,U),ch=d(U),k(_e.$$.fragment,U),hh=d(U),k(ve.$$.fragment,U),dh=d(U),k(be.$$.fragment,U),gh=d(U),k(ye.$$.fragment,U),U.forEach(a),ns.forEach(a),ai=d(e),Ks=r(e,"H2",{class:!0});var Ei=i(Ks);ke=r(Ei,"A",{id:!0,class:!0,href:!0});var Qg=i(ke);Un=r(Qg,"SPAN",{});var Kg=i(Un);k(Ia.$$.fragment,Kg),Kg.forEach(a),Qg.forEach(a),uh=d(Ei),Xn=r(Ei,"SPAN",{});var Ug=i(Xn);fh=n(Ug,"ReformerForQuestionAnswering"),Ug.forEach(a),Ei.forEach(a),ti=d(e),S=r(e,"DIV",{class:!0});var os=i(S);k(Wa.$$.fragment,os),_h=d(os),Us=r(os,"P",{});var Ct=i(Us);vh=n(Ct,`Reformer Model with a span classification head on top for extractive question-answering tasks like SQuAD / TriviaQA
( a linear layer on top of hidden-states output to compute `),Vn=r(Ct,"CODE",{});var Xg=i(Vn);bh=n(Xg,"span start logits"),Xg.forEach(a),yh=n(Ct," and "),Bn=r(Ct,"CODE",{});var Vg=i(Bn);kh=n(Vg,"span end logits"),Vg.forEach(a),wh=n(Ct,"."),Ct.forEach(a),xh=d(os),Qa=r(os,"P",{});var ji=i(Qa);zh=n(ji,"Reformer was proposed in "),Ka=r(ji,"A",{href:!0,rel:!0});var Bg=i(Ka);$h=n(Bg,"Reformer: The Efficient Transformer"),Bg.forEach(a),Th=n(ji,` by Nikita Kitaev,
\u0141ukasz Kaiser, Anselm Levskaya.`),ji.forEach(a),Mh=d(os),Ua=r(os,"P",{});var Li=i(Ua);Rh=n(Li,"This model inherits from "),vt=r(Li,"A",{href:!0});var Gg=i(vt);qh=n(Gg,"PreTrainedModel"),Gg.forEach(a),Eh=n(Li,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Li.forEach(a),jh=d(os),Xa=r(os,"P",{});var Fi=i(Xa);Lh=n(Fi,"This model is also a PyTorch "),Va=r(Fi,"A",{href:!0,rel:!0});var Jg=i(Va);Fh=n(Jg,"torch.nn.Module"),Jg.forEach(a),Ah=n(Fi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Fi.forEach(a),Ch=d(os),Q=r(os,"DIV",{class:!0});var ks=i(Q);k(Ba.$$.fragment,ks),Ph=d(ks),Xs=r(ks,"P",{});var Pt=i(Xs);Sh=n(Pt,"The "),bt=r(Pt,"A",{href:!0});var Yg=i(bt);Oh=n(Yg,"ReformerForQuestionAnswering"),Yg.forEach(a),Nh=n(Pt," forward method, overrides the "),Gn=r(Pt,"CODE",{});var Zg=i(Gn);Dh=n(Zg,"__call__"),Zg.forEach(a),Hh=n(Pt," special method."),Pt.forEach(a),Ih=d(ks),k(we.$$.fragment,ks),Wh=d(ks),k(xe.$$.fragment,ks),Qh=d(ks),k(ze.$$.fragment,ks),ks.forEach(a),os.forEach(a),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Ju)),c(f,"id","reformer"),c(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f,"href","#reformer"),c(g,"class","relative group"),c(Ae,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),c(Ae,"rel","nofollow"),c(Vs,"id","overview"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#overview"),c(xs,"class","relative group"),c(Pe,"href","https://arxiv.org/abs/2001.04451.pdf"),c(Pe,"rel","nofollow"),c(Se,"href","https://huggingface.co/patrickvonplaten"),c(Se,"rel","nofollow"),c(Oe,"href","https://github.com/google/trax/tree/master/trax/models/reformer"),c(Oe,"rel","nofollow"),c(De,"href","https://github.com/pytorch/pytorch/issues/36035"),c(De,"rel","nofollow"),c(Gs,"id","axial-positional-encodings"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#axial-positional-encodings"),c(zs,"class","relative group"),c(Ie,"href","https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29"),c(Ie,"rel","nofollow"),wo.a=xo,zo.a=$o,To.a=Mo,Ro.a=qo,Eo.a=jo,Lo.a=null,Ao.a=Co,Po.a=null,Oo.a=null,Do.a=null,Io.a=null,Qo.a=Ko,Uo.a=Xo,Vo.a=Bo,Go.a=Jo,Yo.a=Zo,er.a=ar,tr.a=nr,rr.a=ir,lr.a=pr,c(Js,"id","lsh-self-attention"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#lsh-self-attention"),c(Ts,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/1509.02897"),c(Xe,"rel","nofollow"),c(Ve,"href","https://arxiv.org/abs/2001.04451"),c(Ve,"rel","nofollow"),c(Be,"href","https://www.pragmatic.ml/reformer-deep-dive/"),c(Be,"rel","nofollow"),ur.a=fr,_r.a=vr,br.a=yr,xr.a=zr,$r.a=Tr,Mr.a=Rr,c(Ys,"id","local-self-attention"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#local-self-attention"),c(Ms,"class","relative group"),Lr.a=Fr,Ar.a=Cr,Pr.a=Sr,c(Zs,"id","training"),c(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zs,"href","#training"),c(Rs,"class","relative group"),c(tt,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(ee,"id","transformers.ReformerConfig"),c(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ee,"href","#transformers.ReformerConfig"),c(qs,"class","relative group"),c(nt,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModel"),c(ea,"href","https://huggingface.co/google/reformer-crime-and-punishment"),c(ea,"rel","nofollow"),c(ot,"href","/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig"),c(rt,"href","/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig"),c(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(te,"id","transformers.ReformerTokenizer"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#transformers.ReformerTokenizer"),c(Ls,"class","relative group"),c(oa,"href","https://github.com/google/sentencepiece"),c(oa,"rel","nofollow"),c(it,"href","/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ne,"id","transformers.ReformerTokenizerFast"),c(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ne,"href","#transformers.ReformerTokenizerFast"),c(Fs,"class","relative group"),c(ma,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),c(ma,"rel","nofollow"),c(pt,"href","/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(is,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(oe,"id","transformers.ReformerModel"),c(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oe,"href","#transformers.ReformerModel"),c(Cs,"class","relative group"),c(ua,"href","https://arxiv.org/abs/2001.04451"),c(ua,"rel","nofollow"),c(mt,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),c(va,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(va,"rel","nofollow"),c(ct,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModel"),c(es,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(le,"id","transformers.ReformerModelWithLMHead"),c(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(le,"href","#transformers.ReformerModelWithLMHead"),c(Ss,"class","relative group"),c(wa,"href","https://arxiv.org/abs/2001.04451"),c(wa,"rel","nofollow"),c(ht,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),c($a,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($a,"rel","nofollow"),c(dt,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(as,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ce,"id","transformers.ReformerForMaskedLM"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#transformers.ReformerForMaskedLM"),c(Ds,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2001.04451"),c(qa,"rel","nofollow"),c(gt,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),c(La,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(La,"rel","nofollow"),c(ut,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ue,"id","transformers.ReformerForSequenceClassification"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#transformers.ReformerForSequenceClassification"),c(Ws,"class","relative group"),c(Sa,"href","https://arxiv.org/abs/2001.04451"),c(Sa,"rel","nofollow"),c(ft,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),c(Da,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Da,"rel","nofollow"),c(_t,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ke,"id","transformers.ReformerForQuestionAnswering"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#transformers.ReformerForQuestionAnswering"),c(Ks,"class","relative group"),c(Ka,"href","https://arxiv.org/abs/2001.04451"),c(Ka,"rel","nofollow"),c(vt,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),c(Va,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Va,"rel","nofollow"),c(bt,"href","/docs/transformers/pr_17196/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,m){s(document.head,p),u(e,v,m),u(e,g,m),s(g,f),s(f,b),w(l,b,null),s(g,_),s(g,L),s(L,Ai),u(e,co,m),u(e,ws,m),s(ws,St),s(St,Ci),s(ws,Pi),s(ws,Ae),s(Ae,Si),s(ws,Oi),u(e,ho,m),u(e,xs,m),s(xs,Vs),s(Vs,Ot),w(Ce,Ot,null),s(xs,Ni),s(xs,Nt),s(Nt,Di),u(e,go,m),u(e,Bs,m),s(Bs,Hi),s(Bs,Pe),s(Pe,Ii),s(Bs,Wi),u(e,uo,m),u(e,st,m),s(st,Qi),u(e,fo,m),u(e,et,m),s(et,Dt),s(Dt,Ki),u(e,_o,m),u(e,ms,m),s(ms,Ui),s(ms,Se),s(Se,Xi),s(ms,Vi),s(ms,Oe),s(Oe,Bi),s(ms,Gi),u(e,vo,m),u(e,Ne,m),s(Ne,Ht),s(Ht,Ji),s(Ne,Yi),u(e,bo,m),u(e,at,m),s(at,cs),s(cs,Zi),s(cs,It),s(It,sl),s(cs,el),s(cs,Wt),s(Wt,al),s(cs,tl),s(cs,De),s(De,nl),u(e,yo,m),u(e,zs,m),s(zs,Gs),s(Gs,Qt),w(He,Qt,null),s(zs,ol),s(zs,Kt),s(Kt,rl),u(e,ko,m),u(e,q,m),s(q,il),s(q,Ie),s(Ie,ll),s(q,pl),wo.m(su,q),s(q,xo),s(q,Ut),s(Ut,ml),s(q,cl),zo.m(eu,q),s(q,$o),To.m(au,q),s(q,Mo),s(q,Xt),s(Xt,hl),s(q,dl),Ro.m(tu,q),s(q,qo),s(q,Vt),s(Vt,gl),s(q,ul),Eo.m(nu,q),s(q,jo),Lo.m(ou,q),u(e,Fo,m),u(e,$s,m),s($s,fl),Ao.m(ru,$s),s($s,Co),Po.m(iu,$s),u(e,So,m),u(e,We,m),s(We,_l),Oo.m(lu,We),u(e,No,m),u(e,Qe,m),s(Qe,vl),Do.m(pu,Qe),u(e,Ho,m),u(e,Ke,m),s(Ke,bl),Io.m(mu,Ke),u(e,Wo,m),u(e,F,m),s(F,yl),Qo.m(cu,F),s(F,Ko),Uo.m(hu,F),s(F,Xo),s(F,Bt),s(Bt,kl),s(F,wl),Vo.m(du,F),s(F,Bo),Go.m(gu,F),s(F,Jo),Yo.m(uu,F),s(F,Zo),u(e,sr,m),u(e,hs,m),s(hs,xl),er.m(fu,hs),s(hs,ar),tr.m(_u,hs),s(hs,nr),u(e,or,m),u(e,E,m),s(E,zl),s(E,Gt),s(Gt,$l),s(E,Tl),rr.m(vu,E),s(E,ir),s(E,Jt),s(Jt,Ml),s(E,Rl),s(E,Yt),s(Yt,ql),s(E,El),lr.m(bu,E),s(E,pr),s(E,Zt),s(Zt,jl),s(E,Ll),s(E,sn),s(sn,Fl),s(E,Al),s(E,en),s(en,Cl),s(E,Pl),u(e,mr,m),u(e,Ts,m),s(Ts,Js),s(Js,an),w(Ue,an,null),s(Ts,Sl),s(Ts,tn),s(tn,Ol),u(e,cr,m),u(e,J,m),s(J,Nl),s(J,Xe),s(Xe,Dl),s(J,Hl),s(J,nn),s(nn,Il),s(J,Wl),s(J,on),s(on,Ql),s(J,Kl),u(e,hr,m),u(e,O,m),s(O,Ul),s(O,rn),s(rn,Xl),s(O,Vl),s(O,ln),s(ln,Bl),s(O,Gl),s(O,pn),s(pn,Jl),s(O,Yl),s(O,mn),s(mn,Zl),s(O,sp),s(O,cn),s(cn,ep),s(O,ap),u(e,dr,m),u(e,ds,m),s(ds,tp),s(ds,Ve),s(Ve,np),s(ds,op),s(ds,Be),s(Be,rp),s(ds,ip),u(e,gr,m),u(e,I,m),s(I,lp),s(I,hn),s(hn,pp),s(I,mp),ur.m(yu,I),s(I,fr),_r.m(ku,I),s(I,vr),br.m(wu,I),s(I,yr),u(e,kr,m),u(e,gs,m),s(gs,cp),s(gs,dn),s(dn,hp),s(gs,dp),s(gs,gn),s(gn,gp),s(gs,up),u(e,wr,m),u(e,Y,m),s(Y,fp),xr.m(xu,Y),s(Y,zr),$r.m(zu,Y),s(Y,Tr),Mr.m($u,Y),s(Y,Rr),u(e,qr,m),u(e,Ms,m),s(Ms,Ys),s(Ys,un),w(Ge,un,null),s(Ms,_p),s(Ms,fn),s(fn,vp),u(e,Er,m),u(e,Z,m),s(Z,bp),s(Z,_n),s(_n,yp),s(Z,kp),s(Z,vn),s(vn,wp),s(Z,xp),s(Z,bn),s(bn,zp),s(Z,$p),u(e,jr,m),u(e,ss,m),s(ss,Tp),Lr.m(Tu,ss),s(ss,Fr),Ar.m(Mu,ss),s(ss,Cr),Pr.m(Ru,ss),s(ss,Sr),u(e,Or,m),u(e,Rs,m),s(Rs,Zs),s(Zs,yn),w(Je,yn,null),s(Rs,Mp),s(Rs,kn),s(kn,Rp),u(e,Nr,m),u(e,us,m),s(us,qp),s(us,wn),s(wn,Ep),s(us,jp),s(us,xn),s(xn,Lp),s(us,Fp),u(e,Dr,m),u(e,se,m),s(se,Ap),s(se,tt),s(tt,Cp),s(se,Pp),u(e,Hr,m),w(Ye,e,m),u(e,Ir,m),u(e,qs,m),s(qs,ee),s(ee,zn),w(Ze,zn,null),s(qs,Sp),s(qs,$n),s($n,Op),u(e,Wr,m),u(e,B,m),w(sa,B,null),s(B,Np),s(B,Es),s(Es,Dp),s(Es,nt),s(nt,Hp),s(Es,Ip),s(Es,ea),s(ea,Wp),s(Es,Qp),s(B,Kp),s(B,js),s(js,Up),s(js,ot),s(ot,Xp),s(js,Vp),s(js,rt),s(rt,Bp),s(js,Gp),s(B,Jp),w(ae,B,null),u(e,Qr,m),u(e,Ls,m),s(Ls,te),s(te,Tn),w(aa,Tn,null),s(Ls,Yp),s(Ls,Mn),s(Mn,Zp),u(e,Kr,m),u(e,G,m),w(ta,G,null),s(G,sm),s(G,na),s(na,em),s(na,oa),s(oa,am),s(na,tm),s(G,nm),s(G,ra),s(ra,om),s(ra,it),s(it,rm),s(ra,im),s(G,lm),s(G,lt),w(ia,lt,null),u(e,Ur,m),u(e,Fs,m),s(Fs,ne),s(ne,Rn),w(la,Rn,null),s(Fs,pm),s(Fs,qn),s(qn,mm),u(e,Xr,m),u(e,is,m),w(pa,is,null),s(is,cm),s(is,As),s(As,hm),s(As,En),s(En,dm),s(As,gm),s(As,ma),s(ma,um),s(As,fm),s(is,_m),s(is,ca),s(ca,vm),s(ca,pt),s(pt,bm),s(ca,ym),u(e,Vr,m),u(e,Cs,m),s(Cs,oe),s(oe,jn),w(ha,jn,null),s(Cs,km),s(Cs,Ln),s(Ln,wm),u(e,Br,m),u(e,N,m),w(da,N,null),s(N,xm),s(N,ga),s(ga,zm),s(ga,ua),s(ua,$m),s(ga,Tm),s(N,Mm),s(N,fa),s(fa,Rm),s(fa,mt),s(mt,qm),s(fa,Em),s(N,jm),s(N,_a),s(_a,Lm),s(_a,va),s(va,Fm),s(_a,Am),s(N,Cm),s(N,es),w(ba,es,null),s(es,Pm),s(es,Ps),s(Ps,Sm),s(Ps,ct),s(ct,Om),s(Ps,Nm),s(Ps,Fn),s(Fn,Dm),s(Ps,Hm),s(es,Im),w(re,es,null),s(es,Wm),w(ie,es,null),u(e,Gr,m),u(e,Ss,m),s(Ss,le),s(le,An),w(ya,An,null),s(Ss,Qm),s(Ss,Cn),s(Cn,Km),u(e,Jr,m),u(e,D,m),w(ka,D,null),s(D,Um),s(D,Os),s(Os,Xm),s(Os,Pn),s(Pn,Vm),s(Os,Bm),s(Os,wa),s(wa,Gm),s(Os,Jm),s(D,Ym),s(D,xa),s(xa,Zm),s(xa,ht),s(ht,sc),s(xa,ec),s(D,ac),s(D,za),s(za,tc),s(za,$a),s($a,nc),s(za,oc),s(D,rc),s(D,as),w(Ta,as,null),s(as,ic),s(as,Ns),s(Ns,lc),s(Ns,dt),s(dt,pc),s(Ns,mc),s(Ns,Sn),s(Sn,cc),s(Ns,hc),s(as,dc),w(pe,as,null),s(as,gc),w(me,as,null),u(e,Yr,m),u(e,Ds,m),s(Ds,ce),s(ce,On),w(Ma,On,null),s(Ds,uc),s(Ds,Nn),s(Nn,fc),u(e,Zr,m),u(e,H,m),w(Ra,H,null),s(H,_c),s(H,Hs),s(Hs,vc),s(Hs,Dn),s(Dn,bc),s(Hs,yc),s(Hs,qa),s(qa,kc),s(Hs,wc),s(H,xc),s(H,Ea),s(Ea,zc),s(Ea,gt),s(gt,$c),s(Ea,Tc),s(H,Mc),s(H,ja),s(ja,Rc),s(ja,La),s(La,qc),s(ja,Ec),s(H,jc),s(H,W),w(Fa,W,null),s(W,Lc),s(W,Is),s(Is,Fc),s(Is,ut),s(ut,Ac),s(Is,Cc),s(Is,Hn),s(Hn,Pc),s(Is,Sc),s(W,Oc),w(he,W,null),s(W,Nc),w(de,W,null),s(W,Dc),w(ge,W,null),u(e,si,m),u(e,Ws,m),s(Ws,ue),s(ue,In),w(Aa,In,null),s(Ws,Hc),s(Ws,Wn),s(Wn,Ic),u(e,ei,m),u(e,P,m),w(Ca,P,null),s(P,Wc),s(P,Qn),s(Qn,Qc),s(P,Kc),s(P,Pa),s(Pa,Uc),s(Pa,Sa),s(Sa,Xc),s(Pa,Vc),s(P,Bc),s(P,Oa),s(Oa,Gc),s(Oa,ft),s(ft,Jc),s(Oa,Yc),s(P,Zc),s(P,Na),s(Na,sh),s(Na,Da),s(Da,eh),s(Na,ah),s(P,th),s(P,A),w(Ha,A,null),s(A,nh),s(A,Qs),s(Qs,oh),s(Qs,_t),s(_t,rh),s(Qs,ih),s(Qs,Kn),s(Kn,lh),s(Qs,ph),s(A,mh),w(fe,A,null),s(A,ch),w(_e,A,null),s(A,hh),w(ve,A,null),s(A,dh),w(be,A,null),s(A,gh),w(ye,A,null),u(e,ai,m),u(e,Ks,m),s(Ks,ke),s(ke,Un),w(Ia,Un,null),s(Ks,uh),s(Ks,Xn),s(Xn,fh),u(e,ti,m),u(e,S,m),w(Wa,S,null),s(S,_h),s(S,Us),s(Us,vh),s(Us,Vn),s(Vn,bh),s(Us,yh),s(Us,Bn),s(Bn,kh),s(Us,wh),s(S,xh),s(S,Qa),s(Qa,zh),s(Qa,Ka),s(Ka,$h),s(Qa,Th),s(S,Mh),s(S,Ua),s(Ua,Rh),s(Ua,vt),s(vt,qh),s(Ua,Eh),s(S,jh),s(S,Xa),s(Xa,Lh),s(Xa,Va),s(Va,Fh),s(Xa,Ah),s(S,Ch),s(S,Q),w(Ba,Q,null),s(Q,Ph),s(Q,Xs),s(Xs,Sh),s(Xs,bt),s(bt,Oh),s(Xs,Nh),s(Xs,Gn),s(Gn,Dh),s(Xs,Hh),s(Q,Ih),w(we,Q,null),s(Q,Wh),w(xe,Q,null),s(Q,Qh),w(ze,Q,null),ni=!0},p(e,[m]){const Ga={};m&2&&(Ga.$$scope={dirty:m,ctx:e}),ae.$set(Ga);const Jn={};m&2&&(Jn.$$scope={dirty:m,ctx:e}),re.$set(Jn);const Yn={};m&2&&(Yn.$$scope={dirty:m,ctx:e}),ie.$set(Yn);const Zn={};m&2&&(Zn.$$scope={dirty:m,ctx:e}),pe.$set(Zn);const $e={};m&2&&($e.$$scope={dirty:m,ctx:e}),me.$set($e);const so={};m&2&&(so.$$scope={dirty:m,ctx:e}),he.$set(so);const eo={};m&2&&(eo.$$scope={dirty:m,ctx:e}),de.$set(eo);const Ja={};m&2&&(Ja.$$scope={dirty:m,ctx:e}),ge.$set(Ja);const ao={};m&2&&(ao.$$scope={dirty:m,ctx:e}),fe.$set(ao);const to={};m&2&&(to.$$scope={dirty:m,ctx:e}),_e.$set(to);const no={};m&2&&(no.$$scope={dirty:m,ctx:e}),ve.$set(no);const Ya={};m&2&&(Ya.$$scope={dirty:m,ctx:e}),be.$set(Ya);const oo={};m&2&&(oo.$$scope={dirty:m,ctx:e}),ye.$set(oo);const ro={};m&2&&(ro.$$scope={dirty:m,ctx:e}),we.$set(ro);const io={};m&2&&(io.$$scope={dirty:m,ctx:e}),xe.$set(io);const lo={};m&2&&(lo.$$scope={dirty:m,ctx:e}),ze.$set(lo)},i(e){ni||(x(l.$$.fragment,e),x(Ce.$$.fragment,e),x(He.$$.fragment,e),x(Ue.$$.fragment,e),x(Ge.$$.fragment,e),x(Je.$$.fragment,e),x(Ye.$$.fragment,e),x(Ze.$$.fragment,e),x(sa.$$.fragment,e),x(ae.$$.fragment,e),x(aa.$$.fragment,e),x(ta.$$.fragment,e),x(ia.$$.fragment,e),x(la.$$.fragment,e),x(pa.$$.fragment,e),x(ha.$$.fragment,e),x(da.$$.fragment,e),x(ba.$$.fragment,e),x(re.$$.fragment,e),x(ie.$$.fragment,e),x(ya.$$.fragment,e),x(ka.$$.fragment,e),x(Ta.$$.fragment,e),x(pe.$$.fragment,e),x(me.$$.fragment,e),x(Ma.$$.fragment,e),x(Ra.$$.fragment,e),x(Fa.$$.fragment,e),x(he.$$.fragment,e),x(de.$$.fragment,e),x(ge.$$.fragment,e),x(Aa.$$.fragment,e),x(Ca.$$.fragment,e),x(Ha.$$.fragment,e),x(fe.$$.fragment,e),x(_e.$$.fragment,e),x(ve.$$.fragment,e),x(be.$$.fragment,e),x(ye.$$.fragment,e),x(Ia.$$.fragment,e),x(Wa.$$.fragment,e),x(Ba.$$.fragment,e),x(we.$$.fragment,e),x(xe.$$.fragment,e),x(ze.$$.fragment,e),ni=!0)},o(e){z(l.$$.fragment,e),z(Ce.$$.fragment,e),z(He.$$.fragment,e),z(Ue.$$.fragment,e),z(Ge.$$.fragment,e),z(Je.$$.fragment,e),z(Ye.$$.fragment,e),z(Ze.$$.fragment,e),z(sa.$$.fragment,e),z(ae.$$.fragment,e),z(aa.$$.fragment,e),z(ta.$$.fragment,e),z(ia.$$.fragment,e),z(la.$$.fragment,e),z(pa.$$.fragment,e),z(ha.$$.fragment,e),z(da.$$.fragment,e),z(ba.$$.fragment,e),z(re.$$.fragment,e),z(ie.$$.fragment,e),z(ya.$$.fragment,e),z(ka.$$.fragment,e),z(Ta.$$.fragment,e),z(pe.$$.fragment,e),z(me.$$.fragment,e),z(Ma.$$.fragment,e),z(Ra.$$.fragment,e),z(Fa.$$.fragment,e),z(he.$$.fragment,e),z(de.$$.fragment,e),z(ge.$$.fragment,e),z(Aa.$$.fragment,e),z(Ca.$$.fragment,e),z(Ha.$$.fragment,e),z(fe.$$.fragment,e),z(_e.$$.fragment,e),z(ve.$$.fragment,e),z(be.$$.fragment,e),z(ye.$$.fragment,e),z(Ia.$$.fragment,e),z(Wa.$$.fragment,e),z(Ba.$$.fragment,e),z(we.$$.fragment,e),z(xe.$$.fragment,e),z(ze.$$.fragment,e),ni=!1},d(e){a(p),e&&a(v),e&&a(g),$(l),e&&a(co),e&&a(ws),e&&a(ho),e&&a(xs),$(Ce),e&&a(go),e&&a(Bs),e&&a(uo),e&&a(st),e&&a(fo),e&&a(et),e&&a(_o),e&&a(ms),e&&a(vo),e&&a(Ne),e&&a(bo),e&&a(at),e&&a(yo),e&&a(zs),$(He),e&&a(ko),e&&a(q),e&&a(Fo),e&&a($s),e&&a(So),e&&a(We),e&&a(No),e&&a(Qe),e&&a(Ho),e&&a(Ke),e&&a(Wo),e&&a(F),e&&a(sr),e&&a(hs),e&&a(or),e&&a(E),e&&a(mr),e&&a(Ts),$(Ue),e&&a(cr),e&&a(J),e&&a(hr),e&&a(O),e&&a(dr),e&&a(ds),e&&a(gr),e&&a(I),e&&a(kr),e&&a(gs),e&&a(wr),e&&a(Y),e&&a(qr),e&&a(Ms),$(Ge),e&&a(Er),e&&a(Z),e&&a(jr),e&&a(ss),e&&a(Or),e&&a(Rs),$(Je),e&&a(Nr),e&&a(us),e&&a(Dr),e&&a(se),e&&a(Hr),$(Ye,e),e&&a(Ir),e&&a(qs),$(Ze),e&&a(Wr),e&&a(B),$(sa),$(ae),e&&a(Qr),e&&a(Ls),$(aa),e&&a(Kr),e&&a(G),$(ta),$(ia),e&&a(Ur),e&&a(Fs),$(la),e&&a(Xr),e&&a(is),$(pa),e&&a(Vr),e&&a(Cs),$(ha),e&&a(Br),e&&a(N),$(da),$(ba),$(re),$(ie),e&&a(Gr),e&&a(Ss),$(ya),e&&a(Jr),e&&a(D),$(ka),$(Ta),$(pe),$(me),e&&a(Yr),e&&a(Ds),$(Ma),e&&a(Zr),e&&a(H),$(Ra),$(Fa),$(he),$(de),$(ge),e&&a(si),e&&a(Ws),$(Aa),e&&a(ei),e&&a(P),$(Ca),$(Ha),$(fe),$(_e),$(ve),$(be),$(ye),e&&a(ai),e&&a(Ks),$(Ia),e&&a(ti),e&&a(S),$(Wa),$(Ba),$(we),$(xe),$(ze)}}}const Ju={local:"reformer",sections:[{local:"overview",title:"Overview"},{local:"axial-positional-encodings",title:"Axial Positional Encodings"},{local:"lsh-self-attention",title:"LSH Self Attention"},{local:"local-self-attention",title:"Local Self Attention"},{local:"training",title:"Training"},{local:"transformers.ReformerConfig",title:"ReformerConfig"},{local:"transformers.ReformerTokenizer",title:"ReformerTokenizer"},{local:"transformers.ReformerTokenizerFast",title:"ReformerTokenizerFast"},{local:"transformers.ReformerModel",title:"ReformerModel"},{local:"transformers.ReformerModelWithLMHead",title:"ReformerModelWithLMHead"},{local:"transformers.ReformerForMaskedLM",title:"ReformerForMaskedLM"},{local:"transformers.ReformerForSequenceClassification",title:"ReformerForSequenceClassification"},{local:"transformers.ReformerForQuestionAnswering",title:"ReformerForQuestionAnswering"}],title:"Reformer"};function Yu(T){return Fu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class of extends qu{constructor(p){super();Eu(this,p,Yu,Gu,ju,{})}}export{of as default,Ju as metadata};
