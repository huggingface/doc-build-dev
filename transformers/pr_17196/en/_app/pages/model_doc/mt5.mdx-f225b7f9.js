import{S as kd,i as vd,s as Td,e as r,k as p,w as k,t as i,M as $d,c as n,d as s,m,a as o,x as v,h as l,b as d,F as t,g as f,y as T,q as $,o as b,B as w,v as bd,L as De}from"../../chunks/vendor-6b77c823.js";import{D as E}from"../../chunks/Docstring-1088f2fb.js";import{C as Ie}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as C}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Ne}from"../../chunks/ExampleCodeBlock-5212b321.js";function wd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function yd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function xd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function Md(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function zd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function Ed(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function qd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function Fd(z){let h,x,_,u,y;return u=new Ie({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){h=r("p"),x=i("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=l(g,"Examples:"),g.forEach(s),_=m(a),v(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),T(u,a,g),y=!0},p:De,i(a){y||($(u.$$.fragment,a),y=!0)},o(a){b(u.$$.fragment,a),y=!1},d(a){a&&s(h),a&&s(_),w(u,a)}}}function jd(z){let h,x,_,u,y,a,g,zs,Dn,xr,X,le,Es,Ge,In,qs,Gn,Mr,de,On,Oe,Vn,Un,zr,Ut,Wn,Er,Wt,Fs,Bn,qr,pe,Hn,Ve,Rn,Xn,Fr,Bt,Kn,jr,F,js,Cs,Ue,Jn,Qn,Ps,As,We,Yn,Zn,Ss,Ls,Be,eo,to,Ns,Ds,He,so,ro,Is,Ht,Re,no,oo,Cr,U,ao,Xe,io,lo,Ke,po,mo,Pr,K,me,Gs,Je,co,Os,fo,Ar,P,Qe,uo,A,ho,Rt,go,_o,Xt,ko,vo,Ye,To,$o,bo,J,wo,Kt,yo,xo,Jt,Mo,zo,Sr,Q,ce,Vs,Ze,Eo,Us,qo,Lr,M,et,Fo,tt,jo,st,Co,Po,Ao,rt,So,Qt,Lo,No,Do,W,nt,Io,Ws,Go,Oo,ot,Yt,Vo,Bs,Uo,Wo,Zt,Bo,Hs,Ho,Ro,fe,at,Xo,Rs,Ko,Jo,ue,it,Qo,Xs,Yo,Zo,he,lt,ea,dt,ta,Ks,sa,ra,Nr,ge,na,es,oa,aa,Dr,Y,_e,Js,pt,ia,Qs,la,Ir,q,mt,da,Z,pa,Ys,ma,ca,ct,fa,ua,ha,ft,ga,ts,_a,ka,va,B,ut,Ta,Zs,$a,ba,ht,ss,wa,er,ya,xa,rs,Ma,tr,za,Ea,ke,gt,qa,sr,Fa,Gr,ve,ja,ns,Ca,Pa,Or,ee,Te,rr,_t,Aa,nr,Sa,Vr,S,kt,La,vt,Na,os,Da,Ia,Ga,$e,Ur,te,be,or,Tt,Oa,ar,Va,Wr,L,$t,Ua,bt,Wa,as,Ba,Ha,Ra,we,Br,se,ye,ir,wt,Xa,lr,Ka,Hr,N,yt,Ja,xt,Qa,is,Ya,Za,ei,xe,Rr,re,Me,dr,Mt,ti,pr,si,Xr,D,zt,ri,Et,ni,ls,oi,ai,ii,ze,Kr,ne,Ee,mr,qt,li,cr,di,Jr,I,Ft,pi,jt,mi,ds,ci,fi,ui,qe,Qr,oe,Fe,fr,Ct,hi,ur,gi,Yr,G,Pt,_i,At,ki,ps,vi,Ti,$i,je,Zr,ae,Ce,hr,St,bi,gr,wi,en,O,Lt,yi,Nt,xi,ms,Mi,zi,Ei,Pe,tn,ie,Ae,_r,Dt,qi,kr,Fi,sn,V,It,ji,Gt,Ci,cs,Pi,Ai,Si,Se,rn;return a=new C({}),Ge=new C({}),Je=new C({}),Qe=new E({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/pr_17196/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/configuration_mt5.py#L24"}}),Ze=new C({}),et=new E({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5.py#L55"}}),nt=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5.py#L249",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),at=new E({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5.py#L310"}}),it=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5.py#L227",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new E({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new C({}),mt=new E({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5_fast.py#L65"}}),ut=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),gt=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/t5/tokenization_t5_fast.py#L217",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),_t=new C({}),kt=new E({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_mt5.py#L28"}}),$e=new Ne({props:{anchor:"transformers.MT5Model.example",$$slots:{default:[wd]},$$scope:{ctx:z}}}),Tt=new C({}),$t=new E({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_mt5.py#L62"}}),we=new Ne({props:{anchor:"transformers.MT5ForConditionalGeneration.example",$$slots:{default:[yd]},$$scope:{ctx:z}}}),wt=new C({}),yt=new E({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_mt5.py#L94"}}),xe=new Ne({props:{anchor:"transformers.MT5EncoderModel.example",$$slots:{default:[xd]},$$scope:{ctx:z}}}),Mt=new C({}),zt=new E({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),ze=new Ne({props:{anchor:"transformers.TFMT5Model.example",$$slots:{default:[Md]},$$scope:{ctx:z}}}),qt=new C({}),Ft=new E({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_tf_mt5.py#L53"}}),qe=new Ne({props:{anchor:"transformers.TFMT5ForConditionalGeneration.example",$$slots:{default:[zd]},$$scope:{ctx:z}}}),Ct=new C({}),Pt=new E({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_tf_mt5.py#L79"}}),je=new Ne({props:{anchor:"transformers.TFMT5EncoderModel.example",$$slots:{default:[Ed]},$$scope:{ctx:z}}}),St=new C({}),Lt=new E({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_flax_mt5.py#L43"}}),Pe=new Ne({props:{anchor:"transformers.FlaxMT5Model.example",$$slots:{default:[qd]},$$scope:{ctx:z}}}),Dt=new C({}),It=new E({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/mt5/modeling_flax_mt5.py#L70"}}),Se=new Ne({props:{anchor:"transformers.FlaxMT5ForConditionalGeneration.example",$$slots:{default:[Fd]},$$scope:{ctx:z}}}),{c(){h=r("meta"),x=p(),_=r("h1"),u=r("a"),y=r("span"),k(a.$$.fragment),g=p(),zs=r("span"),Dn=i("mT5"),xr=p(),X=r("h2"),le=r("a"),Es=r("span"),k(Ge.$$.fragment),In=p(),qs=r("span"),Gn=i("Overview"),Mr=p(),de=r("p"),On=i("The mT5 model was presented in "),Oe=r("a"),Vn=i("mT5: A massively multilingual pre-trained text-to-text transformer"),Un=i(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),zr=p(),Ut=r("p"),Wn=i("The abstract from the paper is the following:"),Er=p(),Wt=r("p"),Fs=r("em"),Bn=i(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),qr=p(),pe=r("p"),Hn=i("Note: mT5 was only pre-trained on "),Ve=r("a"),Rn=i("mC4"),Xn=i(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Fr=p(),Bt=r("p"),Kn=i("Google has released the following variants:"),jr=p(),F=r("ul"),js=r("li"),Cs=r("p"),Ue=r("a"),Jn=i("google/mt5-small"),Qn=p(),Ps=r("li"),As=r("p"),We=r("a"),Yn=i("google/mt5-base"),Zn=p(),Ss=r("li"),Ls=r("p"),Be=r("a"),eo=i("google/mt5-large"),to=p(),Ns=r("li"),Ds=r("p"),He=r("a"),so=i("google/mt5-xl"),ro=p(),Is=r("li"),Ht=r("p"),Re=r("a"),no=i("google/mt5-xxl"),oo=i("."),Cr=p(),U=r("p"),ao=i("This model was contributed by "),Xe=r("a"),io=i("patrickvonplaten"),lo=i(`. The original code can be
found `),Ke=r("a"),po=i("here"),mo=i("."),Pr=p(),K=r("h2"),me=r("a"),Gs=r("span"),k(Je.$$.fragment),co=p(),Os=r("span"),fo=i("MT5Config"),Ar=p(),P=r("div"),k(Qe.$$.fragment),uo=p(),A=r("p"),ho=i("This is the configuration class to store the configuration of a "),Rt=r("a"),go=i("MT5Model"),_o=i(" or a "),Xt=r("a"),ko=i("TFMT5Model"),vo=i(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),Ye=r("a"),To=i("google/mt5-small"),$o=i(" architecture."),bo=p(),J=r("p"),wo=i("Configuration objects inherit from "),Kt=r("a"),yo=i("PretrainedConfig"),xo=i(` and can be used to control the model outputs. Read the
documentation from `),Jt=r("a"),Mo=i("PretrainedConfig"),zo=i(" for more information."),Sr=p(),Q=r("h2"),ce=r("a"),Vs=r("span"),k(Ze.$$.fragment),Eo=p(),Us=r("span"),qo=i("MT5Tokenizer"),Lr=p(),M=r("div"),k(et.$$.fragment),Fo=p(),tt=r("p"),jo=i("Construct a T5 tokenizer. Based on "),st=r("a"),Co=i("SentencePiece"),Po=i("."),Ao=p(),rt=r("p"),So=i("This tokenizer inherits from "),Qt=r("a"),Lo=i("PreTrainedTokenizer"),No=i(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Do=p(),W=r("div"),k(nt.$$.fragment),Io=p(),Ws=r("p"),Go=i(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Oo=p(),ot=r("ul"),Yt=r("li"),Vo=i("single sequence: "),Bs=r("code"),Uo=i("X </s>"),Wo=p(),Zt=r("li"),Bo=i("pair of sequences: "),Hs=r("code"),Ho=i("A </s> B </s>"),Ro=p(),fe=r("div"),k(at.$$.fragment),Xo=p(),Rs=r("p"),Ko=i("Converts a sequence of tokens (string) in a single string."),Jo=p(),ue=r("div"),k(it.$$.fragment),Qo=p(),Xs=r("p"),Yo=i(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Zo=p(),he=r("div"),k(lt.$$.fragment),ea=p(),dt=r("p"),ta=i(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ks=r("code"),sa=i("prepare_for_model"),ra=i(" method."),Nr=p(),ge=r("p"),na=i("See "),es=r("a"),oa=i("T5Tokenizer"),aa=i(" for all details."),Dr=p(),Y=r("h2"),_e=r("a"),Js=r("span"),k(pt.$$.fragment),ia=p(),Qs=r("span"),la=i("MT5TokenizerFast"),Ir=p(),q=r("div"),k(mt.$$.fragment),da=p(),Z=r("p"),pa=i("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Ys=r("em"),ma=i("tokenizers"),ca=i(` library). Based on
`),ct=r("a"),fa=i("Unigram"),ua=i("."),ha=p(),ft=r("p"),ga=i("This tokenizer inherits from "),ts=r("a"),_a=i("PreTrainedTokenizerFast"),ka=i(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),va=p(),B=r("div"),k(ut.$$.fragment),Ta=p(),Zs=r("p"),$a=i(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ba=p(),ht=r("ul"),ss=r("li"),wa=i("single sequence: "),er=r("code"),ya=i("X </s>"),xa=p(),rs=r("li"),Ma=i("pair of sequences: "),tr=r("code"),za=i("A </s> B </s>"),Ea=p(),ke=r("div"),k(gt.$$.fragment),qa=p(),sr=r("p"),Fa=i(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Gr=p(),ve=r("p"),ja=i("See "),ns=r("a"),Ca=i("T5TokenizerFast"),Pa=i(" for all details."),Or=p(),ee=r("h2"),Te=r("a"),rr=r("span"),k(_t.$$.fragment),Aa=p(),nr=r("span"),Sa=i("MT5Model"),Vr=p(),S=r("div"),k(kt.$$.fragment),La=p(),vt=r("p"),Na=i("This class overrides "),os=r("a"),Da=i("T5Model"),Ia=i(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ga=p(),k($e.$$.fragment),Ur=p(),te=r("h2"),be=r("a"),or=r("span"),k(Tt.$$.fragment),Oa=p(),ar=r("span"),Va=i("MT5ForConditionalGeneration"),Wr=p(),L=r("div"),k($t.$$.fragment),Ua=p(),bt=r("p"),Wa=i("This class overrides "),as=r("a"),Ba=i("T5ForConditionalGeneration"),Ha=i(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ra=p(),k(we.$$.fragment),Br=p(),se=r("h2"),ye=r("a"),ir=r("span"),k(wt.$$.fragment),Xa=p(),lr=r("span"),Ka=i("MT5EncoderModel"),Hr=p(),N=r("div"),k(yt.$$.fragment),Ja=p(),xt=r("p"),Qa=i("This class overrides "),is=r("a"),Ya=i("T5EncoderModel"),Za=i(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ei=p(),k(xe.$$.fragment),Rr=p(),re=r("h2"),Me=r("a"),dr=r("span"),k(Mt.$$.fragment),ti=p(),pr=r("span"),si=i("TFMT5Model"),Xr=p(),D=r("div"),k(zt.$$.fragment),ri=p(),Et=r("p"),ni=i("This class overrides "),ls=r("a"),oi=i("TFT5Model"),ai=i(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),ii=p(),k(ze.$$.fragment),Kr=p(),ne=r("h2"),Ee=r("a"),mr=r("span"),k(qt.$$.fragment),li=p(),cr=r("span"),di=i("TFMT5ForConditionalGeneration"),Jr=p(),I=r("div"),k(Ft.$$.fragment),pi=p(),jt=r("p"),mi=i("This class overrides "),ds=r("a"),ci=i("TFT5ForConditionalGeneration"),fi=i(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ui=p(),k(qe.$$.fragment),Qr=p(),oe=r("h2"),Fe=r("a"),fr=r("span"),k(Ct.$$.fragment),hi=p(),ur=r("span"),gi=i("TFMT5EncoderModel"),Yr=p(),G=r("div"),k(Pt.$$.fragment),_i=p(),At=r("p"),ki=i("This class overrides "),ps=r("a"),vi=i("TFT5EncoderModel"),Ti=i(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),$i=p(),k(je.$$.fragment),Zr=p(),ae=r("h2"),Ce=r("a"),hr=r("span"),k(St.$$.fragment),bi=p(),gr=r("span"),wi=i("FlaxMT5Model"),en=p(),O=r("div"),k(Lt.$$.fragment),yi=p(),Nt=r("p"),xi=i("This class overrides "),ms=r("a"),Mi=i("FlaxT5Model"),zi=i(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ei=p(),k(Pe.$$.fragment),tn=p(),ie=r("h2"),Ae=r("a"),_r=r("span"),k(Dt.$$.fragment),qi=p(),kr=r("span"),Fi=i("FlaxMT5ForConditionalGeneration"),sn=p(),V=r("div"),k(It.$$.fragment),ji=p(),Gt=r("p"),Ci=i("This class overrides "),cs=r("a"),Pi=i("FlaxT5ForConditionalGeneration"),Ai=i(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Si=p(),k(Se.$$.fragment),this.h()},l(e){const c=$d('[data-svelte="svelte-1phssyn"]',document.head);h=n(c,"META",{name:!0,content:!0}),c.forEach(s),x=m(e),_=n(e,"H1",{class:!0});var Ot=o(_);u=n(Ot,"A",{id:!0,class:!0,href:!0});var vr=o(u);y=n(vr,"SPAN",{});var Tr=o(y);v(a.$$.fragment,Tr),Tr.forEach(s),vr.forEach(s),g=m(Ot),zs=n(Ot,"SPAN",{});var $r=o(zs);Dn=l($r,"mT5"),$r.forEach(s),Ot.forEach(s),xr=m(e),X=n(e,"H2",{class:!0});var Vt=o(X);le=n(Vt,"A",{id:!0,class:!0,href:!0});var br=o(le);Es=n(br,"SPAN",{});var wr=o(Es);v(Ge.$$.fragment,wr),wr.forEach(s),br.forEach(s),In=m(Vt),qs=n(Vt,"SPAN",{});var yr=o(qs);Gn=l(yr,"Overview"),yr.forEach(s),Vt.forEach(s),Mr=m(e),de=n(e,"P",{});var nn=o(de);On=l(nn,"The mT5 model was presented in "),Oe=n(nn,"A",{href:!0,rel:!0});var Oi=o(Oe);Vn=l(Oi,"mT5: A massively multilingual pre-trained text-to-text transformer"),Oi.forEach(s),Un=l(nn,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),nn.forEach(s),zr=m(e),Ut=n(e,"P",{});var Vi=o(Ut);Wn=l(Vi,"The abstract from the paper is the following:"),Vi.forEach(s),Er=m(e),Wt=n(e,"P",{});var Ui=o(Wt);Fs=n(Ui,"EM",{});var Wi=o(Fs);Bn=l(Wi,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Wi.forEach(s),Ui.forEach(s),qr=m(e),pe=n(e,"P",{});var on=o(pe);Hn=l(on,"Note: mT5 was only pre-trained on "),Ve=n(on,"A",{href:!0,rel:!0});var Bi=o(Ve);Rn=l(Bi,"mC4"),Bi.forEach(s),Xn=l(on,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),on.forEach(s),Fr=m(e),Bt=n(e,"P",{});var Hi=o(Bt);Kn=l(Hi,"Google has released the following variants:"),Hi.forEach(s),jr=m(e),F=n(e,"UL",{});var H=o(F);js=n(H,"LI",{});var Ri=o(js);Cs=n(Ri,"P",{});var Xi=o(Cs);Ue=n(Xi,"A",{href:!0,rel:!0});var Ki=o(Ue);Jn=l(Ki,"google/mt5-small"),Ki.forEach(s),Xi.forEach(s),Ri.forEach(s),Qn=m(H),Ps=n(H,"LI",{});var Ji=o(Ps);As=n(Ji,"P",{});var Qi=o(As);We=n(Qi,"A",{href:!0,rel:!0});var Yi=o(We);Yn=l(Yi,"google/mt5-base"),Yi.forEach(s),Qi.forEach(s),Ji.forEach(s),Zn=m(H),Ss=n(H,"LI",{});var Zi=o(Ss);Ls=n(Zi,"P",{});var el=o(Ls);Be=n(el,"A",{href:!0,rel:!0});var tl=o(Be);eo=l(tl,"google/mt5-large"),tl.forEach(s),el.forEach(s),Zi.forEach(s),to=m(H),Ns=n(H,"LI",{});var sl=o(Ns);Ds=n(sl,"P",{});var rl=o(Ds);He=n(rl,"A",{href:!0,rel:!0});var nl=o(He);so=l(nl,"google/mt5-xl"),nl.forEach(s),rl.forEach(s),sl.forEach(s),ro=m(H),Is=n(H,"LI",{});var ol=o(Is);Ht=n(ol,"P",{});var Li=o(Ht);Re=n(Li,"A",{href:!0,rel:!0});var al=o(Re);no=l(al,"google/mt5-xxl"),al.forEach(s),oo=l(Li,"."),Li.forEach(s),ol.forEach(s),H.forEach(s),Cr=m(e),U=n(e,"P",{});var fs=o(U);ao=l(fs,"This model was contributed by "),Xe=n(fs,"A",{href:!0,rel:!0});var il=o(Xe);io=l(il,"patrickvonplaten"),il.forEach(s),lo=l(fs,`. The original code can be
found `),Ke=n(fs,"A",{href:!0,rel:!0});var ll=o(Ke);po=l(ll,"here"),ll.forEach(s),mo=l(fs,"."),fs.forEach(s),Pr=m(e),K=n(e,"H2",{class:!0});var an=o(K);me=n(an,"A",{id:!0,class:!0,href:!0});var dl=o(me);Gs=n(dl,"SPAN",{});var pl=o(Gs);v(Je.$$.fragment,pl),pl.forEach(s),dl.forEach(s),co=m(an),Os=n(an,"SPAN",{});var ml=o(Os);fo=l(ml,"MT5Config"),ml.forEach(s),an.forEach(s),Ar=m(e),P=n(e,"DIV",{class:!0});var us=o(P);v(Qe.$$.fragment,us),uo=m(us),A=n(us,"P",{});var Le=o(A);ho=l(Le,"This is the configuration class to store the configuration of a "),Rt=n(Le,"A",{href:!0});var cl=o(Rt);go=l(cl,"MT5Model"),cl.forEach(s),_o=l(Le," or a "),Xt=n(Le,"A",{href:!0});var fl=o(Xt);ko=l(fl,"TFMT5Model"),fl.forEach(s),vo=l(Le,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),Ye=n(Le,"A",{href:!0,rel:!0});var ul=o(Ye);To=l(ul,"google/mt5-small"),ul.forEach(s),$o=l(Le," architecture."),Le.forEach(s),bo=m(us),J=n(us,"P",{});var hs=o(J);wo=l(hs,"Configuration objects inherit from "),Kt=n(hs,"A",{href:!0});var hl=o(Kt);yo=l(hl,"PretrainedConfig"),hl.forEach(s),xo=l(hs,` and can be used to control the model outputs. Read the
documentation from `),Jt=n(hs,"A",{href:!0});var gl=o(Jt);Mo=l(gl,"PretrainedConfig"),gl.forEach(s),zo=l(hs," for more information."),hs.forEach(s),us.forEach(s),Sr=m(e),Q=n(e,"H2",{class:!0});var ln=o(Q);ce=n(ln,"A",{id:!0,class:!0,href:!0});var _l=o(ce);Vs=n(_l,"SPAN",{});var kl=o(Vs);v(Ze.$$.fragment,kl),kl.forEach(s),_l.forEach(s),Eo=m(ln),Us=n(ln,"SPAN",{});var vl=o(Us);qo=l(vl,"MT5Tokenizer"),vl.forEach(s),ln.forEach(s),Lr=m(e),M=n(e,"DIV",{class:!0});var j=o(M);v(et.$$.fragment,j),Fo=m(j),tt=n(j,"P",{});var dn=o(tt);jo=l(dn,"Construct a T5 tokenizer. Based on "),st=n(dn,"A",{href:!0,rel:!0});var Tl=o(st);Co=l(Tl,"SentencePiece"),Tl.forEach(s),Po=l(dn,"."),dn.forEach(s),Ao=m(j),rt=n(j,"P",{});var pn=o(rt);So=l(pn,"This tokenizer inherits from "),Qt=n(pn,"A",{href:!0});var $l=o(Qt);Lo=l($l,"PreTrainedTokenizer"),$l.forEach(s),No=l(pn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),pn.forEach(s),Do=m(j),W=n(j,"DIV",{class:!0});var gs=o(W);v(nt.$$.fragment,gs),Io=m(gs),Ws=n(gs,"P",{});var bl=o(Ws);Go=l(bl,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),bl.forEach(s),Oo=m(gs),ot=n(gs,"UL",{});var mn=o(ot);Yt=n(mn,"LI",{});var Ni=o(Yt);Vo=l(Ni,"single sequence: "),Bs=n(Ni,"CODE",{});var wl=o(Bs);Uo=l(wl,"X </s>"),wl.forEach(s),Ni.forEach(s),Wo=m(mn),Zt=n(mn,"LI",{});var Di=o(Zt);Bo=l(Di,"pair of sequences: "),Hs=n(Di,"CODE",{});var yl=o(Hs);Ho=l(yl,"A </s> B </s>"),yl.forEach(s),Di.forEach(s),mn.forEach(s),gs.forEach(s),Ro=m(j),fe=n(j,"DIV",{class:!0});var cn=o(fe);v(at.$$.fragment,cn),Xo=m(cn),Rs=n(cn,"P",{});var xl=o(Rs);Ko=l(xl,"Converts a sequence of tokens (string) in a single string."),xl.forEach(s),cn.forEach(s),Jo=m(j),ue=n(j,"DIV",{class:!0});var fn=o(ue);v(it.$$.fragment,fn),Qo=m(fn),Xs=n(fn,"P",{});var Ml=o(Xs);Yo=l(Ml,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Ml.forEach(s),fn.forEach(s),Zo=m(j),he=n(j,"DIV",{class:!0});var un=o(he);v(lt.$$.fragment,un),ea=m(un),dt=n(un,"P",{});var hn=o(dt);ta=l(hn,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ks=n(hn,"CODE",{});var zl=o(Ks);sa=l(zl,"prepare_for_model"),zl.forEach(s),ra=l(hn," method."),hn.forEach(s),un.forEach(s),j.forEach(s),Nr=m(e),ge=n(e,"P",{});var gn=o(ge);na=l(gn,"See "),es=n(gn,"A",{href:!0});var El=o(es);oa=l(El,"T5Tokenizer"),El.forEach(s),aa=l(gn," for all details."),gn.forEach(s),Dr=m(e),Y=n(e,"H2",{class:!0});var _n=o(Y);_e=n(_n,"A",{id:!0,class:!0,href:!0});var ql=o(_e);Js=n(ql,"SPAN",{});var Fl=o(Js);v(pt.$$.fragment,Fl),Fl.forEach(s),ql.forEach(s),ia=m(_n),Qs=n(_n,"SPAN",{});var jl=o(Qs);la=l(jl,"MT5TokenizerFast"),jl.forEach(s),_n.forEach(s),Ir=m(e),q=n(e,"DIV",{class:!0});var R=o(q);v(mt.$$.fragment,R),da=m(R),Z=n(R,"P",{});var _s=o(Z);pa=l(_s,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Ys=n(_s,"EM",{});var Cl=o(Ys);ma=l(Cl,"tokenizers"),Cl.forEach(s),ca=l(_s,` library). Based on
`),ct=n(_s,"A",{href:!0,rel:!0});var Pl=o(ct);fa=l(Pl,"Unigram"),Pl.forEach(s),ua=l(_s,"."),_s.forEach(s),ha=m(R),ft=n(R,"P",{});var kn=o(ft);ga=l(kn,"This tokenizer inherits from "),ts=n(kn,"A",{href:!0});var Al=o(ts);_a=l(Al,"PreTrainedTokenizerFast"),Al.forEach(s),ka=l(kn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),kn.forEach(s),va=m(R),B=n(R,"DIV",{class:!0});var ks=o(B);v(ut.$$.fragment,ks),Ta=m(ks),Zs=n(ks,"P",{});var Sl=o(Zs);$a=l(Sl,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Sl.forEach(s),ba=m(ks),ht=n(ks,"UL",{});var vn=o(ht);ss=n(vn,"LI",{});var Ii=o(ss);wa=l(Ii,"single sequence: "),er=n(Ii,"CODE",{});var Ll=o(er);ya=l(Ll,"X </s>"),Ll.forEach(s),Ii.forEach(s),xa=m(vn),rs=n(vn,"LI",{});var Gi=o(rs);Ma=l(Gi,"pair of sequences: "),tr=n(Gi,"CODE",{});var Nl=o(tr);za=l(Nl,"A </s> B </s>"),Nl.forEach(s),Gi.forEach(s),vn.forEach(s),ks.forEach(s),Ea=m(R),ke=n(R,"DIV",{class:!0});var Tn=o(ke);v(gt.$$.fragment,Tn),qa=m(Tn),sr=n(Tn,"P",{});var Dl=o(sr);Fa=l(Dl,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Dl.forEach(s),Tn.forEach(s),R.forEach(s),Gr=m(e),ve=n(e,"P",{});var $n=o(ve);ja=l($n,"See "),ns=n($n,"A",{href:!0});var Il=o(ns);Ca=l(Il,"T5TokenizerFast"),Il.forEach(s),Pa=l($n," for all details."),$n.forEach(s),Or=m(e),ee=n(e,"H2",{class:!0});var bn=o(ee);Te=n(bn,"A",{id:!0,class:!0,href:!0});var Gl=o(Te);rr=n(Gl,"SPAN",{});var Ol=o(rr);v(_t.$$.fragment,Ol),Ol.forEach(s),Gl.forEach(s),Aa=m(bn),nr=n(bn,"SPAN",{});var Vl=o(nr);Sa=l(Vl,"MT5Model"),Vl.forEach(s),bn.forEach(s),Vr=m(e),S=n(e,"DIV",{class:!0});var vs=o(S);v(kt.$$.fragment,vs),La=m(vs),vt=n(vs,"P",{});var wn=o(vt);Na=l(wn,"This class overrides "),os=n(wn,"A",{href:!0});var Ul=o(os);Da=l(Ul,"T5Model"),Ul.forEach(s),Ia=l(wn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),wn.forEach(s),Ga=m(vs),v($e.$$.fragment,vs),vs.forEach(s),Ur=m(e),te=n(e,"H2",{class:!0});var yn=o(te);be=n(yn,"A",{id:!0,class:!0,href:!0});var Wl=o(be);or=n(Wl,"SPAN",{});var Bl=o(or);v(Tt.$$.fragment,Bl),Bl.forEach(s),Wl.forEach(s),Oa=m(yn),ar=n(yn,"SPAN",{});var Hl=o(ar);Va=l(Hl,"MT5ForConditionalGeneration"),Hl.forEach(s),yn.forEach(s),Wr=m(e),L=n(e,"DIV",{class:!0});var Ts=o(L);v($t.$$.fragment,Ts),Ua=m(Ts),bt=n(Ts,"P",{});var xn=o(bt);Wa=l(xn,"This class overrides "),as=n(xn,"A",{href:!0});var Rl=o(as);Ba=l(Rl,"T5ForConditionalGeneration"),Rl.forEach(s),Ha=l(xn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),xn.forEach(s),Ra=m(Ts),v(we.$$.fragment,Ts),Ts.forEach(s),Br=m(e),se=n(e,"H2",{class:!0});var Mn=o(se);ye=n(Mn,"A",{id:!0,class:!0,href:!0});var Xl=o(ye);ir=n(Xl,"SPAN",{});var Kl=o(ir);v(wt.$$.fragment,Kl),Kl.forEach(s),Xl.forEach(s),Xa=m(Mn),lr=n(Mn,"SPAN",{});var Jl=o(lr);Ka=l(Jl,"MT5EncoderModel"),Jl.forEach(s),Mn.forEach(s),Hr=m(e),N=n(e,"DIV",{class:!0});var $s=o(N);v(yt.$$.fragment,$s),Ja=m($s),xt=n($s,"P",{});var zn=o(xt);Qa=l(zn,"This class overrides "),is=n(zn,"A",{href:!0});var Ql=o(is);Ya=l(Ql,"T5EncoderModel"),Ql.forEach(s),Za=l(zn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),zn.forEach(s),ei=m($s),v(xe.$$.fragment,$s),$s.forEach(s),Rr=m(e),re=n(e,"H2",{class:!0});var En=o(re);Me=n(En,"A",{id:!0,class:!0,href:!0});var Yl=o(Me);dr=n(Yl,"SPAN",{});var Zl=o(dr);v(Mt.$$.fragment,Zl),Zl.forEach(s),Yl.forEach(s),ti=m(En),pr=n(En,"SPAN",{});var ed=o(pr);si=l(ed,"TFMT5Model"),ed.forEach(s),En.forEach(s),Xr=m(e),D=n(e,"DIV",{class:!0});var bs=o(D);v(zt.$$.fragment,bs),ri=m(bs),Et=n(bs,"P",{});var qn=o(Et);ni=l(qn,"This class overrides "),ls=n(qn,"A",{href:!0});var td=o(ls);oi=l(td,"TFT5Model"),td.forEach(s),ai=l(qn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),qn.forEach(s),ii=m(bs),v(ze.$$.fragment,bs),bs.forEach(s),Kr=m(e),ne=n(e,"H2",{class:!0});var Fn=o(ne);Ee=n(Fn,"A",{id:!0,class:!0,href:!0});var sd=o(Ee);mr=n(sd,"SPAN",{});var rd=o(mr);v(qt.$$.fragment,rd),rd.forEach(s),sd.forEach(s),li=m(Fn),cr=n(Fn,"SPAN",{});var nd=o(cr);di=l(nd,"TFMT5ForConditionalGeneration"),nd.forEach(s),Fn.forEach(s),Jr=m(e),I=n(e,"DIV",{class:!0});var ws=o(I);v(Ft.$$.fragment,ws),pi=m(ws),jt=n(ws,"P",{});var jn=o(jt);mi=l(jn,"This class overrides "),ds=n(jn,"A",{href:!0});var od=o(ds);ci=l(od,"TFT5ForConditionalGeneration"),od.forEach(s),fi=l(jn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),jn.forEach(s),ui=m(ws),v(qe.$$.fragment,ws),ws.forEach(s),Qr=m(e),oe=n(e,"H2",{class:!0});var Cn=o(oe);Fe=n(Cn,"A",{id:!0,class:!0,href:!0});var ad=o(Fe);fr=n(ad,"SPAN",{});var id=o(fr);v(Ct.$$.fragment,id),id.forEach(s),ad.forEach(s),hi=m(Cn),ur=n(Cn,"SPAN",{});var ld=o(ur);gi=l(ld,"TFMT5EncoderModel"),ld.forEach(s),Cn.forEach(s),Yr=m(e),G=n(e,"DIV",{class:!0});var ys=o(G);v(Pt.$$.fragment,ys),_i=m(ys),At=n(ys,"P",{});var Pn=o(At);ki=l(Pn,"This class overrides "),ps=n(Pn,"A",{href:!0});var dd=o(ps);vi=l(dd,"TFT5EncoderModel"),dd.forEach(s),Ti=l(Pn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Pn.forEach(s),$i=m(ys),v(je.$$.fragment,ys),ys.forEach(s),Zr=m(e),ae=n(e,"H2",{class:!0});var An=o(ae);Ce=n(An,"A",{id:!0,class:!0,href:!0});var pd=o(Ce);hr=n(pd,"SPAN",{});var md=o(hr);v(St.$$.fragment,md),md.forEach(s),pd.forEach(s),bi=m(An),gr=n(An,"SPAN",{});var cd=o(gr);wi=l(cd,"FlaxMT5Model"),cd.forEach(s),An.forEach(s),en=m(e),O=n(e,"DIV",{class:!0});var xs=o(O);v(Lt.$$.fragment,xs),yi=m(xs),Nt=n(xs,"P",{});var Sn=o(Nt);xi=l(Sn,"This class overrides "),ms=n(Sn,"A",{href:!0});var fd=o(ms);Mi=l(fd,"FlaxT5Model"),fd.forEach(s),zi=l(Sn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Sn.forEach(s),Ei=m(xs),v(Pe.$$.fragment,xs),xs.forEach(s),tn=m(e),ie=n(e,"H2",{class:!0});var Ln=o(ie);Ae=n(Ln,"A",{id:!0,class:!0,href:!0});var ud=o(Ae);_r=n(ud,"SPAN",{});var hd=o(_r);v(Dt.$$.fragment,hd),hd.forEach(s),ud.forEach(s),qi=m(Ln),kr=n(Ln,"SPAN",{});var gd=o(kr);Fi=l(gd,"FlaxMT5ForConditionalGeneration"),gd.forEach(s),Ln.forEach(s),sn=m(e),V=n(e,"DIV",{class:!0});var Ms=o(V);v(It.$$.fragment,Ms),ji=m(Ms),Gt=n(Ms,"P",{});var Nn=o(Gt);Ci=l(Nn,"This class overrides "),cs=n(Nn,"A",{href:!0});var _d=o(cs);Pi=l(_d,"FlaxT5ForConditionalGeneration"),_d.forEach(s),Ai=l(Nn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Nn.forEach(s),Si=m(Ms),v(Se.$$.fragment,Ms),Ms.forEach(s),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(Cd)),d(u,"id","mt5"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#mt5"),d(_,"class","relative group"),d(le,"id","overview"),d(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(le,"href","#overview"),d(X,"class","relative group"),d(Oe,"href","https://arxiv.org/abs/2010.11934"),d(Oe,"rel","nofollow"),d(Ve,"href","https://huggingface.co/datasets/mc4"),d(Ve,"rel","nofollow"),d(Ue,"href","https://huggingface.co/google/mt5-small"),d(Ue,"rel","nofollow"),d(We,"href","https://huggingface.co/google/mt5-base"),d(We,"rel","nofollow"),d(Be,"href","https://huggingface.co/google/mt5-large"),d(Be,"rel","nofollow"),d(He,"href","https://huggingface.co/google/mt5-xl"),d(He,"rel","nofollow"),d(Re,"href","https://huggingface.co/google/mt5-xxl"),d(Re,"rel","nofollow"),d(Xe,"href","https://huggingface.co/patrickvonplaten"),d(Xe,"rel","nofollow"),d(Ke,"href","https://github.com/google-research/multilingual-t5"),d(Ke,"rel","nofollow"),d(me,"id","transformers.MT5Config"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#transformers.MT5Config"),d(K,"class","relative group"),d(Rt,"href","/docs/transformers/pr_17196/en/model_doc/mt5#transformers.MT5Model"),d(Xt,"href","/docs/transformers/pr_17196/en/model_doc/mt5#transformers.TFMT5Model"),d(Ye,"href","https://huggingface.co/google/mt5-small"),d(Ye,"rel","nofollow"),d(Kt,"href","/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig"),d(Jt,"href","/docs/transformers/pr_17196/en/main_classes/configuration#transformers.PretrainedConfig"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ce,"id","transformers.T5Tokenizer"),d(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ce,"href","#transformers.T5Tokenizer"),d(Q,"class","relative group"),d(st,"href","https://github.com/google/sentencepiece"),d(st,"rel","nofollow"),d(Qt,"href","/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(es,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5Tokenizer"),d(_e,"id","transformers.T5TokenizerFast"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#transformers.T5TokenizerFast"),d(Y,"class","relative group"),d(ct,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),d(ct,"rel","nofollow"),d(ts,"href","/docs/transformers/pr_17196/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ns,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5TokenizerFast"),d(Te,"id","transformers.MT5Model"),d(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Te,"href","#transformers.MT5Model"),d(ee,"class","relative group"),d(os,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5Model"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(be,"id","transformers.MT5ForConditionalGeneration"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#transformers.MT5ForConditionalGeneration"),d(te,"class","relative group"),d(as,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ye,"id","transformers.MT5EncoderModel"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#transformers.MT5EncoderModel"),d(se,"class","relative group"),d(is,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.T5EncoderModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Me,"id","transformers.TFMT5Model"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#transformers.TFMT5Model"),d(re,"class","relative group"),d(ls,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.TFT5Model"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ee,"id","transformers.TFMT5ForConditionalGeneration"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#transformers.TFMT5ForConditionalGeneration"),d(ne,"class","relative group"),d(ds,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fe,"id","transformers.TFMT5EncoderModel"),d(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fe,"href","#transformers.TFMT5EncoderModel"),d(oe,"class","relative group"),d(ps,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.TFT5EncoderModel"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.FlaxMT5Model"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.FlaxMT5Model"),d(ae,"class","relative group"),d(ms,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.FlaxT5Model"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"id","transformers.FlaxMT5ForConditionalGeneration"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#transformers.FlaxMT5ForConditionalGeneration"),d(ie,"class","relative group"),d(cs,"href","/docs/transformers/pr_17196/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,h),f(e,x,c),f(e,_,c),t(_,u),t(u,y),T(a,y,null),t(_,g),t(_,zs),t(zs,Dn),f(e,xr,c),f(e,X,c),t(X,le),t(le,Es),T(Ge,Es,null),t(X,In),t(X,qs),t(qs,Gn),f(e,Mr,c),f(e,de,c),t(de,On),t(de,Oe),t(Oe,Vn),t(de,Un),f(e,zr,c),f(e,Ut,c),t(Ut,Wn),f(e,Er,c),f(e,Wt,c),t(Wt,Fs),t(Fs,Bn),f(e,qr,c),f(e,pe,c),t(pe,Hn),t(pe,Ve),t(Ve,Rn),t(pe,Xn),f(e,Fr,c),f(e,Bt,c),t(Bt,Kn),f(e,jr,c),f(e,F,c),t(F,js),t(js,Cs),t(Cs,Ue),t(Ue,Jn),t(F,Qn),t(F,Ps),t(Ps,As),t(As,We),t(We,Yn),t(F,Zn),t(F,Ss),t(Ss,Ls),t(Ls,Be),t(Be,eo),t(F,to),t(F,Ns),t(Ns,Ds),t(Ds,He),t(He,so),t(F,ro),t(F,Is),t(Is,Ht),t(Ht,Re),t(Re,no),t(Ht,oo),f(e,Cr,c),f(e,U,c),t(U,ao),t(U,Xe),t(Xe,io),t(U,lo),t(U,Ke),t(Ke,po),t(U,mo),f(e,Pr,c),f(e,K,c),t(K,me),t(me,Gs),T(Je,Gs,null),t(K,co),t(K,Os),t(Os,fo),f(e,Ar,c),f(e,P,c),T(Qe,P,null),t(P,uo),t(P,A),t(A,ho),t(A,Rt),t(Rt,go),t(A,_o),t(A,Xt),t(Xt,ko),t(A,vo),t(A,Ye),t(Ye,To),t(A,$o),t(P,bo),t(P,J),t(J,wo),t(J,Kt),t(Kt,yo),t(J,xo),t(J,Jt),t(Jt,Mo),t(J,zo),f(e,Sr,c),f(e,Q,c),t(Q,ce),t(ce,Vs),T(Ze,Vs,null),t(Q,Eo),t(Q,Us),t(Us,qo),f(e,Lr,c),f(e,M,c),T(et,M,null),t(M,Fo),t(M,tt),t(tt,jo),t(tt,st),t(st,Co),t(tt,Po),t(M,Ao),t(M,rt),t(rt,So),t(rt,Qt),t(Qt,Lo),t(rt,No),t(M,Do),t(M,W),T(nt,W,null),t(W,Io),t(W,Ws),t(Ws,Go),t(W,Oo),t(W,ot),t(ot,Yt),t(Yt,Vo),t(Yt,Bs),t(Bs,Uo),t(ot,Wo),t(ot,Zt),t(Zt,Bo),t(Zt,Hs),t(Hs,Ho),t(M,Ro),t(M,fe),T(at,fe,null),t(fe,Xo),t(fe,Rs),t(Rs,Ko),t(M,Jo),t(M,ue),T(it,ue,null),t(ue,Qo),t(ue,Xs),t(Xs,Yo),t(M,Zo),t(M,he),T(lt,he,null),t(he,ea),t(he,dt),t(dt,ta),t(dt,Ks),t(Ks,sa),t(dt,ra),f(e,Nr,c),f(e,ge,c),t(ge,na),t(ge,es),t(es,oa),t(ge,aa),f(e,Dr,c),f(e,Y,c),t(Y,_e),t(_e,Js),T(pt,Js,null),t(Y,ia),t(Y,Qs),t(Qs,la),f(e,Ir,c),f(e,q,c),T(mt,q,null),t(q,da),t(q,Z),t(Z,pa),t(Z,Ys),t(Ys,ma),t(Z,ca),t(Z,ct),t(ct,fa),t(Z,ua),t(q,ha),t(q,ft),t(ft,ga),t(ft,ts),t(ts,_a),t(ft,ka),t(q,va),t(q,B),T(ut,B,null),t(B,Ta),t(B,Zs),t(Zs,$a),t(B,ba),t(B,ht),t(ht,ss),t(ss,wa),t(ss,er),t(er,ya),t(ht,xa),t(ht,rs),t(rs,Ma),t(rs,tr),t(tr,za),t(q,Ea),t(q,ke),T(gt,ke,null),t(ke,qa),t(ke,sr),t(sr,Fa),f(e,Gr,c),f(e,ve,c),t(ve,ja),t(ve,ns),t(ns,Ca),t(ve,Pa),f(e,Or,c),f(e,ee,c),t(ee,Te),t(Te,rr),T(_t,rr,null),t(ee,Aa),t(ee,nr),t(nr,Sa),f(e,Vr,c),f(e,S,c),T(kt,S,null),t(S,La),t(S,vt),t(vt,Na),t(vt,os),t(os,Da),t(vt,Ia),t(S,Ga),T($e,S,null),f(e,Ur,c),f(e,te,c),t(te,be),t(be,or),T(Tt,or,null),t(te,Oa),t(te,ar),t(ar,Va),f(e,Wr,c),f(e,L,c),T($t,L,null),t(L,Ua),t(L,bt),t(bt,Wa),t(bt,as),t(as,Ba),t(bt,Ha),t(L,Ra),T(we,L,null),f(e,Br,c),f(e,se,c),t(se,ye),t(ye,ir),T(wt,ir,null),t(se,Xa),t(se,lr),t(lr,Ka),f(e,Hr,c),f(e,N,c),T(yt,N,null),t(N,Ja),t(N,xt),t(xt,Qa),t(xt,is),t(is,Ya),t(xt,Za),t(N,ei),T(xe,N,null),f(e,Rr,c),f(e,re,c),t(re,Me),t(Me,dr),T(Mt,dr,null),t(re,ti),t(re,pr),t(pr,si),f(e,Xr,c),f(e,D,c),T(zt,D,null),t(D,ri),t(D,Et),t(Et,ni),t(Et,ls),t(ls,oi),t(Et,ai),t(D,ii),T(ze,D,null),f(e,Kr,c),f(e,ne,c),t(ne,Ee),t(Ee,mr),T(qt,mr,null),t(ne,li),t(ne,cr),t(cr,di),f(e,Jr,c),f(e,I,c),T(Ft,I,null),t(I,pi),t(I,jt),t(jt,mi),t(jt,ds),t(ds,ci),t(jt,fi),t(I,ui),T(qe,I,null),f(e,Qr,c),f(e,oe,c),t(oe,Fe),t(Fe,fr),T(Ct,fr,null),t(oe,hi),t(oe,ur),t(ur,gi),f(e,Yr,c),f(e,G,c),T(Pt,G,null),t(G,_i),t(G,At),t(At,ki),t(At,ps),t(ps,vi),t(At,Ti),t(G,$i),T(je,G,null),f(e,Zr,c),f(e,ae,c),t(ae,Ce),t(Ce,hr),T(St,hr,null),t(ae,bi),t(ae,gr),t(gr,wi),f(e,en,c),f(e,O,c),T(Lt,O,null),t(O,yi),t(O,Nt),t(Nt,xi),t(Nt,ms),t(ms,Mi),t(Nt,zi),t(O,Ei),T(Pe,O,null),f(e,tn,c),f(e,ie,c),t(ie,Ae),t(Ae,_r),T(Dt,_r,null),t(ie,qi),t(ie,kr),t(kr,Fi),f(e,sn,c),f(e,V,c),T(It,V,null),t(V,ji),t(V,Gt),t(Gt,Ci),t(Gt,cs),t(cs,Pi),t(Gt,Ai),t(V,Si),T(Se,V,null),rn=!0},p(e,[c]){const Ot={};c&2&&(Ot.$$scope={dirty:c,ctx:e}),$e.$set(Ot);const vr={};c&2&&(vr.$$scope={dirty:c,ctx:e}),we.$set(vr);const Tr={};c&2&&(Tr.$$scope={dirty:c,ctx:e}),xe.$set(Tr);const $r={};c&2&&($r.$$scope={dirty:c,ctx:e}),ze.$set($r);const Vt={};c&2&&(Vt.$$scope={dirty:c,ctx:e}),qe.$set(Vt);const br={};c&2&&(br.$$scope={dirty:c,ctx:e}),je.$set(br);const wr={};c&2&&(wr.$$scope={dirty:c,ctx:e}),Pe.$set(wr);const yr={};c&2&&(yr.$$scope={dirty:c,ctx:e}),Se.$set(yr)},i(e){rn||($(a.$$.fragment,e),$(Ge.$$.fragment,e),$(Je.$$.fragment,e),$(Qe.$$.fragment,e),$(Ze.$$.fragment,e),$(et.$$.fragment,e),$(nt.$$.fragment,e),$(at.$$.fragment,e),$(it.$$.fragment,e),$(lt.$$.fragment,e),$(pt.$$.fragment,e),$(mt.$$.fragment,e),$(ut.$$.fragment,e),$(gt.$$.fragment,e),$(_t.$$.fragment,e),$(kt.$$.fragment,e),$($e.$$.fragment,e),$(Tt.$$.fragment,e),$($t.$$.fragment,e),$(we.$$.fragment,e),$(wt.$$.fragment,e),$(yt.$$.fragment,e),$(xe.$$.fragment,e),$(Mt.$$.fragment,e),$(zt.$$.fragment,e),$(ze.$$.fragment,e),$(qt.$$.fragment,e),$(Ft.$$.fragment,e),$(qe.$$.fragment,e),$(Ct.$$.fragment,e),$(Pt.$$.fragment,e),$(je.$$.fragment,e),$(St.$$.fragment,e),$(Lt.$$.fragment,e),$(Pe.$$.fragment,e),$(Dt.$$.fragment,e),$(It.$$.fragment,e),$(Se.$$.fragment,e),rn=!0)},o(e){b(a.$$.fragment,e),b(Ge.$$.fragment,e),b(Je.$$.fragment,e),b(Qe.$$.fragment,e),b(Ze.$$.fragment,e),b(et.$$.fragment,e),b(nt.$$.fragment,e),b(at.$$.fragment,e),b(it.$$.fragment,e),b(lt.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ut.$$.fragment,e),b(gt.$$.fragment,e),b(_t.$$.fragment,e),b(kt.$$.fragment,e),b($e.$$.fragment,e),b(Tt.$$.fragment,e),b($t.$$.fragment,e),b(we.$$.fragment,e),b(wt.$$.fragment,e),b(yt.$$.fragment,e),b(xe.$$.fragment,e),b(Mt.$$.fragment,e),b(zt.$$.fragment,e),b(ze.$$.fragment,e),b(qt.$$.fragment,e),b(Ft.$$.fragment,e),b(qe.$$.fragment,e),b(Ct.$$.fragment,e),b(Pt.$$.fragment,e),b(je.$$.fragment,e),b(St.$$.fragment,e),b(Lt.$$.fragment,e),b(Pe.$$.fragment,e),b(Dt.$$.fragment,e),b(It.$$.fragment,e),b(Se.$$.fragment,e),rn=!1},d(e){s(h),e&&s(x),e&&s(_),w(a),e&&s(xr),e&&s(X),w(Ge),e&&s(Mr),e&&s(de),e&&s(zr),e&&s(Ut),e&&s(Er),e&&s(Wt),e&&s(qr),e&&s(pe),e&&s(Fr),e&&s(Bt),e&&s(jr),e&&s(F),e&&s(Cr),e&&s(U),e&&s(Pr),e&&s(K),w(Je),e&&s(Ar),e&&s(P),w(Qe),e&&s(Sr),e&&s(Q),w(Ze),e&&s(Lr),e&&s(M),w(et),w(nt),w(at),w(it),w(lt),e&&s(Nr),e&&s(ge),e&&s(Dr),e&&s(Y),w(pt),e&&s(Ir),e&&s(q),w(mt),w(ut),w(gt),e&&s(Gr),e&&s(ve),e&&s(Or),e&&s(ee),w(_t),e&&s(Vr),e&&s(S),w(kt),w($e),e&&s(Ur),e&&s(te),w(Tt),e&&s(Wr),e&&s(L),w($t),w(we),e&&s(Br),e&&s(se),w(wt),e&&s(Hr),e&&s(N),w(yt),w(xe),e&&s(Rr),e&&s(re),w(Mt),e&&s(Xr),e&&s(D),w(zt),w(ze),e&&s(Kr),e&&s(ne),w(qt),e&&s(Jr),e&&s(I),w(Ft),w(qe),e&&s(Qr),e&&s(oe),w(Ct),e&&s(Yr),e&&s(G),w(Pt),w(je),e&&s(Zr),e&&s(ae),w(St),e&&s(en),e&&s(O),w(Lt),w(Pe),e&&s(tn),e&&s(ie),w(Dt),e&&s(sn),e&&s(V),w(It),w(Se)}}}const Cd={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"}],title:"mT5"};function Pd(z){return bd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Id extends kd{constructor(h){super();vd(this,h,Pd,jd,Td,{})}}export{Id as default,Cd as metadata};
