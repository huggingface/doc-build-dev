import{S as pu,i as hu,s as fu,e as n,k as l,w as k,t as a,M as uu,c as r,d as t,m as c,a as s,x as $,h as i,b as p,F as e,g as f,y as P,q as w,o as E,B as R,v as mu,L as Sn}from"../../chunks/vendor-6b77c823.js";import{T as xt}from"../../chunks/Tip-39098574.js";import{D as U}from"../../chunks/Docstring-1088f2fb.js";import{C as Hn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ge}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as In}from"../../chunks/ExampleCodeBlock-5212b321.js";function gu(F){let h,T,v,u,b;return u=new Hn({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),{c(){h=n("p"),T=a("with the format:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"with the format:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function _u(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function vu(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function bu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function Tu(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function ku(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function $u(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function Pu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,S,ee,ve,H,be,he,B,I,te,oe,q,A,ne,V,fe,re,W,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),S=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),H=n("code"),be=a("model(inputs)"),he=a("."),B=l(),I=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),W=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),S=r(C,"CODE",{});var me=s(S);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),H=r(C,"CODE",{});var Ne=s(H);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),B=c(_),I=r(_,"P",{});var ae=s(I);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),W=r(Y,"CODE",{});var xe=s(W);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var L=s(Q);K=i(L,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(L,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),L.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,S),e(S,ee),e(D,ve),e(D,H),e(H,be),e(D,he),f(_,B,y),f(_,I,y),e(I,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,W),e(W,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(B),_&&t(I),_&&t(oe),_&&t(q)}}}function wu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function Eu(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function Ru(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,S,ee,ve,H,be,he,B,I,te,oe,q,A,ne,V,fe,re,W,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),S=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),H=n("code"),be=a("model(inputs)"),he=a("."),B=l(),I=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),W=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),S=r(C,"CODE",{});var me=s(S);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),H=r(C,"CODE",{});var Ne=s(H);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),B=c(_),I=r(_,"P",{});var ae=s(I);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),W=r(Y,"CODE",{});var xe=s(W);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var L=s(Q);K=i(L,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(L,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),L.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,S),e(S,ee),e(D,ve),e(D,H),e(H,be),e(D,he),f(_,B,y),f(_,I,y),e(I,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,W),e(W,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(B),_&&t(I),_&&t(oe),_&&t(q)}}}function Du(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function yu(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function xu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,S,ee,ve,H,be,he,B,I,te,oe,q,A,ne,V,fe,re,W,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),S=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),H=n("code"),be=a("model(inputs)"),he=a("."),B=l(),I=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),W=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),S=r(C,"CODE",{});var me=s(S);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),H=r(C,"CODE",{});var Ne=s(H);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),B=c(_),I=r(_,"P",{});var ae=s(I);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),W=r(Y,"CODE",{});var xe=s(W);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var L=s(Q);K=i(L,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(L,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),L.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,S),e(S,ee),e(D,ve),e(D,H),e(H,be),e(D,he),f(_,B,y),f(_,I,y),e(I,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,W),e(W,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(B),_&&t(I),_&&t(oe),_&&t(q)}}}function zu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function qu(F){let h,T,v,u,b;return u=new Hn({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:Sn,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function Fu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,S,ee,ve,H,be,he,B,I,te,oe,q,A,ne,V,fe,re,W,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se,_,y,G,Oe,ze,C,me,Ne,ae,O,Y,je,xe,X,Me,Qe,L,Le,Bn,hi,fi,Wn,ui,mi,Un,gi,_i,$o,vi,bi,Ti,Po,ki,Vn,$i,Pi,Ys,at,zt,Or,wo,wi,Nr,Ei,Xs,qe,Eo,Ri,jr,Di,yi,qt,Kn,xi,zi,Yn,qi,Fi,Ci,Ro,Ai,Xn,Oi,Ni,Js,it,Ft,Mr,Do,ji,Qr,Mi,Gs,Fe,yo,Qi,xo,Li,Lr,Ii,Si,Hi,Ct,Jn,Bi,Wi,Gn,Ui,Vi,Ki,zo,Yi,Zn,Xi,Ji,Zs,dt,At,Ir,qo,Gi,Sr,Zi,ea,Ce,Fo,ed,Hr,td,od,Ot,er,nd,rd,tr,sd,ad,id,Co,dd,or,ld,cd,ta,lt,Nt,Br,Ao,pd,Wr,hd,oa,Ae,Oo,fd,No,ud,Ur,md,gd,_d,jt,nr,vd,bd,rr,Td,kd,$d,jo,Pd,sr,wd,Ed,na,ct,Mt,Vr,Mo,Rd,Kr,Dd,ra,ie,Qo,yd,Yr,xd,zd,Ze,ar,qd,Fd,ir,Cd,Ad,dr,Od,Nd,jd,Lo,Md,lr,Qd,Ld,Id,et,Sd,Xr,Hd,Bd,Jr,Wd,Ud,Gr,Vd,Kd,Qt,sa,pt,Lt,Zr,Io,Yd,es,Xd,aa,de,So,Jd,Ho,Gd,ts,Zd,el,tl,tt,cr,ol,nl,pr,rl,sl,hr,al,il,dl,Bo,ll,fr,cl,pl,hl,Ge,fl,os,ul,ml,ns,gl,_l,rs,vl,bl,Tl,ss,kl,ia,ht,It,as,Wo,$l,is,Pl,da,ft,Uo,wl,Vo,El,ur,Rl,Dl,la,ut,Ko,yl,Yo,xl,mr,zl,ql,ca,mt,Xo,Fl,Jo,Cl,gr,Al,Ol,pa,gt,St,ds,Go,Nl,ls,jl,ha,Re,Zo,Ml,cs,Ql,Ll,en,Il,_r,Sl,Hl,Bl,tn,Wl,on,Ul,Vl,Kl,Ie,nn,Yl,_t,Xl,vr,Jl,Gl,ps,Zl,ec,tc,Ht,oc,Bt,fa,vt,Wt,hs,rn,nc,fs,rc,ua,De,sn,sc,us,ac,ic,an,dc,br,lc,cc,pc,dn,hc,ln,fc,uc,mc,Se,cn,gc,bt,_c,Tr,vc,bc,ms,Tc,kc,$c,Ut,Pc,Vt,ma,Tt,Kt,gs,pn,wc,_s,Ec,ga,ye,hn,Rc,vs,Dc,yc,fn,xc,kr,zc,qc,Fc,un,Cc,mn,Ac,Oc,Nc,He,gn,jc,kt,Mc,$r,Qc,Lc,bs,Ic,Sc,Hc,Yt,Bc,Xt,_a,$t,Jt,Ts,_n,Wc,ks,Uc,va,le,vn,Vc,$s,Kc,Yc,bn,Xc,Pr,Jc,Gc,Zc,Tn,ep,kn,tp,op,np,Gt,rp,Be,$n,sp,Pt,ap,wr,ip,dp,Ps,lp,cp,pp,Zt,hp,eo,ba,wt,to,ws,Pn,fp,Es,up,Ta,ce,wn,mp,Rs,gp,_p,En,vp,Er,bp,Tp,kp,Rn,$p,Dn,Pp,wp,Ep,oo,Rp,We,yn,Dp,Et,yp,Rr,xp,zp,Ds,qp,Fp,Cp,no,Ap,ro,ka,Rt,so,ys,xn,Op,xs,Np,$a,pe,zn,jp,zs,Mp,Qp,qn,Lp,Dr,Ip,Sp,Hp,Fn,Bp,Cn,Wp,Up,Vp,ao,Kp,Ue,An,Yp,Dt,Xp,yr,Jp,Gp,qs,Zp,eh,th,io,oh,lo,Pa;return d=new ge({}),ee=new ge({}),_=new ge({}),me=new U({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/configuration_dpr.py#L45"}}),wo=new ge({}),Eo=new U({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr.py#L113"}}),Do=new ge({}),yo=new U({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr_fast.py#L114"}}),qo=new ge({}),Fo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr.py#L129"}}),Ao=new ge({}),Oo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr_fast.py#L131"}}),Mo=new ge({}),Qo=new U({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr.py#L396",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Qt=new In({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[gu]},$$scope:{ctx:F}}}),Io=new ge({}),So=new U({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_17196/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/tokenization_dpr_fast.py#L394",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Wo=new ge({}),Uo=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L62"}}),Ko=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L90"}}),Xo=new U({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Go=new ge({}),Zo=new U({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L446"}}),nn=new U({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L454",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ht=new xt({props:{$$slots:{default:[_u]},$$scope:{ctx:F}}}),Bt=new In({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[vu]},$$scope:{ctx:F}}}),rn=new ge({}),sn=new U({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L527"}}),cn=new U({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L535",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ut=new xt({props:{$$slots:{default:[bu]},$$scope:{ctx:F}}}),Vt=new In({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[Tu]},$$scope:{ctx:F}}}),pn=new ge({}),hn=new U({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L608"}}),gn=new U({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_dpr.py#L616",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Yt=new xt({props:{$$slots:{default:[ku]},$$scope:{ctx:F}}}),Xt=new In({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[$u]},$$scope:{ctx:F}}}),_n=new ge({}),vn=new U({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L535"}}),Gt=new xt({props:{$$slots:{default:[Pu]},$$scope:{ctx:F}}}),$n=new U({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L547",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Zt=new xt({props:{$$slots:{default:[wu]},$$scope:{ctx:F}}}),eo=new In({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[Eu]},$$scope:{ctx:F}}}),Pn=new ge({}),wn=new U({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L622"}}),oo=new xt({props:{$$slots:{default:[Ru]},$$scope:{ctx:F}}}),yn=new U({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L634",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),no=new xt({props:{$$slots:{default:[Du]},$$scope:{ctx:F}}}),ro=new In({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[yu]},$$scope:{ctx:F}}}),xn=new ge({}),zn=new U({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L708"}}),ao=new xt({props:{$$slots:{default:[xu]},$$scope:{ctx:F}}}),An=new U({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_17196/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/vr_17196/src/transformers/models/dpr/modeling_tf_dpr.py#L720",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),io=new xt({props:{$$slots:{default:[zu]},$$scope:{ctx:F}}}),lo=new In({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[qu]},$$scope:{ctx:F}}}),{c(){h=n("meta"),T=l(),v=n("h1"),u=n("a"),b=n("span"),k(d.$$.fragment),g=l(),x=n("span"),_e=a("DPR"),Z=l(),D=n("h2"),J=n("a"),S=n("span"),k(ee.$$.fragment),ve=l(),H=n("span"),be=a("Overview"),he=l(),B=n("p"),I=a(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=n("a"),oe=a("Dense Passage Retrieval for Open-Domain Question Answering"),q=a(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),A=l(),ne=n("p"),V=a("The abstract from the paper is the following:"),fe=l(),re=n("p"),W=n("em"),Te=a(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ue=l(),z=n("p"),ke=a("This model was contributed by "),j=n("a"),$e=a("lhoestq"),Pe=a(". The original code can be found "),M=n("a"),we=a("here"),Ee=a("."),Q=l(),K=n("h2"),N=n("a"),se=n("span"),k(_.$$.fragment),y=l(),G=n("span"),Oe=a("DPRConfig"),ze=l(),C=n("div"),k(me.$$.fragment),Ne=l(),ae=n("p"),O=n("a"),Y=a("DPRConfig"),je=a(" is the configuration class to store the configuration of a "),xe=n("em"),X=a("DPRModel"),Me=a("."),Qe=l(),L=n("p"),Le=a("This is the configuration class to store the configuration of a "),Bn=n("a"),hi=a("DPRContextEncoder"),fi=a(", "),Wn=n("a"),ui=a("DPRQuestionEncoder"),mi=a(`, or a
`),Un=n("a"),gi=a("DPRReader"),_i=a(`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),$o=n("a"),vi=a("facebook/dpr-ctx_encoder-single-nq-base"),bi=a(`
architecture.`),Ti=l(),Po=n("p"),ki=a("This class is a subclass of "),Vn=n("a"),$i=a("BertConfig"),Pi=a(". Please check the superclass for the documentation of all kwargs."),Ys=l(),at=n("h2"),zt=n("a"),Or=n("span"),k(wo.$$.fragment),wi=l(),Nr=n("span"),Ei=a("DPRContextEncoderTokenizer"),Xs=l(),qe=n("div"),k(Eo.$$.fragment),Ri=l(),jr=n("p"),Di=a("Construct a DPRContextEncoder tokenizer."),yi=l(),qt=n("p"),Kn=n("a"),xi=a("DPRContextEncoderTokenizer"),zi=a(" is identical to "),Yn=n("a"),qi=a("BertTokenizer"),Fi=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ci=l(),Ro=n("p"),Ai=a("Refer to superclass "),Xn=n("a"),Oi=a("BertTokenizer"),Ni=a(" for usage examples and documentation concerning parameters."),Js=l(),it=n("h2"),Ft=n("a"),Mr=n("span"),k(Do.$$.fragment),ji=l(),Qr=n("span"),Mi=a("DPRContextEncoderTokenizerFast"),Gs=l(),Fe=n("div"),k(yo.$$.fragment),Qi=l(),xo=n("p"),Li=a("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Lr=n("em"),Ii=a("tokenizers"),Si=a(" library)."),Hi=l(),Ct=n("p"),Jn=n("a"),Bi=a("DPRContextEncoderTokenizerFast"),Wi=a(" is identical to "),Gn=n("a"),Ui=a("BertTokenizerFast"),Vi=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ki=l(),zo=n("p"),Yi=a("Refer to superclass "),Zn=n("a"),Xi=a("BertTokenizerFast"),Ji=a(" for usage examples and documentation concerning parameters."),Zs=l(),dt=n("h2"),At=n("a"),Ir=n("span"),k(qo.$$.fragment),Gi=l(),Sr=n("span"),Zi=a("DPRQuestionEncoderTokenizer"),ea=l(),Ce=n("div"),k(Fo.$$.fragment),ed=l(),Hr=n("p"),td=a("Constructs a DPRQuestionEncoder tokenizer."),od=l(),Ot=n("p"),er=n("a"),nd=a("DPRQuestionEncoderTokenizer"),rd=a(" is identical to "),tr=n("a"),sd=a("BertTokenizer"),ad=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),id=l(),Co=n("p"),dd=a("Refer to superclass "),or=n("a"),ld=a("BertTokenizer"),cd=a(" for usage examples and documentation concerning parameters."),ta=l(),lt=n("h2"),Nt=n("a"),Br=n("span"),k(Ao.$$.fragment),pd=l(),Wr=n("span"),hd=a("DPRQuestionEncoderTokenizerFast"),oa=l(),Ae=n("div"),k(Oo.$$.fragment),fd=l(),No=n("p"),ud=a("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Ur=n("em"),md=a("tokenizers"),gd=a(" library)."),_d=l(),jt=n("p"),nr=n("a"),vd=a("DPRQuestionEncoderTokenizerFast"),bd=a(" is identical to "),rr=n("a"),Td=a("BertTokenizerFast"),kd=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),$d=l(),jo=n("p"),Pd=a("Refer to superclass "),sr=n("a"),wd=a("BertTokenizerFast"),Ed=a(" for usage examples and documentation concerning parameters."),na=l(),ct=n("h2"),Mt=n("a"),Vr=n("span"),k(Mo.$$.fragment),Rd=l(),Kr=n("span"),Dd=a("DPRReaderTokenizer"),ra=l(),ie=n("div"),k(Qo.$$.fragment),yd=l(),Yr=n("p"),xd=a("Construct a DPRReader tokenizer."),zd=l(),Ze=n("p"),ar=n("a"),qd=a("DPRReaderTokenizer"),Fd=a(" is almost identical to "),ir=n("a"),Cd=a("BertTokenizer"),Ad=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),dr=n("a"),Od=a("DPRReader"),Nd=a(" model."),jd=l(),Lo=n("p"),Md=a("Refer to superclass "),lr=n("a"),Qd=a("BertTokenizer"),Ld=a(" for usage examples and documentation concerning parameters."),Id=l(),et=n("p"),Sd=a("Return a dictionary with the token ids of the input strings and other information to give to "),Xr=n("code"),Hd=a(".decode_best_spans"),Bd=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Jr=n("code"),Wd=a("input_ids"),Ud=a(" is a matrix of size "),Gr=n("code"),Vd=a("(n_passages, sequence_length)"),Kd=l(),k(Qt.$$.fragment),sa=l(),pt=n("h2"),Lt=n("a"),Zr=n("span"),k(Io.$$.fragment),Yd=l(),es=n("span"),Xd=a("DPRReaderTokenizerFast"),aa=l(),de=n("div"),k(So.$$.fragment),Jd=l(),Ho=n("p"),Gd=a("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),ts=n("em"),Zd=a("tokenizers"),el=a(" library)."),tl=l(),tt=n("p"),cr=n("a"),ol=a("DPRReaderTokenizerFast"),nl=a(" is almost identical to "),pr=n("a"),rl=a("BertTokenizerFast"),sl=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),hr=n("a"),al=a("DPRReader"),il=a(" model."),dl=l(),Bo=n("p"),ll=a("Refer to superclass "),fr=n("a"),cl=a("BertTokenizerFast"),pl=a(" for usage examples and documentation concerning parameters."),hl=l(),Ge=n("p"),fl=a("Return a dictionary with the token ids of the input strings and other information to give to "),os=n("code"),ul=a(".decode_best_spans"),ml=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ns=n("code"),gl=a("input_ids"),_l=a(" is a matrix of size "),rs=n("code"),vl=a("(n_passages, sequence_length)"),bl=a(`
with the format:`),Tl=l(),ss=n("p"),kl=a("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),ia=l(),ht=n("h2"),It=n("a"),as=n("span"),k(Wo.$$.fragment),$l=l(),is=n("span"),Pl=a("DPR specific outputs"),da=l(),ft=n("div"),k(Uo.$$.fragment),wl=l(),Vo=n("p"),El=a("Class for outputs of "),ur=n("a"),Rl=a("DPRQuestionEncoder"),Dl=a("."),la=l(),ut=n("div"),k(Ko.$$.fragment),yl=l(),Yo=n("p"),xl=a("Class for outputs of "),mr=n("a"),zl=a("DPRQuestionEncoder"),ql=a("."),ca=l(),mt=n("div"),k(Xo.$$.fragment),Fl=l(),Jo=n("p"),Cl=a("Class for outputs of "),gr=n("a"),Al=a("DPRQuestionEncoder"),Ol=a("."),pa=l(),gt=n("h2"),St=n("a"),ds=n("span"),k(Go.$$.fragment),Nl=l(),ls=n("span"),jl=a("DPRContextEncoder"),ha=l(),Re=n("div"),k(Zo.$$.fragment),Ml=l(),cs=n("p"),Ql=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Ll=l(),en=n("p"),Il=a("This model inherits from "),_r=n("a"),Sl=a("PreTrainedModel"),Hl=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bl=l(),tn=n("p"),Wl=a("This model is also a PyTorch "),on=n("a"),Ul=a("torch.nn.Module"),Vl=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Kl=l(),Ie=n("div"),k(nn.$$.fragment),Yl=l(),_t=n("p"),Xl=a("The "),vr=n("a"),Jl=a("DPRContextEncoder"),Gl=a(" forward method, overrides the "),ps=n("code"),Zl=a("__call__"),ec=a(" special method."),tc=l(),k(Ht.$$.fragment),oc=l(),k(Bt.$$.fragment),fa=l(),vt=n("h2"),Wt=n("a"),hs=n("span"),k(rn.$$.fragment),nc=l(),fs=n("span"),rc=a("DPRQuestionEncoder"),ua=l(),De=n("div"),k(sn.$$.fragment),sc=l(),us=n("p"),ac=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),ic=l(),an=n("p"),dc=a("This model inherits from "),br=n("a"),lc=a("PreTrainedModel"),cc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pc=l(),dn=n("p"),hc=a("This model is also a PyTorch "),ln=n("a"),fc=a("torch.nn.Module"),uc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mc=l(),Se=n("div"),k(cn.$$.fragment),gc=l(),bt=n("p"),_c=a("The "),Tr=n("a"),vc=a("DPRQuestionEncoder"),bc=a(" forward method, overrides the "),ms=n("code"),Tc=a("__call__"),kc=a(" special method."),$c=l(),k(Ut.$$.fragment),Pc=l(),k(Vt.$$.fragment),ma=l(),Tt=n("h2"),Kt=n("a"),gs=n("span"),k(pn.$$.fragment),wc=l(),_s=n("span"),Ec=a("DPRReader"),ga=l(),ye=n("div"),k(hn.$$.fragment),Rc=l(),vs=n("p"),Dc=a("The bare DPRReader transformer outputting span predictions."),yc=l(),fn=n("p"),xc=a("This model inherits from "),kr=n("a"),zc=a("PreTrainedModel"),qc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fc=l(),un=n("p"),Cc=a("This model is also a PyTorch "),mn=n("a"),Ac=a("torch.nn.Module"),Oc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nc=l(),He=n("div"),k(gn.$$.fragment),jc=l(),kt=n("p"),Mc=a("The "),$r=n("a"),Qc=a("DPRReader"),Lc=a(" forward method, overrides the "),bs=n("code"),Ic=a("__call__"),Sc=a(" special method."),Hc=l(),k(Yt.$$.fragment),Bc=l(),k(Xt.$$.fragment),_a=l(),$t=n("h2"),Jt=n("a"),Ts=n("span"),k(_n.$$.fragment),Wc=l(),ks=n("span"),Uc=a("TFDPRContextEncoder"),va=l(),le=n("div"),k(vn.$$.fragment),Vc=l(),$s=n("p"),Kc=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Yc=l(),bn=n("p"),Xc=a("This model inherits from "),Pr=n("a"),Jc=a("TFPreTrainedModel"),Gc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zc=l(),Tn=n("p"),ep=a("This model is also a Tensorflow "),kn=n("a"),tp=a("tf.keras.Model"),op=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),np=l(),k(Gt.$$.fragment),rp=l(),Be=n("div"),k($n.$$.fragment),sp=l(),Pt=n("p"),ap=a("The "),wr=n("a"),ip=a("TFDPRContextEncoder"),dp=a(" forward method, overrides the "),Ps=n("code"),lp=a("__call__"),cp=a(" special method."),pp=l(),k(Zt.$$.fragment),hp=l(),k(eo.$$.fragment),ba=l(),wt=n("h2"),to=n("a"),ws=n("span"),k(Pn.$$.fragment),fp=l(),Es=n("span"),up=a("TFDPRQuestionEncoder"),Ta=l(),ce=n("div"),k(wn.$$.fragment),mp=l(),Rs=n("p"),gp=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),_p=l(),En=n("p"),vp=a("This model inherits from "),Er=n("a"),bp=a("TFPreTrainedModel"),Tp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kp=l(),Rn=n("p"),$p=a("This model is also a Tensorflow "),Dn=n("a"),Pp=a("tf.keras.Model"),wp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Ep=l(),k(oo.$$.fragment),Rp=l(),We=n("div"),k(yn.$$.fragment),Dp=l(),Et=n("p"),yp=a("The "),Rr=n("a"),xp=a("TFDPRQuestionEncoder"),zp=a(" forward method, overrides the "),Ds=n("code"),qp=a("__call__"),Fp=a(" special method."),Cp=l(),k(no.$$.fragment),Ap=l(),k(ro.$$.fragment),ka=l(),Rt=n("h2"),so=n("a"),ys=n("span"),k(xn.$$.fragment),Op=l(),xs=n("span"),Np=a("TFDPRReader"),$a=l(),pe=n("div"),k(zn.$$.fragment),jp=l(),zs=n("p"),Mp=a("The bare DPRReader transformer outputting span predictions."),Qp=l(),qn=n("p"),Lp=a("This model inherits from "),Dr=n("a"),Ip=a("TFPreTrainedModel"),Sp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hp=l(),Fn=n("p"),Bp=a("This model is also a Tensorflow "),Cn=n("a"),Wp=a("tf.keras.Model"),Up=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Vp=l(),k(ao.$$.fragment),Kp=l(),Ue=n("div"),k(An.$$.fragment),Yp=l(),Dt=n("p"),Xp=a("The "),yr=n("a"),Jp=a("TFDPRReader"),Gp=a(" forward method, overrides the "),qs=n("code"),Zp=a("__call__"),eh=a(" special method."),th=l(),k(io.$$.fragment),oh=l(),k(lo.$$.fragment),this.h()},l(o){const m=uu('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),T=c(o),v=r(o,"H1",{class:!0});var On=s(v);u=r(On,"A",{id:!0,class:!0,href:!0});var Fs=s(u);b=r(Fs,"SPAN",{});var Cs=s(b);$(d.$$.fragment,Cs),Cs.forEach(t),Fs.forEach(t),g=c(On),x=r(On,"SPAN",{});var As=s(x);_e=i(As,"DPR"),As.forEach(t),On.forEach(t),Z=c(o),D=r(o,"H2",{class:!0});var Nn=s(D);J=r(Nn,"A",{id:!0,class:!0,href:!0});var Os=s(J);S=r(Os,"SPAN",{});var Ns=s(S);$(ee.$$.fragment,Ns),Ns.forEach(t),Os.forEach(t),ve=c(Nn),H=r(Nn,"SPAN",{});var js=s(H);be=i(js,"Overview"),js.forEach(t),Nn.forEach(t),he=c(o),B=r(o,"P",{});var jn=s(B);I=i(jn,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=r(jn,"A",{href:!0,rel:!0});var Ms=s(te);oe=i(Ms,"Dense Passage Retrieval for Open-Domain Question Answering"),Ms.forEach(t),q=i(jn,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),jn.forEach(t),A=c(o),ne=r(o,"P",{});var Qs=s(ne);V=i(Qs,"The abstract from the paper is the following:"),Qs.forEach(t),fe=c(o),re=r(o,"P",{});var Ls=s(re);W=r(Ls,"EM",{});var Is=s(W);Te=i(Is,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),Is.forEach(t),Ls.forEach(t),ue=c(o),z=r(o,"P",{});var yt=s(z);ke=i(yt,"This model was contributed by "),j=r(yt,"A",{href:!0,rel:!0});var Ss=s(j);$e=i(Ss,"lhoestq"),Ss.forEach(t),Pe=i(yt,". The original code can be found "),M=r(yt,"A",{href:!0,rel:!0});var Hs=s(M);we=i(Hs,"here"),Hs.forEach(t),Ee=i(yt,"."),yt.forEach(t),Q=c(o),K=r(o,"H2",{class:!0});var wa=s(K);N=r(wa,"A",{id:!0,class:!0,href:!0});var nh=s(N);se=r(nh,"SPAN",{});var rh=s(se);$(_.$$.fragment,rh),rh.forEach(t),nh.forEach(t),y=c(wa),G=r(wa,"SPAN",{});var sh=s(G);Oe=i(sh,"DPRConfig"),sh.forEach(t),wa.forEach(t),ze=c(o),C=r(o,"DIV",{class:!0});var co=s(C);$(me.$$.fragment,co),Ne=c(co),ae=r(co,"P",{});var Bs=s(ae);O=r(Bs,"A",{href:!0});var ah=s(O);Y=i(ah,"DPRConfig"),ah.forEach(t),je=i(Bs," is the configuration class to store the configuration of a "),xe=r(Bs,"EM",{});var ih=s(xe);X=i(ih,"DPRModel"),ih.forEach(t),Me=i(Bs,"."),Bs.forEach(t),Qe=c(co),L=r(co,"P",{});var ot=s(L);Le=i(ot,"This is the configuration class to store the configuration of a "),Bn=r(ot,"A",{href:!0});var dh=s(Bn);hi=i(dh,"DPRContextEncoder"),dh.forEach(t),fi=i(ot,", "),Wn=r(ot,"A",{href:!0});var lh=s(Wn);ui=i(lh,"DPRQuestionEncoder"),lh.forEach(t),mi=i(ot,`, or a
`),Un=r(ot,"A",{href:!0});var ch=s(Un);gi=i(ch,"DPRReader"),ch.forEach(t),_i=i(ot,`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),$o=r(ot,"A",{href:!0,rel:!0});var ph=s($o);vi=i(ph,"facebook/dpr-ctx_encoder-single-nq-base"),ph.forEach(t),bi=i(ot,`
architecture.`),ot.forEach(t),Ti=c(co),Po=r(co,"P",{});var Ea=s(Po);ki=i(Ea,"This class is a subclass of "),Vn=r(Ea,"A",{href:!0});var hh=s(Vn);$i=i(hh,"BertConfig"),hh.forEach(t),Pi=i(Ea,". Please check the superclass for the documentation of all kwargs."),Ea.forEach(t),co.forEach(t),Ys=c(o),at=r(o,"H2",{class:!0});var Ra=s(at);zt=r(Ra,"A",{id:!0,class:!0,href:!0});var fh=s(zt);Or=r(fh,"SPAN",{});var uh=s(Or);$(wo.$$.fragment,uh),uh.forEach(t),fh.forEach(t),wi=c(Ra),Nr=r(Ra,"SPAN",{});var mh=s(Nr);Ei=i(mh,"DPRContextEncoderTokenizer"),mh.forEach(t),Ra.forEach(t),Xs=c(o),qe=r(o,"DIV",{class:!0});var po=s(qe);$(Eo.$$.fragment,po),Ri=c(po),jr=r(po,"P",{});var gh=s(jr);Di=i(gh,"Construct a DPRContextEncoder tokenizer."),gh.forEach(t),yi=c(po),qt=r(po,"P",{});var Ws=s(qt);Kn=r(Ws,"A",{href:!0});var _h=s(Kn);xi=i(_h,"DPRContextEncoderTokenizer"),_h.forEach(t),zi=i(Ws," is identical to "),Yn=r(Ws,"A",{href:!0});var vh=s(Yn);qi=i(vh,"BertTokenizer"),vh.forEach(t),Fi=i(Ws,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ws.forEach(t),Ci=c(po),Ro=r(po,"P",{});var Da=s(Ro);Ai=i(Da,"Refer to superclass "),Xn=r(Da,"A",{href:!0});var bh=s(Xn);Oi=i(bh,"BertTokenizer"),bh.forEach(t),Ni=i(Da," for usage examples and documentation concerning parameters."),Da.forEach(t),po.forEach(t),Js=c(o),it=r(o,"H2",{class:!0});var ya=s(it);Ft=r(ya,"A",{id:!0,class:!0,href:!0});var Th=s(Ft);Mr=r(Th,"SPAN",{});var kh=s(Mr);$(Do.$$.fragment,kh),kh.forEach(t),Th.forEach(t),ji=c(ya),Qr=r(ya,"SPAN",{});var $h=s(Qr);Mi=i($h,"DPRContextEncoderTokenizerFast"),$h.forEach(t),ya.forEach(t),Gs=c(o),Fe=r(o,"DIV",{class:!0});var ho=s(Fe);$(yo.$$.fragment,ho),Qi=c(ho),xo=r(ho,"P",{});var xa=s(xo);Li=i(xa,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Lr=r(xa,"EM",{});var Ph=s(Lr);Ii=i(Ph,"tokenizers"),Ph.forEach(t),Si=i(xa," library)."),xa.forEach(t),Hi=c(ho),Ct=r(ho,"P",{});var Us=s(Ct);Jn=r(Us,"A",{href:!0});var wh=s(Jn);Bi=i(wh,"DPRContextEncoderTokenizerFast"),wh.forEach(t),Wi=i(Us," is identical to "),Gn=r(Us,"A",{href:!0});var Eh=s(Gn);Ui=i(Eh,"BertTokenizerFast"),Eh.forEach(t),Vi=i(Us,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Us.forEach(t),Ki=c(ho),zo=r(ho,"P",{});var za=s(zo);Yi=i(za,"Refer to superclass "),Zn=r(za,"A",{href:!0});var Rh=s(Zn);Xi=i(Rh,"BertTokenizerFast"),Rh.forEach(t),Ji=i(za," for usage examples and documentation concerning parameters."),za.forEach(t),ho.forEach(t),Zs=c(o),dt=r(o,"H2",{class:!0});var qa=s(dt);At=r(qa,"A",{id:!0,class:!0,href:!0});var Dh=s(At);Ir=r(Dh,"SPAN",{});var yh=s(Ir);$(qo.$$.fragment,yh),yh.forEach(t),Dh.forEach(t),Gi=c(qa),Sr=r(qa,"SPAN",{});var xh=s(Sr);Zi=i(xh,"DPRQuestionEncoderTokenizer"),xh.forEach(t),qa.forEach(t),ea=c(o),Ce=r(o,"DIV",{class:!0});var fo=s(Ce);$(Fo.$$.fragment,fo),ed=c(fo),Hr=r(fo,"P",{});var zh=s(Hr);td=i(zh,"Constructs a DPRQuestionEncoder tokenizer."),zh.forEach(t),od=c(fo),Ot=r(fo,"P",{});var Vs=s(Ot);er=r(Vs,"A",{href:!0});var qh=s(er);nd=i(qh,"DPRQuestionEncoderTokenizer"),qh.forEach(t),rd=i(Vs," is identical to "),tr=r(Vs,"A",{href:!0});var Fh=s(tr);sd=i(Fh,"BertTokenizer"),Fh.forEach(t),ad=i(Vs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Vs.forEach(t),id=c(fo),Co=r(fo,"P",{});var Fa=s(Co);dd=i(Fa,"Refer to superclass "),or=r(Fa,"A",{href:!0});var Ch=s(or);ld=i(Ch,"BertTokenizer"),Ch.forEach(t),cd=i(Fa," for usage examples and documentation concerning parameters."),Fa.forEach(t),fo.forEach(t),ta=c(o),lt=r(o,"H2",{class:!0});var Ca=s(lt);Nt=r(Ca,"A",{id:!0,class:!0,href:!0});var Ah=s(Nt);Br=r(Ah,"SPAN",{});var Oh=s(Br);$(Ao.$$.fragment,Oh),Oh.forEach(t),Ah.forEach(t),pd=c(Ca),Wr=r(Ca,"SPAN",{});var Nh=s(Wr);hd=i(Nh,"DPRQuestionEncoderTokenizerFast"),Nh.forEach(t),Ca.forEach(t),oa=c(o),Ae=r(o,"DIV",{class:!0});var uo=s(Ae);$(Oo.$$.fragment,uo),fd=c(uo),No=r(uo,"P",{});var Aa=s(No);ud=i(Aa,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Ur=r(Aa,"EM",{});var jh=s(Ur);md=i(jh,"tokenizers"),jh.forEach(t),gd=i(Aa," library)."),Aa.forEach(t),_d=c(uo),jt=r(uo,"P",{});var Ks=s(jt);nr=r(Ks,"A",{href:!0});var Mh=s(nr);vd=i(Mh,"DPRQuestionEncoderTokenizerFast"),Mh.forEach(t),bd=i(Ks," is identical to "),rr=r(Ks,"A",{href:!0});var Qh=s(rr);Td=i(Qh,"BertTokenizerFast"),Qh.forEach(t),kd=i(Ks,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ks.forEach(t),$d=c(uo),jo=r(uo,"P",{});var Oa=s(jo);Pd=i(Oa,"Refer to superclass "),sr=r(Oa,"A",{href:!0});var Lh=s(sr);wd=i(Lh,"BertTokenizerFast"),Lh.forEach(t),Ed=i(Oa," for usage examples and documentation concerning parameters."),Oa.forEach(t),uo.forEach(t),na=c(o),ct=r(o,"H2",{class:!0});var Na=s(ct);Mt=r(Na,"A",{id:!0,class:!0,href:!0});var Ih=s(Mt);Vr=r(Ih,"SPAN",{});var Sh=s(Vr);$(Mo.$$.fragment,Sh),Sh.forEach(t),Ih.forEach(t),Rd=c(Na),Kr=r(Na,"SPAN",{});var Hh=s(Kr);Dd=i(Hh,"DPRReaderTokenizer"),Hh.forEach(t),Na.forEach(t),ra=c(o),ie=r(o,"DIV",{class:!0});var Ve=s(ie);$(Qo.$$.fragment,Ve),yd=c(Ve),Yr=r(Ve,"P",{});var Bh=s(Yr);xd=i(Bh,"Construct a DPRReader tokenizer."),Bh.forEach(t),zd=c(Ve),Ze=r(Ve,"P",{});var Mn=s(Ze);ar=r(Mn,"A",{href:!0});var Wh=s(ar);qd=i(Wh,"DPRReaderTokenizer"),Wh.forEach(t),Fd=i(Mn," is almost identical to "),ir=r(Mn,"A",{href:!0});var Uh=s(ir);Cd=i(Uh,"BertTokenizer"),Uh.forEach(t),Ad=i(Mn,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),dr=r(Mn,"A",{href:!0});var Vh=s(dr);Od=i(Vh,"DPRReader"),Vh.forEach(t),Nd=i(Mn," model."),Mn.forEach(t),jd=c(Ve),Lo=r(Ve,"P",{});var ja=s(Lo);Md=i(ja,"Refer to superclass "),lr=r(ja,"A",{href:!0});var Kh=s(lr);Qd=i(Kh,"BertTokenizer"),Kh.forEach(t),Ld=i(ja," for usage examples and documentation concerning parameters."),ja.forEach(t),Id=c(Ve),et=r(Ve,"P",{});var Qn=s(et);Sd=i(Qn,"Return a dictionary with the token ids of the input strings and other information to give to "),Xr=r(Qn,"CODE",{});var Yh=s(Xr);Hd=i(Yh,".decode_best_spans"),Yh.forEach(t),Bd=i(Qn,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Jr=r(Qn,"CODE",{});var Xh=s(Jr);Wd=i(Xh,"input_ids"),Xh.forEach(t),Ud=i(Qn," is a matrix of size "),Gr=r(Qn,"CODE",{});var Jh=s(Gr);Vd=i(Jh,"(n_passages, sequence_length)"),Jh.forEach(t),Qn.forEach(t),Kd=c(Ve),$(Qt.$$.fragment,Ve),Ve.forEach(t),sa=c(o),pt=r(o,"H2",{class:!0});var Ma=s(pt);Lt=r(Ma,"A",{id:!0,class:!0,href:!0});var Gh=s(Lt);Zr=r(Gh,"SPAN",{});var Zh=s(Zr);$(Io.$$.fragment,Zh),Zh.forEach(t),Gh.forEach(t),Yd=c(Ma),es=r(Ma,"SPAN",{});var ef=s(es);Xd=i(ef,"DPRReaderTokenizerFast"),ef.forEach(t),Ma.forEach(t),aa=c(o),de=r(o,"DIV",{class:!0});var Ke=s(de);$(So.$$.fragment,Ke),Jd=c(Ke),Ho=r(Ke,"P",{});var Qa=s(Ho);Gd=i(Qa,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),ts=r(Qa,"EM",{});var tf=s(ts);Zd=i(tf,"tokenizers"),tf.forEach(t),el=i(Qa," library)."),Qa.forEach(t),tl=c(Ke),tt=r(Ke,"P",{});var Ln=s(tt);cr=r(Ln,"A",{href:!0});var of=s(cr);ol=i(of,"DPRReaderTokenizerFast"),of.forEach(t),nl=i(Ln," is almost identical to "),pr=r(Ln,"A",{href:!0});var nf=s(pr);rl=i(nf,"BertTokenizerFast"),nf.forEach(t),sl=i(Ln,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),hr=r(Ln,"A",{href:!0});var rf=s(hr);al=i(rf,"DPRReader"),rf.forEach(t),il=i(Ln," model."),Ln.forEach(t),dl=c(Ke),Bo=r(Ke,"P",{});var La=s(Bo);ll=i(La,"Refer to superclass "),fr=r(La,"A",{href:!0});var sf=s(fr);cl=i(sf,"BertTokenizerFast"),sf.forEach(t),pl=i(La," for usage examples and documentation concerning parameters."),La.forEach(t),hl=c(Ke),Ge=r(Ke,"P",{});var mo=s(Ge);fl=i(mo,"Return a dictionary with the token ids of the input strings and other information to give to "),os=r(mo,"CODE",{});var af=s(os);ul=i(af,".decode_best_spans"),af.forEach(t),ml=i(mo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ns=r(mo,"CODE",{});var df=s(ns);gl=i(df,"input_ids"),df.forEach(t),_l=i(mo," is a matrix of size "),rs=r(mo,"CODE",{});var lf=s(rs);vl=i(lf,"(n_passages, sequence_length)"),lf.forEach(t),bl=i(mo,`
with the format:`),mo.forEach(t),Tl=c(Ke),ss=r(Ke,"P",{});var cf=s(ss);kl=i(cf,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),cf.forEach(t),Ke.forEach(t),ia=c(o),ht=r(o,"H2",{class:!0});var Ia=s(ht);It=r(Ia,"A",{id:!0,class:!0,href:!0});var pf=s(It);as=r(pf,"SPAN",{});var hf=s(as);$(Wo.$$.fragment,hf),hf.forEach(t),pf.forEach(t),$l=c(Ia),is=r(Ia,"SPAN",{});var ff=s(is);Pl=i(ff,"DPR specific outputs"),ff.forEach(t),Ia.forEach(t),da=c(o),ft=r(o,"DIV",{class:!0});var Sa=s(ft);$(Uo.$$.fragment,Sa),wl=c(Sa),Vo=r(Sa,"P",{});var Ha=s(Vo);El=i(Ha,"Class for outputs of "),ur=r(Ha,"A",{href:!0});var uf=s(ur);Rl=i(uf,"DPRQuestionEncoder"),uf.forEach(t),Dl=i(Ha,"."),Ha.forEach(t),Sa.forEach(t),la=c(o),ut=r(o,"DIV",{class:!0});var Ba=s(ut);$(Ko.$$.fragment,Ba),yl=c(Ba),Yo=r(Ba,"P",{});var Wa=s(Yo);xl=i(Wa,"Class for outputs of "),mr=r(Wa,"A",{href:!0});var mf=s(mr);zl=i(mf,"DPRQuestionEncoder"),mf.forEach(t),ql=i(Wa,"."),Wa.forEach(t),Ba.forEach(t),ca=c(o),mt=r(o,"DIV",{class:!0});var Ua=s(mt);$(Xo.$$.fragment,Ua),Fl=c(Ua),Jo=r(Ua,"P",{});var Va=s(Jo);Cl=i(Va,"Class for outputs of "),gr=r(Va,"A",{href:!0});var gf=s(gr);Al=i(gf,"DPRQuestionEncoder"),gf.forEach(t),Ol=i(Va,"."),Va.forEach(t),Ua.forEach(t),pa=c(o),gt=r(o,"H2",{class:!0});var Ka=s(gt);St=r(Ka,"A",{id:!0,class:!0,href:!0});var _f=s(St);ds=r(_f,"SPAN",{});var vf=s(ds);$(Go.$$.fragment,vf),vf.forEach(t),_f.forEach(t),Nl=c(Ka),ls=r(Ka,"SPAN",{});var bf=s(ls);jl=i(bf,"DPRContextEncoder"),bf.forEach(t),Ka.forEach(t),ha=c(o),Re=r(o,"DIV",{class:!0});var nt=s(Re);$(Zo.$$.fragment,nt),Ml=c(nt),cs=r(nt,"P",{});var Tf=s(cs);Ql=i(Tf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Tf.forEach(t),Ll=c(nt),en=r(nt,"P",{});var Ya=s(en);Il=i(Ya,"This model inherits from "),_r=r(Ya,"A",{href:!0});var kf=s(_r);Sl=i(kf,"PreTrainedModel"),kf.forEach(t),Hl=i(Ya,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ya.forEach(t),Bl=c(nt),tn=r(nt,"P",{});var Xa=s(tn);Wl=i(Xa,"This model is also a PyTorch "),on=r(Xa,"A",{href:!0,rel:!0});var $f=s(on);Ul=i($f,"torch.nn.Module"),$f.forEach(t),Vl=i(Xa,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xa.forEach(t),Kl=c(nt),Ie=r(nt,"DIV",{class:!0});var go=s(Ie);$(nn.$$.fragment,go),Yl=c(go),_t=r(go,"P",{});var xr=s(_t);Xl=i(xr,"The "),vr=r(xr,"A",{href:!0});var Pf=s(vr);Jl=i(Pf,"DPRContextEncoder"),Pf.forEach(t),Gl=i(xr," forward method, overrides the "),ps=r(xr,"CODE",{});var wf=s(ps);Zl=i(wf,"__call__"),wf.forEach(t),ec=i(xr," special method."),xr.forEach(t),tc=c(go),$(Ht.$$.fragment,go),oc=c(go),$(Bt.$$.fragment,go),go.forEach(t),nt.forEach(t),fa=c(o),vt=r(o,"H2",{class:!0});var Ja=s(vt);Wt=r(Ja,"A",{id:!0,class:!0,href:!0});var Ef=s(Wt);hs=r(Ef,"SPAN",{});var Rf=s(hs);$(rn.$$.fragment,Rf),Rf.forEach(t),Ef.forEach(t),nc=c(Ja),fs=r(Ja,"SPAN",{});var Df=s(fs);rc=i(Df,"DPRQuestionEncoder"),Df.forEach(t),Ja.forEach(t),ua=c(o),De=r(o,"DIV",{class:!0});var rt=s(De);$(sn.$$.fragment,rt),sc=c(rt),us=r(rt,"P",{});var yf=s(us);ac=i(yf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),yf.forEach(t),ic=c(rt),an=r(rt,"P",{});var Ga=s(an);dc=i(Ga,"This model inherits from "),br=r(Ga,"A",{href:!0});var xf=s(br);lc=i(xf,"PreTrainedModel"),xf.forEach(t),cc=i(Ga,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ga.forEach(t),pc=c(rt),dn=r(rt,"P",{});var Za=s(dn);hc=i(Za,"This model is also a PyTorch "),ln=r(Za,"A",{href:!0,rel:!0});var zf=s(ln);fc=i(zf,"torch.nn.Module"),zf.forEach(t),uc=i(Za,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Za.forEach(t),mc=c(rt),Se=r(rt,"DIV",{class:!0});var _o=s(Se);$(cn.$$.fragment,_o),gc=c(_o),bt=r(_o,"P",{});var zr=s(bt);_c=i(zr,"The "),Tr=r(zr,"A",{href:!0});var qf=s(Tr);vc=i(qf,"DPRQuestionEncoder"),qf.forEach(t),bc=i(zr," forward method, overrides the "),ms=r(zr,"CODE",{});var Ff=s(ms);Tc=i(Ff,"__call__"),Ff.forEach(t),kc=i(zr," special method."),zr.forEach(t),$c=c(_o),$(Ut.$$.fragment,_o),Pc=c(_o),$(Vt.$$.fragment,_o),_o.forEach(t),rt.forEach(t),ma=c(o),Tt=r(o,"H2",{class:!0});var ei=s(Tt);Kt=r(ei,"A",{id:!0,class:!0,href:!0});var Cf=s(Kt);gs=r(Cf,"SPAN",{});var Af=s(gs);$(pn.$$.fragment,Af),Af.forEach(t),Cf.forEach(t),wc=c(ei),_s=r(ei,"SPAN",{});var Of=s(_s);Ec=i(Of,"DPRReader"),Of.forEach(t),ei.forEach(t),ga=c(o),ye=r(o,"DIV",{class:!0});var st=s(ye);$(hn.$$.fragment,st),Rc=c(st),vs=r(st,"P",{});var Nf=s(vs);Dc=i(Nf,"The bare DPRReader transformer outputting span predictions."),Nf.forEach(t),yc=c(st),fn=r(st,"P",{});var ti=s(fn);xc=i(ti,"This model inherits from "),kr=r(ti,"A",{href:!0});var jf=s(kr);zc=i(jf,"PreTrainedModel"),jf.forEach(t),qc=i(ti,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ti.forEach(t),Fc=c(st),un=r(st,"P",{});var oi=s(un);Cc=i(oi,"This model is also a PyTorch "),mn=r(oi,"A",{href:!0,rel:!0});var Mf=s(mn);Ac=i(Mf,"torch.nn.Module"),Mf.forEach(t),Oc=i(oi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),oi.forEach(t),Nc=c(st),He=r(st,"DIV",{class:!0});var vo=s(He);$(gn.$$.fragment,vo),jc=c(vo),kt=r(vo,"P",{});var qr=s(kt);Mc=i(qr,"The "),$r=r(qr,"A",{href:!0});var Qf=s($r);Qc=i(Qf,"DPRReader"),Qf.forEach(t),Lc=i(qr," forward method, overrides the "),bs=r(qr,"CODE",{});var Lf=s(bs);Ic=i(Lf,"__call__"),Lf.forEach(t),Sc=i(qr," special method."),qr.forEach(t),Hc=c(vo),$(Yt.$$.fragment,vo),Bc=c(vo),$(Xt.$$.fragment,vo),vo.forEach(t),st.forEach(t),_a=c(o),$t=r(o,"H2",{class:!0});var ni=s($t);Jt=r(ni,"A",{id:!0,class:!0,href:!0});var If=s(Jt);Ts=r(If,"SPAN",{});var Sf=s(Ts);$(_n.$$.fragment,Sf),Sf.forEach(t),If.forEach(t),Wc=c(ni),ks=r(ni,"SPAN",{});var Hf=s(ks);Uc=i(Hf,"TFDPRContextEncoder"),Hf.forEach(t),ni.forEach(t),va=c(o),le=r(o,"DIV",{class:!0});var Ye=s(le);$(vn.$$.fragment,Ye),Vc=c(Ye),$s=r(Ye,"P",{});var Bf=s($s);Kc=i(Bf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Bf.forEach(t),Yc=c(Ye),bn=r(Ye,"P",{});var ri=s(bn);Xc=i(ri,"This model inherits from "),Pr=r(ri,"A",{href:!0});var Wf=s(Pr);Jc=i(Wf,"TFPreTrainedModel"),Wf.forEach(t),Gc=i(ri,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ri.forEach(t),Zc=c(Ye),Tn=r(Ye,"P",{});var si=s(Tn);ep=i(si,"This model is also a Tensorflow "),kn=r(si,"A",{href:!0,rel:!0});var Uf=s(kn);tp=i(Uf,"tf.keras.Model"),Uf.forEach(t),op=i(si,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),si.forEach(t),np=c(Ye),$(Gt.$$.fragment,Ye),rp=c(Ye),Be=r(Ye,"DIV",{class:!0});var bo=s(Be);$($n.$$.fragment,bo),sp=c(bo),Pt=r(bo,"P",{});var Fr=s(Pt);ap=i(Fr,"The "),wr=r(Fr,"A",{href:!0});var Vf=s(wr);ip=i(Vf,"TFDPRContextEncoder"),Vf.forEach(t),dp=i(Fr," forward method, overrides the "),Ps=r(Fr,"CODE",{});var Kf=s(Ps);lp=i(Kf,"__call__"),Kf.forEach(t),cp=i(Fr," special method."),Fr.forEach(t),pp=c(bo),$(Zt.$$.fragment,bo),hp=c(bo),$(eo.$$.fragment,bo),bo.forEach(t),Ye.forEach(t),ba=c(o),wt=r(o,"H2",{class:!0});var ai=s(wt);to=r(ai,"A",{id:!0,class:!0,href:!0});var Yf=s(to);ws=r(Yf,"SPAN",{});var Xf=s(ws);$(Pn.$$.fragment,Xf),Xf.forEach(t),Yf.forEach(t),fp=c(ai),Es=r(ai,"SPAN",{});var Jf=s(Es);up=i(Jf,"TFDPRQuestionEncoder"),Jf.forEach(t),ai.forEach(t),Ta=c(o),ce=r(o,"DIV",{class:!0});var Xe=s(ce);$(wn.$$.fragment,Xe),mp=c(Xe),Rs=r(Xe,"P",{});var Gf=s(Rs);gp=i(Gf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Gf.forEach(t),_p=c(Xe),En=r(Xe,"P",{});var ii=s(En);vp=i(ii,"This model inherits from "),Er=r(ii,"A",{href:!0});var Zf=s(Er);bp=i(Zf,"TFPreTrainedModel"),Zf.forEach(t),Tp=i(ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ii.forEach(t),kp=c(Xe),Rn=r(Xe,"P",{});var di=s(Rn);$p=i(di,"This model is also a Tensorflow "),Dn=r(di,"A",{href:!0,rel:!0});var eu=s(Dn);Pp=i(eu,"tf.keras.Model"),eu.forEach(t),wp=i(di,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),di.forEach(t),Ep=c(Xe),$(oo.$$.fragment,Xe),Rp=c(Xe),We=r(Xe,"DIV",{class:!0});var To=s(We);$(yn.$$.fragment,To),Dp=c(To),Et=r(To,"P",{});var Cr=s(Et);yp=i(Cr,"The "),Rr=r(Cr,"A",{href:!0});var tu=s(Rr);xp=i(tu,"TFDPRQuestionEncoder"),tu.forEach(t),zp=i(Cr," forward method, overrides the "),Ds=r(Cr,"CODE",{});var ou=s(Ds);qp=i(ou,"__call__"),ou.forEach(t),Fp=i(Cr," special method."),Cr.forEach(t),Cp=c(To),$(no.$$.fragment,To),Ap=c(To),$(ro.$$.fragment,To),To.forEach(t),Xe.forEach(t),ka=c(o),Rt=r(o,"H2",{class:!0});var li=s(Rt);so=r(li,"A",{id:!0,class:!0,href:!0});var nu=s(so);ys=r(nu,"SPAN",{});var ru=s(ys);$(xn.$$.fragment,ru),ru.forEach(t),nu.forEach(t),Op=c(li),xs=r(li,"SPAN",{});var su=s(xs);Np=i(su,"TFDPRReader"),su.forEach(t),li.forEach(t),$a=c(o),pe=r(o,"DIV",{class:!0});var Je=s(pe);$(zn.$$.fragment,Je),jp=c(Je),zs=r(Je,"P",{});var au=s(zs);Mp=i(au,"The bare DPRReader transformer outputting span predictions."),au.forEach(t),Qp=c(Je),qn=r(Je,"P",{});var ci=s(qn);Lp=i(ci,"This model inherits from "),Dr=r(ci,"A",{href:!0});var iu=s(Dr);Ip=i(iu,"TFPreTrainedModel"),iu.forEach(t),Sp=i(ci,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ci.forEach(t),Hp=c(Je),Fn=r(Je,"P",{});var pi=s(Fn);Bp=i(pi,"This model is also a Tensorflow "),Cn=r(pi,"A",{href:!0,rel:!0});var du=s(Cn);Wp=i(du,"tf.keras.Model"),du.forEach(t),Up=i(pi,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),pi.forEach(t),Vp=c(Je),$(ao.$$.fragment,Je),Kp=c(Je),Ue=r(Je,"DIV",{class:!0});var ko=s(Ue);$(An.$$.fragment,ko),Yp=c(ko),Dt=r(ko,"P",{});var Ar=s(Dt);Xp=i(Ar,"The "),yr=r(Ar,"A",{href:!0});var lu=s(yr);Jp=i(lu,"TFDPRReader"),lu.forEach(t),Gp=i(Ar," forward method, overrides the "),qs=r(Ar,"CODE",{});var cu=s(qs);Zp=i(cu,"__call__"),cu.forEach(t),eh=i(Ar," special method."),Ar.forEach(t),th=c(ko),$(io.$$.fragment,ko),oh=c(ko),$(lo.$$.fragment,ko),ko.forEach(t),Je.forEach(t),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(Cu)),p(u,"id","dpr"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#dpr"),p(v,"class","relative group"),p(J,"id","overview"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#overview"),p(D,"class","relative group"),p(te,"href","https://arxiv.org/abs/2004.04906"),p(te,"rel","nofollow"),p(j,"href","https://huggingface.co/lhoestq"),p(j,"rel","nofollow"),p(M,"href","https://github.com/facebookresearch/DPR"),p(M,"rel","nofollow"),p(N,"id","transformers.DPRConfig"),p(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(N,"href","#transformers.DPRConfig"),p(K,"class","relative group"),p(O,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRConfig"),p(Bn,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Wn,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Un,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReader"),p($o,"href","https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base"),p($o,"rel","nofollow"),p(Vn,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertConfig"),p(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(zt,"id","transformers.DPRContextEncoderTokenizer"),p(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(zt,"href","#transformers.DPRContextEncoderTokenizer"),p(at,"class","relative group"),p(Kn,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),p(Yn,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(Xn,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ft,"id","transformers.DPRContextEncoderTokenizerFast"),p(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ft,"href","#transformers.DPRContextEncoderTokenizerFast"),p(it,"class","relative group"),p(Jn,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),p(Gn,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(Zn,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(At,"id","transformers.DPRQuestionEncoderTokenizer"),p(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(At,"href","#transformers.DPRQuestionEncoderTokenizer"),p(dt,"class","relative group"),p(er,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),p(tr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(or,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Nt,"id","transformers.DPRQuestionEncoderTokenizerFast"),p(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Nt,"href","#transformers.DPRQuestionEncoderTokenizerFast"),p(lt,"class","relative group"),p(nr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),p(rr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(sr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Mt,"id","transformers.DPRReaderTokenizer"),p(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Mt,"href","#transformers.DPRReaderTokenizer"),p(ct,"class","relative group"),p(ar,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReaderTokenizer"),p(ir,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(dr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReader"),p(lr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizer"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Lt,"id","transformers.DPRReaderTokenizerFast"),p(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Lt,"href","#transformers.DPRReaderTokenizerFast"),p(pt,"class","relative group"),p(cr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),p(pr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(hr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReader"),p(fr,"href","/docs/transformers/pr_17196/en/model_doc/bert#transformers.BertTokenizerFast"),p(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(It,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(It,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(ht,"class","relative group"),p(ur,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(mr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(gr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(St,"id","transformers.DPRContextEncoder"),p(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(St,"href","#transformers.DPRContextEncoder"),p(gt,"class","relative group"),p(_r,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),p(on,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(on,"rel","nofollow"),p(vr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Wt,"id","transformers.DPRQuestionEncoder"),p(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Wt,"href","#transformers.DPRQuestionEncoder"),p(vt,"class","relative group"),p(br,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),p(ln,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(ln,"rel","nofollow"),p(Tr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Kt,"id","transformers.DPRReader"),p(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Kt,"href","#transformers.DPRReader"),p(Tt,"class","relative group"),p(kr,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.PreTrainedModel"),p(mn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(mn,"rel","nofollow"),p($r,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.DPRReader"),p(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Jt,"id","transformers.TFDPRContextEncoder"),p(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Jt,"href","#transformers.TFDPRContextEncoder"),p($t,"class","relative group"),p(Pr,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel"),p(kn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(kn,"rel","nofollow"),p(wr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.TFDPRContextEncoder"),p(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(to,"id","transformers.TFDPRQuestionEncoder"),p(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(to,"href","#transformers.TFDPRQuestionEncoder"),p(wt,"class","relative group"),p(Er,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel"),p(Dn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Dn,"rel","nofollow"),p(Rr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),p(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(so,"id","transformers.TFDPRReader"),p(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(so,"href","#transformers.TFDPRReader"),p(Rt,"class","relative group"),p(Dr,"href","/docs/transformers/pr_17196/en/main_classes/model#transformers.TFPreTrainedModel"),p(Cn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Cn,"rel","nofollow"),p(yr,"href","/docs/transformers/pr_17196/en/model_doc/dpr#transformers.TFDPRReader"),p(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,T,m),f(o,v,m),e(v,u),e(u,b),P(d,b,null),e(v,g),e(v,x),e(x,_e),f(o,Z,m),f(o,D,m),e(D,J),e(J,S),P(ee,S,null),e(D,ve),e(D,H),e(H,be),f(o,he,m),f(o,B,m),e(B,I),e(B,te),e(te,oe),e(B,q),f(o,A,m),f(o,ne,m),e(ne,V),f(o,fe,m),f(o,re,m),e(re,W),e(W,Te),f(o,ue,m),f(o,z,m),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(z,Ee),f(o,Q,m),f(o,K,m),e(K,N),e(N,se),P(_,se,null),e(K,y),e(K,G),e(G,Oe),f(o,ze,m),f(o,C,m),P(me,C,null),e(C,Ne),e(C,ae),e(ae,O),e(O,Y),e(ae,je),e(ae,xe),e(xe,X),e(ae,Me),e(C,Qe),e(C,L),e(L,Le),e(L,Bn),e(Bn,hi),e(L,fi),e(L,Wn),e(Wn,ui),e(L,mi),e(L,Un),e(Un,gi),e(L,_i),e(L,$o),e($o,vi),e(L,bi),e(C,Ti),e(C,Po),e(Po,ki),e(Po,Vn),e(Vn,$i),e(Po,Pi),f(o,Ys,m),f(o,at,m),e(at,zt),e(zt,Or),P(wo,Or,null),e(at,wi),e(at,Nr),e(Nr,Ei),f(o,Xs,m),f(o,qe,m),P(Eo,qe,null),e(qe,Ri),e(qe,jr),e(jr,Di),e(qe,yi),e(qe,qt),e(qt,Kn),e(Kn,xi),e(qt,zi),e(qt,Yn),e(Yn,qi),e(qt,Fi),e(qe,Ci),e(qe,Ro),e(Ro,Ai),e(Ro,Xn),e(Xn,Oi),e(Ro,Ni),f(o,Js,m),f(o,it,m),e(it,Ft),e(Ft,Mr),P(Do,Mr,null),e(it,ji),e(it,Qr),e(Qr,Mi),f(o,Gs,m),f(o,Fe,m),P(yo,Fe,null),e(Fe,Qi),e(Fe,xo),e(xo,Li),e(xo,Lr),e(Lr,Ii),e(xo,Si),e(Fe,Hi),e(Fe,Ct),e(Ct,Jn),e(Jn,Bi),e(Ct,Wi),e(Ct,Gn),e(Gn,Ui),e(Ct,Vi),e(Fe,Ki),e(Fe,zo),e(zo,Yi),e(zo,Zn),e(Zn,Xi),e(zo,Ji),f(o,Zs,m),f(o,dt,m),e(dt,At),e(At,Ir),P(qo,Ir,null),e(dt,Gi),e(dt,Sr),e(Sr,Zi),f(o,ea,m),f(o,Ce,m),P(Fo,Ce,null),e(Ce,ed),e(Ce,Hr),e(Hr,td),e(Ce,od),e(Ce,Ot),e(Ot,er),e(er,nd),e(Ot,rd),e(Ot,tr),e(tr,sd),e(Ot,ad),e(Ce,id),e(Ce,Co),e(Co,dd),e(Co,or),e(or,ld),e(Co,cd),f(o,ta,m),f(o,lt,m),e(lt,Nt),e(Nt,Br),P(Ao,Br,null),e(lt,pd),e(lt,Wr),e(Wr,hd),f(o,oa,m),f(o,Ae,m),P(Oo,Ae,null),e(Ae,fd),e(Ae,No),e(No,ud),e(No,Ur),e(Ur,md),e(No,gd),e(Ae,_d),e(Ae,jt),e(jt,nr),e(nr,vd),e(jt,bd),e(jt,rr),e(rr,Td),e(jt,kd),e(Ae,$d),e(Ae,jo),e(jo,Pd),e(jo,sr),e(sr,wd),e(jo,Ed),f(o,na,m),f(o,ct,m),e(ct,Mt),e(Mt,Vr),P(Mo,Vr,null),e(ct,Rd),e(ct,Kr),e(Kr,Dd),f(o,ra,m),f(o,ie,m),P(Qo,ie,null),e(ie,yd),e(ie,Yr),e(Yr,xd),e(ie,zd),e(ie,Ze),e(Ze,ar),e(ar,qd),e(Ze,Fd),e(Ze,ir),e(ir,Cd),e(Ze,Ad),e(Ze,dr),e(dr,Od),e(Ze,Nd),e(ie,jd),e(ie,Lo),e(Lo,Md),e(Lo,lr),e(lr,Qd),e(Lo,Ld),e(ie,Id),e(ie,et),e(et,Sd),e(et,Xr),e(Xr,Hd),e(et,Bd),e(et,Jr),e(Jr,Wd),e(et,Ud),e(et,Gr),e(Gr,Vd),e(ie,Kd),P(Qt,ie,null),f(o,sa,m),f(o,pt,m),e(pt,Lt),e(Lt,Zr),P(Io,Zr,null),e(pt,Yd),e(pt,es),e(es,Xd),f(o,aa,m),f(o,de,m),P(So,de,null),e(de,Jd),e(de,Ho),e(Ho,Gd),e(Ho,ts),e(ts,Zd),e(Ho,el),e(de,tl),e(de,tt),e(tt,cr),e(cr,ol),e(tt,nl),e(tt,pr),e(pr,rl),e(tt,sl),e(tt,hr),e(hr,al),e(tt,il),e(de,dl),e(de,Bo),e(Bo,ll),e(Bo,fr),e(fr,cl),e(Bo,pl),e(de,hl),e(de,Ge),e(Ge,fl),e(Ge,os),e(os,ul),e(Ge,ml),e(Ge,ns),e(ns,gl),e(Ge,_l),e(Ge,rs),e(rs,vl),e(Ge,bl),e(de,Tl),e(de,ss),e(ss,kl),f(o,ia,m),f(o,ht,m),e(ht,It),e(It,as),P(Wo,as,null),e(ht,$l),e(ht,is),e(is,Pl),f(o,da,m),f(o,ft,m),P(Uo,ft,null),e(ft,wl),e(ft,Vo),e(Vo,El),e(Vo,ur),e(ur,Rl),e(Vo,Dl),f(o,la,m),f(o,ut,m),P(Ko,ut,null),e(ut,yl),e(ut,Yo),e(Yo,xl),e(Yo,mr),e(mr,zl),e(Yo,ql),f(o,ca,m),f(o,mt,m),P(Xo,mt,null),e(mt,Fl),e(mt,Jo),e(Jo,Cl),e(Jo,gr),e(gr,Al),e(Jo,Ol),f(o,pa,m),f(o,gt,m),e(gt,St),e(St,ds),P(Go,ds,null),e(gt,Nl),e(gt,ls),e(ls,jl),f(o,ha,m),f(o,Re,m),P(Zo,Re,null),e(Re,Ml),e(Re,cs),e(cs,Ql),e(Re,Ll),e(Re,en),e(en,Il),e(en,_r),e(_r,Sl),e(en,Hl),e(Re,Bl),e(Re,tn),e(tn,Wl),e(tn,on),e(on,Ul),e(tn,Vl),e(Re,Kl),e(Re,Ie),P(nn,Ie,null),e(Ie,Yl),e(Ie,_t),e(_t,Xl),e(_t,vr),e(vr,Jl),e(_t,Gl),e(_t,ps),e(ps,Zl),e(_t,ec),e(Ie,tc),P(Ht,Ie,null),e(Ie,oc),P(Bt,Ie,null),f(o,fa,m),f(o,vt,m),e(vt,Wt),e(Wt,hs),P(rn,hs,null),e(vt,nc),e(vt,fs),e(fs,rc),f(o,ua,m),f(o,De,m),P(sn,De,null),e(De,sc),e(De,us),e(us,ac),e(De,ic),e(De,an),e(an,dc),e(an,br),e(br,lc),e(an,cc),e(De,pc),e(De,dn),e(dn,hc),e(dn,ln),e(ln,fc),e(dn,uc),e(De,mc),e(De,Se),P(cn,Se,null),e(Se,gc),e(Se,bt),e(bt,_c),e(bt,Tr),e(Tr,vc),e(bt,bc),e(bt,ms),e(ms,Tc),e(bt,kc),e(Se,$c),P(Ut,Se,null),e(Se,Pc),P(Vt,Se,null),f(o,ma,m),f(o,Tt,m),e(Tt,Kt),e(Kt,gs),P(pn,gs,null),e(Tt,wc),e(Tt,_s),e(_s,Ec),f(o,ga,m),f(o,ye,m),P(hn,ye,null),e(ye,Rc),e(ye,vs),e(vs,Dc),e(ye,yc),e(ye,fn),e(fn,xc),e(fn,kr),e(kr,zc),e(fn,qc),e(ye,Fc),e(ye,un),e(un,Cc),e(un,mn),e(mn,Ac),e(un,Oc),e(ye,Nc),e(ye,He),P(gn,He,null),e(He,jc),e(He,kt),e(kt,Mc),e(kt,$r),e($r,Qc),e(kt,Lc),e(kt,bs),e(bs,Ic),e(kt,Sc),e(He,Hc),P(Yt,He,null),e(He,Bc),P(Xt,He,null),f(o,_a,m),f(o,$t,m),e($t,Jt),e(Jt,Ts),P(_n,Ts,null),e($t,Wc),e($t,ks),e(ks,Uc),f(o,va,m),f(o,le,m),P(vn,le,null),e(le,Vc),e(le,$s),e($s,Kc),e(le,Yc),e(le,bn),e(bn,Xc),e(bn,Pr),e(Pr,Jc),e(bn,Gc),e(le,Zc),e(le,Tn),e(Tn,ep),e(Tn,kn),e(kn,tp),e(Tn,op),e(le,np),P(Gt,le,null),e(le,rp),e(le,Be),P($n,Be,null),e(Be,sp),e(Be,Pt),e(Pt,ap),e(Pt,wr),e(wr,ip),e(Pt,dp),e(Pt,Ps),e(Ps,lp),e(Pt,cp),e(Be,pp),P(Zt,Be,null),e(Be,hp),P(eo,Be,null),f(o,ba,m),f(o,wt,m),e(wt,to),e(to,ws),P(Pn,ws,null),e(wt,fp),e(wt,Es),e(Es,up),f(o,Ta,m),f(o,ce,m),P(wn,ce,null),e(ce,mp),e(ce,Rs),e(Rs,gp),e(ce,_p),e(ce,En),e(En,vp),e(En,Er),e(Er,bp),e(En,Tp),e(ce,kp),e(ce,Rn),e(Rn,$p),e(Rn,Dn),e(Dn,Pp),e(Rn,wp),e(ce,Ep),P(oo,ce,null),e(ce,Rp),e(ce,We),P(yn,We,null),e(We,Dp),e(We,Et),e(Et,yp),e(Et,Rr),e(Rr,xp),e(Et,zp),e(Et,Ds),e(Ds,qp),e(Et,Fp),e(We,Cp),P(no,We,null),e(We,Ap),P(ro,We,null),f(o,ka,m),f(o,Rt,m),e(Rt,so),e(so,ys),P(xn,ys,null),e(Rt,Op),e(Rt,xs),e(xs,Np),f(o,$a,m),f(o,pe,m),P(zn,pe,null),e(pe,jp),e(pe,zs),e(zs,Mp),e(pe,Qp),e(pe,qn),e(qn,Lp),e(qn,Dr),e(Dr,Ip),e(qn,Sp),e(pe,Hp),e(pe,Fn),e(Fn,Bp),e(Fn,Cn),e(Cn,Wp),e(Fn,Up),e(pe,Vp),P(ao,pe,null),e(pe,Kp),e(pe,Ue),P(An,Ue,null),e(Ue,Yp),e(Ue,Dt),e(Dt,Xp),e(Dt,yr),e(yr,Jp),e(Dt,Gp),e(Dt,qs),e(qs,Zp),e(Dt,eh),e(Ue,th),P(io,Ue,null),e(Ue,oh),P(lo,Ue,null),Pa=!0},p(o,[m]){const On={};m&2&&(On.$$scope={dirty:m,ctx:o}),Qt.$set(On);const Fs={};m&2&&(Fs.$$scope={dirty:m,ctx:o}),Ht.$set(Fs);const Cs={};m&2&&(Cs.$$scope={dirty:m,ctx:o}),Bt.$set(Cs);const As={};m&2&&(As.$$scope={dirty:m,ctx:o}),Ut.$set(As);const Nn={};m&2&&(Nn.$$scope={dirty:m,ctx:o}),Vt.$set(Nn);const Os={};m&2&&(Os.$$scope={dirty:m,ctx:o}),Yt.$set(Os);const Ns={};m&2&&(Ns.$$scope={dirty:m,ctx:o}),Xt.$set(Ns);const js={};m&2&&(js.$$scope={dirty:m,ctx:o}),Gt.$set(js);const jn={};m&2&&(jn.$$scope={dirty:m,ctx:o}),Zt.$set(jn);const Ms={};m&2&&(Ms.$$scope={dirty:m,ctx:o}),eo.$set(Ms);const Qs={};m&2&&(Qs.$$scope={dirty:m,ctx:o}),oo.$set(Qs);const Ls={};m&2&&(Ls.$$scope={dirty:m,ctx:o}),no.$set(Ls);const Is={};m&2&&(Is.$$scope={dirty:m,ctx:o}),ro.$set(Is);const yt={};m&2&&(yt.$$scope={dirty:m,ctx:o}),ao.$set(yt);const Ss={};m&2&&(Ss.$$scope={dirty:m,ctx:o}),io.$set(Ss);const Hs={};m&2&&(Hs.$$scope={dirty:m,ctx:o}),lo.$set(Hs)},i(o){Pa||(w(d.$$.fragment,o),w(ee.$$.fragment,o),w(_.$$.fragment,o),w(me.$$.fragment,o),w(wo.$$.fragment,o),w(Eo.$$.fragment,o),w(Do.$$.fragment,o),w(yo.$$.fragment,o),w(qo.$$.fragment,o),w(Fo.$$.fragment,o),w(Ao.$$.fragment,o),w(Oo.$$.fragment,o),w(Mo.$$.fragment,o),w(Qo.$$.fragment,o),w(Qt.$$.fragment,o),w(Io.$$.fragment,o),w(So.$$.fragment,o),w(Wo.$$.fragment,o),w(Uo.$$.fragment,o),w(Ko.$$.fragment,o),w(Xo.$$.fragment,o),w(Go.$$.fragment,o),w(Zo.$$.fragment,o),w(nn.$$.fragment,o),w(Ht.$$.fragment,o),w(Bt.$$.fragment,o),w(rn.$$.fragment,o),w(sn.$$.fragment,o),w(cn.$$.fragment,o),w(Ut.$$.fragment,o),w(Vt.$$.fragment,o),w(pn.$$.fragment,o),w(hn.$$.fragment,o),w(gn.$$.fragment,o),w(Yt.$$.fragment,o),w(Xt.$$.fragment,o),w(_n.$$.fragment,o),w(vn.$$.fragment,o),w(Gt.$$.fragment,o),w($n.$$.fragment,o),w(Zt.$$.fragment,o),w(eo.$$.fragment,o),w(Pn.$$.fragment,o),w(wn.$$.fragment,o),w(oo.$$.fragment,o),w(yn.$$.fragment,o),w(no.$$.fragment,o),w(ro.$$.fragment,o),w(xn.$$.fragment,o),w(zn.$$.fragment,o),w(ao.$$.fragment,o),w(An.$$.fragment,o),w(io.$$.fragment,o),w(lo.$$.fragment,o),Pa=!0)},o(o){E(d.$$.fragment,o),E(ee.$$.fragment,o),E(_.$$.fragment,o),E(me.$$.fragment,o),E(wo.$$.fragment,o),E(Eo.$$.fragment,o),E(Do.$$.fragment,o),E(yo.$$.fragment,o),E(qo.$$.fragment,o),E(Fo.$$.fragment,o),E(Ao.$$.fragment,o),E(Oo.$$.fragment,o),E(Mo.$$.fragment,o),E(Qo.$$.fragment,o),E(Qt.$$.fragment,o),E(Io.$$.fragment,o),E(So.$$.fragment,o),E(Wo.$$.fragment,o),E(Uo.$$.fragment,o),E(Ko.$$.fragment,o),E(Xo.$$.fragment,o),E(Go.$$.fragment,o),E(Zo.$$.fragment,o),E(nn.$$.fragment,o),E(Ht.$$.fragment,o),E(Bt.$$.fragment,o),E(rn.$$.fragment,o),E(sn.$$.fragment,o),E(cn.$$.fragment,o),E(Ut.$$.fragment,o),E(Vt.$$.fragment,o),E(pn.$$.fragment,o),E(hn.$$.fragment,o),E(gn.$$.fragment,o),E(Yt.$$.fragment,o),E(Xt.$$.fragment,o),E(_n.$$.fragment,o),E(vn.$$.fragment,o),E(Gt.$$.fragment,o),E($n.$$.fragment,o),E(Zt.$$.fragment,o),E(eo.$$.fragment,o),E(Pn.$$.fragment,o),E(wn.$$.fragment,o),E(oo.$$.fragment,o),E(yn.$$.fragment,o),E(no.$$.fragment,o),E(ro.$$.fragment,o),E(xn.$$.fragment,o),E(zn.$$.fragment,o),E(ao.$$.fragment,o),E(An.$$.fragment,o),E(io.$$.fragment,o),E(lo.$$.fragment,o),Pa=!1},d(o){t(h),o&&t(T),o&&t(v),R(d),o&&t(Z),o&&t(D),R(ee),o&&t(he),o&&t(B),o&&t(A),o&&t(ne),o&&t(fe),o&&t(re),o&&t(ue),o&&t(z),o&&t(Q),o&&t(K),R(_),o&&t(ze),o&&t(C),R(me),o&&t(Ys),o&&t(at),R(wo),o&&t(Xs),o&&t(qe),R(Eo),o&&t(Js),o&&t(it),R(Do),o&&t(Gs),o&&t(Fe),R(yo),o&&t(Zs),o&&t(dt),R(qo),o&&t(ea),o&&t(Ce),R(Fo),o&&t(ta),o&&t(lt),R(Ao),o&&t(oa),o&&t(Ae),R(Oo),o&&t(na),o&&t(ct),R(Mo),o&&t(ra),o&&t(ie),R(Qo),R(Qt),o&&t(sa),o&&t(pt),R(Io),o&&t(aa),o&&t(de),R(So),o&&t(ia),o&&t(ht),R(Wo),o&&t(da),o&&t(ft),R(Uo),o&&t(la),o&&t(ut),R(Ko),o&&t(ca),o&&t(mt),R(Xo),o&&t(pa),o&&t(gt),R(Go),o&&t(ha),o&&t(Re),R(Zo),R(nn),R(Ht),R(Bt),o&&t(fa),o&&t(vt),R(rn),o&&t(ua),o&&t(De),R(sn),R(cn),R(Ut),R(Vt),o&&t(ma),o&&t(Tt),R(pn),o&&t(ga),o&&t(ye),R(hn),R(gn),R(Yt),R(Xt),o&&t(_a),o&&t($t),R(_n),o&&t(va),o&&t(le),R(vn),R(Gt),R($n),R(Zt),R(eo),o&&t(ba),o&&t(wt),R(Pn),o&&t(Ta),o&&t(ce),R(wn),R(oo),R(yn),R(no),R(ro),o&&t(ka),o&&t(Rt),R(xn),o&&t($a),o&&t(pe),R(zn),R(ao),R(An),R(io),R(lo)}}}const Cu={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function Au(F){return mu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Iu extends pu{constructor(h){super();hu(this,h,Au,Fu,fu,{})}}export{Iu as default,Cu as metadata};
