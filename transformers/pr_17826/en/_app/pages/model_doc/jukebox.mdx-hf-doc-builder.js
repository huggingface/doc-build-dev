import{S as Ha,i as Oa,s as Ra,e as t,k as c,t as s,w as W,M as Va,c as n,d as o,m as u,a as r,h as i,x as L,b as d,G as a,g as m,y as D,q as H,o as O,B as R,v as Za,L as Ga}from"../../chunks/vendor-hf-doc-builder.js";import{D as Ba}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ka}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as qe}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Qa}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ua(ze){let p,j,$,v,g;return v=new Ka({props:{code:`from transformers import JukeboxModel, JukeboxConfig

# Initializing a Jukebox configuration
configuration = JukeboxConfig()

# Initializing a model from the configuration
model = JukeboxModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxModel, JukeboxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Jukebox configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = JukeboxConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=t("p"),j=s("Example:"),$=c(),W(v.$$.fragment)},l(f){p=n(f,"P",{});var b=r(p);j=i(b,"Example:"),b.forEach(o),$=u(f),L(v.$$.fragment,f)},m(f,b){m(f,p,b),a(p,j),m(f,$,b),D(v,f,b),g=!0},p:Ga,i(f){g||(H(v.$$.fragment,f),g=!0)},o(f){O(v.$$.fragment,f),g=!1},d(f){f&&o(p),f&&o($),R(v,f)}}}function Xa(ze){let p,j,$,v,g,f,b,_,he,so,io,_e,mo,co,V,uo,Z,fo,po,ho,ve,_o,vo,ge,go,Ne,Se,Fe,J,P,be,G,bo,ke,ko,Ie,E,q,xe,B,xo,we,wo,Me,z,yo,K,$o,Jo,We,re,Eo,Le,le,To,De,se,Co,He,ie,Ao,Oe,me,jo,Re,x,Po,Q,qo,zo,U,No,So,Ve,T,N,ye,X,Fo,$e,Io,Ze,h,Y,Mo,ee,Wo,Je,Lo,Do,Ho,k,Oo,de,Ro,Vo,ce,Zo,Go,oe,Bo,Ko,Qo,ae,Uo,Ee,Xo,Yo,ea,S,Ge,C,F,Te,te,oa,Ce,aa,Be,ue,ta,Ke,A,I,Ae,ne,na,je,ra,Qe,fe,la,Ue;return G=new qe({}),B=new qe({}),X=new qe({}),Y=new Ba({props:{name:"class transformers.JukeboxConfig",anchor:"transformers.JukeboxConfig",parameters:[{name:"vocab_size",val:" = 50257"},{name:"n_positions",val:" = 1024"},{name:"n_embd",val:" = 768"},{name:"n_layer",val:" = 12"},{name:"n_head",val:" = 12"},{name:"n_inner",val:" = None"},{name:"emb_dropout",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"scale_attn_weights",val:" = True"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 50256"},{name:"eos_token_id",val:" = 50256"},{name:"scale_attn_by_inverse_layer_idx",val:" = False"},{name:"reorder_and_upcast_attn",val:" = False"},{name:"sr",val:" = 16000"},{name:"sample_length",val:" = None"},{name:"sample_length_in_seconds",val:" = 1"},{name:"y_bins",val:" = [(120, 4111), (120, 4111), (120, 4111)]"},{name:"use_nonrelative_specloss",val:" = True"},{name:"copy_input",val:" = False"},{name:"resid_dropout",val:" = 0.0"},{name:"mlp_init_scale",val:" = 0.02"},{name:"attn_dropout",val:" = 0.0"},{name:"attn_init_scale",val:" = 1.0"},{name:"activation_function",val:" = 'gelu_new'"},{name:"sample_hop_length",val:" = 30000"},{name:"hop_length",val:" = 256"},{name:"multispec_loss_n_fft",val:" = (2048, 1024, 512)"},{name:"multispec_loss_hop_length",val:" = (240, 120, 50)"},{name:"multispec_loss_window_size",val:" = (1200, 600, 240)"},{name:"vq_vae_levels",val:" = 3"},{name:"vq_vae_downs_t",val:" = (3, 2, 2)"},{name:"vq_vae_strides_t",val:" = (2, 2, 2)"},{name:"vq_vae_emmbedding_width",val:" = 2048"},{name:"vq_vae_codebook_dimension",val:" = 2048"},{name:"vq_vae_width",val:" = 64"},{name:"vq_vae_depth",val:" = 4"},{name:"vq_vae_m_conv",val:" = 1"},{name:"vq_vae_dilation_growth_rate",val:" = 3"},{name:"vq_vae_dilation_cycle",val:" = None"},{name:"vq_vae_multipliers",val:" = (2, 1, 1)"},{name:"vq_vae_lmu",val:" = 0.99"},{name:"vq_vae_commit",val:" = 0.02"},{name:"vq_vae_conv_block_depth",val:" = 4"},{name:"vq_vae_conv_block_width",val:" = 64"},{name:"spectral",val:" = 0.0"},{name:"multispectral",val:" = 1.0"},{name:"vq_vae_reverse_decoder_dilation",val:" = 1"},{name:"nb_priors",val:" = 3"},{name:"spread",val:" = None"},{name:"prime_spread",val:" = None"},{name:"zero_out",val:" = False"},{name:"res_scale",val:" = False"},{name:"pos_init",val:" = False"},{name:"cond_zero_out",val:" = False"},{name:"n_ctx",val:" = (8192, 8192, 8192)"},{name:"t_bins",val:" = 128"},{name:"downs_t",val:" = (3, 2, 2)"},{name:"strides_t",val:" = (2, 2, 2)"},{name:"single_enc_dec",val:" = [True, False, False]"},{name:"labels",val:" = False"},{name:"merged_decoder",val:" = [True, False, False]"},{name:"priors_width",val:" = [4096, 2048, 1024]"},{name:"l_bins",val:" = 256"},{name:"width",val:" = [4800, 1920, 128]"},{name:"depth",val:" = [79, 72, 72]"},{name:"n_heads",val:" = [8, 1, 1]"},{name:"use_tokens",val:" = [True, False, False]"},{name:"n_tokens",val:" = [512, 0, 0]"},{name:"attn_order",val:" = [10, 2, 2]"},{name:"blocks",val:" = 16"},{name:"c_res",val:" = 1"},{name:"init_scale",val:" = [0.7, 1, 1]"},{name:"cond_depth",val:" = [3, 16, 16]"},{name:"cond_width",val:" = [128, 1024, 1024]"},{name:"cond_dilation_growth_rate",val:" = [1, 3, 3]"},{name:"cond_dilation_cycle",val:" = [None, 8, 8]"},{name:"cond_c_res",val:" = [0, 1, 1]"},{name:"cond_res_scale",val:" = False"},{name:"prime_width",val:" = [128, 128, 128]"},{name:"prime_depth",val:" = [18, 3, 3]"},{name:"prime_cond_c_res",val:" = [0, 1, 1]"},{name:"prime_heads",val:" = 4"},{name:"prime_m_attn",val:" = 0.25"},{name:"prime_m_mlp",val:" = 1.0"},{name:"prime_blocks",val:" = 32"},{name:"prime_init_scale",val:" = [0.1, 0.4, 0.4]"},{name:"prime_c_res",val:" = 1"},{name:"prime_loss_fraction",val:" = [0.4, 0.0, 0.0]"},{name:"prime_attn_order",val:" = [2, 0, 0]"},{name:"prime_attn_dropout",val:" = 0.0"},{name:"prime_resid_dropout",val:" = 0.0"},{name:"prime_emb_dropout",val:" = 0.0"},{name:"prime_zero_out",val:" = False"},{name:"prime_res_scale",val:" = False"},{name:"prime_pos_init",val:" = False"},{name:"min_duration",val:" = 1"},{name:"max_duration",val:" = 600.0"},{name:"fp16_params",val:" = True"},{name:"alignment_layer",val:" = [68, None, None]"},{name:"alignment_head",val:" = [2, None, None]"},{name:"m_attn",val:" = 0.25"},{name:"n_vocab",val:" = 80"},{name:"cond_m_conv",val:" = 1"},{name:"max_bow_genre_size",val:" = 1"},{name:"name",val:" = 'AudioSamples'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50257) &#x2014;
Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <code>JukeboxModel</code>].`,name:"vocab_size"},{anchor:"transformers.JukeboxConfig.n_positions",description:`<strong>n_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"n_positions"},{anchor:"transformers.JukeboxConfig.n_embd",description:`<strong>n_embd</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the embeddings and hidden states.`,name:"n_embd"},{anchor:"transformers.JukeboxConfig.n_layer",description:`<strong>n_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layer"},{anchor:"transformers.JukeboxConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.JukeboxConfig.n_inner",description:`<strong>n_inner</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
Dimensionality of the inner feed-forward layers. <code>None</code> will set it to 4 times n_embd`,name:"n_inner"},{anchor:"transformers.JukeboxConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
Activation function, to be selected in the list <code>[&quot;relu&quot;, &quot;silu&quot;, &quot;gelu&quot;, &quot;tanh&quot;, &quot;gelu_new&quot;]</code>.`,name:"activation_function"},{anchor:"transformers.JukeboxConfig.resid_dropout",description:`<strong>resid_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"resid_dropout"},{anchor:"transformers.JukeboxConfig.embd_pdrop",description:`<strong>embd_pdrop</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the embeddings.`,name:"embd_pdrop"},{anchor:"transformers.JukeboxConfig.attn_dropout",description:`<strong>attn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attn_dropout"},{anchor:"transformers.JukeboxConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon to use in the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.JukeboxConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.JukeboxConfig.scale_attn_weights",description:`<strong>scale_attn_weights</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Scale attention weights by dividing by sqrt(hidden_size)..`,name:"scale_attn_weights"},{anchor:"transformers.JukeboxConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.JukeboxConfig.scale_attn_by_inverse_layer_idx",description:`<strong>scale_attn_by_inverse_layer_idx</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to additionally scale attention weights by <code>1 / layer_idx + 1</code>.`,name:"scale_attn_by_inverse_layer_idx"},{anchor:"transformers.JukeboxConfig.reorder_and_upcast_attn",description:`<strong>reorder_and_upcast_attn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to scale keys (K) prior to computing attention (dot-product) and upcast attention
dot-product/softmax to float() when training with mixed precision.`,name:"reorder_and_upcast_attn"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/configuration_jukebox.py#L30"}}),S=new Qa({props:{anchor:"transformers.JukeboxConfig.example",$$slots:{default:[Ua]},$$scope:{ctx:ze}}}),te=new qe({}),ne=new qe({}),{c(){p=t("meta"),j=c(),$=t("hr"),v=c(),g=t("p"),f=s("language:"),b=c(),_=t("ul"),he=t("li"),so=s("\u201CList of ISO 639-1 code for your language\u201D"),io=c(),_e=t("li"),mo=s("en"),co=c(),V=t("li"),uo=s(`lang2
thumbnail: \u201D`),Z=t("a"),fo=s("https://cdn.openai.com/research-covers/jukebox/2x-no-mark.jpg\u201D"),po=s(`
tags:`),ho=c(),ve=t("li"),_o=s("MusicGeneration"),vo=c(),ge=t("li"),go=s("transformers"),Ne=c(),Se=t("hr"),Fe=c(),J=t("h1"),P=t("a"),be=t("span"),W(G.$$.fragment),bo=c(),ke=t("span"),ko=s("Jukebox"),Ie=c(),E=t("h2"),q=t("a"),xe=t("span"),W(B.$$.fragment),xo=c(),we=t("span"),wo=s("Overview"),Me=c(),z=t("p"),yo=s("The Jukebox model was proposed in "),K=t("a"),$o=s("Jukebox: A generative model for music"),Jo=s(`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever.`),We=c(),re=t("p"),Eo=s(`This model proposes a generative music model which can be produce minute long samples which can bne conditionned on
artist, genre and lyrics.`),Le=c(),le=t("p"),To=s("The abstract from the paper is the following:"),De=c(),se=t("p"),Co=s(`We introduce Jukebox, a model that generates
music with singing in the raw audio domain. We
tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes,
and modeling those using autoregressive Transformers. We show that the combined model at
scale can generate high-fidelity and diverse songs
with coherence up to multiple minutes. We can
condition on artist and genre to steer the musical
and vocal style, and on unaligned lyrics to make
the singing more controllable. We are releasing
thousands of non cherry-picked samples, along
with model weights and code.`),He=c(),ie=t("p"),Ao=s("Tips:"),Oe=c(),me=t("p"),jo=s("This model is very slow for now, and takes 18h to generate a minute long audio."),Re=c(),x=t("p"),Po=s("This model was contributed by "),Q=t("a"),qo=s("Arthur Zucker"),zo=s(`.
The original code can be found `),U=t("a"),No=s("here"),So=s("."),Ve=c(),T=t("h2"),N=t("a"),ye=t("span"),W(X.$$.fragment),Fo=c(),$e=t("span"),Io=s("JukeboxConfig"),Ze=c(),h=t("div"),W(Y.$$.fragment),Mo=c(),ee=t("p"),Wo=s("This is the configuration class to store the configuration of a "),Je=t("code"),Lo=s("JukeboxModel"),Do=s("."),Ho=c(),k=t("p"),Oo=s("Configuration objects inherit from "),de=t("a"),Ro=s("PretrainedConfig"),Vo=s(` and can be used to control the model outputs. Read the
documentation from `),ce=t("a"),Zo=s("PretrainedConfig"),Go=s(` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of the Speech2Text
`),oe=t("a"),Bo=s("ArthurZ/jukebox-1b-lyrics"),Ko=s(" architecture."),Qo=c(),ae=t("p"),Uo=s(`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsamoling =
(5,3), and strides = (2, 2) will downsample the audio by 2`),Ee=t("strong"),Xo=s("5 = 32 to get the first level of codes, and 2"),Yo=s(`8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),ea=c(),W(S.$$.fragment),Ge=c(),C=t("h2"),F=t("a"),Te=t("span"),W(te.$$.fragment),oa=c(),Ce=t("span"),aa=s("JukeboxTokenizer"),Be=c(),ue=t("p"),ta=s("[[autodoc]] JukeboxTokenizer - save_vocabulary"),Ke=c(),A=t("h2"),I=t("a"),Ae=t("span"),W(ne.$$.fragment),na=c(),je=t("span"),ra=s("JukeboxModel"),Qe=c(),fe=t("p"),la=s("[[autodoc]] JukeboxModel - forward"),this.h()},l(e){const l=Va('[data-svelte="svelte-1phssyn"]',document.head);p=n(l,"META",{name:!0,content:!0}),l.forEach(o),j=u(e),$=n(e,"HR",{}),v=u(e),g=n(e,"P",{});var Pe=r(g);f=i(Pe,"language:"),Pe.forEach(o),b=u(e),_=n(e,"UL",{});var w=r(_);he=n(w,"LI",{});var sa=r(he);so=i(sa,"\u201CList of ISO 639-1 code for your language\u201D"),sa.forEach(o),io=u(w),_e=n(w,"LI",{});var ia=r(_e);mo=i(ia,"en"),ia.forEach(o),co=u(w),V=n(w,"LI",{});var Xe=r(V);uo=i(Xe,`lang2
thumbnail: \u201D`),Z=n(Xe,"A",{href:!0,rel:!0});var ma=r(Z);fo=i(ma,"https://cdn.openai.com/research-covers/jukebox/2x-no-mark.jpg\u201D"),ma.forEach(o),po=i(Xe,`
tags:`),Xe.forEach(o),ho=u(w),ve=n(w,"LI",{});var da=r(ve);_o=i(da,"MusicGeneration"),da.forEach(o),vo=u(w),ge=n(w,"LI",{});var ca=r(ge);go=i(ca,"transformers"),ca.forEach(o),w.forEach(o),Ne=u(e),Se=n(e,"HR",{}),Fe=u(e),J=n(e,"H1",{class:!0});var Ye=r(J);P=n(Ye,"A",{id:!0,class:!0,href:!0});var ua=r(P);be=n(ua,"SPAN",{});var fa=r(be);L(G.$$.fragment,fa),fa.forEach(o),ua.forEach(o),bo=u(Ye),ke=n(Ye,"SPAN",{});var pa=r(ke);ko=i(pa,"Jukebox"),pa.forEach(o),Ye.forEach(o),Ie=u(e),E=n(e,"H2",{class:!0});var eo=r(E);q=n(eo,"A",{id:!0,class:!0,href:!0});var ha=r(q);xe=n(ha,"SPAN",{});var _a=r(xe);L(B.$$.fragment,_a),_a.forEach(o),ha.forEach(o),xo=u(eo),we=n(eo,"SPAN",{});var va=r(we);wo=i(va,"Overview"),va.forEach(o),eo.forEach(o),Me=u(e),z=n(e,"P",{});var oo=r(z);yo=i(oo,"The Jukebox model was proposed in "),K=n(oo,"A",{href:!0,rel:!0});var ga=r(K);$o=i(ga,"Jukebox: A generative model for music"),ga.forEach(o),Jo=i(oo,`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever.`),oo.forEach(o),We=u(e),re=n(e,"P",{});var ba=r(re);Eo=i(ba,`This model proposes a generative music model which can be produce minute long samples which can bne conditionned on
artist, genre and lyrics.`),ba.forEach(o),Le=u(e),le=n(e,"P",{});var ka=r(le);To=i(ka,"The abstract from the paper is the following:"),ka.forEach(o),De=u(e),se=n(e,"P",{});var xa=r(se);Co=i(xa,`We introduce Jukebox, a model that generates
music with singing in the raw audio domain. We
tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes,
and modeling those using autoregressive Transformers. We show that the combined model at
scale can generate high-fidelity and diverse songs
with coherence up to multiple minutes. We can
condition on artist and genre to steer the musical
and vocal style, and on unaligned lyrics to make
the singing more controllable. We are releasing
thousands of non cherry-picked samples, along
with model weights and code.`),xa.forEach(o),He=u(e),ie=n(e,"P",{});var wa=r(ie);Ao=i(wa,"Tips:"),wa.forEach(o),Oe=u(e),me=n(e,"P",{});var ya=r(me);jo=i(ya,"This model is very slow for now, and takes 18h to generate a minute long audio."),ya.forEach(o),Re=u(e),x=n(e,"P",{});var pe=r(x);Po=i(pe,"This model was contributed by "),Q=n(pe,"A",{href:!0,rel:!0});var $a=r(Q);qo=i($a,"Arthur Zucker"),$a.forEach(o),zo=i(pe,`.
The original code can be found `),U=n(pe,"A",{href:!0,rel:!0});var Ja=r(U);No=i(Ja,"here"),Ja.forEach(o),So=i(pe,"."),pe.forEach(o),Ve=u(e),T=n(e,"H2",{class:!0});var ao=r(T);N=n(ao,"A",{id:!0,class:!0,href:!0});var Ea=r(N);ye=n(Ea,"SPAN",{});var Ta=r(ye);L(X.$$.fragment,Ta),Ta.forEach(o),Ea.forEach(o),Fo=u(ao),$e=n(ao,"SPAN",{});var Ca=r($e);Io=i(Ca,"JukeboxConfig"),Ca.forEach(o),ao.forEach(o),Ze=u(e),h=n(e,"DIV",{class:!0});var y=r(h);L(Y.$$.fragment,y),Mo=u(y),ee=n(y,"P",{});var to=r(ee);Wo=i(to,"This is the configuration class to store the configuration of a "),Je=n(to,"CODE",{});var Aa=r(Je);Lo=i(Aa,"JukeboxModel"),Aa.forEach(o),Do=i(to,"."),to.forEach(o),Ho=u(y),k=n(y,"P",{});var M=r(k);Oo=i(M,"Configuration objects inherit from "),de=n(M,"A",{href:!0});var ja=r(de);Ro=i(ja,"PretrainedConfig"),ja.forEach(o),Vo=i(M,` and can be used to control the model outputs. Read the
documentation from `),ce=n(M,"A",{href:!0});var Pa=r(ce);Zo=i(Pa,"PretrainedConfig"),Pa.forEach(o),Go=i(M,` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of the Speech2Text
`),oe=n(M,"A",{href:!0,rel:!0});var qa=r(oe);Bo=i(qa,"ArthurZ/jukebox-1b-lyrics"),qa.forEach(o),Ko=i(M," architecture."),M.forEach(o),Qo=u(y),ae=n(y,"P",{});var no=r(ae);Uo=i(no,`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsamoling =
(5,3), and strides = (2, 2) will downsample the audio by 2`),Ee=n(no,"STRONG",{});var za=r(Ee);Xo=i(za,"5 = 32 to get the first level of codes, and 2"),za.forEach(o),Yo=i(no,`8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),no.forEach(o),ea=u(y),L(S.$$.fragment,y),y.forEach(o),Ge=u(e),C=n(e,"H2",{class:!0});var ro=r(C);F=n(ro,"A",{id:!0,class:!0,href:!0});var Na=r(F);Te=n(Na,"SPAN",{});var Sa=r(Te);L(te.$$.fragment,Sa),Sa.forEach(o),Na.forEach(o),oa=u(ro),Ce=n(ro,"SPAN",{});var Fa=r(Ce);aa=i(Fa,"JukeboxTokenizer"),Fa.forEach(o),ro.forEach(o),Be=u(e),ue=n(e,"P",{});var Ia=r(ue);ta=i(Ia,"[[autodoc]] JukeboxTokenizer - save_vocabulary"),Ia.forEach(o),Ke=u(e),A=n(e,"H2",{class:!0});var lo=r(A);I=n(lo,"A",{id:!0,class:!0,href:!0});var Ma=r(I);Ae=n(Ma,"SPAN",{});var Wa=r(Ae);L(ne.$$.fragment,Wa),Wa.forEach(o),Ma.forEach(o),na=u(lo),je=n(lo,"SPAN",{});var La=r(je);ra=i(La,"JukeboxModel"),La.forEach(o),lo.forEach(o),Qe=u(e),fe=n(e,"P",{});var Da=r(fe);la=i(Da,"[[autodoc]] JukeboxModel - forward"),Da.forEach(o),this.h()},h(){d(p,"name","hf:doc:metadata"),d(p,"content",JSON.stringify(Ya)),d(Z,"href","https://cdn.openai.com/research-covers/jukebox/2x-no-mark.jpg%22"),d(Z,"rel","nofollow"),d(P,"id","jukebox"),d(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P,"href","#jukebox"),d(J,"class","relative group"),d(q,"id","overview"),d(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q,"href","#overview"),d(E,"class","relative group"),d(K,"href","https://arxiv.org/pdf/2005.00341.pdf"),d(K,"rel","nofollow"),d(Q,"href","https://huggingface.co/ArthurZ"),d(Q,"rel","nofollow"),d(U,"href","https://github.com/openai/jukebox"),d(U,"rel","nofollow"),d(N,"id","transformers.JukeboxConfig"),d(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(N,"href","#transformers.JukeboxConfig"),d(T,"class","relative group"),d(de,"href","/docs/transformers/pr_17826/en/main_classes/configuration#transformers.PretrainedConfig"),d(ce,"href","/docs/transformers/pr_17826/en/main_classes/configuration#transformers.PretrainedConfig"),d(oe,"href","https://huggingface.co/ArthurZ/jukebox-1b-lyrics"),d(oe,"rel","nofollow"),d(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"id","jukeboxtokenizer"),d(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(F,"href","#jukeboxtokenizer"),d(C,"class","relative group"),d(I,"id","jukeboxmodel"),d(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I,"href","#jukeboxmodel"),d(A,"class","relative group")},m(e,l){a(document.head,p),m(e,j,l),m(e,$,l),m(e,v,l),m(e,g,l),a(g,f),m(e,b,l),m(e,_,l),a(_,he),a(he,so),a(_,io),a(_,_e),a(_e,mo),a(_,co),a(_,V),a(V,uo),a(V,Z),a(Z,fo),a(V,po),a(_,ho),a(_,ve),a(ve,_o),a(_,vo),a(_,ge),a(ge,go),m(e,Ne,l),m(e,Se,l),m(e,Fe,l),m(e,J,l),a(J,P),a(P,be),D(G,be,null),a(J,bo),a(J,ke),a(ke,ko),m(e,Ie,l),m(e,E,l),a(E,q),a(q,xe),D(B,xe,null),a(E,xo),a(E,we),a(we,wo),m(e,Me,l),m(e,z,l),a(z,yo),a(z,K),a(K,$o),a(z,Jo),m(e,We,l),m(e,re,l),a(re,Eo),m(e,Le,l),m(e,le,l),a(le,To),m(e,De,l),m(e,se,l),a(se,Co),m(e,He,l),m(e,ie,l),a(ie,Ao),m(e,Oe,l),m(e,me,l),a(me,jo),m(e,Re,l),m(e,x,l),a(x,Po),a(x,Q),a(Q,qo),a(x,zo),a(x,U),a(U,No),a(x,So),m(e,Ve,l),m(e,T,l),a(T,N),a(N,ye),D(X,ye,null),a(T,Fo),a(T,$e),a($e,Io),m(e,Ze,l),m(e,h,l),D(Y,h,null),a(h,Mo),a(h,ee),a(ee,Wo),a(ee,Je),a(Je,Lo),a(ee,Do),a(h,Ho),a(h,k),a(k,Oo),a(k,de),a(de,Ro),a(k,Vo),a(k,ce),a(ce,Zo),a(k,Go),a(k,oe),a(oe,Bo),a(k,Ko),a(h,Qo),a(h,ae),a(ae,Uo),a(ae,Ee),a(Ee,Xo),a(ae,Yo),a(h,ea),D(S,h,null),m(e,Ge,l),m(e,C,l),a(C,F),a(F,Te),D(te,Te,null),a(C,oa),a(C,Ce),a(Ce,aa),m(e,Be,l),m(e,ue,l),a(ue,ta),m(e,Ke,l),m(e,A,l),a(A,I),a(I,Ae),D(ne,Ae,null),a(A,na),a(A,je),a(je,ra),m(e,Qe,l),m(e,fe,l),a(fe,la),Ue=!0},p(e,[l]){const Pe={};l&2&&(Pe.$$scope={dirty:l,ctx:e}),S.$set(Pe)},i(e){Ue||(H(G.$$.fragment,e),H(B.$$.fragment,e),H(X.$$.fragment,e),H(Y.$$.fragment,e),H(S.$$.fragment,e),H(te.$$.fragment,e),H(ne.$$.fragment,e),Ue=!0)},o(e){O(G.$$.fragment,e),O(B.$$.fragment,e),O(X.$$.fragment,e),O(Y.$$.fragment,e),O(S.$$.fragment,e),O(te.$$.fragment,e),O(ne.$$.fragment,e),Ue=!1},d(e){o(p),e&&o(j),e&&o($),e&&o(v),e&&o(g),e&&o(b),e&&o(_),e&&o(Ne),e&&o(Se),e&&o(Fe),e&&o(J),R(G),e&&o(Ie),e&&o(E),R(B),e&&o(Me),e&&o(z),e&&o(We),e&&o(re),e&&o(Le),e&&o(le),e&&o(De),e&&o(se),e&&o(He),e&&o(ie),e&&o(Oe),e&&o(me),e&&o(Re),e&&o(x),e&&o(Ve),e&&o(T),R(X),e&&o(Ze),e&&o(h),R(Y),R(S),e&&o(Ge),e&&o(C),R(te),e&&o(Be),e&&o(ue),e&&o(Ke),e&&o(A),R(ne),e&&o(Qe),e&&o(fe)}}}const Ya={local:"jukebox",sections:[{local:"overview",title:"Overview"},{local:"transformers.JukeboxConfig",title:"JukeboxConfig"},{local:"jukeboxtokenizer",title:"JukeboxTokenizer"},{local:"jukeboxmodel",title:"JukeboxModel"}],title:"Jukebox"};function et(ze){return Za(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lt extends Ha{constructor(p){super();Oa(this,p,et,Xa,Ra,{})}}export{lt as default,Ya as metadata};
