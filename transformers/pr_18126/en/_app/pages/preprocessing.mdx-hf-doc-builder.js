import{S as fb,i as jb,s as db,e as t,k as u,w as b,t as r,M as gb,c as l,d as a,m as c,a as p,x as f,h as o,b as m,N as mb,G as e,g as h,y as j,q as d,o as g,B as _,v as _b,L as ib}from"../chunks/vendor-hf-doc-builder.js";import{T as $b}from"../chunks/Tip-hf-doc-builder.js";import{Y as vb}from"../chunks/Youtube-hf-doc-builder.js";import{I as x}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as v}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as wb}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as kb,M as bb}from"../chunks/Markdown-hf-doc-builder.js";function yb(P){let $,w,i,k,y;return{c(){$=t("p"),w=r("If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),i=t("em"),k=r("vocab"),y=r(") during pretraining.")},l(E){$=l(E,"P",{});var T=p($);w=o(T,"If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),i=l(T,"EM",{});var hs=p(i);k=o(hs,"vocab"),hs.forEach(a),y=o(T,") during pretraining."),T.forEach(a)},m(E,T){h(E,$,T),e($,w),e($,i),e(i,k),e($,y)},d(E){E&&a($)}}}function xb(P){let $,w;return $=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                      [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
                      [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),{c(){b($.$$.fragment)},l(i){f($.$$.fragment,i)},m(i,k){j($,i,k),w=!0},p:ib,i(i){w||(d($.$$.fragment,i),w=!0)},o(i){g($.$$.fragment,i),w=!1},d(i){_($,i)}}}function Eb(P){let $,w;return $=new bb({props:{$$slots:{default:[xb]},$$scope:{ctx:P}}}),{c(){b($.$$.fragment)},l(i){f($.$$.fragment,i)},m(i,k){j($,i,k),w=!0},p(i,k){const y={};k&2&&(y.$$scope={dirty:k,ctx:i}),$.$set(y)},i(i){w||(d($.$$.fragment,i),w=!0)},o(i){g($.$$.fragment,i),w=!1},d(i){_($,i)}}}function qb(P){let $,w;return $=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
       [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
      dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}`}}),{c(){b($.$$.fragment)},l(i){f($.$$.fragment,i)},m(i,k){j($,i,k),w=!0},p:ib,i(i){w||(d($.$$.fragment,i),w=!0)},o(i){g($.$$.fragment,i),w=!1},d(i){_($,i)}}}function Ab(P){let $,w;return $=new bb({props:{$$slots:{default:[qb]},$$scope:{ctx:P}}}),{c(){b($.$$.fragment)},l(i){f($.$$.fragment,i)},m(i,k){j($,i,k),w=!0},p(i,k){const y={};k&2&&(y.$$scope={dirty:k,ctx:i}),$.$set(y)},i(i){w||(d($.$$.fragment,i),w=!0)},o(i){g($.$$.fragment,i),w=!1},d(i){_($,i)}}}function Pb(P){let $,w,i,k,y,E,T,hs,$r,It,Js,Ft,oe,vr,Ht,C,mn,wr,kr,bn,yr,xr,fn,Er,Rt,V,us,jn,Ms,qr,dn,Ar,Bt,Us,Wt,D,Pr,he,zr,Tr,gn,Cr,Dr,Jt,cs,Mt,S,Sr,ue,Lr,Nr,_n,Or,Ir,Ut,G,ms,$n,Vs,Fr,vn,Hr,Vt,is,Rr,ce,Br,Wr,Gt,Gs,Yt,me,Jr,Kt,Ys,Qt,ie,Mr,Xt,L,be,fe,Ur,Vr,Gr,je,de,Yr,Kr,Qr,ge,_e,Xr,Zr,Zt,bs,so,wn,ao,eo,sl,Ks,al,N,no,kn,to,lo,yn,po,ro,el,$e,oo,nl,Qs,tl,Y,fs,xn,Xs,ho,En,uo,ll,js,co,qn,mo,io,pl,O,bo,An,fo,jo,Pn,go,_o,rl,Zs,ol,ds,$o,zn,vo,wo,hl,K,gs,Tn,sa,ko,Cn,yo,ul,ve,xo,cl,I,Eo,Dn,qo,Ao,Sn,Po,zo,ml,aa,il,Q,_s,Ln,ea,To,Nn,Co,bl,we,Do,fl,q,So,On,Lo,No,In,Oo,Io,Fn,Fo,Ho,jl,$s,dl,X,vs,Hn,na,Ro,Rn,Bo,gl,ws,Wo,ke,Jo,Mo,_l,ta,$l,F,Uo,la,Vo,Go,pa,Yo,Ko,vl,ra,wl,H,Qo,Bn,Xo,Zo,Wn,sh,ah,kl,oa,yl,ye,eh,xl,R,xe,Jn,nh,th,lh,Ee,Mn,ph,rh,oh,qe,Un,hh,uh,El,Z,ks,Vn,ha,ch,Gn,mh,ql,ys,ih,ua,bh,fh,Al,xs,jh,ca,dh,gh,Pl,ma,zl,Ae,ia,_h,ba,$h,vh,Tl,fa,Cl,ja,Yn,wh,Dl,da,Sl,Es,kh,Kn,yh,xh,Ll,ss,qs,Qn,ga,Eh,Xn,qh,Nl,A,Ah,Zn,Ph,zh,st,Th,Ch,at,Dh,Sh,Ol,As,Lh,Pe,Nh,Oh,Il,_a,Fl,B,Ih,et,Fh,Hh,nt,Rh,Bh,Hl,$a,Rl,as,Ps,tt,va,Wh,lt,Jh,Bl,ze,Mh,Wl,wa,Jl,Te,Uh,Ml,ka,Ul,Ce,Vh,Vl,ya,Gl,De,Gh,Yl,xa,Kl,Se,Yh,Ql,es,zs,pt,Ea,Kh,rt,Qh,Xl,Le,Xh,Zl,W,Zh,qa,su,au,ot,eu,nu,sp,Aa,ap,Ts,tu,Pa,ht,lu,pu,ep,za,np,Ne,Oe,qc,tp,ns,Cs,ut,Ta,ru,ct,ou,lp,Ds,hu,Ie,uu,cu,pp,Ca,rp,ts,Ss,mt,Da,mu,it,iu,op,Ls,bu,Sa,bt,fu,ju,hp,Fe,z,du,La,ft,gu,_u,Na,jt,$u,vu,Oa,dt,wu,ku,up,Ia,cp,Fa,ls,yu,He,gt,xu,Eu,_t,qu,Au,mp,Ha,ip,Ra,Ba,Pu,Wa,$t,zu,Tu,bp,Ja,fp,Ma,Ua,Cu,vt,Du,Su,jp,Va,dp,Re,Lu,gp,Ga,_p,Be,We,Ac,$p,ps,Ns,wt,Ya,Nu,kt,Ou,vp,Je,Iu,wp,Os,yt,Fu,Hu,xt,Ru,kp,Is,Bu,Ka,Wu,Ju,yp,Qa,xp,J,Mu,Et,Uu,Vu,qt,Gu,Yu,Ep,Xa,qp,M,Ku,At,Qu,Xu,Pt,Zu,sc,Ap,Za,Pp,Fs,ac,Me,ec,nc,zp,se,Tp,rs,Hs,zt,ae,tc,Tt,lc,Cp,Ue,pc,Dp,ee,Sp,Ve,os,rc,Ct,oc,hc,Dt,uc,cc,Lp,ne,Np,te,le,mc,St,ic,bc,Op,pe,Ip,U,fc,Lt,jc,dc,Nt,gc,_c,Fp,Ge,$c,Hp;return E=new x({}),Js=new wb({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"}]}}),Ms=new x({}),Us=new vb({props:{id:"Yffk5aydLzg"}}),cs=new $b({props:{$$slots:{default:[yb]},$$scope:{ctx:P}}}),Vs=new x({}),Gs=new v({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Ys=new v({props:{code:`encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2079</span>, <span class="hljs-number">2025</span>, <span class="hljs-number">19960</span>, <span class="hljs-number">10362</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">3821</span>, <span class="hljs-number">1997</span>, <span class="hljs-number">16657</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">2027</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">11259</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">4248</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">4963</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ks=new v({props:{code:'tokenizer.decode(encoded_input["input_ids"])',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#x27;</span>`}}),Qs=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Xs=new x({}),Zs=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),sa=new x({}),aa=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),ea=new x({}),$s=new kb({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ab],pytorch:[Eb]},$$scope:{ctx:P}}}),na=new x({}),ta=new v({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),ra=new v({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),oa=new v({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),ha=new x({}),ma=new v({props:{code:`dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
dataset[0]["audio"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),fa=new v({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),da=new v({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),ga=new x({}),_a=new v({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),$a=new v({props:{code:`audio_input = [dataset[0]["audio"]["array"]]
feature_extractor(audio_input, sampling_rate=16000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>audio_input = [dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor(audio_input, sampling_rate=<span class="hljs-number">16000</span>)
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">3.8106556e-04</span>,  <span class="hljs-number">2.7506407e-03</span>,  <span class="hljs-number">2.8015103e-03</span>, ...,
        <span class="hljs-number">5.6335266e-04</span>,  <span class="hljs-number">4.6588284e-06</span>, -<span class="hljs-number">1.7142107e-04</span>], dtype=float32)]}`}}),va=new x({}),wa=new v({props:{code:`dataset[0]["audio"]["array"].shape

dataset[1]["audio"]["array"].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">173398</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">106496</span>,)`}}),ka=new v({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),ya=new v({props:{code:"processed_dataset = preprocess_function(dataset[:5])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])'}}),xa=new v({props:{code:`processed_dataset["input_values"][0].shape

processed_dataset["input_values"][1].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">100000</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">100000</span>,)`}}),Ea=new x({}),Aa=new v({props:{code:`from datasets import load_dataset

dataset = load_dataset("food101", split="train[:100]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`}}),za=new v({props:{code:'dataset[0]["image"]',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]'}}),Ta=new x({}),Ca=new v({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)`}}),Da=new x({}),Ia=new v({props:{code:`from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose(
    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose(
<span class="hljs-meta">... </span>    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor(), normalize]
<span class="hljs-meta">... </span>)`}}),Ha=new v({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Ja=new v({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Va=new v({props:{code:'dataset[0]["image"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at <span class="hljs-number">0x7F1A7B0630D0</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: tensor([[[ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.1216</span>,  ..., -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>],
          [-<span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.1294</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9843</span>, -<span class="hljs-number">0.9922</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.1137</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9686</span>, -<span class="hljs-number">0.8667</span>],
          ...,
          [ <span class="hljs-number">0.0275</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.0510</span>,  ..., -<span class="hljs-number">0.1137</span>, -<span class="hljs-number">0.1216</span>, -<span class="hljs-number">0.0824</span>],
          [ <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.0667</span>,  ..., -<span class="hljs-number">0.0588</span>, -<span class="hljs-number">0.0745</span>, -<span class="hljs-number">0.0980</span>],
          [ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0431</span>,  ..., -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0588</span>]],
 
         [[ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.2863</span>,  ..., -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>],
          [ <span class="hljs-number">0.1608</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.3098</span>,  ..., -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>],
          [ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2706</span>,  <span class="hljs-number">0.3020</span>,  ..., -<span class="hljs-number">0.9608</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.8275</span>],
          ...,
          [-<span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.2392</span>, -<span class="hljs-number">0.2471</span>, -<span class="hljs-number">0.2078</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0196</span>,  ..., -<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.2000</span>, -<span class="hljs-number">0.2235</span>],
          [-<span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.1529</span>]],
 
         [[ <span class="hljs-number">0.3961</span>,  <span class="hljs-number">0.4431</span>,  <span class="hljs-number">0.4980</span>,  ..., -<span class="hljs-number">0.9216</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9216</span>],
          [ <span class="hljs-number">0.3569</span>,  <span class="hljs-number">0.4510</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9059</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9137</span>],
          [ <span class="hljs-number">0.4118</span>,  <span class="hljs-number">0.4745</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.8902</span>, -<span class="hljs-number">0.7804</span>],
          ...,
          [-<span class="hljs-number">0.2314</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.2078</span>,  ..., -<span class="hljs-number">0.4196</span>, -<span class="hljs-number">0.4275</span>, -<span class="hljs-number">0.3882</span>],
          [-<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.1686</span>, -<span class="hljs-number">0.2000</span>,  ..., -<span class="hljs-number">0.3647</span>, -<span class="hljs-number">0.3804</span>, -<span class="hljs-number">0.4039</span>],
          [-<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>,  ..., -<span class="hljs-number">0.2941</span>, -<span class="hljs-number">0.2863</span>, -<span class="hljs-number">0.3412</span>]]])}`}}),Ga=new v({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),Ya=new x({}),Qa=new v({props:{code:`from datasets import load_dataset

lj_speech = load_dataset("lj_speech", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = load_dataset(<span class="hljs-string">&quot;lj_speech&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Xa=new v({props:{code:'lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.<span class="hljs-built_in">map</span>(remove_columns=[<span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;normalized_text&quot;</span>])'}}),Za=new v({props:{code:`lj_speech[0]["audio"]

lj_speech[0]["text"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">7.3242188e-04</span>, -<span class="hljs-number">7.6293945e-04</span>, -<span class="hljs-number">6.4086914e-04</span>, ...,
         <span class="hljs-number">7.3242188e-04</span>,  <span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">22050</span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
<span class="hljs-string">&#x27;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition&#x27;</span>`}}),se=new v({props:{code:'lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),ae=new x({}),ee=new v({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),ne=new v({props:{code:`def prepare_dataset(example):
    audio = example["audio"]

    example["input_values"] = processor(audio["array"], sampling_rate=16000)

    with processor.as_target_processor():
        example["labels"] = processor(example["text"]).input_ids
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    audio = example[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    example[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        example[<span class="hljs-string">&quot;labels&quot;</span>] = processor(example[<span class="hljs-string">&quot;text&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),pe=new v({props:{code:"prepare_dataset(lj_speech[0])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>prepare_dataset(lj_speech[<span class="hljs-number">0</span>])'}}),{c(){$=t("meta"),w=u(),i=t("h1"),k=t("a"),y=t("span"),b(E.$$.fragment),T=u(),hs=t("span"),$r=r("Preprocess"),It=u(),b(Js.$$.fragment),Ft=u(),oe=t("p"),vr=r("Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:"),Ht=u(),C=t("ul"),mn=t("li"),wr=r("Preprocess textual data with a tokenizer."),kr=u(),bn=t("li"),yr=r("Preprocess image or audio data with a feature extractor."),xr=u(),fn=t("li"),Er=r("Preprocess data for a multimodal task with a processor."),Rt=u(),V=t("h2"),us=t("a"),jn=t("span"),b(Ms.$$.fragment),qr=u(),dn=t("span"),Ar=r("NLP"),Bt=u(),b(Us.$$.fragment),Wt=u(),D=t("p"),Pr=r("The main tool for processing textual data is a "),he=t("a"),zr=r("tokenizer"),Tr=r(". A tokenizer starts by splitting text into "),gn=t("em"),Cr=r("tokens"),Dr=r(" according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer."),Jt=u(),b(cs.$$.fragment),Mt=u(),S=t("p"),Sr=r("Get started quickly by loading a pretrained tokenizer with the "),ue=t("a"),Lr=r("AutoTokenizer"),Nr=r(" class. This downloads the "),_n=t("em"),Or=r("vocab"),Ir=r(" used when a model is pretrained."),Ut=u(),G=t("h3"),ms=t("a"),$n=t("span"),b(Vs.$$.fragment),Fr=u(),vn=t("span"),Hr=r("Tokenize"),Vt=u(),is=t("p"),Rr=r("Load a pretrained tokenizer with "),ce=t("a"),Br=r("AutoTokenizer.from_pretrained()"),Wr=r(":"),Gt=u(),b(Gs.$$.fragment),Yt=u(),me=t("p"),Jr=r("Then pass your sentence to the tokenizer:"),Kt=u(),b(Ys.$$.fragment),Qt=u(),ie=t("p"),Mr=r("The tokenizer returns a dictionary with three important itmes:"),Xt=u(),L=t("ul"),be=t("li"),fe=t("a"),Ur=r("input_ids"),Vr=r(" are the indices corresponding to each token in the sentence."),Gr=u(),je=t("li"),de=t("a"),Yr=r("attention_mask"),Kr=r(" indicates whether a token should be attended to or not."),Qr=u(),ge=t("li"),_e=t("a"),Xr=r("token_type_ids"),Zr=r(" identifies which sequence a token belongs to when there is more than one sequence."),Zt=u(),bs=t("p"),so=r("You can decode the "),wn=t("code"),ao=r("input_ids"),eo=r(" to return the original input:"),sl=u(),b(Ks.$$.fragment),al=u(),N=t("p"),no=r("As you can see, the tokenizer added two special tokens - "),kn=t("code"),to=r("CLS"),lo=r(" and "),yn=t("code"),po=r("SEP"),ro=r(` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.`),el=u(),$e=t("p"),oo=r("If there are several sentences you want to process, pass the sentences as a list to the tokenizer:"),nl=u(),b(Qs.$$.fragment),tl=u(),Y=t("h3"),fs=t("a"),xn=t("span"),b(Xs.$$.fragment),ho=u(),En=t("span"),uo=r("Pad"),ll=u(),js=t("p"),co=r("This brings us to an important topic. When you process a batch of sentences, they aren\u2019t always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),qn=t("em"),mo=r("padding token"),io=r(" to sentences with fewer tokens."),pl=u(),O=t("p"),bo=r("Set the "),An=t("code"),fo=r("padding"),jo=r(" parameter to "),Pn=t("code"),go=r("True"),_o=r(" to pad the shorter sequences in the batch to match the longest sequence:"),rl=u(),b(Zs.$$.fragment),ol=u(),ds=t("p"),$o=r("Notice the tokenizer padded the first and third sentences with a "),zn=t("code"),vo=r("0"),wo=r(" because they are shorter!"),hl=u(),K=t("h3"),gs=t("a"),Tn=t("span"),b(sa.$$.fragment),ko=u(),Cn=t("span"),yo=r("Truncation"),ul=u(),ve=t("p"),xo=r("On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length."),cl=u(),I=t("p"),Eo=r("Set the "),Dn=t("code"),qo=r("truncation"),Ao=r(" parameter to "),Sn=t("code"),Po=r("True"),zo=r(" to truncate a sequence to the maximum length accepted by the model:"),ml=u(),b(aa.$$.fragment),il=u(),Q=t("h3"),_s=t("a"),Ln=t("span"),b(ea.$$.fragment),To=u(),Nn=t("span"),Co=r("Build tensors"),bl=u(),we=t("p"),Do=r("Finally, you want the tokenizer to return the actual tensors that are fed to the model."),fl=u(),q=t("p"),So=r("Set the "),On=t("code"),Lo=r("return_tensors"),No=r(" parameter to either "),In=t("code"),Oo=r("pt"),Io=r(" for PyTorch, or "),Fn=t("code"),Fo=r("tf"),Ho=r(" for TensorFlow:"),jl=u(),b($s.$$.fragment),dl=u(),X=t("h2"),vs=t("a"),Hn=t("span"),b(na.$$.fragment),Ro=u(),Rn=t("span"),Bo=r("Audio"),gl=u(),ws=t("p"),Wo=r("Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A "),ke=t("a"),Jo=r("feature extractor"),Mo=r(" is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),_l=u(),b(ta.$$.fragment),$l=u(),F=t("p"),Uo=r("Load the "),la=t("a"),Vo=r("MInDS-14"),Go=r(" dataset (see the \u{1F917} "),pa=t("a"),Yo=r("Datasets tutorial"),Ko=r(" for more details on how to load a dataset):"),vl=u(),b(ra.$$.fragment),wl=u(),H=t("p"),Qo=r("Access the first element of the "),Bn=t("code"),Xo=r("audio"),Zo=r(" column to take a look at the input. Calling the "),Wn=t("code"),sh=r("audio"),ah=r(" column will automatically load and resample the audio file:"),kl=u(),b(oa.$$.fragment),yl=u(),ye=t("p"),eh=r("This returns three items:"),xl=u(),R=t("ul"),xe=t("li"),Jn=t("code"),nh=r("array"),th=r(" is the speech signal loaded - and potentially resampled - as a 1D array."),lh=u(),Ee=t("li"),Mn=t("code"),ph=r("path"),rh=r(" points to the location of the audio file."),oh=u(),qe=t("li"),Un=t("code"),hh=r("sampling_rate"),uh=r(" refers to how many data points in the speech signal are measured per second."),El=u(),Z=t("h3"),ks=t("a"),Vn=t("span"),b(ha.$$.fragment),ch=u(),Gn=t("span"),mh=r("Resample"),ql=u(),ys=t("p"),ih=r("For this tutorial, you will use the "),ua=t("a"),bh=r("Wav2Vec2"),fh=r(" model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),Al=u(),xs=t("p"),jh=r("For example, the "),ca=t("a"),dh=r("MInDS-14"),gh=r(" dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),Pl=u(),b(ma.$$.fragment),zl=u(),Ae=t("ol"),ia=t("li"),_h=r("Use \u{1F917} Datasets\u2019 "),ba=t("a"),$h=r("cast_column"),vh=r(" method to upsample the sampling rate to 16kHz:"),Tl=u(),b(fa.$$.fragment),Cl=u(),ja=t("ol"),Yn=t("li"),wh=r("Load the audio file:"),Dl=u(),b(da.$$.fragment),Sl=u(),Es=t("p"),kh=r("As you can see, the "),Kn=t("code"),yh=r("sampling_rate"),xh=r(" is now 16kHz!"),Ll=u(),ss=t("h3"),qs=t("a"),Qn=t("span"),b(ga.$$.fragment),Eh=u(),Xn=t("span"),qh=r("Feature extractor"),Nl=u(),A=t("p"),Ah=r("The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),Zn=t("code"),Ph=r("0"),zh=r(" is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),st=t("code"),Th=r("0"),Ch=r(" - interpreted as silence - to "),at=t("code"),Dh=r("array"),Sh=r("."),Ol=u(),As=t("p"),Lh=r("Load the feature extractor with "),Pe=t("a"),Nh=r("AutoFeatureExtractor.from_pretrained()"),Oh=r(":"),Il=u(),b(_a.$$.fragment),Fl=u(),B=t("p"),Ih=r("Pass the audio "),et=t("code"),Fh=r("array"),Hh=r(" to the feature extractor. We also recommend adding the "),nt=t("code"),Rh=r("sampling_rate"),Bh=r(" argument in the feature extractor in order to better debug any silent errors that may occur."),Hl=u(),b($a.$$.fragment),Rl=u(),as=t("h3"),Ps=t("a"),tt=t("span"),b(va.$$.fragment),Wh=u(),lt=t("span"),Jh=r("Pad and truncate"),Bl=u(),ze=t("p"),Mh=r("Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),Wl=u(),b(wa.$$.fragment),Jl=u(),Te=t("p"),Uh=r("As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),Ml=u(),b(ka.$$.fragment),Ul=u(),Ce=t("p"),Vh=r("Apply the function to the the first few examples in the dataset:"),Vl=u(),b(ya.$$.fragment),Gl=u(),De=t("p"),Gh=r("Now take another look at the processed sample lengths:"),Yl=u(),b(xa.$$.fragment),Kl=u(),Se=t("p"),Yh=r("The lengths of the first two samples now match the maximum length you specified."),Ql=u(),es=t("h2"),zs=t("a"),pt=t("span"),b(Ea.$$.fragment),Kh=u(),rt=t("span"),Qh=r("Vision"),Xl=u(),Le=t("p"),Xh=r("A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),Zl=u(),W=t("p"),Zh=r("Let\u2019s load the "),qa=t("a"),su=r("food101"),au=r(" dataset for this tutorial. Use \u{1F917} Datasets "),ot=t("code"),eu=r("split"),nu=r(" parameter to only load a small sample from the training split since the dataset is quite large:"),sp=u(),b(Aa.$$.fragment),ap=u(),Ts=t("p"),tu=r("Next, take a look at the image with \u{1F917} Datasets "),Pa=t("a"),ht=t("code"),lu=r("Image"),pu=r(" feature:"),ep=u(),b(za.$$.fragment),np=u(),Ne=t("p"),Oe=t("img"),tp=u(),ns=t("h3"),Cs=t("a"),ut=t("span"),b(Ta.$$.fragment),ru=u(),ct=t("span"),ou=r("Feature extractor"),lp=u(),Ds=t("p"),hu=r("Load the feature extractor with "),Ie=t("a"),uu=r("AutoFeatureExtractor.from_pretrained()"),cu=r(":"),pp=u(),b(Ca.$$.fragment),rp=u(),ts=t("h3"),Ss=t("a"),mt=t("span"),b(Da.$$.fragment),mu=u(),it=t("span"),iu=r("Data augmentation"),op=u(),Ls=t("p"),bu=r("For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),Sa=t("a"),bt=t("code"),fu=r("transforms"),ju=r(" module."),hp=u(),Fe=t("ol"),z=t("li"),du=r("Normalize the image and use "),La=t("a"),ft=t("code"),gu=r("Compose"),_u=r(" to chain some transforms - "),Na=t("a"),jt=t("code"),$u=r("RandomResizedCrop"),vu=r(" and "),Oa=t("a"),dt=t("code"),wu=r("ColorJitter"),ku=r(" - together:"),up=u(),b(Ia.$$.fragment),cp=u(),Fa=t("ol"),ls=t("li"),yu=r("The model accepts "),He=t("a"),gt=t("code"),xu=r("pixel_values"),Eu=r(" as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),_t=t("code"),qu=r("pixel_values"),Au=r(" from the transforms:"),mp=u(),b(Ha.$$.fragment),ip=u(),Ra=t("ol"),Ba=t("li"),Pu=r("Then use \u{1F917} Datasets "),Wa=t("a"),$t=t("code"),zu=r("set_transform"),Tu=r(" to apply the transforms on-the-fly:"),bp=u(),b(Ja.$$.fragment),fp=u(),Ma=t("ol"),Ua=t("li"),Cu=r("Now when you access the image, you will notice the feature extractor has added the model input "),vt=t("code"),Du=r("pixel_values"),Su=r(":"),jp=u(),b(Va.$$.fragment),dp=u(),Re=t("p"),Lu=r("Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),gp=u(),b(Ga.$$.fragment),_p=u(),Be=t("p"),We=t("img"),$p=u(),ps=t("h2"),Ns=t("a"),wt=t("span"),b(Ya.$$.fragment),Nu=u(),kt=t("span"),Ou=r("Multimodal"),vp=u(),Je=t("p"),Iu=r("For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),wp=u(),Os=t("ul"),yt=t("li"),Fu=r("Feature extractor to preprocess the audio data."),Hu=u(),xt=t("li"),Ru=r("Tokenizer to process the text."),kp=u(),Is=t("p"),Bu=r("Let\u2019s return to the "),Ka=t("a"),Wu=r("LJ Speech"),Ju=r(" dataset:"),yp=u(),b(Qa.$$.fragment),xp=u(),J=t("p"),Mu=r("Since you are mainly interested in the "),Et=t("code"),Uu=r("audio"),Vu=r(" and "),qt=t("code"),Gu=r("text"),Yu=r(" column, remove the other columns:"),Ep=u(),b(Xa.$$.fragment),qp=u(),M=t("p"),Ku=r("Now take a look at the "),At=t("code"),Qu=r("audio"),Xu=r(" and "),Pt=t("code"),Zu=r("text"),sc=r(" columns:"),Ap=u(),b(Za.$$.fragment),Pp=u(),Fs=t("p"),ac=r("Remember from the earlier section on processing audio data, you should always "),Me=t("a"),ec=r("resample"),nc=r(" your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),zp=u(),b(se.$$.fragment),Tp=u(),rs=t("h3"),Hs=t("a"),zt=t("span"),b(ae.$$.fragment),tc=u(),Tt=t("span"),lc=r("Processor"),Cp=u(),Ue=t("p"),pc=r("A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),Dp=u(),b(ee.$$.fragment),Sp=u(),Ve=t("ol"),os=t("li"),rc=r("Create a function to process the audio data to "),Ct=t("code"),oc=r("input_values"),hc=r(", and tokenizes the text to "),Dt=t("code"),uc=r("labels"),cc=r(". These are your inputs to the model:"),Lp=u(),b(ne.$$.fragment),Np=u(),te=t("ol"),le=t("li"),mc=r("Apply the "),St=t("code"),ic=r("prepare_dataset"),bc=r(" function to a sample:"),Op=u(),b(pe.$$.fragment),Ip=u(),U=t("p"),fc=r("Notice the processor has added "),Lt=t("code"),jc=r("input_values"),dc=r(" and "),Nt=t("code"),gc=r("labels"),_c=r(". The sampling rate has also been correctly downsampled to 16kHz."),Fp=u(),Ge=t("p"),$c=r("Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),this.h()},l(s){const n=gb('[data-svelte="svelte-1phssyn"]',document.head);$=l(n,"META",{name:!0,content:!0}),n.forEach(a),w=c(s),i=l(s,"H1",{class:!0});var re=p(i);k=l(re,"A",{id:!0,class:!0,href:!0});var Ot=p(k);y=l(Ot,"SPAN",{});var Pc=p(y);f(E.$$.fragment,Pc),Pc.forEach(a),Ot.forEach(a),T=c(re),hs=l(re,"SPAN",{});var zc=p(hs);$r=o(zc,"Preprocess"),zc.forEach(a),re.forEach(a),It=c(s),f(Js.$$.fragment,s),Ft=c(s),oe=l(s,"P",{});var Tc=p(oe);vr=o(Tc,"Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:"),Tc.forEach(a),Ht=c(s),C=l(s,"UL",{});var Ye=p(C);mn=l(Ye,"LI",{});var Cc=p(mn);wr=o(Cc,"Preprocess textual data with a tokenizer."),Cc.forEach(a),kr=c(Ye),bn=l(Ye,"LI",{});var Dc=p(bn);yr=o(Dc,"Preprocess image or audio data with a feature extractor."),Dc.forEach(a),xr=c(Ye),fn=l(Ye,"LI",{});var Sc=p(fn);Er=o(Sc,"Preprocess data for a multimodal task with a processor."),Sc.forEach(a),Ye.forEach(a),Rt=c(s),V=l(s,"H2",{class:!0});var Rp=p(V);us=l(Rp,"A",{id:!0,class:!0,href:!0});var Lc=p(us);jn=l(Lc,"SPAN",{});var Nc=p(jn);f(Ms.$$.fragment,Nc),Nc.forEach(a),Lc.forEach(a),qr=c(Rp),dn=l(Rp,"SPAN",{});var Oc=p(dn);Ar=o(Oc,"NLP"),Oc.forEach(a),Rp.forEach(a),Bt=c(s),f(Us.$$.fragment,s),Wt=c(s),D=l(s,"P",{});var Ke=p(D);Pr=o(Ke,"The main tool for processing textual data is a "),he=l(Ke,"A",{href:!0});var Ic=p(he);zr=o(Ic,"tokenizer"),Ic.forEach(a),Tr=o(Ke,". A tokenizer starts by splitting text into "),gn=l(Ke,"EM",{});var Fc=p(gn);Cr=o(Fc,"tokens"),Fc.forEach(a),Dr=o(Ke," according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer."),Ke.forEach(a),Jt=c(s),f(cs.$$.fragment,s),Mt=c(s),S=l(s,"P",{});var Qe=p(S);Sr=o(Qe,"Get started quickly by loading a pretrained tokenizer with the "),ue=l(Qe,"A",{href:!0});var Hc=p(ue);Lr=o(Hc,"AutoTokenizer"),Hc.forEach(a),Nr=o(Qe," class. This downloads the "),_n=l(Qe,"EM",{});var Rc=p(_n);Or=o(Rc,"vocab"),Rc.forEach(a),Ir=o(Qe," used when a model is pretrained."),Qe.forEach(a),Ut=c(s),G=l(s,"H3",{class:!0});var Bp=p(G);ms=l(Bp,"A",{id:!0,class:!0,href:!0});var Bc=p(ms);$n=l(Bc,"SPAN",{});var Wc=p($n);f(Vs.$$.fragment,Wc),Wc.forEach(a),Bc.forEach(a),Fr=c(Bp),vn=l(Bp,"SPAN",{});var Jc=p(vn);Hr=o(Jc,"Tokenize"),Jc.forEach(a),Bp.forEach(a),Vt=c(s),is=l(s,"P",{});var Wp=p(is);Rr=o(Wp,"Load a pretrained tokenizer with "),ce=l(Wp,"A",{href:!0});var Mc=p(ce);Br=o(Mc,"AutoTokenizer.from_pretrained()"),Mc.forEach(a),Wr=o(Wp,":"),Wp.forEach(a),Gt=c(s),f(Gs.$$.fragment,s),Yt=c(s),me=l(s,"P",{});var Uc=p(me);Jr=o(Uc,"Then pass your sentence to the tokenizer:"),Uc.forEach(a),Kt=c(s),f(Ys.$$.fragment,s),Qt=c(s),ie=l(s,"P",{});var Vc=p(ie);Mr=o(Vc,"The tokenizer returns a dictionary with three important itmes:"),Vc.forEach(a),Xt=c(s),L=l(s,"UL",{});var Xe=p(L);be=l(Xe,"LI",{});var vc=p(be);fe=l(vc,"A",{href:!0});var Gc=p(fe);Ur=o(Gc,"input_ids"),Gc.forEach(a),Vr=o(vc," are the indices corresponding to each token in the sentence."),vc.forEach(a),Gr=c(Xe),je=l(Xe,"LI",{});var wc=p(je);de=l(wc,"A",{href:!0});var Yc=p(de);Yr=o(Yc,"attention_mask"),Yc.forEach(a),Kr=o(wc," indicates whether a token should be attended to or not."),wc.forEach(a),Qr=c(Xe),ge=l(Xe,"LI",{});var kc=p(ge);_e=l(kc,"A",{href:!0});var Kc=p(_e);Xr=o(Kc,"token_type_ids"),Kc.forEach(a),Zr=o(kc," identifies which sequence a token belongs to when there is more than one sequence."),kc.forEach(a),Xe.forEach(a),Zt=c(s),bs=l(s,"P",{});var Jp=p(bs);so=o(Jp,"You can decode the "),wn=l(Jp,"CODE",{});var Qc=p(wn);ao=o(Qc,"input_ids"),Qc.forEach(a),eo=o(Jp," to return the original input:"),Jp.forEach(a),sl=c(s),f(Ks.$$.fragment,s),al=c(s),N=l(s,"P",{});var Ze=p(N);no=o(Ze,"As you can see, the tokenizer added two special tokens - "),kn=l(Ze,"CODE",{});var Xc=p(kn);to=o(Xc,"CLS"),Xc.forEach(a),lo=o(Ze," and "),yn=l(Ze,"CODE",{});var Zc=p(yn);po=o(Zc,"SEP"),Zc.forEach(a),ro=o(Ze,` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.`),Ze.forEach(a),el=c(s),$e=l(s,"P",{});var sm=p($e);oo=o(sm,"If there are several sentences you want to process, pass the sentences as a list to the tokenizer:"),sm.forEach(a),nl=c(s),f(Qs.$$.fragment,s),tl=c(s),Y=l(s,"H3",{class:!0});var Mp=p(Y);fs=l(Mp,"A",{id:!0,class:!0,href:!0});var am=p(fs);xn=l(am,"SPAN",{});var em=p(xn);f(Xs.$$.fragment,em),em.forEach(a),am.forEach(a),ho=c(Mp),En=l(Mp,"SPAN",{});var nm=p(En);uo=o(nm,"Pad"),nm.forEach(a),Mp.forEach(a),ll=c(s),js=l(s,"P",{});var Up=p(js);co=o(Up,"This brings us to an important topic. When you process a batch of sentences, they aren\u2019t always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),qn=l(Up,"EM",{});var tm=p(qn);mo=o(tm,"padding token"),tm.forEach(a),io=o(Up," to sentences with fewer tokens."),Up.forEach(a),pl=c(s),O=l(s,"P",{});var sn=p(O);bo=o(sn,"Set the "),An=l(sn,"CODE",{});var lm=p(An);fo=o(lm,"padding"),lm.forEach(a),jo=o(sn," parameter to "),Pn=l(sn,"CODE",{});var pm=p(Pn);go=o(pm,"True"),pm.forEach(a),_o=o(sn," to pad the shorter sequences in the batch to match the longest sequence:"),sn.forEach(a),rl=c(s),f(Zs.$$.fragment,s),ol=c(s),ds=l(s,"P",{});var Vp=p(ds);$o=o(Vp,"Notice the tokenizer padded the first and third sentences with a "),zn=l(Vp,"CODE",{});var rm=p(zn);vo=o(rm,"0"),rm.forEach(a),wo=o(Vp," because they are shorter!"),Vp.forEach(a),hl=c(s),K=l(s,"H3",{class:!0});var Gp=p(K);gs=l(Gp,"A",{id:!0,class:!0,href:!0});var om=p(gs);Tn=l(om,"SPAN",{});var hm=p(Tn);f(sa.$$.fragment,hm),hm.forEach(a),om.forEach(a),ko=c(Gp),Cn=l(Gp,"SPAN",{});var um=p(Cn);yo=o(um,"Truncation"),um.forEach(a),Gp.forEach(a),ul=c(s),ve=l(s,"P",{});var cm=p(ve);xo=o(cm,"On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length."),cm.forEach(a),cl=c(s),I=l(s,"P",{});var an=p(I);Eo=o(an,"Set the "),Dn=l(an,"CODE",{});var mm=p(Dn);qo=o(mm,"truncation"),mm.forEach(a),Ao=o(an," parameter to "),Sn=l(an,"CODE",{});var im=p(Sn);Po=o(im,"True"),im.forEach(a),zo=o(an," to truncate a sequence to the maximum length accepted by the model:"),an.forEach(a),ml=c(s),f(aa.$$.fragment,s),il=c(s),Q=l(s,"H3",{class:!0});var Yp=p(Q);_s=l(Yp,"A",{id:!0,class:!0,href:!0});var bm=p(_s);Ln=l(bm,"SPAN",{});var fm=p(Ln);f(ea.$$.fragment,fm),fm.forEach(a),bm.forEach(a),To=c(Yp),Nn=l(Yp,"SPAN",{});var jm=p(Nn);Co=o(jm,"Build tensors"),jm.forEach(a),Yp.forEach(a),bl=c(s),we=l(s,"P",{});var dm=p(we);Do=o(dm,"Finally, you want the tokenizer to return the actual tensors that are fed to the model."),dm.forEach(a),fl=c(s),q=l(s,"P",{});var Rs=p(q);So=o(Rs,"Set the "),On=l(Rs,"CODE",{});var gm=p(On);Lo=o(gm,"return_tensors"),gm.forEach(a),No=o(Rs," parameter to either "),In=l(Rs,"CODE",{});var _m=p(In);Oo=o(_m,"pt"),_m.forEach(a),Io=o(Rs," for PyTorch, or "),Fn=l(Rs,"CODE",{});var $m=p(Fn);Fo=o($m,"tf"),$m.forEach(a),Ho=o(Rs," for TensorFlow:"),Rs.forEach(a),jl=c(s),f($s.$$.fragment,s),dl=c(s),X=l(s,"H2",{class:!0});var Kp=p(X);vs=l(Kp,"A",{id:!0,class:!0,href:!0});var vm=p(vs);Hn=l(vm,"SPAN",{});var wm=p(Hn);f(na.$$.fragment,wm),wm.forEach(a),vm.forEach(a),Ro=c(Kp),Rn=l(Kp,"SPAN",{});var km=p(Rn);Bo=o(km,"Audio"),km.forEach(a),Kp.forEach(a),gl=c(s),ws=l(s,"P",{});var Qp=p(ws);Wo=o(Qp,"Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A "),ke=l(Qp,"A",{href:!0});var ym=p(ke);Jo=o(ym,"feature extractor"),ym.forEach(a),Mo=o(Qp," is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),Qp.forEach(a),_l=c(s),f(ta.$$.fragment,s),$l=c(s),F=l(s,"P",{});var en=p(F);Uo=o(en,"Load the "),la=l(en,"A",{href:!0,rel:!0});var xm=p(la);Vo=o(xm,"MInDS-14"),xm.forEach(a),Go=o(en," dataset (see the \u{1F917} "),pa=l(en,"A",{href:!0,rel:!0});var Em=p(pa);Yo=o(Em,"Datasets tutorial"),Em.forEach(a),Ko=o(en," for more details on how to load a dataset):"),en.forEach(a),vl=c(s),f(ra.$$.fragment,s),wl=c(s),H=l(s,"P",{});var nn=p(H);Qo=o(nn,"Access the first element of the "),Bn=l(nn,"CODE",{});var qm=p(Bn);Xo=o(qm,"audio"),qm.forEach(a),Zo=o(nn," column to take a look at the input. Calling the "),Wn=l(nn,"CODE",{});var Am=p(Wn);sh=o(Am,"audio"),Am.forEach(a),ah=o(nn," column will automatically load and resample the audio file:"),nn.forEach(a),kl=c(s),f(oa.$$.fragment,s),yl=c(s),ye=l(s,"P",{});var Pm=p(ye);eh=o(Pm,"This returns three items:"),Pm.forEach(a),xl=c(s),R=l(s,"UL",{});var tn=p(R);xe=l(tn,"LI",{});var yc=p(xe);Jn=l(yc,"CODE",{});var zm=p(Jn);nh=o(zm,"array"),zm.forEach(a),th=o(yc," is the speech signal loaded - and potentially resampled - as a 1D array."),yc.forEach(a),lh=c(tn),Ee=l(tn,"LI",{});var xc=p(Ee);Mn=l(xc,"CODE",{});var Tm=p(Mn);ph=o(Tm,"path"),Tm.forEach(a),rh=o(xc," points to the location of the audio file."),xc.forEach(a),oh=c(tn),qe=l(tn,"LI",{});var Ec=p(qe);Un=l(Ec,"CODE",{});var Cm=p(Un);hh=o(Cm,"sampling_rate"),Cm.forEach(a),uh=o(Ec," refers to how many data points in the speech signal are measured per second."),Ec.forEach(a),tn.forEach(a),El=c(s),Z=l(s,"H3",{class:!0});var Xp=p(Z);ks=l(Xp,"A",{id:!0,class:!0,href:!0});var Dm=p(ks);Vn=l(Dm,"SPAN",{});var Sm=p(Vn);f(ha.$$.fragment,Sm),Sm.forEach(a),Dm.forEach(a),ch=c(Xp),Gn=l(Xp,"SPAN",{});var Lm=p(Gn);mh=o(Lm,"Resample"),Lm.forEach(a),Xp.forEach(a),ql=c(s),ys=l(s,"P",{});var Zp=p(ys);ih=o(Zp,"For this tutorial, you will use the "),ua=l(Zp,"A",{href:!0,rel:!0});var Nm=p(ua);bh=o(Nm,"Wav2Vec2"),Nm.forEach(a),fh=o(Zp," model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),Zp.forEach(a),Al=c(s),xs=l(s,"P",{});var sr=p(xs);jh=o(sr,"For example, the "),ca=l(sr,"A",{href:!0,rel:!0});var Om=p(ca);dh=o(Om,"MInDS-14"),Om.forEach(a),gh=o(sr," dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),sr.forEach(a),Pl=c(s),f(ma.$$.fragment,s),zl=c(s),Ae=l(s,"OL",{});var Im=p(Ae);ia=l(Im,"LI",{});var ar=p(ia);_h=o(ar,"Use \u{1F917} Datasets\u2019 "),ba=l(ar,"A",{href:!0,rel:!0});var Fm=p(ba);$h=o(Fm,"cast_column"),Fm.forEach(a),vh=o(ar," method to upsample the sampling rate to 16kHz:"),ar.forEach(a),Im.forEach(a),Tl=c(s),f(fa.$$.fragment,s),Cl=c(s),ja=l(s,"OL",{start:!0});var Hm=p(ja);Yn=l(Hm,"LI",{});var Rm=p(Yn);wh=o(Rm,"Load the audio file:"),Rm.forEach(a),Hm.forEach(a),Dl=c(s),f(da.$$.fragment,s),Sl=c(s),Es=l(s,"P",{});var er=p(Es);kh=o(er,"As you can see, the "),Kn=l(er,"CODE",{});var Bm=p(Kn);yh=o(Bm,"sampling_rate"),Bm.forEach(a),xh=o(er," is now 16kHz!"),er.forEach(a),Ll=c(s),ss=l(s,"H3",{class:!0});var nr=p(ss);qs=l(nr,"A",{id:!0,class:!0,href:!0});var Wm=p(qs);Qn=l(Wm,"SPAN",{});var Jm=p(Qn);f(ga.$$.fragment,Jm),Jm.forEach(a),Wm.forEach(a),Eh=c(nr),Xn=l(nr,"SPAN",{});var Mm=p(Xn);qh=o(Mm,"Feature extractor"),Mm.forEach(a),nr.forEach(a),Nl=c(s),A=l(s,"P",{});var Bs=p(A);Ah=o(Bs,"The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),Zn=l(Bs,"CODE",{});var Um=p(Zn);Ph=o(Um,"0"),Um.forEach(a),zh=o(Bs," is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),st=l(Bs,"CODE",{});var Vm=p(st);Th=o(Vm,"0"),Vm.forEach(a),Ch=o(Bs," - interpreted as silence - to "),at=l(Bs,"CODE",{});var Gm=p(at);Dh=o(Gm,"array"),Gm.forEach(a),Sh=o(Bs,"."),Bs.forEach(a),Ol=c(s),As=l(s,"P",{});var tr=p(As);Lh=o(tr,"Load the feature extractor with "),Pe=l(tr,"A",{href:!0});var Ym=p(Pe);Nh=o(Ym,"AutoFeatureExtractor.from_pretrained()"),Ym.forEach(a),Oh=o(tr,":"),tr.forEach(a),Il=c(s),f(_a.$$.fragment,s),Fl=c(s),B=l(s,"P",{});var ln=p(B);Ih=o(ln,"Pass the audio "),et=l(ln,"CODE",{});var Km=p(et);Fh=o(Km,"array"),Km.forEach(a),Hh=o(ln," to the feature extractor. We also recommend adding the "),nt=l(ln,"CODE",{});var Qm=p(nt);Rh=o(Qm,"sampling_rate"),Qm.forEach(a),Bh=o(ln," argument in the feature extractor in order to better debug any silent errors that may occur."),ln.forEach(a),Hl=c(s),f($a.$$.fragment,s),Rl=c(s),as=l(s,"H3",{class:!0});var lr=p(as);Ps=l(lr,"A",{id:!0,class:!0,href:!0});var Xm=p(Ps);tt=l(Xm,"SPAN",{});var Zm=p(tt);f(va.$$.fragment,Zm),Zm.forEach(a),Xm.forEach(a),Wh=c(lr),lt=l(lr,"SPAN",{});var si=p(lt);Jh=o(si,"Pad and truncate"),si.forEach(a),lr.forEach(a),Bl=c(s),ze=l(s,"P",{});var ai=p(ze);Mh=o(ai,"Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),ai.forEach(a),Wl=c(s),f(wa.$$.fragment,s),Jl=c(s),Te=l(s,"P",{});var ei=p(Te);Uh=o(ei,"As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),ei.forEach(a),Ml=c(s),f(ka.$$.fragment,s),Ul=c(s),Ce=l(s,"P",{});var ni=p(Ce);Vh=o(ni,"Apply the function to the the first few examples in the dataset:"),ni.forEach(a),Vl=c(s),f(ya.$$.fragment,s),Gl=c(s),De=l(s,"P",{});var ti=p(De);Gh=o(ti,"Now take another look at the processed sample lengths:"),ti.forEach(a),Yl=c(s),f(xa.$$.fragment,s),Kl=c(s),Se=l(s,"P",{});var li=p(Se);Yh=o(li,"The lengths of the first two samples now match the maximum length you specified."),li.forEach(a),Ql=c(s),es=l(s,"H2",{class:!0});var pr=p(es);zs=l(pr,"A",{id:!0,class:!0,href:!0});var pi=p(zs);pt=l(pi,"SPAN",{});var ri=p(pt);f(Ea.$$.fragment,ri),ri.forEach(a),pi.forEach(a),Kh=c(pr),rt=l(pr,"SPAN",{});var oi=p(rt);Qh=o(oi,"Vision"),oi.forEach(a),pr.forEach(a),Xl=c(s),Le=l(s,"P",{});var hi=p(Le);Xh=o(hi,"A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),hi.forEach(a),Zl=c(s),W=l(s,"P",{});var pn=p(W);Zh=o(pn,"Let\u2019s load the "),qa=l(pn,"A",{href:!0,rel:!0});var ui=p(qa);su=o(ui,"food101"),ui.forEach(a),au=o(pn," dataset for this tutorial. Use \u{1F917} Datasets "),ot=l(pn,"CODE",{});var ci=p(ot);eu=o(ci,"split"),ci.forEach(a),nu=o(pn," parameter to only load a small sample from the training split since the dataset is quite large:"),pn.forEach(a),sp=c(s),f(Aa.$$.fragment,s),ap=c(s),Ts=l(s,"P",{});var rr=p(Ts);tu=o(rr,"Next, take a look at the image with \u{1F917} Datasets "),Pa=l(rr,"A",{href:!0,rel:!0});var mi=p(Pa);ht=l(mi,"CODE",{});var ii=p(ht);lu=o(ii,"Image"),ii.forEach(a),mi.forEach(a),pu=o(rr," feature:"),rr.forEach(a),ep=c(s),f(za.$$.fragment,s),np=c(s),Ne=l(s,"P",{});var bi=p(Ne);Oe=l(bi,"IMG",{src:!0,alt:!0}),bi.forEach(a),tp=c(s),ns=l(s,"H3",{class:!0});var or=p(ns);Cs=l(or,"A",{id:!0,class:!0,href:!0});var fi=p(Cs);ut=l(fi,"SPAN",{});var ji=p(ut);f(Ta.$$.fragment,ji),ji.forEach(a),fi.forEach(a),ru=c(or),ct=l(or,"SPAN",{});var di=p(ct);ou=o(di,"Feature extractor"),di.forEach(a),or.forEach(a),lp=c(s),Ds=l(s,"P",{});var hr=p(Ds);hu=o(hr,"Load the feature extractor with "),Ie=l(hr,"A",{href:!0});var gi=p(Ie);uu=o(gi,"AutoFeatureExtractor.from_pretrained()"),gi.forEach(a),cu=o(hr,":"),hr.forEach(a),pp=c(s),f(Ca.$$.fragment,s),rp=c(s),ts=l(s,"H3",{class:!0});var ur=p(ts);Ss=l(ur,"A",{id:!0,class:!0,href:!0});var _i=p(Ss);mt=l(_i,"SPAN",{});var $i=p(mt);f(Da.$$.fragment,$i),$i.forEach(a),_i.forEach(a),mu=c(ur),it=l(ur,"SPAN",{});var vi=p(it);iu=o(vi,"Data augmentation"),vi.forEach(a),ur.forEach(a),op=c(s),Ls=l(s,"P",{});var cr=p(Ls);bu=o(cr,"For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),Sa=l(cr,"A",{href:!0,rel:!0});var wi=p(Sa);bt=l(wi,"CODE",{});var ki=p(bt);fu=o(ki,"transforms"),ki.forEach(a),wi.forEach(a),ju=o(cr," module."),cr.forEach(a),hp=c(s),Fe=l(s,"OL",{});var yi=p(Fe);z=l(yi,"LI",{});var Ws=p(z);du=o(Ws,"Normalize the image and use "),La=l(Ws,"A",{href:!0,rel:!0});var xi=p(La);ft=l(xi,"CODE",{});var Ei=p(ft);gu=o(Ei,"Compose"),Ei.forEach(a),xi.forEach(a),_u=o(Ws," to chain some transforms - "),Na=l(Ws,"A",{href:!0,rel:!0});var qi=p(Na);jt=l(qi,"CODE",{});var Ai=p(jt);$u=o(Ai,"RandomResizedCrop"),Ai.forEach(a),qi.forEach(a),vu=o(Ws," and "),Oa=l(Ws,"A",{href:!0,rel:!0});var Pi=p(Oa);dt=l(Pi,"CODE",{});var zi=p(dt);wu=o(zi,"ColorJitter"),zi.forEach(a),Pi.forEach(a),ku=o(Ws," - together:"),Ws.forEach(a),yi.forEach(a),up=c(s),f(Ia.$$.fragment,s),cp=c(s),Fa=l(s,"OL",{start:!0});var Ti=p(Fa);ls=l(Ti,"LI",{});var rn=p(ls);yu=o(rn,"The model accepts "),He=l(rn,"A",{href:!0});var Ci=p(He);gt=l(Ci,"CODE",{});var Di=p(gt);xu=o(Di,"pixel_values"),Di.forEach(a),Ci.forEach(a),Eu=o(rn," as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),_t=l(rn,"CODE",{});var Si=p(_t);qu=o(Si,"pixel_values"),Si.forEach(a),Au=o(rn," from the transforms:"),rn.forEach(a),Ti.forEach(a),mp=c(s),f(Ha.$$.fragment,s),ip=c(s),Ra=l(s,"OL",{start:!0});var Li=p(Ra);Ba=l(Li,"LI",{});var mr=p(Ba);Pu=o(mr,"Then use \u{1F917} Datasets "),Wa=l(mr,"A",{href:!0,rel:!0});var Ni=p(Wa);$t=l(Ni,"CODE",{});var Oi=p($t);zu=o(Oi,"set_transform"),Oi.forEach(a),Ni.forEach(a),Tu=o(mr," to apply the transforms on-the-fly:"),mr.forEach(a),Li.forEach(a),bp=c(s),f(Ja.$$.fragment,s),fp=c(s),Ma=l(s,"OL",{start:!0});var Ii=p(Ma);Ua=l(Ii,"LI",{});var ir=p(Ua);Cu=o(ir,"Now when you access the image, you will notice the feature extractor has added the model input "),vt=l(ir,"CODE",{});var Fi=p(vt);Du=o(Fi,"pixel_values"),Fi.forEach(a),Su=o(ir,":"),ir.forEach(a),Ii.forEach(a),jp=c(s),f(Va.$$.fragment,s),dp=c(s),Re=l(s,"P",{});var Hi=p(Re);Lu=o(Hi,"Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),Hi.forEach(a),gp=c(s),f(Ga.$$.fragment,s),_p=c(s),Be=l(s,"P",{});var Ri=p(Be);We=l(Ri,"IMG",{src:!0,alt:!0}),Ri.forEach(a),$p=c(s),ps=l(s,"H2",{class:!0});var br=p(ps);Ns=l(br,"A",{id:!0,class:!0,href:!0});var Bi=p(Ns);wt=l(Bi,"SPAN",{});var Wi=p(wt);f(Ya.$$.fragment,Wi),Wi.forEach(a),Bi.forEach(a),Nu=c(br),kt=l(br,"SPAN",{});var Ji=p(kt);Ou=o(Ji,"Multimodal"),Ji.forEach(a),br.forEach(a),vp=c(s),Je=l(s,"P",{});var Mi=p(Je);Iu=o(Mi,"For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),Mi.forEach(a),wp=c(s),Os=l(s,"UL",{});var fr=p(Os);yt=l(fr,"LI",{});var Ui=p(yt);Fu=o(Ui,"Feature extractor to preprocess the audio data."),Ui.forEach(a),Hu=c(fr),xt=l(fr,"LI",{});var Vi=p(xt);Ru=o(Vi,"Tokenizer to process the text."),Vi.forEach(a),fr.forEach(a),kp=c(s),Is=l(s,"P",{});var jr=p(Is);Bu=o(jr,"Let\u2019s return to the "),Ka=l(jr,"A",{href:!0,rel:!0});var Gi=p(Ka);Wu=o(Gi,"LJ Speech"),Gi.forEach(a),Ju=o(jr," dataset:"),jr.forEach(a),yp=c(s),f(Qa.$$.fragment,s),xp=c(s),J=l(s,"P",{});var on=p(J);Mu=o(on,"Since you are mainly interested in the "),Et=l(on,"CODE",{});var Yi=p(Et);Uu=o(Yi,"audio"),Yi.forEach(a),Vu=o(on," and "),qt=l(on,"CODE",{});var Ki=p(qt);Gu=o(Ki,"text"),Ki.forEach(a),Yu=o(on," column, remove the other columns:"),on.forEach(a),Ep=c(s),f(Xa.$$.fragment,s),qp=c(s),M=l(s,"P",{});var hn=p(M);Ku=o(hn,"Now take a look at the "),At=l(hn,"CODE",{});var Qi=p(At);Qu=o(Qi,"audio"),Qi.forEach(a),Xu=o(hn," and "),Pt=l(hn,"CODE",{});var Xi=p(Pt);Zu=o(Xi,"text"),Xi.forEach(a),sc=o(hn," columns:"),hn.forEach(a),Ap=c(s),f(Za.$$.fragment,s),Pp=c(s),Fs=l(s,"P",{});var dr=p(Fs);ac=o(dr,"Remember from the earlier section on processing audio data, you should always "),Me=l(dr,"A",{href:!0});var Zi=p(Me);ec=o(Zi,"resample"),Zi.forEach(a),nc=o(dr," your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),dr.forEach(a),zp=c(s),f(se.$$.fragment,s),Tp=c(s),rs=l(s,"H3",{class:!0});var gr=p(rs);Hs=l(gr,"A",{id:!0,class:!0,href:!0});var sb=p(Hs);zt=l(sb,"SPAN",{});var ab=p(zt);f(ae.$$.fragment,ab),ab.forEach(a),sb.forEach(a),tc=c(gr),Tt=l(gr,"SPAN",{});var eb=p(Tt);lc=o(eb,"Processor"),eb.forEach(a),gr.forEach(a),Cp=c(s),Ue=l(s,"P",{});var nb=p(Ue);pc=o(nb,"A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),nb.forEach(a),Dp=c(s),f(ee.$$.fragment,s),Sp=c(s),Ve=l(s,"OL",{});var tb=p(Ve);os=l(tb,"LI",{});var un=p(os);rc=o(un,"Create a function to process the audio data to "),Ct=l(un,"CODE",{});var lb=p(Ct);oc=o(lb,"input_values"),lb.forEach(a),hc=o(un,", and tokenizes the text to "),Dt=l(un,"CODE",{});var pb=p(Dt);uc=o(pb,"labels"),pb.forEach(a),cc=o(un,". These are your inputs to the model:"),un.forEach(a),tb.forEach(a),Lp=c(s),f(ne.$$.fragment,s),Np=c(s),te=l(s,"OL",{start:!0});var rb=p(te);le=l(rb,"LI",{});var _r=p(le);mc=o(_r,"Apply the "),St=l(_r,"CODE",{});var ob=p(St);ic=o(ob,"prepare_dataset"),ob.forEach(a),bc=o(_r," function to a sample:"),_r.forEach(a),rb.forEach(a),Op=c(s),f(pe.$$.fragment,s),Ip=c(s),U=l(s,"P",{});var cn=p(U);fc=o(cn,"Notice the processor has added "),Lt=l(cn,"CODE",{});var hb=p(Lt);jc=o(hb,"input_values"),hb.forEach(a),dc=o(cn," and "),Nt=l(cn,"CODE",{});var ub=p(Nt);gc=o(ub,"labels"),ub.forEach(a),_c=o(cn,". The sampling rate has also been correctly downsampled to 16kHz."),cn.forEach(a),Fp=c(s),Ge=l(s,"P",{});var cb=p(Ge);$c=o(cb,"Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),cb.forEach(a),this.h()},h(){m($,"name","hf:doc:metadata"),m($,"content",JSON.stringify(zb)),m(k,"id","preprocess"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#preprocess"),m(i,"class","relative group"),m(us,"id","nlp"),m(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(us,"href","#nlp"),m(V,"class","relative group"),m(he,"href","main_classes/tokenizer"),m(ue,"href","/docs/transformers/pr_18126/en/model_doc/auto#transformers.AutoTokenizer"),m(ms,"id","tokenize"),m(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ms,"href","#tokenize"),m(G,"class","relative group"),m(ce,"href","/docs/transformers/pr_18126/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),m(fe,"href","glossary#input-ids"),m(de,"href","glossary#attention-mask"),m(_e,"href","glossary#token-type-ids"),m(fs,"id","pad"),m(fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fs,"href","#pad"),m(Y,"class","relative group"),m(gs,"id","truncation"),m(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(gs,"href","#truncation"),m(K,"class","relative group"),m(_s,"id","build-tensors"),m(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_s,"href","#build-tensors"),m(Q,"class","relative group"),m(vs,"id","audio"),m(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(vs,"href","#audio"),m(X,"class","relative group"),m(ke,"href","main_classes/feature_extractor"),m(la,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(la,"rel","nofollow"),m(pa,"href","https://huggingface.co/docs/datasets/load_hub.html"),m(pa,"rel","nofollow"),m(ks,"id","resample"),m(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ks,"href","#resample"),m(Z,"class","relative group"),m(ua,"href","https://huggingface.co/facebook/wav2vec2-base"),m(ua,"rel","nofollow"),m(ca,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(ca,"rel","nofollow"),m(ba,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.cast_column"),m(ba,"rel","nofollow"),m(ja,"start","2"),m(qs,"id","feature-extractor"),m(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qs,"href","#feature-extractor"),m(ss,"class","relative group"),m(Pe,"href","/docs/transformers/pr_18126/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),m(Ps,"id","pad-and-truncate"),m(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ps,"href","#pad-and-truncate"),m(as,"class","relative group"),m(zs,"id","vision"),m(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(zs,"href","#vision"),m(es,"class","relative group"),m(qa,"href","https://huggingface.co/datasets/food101"),m(qa,"rel","nofollow"),m(Pa,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image"),m(Pa,"rel","nofollow"),mb(Oe.src,qc="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png")||m(Oe,"src",qc),m(Oe,"alt","vision-preprocess-tutorial.png"),m(Cs,"id","feature-extractor"),m(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Cs,"href","#feature-extractor"),m(ns,"class","relative group"),m(Ie,"href","/docs/transformers/pr_18126/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),m(Ss,"id","data-augmentation"),m(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ss,"href","#data-augmentation"),m(ts,"class","relative group"),m(Sa,"href","https://pytorch.org/vision/stable/transforms.html"),m(Sa,"rel","nofollow"),m(La,"href","https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html"),m(La,"rel","nofollow"),m(Na,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html"),m(Na,"rel","nofollow"),m(Oa,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html"),m(Oa,"rel","nofollow"),m(He,"href","model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values"),m(Fa,"start","2"),m(Wa,"href","https://huggingface.co/docs/datasets/process.html#format-transform"),m(Wa,"rel","nofollow"),m(Ra,"start","3"),m(Ma,"start","4"),mb(We.src,Ac="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png")||m(We,"src",Ac),m(We,"alt","preprocessed_image"),m(Ns,"id","multimodal"),m(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ns,"href","#multimodal"),m(ps,"class","relative group"),m(Ka,"href","https://huggingface.co/datasets/lj_speech"),m(Ka,"rel","nofollow"),m(Me,"href","preprocessing#audio"),m(Hs,"id","processor"),m(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Hs,"href","#processor"),m(rs,"class","relative group"),m(te,"start","2")},m(s,n){e(document.head,$),h(s,w,n),h(s,i,n),e(i,k),e(k,y),j(E,y,null),e(i,T),e(i,hs),e(hs,$r),h(s,It,n),j(Js,s,n),h(s,Ft,n),h(s,oe,n),e(oe,vr),h(s,Ht,n),h(s,C,n),e(C,mn),e(mn,wr),e(C,kr),e(C,bn),e(bn,yr),e(C,xr),e(C,fn),e(fn,Er),h(s,Rt,n),h(s,V,n),e(V,us),e(us,jn),j(Ms,jn,null),e(V,qr),e(V,dn),e(dn,Ar),h(s,Bt,n),j(Us,s,n),h(s,Wt,n),h(s,D,n),e(D,Pr),e(D,he),e(he,zr),e(D,Tr),e(D,gn),e(gn,Cr),e(D,Dr),h(s,Jt,n),j(cs,s,n),h(s,Mt,n),h(s,S,n),e(S,Sr),e(S,ue),e(ue,Lr),e(S,Nr),e(S,_n),e(_n,Or),e(S,Ir),h(s,Ut,n),h(s,G,n),e(G,ms),e(ms,$n),j(Vs,$n,null),e(G,Fr),e(G,vn),e(vn,Hr),h(s,Vt,n),h(s,is,n),e(is,Rr),e(is,ce),e(ce,Br),e(is,Wr),h(s,Gt,n),j(Gs,s,n),h(s,Yt,n),h(s,me,n),e(me,Jr),h(s,Kt,n),j(Ys,s,n),h(s,Qt,n),h(s,ie,n),e(ie,Mr),h(s,Xt,n),h(s,L,n),e(L,be),e(be,fe),e(fe,Ur),e(be,Vr),e(L,Gr),e(L,je),e(je,de),e(de,Yr),e(je,Kr),e(L,Qr),e(L,ge),e(ge,_e),e(_e,Xr),e(ge,Zr),h(s,Zt,n),h(s,bs,n),e(bs,so),e(bs,wn),e(wn,ao),e(bs,eo),h(s,sl,n),j(Ks,s,n),h(s,al,n),h(s,N,n),e(N,no),e(N,kn),e(kn,to),e(N,lo),e(N,yn),e(yn,po),e(N,ro),h(s,el,n),h(s,$e,n),e($e,oo),h(s,nl,n),j(Qs,s,n),h(s,tl,n),h(s,Y,n),e(Y,fs),e(fs,xn),j(Xs,xn,null),e(Y,ho),e(Y,En),e(En,uo),h(s,ll,n),h(s,js,n),e(js,co),e(js,qn),e(qn,mo),e(js,io),h(s,pl,n),h(s,O,n),e(O,bo),e(O,An),e(An,fo),e(O,jo),e(O,Pn),e(Pn,go),e(O,_o),h(s,rl,n),j(Zs,s,n),h(s,ol,n),h(s,ds,n),e(ds,$o),e(ds,zn),e(zn,vo),e(ds,wo),h(s,hl,n),h(s,K,n),e(K,gs),e(gs,Tn),j(sa,Tn,null),e(K,ko),e(K,Cn),e(Cn,yo),h(s,ul,n),h(s,ve,n),e(ve,xo),h(s,cl,n),h(s,I,n),e(I,Eo),e(I,Dn),e(Dn,qo),e(I,Ao),e(I,Sn),e(Sn,Po),e(I,zo),h(s,ml,n),j(aa,s,n),h(s,il,n),h(s,Q,n),e(Q,_s),e(_s,Ln),j(ea,Ln,null),e(Q,To),e(Q,Nn),e(Nn,Co),h(s,bl,n),h(s,we,n),e(we,Do),h(s,fl,n),h(s,q,n),e(q,So),e(q,On),e(On,Lo),e(q,No),e(q,In),e(In,Oo),e(q,Io),e(q,Fn),e(Fn,Fo),e(q,Ho),h(s,jl,n),j($s,s,n),h(s,dl,n),h(s,X,n),e(X,vs),e(vs,Hn),j(na,Hn,null),e(X,Ro),e(X,Rn),e(Rn,Bo),h(s,gl,n),h(s,ws,n),e(ws,Wo),e(ws,ke),e(ke,Jo),e(ws,Mo),h(s,_l,n),j(ta,s,n),h(s,$l,n),h(s,F,n),e(F,Uo),e(F,la),e(la,Vo),e(F,Go),e(F,pa),e(pa,Yo),e(F,Ko),h(s,vl,n),j(ra,s,n),h(s,wl,n),h(s,H,n),e(H,Qo),e(H,Bn),e(Bn,Xo),e(H,Zo),e(H,Wn),e(Wn,sh),e(H,ah),h(s,kl,n),j(oa,s,n),h(s,yl,n),h(s,ye,n),e(ye,eh),h(s,xl,n),h(s,R,n),e(R,xe),e(xe,Jn),e(Jn,nh),e(xe,th),e(R,lh),e(R,Ee),e(Ee,Mn),e(Mn,ph),e(Ee,rh),e(R,oh),e(R,qe),e(qe,Un),e(Un,hh),e(qe,uh),h(s,El,n),h(s,Z,n),e(Z,ks),e(ks,Vn),j(ha,Vn,null),e(Z,ch),e(Z,Gn),e(Gn,mh),h(s,ql,n),h(s,ys,n),e(ys,ih),e(ys,ua),e(ua,bh),e(ys,fh),h(s,Al,n),h(s,xs,n),e(xs,jh),e(xs,ca),e(ca,dh),e(xs,gh),h(s,Pl,n),j(ma,s,n),h(s,zl,n),h(s,Ae,n),e(Ae,ia),e(ia,_h),e(ia,ba),e(ba,$h),e(ia,vh),h(s,Tl,n),j(fa,s,n),h(s,Cl,n),h(s,ja,n),e(ja,Yn),e(Yn,wh),h(s,Dl,n),j(da,s,n),h(s,Sl,n),h(s,Es,n),e(Es,kh),e(Es,Kn),e(Kn,yh),e(Es,xh),h(s,Ll,n),h(s,ss,n),e(ss,qs),e(qs,Qn),j(ga,Qn,null),e(ss,Eh),e(ss,Xn),e(Xn,qh),h(s,Nl,n),h(s,A,n),e(A,Ah),e(A,Zn),e(Zn,Ph),e(A,zh),e(A,st),e(st,Th),e(A,Ch),e(A,at),e(at,Dh),e(A,Sh),h(s,Ol,n),h(s,As,n),e(As,Lh),e(As,Pe),e(Pe,Nh),e(As,Oh),h(s,Il,n),j(_a,s,n),h(s,Fl,n),h(s,B,n),e(B,Ih),e(B,et),e(et,Fh),e(B,Hh),e(B,nt),e(nt,Rh),e(B,Bh),h(s,Hl,n),j($a,s,n),h(s,Rl,n),h(s,as,n),e(as,Ps),e(Ps,tt),j(va,tt,null),e(as,Wh),e(as,lt),e(lt,Jh),h(s,Bl,n),h(s,ze,n),e(ze,Mh),h(s,Wl,n),j(wa,s,n),h(s,Jl,n),h(s,Te,n),e(Te,Uh),h(s,Ml,n),j(ka,s,n),h(s,Ul,n),h(s,Ce,n),e(Ce,Vh),h(s,Vl,n),j(ya,s,n),h(s,Gl,n),h(s,De,n),e(De,Gh),h(s,Yl,n),j(xa,s,n),h(s,Kl,n),h(s,Se,n),e(Se,Yh),h(s,Ql,n),h(s,es,n),e(es,zs),e(zs,pt),j(Ea,pt,null),e(es,Kh),e(es,rt),e(rt,Qh),h(s,Xl,n),h(s,Le,n),e(Le,Xh),h(s,Zl,n),h(s,W,n),e(W,Zh),e(W,qa),e(qa,su),e(W,au),e(W,ot),e(ot,eu),e(W,nu),h(s,sp,n),j(Aa,s,n),h(s,ap,n),h(s,Ts,n),e(Ts,tu),e(Ts,Pa),e(Pa,ht),e(ht,lu),e(Ts,pu),h(s,ep,n),j(za,s,n),h(s,np,n),h(s,Ne,n),e(Ne,Oe),h(s,tp,n),h(s,ns,n),e(ns,Cs),e(Cs,ut),j(Ta,ut,null),e(ns,ru),e(ns,ct),e(ct,ou),h(s,lp,n),h(s,Ds,n),e(Ds,hu),e(Ds,Ie),e(Ie,uu),e(Ds,cu),h(s,pp,n),j(Ca,s,n),h(s,rp,n),h(s,ts,n),e(ts,Ss),e(Ss,mt),j(Da,mt,null),e(ts,mu),e(ts,it),e(it,iu),h(s,op,n),h(s,Ls,n),e(Ls,bu),e(Ls,Sa),e(Sa,bt),e(bt,fu),e(Ls,ju),h(s,hp,n),h(s,Fe,n),e(Fe,z),e(z,du),e(z,La),e(La,ft),e(ft,gu),e(z,_u),e(z,Na),e(Na,jt),e(jt,$u),e(z,vu),e(z,Oa),e(Oa,dt),e(dt,wu),e(z,ku),h(s,up,n),j(Ia,s,n),h(s,cp,n),h(s,Fa,n),e(Fa,ls),e(ls,yu),e(ls,He),e(He,gt),e(gt,xu),e(ls,Eu),e(ls,_t),e(_t,qu),e(ls,Au),h(s,mp,n),j(Ha,s,n),h(s,ip,n),h(s,Ra,n),e(Ra,Ba),e(Ba,Pu),e(Ba,Wa),e(Wa,$t),e($t,zu),e(Ba,Tu),h(s,bp,n),j(Ja,s,n),h(s,fp,n),h(s,Ma,n),e(Ma,Ua),e(Ua,Cu),e(Ua,vt),e(vt,Du),e(Ua,Su),h(s,jp,n),j(Va,s,n),h(s,dp,n),h(s,Re,n),e(Re,Lu),h(s,gp,n),j(Ga,s,n),h(s,_p,n),h(s,Be,n),e(Be,We),h(s,$p,n),h(s,ps,n),e(ps,Ns),e(Ns,wt),j(Ya,wt,null),e(ps,Nu),e(ps,kt),e(kt,Ou),h(s,vp,n),h(s,Je,n),e(Je,Iu),h(s,wp,n),h(s,Os,n),e(Os,yt),e(yt,Fu),e(Os,Hu),e(Os,xt),e(xt,Ru),h(s,kp,n),h(s,Is,n),e(Is,Bu),e(Is,Ka),e(Ka,Wu),e(Is,Ju),h(s,yp,n),j(Qa,s,n),h(s,xp,n),h(s,J,n),e(J,Mu),e(J,Et),e(Et,Uu),e(J,Vu),e(J,qt),e(qt,Gu),e(J,Yu),h(s,Ep,n),j(Xa,s,n),h(s,qp,n),h(s,M,n),e(M,Ku),e(M,At),e(At,Qu),e(M,Xu),e(M,Pt),e(Pt,Zu),e(M,sc),h(s,Ap,n),j(Za,s,n),h(s,Pp,n),h(s,Fs,n),e(Fs,ac),e(Fs,Me),e(Me,ec),e(Fs,nc),h(s,zp,n),j(se,s,n),h(s,Tp,n),h(s,rs,n),e(rs,Hs),e(Hs,zt),j(ae,zt,null),e(rs,tc),e(rs,Tt),e(Tt,lc),h(s,Cp,n),h(s,Ue,n),e(Ue,pc),h(s,Dp,n),j(ee,s,n),h(s,Sp,n),h(s,Ve,n),e(Ve,os),e(os,rc),e(os,Ct),e(Ct,oc),e(os,hc),e(os,Dt),e(Dt,uc),e(os,cc),h(s,Lp,n),j(ne,s,n),h(s,Np,n),h(s,te,n),e(te,le),e(le,mc),e(le,St),e(St,ic),e(le,bc),h(s,Op,n),j(pe,s,n),h(s,Ip,n),h(s,U,n),e(U,fc),e(U,Lt),e(Lt,jc),e(U,dc),e(U,Nt),e(Nt,gc),e(U,_c),h(s,Fp,n),h(s,Ge,n),e(Ge,$c),Hp=!0},p(s,[n]){const re={};n&2&&(re.$$scope={dirty:n,ctx:s}),cs.$set(re);const Ot={};n&2&&(Ot.$$scope={dirty:n,ctx:s}),$s.$set(Ot)},i(s){Hp||(d(E.$$.fragment,s),d(Js.$$.fragment,s),d(Ms.$$.fragment,s),d(Us.$$.fragment,s),d(cs.$$.fragment,s),d(Vs.$$.fragment,s),d(Gs.$$.fragment,s),d(Ys.$$.fragment,s),d(Ks.$$.fragment,s),d(Qs.$$.fragment,s),d(Xs.$$.fragment,s),d(Zs.$$.fragment,s),d(sa.$$.fragment,s),d(aa.$$.fragment,s),d(ea.$$.fragment,s),d($s.$$.fragment,s),d(na.$$.fragment,s),d(ta.$$.fragment,s),d(ra.$$.fragment,s),d(oa.$$.fragment,s),d(ha.$$.fragment,s),d(ma.$$.fragment,s),d(fa.$$.fragment,s),d(da.$$.fragment,s),d(ga.$$.fragment,s),d(_a.$$.fragment,s),d($a.$$.fragment,s),d(va.$$.fragment,s),d(wa.$$.fragment,s),d(ka.$$.fragment,s),d(ya.$$.fragment,s),d(xa.$$.fragment,s),d(Ea.$$.fragment,s),d(Aa.$$.fragment,s),d(za.$$.fragment,s),d(Ta.$$.fragment,s),d(Ca.$$.fragment,s),d(Da.$$.fragment,s),d(Ia.$$.fragment,s),d(Ha.$$.fragment,s),d(Ja.$$.fragment,s),d(Va.$$.fragment,s),d(Ga.$$.fragment,s),d(Ya.$$.fragment,s),d(Qa.$$.fragment,s),d(Xa.$$.fragment,s),d(Za.$$.fragment,s),d(se.$$.fragment,s),d(ae.$$.fragment,s),d(ee.$$.fragment,s),d(ne.$$.fragment,s),d(pe.$$.fragment,s),Hp=!0)},o(s){g(E.$$.fragment,s),g(Js.$$.fragment,s),g(Ms.$$.fragment,s),g(Us.$$.fragment,s),g(cs.$$.fragment,s),g(Vs.$$.fragment,s),g(Gs.$$.fragment,s),g(Ys.$$.fragment,s),g(Ks.$$.fragment,s),g(Qs.$$.fragment,s),g(Xs.$$.fragment,s),g(Zs.$$.fragment,s),g(sa.$$.fragment,s),g(aa.$$.fragment,s),g(ea.$$.fragment,s),g($s.$$.fragment,s),g(na.$$.fragment,s),g(ta.$$.fragment,s),g(ra.$$.fragment,s),g(oa.$$.fragment,s),g(ha.$$.fragment,s),g(ma.$$.fragment,s),g(fa.$$.fragment,s),g(da.$$.fragment,s),g(ga.$$.fragment,s),g(_a.$$.fragment,s),g($a.$$.fragment,s),g(va.$$.fragment,s),g(wa.$$.fragment,s),g(ka.$$.fragment,s),g(ya.$$.fragment,s),g(xa.$$.fragment,s),g(Ea.$$.fragment,s),g(Aa.$$.fragment,s),g(za.$$.fragment,s),g(Ta.$$.fragment,s),g(Ca.$$.fragment,s),g(Da.$$.fragment,s),g(Ia.$$.fragment,s),g(Ha.$$.fragment,s),g(Ja.$$.fragment,s),g(Va.$$.fragment,s),g(Ga.$$.fragment,s),g(Ya.$$.fragment,s),g(Qa.$$.fragment,s),g(Xa.$$.fragment,s),g(Za.$$.fragment,s),g(se.$$.fragment,s),g(ae.$$.fragment,s),g(ee.$$.fragment,s),g(ne.$$.fragment,s),g(pe.$$.fragment,s),Hp=!1},d(s){a($),s&&a(w),s&&a(i),_(E),s&&a(It),_(Js,s),s&&a(Ft),s&&a(oe),s&&a(Ht),s&&a(C),s&&a(Rt),s&&a(V),_(Ms),s&&a(Bt),_(Us,s),s&&a(Wt),s&&a(D),s&&a(Jt),_(cs,s),s&&a(Mt),s&&a(S),s&&a(Ut),s&&a(G),_(Vs),s&&a(Vt),s&&a(is),s&&a(Gt),_(Gs,s),s&&a(Yt),s&&a(me),s&&a(Kt),_(Ys,s),s&&a(Qt),s&&a(ie),s&&a(Xt),s&&a(L),s&&a(Zt),s&&a(bs),s&&a(sl),_(Ks,s),s&&a(al),s&&a(N),s&&a(el),s&&a($e),s&&a(nl),_(Qs,s),s&&a(tl),s&&a(Y),_(Xs),s&&a(ll),s&&a(js),s&&a(pl),s&&a(O),s&&a(rl),_(Zs,s),s&&a(ol),s&&a(ds),s&&a(hl),s&&a(K),_(sa),s&&a(ul),s&&a(ve),s&&a(cl),s&&a(I),s&&a(ml),_(aa,s),s&&a(il),s&&a(Q),_(ea),s&&a(bl),s&&a(we),s&&a(fl),s&&a(q),s&&a(jl),_($s,s),s&&a(dl),s&&a(X),_(na),s&&a(gl),s&&a(ws),s&&a(_l),_(ta,s),s&&a($l),s&&a(F),s&&a(vl),_(ra,s),s&&a(wl),s&&a(H),s&&a(kl),_(oa,s),s&&a(yl),s&&a(ye),s&&a(xl),s&&a(R),s&&a(El),s&&a(Z),_(ha),s&&a(ql),s&&a(ys),s&&a(Al),s&&a(xs),s&&a(Pl),_(ma,s),s&&a(zl),s&&a(Ae),s&&a(Tl),_(fa,s),s&&a(Cl),s&&a(ja),s&&a(Dl),_(da,s),s&&a(Sl),s&&a(Es),s&&a(Ll),s&&a(ss),_(ga),s&&a(Nl),s&&a(A),s&&a(Ol),s&&a(As),s&&a(Il),_(_a,s),s&&a(Fl),s&&a(B),s&&a(Hl),_($a,s),s&&a(Rl),s&&a(as),_(va),s&&a(Bl),s&&a(ze),s&&a(Wl),_(wa,s),s&&a(Jl),s&&a(Te),s&&a(Ml),_(ka,s),s&&a(Ul),s&&a(Ce),s&&a(Vl),_(ya,s),s&&a(Gl),s&&a(De),s&&a(Yl),_(xa,s),s&&a(Kl),s&&a(Se),s&&a(Ql),s&&a(es),_(Ea),s&&a(Xl),s&&a(Le),s&&a(Zl),s&&a(W),s&&a(sp),_(Aa,s),s&&a(ap),s&&a(Ts),s&&a(ep),_(za,s),s&&a(np),s&&a(Ne),s&&a(tp),s&&a(ns),_(Ta),s&&a(lp),s&&a(Ds),s&&a(pp),_(Ca,s),s&&a(rp),s&&a(ts),_(Da),s&&a(op),s&&a(Ls),s&&a(hp),s&&a(Fe),s&&a(up),_(Ia,s),s&&a(cp),s&&a(Fa),s&&a(mp),_(Ha,s),s&&a(ip),s&&a(Ra),s&&a(bp),_(Ja,s),s&&a(fp),s&&a(Ma),s&&a(jp),_(Va,s),s&&a(dp),s&&a(Re),s&&a(gp),_(Ga,s),s&&a(_p),s&&a(Be),s&&a($p),s&&a(ps),_(Ya),s&&a(vp),s&&a(Je),s&&a(wp),s&&a(Os),s&&a(kp),s&&a(Is),s&&a(yp),_(Qa,s),s&&a(xp),s&&a(J),s&&a(Ep),_(Xa,s),s&&a(qp),s&&a(M),s&&a(Ap),_(Za,s),s&&a(Pp),s&&a(Fs),s&&a(zp),_(se,s),s&&a(Tp),s&&a(rs),_(ae),s&&a(Cp),s&&a(Ue),s&&a(Dp),_(ee,s),s&&a(Sp),s&&a(Ve),s&&a(Lp),_(ne,s),s&&a(Np),s&&a(te),s&&a(Op),_(pe,s),s&&a(Ip),s&&a(U),s&&a(Fp),s&&a(Ge)}}}const zb={local:"preprocess",sections:[{local:"nlp",sections:[{local:"tokenize",title:"Tokenize"},{local:"pad",title:"Pad"},{local:"truncation",title:"Truncation"},{local:"build-tensors",title:"Build tensors"}],title:"NLP"},{local:"audio",sections:[{local:"resample",title:"Resample"},{local:"feature-extractor",title:"Feature extractor"},{local:"pad-and-truncate",title:"Pad and truncate"}],title:"Audio"},{local:"vision",sections:[{local:"feature-extractor",title:"Feature extractor"},{local:"data-augmentation",title:"Data augmentation"}],title:"Vision"},{local:"multimodal",sections:[{local:"processor",title:"Processor"}],title:"Multimodal"}],title:"Preprocess"};function Tb(P){return _b(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fb extends fb{constructor($){super();jb(this,$,Tb,Pb,db,{})}}export{Fb as default,zb as metadata};
