import{S as er,i as tr,s as ar,e as s,k as i,w as _,t as r,M as sr,c as n,d as a,m as p,a as o,x as k,h as l,b as c,G as e,g as h,y as b,q as v,o as w,B as y,v as nr,L as Zo}from"../../chunks/vendor-hf-doc-builder.js";import{D as le}from"../../chunks/Docstring-hf-doc-builder.js";import{C as xa}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as tt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Qo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function or(Oe){let g,x,$,u,q;return u=new xa({props:{code:`from transformers import NllbTokenizer

tokenizer = NllbTokenizer.from_pretrained(
    "facebook/nllb-200-distilled-600M", src_lang="eng_Latn", tgt_lang="fra_Latn"
)
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(expected_translation_french, return_tensors="pt")
inputs["labels"] = labels["input_ids"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]`}}),{c(){g=s("p"),x=r("Examples:"),$=i(),_(u.$$.fragment)},l(f){g=n(f,"P",{});var L=o(g);x=l(L,"Examples:"),L.forEach(a),$=p(f),k(u.$$.fragment,f)},m(f,L){h(f,g,L),e(g,x),h(f,$,L),b(u,f,L),q=!0},p:Zo,i(f){q||(v(u.$$.fragment,f),q=!0)},o(f){w(u.$$.fragment,f),q=!1},d(f){f&&a(g),f&&a($),y(u,f)}}}function rr(Oe){let g,x,$,u,q;return u=new xa({props:{code:`from transformers import NllbTokenizerFast

tokenizer = NllbTokenizerFast.from_pretrained(
    "facebook/nllb-200-distilled-600M", src_lang="eng_Latn", tgt_lang="fra_Latn"
)
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(expected_translation_french, return_tensors="pt")
inputs["labels"] = labels["input_ids"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizerFast.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]`}}),{c(){g=s("p"),x=r("Examples:"),$=i(),_(u.$$.fragment)},l(f){g=n(f,"P",{});var L=o(g);x=l(L,"Examples:"),L.forEach(a),$=p(f),k(u.$$.fragment,f)},m(f,L){h(f,g,L),e(g,x),h(f,$,L),b(u,f,L),q=!0},p:Zo,i(f){q||(v(u.$$.fragment,f),q=!0)},o(f){w(u.$$.fragment,f),q=!1},d(f){f&&a(g),f&&a($),y(u,f)}}}function lr(Oe){let g,x,$,u,q,f,L,at,ja,Rt,D,st,Sa,Aa,ie,Pa,Ma,Jt,F,X,nt,pe,Ca,ot,Ba,Xt,W,Da,ce,Fa,Oa,Wt,Ie,Ia,Kt,Ge,rt,Ga,Vt,He,Ha,Yt,A,Ua,de,Ra,Ja,he,Xa,Wa,Qt,O,K,lt,fe,Ka,it,Va,Zt,P,Ya,pt,Qa,Za,ct,es,ts,ea,M,as,dt,ss,ns,ue,os,rs,ta,ge,aa,I,V,ht,me,ls,ft,is,sa,C,ps,ut,cs,ds,gt,hs,fs,na,Ue,us,oa,_e,ra,G,Y,mt,ke,gs,_t,ms,la,E,be,_s,kt,ks,bs,S,vs,Re,ws,ys,Je,$s,Es,ve,zs,Ls,qs,H,Ns,bt,Ts,xs,vt,js,Ss,As,Q,Ps,Z,we,Ms,wt,Cs,Bs,j,ye,Ds,$e,Fs,yt,Os,Is,Gs,Ee,ze,$t,Hs,Us,Et,Rs,Js,Le,zt,Xs,Ws,Lt,Ks,Vs,qt,Ys,ia,U,ee,Nt,qe,Qs,Tt,Zs,pa,m,Ne,en,R,tn,xt,an,sn,Te,nn,on,rn,xe,ln,Xe,pn,cn,dn,J,hn,jt,fn,un,St,gn,mn,_n,te,kn,N,je,bn,At,vn,wn,Se,yn,Pt,$n,En,zn,Ae,Pe,Mt,Ln,qn,Ct,Nn,Tn,Me,Bt,xn,jn,Dt,Sn,An,Ft,Pn,Mn,ae,Ce,Cn,Ot,Bn,Dn,se,Be,Fn,It,On,In,ne,De,Gn,Gt,Hn,ca;return f=new tt({}),pe=new tt({}),fe=new tt({}),ge=new xa({props:{code:`from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M")

article = "UN Chief says there is no military solution in Syria"
inputs = tokenizer(article, return_tensors="pt")

translated_tokens = model.generate(
    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["fra_Latn"], max_length=30
)
tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translated_tokens = model.generate(
<span class="hljs-meta">... </span>    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[<span class="hljs-string">&quot;fra_Latn&quot;</span>], max_length=<span class="hljs-number">30</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
Le chef de l<span class="hljs-string">&#x27;ONU dit qu&#x27;</span>il n<span class="hljs-string">&#x27;y a pas de solution militaire en Syrie</span>`}}),me=new tt({}),_e=new xa({props:{code:`from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "facebook/nllb-200-distilled-600M", use_auth_token=True, src_lang="ron_Latn"
)
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M", use_auth_token=True)

article = "\u015Eeful ONU spune c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \xEEn Siria"
inputs = tokenizer(article, return_tensors="pt")

translated_tokens = model.generate(
    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["deu_Latn"], max_length=30
)
tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, use_auth_token=<span class="hljs-literal">True</span>, src_lang=<span class="hljs-string">&quot;ron_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, use_auth_token=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;\u015Eeful ONU spune c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \xEEn Siria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translated_tokens = model.generate(
<span class="hljs-meta">... </span>    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[<span class="hljs-string">&quot;deu_Latn&quot;</span>], max_length=<span class="hljs-number">30</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
UN-Chef sagt, es gibt keine milit\xE4rische L\xF6sung <span class="hljs-keyword">in</span> Syrien`}}),ke=new tt({}),be=new le({props:{name:"class transformers.NllbTokenizer",anchor:"transformers.NllbTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"tokenizer_file",val:" = None"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb.py#L50"}}),Q=new Qo({props:{anchor:"transformers.NllbTokenizer.example",$$slots:{default:[or]},$$scope:{ctx:Oe}}}),we=new le({props:{name:"as_target_tokenizer",anchor:"transformers.NllbTokenizer.as_target_tokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb.py#L341"}}),ye=new le({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb.py#L220",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),qe=new tt({}),Ne=new le({props:{name:"class transformers.NllbTokenizerFast",anchor:"transformers.NllbTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb_fast.py#L62"}}),te=new Qo({props:{anchor:"transformers.NllbTokenizerFast.example",$$slots:{default:[rr]},$$scope:{ctx:Oe}}}),je=new le({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb_fast.py#L163",returnDescription:`
<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ce=new le({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb_fast.py#L192",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Be=new le({props:{name:"set_src_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_src_lang_special_tokens",parameters:[{name:"src_lang",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb_fast.py#L251"}}),De=new le({props:{name:"set_tgt_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_tgt_lang_special_tokens",parameters:[{name:"lang",val:": str"}],source:"https://github.com/huggingface/transformers/blob/vr_18126/src/transformers/models/nllb/tokenization_nllb_fast.py#L266"}}),{c(){g=s("meta"),x=i(),$=s("h1"),u=s("a"),q=s("span"),_(f.$$.fragment),L=i(),at=s("span"),ja=r("NLLB"),Rt=i(),D=s("p"),st=s("strong"),Sa=r("DISCLAIMER:"),Aa=r(" If you see something strange, file a "),ie=s("a"),Pa=r("Github Issue"),Ma=r(` and assign
@LysandreJik`),Jt=i(),F=s("h2"),X=s("a"),nt=s("span"),_(pe.$$.fragment),Ca=i(),ot=s("span"),Ba=r("Overview of NLLB"),Xt=i(),W=s("p"),Da=r("The NLLB model was presented in "),ce=s("a"),Fa=r("No Language Left Behind: Scaling Human-Centered Machine Translation"),Oa=r(` by Marta R. Costa-juss\xE0, James Cross, Onur \xC7elebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\xE1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.`),Wt=i(),Ie=s("p"),Ia=r("The abstract of the paper is the following:"),Kt=i(),Ge=s("p"),rt=s("em"),Ga=r(`Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.
However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the
200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by
first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.`),Vt=i(),He=s("p"),Ha=r("This implementation contains the dense models available on release. Let us know via a GitHub issue if you would like to see the MoE models as well."),Yt=i(),A=s("p"),Ua=r("This model was contributed by "),de=s("a"),Ra=r("Lysandre"),Ja=r(". The authors\u2019 code can be found "),he=s("a"),Xa=r("here"),Wa=r("."),Qt=i(),O=s("h2"),K=s("a"),lt=s("span"),_(fe.$$.fragment),Ka=i(),it=s("span"),Va=r("Generating with NLLB"),Zt=i(),P=s("p"),Ya=r("While generating the target text set the "),pt=s("code"),Qa=r("forced_bos_token_id"),Za=r(` to the target language id. The following
example shows how to translate English to French using the `),ct=s("em"),es=r("facebook/nllb-200-distilled-600M"),ts=r(" model."),ea=i(),M=s("p"),as=r("Note that we\u2019re using the BCP-47 code for French "),dt=s("code"),ss=r("fra_Latn"),ns=r(". See "),ue=s("a"),os=r("here"),rs=r(`
for the list of all BCP-47 in the Flores 200 dataset.`),ta=i(),_(ge.$$.fragment),aa=i(),I=s("h3"),V=s("a"),ht=s("span"),_(me.$$.fragment),ls=i(),ft=s("span"),is=r("Generating from any other language than English"),sa=i(),C=s("p"),ps=r("English ("),ut=s("code"),cs=r("eng_Latn"),ds=r(`) is set as the default language from which to translate. In order to specify that you\u2019d like to translate from a different language,
you should specify the BCP-47 code in the `),gt=s("code"),hs=r("src_lang"),fs=r(" keyword argument of the tokenizer initialization."),na=i(),Ue=s("p"),us=r("See example below for a translation from romanian to german:"),oa=i(),_(_e.$$.fragment),ra=i(),G=s("h2"),Y=s("a"),mt=s("span"),_(ke.$$.fragment),gs=i(),_t=s("span"),ms=r("NllbTokenizer"),la=i(),E=s("div"),_(be.$$.fragment),_s=i(),kt=s("p"),ks=r("Construct an NLLB tokenizer."),bs=i(),S=s("p"),vs=r("Adapted from "),Re=s("a"),ws=r("RobertaTokenizer"),ys=r(" and "),Je=s("a"),$s=r("XLNetTokenizer"),Es=r(`. Based on
`),ve=s("a"),zs=r("SentencePiece"),Ls=r("."),qs=i(),H=s("p"),Ns=r("The tokenization method is "),bt=s("code"),Ts=r("<tokens> <eos> <language code>"),xs=r(" for source language documents, and "),vt=s("code"),js=r("<language code> <tokens> <eos>"),Ss=r(" for target language documents."),As=i(),_(Q.$$.fragment),Ps=i(),Z=s("div"),_(we.$$.fragment),Ms=i(),wt=s("p"),Cs=r(`Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to
sequence-to-sequence models that need a slightly different processing for the labels.`),Bs=i(),j=s("div"),_(ye.$$.fragment),Ds=i(),$e=s("p"),Fs=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An NLLB sequence has the following format, where `),yt=s("code"),Os=r("X"),Is=r(" represents the sequence:"),Gs=i(),Ee=s("ul"),ze=s("li"),$t=s("code"),Hs=r("input_ids"),Us=r(" (for encoder) "),Et=s("code"),Rs=r("X [eos, src_lang_code]"),Js=i(),Le=s("li"),zt=s("code"),Xs=r("decoder_input_ids"),Ws=r(": (for decoder) "),Lt=s("code"),Ks=r("X [eos, tgt_lang_code]"),Vs=i(),qt=s("p"),Ys=r(`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`),ia=i(),U=s("h2"),ee=s("a"),Nt=s("span"),_(qe.$$.fragment),Qs=i(),Tt=s("span"),Zs=r("NllbTokenizerFast"),pa=i(),m=s("div"),_(Ne.$$.fragment),en=i(),R=s("p"),tn=r("Construct a \u201Cfast\u201D NLLB tokenizer (backed by HuggingFace\u2019s "),xt=s("em"),an=r("tokenizers"),sn=r(` library). Based on
`),Te=s("a"),nn=r("BPE"),on=r("."),rn=i(),xe=s("p"),ln=r("This tokenizer inherits from "),Xe=s("a"),pn=r("PreTrainedTokenizerFast"),cn=r(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),dn=i(),J=s("p"),hn=r("The tokenization method is "),jt=s("code"),fn=r("<tokens> <eos> <language code>"),un=r(" for source language documents, and "),St=s("code"),gn=r("<language code> <tokens> <eos>"),mn=r(" for target language documents."),_n=i(),_(te.$$.fragment),kn=i(),N=s("div"),_(je.$$.fragment),bn=i(),At=s("p"),vn=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. The special tokens depend on calling set_lang.`),wn=i(),Se=s("p"),yn=r("An NLLB sequence has the following format, where "),Pt=s("code"),$n=r("X"),En=r(" represents the sequence:"),zn=i(),Ae=s("ul"),Pe=s("li"),Mt=s("code"),Ln=r("input_ids"),qn=r(" (for encoder) "),Ct=s("code"),Nn=r("X [eos, src_lang_code]"),Tn=i(),Me=s("li"),Bt=s("code"),xn=r("decoder_input_ids"),jn=r(": (for decoder) "),Dt=s("code"),Sn=r("X [eos, tgt_lang_code]"),An=i(),Ft=s("p"),Pn=r(`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`),Mn=i(),ae=s("div"),_(Ce.$$.fragment),Cn=i(),Ot=s("p"),Bn=r(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.`),Dn=i(),se=s("div"),_(Be.$$.fragment),Fn=i(),It=s("p"),On=r("Reset the special tokens to the source lang setting. No prefix and suffix=[eos, src_lang_code]."),In=i(),ne=s("div"),_(De.$$.fragment),Gn=i(),Gt=s("p"),Hn=r("Reset the special tokens to the target language setting. No prefix and suffix=[eos, tgt_lang_code]."),this.h()},l(t){const d=sr('[data-svelte="svelte-1phssyn"]',document.head);g=n(d,"META",{name:!0,content:!0}),d.forEach(a),x=p(t),$=n(t,"H1",{class:!0});var Fe=o($);u=n(Fe,"A",{id:!0,class:!0,href:!0});var Ht=o(u);q=n(Ht,"SPAN",{});var Un=o(q);k(f.$$.fragment,Un),Un.forEach(a),Ht.forEach(a),L=p(Fe),at=n(Fe,"SPAN",{});var Rn=o(at);ja=l(Rn,"NLLB"),Rn.forEach(a),Fe.forEach(a),Rt=p(t),D=n(t,"P",{});var Ut=o(D);st=n(Ut,"STRONG",{});var Jn=o(st);Sa=l(Jn,"DISCLAIMER:"),Jn.forEach(a),Aa=l(Ut," If you see something strange, file a "),ie=n(Ut,"A",{href:!0,rel:!0});var Xn=o(ie);Pa=l(Xn,"Github Issue"),Xn.forEach(a),Ma=l(Ut,` and assign
@LysandreJik`),Ut.forEach(a),Jt=p(t),F=n(t,"H2",{class:!0});var da=o(F);X=n(da,"A",{id:!0,class:!0,href:!0});var Wn=o(X);nt=n(Wn,"SPAN",{});var Kn=o(nt);k(pe.$$.fragment,Kn),Kn.forEach(a),Wn.forEach(a),Ca=p(da),ot=n(da,"SPAN",{});var Vn=o(ot);Ba=l(Vn,"Overview of NLLB"),Vn.forEach(a),da.forEach(a),Xt=p(t),W=n(t,"P",{});var ha=o(W);Da=l(ha,"The NLLB model was presented in "),ce=n(ha,"A",{href:!0,rel:!0});var Yn=o(ce);Fa=l(Yn,"No Language Left Behind: Scaling Human-Centered Machine Translation"),Yn.forEach(a),Oa=l(ha,` by Marta R. Costa-juss\xE0, James Cross, Onur \xC7elebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\xE1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.`),ha.forEach(a),Wt=p(t),Ie=n(t,"P",{});var Qn=o(Ie);Ia=l(Qn,"The abstract of the paper is the following:"),Qn.forEach(a),Kt=p(t),Ge=n(t,"P",{});var Zn=o(Ge);rt=n(Zn,"EM",{});var eo=o(rt);Ga=l(eo,`Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.
However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the
200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by
first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.`),eo.forEach(a),Zn.forEach(a),Vt=p(t),He=n(t,"P",{});var to=o(He);Ha=l(to,"This implementation contains the dense models available on release. Let us know via a GitHub issue if you would like to see the MoE models as well."),to.forEach(a),Yt=p(t),A=n(t,"P",{});var We=o(A);Ua=l(We,"This model was contributed by "),de=n(We,"A",{href:!0,rel:!0});var ao=o(de);Ra=l(ao,"Lysandre"),ao.forEach(a),Ja=l(We,". The authors\u2019 code can be found "),he=n(We,"A",{href:!0,rel:!0});var so=o(he);Xa=l(so,"here"),so.forEach(a),Wa=l(We,"."),We.forEach(a),Qt=p(t),O=n(t,"H2",{class:!0});var fa=o(O);K=n(fa,"A",{id:!0,class:!0,href:!0});var no=o(K);lt=n(no,"SPAN",{});var oo=o(lt);k(fe.$$.fragment,oo),oo.forEach(a),no.forEach(a),Ka=p(fa),it=n(fa,"SPAN",{});var ro=o(it);Va=l(ro,"Generating with NLLB"),ro.forEach(a),fa.forEach(a),Zt=p(t),P=n(t,"P",{});var Ke=o(P);Ya=l(Ke,"While generating the target text set the "),pt=n(Ke,"CODE",{});var lo=o(pt);Qa=l(lo,"forced_bos_token_id"),lo.forEach(a),Za=l(Ke,` to the target language id. The following
example shows how to translate English to French using the `),ct=n(Ke,"EM",{});var io=o(ct);es=l(io,"facebook/nllb-200-distilled-600M"),io.forEach(a),ts=l(Ke," model."),Ke.forEach(a),ea=p(t),M=n(t,"P",{});var Ve=o(M);as=l(Ve,"Note that we\u2019re using the BCP-47 code for French "),dt=n(Ve,"CODE",{});var po=o(dt);ss=l(po,"fra_Latn"),po.forEach(a),ns=l(Ve,". See "),ue=n(Ve,"A",{href:!0,rel:!0});var co=o(ue);os=l(co,"here"),co.forEach(a),rs=l(Ve,`
for the list of all BCP-47 in the Flores 200 dataset.`),Ve.forEach(a),ta=p(t),k(ge.$$.fragment,t),aa=p(t),I=n(t,"H3",{class:!0});var ua=o(I);V=n(ua,"A",{id:!0,class:!0,href:!0});var ho=o(V);ht=n(ho,"SPAN",{});var fo=o(ht);k(me.$$.fragment,fo),fo.forEach(a),ho.forEach(a),ls=p(ua),ft=n(ua,"SPAN",{});var uo=o(ft);is=l(uo,"Generating from any other language than English"),uo.forEach(a),ua.forEach(a),sa=p(t),C=n(t,"P",{});var Ye=o(C);ps=l(Ye,"English ("),ut=n(Ye,"CODE",{});var go=o(ut);cs=l(go,"eng_Latn"),go.forEach(a),ds=l(Ye,`) is set as the default language from which to translate. In order to specify that you\u2019d like to translate from a different language,
you should specify the BCP-47 code in the `),gt=n(Ye,"CODE",{});var mo=o(gt);hs=l(mo,"src_lang"),mo.forEach(a),fs=l(Ye," keyword argument of the tokenizer initialization."),Ye.forEach(a),na=p(t),Ue=n(t,"P",{});var _o=o(Ue);us=l(_o,"See example below for a translation from romanian to german:"),_o.forEach(a),oa=p(t),k(_e.$$.fragment,t),ra=p(t),G=n(t,"H2",{class:!0});var ga=o(G);Y=n(ga,"A",{id:!0,class:!0,href:!0});var ko=o(Y);mt=n(ko,"SPAN",{});var bo=o(mt);k(ke.$$.fragment,bo),bo.forEach(a),ko.forEach(a),gs=p(ga),_t=n(ga,"SPAN",{});var vo=o(_t);ms=l(vo,"NllbTokenizer"),vo.forEach(a),ga.forEach(a),la=p(t),E=n(t,"DIV",{class:!0});var T=o(E);k(be.$$.fragment,T),_s=p(T),kt=n(T,"P",{});var wo=o(kt);ks=l(wo,"Construct an NLLB tokenizer."),wo.forEach(a),bs=p(T),S=n(T,"P",{});var oe=o(S);vs=l(oe,"Adapted from "),Re=n(oe,"A",{href:!0});var yo=o(Re);ws=l(yo,"RobertaTokenizer"),yo.forEach(a),ys=l(oe," and "),Je=n(oe,"A",{href:!0});var $o=o(Je);$s=l($o,"XLNetTokenizer"),$o.forEach(a),Es=l(oe,`. Based on
`),ve=n(oe,"A",{href:!0,rel:!0});var Eo=o(ve);zs=l(Eo,"SentencePiece"),Eo.forEach(a),Ls=l(oe,"."),oe.forEach(a),qs=p(T),H=n(T,"P",{});var Qe=o(H);Ns=l(Qe,"The tokenization method is "),bt=n(Qe,"CODE",{});var zo=o(bt);Ts=l(zo,"<tokens> <eos> <language code>"),zo.forEach(a),xs=l(Qe," for source language documents, and "),vt=n(Qe,"CODE",{});var Lo=o(vt);js=l(Lo,"<language code> <tokens> <eos>"),Lo.forEach(a),Ss=l(Qe," for target language documents."),Qe.forEach(a),As=p(T),k(Q.$$.fragment,T),Ps=p(T),Z=n(T,"DIV",{class:!0});var ma=o(Z);k(we.$$.fragment,ma),Ms=p(ma),wt=n(ma,"P",{});var qo=o(wt);Cs=l(qo,`Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to
sequence-to-sequence models that need a slightly different processing for the labels.`),qo.forEach(a),ma.forEach(a),Bs=p(T),j=n(T,"DIV",{class:!0});var re=o(j);k(ye.$$.fragment,re),Ds=p(re),$e=n(re,"P",{});var _a=o($e);Fs=l(_a,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An NLLB sequence has the following format, where `),yt=n(_a,"CODE",{});var No=o(yt);Os=l(No,"X"),No.forEach(a),Is=l(_a," represents the sequence:"),_a.forEach(a),Gs=p(re),Ee=n(re,"UL",{});var ka=o(Ee);ze=n(ka,"LI",{});var ba=o(ze);$t=n(ba,"CODE",{});var To=o($t);Hs=l(To,"input_ids"),To.forEach(a),Us=l(ba," (for encoder) "),Et=n(ba,"CODE",{});var xo=o(Et);Rs=l(xo,"X [eos, src_lang_code]"),xo.forEach(a),ba.forEach(a),Js=p(ka),Le=n(ka,"LI",{});var va=o(Le);zt=n(va,"CODE",{});var jo=o(zt);Xs=l(jo,"decoder_input_ids"),jo.forEach(a),Ws=l(va,": (for decoder) "),Lt=n(va,"CODE",{});var So=o(Lt);Ks=l(So,"X [eos, tgt_lang_code]"),So.forEach(a),va.forEach(a),ka.forEach(a),Vs=p(re),qt=n(re,"P",{});var Ao=o(qt);Ys=l(Ao,`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`),Ao.forEach(a),re.forEach(a),T.forEach(a),ia=p(t),U=n(t,"H2",{class:!0});var wa=o(U);ee=n(wa,"A",{id:!0,class:!0,href:!0});var Po=o(ee);Nt=n(Po,"SPAN",{});var Mo=o(Nt);k(qe.$$.fragment,Mo),Mo.forEach(a),Po.forEach(a),Qs=p(wa),Tt=n(wa,"SPAN",{});var Co=o(Tt);Zs=l(Co,"NllbTokenizerFast"),Co.forEach(a),wa.forEach(a),pa=p(t),m=n(t,"DIV",{class:!0});var z=o(m);k(Ne.$$.fragment,z),en=p(z),R=n(z,"P",{});var Ze=o(R);tn=l(Ze,"Construct a \u201Cfast\u201D NLLB tokenizer (backed by HuggingFace\u2019s "),xt=n(Ze,"EM",{});var Bo=o(xt);an=l(Bo,"tokenizers"),Bo.forEach(a),sn=l(Ze,` library). Based on
`),Te=n(Ze,"A",{href:!0,rel:!0});var Do=o(Te);nn=l(Do,"BPE"),Do.forEach(a),on=l(Ze,"."),Ze.forEach(a),rn=p(z),xe=n(z,"P",{});var ya=o(xe);ln=l(ya,"This tokenizer inherits from "),Xe=n(ya,"A",{href:!0});var Fo=o(Xe);pn=l(Fo,"PreTrainedTokenizerFast"),Fo.forEach(a),cn=l(ya,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),ya.forEach(a),dn=p(z),J=n(z,"P",{});var et=o(J);hn=l(et,"The tokenization method is "),jt=n(et,"CODE",{});var Oo=o(jt);fn=l(Oo,"<tokens> <eos> <language code>"),Oo.forEach(a),un=l(et," for source language documents, and "),St=n(et,"CODE",{});var Io=o(St);gn=l(Io,"<language code> <tokens> <eos>"),Io.forEach(a),mn=l(et," for target language documents."),et.forEach(a),_n=p(z),k(te.$$.fragment,z),kn=p(z),N=n(z,"DIV",{class:!0});var B=o(N);k(je.$$.fragment,B),bn=p(B),At=n(B,"P",{});var Go=o(At);vn=l(Go,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. The special tokens depend on calling set_lang.`),Go.forEach(a),wn=p(B),Se=n(B,"P",{});var $a=o(Se);yn=l($a,"An NLLB sequence has the following format, where "),Pt=n($a,"CODE",{});var Ho=o(Pt);$n=l(Ho,"X"),Ho.forEach(a),En=l($a," represents the sequence:"),$a.forEach(a),zn=p(B),Ae=n(B,"UL",{});var Ea=o(Ae);Pe=n(Ea,"LI",{});var za=o(Pe);Mt=n(za,"CODE",{});var Uo=o(Mt);Ln=l(Uo,"input_ids"),Uo.forEach(a),qn=l(za," (for encoder) "),Ct=n(za,"CODE",{});var Ro=o(Ct);Nn=l(Ro,"X [eos, src_lang_code]"),Ro.forEach(a),za.forEach(a),Tn=p(Ea),Me=n(Ea,"LI",{});var La=o(Me);Bt=n(La,"CODE",{});var Jo=o(Bt);xn=l(Jo,"decoder_input_ids"),Jo.forEach(a),jn=l(La,": (for decoder) "),Dt=n(La,"CODE",{});var Xo=o(Dt);Sn=l(Xo,"X [eos, tgt_lang_code]"),Xo.forEach(a),La.forEach(a),Ea.forEach(a),An=p(B),Ft=n(B,"P",{});var Wo=o(Ft);Pn=l(Wo,`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`),Wo.forEach(a),B.forEach(a),Mn=p(z),ae=n(z,"DIV",{class:!0});var qa=o(ae);k(Ce.$$.fragment,qa),Cn=p(qa),Ot=n(qa,"P",{});var Ko=o(Ot);Bn=l(Ko,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.`),Ko.forEach(a),qa.forEach(a),Dn=p(z),se=n(z,"DIV",{class:!0});var Na=o(se);k(Be.$$.fragment,Na),Fn=p(Na),It=n(Na,"P",{});var Vo=o(It);On=l(Vo,"Reset the special tokens to the source lang setting. No prefix and suffix=[eos, src_lang_code]."),Vo.forEach(a),Na.forEach(a),In=p(z),ne=n(z,"DIV",{class:!0});var Ta=o(ne);k(De.$$.fragment,Ta),Gn=p(Ta),Gt=n(Ta,"P",{});var Yo=o(Gt);Hn=l(Yo,"Reset the special tokens to the target language setting. No prefix and suffix=[eos, tgt_lang_code]."),Yo.forEach(a),Ta.forEach(a),z.forEach(a),this.h()},h(){c(g,"name","hf:doc:metadata"),c(g,"content",JSON.stringify(ir)),c(u,"id","nllb"),c(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u,"href","#nllb"),c($,"class","relative group"),c(ie,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=bug&template=bug-report.yml"),c(ie,"rel","nofollow"),c(X,"id","overview-of-nllb"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#overview-of-nllb"),c(F,"class","relative group"),c(ce,"href","https://arxiv.org/abs/2207.04672"),c(ce,"rel","nofollow"),c(de,"href","https://huggingface.co/lysandre"),c(de,"rel","nofollow"),c(he,"href","https://github.com/facebookresearch/fairseq/tree/nllb"),c(he,"rel","nofollow"),c(K,"id","generating-with-nllb"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#generating-with-nllb"),c(O,"class","relative group"),c(ue,"href","https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200"),c(ue,"rel","nofollow"),c(V,"id","generating-from-any-other-language-than-english"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#generating-from-any-other-language-than-english"),c(I,"class","relative group"),c(Y,"id","transformers.NllbTokenizer"),c(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y,"href","#transformers.NllbTokenizer"),c(G,"class","relative group"),c(Re,"href","/docs/transformers/pr_18126/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Je,"href","/docs/transformers/pr_18126/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(ve,"href","https://github.com/google/sentencepiece"),c(ve,"rel","nofollow"),c(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ee,"id","transformers.NllbTokenizerFast"),c(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ee,"href","#transformers.NllbTokenizerFast"),c(U,"class","relative group"),c(Te,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models"),c(Te,"rel","nofollow"),c(Xe,"href","/docs/transformers/pr_18126/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,d){e(document.head,g),h(t,x,d),h(t,$,d),e($,u),e(u,q),b(f,q,null),e($,L),e($,at),e(at,ja),h(t,Rt,d),h(t,D,d),e(D,st),e(st,Sa),e(D,Aa),e(D,ie),e(ie,Pa),e(D,Ma),h(t,Jt,d),h(t,F,d),e(F,X),e(X,nt),b(pe,nt,null),e(F,Ca),e(F,ot),e(ot,Ba),h(t,Xt,d),h(t,W,d),e(W,Da),e(W,ce),e(ce,Fa),e(W,Oa),h(t,Wt,d),h(t,Ie,d),e(Ie,Ia),h(t,Kt,d),h(t,Ge,d),e(Ge,rt),e(rt,Ga),h(t,Vt,d),h(t,He,d),e(He,Ha),h(t,Yt,d),h(t,A,d),e(A,Ua),e(A,de),e(de,Ra),e(A,Ja),e(A,he),e(he,Xa),e(A,Wa),h(t,Qt,d),h(t,O,d),e(O,K),e(K,lt),b(fe,lt,null),e(O,Ka),e(O,it),e(it,Va),h(t,Zt,d),h(t,P,d),e(P,Ya),e(P,pt),e(pt,Qa),e(P,Za),e(P,ct),e(ct,es),e(P,ts),h(t,ea,d),h(t,M,d),e(M,as),e(M,dt),e(dt,ss),e(M,ns),e(M,ue),e(ue,os),e(M,rs),h(t,ta,d),b(ge,t,d),h(t,aa,d),h(t,I,d),e(I,V),e(V,ht),b(me,ht,null),e(I,ls),e(I,ft),e(ft,is),h(t,sa,d),h(t,C,d),e(C,ps),e(C,ut),e(ut,cs),e(C,ds),e(C,gt),e(gt,hs),e(C,fs),h(t,na,d),h(t,Ue,d),e(Ue,us),h(t,oa,d),b(_e,t,d),h(t,ra,d),h(t,G,d),e(G,Y),e(Y,mt),b(ke,mt,null),e(G,gs),e(G,_t),e(_t,ms),h(t,la,d),h(t,E,d),b(be,E,null),e(E,_s),e(E,kt),e(kt,ks),e(E,bs),e(E,S),e(S,vs),e(S,Re),e(Re,ws),e(S,ys),e(S,Je),e(Je,$s),e(S,Es),e(S,ve),e(ve,zs),e(S,Ls),e(E,qs),e(E,H),e(H,Ns),e(H,bt),e(bt,Ts),e(H,xs),e(H,vt),e(vt,js),e(H,Ss),e(E,As),b(Q,E,null),e(E,Ps),e(E,Z),b(we,Z,null),e(Z,Ms),e(Z,wt),e(wt,Cs),e(E,Bs),e(E,j),b(ye,j,null),e(j,Ds),e(j,$e),e($e,Fs),e($e,yt),e(yt,Os),e($e,Is),e(j,Gs),e(j,Ee),e(Ee,ze),e(ze,$t),e($t,Hs),e(ze,Us),e(ze,Et),e(Et,Rs),e(Ee,Js),e(Ee,Le),e(Le,zt),e(zt,Xs),e(Le,Ws),e(Le,Lt),e(Lt,Ks),e(j,Vs),e(j,qt),e(qt,Ys),h(t,ia,d),h(t,U,d),e(U,ee),e(ee,Nt),b(qe,Nt,null),e(U,Qs),e(U,Tt),e(Tt,Zs),h(t,pa,d),h(t,m,d),b(Ne,m,null),e(m,en),e(m,R),e(R,tn),e(R,xt),e(xt,an),e(R,sn),e(R,Te),e(Te,nn),e(R,on),e(m,rn),e(m,xe),e(xe,ln),e(xe,Xe),e(Xe,pn),e(xe,cn),e(m,dn),e(m,J),e(J,hn),e(J,jt),e(jt,fn),e(J,un),e(J,St),e(St,gn),e(J,mn),e(m,_n),b(te,m,null),e(m,kn),e(m,N),b(je,N,null),e(N,bn),e(N,At),e(At,vn),e(N,wn),e(N,Se),e(Se,yn),e(Se,Pt),e(Pt,$n),e(Se,En),e(N,zn),e(N,Ae),e(Ae,Pe),e(Pe,Mt),e(Mt,Ln),e(Pe,qn),e(Pe,Ct),e(Ct,Nn),e(Ae,Tn),e(Ae,Me),e(Me,Bt),e(Bt,xn),e(Me,jn),e(Me,Dt),e(Dt,Sn),e(N,An),e(N,Ft),e(Ft,Pn),e(m,Mn),e(m,ae),b(Ce,ae,null),e(ae,Cn),e(ae,Ot),e(Ot,Bn),e(m,Dn),e(m,se),b(Be,se,null),e(se,Fn),e(se,It),e(It,On),e(m,In),e(m,ne),b(De,ne,null),e(ne,Gn),e(ne,Gt),e(Gt,Hn),ca=!0},p(t,[d]){const Fe={};d&2&&(Fe.$$scope={dirty:d,ctx:t}),Q.$set(Fe);const Ht={};d&2&&(Ht.$$scope={dirty:d,ctx:t}),te.$set(Ht)},i(t){ca||(v(f.$$.fragment,t),v(pe.$$.fragment,t),v(fe.$$.fragment,t),v(ge.$$.fragment,t),v(me.$$.fragment,t),v(_e.$$.fragment,t),v(ke.$$.fragment,t),v(be.$$.fragment,t),v(Q.$$.fragment,t),v(we.$$.fragment,t),v(ye.$$.fragment,t),v(qe.$$.fragment,t),v(Ne.$$.fragment,t),v(te.$$.fragment,t),v(je.$$.fragment,t),v(Ce.$$.fragment,t),v(Be.$$.fragment,t),v(De.$$.fragment,t),ca=!0)},o(t){w(f.$$.fragment,t),w(pe.$$.fragment,t),w(fe.$$.fragment,t),w(ge.$$.fragment,t),w(me.$$.fragment,t),w(_e.$$.fragment,t),w(ke.$$.fragment,t),w(be.$$.fragment,t),w(Q.$$.fragment,t),w(we.$$.fragment,t),w(ye.$$.fragment,t),w(qe.$$.fragment,t),w(Ne.$$.fragment,t),w(te.$$.fragment,t),w(je.$$.fragment,t),w(Ce.$$.fragment,t),w(Be.$$.fragment,t),w(De.$$.fragment,t),ca=!1},d(t){a(g),t&&a(x),t&&a($),y(f),t&&a(Rt),t&&a(D),t&&a(Jt),t&&a(F),y(pe),t&&a(Xt),t&&a(W),t&&a(Wt),t&&a(Ie),t&&a(Kt),t&&a(Ge),t&&a(Vt),t&&a(He),t&&a(Yt),t&&a(A),t&&a(Qt),t&&a(O),y(fe),t&&a(Zt),t&&a(P),t&&a(ea),t&&a(M),t&&a(ta),y(ge,t),t&&a(aa),t&&a(I),y(me),t&&a(sa),t&&a(C),t&&a(na),t&&a(Ue),t&&a(oa),y(_e,t),t&&a(ra),t&&a(G),y(ke),t&&a(la),t&&a(E),y(be),y(Q),y(we),y(ye),t&&a(ia),t&&a(U),y(qe),t&&a(pa),t&&a(m),y(Ne),y(te),y(je),y(Ce),y(Be),y(De)}}}const ir={local:"nllb",sections:[{local:"overview-of-nllb",title:"Overview of NLLB"},{local:"generating-with-nllb",sections:[{local:"generating-from-any-other-language-than-english",title:"Generating from any other language than English"}],title:"Generating with NLLB"},{local:"transformers.NllbTokenizer",title:"NllbTokenizer"},{local:"transformers.NllbTokenizerFast",title:"NllbTokenizerFast"}],title:"NLLB"};function pr(Oe){return nr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gr extends er{constructor(g){super();tr(this,g,pr,lr,ar,{})}}export{gr as default,ir as metadata};
