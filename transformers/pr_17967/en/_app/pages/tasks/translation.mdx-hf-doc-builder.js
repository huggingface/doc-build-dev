import{S as Cs,i as Os,s as Is,e as i,k as _,w as v,t as o,M as Us,c as p,d as t,m as $,a as f,x as S,h as l,b as k,G as s,g as m,y,q as E,o as T,B as x,v as Ns,L as Ms}from"../../chunks/vendor-hf-doc-builder.js";import{T as zt}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Ls}from"../../chunks/Youtube-hf-doc-builder.js";import{I as xt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as re}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Ds,M as At}from"../../chunks/Markdown-hf-doc-builder.js";function Bs(P){let a,c,r,u,w;return{c(){a=i("p"),c=o("See the translation "),r=i("a"),u=o("task page"),w=o(" for more information about its associated models, datasets, and metrics."),this.h()},l(g){a=p(g,"P",{});var q=f(a);c=l(q,"See the translation "),r=p(q,"A",{href:!0,rel:!0});var A=f(r);u=l(A,"task page"),A.forEach(t),w=l(q," for more information about its associated models, datasets, and metrics."),q.forEach(t),this.h()},h(){k(r,"href","https://huggingface.co/tasks/translation"),k(r,"rel","nofollow")},m(g,q){m(g,a,q),s(a,c),s(a,r),s(r,u),s(a,w)},d(g){g&&t(a)}}}function Ws(P){let a,c;return a=new re({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p:Ms,i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Ys(P){let a,c;return a=new At({props:{$$slots:{default:[Ws]},$$scope:{ctx:P}}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p(r,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:r}),a.$set(w)},i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Hs(P){let a,c;return a=new re({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p:Ms,i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Zs(P){let a,c;return a=new At({props:{$$slots:{default:[Hs]},$$scope:{ctx:P}}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p(r,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:r}),a.$set(w)},i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Js(P){let a,c,r,u,w,g,q,A;return{c(){a=i("p"),c=o("If you aren\u2019t familiar with fine-tuning a model with the "),r=i("a"),u=o("Trainer"),w=o(", take a look at the basic tutorial "),g=i("a"),q=o("here"),A=o("!"),this.h()},l(z){a=p(z,"P",{});var j=f(a);c=l(j,"If you aren\u2019t familiar with fine-tuning a model with the "),r=p(j,"A",{href:!0});var L=f(r);u=l(L,"Trainer"),L.forEach(t),w=l(j,", take a look at the basic tutorial "),g=p(j,"A",{href:!0});var N=f(g);q=l(N,"here"),N.forEach(t),A=l(j,"!"),j.forEach(t),this.h()},h(){k(r,"href","/docs/transformers/pr_17967/en/main_classes/trainer#transformers.Trainer"),k(g,"href","../training#finetune-with-trainer")},m(z,j){m(z,a,j),s(a,c),s(a,r),s(r,u),s(a,w),s(a,g),s(g,q),s(a,A)},d(z){z&&t(a)}}}function Ks(P){let a,c,r,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,ne,ee,X,de,D,O,K,I,ge,H,R,he;return q=new re({props:{code:`from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),z=new zt({props:{$$slots:{default:[Js]},$$scope:{ctx:P}}}),R=new re({props:{code:`training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){a=i("p"),c=o("Load T5 with "),r=i("a"),u=o("AutoModelForSeq2SeqLM"),w=o(":"),g=_(),v(q.$$.fragment),A=_(),v(z.$$.fragment),j=_(),L=i("p"),N=o("At this point, only three steps remain:"),B=_(),W=i("ol"),M=i("li"),Y=o("Define your training hyperparameters in "),J=i("a"),Q=o("Seq2SeqTrainingArguments"),ue=o("."),V=_(),C=i("li"),G=o("Pass the training arguments to "),ne=i("a"),ee=o("Seq2SeqTrainer"),X=o(" along with the model, dataset, tokenizer, and data collator."),de=_(),D=i("li"),O=o("Call "),K=i("a"),I=o("train()"),ge=o(" to fine-tune your model."),H=_(),v(R.$$.fragment),this.h()},l(d){a=p(d,"P",{});var F=f(a);c=l(F,"Load T5 with "),r=p(F,"A",{href:!0});var Z=f(r);u=l(Z,"AutoModelForSeq2SeqLM"),Z.forEach(t),w=l(F,":"),F.forEach(t),g=$(d),S(q.$$.fragment,d),A=$(d),S(z.$$.fragment,d),j=$(d),L=p(d,"P",{});var te=f(L);N=l(te,"At this point, only three steps remain:"),te.forEach(t),B=$(d),W=p(d,"OL",{});var U=f(W);M=p(U,"LI",{});var se=f(M);Y=l(se,"Define your training hyperparameters in "),J=p(se,"A",{href:!0});var oe=f(J);Q=l(oe,"Seq2SeqTrainingArguments"),oe.forEach(t),ue=l(se,"."),se.forEach(t),V=$(U),C=p(U,"LI",{});var ae=f(C);G=l(ae,"Pass the training arguments to "),ne=p(ae,"A",{href:!0});var le=f(ne);ee=l(le,"Seq2SeqTrainer"),le.forEach(t),X=l(ae," along with the model, dataset, tokenizer, and data collator."),ae.forEach(t),de=$(U),D=p(U,"LI",{});var ce=f(D);O=l(ce,"Call "),K=p(ce,"A",{href:!0});var ie=f(K);I=l(ie,"train()"),ie.forEach(t),ge=l(ce," to fine-tune your model."),ce.forEach(t),U.forEach(t),H=$(d),S(R.$$.fragment,d),this.h()},h(){k(r,"href","/docs/transformers/pr_17967/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"),k(J,"href","/docs/transformers/pr_17967/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),k(ne,"href","/docs/transformers/pr_17967/en/main_classes/trainer#transformers.Seq2SeqTrainer"),k(K,"href","/docs/transformers/pr_17967/en/main_classes/trainer#transformers.Trainer.train")},m(d,F){m(d,a,F),s(a,c),s(a,r),s(r,u),s(a,w),m(d,g,F),y(q,d,F),m(d,A,F),y(z,d,F),m(d,j,F),m(d,L,F),s(L,N),m(d,B,F),m(d,W,F),s(W,M),s(M,Y),s(M,J),s(J,Q),s(M,ue),s(W,V),s(W,C),s(C,G),s(C,ne),s(ne,ee),s(C,X),s(W,de),s(W,D),s(D,O),s(D,K),s(K,I),s(D,ge),m(d,H,F),y(R,d,F),he=!0},p(d,F){const Z={};F&2&&(Z.$$scope={dirty:F,ctx:d}),z.$set(Z)},i(d){he||(E(q.$$.fragment,d),E(z.$$.fragment,d),E(R.$$.fragment,d),he=!0)},o(d){T(q.$$.fragment,d),T(z.$$.fragment,d),T(R.$$.fragment,d),he=!1},d(d){d&&t(a),d&&t(g),x(q,d),d&&t(A),x(z,d),d&&t(j),d&&t(L),d&&t(B),d&&t(W),d&&t(H),x(R,d)}}}function Rs(P){let a,c;return a=new At({props:{$$slots:{default:[Ks]},$$scope:{ctx:P}}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p(r,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:r}),a.$set(w)},i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Gs(P){let a,c,r,u,w;return{c(){a=i("p"),c=o("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=i("a"),u=o("here"),w=o("!"),this.h()},l(g){a=p(g,"P",{});var q=f(a);c=l(q,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=p(q,"A",{href:!0});var A=f(r);u=l(A,"here"),A.forEach(t),w=l(q,"!"),q.forEach(t),this.h()},h(){k(r,"href","training#finetune-with-keras")},m(g,q){m(g,a,q),s(a,c),s(a,r),s(r,u),s(a,w)},d(g){g&&t(a)}}}function Xs(P){let a,c,r,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,ne,ee,X,de,D,O,K,I,ge,H,R,he,d,F,Z,te,U,se,oe,ae,le,ce,ie,ke,je;return B=new re({props:{code:`tf_train_set = tokenized_books["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = tokenized_books["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_books[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = tokenized_books[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),M=new zt({props:{$$slots:{default:[Gs]},$$scope:{ctx:P}}}),V=new re({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),O=new re({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Z=new re({props:{code:"model.compile(optimizer=optimizer)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),ke=new re({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){a=i("p"),c=o("To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=i("code"),u=o("tf.data.Dataset"),w=o(" format with "),g=i("a"),q=o("to_tf_dataset"),A=o(". Specify inputs and labels in "),z=i("code"),j=o("columns"),L=o(", whether to shuffle the dataset order, batch size, and the data collator:"),N=_(),v(B.$$.fragment),W=_(),v(M.$$.fragment),Y=_(),J=i("p"),Q=o("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ue=_(),v(V.$$.fragment),C=_(),G=i("p"),ne=o("Load T5 with "),ee=i("a"),X=o("TFAutoModelForSeq2SeqLM"),de=o(":"),D=_(),v(O.$$.fragment),K=_(),I=i("p"),ge=o("Configure the model for training with "),H=i("a"),R=i("code"),he=o("compile"),d=o(":"),F=_(),v(Z.$$.fragment),te=_(),U=i("p"),se=o("Call "),oe=i("a"),ae=i("code"),le=o("fit"),ce=o(" to fine-tune the model:"),ie=_(),v(ke.$$.fragment),this.h()},l(n){a=p(n,"P",{});var b=f(a);c=l(b,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=p(b,"CODE",{});var pe=f(r);u=l(pe,"tf.data.Dataset"),pe.forEach(t),w=l(b," format with "),g=p(b,"A",{href:!0,rel:!0});var We=f(g);q=l(We,"to_tf_dataset"),We.forEach(t),A=l(b,". Specify inputs and labels in "),z=p(b,"CODE",{});var Se=f(z);j=l(Se,"columns"),Se.forEach(t),L=l(b,", whether to shuffle the dataset order, batch size, and the data collator:"),b.forEach(t),N=$(n),S(B.$$.fragment,n),W=$(n),S(M.$$.fragment,n),Y=$(n),J=p(n,"P",{});var Ye=f(J);Q=l(Ye,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Ye.forEach(t),ue=$(n),S(V.$$.fragment,n),C=$(n),G=p(n,"P",{});var ye=f(G);ne=l(ye,"Load T5 with "),ee=p(ye,"A",{href:!0});var Le=f(ee);X=l(Le,"TFAutoModelForSeq2SeqLM"),Le.forEach(t),de=l(ye,":"),ye.forEach(t),D=$(n),S(O.$$.fragment,n),K=$(n),I=p(n,"P",{});var fe=f(I);ge=l(fe,"Configure the model for training with "),H=p(fe,"A",{href:!0,rel:!0});var we=f(H);R=p(we,"CODE",{});var Ee=f(R);he=l(Ee,"compile"),Ee.forEach(t),we.forEach(t),d=l(fe,":"),fe.forEach(t),F=$(n),S(Z.$$.fragment,n),te=$(n),U=p(n,"P",{});var _e=f(U);se=l(_e,"Call "),oe=p(_e,"A",{href:!0,rel:!0});var He=f(oe);ae=p(He,"CODE",{});var Te=f(ae);le=l(Te,"fit"),Te.forEach(t),He.forEach(t),ce=l(_e," to fine-tune the model:"),_e.forEach(t),ie=$(n),S(ke.$$.fragment,n),this.h()},h(){k(g,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),k(g,"rel","nofollow"),k(ee,"href","/docs/transformers/pr_17967/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM"),k(H,"href","https://keras.io/api/models/model_training_apis/#compile-method"),k(H,"rel","nofollow"),k(oe,"href","https://keras.io/api/models/model_training_apis/#fit-method"),k(oe,"rel","nofollow")},m(n,b){m(n,a,b),s(a,c),s(a,r),s(r,u),s(a,w),s(a,g),s(g,q),s(a,A),s(a,z),s(z,j),s(a,L),m(n,N,b),y(B,n,b),m(n,W,b),y(M,n,b),m(n,Y,b),m(n,J,b),s(J,Q),m(n,ue,b),y(V,n,b),m(n,C,b),m(n,G,b),s(G,ne),s(G,ee),s(ee,X),s(G,de),m(n,D,b),y(O,n,b),m(n,K,b),m(n,I,b),s(I,ge),s(I,H),s(H,R),s(R,he),s(I,d),m(n,F,b),y(Z,n,b),m(n,te,b),m(n,U,b),s(U,se),s(U,oe),s(oe,ae),s(ae,le),s(U,ce),m(n,ie,b),y(ke,n,b),je=!0},p(n,b){const pe={};b&2&&(pe.$$scope={dirty:b,ctx:n}),M.$set(pe)},i(n){je||(E(B.$$.fragment,n),E(M.$$.fragment,n),E(V.$$.fragment,n),E(O.$$.fragment,n),E(Z.$$.fragment,n),E(ke.$$.fragment,n),je=!0)},o(n){T(B.$$.fragment,n),T(M.$$.fragment,n),T(V.$$.fragment,n),T(O.$$.fragment,n),T(Z.$$.fragment,n),T(ke.$$.fragment,n),je=!1},d(n){n&&t(a),n&&t(N),x(B,n),n&&t(W),x(M,n),n&&t(Y),n&&t(J),n&&t(ue),x(V,n),n&&t(C),n&&t(G),n&&t(D),x(O,n),n&&t(K),n&&t(I),n&&t(F),x(Z,n),n&&t(te),n&&t(U),n&&t(ie),x(ke,n)}}}function Qs(P){let a,c;return a=new At({props:{$$slots:{default:[Xs]},$$scope:{ctx:P}}}),{c(){v(a.$$.fragment)},l(r){S(a.$$.fragment,r)},m(r,u){y(a,r,u),c=!0},p(r,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:r}),a.$set(w)},i(r){c||(E(a.$$.fragment,r),c=!0)},o(r){T(a.$$.fragment,r),c=!1},d(r){x(a,r)}}}function Vs(P){let a,c,r,u,w,g,q,A;return{c(){a=i("p"),c=o(`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),r=i("a"),u=o("PyTorch notebook"),w=o(`
or `),g=i("a"),q=o("TensorFlow notebook"),A=o("."),this.h()},l(z){a=p(z,"P",{});var j=f(a);c=l(j,`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),r=p(j,"A",{href:!0,rel:!0});var L=f(r);u=l(L,"PyTorch notebook"),L.forEach(t),w=l(j,`
or `),g=p(j,"A",{href:!0,rel:!0});var N=f(g);q=l(N,"TensorFlow notebook"),N.forEach(t),A=l(j,"."),j.forEach(t),this.h()},h(){k(r,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb"),k(r,"rel","nofollow"),k(g,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb"),k(g,"rel","nofollow")},m(z,j){m(z,a,j),s(a,c),s(a,r),s(r,u),s(a,w),s(a,g),s(g,q),s(a,A)},d(z){z&&t(a)}}}function ea(P){let a,c,r,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,ne,ee,X,de,D,O,K,I,ge,H,R,he,d,F,Z,te,U,se,oe,ae,le,ce,ie,ke,je,n,b,pe,We,Se,Ye,ye,Le,fe,we,Ee,_e,He,Te,Ft,ft,De,mt,Ze,Pt,ht,Me,ct,Je,Lt,ut,be,Xe,Dt,Mt,Qe,Ct,Ot,Ce,It,Ve,Ut,Nt,dt,Oe,_t,$e,Bt,Ie,Wt,Yt,et,Ht,Zt,tt,Jt,Kt,$t,Ue,gt,me,Rt,Ke,Gt,Xt,st,Qt,Vt,at,es,ts,rt,ss,as,kt,xe,wt,ve,ze,nt,Ne,rs,ot,ns,bt,Ae,qt,Fe,jt;return g=new xt({}),L=new Ls({props:{id:"1JvfrvZgi6c"}}),X=new zt({props:{$$slots:{default:[Bs]},$$scope:{ctx:P}}}),I=new xt({}),te=new re({props:{code:`from datasets import load_dataset

books = load_dataset("opus_books", "en-fr")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`}}),le=new re({props:{code:'books = books["train"].train_test_split(test_size=0.2)',highlighted:'books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),n=new re({props:{code:'books["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau \xE9lev\xE9 ne mesurait que quelques toises, et bient\xF4t nous f\xFBmes rentr\xE9s dans notre \xE9l\xE9ment.&#x27;</span>}}`}}),_e=new xt({}),De=new Ls({props:{id:"XAR8jnZZuUs"}}),Me=new re({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Oe=new re({props:{code:`source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>        labels = tokenizer(targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`}}),Ue=new re({props:{code:"tokenized_books = books.map(preprocess_function, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),xe=new Ds({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Zs],pytorch:[Ys]},$$scope:{ctx:P}}}),Ne=new xt({}),Ae=new Ds({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qs],pytorch:[Rs]},$$scope:{ctx:P}}}),Fe=new zt({props:{$$slots:{default:[Vs]},$$scope:{ctx:P}}}),{c(){a=i("meta"),c=_(),r=i("h1"),u=i("a"),w=i("span"),v(g.$$.fragment),q=_(),A=i("span"),z=o("Translation"),j=_(),v(L.$$.fragment),N=_(),B=i("p"),W=o("Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),M=_(),Y=i("p"),J=o("This guide will show you how to fine-tune "),Q=i("a"),ue=o("T5"),V=o(" on the English-French subset of the "),C=i("a"),G=o("OPUS Books"),ne=o(" dataset to translate English text to French."),ee=_(),v(X.$$.fragment),de=_(),D=i("h2"),O=i("a"),K=i("span"),v(I.$$.fragment),ge=_(),H=i("span"),R=o("Load OPUS Books dataset"),he=_(),d=i("p"),F=o("Load the OPUS Books dataset from the \u{1F917} Datasets library:"),Z=_(),v(te.$$.fragment),U=_(),se=i("p"),oe=o("Split this dataset into a train and test set:"),ae=_(),v(le.$$.fragment),ce=_(),ie=i("p"),ke=o("Then take a look at an example:"),je=_(),v(n.$$.fragment),b=_(),pe=i("p"),We=o("The "),Se=i("code"),Ye=o("translation"),ye=o(" field is a dictionary containing the English and French translations of the text."),Le=_(),fe=i("h2"),we=i("a"),Ee=i("span"),v(_e.$$.fragment),He=_(),Te=i("span"),Ft=o("Preprocess"),ft=_(),v(De.$$.fragment),mt=_(),Ze=i("p"),Pt=o("Load the T5 tokenizer to process the language pairs:"),ht=_(),v(Me.$$.fragment),ct=_(),Je=i("p"),Lt=o("The preprocessing function needs to:"),ut=_(),be=i("ol"),Xe=i("li"),Dt=o("Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Mt=_(),Qe=i("li"),Ct=o("Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),Ot=_(),Ce=i("li"),It=o("Truncate sequences to be no longer than the maximum length set by the "),Ve=i("code"),Ut=o("max_length"),Nt=o(" parameter."),dt=_(),v(Oe.$$.fragment),_t=_(),$e=i("p"),Bt=o("Use \u{1F917} Datasets "),Ie=i("a"),Wt=o("map"),Yt=o(" function to apply the preprocessing function over the entire dataset. You can speed up the "),et=i("code"),Ht=o("map"),Zt=o(" function by setting "),tt=i("code"),Jt=o("batched=True"),Kt=o(" to process multiple elements of the dataset at once:"),$t=_(),v(Ue.$$.fragment),gt=_(),me=i("p"),Rt=o("Use "),Ke=i("a"),Gt=o("DataCollatorForSeq2Seq"),Xt=o(" to create a batch of examples. It will also "),st=i("em"),Qt=o("dynamically pad"),Vt=o(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),at=i("code"),es=o("tokenizer"),ts=o(" function by setting "),rt=i("code"),ss=o("padding=True"),as=o(", dynamic padding is more efficient."),kt=_(),v(xe.$$.fragment),wt=_(),ve=i("h2"),ze=i("a"),nt=i("span"),v(Ne.$$.fragment),rs=_(),ot=i("span"),ns=o("Train"),bt=_(),v(Ae.$$.fragment),qt=_(),v(Fe.$$.fragment),this.h()},l(e){const h=Us('[data-svelte="svelte-1phssyn"]',document.head);a=p(h,"META",{name:!0,content:!0}),h.forEach(t),c=$(e),r=p(e,"H1",{class:!0});var Be=f(r);u=p(Be,"A",{id:!0,class:!0,href:!0});var lt=f(u);w=p(lt,"SPAN",{});var it=f(w);S(g.$$.fragment,it),it.forEach(t),lt.forEach(t),q=$(Be),A=p(Be,"SPAN",{});var pt=f(A);z=l(pt,"Translation"),pt.forEach(t),Be.forEach(t),j=$(e),S(L.$$.fragment,e),N=$(e),B=p(e,"P",{});var os=f(B);W=l(os,"Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),os.forEach(t),M=$(e),Y=p(e,"P",{});var Re=f(Y);J=l(Re,"This guide will show you how to fine-tune "),Q=p(Re,"A",{href:!0,rel:!0});var ls=f(Q);ue=l(ls,"T5"),ls.forEach(t),V=l(Re," on the English-French subset of the "),C=p(Re,"A",{href:!0,rel:!0});var is=f(C);G=l(is,"OPUS Books"),is.forEach(t),ne=l(Re," dataset to translate English text to French."),Re.forEach(t),ee=$(e),S(X.$$.fragment,e),de=$(e),D=p(e,"H2",{class:!0});var vt=f(D);O=p(vt,"A",{id:!0,class:!0,href:!0});var ps=f(O);K=p(ps,"SPAN",{});var fs=f(K);S(I.$$.fragment,fs),fs.forEach(t),ps.forEach(t),ge=$(vt),H=p(vt,"SPAN",{});var ms=f(H);R=l(ms,"Load OPUS Books dataset"),ms.forEach(t),vt.forEach(t),he=$(e),d=p(e,"P",{});var hs=f(d);F=l(hs,"Load the OPUS Books dataset from the \u{1F917} Datasets library:"),hs.forEach(t),Z=$(e),S(te.$$.fragment,e),U=$(e),se=p(e,"P",{});var cs=f(se);oe=l(cs,"Split this dataset into a train and test set:"),cs.forEach(t),ae=$(e),S(le.$$.fragment,e),ce=$(e),ie=p(e,"P",{});var us=f(ie);ke=l(us,"Then take a look at an example:"),us.forEach(t),je=$(e),S(n.$$.fragment,e),b=$(e),pe=p(e,"P",{});var St=f(pe);We=l(St,"The "),Se=p(St,"CODE",{});var ds=f(Se);Ye=l(ds,"translation"),ds.forEach(t),ye=l(St," field is a dictionary containing the English and French translations of the text."),St.forEach(t),Le=$(e),fe=p(e,"H2",{class:!0});var yt=f(fe);we=p(yt,"A",{id:!0,class:!0,href:!0});var _s=f(we);Ee=p(_s,"SPAN",{});var $s=f(Ee);S(_e.$$.fragment,$s),$s.forEach(t),_s.forEach(t),He=$(yt),Te=p(yt,"SPAN",{});var gs=f(Te);Ft=l(gs,"Preprocess"),gs.forEach(t),yt.forEach(t),ft=$(e),S(De.$$.fragment,e),mt=$(e),Ze=p(e,"P",{});var ks=f(Ze);Pt=l(ks,"Load the T5 tokenizer to process the language pairs:"),ks.forEach(t),ht=$(e),S(Me.$$.fragment,e),ct=$(e),Je=p(e,"P",{});var ws=f(Je);Lt=l(ws,"The preprocessing function needs to:"),ws.forEach(t),ut=$(e),be=p(e,"OL",{});var Ge=f(be);Xe=p(Ge,"LI",{});var bs=f(Xe);Dt=l(bs,"Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),bs.forEach(t),Mt=$(Ge),Qe=p(Ge,"LI",{});var qs=f(Qe);Ct=l(qs,"Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),qs.forEach(t),Ot=$(Ge),Ce=p(Ge,"LI",{});var Et=f(Ce);It=l(Et,"Truncate sequences to be no longer than the maximum length set by the "),Ve=p(Et,"CODE",{});var js=f(Ve);Ut=l(js,"max_length"),js.forEach(t),Nt=l(Et," parameter."),Et.forEach(t),Ge.forEach(t),dt=$(e),S(Oe.$$.fragment,e),_t=$(e),$e=p(e,"P",{});var Pe=f($e);Bt=l(Pe,"Use \u{1F917} Datasets "),Ie=p(Pe,"A",{href:!0,rel:!0});var vs=f(Ie);Wt=l(vs,"map"),vs.forEach(t),Yt=l(Pe," function to apply the preprocessing function over the entire dataset. You can speed up the "),et=p(Pe,"CODE",{});var Ss=f(et);Ht=l(Ss,"map"),Ss.forEach(t),Zt=l(Pe," function by setting "),tt=p(Pe,"CODE",{});var ys=f(tt);Jt=l(ys,"batched=True"),ys.forEach(t),Kt=l(Pe," to process multiple elements of the dataset at once:"),Pe.forEach(t),$t=$(e),S(Ue.$$.fragment,e),gt=$(e),me=p(e,"P",{});var qe=f(me);Rt=l(qe,"Use "),Ke=p(qe,"A",{href:!0});var Es=f(Ke);Gt=l(Es,"DataCollatorForSeq2Seq"),Es.forEach(t),Xt=l(qe," to create a batch of examples. It will also "),st=p(qe,"EM",{});var Ts=f(st);Qt=l(Ts,"dynamically pad"),Ts.forEach(t),Vt=l(qe," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),at=p(qe,"CODE",{});var xs=f(at);es=l(xs,"tokenizer"),xs.forEach(t),ts=l(qe," function by setting "),rt=p(qe,"CODE",{});var zs=f(rt);ss=l(zs,"padding=True"),zs.forEach(t),as=l(qe,", dynamic padding is more efficient."),qe.forEach(t),kt=$(e),S(xe.$$.fragment,e),wt=$(e),ve=p(e,"H2",{class:!0});var Tt=f(ve);ze=p(Tt,"A",{id:!0,class:!0,href:!0});var As=f(ze);nt=p(As,"SPAN",{});var Fs=f(nt);S(Ne.$$.fragment,Fs),Fs.forEach(t),As.forEach(t),rs=$(Tt),ot=p(Tt,"SPAN",{});var Ps=f(ot);ns=l(Ps,"Train"),Ps.forEach(t),Tt.forEach(t),bt=$(e),S(Ae.$$.fragment,e),qt=$(e),S(Fe.$$.fragment,e),this.h()},h(){k(a,"name","hf:doc:metadata"),k(a,"content",JSON.stringify(ta)),k(u,"id","translation"),k(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(u,"href","#translation"),k(r,"class","relative group"),k(Q,"href","https://huggingface.co/t5-small"),k(Q,"rel","nofollow"),k(C,"href","https://huggingface.co/datasets/opus_books"),k(C,"rel","nofollow"),k(O,"id","load-opus-books-dataset"),k(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(O,"href","#load-opus-books-dataset"),k(D,"class","relative group"),k(we,"id","preprocess"),k(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(we,"href","#preprocess"),k(fe,"class","relative group"),k(Ie,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),k(Ie,"rel","nofollow"),k(Ke,"href","/docs/transformers/pr_17967/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq"),k(ze,"id","train"),k(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(ze,"href","#train"),k(ve,"class","relative group")},m(e,h){s(document.head,a),m(e,c,h),m(e,r,h),s(r,u),s(u,w),y(g,w,null),s(r,q),s(r,A),s(A,z),m(e,j,h),y(L,e,h),m(e,N,h),m(e,B,h),s(B,W),m(e,M,h),m(e,Y,h),s(Y,J),s(Y,Q),s(Q,ue),s(Y,V),s(Y,C),s(C,G),s(Y,ne),m(e,ee,h),y(X,e,h),m(e,de,h),m(e,D,h),s(D,O),s(O,K),y(I,K,null),s(D,ge),s(D,H),s(H,R),m(e,he,h),m(e,d,h),s(d,F),m(e,Z,h),y(te,e,h),m(e,U,h),m(e,se,h),s(se,oe),m(e,ae,h),y(le,e,h),m(e,ce,h),m(e,ie,h),s(ie,ke),m(e,je,h),y(n,e,h),m(e,b,h),m(e,pe,h),s(pe,We),s(pe,Se),s(Se,Ye),s(pe,ye),m(e,Le,h),m(e,fe,h),s(fe,we),s(we,Ee),y(_e,Ee,null),s(fe,He),s(fe,Te),s(Te,Ft),m(e,ft,h),y(De,e,h),m(e,mt,h),m(e,Ze,h),s(Ze,Pt),m(e,ht,h),y(Me,e,h),m(e,ct,h),m(e,Je,h),s(Je,Lt),m(e,ut,h),m(e,be,h),s(be,Xe),s(Xe,Dt),s(be,Mt),s(be,Qe),s(Qe,Ct),s(be,Ot),s(be,Ce),s(Ce,It),s(Ce,Ve),s(Ve,Ut),s(Ce,Nt),m(e,dt,h),y(Oe,e,h),m(e,_t,h),m(e,$e,h),s($e,Bt),s($e,Ie),s(Ie,Wt),s($e,Yt),s($e,et),s(et,Ht),s($e,Zt),s($e,tt),s(tt,Jt),s($e,Kt),m(e,$t,h),y(Ue,e,h),m(e,gt,h),m(e,me,h),s(me,Rt),s(me,Ke),s(Ke,Gt),s(me,Xt),s(me,st),s(st,Qt),s(me,Vt),s(me,at),s(at,es),s(me,ts),s(me,rt),s(rt,ss),s(me,as),m(e,kt,h),y(xe,e,h),m(e,wt,h),m(e,ve,h),s(ve,ze),s(ze,nt),y(Ne,nt,null),s(ve,rs),s(ve,ot),s(ot,ns),m(e,bt,h),y(Ae,e,h),m(e,qt,h),y(Fe,e,h),jt=!0},p(e,[h]){const Be={};h&2&&(Be.$$scope={dirty:h,ctx:e}),X.$set(Be);const lt={};h&2&&(lt.$$scope={dirty:h,ctx:e}),xe.$set(lt);const it={};h&2&&(it.$$scope={dirty:h,ctx:e}),Ae.$set(it);const pt={};h&2&&(pt.$$scope={dirty:h,ctx:e}),Fe.$set(pt)},i(e){jt||(E(g.$$.fragment,e),E(L.$$.fragment,e),E(X.$$.fragment,e),E(I.$$.fragment,e),E(te.$$.fragment,e),E(le.$$.fragment,e),E(n.$$.fragment,e),E(_e.$$.fragment,e),E(De.$$.fragment,e),E(Me.$$.fragment,e),E(Oe.$$.fragment,e),E(Ue.$$.fragment,e),E(xe.$$.fragment,e),E(Ne.$$.fragment,e),E(Ae.$$.fragment,e),E(Fe.$$.fragment,e),jt=!0)},o(e){T(g.$$.fragment,e),T(L.$$.fragment,e),T(X.$$.fragment,e),T(I.$$.fragment,e),T(te.$$.fragment,e),T(le.$$.fragment,e),T(n.$$.fragment,e),T(_e.$$.fragment,e),T(De.$$.fragment,e),T(Me.$$.fragment,e),T(Oe.$$.fragment,e),T(Ue.$$.fragment,e),T(xe.$$.fragment,e),T(Ne.$$.fragment,e),T(Ae.$$.fragment,e),T(Fe.$$.fragment,e),jt=!1},d(e){t(a),e&&t(c),e&&t(r),x(g),e&&t(j),x(L,e),e&&t(N),e&&t(B),e&&t(M),e&&t(Y),e&&t(ee),x(X,e),e&&t(de),e&&t(D),x(I),e&&t(he),e&&t(d),e&&t(Z),x(te,e),e&&t(U),e&&t(se),e&&t(ae),x(le,e),e&&t(ce),e&&t(ie),e&&t(je),x(n,e),e&&t(b),e&&t(pe),e&&t(Le),e&&t(fe),x(_e),e&&t(ft),x(De,e),e&&t(mt),e&&t(Ze),e&&t(ht),x(Me,e),e&&t(ct),e&&t(Je),e&&t(ut),e&&t(be),e&&t(dt),x(Oe,e),e&&t(_t),e&&t($e),e&&t($t),x(Ue,e),e&&t(gt),e&&t(me),e&&t(kt),x(xe,e),e&&t(wt),e&&t(ve),x(Ne),e&&t(bt),x(Ae,e),e&&t(qt),x(Fe,e)}}}const ta={local:"translation",sections:[{local:"load-opus-books-dataset",title:"Load OPUS Books dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Translation"};function sa(P){return Ns(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pa extends Cs{constructor(a){super();Os(this,a,sa,ea,Is,{})}}export{pa as default,ta as metadata};
