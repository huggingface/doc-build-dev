import{S as xp,i as Ap,s as Ip,e as i,k as d,w as q,t as r,M as Cp,c as p,d as a,m as f,x as w,a as m,h as n,b as E,N as Lt,G as s,g as u,y,o as v,p as me,q as $,B as P,v as Mp,n as ue}from"../../chunks/vendor-hf-doc-builder.js";import{T as Pp}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Tp}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ba}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as I}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as zp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Np}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Sp(k){let o,c;return o=new zp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"}]}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Op(k){let o,c;return o=new zp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"}]}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Fp(k){let o;return{c(){o=r("Esta \xE9 a primeira se\xE7\xE3o onde o conte\xFAdo \xE9 ligeiramente diferente, dependendo se voc\xEA usa PyTorch e TensorFlow. Para selecionar a plataforma que voc\xEA prefere, basta alterar no bot\xE3o no topo.")},l(c){o=n(c,"Esta \xE9 a primeira se\xE7\xE3o onde o conte\xFAdo \xE9 ligeiramente diferente, dependendo se voc\xEA usa PyTorch e TensorFlow. Para selecionar a plataforma que voc\xEA prefere, basta alterar no bot\xE3o no topo.")},m(c,t){u(c,o,t)},d(c){c&&a(o)}}}function Dp(k){let o,c;return o=new Tp({props:{id:"wVN12smEvqg"}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Lp(k){let o,c;return o=new Tp({props:{id:"1pedAIvTWXk"}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Vp(k){let o,c;return o=new I({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Gp(k){let o,c;return o=new I({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Hp(k){let o,c,t,_,b;return _=new I({props:{code:`{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;, 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;
}`}}),{c(){o=i("p"),c=r("Eis como s\xE3o os resultados como tensores TensorFlow:"),t=d(),q(_.$$.fragment)},l(h){o=p(h,"P",{});var j=m(o);c=n(j,"Eis como s\xE3o os resultados como tensores TensorFlow:"),j.forEach(a),t=f(h),w(_.$$.fragment,h)},m(h,j){u(h,o,j),s(o,c),u(h,t,j),y(_,h,j),b=!0},i(h){b||($(_.$$.fragment,h),b=!0)},o(h){v(_.$$.fragment,h),b=!1},d(h){h&&a(o),h&&a(t),P(_,h)}}}function Up(k){let o,c,t,_,b;return _=new I({props:{code:`{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
    ]), 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
    ])
}`}}),{c(){o=i("p"),c=r("Eis como s\xE3o os resultados como tensores PyTorch:"),t=d(),q(_.$$.fragment)},l(h){o=p(h,"P",{});var j=m(o);c=n(j,"Eis como s\xE3o os resultados como tensores PyTorch:"),j.forEach(a),t=f(h),w(_.$$.fragment,h)},m(h,j){u(h,o,j),s(o,c),u(h,t,j),y(_,h,j),b=!0},i(h){b||($(_.$$.fragment,h),b=!0)},o(h){v(_.$$.fragment,h),b=!1},d(h){h&&a(o),h&&a(t),P(_,h)}}}function Rp(k){let o,c,t,_,b;return _=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModel.from_pretrained(checkpoint)`}}),{c(){o=i("p"),c=r("Podemos baixar nosso modelo pr\xE9-treinado da mesma forma que fizemos com nosso tokenizer. \u{1F917} Transformers fornece uma classe \u201CTFAutoModel\u201D que tamb\xE9m tem um m\xE9todo \u201Cfrom_pretrained\u201D:"),t=d(),q(_.$$.fragment)},l(h){o=p(h,"P",{});var j=m(o);c=n(j,"Podemos baixar nosso modelo pr\xE9-treinado da mesma forma que fizemos com nosso tokenizer. \u{1F917} Transformers fornece uma classe \u201CTFAutoModel\u201D que tamb\xE9m tem um m\xE9todo \u201Cfrom_pretrained\u201D:"),j.forEach(a),t=f(h),w(_.$$.fragment,h)},m(h,j){u(h,o,j),s(o,c),u(h,t,j),y(_,h,j),b=!0},i(h){b||($(_.$$.fragment,h),b=!0)},o(h){v(_.$$.fragment,h),b=!1},d(h){h&&a(o),h&&a(t),P(_,h)}}}function Bp(k){let o,c,t,_,b,h,j,N,C,A,F;return A=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModel.from_pretrained(checkpoint)`}}),{c(){o=i("p"),c=r("Podemos baixar nosso modelo pr\xE9-treinado da mesma forma que fizemos com nosso tokenizer. \u{1F917} Transformers fornece uma classe "),t=i("code"),_=r("AutoModel"),b=r(" que tamb\xE9m tem um m\xE9todo "),h=i("code"),j=r("from_pretrained()"),N=r(":"),C=d(),q(A.$$.fragment)},l(x){o=p(x,"P",{});var T=m(o);c=n(T,"Podemos baixar nosso modelo pr\xE9-treinado da mesma forma que fizemos com nosso tokenizer. \u{1F917} Transformers fornece uma classe "),t=p(T,"CODE",{});var M=m(t);_=n(M,"AutoModel"),M.forEach(a),b=n(T," que tamb\xE9m tem um m\xE9todo "),h=p(T,"CODE",{});var g=m(h);j=n(g,"from_pretrained()"),g.forEach(a),N=n(T,":"),T.forEach(a),C=f(x),w(A.$$.fragment,x)},m(x,T){u(x,o,T),s(o,c),s(o,t),s(t,_),s(o,b),s(o,h),s(h,j),s(o,N),u(x,C,T),y(A,x,T),F=!0},i(x){F||($(A.$$.fragment,x),F=!0)},o(x){v(A.$$.fragment,x),F=!1},d(x){x&&a(o),x&&a(C),P(A,x)}}}function Qp(k){let o,c,t,_;return o=new I({props:{code:`outputs = model(inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),t=new I({props:{code:"(2, 16, 768)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>)'}}),{c(){q(o.$$.fragment),c=d(),q(t.$$.fragment)},l(b){w(o.$$.fragment,b),c=f(b),w(t.$$.fragment,b)},m(b,h){y(o,b,h),u(b,c,h),y(t,b,h),_=!0},i(b){_||($(o.$$.fragment,b),$(t.$$.fragment,b),_=!0)},o(b){v(o.$$.fragment,b),v(t.$$.fragment,b),_=!1},d(b){P(o,b),b&&a(c),P(t,b)}}}function Yp(k){let o,c,t,_;return o=new I({props:{code:`outputs = model(**inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(**inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),t=new I({props:{code:"torch.Size([2, 16, 768])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>])'}}),{c(){q(o.$$.fragment),c=d(),q(t.$$.fragment)},l(b){w(o.$$.fragment,b),c=f(b),w(t.$$.fragment,b)},m(b,h){y(o,b,h),u(b,c,h),y(t,b,h),_=!0},i(b){_||($(o.$$.fragment,b),$(t.$$.fragment,b),_=!0)},o(b){v(o.$$.fragment,b),v(t.$$.fragment,b),_=!1},d(b){P(o,b),b&&a(c),P(t,b)}}}function Jp(k){let o,c,t,_,b,h,j,N,C,A,F,x,T,M;return T=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`}}),{c(){o=i("p"),c=r("Para nosso exemplo, precisaremos de um modelo com uma "),t=i("em"),_=r("head"),b=r(" de classifica\xE7\xE3o em sequencia (para poder classificar as senten\xE7as como positivas ou negativas). Portanto, n\xE3o utilizaremos a classe "),h=i("code"),j=r("TFAutoModel"),N=r(", mas sim, a classe "),C=i("code"),A=r("TFAutoModelForSequenceClassification"),F=r(":"),x=d(),q(T.$$.fragment)},l(g){o=p(g,"P",{});var z=m(o);c=n(z,"Para nosso exemplo, precisaremos de um modelo com uma "),t=p(z,"EM",{});var ce=m(t);_=n(ce,"head"),ce.forEach(a),b=n(z," de classifica\xE7\xE3o em sequencia (para poder classificar as senten\xE7as como positivas ou negativas). Portanto, n\xE3o utilizaremos a classe "),h=p(z,"CODE",{});var L=m(h);j=n(L,"TFAutoModel"),L.forEach(a),N=n(z,", mas sim, a classe "),C=p(z,"CODE",{});var V=m(C);A=n(V,"TFAutoModelForSequenceClassification"),V.forEach(a),F=n(z,":"),z.forEach(a),x=f(g),w(T.$$.fragment,g)},m(g,z){u(g,o,z),s(o,c),s(o,t),s(t,_),s(o,b),s(o,h),s(h,j),s(o,N),s(o,C),s(C,A),s(o,F),u(g,x,z),y(T,g,z),M=!0},i(g){M||($(T.$$.fragment,g),M=!0)},o(g){v(T.$$.fragment,g),M=!1},d(g){g&&a(o),g&&a(x),P(T,g)}}}function Wp(k){let o,c,t,_,b,h,j,N,C,A,F,x,T,M;return T=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`}}),{c(){o=i("p"),c=r("Para nosso exemplo, precisaremos de um modelo com uma "),t=i("em"),_=r("head"),b=r(" de classifica\xE7\xE3o em sequencia (para poder classificar as senten\xE7as como positivas ou negativas). Portanto, n\xE3o utilizaremos a classe "),h=i("code"),j=r("AutoModel"),N=r(", mas sim, a classe "),C=i("code"),A=r("AutoModelForSequenceClassification"),F=r(":"),x=d(),q(T.$$.fragment)},l(g){o=p(g,"P",{});var z=m(o);c=n(z,"Para nosso exemplo, precisaremos de um modelo com uma "),t=p(z,"EM",{});var ce=m(t);_=n(ce,"head"),ce.forEach(a),b=n(z," de classifica\xE7\xE3o em sequencia (para poder classificar as senten\xE7as como positivas ou negativas). Portanto, n\xE3o utilizaremos a classe "),h=p(z,"CODE",{});var L=m(h);j=n(L,"AutoModel"),L.forEach(a),N=n(z,", mas sim, a classe "),C=p(z,"CODE",{});var V=m(C);A=n(V,"AutoModelForSequenceClassification"),V.forEach(a),F=n(z,":"),z.forEach(a),x=f(g),w(T.$$.fragment,g)},m(g,z){u(g,o,z),s(o,c),s(o,t),s(t,_),s(o,b),s(o,h),s(h,j),s(o,N),s(o,C),s(C,A),s(o,F),u(g,x,z),y(T,g,z),M=!0},i(g){M||($(T.$$.fragment,g),M=!0)},o(g){v(T.$$.fragment,g),M=!1},d(g){g&&a(o),g&&a(x),P(T,g)}}}function Xp(k){let o,c;return o=new I({props:{code:"(2, 2)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)'}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Kp(k){let o,c;return o=new I({props:{code:"torch.Size([2, 2])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function Zp(k){let o,c;return o=new I({props:{code:`<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
    array([[-<span class="hljs-number">1.5606991</span>,  <span class="hljs-number">1.6122842</span>],
           [ <span class="hljs-number">4.169231</span> , -<span class="hljs-number">3.3464472</span>]], dtype=float32)&gt;`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function em(k){let o,c;return o=new I({props:{code:`tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[-<span class="hljs-number">1.5607</span>,  <span class="hljs-number">1.6123</span>],
        [ <span class="hljs-number">4.1692</span>, -<span class="hljs-number">3.3464</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function sm(k){let o,c;return o=new I({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

predictions = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function am(k){let o,c;return o=new I({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function om(k){let o,c;return o=new I({props:{code:`tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[<span class="hljs-number">4.01951671e-02</span> <span class="hljs-number">9.59804833e-01</span>]
 [<span class="hljs-number">9.9945587e-01</span> <span class="hljs-number">5.4418424e-04</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function tm(k){let o,c;return o=new I({props:{code:`tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)`,highlighted:`tensor([[<span class="hljs-number">4.0195e-02</span>, <span class="hljs-number">9.5980e-01</span>],
        [<span class="hljs-number">9.9946e-01</span>, <span class="hljs-number">5.4418e-04</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`}}),{c(){q(o.$$.fragment)},l(t){w(o.$$.fragment,t)},m(t,_){y(o,t,_),c=!0},i(t){c||($(o.$$.fragment,t),c=!0)},o(t){v(o.$$.fragment,t),c=!1},d(t){P(o,t)}}}function rm(k){let o,c,t,_,b,h,j,N;return{c(){o=i("p"),c=r("\u270F\uFE0F "),t=i("strong"),_=r("Experimente!"),b=r(" Escolha duas (ou mais) textos pr\xF3prios e passe-os atrav\xE9s do pipeline "),h=i("code"),j=r("sentiment-analysis"),N=r(". Em seguida, replique as etapas que voc\xEA mesmo viu aqui e verifique se voc\xEA obt\xE9m os mesmos resultados!")},l(C){o=p(C,"P",{});var A=m(o);c=n(A,"\u270F\uFE0F "),t=p(A,"STRONG",{});var F=m(t);_=n(F,"Experimente!"),F.forEach(a),b=n(A," Escolha duas (ou mais) textos pr\xF3prios e passe-os atrav\xE9s do pipeline "),h=p(A,"CODE",{});var x=m(h);j=n(x,"sentiment-analysis"),x.forEach(a),N=n(A,". Em seguida, replique as etapas que voc\xEA mesmo viu aqui e verifique se voc\xEA obt\xE9m os mesmos resultados!"),A.forEach(a)},m(C,A){u(C,o,A),s(o,c),s(o,t),s(t,_),s(o,b),s(o,h),s(h,j),s(o,N)},d(C){C&&a(o)}}}function nm(k){let o,c,t,_,b,h,j,N,C,A,F,x,T,M,g,z,ce,L,V,Ts,we,Vt,zs,Gt,Ht,Po,We,To,xs,Ut,zo,Xe,xo,ye,Rt,As,Bt,Qt,Ao,ve,Ke,xl,Yt,Ze,Al,Io,Is,Jt,Co,$e,Pe,va,es,Wt,$a,Xt,Mo,Te,Kt,ka,Zt,er,No,de,ss,sr,Ea,ar,or,tr,ja,rr,nr,ga,lr,So,H,ir,as,pr,mr,qa,ur,cr,wa,dr,fr,Oo,U,hr,ya,_r,br,Pa,vr,$r,os,kr,Er,Fo,ts,Do,Cs,jr,Lo,ze,gr,Ta,qr,wr,Vo,xe,yr,za,Pr,Tr,Go,B,Q,Ms,Ns,zr,Ho,Y,J,Ss,Ae,xr,xa,Ar,Ir,Uo,ke,Ie,Aa,rs,Cr,Ia,Mr,Ro,W,X,Os,Fs,Nr,Bo,R,Sr,Ca,Or,Fr,Ma,Dr,Lr,Na,Vr,Gr,Qo,Ds,Hr,Yo,fe,Ur,Sa,Rr,Br,Ls,Qr,Yr,Jo,Ee,Ce,Oa,ns,Jr,Fa,Wr,Wo,Vs,Xr,Xo,he,Gs,Da,Kr,Zr,en,Hs,La,sn,an,on,Us,Va,tn,rn,Ko,Rs,nn,Zo,Bs,ln,et,K,Z,Qs,_e,pn,Ga,mn,un,Ha,cn,dn,st,je,Me,Ua,ls,fn,Ra,hn,at,Ne,_n,Ba,bn,vn,ot,ge,is,Il,$n,ps,Cl,tt,Se,kn,Qa,En,jn,rt,Ys,gn,nt,Oe,qn,Ya,wn,yn,lt,O,Js,Ja,Pn,Tn,zn,Wa,Xa,xn,An,Ka,Za,In,Cn,eo,so,Mn,Nn,ao,oo,Sn,On,to,ro,Fn,Dn,no,lo,Ln,Vn,io,Gn,it,ee,se,Ws,Fe,Hn,po,Un,Rn,pt,ms,mt,ae,oe,Xs,Ks,Bn,ut,qe,De,mo,us,Qn,uo,Yn,ct,Zs,Jn,dt,cs,ft,te,re,ea,S,Wn,co,Xn,Kn,fo,Zn,el,ho,sl,al,ds,ol,tl,_o,rl,nl,bo,ll,il,vo,pl,ml,$o,ul,cl,ht,ne,le,sa,ie,pe,aa,be,dl,ko,fl,hl,Eo,_l,bl,_t,Le,vl,jo,$l,kl,bt,fs,vt,hs,$t,oa,El,kt,Ve,go,jl,gl,qo,ql,Et,ta,wl,jt,Ge,gt;t=new Np({props:{fw:k[0]}}),N=new ba({});const Ml=[Op,Sp],_s=[];function Nl(e,l){return e[0]==="pt"?0:1}T=Nl(k),M=_s[T]=Ml[T](k),z=new Pp({props:{$$slots:{default:[Fp]},$$scope:{ctx:k}}});const Sl=[Lp,Dp],bs=[];function Ol(e,l){return e[0]==="pt"?0:1}L=Ol(k),V=bs[L]=Sl[L](k),We=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)
classifier(
    [
        <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
        <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
    ]
)`}}),Xe=new I({props:{code:`[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]`,highlighted:`[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>},
 {<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994558095932007</span>}]`}}),es=new ba({}),ts=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`}});const Fl=[Gp,Vp],vs=[];function Dl(e,l){return e[0]==="pt"?0:1}B=Dl(k),Q=vs[B]=Fl[B](k);const Ll=[Up,Hp],$s=[];function Vl(e,l){return e[0]==="pt"?0:1}Y=Vl(k),J=$s[Y]=Ll[Y](k),rs=new ba({});const Gl=[Bp,Rp],ks=[];function Hl(e,l){return e[0]==="pt"?0:1}W=Hl(k),X=ks[W]=Gl[W](k),ns=new ba({});const Ul=[Yp,Qp],Es=[];function Rl(e,l){return e[0]==="pt"?0:1}K=Rl(k),Z=Es[K]=Ul[K](k),ls=new ba({});const Bl=[Wp,Jp],js=[];function Ql(e,l){return e[0]==="pt"?0:1}ee=Ql(k),se=js[ee]=Bl[ee](k),ms=new I({props:{code:"print(outputs.logits.shape)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits.shape)'}});const Yl=[Kp,Xp],gs=[];function Jl(e,l){return e[0]==="pt"?0:1}ae=Jl(k),oe=gs[ae]=Yl[ae](k),us=new ba({}),cs=new I({props:{code:"print(outputs.logits)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits)'}});const Wl=[em,Zp],qs=[];function Xl(e,l){return e[0]==="pt"?0:1}te=Xl(k),re=qs[te]=Wl[te](k);const Kl=[am,sm],ws=[];function Zl(e,l){return e[0]==="pt"?0:1}ne=Zl(k),le=ws[ne]=Kl[ne](k);const ei=[tm,om],ys=[];function si(e,l){return e[0]==="pt"?0:1}return ie=si(k),pe=ys[ie]=ei[ie](k),fs=new I({props:{code:"model.config.id2label",highlighted:"model.config.id2label"}}),hs=new I({props:{code:"{0: 'NEGATIVE', 1: 'POSITIVE'}",highlighted:'{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>}'}}),Ge=new Pp({props:{$$slots:{default:[rm]},$$scope:{ctx:k}}}),{c(){o=i("meta"),c=d(),q(t.$$.fragment),_=d(),b=i("h1"),h=i("a"),j=i("span"),q(N.$$.fragment),C=d(),A=i("span"),F=r("Por dentro da fun\xE7\xE3o pipeline"),x=d(),M.c(),g=d(),q(z.$$.fragment),ce=d(),V.c(),Ts=d(),we=i("p"),Vt=r("Vamos come\xE7ar com um exemplo completo, dando uma olhada no que acontece dentro da fun\xE7\xE3o quando executamos o seguinte c\xF3digo no "),zs=i("a"),Gt=r("Cap\xEDtulo 1"),Ht=r(":"),Po=d(),q(We.$$.fragment),To=d(),xs=i("p"),Ut=r("tendo o resultado:"),zo=d(),q(Xe.$$.fragment),xo=d(),ye=i("p"),Rt=r("Como visto no "),As=i("a"),Bt=r("Cap\xEDtulo 1"),Qt=r(", este pipeline agrupa os tr\xEAs passos: o pr\xE9-processamento, passagem das entradas atrav\xE9s do modelo, e o p\xF3s-processamento:"),Ao=d(),ve=i("div"),Ke=i("img"),Yt=d(),Ze=i("img"),Io=d(),Is=i("p"),Jt=r("Vamos rever rapidamente cada um deles."),Co=d(),$e=i("h2"),Pe=i("a"),va=i("span"),q(es.$$.fragment),Wt=d(),$a=i("span"),Xt=r("Pr\xE9-processamento com o tokenizer"),Mo=d(),Te=i("p"),Kt=r("Como outras redes neurais, os Transformers n\xE3o podem processar texto bruto diretamente, portanto, o primeiro passo do nosso pipeline \xE9 converter as entradas de texto em n\xFAmeros que o modelo possa fazer sentido. Para fazer isso, usamos um "),ka=i("em"),Zt=r("tokenizer"),er=r(", que ser\xE1 respons\xE1vel por:"),No=d(),de=i("ul"),ss=i("li"),sr=r("Dividir a entrada em palavras, sub-palavras ou s\xEDmbolos (como pontua\xE7\xE3o) que s\xE3o chamados de "),Ea=i("em"),ar=r("tokens"),or=r("."),tr=d(),ja=i("li"),rr=r("Mapeando cada ficha para um n\xFAmero inteiro"),nr=d(),ga=i("li"),lr=r("Adicionando entradas adicionais que podem ser \xFAteis ao modelo"),So=d(),H=i("p"),ir=r("Todo esse pr\xE9-processamento precisa ser feito exatamente da mesma forma que quando o modelo foi pr\xE9-treinado, ent\xE3o precisamos primeiro baixar essas informa\xE7\xF5es do "),as=i("a"),pr=r("Model Hub"),mr=r(". Para isso, utilizamos a classe "),qa=i("code"),ur=r("AutoTokenizer"),cr=r(" e seu m\xE9todo "),wa=i("code"),dr=r("from_pretrained()"),fr=r(". Utilizando o nome do ponto de verifica\xE7\xE3o de nosso modelo, ele ir\xE1 buscar automaticamente os dados associados ao tokenizer do modelo e armazen\xE1-los em cache (portanto, ele s\xF3 \xE9 baixado na primeira vez que voc\xEA executar o c\xF3digo abaixo)."),Oo=d(),U=i("p"),hr=r("Desde que o checkpoint default do pipeline "),ya=i("code"),_r=r("sentiment-analysis"),br=r(" \xE9 "),Pa=i("code"),vr=r("distilbert-base-uncased-finetuned-sst-2-english"),$r=r(" (voc\xEA pode ver o card do modelo "),os=i("a"),kr=r("aqui"),Er=r("), ent\xE3o executamos o seguinte:"),Fo=d(),q(ts.$$.fragment),Do=d(),Cs=i("p"),jr=r("Assim que tivermos o tokenizer, podemos passar diretamente nossas frases para ele e receberemos de volta um dicion\xE1rio que est\xE1 pronto para alimentar nosso modelo! A \xFAnica coisa que falta fazer \xE9 converter a lista de identifica\xE7\xF5es de entrada em tensores."),Lo=d(),ze=i("p"),gr=r("Voc\xEA pode usar \u{1F917} Transformers sem ter que se preocupar com qual estrutura ML \xE9 usada como backend; pode ser PyTorch ou TensorFlow, ou Flax para alguns modelos. Entretanto, os Transformers s\xF3 aceitam "),Ta=i("em"),qr=r("tensores"),wr=r(" como entrada. Se esta \xE9 a primeira vez que voc\xEA ouve falar de tensores, voc\xEA pode pensar neles como matrizes da NumPy. Uma matriz NumPy pode ser um escalar (0D), um vetor (1D), uma matriz (2D), ou ter mais dimens\xF5es. \xC9 efetivamente um tensor; os tensores de outras estruturas ML comportam-se de forma semelhante, e geralmente s\xE3o t\xE3o simples de instanciar como os arrays da NumPy."),Vo=d(),xe=i("p"),yr=r("Para especificar o tipo de tensores que queremos recuperar (PyTorch, TensorFlow ou NumPy), utilizamos o argumento "),za=i("code"),Pr=r("return_tensors"),Tr=r(":"),Go=d(),Q.c(),Ms=d(),Ns=i("p"),zr=r("N\xE3o se preocupe ainda com o truncamento e o padding; explicaremos isso mais tarde. As principais coisas a lembrar aqui s\xE3o que voc\xEA pode passar uma frase ou uma lista de frases, bem como especificar o tipo de tensores que voc\xEA quer recuperar (se nenhum tipo for passado, voc\xEA receber\xE1 uma lista de listas como resultado)."),Ho=d(),J.c(),Ss=d(),Ae=i("p"),xr=r("A sa\xEDda em si \xE9 um dicion\xE1rio contendo duas chaves, "),xa=i("code"),Ar=r("input_ids' e "),Ir=r("attention_mask\u2019. O `input_ids\u2019 cont\xE9m duas linhas de inteiros (uma para cada frase) que s\xE3o os identificadores \xFAnicos dos tokens em cada frase. Explicaremos o que \xE9 a \u201Cm\xE1scara de aten\xE7\xE3o\u201D (attention mask) mais adiante neste cap\xEDtulo."),Uo=d(),ke=i("h2"),Ie=i("a"),Aa=i("span"),q(rs.$$.fragment),Cr=d(),Ia=i("span"),Mr=r("Indo adianta pelo modelo"),Ro=d(),X.c(),Os=d(),Fs=i("p"),Nr=r("Neste trecho de c\xF3digo, fizemos o download do mesmo checkpoint que usamos anteriormente em nosso pipeline (j\xE1 deveria estar em cache) e instanciamos um modelo com ele."),Bo=d(),R=i("p"),Sr=r("Esta arquitetura cont\xE9m apenas o m\xF3dulo base do Transformer: dadas algumas entradas, ele produz o que chamaremos de "),Ca=i("em"),Or=r("hidden states"),Fr=r(" (estados ocultos), tamb\xE9m conhecidos como "),Ma=i("em"),Dr=r("features"),Lr=r(" (caracter\xEDsticas). Para cada modelo de entrada, recuperaremos um vetor de alta dimensionalidade representando a "),Na=i("strong"),Vr=r("compreens\xE3o contextual dessa entrada pelo Transformer*"),Gr=r("."),Qo=d(),Ds=i("p"),Hr=r("Se isto n\xE3o faz sentido, n\xE3o se preocupe com isso. Explicaremos tudo isso mais tarde."),Yo=d(),fe=i("p"),Ur=r("Embora estes hidden states possam ser \xFAteis por si mesmos, eles geralmente s\xE3o entradas para outra parte do modelo, conhecida como "),Sa=i("em"),Rr=r("head"),Br=r(" (cabe\xE7a). No "),Ls=i("a"),Qr=r("Cap\xEDtulo 1"),Yr=r(", as diferentes tarefas poderiam ter sido realizadas com a mesma arquitetura, mas cada uma destas tarefas teria uma head diferente associada a ela."),Jo=d(),Ee=i("h3"),Ce=i("a"),Oa=i("span"),q(ns.$$.fragment),Jr=d(),Fa=i("span"),Wr=r("Um vetor de alta dimensionalidade?"),Wo=d(),Vs=i("p"),Xr=r("A sa\xEDda vetorial pelo m\xF3dulo do Transformer \xE9 geralmente grande. Geralmente tem tr\xEAs dimens\xF5es:"),Xo=d(),he=i("ul"),Gs=i("li"),Da=i("strong"),Kr=r("Tamanho do lote"),Zr=r(" (Batch size): O n\xFAmero de sequ\xEAncias processadas de cada vez (2 em nosso exemplo)."),en=d(),Hs=i("li"),La=i("strong"),sn=r("Tamanho da sequencia"),an=r(" (Sequence length): O comprimento da representa\xE7\xE3o num\xE9rica da sequ\xEAncia (16 em nosso exemplo)."),on=d(),Us=i("li"),Va=i("strong"),tn=r("Tamanho oculto"),rn=r(" (Hidden size): A dimens\xE3o vetorial de cada modelo de entrada."),Ko=d(),Rs=i("p"),nn=r("Diz-se que \xE9 \u201Cde alta dimensionalidade\u201D por causa do \xFAltimo valor. O tamanho oculto pode ser muito grande (768 \xE9 comum para modelos menores, e em modelos maiores isto pode chegar a 3072 ou mais)."),Zo=d(),Bs=i("p"),ln=r("Podemos ver isso se alimentarmos os inputs que pr\xE9-processamos para nosso modelo:"),et=d(),Z.c(),Qs=d(),_e=i("p"),pn=r("Observe que as sa\xEDdas dos \u{1F917} Transformer se comportam como \u2018tuplas nomeadas\u2019 (namedtuple) ou dicion\xE1rios. Voc\xEA pode acessar os elementos por atributos (como fizemos) ou por chave ("),Ga=i("code"),mn=r('outputs["last_hidden_state"]'),un=r("), ou mesmo por \xEDndice se voc\xEA souber exatamente onde o que est\xE1 procurando ("),Ha=i("code"),cn=r("outputs[0]"),dn=r(")."),st=d(),je=i("h3"),Me=i("a"),Ua=i("span"),q(ls.$$.fragment),fn=d(),Ra=i("span"),hn=r("Cabe\xE7a do modelo (model heads): Fazendo sentido a partir dos n\xFAmeros"),at=d(),Ne=i("p"),_n=r("As "),Ba=i("em"),bn=r("heads"),vn=r(" do modelo usam o vetor de alta dimensionalidade dos hidden states como entrada e os projetam em uma dimens\xE3o diferente. Eles s\xE3o geralmente compostos de uma ou algumas camadas lineares:"),ot=d(),ge=i("div"),is=i("img"),$n=d(),ps=i("img"),tt=d(),Se=i("p"),kn=r("A sa\xEDda do Transformer \xE9 enviada diretamente para a "),Qa=i("em"),En=r("head"),jn=r(" do modelo a ser processado."),rt=d(),Ys=i("p"),gn=r("Neste diagrama, o modelo \xE9 representado por sua camada de embeddings (vetores) e pelas camadas subsequentes. A camada de embeddings converte cada ID de entrada na entrada tokenizada em um vetor que representa o token associado. As camadas subsequentes manipulam esses vetores usando o mecanismo de aten\xE7\xE3o para produzir a representa\xE7\xE3o final das senten\xE7as."),nt=d(),Oe=i("p"),qn=r("H\xE1 muitas arquiteturas diferentes dispon\xEDveis no \u{1F917} Transformers, com cada uma projetada em torno de uma tarefa espec\xEDfica. Aqui est\xE1 uma lista por "),Ya=i("strong"),wn=r("algumas"),yn=r(" destas tarefas:"),lt=d(),O=i("ul"),Js=i("li"),Ja=i("code"),Pn=r("*Model"),Tn=r(" (recuperar os hidden states)"),zn=d(),Wa=i("li"),Xa=i("code"),xn=r("*ForCausalLM"),An=d(),Ka=i("li"),Za=i("code"),In=r("*ForMaskedLM"),Cn=d(),eo=i("li"),so=i("code"),Mn=r("*ForMultipleChoice"),Nn=d(),ao=i("li"),oo=i("code"),Sn=r("*ForQuestionAnswering"),On=d(),to=i("li"),ro=i("code"),Fn=r("*ForSequenceClassification"),Dn=d(),no=i("li"),lo=i("code"),Ln=r("*ForTokenClassification"),Vn=d(),io=i("li"),Gn=r("e outros \u{1F917}"),it=d(),se.c(),Ws=d(),Fe=i("p"),Hn=r("Agora se observarmos o tamanho dos nossos inputs, a dimensionalidade ser\xE1 muito menor: a "),po=i("em"),Un=r("head"),Rn=r(" do modelo toma como entrada os vetores de alta dimensionalidade que vimos anteriormente, e os vetores de sa\xEDda contendo dois valores (um por label):"),pt=d(),q(ms.$$.fragment),mt=d(),oe.c(),Xs=d(),Ks=i("p"),Bn=r("Como temos apenas duas senten\xE7as e duas labels, o resultado que obtemos de nosso modelo \xE9 de tamanho 2 x 2."),ut=d(),qe=i("h2"),De=i("a"),mo=i("span"),q(us.$$.fragment),Qn=d(),uo=i("span"),Yn=r("P\xF3s-processamento da sa\xEDda"),ct=d(),Zs=i("p"),Jn=r("Os valores que obtemos como resultado de nosso modelo n\xE3o fazem necessariamente sentido sozinhos. Vamos dar uma olhada:"),dt=d(),q(cs.$$.fragment),ft=d(),re.c(),ea=d(),S=i("p"),Wn=r("Nosso modelo previu "),co=i("code"),Xn=r("[-1.5607, 1.6123]"),Kn=r(" para a primeira frase e "),fo=i("code"),Zn=r("[ 4.1692, -3.3464]"),el=r(" para a segunda. Essas n\xE3o s\xE3o probabilidades, mas "),ho=i("em"),sl=r("logits"),al=r(", a pontua\xE7\xE3o bruta e n\xE3o normalizada produzida pela \xFAltima camada do modelo. Para serem convertidos em probabilidades, eles precisam passar por uma camada "),ds=i("a"),ol=r("SoftMax"),tl=r(" (todas sa\xEDdas dos \u{1F917} Transformers produzem  "),_o=i("em"),rl=r("logits"),nl=r(", j\xE1 que a fun\xE7\xE3o de "),bo=i("em"),ll=r("loss"),il=r(" (perda) para treinamento geralmente fundir\xE1 a \xFAltima fun\xE7\xE3o de ativa\xE7\xE3o, como SoftMax, com a fun\xE7\xE3o de "),vo=i("em"),pl=r("loss"),ml=r(" real, por exemplo a "),$o=i("em"),ul=r("cross entropy"),cl=r("):"),ht=d(),le.c(),sa=d(),pe.c(),aa=d(),be=i("p"),dl=r("Agora podemos ver que o modelo previu "),ko=i("code"),fl=r("[0.0402, 0.9598]"),hl=r(" para a primeira frase e "),Eo=i("code"),_l=r("[0.9995, 0.0005]"),bl=r(" para a segunda. Estas s\xE3o pontua\xE7\xF5es de probabilidade reconhec\xEDveis."),_t=d(),Le=i("p"),vl=r("Para obter as etiquetas correspondentes a cada posi\xE7\xE3o, podemos inspecionar o atributo "),jo=i("code"),$l=r("id2label"),kl=r(" da configura\xE7\xE3o do modelo (mais sobre isso na pr\xF3xima se\xE7\xE3o):"),bt=d(),q(fs.$$.fragment),vt=d(),q(hs.$$.fragment),$t=d(),oa=i("p"),El=r("Agora podemos concluir que o modelo previu o seguinte:"),kt=d(),Ve=i("ul"),go=i("li"),jl=r("A primeira frase: NEGATIVE: 0.0402, POSITIVE: 0.9598"),gl=d(),qo=i("li"),ql=r("Segunda frase: NEGATIVE: 0.9995, POSITIVE: 0.0005"),Et=d(),ta=i("p"),wl=r("Reproduzimos com sucesso as tr\xEAs etapas do pipeline: o pr\xE9-processamento, passagem das entradas atrav\xE9s do modelo, e o p\xF3s-processamento! Agora, vamos levar algum tempo para mergulhar mais fundo em cada uma dessas etapas."),jt=d(),q(Ge.$$.fragment),this.h()},l(e){const l=Cp('[data-svelte="svelte-1phssyn"]',document.head);o=p(l,"META",{name:!0,content:!0}),l.forEach(a),c=f(e),w(t.$$.fragment,e),_=f(e),b=p(e,"H1",{class:!0});var Ps=m(b);h=p(Ps,"A",{id:!0,class:!0,href:!0});var ra=m(h);j=p(ra,"SPAN",{});var wo=m(j);w(N.$$.fragment,wo),wo.forEach(a),ra.forEach(a),C=f(Ps),A=p(Ps,"SPAN",{});var na=m(A);F=n(na,"Por dentro da fun\xE7\xE3o pipeline"),na.forEach(a),Ps.forEach(a),x=f(e),M.l(e),g=f(e),w(z.$$.fragment,e),ce=f(e),V.l(e),Ts=f(e),we=p(e,"P",{});var He=m(we);Vt=n(He,"Vamos come\xE7ar com um exemplo completo, dando uma olhada no que acontece dentro da fun\xE7\xE3o quando executamos o seguinte c\xF3digo no "),zs=p(He,"A",{href:!0});var la=m(zs);Gt=n(la,"Cap\xEDtulo 1"),la.forEach(a),Ht=n(He,":"),He.forEach(a),Po=f(e),w(We.$$.fragment,e),To=f(e),xs=p(e,"P",{});var ia=m(xs);Ut=n(ia,"tendo o resultado:"),ia.forEach(a),zo=f(e),w(Xe.$$.fragment,e),xo=f(e),ye=p(e,"P",{});var Ue=m(ye);Rt=n(Ue,"Como visto no "),As=p(Ue,"A",{href:!0});var pa=m(As);Bt=n(pa,"Cap\xEDtulo 1"),pa.forEach(a),Qt=n(Ue,", este pipeline agrupa os tr\xEAs passos: o pr\xE9-processamento, passagem das entradas atrav\xE9s do modelo, e o p\xF3s-processamento:"),Ue.forEach(a),Ao=f(e),ve=p(e,"DIV",{class:!0});var Re=m(ve);Ke=p(Re,"IMG",{class:!0,src:!0,alt:!0}),Yt=f(Re),Ze=p(Re,"IMG",{class:!0,src:!0,alt:!0}),Re.forEach(a),Io=f(e),Is=p(e,"P",{});var ma=m(Is);Jt=n(ma,"Vamos rever rapidamente cada um deles."),ma.forEach(a),Co=f(e),$e=p(e,"H2",{class:!0});var Be=m($e);Pe=p(Be,"A",{id:!0,class:!0,href:!0});var ua=m(Pe);va=p(ua,"SPAN",{});var yo=m(va);w(es.$$.fragment,yo),yo.forEach(a),ua.forEach(a),Wt=f(Be),$a=p(Be,"SPAN",{});var ai=m($a);Xt=n(ai,"Pr\xE9-processamento com o tokenizer"),ai.forEach(a),Be.forEach(a),Mo=f(e),Te=p(e,"P",{});var qt=m(Te);Kt=n(qt,"Como outras redes neurais, os Transformers n\xE3o podem processar texto bruto diretamente, portanto, o primeiro passo do nosso pipeline \xE9 converter as entradas de texto em n\xFAmeros que o modelo possa fazer sentido. Para fazer isso, usamos um "),ka=p(qt,"EM",{});var oi=m(ka);Zt=n(oi,"tokenizer"),oi.forEach(a),er=n(qt,", que ser\xE1 respons\xE1vel por:"),qt.forEach(a),No=f(e),de=p(e,"UL",{});var ca=m(de);ss=p(ca,"LI",{});var wt=m(ss);sr=n(wt,"Dividir a entrada em palavras, sub-palavras ou s\xEDmbolos (como pontua\xE7\xE3o) que s\xE3o chamados de "),Ea=p(wt,"EM",{});var ti=m(Ea);ar=n(ti,"tokens"),ti.forEach(a),or=n(wt,"."),wt.forEach(a),tr=f(ca),ja=p(ca,"LI",{});var ri=m(ja);rr=n(ri,"Mapeando cada ficha para um n\xFAmero inteiro"),ri.forEach(a),nr=f(ca),ga=p(ca,"LI",{});var ni=m(ga);lr=n(ni,"Adicionando entradas adicionais que podem ser \xFAteis ao modelo"),ni.forEach(a),ca.forEach(a),So=f(e),H=p(e,"P",{});var Qe=m(H);ir=n(Qe,"Todo esse pr\xE9-processamento precisa ser feito exatamente da mesma forma que quando o modelo foi pr\xE9-treinado, ent\xE3o precisamos primeiro baixar essas informa\xE7\xF5es do "),as=p(Qe,"A",{href:!0,rel:!0});var li=m(as);pr=n(li,"Model Hub"),li.forEach(a),mr=n(Qe,". Para isso, utilizamos a classe "),qa=p(Qe,"CODE",{});var ii=m(qa);ur=n(ii,"AutoTokenizer"),ii.forEach(a),cr=n(Qe," e seu m\xE9todo "),wa=p(Qe,"CODE",{});var pi=m(wa);dr=n(pi,"from_pretrained()"),pi.forEach(a),fr=n(Qe,". Utilizando o nome do ponto de verifica\xE7\xE3o de nosso modelo, ele ir\xE1 buscar automaticamente os dados associados ao tokenizer do modelo e armazen\xE1-los em cache (portanto, ele s\xF3 \xE9 baixado na primeira vez que voc\xEA executar o c\xF3digo abaixo)."),Qe.forEach(a),Oo=f(e),U=p(e,"P",{});var Ye=m(U);hr=n(Ye,"Desde que o checkpoint default do pipeline "),ya=p(Ye,"CODE",{});var mi=m(ya);_r=n(mi,"sentiment-analysis"),mi.forEach(a),br=n(Ye," \xE9 "),Pa=p(Ye,"CODE",{});var ui=m(Pa);vr=n(ui,"distilbert-base-uncased-finetuned-sst-2-english"),ui.forEach(a),$r=n(Ye," (voc\xEA pode ver o card do modelo "),os=p(Ye,"A",{href:!0,rel:!0});var ci=m(os);kr=n(ci,"aqui"),ci.forEach(a),Er=n(Ye,"), ent\xE3o executamos o seguinte:"),Ye.forEach(a),Fo=f(e),w(ts.$$.fragment,e),Do=f(e),Cs=p(e,"P",{});var di=m(Cs);jr=n(di,"Assim que tivermos o tokenizer, podemos passar diretamente nossas frases para ele e receberemos de volta um dicion\xE1rio que est\xE1 pronto para alimentar nosso modelo! A \xFAnica coisa que falta fazer \xE9 converter a lista de identifica\xE7\xF5es de entrada em tensores."),di.forEach(a),Lo=f(e),ze=p(e,"P",{});var yt=m(ze);gr=n(yt,"Voc\xEA pode usar \u{1F917} Transformers sem ter que se preocupar com qual estrutura ML \xE9 usada como backend; pode ser PyTorch ou TensorFlow, ou Flax para alguns modelos. Entretanto, os Transformers s\xF3 aceitam "),Ta=p(yt,"EM",{});var fi=m(Ta);qr=n(fi,"tensores"),fi.forEach(a),wr=n(yt," como entrada. Se esta \xE9 a primeira vez que voc\xEA ouve falar de tensores, voc\xEA pode pensar neles como matrizes da NumPy. Uma matriz NumPy pode ser um escalar (0D), um vetor (1D), uma matriz (2D), ou ter mais dimens\xF5es. \xC9 efetivamente um tensor; os tensores de outras estruturas ML comportam-se de forma semelhante, e geralmente s\xE3o t\xE3o simples de instanciar como os arrays da NumPy."),yt.forEach(a),Vo=f(e),xe=p(e,"P",{});var Pt=m(xe);yr=n(Pt,"Para especificar o tipo de tensores que queremos recuperar (PyTorch, TensorFlow ou NumPy), utilizamos o argumento "),za=p(Pt,"CODE",{});var hi=m(za);Pr=n(hi,"return_tensors"),hi.forEach(a),Tr=n(Pt,":"),Pt.forEach(a),Go=f(e),Q.l(e),Ms=f(e),Ns=p(e,"P",{});var _i=m(Ns);zr=n(_i,"N\xE3o se preocupe ainda com o truncamento e o padding; explicaremos isso mais tarde. As principais coisas a lembrar aqui s\xE3o que voc\xEA pode passar uma frase ou uma lista de frases, bem como especificar o tipo de tensores que voc\xEA quer recuperar (se nenhum tipo for passado, voc\xEA receber\xE1 uma lista de listas como resultado)."),_i.forEach(a),Ho=f(e),J.l(e),Ss=f(e),Ae=p(e,"P",{});var Tt=m(Ae);xr=n(Tt,"A sa\xEDda em si \xE9 um dicion\xE1rio contendo duas chaves, "),xa=p(Tt,"CODE",{});var bi=m(xa);Ar=n(bi,"input_ids' e "),bi.forEach(a),Ir=n(Tt,"attention_mask\u2019. O `input_ids\u2019 cont\xE9m duas linhas de inteiros (uma para cada frase) que s\xE3o os identificadores \xFAnicos dos tokens em cada frase. Explicaremos o que \xE9 a \u201Cm\xE1scara de aten\xE7\xE3o\u201D (attention mask) mais adiante neste cap\xEDtulo."),Tt.forEach(a),Uo=f(e),ke=p(e,"H2",{class:!0});var zt=m(ke);Ie=p(zt,"A",{id:!0,class:!0,href:!0});var vi=m(Ie);Aa=p(vi,"SPAN",{});var $i=m(Aa);w(rs.$$.fragment,$i),$i.forEach(a),vi.forEach(a),Cr=f(zt),Ia=p(zt,"SPAN",{});var ki=m(Ia);Mr=n(ki,"Indo adianta pelo modelo"),ki.forEach(a),zt.forEach(a),Ro=f(e),X.l(e),Os=f(e),Fs=p(e,"P",{});var Ei=m(Fs);Nr=n(Ei,"Neste trecho de c\xF3digo, fizemos o download do mesmo checkpoint que usamos anteriormente em nosso pipeline (j\xE1 deveria estar em cache) e instanciamos um modelo com ele."),Ei.forEach(a),Bo=f(e),R=p(e,"P",{});var Je=m(R);Sr=n(Je,"Esta arquitetura cont\xE9m apenas o m\xF3dulo base do Transformer: dadas algumas entradas, ele produz o que chamaremos de "),Ca=p(Je,"EM",{});var ji=m(Ca);Or=n(ji,"hidden states"),ji.forEach(a),Fr=n(Je," (estados ocultos), tamb\xE9m conhecidos como "),Ma=p(Je,"EM",{});var gi=m(Ma);Dr=n(gi,"features"),gi.forEach(a),Lr=n(Je," (caracter\xEDsticas). Para cada modelo de entrada, recuperaremos um vetor de alta dimensionalidade representando a "),Na=p(Je,"STRONG",{});var qi=m(Na);Vr=n(qi,"compreens\xE3o contextual dessa entrada pelo Transformer*"),qi.forEach(a),Gr=n(Je,"."),Je.forEach(a),Qo=f(e),Ds=p(e,"P",{});var wi=m(Ds);Hr=n(wi,"Se isto n\xE3o faz sentido, n\xE3o se preocupe com isso. Explicaremos tudo isso mais tarde."),wi.forEach(a),Yo=f(e),fe=p(e,"P",{});var da=m(fe);Ur=n(da,"Embora estes hidden states possam ser \xFAteis por si mesmos, eles geralmente s\xE3o entradas para outra parte do modelo, conhecida como "),Sa=p(da,"EM",{});var yi=m(Sa);Rr=n(yi,"head"),yi.forEach(a),Br=n(da," (cabe\xE7a). No "),Ls=p(da,"A",{href:!0});var Pi=m(Ls);Qr=n(Pi,"Cap\xEDtulo 1"),Pi.forEach(a),Yr=n(da,", as diferentes tarefas poderiam ter sido realizadas com a mesma arquitetura, mas cada uma destas tarefas teria uma head diferente associada a ela."),da.forEach(a),Jo=f(e),Ee=p(e,"H3",{class:!0});var xt=m(Ee);Ce=p(xt,"A",{id:!0,class:!0,href:!0});var Ti=m(Ce);Oa=p(Ti,"SPAN",{});var zi=m(Oa);w(ns.$$.fragment,zi),zi.forEach(a),Ti.forEach(a),Jr=f(xt),Fa=p(xt,"SPAN",{});var xi=m(Fa);Wr=n(xi,"Um vetor de alta dimensionalidade?"),xi.forEach(a),xt.forEach(a),Wo=f(e),Vs=p(e,"P",{});var Ai=m(Vs);Xr=n(Ai,"A sa\xEDda vetorial pelo m\xF3dulo do Transformer \xE9 geralmente grande. Geralmente tem tr\xEAs dimens\xF5es:"),Ai.forEach(a),Xo=f(e),he=p(e,"UL",{});var fa=m(he);Gs=p(fa,"LI",{});var yl=m(Gs);Da=p(yl,"STRONG",{});var Ii=m(Da);Kr=n(Ii,"Tamanho do lote"),Ii.forEach(a),Zr=n(yl," (Batch size): O n\xFAmero de sequ\xEAncias processadas de cada vez (2 em nosso exemplo)."),yl.forEach(a),en=f(fa),Hs=p(fa,"LI",{});var Pl=m(Hs);La=p(Pl,"STRONG",{});var Ci=m(La);sn=n(Ci,"Tamanho da sequencia"),Ci.forEach(a),an=n(Pl," (Sequence length): O comprimento da representa\xE7\xE3o num\xE9rica da sequ\xEAncia (16 em nosso exemplo)."),Pl.forEach(a),on=f(fa),Us=p(fa,"LI",{});var Tl=m(Us);Va=p(Tl,"STRONG",{});var Mi=m(Va);tn=n(Mi,"Tamanho oculto"),Mi.forEach(a),rn=n(Tl," (Hidden size): A dimens\xE3o vetorial de cada modelo de entrada."),Tl.forEach(a),fa.forEach(a),Ko=f(e),Rs=p(e,"P",{});var Ni=m(Rs);nn=n(Ni,"Diz-se que \xE9 \u201Cde alta dimensionalidade\u201D por causa do \xFAltimo valor. O tamanho oculto pode ser muito grande (768 \xE9 comum para modelos menores, e em modelos maiores isto pode chegar a 3072 ou mais)."),Ni.forEach(a),Zo=f(e),Bs=p(e,"P",{});var Si=m(Bs);ln=n(Si,"Podemos ver isso se alimentarmos os inputs que pr\xE9-processamos para nosso modelo:"),Si.forEach(a),et=f(e),Z.l(e),Qs=f(e),_e=p(e,"P",{});var ha=m(_e);pn=n(ha,"Observe que as sa\xEDdas dos \u{1F917} Transformer se comportam como \u2018tuplas nomeadas\u2019 (namedtuple) ou dicion\xE1rios. Voc\xEA pode acessar os elementos por atributos (como fizemos) ou por chave ("),Ga=p(ha,"CODE",{});var Oi=m(Ga);mn=n(Oi,'outputs["last_hidden_state"]'),Oi.forEach(a),un=n(ha,"), ou mesmo por \xEDndice se voc\xEA souber exatamente onde o que est\xE1 procurando ("),Ha=p(ha,"CODE",{});var Fi=m(Ha);cn=n(Fi,"outputs[0]"),Fi.forEach(a),dn=n(ha,")."),ha.forEach(a),st=f(e),je=p(e,"H3",{class:!0});var At=m(je);Me=p(At,"A",{id:!0,class:!0,href:!0});var Di=m(Me);Ua=p(Di,"SPAN",{});var Li=m(Ua);w(ls.$$.fragment,Li),Li.forEach(a),Di.forEach(a),fn=f(At),Ra=p(At,"SPAN",{});var Vi=m(Ra);hn=n(Vi,"Cabe\xE7a do modelo (model heads): Fazendo sentido a partir dos n\xFAmeros"),Vi.forEach(a),At.forEach(a),at=f(e),Ne=p(e,"P",{});var It=m(Ne);_n=n(It,"As "),Ba=p(It,"EM",{});var Gi=m(Ba);bn=n(Gi,"heads"),Gi.forEach(a),vn=n(It," do modelo usam o vetor de alta dimensionalidade dos hidden states como entrada e os projetam em uma dimens\xE3o diferente. Eles s\xE3o geralmente compostos de uma ou algumas camadas lineares:"),It.forEach(a),ot=f(e),ge=p(e,"DIV",{class:!0});var Ct=m(ge);is=p(Ct,"IMG",{class:!0,src:!0,alt:!0}),$n=f(Ct),ps=p(Ct,"IMG",{class:!0,src:!0,alt:!0}),Ct.forEach(a),tt=f(e),Se=p(e,"P",{});var Mt=m(Se);kn=n(Mt,"A sa\xEDda do Transformer \xE9 enviada diretamente para a "),Qa=p(Mt,"EM",{});var Hi=m(Qa);En=n(Hi,"head"),Hi.forEach(a),jn=n(Mt," do modelo a ser processado."),Mt.forEach(a),rt=f(e),Ys=p(e,"P",{});var Ui=m(Ys);gn=n(Ui,"Neste diagrama, o modelo \xE9 representado por sua camada de embeddings (vetores) e pelas camadas subsequentes. A camada de embeddings converte cada ID de entrada na entrada tokenizada em um vetor que representa o token associado. As camadas subsequentes manipulam esses vetores usando o mecanismo de aten\xE7\xE3o para produzir a representa\xE7\xE3o final das senten\xE7as."),Ui.forEach(a),nt=f(e),Oe=p(e,"P",{});var Nt=m(Oe);qn=n(Nt,"H\xE1 muitas arquiteturas diferentes dispon\xEDveis no \u{1F917} Transformers, com cada uma projetada em torno de uma tarefa espec\xEDfica. Aqui est\xE1 uma lista por "),Ya=p(Nt,"STRONG",{});var Ri=m(Ya);wn=n(Ri,"algumas"),Ri.forEach(a),yn=n(Nt," destas tarefas:"),Nt.forEach(a),lt=f(e),O=p(e,"UL",{});var G=m(O);Js=p(G,"LI",{});var zl=m(Js);Ja=p(zl,"CODE",{});var Bi=m(Ja);Pn=n(Bi,"*Model"),Bi.forEach(a),Tn=n(zl," (recuperar os hidden states)"),zl.forEach(a),zn=f(G),Wa=p(G,"LI",{});var Qi=m(Wa);Xa=p(Qi,"CODE",{});var Yi=m(Xa);xn=n(Yi,"*ForCausalLM"),Yi.forEach(a),Qi.forEach(a),An=f(G),Ka=p(G,"LI",{});var Ji=m(Ka);Za=p(Ji,"CODE",{});var Wi=m(Za);In=n(Wi,"*ForMaskedLM"),Wi.forEach(a),Ji.forEach(a),Cn=f(G),eo=p(G,"LI",{});var Xi=m(eo);so=p(Xi,"CODE",{});var Ki=m(so);Mn=n(Ki,"*ForMultipleChoice"),Ki.forEach(a),Xi.forEach(a),Nn=f(G),ao=p(G,"LI",{});var Zi=m(ao);oo=p(Zi,"CODE",{});var ep=m(oo);Sn=n(ep,"*ForQuestionAnswering"),ep.forEach(a),Zi.forEach(a),On=f(G),to=p(G,"LI",{});var sp=m(to);ro=p(sp,"CODE",{});var ap=m(ro);Fn=n(ap,"*ForSequenceClassification"),ap.forEach(a),sp.forEach(a),Dn=f(G),no=p(G,"LI",{});var op=m(no);lo=p(op,"CODE",{});var tp=m(lo);Ln=n(tp,"*ForTokenClassification"),tp.forEach(a),op.forEach(a),Vn=f(G),io=p(G,"LI",{});var rp=m(io);Gn=n(rp,"e outros \u{1F917}"),rp.forEach(a),G.forEach(a),it=f(e),se.l(e),Ws=f(e),Fe=p(e,"P",{});var St=m(Fe);Hn=n(St,"Agora se observarmos o tamanho dos nossos inputs, a dimensionalidade ser\xE1 muito menor: a "),po=p(St,"EM",{});var np=m(po);Un=n(np,"head"),np.forEach(a),Rn=n(St," do modelo toma como entrada os vetores de alta dimensionalidade que vimos anteriormente, e os vetores de sa\xEDda contendo dois valores (um por label):"),St.forEach(a),pt=f(e),w(ms.$$.fragment,e),mt=f(e),oe.l(e),Xs=f(e),Ks=p(e,"P",{});var lp=m(Ks);Bn=n(lp,"Como temos apenas duas senten\xE7as e duas labels, o resultado que obtemos de nosso modelo \xE9 de tamanho 2 x 2."),lp.forEach(a),ut=f(e),qe=p(e,"H2",{class:!0});var Ot=m(qe);De=p(Ot,"A",{id:!0,class:!0,href:!0});var ip=m(De);mo=p(ip,"SPAN",{});var pp=m(mo);w(us.$$.fragment,pp),pp.forEach(a),ip.forEach(a),Qn=f(Ot),uo=p(Ot,"SPAN",{});var mp=m(uo);Yn=n(mp,"P\xF3s-processamento da sa\xEDda"),mp.forEach(a),Ot.forEach(a),ct=f(e),Zs=p(e,"P",{});var up=m(Zs);Jn=n(up,"Os valores que obtemos como resultado de nosso modelo n\xE3o fazem necessariamente sentido sozinhos. Vamos dar uma olhada:"),up.forEach(a),dt=f(e),w(cs.$$.fragment,e),ft=f(e),re.l(e),ea=f(e),S=p(e,"P",{});var D=m(S);Wn=n(D,"Nosso modelo previu "),co=p(D,"CODE",{});var cp=m(co);Xn=n(cp,"[-1.5607, 1.6123]"),cp.forEach(a),Kn=n(D," para a primeira frase e "),fo=p(D,"CODE",{});var dp=m(fo);Zn=n(dp,"[ 4.1692, -3.3464]"),dp.forEach(a),el=n(D," para a segunda. Essas n\xE3o s\xE3o probabilidades, mas "),ho=p(D,"EM",{});var fp=m(ho);sl=n(fp,"logits"),fp.forEach(a),al=n(D,", a pontua\xE7\xE3o bruta e n\xE3o normalizada produzida pela \xFAltima camada do modelo. Para serem convertidos em probabilidades, eles precisam passar por uma camada "),ds=p(D,"A",{href:!0,rel:!0});var hp=m(ds);ol=n(hp,"SoftMax"),hp.forEach(a),tl=n(D," (todas sa\xEDdas dos \u{1F917} Transformers produzem  "),_o=p(D,"EM",{});var _p=m(_o);rl=n(_p,"logits"),_p.forEach(a),nl=n(D,", j\xE1 que a fun\xE7\xE3o de "),bo=p(D,"EM",{});var bp=m(bo);ll=n(bp,"loss"),bp.forEach(a),il=n(D," (perda) para treinamento geralmente fundir\xE1 a \xFAltima fun\xE7\xE3o de ativa\xE7\xE3o, como SoftMax, com a fun\xE7\xE3o de "),vo=p(D,"EM",{});var vp=m(vo);pl=n(vp,"loss"),vp.forEach(a),ml=n(D," real, por exemplo a "),$o=p(D,"EM",{});var $p=m($o);ul=n($p,"cross entropy"),$p.forEach(a),cl=n(D,"):"),D.forEach(a),ht=f(e),le.l(e),sa=f(e),pe.l(e),aa=f(e),be=p(e,"P",{});var _a=m(be);dl=n(_a,"Agora podemos ver que o modelo previu "),ko=p(_a,"CODE",{});var kp=m(ko);fl=n(kp,"[0.0402, 0.9598]"),kp.forEach(a),hl=n(_a," para a primeira frase e "),Eo=p(_a,"CODE",{});var Ep=m(Eo);_l=n(Ep,"[0.9995, 0.0005]"),Ep.forEach(a),bl=n(_a," para a segunda. Estas s\xE3o pontua\xE7\xF5es de probabilidade reconhec\xEDveis."),_a.forEach(a),_t=f(e),Le=p(e,"P",{});var Ft=m(Le);vl=n(Ft,"Para obter as etiquetas correspondentes a cada posi\xE7\xE3o, podemos inspecionar o atributo "),jo=p(Ft,"CODE",{});var jp=m(jo);$l=n(jp,"id2label"),jp.forEach(a),kl=n(Ft," da configura\xE7\xE3o do modelo (mais sobre isso na pr\xF3xima se\xE7\xE3o):"),Ft.forEach(a),bt=f(e),w(fs.$$.fragment,e),vt=f(e),w(hs.$$.fragment,e),$t=f(e),oa=p(e,"P",{});var gp=m(oa);El=n(gp,"Agora podemos concluir que o modelo previu o seguinte:"),gp.forEach(a),kt=f(e),Ve=p(e,"UL",{});var Dt=m(Ve);go=p(Dt,"LI",{});var qp=m(go);jl=n(qp,"A primeira frase: NEGATIVE: 0.0402, POSITIVE: 0.9598"),qp.forEach(a),gl=f(Dt),qo=p(Dt,"LI",{});var wp=m(qo);ql=n(wp,"Segunda frase: NEGATIVE: 0.9995, POSITIVE: 0.0005"),wp.forEach(a),Dt.forEach(a),Et=f(e),ta=p(e,"P",{});var yp=m(ta);wl=n(yp,"Reproduzimos com sucesso as tr\xEAs etapas do pipeline: o pr\xE9-processamento, passagem das entradas atrav\xE9s do modelo, e o p\xF3s-processamento! Agora, vamos levar algum tempo para mergulhar mais fundo em cada uma dessas etapas."),yp.forEach(a),jt=f(e),w(Ge.$$.fragment,e),this.h()},h(){E(o,"name","hf:doc:metadata"),E(o,"content",JSON.stringify(lm)),E(h,"id","por-dentro-da-funo-pipeline"),E(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(h,"href","#por-dentro-da-funo-pipeline"),E(b,"class","relative group"),E(zs,"href","/course/pt/chapter1"),E(As,"href","/course/pt/chapter1"),E(Ke,"class","block dark:hidden"),Lt(Ke.src,xl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg")||E(Ke,"src",xl),E(Ke,"alt","O pipeline NLP completa: tokeniza\xE7\xE3o do texto, convers\xE3o para IDs, e infer\xEAncia atrav\xE9s do Transformer e pela 'cabe\xE7a' do modelo."),E(Ze,"class","hidden dark:block"),Lt(Ze.src,Al="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg")||E(Ze,"src",Al),E(Ze,"alt","O pipeline NLP completa: tokeniza\xE7\xE3o do texto, convers\xE3o para IDs, e infer\xEAncia atrav\xE9s do Transformer e pela 'cabe\xE7a' do modelo."),E(ve,"class","flex justify-center"),E(Pe,"id","prprocessamento-com-o-tokenizer"),E(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Pe,"href","#prprocessamento-com-o-tokenizer"),E($e,"class","relative group"),E(as,"href","https://huggingface.co/models"),E(as,"rel","nofollow"),E(os,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),E(os,"rel","nofollow"),E(Ie,"id","indo-adianta-pelo-modelo"),E(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ie,"href","#indo-adianta-pelo-modelo"),E(ke,"class","relative group"),E(Ls,"href","/course/pt/chapter1"),E(Ce,"id","um-vetor-de-alta-dimensionalidade"),E(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ce,"href","#um-vetor-de-alta-dimensionalidade"),E(Ee,"class","relative group"),E(Me,"id","cabea-do-modelo-model-heads-fazendo-sentido-a-partir-dos-nmeros"),E(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Me,"href","#cabea-do-modelo-model-heads-fazendo-sentido-a-partir-dos-nmeros"),E(je,"class","relative group"),E(is,"class","block dark:hidden"),Lt(is.src,Il="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg")||E(is,"src",Il),E(is,"alt","Uma rede Transformer ao lado de sua head."),E(ps,"class","hidden dark:block"),Lt(ps.src,Cl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg")||E(ps,"src",Cl),E(ps,"alt","Uma rede Transformer ao lado de sua head."),E(ge,"class","flex justify-center"),E(De,"id","psprocessamento-da-sada"),E(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(De,"href","#psprocessamento-da-sada"),E(qe,"class","relative group"),E(ds,"href","https://en.wikipedia.org/wiki/Softmax_function"),E(ds,"rel","nofollow")},m(e,l){s(document.head,o),u(e,c,l),y(t,e,l),u(e,_,l),u(e,b,l),s(b,h),s(h,j),y(N,j,null),s(b,C),s(b,A),s(A,F),u(e,x,l),_s[T].m(e,l),u(e,g,l),y(z,e,l),u(e,ce,l),bs[L].m(e,l),u(e,Ts,l),u(e,we,l),s(we,Vt),s(we,zs),s(zs,Gt),s(we,Ht),u(e,Po,l),y(We,e,l),u(e,To,l),u(e,xs,l),s(xs,Ut),u(e,zo,l),y(Xe,e,l),u(e,xo,l),u(e,ye,l),s(ye,Rt),s(ye,As),s(As,Bt),s(ye,Qt),u(e,Ao,l),u(e,ve,l),s(ve,Ke),s(ve,Yt),s(ve,Ze),u(e,Io,l),u(e,Is,l),s(Is,Jt),u(e,Co,l),u(e,$e,l),s($e,Pe),s(Pe,va),y(es,va,null),s($e,Wt),s($e,$a),s($a,Xt),u(e,Mo,l),u(e,Te,l),s(Te,Kt),s(Te,ka),s(ka,Zt),s(Te,er),u(e,No,l),u(e,de,l),s(de,ss),s(ss,sr),s(ss,Ea),s(Ea,ar),s(ss,or),s(de,tr),s(de,ja),s(ja,rr),s(de,nr),s(de,ga),s(ga,lr),u(e,So,l),u(e,H,l),s(H,ir),s(H,as),s(as,pr),s(H,mr),s(H,qa),s(qa,ur),s(H,cr),s(H,wa),s(wa,dr),s(H,fr),u(e,Oo,l),u(e,U,l),s(U,hr),s(U,ya),s(ya,_r),s(U,br),s(U,Pa),s(Pa,vr),s(U,$r),s(U,os),s(os,kr),s(U,Er),u(e,Fo,l),y(ts,e,l),u(e,Do,l),u(e,Cs,l),s(Cs,jr),u(e,Lo,l),u(e,ze,l),s(ze,gr),s(ze,Ta),s(Ta,qr),s(ze,wr),u(e,Vo,l),u(e,xe,l),s(xe,yr),s(xe,za),s(za,Pr),s(xe,Tr),u(e,Go,l),vs[B].m(e,l),u(e,Ms,l),u(e,Ns,l),s(Ns,zr),u(e,Ho,l),$s[Y].m(e,l),u(e,Ss,l),u(e,Ae,l),s(Ae,xr),s(Ae,xa),s(xa,Ar),s(Ae,Ir),u(e,Uo,l),u(e,ke,l),s(ke,Ie),s(Ie,Aa),y(rs,Aa,null),s(ke,Cr),s(ke,Ia),s(Ia,Mr),u(e,Ro,l),ks[W].m(e,l),u(e,Os,l),u(e,Fs,l),s(Fs,Nr),u(e,Bo,l),u(e,R,l),s(R,Sr),s(R,Ca),s(Ca,Or),s(R,Fr),s(R,Ma),s(Ma,Dr),s(R,Lr),s(R,Na),s(Na,Vr),s(R,Gr),u(e,Qo,l),u(e,Ds,l),s(Ds,Hr),u(e,Yo,l),u(e,fe,l),s(fe,Ur),s(fe,Sa),s(Sa,Rr),s(fe,Br),s(fe,Ls),s(Ls,Qr),s(fe,Yr),u(e,Jo,l),u(e,Ee,l),s(Ee,Ce),s(Ce,Oa),y(ns,Oa,null),s(Ee,Jr),s(Ee,Fa),s(Fa,Wr),u(e,Wo,l),u(e,Vs,l),s(Vs,Xr),u(e,Xo,l),u(e,he,l),s(he,Gs),s(Gs,Da),s(Da,Kr),s(Gs,Zr),s(he,en),s(he,Hs),s(Hs,La),s(La,sn),s(Hs,an),s(he,on),s(he,Us),s(Us,Va),s(Va,tn),s(Us,rn),u(e,Ko,l),u(e,Rs,l),s(Rs,nn),u(e,Zo,l),u(e,Bs,l),s(Bs,ln),u(e,et,l),Es[K].m(e,l),u(e,Qs,l),u(e,_e,l),s(_e,pn),s(_e,Ga),s(Ga,mn),s(_e,un),s(_e,Ha),s(Ha,cn),s(_e,dn),u(e,st,l),u(e,je,l),s(je,Me),s(Me,Ua),y(ls,Ua,null),s(je,fn),s(je,Ra),s(Ra,hn),u(e,at,l),u(e,Ne,l),s(Ne,_n),s(Ne,Ba),s(Ba,bn),s(Ne,vn),u(e,ot,l),u(e,ge,l),s(ge,is),s(ge,$n),s(ge,ps),u(e,tt,l),u(e,Se,l),s(Se,kn),s(Se,Qa),s(Qa,En),s(Se,jn),u(e,rt,l),u(e,Ys,l),s(Ys,gn),u(e,nt,l),u(e,Oe,l),s(Oe,qn),s(Oe,Ya),s(Ya,wn),s(Oe,yn),u(e,lt,l),u(e,O,l),s(O,Js),s(Js,Ja),s(Ja,Pn),s(Js,Tn),s(O,zn),s(O,Wa),s(Wa,Xa),s(Xa,xn),s(O,An),s(O,Ka),s(Ka,Za),s(Za,In),s(O,Cn),s(O,eo),s(eo,so),s(so,Mn),s(O,Nn),s(O,ao),s(ao,oo),s(oo,Sn),s(O,On),s(O,to),s(to,ro),s(ro,Fn),s(O,Dn),s(O,no),s(no,lo),s(lo,Ln),s(O,Vn),s(O,io),s(io,Gn),u(e,it,l),js[ee].m(e,l),u(e,Ws,l),u(e,Fe,l),s(Fe,Hn),s(Fe,po),s(po,Un),s(Fe,Rn),u(e,pt,l),y(ms,e,l),u(e,mt,l),gs[ae].m(e,l),u(e,Xs,l),u(e,Ks,l),s(Ks,Bn),u(e,ut,l),u(e,qe,l),s(qe,De),s(De,mo),y(us,mo,null),s(qe,Qn),s(qe,uo),s(uo,Yn),u(e,ct,l),u(e,Zs,l),s(Zs,Jn),u(e,dt,l),y(cs,e,l),u(e,ft,l),qs[te].m(e,l),u(e,ea,l),u(e,S,l),s(S,Wn),s(S,co),s(co,Xn),s(S,Kn),s(S,fo),s(fo,Zn),s(S,el),s(S,ho),s(ho,sl),s(S,al),s(S,ds),s(ds,ol),s(S,tl),s(S,_o),s(_o,rl),s(S,nl),s(S,bo),s(bo,ll),s(S,il),s(S,vo),s(vo,pl),s(S,ml),s(S,$o),s($o,ul),s(S,cl),u(e,ht,l),ws[ne].m(e,l),u(e,sa,l),ys[ie].m(e,l),u(e,aa,l),u(e,be,l),s(be,dl),s(be,ko),s(ko,fl),s(be,hl),s(be,Eo),s(Eo,_l),s(be,bl),u(e,_t,l),u(e,Le,l),s(Le,vl),s(Le,jo),s(jo,$l),s(Le,kl),u(e,bt,l),y(fs,e,l),u(e,vt,l),y(hs,e,l),u(e,$t,l),u(e,oa,l),s(oa,El),u(e,kt,l),u(e,Ve,l),s(Ve,go),s(go,jl),s(Ve,gl),s(Ve,qo),s(qo,ql),u(e,Et,l),u(e,ta,l),s(ta,wl),u(e,jt,l),y(Ge,e,l),gt=!0},p(e,[l]){const Ps={};l&1&&(Ps.fw=e[0]),t.$set(Ps);let ra=T;T=Nl(e),T!==ra&&(ue(),v(_s[ra],1,1,()=>{_s[ra]=null}),me(),M=_s[T],M||(M=_s[T]=Ml[T](e),M.c()),$(M,1),M.m(g.parentNode,g));const wo={};l&2&&(wo.$$scope={dirty:l,ctx:e}),z.$set(wo);let na=L;L=Ol(e),L!==na&&(ue(),v(bs[na],1,1,()=>{bs[na]=null}),me(),V=bs[L],V||(V=bs[L]=Sl[L](e),V.c()),$(V,1),V.m(Ts.parentNode,Ts));let He=B;B=Dl(e),B!==He&&(ue(),v(vs[He],1,1,()=>{vs[He]=null}),me(),Q=vs[B],Q||(Q=vs[B]=Fl[B](e),Q.c()),$(Q,1),Q.m(Ms.parentNode,Ms));let la=Y;Y=Vl(e),Y!==la&&(ue(),v($s[la],1,1,()=>{$s[la]=null}),me(),J=$s[Y],J||(J=$s[Y]=Ll[Y](e),J.c()),$(J,1),J.m(Ss.parentNode,Ss));let ia=W;W=Hl(e),W!==ia&&(ue(),v(ks[ia],1,1,()=>{ks[ia]=null}),me(),X=ks[W],X||(X=ks[W]=Gl[W](e),X.c()),$(X,1),X.m(Os.parentNode,Os));let Ue=K;K=Rl(e),K!==Ue&&(ue(),v(Es[Ue],1,1,()=>{Es[Ue]=null}),me(),Z=Es[K],Z||(Z=Es[K]=Ul[K](e),Z.c()),$(Z,1),Z.m(Qs.parentNode,Qs));let pa=ee;ee=Ql(e),ee!==pa&&(ue(),v(js[pa],1,1,()=>{js[pa]=null}),me(),se=js[ee],se||(se=js[ee]=Bl[ee](e),se.c()),$(se,1),se.m(Ws.parentNode,Ws));let Re=ae;ae=Jl(e),ae!==Re&&(ue(),v(gs[Re],1,1,()=>{gs[Re]=null}),me(),oe=gs[ae],oe||(oe=gs[ae]=Yl[ae](e),oe.c()),$(oe,1),oe.m(Xs.parentNode,Xs));let ma=te;te=Xl(e),te!==ma&&(ue(),v(qs[ma],1,1,()=>{qs[ma]=null}),me(),re=qs[te],re||(re=qs[te]=Wl[te](e),re.c()),$(re,1),re.m(ea.parentNode,ea));let Be=ne;ne=Zl(e),ne!==Be&&(ue(),v(ws[Be],1,1,()=>{ws[Be]=null}),me(),le=ws[ne],le||(le=ws[ne]=Kl[ne](e),le.c()),$(le,1),le.m(sa.parentNode,sa));let ua=ie;ie=si(e),ie!==ua&&(ue(),v(ys[ua],1,1,()=>{ys[ua]=null}),me(),pe=ys[ie],pe||(pe=ys[ie]=ei[ie](e),pe.c()),$(pe,1),pe.m(aa.parentNode,aa));const yo={};l&2&&(yo.$$scope={dirty:l,ctx:e}),Ge.$set(yo)},i(e){gt||($(t.$$.fragment,e),$(N.$$.fragment,e),$(M),$(z.$$.fragment,e),$(V),$(We.$$.fragment,e),$(Xe.$$.fragment,e),$(es.$$.fragment,e),$(ts.$$.fragment,e),$(Q),$(J),$(rs.$$.fragment,e),$(X),$(ns.$$.fragment,e),$(Z),$(ls.$$.fragment,e),$(se),$(ms.$$.fragment,e),$(oe),$(us.$$.fragment,e),$(cs.$$.fragment,e),$(re),$(le),$(pe),$(fs.$$.fragment,e),$(hs.$$.fragment,e),$(Ge.$$.fragment,e),gt=!0)},o(e){v(t.$$.fragment,e),v(N.$$.fragment,e),v(M),v(z.$$.fragment,e),v(V),v(We.$$.fragment,e),v(Xe.$$.fragment,e),v(es.$$.fragment,e),v(ts.$$.fragment,e),v(Q),v(J),v(rs.$$.fragment,e),v(X),v(ns.$$.fragment,e),v(Z),v(ls.$$.fragment,e),v(se),v(ms.$$.fragment,e),v(oe),v(us.$$.fragment,e),v(cs.$$.fragment,e),v(re),v(le),v(pe),v(fs.$$.fragment,e),v(hs.$$.fragment,e),v(Ge.$$.fragment,e),gt=!1},d(e){a(o),e&&a(c),P(t,e),e&&a(_),e&&a(b),P(N),e&&a(x),_s[T].d(e),e&&a(g),P(z,e),e&&a(ce),bs[L].d(e),e&&a(Ts),e&&a(we),e&&a(Po),P(We,e),e&&a(To),e&&a(xs),e&&a(zo),P(Xe,e),e&&a(xo),e&&a(ye),e&&a(Ao),e&&a(ve),e&&a(Io),e&&a(Is),e&&a(Co),e&&a($e),P(es),e&&a(Mo),e&&a(Te),e&&a(No),e&&a(de),e&&a(So),e&&a(H),e&&a(Oo),e&&a(U),e&&a(Fo),P(ts,e),e&&a(Do),e&&a(Cs),e&&a(Lo),e&&a(ze),e&&a(Vo),e&&a(xe),e&&a(Go),vs[B].d(e),e&&a(Ms),e&&a(Ns),e&&a(Ho),$s[Y].d(e),e&&a(Ss),e&&a(Ae),e&&a(Uo),e&&a(ke),P(rs),e&&a(Ro),ks[W].d(e),e&&a(Os),e&&a(Fs),e&&a(Bo),e&&a(R),e&&a(Qo),e&&a(Ds),e&&a(Yo),e&&a(fe),e&&a(Jo),e&&a(Ee),P(ns),e&&a(Wo),e&&a(Vs),e&&a(Xo),e&&a(he),e&&a(Ko),e&&a(Rs),e&&a(Zo),e&&a(Bs),e&&a(et),Es[K].d(e),e&&a(Qs),e&&a(_e),e&&a(st),e&&a(je),P(ls),e&&a(at),e&&a(Ne),e&&a(ot),e&&a(ge),e&&a(tt),e&&a(Se),e&&a(rt),e&&a(Ys),e&&a(nt),e&&a(Oe),e&&a(lt),e&&a(O),e&&a(it),js[ee].d(e),e&&a(Ws),e&&a(Fe),e&&a(pt),P(ms,e),e&&a(mt),gs[ae].d(e),e&&a(Xs),e&&a(Ks),e&&a(ut),e&&a(qe),P(us),e&&a(ct),e&&a(Zs),e&&a(dt),P(cs,e),e&&a(ft),qs[te].d(e),e&&a(ea),e&&a(S),e&&a(ht),ws[ne].d(e),e&&a(sa),ys[ie].d(e),e&&a(aa),e&&a(be),e&&a(_t),e&&a(Le),e&&a(bt),P(fs,e),e&&a(vt),P(hs,e),e&&a($t),e&&a(oa),e&&a(kt),e&&a(Ve),e&&a(Et),e&&a(ta),e&&a(jt),P(Ge,e)}}}const lm={local:"por-dentro-da-funo-pipeline",sections:[{local:"prprocessamento-com-o-tokenizer",title:"Pr\xE9-processamento com o tokenizer"},{local:"indo-adianta-pelo-modelo",sections:[{local:"um-vetor-de-alta-dimensionalidade",title:"Um vetor de alta dimensionalidade?"},{local:"cabea-do-modelo-model-heads-fazendo-sentido-a-partir-dos-nmeros",title:"Cabe\xE7a do modelo (model heads): Fazendo sentido a partir dos n\xFAmeros"}],title:"Indo adianta pelo modelo"},{local:"psprocessamento-da-sada",title:"P\xF3s-processamento da sa\xEDda"}],title:"Por dentro da fun\xE7\xE3o pipeline"};function im(k,o,c){let t="pt";return Mp(()=>{const _=new URLSearchParams(window.location.search);c(0,t=_.get("fw")||"pt")}),[t]}class _m extends xp{constructor(o){super();Ap(this,o,im,nm,Ip,{})}}export{_m as default,lm as metadata};
