import{S as hu,i as vu,s as Eu,e as r,k as u,w as v,t as n,M as ku,c as l,d as s,m as p,a as o,x as E,h as a,b as m,N as du,G as e,g as c,y as k,q as _,o as b,B as g,v as _u}from"../../chunks/vendor-hf-doc-builder.js";import{T as bu}from"../../chunks/Tip-hf-doc-builder.js";import{Y as fu}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ys}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as gu}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function $u(Ks){let d,de,P,D,re,M,Ne,le,oe,fe,O,Y,f,Se,U,He,Be;return{c(){d=r("p"),de=n("\u270F\uFE0F "),P=r("strong"),D=n("Essayez !"),re=n(" Chargez un "),M=r("em"),Ne=n("tokenizer"),le=n(" depuis le "),oe=r("em"),fe=n("checkpoint"),O=u(),Y=r("code"),f=n("bert-base-cased"),Se=n(" et passez-lui le m\xEAme exemple. Quelles sont les principales diff\xE9rences que vous pouvez voir entre les versions cas\xE9e et non cas\xE9e du "),U=r("em"),He=n("tokenizer"),Be=n(" ?")},l(R){d=l(R,"P",{});var $=o(d);de=a($,"\u270F\uFE0F "),P=l($,"STRONG",{});var ct=o(P);D=a(ct,"Essayez !"),ct.forEach(s),re=a($," Chargez un "),M=l($,"EM",{});var he=o(M);Ne=a(he,"tokenizer"),he.forEach(s),le=a($," depuis le "),oe=l($,"EM",{});var mt=o(oe);fe=a(mt,"checkpoint"),mt.forEach(s),O=p($),Y=l($,"CODE",{});var dt=o(Y);f=a(dt,"bert-base-cased"),dt.forEach(s),Se=a($," et passez-lui le m\xEAme exemple. Quelles sont les principales diff\xE9rences que vous pouvez voir entre les versions cas\xE9e et non cas\xE9e du "),U=l($,"EM",{});var ve=o(U);He=a(ve,"tokenizer"),ve.forEach(s),Be=a($," ?"),$.forEach(s)},m(R,$){c(R,d,$),e(d,de),e(d,P),e(P,D),e(d,re),e(d,M),e(M,Ne),e(d,le),e(d,oe),e(oe,fe),e(d,O),e(d,Y),e(Y,f),e(d,Se),e(d,U),e(U,He),e(d,Be)},d(R){R&&s(d)}}}function zu(Ks){let d,de,P,D,re,M,Ne,le,oe,fe,O,Y,f,Se,U,He,Be,R,$,ct,he,mt,dt,ve,Vn,Wn,St,Jn,Yn,Qs,ie,Oe,$o,Kn,Ue,zo,Xs,w,Qn,Ht,Xn,Zn,Bt,ea,ta,Ot,sa,na,Ut,aa,ra,Zs,ue,Ee,Rt,Re,la,It,oa,en,Ie,tn,K,ia,Le,ua,pa,Lt,ca,ma,sn,z,da,Gt,fa,ha,Ft,va,Ea,Vt,ka,_a,Wt,ba,ga,Jt,$a,za,nn,Ge,an,Fe,rn,C,qa,Yt,xa,ja,Kt,Pa,wa,Qt,Ta,ya,ln,Ve,on,We,un,Q,Ma,Xt,Da,Ca,Zt,Aa,Na,pn,ke,cn,pe,_e,es,Je,Sa,ts,Ha,mn,Ye,dn,q,Ba,ss,Oa,Ua,ft,Ra,Ia,ns,La,Ga,as,Fa,Va,rs,Wa,Ja,fn,T,Ya,ls,Ka,Qa,os,Xa,Za,is,er,tr,us,sr,nr,hn,Ke,vn,Qe,En,x,ar,ps,rr,lr,cs,or,ir,ms,ur,pr,ds,cr,mr,fs,dr,fr,kn,A,hr,hs,vr,Er,vs,kr,_r,Es,br,gr,_n,Xe,bn,X,$r,ks,zr,qr,_s,xr,jr,gn,Ze,$n,Z,Pr,bs,wr,Tr,gs,yr,Mr,zn,be,Dr,$s,Cr,Ar,qn,et,xn,tt,jn,h,Nr,zs,Sr,Hr,qs,Br,Or,xs,Ur,Rr,js,Ir,Lr,Ps,Gr,Fr,ws,Vr,Wr,Ts,Jr,Yr,Pn,ge,Kr,ys,Qr,Xr,wn,ce,$e,Ms,st,Zr,Ds,el,Tn,I,nt,tl,sl,Cs,nl,al,ht,rl,ll,yn,y,ol,As,il,ul,Ns,pl,cl,Ss,ml,dl,Hs,fl,hl,Mn,me,ze,Bs,at,vl,Os,El,Dn,vt,kl,Cn,qe,Us,L,Et,_l,bl,kt,gl,$l,_t,zl,ql,bt,xl,jl,G,F,gt,Pl,wl,rt,Tl,Rs,yl,Ml,lt,Dl,Is,Cl,Al,ot,Nl,Ls,Sl,Hl,V,$t,Bl,Ol,xe,Ul,Gs,Rl,Il,Ll,ee,Gl,Fs,Fl,Vl,Vs,Wl,Jl,Yl,je,Kl,Ws,Ql,Xl,Zl,W,zt,eo,to,qt,so,no,xt,ao,ro,it,lo,Js,oo,io,J,jt,uo,po,Pt,co,mo,wt,fo,ho,Tt,vo,An,yt,Eo,Nn;return M=new Ys({}),O=new gu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"}]}}),Re=new Ys({}),Ie=new fu({props:{id:"4IIC2jI9CaU"}}),Ge=new ae({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(tokenizer.backend_tokenizer))`}}),Fe=new ae({props:{code:"<class 'tokenizers.Tokenizer'>",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;tokenizers.Tokenizer&#x27;</span>&gt;'}}),Ve=new ae({props:{code:'print(tokenizer.backend_tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),We=new ae({props:{code:"'hello how are u?'",highlighted:'<span class="hljs-string">&#x27;hello how are u?&#x27;</span>'}}),ke=new bu({props:{$$slots:{default:[$u]},$$scope:{ctx:Ks}}}),Je=new Ys({}),Ye=new fu({props:{id:"grlLV8AIXug"}}),Ke=new ae({props:{code:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")',highlighted:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)'}}),Qe=new ae({props:{code:"[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]",highlighted:'[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;you&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">19</span>)), (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]'}}),Xe=new ae({props:{code:`tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),Ze=new ae({props:{code:`[('Hello', (0, 5)), (',', (5, 6)), ('\u0120how', (6, 10)), ('\u0120are', (10, 14)), ('\u0120', (14, 15)), ('\u0120you', (15, 19)),
 ('?', (19, 20))]`,highlighted:`[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u0120how&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120are&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u0120&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)), (<span class="hljs-string">&#x27;\u0120you&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">19</span>)),
 (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]`}}),et=new ae({props:{code:`tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),tt=new ae({props:{code:"[('\u2581Hello,', (0, 6)), ('\u2581how', (7, 10)), ('\u2581are', (11, 14)), ('\u2581you?', (16, 20))]",highlighted:'[(<span class="hljs-string">&#x27;\u2581Hello,&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u2581how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581you?&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">20</span>))]'}}),st=new Ys({}),at=new Ys({}),{c(){d=r("meta"),de=u(),P=r("h1"),D=r("a"),re=r("span"),v(M.$$.fragment),Ne=u(),le=r("span"),oe=n("Normalisation et pr\xE9tokenization"),fe=u(),v(O.$$.fragment),Y=u(),f=r("p"),Se=n("Avant de nous plonger plus profond\xE9ment dans les trois algorithmes de tok\xE9nisation en sous-mots les plus courants utilis\xE9s avec les "),U=r("em"),He=n("transformers"),Be=n(" ("),R=r("em"),$=n("Byte-Pair Encoding"),ct=n(" (BPE), "),he=r("em"),mt=n("WordPiece"),dt=n(" et "),ve=r("em"),Vn=n("Unigram"),Wn=n("), nous allons d\u2019abord examiner le pr\xE9traitement que chaque "),St=r("em"),Jn=n("tokenizer"),Yn=n(" applique au texte. Voici un aper\xE7u de haut niveau des \xE9tapes du pipeline de tokenisation :"),Qs=u(),ie=r("div"),Oe=r("img"),Kn=u(),Ue=r("img"),Xs=u(),w=r("p"),Qn=n("Avant de diviser un texte en sous-"),Ht=r("em"),Xn=n("tokens"),Zn=n(" (selon le mod\xE8le), le "),Bt=r("em"),ea=n("tokenizer"),ta=n(" effectue deux \xE9tapes : la "),Ot=r("em"),sa=n("normalisation"),na=n(" et la "),Ut=r("em"),aa=n("pr\xE9tok\xE9nisation"),ra=n("."),Zs=u(),ue=r("h2"),Ee=r("a"),Rt=r("span"),v(Re.$$.fragment),la=u(),It=r("span"),oa=n("Normalisation"),en=u(),v(Ie.$$.fragment),tn=u(),K=r("p"),ia=n("L\u2019\xE9tape de normalisation implique un nettoyage g\xE9n\xE9ral, comme la suppression des espaces inutiles, la mise en minuscules et/ou la suppression des accents. Si vous \xEAtes familier avec la "),Le=r("a"),ua=n("normalisation Unicode"),pa=n(" (comme NFC ou NFKC), c\u2019est aussi quelque chose que le "),Lt=r("em"),ca=n("tokenizer"),ma=n(" peut appliquer."),sn=u(),z=r("p"),da=n("Le "),Gt=r("code"),fa=n("tokenizer"),ha=n(" de \u{1F917} "),Ft=r("em"),va=n("Transformers"),Ea=n(" poss\xE8de un attribut appel\xE9 "),Vt=r("code"),ka=n("backend_tokenizer"),_a=n(" qui donne acc\xE8s au "),Wt=r("em"),ba=n("tokenizer"),ga=n(" sous-jacent de la biblioth\xE8que \u{1F917} "),Jt=r("em"),$a=n("Tokenizers"),za=n(" :"),nn=u(),v(Ge.$$.fragment),an=u(),v(Fe.$$.fragment),rn=u(),C=r("p"),qa=n("L\u2019attribut "),Yt=r("code"),xa=n("normalizer"),ja=n(" de l\u2019objet "),Kt=r("code"),Pa=n("tokenizer"),wa=n(" poss\xE8de une m\xE9thode "),Qt=r("code"),Ta=n("normalize_str()"),ya=n(" que nous pouvons utiliser pour voir comment la normalisation est effectu\xE9e :"),ln=u(),v(Ve.$$.fragment),on=u(),v(We.$$.fragment),un=u(),Q=r("p"),Ma=n("Dans cet exemple, puisque nous avons choisi le "),Xt=r("em"),Da=n("checkpoint"),Ca=u(),Zt=r("code"),Aa=n("bert-base-uncased"),Na=n(", la normalisation a mis le texte en minuscule et supprim\xE9 les accents."),pn=u(),v(ke.$$.fragment),cn=u(),pe=r("h2"),_e=r("a"),es=r("span"),v(Je.$$.fragment),Sa=u(),ts=r("span"),Ha=n("Pr\xE9tokenization"),mn=u(),v(Ye.$$.fragment),dn=u(),q=r("p"),Ba=n("Comme nous le verrons dans les sections suivantes, un "),ss=r("em"),Oa=n("tokenizer"),Ua=n(" ne peut pas \xEAtre entra\xEEn\xE9 uniquement sur du texte brut. Au lieu de cela, nous devons d\u2019abord diviser les textes en petites entit\xE9s, comme des mots. C\u2019est l\xE0 qu\u2019intervient l\u2019\xE9tape de pr\xE9tok\xE9nisation. Comme nous l\u2019avons vu dans le "),ft=r("a"),Ra=n("chapitre 2"),Ia=n(", un "),ns=r("em"),La=n("tokenizer"),Ga=n(" bas\xE9 sur les mots peut simplement diviser un texte brut en mots sur les espaces et la ponctuation. Ces mots constitueront les limites des sous-"),as=r("em"),Fa=n("tokens"),Va=n(" que le "),rs=r("em"),Wa=n("tokenizer"),Ja=n(" peut apprendre pendant son entra\xEEnement."),fn=u(),T=r("p"),Ya=n("Pour voir comment un "),ls=r("em"),Ka=n("tokenizer"),Qa=n(" rapide effectue la pr\xE9tok\xE9nisation, nous pouvons utiliser la m\xE9thode "),os=r("code"),Xa=n("pre_tokenize_str()"),Za=n("de l\u2019attribut "),is=r("code"),er=n("pre_tokenizer"),tr=n(" de l\u2019objet "),us=r("code"),sr=n("tokenizer"),nr=n(" :"),hn=u(),v(Ke.$$.fragment),vn=u(),v(Qe.$$.fragment),En=u(),x=r("p"),ar=n("Remarquez que le "),ps=r("em"),rr=n("tokenizer"),lr=n(" garde d\xE9j\xE0 la trace des "),cs=r("em"),or=n("offsets"),ir=n(", ce qui lui permet de nous donner la correspondance des d\xE9calages que nous avons utilis\xE9e dans la section pr\xE9c\xE9dente. Ici, le "),ms=r("em"),ur=n("tokenizer"),pr=n(" ignore les deux espaces et les remplace par un seul, mais le d\xE9calage saute entre "),ds=r("code"),cr=n("are"),mr=n(" et "),fs=r("code"),dr=n("you"),fr=n(" pour en tenir compte."),kn=u(),A=r("p"),hr=n("Puisque nous utilisons le "),hs=r("em"),vr=n("tokenizer"),Er=n(" de BERT, la pr\xE9tok\xE9nisation implique la s\xE9paration des espaces et de la ponctuation. D\u2019autres "),vs=r("em"),kr=n("tokenizers"),_r=n(" peuvent avoir des r\xE8gles diff\xE9rentes pour cette \xE9tape. Par exemple, si nous utilisons le "),Es=r("em"),br=n("tokenizer"),gr=n(" du GPT-2 :"),_n=u(),v(Xe.$$.fragment),bn=u(),X=r("p"),$r=n("Il s\xE9parera aussi sur les espaces et la ponctuation mais gardera les espaces et les remplacera par un symbole "),ks=r("code"),zr=n("\u0120"),qr=n(", ce qui lui permettra de r\xE9cup\xE9rer les espaces originaux si nous d\xE9codons les "),_s=r("em"),xr=n("tokens"),jr=n(" :"),gn=u(),v(Ze.$$.fragment),$n=u(),Z=r("p"),Pr=n("Notez \xE9galement que, contrairement au "),bs=r("em"),wr=n("tokenizer"),Tr=n(" de BERT, ce "),gs=r("em"),yr=n("tokenizer"),Mr=n(" n\u2019ignore pas les doubles espaces."),zn=u(),be=r("p"),Dr=n("Pour un dernier exemple, regardons le "),$s=r("em"),Cr=n("tokenizer"),Ar=n(" du 5, qui est bas\xE9 sur l\u2019algorithme SentencePiece :"),qn=u(),v(et.$$.fragment),xn=u(),v(tt.$$.fragment),jn=u(),h=r("p"),Nr=n("Comme le "),zs=r("em"),Sr=n("tokenizer"),Hr=n(" du GPT-2, celui-ci garde les espaces et les remplace par un "),qs=r("em"),Br=n("token"),Or=n(" sp\xE9cifique ("),xs=r("code"),Ur=n("_"),Rr=n("), mais le "),js=r("em"),Ir=n("tokenizer"),Lr=n(" du T5 ne s\xE9pare que sur les espaces, pas sur la ponctuation. Notez \xE9galement qu\u2019il a ajout\xE9 un espace par d\xE9faut au d\xE9but de la phrase (avant "),Ps=r("code"),Gr=n("Hello"),Fr=n(") et il ignore le double espace entre "),ws=r("code"),Vr=n("are"),Wr=n(" et "),Ts=r("code"),Jr=n("you"),Yr=n("."),Pn=u(),ge=r("p"),Kr=n("Maintenant que nous avons vu un peu comment les diff\xE9rents "),ys=r("em"),Qr=n("tokenizers"),Xr=n(" traitent le texte, nous pouvons commencer \xE0 explorer les algorithmes sous-jacents eux-m\xEAmes. Nous commencerons par jeter un coup d\u2019oeil rapide sur le tr\xE8s r\xE9pandu SentencePiece, puis au cours des trois sections suivantes nous examinerons le fonctionnement des trois principaux algorithmes utilis\xE9s pour la tokenisation en sous-mots."),wn=u(),ce=r("h2"),$e=r("a"),Ms=r("span"),v(st.$$.fragment),Zr=u(),Ds=r("span"),el=n("SentencePiece"),Tn=u(),I=r("p"),nt=r("a"),tl=n("SentencePiece"),sl=n(" est un algorithme de tokenisation pour le pr\xE9traitement du texte que vous pouvez utiliser avec n\u2019importe lequel des mod\xE8les que nous verrons dans les trois prochaines sections. Il consid\xE8re le texte comme une s\xE9quence de caract\xE8res Unicode et il remplace les espaces par un caract\xE8re sp\xE9cial : "),Cs=r("code"),nl=n("\u2581"),al=n(". Utilis\xE9 en conjonction avec l\u2019algorithme Unigram (voir la "),ht=r("a"),rl=n("section 7"),ll=n("), il ne n\xE9cessite m\xEAme pas d\u2019\xE9tape de pr\xE9tok\xE9nisation, ce qui est tr\xE8s utile pour les langues o\xF9 le caract\xE8re espace n\u2019est pas utilis\xE9 (comme le chinois ou le japonais)."),yn=u(),y=r("p"),ol=n("L\u2019autre caract\xE9ristique principale de SentencePiece est le "),As=r("em"),il=n("tokenisation r\xE9versible"),ul=n(" : comme il n\u2019y a pas de traitement sp\xE9cial des espaces, le d\xE9codage des "),Ns=r("em"),pl=n("tokens"),cl=n(" se fait simplement en les concat\xE9nant et en rempla\xE7ant les "),Ss=r("code"),ml=n("_"),dl=n(" par des espaces, ce qui donne le texte normalis\xE9. Comme nous l\u2019avons vu pr\xE9c\xE9demment, le "),Hs=r("em"),fl=n("tokenizer"),hl=n(" de BERT supprime les espaces r\xE9p\xE9titifs, donc sa tokenisation n\u2019est pas r\xE9versible."),Mn=u(),me=r("h2"),ze=r("a"),Bs=r("span"),v(at.$$.fragment),vl=u(),Os=r("span"),El=n("Vue d'ensemble des algorithmes"),Dn=u(),vt=r("p"),kl=n("Dans les sections suivantes, nous allons nous plonger dans les trois principaux algorithmes de tokenisation en sous-mots : BPE (utilis\xE9 par GPT-2 et autres), WordPiece (utilis\xE9 par exemple par BERT), et Unigram (utilis\xE9 par T5 et autres). Avant de commencer, voici un rapide aper\xE7u du fonctionnement de chacun d\u2019entre eux. N\u2019h\xE9sitez pas \xE0 revenir \xE0 ce tableau apr\xE8s avoir lu chacune des sections suivantes si cela n\u2019a pas encore de sens pour vous."),Cn=u(),qe=r("table"),Us=r("thead"),L=r("tr"),Et=r("th"),_l=n("Mod\xE8le"),bl=u(),kt=r("th"),gl=n("BPE"),$l=u(),_t=r("th"),zl=n("WordPiece"),ql=u(),bt=r("th"),xl=n("Unigramme"),jl=u(),G=r("tbody"),F=r("tr"),gt=r("td"),Pl=n("Entra\xEEnement"),wl=u(),rt=r("td"),Tl=n("Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Rs=r("em"),yl=n("tokens"),Ml=u(),lt=r("td"),Dl=n("Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Is=r("em"),Cl=n("tokens"),Al=u(),ot=r("td"),Nl=n("Part d\u2019un grand vocabulaire et apprend des r\xE8gles pour supprimer les "),Ls=r("em"),Sl=n("tokens"),Hl=u(),V=r("tr"),$t=r("td"),Bl=n("\xC9tape d\u2019entra\xEEnement"),Ol=u(),xe=r("td"),Ul=n("Fusionne les "),Gs=r("em"),Rl=n("tokens"),Il=n(" correspondant \xE0 la paire la plus commune"),Ll=u(),ee=r("td"),Gl=n("Fusionne les "),Fs=r("em"),Fl=n("tokens"),Vl=n(" correspondant \xE0 la paire ayant le meilleur score bas\xE9 sur la fr\xE9quence de la paire, en privil\xE9giant les paires o\xF9 chaque "),Vs=r("em"),Wl=n("token"),Jl=n(" individuel est moins fr\xE9quent"),Yl=u(),je=r("td"),Kl=n("Supprime tous les "),Ws=r("em"),Ql=n("tokens"),Xl=n(" du vocabulaire qui minimiseront la perte calcul\xE9e sur le corpus entier"),Zl=u(),W=r("tr"),zt=r("td"),eo=n("Apprend"),to=u(),qt=r("td"),so=n("A fusionner des r\xE8gles et un vocabulaire"),no=u(),xt=r("td"),ao=n("Juste un vocabulaire"),ro=u(),it=r("td"),lo=n("Un vocabulaire avec un score pour chaque "),Js=r("em"),oo=n("token"),io=u(),J=r("tr"),jt=r("td"),uo=n("Encodage"),po=u(),Pt=r("td"),co=n("D\xE9coupe un mot en caract\xE8res et applique les fusions apprises pendant l\u2019entra\xEEnement"),mo=u(),wt=r("td"),fo=n("Trouve le plus long sous-mot depuis le d\xE9but qui est dans le vocabulaire puis fait de m\xEAme pour le reste du mot"),ho=u(),Tt=r("td"),vo=n("Trouve la division la plus probable en tokens, en utilisant les scores appris pendant l\u2019entra\xEEnement"),An=u(),yt=r("p"),Eo=n("Maintenant, plongeons dans le BPE !"),this.h()},l(t){const i=ku('[data-svelte="svelte-1phssyn"]',document.head);d=l(i,"META",{name:!0,content:!0}),i.forEach(s),de=p(t),P=l(t,"H1",{class:!0});var ut=o(P);D=l(ut,"A",{id:!0,class:!0,href:!0});var qo=o(D);re=l(qo,"SPAN",{});var xo=o(re);E(M.$$.fragment,xo),xo.forEach(s),qo.forEach(s),Ne=p(ut),le=l(ut,"SPAN",{});var jo=o(le);oe=a(jo,"Normalisation et pr\xE9tokenization"),jo.forEach(s),ut.forEach(s),fe=p(t),E(O.$$.fragment,t),Y=p(t),f=l(t,"P",{});var N=o(f);Se=a(N,"Avant de nous plonger plus profond\xE9ment dans les trois algorithmes de tok\xE9nisation en sous-mots les plus courants utilis\xE9s avec les "),U=l(N,"EM",{});var Po=o(U);He=a(Po,"transformers"),Po.forEach(s),Be=a(N," ("),R=l(N,"EM",{});var wo=o(R);$=a(wo,"Byte-Pair Encoding"),wo.forEach(s),ct=a(N," (BPE), "),he=l(N,"EM",{});var To=o(he);mt=a(To,"WordPiece"),To.forEach(s),dt=a(N," et "),ve=l(N,"EM",{});var yo=o(ve);Vn=a(yo,"Unigram"),yo.forEach(s),Wn=a(N,"), nous allons d\u2019abord examiner le pr\xE9traitement que chaque "),St=l(N,"EM",{});var Mo=o(St);Jn=a(Mo,"tokenizer"),Mo.forEach(s),Yn=a(N," applique au texte. Voici un aper\xE7u de haut niveau des \xE9tapes du pipeline de tokenisation :"),N.forEach(s),Qs=p(t),ie=l(t,"DIV",{class:!0});var Sn=o(ie);Oe=l(Sn,"IMG",{class:!0,src:!0,alt:!0}),Kn=p(Sn),Ue=l(Sn,"IMG",{class:!0,src:!0,alt:!0}),Sn.forEach(s),Xs=p(t),w=l(t,"P",{});var te=o(w);Qn=a(te,"Avant de diviser un texte en sous-"),Ht=l(te,"EM",{});var Do=o(Ht);Xn=a(Do,"tokens"),Do.forEach(s),Zn=a(te," (selon le mod\xE8le), le "),Bt=l(te,"EM",{});var Co=o(Bt);ea=a(Co,"tokenizer"),Co.forEach(s),ta=a(te," effectue deux \xE9tapes : la "),Ot=l(te,"EM",{});var Ao=o(Ot);sa=a(Ao,"normalisation"),Ao.forEach(s),na=a(te," et la "),Ut=l(te,"EM",{});var No=o(Ut);aa=a(No,"pr\xE9tok\xE9nisation"),No.forEach(s),ra=a(te,"."),te.forEach(s),Zs=p(t),ue=l(t,"H2",{class:!0});var Hn=o(ue);Ee=l(Hn,"A",{id:!0,class:!0,href:!0});var So=o(Ee);Rt=l(So,"SPAN",{});var Ho=o(Rt);E(Re.$$.fragment,Ho),Ho.forEach(s),So.forEach(s),la=p(Hn),It=l(Hn,"SPAN",{});var Bo=o(It);oa=a(Bo,"Normalisation"),Bo.forEach(s),Hn.forEach(s),en=p(t),E(Ie.$$.fragment,t),tn=p(t),K=l(t,"P",{});var Mt=o(K);ia=a(Mt,"L\u2019\xE9tape de normalisation implique un nettoyage g\xE9n\xE9ral, comme la suppression des espaces inutiles, la mise en minuscules et/ou la suppression des accents. Si vous \xEAtes familier avec la "),Le=l(Mt,"A",{href:!0,rel:!0});var Oo=o(Le);ua=a(Oo,"normalisation Unicode"),Oo.forEach(s),pa=a(Mt," (comme NFC ou NFKC), c\u2019est aussi quelque chose que le "),Lt=l(Mt,"EM",{});var Uo=o(Lt);ca=a(Uo,"tokenizer"),Uo.forEach(s),ma=a(Mt," peut appliquer."),Mt.forEach(s),sn=p(t),z=l(t,"P",{});var S=o(z);da=a(S,"Le "),Gt=l(S,"CODE",{});var Ro=o(Gt);fa=a(Ro,"tokenizer"),Ro.forEach(s),ha=a(S," de \u{1F917} "),Ft=l(S,"EM",{});var Io=o(Ft);va=a(Io,"Transformers"),Io.forEach(s),Ea=a(S," poss\xE8de un attribut appel\xE9 "),Vt=l(S,"CODE",{});var Lo=o(Vt);ka=a(Lo,"backend_tokenizer"),Lo.forEach(s),_a=a(S," qui donne acc\xE8s au "),Wt=l(S,"EM",{});var Go=o(Wt);ba=a(Go,"tokenizer"),Go.forEach(s),ga=a(S," sous-jacent de la biblioth\xE8que \u{1F917} "),Jt=l(S,"EM",{});var Fo=o(Jt);$a=a(Fo,"Tokenizers"),Fo.forEach(s),za=a(S," :"),S.forEach(s),nn=p(t),E(Ge.$$.fragment,t),an=p(t),E(Fe.$$.fragment,t),rn=p(t),C=l(t,"P",{});var Pe=o(C);qa=a(Pe,"L\u2019attribut "),Yt=l(Pe,"CODE",{});var Vo=o(Yt);xa=a(Vo,"normalizer"),Vo.forEach(s),ja=a(Pe," de l\u2019objet "),Kt=l(Pe,"CODE",{});var Wo=o(Kt);Pa=a(Wo,"tokenizer"),Wo.forEach(s),wa=a(Pe," poss\xE8de une m\xE9thode "),Qt=l(Pe,"CODE",{});var Jo=o(Qt);Ta=a(Jo,"normalize_str()"),Jo.forEach(s),ya=a(Pe," que nous pouvons utiliser pour voir comment la normalisation est effectu\xE9e :"),Pe.forEach(s),ln=p(t),E(Ve.$$.fragment,t),on=p(t),E(We.$$.fragment,t),un=p(t),Q=l(t,"P",{});var Dt=o(Q);Ma=a(Dt,"Dans cet exemple, puisque nous avons choisi le "),Xt=l(Dt,"EM",{});var Yo=o(Xt);Da=a(Yo,"checkpoint"),Yo.forEach(s),Ca=p(Dt),Zt=l(Dt,"CODE",{});var Ko=o(Zt);Aa=a(Ko,"bert-base-uncased"),Ko.forEach(s),Na=a(Dt,", la normalisation a mis le texte en minuscule et supprim\xE9 les accents."),Dt.forEach(s),pn=p(t),E(ke.$$.fragment,t),cn=p(t),pe=l(t,"H2",{class:!0});var Bn=o(pe);_e=l(Bn,"A",{id:!0,class:!0,href:!0});var Qo=o(_e);es=l(Qo,"SPAN",{});var Xo=o(es);E(Je.$$.fragment,Xo),Xo.forEach(s),Qo.forEach(s),Sa=p(Bn),ts=l(Bn,"SPAN",{});var Zo=o(ts);Ha=a(Zo,"Pr\xE9tokenization"),Zo.forEach(s),Bn.forEach(s),mn=p(t),E(Ye.$$.fragment,t),dn=p(t),q=l(t,"P",{});var H=o(q);Ba=a(H,"Comme nous le verrons dans les sections suivantes, un "),ss=l(H,"EM",{});var ei=o(ss);Oa=a(ei,"tokenizer"),ei.forEach(s),Ua=a(H," ne peut pas \xEAtre entra\xEEn\xE9 uniquement sur du texte brut. Au lieu de cela, nous devons d\u2019abord diviser les textes en petites entit\xE9s, comme des mots. C\u2019est l\xE0 qu\u2019intervient l\u2019\xE9tape de pr\xE9tok\xE9nisation. Comme nous l\u2019avons vu dans le "),ft=l(H,"A",{href:!0});var ti=o(ft);Ra=a(ti,"chapitre 2"),ti.forEach(s),Ia=a(H,", un "),ns=l(H,"EM",{});var si=o(ns);La=a(si,"tokenizer"),si.forEach(s),Ga=a(H," bas\xE9 sur les mots peut simplement diviser un texte brut en mots sur les espaces et la ponctuation. Ces mots constitueront les limites des sous-"),as=l(H,"EM",{});var ni=o(as);Fa=a(ni,"tokens"),ni.forEach(s),Va=a(H," que le "),rs=l(H,"EM",{});var ai=o(rs);Wa=a(ai,"tokenizer"),ai.forEach(s),Ja=a(H," peut apprendre pendant son entra\xEEnement."),H.forEach(s),fn=p(t),T=l(t,"P",{});var se=o(T);Ya=a(se,"Pour voir comment un "),ls=l(se,"EM",{});var ri=o(ls);Ka=a(ri,"tokenizer"),ri.forEach(s),Qa=a(se," rapide effectue la pr\xE9tok\xE9nisation, nous pouvons utiliser la m\xE9thode "),os=l(se,"CODE",{});var li=o(os);Xa=a(li,"pre_tokenize_str()"),li.forEach(s),Za=a(se,"de l\u2019attribut "),is=l(se,"CODE",{});var oi=o(is);er=a(oi,"pre_tokenizer"),oi.forEach(s),tr=a(se," de l\u2019objet "),us=l(se,"CODE",{});var ii=o(us);sr=a(ii,"tokenizer"),ii.forEach(s),nr=a(se," :"),se.forEach(s),hn=p(t),E(Ke.$$.fragment,t),vn=p(t),E(Qe.$$.fragment,t),En=p(t),x=l(t,"P",{});var B=o(x);ar=a(B,"Remarquez que le "),ps=l(B,"EM",{});var ui=o(ps);rr=a(ui,"tokenizer"),ui.forEach(s),lr=a(B," garde d\xE9j\xE0 la trace des "),cs=l(B,"EM",{});var pi=o(cs);or=a(pi,"offsets"),pi.forEach(s),ir=a(B,", ce qui lui permet de nous donner la correspondance des d\xE9calages que nous avons utilis\xE9e dans la section pr\xE9c\xE9dente. Ici, le "),ms=l(B,"EM",{});var ci=o(ms);ur=a(ci,"tokenizer"),ci.forEach(s),pr=a(B," ignore les deux espaces et les remplace par un seul, mais le d\xE9calage saute entre "),ds=l(B,"CODE",{});var mi=o(ds);cr=a(mi,"are"),mi.forEach(s),mr=a(B," et "),fs=l(B,"CODE",{});var di=o(fs);dr=a(di,"you"),di.forEach(s),fr=a(B," pour en tenir compte."),B.forEach(s),kn=p(t),A=l(t,"P",{});var we=o(A);hr=a(we,"Puisque nous utilisons le "),hs=l(we,"EM",{});var fi=o(hs);vr=a(fi,"tokenizer"),fi.forEach(s),Er=a(we," de BERT, la pr\xE9tok\xE9nisation implique la s\xE9paration des espaces et de la ponctuation. D\u2019autres "),vs=l(we,"EM",{});var hi=o(vs);kr=a(hi,"tokenizers"),hi.forEach(s),_r=a(we," peuvent avoir des r\xE8gles diff\xE9rentes pour cette \xE9tape. Par exemple, si nous utilisons le "),Es=l(we,"EM",{});var vi=o(Es);br=a(vi,"tokenizer"),vi.forEach(s),gr=a(we," du GPT-2 :"),we.forEach(s),_n=p(t),E(Xe.$$.fragment,t),bn=p(t),X=l(t,"P",{});var Ct=o(X);$r=a(Ct,"Il s\xE9parera aussi sur les espaces et la ponctuation mais gardera les espaces et les remplacera par un symbole "),ks=l(Ct,"CODE",{});var Ei=o(ks);zr=a(Ei,"\u0120"),Ei.forEach(s),qr=a(Ct,", ce qui lui permettra de r\xE9cup\xE9rer les espaces originaux si nous d\xE9codons les "),_s=l(Ct,"EM",{});var ki=o(_s);xr=a(ki,"tokens"),ki.forEach(s),jr=a(Ct," :"),Ct.forEach(s),gn=p(t),E(Ze.$$.fragment,t),$n=p(t),Z=l(t,"P",{});var At=o(Z);Pr=a(At,"Notez \xE9galement que, contrairement au "),bs=l(At,"EM",{});var _i=o(bs);wr=a(_i,"tokenizer"),_i.forEach(s),Tr=a(At," de BERT, ce "),gs=l(At,"EM",{});var bi=o(gs);yr=a(bi,"tokenizer"),bi.forEach(s),Mr=a(At," n\u2019ignore pas les doubles espaces."),At.forEach(s),zn=p(t),be=l(t,"P",{});var On=o(be);Dr=a(On,"Pour un dernier exemple, regardons le "),$s=l(On,"EM",{});var gi=o($s);Cr=a(gi,"tokenizer"),gi.forEach(s),Ar=a(On," du 5, qui est bas\xE9 sur l\u2019algorithme SentencePiece :"),On.forEach(s),qn=p(t),E(et.$$.fragment,t),xn=p(t),E(tt.$$.fragment,t),jn=p(t),h=l(t,"P",{});var j=o(h);Nr=a(j,"Comme le "),zs=l(j,"EM",{});var $i=o(zs);Sr=a($i,"tokenizer"),$i.forEach(s),Hr=a(j," du GPT-2, celui-ci garde les espaces et les remplace par un "),qs=l(j,"EM",{});var zi=o(qs);Br=a(zi,"token"),zi.forEach(s),Or=a(j," sp\xE9cifique ("),xs=l(j,"CODE",{});var qi=o(xs);Ur=a(qi,"_"),qi.forEach(s),Rr=a(j,"), mais le "),js=l(j,"EM",{});var xi=o(js);Ir=a(xi,"tokenizer"),xi.forEach(s),Lr=a(j," du T5 ne s\xE9pare que sur les espaces, pas sur la ponctuation. Notez \xE9galement qu\u2019il a ajout\xE9 un espace par d\xE9faut au d\xE9but de la phrase (avant "),Ps=l(j,"CODE",{});var ji=o(Ps);Gr=a(ji,"Hello"),ji.forEach(s),Fr=a(j,") et il ignore le double espace entre "),ws=l(j,"CODE",{});var Pi=o(ws);Vr=a(Pi,"are"),Pi.forEach(s),Wr=a(j," et "),Ts=l(j,"CODE",{});var wi=o(Ts);Jr=a(wi,"you"),wi.forEach(s),Yr=a(j,"."),j.forEach(s),Pn=p(t),ge=l(t,"P",{});var Un=o(ge);Kr=a(Un,"Maintenant que nous avons vu un peu comment les diff\xE9rents "),ys=l(Un,"EM",{});var Ti=o(ys);Qr=a(Ti,"tokenizers"),Ti.forEach(s),Xr=a(Un," traitent le texte, nous pouvons commencer \xE0 explorer les algorithmes sous-jacents eux-m\xEAmes. Nous commencerons par jeter un coup d\u2019oeil rapide sur le tr\xE8s r\xE9pandu SentencePiece, puis au cours des trois sections suivantes nous examinerons le fonctionnement des trois principaux algorithmes utilis\xE9s pour la tokenisation en sous-mots."),Un.forEach(s),wn=p(t),ce=l(t,"H2",{class:!0});var Rn=o(ce);$e=l(Rn,"A",{id:!0,class:!0,href:!0});var yi=o($e);Ms=l(yi,"SPAN",{});var Mi=o(Ms);E(st.$$.fragment,Mi),Mi.forEach(s),yi.forEach(s),Zr=p(Rn),Ds=l(Rn,"SPAN",{});var Di=o(Ds);el=a(Di,"SentencePiece"),Di.forEach(s),Rn.forEach(s),Tn=p(t),I=l(t,"P",{});var pt=o(I);nt=l(pt,"A",{href:!0,rel:!0});var Ci=o(nt);tl=a(Ci,"SentencePiece"),Ci.forEach(s),sl=a(pt," est un algorithme de tokenisation pour le pr\xE9traitement du texte que vous pouvez utiliser avec n\u2019importe lequel des mod\xE8les que nous verrons dans les trois prochaines sections. Il consid\xE8re le texte comme une s\xE9quence de caract\xE8res Unicode et il remplace les espaces par un caract\xE8re sp\xE9cial : "),Cs=l(pt,"CODE",{});var Ai=o(Cs);nl=a(Ai,"\u2581"),Ai.forEach(s),al=a(pt,". Utilis\xE9 en conjonction avec l\u2019algorithme Unigram (voir la "),ht=l(pt,"A",{href:!0});var Ni=o(ht);rl=a(Ni,"section 7"),Ni.forEach(s),ll=a(pt,"), il ne n\xE9cessite m\xEAme pas d\u2019\xE9tape de pr\xE9tok\xE9nisation, ce qui est tr\xE8s utile pour les langues o\xF9 le caract\xE8re espace n\u2019est pas utilis\xE9 (comme le chinois ou le japonais)."),pt.forEach(s),yn=p(t),y=l(t,"P",{});var ne=o(y);ol=a(ne,"L\u2019autre caract\xE9ristique principale de SentencePiece est le "),As=l(ne,"EM",{});var Si=o(As);il=a(Si,"tokenisation r\xE9versible"),Si.forEach(s),ul=a(ne," : comme il n\u2019y a pas de traitement sp\xE9cial des espaces, le d\xE9codage des "),Ns=l(ne,"EM",{});var Hi=o(Ns);pl=a(Hi,"tokens"),Hi.forEach(s),cl=a(ne," se fait simplement en les concat\xE9nant et en rempla\xE7ant les "),Ss=l(ne,"CODE",{});var Bi=o(Ss);ml=a(Bi,"_"),Bi.forEach(s),dl=a(ne," par des espaces, ce qui donne le texte normalis\xE9. Comme nous l\u2019avons vu pr\xE9c\xE9demment, le "),Hs=l(ne,"EM",{});var Oi=o(Hs);fl=a(Oi,"tokenizer"),Oi.forEach(s),hl=a(ne," de BERT supprime les espaces r\xE9p\xE9titifs, donc sa tokenisation n\u2019est pas r\xE9versible."),ne.forEach(s),Mn=p(t),me=l(t,"H2",{class:!0});var In=o(me);ze=l(In,"A",{id:!0,class:!0,href:!0});var Ui=o(ze);Bs=l(Ui,"SPAN",{});var Ri=o(Bs);E(at.$$.fragment,Ri),Ri.forEach(s),Ui.forEach(s),vl=p(In),Os=l(In,"SPAN",{});var Ii=o(Os);El=a(Ii,"Vue d'ensemble des algorithmes"),Ii.forEach(s),In.forEach(s),Dn=p(t),vt=l(t,"P",{});var Li=o(vt);kl=a(Li,"Dans les sections suivantes, nous allons nous plonger dans les trois principaux algorithmes de tokenisation en sous-mots : BPE (utilis\xE9 par GPT-2 et autres), WordPiece (utilis\xE9 par exemple par BERT), et Unigram (utilis\xE9 par T5 et autres). Avant de commencer, voici un rapide aper\xE7u du fonctionnement de chacun d\u2019entre eux. N\u2019h\xE9sitez pas \xE0 revenir \xE0 ce tableau apr\xE8s avoir lu chacune des sections suivantes si cela n\u2019a pas encore de sens pour vous."),Li.forEach(s),Cn=p(t),qe=l(t,"TABLE",{});var Ln=o(qe);Us=l(Ln,"THEAD",{});var Gi=o(Us);L=l(Gi,"TR",{});var Te=o(L);Et=l(Te,"TH",{align:!0});var Fi=o(Et);_l=a(Fi,"Mod\xE8le"),Fi.forEach(s),bl=p(Te),kt=l(Te,"TH",{align:!0});var Vi=o(kt);gl=a(Vi,"BPE"),Vi.forEach(s),$l=p(Te),_t=l(Te,"TH",{align:!0});var Wi=o(_t);zl=a(Wi,"WordPiece"),Wi.forEach(s),ql=p(Te),bt=l(Te,"TH",{align:!0});var Ji=o(bt);xl=a(Ji,"Unigramme"),Ji.forEach(s),Te.forEach(s),Gi.forEach(s),jl=p(Ln),G=l(Ln,"TBODY",{});var ye=o(G);F=l(ye,"TR",{});var Me=o(F);gt=l(Me,"TD",{align:!0});var Yi=o(gt);Pl=a(Yi,"Entra\xEEnement"),Yi.forEach(s),wl=p(Me),rt=l(Me,"TD",{align:!0});var ko=o(rt);Tl=a(ko,"Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Rs=l(ko,"EM",{});var Ki=o(Rs);yl=a(Ki,"tokens"),Ki.forEach(s),ko.forEach(s),Ml=p(Me),lt=l(Me,"TD",{align:!0});var _o=o(lt);Dl=a(_o,"Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Is=l(_o,"EM",{});var Qi=o(Is);Cl=a(Qi,"tokens"),Qi.forEach(s),_o.forEach(s),Al=p(Me),ot=l(Me,"TD",{align:!0});var bo=o(ot);Nl=a(bo,"Part d\u2019un grand vocabulaire et apprend des r\xE8gles pour supprimer les "),Ls=l(bo,"EM",{});var Xi=o(Ls);Sl=a(Xi,"tokens"),Xi.forEach(s),bo.forEach(s),Me.forEach(s),Hl=p(ye),V=l(ye,"TR",{});var De=o(V);$t=l(De,"TD",{align:!0});var Zi=o($t);Bl=a(Zi,"\xC9tape d\u2019entra\xEEnement"),Zi.forEach(s),Ol=p(De),xe=l(De,"TD",{align:!0});var Gn=o(xe);Ul=a(Gn,"Fusionne les "),Gs=l(Gn,"EM",{});var eu=o(Gs);Rl=a(eu,"tokens"),eu.forEach(s),Il=a(Gn," correspondant \xE0 la paire la plus commune"),Gn.forEach(s),Ll=p(De),ee=l(De,"TD",{align:!0});var Nt=o(ee);Gl=a(Nt,"Fusionne les "),Fs=l(Nt,"EM",{});var tu=o(Fs);Fl=a(tu,"tokens"),tu.forEach(s),Vl=a(Nt," correspondant \xE0 la paire ayant le meilleur score bas\xE9 sur la fr\xE9quence de la paire, en privil\xE9giant les paires o\xF9 chaque "),Vs=l(Nt,"EM",{});var su=o(Vs);Wl=a(su,"token"),su.forEach(s),Jl=a(Nt," individuel est moins fr\xE9quent"),Nt.forEach(s),Yl=p(De),je=l(De,"TD",{align:!0});var Fn=o(je);Kl=a(Fn,"Supprime tous les "),Ws=l(Fn,"EM",{});var nu=o(Ws);Ql=a(nu,"tokens"),nu.forEach(s),Xl=a(Fn," du vocabulaire qui minimiseront la perte calcul\xE9e sur le corpus entier"),Fn.forEach(s),De.forEach(s),Zl=p(ye),W=l(ye,"TR",{});var Ce=o(W);zt=l(Ce,"TD",{align:!0});var au=o(zt);eo=a(au,"Apprend"),au.forEach(s),to=p(Ce),qt=l(Ce,"TD",{align:!0});var ru=o(qt);so=a(ru,"A fusionner des r\xE8gles et un vocabulaire"),ru.forEach(s),no=p(Ce),xt=l(Ce,"TD",{align:!0});var lu=o(xt);ao=a(lu,"Juste un vocabulaire"),lu.forEach(s),ro=p(Ce),it=l(Ce,"TD",{align:!0});var go=o(it);lo=a(go,"Un vocabulaire avec un score pour chaque "),Js=l(go,"EM",{});var ou=o(Js);oo=a(ou,"token"),ou.forEach(s),go.forEach(s),Ce.forEach(s),io=p(ye),J=l(ye,"TR",{});var Ae=o(J);jt=l(Ae,"TD",{align:!0});var iu=o(jt);uo=a(iu,"Encodage"),iu.forEach(s),po=p(Ae),Pt=l(Ae,"TD",{align:!0});var uu=o(Pt);co=a(uu,"D\xE9coupe un mot en caract\xE8res et applique les fusions apprises pendant l\u2019entra\xEEnement"),uu.forEach(s),mo=p(Ae),wt=l(Ae,"TD",{align:!0});var pu=o(wt);fo=a(pu,"Trouve le plus long sous-mot depuis le d\xE9but qui est dans le vocabulaire puis fait de m\xEAme pour le reste du mot"),pu.forEach(s),ho=p(Ae),Tt=l(Ae,"TD",{align:!0});var cu=o(Tt);vo=a(cu,"Trouve la division la plus probable en tokens, en utilisant les scores appris pendant l\u2019entra\xEEnement"),cu.forEach(s),Ae.forEach(s),ye.forEach(s),Ln.forEach(s),An=p(t),yt=l(t,"P",{});var mu=o(yt);Eo=a(mu,"Maintenant, plongeons dans le BPE !"),mu.forEach(s),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(qu)),m(D,"id","normalisation-et-prtokenization"),m(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(D,"href","#normalisation-et-prtokenization"),m(P,"class","relative group"),m(Oe,"class","block dark:hidden"),du(Oe.src,$o="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||m(Oe,"src",$o),m(Oe,"alt","The tokenization pipeline."),m(Ue,"class","hidden dark:block"),du(Ue.src,zo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||m(Ue,"src",zo),m(Ue,"alt","The tokenization pipeline."),m(ie,"class","flex justify-center"),m(Ee,"id","normalisation"),m(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ee,"href","#normalisation"),m(ue,"class","relative group"),m(Le,"href","http://www.unicode.org/reports/tr15/"),m(Le,"rel","nofollow"),m(_e,"id","prtokenization"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#prtokenization"),m(pe,"class","relative group"),m(ft,"href","/course/fr/chapter2"),m($e,"id","sentencepiece"),m($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m($e,"href","#sentencepiece"),m(ce,"class","relative group"),m(nt,"href","https://github.com/google/sentencepiece"),m(nt,"rel","nofollow"),m(ht,"href","/course/fr/chapter7/7"),m(ze,"id","vue-densemble-des-algorithmes"),m(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ze,"href","#vue-densemble-des-algorithmes"),m(me,"class","relative group"),m(Et,"align","center"),m(kt,"align","center"),m(_t,"align","center"),m(bt,"align","center"),m(gt,"align","center"),m(rt,"align","center"),m(lt,"align","center"),m(ot,"align","center"),m($t,"align","center"),m(xe,"align","center"),m(ee,"align","center"),m(je,"align","center"),m(zt,"align","center"),m(qt,"align","center"),m(xt,"align","center"),m(it,"align","center"),m(jt,"align","center"),m(Pt,"align","center"),m(wt,"align","center"),m(Tt,"align","center")},m(t,i){e(document.head,d),c(t,de,i),c(t,P,i),e(P,D),e(D,re),k(M,re,null),e(P,Ne),e(P,le),e(le,oe),c(t,fe,i),k(O,t,i),c(t,Y,i),c(t,f,i),e(f,Se),e(f,U),e(U,He),e(f,Be),e(f,R),e(R,$),e(f,ct),e(f,he),e(he,mt),e(f,dt),e(f,ve),e(ve,Vn),e(f,Wn),e(f,St),e(St,Jn),e(f,Yn),c(t,Qs,i),c(t,ie,i),e(ie,Oe),e(ie,Kn),e(ie,Ue),c(t,Xs,i),c(t,w,i),e(w,Qn),e(w,Ht),e(Ht,Xn),e(w,Zn),e(w,Bt),e(Bt,ea),e(w,ta),e(w,Ot),e(Ot,sa),e(w,na),e(w,Ut),e(Ut,aa),e(w,ra),c(t,Zs,i),c(t,ue,i),e(ue,Ee),e(Ee,Rt),k(Re,Rt,null),e(ue,la),e(ue,It),e(It,oa),c(t,en,i),k(Ie,t,i),c(t,tn,i),c(t,K,i),e(K,ia),e(K,Le),e(Le,ua),e(K,pa),e(K,Lt),e(Lt,ca),e(K,ma),c(t,sn,i),c(t,z,i),e(z,da),e(z,Gt),e(Gt,fa),e(z,ha),e(z,Ft),e(Ft,va),e(z,Ea),e(z,Vt),e(Vt,ka),e(z,_a),e(z,Wt),e(Wt,ba),e(z,ga),e(z,Jt),e(Jt,$a),e(z,za),c(t,nn,i),k(Ge,t,i),c(t,an,i),k(Fe,t,i),c(t,rn,i),c(t,C,i),e(C,qa),e(C,Yt),e(Yt,xa),e(C,ja),e(C,Kt),e(Kt,Pa),e(C,wa),e(C,Qt),e(Qt,Ta),e(C,ya),c(t,ln,i),k(Ve,t,i),c(t,on,i),k(We,t,i),c(t,un,i),c(t,Q,i),e(Q,Ma),e(Q,Xt),e(Xt,Da),e(Q,Ca),e(Q,Zt),e(Zt,Aa),e(Q,Na),c(t,pn,i),k(ke,t,i),c(t,cn,i),c(t,pe,i),e(pe,_e),e(_e,es),k(Je,es,null),e(pe,Sa),e(pe,ts),e(ts,Ha),c(t,mn,i),k(Ye,t,i),c(t,dn,i),c(t,q,i),e(q,Ba),e(q,ss),e(ss,Oa),e(q,Ua),e(q,ft),e(ft,Ra),e(q,Ia),e(q,ns),e(ns,La),e(q,Ga),e(q,as),e(as,Fa),e(q,Va),e(q,rs),e(rs,Wa),e(q,Ja),c(t,fn,i),c(t,T,i),e(T,Ya),e(T,ls),e(ls,Ka),e(T,Qa),e(T,os),e(os,Xa),e(T,Za),e(T,is),e(is,er),e(T,tr),e(T,us),e(us,sr),e(T,nr),c(t,hn,i),k(Ke,t,i),c(t,vn,i),k(Qe,t,i),c(t,En,i),c(t,x,i),e(x,ar),e(x,ps),e(ps,rr),e(x,lr),e(x,cs),e(cs,or),e(x,ir),e(x,ms),e(ms,ur),e(x,pr),e(x,ds),e(ds,cr),e(x,mr),e(x,fs),e(fs,dr),e(x,fr),c(t,kn,i),c(t,A,i),e(A,hr),e(A,hs),e(hs,vr),e(A,Er),e(A,vs),e(vs,kr),e(A,_r),e(A,Es),e(Es,br),e(A,gr),c(t,_n,i),k(Xe,t,i),c(t,bn,i),c(t,X,i),e(X,$r),e(X,ks),e(ks,zr),e(X,qr),e(X,_s),e(_s,xr),e(X,jr),c(t,gn,i),k(Ze,t,i),c(t,$n,i),c(t,Z,i),e(Z,Pr),e(Z,bs),e(bs,wr),e(Z,Tr),e(Z,gs),e(gs,yr),e(Z,Mr),c(t,zn,i),c(t,be,i),e(be,Dr),e(be,$s),e($s,Cr),e(be,Ar),c(t,qn,i),k(et,t,i),c(t,xn,i),k(tt,t,i),c(t,jn,i),c(t,h,i),e(h,Nr),e(h,zs),e(zs,Sr),e(h,Hr),e(h,qs),e(qs,Br),e(h,Or),e(h,xs),e(xs,Ur),e(h,Rr),e(h,js),e(js,Ir),e(h,Lr),e(h,Ps),e(Ps,Gr),e(h,Fr),e(h,ws),e(ws,Vr),e(h,Wr),e(h,Ts),e(Ts,Jr),e(h,Yr),c(t,Pn,i),c(t,ge,i),e(ge,Kr),e(ge,ys),e(ys,Qr),e(ge,Xr),c(t,wn,i),c(t,ce,i),e(ce,$e),e($e,Ms),k(st,Ms,null),e(ce,Zr),e(ce,Ds),e(Ds,el),c(t,Tn,i),c(t,I,i),e(I,nt),e(nt,tl),e(I,sl),e(I,Cs),e(Cs,nl),e(I,al),e(I,ht),e(ht,rl),e(I,ll),c(t,yn,i),c(t,y,i),e(y,ol),e(y,As),e(As,il),e(y,ul),e(y,Ns),e(Ns,pl),e(y,cl),e(y,Ss),e(Ss,ml),e(y,dl),e(y,Hs),e(Hs,fl),e(y,hl),c(t,Mn,i),c(t,me,i),e(me,ze),e(ze,Bs),k(at,Bs,null),e(me,vl),e(me,Os),e(Os,El),c(t,Dn,i),c(t,vt,i),e(vt,kl),c(t,Cn,i),c(t,qe,i),e(qe,Us),e(Us,L),e(L,Et),e(Et,_l),e(L,bl),e(L,kt),e(kt,gl),e(L,$l),e(L,_t),e(_t,zl),e(L,ql),e(L,bt),e(bt,xl),e(qe,jl),e(qe,G),e(G,F),e(F,gt),e(gt,Pl),e(F,wl),e(F,rt),e(rt,Tl),e(rt,Rs),e(Rs,yl),e(F,Ml),e(F,lt),e(lt,Dl),e(lt,Is),e(Is,Cl),e(F,Al),e(F,ot),e(ot,Nl),e(ot,Ls),e(Ls,Sl),e(G,Hl),e(G,V),e(V,$t),e($t,Bl),e(V,Ol),e(V,xe),e(xe,Ul),e(xe,Gs),e(Gs,Rl),e(xe,Il),e(V,Ll),e(V,ee),e(ee,Gl),e(ee,Fs),e(Fs,Fl),e(ee,Vl),e(ee,Vs),e(Vs,Wl),e(ee,Jl),e(V,Yl),e(V,je),e(je,Kl),e(je,Ws),e(Ws,Ql),e(je,Xl),e(G,Zl),e(G,W),e(W,zt),e(zt,eo),e(W,to),e(W,qt),e(qt,so),e(W,no),e(W,xt),e(xt,ao),e(W,ro),e(W,it),e(it,lo),e(it,Js),e(Js,oo),e(G,io),e(G,J),e(J,jt),e(jt,uo),e(J,po),e(J,Pt),e(Pt,co),e(J,mo),e(J,wt),e(wt,fo),e(J,ho),e(J,Tt),e(Tt,vo),c(t,An,i),c(t,yt,i),e(yt,Eo),Nn=!0},p(t,[i]){const ut={};i&2&&(ut.$$scope={dirty:i,ctx:t}),ke.$set(ut)},i(t){Nn||(_(M.$$.fragment,t),_(O.$$.fragment,t),_(Re.$$.fragment,t),_(Ie.$$.fragment,t),_(Ge.$$.fragment,t),_(Fe.$$.fragment,t),_(Ve.$$.fragment,t),_(We.$$.fragment,t),_(ke.$$.fragment,t),_(Je.$$.fragment,t),_(Ye.$$.fragment,t),_(Ke.$$.fragment,t),_(Qe.$$.fragment,t),_(Xe.$$.fragment,t),_(Ze.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(st.$$.fragment,t),_(at.$$.fragment,t),Nn=!0)},o(t){b(M.$$.fragment,t),b(O.$$.fragment,t),b(Re.$$.fragment,t),b(Ie.$$.fragment,t),b(Ge.$$.fragment,t),b(Fe.$$.fragment,t),b(Ve.$$.fragment,t),b(We.$$.fragment,t),b(ke.$$.fragment,t),b(Je.$$.fragment,t),b(Ye.$$.fragment,t),b(Ke.$$.fragment,t),b(Qe.$$.fragment,t),b(Xe.$$.fragment,t),b(Ze.$$.fragment,t),b(et.$$.fragment,t),b(tt.$$.fragment,t),b(st.$$.fragment,t),b(at.$$.fragment,t),Nn=!1},d(t){s(d),t&&s(de),t&&s(P),g(M),t&&s(fe),g(O,t),t&&s(Y),t&&s(f),t&&s(Qs),t&&s(ie),t&&s(Xs),t&&s(w),t&&s(Zs),t&&s(ue),g(Re),t&&s(en),g(Ie,t),t&&s(tn),t&&s(K),t&&s(sn),t&&s(z),t&&s(nn),g(Ge,t),t&&s(an),g(Fe,t),t&&s(rn),t&&s(C),t&&s(ln),g(Ve,t),t&&s(on),g(We,t),t&&s(un),t&&s(Q),t&&s(pn),g(ke,t),t&&s(cn),t&&s(pe),g(Je),t&&s(mn),g(Ye,t),t&&s(dn),t&&s(q),t&&s(fn),t&&s(T),t&&s(hn),g(Ke,t),t&&s(vn),g(Qe,t),t&&s(En),t&&s(x),t&&s(kn),t&&s(A),t&&s(_n),g(Xe,t),t&&s(bn),t&&s(X),t&&s(gn),g(Ze,t),t&&s($n),t&&s(Z),t&&s(zn),t&&s(be),t&&s(qn),g(et,t),t&&s(xn),g(tt,t),t&&s(jn),t&&s(h),t&&s(Pn),t&&s(ge),t&&s(wn),t&&s(ce),g(st),t&&s(Tn),t&&s(I),t&&s(yn),t&&s(y),t&&s(Mn),t&&s(me),g(at),t&&s(Dn),t&&s(vt),t&&s(Cn),t&&s(qe),t&&s(An),t&&s(yt)}}}const qu={local:"normalisation-et-prtokenization",sections:[{local:"normalisation",title:"Normalisation"},{local:"prtokenization",title:"Pr\xE9tokenization"},{local:"sentencepiece",title:"SentencePiece"},{local:"vue-densemble-des-algorithmes",title:"Vue d'ensemble des algorithmes"}],title:"Normalisation et pr\xE9tokenization"};function xu(Ks){return _u(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Du extends hu{constructor(d){super();vu(this,d,xu,zu,Eu,{})}}export{Du as default,qu as metadata};
