import{S as kr,i as gr,s as xr,e as s,k as f,w as b,t as N,l as $r,M as _r,c as o,d as t,m,x as w,a,h as L,b as l,F as r,g as d,y as A,o as k,p as vr,q as g,B as y,v as br,n as qr}from"../../chunks/vendor-1e8b365d.js";import{I as M}from"../../chunks/IconCopyLink-483c28ba.js";import{C as Ut}from"../../chunks/CodeBlock-e5764662.js";import{Q as T}from"../../chunks/Question-31426fe2.js";import{F as wr}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Ar(H){let c,p,v,h,E,x,S,z,q,P;return h=new M({}),q=new T({props:{choices:[{text:"Un mod\xE8le qui s'entra\xEEne automatiquement sur vos donn\xE9es",explain:"Incorrect. Vous confondez cela avec notre produit <a href='https://huggingface.co/autotrain>AutoTrain</a>"},{text:"Un objet qui renvoie la bonne architecture bas\xE9e sur le <i>checkpoint</i> .",explain:"Exactement : <code>TFAutoModel</code> a seulement besoin de conna\xEEtre le <i>checkpoint</i> \xE0 partir duquel il doit s'initialiser pour retourner \xE0 la bonne architecture.",correct:!0},{text:"Un mod\xE8le qui d\xE9tecte automatiquement la langue utilis\xE9e pour ses entr\xE9es afin de charger les bonnes pond\xE9rations.",explain:"Incorrect. Bien que certains <i>checkpoints</i> et mod\xE8les soient capables de g\xE9rer plusieurs langues, il n'existe pas d'outils int\xE9gr\xE9s pour la s\xE9lection automatique des <i>checkpoints</i> en fonction de la langue. Vous devez vous rendre sur le <a href='https://huggingface.co/models'>Hub des mod\xE8les</a> pour trouver le meilleur <i>checkpoint</i> pour votre t\xE2che !"}]}}),{c(){c=s("h3"),p=s("a"),v=s("span"),b(h.$$.fragment),E=f(),x=s("span"),S=N("5. What is an AutoModel?"),z=f(),b(q.$$.fragment),this.h()},l(i){c=o(i,"H3",{class:!0});var _=a(c);p=o(_,"A",{id:!0,class:!0,href:!0});var n=a(p);v=o(n,"SPAN",{});var $=a(v);w(h.$$.fragment,$),$.forEach(t),n.forEach(t),E=m(_),x=o(_,"SPAN",{});var j=a(x);S=L(j,"5. What is an AutoModel?"),j.forEach(t),_.forEach(t),z=m(i),w(q.$$.fragment,i),this.h()},h(){l(p,"id","5.-what-is-an-automodel?"),l(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(p,"href","#5.-what-is-an-automodel?"),l(c,"class","relative group")},m(i,_){d(i,c,_),r(c,p),r(p,v),A(h,v,null),r(c,E),r(c,x),r(x,S),d(i,z,_),A(q,i,_),P=!0},i(i){P||(g(h.$$.fragment,i),g(q.$$.fragment,i),P=!0)},o(i){k(h.$$.fragment,i),k(q.$$.fragment,i),P=!1},d(i){i&&t(c),y(h),i&&t(z),y(q,i)}}}function yr(H){let c,p,v,h,E,x,S,z,q,P;return h=new M({}),q=new T({props:{choices:[{text:"Un mod\xE8le qui s'entra\xEEne automatiquement sur vos donn\xE9es",explain:"Incorrect. Vous confondez cela avec notre produit <a href='https://huggingface.co/autotrain>AutoTrain</a>"},{text:"Un objet qui renvoie la bonne architecture bas\xE9e sur le <i>checkpoint</i> .",explain:"Exactement : <code>AutoModel</code> a seulement besoin de conna\xEEtre le <i>checkpoint</i> \xE0 partir duquel il doit s'initialiser pour retourner \xE0 la bonne architecture.",correct:!0},{text:"Un mod\xE8le qui d\xE9tecte automatiquement la langue utilis\xE9e pour ses entr\xE9es afin de charger les bonnes pond\xE9rations.",explain:"Incorrect. Bien que certains <i>checkpoints</i> et mod\xE8les soient capables de g\xE9rer plusieurs langues, il n'existe pas d'outils int\xE9gr\xE9s pour la s\xE9lection automatique des <i>checkpoints</i> en fonction de la langue. Vous devez vous rendre sur le <a href='https://huggingface.co/models'>Hub des mod\xE8les</a> pour trouver le meilleur <i>checkpoint</i> pour votre t\xE2che !"}]}}),{c(){c=s("h3"),p=s("a"),v=s("span"),b(h.$$.fragment),E=f(),x=s("span"),S=N("5. Qu\u2019est-ce qu\u2019un AutoModel?"),z=f(),b(q.$$.fragment),this.h()},l(i){c=o(i,"H3",{class:!0});var _=a(c);p=o(_,"A",{id:!0,class:!0,href:!0});var n=a(p);v=o(n,"SPAN",{});var $=a(v);w(h.$$.fragment,$),$.forEach(t),n.forEach(t),E=m(_),x=o(_,"SPAN",{});var j=a(x);S=L(j,"5. Qu\u2019est-ce qu\u2019un AutoModel?"),j.forEach(t),_.forEach(t),z=m(i),w(q.$$.fragment,i),this.h()},h(){l(p,"id","5.-qu\u2019est-ce-qu\u2019un-automodel?"),l(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(p,"href","#5.-qu\u2019est-ce-qu\u2019un-automodel?"),l(c,"class","relative group")},m(i,_){d(i,c,_),r(c,p),r(p,v),A(h,v,null),r(c,E),r(c,x),r(x,S),d(i,z,_),A(q,i,_),P=!0},i(i){P||(g(h.$$.fragment,i),g(q.$$.fragment,i),P=!0)},o(i){k(h.$$.fragment,i),k(q.$$.fragment,i),P=!1},d(i){i&&t(c),y(h),i&&t(z),y(q,i)}}}function zr(H){let c,p,v,h,E,x,S,z,q,P,i,_;return h=new M({}),q=new Ut({props:{code:`from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),i=new T({props:{choices:[{text:"Non, \xE7a semble correct.",explain:"Malheureusement, coupler un mod\xE8le avec un <i>tokenizer</i> qui a \xE9t\xE9 entra\xEEn\xE9 avec un <i>checkpoint</i> diff\xE9rent est rarement une bonne id\xE9e. Le mod\xE8le n'a pas \xE9t\xE9 entra\xEEn\xE9 pour donner du sens \xE0 la sortie de ce <i>tokenizer</i> donc la sortie du mod\xE8le (s'il peut m\xEAme fonctionner !) n'aura aucun sens."},{text:" Le <i>tokenizer</i> et le mod\xE8le doivent toujours provenir du m\xEAme <i>checkpoint</i>.",explain:"C\u2019est juste !",correct:!0},{text:" C'est une bonne pratique de faire du <i>padding</i> et de troncage avec le <i>tokenizer</i> car chaque entr\xE9e est un batch.",explain:"Il est vrai que chaque entr\xE9e de mod\xE8le doit \xEAtre un batch. Cependant, tronquer ou compl\xE9ter cette s\xE9quence n'aurait pas n\xE9cessairement de sens puisqu'il n'y en a qu'une seule. Il s'agit l\xE0 de techniques permettant de mettre en batch une liste de phrases."}]}}),{c(){c=s("h3"),p=s("a"),v=s("span"),b(h.$$.fragment),E=f(),x=s("span"),S=N("10. Y a-t-il un probl\xE8me avec le code suivant ?"),z=f(),b(q.$$.fragment),P=f(),b(i.$$.fragment),this.h()},l(n){c=o(n,"H3",{class:!0});var $=a(c);p=o($,"A",{id:!0,class:!0,href:!0});var j=a(p);v=o(j,"SPAN",{});var C=a(v);w(h.$$.fragment,C),C.forEach(t),j.forEach(t),E=m($),x=o($,"SPAN",{});var O=a(x);S=L(O,"10. Y a-t-il un probl\xE8me avec le code suivant ?"),O.forEach(t),$.forEach(t),z=m(n),w(q.$$.fragment,n),P=m(n),w(i.$$.fragment,n),this.h()},h(){l(p,"id","10.-y-a-t-il-un-probl\xE8me-avec-le-code-suivant-?"),l(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(p,"href","#10.-y-a-t-il-un-probl\xE8me-avec-le-code-suivant-?"),l(c,"class","relative group")},m(n,$){d(n,c,$),r(c,p),r(p,v),A(h,v,null),r(c,E),r(c,x),r(x,S),d(n,z,$),A(q,n,$),d(n,P,$),A(i,n,$),_=!0},i(n){_||(g(h.$$.fragment,n),g(q.$$.fragment,n),g(i.$$.fragment,n),_=!0)},o(n){k(h.$$.fragment,n),k(q.$$.fragment,n),k(i.$$.fragment,n),_=!1},d(n){n&&t(c),y(h),n&&t(z),y(q,n),n&&t(P),y(i,n)}}}function Er(H){let c,p,v,h,E,x,S,z,q,P,i,_;return h=new M({}),q=new Ut({props:{code:`from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),i=new T({props:{choices:[{text:"Non, \xE7a semble correct.",explain:"Malheureusement, coupler un mod\xE8le avec un <i>tokenizer</i> qui a \xE9t\xE9 entra\xEEn\xE9 avec un <i>checkpoint</i> diff\xE9rent est rarement une bonne id\xE9e. Le mod\xE8le n'a pas \xE9t\xE9 entra\xEEn\xE9 pour donner du sens \xE0 la sortie de ce <i>tokenizer</i> donc la sortie du mod\xE8le (s'il peut m\xEAme fonctionner !) n'aura aucun sens."},{text:" Le <i>tokenizer</i> et le mod\xE8le doivent toujours provenir du m\xEAme <i>checkpoint</i>.",explain:"C\u2019est juste !",correct:!0},{text:" C'est une bonne pratique de faire du <i>padding</i> et de troncage avec le <i>tokenizer</i> car chaque entr\xE9e est un batch.",explain:"Il est vrai que chaque entr\xE9e de mod\xE8le doit \xEAtre un batch. Cependant, tronquer ou compl\xE9ter cette s\xE9quence n'aurait pas n\xE9cessairement de sens puisqu'il n'y en a qu'une seule. Il s'agit l\xE0 de techniques permettant de mettre en batch une liste de phrases."}]}}),{c(){c=s("h3"),p=s("a"),v=s("span"),b(h.$$.fragment),E=f(),x=s("span"),S=N("10. Y a-t-il un probl\xE8me avec le code suivant ?"),z=f(),b(q.$$.fragment),P=f(),b(i.$$.fragment),this.h()},l(n){c=o(n,"H3",{class:!0});var $=a(c);p=o($,"A",{id:!0,class:!0,href:!0});var j=a(p);v=o(j,"SPAN",{});var C=a(v);w(h.$$.fragment,C),C.forEach(t),j.forEach(t),E=m($),x=o($,"SPAN",{});var O=a(x);S=L(O,"10. Y a-t-il un probl\xE8me avec le code suivant ?"),O.forEach(t),$.forEach(t),z=m(n),w(q.$$.fragment,n),P=m(n),w(i.$$.fragment,n),this.h()},h(){l(p,"id","10.-y-a-t-il-un-probl\xE8me-avec-le-code-suivant-?"),l(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(p,"href","#10.-y-a-t-il-un-probl\xE8me-avec-le-code-suivant-?"),l(c,"class","relative group")},m(n,$){d(n,c,$),r(c,p),r(p,v),A(h,v,null),r(c,E),r(c,x),r(x,S),d(n,z,$),A(q,n,$),d(n,P,$),A(i,n,$),_=!0},i(n){_||(g(h.$$.fragment,n),g(q.$$.fragment,n),g(i.$$.fragment,n),_=!0)},o(n){k(h.$$.fragment,n),k(q.$$.fragment,n),k(i.$$.fragment,n),_=!1},d(n){n&&t(c),y(h),n&&t(z),y(q,n),n&&t(P),y(i,n)}}}function Pr(H){let c,p,v,h,E,x,S,z,q,P,i,_,n,$,j,C,O,Pe,kt,Ve,ne,Ye,B,G,Se,se,gt,oe,xt,Ne,_t,bt,We,ie,De,V,K,Le,ae,wt,je,At,Re,le,Je,Y,X,Ce,ue,yt,Me,zt,Ge,ce,Ke,I,Q,Ae,W,Z,Te,pe,Et,He,Pt,Xe,de,Ze,D,ee,Ie,fe,St,Qe,Nt,et,me,tt,R,te,Ue,he,Lt,$e,jt,Fe,Ct,Mt,rt,ve,nt,J,re,Oe,qe,Tt,ke,Ht,Be,It,Qt,st,ge,ot,xe,it,U,F,ye,at;v=new wr({props:{fw:H[0]}}),z=new M({}),C=new M({}),ne=new T({props:{choices:[{text:" Tout d'abord, le mod\xE8le, qui traite le texte et renvoie des pr\xE9dictions brutes. Puis le <i>tokenizer</i> donne un sens \xE0 ces pr\xE9dictions et les reconvertit en texte si n\xE9cessaire.",explain:" Le mod\xE8le ne peut pas comprendre le texte ! Le <i>tokenizer</i> doit d'abord tokeniser le texte et le convertir en identifiants afin qu'il soit compr\xE9hensible par le mod\xE8le."},{text:" Tout d'abord, le <i>tokenizer</i>, qui traite le texte et renvoie des identifiants. Puis le mod\xE8le traite ces identifiants et produit une pr\xE9diction, qui peut \xEAtre du texte.",explain:" La pr\xE9diction du mod\xE8le ne peut pas \xEAtre du texte imm\xE9diatement. Le <i>tokenizer</i> doit \xEAtre utilis\xE9 afin de reconvertir la pr\xE9diction en texte !"},{text:" Le <i>tokenizer</i> traite le texte et renvoie des identifiants. Le mod\xE8le traite ces identifiants et produit une pr\xE9diction. Le <i>tokenizer</i> peut alors \xEAtre utilis\xE9 \xE0 nouveau pour reconvertir ces pr\xE9dictions en texte.",explain:" C\u2019est correct ! Le <i>tokenizer</i> peut \xEAtre utilis\xE9 \xE0 la fois pour la tokenisation et la d\xE9-tok\xE9nisation.",correct:!0}]}}),se=new M({}),ie=new T({props:{choices:[{text:"2: la longueur de la s\xE9quence et la taille du batch",explain:"Faux ! Le tenseur produit par le mod\xE8le poss\xE8de une troisi\xE8me dimension : la taille cach\xE9e."},{text:"2: la longueur de la s\xE9quence et la taille cach\xE9e",explain:"Faux ! Tous les <i>transformers</i>  g\xE8rent les batchs, m\xEAme avec une seule s\xE9quence ce serait une taille de batch de 1 !"},{text:"3: la longueur de la s\xE9quence, la taille du batch et la taille cach\xE9e.",explain:"C\u2019est correct !",correct:!0}]}}),ae=new M({}),le=new T({props:{choices:[{text:"WordPiece",explain:"Oui, c'est un exemple de tokenisation en sous-mots !",correct:!0},{text:"La tokenization bas\xE9e sur les caract\xE8res",explain:"La tokenization bas\xE9e sur les caract\xE8res n\u2019est pas un type de tokenisation en sous-mots."},{text:"D\xE9coupage sur les espaces et la ponctuation",explain:"C\u2019est une tokenisation bas\xE9e sur les mots !"},{text:"BPE",explain:"Oui, c'est un exemple de tokenisation en sous-mots !",correct:!0},{text:"Unigram",explain:"Oui, c'est un exemple de tokenisation en sous-mots !",correct:!0},{text:"Aucune des propositions ci-dessus",explain:"Incorrect!"}]}}),ue=new M({}),ce=new T({props:{choices:[{text:" Un composant du <i>transformer</i>  de base qui redirige les tenseurs vers leurs couches correctes.",explain:"Incorrect ! Il n'y a pas de tel composant."},{text:"\xC9galement connu sous le nom de m\xE9canisme d'auto-attention, il adapte la repr\xE9sentation d'un <i>token</i>  en fonction des autres <i>tokens</i>  de la s\xE9quence.",explain:"Incorrect ! La couche d'auto-attention contient des t\xEAtes d'attention mais ce ne sont pas des t\xEAtes d'adaptation."},{text:"Un composant suppl\xE9mentaire, g\xE9n\xE9ralement constitu\xE9 d'une ou plusieurs couches, pour convertir les pr\xE9dictions du <i>transformer</i>  en une sortie sp\xE9cifique \xE0 la t\xE2che.",explain:"C'est exact. Les t\xEAtes d'adaptation, aussi appel\xE9es simplement t\xEAtes, se pr\xE9sentent sous diff\xE9rentes formes : t\xEAtes de mod\xE9lisation du langage, t\xEAtes de r\xE9ponse aux questions, t\xEAtes de classification des s\xE9quences, etc.",correct:!0}]}});const Ft=[yr,Ar],_e=[];function Ot(e,u){return e[0]==="pt"?0:1}I=Ot(H),Q=_e[I]=Ft[I](H),pe=new M({}),de=new T({props:{choices:[{text:"La troncature",explain:" Oui, la troncature est une fa\xE7on correcte d'\xE9galiser les s\xE9quences pour qu'elles s'inscrivent dans une forme rectangulaire. Mais est-ce la seule ?",correct:!0},{text:"Retourner les tenseurs",explain:"Alors que les autres techniques vous permettent de renvoyer des tenseurs rectangulaires, retourner les tenseurs n'est pas utile lorsque vous mettez en batch des s\xE9quences."},{text:"Le <i>padding</i>",explain:"Oui, le <i>padding</i> est une fa\xE7on correcte d'\xE9galiser les s\xE9quences pour qu'elles tiennent dans une forme rectangulaire. Mais est-ce le seul moyen ?",correct:!0},{text:"Les masques d'attention ",explain:"Absolument ! Les masques d'attention sont d'une importance capitale lorsqu'on manipule des s\xE9quences de longueurs diff\xE9rentes. Ce n'est cependant pas la seule technique \xE0 laquelle il faut faire attention.",correct:!0}]}}),fe=new M({}),me=new T({props:{choices:[{text:"Elle adoucit les logits pour qu'ils soient plus fiables.",explain:"Non, la fonction SoftMax n'affecte pas la fiabilit\xE9 des r\xE9sultats."},{text:"Elle applique une limite inf\xE9rieure et sup\xE9rieure pour qu'ils soient compr\xE9hensibles.",explain:"C'est exact ! Les valeurs r\xE9sultantes sont comprises entre 0 et 1. Ce n'est cependant pas la seule raison pour laquelle nous utilisons une fonction SoftMax.",correct:!0},{text:"La somme totale des sorties est alors \xE9gale \xE0 1, ce qui permet une interpr\xE9tation probabiliste.",explain:"C'est exact ! Mais ce n'est pas la seule raison pour laquelle nous utilisons une fonction SoftMax.",correct:!0}]}}),he=new M({}),ve=new T({props:{choices:[{text:"<code>encode</code>, car elle peut encoder du texte en identifiants et des identifiants en pr\xE9dictions.",explain:"Faux ! Bien que la m\xE9thode <code>encode</code> existe sur les <i>tokenizer</i>, elle n'existe pas sur les mod\xE8les."},{text:"Appeler directement l'objet tokenizer",explain:" Exactement ! La m\xE9thode <code>__call__</code> du <i>tokenizer</i> est une m\xE9thode tr\xE8s puissante qui peut traiter \xE0 peu pr\xE8s tout. C'est \xE9galement la m\xE9thode utilis\xE9e pour r\xE9cup\xE9rer les pr\xE9dictions d'un mod\xE8le.",correct:!0},{text:"<code>pad</code>",explain:"C'est faux ! Le <i>padding</i> est tr\xE8s utile mais ce n'est qu'une partie de l'API <i>tokenizer</i>."},{text:"<code>tokenize</code>",explain:"La m\xE9thode <code>tokenize</code> est est sans doute l'une des m\xE9thodes les plus utiles, mais elle ne constitue pas le c\u0153ur de l'API <i>tokenizer</i>."}]}}),qe=new M({}),ge=new Ut({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
result = tokenizer.tokenize(<span class="hljs-string">&quot;Hello!&quot;</span>)`}}),xe=new T({props:{choices:[{text:"Une liste de cha\xEEnes de caract\xE8res, chaque cha\xEEne \xE9tant un <i>token</i>.",explain:" Absolument ! Convertissez cela en identifiants, et donnez-les \xE0 un mod\xE8le !",correct:!0},{text:"Une liste d'identifiants",explain:"C'est faux, c'est \xE0 cela que la m\xE9thode <code>__call__</code> ou la m\xE9thode <code>convert_tokens_to_ids</code> sert !"},{text:"Une cha\xEEne contenant tous les <i>tokens</i>",explain:"Ce serait sous-optimal car le but est de diviser la cha\xEEne de caract\xE8res en plusieurs \xE9l\xE9ments."}]}});const Bt=[Er,zr],be=[];function Vt(e,u){return e[0]==="pt"?0:1}return U=Vt(H),F=be[U]=Bt[U](H),{c(){c=s("meta"),p=f(),b(v.$$.fragment),h=f(),E=s("h1"),x=s("a"),S=s("span"),b(z.$$.fragment),q=f(),P=s("span"),i=N("Quiz de fin de chapitre"),_=f(),n=s("h3"),$=s("a"),j=s("span"),b(C.$$.fragment),O=f(),Pe=s("span"),kt=N("1. Quel est l\u2019ordre du pipeline de mod\xE9lisation du langage ?"),Ve=f(),b(ne.$$.fragment),Ye=f(),B=s("h3"),G=s("a"),Se=s("span"),b(se.$$.fragment),gt=f(),oe=s("span"),xt=N("2. Combien de dimensions le tenseur produit par le "),Ne=s("i"),_t=N("transformer"),bt=N(" de base poss\xE8de-t-il et quelles sont-elles ?"),We=f(),b(ie.$$.fragment),De=f(),V=s("h3"),K=s("a"),Le=s("span"),b(ae.$$.fragment),wt=f(),je=s("span"),At=N("3. Lequel des \xE9l\xE9ments suivants est un exemple de tokenisation en sous-mots ?"),Re=f(),b(le.$$.fragment),Je=f(),Y=s("h3"),X=s("a"),Ce=s("span"),b(ue.$$.fragment),yt=f(),Me=s("span"),zt=N("4. Qu\u2019est-ce qu\u2019une t\xEAte de mod\xE8le ?"),Ge=f(),b(ce.$$.fragment),Ke=f(),Q.c(),Ae=f(),W=s("h3"),Z=s("a"),Te=s("span"),b(pe.$$.fragment),Et=f(),He=s("span"),Pt=N("6. Quelles sont les techniques \xE0 conna\xEEtre lors de la mise en batch de s\xE9quences de longueurs diff\xE9rentes ?"),Xe=f(),b(de.$$.fragment),Ze=f(),D=s("h3"),ee=s("a"),Ie=s("span"),b(fe.$$.fragment),St=f(),Qe=s("span"),Nt=N("7. Quel est l\u2019int\xE9r\xEAt d\u2019appliquer une fonction SoftMax aux logits produits par un mod\xE8le de classification de s\xE9quences ?"),et=f(),b(me.$$.fragment),tt=f(),R=s("h3"),te=s("a"),Ue=s("span"),b(he.$$.fragment),Lt=f(),$e=s("span"),jt=N("8. Autour de quelle m\xE9thode s\u2019articule la majeure partie de l\u2019API "),Fe=s("i"),Ct=N("tokenizer"),Mt=N(" ?"),rt=f(),b(ve.$$.fragment),nt=f(),J=s("h3"),re=s("a"),Oe=s("span"),b(qe.$$.fragment),Tt=f(),ke=s("span"),Ht=N("9. Que contient la variable "),Be=s("code"),It=N("result"),Qt=N(" dans cet exemple de code ?"),st=f(),b(ge.$$.fragment),ot=f(),b(xe.$$.fragment),it=f(),F.c(),ye=$r(),this.h()},l(e){const u=_r('[data-svelte="svelte-1phssyn"]',document.head);c=o(u,"META",{name:!0,content:!0}),u.forEach(t),p=m(e),w(v.$$.fragment,e),h=m(e),E=o(e,"H1",{class:!0});var we=a(E);x=o(we,"A",{id:!0,class:!0,href:!0});var ze=a(x);S=o(ze,"SPAN",{});var Ee=a(S);w(z.$$.fragment,Ee),Ee.forEach(t),ze.forEach(t),q=m(we),P=o(we,"SPAN",{});var Yt=a(P);i=L(Yt,"Quiz de fin de chapitre"),Yt.forEach(t),we.forEach(t),_=m(e),n=o(e,"H3",{class:!0});var lt=a(n);$=o(lt,"A",{id:!0,class:!0,href:!0});var Wt=a($);j=o(Wt,"SPAN",{});var Dt=a(j);w(C.$$.fragment,Dt),Dt.forEach(t),Wt.forEach(t),O=m(lt),Pe=o(lt,"SPAN",{});var Rt=a(Pe);kt=L(Rt,"1. Quel est l\u2019ordre du pipeline de mod\xE9lisation du langage ?"),Rt.forEach(t),lt.forEach(t),Ve=m(e),w(ne.$$.fragment,e),Ye=m(e),B=o(e,"H3",{class:!0});var ut=a(B);G=o(ut,"A",{id:!0,class:!0,href:!0});var Jt=a(G);Se=o(Jt,"SPAN",{});var Gt=a(Se);w(se.$$.fragment,Gt),Gt.forEach(t),Jt.forEach(t),gt=m(ut),oe=o(ut,"SPAN",{});var ct=a(oe);xt=L(ct,"2. Combien de dimensions le tenseur produit par le "),Ne=o(ct,"I",{});var Kt=a(Ne);_t=L(Kt,"transformer"),Kt.forEach(t),bt=L(ct," de base poss\xE8de-t-il et quelles sont-elles ?"),ct.forEach(t),ut.forEach(t),We=m(e),w(ie.$$.fragment,e),De=m(e),V=o(e,"H3",{class:!0});var pt=a(V);K=o(pt,"A",{id:!0,class:!0,href:!0});var Xt=a(K);Le=o(Xt,"SPAN",{});var Zt=a(Le);w(ae.$$.fragment,Zt),Zt.forEach(t),Xt.forEach(t),wt=m(pt),je=o(pt,"SPAN",{});var er=a(je);At=L(er,"3. Lequel des \xE9l\xE9ments suivants est un exemple de tokenisation en sous-mots ?"),er.forEach(t),pt.forEach(t),Re=m(e),w(le.$$.fragment,e),Je=m(e),Y=o(e,"H3",{class:!0});var dt=a(Y);X=o(dt,"A",{id:!0,class:!0,href:!0});var tr=a(X);Ce=o(tr,"SPAN",{});var rr=a(Ce);w(ue.$$.fragment,rr),rr.forEach(t),tr.forEach(t),yt=m(dt),Me=o(dt,"SPAN",{});var nr=a(Me);zt=L(nr,"4. Qu\u2019est-ce qu\u2019une t\xEAte de mod\xE8le ?"),nr.forEach(t),dt.forEach(t),Ge=m(e),w(ce.$$.fragment,e),Ke=m(e),Q.l(e),Ae=m(e),W=o(e,"H3",{class:!0});var ft=a(W);Z=o(ft,"A",{id:!0,class:!0,href:!0});var sr=a(Z);Te=o(sr,"SPAN",{});var or=a(Te);w(pe.$$.fragment,or),or.forEach(t),sr.forEach(t),Et=m(ft),He=o(ft,"SPAN",{});var ir=a(He);Pt=L(ir,"6. Quelles sont les techniques \xE0 conna\xEEtre lors de la mise en batch de s\xE9quences de longueurs diff\xE9rentes ?"),ir.forEach(t),ft.forEach(t),Xe=m(e),w(de.$$.fragment,e),Ze=m(e),D=o(e,"H3",{class:!0});var mt=a(D);ee=o(mt,"A",{id:!0,class:!0,href:!0});var ar=a(ee);Ie=o(ar,"SPAN",{});var lr=a(Ie);w(fe.$$.fragment,lr),lr.forEach(t),ar.forEach(t),St=m(mt),Qe=o(mt,"SPAN",{});var ur=a(Qe);Nt=L(ur,"7. Quel est l\u2019int\xE9r\xEAt d\u2019appliquer une fonction SoftMax aux logits produits par un mod\xE8le de classification de s\xE9quences ?"),ur.forEach(t),mt.forEach(t),et=m(e),w(me.$$.fragment,e),tt=m(e),R=o(e,"H3",{class:!0});var ht=a(R);te=o(ht,"A",{id:!0,class:!0,href:!0});var cr=a(te);Ue=o(cr,"SPAN",{});var pr=a(Ue);w(he.$$.fragment,pr),pr.forEach(t),cr.forEach(t),Lt=m(ht),$e=o(ht,"SPAN",{});var $t=a($e);jt=L($t,"8. Autour de quelle m\xE9thode s\u2019articule la majeure partie de l\u2019API "),Fe=o($t,"I",{});var dr=a(Fe);Ct=L(dr,"tokenizer"),dr.forEach(t),Mt=L($t," ?"),$t.forEach(t),ht.forEach(t),rt=m(e),w(ve.$$.fragment,e),nt=m(e),J=o(e,"H3",{class:!0});var vt=a(J);re=o(vt,"A",{id:!0,class:!0,href:!0});var fr=a(re);Oe=o(fr,"SPAN",{});var mr=a(Oe);w(qe.$$.fragment,mr),mr.forEach(t),fr.forEach(t),Tt=m(vt),ke=o(vt,"SPAN",{});var qt=a(ke);Ht=L(qt,"9. Que contient la variable "),Be=o(qt,"CODE",{});var hr=a(Be);It=L(hr,"result"),hr.forEach(t),Qt=L(qt," dans cet exemple de code ?"),qt.forEach(t),vt.forEach(t),st=m(e),w(ge.$$.fragment,e),ot=m(e),w(xe.$$.fragment,e),it=m(e),F.l(e),ye=$r(),this.h()},h(){l(c,"name","hf:doc:metadata"),l(c,"content",JSON.stringify(Sr)),l(x,"id","quiz-de-fin-de-chapitre"),l(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(x,"href","#quiz-de-fin-de-chapitre"),l(E,"class","relative group"),l($,"id","1.-quel-est-l\u2019ordre-du-pipeline-de-mod\xE9lisation-du-langage-?"),l($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($,"href","#1.-quel-est-l\u2019ordre-du-pipeline-de-mod\xE9lisation-du-langage-?"),l(n,"class","relative group"),l(G,"id","2.-combien-de-dimensions-le-tenseur-produit-par-le-<i>transformer</i>-de-base-poss\xE8de-t-il-et-quelles-sont-elles-?"),l(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(G,"href","#2.-combien-de-dimensions-le-tenseur-produit-par-le-<i>transformer</i>-de-base-poss\xE8de-t-il-et-quelles-sont-elles-?"),l(B,"class","relative group"),l(K,"id","3.-lequel-des-\xE9l\xE9ments-suivants-est-un-exemple-de-tokenisation-en-sous-mots-?"),l(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(K,"href","#3.-lequel-des-\xE9l\xE9ments-suivants-est-un-exemple-de-tokenisation-en-sous-mots-?"),l(V,"class","relative group"),l(X,"id","4.-qu\u2019est-ce-qu\u2019une-t\xEAte-de-mod\xE8le-?"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#4.-qu\u2019est-ce-qu\u2019une-t\xEAte-de-mod\xE8le-?"),l(Y,"class","relative group"),l(Z,"id","6.-quelles-sont-les-techniques-\xE0-conna\xEEtre-lors-de-la-mise-en-batch-de-s\xE9quences-de-longueurs-diff\xE9rentes-?"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#6.-quelles-sont-les-techniques-\xE0-conna\xEEtre-lors-de-la-mise-en-batch-de-s\xE9quences-de-longueurs-diff\xE9rentes-?"),l(W,"class","relative group"),l(ee,"id","7.-quel-est-l\u2019int\xE9r\xEAt-d\u2019appliquer-une-fonction-softmax-aux-logits-produits-par-un-mod\xE8le-de-classification-de-s\xE9quences-?"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#7.-quel-est-l\u2019int\xE9r\xEAt-d\u2019appliquer-une-fonction-softmax-aux-logits-produits-par-un-mod\xE8le-de-classification-de-s\xE9quences-?"),l(D,"class","relative group"),l(te,"id","8.-autour-de-quelle-m\xE9thode-s\u2019articule-la-majeure-partie-de-l\u2019api-<i>tokenizer</i>-?"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#8.-autour-de-quelle-m\xE9thode-s\u2019articule-la-majeure-partie-de-l\u2019api-<i>tokenizer</i>-?"),l(R,"class","relative group"),l(re,"id","9.-que-contient-la-variable-<code>result</code>-dans-cet-exemple-de-code-?"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#9.-que-contient-la-variable-<code>result</code>-dans-cet-exemple-de-code-?"),l(J,"class","relative group")},m(e,u){r(document.head,c),d(e,p,u),A(v,e,u),d(e,h,u),d(e,E,u),r(E,x),r(x,S),A(z,S,null),r(E,q),r(E,P),r(P,i),d(e,_,u),d(e,n,u),r(n,$),r($,j),A(C,j,null),r(n,O),r(n,Pe),r(Pe,kt),d(e,Ve,u),A(ne,e,u),d(e,Ye,u),d(e,B,u),r(B,G),r(G,Se),A(se,Se,null),r(B,gt),r(B,oe),r(oe,xt),r(oe,Ne),r(Ne,_t),r(oe,bt),d(e,We,u),A(ie,e,u),d(e,De,u),d(e,V,u),r(V,K),r(K,Le),A(ae,Le,null),r(V,wt),r(V,je),r(je,At),d(e,Re,u),A(le,e,u),d(e,Je,u),d(e,Y,u),r(Y,X),r(X,Ce),A(ue,Ce,null),r(Y,yt),r(Y,Me),r(Me,zt),d(e,Ge,u),A(ce,e,u),d(e,Ke,u),_e[I].m(e,u),d(e,Ae,u),d(e,W,u),r(W,Z),r(Z,Te),A(pe,Te,null),r(W,Et),r(W,He),r(He,Pt),d(e,Xe,u),A(de,e,u),d(e,Ze,u),d(e,D,u),r(D,ee),r(ee,Ie),A(fe,Ie,null),r(D,St),r(D,Qe),r(Qe,Nt),d(e,et,u),A(me,e,u),d(e,tt,u),d(e,R,u),r(R,te),r(te,Ue),A(he,Ue,null),r(R,Lt),r(R,$e),r($e,jt),r($e,Fe),r(Fe,Ct),r($e,Mt),d(e,rt,u),A(ve,e,u),d(e,nt,u),d(e,J,u),r(J,re),r(re,Oe),A(qe,Oe,null),r(J,Tt),r(J,ke),r(ke,Ht),r(ke,Be),r(Be,It),r(ke,Qt),d(e,st,u),A(ge,e,u),d(e,ot,u),A(xe,e,u),d(e,it,u),be[U].m(e,u),d(e,ye,u),at=!0},p(e,[u]){const we={};u&1&&(we.fw=e[0]),v.$set(we);let ze=I;I=Ot(e),I!==ze&&(qr(),k(_e[ze],1,1,()=>{_e[ze]=null}),vr(),Q=_e[I],Q||(Q=_e[I]=Ft[I](e),Q.c()),g(Q,1),Q.m(Ae.parentNode,Ae));let Ee=U;U=Vt(e),U!==Ee&&(qr(),k(be[Ee],1,1,()=>{be[Ee]=null}),vr(),F=be[U],F||(F=be[U]=Bt[U](e),F.c()),g(F,1),F.m(ye.parentNode,ye))},i(e){at||(g(v.$$.fragment,e),g(z.$$.fragment,e),g(C.$$.fragment,e),g(ne.$$.fragment,e),g(se.$$.fragment,e),g(ie.$$.fragment,e),g(ae.$$.fragment,e),g(le.$$.fragment,e),g(ue.$$.fragment,e),g(ce.$$.fragment,e),g(Q),g(pe.$$.fragment,e),g(de.$$.fragment,e),g(fe.$$.fragment,e),g(me.$$.fragment,e),g(he.$$.fragment,e),g(ve.$$.fragment,e),g(qe.$$.fragment,e),g(ge.$$.fragment,e),g(xe.$$.fragment,e),g(F),at=!0)},o(e){k(v.$$.fragment,e),k(z.$$.fragment,e),k(C.$$.fragment,e),k(ne.$$.fragment,e),k(se.$$.fragment,e),k(ie.$$.fragment,e),k(ae.$$.fragment,e),k(le.$$.fragment,e),k(ue.$$.fragment,e),k(ce.$$.fragment,e),k(Q),k(pe.$$.fragment,e),k(de.$$.fragment,e),k(fe.$$.fragment,e),k(me.$$.fragment,e),k(he.$$.fragment,e),k(ve.$$.fragment,e),k(qe.$$.fragment,e),k(ge.$$.fragment,e),k(xe.$$.fragment,e),k(F),at=!1},d(e){t(c),e&&t(p),y(v,e),e&&t(h),e&&t(E),y(z),e&&t(_),e&&t(n),y(C),e&&t(Ve),y(ne,e),e&&t(Ye),e&&t(B),y(se),e&&t(We),y(ie,e),e&&t(De),e&&t(V),y(ae),e&&t(Re),y(le,e),e&&t(Je),e&&t(Y),y(ue),e&&t(Ge),y(ce,e),e&&t(Ke),_e[I].d(e),e&&t(Ae),e&&t(W),y(pe),e&&t(Xe),y(de,e),e&&t(Ze),e&&t(D),y(fe),e&&t(et),y(me,e),e&&t(tt),e&&t(R),y(he),e&&t(rt),y(ve,e),e&&t(nt),e&&t(J),y(qe),e&&t(st),y(ge,e),e&&t(ot),y(xe,e),e&&t(it),be[U].d(e),e&&t(ye)}}}const Sr={local:"quiz-de-fin-de-chapitre",title:"Quiz de fin de chapitre"};function Nr(H,c,p){let v="pt";return br(()=>{const h=new URLSearchParams(window.location.search);p(0,v=h.get("fw")||"pt")}),[v]}class Hr extends kr{constructor(c){super();gr(this,c,Nr,Pr,xr,{})}}export{Hr as default,Sr as metadata};
