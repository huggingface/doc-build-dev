import{S as uc,i as mc,s as pc,e as r,k as m,w as _,t as n,M as cc,c as a,d as s,m as p,x as b,a as l,h as o,b as d,N as ms,F as t,g as u,y as E,o as v,p as dc,q as h,B as $,v as fc,n as vc}from"../../chunks/vendor-1e8b365d.js";import{T as hc}from"../../chunks/Tip-62b14c6e.js";import{Y as Qn}from"../../chunks/Youtube-c2a8cc39.js";import{I as ce}from"../../chunks/IconCopyLink-483c28ba.js";import{C as V}from"../../chunks/CodeBlock-e5764662.js";import{D as ic}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as kc}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function _c(J){let c,q;return c=new ic({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,x){E(c,f,x),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function bc(J){let c,q;return c=new ic({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,x){E(c,f,x),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function Ec(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R;return{c(){c=r("p"),q=n("Similaire \xE0 "),f=r("code"),x=n("TFAutoModel"),w=n(", la classe "),g=r("code"),P=n("AutoTokenizer"),A=n(" r\xE9cup\xE8re la classe de "),S=r("em"),G=n("tokenizer"),X=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=r("em"),y=n("checkpoint"),M=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=r("em"),H=n("checkpoint"),R=n(" :")},l(z){c=a(z,"P",{});var k=l(c);q=o(k,"Similaire \xE0 "),f=a(k,"CODE",{});var Z=l(f);x=o(Z,"TFAutoModel"),Z.forEach(s),w=o(k,", la classe "),g=a(k,"CODE",{});var de=l(g);P=o(de,"AutoTokenizer"),de.forEach(s),A=o(k," r\xE9cup\xE8re la classe de "),S=a(k,"EM",{});var fe=l(S);G=o(fe,"tokenizer"),fe.forEach(s),X=o(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=a(k,"EM",{});var ee=l(T);y=o(ee,"checkpoint"),ee.forEach(s),M=o(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=a(k,"EM",{});var ve=l(C);H=o(ve,"checkpoint"),ve.forEach(s),R=o(k," :"),k.forEach(s)},m(z,k){u(z,c,k),t(c,q),t(c,f),t(f,x),t(c,w),t(c,g),t(g,P),t(c,A),t(c,S),t(S,G),t(c,X),t(c,T),t(T,y),t(c,M),t(c,C),t(C,H),t(c,R)},d(z){z&&s(c)}}}function $c(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R;return{c(){c=r("p"),q=n("Similaire \xE0 "),f=r("code"),x=n("AutoModel"),w=n(", la classe "),g=r("code"),P=n("AutoTokenizer"),A=n(" r\xE9cup\xE8re la classe de "),S=r("em"),G=n("tokenizer"),X=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=r("em"),y=n("checkpoint"),M=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=r("em"),H=n("checkpoint"),R=n(" :")},l(z){c=a(z,"P",{});var k=l(c);q=o(k,"Similaire \xE0 "),f=a(k,"CODE",{});var Z=l(f);x=o(Z,"AutoModel"),Z.forEach(s),w=o(k,", la classe "),g=a(k,"CODE",{});var de=l(g);P=o(de,"AutoTokenizer"),de.forEach(s),A=o(k," r\xE9cup\xE8re la classe de "),S=a(k,"EM",{});var fe=l(S);G=o(fe,"tokenizer"),fe.forEach(s),X=o(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=a(k,"EM",{});var ee=l(T);y=o(ee,"checkpoint"),ee.forEach(s),M=o(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=a(k,"EM",{});var ve=l(C);H=o(ve,"checkpoint"),ve.forEach(s),R=o(k," :"),k.forEach(s)},m(z,k){u(z,c,k),t(c,q),t(c,f),t(f,x),t(c,w),t(c,g),t(g,P),t(c,A),t(c,S),t(S,G),t(c,X),t(c,T),t(T,y),t(c,M),t(c,C),t(C,H),t(c,R)},d(z){z&&s(c)}}}function qc(J){let c,q,f,x,w;return{c(){c=r("p"),q=n("\u270F\uFE0F "),f=r("strong"),x=n("Essayez !"),w=n(" Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life. \xBB et \xAB I hate this so much! \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !")},l(g){c=a(g,"P",{});var P=l(c);q=o(P,"\u270F\uFE0F "),f=a(P,"STRONG",{});var A=l(f);x=o(A,"Essayez !"),A.forEach(s),w=o(P," Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life. \xBB et \xAB I hate this so much! \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !"),P.forEach(s)},m(g,P){u(g,c,P),t(c,q),t(c,f),t(f,x),t(c,w)},d(g){g&&s(c)}}}function gc(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R,z,k,Z,de,fe,ee,ve,Rr,Xn,Ot,Fr,Zn,st,eo,ye,Kr,ps,Yr,Wr,to,Bt,Qr,so,he,Me,cs,nt,Xr,ds,Zr,no,ot,oo,Ae,ea,fs,ta,sa,ro,ke,rt,Gu,na,at,Ru,ao,Ce,oa,vs,ra,aa,lo,lt,io,it,uo,F,la,hs,ia,ua,ks,ma,pa,_s,ca,da,mo,Vt,fa,po,ne,va,bs,ha,ka,Es,_a,ba,co,te,Ea,$s,$a,qa,qs,ga,xa,se,za,gs,wa,Pa,xs,ja,ya,zs,Ma,Aa,fo,oe,Ca,ws,La,Ta,Ps,Na,Da,vo,_e,Le,js,ut,Ia,ys,Sa,ho,mt,ko,Te,Ha,Ms,Ua,Oa,_o,Ne,As,Ba,Va,pt,Ja,Cs,Ga,Ra,bo,Jt,Fa,Eo,be,ct,Fu,Ka,dt,Ku,$o,Gt,Ya,qo,N,Wa,Ls,Qa,Xa,Ts,Za,el,Ns,tl,sl,Ds,nl,ol,Is,rl,al,go,De,ll,Ss,il,ul,xo,Ee,Ie,Hs,ft,ml,Us,pl,zo,vt,wo,Rt,cl,Po,Ft,dl,jo,Kt,fl,yo,$e,ht,Yu,vl,kt,Wu,Mo,K,hl,Os,kl,_l,Bs,bl,El,Vs,$l,ql,Ao,Yt,gl,Co,qe,Se,Js,_t,xl,Gs,zl,Lo,Wt,wl,To,re,bt,Pl,Rs,jl,yl,Ml,Et,Al,Fs,Cl,Ll,Tl,He,Ks,Nl,Dl,Ys,Il,Sl,No,Qt,Hl,Do,ge,Ue,Ws,$t,Ul,Qs,Ol,Io,L,Bl,Xs,Vl,Jl,Zs,Gl,Rl,en,Fl,Kl,tn,Yl,Wl,sn,Ql,Xl,nn,Zl,ei,So,Y,ti,on,si,ni,rn,oi,ri,an,ai,li,Ho,qt,Uo,Xt,gt,Oo,Oe,ii,ln,ui,mi,Bo,xt,Vo,zt,Jo,Zt,pi,Go,wt,Ro,D,ci,un,di,fi,es,vi,hi,mn,ki,_i,pn,bi,Ei,cn,$i,qi,Fo,xe,Be,dn,Pt,gi,fn,xi,Ko,jt,Yo,Ve,zi,vn,wi,Pi,Wo,ae,ji,hn,yi,Mi,kn,Ai,Ci,Qo,U,Li,_n,Ti,Ni,bn,Di,Ii,En,Si,Hi,$n,Ui,Oi,Xo,Je,Bi,qn,Vi,Ji,Zo,ze,Ge,gn,yt,Gi,xn,Ri,er,le,Fi,zn,Ki,Yi,wn,Wi,Qi,tr,Mt,sr,Re,Xi,Pn,Zi,eu,nr,At,or,j,tu,jn,su,nu,yn,ou,ru,Mn,au,lu,An,iu,uu,Cn,mu,pu,Ln,cu,du,Tn,fu,vu,rr,we,Fe,Nn,Ct,hu,Dn,ku,ar,ie,_u,In,bu,Eu,Sn,$u,qu,lr,Lt,ir,Tt,ur,Ke,gu,Hn,xu,zu,mr,Ye,pr,Pe,We,Un,Nt,wu,On,Pu,cr,ue,ju,Bn,yu,Mu,Vn,Au,Cu,dr,Dt,fr,It,vr,O,Lu,Jn,Tu,Nu,Gn,Du,Iu,Rn,Su,Hu,Fn,Uu,Ou,hr,Qe,Bu,Kn,Vu,Ju,kr;f=new kc({props:{fw:J[0]}}),A=new ce({});const Qu=[bc,_c],St=[];function Xu(e,i){return e[0]==="pt"?0:1}y=Xu(J),M=St[y]=Qu[y](J),H=new Qn({props:{id:"VFp38yj8h3A"}}),st=new V({props:{code:"Jim Henson was a puppeteer # Jim Henson \xE9tait un marionnettiste.",highlighted:'Jim Henson was <span class="hljs-keyword">a</span> puppeteer <span class="hljs-comment"># Jim Henson \xE9tait un marionnettiste.</span>'}}),nt=new ce({}),ot=new Qn({props:{id:"nhJxYji1aho"}}),lt=new V({props:{code:`tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson was a puppeteer&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),it=new V({props:{code:"['Jim', 'Henson', 'was', 'a', 'puppeteer'] # ['Jim', 'Henson', \xE9tait, 'un', 'marionnettiste']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;puppeteer&#x27;</span>] <span class="hljs-comment"># [&#x27;Jim&#x27;, &#x27;Henson&#x27;, \xE9tait, &#x27;un&#x27;, &#x27;marionnettiste&#x27;]</span>'}}),ut=new ce({}),mt=new Qn({props:{id:"ssLq_EK2jLE"}}),ft=new ce({}),vt=new Qn({props:{id:"zHvTiHr506c"}}),_t=new ce({}),$t=new ce({}),qt=new V({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function Zu(e,i){return e[0]==="pt"?$c:Ec}let _r=Zu(J),je=_r(J);return gt=new V({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),xt=new V({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),zt=new V({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),wt=new V({props:{code:'tokenizer.save_pretrained("directory_on_my_computer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}}),Pt=new ce({}),jt=new Qn({props:{id:"Yffk5aydLzg"}}),yt=new ce({}),Mt=new V({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),At=new V({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),Ct=new ce({}),Lt=new V({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),Tt=new V({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),Ye=new hc({props:{$$slots:{default:[qc]},$$scope:{ctx:J}}}),Nt=new ce({}),Dt=new V({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),It=new V({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){c=r("meta"),q=m(),_(f.$$.fragment),x=m(),w=r("h1"),g=r("a"),P=r("span"),_(A.$$.fragment),S=m(),G=r("span"),X=n("Les *tokenizers*"),T=m(),M.c(),C=m(),_(H.$$.fragment),R=m(),z=r("p"),k=n("Les "),Z=r("em"),de=n("tokenizers"),fe=n(" sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),ee=r("em"),ve=n("tokenizers"),Rr=n(" doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),Xn=m(),Ot=r("p"),Fr=n("Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),Zn=m(),_(st.$$.fragment),eo=m(),ye=r("p"),Kr=n("Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),ps=r("em"),Yr=n("tokenizers"),Wr=n(" et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),to=m(),Bt=r("p"),Qr=n("Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),so=m(),he=r("h2"),Me=r("a"),cs=r("span"),_(nt.$$.fragment),Xr=m(),ds=r("span"),Zr=n("*Tokenizer* bas\xE9 sur les mots"),no=m(),_(ot.$$.fragment),oo=m(),Ae=r("p"),ea=n("Le premier type de "),fs=r("em"),ta=n("tokenizer"),sa=n(" qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),ro=m(),ke=r("div"),rt=r("img"),na=m(),at=r("img"),ao=m(),Ce=r("p"),oa=n("Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),vs=r("code"),ra=n("split()"),aa=n(" de Python :"),lo=m(),_(lt.$$.fragment),io=m(),_(it.$$.fragment),uo=m(),F=r("p"),la=n("Il existe \xE9galement des variantes des "),hs=r("em"),ia=n("tokenizers"),ua=n(" bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),ks=r("em"),ma=n("tokenizers"),pa=n(" nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),_s=r("em"),ca=n("tokens"),da=n(" ind\xE9pendants que nous avons dans notre corpus."),mo=m(),Vt=r("p"),fa=n("Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),po=m(),ne=r("p"),va=n("Si nous voulons couvrir compl\xE8tement une langue avec un "),bs=r("em"),ha=n("tokenizer"),ka=n(" bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE8re une \xE9norme quantit\xE9 de "),Es=r("em"),_a=n("tokens"),ba=n(". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),co=m(),te=r("p"),Ea=n("Enfin, nous avons besoin d\u2019un "),$s=r("em"),$a=n("token"),qa=n(" personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),qs=r("em"),ga=n("token"),xa=n(" \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),se=r("unk"),za=n("; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),gs=r("em"),wa=n("tokenizer"),Pa=n(" produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),xs=r("em"),ja=n("tokenizer"),ya=n(" transforme le moins de mots possible en "),zs=r("em"),Ma=n("token"),Aa=n(" inconnu."),fo=m(),oe=r("p"),Ca=n("Une fa\xE7on de r\xE9duire la quantit\xE9 de "),ws=r("em"),La=n("tokens"),Ta=n(" inconnus est d\u2019aller un niveau plus profond, en utilisant un "),Ps=r("em"),Na=n("tokenizer"),Da=n(" bas\xE9 sur les caract\xE8res."),vo=m(),_e=r("h2"),Le=r("a"),js=r("span"),_(ut.$$.fragment),Ia=m(),ys=r("span"),Sa=n("*Tokenizer* bas\xE9 sur les caract\xE8res"),ho=m(),_(mt.$$.fragment),ko=m(),Te=r("p"),Ha=n("Les "),Ms=r("em"),Ua=n("tokenizers"),Oa=n(" bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),_o=m(),Ne=r("ul"),As=r("li"),Ba=n("le vocabulaire est beaucoup plus petit"),Va=m(),pt=r("li"),Ja=n("il y a beaucoup moins de "),Cs=r("em"),Ga=n("tokens"),Ra=n(" hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),bo=m(),Jt=r("p"),Fa=n("Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),Eo=m(),be=r("div"),ct=r("img"),Ka=m(),dt=r("img"),$o=m(),Gt=r("p"),Ya=n("Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),qo=m(),N=r("p"),Wa=n("Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ls=r("em"),Qa=n("tokens"),Xa=n(" \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),Ts=r("em"),Za=n("tokenizer"),el=n(" bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Ns=r("em"),tl=n("token"),sl=n(", avec un "),Ds=r("em"),nl=n("tokenizer"),ol=n(" bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Is=r("em"),rl=n("tokens"),al=n(" voire plus."),go=m(),De=r("p"),ll=n("Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Ss=r("em"),il=n("tok\xE9nisation en sous-mots"),ul=n("."),xo=m(),Ee=r("h2"),Ie=r("a"),Hs=r("span"),_(ft.$$.fragment),ml=m(),Us=r("span"),pl=n("Tok\xE9nisation en sous-mots"),zo=m(),_(vt.$$.fragment),wo=m(),Rt=r("p"),cl=n("Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),Po=m(),Ft=r("p"),dl=n("Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),jo=m(),Kt=r("p"),fl=n("Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB Let\u2019s do tokenization ! \xBB :"),yo=m(),$e=r("div"),ht=r("img"),vl=m(),kt=r("img"),Mo=m(),K=r("p"),hl=n("Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB tokenization \xBB a \xE9t\xE9 divis\xE9 en \xAB token \xBB et \xAB ization \xBB : deux "),Os=r("em"),kl=n("tokens"),_l=n(" qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Bs=r("em"),bl=n("tokens"),El=n(" sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Vs=r("em"),$l=n("token"),ql=n(" inconnu."),Ao=m(),Yt=r("p"),gl=n("Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),Co=m(),qe=r("h3"),Se=r("a"),Js=r("span"),_(_t.$$.fragment),xl=m(),Gs=r("span"),zl=n("Et plus encore !"),Lo=m(),Wt=r("p"),wl=n("Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),To=m(),re=r("ul"),bt=r("li"),Pl=n("le "),Rs=r("em"),jl=n("Byte-level BPE"),yl=n(" utilis\xE9 par exemple dans le GPT-2"),Ml=m(),Et=r("li"),Al=n("le "),Fs=r("em"),Cl=n("WordPiece"),Ll=n(" utilis\xE9 par exemple dans BERT"),Tl=m(),He=r("li"),Ks=r("em"),Nl=n("SentencePiece"),Dl=n(" ou "),Ys=r("em"),Il=n("Unigram"),Sl=n(", utilis\xE9s dans plusieurs mod\xE8les multilingues."),No=m(),Qt=r("p"),Hl=n("Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),Do=m(),ge=r("h2"),Ue=r("a"),Ws=r("span"),_($t.$$.fragment),Ul=m(),Qs=r("span"),Ol=n("Chargement et sauvegarde"),Io=m(),L=r("p"),Bl=n("Le chargement et la sauvegarde des "),Xs=r("em"),Vl=n("tokenizers"),Jl=n(" est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),Zs=r("code"),Gl=n("from_pretrained()"),Rl=n(" et "),en=r("code"),Fl=n("save_pretrained()"),Kl=n(". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),tn=r("em"),Yl=n("tokenizer"),Wl=n(" (un peu comme l\u2019"),sn=r("em"),Ql=n("architecture"),Xl=n(" du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),nn=r("em"),Zl=n("poids"),ei=n(" du mod\xE8le)."),So=m(),Y=r("p"),ti=n("Le chargement du "),on=r("em"),si=n("tokenizer"),ni=n(" de BERT entra\xEEn\xE9 avec le m\xEAme "),rn=r("em"),oi=n("checkpoint"),ri=n(" que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),an=r("code"),ai=n("BertTokenizer"),li=n(" :"),Ho=m(),_(qt.$$.fragment),Uo=m(),je.c(),Xt=m(),_(gt.$$.fragment),Oo=m(),Oe=r("p"),ii=n("Nous pouvons \xE0 pr\xE9sent utiliser le "),ln=r("em"),ui=n("tokenizer"),mi=n(" comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),Bo=m(),_(xt.$$.fragment),Vo=m(),_(zt.$$.fragment),Jo=m(),Zt=r("p"),pi=n("La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),Go=m(),_(wt.$$.fragment),Ro=m(),D=r("p"),ci=n("Nous parlerons plus en d\xE9tail des "),un=r("code"),di=n("token_type_ids"),fi=n(" au "),es=r("a"),vi=n("Chapitre 3"),hi=n(" et nous expliquerons la cl\xE9 "),mn=r("code"),ki=n("attention_mask"),_i=n(" un peu plus tard. Tout d\u2019abord, voyons comment les "),pn=r("code"),bi=n("input_ids"),Ei=n(" sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),cn=r("em"),$i=n("tokenizer"),qi=n("."),Fo=m(),xe=r("h2"),Be=r("a"),dn=r("span"),_(Pt.$$.fragment),gi=m(),fn=r("span"),xi=n("Encodage"),Ko=m(),_(jt.$$.fragment),Yo=m(),Ve=r("p"),zi=n("La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),vn=r("em"),wi=n("encodage"),Pi=n(". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),Wo=m(),ae=r("p"),ji=n("Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),hn=r("em"),yi=n("tokens"),Mi=n(". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),kn=r("em"),Ai=n("tokenizer"),Ci=n(" en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),Qo=m(),U=r("p"),Li=n("La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),_n=r("em"),Ti=n("tokens"),Ni=n(" en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),bn=r("em"),Di=n("tokenizer"),Ii=n(" poss\xE8de un "),En=r("em"),Si=n("vocabulaire"),Hi=n(", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),$n=r("code"),Ui=n("from_pretrained()"),Oi=n(". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),Xo=m(),Je=r("p"),Bi=n("Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),qn=r("em"),Vi=n("tokenizer"),Ji=n(" directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),Zo=m(),ze=r("h3"),Ge=r("a"),gn=r("span"),_(yt.$$.fragment),Gi=m(),xn=r("span"),Ri=n("Tokenisation"),er=m(),le=r("p"),Fi=n("Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),zn=r("code"),Ki=n("tokenize()"),Yi=n(" du "),wn=r("em"),Wi=n("tokenizer"),Qi=n(" :"),tr=m(),_(Mt.$$.fragment),sr=m(),Re=r("p"),Xi=n("La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),Pn=r("em"),Zi=n("tokens"),eu=n(" :"),nr=m(),_(At.$$.fragment),or=m(),j=r("p"),tu=n("Ce "),jn=r("em"),su=n("tokenizer"),nu=n(" est un "),yn=r("em"),ou=n("tokenizer"),ru=n(" de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),Mn=r("em"),au=n("tokens"),lu=n(" qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),An=r("code"),iu=n("transformer"),uu=n(" qui est divis\xE9 en deux "),Cn=r("em"),mu=n("tokens"),pu=n(" : "),Ln=r("code"),cu=n("transform"),du=n(" et "),Tn=r("code"),fu=n("##er"),vu=n("."),rr=m(),we=r("h3"),Fe=r("a"),Nn=r("span"),_(Ct.$$.fragment),hu=m(),Dn=r("span"),ku=n("De *tokens* aux identifiants d'entr\xE9e"),ar=m(),ie=r("p"),_u=n("La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),In=r("code"),bu=n("convert_tokens_to_ids()"),Eu=n(" du "),Sn=r("em"),$u=n("tokenizer"),qu=n(" :"),lr=m(),_(Lt.$$.fragment),ir=m(),_(Tt.$$.fragment),ur=m(),Ke=r("p"),gu=n("Une fois converties en tenseur dans le "),Hn=r("em"),xu=n("framework"),zu=n(" appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),mr=m(),_(Ye.$$.fragment),pr=m(),Pe=r("h2"),We=r("a"),Un=r("span"),_(Nt.$$.fragment),wu=m(),On=r("span"),Pu=n("D\xE9codage"),cr=m(),ue=r("p"),ju=n("Le "),Bn=r("em"),yu=n("d\xE9codage"),Mu=n(" va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Vn=r("code"),Au=n("decode()"),Cu=n(" comme suit :"),dr=m(),_(Dt.$$.fragment),fr=m(),_(It.$$.fragment),vr=m(),O=r("p"),Lu=n("Notez que la m\xE9thode "),Jn=r("code"),Tu=n("decode"),Nu=n(" non seulement reconvertit les indices en "),Gn=r("em"),Du=n("tokens"),Iu=n(" mais regroupe \xE9galement les "),Rn=r("em"),Su=n("tokens"),Hu=n(" faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),Fn=r("em"),Uu=n("prompt"),Ou=n(", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),hr=m(),Qe=r("p"),Bu=n("Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),Kn=r("em"),Vu=n("tokenizer"),Ju=n(" peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),this.h()},l(e){const i=cc('[data-svelte="svelte-1phssyn"]',document.head);c=a(i,"META",{name:!0,content:!0}),i.forEach(s),q=p(e),b(f.$$.fragment,e),x=p(e),w=a(e,"H1",{class:!0});var Ht=l(w);g=a(Ht,"A",{id:!0,class:!0,href:!0});var ts=l(g);P=a(ts,"SPAN",{});var Yn=l(P);b(A.$$.fragment,Yn),Yn.forEach(s),ts.forEach(s),S=p(Ht),G=a(Ht,"SPAN",{});var em=l(G);X=o(em,"Les *tokenizers*"),em.forEach(s),Ht.forEach(s),T=p(e),M.l(e),C=p(e),b(H.$$.fragment,e),R=p(e),z=a(e,"P",{});var ss=l(z);k=o(ss,"Les "),Z=a(ss,"EM",{});var tm=l(Z);de=o(tm,"tokenizers"),tm.forEach(s),fe=o(ss," sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),ee=a(ss,"EM",{});var sm=l(ee);ve=o(sm,"tokenizers"),sm.forEach(s),Rr=o(ss," doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),ss.forEach(s),Xn=p(e),Ot=a(e,"P",{});var nm=l(Ot);Fr=o(nm,"Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),nm.forEach(s),Zn=p(e),b(st.$$.fragment,e),eo=p(e),ye=a(e,"P",{});var br=l(ye);Kr=o(br,"Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),ps=a(br,"EM",{});var om=l(ps);Yr=o(om,"tokenizers"),om.forEach(s),Wr=o(br," et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),br.forEach(s),to=p(e),Bt=a(e,"P",{});var rm=l(Bt);Qr=o(rm,"Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),rm.forEach(s),so=p(e),he=a(e,"H2",{class:!0});var Er=l(he);Me=a(Er,"A",{id:!0,class:!0,href:!0});var am=l(Me);cs=a(am,"SPAN",{});var lm=l(cs);b(nt.$$.fragment,lm),lm.forEach(s),am.forEach(s),Xr=p(Er),ds=a(Er,"SPAN",{});var im=l(ds);Zr=o(im,"*Tokenizer* bas\xE9 sur les mots"),im.forEach(s),Er.forEach(s),no=p(e),b(ot.$$.fragment,e),oo=p(e),Ae=a(e,"P",{});var $r=l(Ae);ea=o($r,"Le premier type de "),fs=a($r,"EM",{});var um=l(fs);ta=o(um,"tokenizer"),um.forEach(s),sa=o($r," qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),$r.forEach(s),ro=p(e),ke=a(e,"DIV",{class:!0});var qr=l(ke);rt=a(qr,"IMG",{class:!0,src:!0,alt:!0}),na=p(qr),at=a(qr,"IMG",{class:!0,src:!0,alt:!0}),qr.forEach(s),ao=p(e),Ce=a(e,"P",{});var gr=l(Ce);oa=o(gr,"Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),vs=a(gr,"CODE",{});var mm=l(vs);ra=o(mm,"split()"),mm.forEach(s),aa=o(gr," de Python :"),gr.forEach(s),lo=p(e),b(lt.$$.fragment,e),io=p(e),b(it.$$.fragment,e),uo=p(e),F=a(e,"P",{});var Xe=l(F);la=o(Xe,"Il existe \xE9galement des variantes des "),hs=a(Xe,"EM",{});var pm=l(hs);ia=o(pm,"tokenizers"),pm.forEach(s),ua=o(Xe," bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),ks=a(Xe,"EM",{});var cm=l(ks);ma=o(cm,"tokenizers"),cm.forEach(s),pa=o(Xe," nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),_s=a(Xe,"EM",{});var dm=l(_s);ca=o(dm,"tokens"),dm.forEach(s),da=o(Xe," ind\xE9pendants que nous avons dans notre corpus."),Xe.forEach(s),mo=p(e),Vt=a(e,"P",{});var fm=l(Vt);fa=o(fm,"Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),fm.forEach(s),po=p(e),ne=a(e,"P",{});var ns=l(ne);va=o(ns,"Si nous voulons couvrir compl\xE8tement une langue avec un "),bs=a(ns,"EM",{});var vm=l(bs);ha=o(vm,"tokenizer"),vm.forEach(s),ka=o(ns," bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE8re une \xE9norme quantit\xE9 de "),Es=a(ns,"EM",{});var hm=l(Es);_a=o(hm,"tokens"),hm.forEach(s),ba=o(ns,". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),ns.forEach(s),co=p(e),te=a(e,"P",{});var Ut=l(te);Ea=o(Ut,"Enfin, nous avons besoin d\u2019un "),$s=a(Ut,"EM",{});var km=l($s);$a=o(km,"token"),km.forEach(s),qa=o(Ut," personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),qs=a(Ut,"EM",{});var _m=l(qs);ga=o(_m,"token"),_m.forEach(s),xa=o(Ut," \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),se=a(Ut,"UNK",{});var Ze=l(se);za=o(Ze,"; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),gs=a(Ze,"EM",{});var bm=l(gs);wa=o(bm,"tokenizer"),bm.forEach(s),Pa=o(Ze," produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),xs=a(Ze,"EM",{});var Em=l(xs);ja=o(Em,"tokenizer"),Em.forEach(s),ya=o(Ze," transforme le moins de mots possible en "),zs=a(Ze,"EM",{});var $m=l(zs);Ma=o($m,"token"),$m.forEach(s),Aa=o(Ze," inconnu."),Ze.forEach(s),Ut.forEach(s),fo=p(e),oe=a(e,"P",{});var os=l(oe);Ca=o(os,"Une fa\xE7on de r\xE9duire la quantit\xE9 de "),ws=a(os,"EM",{});var qm=l(ws);La=o(qm,"tokens"),qm.forEach(s),Ta=o(os," inconnus est d\u2019aller un niveau plus profond, en utilisant un "),Ps=a(os,"EM",{});var gm=l(Ps);Na=o(gm,"tokenizer"),gm.forEach(s),Da=o(os," bas\xE9 sur les caract\xE8res."),os.forEach(s),vo=p(e),_e=a(e,"H2",{class:!0});var xr=l(_e);Le=a(xr,"A",{id:!0,class:!0,href:!0});var xm=l(Le);js=a(xm,"SPAN",{});var zm=l(js);b(ut.$$.fragment,zm),zm.forEach(s),xm.forEach(s),Ia=p(xr),ys=a(xr,"SPAN",{});var wm=l(ys);Sa=o(wm,"*Tokenizer* bas\xE9 sur les caract\xE8res"),wm.forEach(s),xr.forEach(s),ho=p(e),b(mt.$$.fragment,e),ko=p(e),Te=a(e,"P",{});var zr=l(Te);Ha=o(zr,"Les "),Ms=a(zr,"EM",{});var Pm=l(Ms);Ua=o(Pm,"tokenizers"),Pm.forEach(s),Oa=o(zr," bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),zr.forEach(s),_o=p(e),Ne=a(e,"UL",{});var wr=l(Ne);As=a(wr,"LI",{});var jm=l(As);Ba=o(jm,"le vocabulaire est beaucoup plus petit"),jm.forEach(s),Va=p(wr),pt=a(wr,"LI",{});var Pr=l(pt);Ja=o(Pr,"il y a beaucoup moins de "),Cs=a(Pr,"EM",{});var ym=l(Cs);Ga=o(ym,"tokens"),ym.forEach(s),Ra=o(Pr," hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),Pr.forEach(s),wr.forEach(s),bo=p(e),Jt=a(e,"P",{});var Mm=l(Jt);Fa=o(Mm,"Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),Mm.forEach(s),Eo=p(e),be=a(e,"DIV",{class:!0});var jr=l(be);ct=a(jr,"IMG",{class:!0,src:!0,alt:!0}),Ka=p(jr),dt=a(jr,"IMG",{class:!0,src:!0,alt:!0}),jr.forEach(s),$o=p(e),Gt=a(e,"P",{});var Am=l(Gt);Ya=o(Am,"Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),Am.forEach(s),qo=p(e),N=a(e,"P",{});var W=l(N);Wa=o(W,"Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ls=a(W,"EM",{});var Cm=l(Ls);Qa=o(Cm,"tokens"),Cm.forEach(s),Xa=o(W," \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),Ts=a(W,"EM",{});var Lm=l(Ts);Za=o(Lm,"tokenizer"),Lm.forEach(s),el=o(W," bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Ns=a(W,"EM",{});var Tm=l(Ns);tl=o(Tm,"token"),Tm.forEach(s),sl=o(W,", avec un "),Ds=a(W,"EM",{});var Nm=l(Ds);nl=o(Nm,"tokenizer"),Nm.forEach(s),ol=o(W," bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Is=a(W,"EM",{});var Dm=l(Is);rl=o(Dm,"tokens"),Dm.forEach(s),al=o(W," voire plus."),W.forEach(s),go=p(e),De=a(e,"P",{});var yr=l(De);ll=o(yr,"Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Ss=a(yr,"EM",{});var Im=l(Ss);il=o(Im,"tok\xE9nisation en sous-mots"),Im.forEach(s),ul=o(yr,"."),yr.forEach(s),xo=p(e),Ee=a(e,"H2",{class:!0});var Mr=l(Ee);Ie=a(Mr,"A",{id:!0,class:!0,href:!0});var Sm=l(Ie);Hs=a(Sm,"SPAN",{});var Hm=l(Hs);b(ft.$$.fragment,Hm),Hm.forEach(s),Sm.forEach(s),ml=p(Mr),Us=a(Mr,"SPAN",{});var Um=l(Us);pl=o(Um,"Tok\xE9nisation en sous-mots"),Um.forEach(s),Mr.forEach(s),zo=p(e),b(vt.$$.fragment,e),wo=p(e),Rt=a(e,"P",{});var Om=l(Rt);cl=o(Om,"Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),Om.forEach(s),Po=p(e),Ft=a(e,"P",{});var Bm=l(Ft);dl=o(Bm,"Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),Bm.forEach(s),jo=p(e),Kt=a(e,"P",{});var Vm=l(Kt);fl=o(Vm,"Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB Let\u2019s do tokenization ! \xBB :"),Vm.forEach(s),yo=p(e),$e=a(e,"DIV",{class:!0});var Ar=l($e);ht=a(Ar,"IMG",{class:!0,src:!0,alt:!0}),vl=p(Ar),kt=a(Ar,"IMG",{class:!0,src:!0,alt:!0}),Ar.forEach(s),Mo=p(e),K=a(e,"P",{});var et=l(K);hl=o(et,"Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB tokenization \xBB a \xE9t\xE9 divis\xE9 en \xAB token \xBB et \xAB ization \xBB : deux "),Os=a(et,"EM",{});var Jm=l(Os);kl=o(Jm,"tokens"),Jm.forEach(s),_l=o(et," qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Bs=a(et,"EM",{});var Gm=l(Bs);bl=o(Gm,"tokens"),Gm.forEach(s),El=o(et," sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Vs=a(et,"EM",{});var Rm=l(Vs);$l=o(Rm,"token"),Rm.forEach(s),ql=o(et," inconnu."),et.forEach(s),Ao=p(e),Yt=a(e,"P",{});var Fm=l(Yt);gl=o(Fm,"Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),Fm.forEach(s),Co=p(e),qe=a(e,"H3",{class:!0});var Cr=l(qe);Se=a(Cr,"A",{id:!0,class:!0,href:!0});var Km=l(Se);Js=a(Km,"SPAN",{});var Ym=l(Js);b(_t.$$.fragment,Ym),Ym.forEach(s),Km.forEach(s),xl=p(Cr),Gs=a(Cr,"SPAN",{});var Wm=l(Gs);zl=o(Wm,"Et plus encore !"),Wm.forEach(s),Cr.forEach(s),Lo=p(e),Wt=a(e,"P",{});var Qm=l(Wt);wl=o(Qm,"Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),Qm.forEach(s),To=p(e),re=a(e,"UL",{});var rs=l(re);bt=a(rs,"LI",{});var Lr=l(bt);Pl=o(Lr,"le "),Rs=a(Lr,"EM",{});var Xm=l(Rs);jl=o(Xm,"Byte-level BPE"),Xm.forEach(s),yl=o(Lr," utilis\xE9 par exemple dans le GPT-2"),Lr.forEach(s),Ml=p(rs),Et=a(rs,"LI",{});var Tr=l(Et);Al=o(Tr,"le "),Fs=a(Tr,"EM",{});var Zm=l(Fs);Cl=o(Zm,"WordPiece"),Zm.forEach(s),Ll=o(Tr," utilis\xE9 par exemple dans BERT"),Tr.forEach(s),Tl=p(rs),He=a(rs,"LI",{});var Wn=l(He);Ks=a(Wn,"EM",{});var ep=l(Ks);Nl=o(ep,"SentencePiece"),ep.forEach(s),Dl=o(Wn," ou "),Ys=a(Wn,"EM",{});var tp=l(Ys);Il=o(tp,"Unigram"),tp.forEach(s),Sl=o(Wn,", utilis\xE9s dans plusieurs mod\xE8les multilingues."),Wn.forEach(s),rs.forEach(s),No=p(e),Qt=a(e,"P",{});var sp=l(Qt);Hl=o(sp,"Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),sp.forEach(s),Do=p(e),ge=a(e,"H2",{class:!0});var Nr=l(ge);Ue=a(Nr,"A",{id:!0,class:!0,href:!0});var np=l(Ue);Ws=a(np,"SPAN",{});var op=l(Ws);b($t.$$.fragment,op),op.forEach(s),np.forEach(s),Ul=p(Nr),Qs=a(Nr,"SPAN",{});var rp=l(Qs);Ol=o(rp,"Chargement et sauvegarde"),rp.forEach(s),Nr.forEach(s),Io=p(e),L=a(e,"P",{});var B=l(L);Bl=o(B,"Le chargement et la sauvegarde des "),Xs=a(B,"EM",{});var ap=l(Xs);Vl=o(ap,"tokenizers"),ap.forEach(s),Jl=o(B," est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),Zs=a(B,"CODE",{});var lp=l(Zs);Gl=o(lp,"from_pretrained()"),lp.forEach(s),Rl=o(B," et "),en=a(B,"CODE",{});var ip=l(en);Fl=o(ip,"save_pretrained()"),ip.forEach(s),Kl=o(B,". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),tn=a(B,"EM",{});var up=l(tn);Yl=o(up,"tokenizer"),up.forEach(s),Wl=o(B," (un peu comme l\u2019"),sn=a(B,"EM",{});var mp=l(sn);Ql=o(mp,"architecture"),mp.forEach(s),Xl=o(B," du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),nn=a(B,"EM",{});var pp=l(nn);Zl=o(pp,"poids"),pp.forEach(s),ei=o(B," du mod\xE8le)."),B.forEach(s),So=p(e),Y=a(e,"P",{});var tt=l(Y);ti=o(tt,"Le chargement du "),on=a(tt,"EM",{});var cp=l(on);si=o(cp,"tokenizer"),cp.forEach(s),ni=o(tt," de BERT entra\xEEn\xE9 avec le m\xEAme "),rn=a(tt,"EM",{});var dp=l(rn);oi=o(dp,"checkpoint"),dp.forEach(s),ri=o(tt," que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),an=a(tt,"CODE",{});var fp=l(an);ai=o(fp,"BertTokenizer"),fp.forEach(s),li=o(tt," :"),tt.forEach(s),Ho=p(e),b(qt.$$.fragment,e),Uo=p(e),je.l(e),Xt=p(e),b(gt.$$.fragment,e),Oo=p(e),Oe=a(e,"P",{});var Dr=l(Oe);ii=o(Dr,"Nous pouvons \xE0 pr\xE9sent utiliser le "),ln=a(Dr,"EM",{});var vp=l(ln);ui=o(vp,"tokenizer"),vp.forEach(s),mi=o(Dr," comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),Dr.forEach(s),Bo=p(e),b(xt.$$.fragment,e),Vo=p(e),b(zt.$$.fragment,e),Jo=p(e),Zt=a(e,"P",{});var hp=l(Zt);pi=o(hp,"La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),hp.forEach(s),Go=p(e),b(wt.$$.fragment,e),Ro=p(e),D=a(e,"P",{});var Q=l(D);ci=o(Q,"Nous parlerons plus en d\xE9tail des "),un=a(Q,"CODE",{});var kp=l(un);di=o(kp,"token_type_ids"),kp.forEach(s),fi=o(Q," au "),es=a(Q,"A",{href:!0});var _p=l(es);vi=o(_p,"Chapitre 3"),_p.forEach(s),hi=o(Q," et nous expliquerons la cl\xE9 "),mn=a(Q,"CODE",{});var bp=l(mn);ki=o(bp,"attention_mask"),bp.forEach(s),_i=o(Q," un peu plus tard. Tout d\u2019abord, voyons comment les "),pn=a(Q,"CODE",{});var Ep=l(pn);bi=o(Ep,"input_ids"),Ep.forEach(s),Ei=o(Q," sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),cn=a(Q,"EM",{});var $p=l(cn);$i=o($p,"tokenizer"),$p.forEach(s),qi=o(Q,"."),Q.forEach(s),Fo=p(e),xe=a(e,"H2",{class:!0});var Ir=l(xe);Be=a(Ir,"A",{id:!0,class:!0,href:!0});var qp=l(Be);dn=a(qp,"SPAN",{});var gp=l(dn);b(Pt.$$.fragment,gp),gp.forEach(s),qp.forEach(s),gi=p(Ir),fn=a(Ir,"SPAN",{});var xp=l(fn);xi=o(xp,"Encodage"),xp.forEach(s),Ir.forEach(s),Ko=p(e),b(jt.$$.fragment,e),Yo=p(e),Ve=a(e,"P",{});var Sr=l(Ve);zi=o(Sr,"La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),vn=a(Sr,"EM",{});var zp=l(vn);wi=o(zp,"encodage"),zp.forEach(s),Pi=o(Sr,". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),Sr.forEach(s),Wo=p(e),ae=a(e,"P",{});var as=l(ae);ji=o(as,"Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),hn=a(as,"EM",{});var wp=l(hn);yi=o(wp,"tokens"),wp.forEach(s),Mi=o(as,". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),kn=a(as,"EM",{});var Pp=l(kn);Ai=o(Pp,"tokenizer"),Pp.forEach(s),Ci=o(as," en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),as.forEach(s),Qo=p(e),U=a(e,"P",{});var me=l(U);Li=o(me,"La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),_n=a(me,"EM",{});var jp=l(_n);Ti=o(jp,"tokens"),jp.forEach(s),Ni=o(me," en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),bn=a(me,"EM",{});var yp=l(bn);Di=o(yp,"tokenizer"),yp.forEach(s),Ii=o(me," poss\xE8de un "),En=a(me,"EM",{});var Mp=l(En);Si=o(Mp,"vocabulaire"),Mp.forEach(s),Hi=o(me,", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),$n=a(me,"CODE",{});var Ap=l($n);Ui=o(Ap,"from_pretrained()"),Ap.forEach(s),Oi=o(me,". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),me.forEach(s),Xo=p(e),Je=a(e,"P",{});var Hr=l(Je);Bi=o(Hr,"Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),qn=a(Hr,"EM",{});var Cp=l(qn);Vi=o(Cp,"tokenizer"),Cp.forEach(s),Ji=o(Hr," directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),Hr.forEach(s),Zo=p(e),ze=a(e,"H3",{class:!0});var Ur=l(ze);Ge=a(Ur,"A",{id:!0,class:!0,href:!0});var Lp=l(Ge);gn=a(Lp,"SPAN",{});var Tp=l(gn);b(yt.$$.fragment,Tp),Tp.forEach(s),Lp.forEach(s),Gi=p(Ur),xn=a(Ur,"SPAN",{});var Np=l(xn);Ri=o(Np,"Tokenisation"),Np.forEach(s),Ur.forEach(s),er=p(e),le=a(e,"P",{});var ls=l(le);Fi=o(ls,"Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),zn=a(ls,"CODE",{});var Dp=l(zn);Ki=o(Dp,"tokenize()"),Dp.forEach(s),Yi=o(ls," du "),wn=a(ls,"EM",{});var Ip=l(wn);Wi=o(Ip,"tokenizer"),Ip.forEach(s),Qi=o(ls," :"),ls.forEach(s),tr=p(e),b(Mt.$$.fragment,e),sr=p(e),Re=a(e,"P",{});var Or=l(Re);Xi=o(Or,"La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),Pn=a(Or,"EM",{});var Sp=l(Pn);Zi=o(Sp,"tokens"),Sp.forEach(s),eu=o(Or," :"),Or.forEach(s),nr=p(e),b(At.$$.fragment,e),or=p(e),j=a(e,"P",{});var I=l(j);tu=o(I,"Ce "),jn=a(I,"EM",{});var Hp=l(jn);su=o(Hp,"tokenizer"),Hp.forEach(s),nu=o(I," est un "),yn=a(I,"EM",{});var Up=l(yn);ou=o(Up,"tokenizer"),Up.forEach(s),ru=o(I," de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),Mn=a(I,"EM",{});var Op=l(Mn);au=o(Op,"tokens"),Op.forEach(s),lu=o(I," qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),An=a(I,"CODE",{});var Bp=l(An);iu=o(Bp,"transformer"),Bp.forEach(s),uu=o(I," qui est divis\xE9 en deux "),Cn=a(I,"EM",{});var Vp=l(Cn);mu=o(Vp,"tokens"),Vp.forEach(s),pu=o(I," : "),Ln=a(I,"CODE",{});var Jp=l(Ln);cu=o(Jp,"transform"),Jp.forEach(s),du=o(I," et "),Tn=a(I,"CODE",{});var Gp=l(Tn);fu=o(Gp,"##er"),Gp.forEach(s),vu=o(I,"."),I.forEach(s),rr=p(e),we=a(e,"H3",{class:!0});var Br=l(we);Fe=a(Br,"A",{id:!0,class:!0,href:!0});var Rp=l(Fe);Nn=a(Rp,"SPAN",{});var Fp=l(Nn);b(Ct.$$.fragment,Fp),Fp.forEach(s),Rp.forEach(s),hu=p(Br),Dn=a(Br,"SPAN",{});var Kp=l(Dn);ku=o(Kp,"De *tokens* aux identifiants d'entr\xE9e"),Kp.forEach(s),Br.forEach(s),ar=p(e),ie=a(e,"P",{});var is=l(ie);_u=o(is,"La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),In=a(is,"CODE",{});var Yp=l(In);bu=o(Yp,"convert_tokens_to_ids()"),Yp.forEach(s),Eu=o(is," du "),Sn=a(is,"EM",{});var Wp=l(Sn);$u=o(Wp,"tokenizer"),Wp.forEach(s),qu=o(is," :"),is.forEach(s),lr=p(e),b(Lt.$$.fragment,e),ir=p(e),b(Tt.$$.fragment,e),ur=p(e),Ke=a(e,"P",{});var Vr=l(Ke);gu=o(Vr,"Une fois converties en tenseur dans le "),Hn=a(Vr,"EM",{});var Qp=l(Hn);xu=o(Qp,"framework"),Qp.forEach(s),zu=o(Vr," appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),Vr.forEach(s),mr=p(e),b(Ye.$$.fragment,e),pr=p(e),Pe=a(e,"H2",{class:!0});var Jr=l(Pe);We=a(Jr,"A",{id:!0,class:!0,href:!0});var Xp=l(We);Un=a(Xp,"SPAN",{});var Zp=l(Un);b(Nt.$$.fragment,Zp),Zp.forEach(s),Xp.forEach(s),wu=p(Jr),On=a(Jr,"SPAN",{});var ec=l(On);Pu=o(ec,"D\xE9codage"),ec.forEach(s),Jr.forEach(s),cr=p(e),ue=a(e,"P",{});var us=l(ue);ju=o(us,"Le "),Bn=a(us,"EM",{});var tc=l(Bn);yu=o(tc,"d\xE9codage"),tc.forEach(s),Mu=o(us," va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Vn=a(us,"CODE",{});var sc=l(Vn);Au=o(sc,"decode()"),sc.forEach(s),Cu=o(us," comme suit :"),us.forEach(s),dr=p(e),b(Dt.$$.fragment,e),fr=p(e),b(It.$$.fragment,e),vr=p(e),O=a(e,"P",{});var pe=l(O);Lu=o(pe,"Notez que la m\xE9thode "),Jn=a(pe,"CODE",{});var nc=l(Jn);Tu=o(nc,"decode"),nc.forEach(s),Nu=o(pe," non seulement reconvertit les indices en "),Gn=a(pe,"EM",{});var oc=l(Gn);Du=o(oc,"tokens"),oc.forEach(s),Iu=o(pe," mais regroupe \xE9galement les "),Rn=a(pe,"EM",{});var rc=l(Rn);Su=o(rc,"tokens"),rc.forEach(s),Hu=o(pe," faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),Fn=a(pe,"EM",{});var ac=l(Fn);Uu=o(ac,"prompt"),ac.forEach(s),Ou=o(pe,", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),pe.forEach(s),hr=p(e),Qe=a(e,"P",{});var Gr=l(Qe);Bu=o(Gr,"Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),Kn=a(Gr,"EM",{});var lc=l(Kn);Vu=o(lc,"tokenizer"),lc.forEach(s),Ju=o(Gr," peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),Gr.forEach(s),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(xc)),d(g,"id","les-tokenizers"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#les-tokenizers"),d(w,"class","relative group"),d(Me,"id","tokenizer-bas-sur-les-mots"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#tokenizer-bas-sur-les-mots"),d(he,"class","relative group"),d(rt,"class","block dark:hidden"),ms(rt.src,Gu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||d(rt,"src",Gu),d(rt,"alt","An example of word-based tokenization."),d(at,"class","hidden dark:block"),ms(at.src,Ru="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||d(at,"src",Ru),d(at,"alt","An example of word-based tokenization."),d(ke,"class","flex justify-center"),d(Le,"id","tokenizer-bas-sur-les-caractres"),d(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Le,"href","#tokenizer-bas-sur-les-caractres"),d(_e,"class","relative group"),d(ct,"class","block dark:hidden"),ms(ct.src,Fu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||d(ct,"src",Fu),d(ct,"alt","An example of character-based tokenization."),d(dt,"class","hidden dark:block"),ms(dt.src,Ku="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||d(dt,"src",Ku),d(dt,"alt","An example of character-based tokenization."),d(be,"class","flex justify-center"),d(Ie,"id","toknisation-en-sousmots"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#toknisation-en-sousmots"),d(Ee,"class","relative group"),d(ht,"class","block dark:hidden"),ms(ht.src,Yu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||d(ht,"src",Yu),d(ht,"alt","A subword tokenization algorithm."),d(kt,"class","hidden dark:block"),ms(kt.src,Wu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||d(kt,"src",Wu),d(kt,"alt","A subword tokenization algorithm."),d($e,"class","flex justify-center"),d(Se,"id","et-plus-encore"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#et-plus-encore"),d(qe,"class","relative group"),d(Ue,"id","chargement-et-sauvegarde"),d(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ue,"href","#chargement-et-sauvegarde"),d(ge,"class","relative group"),d(es,"href","/course/fr/chapter3"),d(Be,"id","encodage"),d(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Be,"href","#encodage"),d(xe,"class","relative group"),d(Ge,"id","tokenisation"),d(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ge,"href","#tokenisation"),d(ze,"class","relative group"),d(Fe,"id","de-tokens-aux-identifiants-dentre"),d(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fe,"href","#de-tokens-aux-identifiants-dentre"),d(we,"class","relative group"),d(We,"id","dcodage"),d(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(We,"href","#dcodage"),d(Pe,"class","relative group")},m(e,i){t(document.head,c),u(e,q,i),E(f,e,i),u(e,x,i),u(e,w,i),t(w,g),t(g,P),E(A,P,null),t(w,S),t(w,G),t(G,X),u(e,T,i),St[y].m(e,i),u(e,C,i),E(H,e,i),u(e,R,i),u(e,z,i),t(z,k),t(z,Z),t(Z,de),t(z,fe),t(z,ee),t(ee,ve),t(z,Rr),u(e,Xn,i),u(e,Ot,i),t(Ot,Fr),u(e,Zn,i),E(st,e,i),u(e,eo,i),u(e,ye,i),t(ye,Kr),t(ye,ps),t(ps,Yr),t(ye,Wr),u(e,to,i),u(e,Bt,i),t(Bt,Qr),u(e,so,i),u(e,he,i),t(he,Me),t(Me,cs),E(nt,cs,null),t(he,Xr),t(he,ds),t(ds,Zr),u(e,no,i),E(ot,e,i),u(e,oo,i),u(e,Ae,i),t(Ae,ea),t(Ae,fs),t(fs,ta),t(Ae,sa),u(e,ro,i),u(e,ke,i),t(ke,rt),t(ke,na),t(ke,at),u(e,ao,i),u(e,Ce,i),t(Ce,oa),t(Ce,vs),t(vs,ra),t(Ce,aa),u(e,lo,i),E(lt,e,i),u(e,io,i),E(it,e,i),u(e,uo,i),u(e,F,i),t(F,la),t(F,hs),t(hs,ia),t(F,ua),t(F,ks),t(ks,ma),t(F,pa),t(F,_s),t(_s,ca),t(F,da),u(e,mo,i),u(e,Vt,i),t(Vt,fa),u(e,po,i),u(e,ne,i),t(ne,va),t(ne,bs),t(bs,ha),t(ne,ka),t(ne,Es),t(Es,_a),t(ne,ba),u(e,co,i),u(e,te,i),t(te,Ea),t(te,$s),t($s,$a),t(te,qa),t(te,qs),t(qs,ga),t(te,xa),t(te,se),t(se,za),t(se,gs),t(gs,wa),t(se,Pa),t(se,xs),t(xs,ja),t(se,ya),t(se,zs),t(zs,Ma),t(se,Aa),u(e,fo,i),u(e,oe,i),t(oe,Ca),t(oe,ws),t(ws,La),t(oe,Ta),t(oe,Ps),t(Ps,Na),t(oe,Da),u(e,vo,i),u(e,_e,i),t(_e,Le),t(Le,js),E(ut,js,null),t(_e,Ia),t(_e,ys),t(ys,Sa),u(e,ho,i),E(mt,e,i),u(e,ko,i),u(e,Te,i),t(Te,Ha),t(Te,Ms),t(Ms,Ua),t(Te,Oa),u(e,_o,i),u(e,Ne,i),t(Ne,As),t(As,Ba),t(Ne,Va),t(Ne,pt),t(pt,Ja),t(pt,Cs),t(Cs,Ga),t(pt,Ra),u(e,bo,i),u(e,Jt,i),t(Jt,Fa),u(e,Eo,i),u(e,be,i),t(be,ct),t(be,Ka),t(be,dt),u(e,$o,i),u(e,Gt,i),t(Gt,Ya),u(e,qo,i),u(e,N,i),t(N,Wa),t(N,Ls),t(Ls,Qa),t(N,Xa),t(N,Ts),t(Ts,Za),t(N,el),t(N,Ns),t(Ns,tl),t(N,sl),t(N,Ds),t(Ds,nl),t(N,ol),t(N,Is),t(Is,rl),t(N,al),u(e,go,i),u(e,De,i),t(De,ll),t(De,Ss),t(Ss,il),t(De,ul),u(e,xo,i),u(e,Ee,i),t(Ee,Ie),t(Ie,Hs),E(ft,Hs,null),t(Ee,ml),t(Ee,Us),t(Us,pl),u(e,zo,i),E(vt,e,i),u(e,wo,i),u(e,Rt,i),t(Rt,cl),u(e,Po,i),u(e,Ft,i),t(Ft,dl),u(e,jo,i),u(e,Kt,i),t(Kt,fl),u(e,yo,i),u(e,$e,i),t($e,ht),t($e,vl),t($e,kt),u(e,Mo,i),u(e,K,i),t(K,hl),t(K,Os),t(Os,kl),t(K,_l),t(K,Bs),t(Bs,bl),t(K,El),t(K,Vs),t(Vs,$l),t(K,ql),u(e,Ao,i),u(e,Yt,i),t(Yt,gl),u(e,Co,i),u(e,qe,i),t(qe,Se),t(Se,Js),E(_t,Js,null),t(qe,xl),t(qe,Gs),t(Gs,zl),u(e,Lo,i),u(e,Wt,i),t(Wt,wl),u(e,To,i),u(e,re,i),t(re,bt),t(bt,Pl),t(bt,Rs),t(Rs,jl),t(bt,yl),t(re,Ml),t(re,Et),t(Et,Al),t(Et,Fs),t(Fs,Cl),t(Et,Ll),t(re,Tl),t(re,He),t(He,Ks),t(Ks,Nl),t(He,Dl),t(He,Ys),t(Ys,Il),t(He,Sl),u(e,No,i),u(e,Qt,i),t(Qt,Hl),u(e,Do,i),u(e,ge,i),t(ge,Ue),t(Ue,Ws),E($t,Ws,null),t(ge,Ul),t(ge,Qs),t(Qs,Ol),u(e,Io,i),u(e,L,i),t(L,Bl),t(L,Xs),t(Xs,Vl),t(L,Jl),t(L,Zs),t(Zs,Gl),t(L,Rl),t(L,en),t(en,Fl),t(L,Kl),t(L,tn),t(tn,Yl),t(L,Wl),t(L,sn),t(sn,Ql),t(L,Xl),t(L,nn),t(nn,Zl),t(L,ei),u(e,So,i),u(e,Y,i),t(Y,ti),t(Y,on),t(on,si),t(Y,ni),t(Y,rn),t(rn,oi),t(Y,ri),t(Y,an),t(an,ai),t(Y,li),u(e,Ho,i),E(qt,e,i),u(e,Uo,i),je.m(e,i),u(e,Xt,i),E(gt,e,i),u(e,Oo,i),u(e,Oe,i),t(Oe,ii),t(Oe,ln),t(ln,ui),t(Oe,mi),u(e,Bo,i),E(xt,e,i),u(e,Vo,i),E(zt,e,i),u(e,Jo,i),u(e,Zt,i),t(Zt,pi),u(e,Go,i),E(wt,e,i),u(e,Ro,i),u(e,D,i),t(D,ci),t(D,un),t(un,di),t(D,fi),t(D,es),t(es,vi),t(D,hi),t(D,mn),t(mn,ki),t(D,_i),t(D,pn),t(pn,bi),t(D,Ei),t(D,cn),t(cn,$i),t(D,qi),u(e,Fo,i),u(e,xe,i),t(xe,Be),t(Be,dn),E(Pt,dn,null),t(xe,gi),t(xe,fn),t(fn,xi),u(e,Ko,i),E(jt,e,i),u(e,Yo,i),u(e,Ve,i),t(Ve,zi),t(Ve,vn),t(vn,wi),t(Ve,Pi),u(e,Wo,i),u(e,ae,i),t(ae,ji),t(ae,hn),t(hn,yi),t(ae,Mi),t(ae,kn),t(kn,Ai),t(ae,Ci),u(e,Qo,i),u(e,U,i),t(U,Li),t(U,_n),t(_n,Ti),t(U,Ni),t(U,bn),t(bn,Di),t(U,Ii),t(U,En),t(En,Si),t(U,Hi),t(U,$n),t($n,Ui),t(U,Oi),u(e,Xo,i),u(e,Je,i),t(Je,Bi),t(Je,qn),t(qn,Vi),t(Je,Ji),u(e,Zo,i),u(e,ze,i),t(ze,Ge),t(Ge,gn),E(yt,gn,null),t(ze,Gi),t(ze,xn),t(xn,Ri),u(e,er,i),u(e,le,i),t(le,Fi),t(le,zn),t(zn,Ki),t(le,Yi),t(le,wn),t(wn,Wi),t(le,Qi),u(e,tr,i),E(Mt,e,i),u(e,sr,i),u(e,Re,i),t(Re,Xi),t(Re,Pn),t(Pn,Zi),t(Re,eu),u(e,nr,i),E(At,e,i),u(e,or,i),u(e,j,i),t(j,tu),t(j,jn),t(jn,su),t(j,nu),t(j,yn),t(yn,ou),t(j,ru),t(j,Mn),t(Mn,au),t(j,lu),t(j,An),t(An,iu),t(j,uu),t(j,Cn),t(Cn,mu),t(j,pu),t(j,Ln),t(Ln,cu),t(j,du),t(j,Tn),t(Tn,fu),t(j,vu),u(e,rr,i),u(e,we,i),t(we,Fe),t(Fe,Nn),E(Ct,Nn,null),t(we,hu),t(we,Dn),t(Dn,ku),u(e,ar,i),u(e,ie,i),t(ie,_u),t(ie,In),t(In,bu),t(ie,Eu),t(ie,Sn),t(Sn,$u),t(ie,qu),u(e,lr,i),E(Lt,e,i),u(e,ir,i),E(Tt,e,i),u(e,ur,i),u(e,Ke,i),t(Ke,gu),t(Ke,Hn),t(Hn,xu),t(Ke,zu),u(e,mr,i),E(Ye,e,i),u(e,pr,i),u(e,Pe,i),t(Pe,We),t(We,Un),E(Nt,Un,null),t(Pe,wu),t(Pe,On),t(On,Pu),u(e,cr,i),u(e,ue,i),t(ue,ju),t(ue,Bn),t(Bn,yu),t(ue,Mu),t(ue,Vn),t(Vn,Au),t(ue,Cu),u(e,dr,i),E(Dt,e,i),u(e,fr,i),E(It,e,i),u(e,vr,i),u(e,O,i),t(O,Lu),t(O,Jn),t(Jn,Tu),t(O,Nu),t(O,Gn),t(Gn,Du),t(O,Iu),t(O,Rn),t(Rn,Su),t(O,Hu),t(O,Fn),t(Fn,Uu),t(O,Ou),u(e,hr,i),u(e,Qe,i),t(Qe,Bu),t(Qe,Kn),t(Kn,Vu),t(Qe,Ju),kr=!0},p(e,[i]){const Ht={};i&1&&(Ht.fw=e[0]),f.$set(Ht);let ts=y;y=Xu(e),y!==ts&&(vc(),v(St[ts],1,1,()=>{St[ts]=null}),dc(),M=St[y],M||(M=St[y]=Qu[y](e),M.c()),h(M,1),M.m(C.parentNode,C)),_r!==(_r=Zu(e))&&(je.d(1),je=_r(e),je&&(je.c(),je.m(Xt.parentNode,Xt)));const Yn={};i&2&&(Yn.$$scope={dirty:i,ctx:e}),Ye.$set(Yn)},i(e){kr||(h(f.$$.fragment,e),h(A.$$.fragment,e),h(M),h(H.$$.fragment,e),h(st.$$.fragment,e),h(nt.$$.fragment,e),h(ot.$$.fragment,e),h(lt.$$.fragment,e),h(it.$$.fragment,e),h(ut.$$.fragment,e),h(mt.$$.fragment,e),h(ft.$$.fragment,e),h(vt.$$.fragment,e),h(_t.$$.fragment,e),h($t.$$.fragment,e),h(qt.$$.fragment,e),h(gt.$$.fragment,e),h(xt.$$.fragment,e),h(zt.$$.fragment,e),h(wt.$$.fragment,e),h(Pt.$$.fragment,e),h(jt.$$.fragment,e),h(yt.$$.fragment,e),h(Mt.$$.fragment,e),h(At.$$.fragment,e),h(Ct.$$.fragment,e),h(Lt.$$.fragment,e),h(Tt.$$.fragment,e),h(Ye.$$.fragment,e),h(Nt.$$.fragment,e),h(Dt.$$.fragment,e),h(It.$$.fragment,e),kr=!0)},o(e){v(f.$$.fragment,e),v(A.$$.fragment,e),v(M),v(H.$$.fragment,e),v(st.$$.fragment,e),v(nt.$$.fragment,e),v(ot.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(ft.$$.fragment,e),v(vt.$$.fragment,e),v(_t.$$.fragment,e),v($t.$$.fragment,e),v(qt.$$.fragment,e),v(gt.$$.fragment,e),v(xt.$$.fragment,e),v(zt.$$.fragment,e),v(wt.$$.fragment,e),v(Pt.$$.fragment,e),v(jt.$$.fragment,e),v(yt.$$.fragment,e),v(Mt.$$.fragment,e),v(At.$$.fragment,e),v(Ct.$$.fragment,e),v(Lt.$$.fragment,e),v(Tt.$$.fragment,e),v(Ye.$$.fragment,e),v(Nt.$$.fragment,e),v(Dt.$$.fragment,e),v(It.$$.fragment,e),kr=!1},d(e){s(c),e&&s(q),$(f,e),e&&s(x),e&&s(w),$(A),e&&s(T),St[y].d(e),e&&s(C),$(H,e),e&&s(R),e&&s(z),e&&s(Xn),e&&s(Ot),e&&s(Zn),$(st,e),e&&s(eo),e&&s(ye),e&&s(to),e&&s(Bt),e&&s(so),e&&s(he),$(nt),e&&s(no),$(ot,e),e&&s(oo),e&&s(Ae),e&&s(ro),e&&s(ke),e&&s(ao),e&&s(Ce),e&&s(lo),$(lt,e),e&&s(io),$(it,e),e&&s(uo),e&&s(F),e&&s(mo),e&&s(Vt),e&&s(po),e&&s(ne),e&&s(co),e&&s(te),e&&s(fo),e&&s(oe),e&&s(vo),e&&s(_e),$(ut),e&&s(ho),$(mt,e),e&&s(ko),e&&s(Te),e&&s(_o),e&&s(Ne),e&&s(bo),e&&s(Jt),e&&s(Eo),e&&s(be),e&&s($o),e&&s(Gt),e&&s(qo),e&&s(N),e&&s(go),e&&s(De),e&&s(xo),e&&s(Ee),$(ft),e&&s(zo),$(vt,e),e&&s(wo),e&&s(Rt),e&&s(Po),e&&s(Ft),e&&s(jo),e&&s(Kt),e&&s(yo),e&&s($e),e&&s(Mo),e&&s(K),e&&s(Ao),e&&s(Yt),e&&s(Co),e&&s(qe),$(_t),e&&s(Lo),e&&s(Wt),e&&s(To),e&&s(re),e&&s(No),e&&s(Qt),e&&s(Do),e&&s(ge),$($t),e&&s(Io),e&&s(L),e&&s(So),e&&s(Y),e&&s(Ho),$(qt,e),e&&s(Uo),je.d(e),e&&s(Xt),$(gt,e),e&&s(Oo),e&&s(Oe),e&&s(Bo),$(xt,e),e&&s(Vo),$(zt,e),e&&s(Jo),e&&s(Zt),e&&s(Go),$(wt,e),e&&s(Ro),e&&s(D),e&&s(Fo),e&&s(xe),$(Pt),e&&s(Ko),$(jt,e),e&&s(Yo),e&&s(Ve),e&&s(Wo),e&&s(ae),e&&s(Qo),e&&s(U),e&&s(Xo),e&&s(Je),e&&s(Zo),e&&s(ze),$(yt),e&&s(er),e&&s(le),e&&s(tr),$(Mt,e),e&&s(sr),e&&s(Re),e&&s(nr),$(At,e),e&&s(or),e&&s(j),e&&s(rr),e&&s(we),$(Ct),e&&s(ar),e&&s(ie),e&&s(lr),$(Lt,e),e&&s(ir),$(Tt,e),e&&s(ur),e&&s(Ke),e&&s(mr),$(Ye,e),e&&s(pr),e&&s(Pe),$(Nt),e&&s(cr),e&&s(ue),e&&s(dr),$(Dt,e),e&&s(fr),$(It,e),e&&s(vr),e&&s(O),e&&s(hr),e&&s(Qe)}}}const xc={local:"les-tokenizers",sections:[{local:"tokenizer-bas-sur-les-mots",title:"*Tokenizer* bas\xE9 sur les mots"},{local:"tokenizer-bas-sur-les-caractres",title:"*Tokenizer* bas\xE9 sur les caract\xE8res"},{local:"toknisation-en-sousmots",sections:[{local:"et-plus-encore",title:"Et plus encore !"}],title:"Tok\xE9nisation en sous-mots"},{local:"chargement-et-sauvegarde",title:"Chargement et sauvegarde"},{local:"encodage",sections:[{local:"tokenisation",title:"Tokenisation"},{local:"de-tokens-aux-identifiants-dentre",title:"De *tokens* aux identifiants d'entr\xE9e"}],title:"Encodage"},{local:"dcodage",title:"D\xE9codage"}],title:"Les *tokenizers*"};function zc(J,c,q){let f="pt";return fc(()=>{const x=new URLSearchParams(window.location.search);q(0,f=x.get("fw")||"pt")}),[f]}class Lc extends uc{constructor(c){super();mc(this,c,zc,gc,pc,{})}}export{Lc as default,xc as metadata};
