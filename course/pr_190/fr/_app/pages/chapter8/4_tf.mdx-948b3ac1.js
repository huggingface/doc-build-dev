import{S as Zd,i as sb,s as eb,e as r,k as i,w as b,t as a,M as nb,c as t,d as n,m,x as h,a as p,h as l,b as c,F as e,g as o,y as f,q as v,o as j,B as q,v as ab}from"../../chunks/vendor-1e8b365d.js";import{T as Yl}from"../../chunks/Tip-62b14c6e.js";import{Y as lb}from"../../chunks/Youtube-c2a8cc39.js";import{I as ss}from"../../chunks/IconCopyLink-483c28ba.js";import{C as k}from"../../chunks/CodeBlock-e5764662.js";import{D as rb}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as tb}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function pb(V){let d,w,_,z,C,E,y,P,N,D,ps,U,es,Ds,T,I,qs,_s,ve,us,Ss,os,$s;return{c(){d=r("p"),w=a("\u270F\uFE0F "),_=r("em"),z=a("A votre tour !"),C=a(" Comme d\xE9fi optionnel apr\xE8s avoir r\xE9solu les autres probl\xE8mes, vous pouvez essayer de revenir \xE0 cette \xE9tape et faire fonctionner le mod\xE8le avec la perte originale calcul\xE9e par Keras au lieu de la perte interne. Vous devrez ajouter "),E=r("code"),y=a('"labels"'),P=a(" \xE0 l\u2019argument "),N=r("code"),D=a("label_cols"),ps=a(" de "),U=r("code"),es=a("to_tf_dataset()"),Ds=a(" pour vous assurer que les labels sont correctement sortis, ce qui vous donnera des gradients. Mais il y a un autre probl\xE8me avec la perte que nous avons sp\xE9cifi\xE9e. L\u2019entra\xEEnement fonctionnera toujours avec ce probl\xE8me mais l\u2019apprentissage sera tr\xE8s lent et se stabilisera \xE0 une perte d\u2019entra\xEEnement \xE9lev\xE9e. Pouvez-vous trouver ce que c\u2019est ?"),T=i(),I=r("p"),qs=a("Un indice cod\xE9 en ROT13, si vous \xEAtes coinc\xE9 : Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),_s=r("code"),ve=a("ybtvgf"),us=a(". Jung ner ybtvgf ?"),Ss=i(),os=r("p"),$s=a("Et un deuxi\xE8me indice : Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf ny gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf ?")},l(x){d=t(x,"P",{});var A=p(d);w=l(A,"\u270F\uFE0F "),_=t(A,"EM",{});var je=p(_);z=l(je,"A votre tour !"),je.forEach(n),C=l(A," Comme d\xE9fi optionnel apr\xE8s avoir r\xE9solu les autres probl\xE8mes, vous pouvez essayer de revenir \xE0 cette \xE9tape et faire fonctionner le mod\xE8le avec la perte originale calcul\xE9e par Keras au lieu de la perte interne. Vous devrez ajouter "),E=t(A,"CODE",{});var G=p(E);y=l(G,'"labels"'),G.forEach(n),P=l(A," \xE0 l\u2019argument "),N=t(A,"CODE",{});var ns=p(N);D=l(ns,"label_cols"),ns.forEach(n),ps=l(A," de "),U=t(A,"CODE",{});var Ts=p(U);es=l(Ts,"to_tf_dataset()"),Ts.forEach(n),Ds=l(A," pour vous assurer que les labels sont correctement sortis, ce qui vous donnera des gradients. Mais il y a un autre probl\xE8me avec la perte que nous avons sp\xE9cifi\xE9e. L\u2019entra\xEEnement fonctionnera toujours avec ce probl\xE8me mais l\u2019apprentissage sera tr\xE8s lent et se stabilisera \xE0 une perte d\u2019entra\xEEnement \xE9lev\xE9e. Pouvez-vous trouver ce que c\u2019est ?"),A.forEach(n),T=m(x),I=t(x,"P",{});var K=p(I);qs=l(K,"Un indice cod\xE9 en ROT13, si vous \xEAtes coinc\xE9 : Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),_s=t(K,"CODE",{});var hn=p(_s);ve=l(hn,"ybtvgf"),hn.forEach(n),us=l(K,". Jung ner ybtvgf ?"),K.forEach(n),Ss=m(x),os=t(x,"P",{});var Os=p(os);$s=l(Os,"Et un deuxi\xE8me indice : Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf ny gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf ?"),Os.forEach(n)},m(x,A){o(x,d,A),e(d,w),e(d,_),e(_,z),e(d,C),e(d,E),e(E,y),e(d,P),e(d,N),e(N,D),e(d,ps),e(d,U),e(U,es),e(d,Ds),o(x,T,A),o(x,I,A),e(I,qs),e(I,_s),e(_s,ve),e(I,us),o(x,Ss,A),o(x,os,A),e(os,$s)},d(x){x&&n(d),x&&n(T),x&&n(I),x&&n(Ss),x&&n(os)}}}function ub(V){let d,w,_,z,C,E,y,P;return{c(){d=r("p"),w=a("\u{1F4A1} Vous pouvez \xE9galement importer la fonction "),_=r("code"),z=a("create_optimizer()"),C=a(" de \u{1F917} "),E=r("i"),y=a("Transformers"),P=a(" qui vous donnera un optimiseur AdamW avec une d\xE9croissance du taux des poids correcte ainsi qu\u2019un r\xE9chauffement et une d\xE9croissance du taux d\u2019apprentissage. Cet optimiseur produira souvent des r\xE9sultats l\xE9g\xE8rement meilleurs que ceux que vous obtenez avec l\u2019optimiseur Adam par d\xE9faut.")},l(N){d=t(N,"P",{});var D=p(d);w=l(D,"\u{1F4A1} Vous pouvez \xE9galement importer la fonction "),_=t(D,"CODE",{});var ps=p(_);z=l(ps,"create_optimizer()"),ps.forEach(n),C=l(D," de \u{1F917} "),E=t(D,"I",{});var U=p(E);y=l(U,"Transformers"),U.forEach(n),P=l(D," qui vous donnera un optimiseur AdamW avec une d\xE9croissance du taux des poids correcte ainsi qu\u2019un r\xE9chauffement et une d\xE9croissance du taux d\u2019apprentissage. Cet optimiseur produira souvent des r\xE9sultats l\xE9g\xE8rement meilleurs que ceux que vous obtenez avec l\u2019optimiseur Adam par d\xE9faut."),D.forEach(n)},m(N,D){o(N,d,D),e(d,w),e(d,_),e(_,z),e(d,C),e(d,E),e(E,y),e(d,P)},d(N){N&&n(d)}}}function ob(V){let d,w,_,z,C;return{c(){d=r("p"),w=a("Dans la prochaine partie du cours, nous examinerons des techniques plus avanc\xE9es qui peuvent vous aider \xE0 r\xE9duire votre empreinte m\xE9moire et vous permettre de "),_=r("i"),z=a("finetuner"),C=a(" les plus grands mod\xE8les.")},l(E){d=t(E,"P",{});var y=p(d);w=l(y,"Dans la prochaine partie du cours, nous examinerons des techniques plus avanc\xE9es qui peuvent vous aider \xE0 r\xE9duire votre empreinte m\xE9moire et vous permettre de "),_=t(y,"I",{});var P=p(_);z=l(P,"finetuner"),P.forEach(n),C=l(y," les plus grands mod\xE8les."),y.forEach(n)},m(E,y){o(E,d,y),e(d,w),e(d,_),e(_,z),e(d,C)},d(E){E&&n(d)}}}function ib(V){let d,w;return{c(){d=r("p"),w=a("\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes.")},l(_){d=t(_,"P",{});var z=p(d);w=l(z,"\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes."),z.forEach(n)},m(_,z){o(_,d,z),e(d,w)},d(_){_&&n(d)}}}function mb(V){let d,w,_,z,C;return{c(){d=r("p"),w=a("\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),_=r("code"),z=a("Trainer"),C=a(" apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet.")},l(E){d=t(E,"P",{});var y=p(d);w=l(y,"\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),_=t(y,"CODE",{});var P=p(_);z=l(P,"Trainer"),P.forEach(n),C=l(y," apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet."),y.forEach(n)},m(E,y){o(E,d,y),e(d,w),e(d,_),e(_,z),e(d,C)},d(E){E&&n(d)}}}function cb(V){let d,w,_,z,C,E,y,P,N,D,ps,U,es,Ds,T,I,qs,_s,ve,us,Ss,os,$s,x,A,je,G,ns,Ts,K,hn,Os,cp,Wl,qe,Zl,is,dp,Rn,bp,hp,Bn,fp,vp,sr,Ms,jp,Jn,qp,_p,er,ms,$p,Xn,Ep,gp,_e,zp,yp,nr,$e,ar,Ns,wp,Qn,kp,Cp,lr,fn,Pp,rr,Ee,tr,vn,xp,pr,Es,Ls,Yn,ge,Ap,Wn,Dp,ur,jn,Sp,or,S,Tp,Zn,Op,Mp,sa,Np,Lp,ea,Fp,Vp,na,Up,Ip,aa,Gp,Kp,la,Hp,Rp,ir,ze,mr,as,ra,Bp,Jp,ta,Xp,Qp,pa,Yp,Wp,cr,ye,dr,H,Zp,ua,su,eu,oa,nu,au,ia,lu,ru,br,cs,tu,ma,pu,uu,ca,ou,iu,hr,ds,mu,da,cu,du,ba,bu,hu,fr,we,vr,qn,fu,jr,Fs,qr,Vs,vu,ha,ju,qu,_r,ke,$r,_n,_u,Er,Ce,fa,$u,Eu,gr,gs,Us,va,Pe,gu,ja,zu,zr,ls,qa,yu,wu,_a,ku,Cu,$a,Pu,xu,yr,bs,Au,Ea,Du,Su,ga,Tu,Ou,wr,xe,kr,Ae,Cr,$,Mu,za,Nu,Lu,ya,Fu,Vu,wa,Uu,Iu,ka,Gu,Ku,Ca,Hu,Ru,Pa,Bu,Ju,xa,Xu,Qu,Aa,Yu,Wu,Da,Zu,so,Sa,eo,no,Ta,ao,lo,Oa,ro,to,Ma,po,uo,Na,oo,io,Pr,R,mo,La,co,bo,Fa,ho,fo,Va,vo,jo,xr,De,Ar,$n,qo,Dr,Se,Sr,rs,Ua,_o,$o,Ia,Eo,go,Ga,zo,yo,Tr,Te,Or,Oe,Mr,En,wo,Nr,Me,Lr,Ne,Fr,gn,ko,Vr,Le,Ur,Fe,Ir,hs,Co,Ka,Po,xo,Ha,Ao,Do,Gr,Ve,Kr,Ue,Hr,Is,So,Ra,To,Oo,Rr,Ie,Br,Ge,Jr,Gs,Mo,Ba,No,Lo,Xr,zs,Ks,Ja,Ke,Fo,Xa,Vo,Qr,fs,Uo,Qa,Io,Go,Ya,Ko,Ho,Yr,Hs,Ro,He,Bo,Jo,Wr,L,Xo,Wa,Qo,Yo,Za,Wo,Zo,sl,si,ei,el,ni,ai,Zr,Re,st,Rs,et,Bs,li,nl,ri,ti,nt,Be,at,Je,lt,zn,pi,rt,ys,Js,al,Xe,ui,ll,oi,tt,yn,ii,pt,ws,Xs,rl,Qe,mi,tl,ci,ut,B,di,pl,bi,hi,ul,fi,vi,ol,ji,qi,ot,Qs,it,ks,Ys,il,Ye,_i,ml,$i,mt,J,Ei,cl,gi,zi,dl,yi,wi,bl,ki,Ci,ct,O,Pi,hl,xi,Ai,fl,Di,Si,vl,Ti,Oi,jl,Mi,Ni,ql,Li,Fi,dt,Ws,Vi,_l,Ui,Ii,bt,Cs,Zs,$l,We,Gi,El,Ki,ht,M,Hi,gl,Ri,Bi,zl,Ji,Xi,yl,Qi,Yi,wl,Wi,Zi,kl,sm,em,ft,Ze,vt,wn,nm,jt,sn,qt,kn,am,_t,X,Cl,lm,rm,Pl,tm,pm,xl,um,om,Al,im,$t,se,mm,Dl,cm,dm,Et,Cn,bm,gt,Pn,hm,zt,Ps,ee,Sl,en,fm,Tl,vm,yt,xn,jm,wt,vs,qm,Ol,_m,$m,Ml,Em,gm,kt,nn,Ct,ne,Pt,ae,zm,Nl,ym,wm,xt,An,km,At,le,Dt,xs,re,Ll,an,Cm,Fl,Pm,St,te,xm,Vl,Am,Dm,Tt,pe,Sm,Ul,Tm,Om,Ot,Dn,Mm,Mt,As,ue,Il,ln,Nm,Gl,Lm,Nt,oe,Fm,rn,Vm,Um,Lt,Sn,Im,Ft,Q,Tn,tn,Gm,Km,Hm,On,pn,Rm,Bm,Jm,Mn,un,Xm,Qm,Ym,Nn,on,Wm,Zm,Vt,Y,sc,Kl,ec,nc,Hl,ac,lc,Rl,rc,tc,Ut;return _=new tb({props:{fw:V[0]}}),P=new ss({}),es=new rb({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"}]}}),K=new ss({}),qe=new lb({props:{id:"N9kO52itd0Q"}}),$e=new k({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)

train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=<span class="hljs-string">&quot;adam&quot;</span>)

model.fit(train_dataset)`}}),Ee=new k({props:{code:"ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']",highlighted:'ValueError: No gradients provided <span class="hljs-keyword">for</span> <span class="hljs-built_in">any</span> variable: [<span class="hljs-string">&#x27;tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>]'}}),ge=new ss({}),ze=new k({props:{code:`for batch in train_dataset:
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>`}}),ye=new k({props:{code:`{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        ...,
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[ <span class="hljs-number">101</span>, <span class="hljs-number">2174</span>, <span class="hljs-number">1010</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3174</span>, <span class="hljs-number">2420</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">2044</span>, <span class="hljs-number">2048</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        ...,
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3398</span>, <span class="hljs-number">3398</span>, ..., <span class="hljs-number">2051</span>, <span class="hljs-number">2894</span>,  <span class="hljs-number">102</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">4124</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">2070</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>]])&gt;}`}}),we=new k({props:{code:'model.compile(optimizer="adam")',highlighted:'model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>)'}}),Fs=new Yl({props:{$$slots:{default:[pb]},$$scope:{ctx:V}}}),ke=new k({props:{code:"  246/24543 [..............................] - ETA: 15:52 - loss: nan",highlighted:'  <span class="hljs-number">246</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">52</span> - loss: nan'}}),Pe=new ss({}),xe=new k({props:{code:"model(batch)",highlighted:"model(batch)"}}),Ae=new k({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),De=new k({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`}}),Se=new k({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([<span class="hljs-number">0.6844486</span> ,        nan,        nan, <span class="hljs-number">0.67127866</span>, <span class="hljs-number">0.7068601</span> ,
              nan, <span class="hljs-number">0.69309855</span>,        nan, <span class="hljs-number">0.65531296</span>,        nan,
              nan,        nan, <span class="hljs-number">0.675402</span>  ,        nan,        nan,
       <span class="hljs-number">0.69831556</span>], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">0.04761693</span>, -<span class="hljs-number">0.06509043</span>],
       [-<span class="hljs-number">0.0481936</span> , -<span class="hljs-number">0.04556257</span>],
       [-<span class="hljs-number">0.0040929</span> , -<span class="hljs-number">0.05848458</span>],
       [-<span class="hljs-number">0.02417453</span>, -<span class="hljs-number">0.0684005</span> ],
       [-<span class="hljs-number">0.02517801</span>, -<span class="hljs-number">0.05241832</span>],
       [-<span class="hljs-number">0.04514256</span>, -<span class="hljs-number">0.0757378</span> ],
       [-<span class="hljs-number">0.02656011</span>, -<span class="hljs-number">0.02646275</span>],
       [ <span class="hljs-number">0.00766164</span>, -<span class="hljs-number">0.04350497</span>],
       [ <span class="hljs-number">0.02060014</span>, -<span class="hljs-number">0.05655622</span>],
       [-<span class="hljs-number">0.02615328</span>, -<span class="hljs-number">0.0447021</span> ],
       [-<span class="hljs-number">0.05119278</span>, -<span class="hljs-number">0.06928903</span>],
       [-<span class="hljs-number">0.02859691</span>, -<span class="hljs-number">0.04879177</span>],
       [-<span class="hljs-number">0.02210129</span>, -<span class="hljs-number">0.05791225</span>],
       [-<span class="hljs-number">0.02363213</span>, -<span class="hljs-number">0.05962167</span>],
       [-<span class="hljs-number">0.05352269</span>, -<span class="hljs-number">0.0481673</span> ],
       [-<span class="hljs-number">0.08141848</span>, -<span class="hljs-number">0.07110836</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),Te=new k({props:{code:`import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`}}),Oe=new k({props:{code:"array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])",highlighted:'array([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>])'}}),Me=new k({props:{code:`input_ids = batch["input_ids"].numpy()
input_ids[indices]`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
input_ids[indices]`}}),Ne=new k({props:{code:`array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])`,highlighted:`array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2032</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">16480</span>,  <span class="hljs-number">3917</span>,  <span class="hljs-number">2594</span>,  <span class="hljs-number">4135</span>,
        <span class="hljs-number">23212</span>,  <span class="hljs-number">3070</span>,  <span class="hljs-number">2214</span>, <span class="hljs-number">10170</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2012</span>,  <span class="hljs-number">4356</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">3183</span>,
         <span class="hljs-number">6838</span>, <span class="hljs-number">12953</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">6147</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2606</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">6838</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3294</span>,  <span class="hljs-number">6625</span>,  <span class="hljs-number">3773</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2214</span>,
         <span class="hljs-number">2158</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">6814</span>,  <span class="hljs-number">2016</span>,  <span class="hljs-number">2234</span>,  <span class="hljs-number">2461</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1998</span>, <span class="hljs-number">13322</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">2053</span>,  <span class="hljs-number">3382</span>,  <span class="hljs-number">2008</span>,
         <span class="hljs-number">2016</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2222</span>,  <span class="hljs-number">3046</span>,  <span class="hljs-number">8103</span>,  <span class="hljs-number">2075</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1012</span>,
          <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">3712</span>,  <span class="hljs-number">4634</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2057</span>,  <span class="hljs-number">8108</span>,
         <span class="hljs-number">2025</span>,  <span class="hljs-number">3404</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1012</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2616</span>, <span class="hljs-number">18449</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">1999</span>,
         <span class="hljs-number">1037</span>,  <span class="hljs-number">9666</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4100</span>,  <span class="hljs-number">8663</span>, <span class="hljs-number">11020</span>,  <span class="hljs-number">6313</span>,  <span class="hljs-number">2791</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2431</span>,  <span class="hljs-number">1011</span>,  <span class="hljs-number">4301</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">5177</span>,
         <span class="hljs-number">2110</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">3977</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">2832</span>,  <span class="hljs-number">2106</span>,  <span class="hljs-number">2025</span>,  <span class="hljs-number">2689</span>,  <span class="hljs-number">2104</span>,
         <span class="hljs-number">2122</span>,  <span class="hljs-number">6214</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">13090</span>,  <span class="hljs-number">5948</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2048</span>,
         <span class="hljs-number">2308</span>,  <span class="hljs-number">2006</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">5001</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2171</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">2170</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3564</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2277</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2195</span>,  <span class="hljs-number">4279</span>,  <span class="hljs-number">2191</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2181</span>,  <span class="hljs-number">2124</span>,  <span class="hljs-number">2004</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2069</span>,  <span class="hljs-number">2028</span>,
         <span class="hljs-number">2451</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2008</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2123</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1056</span>,  <span class="hljs-number">2113</span>,  <span class="hljs-number">2065</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">2428</span>, <span class="hljs-number">10654</span>,  <span class="hljs-number">7347</span>,  <span class="hljs-number">2030</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">7126</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,
         <span class="hljs-number">2291</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">5094</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,  <span class="hljs-number">2291</span>,  <span class="hljs-number">2035</span>,
         <span class="hljs-number">2105</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2029</span>,  <span class="hljs-number">3216</span>,  <span class="hljs-number">2019</span>,  <span class="hljs-number">2503</span>,  <span class="hljs-number">3444</span>,  <span class="hljs-number">1010</span>,
         <span class="hljs-number">6732</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2038</span>, <span class="hljs-number">19840</span>,  <span class="hljs-number">2098</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">9906</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2003</span>,  <span class="hljs-number">2770</span>,  <span class="hljs-number">2041</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4784</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">6732</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">9525</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">4569</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1996</span>, <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2162</span>,
         <span class="hljs-number">2252</span>,  <span class="hljs-number">5689</span>,  <span class="hljs-number">2013</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">7223</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">1996</span>,
        <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2252</span>,  <span class="hljs-number">3062</span>,  <span class="hljs-number">2000</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2598</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>, <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2049</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2025</span>,
        <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]])`}}),Le=new k({props:{code:`labels = batch['labels'].numpy()
labels[indices]`,highlighted:`labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].numpy()
labels[indices]`}}),Fe=new k({props:{code:"array([2, 2, 2, 2, 2, 2, 2, 2, 2])",highlighted:'array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),Ve=new k({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),Ue=new k({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),Ie=new k({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, <span class="hljs-attribute">num_labels</span>=3)
model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">&#x27;adam&#x27;</span>)
model.fit(train_dataset)`}}),Ge=new k({props:{code:"  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032",highlighted:'  <span class="hljs-number">869</span>/<span class="hljs-number">24543</span> [&gt;.............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">29</span> - loss: <span class="hljs-number">1.1032</span>'}}),Ke=new ss({}),Re=new k({props:{code:`from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">5e-5</span>))`}}),Rs=new Yl({props:{$$slots:{default:[ub]},$$scope:{ctx:V}}}),Be=new k({props:{code:"model.fit(train_dataset)",highlighted:"model.fit(train_dataset)"}}),Je=new k({props:{code:"319/24543 [..............................] - ETA: 16:07 - loss: 0.9718",highlighted:'<span class="hljs-number">319</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">16</span>:07 - loss: <span class="hljs-number">0.9718</span>'}}),Xe=new ss({}),Qe=new ss({}),Qs=new Yl({props:{$$slots:{default:[ob]},$$scope:{ctx:V}}}),Ye=new ss({}),We=new ss({}),Ze=new k({props:{code:`input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
tokenizer.decode(input_ids[<span class="hljs-number">0</span>])`}}),sn=new k({props:{code:`labels = batch["labels"].numpy()
label = labels[0]`,highlighted:`labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
label = labels[<span class="hljs-number">0</span>]`}}),en=new ss({}),nn=new k({props:{code:`for batch in train_dataset:
    break

# Assurez-vous que vous avez ex\xE9cut\xE9 model.compile() et d\xE9fini votre optimiseur,
# et vos pertes/m\xE9triques si vous les utilisez.

model.fit(batch, epochs=20)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>

<span class="hljs-comment"># Assurez-vous que vous avez ex\xE9cut\xE9 model.compile() et d\xE9fini votre optimiseur,</span>
<span class="hljs-comment"># et vos pertes/m\xE9triques si vous les utilisez.</span>

model.fit(batch, epochs=<span class="hljs-number">20</span>)`}}),ne=new Yl({props:{$$slots:{default:[ib]},$$scope:{ctx:V}}}),le=new Yl({props:{warning:!0,$$slots:{default:[mb]},$$scope:{ctx:V}}}),an=new ss({}),ln=new ss({}),{c(){d=r("meta"),w=i(),b(_.$$.fragment),z=i(),C=r("h1"),E=r("a"),y=r("span"),b(P.$$.fragment),N=i(),D=r("span"),ps=a("D\xE9bogage du pipeline d'entra\xEEnement"),U=i(),b(es.$$.fragment),Ds=i(),T=r("p"),I=a("Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),qs=r("em"),_s=a("finetuner"),ve=a(" un mod\xE8le sur une t\xE2che donn\xE9e en suivant consciencieusement les conseils du "),us=r("a"),Ss=a("chapitre 7"),os=a(". Mais lorsque vous lancez la commande "),$s=r("code"),x=a("model.fit()"),A=a(", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur mais le mod\xE8le r\xE9sultant est mauvais. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),je=i(),G=r("h2"),ns=r("a"),Ts=r("span"),b(K.$$.fragment),hn=i(),Os=r("span"),cp=a("D\xE9boguer le pipeline d'entra\xEEnement"),Wl=i(),b(qe.$$.fragment),Zl=i(),is=r("p"),dp=a("Le probl\xE8me lorsque vous rencontrez une erreur dans "),Rn=r("code"),bp=a("trainer.train()"),hp=a(" est qu\u2019elle peut provenir de plusieurs sources, car la fonction "),Bn=r("code"),fp=a("Trainer"),vp=a(" assemble g\xE9n\xE9ralement des batchs de choses. Elle convertit les jeux de donn\xE9es en chargeurs de donn\xE9es donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, elle prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, elle calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),sr=i(),Ms=r("p"),jp=a("La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Jn=r("code"),qp=a("trainer.train()"),_p=a(" est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),er=i(),ms=r("p"),$p=a("Pour le d\xE9montrer, nous utiliserons le script suivant qui tente de "),Xn=r("em"),Ep=a("finetuner"),gp=a(" un mod\xE8le DistilBERT sur le "),_e=r("a"),zp=a("jeu de donn\xE9es MNLI"),yp=a(" :"),nr=i(),b($e.$$.fragment),ar=i(),Ns=r("p"),wp=a("Si vous essayez de l\u2019ex\xE9cuter, il se peut que vous obteniez des "),Qn=r("code"),kp=a("VisibleDeprecationWarning"),Cp=a("s lors de la conversion du jeu de donn\xE9es. Il s\u2019agit d\u2019un probl\xE8me UX connu par l\u2019\xE9quipe d\u2019Hugging Face, donc veuillez l\u2019ignorer. Si vous lisez le cours apr\xE8s novembre 2021 et que cela se produit encore, envoyez des tweets de rage \xE0 @carrigmat jusqu\u2019\xE0 ce qu\u2019il le corrige."),lr=i(),fn=r("p"),Pp=a("Le probl\xE8me cependant est que nous avons une erreur flagrante. Et c\u2019est vraiment, terriblement long :"),rr=i(),b(Ee.$$.fragment),tr=i(),vn=r("p"),xp=a("Qu\u2019est-ce que cela signifie ? Nous avons essay\xE9 d\u2019entra\xEEner sur nos donn\xE9es mais nous n\u2019avons pas obtenu de gradient. C\u2019est assez d\xE9concertant. Comment commencer \xE0 d\xE9boguer quelque chose comme \xE7a ? Lorsque l\u2019erreur que vous obtenez ne sugg\xE8re pas imm\xE9diatement l\u2019origine du probl\xE8me, la meilleure solution consiste souvent \xE0 proc\xE9der par \xE9tapes, en s\u2019assurant \xE0 chaque fois que tout semble correct. Et bien s\xFBr, il faut toujours commencer par\u2026"),pr=i(),Es=r("h3"),Ls=r("a"),Yn=r("span"),b(ge.$$.fragment),Ap=i(),Wn=r("span"),Dp=a("V\xE9rifier vos donn\xE9es"),ur=i(),jn=r("p"),Sp=a("Cela va sans dire, mais si vos donn\xE9es sont corrompues, Keras ne sera pas en mesure de les r\xE9parer pour vous. Avant toute chose, vous devez donc jeter un coup d\u2019\u0153il \xE0 ce que contient votre ensemble d\u2019entra\xEEnement."),or=i(),S=r("p"),Tp=a("Bien qu\u2019il soit tentant de regarder dans "),Zn=r("code"),Op=a("raw_datasets"),Mp=a(" et "),sa=r("code"),Np=a("tokenized_datasets"),Lp=a(", nous vous recommandons fortement d\u2019aller voir les donn\xE9es au moment o\xF9 elles vont entrer dans le mod\xE8le. Cela signifie lire une sortie du "),ea=r("code"),Fp=a("tf.data.Dataset"),Vp=a(" que vous avez cr\xE9\xE9 avec la fonction "),na=r("code"),Up=a("to_tf_dataset()"),Ip=a(" ! Alors comment faire ? Les objets "),aa=r("code"),Gp=a("tf.data.Dataset"),Kp=a(" nous donnent des batchs entiers \xE0 la fois et ne supportent pas l\u2019indexation, donc nous ne pouvons pas simplement demander "),la=r("code"),Hp=a("train_dataset[0]"),Rp=a(". Nous pouvons, cependant, lui demander poliment un batch :"),ir=i(),b(ze.$$.fragment),mr=i(),as=r("p"),ra=r("code"),Bp=a("break"),Jp=a(" termine la boucle apr\xE8s une it\xE9ration, donc cela prend le premier batch qui sort de "),ta=r("code"),Xp=a("train_dataset"),Qp=a(" et l\u2019enregistre comme "),pa=r("code"),Yp=a("batch"),Wp=a(". Maintenant, jetons un coup d\u2019oeil \xE0 ce qu\u2019il y a \xE0 l\u2019int\xE9rieur :"),cr=i(),b(ye.$$.fragment),dr=i(),H=r("p"),Zp=a("Cela semble correct. Nous passons les "),ua=r("code"),su=a("labels"),eu=a(", "),oa=r("code"),nu=a("attention_mask"),au=a(", et "),ia=r("code"),lu=a("input_ids"),ru=a(" au mod\xE8le, ce qui devrait \xEAtre tout ce dont il a besoin pour calculer les sorties et la perte. Alors pourquoi n\u2019avons-nous pas de gradient ? Regardez de plus pr\xE8s : nous passons un seul dictionnaire en entr\xE9e mais un batch d\u2019entra\xEEnement est g\xE9n\xE9ralement un tenseur ou un dictionnaire d\u2019entr\xE9e, plus un tenseur d\u2019\xE9tiquettes. Nos \xE9tiquettes sont juste une cl\xE9 dans notre dictionnaire d\u2019entr\xE9e."),br=i(),cs=r("p"),tu=a("Est-ce un probl\xE8me ? Pas toujours, en fait ! Mais c\u2019est l\u2019un des probl\xE8mes les plus courants que vous rencontrerez lorsque vous entra\xEEnerez des "),ma=r("em"),pu=a("transformers"),uu=a(" avec TensorFlow. Nos mod\xE8les peuvent tous calculer la perte en interne, mais pour ce faire, les \xE9tiquettes doivent \xEAtre transmises dans le dictionnaire d\u2019entr\xE9e. C\u2019est la perte qui est utilis\xE9e lorsque nous ne sp\xE9cifions pas de valeur de perte \xE0 "),ca=r("code"),ou=a("compile()"),iu=a(". Keras, d\u2019autre part, s\u2019attend g\xE9n\xE9ralement \xE0 ce que les \xE9tiquettes soient pass\xE9es s\xE9par\xE9ment du dictionnaire d\u2019entr\xE9e, et les calculs de perte \xE9choueront g\xE9n\xE9ralement si vous ne le faites pas."),hr=i(),ds=r("p"),mu=a("Le probl\xE8me est maintenant devenu plus clair : nous avons pass\xE9 un argument "),da=r("code"),cu=a("loss"),du=a(", ce qui signifie que nous demandons \xE0 Keras de calculer les pertes pour nous, mais nous avons pass\xE9 nos \xE9tiquettes comme entr\xE9es au mod\xE8le, et non comme \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend ! Nous devons choisir l\u2019un ou l\u2019autre : soit nous utilisons la perte interne du mod\xE8le et gardons les \xE9tiquettes o\xF9 elles sont, soit nous continuons \xE0 utiliser les pertes de Keras, mais nous d\xE9pla\xE7ons les \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend. Pour simplifier, prenons la premi\xE8re approche. Changez l\u2019appel \xE0 "),ba=r("code"),bu=a("compile()"),hu=a(" pour lire :"),fr=i(),b(we.$$.fragment),vr=i(),qn=r("p"),fu=a("Maintenant, nous allons utiliser la perte interne du mod\xE8le et ce probl\xE8me devrait \xEAtre r\xE9solu !"),jr=i(),b(Fs.$$.fragment),qr=i(),Vs=r("p"),vu=a("Maintenant, essayons d\u2019entra\xEEner. Nous devrions obtenir des gradients maintenant, donc avec un peu de chance nous pouvons juste appeler "),ha=r("code"),ju=a("model.fit()"),qu=a(" et tout fonctionnera bien !"),_r=i(),b(ke.$$.fragment),$r=i(),_n=r("p"),_u=a("Oh non."),Er=i(),Ce=r("p"),fa=r("code"),$u=a("nan"),Eu=a(" n\u2019est pas une valeur de perte tr\xE8s encourageante. Pourtant, nous avons v\xE9rifi\xE9 nos donn\xE9es et elles semblent plut\xF4t bonnes. Si ce n\u2019est pas le probl\xE8me, quelle est la prochaine \xE9tape ? La prochaine \xE9tape \xE9vidente est de\u2026"),gr=i(),gs=r("h3"),Us=r("a"),va=r("span"),b(Pe.$$.fragment),gu=i(),ja=r("span"),zu=a("V\xE9rifier votre mod\xE8le"),zr=i(),ls=r("p"),qa=r("code"),yu=a("model.fit()"),wu=a(" est une fonction tr\xE8s pratique dans Keras, mais elle fait beaucoup de choses pour vous. Cela peut rendre plus difficile de trouver exactement o\xF9 un probl\xE8me est survenu. Si vous d\xE9boguez votre mod\xE8le, une strat\xE9gie qui peut vraiment vous aider est de passer un seul batch au mod\xE8le et d\u2019examiner les sorties de ce batch en d\xE9tail. Une autre astuce vraiment utile est de "),_a=r("code"),ku=a("compiler()"),Cu=a(" le mod\xE8le avec "),$a=r("code"),Pu=a("run_eagerly=True"),xu=a(". Cela le rendra beaucoup plus lent mais les messages d\u2019erreur seront beaucoup plus compr\xE9hensibles car ils indiqueront exactement o\xF9 le probl\xE8me est survenu dans le code de votre mod\xE8le."),yr=i(),bs=r("p"),Au=a("Pour l\u2019instant, cependant, nous n\u2019avons pas besoin de "),Ea=r("code"),Du=a("run_eagerly"),Su=a(". Ex\xE9cutons le "),ga=r("code"),Tu=a("batch"),Ou=a(" que nous avons obtenu pr\xE9c\xE9demment \xE0 travers le mod\xE8le et voyons \xE0 quoi ressemblent les r\xE9sultats :"),wr=i(),b(xe.$$.fragment),kr=i(),b(Ae.$$.fragment),Cr=i(),$=r("p"),Mu=a("Eh bien, c\u2019est d\xE9licat. Tout est \u201Cnan\u201D ! Mais c\u2019est \xE9trange, n\u2019est-ce pas ? Comment tous nos logits pourraient-ils devenir "),za=r("code"),Nu=a("nan"),Lu=a(" ? \u201CNAN\u201D signifie \u201D"),ya=r("em"),Fu=a("not a number"),Vu=a("\u201D. Les valeurs "),wa=r("code"),Uu=a("nan"),Iu=a(" apparaissent souvent quand on effectue une op\xE9ration interdite comme la division par z\xE9ro. Mais une chose tr\xE8s importante \xE0 savoir sur "),ka=r("code"),Gu=a("nan"),Ku=a(" en apprentissage automatique est que cette valeur a tendance \xE0 "),Ca=r("em"),Hu=a("se propager"),Ru=a(". Si vous multipliez un nombre par "),Pa=r("code"),Bu=a("nan"),Ju=a(", le r\xE9sultat sera \xE9galement "),xa=r("code"),Xu=a("nan"),Qu=a(". Et si vous obtenez une valeur "),Aa=r("code"),Yu=a("nan"),Wu=a(` n\u2019importe o\xF9 dans votre sortie, votre perte ou votre gradient, alors elle se propagera rapidement \xE0 travers tout votre mod\xE8le.
Ceci parce que lorsque cette valeur `),Da=r("code"),Zu=a("nan"),so=a(" est propag\xE9e \xE0 travers votre r\xE9seau, vous obtiendrez des gradients "),Sa=r("code"),eo=a("nan"),no=a(", et lorsque les mises \xE0 jour des poids sont calcul\xE9es avec ces gradients, vous obtiendrez des poids "),Ta=r("code"),ao=a("nan"),lo=a(", et ces poids calculeront encore plus de sorties "),Oa=r("code"),ro=a("nan"),to=a(" ! Tr\xE8s vite, le r\xE9seau entier ne sera plus qu\u2019un gros bloc de "),Ma=r("code"),po=a("nan"),uo=a(". Une fois que cela arrive, il est assez difficile de voir o\xF9 le probl\xE8me a commenc\xE9. Comment peut-on isoler l\u2019endroit o\xF9 les "),Na=r("code"),oo=a("nan"),io=a(" se sont introduits en premier ?"),Pr=i(),R=r("p"),mo=a("La r\xE9ponse est d\u2019essayer de "),La=r("em"),co=a("reinitialiser"),bo=a(" notre mod\xE8le. Une fois que nous avons commenc\xE9 l\u2019entra\xEEnement, nous avons eu un "),Fa=r("code"),ho=a("nan"),fo=a(" quelque part et il s\u2019est rapidement propag\xE9 \xE0 travers tout le mod\xE8le. Donc, chargeons le mod\xE8le \xE0 partir d\u2019un checkpoint et ne faisons aucune mise \xE0 jour de poids, et voyons o\xF9 nous obtenons une valeur "),Va=r("code"),vo=a("nan"),jo=a(" :"),xr=i(),b(De.$$.fragment),Ar=i(),$n=r("p"),qo=a("Quand on fait \xE7a, on obtient :"),Dr=i(),b(Se.$$.fragment),Sr=i(),rs=r("p"),Ua=r("em"),_o=a("Maintenant"),$o=a(" on arrive \xE0 quelque chose ! Il n\u2019y a pas de valeurs "),Ia=r("code"),Eo=a("nan"),go=a(" dans nos logits, ce qui est rassurant. Mais nous voyons quelques valeurs "),Ga=r("code"),zo=a("nan"),yo=a(" dans notre perte ! Y a-t-il quelque chose dans ces \xE9chantillons en particulier qui cause ce probl\xE8me ? Voyons de quels \xE9chantillons il s\u2019agit (notez que si vous ex\xE9cutez ce code vous-m\xEAme, vous pouvez obtenir des indices diff\xE9rents parce que le jeu de donn\xE9es a \xE9t\xE9 m\xE9lang\xE9) :"),Tr=i(),b(Te.$$.fragment),Or=i(),b(Oe.$$.fragment),Mr=i(),En=r("p"),wo=a("Examinons les \xE9chantillons d\u2019o\xF9 proviennent ces indices :"),Nr=i(),b(Me.$$.fragment),Lr=i(),b(Ne.$$.fragment),Fr=i(),gn=r("p"),ko=a("Il y a beaucoup de batchs ici mais rien d\u2019inhabituel. Regardons les \xE9tiquettes :"),Vr=i(),b(Le.$$.fragment),Ur=i(),b(Fe.$$.fragment),Ir=i(),hs=r("p"),Co=a("Ah ! Les \xE9chantillons "),Ka=r("code"),Po=a("nan"),xo=a(" ont tous le m\xEAme label. C\u2019est un gros indice. Le fait que nous n\u2019obtenions une perte de "),Ha=r("code"),Ao=a("nan"),Do=a(" que lorsque notre \xE9tiquette vaut 2 sugg\xE8re que c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre d\u2019\xE9tiquettes dans notre mod\xE8le :"),Gr=i(),b(Ve.$$.fragment),Kr=i(),b(Ue.$$.fragment),Hr=i(),Is=r("p"),So=a("Nous voyons maintenant le probl\xE8me : le mod\xE8le pense qu\u2019il n\u2019y a que deux classes, mais les \xE9tiquettes vont jusqu\u2019\xE0 2, ce qui signifie qu\u2019il y a en fait trois classes (car 0 est aussi une classe). C\u2019est ainsi que nous avons obtenu un "),Ra=r("code"),To=a("nan"),Oo=a(". En essayant de calculer la perte pour une classe inexistante ! Essayons de changer cela et de r\xE9ajuster le mod\xE8le :"),Rr=i(),b(Ie.$$.fragment),Br=i(),b(Ge.$$.fragment),Jr=i(),Gs=r("p"),Mo=a("On entra\xEEne ! Plus de "),Ba=r("code"),No=a("nan"),Lo=a(" et nos pertes diminuent\u2026 en quelque sorte. Si vous regardez pendant un certain temps, vous pouvez commencer \xE0 vous impatienter car la valeur des pertes reste obstin\xE9ment \xE9lev\xE9e. Arr\xEAtons l\u2019entra\xEEnement ici et essayons de r\xE9fl\xE9chir \xE0 ce qui pourrait causer ce probl\xE8me. \xC0 ce stade, nous sommes pratiquement s\xFBrs que les donn\xE9es et le mod\xE8le sont corrects, mais notre mod\xE8le n\u2019apprend pas bien. Que reste-t-il d\u2019autre ? Il est temps de\u2026"),Xr=i(),zs=r("h3"),Ks=r("a"),Ja=r("span"),b(Ke.$$.fragment),Fo=i(),Xa=r("span"),Vo=a("V\xE9rifier les hyperparam\xE8tres"),Qr=i(),fs=r("p"),Uo=a("Si vous regardez le code ci-dessus, vous ne verrez peut-\xEAtre aucun hyperparam\xE8tre, sauf peut-\xEAtre le "),Qa=r("code"),Io=a("batch_size"),Go=a(" qui ne semble pas \xEAtre un coupable probable. Cependant, ne soyez pas dupe, il y a toujours des hyperparam\xE8tres. Si vous ne pouvez pas les voir, cela signifie simplement que vous ne connaissez pas leur r\xE9glage. En particulier, souvenez-vous d\u2019une chose essentielle \xE0 propos de Keras : si vous d\xE9finissez une fonction de perte, d\u2019optimisation ou d\u2019activation avec une cha\xEEne, "),Ya=r("em"),Ko=a("tous ses arguments seront d\xE9finis sur leurs valeurs par d\xE9faut"),Ho=a(". Cela signifie que, m\xEAme si l\u2019utilisation de cha\xEEnes de caract\xE8res est tr\xE8s pratique, vous devez \xEAtre tr\xE8s prudent car cela peut facilement vous cacher des \xE9l\xE9ments critiques. (Toute personne essayant le d\xE9fi optionnel ci-dessus devrait prendre bonne note de ce fait)."),Yr=i(),Hs=r("p"),Ro=a("Dans ce cas, o\xF9 avons-nous d\xE9fini un argument avec une cha\xEEne de caract\xE8res ? Au d\xE9part, nous d\xE9finissions la perte avec une cha\xEEne de caract\xE8res, mais nous ne le faisons plus. Cependant, nous le faisons pour l\u2019optimiseur. Cela pourrait-il nous cacher quelque chose ? Jetons un coup d\u2019\u0153il \xE0 "),He=r("a"),Bo=a("ses arguments"),Jo=a("."),Wr=i(),L=r("p"),Xo=a("Y a-t-il quelque chose qui ressort ? C\u2019est exact : le taux d\u2019apprentissage ! Lorsque nous indiquons simplement "),Wa=r("code"),Qo=a("'adam'"),Yo=a(" nous allons obtenir le taux d\u2019apprentissage par d\xE9faut qui est de 0.001 (ou 1e-3). C\u2019est beaucoup trop \xE9lev\xE9 pour un "),Za=r("em"),Wo=a("transformer"),Zo=a(" ! En g\xE9n\xE9ral, nous recommandons d\u2019essayer des taux d\u2019apprentissage entre 1e-5 et 1e-4 pour vos mod\xE8les soit entre 10X et 100X plus petit que la valeur que nous utilisons ici. Cela semble \xEAtre un probl\xE8me majeur, alors essayons de le r\xE9duire. Pour ce faire, nous devons importer l\u2019objet "),sl=r("code"),si=a("optimizer"),ei=a(". Pendant que nous y sommes, r\xE9initialisons le mod\xE8le \xE0 partir du "),el=r("em"),ni=a("checkpoint"),ai=a(" au cas o\xF9 l\u2019entra\xEEnement avec un taux d\u2019apprentissage \xE9lev\xE9 aurait endommag\xE9 ses poids :"),Zr=i(),b(Re.$$.fragment),st=i(),b(Rs.$$.fragment),et=i(),Bs=r("p"),li=a("Maintenant, nous pouvons essayer de "),nl=r("em"),ri=a("finetuner"),ti=a(" le mod\xE8le avec le nouveau taux d\u2019apprentissage :"),nt=i(),b(Be.$$.fragment),at=i(),b(Je.$$.fragment),lt=i(),zn=r("p"),pi=a("Maintenant notre perte va vraiment aller quelque part ! L\u2019entra\xEEnement semble enfin fonctionner. Il y a une le\xE7on \xE0 tirer ici : lorsque votre mod\xE8le fonctionne mais que la perte ne diminue pas, et que vous \xEAtes s\xFBr que vos donn\xE9es sont correctes, c\u2019est une bonne id\xE9e de v\xE9rifier les hyperparam\xE8tres comme le taux d\u2019apprentissage et le taux de d\xE9croissance des poids. Un r\xE9glage trop \xE9lev\xE9 de l\u2019un ou l\u2019autre de ces param\xE8tres risque fort de faire \xAB caler \xBB  l\u2019entra\xEEnement \xE0 une valeur de perte \xE9lev\xE9e."),rt=i(),ys=r("h2"),Js=r("a"),al=r("span"),b(Xe.$$.fragment),ui=i(),ll=r("span"),oi=a("Autres probl\xE8mes potentiels"),tt=i(),yn=r("p"),ii=a("Nous avons couvert les probl\xE8mes dans le script ci-dessus, mais il existe plusieurs autres erreurs courantes auxquelles vous pouvez \xEAtre confront\xE9. Jetons un coup d\u2019oeil \xE0 une liste (tr\xE8s incompl\xE8te)."),pt=i(),ws=r("h3"),Xs=r("a"),rl=r("span"),b(Qe.$$.fragment),mi=i(),tl=r("span"),ci=a("G\xE9rer les erreurs de manque de m\xE9moire"),ut=i(),B=r("p"),di=a("Le signe r\xE9v\xE9lateur d\u2019un manque de m\xE9moire est une erreur du type \u201COOM when allocating tensor\u201D (OOM \xE9tant l\u2019abr\xE9viation de "),pl=r("em"),bi=a("out of memory"),hi=a("). Il s\u2019agit d\u2019un risque tr\xE8s courant lorsque l\u2019on utilise de grands mod\xE8les de langage. Si vous rencontrez ce probl\xE8me, une bonne strat\xE9gie consiste \xE0 diviser par deux la taille de votre batch et \xE0 r\xE9essayer. Gardez \xE0 l\u2019esprit, cependant, que certains mod\xE8les sont "),ul=r("em"),fi=a("tr\xE8s"),vi=a(" grands. Par exemple, le mod\xE8le GPT-2 complet poss\xE8de 1,5 Go de param\xE8tres, ce qui signifie que vous aurez besoin de 6 Go de m\xE9moire rien que pour stocker le mod\xE8le, et 6 autres Go pour ses gradients ! Entra\xEEner le mod\xE8le GPT-2 complet n\xE9cessite g\xE9n\xE9ralement plus de 20 Go de VRAM, quelle que soit la taille du batch utilis\xE9, ce dont seuls quelques GPUs sont dot\xE9s. Des mod\xE8les plus l\xE9gers comme "),ol=r("code"),ji=a("distilbert-base-cased"),qi=a(" sont beaucoup plus faciles \xE0 ex\xE9cuter et s\u2019entra\xEEnent aussi beaucoup plus rapidement."),ot=i(),b(Qs.$$.fragment),it=i(),ks=r("h3"),Ys=r("a"),il=r("span"),b(Ye.$$.fragment),_i=i(),ml=r("span"),$i=a("TensorFlow affam\xE9 \u{1F99B}"),mt=i(),J=r("p"),Ei=a("Une bizarrerie particuli\xE8re de TensorFlow dont vous devez \xEAtre conscient est qu\u2019il s\u2019alloue "),cl=r("em"),gi=a("toute"),zi=a(" la m\xE9moire de votre GPU d\xE8s que vous chargez un mod\xE8le ou que vous effectuez un entra\xEEnement. Puis il divise cette m\xE9moire selon les besoins. Ce comportement est diff\xE9rent de celui d\u2019autres "),dl=r("em"),yi=a("frameworks"),wi=a(", comme PyTorch, qui alloue la m\xE9moire selon les besoins avec CUDA plut\xF4t que de le faire en interne. L\u2019un des avantages de l\u2019approche de TensorFlow est qu\u2019elle peut souvent donner des erreurs utiles lorsque vous manquez de m\xE9moire et qu\u2019elle peut r\xE9cup\xE9rer de cet \xE9tat sans planter tout le noyau CUDA. Mais il y a aussi un inconv\xE9nient important : si vous ex\xE9cutez deux processus TensorFlow en m\xEAme temps alors "),bl=r("strong"),ki=a("vous allez passer un mauvais moment"),Ci=a("."),ct=i(),O=r("p"),Pi=a("Si vous travaillez sur Colab, vous n\u2019avez pas \xE0 vous soucier de cela. Si vous travaillez localement, vous devez absolument faire attention. En particulier, sachez que la fermeture d\u2019un onglet de "),hl=r("em"),xi=a("notebook"),Ai=a(" n\u2019entra\xEEne pas n\xE9cessairement la fermeture de ce "),fl=r("em"),Di=a("notebook"),Si=a(" ! Vous devrez peut-\xEAtre s\xE9lectionner les "),vl=r("em"),Ti=a("notebooks"),Oi=a(" en cours d\u2019ex\xE9cution (ceux qui ont une ic\xF4ne verte) et les fermer manuellement dans la liste des r\xE9pertoires. Tout "),jl=r("em"),Mi=a("notebook"),Ni=a(" en cours d\u2019ex\xE9cution qui utilisait TensorFlow peut encore utiliser une grande partie de la m\xE9moire de votre GPU, ce qui signifie que tout nouveau "),ql=r("em"),Li=a("notebook"),Fi=a(" que vous d\xE9marrez peut rencontrer des probl\xE8mes tr\xE8s \xE9tranges."),dt=i(),Ws=r("p"),Vi=a("Si vous commencez \xE0 obtenir des erreurs concernant CUDA, BLAS ou cuBLAS dans du code qui fonctionnait auparavant, c\u2019est tr\xE8s souvent le coupable. Vous pouvez utiliser une commande comme "),_l=r("code"),Ui=a("nvidia-smi"),Ii=a(" pour v\xE9rifier si la plupart de votre m\xE9moire est libre ou toujours utilis\xE9e. Si elle est toujours utilis\xE9e, c\u2019est que quelque chose d\u2019autre s\u2019y accroche !"),bt=i(),Cs=r("h3"),Zs=r("a"),$l=r("span"),b(We.$$.fragment),Gi=i(),El=r("span"),Ki=a("V\xE9rifiez vos donn\xE9es (encore !)"),ht=i(),M=r("p"),Hi=a("Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. S\u2019il y a un "),gl=r("em"),Ri=a("bug"),Bi=a(" qui corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre jeu de donn\xE9es. Un outil utile ici est "),zl=r("code"),Ji=a("tokenizer.decode()"),Xi=a(". Il transformera les "),yl=r("code"),Qi=a("input_ids"),Yi=a(" en cha\xEEnes de caract\xE8res, afin que vous puissiez visualiser les donn\xE9es et voir si vos donn\xE9es d\u2019entra\xEEnement renseignent ce que vous voulez. Par exemple, apr\xE8s avoir obtenu un "),wl=r("code"),Wi=a("batch"),Zi=a(" de votre "),kl=r("code"),sm=a("tf.data.Dataset"),em=a(" comme nous l\u2019avons fait ci-dessus, vous pouvez d\xE9coder le premier \xE9l\xE9ment comme suit :"),ft=i(),b(Ze.$$.fragment),vt=i(),wn=r("p"),nm=a("Vous pouvez ensuite la comparer avec la premi\xE8re \xE9tiquette, comme suit :"),jt=i(),b(sn.$$.fragment),qt=i(),kn=r("p"),am=a("Une fois que vous pouvez visualiser vos donn\xE9es de cette mani\xE8re, vous pouvez vous poser les questions suivantes :"),_t=i(),X=r("ul"),Cl=r("li"),lm=a("les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),rm=i(),Pl=r("li"),tm=a("\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),pm=i(),xl=r("li"),um=a("y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),om=i(),Al=r("li"),im=a("quelle devrait \xEAtre la perte/m\xE9trique si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),$t=i(),se=r("p"),mm=a("Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le. Si votre mod\xE8le produit des "),Dl=r("em"),cm=a("tokens"),dm=a(", essayez aussi de les d\xE9coder ! Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, cela peut \xEAtre d\xFB au fait que votre jeu de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification). Des techniques telles que le sur\xE9chantillonnage des classes rares peuvent aider. D\u2019autre part, cela peut \xE9galement \xEAtre d\xFB \xE0 des probl\xE8mes d\u2019entra\xEEnement tels que de mauvais r\xE9glages des hyperparam\xE8tres."),Et=i(),Cn=r("p"),bm=a("Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial avant entra\xEEnement est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e. Il y a probablement un bug. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),gt=i(),Pn=r("p"),hm=a("Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),zt=i(),Ps=r("h3"),ee=r("a"),Sl=r("span"),b(en.$$.fragment),fm=i(),Tl=r("span"),vm=a("Surentra\xEEnement du mod\xE8le sur un seul batch"),yt=i(),xn=r("p"),jm=a("Le surentra\xEEnement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse et se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),wt=i(),vs=r("p"),qm=a("Une fois que vous avez d\xE9fini votre "),Ol=r("code"),_m=a("mod\xE8le"),$m=a(", c\u2019est tr\xE8s facile. Il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis de le traiter comme votre jeu de donn\xE9es entier que vous "),Ml=r("em"),Em=a("finetunez"),gm=a(" sur un grand nombre d\u2019\xE9poques :"),kt=i(),b(nn.$$.fragment),Ct=i(),b(ne.$$.fragment),Pt=i(),ae=r("p"),zm=a("Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le "),Nl=r("code"),ym=a("batch"),wm=a(", avec une perte diminuant rapidement vers 0 (ou la valeur minimale pour la perte que vous utilisez)."),xt=i(),An=r("p"),km=a("Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),At=i(),b(le.$$.fragment),Dt=i(),xs=r("h3"),re=r("a"),Ll=r("span"),b(an.$$.fragment),Cm=i(),Fl=r("span"),Pm=a("Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"),St=i(),te=r("p"),xm=a("Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),Vl=r("code"),Am=a("Trainer"),Dm=a(" fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats. Donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),Tt=i(),pe=r("p"),Sm=a("Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 le "),Ul=r("em"),Tm=a("finetuner"),Om=a(" un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),Ot=i(),Dn=r("p"),Mm=a("Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),Mt=i(),As=r("h3"),ue=r("a"),Il=r("span"),b(ln.$$.fragment),Nm=i(),Gl=r("span"),Lm=a("Demander de l'aide"),Nt=i(),oe=r("p"),Fm=a("Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me. Si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),rn=r("a"),Vm=a("forum"),Um=a("."),Lt=i(),Sn=r("p"),Im=a("Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),Ft=i(),Q=r("ul"),Tn=r("li"),tn=r("a"),Gm=a("La reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie"),Km=a(" par Joel Grus"),Hm=i(),On=r("li"),pn=r("a"),Rm=a("Liste de contr\xF4le pour le d\xE9bogage des r\xE9seaux de neurones"),Bm=a(" par Cecelia Shao"),Jm=i(),Mn=r("li"),un=r("a"),Xm=a("Comment tester unitairement le code d\u2019apprentissage automatique"),Qm=a(" par Chase Roberts"),Ym=i(),Nn=r("li"),on=r("a"),Wm=a("Une recette pour entra\xEEner les r\xE9seaux de neurones"),Zm=a(" par Andrej Karpathy"),Vt=i(),Y=r("p"),sc=a("Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019entra\xEEnement ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),Kl=r("em"),ec=a("Transformers"),nc=a(" ou \u{1F917} "),Hl=r("em"),ac=a("Datasets"),lc=a(" qui ne semble pas correct, vous avez peut-\xEAtre trouver un "),Rl=r("em"),rc=a("bug"),tc=a(". Vous devez absolument nous en parler pour qu\u2019on puisse le corriger. Dans la section suivante, nous allons vous expliquer exactement comment faire."),this.h()},l(s){const u=nb('[data-svelte="svelte-1phssyn"]',document.head);d=t(u,"META",{name:!0,content:!0}),u.forEach(n),w=m(s),h(_.$$.fragment,s),z=m(s),C=t(s,"H1",{class:!0});var mn=p(C);E=t(mn,"A",{id:!0,class:!0,href:!0});var Bl=p(E);y=t(Bl,"SPAN",{});var Jl=p(y);h(P.$$.fragment,Jl),Jl.forEach(n),Bl.forEach(n),N=m(mn),D=t(mn,"SPAN",{});var Xl=p(D);ps=l(Xl,"D\xE9bogage du pipeline d'entra\xEEnement"),Xl.forEach(n),mn.forEach(n),U=m(s),h(es.$$.fragment,s),Ds=m(s),T=t(s,"P",{});var ts=p(T);I=l(ts,"Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),qs=t(ts,"EM",{});var Ql=p(qs);_s=l(Ql,"finetuner"),Ql.forEach(n),ve=l(ts," un mod\xE8le sur une t\xE2che donn\xE9e en suivant consciencieusement les conseils du "),us=t(ts,"A",{href:!0});var cc=p(us);Ss=l(cc,"chapitre 7"),cc.forEach(n),os=l(ts,". Mais lorsque vous lancez la commande "),$s=t(ts,"CODE",{});var dc=p($s);x=l(dc,"model.fit()"),dc.forEach(n),A=l(ts,", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur mais le mod\xE8le r\xE9sultant est mauvais. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),ts.forEach(n),je=m(s),G=t(s,"H2",{class:!0});var It=p(G);ns=t(It,"A",{id:!0,class:!0,href:!0});var bc=p(ns);Ts=t(bc,"SPAN",{});var hc=p(Ts);h(K.$$.fragment,hc),hc.forEach(n),bc.forEach(n),hn=m(It),Os=t(It,"SPAN",{});var fc=p(Os);cp=l(fc,"D\xE9boguer le pipeline d'entra\xEEnement"),fc.forEach(n),It.forEach(n),Wl=m(s),h(qe.$$.fragment,s),Zl=m(s),is=t(s,"P",{});var Ln=p(is);dp=l(Ln,"Le probl\xE8me lorsque vous rencontrez une erreur dans "),Rn=t(Ln,"CODE",{});var vc=p(Rn);bp=l(vc,"trainer.train()"),vc.forEach(n),hp=l(Ln," est qu\u2019elle peut provenir de plusieurs sources, car la fonction "),Bn=t(Ln,"CODE",{});var jc=p(Bn);fp=l(jc,"Trainer"),jc.forEach(n),vp=l(Ln," assemble g\xE9n\xE9ralement des batchs de choses. Elle convertit les jeux de donn\xE9es en chargeurs de donn\xE9es donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, elle prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, elle calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),Ln.forEach(n),sr=m(s),Ms=t(s,"P",{});var Gt=p(Ms);jp=l(Gt,"La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Jn=t(Gt,"CODE",{});var qc=p(Jn);qp=l(qc,"trainer.train()"),qc.forEach(n),_p=l(Gt," est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),Gt.forEach(n),er=m(s),ms=t(s,"P",{});var Fn=p(ms);$p=l(Fn,"Pour le d\xE9montrer, nous utiliserons le script suivant qui tente de "),Xn=t(Fn,"EM",{});var _c=p(Xn);Ep=l(_c,"finetuner"),_c.forEach(n),gp=l(Fn," un mod\xE8le DistilBERT sur le "),_e=t(Fn,"A",{href:!0,rel:!0});var $c=p(_e);zp=l($c,"jeu de donn\xE9es MNLI"),$c.forEach(n),yp=l(Fn," :"),Fn.forEach(n),nr=m(s),h($e.$$.fragment,s),ar=m(s),Ns=t(s,"P",{});var Kt=p(Ns);wp=l(Kt,"Si vous essayez de l\u2019ex\xE9cuter, il se peut que vous obteniez des "),Qn=t(Kt,"CODE",{});var Ec=p(Qn);kp=l(Ec,"VisibleDeprecationWarning"),Ec.forEach(n),Cp=l(Kt,"s lors de la conversion du jeu de donn\xE9es. Il s\u2019agit d\u2019un probl\xE8me UX connu par l\u2019\xE9quipe d\u2019Hugging Face, donc veuillez l\u2019ignorer. Si vous lisez le cours apr\xE8s novembre 2021 et que cela se produit encore, envoyez des tweets de rage \xE0 @carrigmat jusqu\u2019\xE0 ce qu\u2019il le corrige."),Kt.forEach(n),lr=m(s),fn=t(s,"P",{});var gc=p(fn);Pp=l(gc,"Le probl\xE8me cependant est que nous avons une erreur flagrante. Et c\u2019est vraiment, terriblement long :"),gc.forEach(n),rr=m(s),h(Ee.$$.fragment,s),tr=m(s),vn=t(s,"P",{});var zc=p(vn);xp=l(zc,"Qu\u2019est-ce que cela signifie ? Nous avons essay\xE9 d\u2019entra\xEEner sur nos donn\xE9es mais nous n\u2019avons pas obtenu de gradient. C\u2019est assez d\xE9concertant. Comment commencer \xE0 d\xE9boguer quelque chose comme \xE7a ? Lorsque l\u2019erreur que vous obtenez ne sugg\xE8re pas imm\xE9diatement l\u2019origine du probl\xE8me, la meilleure solution consiste souvent \xE0 proc\xE9der par \xE9tapes, en s\u2019assurant \xE0 chaque fois que tout semble correct. Et bien s\xFBr, il faut toujours commencer par\u2026"),zc.forEach(n),pr=m(s),Es=t(s,"H3",{class:!0});var Ht=p(Es);Ls=t(Ht,"A",{id:!0,class:!0,href:!0});var yc=p(Ls);Yn=t(yc,"SPAN",{});var wc=p(Yn);h(ge.$$.fragment,wc),wc.forEach(n),yc.forEach(n),Ap=m(Ht),Wn=t(Ht,"SPAN",{});var kc=p(Wn);Dp=l(kc,"V\xE9rifier vos donn\xE9es"),kc.forEach(n),Ht.forEach(n),ur=m(s),jn=t(s,"P",{});var Cc=p(jn);Sp=l(Cc,"Cela va sans dire, mais si vos donn\xE9es sont corrompues, Keras ne sera pas en mesure de les r\xE9parer pour vous. Avant toute chose, vous devez donc jeter un coup d\u2019\u0153il \xE0 ce que contient votre ensemble d\u2019entra\xEEnement."),Cc.forEach(n),or=m(s),S=t(s,"P",{});var F=p(S);Tp=l(F,"Bien qu\u2019il soit tentant de regarder dans "),Zn=t(F,"CODE",{});var Pc=p(Zn);Op=l(Pc,"raw_datasets"),Pc.forEach(n),Mp=l(F," et "),sa=t(F,"CODE",{});var xc=p(sa);Np=l(xc,"tokenized_datasets"),xc.forEach(n),Lp=l(F,", nous vous recommandons fortement d\u2019aller voir les donn\xE9es au moment o\xF9 elles vont entrer dans le mod\xE8le. Cela signifie lire une sortie du "),ea=t(F,"CODE",{});var Ac=p(ea);Fp=l(Ac,"tf.data.Dataset"),Ac.forEach(n),Vp=l(F," que vous avez cr\xE9\xE9 avec la fonction "),na=t(F,"CODE",{});var Dc=p(na);Up=l(Dc,"to_tf_dataset()"),Dc.forEach(n),Ip=l(F," ! Alors comment faire ? Les objets "),aa=t(F,"CODE",{});var Sc=p(aa);Gp=l(Sc,"tf.data.Dataset"),Sc.forEach(n),Kp=l(F," nous donnent des batchs entiers \xE0 la fois et ne supportent pas l\u2019indexation, donc nous ne pouvons pas simplement demander "),la=t(F,"CODE",{});var Tc=p(la);Hp=l(Tc,"train_dataset[0]"),Tc.forEach(n),Rp=l(F,". Nous pouvons, cependant, lui demander poliment un batch :"),F.forEach(n),ir=m(s),h(ze.$$.fragment,s),mr=m(s),as=t(s,"P",{});var cn=p(as);ra=t(cn,"CODE",{});var Oc=p(ra);Bp=l(Oc,"break"),Oc.forEach(n),Jp=l(cn," termine la boucle apr\xE8s une it\xE9ration, donc cela prend le premier batch qui sort de "),ta=t(cn,"CODE",{});var Mc=p(ta);Xp=l(Mc,"train_dataset"),Mc.forEach(n),Qp=l(cn," et l\u2019enregistre comme "),pa=t(cn,"CODE",{});var Nc=p(pa);Yp=l(Nc,"batch"),Nc.forEach(n),Wp=l(cn,". Maintenant, jetons un coup d\u2019oeil \xE0 ce qu\u2019il y a \xE0 l\u2019int\xE9rieur :"),cn.forEach(n),cr=m(s),h(ye.$$.fragment,s),dr=m(s),H=t(s,"P",{});var ie=p(H);Zp=l(ie,"Cela semble correct. Nous passons les "),ua=t(ie,"CODE",{});var Lc=p(ua);su=l(Lc,"labels"),Lc.forEach(n),eu=l(ie,", "),oa=t(ie,"CODE",{});var Fc=p(oa);nu=l(Fc,"attention_mask"),Fc.forEach(n),au=l(ie,", et "),ia=t(ie,"CODE",{});var Vc=p(ia);lu=l(Vc,"input_ids"),Vc.forEach(n),ru=l(ie," au mod\xE8le, ce qui devrait \xEAtre tout ce dont il a besoin pour calculer les sorties et la perte. Alors pourquoi n\u2019avons-nous pas de gradient ? Regardez de plus pr\xE8s : nous passons un seul dictionnaire en entr\xE9e mais un batch d\u2019entra\xEEnement est g\xE9n\xE9ralement un tenseur ou un dictionnaire d\u2019entr\xE9e, plus un tenseur d\u2019\xE9tiquettes. Nos \xE9tiquettes sont juste une cl\xE9 dans notre dictionnaire d\u2019entr\xE9e."),ie.forEach(n),br=m(s),cs=t(s,"P",{});var Vn=p(cs);tu=l(Vn,"Est-ce un probl\xE8me ? Pas toujours, en fait ! Mais c\u2019est l\u2019un des probl\xE8mes les plus courants que vous rencontrerez lorsque vous entra\xEEnerez des "),ma=t(Vn,"EM",{});var Uc=p(ma);pu=l(Uc,"transformers"),Uc.forEach(n),uu=l(Vn," avec TensorFlow. Nos mod\xE8les peuvent tous calculer la perte en interne, mais pour ce faire, les \xE9tiquettes doivent \xEAtre transmises dans le dictionnaire d\u2019entr\xE9e. C\u2019est la perte qui est utilis\xE9e lorsque nous ne sp\xE9cifions pas de valeur de perte \xE0 "),ca=t(Vn,"CODE",{});var Ic=p(ca);ou=l(Ic,"compile()"),Ic.forEach(n),iu=l(Vn,". Keras, d\u2019autre part, s\u2019attend g\xE9n\xE9ralement \xE0 ce que les \xE9tiquettes soient pass\xE9es s\xE9par\xE9ment du dictionnaire d\u2019entr\xE9e, et les calculs de perte \xE9choueront g\xE9n\xE9ralement si vous ne le faites pas."),Vn.forEach(n),hr=m(s),ds=t(s,"P",{});var Un=p(ds);mu=l(Un,"Le probl\xE8me est maintenant devenu plus clair : nous avons pass\xE9 un argument "),da=t(Un,"CODE",{});var Gc=p(da);cu=l(Gc,"loss"),Gc.forEach(n),du=l(Un,", ce qui signifie que nous demandons \xE0 Keras de calculer les pertes pour nous, mais nous avons pass\xE9 nos \xE9tiquettes comme entr\xE9es au mod\xE8le, et non comme \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend ! Nous devons choisir l\u2019un ou l\u2019autre : soit nous utilisons la perte interne du mod\xE8le et gardons les \xE9tiquettes o\xF9 elles sont, soit nous continuons \xE0 utiliser les pertes de Keras, mais nous d\xE9pla\xE7ons les \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend. Pour simplifier, prenons la premi\xE8re approche. Changez l\u2019appel \xE0 "),ba=t(Un,"CODE",{});var Kc=p(ba);bu=l(Kc,"compile()"),Kc.forEach(n),hu=l(Un," pour lire :"),Un.forEach(n),fr=m(s),h(we.$$.fragment,s),vr=m(s),qn=t(s,"P",{});var Hc=p(qn);fu=l(Hc,"Maintenant, nous allons utiliser la perte interne du mod\xE8le et ce probl\xE8me devrait \xEAtre r\xE9solu !"),Hc.forEach(n),jr=m(s),h(Fs.$$.fragment,s),qr=m(s),Vs=t(s,"P",{});var Rt=p(Vs);vu=l(Rt,"Maintenant, essayons d\u2019entra\xEEner. Nous devrions obtenir des gradients maintenant, donc avec un peu de chance nous pouvons juste appeler "),ha=t(Rt,"CODE",{});var Rc=p(ha);ju=l(Rc,"model.fit()"),Rc.forEach(n),qu=l(Rt," et tout fonctionnera bien !"),Rt.forEach(n),_r=m(s),h(ke.$$.fragment,s),$r=m(s),_n=t(s,"P",{});var Bc=p(_n);_u=l(Bc,"Oh non."),Bc.forEach(n),Er=m(s),Ce=t(s,"P",{});var pc=p(Ce);fa=t(pc,"CODE",{});var Jc=p(fa);$u=l(Jc,"nan"),Jc.forEach(n),Eu=l(pc," n\u2019est pas une valeur de perte tr\xE8s encourageante. Pourtant, nous avons v\xE9rifi\xE9 nos donn\xE9es et elles semblent plut\xF4t bonnes. Si ce n\u2019est pas le probl\xE8me, quelle est la prochaine \xE9tape ? La prochaine \xE9tape \xE9vidente est de\u2026"),pc.forEach(n),gr=m(s),gs=t(s,"H3",{class:!0});var Bt=p(gs);Us=t(Bt,"A",{id:!0,class:!0,href:!0});var Xc=p(Us);va=t(Xc,"SPAN",{});var Qc=p(va);h(Pe.$$.fragment,Qc),Qc.forEach(n),Xc.forEach(n),gu=m(Bt),ja=t(Bt,"SPAN",{});var Yc=p(ja);zu=l(Yc,"V\xE9rifier votre mod\xE8le"),Yc.forEach(n),Bt.forEach(n),zr=m(s),ls=t(s,"P",{});var dn=p(ls);qa=t(dn,"CODE",{});var Wc=p(qa);yu=l(Wc,"model.fit()"),Wc.forEach(n),wu=l(dn," est une fonction tr\xE8s pratique dans Keras, mais elle fait beaucoup de choses pour vous. Cela peut rendre plus difficile de trouver exactement o\xF9 un probl\xE8me est survenu. Si vous d\xE9boguez votre mod\xE8le, une strat\xE9gie qui peut vraiment vous aider est de passer un seul batch au mod\xE8le et d\u2019examiner les sorties de ce batch en d\xE9tail. Une autre astuce vraiment utile est de "),_a=t(dn,"CODE",{});var Zc=p(_a);ku=l(Zc,"compiler()"),Zc.forEach(n),Cu=l(dn," le mod\xE8le avec "),$a=t(dn,"CODE",{});var s0=p($a);Pu=l(s0,"run_eagerly=True"),s0.forEach(n),xu=l(dn,". Cela le rendra beaucoup plus lent mais les messages d\u2019erreur seront beaucoup plus compr\xE9hensibles car ils indiqueront exactement o\xF9 le probl\xE8me est survenu dans le code de votre mod\xE8le."),dn.forEach(n),yr=m(s),bs=t(s,"P",{});var In=p(bs);Au=l(In,"Pour l\u2019instant, cependant, nous n\u2019avons pas besoin de "),Ea=t(In,"CODE",{});var e0=p(Ea);Du=l(e0,"run_eagerly"),e0.forEach(n),Su=l(In,". Ex\xE9cutons le "),ga=t(In,"CODE",{});var n0=p(ga);Tu=l(n0,"batch"),n0.forEach(n),Ou=l(In," que nous avons obtenu pr\xE9c\xE9demment \xE0 travers le mod\xE8le et voyons \xE0 quoi ressemblent les r\xE9sultats :"),In.forEach(n),wr=m(s),h(xe.$$.fragment,s),kr=m(s),h(Ae.$$.fragment,s),Cr=m(s),$=t(s,"P",{});var g=p($);Mu=l(g,"Eh bien, c\u2019est d\xE9licat. Tout est \u201Cnan\u201D ! Mais c\u2019est \xE9trange, n\u2019est-ce pas ? Comment tous nos logits pourraient-ils devenir "),za=t(g,"CODE",{});var a0=p(za);Nu=l(a0,"nan"),a0.forEach(n),Lu=l(g," ? \u201CNAN\u201D signifie \u201D"),ya=t(g,"EM",{});var l0=p(ya);Fu=l(l0,"not a number"),l0.forEach(n),Vu=l(g,"\u201D. Les valeurs "),wa=t(g,"CODE",{});var r0=p(wa);Uu=l(r0,"nan"),r0.forEach(n),Iu=l(g," apparaissent souvent quand on effectue une op\xE9ration interdite comme la division par z\xE9ro. Mais une chose tr\xE8s importante \xE0 savoir sur "),ka=t(g,"CODE",{});var t0=p(ka);Gu=l(t0,"nan"),t0.forEach(n),Ku=l(g," en apprentissage automatique est que cette valeur a tendance \xE0 "),Ca=t(g,"EM",{});var p0=p(Ca);Hu=l(p0,"se propager"),p0.forEach(n),Ru=l(g,". Si vous multipliez un nombre par "),Pa=t(g,"CODE",{});var u0=p(Pa);Bu=l(u0,"nan"),u0.forEach(n),Ju=l(g,", le r\xE9sultat sera \xE9galement "),xa=t(g,"CODE",{});var o0=p(xa);Xu=l(o0,"nan"),o0.forEach(n),Qu=l(g,". Et si vous obtenez une valeur "),Aa=t(g,"CODE",{});var i0=p(Aa);Yu=l(i0,"nan"),i0.forEach(n),Wu=l(g,` n\u2019importe o\xF9 dans votre sortie, votre perte ou votre gradient, alors elle se propagera rapidement \xE0 travers tout votre mod\xE8le.
Ceci parce que lorsque cette valeur `),Da=t(g,"CODE",{});var m0=p(Da);Zu=l(m0,"nan"),m0.forEach(n),so=l(g," est propag\xE9e \xE0 travers votre r\xE9seau, vous obtiendrez des gradients "),Sa=t(g,"CODE",{});var c0=p(Sa);eo=l(c0,"nan"),c0.forEach(n),no=l(g,", et lorsque les mises \xE0 jour des poids sont calcul\xE9es avec ces gradients, vous obtiendrez des poids "),Ta=t(g,"CODE",{});var d0=p(Ta);ao=l(d0,"nan"),d0.forEach(n),lo=l(g,", et ces poids calculeront encore plus de sorties "),Oa=t(g,"CODE",{});var b0=p(Oa);ro=l(b0,"nan"),b0.forEach(n),to=l(g," ! Tr\xE8s vite, le r\xE9seau entier ne sera plus qu\u2019un gros bloc de "),Ma=t(g,"CODE",{});var h0=p(Ma);po=l(h0,"nan"),h0.forEach(n),uo=l(g,". Une fois que cela arrive, il est assez difficile de voir o\xF9 le probl\xE8me a commenc\xE9. Comment peut-on isoler l\u2019endroit o\xF9 les "),Na=t(g,"CODE",{});var f0=p(Na);oo=l(f0,"nan"),f0.forEach(n),io=l(g," se sont introduits en premier ?"),g.forEach(n),Pr=m(s),R=t(s,"P",{});var me=p(R);mo=l(me,"La r\xE9ponse est d\u2019essayer de "),La=t(me,"EM",{});var v0=p(La);co=l(v0,"reinitialiser"),v0.forEach(n),bo=l(me," notre mod\xE8le. Une fois que nous avons commenc\xE9 l\u2019entra\xEEnement, nous avons eu un "),Fa=t(me,"CODE",{});var j0=p(Fa);ho=l(j0,"nan"),j0.forEach(n),fo=l(me," quelque part et il s\u2019est rapidement propag\xE9 \xE0 travers tout le mod\xE8le. Donc, chargeons le mod\xE8le \xE0 partir d\u2019un checkpoint et ne faisons aucune mise \xE0 jour de poids, et voyons o\xF9 nous obtenons une valeur "),Va=t(me,"CODE",{});var q0=p(Va);vo=l(q0,"nan"),q0.forEach(n),jo=l(me," :"),me.forEach(n),xr=m(s),h(De.$$.fragment,s),Ar=m(s),$n=t(s,"P",{});var _0=p($n);qo=l(_0,"Quand on fait \xE7a, on obtient :"),_0.forEach(n),Dr=m(s),h(Se.$$.fragment,s),Sr=m(s),rs=t(s,"P",{});var bn=p(rs);Ua=t(bn,"EM",{});var $0=p(Ua);_o=l($0,"Maintenant"),$0.forEach(n),$o=l(bn," on arrive \xE0 quelque chose ! Il n\u2019y a pas de valeurs "),Ia=t(bn,"CODE",{});var E0=p(Ia);Eo=l(E0,"nan"),E0.forEach(n),go=l(bn," dans nos logits, ce qui est rassurant. Mais nous voyons quelques valeurs "),Ga=t(bn,"CODE",{});var g0=p(Ga);zo=l(g0,"nan"),g0.forEach(n),yo=l(bn," dans notre perte ! Y a-t-il quelque chose dans ces \xE9chantillons en particulier qui cause ce probl\xE8me ? Voyons de quels \xE9chantillons il s\u2019agit (notez que si vous ex\xE9cutez ce code vous-m\xEAme, vous pouvez obtenir des indices diff\xE9rents parce que le jeu de donn\xE9es a \xE9t\xE9 m\xE9lang\xE9) :"),bn.forEach(n),Tr=m(s),h(Te.$$.fragment,s),Or=m(s),h(Oe.$$.fragment,s),Mr=m(s),En=t(s,"P",{});var z0=p(En);wo=l(z0,"Examinons les \xE9chantillons d\u2019o\xF9 proviennent ces indices :"),z0.forEach(n),Nr=m(s),h(Me.$$.fragment,s),Lr=m(s),h(Ne.$$.fragment,s),Fr=m(s),gn=t(s,"P",{});var y0=p(gn);ko=l(y0,"Il y a beaucoup de batchs ici mais rien d\u2019inhabituel. Regardons les \xE9tiquettes :"),y0.forEach(n),Vr=m(s),h(Le.$$.fragment,s),Ur=m(s),h(Fe.$$.fragment,s),Ir=m(s),hs=t(s,"P",{});var Gn=p(hs);Co=l(Gn,"Ah ! Les \xE9chantillons "),Ka=t(Gn,"CODE",{});var w0=p(Ka);Po=l(w0,"nan"),w0.forEach(n),xo=l(Gn," ont tous le m\xEAme label. C\u2019est un gros indice. Le fait que nous n\u2019obtenions une perte de "),Ha=t(Gn,"CODE",{});var k0=p(Ha);Ao=l(k0,"nan"),k0.forEach(n),Do=l(Gn," que lorsque notre \xE9tiquette vaut 2 sugg\xE8re que c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre d\u2019\xE9tiquettes dans notre mod\xE8le :"),Gn.forEach(n),Gr=m(s),h(Ve.$$.fragment,s),Kr=m(s),h(Ue.$$.fragment,s),Hr=m(s),Is=t(s,"P",{});var Jt=p(Is);So=l(Jt,"Nous voyons maintenant le probl\xE8me : le mod\xE8le pense qu\u2019il n\u2019y a que deux classes, mais les \xE9tiquettes vont jusqu\u2019\xE0 2, ce qui signifie qu\u2019il y a en fait trois classes (car 0 est aussi une classe). C\u2019est ainsi que nous avons obtenu un "),Ra=t(Jt,"CODE",{});var C0=p(Ra);To=l(C0,"nan"),C0.forEach(n),Oo=l(Jt,". En essayant de calculer la perte pour une classe inexistante ! Essayons de changer cela et de r\xE9ajuster le mod\xE8le :"),Jt.forEach(n),Rr=m(s),h(Ie.$$.fragment,s),Br=m(s),h(Ge.$$.fragment,s),Jr=m(s),Gs=t(s,"P",{});var Xt=p(Gs);Mo=l(Xt,"On entra\xEEne ! Plus de "),Ba=t(Xt,"CODE",{});var P0=p(Ba);No=l(P0,"nan"),P0.forEach(n),Lo=l(Xt," et nos pertes diminuent\u2026 en quelque sorte. Si vous regardez pendant un certain temps, vous pouvez commencer \xE0 vous impatienter car la valeur des pertes reste obstin\xE9ment \xE9lev\xE9e. Arr\xEAtons l\u2019entra\xEEnement ici et essayons de r\xE9fl\xE9chir \xE0 ce qui pourrait causer ce probl\xE8me. \xC0 ce stade, nous sommes pratiquement s\xFBrs que les donn\xE9es et le mod\xE8le sont corrects, mais notre mod\xE8le n\u2019apprend pas bien. Que reste-t-il d\u2019autre ? Il est temps de\u2026"),Xt.forEach(n),Xr=m(s),zs=t(s,"H3",{class:!0});var Qt=p(zs);Ks=t(Qt,"A",{id:!0,class:!0,href:!0});var x0=p(Ks);Ja=t(x0,"SPAN",{});var A0=p(Ja);h(Ke.$$.fragment,A0),A0.forEach(n),x0.forEach(n),Fo=m(Qt),Xa=t(Qt,"SPAN",{});var D0=p(Xa);Vo=l(D0,"V\xE9rifier les hyperparam\xE8tres"),D0.forEach(n),Qt.forEach(n),Qr=m(s),fs=t(s,"P",{});var Kn=p(fs);Uo=l(Kn,"Si vous regardez le code ci-dessus, vous ne verrez peut-\xEAtre aucun hyperparam\xE8tre, sauf peut-\xEAtre le "),Qa=t(Kn,"CODE",{});var S0=p(Qa);Io=l(S0,"batch_size"),S0.forEach(n),Go=l(Kn," qui ne semble pas \xEAtre un coupable probable. Cependant, ne soyez pas dupe, il y a toujours des hyperparam\xE8tres. Si vous ne pouvez pas les voir, cela signifie simplement que vous ne connaissez pas leur r\xE9glage. En particulier, souvenez-vous d\u2019une chose essentielle \xE0 propos de Keras : si vous d\xE9finissez une fonction de perte, d\u2019optimisation ou d\u2019activation avec une cha\xEEne, "),Ya=t(Kn,"EM",{});var T0=p(Ya);Ko=l(T0,"tous ses arguments seront d\xE9finis sur leurs valeurs par d\xE9faut"),T0.forEach(n),Ho=l(Kn,". Cela signifie que, m\xEAme si l\u2019utilisation de cha\xEEnes de caract\xE8res est tr\xE8s pratique, vous devez \xEAtre tr\xE8s prudent car cela peut facilement vous cacher des \xE9l\xE9ments critiques. (Toute personne essayant le d\xE9fi optionnel ci-dessus devrait prendre bonne note de ce fait)."),Kn.forEach(n),Yr=m(s),Hs=t(s,"P",{});var Yt=p(Hs);Ro=l(Yt,"Dans ce cas, o\xF9 avons-nous d\xE9fini un argument avec une cha\xEEne de caract\xE8res ? Au d\xE9part, nous d\xE9finissions la perte avec une cha\xEEne de caract\xE8res, mais nous ne le faisons plus. Cependant, nous le faisons pour l\u2019optimiseur. Cela pourrait-il nous cacher quelque chose ? Jetons un coup d\u2019\u0153il \xE0 "),He=t(Yt,"A",{href:!0,rel:!0});var O0=p(He);Bo=l(O0,"ses arguments"),O0.forEach(n),Jo=l(Yt,"."),Yt.forEach(n),Wr=m(s),L=t(s,"P",{});var js=p(L);Xo=l(js,"Y a-t-il quelque chose qui ressort ? C\u2019est exact : le taux d\u2019apprentissage ! Lorsque nous indiquons simplement "),Wa=t(js,"CODE",{});var M0=p(Wa);Qo=l(M0,"'adam'"),M0.forEach(n),Yo=l(js," nous allons obtenir le taux d\u2019apprentissage par d\xE9faut qui est de 0.001 (ou 1e-3). C\u2019est beaucoup trop \xE9lev\xE9 pour un "),Za=t(js,"EM",{});var N0=p(Za);Wo=l(N0,"transformer"),N0.forEach(n),Zo=l(js," ! En g\xE9n\xE9ral, nous recommandons d\u2019essayer des taux d\u2019apprentissage entre 1e-5 et 1e-4 pour vos mod\xE8les soit entre 10X et 100X plus petit que la valeur que nous utilisons ici. Cela semble \xEAtre un probl\xE8me majeur, alors essayons de le r\xE9duire. Pour ce faire, nous devons importer l\u2019objet "),sl=t(js,"CODE",{});var L0=p(sl);si=l(L0,"optimizer"),L0.forEach(n),ei=l(js,". Pendant que nous y sommes, r\xE9initialisons le mod\xE8le \xE0 partir du "),el=t(js,"EM",{});var F0=p(el);ni=l(F0,"checkpoint"),F0.forEach(n),ai=l(js," au cas o\xF9 l\u2019entra\xEEnement avec un taux d\u2019apprentissage \xE9lev\xE9 aurait endommag\xE9 ses poids :"),js.forEach(n),Zr=m(s),h(Re.$$.fragment,s),st=m(s),h(Rs.$$.fragment,s),et=m(s),Bs=t(s,"P",{});var Wt=p(Bs);li=l(Wt,"Maintenant, nous pouvons essayer de "),nl=t(Wt,"EM",{});var V0=p(nl);ri=l(V0,"finetuner"),V0.forEach(n),ti=l(Wt," le mod\xE8le avec le nouveau taux d\u2019apprentissage :"),Wt.forEach(n),nt=m(s),h(Be.$$.fragment,s),at=m(s),h(Je.$$.fragment,s),lt=m(s),zn=t(s,"P",{});var U0=p(zn);pi=l(U0,"Maintenant notre perte va vraiment aller quelque part ! L\u2019entra\xEEnement semble enfin fonctionner. Il y a une le\xE7on \xE0 tirer ici : lorsque votre mod\xE8le fonctionne mais que la perte ne diminue pas, et que vous \xEAtes s\xFBr que vos donn\xE9es sont correctes, c\u2019est une bonne id\xE9e de v\xE9rifier les hyperparam\xE8tres comme le taux d\u2019apprentissage et le taux de d\xE9croissance des poids. Un r\xE9glage trop \xE9lev\xE9 de l\u2019un ou l\u2019autre de ces param\xE8tres risque fort de faire \xAB caler \xBB  l\u2019entra\xEEnement \xE0 une valeur de perte \xE9lev\xE9e."),U0.forEach(n),rt=m(s),ys=t(s,"H2",{class:!0});var Zt=p(ys);Js=t(Zt,"A",{id:!0,class:!0,href:!0});var I0=p(Js);al=t(I0,"SPAN",{});var G0=p(al);h(Xe.$$.fragment,G0),G0.forEach(n),I0.forEach(n),ui=m(Zt),ll=t(Zt,"SPAN",{});var K0=p(ll);oi=l(K0,"Autres probl\xE8mes potentiels"),K0.forEach(n),Zt.forEach(n),tt=m(s),yn=t(s,"P",{});var H0=p(yn);ii=l(H0,"Nous avons couvert les probl\xE8mes dans le script ci-dessus, mais il existe plusieurs autres erreurs courantes auxquelles vous pouvez \xEAtre confront\xE9. Jetons un coup d\u2019oeil \xE0 une liste (tr\xE8s incompl\xE8te)."),H0.forEach(n),pt=m(s),ws=t(s,"H3",{class:!0});var sp=p(ws);Xs=t(sp,"A",{id:!0,class:!0,href:!0});var R0=p(Xs);rl=t(R0,"SPAN",{});var B0=p(rl);h(Qe.$$.fragment,B0),B0.forEach(n),R0.forEach(n),mi=m(sp),tl=t(sp,"SPAN",{});var J0=p(tl);ci=l(J0,"G\xE9rer les erreurs de manque de m\xE9moire"),J0.forEach(n),sp.forEach(n),ut=m(s),B=t(s,"P",{});var ce=p(B);di=l(ce,"Le signe r\xE9v\xE9lateur d\u2019un manque de m\xE9moire est une erreur du type \u201COOM when allocating tensor\u201D (OOM \xE9tant l\u2019abr\xE9viation de "),pl=t(ce,"EM",{});var X0=p(pl);bi=l(X0,"out of memory"),X0.forEach(n),hi=l(ce,"). Il s\u2019agit d\u2019un risque tr\xE8s courant lorsque l\u2019on utilise de grands mod\xE8les de langage. Si vous rencontrez ce probl\xE8me, une bonne strat\xE9gie consiste \xE0 diviser par deux la taille de votre batch et \xE0 r\xE9essayer. Gardez \xE0 l\u2019esprit, cependant, que certains mod\xE8les sont "),ul=t(ce,"EM",{});var Q0=p(ul);fi=l(Q0,"tr\xE8s"),Q0.forEach(n),vi=l(ce," grands. Par exemple, le mod\xE8le GPT-2 complet poss\xE8de 1,5 Go de param\xE8tres, ce qui signifie que vous aurez besoin de 6 Go de m\xE9moire rien que pour stocker le mod\xE8le, et 6 autres Go pour ses gradients ! Entra\xEEner le mod\xE8le GPT-2 complet n\xE9cessite g\xE9n\xE9ralement plus de 20 Go de VRAM, quelle que soit la taille du batch utilis\xE9, ce dont seuls quelques GPUs sont dot\xE9s. Des mod\xE8les plus l\xE9gers comme "),ol=t(ce,"CODE",{});var Y0=p(ol);ji=l(Y0,"distilbert-base-cased"),Y0.forEach(n),qi=l(ce," sont beaucoup plus faciles \xE0 ex\xE9cuter et s\u2019entra\xEEnent aussi beaucoup plus rapidement."),ce.forEach(n),ot=m(s),h(Qs.$$.fragment,s),it=m(s),ks=t(s,"H3",{class:!0});var ep=p(ks);Ys=t(ep,"A",{id:!0,class:!0,href:!0});var W0=p(Ys);il=t(W0,"SPAN",{});var Z0=p(il);h(Ye.$$.fragment,Z0),Z0.forEach(n),W0.forEach(n),_i=m(ep),ml=t(ep,"SPAN",{});var sd=p(ml);$i=l(sd,"TensorFlow affam\xE9 \u{1F99B}"),sd.forEach(n),ep.forEach(n),mt=m(s),J=t(s,"P",{});var de=p(J);Ei=l(de,"Une bizarrerie particuli\xE8re de TensorFlow dont vous devez \xEAtre conscient est qu\u2019il s\u2019alloue "),cl=t(de,"EM",{});var ed=p(cl);gi=l(ed,"toute"),ed.forEach(n),zi=l(de," la m\xE9moire de votre GPU d\xE8s que vous chargez un mod\xE8le ou que vous effectuez un entra\xEEnement. Puis il divise cette m\xE9moire selon les besoins. Ce comportement est diff\xE9rent de celui d\u2019autres "),dl=t(de,"EM",{});var nd=p(dl);yi=l(nd,"frameworks"),nd.forEach(n),wi=l(de,", comme PyTorch, qui alloue la m\xE9moire selon les besoins avec CUDA plut\xF4t que de le faire en interne. L\u2019un des avantages de l\u2019approche de TensorFlow est qu\u2019elle peut souvent donner des erreurs utiles lorsque vous manquez de m\xE9moire et qu\u2019elle peut r\xE9cup\xE9rer de cet \xE9tat sans planter tout le noyau CUDA. Mais il y a aussi un inconv\xE9nient important : si vous ex\xE9cutez deux processus TensorFlow en m\xEAme temps alors "),bl=t(de,"STRONG",{});var ad=p(bl);ki=l(ad,"vous allez passer un mauvais moment"),ad.forEach(n),Ci=l(de,"."),de.forEach(n),ct=m(s),O=t(s,"P",{});var W=p(O);Pi=l(W,"Si vous travaillez sur Colab, vous n\u2019avez pas \xE0 vous soucier de cela. Si vous travaillez localement, vous devez absolument faire attention. En particulier, sachez que la fermeture d\u2019un onglet de "),hl=t(W,"EM",{});var ld=p(hl);xi=l(ld,"notebook"),ld.forEach(n),Ai=l(W," n\u2019entra\xEEne pas n\xE9cessairement la fermeture de ce "),fl=t(W,"EM",{});var rd=p(fl);Di=l(rd,"notebook"),rd.forEach(n),Si=l(W," ! Vous devrez peut-\xEAtre s\xE9lectionner les "),vl=t(W,"EM",{});var td=p(vl);Ti=l(td,"notebooks"),td.forEach(n),Oi=l(W," en cours d\u2019ex\xE9cution (ceux qui ont une ic\xF4ne verte) et les fermer manuellement dans la liste des r\xE9pertoires. Tout "),jl=t(W,"EM",{});var pd=p(jl);Mi=l(pd,"notebook"),pd.forEach(n),Ni=l(W," en cours d\u2019ex\xE9cution qui utilisait TensorFlow peut encore utiliser une grande partie de la m\xE9moire de votre GPU, ce qui signifie que tout nouveau "),ql=t(W,"EM",{});var ud=p(ql);Li=l(ud,"notebook"),ud.forEach(n),Fi=l(W," que vous d\xE9marrez peut rencontrer des probl\xE8mes tr\xE8s \xE9tranges."),W.forEach(n),dt=m(s),Ws=t(s,"P",{});var np=p(Ws);Vi=l(np,"Si vous commencez \xE0 obtenir des erreurs concernant CUDA, BLAS ou cuBLAS dans du code qui fonctionnait auparavant, c\u2019est tr\xE8s souvent le coupable. Vous pouvez utiliser une commande comme "),_l=t(np,"CODE",{});var od=p(_l);Ui=l(od,"nvidia-smi"),od.forEach(n),Ii=l(np," pour v\xE9rifier si la plupart de votre m\xE9moire est libre ou toujours utilis\xE9e. Si elle est toujours utilis\xE9e, c\u2019est que quelque chose d\u2019autre s\u2019y accroche !"),np.forEach(n),bt=m(s),Cs=t(s,"H3",{class:!0});var ap=p(Cs);Zs=t(ap,"A",{id:!0,class:!0,href:!0});var id=p(Zs);$l=t(id,"SPAN",{});var md=p($l);h(We.$$.fragment,md),md.forEach(n),id.forEach(n),Gi=m(ap),El=t(ap,"SPAN",{});var cd=p(El);Ki=l(cd,"V\xE9rifiez vos donn\xE9es (encore !)"),cd.forEach(n),ap.forEach(n),ht=m(s),M=t(s,"P",{});var Z=p(M);Hi=l(Z,"Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. S\u2019il y a un "),gl=t(Z,"EM",{});var dd=p(gl);Ri=l(dd,"bug"),dd.forEach(n),Bi=l(Z," qui corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre jeu de donn\xE9es. Un outil utile ici est "),zl=t(Z,"CODE",{});var bd=p(zl);Ji=l(bd,"tokenizer.decode()"),bd.forEach(n),Xi=l(Z,". Il transformera les "),yl=t(Z,"CODE",{});var hd=p(yl);Qi=l(hd,"input_ids"),hd.forEach(n),Yi=l(Z," en cha\xEEnes de caract\xE8res, afin que vous puissiez visualiser les donn\xE9es et voir si vos donn\xE9es d\u2019entra\xEEnement renseignent ce que vous voulez. Par exemple, apr\xE8s avoir obtenu un "),wl=t(Z,"CODE",{});var fd=p(wl);Wi=l(fd,"batch"),fd.forEach(n),Zi=l(Z," de votre "),kl=t(Z,"CODE",{});var vd=p(kl);sm=l(vd,"tf.data.Dataset"),vd.forEach(n),em=l(Z," comme nous l\u2019avons fait ci-dessus, vous pouvez d\xE9coder le premier \xE9l\xE9ment comme suit :"),Z.forEach(n),ft=m(s),h(Ze.$$.fragment,s),vt=m(s),wn=t(s,"P",{});var jd=p(wn);nm=l(jd,"Vous pouvez ensuite la comparer avec la premi\xE8re \xE9tiquette, comme suit :"),jd.forEach(n),jt=m(s),h(sn.$$.fragment,s),qt=m(s),kn=t(s,"P",{});var qd=p(kn);am=l(qd,"Une fois que vous pouvez visualiser vos donn\xE9es de cette mani\xE8re, vous pouvez vous poser les questions suivantes :"),qd.forEach(n),_t=m(s),X=t(s,"UL",{});var be=p(X);Cl=t(be,"LI",{});var _d=p(Cl);lm=l(_d,"les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),_d.forEach(n),rm=m(be),Pl=t(be,"LI",{});var $d=p(Pl);tm=l($d,"\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),$d.forEach(n),pm=m(be),xl=t(be,"LI",{});var Ed=p(xl);um=l(Ed,"y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),Ed.forEach(n),om=m(be),Al=t(be,"LI",{});var gd=p(Al);im=l(gd,"quelle devrait \xEAtre la perte/m\xE9trique si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),gd.forEach(n),be.forEach(n),$t=m(s),se=t(s,"P",{});var lp=p(se);mm=l(lp,"Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le. Si votre mod\xE8le produit des "),Dl=t(lp,"EM",{});var zd=p(Dl);cm=l(zd,"tokens"),zd.forEach(n),dm=l(lp,", essayez aussi de les d\xE9coder ! Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, cela peut \xEAtre d\xFB au fait que votre jeu de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification). Des techniques telles que le sur\xE9chantillonnage des classes rares peuvent aider. D\u2019autre part, cela peut \xE9galement \xEAtre d\xFB \xE0 des probl\xE8mes d\u2019entra\xEEnement tels que de mauvais r\xE9glages des hyperparam\xE8tres."),lp.forEach(n),Et=m(s),Cn=t(s,"P",{});var yd=p(Cn);bm=l(yd,"Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial avant entra\xEEnement est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e. Il y a probablement un bug. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),yd.forEach(n),gt=m(s),Pn=t(s,"P",{});var wd=p(Pn);hm=l(wd,"Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),wd.forEach(n),zt=m(s),Ps=t(s,"H3",{class:!0});var rp=p(Ps);ee=t(rp,"A",{id:!0,class:!0,href:!0});var kd=p(ee);Sl=t(kd,"SPAN",{});var Cd=p(Sl);h(en.$$.fragment,Cd),Cd.forEach(n),kd.forEach(n),fm=m(rp),Tl=t(rp,"SPAN",{});var Pd=p(Tl);vm=l(Pd,"Surentra\xEEnement du mod\xE8le sur un seul batch"),Pd.forEach(n),rp.forEach(n),yt=m(s),xn=t(s,"P",{});var xd=p(xn);jm=l(xd,"Le surentra\xEEnement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse et se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),xd.forEach(n),wt=m(s),vs=t(s,"P",{});var Hn=p(vs);qm=l(Hn,"Une fois que vous avez d\xE9fini votre "),Ol=t(Hn,"CODE",{});var Ad=p(Ol);_m=l(Ad,"mod\xE8le"),Ad.forEach(n),$m=l(Hn,", c\u2019est tr\xE8s facile. Il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis de le traiter comme votre jeu de donn\xE9es entier que vous "),Ml=t(Hn,"EM",{});var Dd=p(Ml);Em=l(Dd,"finetunez"),Dd.forEach(n),gm=l(Hn," sur un grand nombre d\u2019\xE9poques :"),Hn.forEach(n),kt=m(s),h(nn.$$.fragment,s),Ct=m(s),h(ne.$$.fragment,s),Pt=m(s),ae=t(s,"P",{});var tp=p(ae);zm=l(tp,"Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le "),Nl=t(tp,"CODE",{});var Sd=p(Nl);ym=l(Sd,"batch"),Sd.forEach(n),wm=l(tp,", avec une perte diminuant rapidement vers 0 (ou la valeur minimale pour la perte que vous utilisez)."),tp.forEach(n),xt=m(s),An=t(s,"P",{});var Td=p(An);km=l(Td,"Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),Td.forEach(n),At=m(s),h(le.$$.fragment,s),Dt=m(s),xs=t(s,"H3",{class:!0});var pp=p(xs);re=t(pp,"A",{id:!0,class:!0,href:!0});var Od=p(re);Ll=t(Od,"SPAN",{});var Md=p(Ll);h(an.$$.fragment,Md),Md.forEach(n),Od.forEach(n),Cm=m(pp),Fl=t(pp,"SPAN",{});var Nd=p(Fl);Pm=l(Nd,"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"),Nd.forEach(n),pp.forEach(n),St=m(s),te=t(s,"P",{});var up=p(te);xm=l(up,"Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),Vl=t(up,"CODE",{});var Ld=p(Vl);Am=l(Ld,"Trainer"),Ld.forEach(n),Dm=l(up," fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats. Donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),up.forEach(n),Tt=m(s),pe=t(s,"P",{});var op=p(pe);Sm=l(op,"Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 le "),Ul=t(op,"EM",{});var Fd=p(Ul);Tm=l(Fd,"finetuner"),Fd.forEach(n),Om=l(op," un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),op.forEach(n),Ot=m(s),Dn=t(s,"P",{});var Vd=p(Dn);Mm=l(Vd,"Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),Vd.forEach(n),Mt=m(s),As=t(s,"H3",{class:!0});var ip=p(As);ue=t(ip,"A",{id:!0,class:!0,href:!0});var Ud=p(ue);Il=t(Ud,"SPAN",{});var Id=p(Il);h(ln.$$.fragment,Id),Id.forEach(n),Ud.forEach(n),Nm=m(ip),Gl=t(ip,"SPAN",{});var Gd=p(Gl);Lm=l(Gd,"Demander de l'aide"),Gd.forEach(n),ip.forEach(n),Nt=m(s),oe=t(s,"P",{});var mp=p(oe);Fm=l(mp,"Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me. Si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),rn=t(mp,"A",{href:!0,rel:!0});var Kd=p(rn);Vm=l(Kd,"forum"),Kd.forEach(n),Um=l(mp,"."),mp.forEach(n),Lt=m(s),Sn=t(s,"P",{});var Hd=p(Sn);Im=l(Hd,"Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),Hd.forEach(n),Ft=m(s),Q=t(s,"UL",{});var he=p(Q);Tn=t(he,"LI",{});var uc=p(Tn);tn=t(uc,"A",{href:!0,rel:!0});var Rd=p(tn);Gm=l(Rd,"La reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie"),Rd.forEach(n),Km=l(uc," par Joel Grus"),uc.forEach(n),Hm=m(he),On=t(he,"LI",{});var oc=p(On);pn=t(oc,"A",{href:!0,rel:!0});var Bd=p(pn);Rm=l(Bd,"Liste de contr\xF4le pour le d\xE9bogage des r\xE9seaux de neurones"),Bd.forEach(n),Bm=l(oc," par Cecelia Shao"),oc.forEach(n),Jm=m(he),Mn=t(he,"LI",{});var ic=p(Mn);un=t(ic,"A",{href:!0,rel:!0});var Jd=p(un);Xm=l(Jd,"Comment tester unitairement le code d\u2019apprentissage automatique"),Jd.forEach(n),Qm=l(ic," par Chase Roberts"),ic.forEach(n),Ym=m(he),Nn=t(he,"LI",{});var mc=p(Nn);on=t(mc,"A",{href:!0,rel:!0});var Xd=p(on);Wm=l(Xd,"Une recette pour entra\xEEner les r\xE9seaux de neurones"),Xd.forEach(n),Zm=l(mc," par Andrej Karpathy"),mc.forEach(n),he.forEach(n),Vt=m(s),Y=t(s,"P",{});var fe=p(Y);sc=l(fe,"Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019entra\xEEnement ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),Kl=t(fe,"EM",{});var Qd=p(Kl);ec=l(Qd,"Transformers"),Qd.forEach(n),nc=l(fe," ou \u{1F917} "),Hl=t(fe,"EM",{});var Yd=p(Hl);ac=l(Yd,"Datasets"),Yd.forEach(n),lc=l(fe," qui ne semble pas correct, vous avez peut-\xEAtre trouver un "),Rl=t(fe,"EM",{});var Wd=p(Rl);rc=l(Wd,"bug"),Wd.forEach(n),tc=l(fe,". Vous devez absolument nous en parler pour qu\u2019on puisse le corriger. Dans la section suivante, nous allons vous expliquer exactement comment faire."),fe.forEach(n),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(db)),c(E,"id","dbogage-du-pipeline-dentranement"),c(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E,"href","#dbogage-du-pipeline-dentranement"),c(C,"class","relative group"),c(us,"href","/course/fr/chapter7"),c(ns,"id","dboguer-le-pipeline-dentranement"),c(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ns,"href","#dboguer-le-pipeline-dentranement"),c(G,"class","relative group"),c(_e,"href","https://huggingface.co/datasets/glue"),c(_e,"rel","nofollow"),c(Ls,"id","vrifier-vos-donnes"),c(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ls,"href","#vrifier-vos-donnes"),c(Es,"class","relative group"),c(Us,"id","vrifier-votre-modle"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#vrifier-votre-modle"),c(gs,"class","relative group"),c(Ks,"id","vrifier-les-hyperparamtres"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#vrifier-les-hyperparamtres"),c(zs,"class","relative group"),c(He,"href","https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"),c(He,"rel","nofollow"),c(Js,"id","autres-problmes-potentiels"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#autres-problmes-potentiels"),c(ys,"class","relative group"),c(Xs,"id","grer-les-erreurs-de-manque-de-mmoire"),c(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xs,"href","#grer-les-erreurs-de-manque-de-mmoire"),c(ws,"class","relative group"),c(Ys,"id","tensorflow-affam"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#tensorflow-affam"),c(ks,"class","relative group"),c(Zs,"id","vrifiez-vos-donnes-encore"),c(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zs,"href","#vrifiez-vos-donnes-encore"),c(Cs,"class","relative group"),c(ee,"id","surentranement-du-modle-sur-un-seul-batch"),c(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ee,"href","#surentranement-du-modle-sur-un-seul-batch"),c(Ps,"class","relative group"),c(re,"id","ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),c(xs,"class","relative group"),c(ue,"id","demander-de-laide"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#demander-de-laide"),c(As,"class","relative group"),c(rn,"href","https://discuss.huggingface.co/"),c(rn,"rel","nofollow"),c(tn,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),c(tn,"rel","nofollow"),c(pn,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),c(pn,"rel","nofollow"),c(un,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),c(un,"rel","nofollow"),c(on,"href","http://karpathy.github.io/2019/04/25/recipe/"),c(on,"rel","nofollow")},m(s,u){e(document.head,d),o(s,w,u),f(_,s,u),o(s,z,u),o(s,C,u),e(C,E),e(E,y),f(P,y,null),e(C,N),e(C,D),e(D,ps),o(s,U,u),f(es,s,u),o(s,Ds,u),o(s,T,u),e(T,I),e(T,qs),e(qs,_s),e(T,ve),e(T,us),e(us,Ss),e(T,os),e(T,$s),e($s,x),e(T,A),o(s,je,u),o(s,G,u),e(G,ns),e(ns,Ts),f(K,Ts,null),e(G,hn),e(G,Os),e(Os,cp),o(s,Wl,u),f(qe,s,u),o(s,Zl,u),o(s,is,u),e(is,dp),e(is,Rn),e(Rn,bp),e(is,hp),e(is,Bn),e(Bn,fp),e(is,vp),o(s,sr,u),o(s,Ms,u),e(Ms,jp),e(Ms,Jn),e(Jn,qp),e(Ms,_p),o(s,er,u),o(s,ms,u),e(ms,$p),e(ms,Xn),e(Xn,Ep),e(ms,gp),e(ms,_e),e(_e,zp),e(ms,yp),o(s,nr,u),f($e,s,u),o(s,ar,u),o(s,Ns,u),e(Ns,wp),e(Ns,Qn),e(Qn,kp),e(Ns,Cp),o(s,lr,u),o(s,fn,u),e(fn,Pp),o(s,rr,u),f(Ee,s,u),o(s,tr,u),o(s,vn,u),e(vn,xp),o(s,pr,u),o(s,Es,u),e(Es,Ls),e(Ls,Yn),f(ge,Yn,null),e(Es,Ap),e(Es,Wn),e(Wn,Dp),o(s,ur,u),o(s,jn,u),e(jn,Sp),o(s,or,u),o(s,S,u),e(S,Tp),e(S,Zn),e(Zn,Op),e(S,Mp),e(S,sa),e(sa,Np),e(S,Lp),e(S,ea),e(ea,Fp),e(S,Vp),e(S,na),e(na,Up),e(S,Ip),e(S,aa),e(aa,Gp),e(S,Kp),e(S,la),e(la,Hp),e(S,Rp),o(s,ir,u),f(ze,s,u),o(s,mr,u),o(s,as,u),e(as,ra),e(ra,Bp),e(as,Jp),e(as,ta),e(ta,Xp),e(as,Qp),e(as,pa),e(pa,Yp),e(as,Wp),o(s,cr,u),f(ye,s,u),o(s,dr,u),o(s,H,u),e(H,Zp),e(H,ua),e(ua,su),e(H,eu),e(H,oa),e(oa,nu),e(H,au),e(H,ia),e(ia,lu),e(H,ru),o(s,br,u),o(s,cs,u),e(cs,tu),e(cs,ma),e(ma,pu),e(cs,uu),e(cs,ca),e(ca,ou),e(cs,iu),o(s,hr,u),o(s,ds,u),e(ds,mu),e(ds,da),e(da,cu),e(ds,du),e(ds,ba),e(ba,bu),e(ds,hu),o(s,fr,u),f(we,s,u),o(s,vr,u),o(s,qn,u),e(qn,fu),o(s,jr,u),f(Fs,s,u),o(s,qr,u),o(s,Vs,u),e(Vs,vu),e(Vs,ha),e(ha,ju),e(Vs,qu),o(s,_r,u),f(ke,s,u),o(s,$r,u),o(s,_n,u),e(_n,_u),o(s,Er,u),o(s,Ce,u),e(Ce,fa),e(fa,$u),e(Ce,Eu),o(s,gr,u),o(s,gs,u),e(gs,Us),e(Us,va),f(Pe,va,null),e(gs,gu),e(gs,ja),e(ja,zu),o(s,zr,u),o(s,ls,u),e(ls,qa),e(qa,yu),e(ls,wu),e(ls,_a),e(_a,ku),e(ls,Cu),e(ls,$a),e($a,Pu),e(ls,xu),o(s,yr,u),o(s,bs,u),e(bs,Au),e(bs,Ea),e(Ea,Du),e(bs,Su),e(bs,ga),e(ga,Tu),e(bs,Ou),o(s,wr,u),f(xe,s,u),o(s,kr,u),f(Ae,s,u),o(s,Cr,u),o(s,$,u),e($,Mu),e($,za),e(za,Nu),e($,Lu),e($,ya),e(ya,Fu),e($,Vu),e($,wa),e(wa,Uu),e($,Iu),e($,ka),e(ka,Gu),e($,Ku),e($,Ca),e(Ca,Hu),e($,Ru),e($,Pa),e(Pa,Bu),e($,Ju),e($,xa),e(xa,Xu),e($,Qu),e($,Aa),e(Aa,Yu),e($,Wu),e($,Da),e(Da,Zu),e($,so),e($,Sa),e(Sa,eo),e($,no),e($,Ta),e(Ta,ao),e($,lo),e($,Oa),e(Oa,ro),e($,to),e($,Ma),e(Ma,po),e($,uo),e($,Na),e(Na,oo),e($,io),o(s,Pr,u),o(s,R,u),e(R,mo),e(R,La),e(La,co),e(R,bo),e(R,Fa),e(Fa,ho),e(R,fo),e(R,Va),e(Va,vo),e(R,jo),o(s,xr,u),f(De,s,u),o(s,Ar,u),o(s,$n,u),e($n,qo),o(s,Dr,u),f(Se,s,u),o(s,Sr,u),o(s,rs,u),e(rs,Ua),e(Ua,_o),e(rs,$o),e(rs,Ia),e(Ia,Eo),e(rs,go),e(rs,Ga),e(Ga,zo),e(rs,yo),o(s,Tr,u),f(Te,s,u),o(s,Or,u),f(Oe,s,u),o(s,Mr,u),o(s,En,u),e(En,wo),o(s,Nr,u),f(Me,s,u),o(s,Lr,u),f(Ne,s,u),o(s,Fr,u),o(s,gn,u),e(gn,ko),o(s,Vr,u),f(Le,s,u),o(s,Ur,u),f(Fe,s,u),o(s,Ir,u),o(s,hs,u),e(hs,Co),e(hs,Ka),e(Ka,Po),e(hs,xo),e(hs,Ha),e(Ha,Ao),e(hs,Do),o(s,Gr,u),f(Ve,s,u),o(s,Kr,u),f(Ue,s,u),o(s,Hr,u),o(s,Is,u),e(Is,So),e(Is,Ra),e(Ra,To),e(Is,Oo),o(s,Rr,u),f(Ie,s,u),o(s,Br,u),f(Ge,s,u),o(s,Jr,u),o(s,Gs,u),e(Gs,Mo),e(Gs,Ba),e(Ba,No),e(Gs,Lo),o(s,Xr,u),o(s,zs,u),e(zs,Ks),e(Ks,Ja),f(Ke,Ja,null),e(zs,Fo),e(zs,Xa),e(Xa,Vo),o(s,Qr,u),o(s,fs,u),e(fs,Uo),e(fs,Qa),e(Qa,Io),e(fs,Go),e(fs,Ya),e(Ya,Ko),e(fs,Ho),o(s,Yr,u),o(s,Hs,u),e(Hs,Ro),e(Hs,He),e(He,Bo),e(Hs,Jo),o(s,Wr,u),o(s,L,u),e(L,Xo),e(L,Wa),e(Wa,Qo),e(L,Yo),e(L,Za),e(Za,Wo),e(L,Zo),e(L,sl),e(sl,si),e(L,ei),e(L,el),e(el,ni),e(L,ai),o(s,Zr,u),f(Re,s,u),o(s,st,u),f(Rs,s,u),o(s,et,u),o(s,Bs,u),e(Bs,li),e(Bs,nl),e(nl,ri),e(Bs,ti),o(s,nt,u),f(Be,s,u),o(s,at,u),f(Je,s,u),o(s,lt,u),o(s,zn,u),e(zn,pi),o(s,rt,u),o(s,ys,u),e(ys,Js),e(Js,al),f(Xe,al,null),e(ys,ui),e(ys,ll),e(ll,oi),o(s,tt,u),o(s,yn,u),e(yn,ii),o(s,pt,u),o(s,ws,u),e(ws,Xs),e(Xs,rl),f(Qe,rl,null),e(ws,mi),e(ws,tl),e(tl,ci),o(s,ut,u),o(s,B,u),e(B,di),e(B,pl),e(pl,bi),e(B,hi),e(B,ul),e(ul,fi),e(B,vi),e(B,ol),e(ol,ji),e(B,qi),o(s,ot,u),f(Qs,s,u),o(s,it,u),o(s,ks,u),e(ks,Ys),e(Ys,il),f(Ye,il,null),e(ks,_i),e(ks,ml),e(ml,$i),o(s,mt,u),o(s,J,u),e(J,Ei),e(J,cl),e(cl,gi),e(J,zi),e(J,dl),e(dl,yi),e(J,wi),e(J,bl),e(bl,ki),e(J,Ci),o(s,ct,u),o(s,O,u),e(O,Pi),e(O,hl),e(hl,xi),e(O,Ai),e(O,fl),e(fl,Di),e(O,Si),e(O,vl),e(vl,Ti),e(O,Oi),e(O,jl),e(jl,Mi),e(O,Ni),e(O,ql),e(ql,Li),e(O,Fi),o(s,dt,u),o(s,Ws,u),e(Ws,Vi),e(Ws,_l),e(_l,Ui),e(Ws,Ii),o(s,bt,u),o(s,Cs,u),e(Cs,Zs),e(Zs,$l),f(We,$l,null),e(Cs,Gi),e(Cs,El),e(El,Ki),o(s,ht,u),o(s,M,u),e(M,Hi),e(M,gl),e(gl,Ri),e(M,Bi),e(M,zl),e(zl,Ji),e(M,Xi),e(M,yl),e(yl,Qi),e(M,Yi),e(M,wl),e(wl,Wi),e(M,Zi),e(M,kl),e(kl,sm),e(M,em),o(s,ft,u),f(Ze,s,u),o(s,vt,u),o(s,wn,u),e(wn,nm),o(s,jt,u),f(sn,s,u),o(s,qt,u),o(s,kn,u),e(kn,am),o(s,_t,u),o(s,X,u),e(X,Cl),e(Cl,lm),e(X,rm),e(X,Pl),e(Pl,tm),e(X,pm),e(X,xl),e(xl,um),e(X,om),e(X,Al),e(Al,im),o(s,$t,u),o(s,se,u),e(se,mm),e(se,Dl),e(Dl,cm),e(se,dm),o(s,Et,u),o(s,Cn,u),e(Cn,bm),o(s,gt,u),o(s,Pn,u),e(Pn,hm),o(s,zt,u),o(s,Ps,u),e(Ps,ee),e(ee,Sl),f(en,Sl,null),e(Ps,fm),e(Ps,Tl),e(Tl,vm),o(s,yt,u),o(s,xn,u),e(xn,jm),o(s,wt,u),o(s,vs,u),e(vs,qm),e(vs,Ol),e(Ol,_m),e(vs,$m),e(vs,Ml),e(Ml,Em),e(vs,gm),o(s,kt,u),f(nn,s,u),o(s,Ct,u),f(ne,s,u),o(s,Pt,u),o(s,ae,u),e(ae,zm),e(ae,Nl),e(Nl,ym),e(ae,wm),o(s,xt,u),o(s,An,u),e(An,km),o(s,At,u),f(le,s,u),o(s,Dt,u),o(s,xs,u),e(xs,re),e(re,Ll),f(an,Ll,null),e(xs,Cm),e(xs,Fl),e(Fl,Pm),o(s,St,u),o(s,te,u),e(te,xm),e(te,Vl),e(Vl,Am),e(te,Dm),o(s,Tt,u),o(s,pe,u),e(pe,Sm),e(pe,Ul),e(Ul,Tm),e(pe,Om),o(s,Ot,u),o(s,Dn,u),e(Dn,Mm),o(s,Mt,u),o(s,As,u),e(As,ue),e(ue,Il),f(ln,Il,null),e(As,Nm),e(As,Gl),e(Gl,Lm),o(s,Nt,u),o(s,oe,u),e(oe,Fm),e(oe,rn),e(rn,Vm),e(oe,Um),o(s,Lt,u),o(s,Sn,u),e(Sn,Im),o(s,Ft,u),o(s,Q,u),e(Q,Tn),e(Tn,tn),e(tn,Gm),e(Tn,Km),e(Q,Hm),e(Q,On),e(On,pn),e(pn,Rm),e(On,Bm),e(Q,Jm),e(Q,Mn),e(Mn,un),e(un,Xm),e(Mn,Qm),e(Q,Ym),e(Q,Nn),e(Nn,on),e(on,Wm),e(Nn,Zm),o(s,Vt,u),o(s,Y,u),e(Y,sc),e(Y,Kl),e(Kl,ec),e(Y,nc),e(Y,Hl),e(Hl,ac),e(Y,lc),e(Y,Rl),e(Rl,rc),e(Y,tc),Ut=!0},p(s,[u]){const mn={};u&1&&(mn.fw=s[0]),_.$set(mn);const Bl={};u&2&&(Bl.$$scope={dirty:u,ctx:s}),Fs.$set(Bl);const Jl={};u&2&&(Jl.$$scope={dirty:u,ctx:s}),Rs.$set(Jl);const Xl={};u&2&&(Xl.$$scope={dirty:u,ctx:s}),Qs.$set(Xl);const ts={};u&2&&(ts.$$scope={dirty:u,ctx:s}),ne.$set(ts);const Ql={};u&2&&(Ql.$$scope={dirty:u,ctx:s}),le.$set(Ql)},i(s){Ut||(v(_.$$.fragment,s),v(P.$$.fragment,s),v(es.$$.fragment,s),v(K.$$.fragment,s),v(qe.$$.fragment,s),v($e.$$.fragment,s),v(Ee.$$.fragment,s),v(ge.$$.fragment,s),v(ze.$$.fragment,s),v(ye.$$.fragment,s),v(we.$$.fragment,s),v(Fs.$$.fragment,s),v(ke.$$.fragment,s),v(Pe.$$.fragment,s),v(xe.$$.fragment,s),v(Ae.$$.fragment,s),v(De.$$.fragment,s),v(Se.$$.fragment,s),v(Te.$$.fragment,s),v(Oe.$$.fragment,s),v(Me.$$.fragment,s),v(Ne.$$.fragment,s),v(Le.$$.fragment,s),v(Fe.$$.fragment,s),v(Ve.$$.fragment,s),v(Ue.$$.fragment,s),v(Ie.$$.fragment,s),v(Ge.$$.fragment,s),v(Ke.$$.fragment,s),v(Re.$$.fragment,s),v(Rs.$$.fragment,s),v(Be.$$.fragment,s),v(Je.$$.fragment,s),v(Xe.$$.fragment,s),v(Qe.$$.fragment,s),v(Qs.$$.fragment,s),v(Ye.$$.fragment,s),v(We.$$.fragment,s),v(Ze.$$.fragment,s),v(sn.$$.fragment,s),v(en.$$.fragment,s),v(nn.$$.fragment,s),v(ne.$$.fragment,s),v(le.$$.fragment,s),v(an.$$.fragment,s),v(ln.$$.fragment,s),Ut=!0)},o(s){j(_.$$.fragment,s),j(P.$$.fragment,s),j(es.$$.fragment,s),j(K.$$.fragment,s),j(qe.$$.fragment,s),j($e.$$.fragment,s),j(Ee.$$.fragment,s),j(ge.$$.fragment,s),j(ze.$$.fragment,s),j(ye.$$.fragment,s),j(we.$$.fragment,s),j(Fs.$$.fragment,s),j(ke.$$.fragment,s),j(Pe.$$.fragment,s),j(xe.$$.fragment,s),j(Ae.$$.fragment,s),j(De.$$.fragment,s),j(Se.$$.fragment,s),j(Te.$$.fragment,s),j(Oe.$$.fragment,s),j(Me.$$.fragment,s),j(Ne.$$.fragment,s),j(Le.$$.fragment,s),j(Fe.$$.fragment,s),j(Ve.$$.fragment,s),j(Ue.$$.fragment,s),j(Ie.$$.fragment,s),j(Ge.$$.fragment,s),j(Ke.$$.fragment,s),j(Re.$$.fragment,s),j(Rs.$$.fragment,s),j(Be.$$.fragment,s),j(Je.$$.fragment,s),j(Xe.$$.fragment,s),j(Qe.$$.fragment,s),j(Qs.$$.fragment,s),j(Ye.$$.fragment,s),j(We.$$.fragment,s),j(Ze.$$.fragment,s),j(sn.$$.fragment,s),j(en.$$.fragment,s),j(nn.$$.fragment,s),j(ne.$$.fragment,s),j(le.$$.fragment,s),j(an.$$.fragment,s),j(ln.$$.fragment,s),Ut=!1},d(s){n(d),s&&n(w),q(_,s),s&&n(z),s&&n(C),q(P),s&&n(U),q(es,s),s&&n(Ds),s&&n(T),s&&n(je),s&&n(G),q(K),s&&n(Wl),q(qe,s),s&&n(Zl),s&&n(is),s&&n(sr),s&&n(Ms),s&&n(er),s&&n(ms),s&&n(nr),q($e,s),s&&n(ar),s&&n(Ns),s&&n(lr),s&&n(fn),s&&n(rr),q(Ee,s),s&&n(tr),s&&n(vn),s&&n(pr),s&&n(Es),q(ge),s&&n(ur),s&&n(jn),s&&n(or),s&&n(S),s&&n(ir),q(ze,s),s&&n(mr),s&&n(as),s&&n(cr),q(ye,s),s&&n(dr),s&&n(H),s&&n(br),s&&n(cs),s&&n(hr),s&&n(ds),s&&n(fr),q(we,s),s&&n(vr),s&&n(qn),s&&n(jr),q(Fs,s),s&&n(qr),s&&n(Vs),s&&n(_r),q(ke,s),s&&n($r),s&&n(_n),s&&n(Er),s&&n(Ce),s&&n(gr),s&&n(gs),q(Pe),s&&n(zr),s&&n(ls),s&&n(yr),s&&n(bs),s&&n(wr),q(xe,s),s&&n(kr),q(Ae,s),s&&n(Cr),s&&n($),s&&n(Pr),s&&n(R),s&&n(xr),q(De,s),s&&n(Ar),s&&n($n),s&&n(Dr),q(Se,s),s&&n(Sr),s&&n(rs),s&&n(Tr),q(Te,s),s&&n(Or),q(Oe,s),s&&n(Mr),s&&n(En),s&&n(Nr),q(Me,s),s&&n(Lr),q(Ne,s),s&&n(Fr),s&&n(gn),s&&n(Vr),q(Le,s),s&&n(Ur),q(Fe,s),s&&n(Ir),s&&n(hs),s&&n(Gr),q(Ve,s),s&&n(Kr),q(Ue,s),s&&n(Hr),s&&n(Is),s&&n(Rr),q(Ie,s),s&&n(Br),q(Ge,s),s&&n(Jr),s&&n(Gs),s&&n(Xr),s&&n(zs),q(Ke),s&&n(Qr),s&&n(fs),s&&n(Yr),s&&n(Hs),s&&n(Wr),s&&n(L),s&&n(Zr),q(Re,s),s&&n(st),q(Rs,s),s&&n(et),s&&n(Bs),s&&n(nt),q(Be,s),s&&n(at),q(Je,s),s&&n(lt),s&&n(zn),s&&n(rt),s&&n(ys),q(Xe),s&&n(tt),s&&n(yn),s&&n(pt),s&&n(ws),q(Qe),s&&n(ut),s&&n(B),s&&n(ot),q(Qs,s),s&&n(it),s&&n(ks),q(Ye),s&&n(mt),s&&n(J),s&&n(ct),s&&n(O),s&&n(dt),s&&n(Ws),s&&n(bt),s&&n(Cs),q(We),s&&n(ht),s&&n(M),s&&n(ft),q(Ze,s),s&&n(vt),s&&n(wn),s&&n(jt),q(sn,s),s&&n(qt),s&&n(kn),s&&n(_t),s&&n(X),s&&n($t),s&&n(se),s&&n(Et),s&&n(Cn),s&&n(gt),s&&n(Pn),s&&n(zt),s&&n(Ps),q(en),s&&n(yt),s&&n(xn),s&&n(wt),s&&n(vs),s&&n(kt),q(nn,s),s&&n(Ct),q(ne,s),s&&n(Pt),s&&n(ae),s&&n(xt),s&&n(An),s&&n(At),q(le,s),s&&n(Dt),s&&n(xs),q(an),s&&n(St),s&&n(te),s&&n(Tt),s&&n(pe),s&&n(Ot),s&&n(Dn),s&&n(Mt),s&&n(As),q(ln),s&&n(Nt),s&&n(oe),s&&n(Lt),s&&n(Sn),s&&n(Ft),s&&n(Q),s&&n(Vt),s&&n(Y)}}}const db={local:"dbogage-du-pipeline-dentranement",sections:[{local:"dboguer-le-pipeline-dentranement",sections:[{local:"vrifier-vos-donnes",title:"V\xE9rifier vos donn\xE9es"},{local:"vrifier-votre-modle",title:"V\xE9rifier votre mod\xE8le"},{local:"vrifier-les-hyperparamtres",title:"V\xE9rifier les hyperparam\xE8tres"}],title:"D\xE9boguer le pipeline d'entra\xEEnement"},{local:"autres-problmes-potentiels",sections:[{local:"grer-les-erreurs-de-manque-de-mmoire",title:"G\xE9rer les erreurs de manque de m\xE9moire"},{local:"tensorflow-affam",title:"TensorFlow affam\xE9 \u{1F99B}"},{local:"vrifiez-vos-donnes-encore",title:"V\xE9rifiez vos donn\xE9es (encore !)"},{local:"surentranement-du-modle-sur-un-seul-batch",title:"Surentra\xEEnement du mod\xE8le sur un seul batch"},{local:"ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base",title:"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"},{local:"demander-de-laide",title:"Demander de l'aide"}],title:"Autres probl\xE8mes potentiels "}],title:"D\xE9bogage du pipeline d'entra\xEEnement"};function bb(V,d,w){let _="pt";return ab(()=>{const z=new URLSearchParams(window.location.search);w(0,_=z.get("fw")||"pt")}),[_]}class Eb extends Zd{constructor(d){super();sb(this,d,bb,cb,eb,{})}}export{Eb as default,db as metadata};
