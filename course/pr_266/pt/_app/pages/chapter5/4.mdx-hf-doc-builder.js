import{S as di,i as mi,s as pi,e as n,k as p,w as g,t as o,M as ci,c as i,d as s,m as c,a as l,x as b,h as t,b as u,G as a,g as d,y as _,q as $,o as j,B as x,v as ui}from"../../chunks/vendor-hf-doc-builder.js";import{T as xs}from"../../chunks/Tip-hf-doc-builder.js";import{Y as fi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Mo}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as q}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as hi}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function gi(H){let m,T,f,w,k,h,O,A,E,v,P,y,I,N;return{c(){m=n("p"),T=o("\u270E Por padr\xE3o, \u{1F917} Datasets descompactar\xE1 os arquivos necess\xE1rios para carregar um dataset. Se voc\xEA quiser preservar espa\xE7o no disco r\xEDgido, voc\xEA pode passar "),f=n("code"),w=o("DownloadConfig(delete_extracted=True)"),k=o(" para o argumento "),h=n("code"),O=o("download_config"),A=o(" de "),E=n("code"),v=o("load_dataset()"),P=o(". Consulte a "),y=n("a"),I=o("documenta\xE7\xE3o"),N=o(" para obter mais detalhes."),this.h()},l(D){m=i(D,"P",{});var C=l(m);T=t(C,"\u270E Por padr\xE3o, \u{1F917} Datasets descompactar\xE1 os arquivos necess\xE1rios para carregar um dataset. Se voc\xEA quiser preservar espa\xE7o no disco r\xEDgido, voc\xEA pode passar "),f=i(C,"CODE",{});var G=l(f);w=t(G,"DownloadConfig(delete_extracted=True)"),G.forEach(s),k=t(C," para o argumento "),h=i(C,"CODE",{});var z=l(h);O=t(z,"download_config"),z.forEach(s),A=t(C," de "),E=i(C,"CODE",{});var S=l(E);v=t(S,"load_dataset()"),S.forEach(s),P=t(C,". Consulte a "),y=i(C,"A",{href:!0,rel:!0});var U=l(y);I=t(U,"documenta\xE7\xE3o"),U.forEach(s),N=t(C," para obter mais detalhes."),C.forEach(s),this.h()},h(){u(y,"href","https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig"),u(y,"rel","nofollow")},m(D,C){d(D,m,C),a(m,T),a(m,f),a(f,w),a(m,k),a(m,h),a(h,O),a(m,A),a(m,E),a(E,v),a(m,P),a(m,y),a(y,I),a(m,N)},d(D){D&&s(m)}}}function bi(H){let m,T,f,w,k,h,O,A,E,v,P,y,I,N,D,C;return{c(){m=n("p"),T=o("\u270F\uFE0F "),f=n("strong"),w=o("Experimente!"),k=o(" Escolha um dos "),h=n("a"),O=o("subconjuntos"),A=o(" da "),E=n("code"),v=o("The Pile"),P=o(" que \xE9 maior que a RAM do seu laptop ou desktop, carregue com \u{1F917} Datasets e me\xE7a a quantidade de RAM usada. Observe que, para obter uma medi\xE7\xE3o precisa, voc\xEA desejar\xE1 fazer isso em um novo processo. Voc\xEA pode encontrar os tamanhos descompactados de cada subconjunto na Tabela 1 do "),y=n("a"),I=o("artigo do "),N=n("code"),D=o("The Pile"),C=o("."),this.h()},l(G){m=i(G,"P",{});var z=l(m);T=t(z,"\u270F\uFE0F "),f=i(z,"STRONG",{});var S=l(f);w=t(S,"Experimente!"),S.forEach(s),k=t(z," Escolha um dos "),h=i(z,"A",{href:!0,rel:!0});var U=l(h);O=t(U,"subconjuntos"),U.forEach(s),A=t(z," da "),E=i(z,"CODE",{});var ne=l(E);v=t(ne,"The Pile"),ne.forEach(s),P=t(z," que \xE9 maior que a RAM do seu laptop ou desktop, carregue com \u{1F917} Datasets e me\xE7a a quantidade de RAM usada. Observe que, para obter uma medi\xE7\xE3o precisa, voc\xEA desejar\xE1 fazer isso em um novo processo. Voc\xEA pode encontrar os tamanhos descompactados de cada subconjunto na Tabela 1 do "),y=i(z,"A",{href:!0,rel:!0});var we=l(y);I=t(we,"artigo do "),N=i(we,"CODE",{});var ba=l(N);D=t(ba,"The Pile"),ba.forEach(s),we.forEach(s),C=t(z,"."),z.forEach(s),this.h()},h(){u(h,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(h,"rel","nofollow"),u(y,"href","https://arxiv.org/abs/2101.00027"),u(y,"rel","nofollow")},m(G,z){d(G,m,z),a(m,T),a(m,f),a(f,w),a(m,k),a(m,h),a(h,O),a(m,A),a(m,E),a(E,v),a(m,P),a(m,y),a(y,I),a(y,N),a(N,D),a(m,C)},d(G){G&&s(m)}}}function _i(H){let m,T,f,w,k,h,O;return{c(){m=n("p"),T=o("\u{1F4A1} Nos notebooks Jupyter, voc\xEA tamb\xE9m pode cronometrar c\xE9lulas usando a "),f=n("a"),w=n("code"),k=o("%%timeit"),h=o(" fun\xE7\xE3o m\xE1gica"),O=o("."),this.h()},l(A){m=i(A,"P",{});var E=l(m);T=t(E,"\u{1F4A1} Nos notebooks Jupyter, voc\xEA tamb\xE9m pode cronometrar c\xE9lulas usando a "),f=i(E,"A",{href:!0,rel:!0});var v=l(f);w=i(v,"CODE",{});var P=l(w);k=t(P,"%%timeit"),P.forEach(s),h=t(v," fun\xE7\xE3o m\xE1gica"),v.forEach(s),O=t(E,"."),E.forEach(s),this.h()},h(){u(f,"href","https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit"),u(f,"rel","nofollow")},m(A,E){d(A,m,E),a(m,T),a(m,f),a(f,w),a(w,k),a(f,h),a(m,O)},d(A){A&&s(m)}}}function $i(H){let m,T,f,w,k,h,O,A;return{c(){m=n("p"),T=o("\u{1F4A1} Para acelerar a tokeniza\xE7\xE3o com streaming voc\xEA pode passar "),f=n("code"),w=o("batched=True"),k=o(", como vimos na \xFAltima se\xE7\xE3o. Ele processar\xE1 os exemplos lote por lote; o tamanho do lote padr\xE3o \xE9 1.000 e pode ser especificado com o argumento "),h=n("code"),O=o("batch_size"),A=o(".")},l(E){m=i(E,"P",{});var v=l(m);T=t(v,"\u{1F4A1} Para acelerar a tokeniza\xE7\xE3o com streaming voc\xEA pode passar "),f=i(v,"CODE",{});var P=l(f);w=t(P,"batched=True"),P.forEach(s),k=t(v,", como vimos na \xFAltima se\xE7\xE3o. Ele processar\xE1 os exemplos lote por lote; o tamanho do lote padr\xE3o \xE9 1.000 e pode ser especificado com o argumento "),h=i(v,"CODE",{});var y=l(h);O=t(y,"batch_size"),y.forEach(s),A=t(v,"."),v.forEach(s)},m(E,v){d(E,m,v),a(m,T),a(m,f),a(f,w),a(m,k),a(m,h),a(h,O),a(m,A)},d(E){E&&s(m)}}}function ji(H){let m,T,f,w,k,h,O,A,E,v,P,y,I;return{c(){m=n("p"),T=o("\u270F\uFE0F "),f=n("strong"),w=o("Experimente!"),k=o(" Use um dos grandes corpora Common Crawl como "),h=n("a"),O=n("code"),A=o("mc4"),E=o(" ou "),v=n("a"),P=n("code"),y=o("oscar"),I=o(" para criar um conjunto de dados multil\xEDngue de streaming que represente as propor\xE7\xF5es faladas de idiomas em um pa\xEDs de sua escolha. Por exemplo, as quatro l\xEDnguas nacionais na Su\xED\xE7a s\xE3o alem\xE3o, franc\xEAs, italiano e romanche, ent\xE3o voc\xEA pode tentar criar um corpus su\xED\xE7o amostrando os subconjuntos do Oscar de acordo com sua propor\xE7\xE3o falada."),this.h()},l(N){m=i(N,"P",{});var D=l(m);T=t(D,"\u270F\uFE0F "),f=i(D,"STRONG",{});var C=l(f);w=t(C,"Experimente!"),C.forEach(s),k=t(D," Use um dos grandes corpora Common Crawl como "),h=i(D,"A",{href:!0,rel:!0});var G=l(h);O=i(G,"CODE",{});var z=l(O);A=t(z,"mc4"),z.forEach(s),G.forEach(s),E=t(D," ou "),v=i(D,"A",{href:!0,rel:!0});var S=l(v);P=i(S,"CODE",{});var U=l(P);y=t(U,"oscar"),U.forEach(s),S.forEach(s),I=t(D," para criar um conjunto de dados multil\xEDngue de streaming que represente as propor\xE7\xF5es faladas de idiomas em um pa\xEDs de sua escolha. Por exemplo, as quatro l\xEDnguas nacionais na Su\xED\xE7a s\xE3o alem\xE3o, franc\xEAs, italiano e romanche, ent\xE3o voc\xEA pode tentar criar um corpus su\xED\xE7o amostrando os subconjuntos do Oscar de acordo com sua propor\xE7\xE3o falada."),D.forEach(s),this.h()},h(){u(h,"href","https://huggingface.co/datasets/mc4"),u(h,"rel","nofollow"),u(v,"href","https://huggingface.co/datasets/oscar"),u(v,"rel","nofollow")},m(N,D){d(N,m,D),a(m,T),a(m,f),a(f,w),a(m,k),a(m,h),a(h,O),a(O,A),a(m,E),a(m,v),a(v,P),a(P,y),a(m,I)},d(N){N&&s(m)}}}function xi(H){let m,T,f,w,k,h,O,A,E,v,P,y,I,N,D,C,G,z,S,U,ne,we,ba,Ca,Go,Bo,vs,ye,Es,ie,Lo,qe,Ho,Uo,ws,oe,le,za,Ae,Fo,Ia,Vo,ys,R,Jo,Ra,Wo,Ko,Pe,Yo,Zo,De,Qo,Xo,ke,et,at,Oe,st,ot,Te,tt,rt,Na,nt,it,qs,Ce,As,de,lt,_a,dt,mt,Ps,ze,Ds,Ie,ks,$a,pt,Os,me,Ts,ja,ct,Cs,Re,zs,Ne,Is,xa,ut,Rs,te,pe,Sa,Se,ft,Ma,ht,Ns,K,gt,Me,Ga,bt,_t,Ba,$t,jt,Ss,Ge,Ms,ce,xt,La,vt,Et,Gs,Be,Bs,Le,Ls,F,wt,Ha,yt,qt,Ua,At,Pt,Fa,Dt,kt,Hs,He,Us,Ue,Fs,va,Ot,Vs,ue,Js,Y,Tt,Fe,Ct,zt,Ve,It,Rt,Ws,B,Nt,Va,St,Mt,Je,Gt,Bt,We,Ja,Lt,Ht,Ke,Ut,Ft,Ks,Ye,Ys,Ze,Zs,Z,Vt,Wa,Jt,Wt,Ka,Kt,Yt,Qs,fe,Xs,re,he,Ya,Qe,Zt,Za,Qt,eo,Q,Xt,Qa,er,ar,Xa,sr,or,ao,Xe,so,L,tr,es,rr,nr,as,ir,lr,ss,dr,mr,os,pr,cr,oo,ea,to,aa,ro,X,ur,ts,fr,hr,Ea,gr,br,no,sa,io,oa,lo,ge,mo,V,_r,rs,$r,jr,ns,xr,vr,is,Er,wr,po,ta,co,ra,uo,J,yr,ls,qr,Ar,ds,Pr,Dr,ms,kr,Or,fo,na,ho,ia,go,be,Tr,ps,Cr,zr,bo,la,_o,W,Ir,cs,Rr,Nr,us,Sr,Mr,fs,Gr,Br,$o,da,jo,ma,xo,_e,Lr,hs,Hr,Ur,vo,pa,Eo,ca,wo,ee,Fr,gs,Vr,Jr,bs,Wr,Kr,yo,wa,Yr,qo,ua,Ao,fa,Po,$e,Do,ya,Zr,ko;return h=new Mo({}),P=new hi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"}]}}),ye=new fi({props:{id:"JwISwTCPPWo"}}),Ae=new Mo({}),Ce=new q({props:{code:"!pip install zstandard",highlighted:"!pip install zstandard"}}),ze=new q({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># This takes a few minutes to run, so go grab a tea or coffee while you wait :)</span>
data_files = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst&quot;</span>
pubmed_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
pubmed_dataset`}}),Ie=new q({props:{code:`Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;meta&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],
    num_rows: <span class="hljs-number">15518009</span>
})`}}),me=new xs({props:{$$slots:{default:[gi]},$$scope:{ctx:H}}}),Re=new q({props:{code:"pubmed_dataset[0]",highlighted:'pubmed_dataset[<span class="hljs-number">0</span>]'}}),Ne=new q({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),Se=new Mo({}),Ge=new q({props:{code:"!pip install psutil",highlighted:"!pip install psutil"}}),Be=new q({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> psutil

<span class="hljs-comment"># Process.memory_info is expressed in bytes, so convert to megabytes</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;RAM used: <span class="hljs-subst">{psutil.Process().memory_info().rss / (<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>):<span class="hljs-number">.2</span>f}</span> MB&quot;</span>)`}}),Le=new q({props:{code:"RAM used: 5678.33 MB",highlighted:'RAM used: <span class="hljs-number">5678.33</span> MB'}}),He=new q({props:{code:`print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of files in dataset : <span class="hljs-subst">{pubmed_dataset.dataset_size}</span>&quot;</span>)
size_gb = pubmed_dataset.dataset_size / (<span class="hljs-number">1024</span>**<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Dataset size (cache file) : <span class="hljs-subst">{size_gb:<span class="hljs-number">.2</span>f}</span> GB&quot;</span>)`}}),Ue=new q({props:{code:`Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB`,highlighted:`Number of files <span class="hljs-keyword">in</span> dataset : <span class="hljs-number">20979437051</span>
Dataset size (cache file) : <span class="hljs-number">19.54</span> GB`}}),ue=new xs({props:{$$slots:{default:[bi]},$$scope:{ctx:H}}}),Ye=new q({props:{code:`

`,highlighted:`<span class="hljs-keyword">import</span> timeit

code_snippet = <span class="hljs-string">&quot;&quot;&quot;batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
&quot;&quot;&quot;</span>

time = timeit.timeit(stmt=code_snippet, number=<span class="hljs-number">1</span>, <span class="hljs-built_in">globals</span>=<span class="hljs-built_in">globals</span>())
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f&quot;Iterated over <span class="hljs-subst">{<span class="hljs-built_in">len</span>(pubmed_dataset)}</span> examples (about <span class="hljs-subst">{size_gb:<span class="hljs-number">.1</span>f}</span> GB) in &quot;</span>
    <span class="hljs-string">f&quot;<span class="hljs-subst">{time:<span class="hljs-number">.1</span>f}</span>s, i.e. <span class="hljs-subst">{size_gb/time:<span class="hljs-number">.3</span>f}</span> GB/s&quot;</span>
)`}}),Ze=new q({props:{code:"'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'",highlighted:'<span class="hljs-string">&#x27;Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s&#x27;</span>'}}),fe=new xs({props:{$$slots:{default:[_i]},$$scope:{ctx:H}}}),Qe=new Mo({}),Xe=new q({props:{code:`pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)`,highlighted:`pubmed_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>, streaming=<span class="hljs-literal">True</span>
)`}}),ea=new q({props:{code:"next(iter(pubmed_dataset_streamed))",highlighted:'<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pubmed_dataset_streamed))'}}),aa=new q({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),sa=new q({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
tokenized_dataset = pubmed_dataset_streamed.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: tokenizer(x[<span class="hljs-string">&quot;text&quot;</span>]))
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tokenized_dataset))`}}),oa=new q({props:{code:"{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">4958</span>, <span class="hljs-number">5178</span>, <span class="hljs-number">4328</span>, <span class="hljs-number">6779</span>, ...], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ...]}'}}),ge=new xs({props:{$$slots:{default:[$i]},$$scope:{ctx:H}}}),ta=new q({props:{code:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))`,highlighted:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(shuffled_dataset))`}}),ra=new q({props:{code:`{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11410799</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...&#x27;</span>}`}}),na=new q({props:{code:`dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)`,highlighted:`dataset_head = pubmed_dataset_streamed.take(<span class="hljs-number">5</span>)
<span class="hljs-built_in">list</span>(dataset_head)`}}),ia=new q({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409575</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409576</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;Hypoxaemia in children with severe pneumonia in Papua New Guinea ...&quot;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409577</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen concentrators and cylinders ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409578</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen supply in rural africa: a personal experience ...&#x27;</span>}]`}}),la=new q({props:{code:`# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)`,highlighted:`<span class="hljs-comment"># Skip the first 1,000 examples and include the rest in the training set</span>
train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)
<span class="hljs-comment"># Take the first 1,000 examples for the validation set</span>
validation_dataset = shuffled_dataset.take(<span class="hljs-number">1000</span>)`}}),da=new q({props:{code:`law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))`,highlighted:`law_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>,
    data_files=<span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst&quot;</span>,
    split=<span class="hljs-string">&quot;train&quot;</span>,
    streaming=<span class="hljs-literal">True</span>,
)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(law_dataset_streamed))`}}),ma=new q({props:{code:`{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
  <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
  <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}`}}),pa=new q({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
<span class="hljs-built_in">list</span>(islice(combined_dataset, <span class="hljs-number">2</span>))`}}),ca=new q({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
   <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
   <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}]`}}),ua=new q({props:{code:`base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))`,highlighted:`base_url = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile/&quot;</span>
data_files = {
    <span class="hljs-string">&quot;train&quot;</span>: [base_url + <span class="hljs-string">&quot;train/&quot;</span> + <span class="hljs-string">f&quot;<span class="hljs-subst">{idx:02d}</span>.jsonl.zst&quot;</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>)],
    <span class="hljs-string">&quot;validation&quot;</span>: base_url + <span class="hljs-string">&quot;val.jsonl.zst&quot;</span>,
    <span class="hljs-string">&quot;test&quot;</span>: base_url + <span class="hljs-string">&quot;test.jsonl.zst&quot;</span>,
}
pile_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pile_dataset[<span class="hljs-string">&quot;train&quot;</span>]))`}}),fa=new q({props:{code:`{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pile_set_name&#x27;</span>: <span class="hljs-string">&#x27;Pile-CC&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...&#x27;</span>}`}}),$e=new xs({props:{$$slots:{default:[ji]},$$scope:{ctx:H}}}),{c(){m=n("meta"),T=p(),f=n("h1"),w=n("a"),k=n("span"),g(h.$$.fragment),O=p(),A=n("span"),E=o("Big data? \u{1F917} Datasets ao resgate"),v=p(),g(P.$$.fragment),y=p(),I=n("p"),N=o("Hoje em dia, n\xE3o \xE9 incomum encontrar-se trabalhando com conjuntos de dados de v\xE1rios gigabytes, especialmente se voc\xEA planeja pr\xE9-treinar um transformer como BERT ou GPT-2 do zero. Nesses casos, at\xE9 mesmo "),D=n("em"),C=o("carregar"),G=o(" os dados pode ser um desafio. Por exemplo, o corpus WebText usado para pr\xE9-treinar o GPT-2 consiste em mais de 8 milh\xF5es de documentos e 40 GB de texto - carregar isso na RAM do seu laptop provavelmente lhe causar\xE1 um ataque card\xEDaco!"),z=p(),S=n("p"),U=o("Felizmente, \u{1F917} Datasets foram projetados para superar essas limita\xE7\xF5es. Ele libera voc\xEA de problemas de gerenciamento de mem\xF3ria tratando conjuntos de dados como arquivos "),ne=n("em"),we=o("memory-mapped"),ba=o(" e de limites de disco r\xEDgido por "),Ca=n("em"),Go=o("streaming"),Bo=o(" das entradas em um corpus."),vs=p(),g(ye.$$.fragment),Es=p(),ie=n("p"),Lo=o("Nesta se\xE7\xE3o, exploraremos esses recursos de \u{1F917} Conjuntos de dados com um enorme corpus de 825 GB conhecido como "),qe=n("a"),Ho=o("the Pile"),Uo=o(". Vamos come\xE7ar!"),ws=p(),oe=n("h2"),le=n("a"),za=n("span"),g(Ae.$$.fragment),Fo=p(),Ia=n("span"),Vo=o("O que \xE9 the Pile?"),ys=p(),R=n("p"),Jo=o("O "),Ra=n("code"),Wo=o("The Pile"),Ko=o(" \xE9 um corpus de texto em ingl\xEAs que foi criado por "),Pe=n("a"),Yo=o("EleutherAI"),Zo=o(" para treinar modelos de linguagem em larga escala. Ele inclui uma gama diversificada de conjuntos de dados, abrangendo artigos cient\xEDficos, reposit\xF3rios de c\xF3digo do GitHub e texto da web filtrado. O corpus de treinamento est\xE1 dispon\xEDvel em "),De=n("a"),Qo=o("blocos de 14 GB"),Xo=o(", e voc\xEA tamb\xE9m pode baixar v\xE1rios dos "),ke=n("a"),et=o("componentes individuais"),at=o(". Vamos come\xE7ar dando uma olhada no conjunto de dados PubMed Abstracts, que \xE9 um corpus de resumos de 15 milh\xF5es de publica\xE7\xF5es biom\xE9dicas no "),Oe=n("a"),st=o("PubMed"),ot=o(". O conjunto de dados est\xE1 em "),Te=n("a"),tt=o("formato JSON Lines"),rt=o(" e \xE9 compactado usando a biblioteca "),Na=n("code"),nt=o("zstandard"),it=o(", ent\xE3o primeiro precisamos instal\xE1-lo:"),qs=p(),g(Ce.$$.fragment),As=p(),de=n("p"),lt=o("Em seguida, podemos carregar o conjunto de dados usando o m\xE9todo para arquivos remotos que aprendemos na "),_a=n("a"),dt=o("se\xE7\xE3o 2"),mt=o(":"),Ps=p(),g(ze.$$.fragment),Ds=p(),g(Ie.$$.fragment),ks=p(),$a=n("p"),pt=o("Podemos ver que h\xE1 15.518.009 linhas e 2 colunas em nosso conjunto de dados - isso \xE9 muito!"),Os=p(),g(me.$$.fragment),Ts=p(),ja=n("p"),ct=o("Vamos inspecionar o conte\xFAdo do primeiro exemplo:"),Cs=p(),g(Re.$$.fragment),zs=p(),g(Ne.$$.fragment),Is=p(),xa=n("p"),ut=o("Ok, isso parece o resumo de um artigo m\xE9dico. Agora vamos ver quanta RAM usamos para carregar o conjunto de dados!"),Rs=p(),te=n("h2"),pe=n("a"),Sa=n("span"),g(Se.$$.fragment),ft=p(),Ma=n("span"),ht=o("A magia do mapeamento de mem\xF3ria"),Ns=p(),K=n("p"),gt=o("Uma maneira simples de medir o uso de mem\xF3ria em Python \xE9 com a biblioteca "),Me=n("a"),Ga=n("code"),bt=o("psutil"),_t=o(", que pode ser instalada com "),Ba=n("code"),$t=o("pip"),jt=o(" da seguinte forma:"),Ss=p(),g(Ge.$$.fragment),Ms=p(),ce=n("p"),xt=o("Ele fornece uma classe "),La=n("code"),vt=o("Process"),Et=o(" que nos permite verificar o uso de mem\xF3ria do processo atual da seguinte forma:"),Gs=p(),g(Be.$$.fragment),Bs=p(),g(Le.$$.fragment),Ls=p(),F=n("p"),wt=o("Aqui o atributo "),Ha=n("code"),yt=o("rss"),qt=o(" refere-se ao "),Ua=n("em"),At=o("tamanho do conjunto residente"),Pt=o(", que \xE9 a fra\xE7\xE3o de mem\xF3ria que um processo ocupa na RAM. Essa medida tamb\xE9m inclui a mem\xF3ria usada pelo interpretador Python e as bibliotecas que carregamos, portanto, a quantidade real de mem\xF3ria usada para carregar o conjunto de dados \xE9 um pouco menor. Para compara\xE7\xE3o, vamos ver o tamanho do conjunto de dados no disco, usando o atributo "),Fa=n("code"),Dt=o("dataset_size"),kt=o(". Como o resultado \xE9 expresso em bytes como antes, precisamos convert\xEA-lo manualmente para gigabytes:"),Hs=p(),g(He.$$.fragment),Us=p(),g(Ue.$$.fragment),Fs=p(),va=n("p"),Ot=o("Legal \u2014 apesar de ter quase 20 GB de tamanho, podemos carregar e acessar o conjunto de dados com muito menos RAM!"),Vs=p(),g(ue.$$.fragment),Js=p(),Y=n("p"),Tt=o("Se voc\xEA estiver familiarizado com Pandas, esse resultado pode ser uma surpresa por causa da famosa [regra de ouro] de Wes Kinney ("),Fe=n("a"),Ct=o("https://wesmckinney.com/blog/apache-arrow-pandas-internals/"),zt=o(") de que voc\xEA normalmente precisa de 5 para 10 vezes mais RAM do que o tamanho do seu conjunto de dados. Ent\xE3o, como \u{1F917} Datasets resolve esse problema de gerenciamento de mem\xF3ria? \u{1F917} Os conjuntos de dados tratam cada conjunto de dados como um "),Ve=n("a"),It=o("arquivo mapeado em mem\xF3ria"),Rt=o(", que fornece um mapeamento entre RAM e armazenamento do sistema de arquivos que permite que a biblioteca acesse e opere em elementos do conjunto de dados sem precisar carreg\xE1-lo totalmente na mem\xF3ria."),Ws=p(),B=n("p"),Nt=o("Arquivos mapeados em mem\xF3ria tamb\xE9m podem ser compartilhados em v\xE1rios processos, o que permite que m\xE9todos como "),Va=n("code"),St=o("Dataset.map()"),Mt=o(" sejam paralelizados sem a necessidade de mover ou copiar o conjunto de dados. Sob o cap\xF4, esses recursos s\xE3o todos realizados pelo formato de mem\xF3ria "),Je=n("a"),Gt=o("Apache Arrow"),Bt=o(" e "),We=n("a"),Ja=n("code"),Lt=o("pyarrow"),Ht=o(", que tornam o carregamento e o processamento de dados extremamente r\xE1pidos. (Para mais detalhes sobre o Apache Arrow e compara\xE7\xF5es com o Pandas, confira "),Ke=n("a"),Ut=o("post do blog de Dejan Simic"),Ft=o(".) Para ver isso em a\xE7\xE3o, vamos executar um pequeno teste de velocidade iterando sobre todos os elementos no conjunto de dados PubMed Abstracts:"),Ks=p(),g(Ye.$$.fragment),Ys=p(),g(Ze.$$.fragment),Zs=p(),Z=n("p"),Vt=o("Aqui usamos o m\xF3dulo "),Wa=n("code"),Jt=o("timeit"),Wt=o(" do Python para medir o tempo de execu\xE7\xE3o do "),Ka=n("code"),Kt=o("code_snippet"),Yt=o(". Normalmente, voc\xEA poder\xE1 iterar em um conjunto de dados a uma velocidade de alguns d\xE9cimos de GB/s a v\xE1rios GB/s. Isso funciona muito bem para a grande maioria dos aplicativos, mas \xE0s vezes voc\xEA ter\xE1 que trabalhar com um conjunto de dados grande demais para ser armazenado no disco r\xEDgido do seu laptop. Por exemplo, se tent\xE1ssemos baixar o Pile por completo, precisar\xEDamos de 825 GB de espa\xE7o livre em disco! Para lidar com esses casos, \u{1F917} Datasets fornece um recurso de streaming que nos permite baixar e acessar elementos em tempo real, sem a necessidade de baixar todo o conjunto de dados. Vamos dar uma olhada em como isso funciona."),Qs=p(),g(fe.$$.fragment),Xs=p(),re=n("h2"),he=n("a"),Ya=n("span"),g(Qe.$$.fragment),Zt=p(),Za=n("span"),Qt=o("Conjuntos de dados em streaming"),eo=p(),Q=n("p"),Xt=o("Para habilitar o streaming do conjunto de dados voc\xEA s\xF3 precisa passar o argumento "),Qa=n("code"),er=o("streaming=True"),ar=o(" para a fun\xE7\xE3o "),Xa=n("code"),sr=o("load_dataset()"),or=o(". Por exemplo, vamos carregar o conjunto de dados PubMed Abstracts novamente, mas em modo streaming:"),ao=p(),g(Xe.$$.fragment),so=p(),L=n("p"),tr=o("Em vez do familiar "),es=n("code"),rr=o("Dataset"),nr=o(" que encontramos em outro lugar neste cap\xEDtulo, o objeto retornado com "),as=n("code"),ir=o("streaming=True"),lr=o(" \xE9 um "),ss=n("code"),dr=o("IterableDataset"),mr=o(". Como o nome sugere, para acessar os elementos de um "),os=n("code"),pr=o("IterableDataset"),cr=o(" precisamos iterar sobre ele. Podemos acessar o primeiro elemento do nosso conjunto de dados transmitido da seguinte forma:"),oo=p(),g(ea.$$.fragment),to=p(),g(aa.$$.fragment),ro=p(),X=n("p"),ur=o("Os elementos de um conjunto de dados transmitido podem ser processados dinamicamente usando "),ts=n("code"),fr=o("IterableDataset.map()"),hr=o(", o que \xE9 \xFAtil durante o treinamento se voc\xEA precisar tokenizar as entradas. O processo \xE9 exatamente o mesmo que usamos para tokenizar nosso conjunto de dados no "),Ea=n("a"),gr=o("Cap\xEDtulo 3"),br=o(", com a \xFAnica diferen\xE7a de que as sa\xEDdas s\xE3o retornadas uma a uma:"),no=p(),g(sa.$$.fragment),io=p(),g(oa.$$.fragment),lo=p(),g(ge.$$.fragment),mo=p(),V=n("p"),_r=o("Voc\xEA tamb\xE9m pode embaralhar um conjunto de dados transmitido usando "),rs=n("code"),$r=o("IterableDataset.shuffle()"),jr=o(", mas, diferentemente de "),ns=n("code"),xr=o("Dataset.shuffle()"),vr=o(", isso apenas embaralha os elementos em um "),is=n("code"),Er=o("buffer_size"),wr=o(" predefinido:"),po=p(),g(ta.$$.fragment),co=p(),g(ra.$$.fragment),uo=p(),J=n("p"),yr=o("Neste exemplo, selecionamos um exemplo aleat\xF3rio dos primeiros 10.000 exemplos no buffer. Uma vez que um exemplo \xE9 acessado, seu lugar no buffer \xE9 preenchido com o pr\xF3ximo exemplo no corpus (ou seja, o 10.001\xBA exemplo no caso acima). Voc\xEA tamb\xE9m pode selecionar elementos de um conjunto de dados transmitido usando as fun\xE7\xF5es "),ls=n("code"),qr=o("IterableDataset.take()"),Ar=o(" e "),ds=n("code"),Pr=o("IterableDataset.skip()"),Dr=o(", que agem de maneira semelhante a "),ms=n("code"),kr=o("Dataset.select()"),Or=o(". Por exemplo, para selecionar os primeiros 5 exemplos no conjunto de dados PubMed Abstracts, podemos fazer o seguinte:"),fo=p(),g(na.$$.fragment),ho=p(),g(ia.$$.fragment),go=p(),be=n("p"),Tr=o("Da mesma forma, voc\xEA pode usar a fun\xE7\xE3o "),ps=n("code"),Cr=o("IterableDataset.skip()"),zr=o(" para criar divis\xF5es de treinamento e valida\xE7\xE3o de um conjunto de dados embaralhado da seguinte forma:"),bo=p(),g(la.$$.fragment),_o=p(),W=n("p"),Ir=o("Vamos completar nossa explora\xE7\xE3o de streaming de conjuntos de dados com um aplicativo comum: combinar v\xE1rios conjuntos de dados para criar um \xFAnico corpus. \u{1F917} Datasets fornece uma fun\xE7\xE3o "),cs=n("code"),Rr=o("interleave_datasets()"),Nr=o(" que converte uma lista de objetos "),us=n("code"),Sr=o("IterableDataset"),Mr=o(" em um \xFAnico "),fs=n("code"),Gr=o("IterableDataset"),Br=o(", onde os elementos do novo conjunto de dados s\xE3o obtidos alternando entre os exemplos de origem. Essa fun\xE7\xE3o \xE9 especialmente \xFAtil quando voc\xEA est\xE1 tentando combinar grandes conjuntos de dados, ent\xE3o, como exemplo, vamos transmitir o subconjunto FreeLaw do Pile, que \xE9 um conjunto de dados de 51 GB de pareceres jur\xEDdicos dos tribunais dos EUA:"),$o=p(),g(da.$$.fragment),jo=p(),g(ma.$$.fragment),xo=p(),_e=n("p"),Lr=o("Esse conjunto de dados \xE9 grande o suficiente para sobrecarregar a RAM da maioria dos laptops, mas conseguimos carreg\xE1-lo e acess\xE1-lo sem suar a camisa! Vamos agora combinar os exemplos dos conjuntos de dados FreeLaw e PubMed Abstracts com a fun\xE7\xE3o "),hs=n("code"),Hr=o("interleave_datasets()"),Ur=o(":"),vo=p(),g(pa.$$.fragment),Eo=p(),g(ca.$$.fragment),wo=p(),ee=n("p"),Fr=o("Aqui usamos a fun\xE7\xE3o "),gs=n("code"),Vr=o("islice()"),Jr=o(" do m\xF3dulo "),bs=n("code"),Wr=o("itertools"),Kr=o(" do Python para selecionar os dois primeiros exemplos do conjunto de dados combinado e podemos ver que eles correspondem aos primeiros exemplos de cada um dos dois conjuntos de dados de origem."),yo=p(),wa=n("p"),Yr=o("Por fim, se voc\xEA quiser transmitir o Pile em sua totalidade de 825 GB, poder\xE1 pegar todos os arquivos preparados da seguinte maneira:"),qo=p(),g(ua.$$.fragment),Ao=p(),g(fa.$$.fragment),Po=p(),g($e.$$.fragment),Do=p(),ya=n("p"),Zr=o("Agora voc\xEA tem todas as ferramentas necess\xE1rias para carregar e processar conjuntos de dados de todas as formas e tamanhos, mas, a menos que tenha muita sorte, chegar\xE1 um ponto em sua jornada de PNL em que voc\xEA ter\xE1 que criar um conjunto de dados para resolver o problema. problema em m\xE3os. Esse \xE9 o tema da pr\xF3xima se\xE7\xE3o!"),this.h()},l(e){const r=ci('[data-svelte="svelte-1phssyn"]',document.head);m=i(r,"META",{name:!0,content:!0}),r.forEach(s),T=c(e),f=i(e,"H1",{class:!0});var ha=l(f);w=i(ha,"A",{id:!0,class:!0,href:!0});var _s=l(w);k=i(_s,"SPAN",{});var $s=l(k);b(h.$$.fragment,$s),$s.forEach(s),_s.forEach(s),O=c(ha),A=i(ha,"SPAN",{});var js=l(A);E=t(js,"Big data? \u{1F917} Datasets ao resgate"),js.forEach(s),ha.forEach(s),v=c(e),b(P.$$.fragment,e),y=c(e),I=i(e,"P",{});var ga=l(I);N=t(ga,"Hoje em dia, n\xE3o \xE9 incomum encontrar-se trabalhando com conjuntos de dados de v\xE1rios gigabytes, especialmente se voc\xEA planeja pr\xE9-treinar um transformer como BERT ou GPT-2 do zero. Nesses casos, at\xE9 mesmo "),D=i(ga,"EM",{});var Qr=l(D);C=t(Qr,"carregar"),Qr.forEach(s),G=t(ga," os dados pode ser um desafio. Por exemplo, o corpus WebText usado para pr\xE9-treinar o GPT-2 consiste em mais de 8 milh\xF5es de documentos e 40 GB de texto - carregar isso na RAM do seu laptop provavelmente lhe causar\xE1 um ataque card\xEDaco!"),ga.forEach(s),z=c(e),S=i(e,"P",{});var qa=l(S);U=t(qa,"Felizmente, \u{1F917} Datasets foram projetados para superar essas limita\xE7\xF5es. Ele libera voc\xEA de problemas de gerenciamento de mem\xF3ria tratando conjuntos de dados como arquivos "),ne=i(qa,"EM",{});var Xr=l(ne);we=t(Xr,"memory-mapped"),Xr.forEach(s),ba=t(qa," e de limites de disco r\xEDgido por "),Ca=i(qa,"EM",{});var en=l(Ca);Go=t(en,"streaming"),en.forEach(s),Bo=t(qa," das entradas em um corpus."),qa.forEach(s),vs=c(e),b(ye.$$.fragment,e),Es=c(e),ie=i(e,"P",{});var Oo=l(ie);Lo=t(Oo,"Nesta se\xE7\xE3o, exploraremos esses recursos de \u{1F917} Conjuntos de dados com um enorme corpus de 825 GB conhecido como "),qe=i(Oo,"A",{href:!0,rel:!0});var an=l(qe);Ho=t(an,"the Pile"),an.forEach(s),Uo=t(Oo,". Vamos come\xE7ar!"),Oo.forEach(s),ws=c(e),oe=i(e,"H2",{class:!0});var To=l(oe);le=i(To,"A",{id:!0,class:!0,href:!0});var sn=l(le);za=i(sn,"SPAN",{});var on=l(za);b(Ae.$$.fragment,on),on.forEach(s),sn.forEach(s),Fo=c(To),Ia=i(To,"SPAN",{});var tn=l(Ia);Vo=t(tn,"O que \xE9 the Pile?"),tn.forEach(s),To.forEach(s),ys=c(e),R=i(e,"P",{});var M=l(R);Jo=t(M,"O "),Ra=i(M,"CODE",{});var rn=l(Ra);Wo=t(rn,"The Pile"),rn.forEach(s),Ko=t(M," \xE9 um corpus de texto em ingl\xEAs que foi criado por "),Pe=i(M,"A",{href:!0,rel:!0});var nn=l(Pe);Yo=t(nn,"EleutherAI"),nn.forEach(s),Zo=t(M," para treinar modelos de linguagem em larga escala. Ele inclui uma gama diversificada de conjuntos de dados, abrangendo artigos cient\xEDficos, reposit\xF3rios de c\xF3digo do GitHub e texto da web filtrado. O corpus de treinamento est\xE1 dispon\xEDvel em "),De=i(M,"A",{href:!0,rel:!0});var ln=l(De);Qo=t(ln,"blocos de 14 GB"),ln.forEach(s),Xo=t(M,", e voc\xEA tamb\xE9m pode baixar v\xE1rios dos "),ke=i(M,"A",{href:!0,rel:!0});var dn=l(ke);et=t(dn,"componentes individuais"),dn.forEach(s),at=t(M,". Vamos come\xE7ar dando uma olhada no conjunto de dados PubMed Abstracts, que \xE9 um corpus de resumos de 15 milh\xF5es de publica\xE7\xF5es biom\xE9dicas no "),Oe=i(M,"A",{href:!0,rel:!0});var mn=l(Oe);st=t(mn,"PubMed"),mn.forEach(s),ot=t(M,". O conjunto de dados est\xE1 em "),Te=i(M,"A",{href:!0,rel:!0});var pn=l(Te);tt=t(pn,"formato JSON Lines"),pn.forEach(s),rt=t(M," e \xE9 compactado usando a biblioteca "),Na=i(M,"CODE",{});var cn=l(Na);nt=t(cn,"zstandard"),cn.forEach(s),it=t(M,", ent\xE3o primeiro precisamos instal\xE1-lo:"),M.forEach(s),qs=c(e),b(Ce.$$.fragment,e),As=c(e),de=i(e,"P",{});var Co=l(de);lt=t(Co,"Em seguida, podemos carregar o conjunto de dados usando o m\xE9todo para arquivos remotos que aprendemos na "),_a=i(Co,"A",{href:!0});var un=l(_a);dt=t(un,"se\xE7\xE3o 2"),un.forEach(s),mt=t(Co,":"),Co.forEach(s),Ps=c(e),b(ze.$$.fragment,e),Ds=c(e),b(Ie.$$.fragment,e),ks=c(e),$a=i(e,"P",{});var fn=l($a);pt=t(fn,"Podemos ver que h\xE1 15.518.009 linhas e 2 colunas em nosso conjunto de dados - isso \xE9 muito!"),fn.forEach(s),Os=c(e),b(me.$$.fragment,e),Ts=c(e),ja=i(e,"P",{});var hn=l(ja);ct=t(hn,"Vamos inspecionar o conte\xFAdo do primeiro exemplo:"),hn.forEach(s),Cs=c(e),b(Re.$$.fragment,e),zs=c(e),b(Ne.$$.fragment,e),Is=c(e),xa=i(e,"P",{});var gn=l(xa);ut=t(gn,"Ok, isso parece o resumo de um artigo m\xE9dico. Agora vamos ver quanta RAM usamos para carregar o conjunto de dados!"),gn.forEach(s),Rs=c(e),te=i(e,"H2",{class:!0});var zo=l(te);pe=i(zo,"A",{id:!0,class:!0,href:!0});var bn=l(pe);Sa=i(bn,"SPAN",{});var _n=l(Sa);b(Se.$$.fragment,_n),_n.forEach(s),bn.forEach(s),ft=c(zo),Ma=i(zo,"SPAN",{});var $n=l(Ma);ht=t($n,"A magia do mapeamento de mem\xF3ria"),$n.forEach(s),zo.forEach(s),Ns=c(e),K=i(e,"P",{});var Aa=l(K);gt=t(Aa,"Uma maneira simples de medir o uso de mem\xF3ria em Python \xE9 com a biblioteca "),Me=i(Aa,"A",{href:!0,rel:!0});var jn=l(Me);Ga=i(jn,"CODE",{});var xn=l(Ga);bt=t(xn,"psutil"),xn.forEach(s),jn.forEach(s),_t=t(Aa,", que pode ser instalada com "),Ba=i(Aa,"CODE",{});var vn=l(Ba);$t=t(vn,"pip"),vn.forEach(s),jt=t(Aa," da seguinte forma:"),Aa.forEach(s),Ss=c(e),b(Ge.$$.fragment,e),Ms=c(e),ce=i(e,"P",{});var Io=l(ce);xt=t(Io,"Ele fornece uma classe "),La=i(Io,"CODE",{});var En=l(La);vt=t(En,"Process"),En.forEach(s),Et=t(Io," que nos permite verificar o uso de mem\xF3ria do processo atual da seguinte forma:"),Io.forEach(s),Gs=c(e),b(Be.$$.fragment,e),Bs=c(e),b(Le.$$.fragment,e),Ls=c(e),F=i(e,"P",{});var je=l(F);wt=t(je,"Aqui o atributo "),Ha=i(je,"CODE",{});var wn=l(Ha);yt=t(wn,"rss"),wn.forEach(s),qt=t(je," refere-se ao "),Ua=i(je,"EM",{});var yn=l(Ua);At=t(yn,"tamanho do conjunto residente"),yn.forEach(s),Pt=t(je,", que \xE9 a fra\xE7\xE3o de mem\xF3ria que um processo ocupa na RAM. Essa medida tamb\xE9m inclui a mem\xF3ria usada pelo interpretador Python e as bibliotecas que carregamos, portanto, a quantidade real de mem\xF3ria usada para carregar o conjunto de dados \xE9 um pouco menor. Para compara\xE7\xE3o, vamos ver o tamanho do conjunto de dados no disco, usando o atributo "),Fa=i(je,"CODE",{});var qn=l(Fa);Dt=t(qn,"dataset_size"),qn.forEach(s),kt=t(je,". Como o resultado \xE9 expresso em bytes como antes, precisamos convert\xEA-lo manualmente para gigabytes:"),je.forEach(s),Hs=c(e),b(He.$$.fragment,e),Us=c(e),b(Ue.$$.fragment,e),Fs=c(e),va=i(e,"P",{});var An=l(va);Ot=t(An,"Legal \u2014 apesar de ter quase 20 GB de tamanho, podemos carregar e acessar o conjunto de dados com muito menos RAM!"),An.forEach(s),Vs=c(e),b(ue.$$.fragment,e),Js=c(e),Y=i(e,"P",{});var Pa=l(Y);Tt=t(Pa,"Se voc\xEA estiver familiarizado com Pandas, esse resultado pode ser uma surpresa por causa da famosa [regra de ouro] de Wes Kinney ("),Fe=i(Pa,"A",{href:!0,rel:!0});var Pn=l(Fe);Ct=t(Pn,"https://wesmckinney.com/blog/apache-arrow-pandas-internals/"),Pn.forEach(s),zt=t(Pa,") de que voc\xEA normalmente precisa de 5 para 10 vezes mais RAM do que o tamanho do seu conjunto de dados. Ent\xE3o, como \u{1F917} Datasets resolve esse problema de gerenciamento de mem\xF3ria? \u{1F917} Os conjuntos de dados tratam cada conjunto de dados como um "),Ve=i(Pa,"A",{href:!0,rel:!0});var Dn=l(Ve);It=t(Dn,"arquivo mapeado em mem\xF3ria"),Dn.forEach(s),Rt=t(Pa,", que fornece um mapeamento entre RAM e armazenamento do sistema de arquivos que permite que a biblioteca acesse e opere em elementos do conjunto de dados sem precisar carreg\xE1-lo totalmente na mem\xF3ria."),Pa.forEach(s),Ws=c(e),B=i(e,"P",{});var ae=l(B);Nt=t(ae,"Arquivos mapeados em mem\xF3ria tamb\xE9m podem ser compartilhados em v\xE1rios processos, o que permite que m\xE9todos como "),Va=i(ae,"CODE",{});var kn=l(Va);St=t(kn,"Dataset.map()"),kn.forEach(s),Mt=t(ae," sejam paralelizados sem a necessidade de mover ou copiar o conjunto de dados. Sob o cap\xF4, esses recursos s\xE3o todos realizados pelo formato de mem\xF3ria "),Je=i(ae,"A",{href:!0,rel:!0});var On=l(Je);Gt=t(On,"Apache Arrow"),On.forEach(s),Bt=t(ae," e "),We=i(ae,"A",{href:!0,rel:!0});var Tn=l(We);Ja=i(Tn,"CODE",{});var Cn=l(Ja);Lt=t(Cn,"pyarrow"),Cn.forEach(s),Tn.forEach(s),Ht=t(ae,", que tornam o carregamento e o processamento de dados extremamente r\xE1pidos. (Para mais detalhes sobre o Apache Arrow e compara\xE7\xF5es com o Pandas, confira "),Ke=i(ae,"A",{href:!0,rel:!0});var zn=l(Ke);Ut=t(zn,"post do blog de Dejan Simic"),zn.forEach(s),Ft=t(ae,".) Para ver isso em a\xE7\xE3o, vamos executar um pequeno teste de velocidade iterando sobre todos os elementos no conjunto de dados PubMed Abstracts:"),ae.forEach(s),Ks=c(e),b(Ye.$$.fragment,e),Ys=c(e),b(Ze.$$.fragment,e),Zs=c(e),Z=i(e,"P",{});var Da=l(Z);Vt=t(Da,"Aqui usamos o m\xF3dulo "),Wa=i(Da,"CODE",{});var In=l(Wa);Jt=t(In,"timeit"),In.forEach(s),Wt=t(Da," do Python para medir o tempo de execu\xE7\xE3o do "),Ka=i(Da,"CODE",{});var Rn=l(Ka);Kt=t(Rn,"code_snippet"),Rn.forEach(s),Yt=t(Da,". Normalmente, voc\xEA poder\xE1 iterar em um conjunto de dados a uma velocidade de alguns d\xE9cimos de GB/s a v\xE1rios GB/s. Isso funciona muito bem para a grande maioria dos aplicativos, mas \xE0s vezes voc\xEA ter\xE1 que trabalhar com um conjunto de dados grande demais para ser armazenado no disco r\xEDgido do seu laptop. Por exemplo, se tent\xE1ssemos baixar o Pile por completo, precisar\xEDamos de 825 GB de espa\xE7o livre em disco! Para lidar com esses casos, \u{1F917} Datasets fornece um recurso de streaming que nos permite baixar e acessar elementos em tempo real, sem a necessidade de baixar todo o conjunto de dados. Vamos dar uma olhada em como isso funciona."),Da.forEach(s),Qs=c(e),b(fe.$$.fragment,e),Xs=c(e),re=i(e,"H2",{class:!0});var Ro=l(re);he=i(Ro,"A",{id:!0,class:!0,href:!0});var Nn=l(he);Ya=i(Nn,"SPAN",{});var Sn=l(Ya);b(Qe.$$.fragment,Sn),Sn.forEach(s),Nn.forEach(s),Zt=c(Ro),Za=i(Ro,"SPAN",{});var Mn=l(Za);Qt=t(Mn,"Conjuntos de dados em streaming"),Mn.forEach(s),Ro.forEach(s),eo=c(e),Q=i(e,"P",{});var ka=l(Q);Xt=t(ka,"Para habilitar o streaming do conjunto de dados voc\xEA s\xF3 precisa passar o argumento "),Qa=i(ka,"CODE",{});var Gn=l(Qa);er=t(Gn,"streaming=True"),Gn.forEach(s),ar=t(ka," para a fun\xE7\xE3o "),Xa=i(ka,"CODE",{});var Bn=l(Xa);sr=t(Bn,"load_dataset()"),Bn.forEach(s),or=t(ka,". Por exemplo, vamos carregar o conjunto de dados PubMed Abstracts novamente, mas em modo streaming:"),ka.forEach(s),ao=c(e),b(Xe.$$.fragment,e),so=c(e),L=i(e,"P",{});var se=l(L);tr=t(se,"Em vez do familiar "),es=i(se,"CODE",{});var Ln=l(es);rr=t(Ln,"Dataset"),Ln.forEach(s),nr=t(se," que encontramos em outro lugar neste cap\xEDtulo, o objeto retornado com "),as=i(se,"CODE",{});var Hn=l(as);ir=t(Hn,"streaming=True"),Hn.forEach(s),lr=t(se," \xE9 um "),ss=i(se,"CODE",{});var Un=l(ss);dr=t(Un,"IterableDataset"),Un.forEach(s),mr=t(se,". Como o nome sugere, para acessar os elementos de um "),os=i(se,"CODE",{});var Fn=l(os);pr=t(Fn,"IterableDataset"),Fn.forEach(s),cr=t(se," precisamos iterar sobre ele. Podemos acessar o primeiro elemento do nosso conjunto de dados transmitido da seguinte forma:"),se.forEach(s),oo=c(e),b(ea.$$.fragment,e),to=c(e),b(aa.$$.fragment,e),ro=c(e),X=i(e,"P",{});var Oa=l(X);ur=t(Oa,"Os elementos de um conjunto de dados transmitido podem ser processados dinamicamente usando "),ts=i(Oa,"CODE",{});var Vn=l(ts);fr=t(Vn,"IterableDataset.map()"),Vn.forEach(s),hr=t(Oa,", o que \xE9 \xFAtil durante o treinamento se voc\xEA precisar tokenizar as entradas. O processo \xE9 exatamente o mesmo que usamos para tokenizar nosso conjunto de dados no "),Ea=i(Oa,"A",{href:!0});var Jn=l(Ea);gr=t(Jn,"Cap\xEDtulo 3"),Jn.forEach(s),br=t(Oa,", com a \xFAnica diferen\xE7a de que as sa\xEDdas s\xE3o retornadas uma a uma:"),Oa.forEach(s),no=c(e),b(sa.$$.fragment,e),io=c(e),b(oa.$$.fragment,e),lo=c(e),b(ge.$$.fragment,e),mo=c(e),V=i(e,"P",{});var xe=l(V);_r=t(xe,"Voc\xEA tamb\xE9m pode embaralhar um conjunto de dados transmitido usando "),rs=i(xe,"CODE",{});var Wn=l(rs);$r=t(Wn,"IterableDataset.shuffle()"),Wn.forEach(s),jr=t(xe,", mas, diferentemente de "),ns=i(xe,"CODE",{});var Kn=l(ns);xr=t(Kn,"Dataset.shuffle()"),Kn.forEach(s),vr=t(xe,", isso apenas embaralha os elementos em um "),is=i(xe,"CODE",{});var Yn=l(is);Er=t(Yn,"buffer_size"),Yn.forEach(s),wr=t(xe," predefinido:"),xe.forEach(s),po=c(e),b(ta.$$.fragment,e),co=c(e),b(ra.$$.fragment,e),uo=c(e),J=i(e,"P",{});var ve=l(J);yr=t(ve,"Neste exemplo, selecionamos um exemplo aleat\xF3rio dos primeiros 10.000 exemplos no buffer. Uma vez que um exemplo \xE9 acessado, seu lugar no buffer \xE9 preenchido com o pr\xF3ximo exemplo no corpus (ou seja, o 10.001\xBA exemplo no caso acima). Voc\xEA tamb\xE9m pode selecionar elementos de um conjunto de dados transmitido usando as fun\xE7\xF5es "),ls=i(ve,"CODE",{});var Zn=l(ls);qr=t(Zn,"IterableDataset.take()"),Zn.forEach(s),Ar=t(ve," e "),ds=i(ve,"CODE",{});var Qn=l(ds);Pr=t(Qn,"IterableDataset.skip()"),Qn.forEach(s),Dr=t(ve,", que agem de maneira semelhante a "),ms=i(ve,"CODE",{});var Xn=l(ms);kr=t(Xn,"Dataset.select()"),Xn.forEach(s),Or=t(ve,". Por exemplo, para selecionar os primeiros 5 exemplos no conjunto de dados PubMed Abstracts, podemos fazer o seguinte:"),ve.forEach(s),fo=c(e),b(na.$$.fragment,e),ho=c(e),b(ia.$$.fragment,e),go=c(e),be=i(e,"P",{});var No=l(be);Tr=t(No,"Da mesma forma, voc\xEA pode usar a fun\xE7\xE3o "),ps=i(No,"CODE",{});var ei=l(ps);Cr=t(ei,"IterableDataset.skip()"),ei.forEach(s),zr=t(No," para criar divis\xF5es de treinamento e valida\xE7\xE3o de um conjunto de dados embaralhado da seguinte forma:"),No.forEach(s),bo=c(e),b(la.$$.fragment,e),_o=c(e),W=i(e,"P",{});var Ee=l(W);Ir=t(Ee,"Vamos completar nossa explora\xE7\xE3o de streaming de conjuntos de dados com um aplicativo comum: combinar v\xE1rios conjuntos de dados para criar um \xFAnico corpus. \u{1F917} Datasets fornece uma fun\xE7\xE3o "),cs=i(Ee,"CODE",{});var ai=l(cs);Rr=t(ai,"interleave_datasets()"),ai.forEach(s),Nr=t(Ee," que converte uma lista de objetos "),us=i(Ee,"CODE",{});var si=l(us);Sr=t(si,"IterableDataset"),si.forEach(s),Mr=t(Ee," em um \xFAnico "),fs=i(Ee,"CODE",{});var oi=l(fs);Gr=t(oi,"IterableDataset"),oi.forEach(s),Br=t(Ee,", onde os elementos do novo conjunto de dados s\xE3o obtidos alternando entre os exemplos de origem. Essa fun\xE7\xE3o \xE9 especialmente \xFAtil quando voc\xEA est\xE1 tentando combinar grandes conjuntos de dados, ent\xE3o, como exemplo, vamos transmitir o subconjunto FreeLaw do Pile, que \xE9 um conjunto de dados de 51 GB de pareceres jur\xEDdicos dos tribunais dos EUA:"),Ee.forEach(s),$o=c(e),b(da.$$.fragment,e),jo=c(e),b(ma.$$.fragment,e),xo=c(e),_e=i(e,"P",{});var So=l(_e);Lr=t(So,"Esse conjunto de dados \xE9 grande o suficiente para sobrecarregar a RAM da maioria dos laptops, mas conseguimos carreg\xE1-lo e acess\xE1-lo sem suar a camisa! Vamos agora combinar os exemplos dos conjuntos de dados FreeLaw e PubMed Abstracts com a fun\xE7\xE3o "),hs=i(So,"CODE",{});var ti=l(hs);Hr=t(ti,"interleave_datasets()"),ti.forEach(s),Ur=t(So,":"),So.forEach(s),vo=c(e),b(pa.$$.fragment,e),Eo=c(e),b(ca.$$.fragment,e),wo=c(e),ee=i(e,"P",{});var Ta=l(ee);Fr=t(Ta,"Aqui usamos a fun\xE7\xE3o "),gs=i(Ta,"CODE",{});var ri=l(gs);Vr=t(ri,"islice()"),ri.forEach(s),Jr=t(Ta," do m\xF3dulo "),bs=i(Ta,"CODE",{});var ni=l(bs);Wr=t(ni,"itertools"),ni.forEach(s),Kr=t(Ta," do Python para selecionar os dois primeiros exemplos do conjunto de dados combinado e podemos ver que eles correspondem aos primeiros exemplos de cada um dos dois conjuntos de dados de origem."),Ta.forEach(s),yo=c(e),wa=i(e,"P",{});var ii=l(wa);Yr=t(ii,"Por fim, se voc\xEA quiser transmitir o Pile em sua totalidade de 825 GB, poder\xE1 pegar todos os arquivos preparados da seguinte maneira:"),ii.forEach(s),qo=c(e),b(ua.$$.fragment,e),Ao=c(e),b(fa.$$.fragment,e),Po=c(e),b($e.$$.fragment,e),Do=c(e),ya=i(e,"P",{});var li=l(ya);Zr=t(li,"Agora voc\xEA tem todas as ferramentas necess\xE1rias para carregar e processar conjuntos de dados de todas as formas e tamanhos, mas, a menos que tenha muita sorte, chegar\xE1 um ponto em sua jornada de PNL em que voc\xEA ter\xE1 que criar um conjunto de dados para resolver o problema. problema em m\xE3os. Esse \xE9 o tema da pr\xF3xima se\xE7\xE3o!"),li.forEach(s),this.h()},h(){u(m,"name","hf:doc:metadata"),u(m,"content",JSON.stringify(vi)),u(w,"id","big-data-datasets-ao-resgate"),u(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(w,"href","#big-data-datasets-ao-resgate"),u(f,"class","relative group"),u(qe,"href","https://pile.eleuther.ai"),u(qe,"rel","nofollow"),u(le,"id","o-que-the-pile"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#o-que-the-pile"),u(oe,"class","relative group"),u(Pe,"href","https://www.eleuther.ai"),u(Pe,"rel","nofollow"),u(De,"href","https://mystic.the-eye.eu/public/AI/pile/"),u(De,"rel","nofollow"),u(ke,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(ke,"rel","nofollow"),u(Oe,"href","https://pubmed.ncbi.nlm.nih.gov/"),u(Oe,"rel","nofollow"),u(Te,"href","https://jsonlines.org"),u(Te,"rel","nofollow"),u(_a,"href","/course/chapter5/2"),u(pe,"id","a-magia-do-mapeamento-de-memria"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#a-magia-do-mapeamento-de-memria"),u(te,"class","relative group"),u(Me,"href","https://psutil.readthedocs.io/en/latest/"),u(Me,"rel","nofollow"),u(Fe,"href","https://wesmckinney.com/blog/apache-arrow-pandas-internals/"),u(Fe,"rel","nofollow"),u(Ve,"href","https://en.wikipedia.org/wiki/Memory-mapped_file"),u(Ve,"rel","nofollow"),u(Je,"href","https://arrow.apache.org"),u(Je,"rel","nofollow"),u(We,"href","https://arrow.apache.org/docs/python/index.html"),u(We,"rel","nofollow"),u(Ke,"href","https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a"),u(Ke,"rel","nofollow"),u(he,"id","conjuntos-de-dados-em-streaming"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#conjuntos-de-dados-em-streaming"),u(re,"class","relative group"),u(Ea,"href","/course/chapter3")},m(e,r){a(document.head,m),d(e,T,r),d(e,f,r),a(f,w),a(w,k),_(h,k,null),a(f,O),a(f,A),a(A,E),d(e,v,r),_(P,e,r),d(e,y,r),d(e,I,r),a(I,N),a(I,D),a(D,C),a(I,G),d(e,z,r),d(e,S,r),a(S,U),a(S,ne),a(ne,we),a(S,ba),a(S,Ca),a(Ca,Go),a(S,Bo),d(e,vs,r),_(ye,e,r),d(e,Es,r),d(e,ie,r),a(ie,Lo),a(ie,qe),a(qe,Ho),a(ie,Uo),d(e,ws,r),d(e,oe,r),a(oe,le),a(le,za),_(Ae,za,null),a(oe,Fo),a(oe,Ia),a(Ia,Vo),d(e,ys,r),d(e,R,r),a(R,Jo),a(R,Ra),a(Ra,Wo),a(R,Ko),a(R,Pe),a(Pe,Yo),a(R,Zo),a(R,De),a(De,Qo),a(R,Xo),a(R,ke),a(ke,et),a(R,at),a(R,Oe),a(Oe,st),a(R,ot),a(R,Te),a(Te,tt),a(R,rt),a(R,Na),a(Na,nt),a(R,it),d(e,qs,r),_(Ce,e,r),d(e,As,r),d(e,de,r),a(de,lt),a(de,_a),a(_a,dt),a(de,mt),d(e,Ps,r),_(ze,e,r),d(e,Ds,r),_(Ie,e,r),d(e,ks,r),d(e,$a,r),a($a,pt),d(e,Os,r),_(me,e,r),d(e,Ts,r),d(e,ja,r),a(ja,ct),d(e,Cs,r),_(Re,e,r),d(e,zs,r),_(Ne,e,r),d(e,Is,r),d(e,xa,r),a(xa,ut),d(e,Rs,r),d(e,te,r),a(te,pe),a(pe,Sa),_(Se,Sa,null),a(te,ft),a(te,Ma),a(Ma,ht),d(e,Ns,r),d(e,K,r),a(K,gt),a(K,Me),a(Me,Ga),a(Ga,bt),a(K,_t),a(K,Ba),a(Ba,$t),a(K,jt),d(e,Ss,r),_(Ge,e,r),d(e,Ms,r),d(e,ce,r),a(ce,xt),a(ce,La),a(La,vt),a(ce,Et),d(e,Gs,r),_(Be,e,r),d(e,Bs,r),_(Le,e,r),d(e,Ls,r),d(e,F,r),a(F,wt),a(F,Ha),a(Ha,yt),a(F,qt),a(F,Ua),a(Ua,At),a(F,Pt),a(F,Fa),a(Fa,Dt),a(F,kt),d(e,Hs,r),_(He,e,r),d(e,Us,r),_(Ue,e,r),d(e,Fs,r),d(e,va,r),a(va,Ot),d(e,Vs,r),_(ue,e,r),d(e,Js,r),d(e,Y,r),a(Y,Tt),a(Y,Fe),a(Fe,Ct),a(Y,zt),a(Y,Ve),a(Ve,It),a(Y,Rt),d(e,Ws,r),d(e,B,r),a(B,Nt),a(B,Va),a(Va,St),a(B,Mt),a(B,Je),a(Je,Gt),a(B,Bt),a(B,We),a(We,Ja),a(Ja,Lt),a(B,Ht),a(B,Ke),a(Ke,Ut),a(B,Ft),d(e,Ks,r),_(Ye,e,r),d(e,Ys,r),_(Ze,e,r),d(e,Zs,r),d(e,Z,r),a(Z,Vt),a(Z,Wa),a(Wa,Jt),a(Z,Wt),a(Z,Ka),a(Ka,Kt),a(Z,Yt),d(e,Qs,r),_(fe,e,r),d(e,Xs,r),d(e,re,r),a(re,he),a(he,Ya),_(Qe,Ya,null),a(re,Zt),a(re,Za),a(Za,Qt),d(e,eo,r),d(e,Q,r),a(Q,Xt),a(Q,Qa),a(Qa,er),a(Q,ar),a(Q,Xa),a(Xa,sr),a(Q,or),d(e,ao,r),_(Xe,e,r),d(e,so,r),d(e,L,r),a(L,tr),a(L,es),a(es,rr),a(L,nr),a(L,as),a(as,ir),a(L,lr),a(L,ss),a(ss,dr),a(L,mr),a(L,os),a(os,pr),a(L,cr),d(e,oo,r),_(ea,e,r),d(e,to,r),_(aa,e,r),d(e,ro,r),d(e,X,r),a(X,ur),a(X,ts),a(ts,fr),a(X,hr),a(X,Ea),a(Ea,gr),a(X,br),d(e,no,r),_(sa,e,r),d(e,io,r),_(oa,e,r),d(e,lo,r),_(ge,e,r),d(e,mo,r),d(e,V,r),a(V,_r),a(V,rs),a(rs,$r),a(V,jr),a(V,ns),a(ns,xr),a(V,vr),a(V,is),a(is,Er),a(V,wr),d(e,po,r),_(ta,e,r),d(e,co,r),_(ra,e,r),d(e,uo,r),d(e,J,r),a(J,yr),a(J,ls),a(ls,qr),a(J,Ar),a(J,ds),a(ds,Pr),a(J,Dr),a(J,ms),a(ms,kr),a(J,Or),d(e,fo,r),_(na,e,r),d(e,ho,r),_(ia,e,r),d(e,go,r),d(e,be,r),a(be,Tr),a(be,ps),a(ps,Cr),a(be,zr),d(e,bo,r),_(la,e,r),d(e,_o,r),d(e,W,r),a(W,Ir),a(W,cs),a(cs,Rr),a(W,Nr),a(W,us),a(us,Sr),a(W,Mr),a(W,fs),a(fs,Gr),a(W,Br),d(e,$o,r),_(da,e,r),d(e,jo,r),_(ma,e,r),d(e,xo,r),d(e,_e,r),a(_e,Lr),a(_e,hs),a(hs,Hr),a(_e,Ur),d(e,vo,r),_(pa,e,r),d(e,Eo,r),_(ca,e,r),d(e,wo,r),d(e,ee,r),a(ee,Fr),a(ee,gs),a(gs,Vr),a(ee,Jr),a(ee,bs),a(bs,Wr),a(ee,Kr),d(e,yo,r),d(e,wa,r),a(wa,Yr),d(e,qo,r),_(ua,e,r),d(e,Ao,r),_(fa,e,r),d(e,Po,r),_($e,e,r),d(e,Do,r),d(e,ya,r),a(ya,Zr),ko=!0},p(e,[r]){const ha={};r&2&&(ha.$$scope={dirty:r,ctx:e}),me.$set(ha);const _s={};r&2&&(_s.$$scope={dirty:r,ctx:e}),ue.$set(_s);const $s={};r&2&&($s.$$scope={dirty:r,ctx:e}),fe.$set($s);const js={};r&2&&(js.$$scope={dirty:r,ctx:e}),ge.$set(js);const ga={};r&2&&(ga.$$scope={dirty:r,ctx:e}),$e.$set(ga)},i(e){ko||($(h.$$.fragment,e),$(P.$$.fragment,e),$(ye.$$.fragment,e),$(Ae.$$.fragment,e),$(Ce.$$.fragment,e),$(ze.$$.fragment,e),$(Ie.$$.fragment,e),$(me.$$.fragment,e),$(Re.$$.fragment,e),$(Ne.$$.fragment,e),$(Se.$$.fragment,e),$(Ge.$$.fragment,e),$(Be.$$.fragment,e),$(Le.$$.fragment,e),$(He.$$.fragment,e),$(Ue.$$.fragment,e),$(ue.$$.fragment,e),$(Ye.$$.fragment,e),$(Ze.$$.fragment,e),$(fe.$$.fragment,e),$(Qe.$$.fragment,e),$(Xe.$$.fragment,e),$(ea.$$.fragment,e),$(aa.$$.fragment,e),$(sa.$$.fragment,e),$(oa.$$.fragment,e),$(ge.$$.fragment,e),$(ta.$$.fragment,e),$(ra.$$.fragment,e),$(na.$$.fragment,e),$(ia.$$.fragment,e),$(la.$$.fragment,e),$(da.$$.fragment,e),$(ma.$$.fragment,e),$(pa.$$.fragment,e),$(ca.$$.fragment,e),$(ua.$$.fragment,e),$(fa.$$.fragment,e),$($e.$$.fragment,e),ko=!0)},o(e){j(h.$$.fragment,e),j(P.$$.fragment,e),j(ye.$$.fragment,e),j(Ae.$$.fragment,e),j(Ce.$$.fragment,e),j(ze.$$.fragment,e),j(Ie.$$.fragment,e),j(me.$$.fragment,e),j(Re.$$.fragment,e),j(Ne.$$.fragment,e),j(Se.$$.fragment,e),j(Ge.$$.fragment,e),j(Be.$$.fragment,e),j(Le.$$.fragment,e),j(He.$$.fragment,e),j(Ue.$$.fragment,e),j(ue.$$.fragment,e),j(Ye.$$.fragment,e),j(Ze.$$.fragment,e),j(fe.$$.fragment,e),j(Qe.$$.fragment,e),j(Xe.$$.fragment,e),j(ea.$$.fragment,e),j(aa.$$.fragment,e),j(sa.$$.fragment,e),j(oa.$$.fragment,e),j(ge.$$.fragment,e),j(ta.$$.fragment,e),j(ra.$$.fragment,e),j(na.$$.fragment,e),j(ia.$$.fragment,e),j(la.$$.fragment,e),j(da.$$.fragment,e),j(ma.$$.fragment,e),j(pa.$$.fragment,e),j(ca.$$.fragment,e),j(ua.$$.fragment,e),j(fa.$$.fragment,e),j($e.$$.fragment,e),ko=!1},d(e){s(m),e&&s(T),e&&s(f),x(h),e&&s(v),x(P,e),e&&s(y),e&&s(I),e&&s(z),e&&s(S),e&&s(vs),x(ye,e),e&&s(Es),e&&s(ie),e&&s(ws),e&&s(oe),x(Ae),e&&s(ys),e&&s(R),e&&s(qs),x(Ce,e),e&&s(As),e&&s(de),e&&s(Ps),x(ze,e),e&&s(Ds),x(Ie,e),e&&s(ks),e&&s($a),e&&s(Os),x(me,e),e&&s(Ts),e&&s(ja),e&&s(Cs),x(Re,e),e&&s(zs),x(Ne,e),e&&s(Is),e&&s(xa),e&&s(Rs),e&&s(te),x(Se),e&&s(Ns),e&&s(K),e&&s(Ss),x(Ge,e),e&&s(Ms),e&&s(ce),e&&s(Gs),x(Be,e),e&&s(Bs),x(Le,e),e&&s(Ls),e&&s(F),e&&s(Hs),x(He,e),e&&s(Us),x(Ue,e),e&&s(Fs),e&&s(va),e&&s(Vs),x(ue,e),e&&s(Js),e&&s(Y),e&&s(Ws),e&&s(B),e&&s(Ks),x(Ye,e),e&&s(Ys),x(Ze,e),e&&s(Zs),e&&s(Z),e&&s(Qs),x(fe,e),e&&s(Xs),e&&s(re),x(Qe),e&&s(eo),e&&s(Q),e&&s(ao),x(Xe,e),e&&s(so),e&&s(L),e&&s(oo),x(ea,e),e&&s(to),x(aa,e),e&&s(ro),e&&s(X),e&&s(no),x(sa,e),e&&s(io),x(oa,e),e&&s(lo),x(ge,e),e&&s(mo),e&&s(V),e&&s(po),x(ta,e),e&&s(co),x(ra,e),e&&s(uo),e&&s(J),e&&s(fo),x(na,e),e&&s(ho),x(ia,e),e&&s(go),e&&s(be),e&&s(bo),x(la,e),e&&s(_o),e&&s(W),e&&s($o),x(da,e),e&&s(jo),x(ma,e),e&&s(xo),e&&s(_e),e&&s(vo),x(pa,e),e&&s(Eo),x(ca,e),e&&s(wo),e&&s(ee),e&&s(yo),e&&s(wa),e&&s(qo),x(ua,e),e&&s(Ao),x(fa,e),e&&s(Po),x($e,e),e&&s(Do),e&&s(ya)}}}const vi={local:"big-data-datasets-ao-resgate",sections:[{local:"o-que-the-pile",title:"O que \xE9 the Pile?"},{local:"a-magia-do-mapeamento-de-memria",title:"A magia do mapeamento de mem\xF3ria"},{local:"conjuntos-de-dados-em-streaming",title:"Conjuntos de dados em streaming"}],title:"Big data? \u{1F917} Datasets ao resgate"};function Ei(H){return ui(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ki extends di{constructor(m){super();mi(this,m,Ei,xi,pi,{})}}export{ki as default,vi as metadata};
