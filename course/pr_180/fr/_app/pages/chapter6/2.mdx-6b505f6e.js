import{S as Wp,i as ec,s as sc,e as r,k as p,w as m,t as n,M as tc,c as o,d as t,m as c,a as l,x as d,h as a,b as x,F as s,g as u,y as f,q as h,o as v,B as _,v as nc}from"../../chunks/vendor-1e8b365d.js";import{T as ac}from"../../chunks/Tip-62b14c6e.js";import{Y as rc}from"../../chunks/Youtube-c2a8cc39.js";import{I as Ea}from"../../chunks/IconCopyLink-483c28ba.js";import{C as k}from"../../chunks/CodeBlock-e5764662.js";import{D as oc}from"../../chunks/DocNotebookDropdown-37d928d3.js";function lc(pn){let E,ue,D,G,ne,T,qe,ae;return{c(){E=r("p"),ue=n("\u26A0\uFE0F Entra\xEEner un "),D=r("em"),G=n("tokenizer"),ne=n(" n\u2019est pas la m\xEAme chose qu\u2019entra\xEEner un mod\xE8le ! L\u2019entra\xEEnement du mod\xE8le utilise la descente de gradient stochastique pour r\xE9duire un peu plus la perte \xE0 chaque batch. Il est al\xE9atoire par nature (ce qui signifie que vous devez d\xE9finir des graines pour obtenir les m\xEAmes r\xE9sultats lorsque vous effectuez deux fois le m\xEAme entra\xEEnement). Entra\xEEner un "),T=r("em"),qe=n("tokenizer"),ae=n(" est un processus statistique qui tente d\u2019identifier les meilleurs sous-mots \xE0 choisir pour un corpus donn\xE9, et les r\xE8gles exactes utilis\xE9es pour les choisir d\xE9pendent de l\u2019algorithme de tok\xE9nisation. C\u2019est un processus d\xE9terministe, ce qui signifie que vous obtenez toujours les m\xEAmes r\xE9sultats lorsque vous vous entra\xEEnez avec le m\xEAme algorithme sur le m\xEAme corpus.")},l(re){E=o(re,"P",{});var I=l(E);ue=a(I,"\u26A0\uFE0F Entra\xEEner un "),D=o(I,"EM",{});var Y=l(D);G=a(Y,"tokenizer"),Y.forEach(t),ne=a(I," n\u2019est pas la m\xEAme chose qu\u2019entra\xEEner un mod\xE8le ! L\u2019entra\xEEnement du mod\xE8le utilise la descente de gradient stochastique pour r\xE9duire un peu plus la perte \xE0 chaque batch. Il est al\xE9atoire par nature (ce qui signifie que vous devez d\xE9finir des graines pour obtenir les m\xEAmes r\xE9sultats lorsque vous effectuez deux fois le m\xEAme entra\xEEnement). Entra\xEEner un "),T=o(I,"EM",{});var be=l(T);qe=a(be,"tokenizer"),be.forEach(t),ae=a(I," est un processus statistique qui tente d\u2019identifier les meilleurs sous-mots \xE0 choisir pour un corpus donn\xE9, et les r\xE8gles exactes utilis\xE9es pour les choisir d\xE9pendent de l\u2019algorithme de tok\xE9nisation. C\u2019est un processus d\xE9terministe, ce qui signifie que vous obtenez toujours les m\xEAmes r\xE9sultats lorsque vous vous entra\xEEnez avec le m\xEAme algorithme sur le m\xEAme corpus."),I.forEach(t)},m(re,I){u(re,E,I),s(E,ue),s(E,D),s(D,G),s(E,ne),s(E,T),s(T,qe),s(E,ae)},d(re){re&&t(E)}}}function ic(pn){let E,ue,D,G,ne,T,qe,ae,re,I,Y,be,$,wa,$s,ya,Pa,js,Ca,Ma,zs,Aa,Da,ns,La,Oa,Es,Na,Sa,ws,Ta,Ia,ys,Ga,Ra,Ps,Ua,Ha,Cs,Va,Fa,cn,ke,mn,pe,dn,oe,ce,Ms,$e,Ka,As,Ya,fn,R,Ba,Ds,Xa,Ja,Ls,Qa,Za,Os,Wa,er,hn,L,sr,me,tr,Ns,nr,ar,Ss,rr,or,je,lr,ir,ze,ur,pr,vn,Ee,_n,as,cr,xn,we,gn,ye,qn,U,mr,Ts,dr,fr,Is,hr,vr,Gs,_r,xr,bn,Pe,kn,rs,gr,$n,Ce,jn,H,qr,Rs,br,kr,Us,$r,jr,Hs,zr,Er,zn,os,wr,En,Me,wn,ls,yr,yn,Ae,Pn,B,Pr,Vs,Cr,Mr,Fs,Ar,Dr,Cn,is,Lr,Mn,De,An,us,Or,Dn,Le,Ln,ps,Nr,On,Oe,Nn,X,Sr,Ks,Tr,Ir,Ys,Gr,Rr,Sn,Ne,Tn,cs,Ur,In,le,de,Bs,Se,Hr,Xs,Vr,Gn,J,Fr,Js,Kr,Yr,Qs,Br,Xr,Rn,Te,Un,Q,Jr,Zs,Qr,Zr,Ws,Wr,eo,Hn,fe,so,et,to,no,Vn,Ie,Fn,Ge,Kn,C,ao,st,ro,oo,tt,lo,io,nt,uo,po,at,co,mo,rt,fo,ho,Yn,Z,vo,ot,_o,xo,lt,go,qo,Bn,Re,Xn,ms,bo,Jn,M,ko,it,$o,jo,ut,zo,Eo,pt,wo,yo,ct,Po,Co,Ue,Mo,Ao,Qn,y,Do,mt,Lo,Oo,dt,No,So,ft,To,Io,ht,Go,Ro,vt,Uo,Ho,ds,Vo,Fo,Zn,j,Ko,_t,Yo,Bo,xt,Xo,Jo,He,Qo,Zo,gt,Wo,el,qt,sl,tl,bt,nl,al,kt,rl,ol,$t,ll,il,Wn,Ve,ea,Fe,sa,q,ul,jt,pl,cl,zt,ml,dl,Et,fl,hl,wt,vl,_l,yt,xl,gl,Pt,ql,bl,Ct,kl,$l,Mt,jl,zl,At,El,wl,Dt,yl,Pl,ta,Ke,na,Ye,aa,fs,Cl,ra,Be,oa,Xe,la,g,Ml,Lt,Al,Dl,Ot,Ll,Ol,Nt,Nl,Sl,St,Tl,Il,Tt,Gl,Rl,It,Ul,Hl,Gt,Vl,Fl,Rt,Kl,Yl,Ut,Bl,Xl,Ht,Jl,Ql,Vt,Zl,Wl,Ft,ei,si,ia,ie,he,Kt,Je,ti,Yt,ni,ua,W,ai,Bt,ri,oi,Xt,li,ii,pa,Qe,ca,A,ui,Jt,pi,ci,Qt,mi,di,Zt,fi,hi,Wt,vi,_i,en,xi,gi,ma,Ze,da,hs,qi,fa,We,ha,ve,bi,sn,ki,$i,va,es,_a,O,ji,tn,zi,Ei,nn,wi,yi,an,Pi,Ci,rn,Mi,Ai,xa,ss,ga,N,Di,on,Li,Oi,vs,Ni,Si,ln,Ti,Ii,un,Gi,Ri,qa;return T=new Ea({}),Y=new oc({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"}]}}),ke=new rc({props:{id:"DJimQynXZsQ"}}),pe=new ac({props:{warning:!0,$$slots:{default:[lc]},$$scope:{ctx:pn}}}),$e=new Ea({}),Ee=new k({props:{code:`from datasets import load_dataset

# Le chargement peut prendre quelques minutes, alors prenez un caf\xE9 ou un th\xE9 pendant que vous attendez !
raw_datasets = load_dataset("code_search_net", "python")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Le chargement peut prendre quelques minutes, alors prenez un caf\xE9 ou un th\xE9 pendant que vous attendez !</span>
raw_datasets = load_dataset(<span class="hljs-string">&quot;code_search_net&quot;</span>, <span class="hljs-string">&quot;python&quot;</span>)`}}),we=new k({props:{code:'raw_datasets["train"]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]'}}),ye=new k({props:{code:`Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;repository_name&#x27;</span>, <span class="hljs-string">&#x27;func_path_in_repository&#x27;</span>, <span class="hljs-string">&#x27;func_name&#x27;</span>, <span class="hljs-string">&#x27;whole_func_string&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_string&#x27;</span>, <span class="hljs-string">&#x27;func_code_tokens&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_string&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_tokens&#x27;</span>, <span class="hljs-string">&#x27;split_name&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_url&#x27;</span>
    ],
    num_rows: <span class="hljs-number">412178</span>
})`}}),Pe=new k({props:{code:'print(raw_datasets["train"][123456]["whole_func_string"])',highlighted:'<span class="hljs-built_in">print</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">123456</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>])'}}),Ce=new k({props:{code:`def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_simple_responses</span>(<span class="hljs-params">
      self, timeout_ms=<span class="hljs-literal">None</span>, info_cb=DEFAULT_MESSAGE_CALLBACK</span>):
    <span class="hljs-string">&quot;&quot;&quot;Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet&#x27;s message.
    &quot;&quot;&quot;</span>
    <span class="hljs-keyword">return</span> self._accept_responses(<span class="hljs-string">&#x27;OKAY&#x27;</span>, info_cb, timeout_ms=timeout_ms)`}}),Me=new k({props:{code:`# Ne d\xE9commentez pas la ligne suivante \xE0 moins que votre jeu de donn\xE9es soit petit !
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]`,highlighted:`<span class="hljs-comment"># Ne d\xE9commentez pas la ligne suivante \xE0 moins que votre jeu de donn\xE9es soit petit !</span>
<span class="hljs-comment"># training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]</span>`}}),Ae=new k({props:{code:`training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)`,highlighted:`training_corpus = (
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
)`}}),De=new k({props:{code:`gen = (i for i in range(10))
print(list(gen))
print(list(gen))`,highlighted:`gen = (i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))`}}),Le=new k({props:{code:`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]`,highlighted:`[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]
[]`}}),Oe=new k({props:{code:`def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">return</span> (
        raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
    )


training_corpus = get_training_corpus()`}}),Ne=new k({props:{code:`def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
    <span class="hljs-keyword">for</span> start_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        samples = dataset[start_idx : start_idx + <span class="hljs-number">1000</span>]
        <span class="hljs-keyword">yield</span> samples[<span class="hljs-string">&quot;whole_func_string&quot;</span>]`}}),Se=new Ea({}),Te=new k({props:{code:`from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)`}}),Ie=new k({props:{code:`example = '''def add_numbers(a, b):
    """Add the two numbers \`a\` and \`b\`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens`,highlighted:`example = <span class="hljs-string">&#x27;&#x27;&#x27;def add_numbers(a, b):
    &quot;&quot;&quot;Add the two numbers \`a\` and \`b\`.&quot;&quot;&quot;
    return a + b&#x27;&#x27;&#x27;</span>

tokens = old_tokenizer.tokenize(example)
tokens`}}),Ge=new k({props:{code:`['def', '\u0120add', '_', 'n', 'umbers', '(', 'a', ',', '\u0120b', '):', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two',
 '\u0120numbers', '\u0120\`', 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`', '."', '""', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;umbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>,\n <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;.&quot;&#x27;</span>, <span class="hljs-string">&#x27;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),Re=new k({props:{code:"tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)",highlighted:'tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="hljs-number">52000</span>)'}}),Ve=new k({props:{code:`tokens = tokenizer.tokenize(example)
tokens`,highlighted:`tokens = tokenizer.tokenize(example)
tokens`}}),Fe=new k({props:{code:`['def', '\u0120add', '_', 'numbers', '(', 'a', ',', '\u0120b', '):', '\u010A\u0120\u0120\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two', '\u0120numbers', '\u0120\`',
 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`."""', '\u010A\u0120\u0120\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;numbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>, <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>,\n <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`.&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),Ke=new k({props:{code:`print(len(tokens))
print(len(old_tokenizer.tokenize(example)))`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(tokens))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(old_tokenizer.tokenize(example)))`}}),Ye=new k({props:{code:`27
36`,highlighted:`<span class="hljs-number">27</span>
<span class="hljs-number">36</span>`}}),Be=new k({props:{code:`example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)`,highlighted:`example = <span class="hljs-string">&quot;&quot;&quot;class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    &quot;&quot;&quot;</span>
tokenizer.tokenize(example)`}}),Xe=new k({props:{code:`['class', '\u0120Linear', 'Layer', '():', '\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'init', '__(', 'self', ',', '\u0120input', '_', 'size', ',',
 '\u0120output', '_', 'size', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'weight', '\u0120=', '\u0120torch', '.', 'randn', '(', 'input', '_',
 'size', ',', '\u0120output', '_', 'size', ')', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'bias', '\u0120=', '\u0120torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', '\u010A\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'call', '__(', 'self', ',', '\u0120x', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120',
 '\u0120return', '\u0120x', '\u0120@', '\u0120self', '.', 'weights', '\u0120+', '\u0120self', '.', 'bias', '\u010A\u0120\u0120\u0120\u0120']`,highlighted:`[<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;\u0120Linear&#x27;</span>, <span class="hljs-string">&#x27;Layer&#x27;</span>, <span class="hljs-string">&#x27;():&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;init&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weight&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;randn&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>,
 <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;zeros&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>,
 <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;call&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;\u0120@&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weights&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120&#x27;</span>]`}}),Je=new Ea({}),Qe=new k({props:{code:'tokenizer.save_pretrained("code-search-net-tokenizer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),Ze=new k({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),We=new k({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),es=new k({props:{code:'tokenizer.push_to_hub("code-search-net-tokenizer")',highlighted:'tokenizer.push_to_hub(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),ss=new k({props:{code:`# Remplacez "huggingface-course" ci-dessous par votre espace de nom r\xE9el pour utiliser votre propre tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")`,highlighted:`<span class="hljs-comment"># Remplacez &quot;huggingface-course&quot; ci-dessous par votre espace de nom r\xE9el pour utiliser votre propre tokenizer</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)`}}),{c(){E=r("meta"),ue=p(),D=r("h1"),G=r("a"),ne=r("span"),m(T.$$.fragment),qe=p(),ae=r("span"),re=n("Entra\xEEner un nouveau *tokenizer* \xE0 partir d'un ancien"),I=p(),m(Y.$$.fragment),be=p(),$=r("p"),wa=n("Si un mod\xE8le de langue n\u2019est pas disponible dans la langue qui vous int\xE9resse ou si votre corpus est tr\xE8s diff\xE9rent de celui sur lequel votre mod\xE8le de langue a \xE9t\xE9 entra\xEEn\xE9, vous voudrez tr\xE8s probablement r\xE9entra\xEEner le mod\xE8le \xE0 partir de z\xE9ro en utilisant un "),$s=r("em"),ya=n("tokenizer"),Pa=n(" adapt\xE9 \xE0 vos donn\xE9es. Pour ce faire, vous devrez entra\xEEner un nouveau "),js=r("em"),Ca=n("tokenizer"),Ma=n(" sur votre ensemble de donn\xE9es. Mais qu\u2019est-ce que cela signifie exactement ? Lorsque nous avons examin\xE9 pour la premi\xE8re fois les "),zs=r("em"),Aa=n("tokenizers"),Da=n(" dans le "),ns=r("a"),La=n("Chapitre 2"),Oa=n(", nous avons vu que la plupart des "),Es=r("em"),Na=n("transformers"),Sa=n(" utilisent un "),ws=r("em"),Ta=n("algorithme de tokenisation des sous-mots"),Ia=n(". Pour identifier les sous-mots qui sont int\xE9ressants et qui apparaissent le plus fr\xE9quemment dans le corpus en question, le "),ys=r("em"),Ga=n("tokenizer"),Ra=n(" doit examiner attentivement tous les textes du corpus \u2014 un processus que nous appelons "),Ps=r("em"),Ua=n("entra\xEEnement"),Ha=n(". Les r\xE8gles exactes qui r\xE9gissent cet apprentissage d\xE9pendent du type de "),Cs=r("em"),Va=n("tokenizer"),Fa=n(" utilis\xE9, et nous passerons en revue les trois principaux algorithmes plus loin dans ce chapitre."),cn=p(),m(ke.$$.fragment),mn=p(),m(pe.$$.fragment),dn=p(),oe=r("h2"),ce=r("a"),Ms=r("span"),m($e.$$.fragment),Ka=p(),As=r("span"),Ya=n("Assemblage d'un corpus"),fn=p(),R=r("p"),Ba=n("Il y a une API tr\xE8s simple dans \u{1F917} "),Ds=r("em"),Xa=n("Transformers"),Ja=n(" que vous pouvez utiliser pour entra\xEEner un nouveau "),Ls=r("em"),Qa=n("tokenizer"),Za=n(" avec les m\xEAmes caract\xE9ristiques qu\u2019un existant : "),Os=r("code"),Wa=n("AutoTokenizer.train_new_from_iterator()"),er=n(". Pour voir cela en action, disons que nous voulons entra\xEEner GPT-2 \xE0 partir de z\xE9ro, mais dans une langue autre que l\u2019anglais. Notre premi\xE8re t\xE2che sera de rassembler des batchs de donn\xE9es dans cette langue dans un corpus d\u2019entra\xEEnement. Pour fournir des exemples que tout le monde pourra comprendre, nous n\u2019utiliserons pas ici une langue comme le russe ou le chinois, mais plut\xF4t une langue anglaise sp\xE9cialis\xE9e : le code Python."),hn=p(),L=r("p"),sr=n("La biblioth\xE8que "),me=r("a"),tr=n("\u{1F917} "),Ns=r("em"),nr=n("Datasets"),ar=n(" peut nous aider \xE0 assembler un corpus de code source Python. Nous allons utiliser la fonction habituelle "),Ss=r("code"),rr=n("load_dataset()"),or=n(" pour t\xE9l\xE9charger et mettre en cache le jeu de donn\xE9es "),je=r("a"),lr=n("CodeSearchNet"),ir=n(". Ce jeu de donn\xE9es a \xE9t\xE9 cr\xE9\xE9 pour le "),ze=r("a"),ur=n("CodeSearchNet challenge"),pr=n(" et contient des millions de fonctions provenant de biblioth\xE8ques open source sur GitHub dans plusieurs langages de programmation. Ici, nous allons charger la partie Python de ce jeu de donn\xE9es :"),vn=p(),m(Ee.$$.fragment),_n=p(),as=r("p"),cr=n("Nous pouvons jeter un coup d\u2019\u0153il \xE0 la r\xE9partition dans le jeu d\u2019entra\xEEnement pour voir \xE0 quelles colonnes nous avons acc\xE8s :"),xn=p(),m(we.$$.fragment),gn=p(),m(ye.$$.fragment),qn=p(),U=r("p"),mr=n("Nous pouvons voir que le jeu de donn\xE9es s\xE9pare les cha\xEEnes de documents du code et sugg\xE8re une tokenization des deux. Ici, nous utiliserons simplement la colonne "),Ts=r("code"),dr=n("whole_func_string"),fr=n(" pour entra\xEEner notre "),Is=r("em"),hr=n("tokenizer"),vr=n(". Nous pouvons regarder un exemple d\u2019une de ces fonctions en indexant dans le split "),Gs=r("code"),_r=n("train"),xr=n(" :"),bn=p(),m(Pe.$$.fragment),kn=p(),rs=r("p"),gr=n("qui devrait afficher ce qui suit :"),$n=p(),m(Ce.$$.fragment),jn=p(),H=r("p"),qr=n("La premi\xE8re chose \xE0 faire est de transformer l\u2019ensemble de donn\xE9es en un "),Rs=r("em"),br=n("it\xE9rateur"),kr=n(" de listes de textes \u2014 par exemple, une liste de listes de textes. L\u2019utilisation de listes de textes permettra \xE0 notre "),Us=r("em"),$r=n("tokenizer"),jr=n(" d\u2019aller plus vite (en s\u2019entra\xEEnant sur des lots de textes au lieu de traiter des textes individuels un par un), et il doit s\u2019agir d\u2019un it\xE9rateur si nous voulons \xE9viter d\u2019avoir tout en m\xE9moire en m\xEAme temps. Si votre corpus est \xE9norme, vous voudrez profiter du fait que \u{1F917} "),Hs=r("em"),zr=n("Datasets"),Er=n(" ne charge pas tout en RAM mais stocke les \xE9l\xE9ments du jeu de donn\xE9es sur le disque."),zn=p(),os=r("p"),wr=n("Faire ce qui suit cr\xE9erait une liste de listes de 1 000 textes chacune, mais chargerait tout en m\xE9moire :"),En=p(),m(Me.$$.fragment),wn=p(),ls=r("p"),yr=n("En utilisant un g\xE9n\xE9rateur Python, nous pouvons \xE9viter que Python ne charge quoi que ce soit en m\xE9moire jusqu\u2019\xE0 ce que cela soit r\xE9ellement n\xE9cessaire. Pour cr\xE9er un tel g\xE9n\xE9rateur, il suffit de remplacer les crochets par des parenth\xE8ses :"),yn=p(),m(Ae.$$.fragment),Pn=p(),B=r("p"),Pr=n("Cette ligne de code ne r\xE9cup\xE8re aucun \xE9l\xE9ment de l\u2019ensemble de donn\xE9es ; elle cr\xE9e simplement un objet que vous pouvez utiliser dans une boucle "),Vs=r("code"),Cr=n("for"),Mr=n(" Python. Les textes ne seront charg\xE9s que lorsque vous en aurez besoin (c\u2019est-\xE0-dire lorsque vous serez \xE0 l\u2019\xE9tape de la boucle "),Fs=r("code"),Ar=n("for"),Dr=n(" qui les requiert), et seulement 1 000 textes \xE0 la fois seront charg\xE9s. De cette fa\xE7on, vous n\u2019\xE9puiserez pas toute votre m\xE9moire, m\xEAme si vous traitez un \xE9norme ensemble de donn\xE9es."),Cn=p(),is=r("p"),Lr=n("Le probl\xE8me avec un objet g\xE9n\xE9rateur est qu\u2019il ne peut \xEAtre utilis\xE9 qu\u2019une seule fois. Ainsi, au lieu que cet objet nous donne deux fois la liste des 10 premiers chiffres :"),Mn=p(),m(De.$$.fragment),An=p(),us=r("p"),Or=n("on les re\xE7oit une fois et ensuite une liste vide :"),Dn=p(),m(Le.$$.fragment),Ln=p(),ps=r("p"),Nr=n("C\u2019est pourquoi nous d\xE9finissons une fonction qui renvoie un g\xE9n\xE9rateur \xE0 la place :"),On=p(),m(Oe.$$.fragment),Nn=p(),X=r("p"),Sr=n("Vous pouvez \xE9galement d\xE9finir votre g\xE9n\xE9rateur \xE0 l\u2019int\xE9rieur d\u2019une boucle "),Ks=r("code"),Tr=n("for"),Ir=n(" en utilisant l\u2019instruction "),Ys=r("code"),Gr=n("yield"),Rr=n(" :"),Sn=p(),m(Ne.$$.fragment),Tn=p(),cs=r("p"),Ur=n("qui produira exactement le m\xEAme g\xE9n\xE9rateur que pr\xE9c\xE9demment, mais vous permet d\u2019utiliser une logique plus complexe que celle que vous pouvez utiliser dans une compr\xE9hension de liste."),In=p(),le=r("h2"),de=r("a"),Bs=r("span"),m(Se.$$.fragment),Hr=p(),Xs=r("span"),Vr=n("Entra\xEEnement d'un nouveau *tokenizer*."),Gn=p(),J=r("p"),Fr=n("Maintenant que nous avons notre corpus sous la forme d\u2019un it\xE9rateur de lots de textes, nous sommes pr\xEAts \xE0 entra\xEEner un nouveau "),Js=r("em"),Kr=n("tokenizer"),Yr=n(". Pour ce faire, nous devons d\u2019abord charger le "),Qs=r("em"),Br=n("tokenizer"),Xr=n(" que nous voulons coupler avec notre mod\xE8le (ici, GPT-2) :"),Rn=p(),m(Te.$$.fragment),Un=p(),Q=r("p"),Jr=n("M\xEAme si nous allons entra\xEEner un nouveau "),Zs=r("em"),Qr=n("tokenizer"),Zr=n(", c\u2019est une bonne id\xE9e de le faire pour \xE9viter de partir enti\xE8rement de z\xE9ro. De cette fa\xE7on, nous n\u2019aurons pas \xE0 sp\xE9cifier l\u2019algorithme de tok\xE9nisation ou les jetons sp\xE9ciaux que nous voulons utiliser ; notre nouveau "),Ws=r("em"),Wr=n("tokenizer"),eo=n(" sera exactement le m\xEAme que GPT-2, et la seule chose qui changera sera le vocabulaire, qui sera d\xE9termin\xE9 par l\u2019Entra\xEEnement sur notre corpus."),Hn=p(),fe=r("p"),so=n("Voyons d\u2019abord comment ce "),et=r("em"),to=n("tokenizer"),no=n(" traiterait un exemple de fonction :"),Vn=p(),m(Ie.$$.fragment),Fn=p(),m(Ge.$$.fragment),Kn=p(),C=r("p"),ao=n("Ce "),st=r("em"),ro=n("tokenizer"),oo=n(" poss\xE8de quelques symboles sp\xE9ciaux, comme "),tt=r("code"),lo=n("\u0120"),io=n(" et "),nt=r("code"),uo=n("\u010A"),po=n(", qui d\xE9signent respectivement les espaces et les retours \xE0 la ligne. Comme on peut le voir, ce n\u2019est pas tr\xE8s efficace : le "),at=r("em"),co=n("tokenizer"),mo=n(" renvoie des jetons individuels pour chaque espace, alors qu\u2019il pourrait regrouper les niveaux d\u2019indentation (puisqu\u2019avoir des ensembles de quatre ou huit espaces va \xEAtre tr\xE8s courant dans le code). Il divise \xE9galement le nom de la fonction de fa\xE7on un peu bizarre, n\u2019\xE9tant pas habitu\xE9 \xE0 voir des mots avec le caract\xE8re "),rt=r("code"),fo=n("_"),ho=n("."),Yn=p(),Z=r("p"),vo=n("Entra\xEEnons un nouveau "),ot=r("em"),_o=n("tokenizer"),xo=n(" et voyons s\u2019il r\xE9sout ces probl\xE8mes. Pour cela, nous allons utiliser la m\xE9thode "),lt=r("code"),go=n("train_new_from_iterator()"),qo=n(" :"),Bn=p(),m(Re.$$.fragment),Xn=p(),ms=r("p"),bo=n("Cette commande peut prendre un peu de temps si votre corpus est tr\xE8s grand, mais pour ce jeu de donn\xE9es de 1,6 Go de textes, elle est tr\xE8s rapide (1 minute 16 secondes sur un CPU AMD Ryzen 9 3900X avec 12 c\u0153urs)."),Jn=p(),M=r("p"),ko=n("Notez que "),it=r("code"),$o=n("AutoTokenizer.train_new_from_iterator()"),jo=n(" ne fonctionne que si le tokenizer que vous utilisez est un tokenizer \xAB rapide \xBB. Comme vous le verrez dans la section suivante, la biblioth\xE8que \u{1F917} "),ut=r("em"),zo=n("Transformers"),Eo=n(" contient deux types de "),pt=r("em"),wo=n("tokenizers"),yo=n(" : certains sont \xE9crits purement en Python et d\u2019autres (les rapides) sont soutenus par la biblioth\xE8que \u{1F917} "),ct=r("em"),Po=n("Tokenizers"),Co=n(", qui est \xE9crite dans le langage de programmation "),Ue=r("a"),Mo=n("Rust"),Ao=n(". Python est le langage le plus souvent utilis\xE9 pour les applications de science des donn\xE9es et d\u2019apprentissage profond, mais lorsque quelque chose doit \xEAtre parall\xE9lis\xE9 pour \xEAtre rapide, il doit \xEAtre \xE9crit dans un autre langage. Par exemple, les multiplications matricielles qui sont au c\u0153ur du calcul du mod\xE8le sont \xE9crites en CUDA, une biblioth\xE8que C optimis\xE9e pour les GPU."),Qn=p(),y=r("p"),Do=n("Entra\xEEner un tout nouveau "),mt=r("em"),Lo=n("tokenizer"),Oo=n(" en Python pur serait atrocement lent, c\u2019est pourquoi nous avons d\xE9velopp\xE9 la biblioth\xE8que \u{1F917} "),dt=r("em"),No=n("Tokenizers"),So=n(". Notez que, tout comme vous n\u2019avez pas eu \xE0 apprendre le langage CUDA pour pouvoir ex\xE9cuter votre mod\xE8le sur un lot d\u2019entr\xE9es sur un GPU, vous n\u2019aurez pas besoin d\u2019apprendre Rust pour utiliser un "),ft=r("em"),To=n("tokenizer"),Io=n(" rapide. La biblioth\xE8que \u{1F917} "),ht=r("em"),Go=n("Tokenizers"),Ro=n(" fournit des liaisons Python pour de nombreuses m\xE9thodes qui appellent en interne un morceau de code en Rust ; par exemple, pour parall\xE9liser l\u2019entra\xEEnement de votre nouveau "),vt=r("em"),Uo=n("tokenizer"),Ho=n(" ou, comme nous l\u2019avons vu dans le "),ds=r("a"),Vo=n("Chapitre 3"),Fo=n(", la tokenisation d\u2019un lot d\u2019entr\xE9es."),Zn=p(),j=r("p"),Ko=n("La plupart des "),_t=r("em"),Yo=n("transformers"),Bo=n(" ont un "),xt=r("em"),Xo=n("tokenizer"),Jo=n(" rapide disponible (il y a quelques exceptions que vous pouvez v\xE9rifier "),He=r("a"),Qo=n("ici"),Zo=n("), et l\u2019API "),gt=r("code"),Wo=n("AutoTokenizer"),el=n(" s\xE9lectionne toujours le "),qt=r("em"),sl=n("tokenizer"),tl=n(" rapide pour vous s\u2019il est disponible. Dans la prochaine section, nous allons jeter un coup d\u2019oeil \xE0 certaines des autres caract\xE9ristiques sp\xE9ciales des "),bt=r("em"),nl=n("tokenizers"),al=n(" rapides, qui seront vraiment utiles pour des t\xE2ches comme la classification des "),kt=r("em"),rl=n("tokens"),ol=n(" et la r\xE9ponse aux questions. Mais avant cela, essayons notre tout nouveau "),$t=r("em"),ll=n("tokenizer"),il=n(" sur l\u2019exemple pr\xE9c\xE9dent :"),Wn=p(),m(Ve.$$.fragment),ea=p(),m(Fe.$$.fragment),sa=p(),q=r("p"),ul=n("Ici, nous voyons \xE0 nouveau les symboles sp\xE9ciaux "),jt=r("code"),pl=n("\u0120"),cl=n(" et "),zt=r("code"),ml=n("\u010A"),dl=n(" qui indiquent les espaces et les retours \xE0 la ligne, mais nous pouvons \xE9galement voir que notre "),Et=r("em"),fl=n("tokenizer"),hl=n(" a appris certains "),wt=r("em"),vl=n("tokens"),_l=n(" qui sont tr\xE8s sp\xE9cifiques \xE0 un corpus de fonctions Python : par exemple, il y a un token "),yt=r("code"),xl=n("\u010A\u0120\u0120\u0120"),gl=n(" qui repr\xE9sente une indentation, et un "),Pt=r("em"),ql=n("token"),bl=p(),Ct=r("code"),kl=n('\u0120"""'),$l=n(" qui repr\xE9sente les trois guillemets qui commencent une docstring. Le "),Mt=r("em"),jl=n("tokenizer"),zl=n(" divise \xE9galement correctement le nom de la fonction sur "),At=r("code"),El=n("_"),wl=n(". Il s\u2019agit d\u2019une repr\xE9sentation assez compacte ; en comparaison, l\u2019utilisation du "),Dt=r("em"),yl=n("tokenizer"),Pl=n(" en anglais simple sur le m\xEAme exemple nous donnera une phrase plus longue :"),ta=p(),m(Ke.$$.fragment),na=p(),m(Ye.$$.fragment),aa=p(),fs=r("p"),Cl=n("Prenons un autre exemple :"),ra=p(),m(Be.$$.fragment),oa=p(),m(Xe.$$.fragment),la=p(),g=r("p"),Ml=n("En plus du "),Lt=r("em"),Al=n("token"),Dl=n(" correspondant \xE0 une indentation, on peut \xE9galement voir ici un "),Ot=r("em"),Ll=n("token"),Ol=n(" pour une double indentation : "),Nt=r("code"),Nl=n("\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),Sl=n(". Les mots sp\xE9ciaux de Python comme "),St=r("code"),Tl=n("class"),Il=n(", "),Tt=r("code"),Gl=n("init"),Rl=n(", "),It=r("code"),Ul=n("call"),Hl=n(", "),Gt=r("code"),Vl=n("self"),Fl=n(", et "),Rt=r("code"),Kl=n("return"),Yl=n(" sont tous tokeniz\xE9s comme un seul token, et nous pouvons voir qu\u2019en plus de s\xE9parer sur "),Ut=r("code"),Bl=n("_"),Xl=n(" et "),Ht=r("code"),Jl=n("."),Ql=n(" le tokenizer s\xE9pare correctement m\xEAme les noms en minuscules : "),Vt=r("code"),Zl=n("LinearLayer"),Wl=n(" est tokeniz\xE9 comme "),Ft=r("code"),ei=n('["\u0120Linear", "Layer"]'),si=n("."),ia=p(),ie=r("h2"),he=r("a"),Kt=r("span"),m(Je.$$.fragment),ti=p(),Yt=r("span"),ni=n("Sauvegarde du *tokenizer*."),ua=p(),W=r("p"),ai=n("Pour \xEAtre s\xFBr de pouvoir l\u2019utiliser plus tard, nous devons sauvegarder notre nouveau "),Bt=r("em"),ri=n("tokenizer"),oi=n(". Comme pour les mod\xE8les, ceci est fait avec la m\xE9thode "),Xt=r("code"),li=n("save_pretrained()"),ii=n(" :"),pa=p(),m(Qe.$$.fragment),ca=p(),A=r("p"),ui=n("Cela cr\xE9era un nouveau dossier nomm\xE9 "),Jt=r("em"),pi=n("code-search-net-tokenizer"),ci=n(", qui contiendra tous les fichiers dont le "),Qt=r("em"),mi=n("tokenizer"),di=n(" a besoin pour \xEAtre recharg\xE9. Si vous souhaitez partager ce "),Zt=r("em"),fi=n("tokenizer"),hi=n(" avec vos coll\xE8gues et amis, vous pouvez le t\xE9l\xE9charger sur le "),Wt=r("em"),vi=n("Hub"),_i=n(" en vous connectant \xE0 votre compte. Si vous travaillez dans un "),en=r("em"),xi=n("notebook"),gi=n(", il existe une fonction pratique pour vous aider \xE0 le faire :"),ma=p(),m(Ze.$$.fragment),da=p(),hs=r("p"),qi=n("Cela affichera un widget o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face. Si vous ne travaillez pas dans un ordinateur portable, tapez simplement la ligne suivante dans votre terminal :"),fa=p(),m(We.$$.fragment),ha=p(),ve=r("p"),bi=n("Une fois que vous vous \xEAtes connect\xE9, vous pouvez pousser votre "),sn=r("em"),ki=n("tokenizer"),$i=n(" en ex\xE9cutant la commande suivante :"),va=p(),m(es.$$.fragment),_a=p(),O=r("p"),ji=n("Cela cr\xE9era un nouveau d\xE9p\xF4t dans votre espace de noms avec le nom "),tn=r("code"),zi=n("code-search-net-tokenizer"),Ei=n(", contenant le fichier "),nn=r("em"),wi=n("tokenizer"),yi=n(". Vous pouvez ensuite charger le "),an=r("em"),Pi=n("tokenizer"),Ci=n(" de n\u2019importe o\xF9 avec la m\xE9thode "),rn=r("code"),Mi=n("from_pretrained()"),Ai=n(" :"),xa=p(),m(ss.$$.fragment),ga=p(),N=r("p"),Di=n("Vous \xEAtes maintenant pr\xEAt \xE0 entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro et \xE0 le "),on=r("em"),Li=n("finetuner"),Oi=n(" sur votre t\xE2che ! Nous y reviendrons au "),vs=r("a"),Ni=n("Chapitre 7"),Si=n(", mais d\u2019abord, dans le reste de ce chapitre, nous allons examiner de plus pr\xE8s les "),ln=r("em"),Ti=n("tokenizers"),Ii=n(" rapides et explorer en d\xE9tail ce qui se passe r\xE9ellement lorsque nous appelons la m\xE9thode "),un=r("code"),Gi=n("train_new_from_iterator()"),Ri=n("."),this.h()},l(e){const i=tc('[data-svelte="svelte-1phssyn"]',document.head);E=o(i,"META",{name:!0,content:!0}),i.forEach(t),ue=c(e),D=o(e,"H1",{class:!0});var ts=l(D);G=o(ts,"A",{id:!0,class:!0,href:!0});var Hi=l(G);ne=o(Hi,"SPAN",{});var Vi=l(ne);d(T.$$.fragment,Vi),Vi.forEach(t),Hi.forEach(t),qe=c(ts),ae=o(ts,"SPAN",{});var Fi=l(ae);re=a(Fi,"Entra\xEEner un nouveau *tokenizer* \xE0 partir d'un ancien"),Fi.forEach(t),ts.forEach(t),I=c(e),d(Y.$$.fragment,e),be=c(e),$=o(e,"P",{});var w=l($);wa=a(w,"Si un mod\xE8le de langue n\u2019est pas disponible dans la langue qui vous int\xE9resse ou si votre corpus est tr\xE8s diff\xE9rent de celui sur lequel votre mod\xE8le de langue a \xE9t\xE9 entra\xEEn\xE9, vous voudrez tr\xE8s probablement r\xE9entra\xEEner le mod\xE8le \xE0 partir de z\xE9ro en utilisant un "),$s=o(w,"EM",{});var Ki=l($s);ya=a(Ki,"tokenizer"),Ki.forEach(t),Pa=a(w," adapt\xE9 \xE0 vos donn\xE9es. Pour ce faire, vous devrez entra\xEEner un nouveau "),js=o(w,"EM",{});var Yi=l(js);Ca=a(Yi,"tokenizer"),Yi.forEach(t),Ma=a(w," sur votre ensemble de donn\xE9es. Mais qu\u2019est-ce que cela signifie exactement ? Lorsque nous avons examin\xE9 pour la premi\xE8re fois les "),zs=o(w,"EM",{});var Bi=l(zs);Aa=a(Bi,"tokenizers"),Bi.forEach(t),Da=a(w," dans le "),ns=o(w,"A",{href:!0});var Xi=l(ns);La=a(Xi,"Chapitre 2"),Xi.forEach(t),Oa=a(w,", nous avons vu que la plupart des "),Es=o(w,"EM",{});var Ji=l(Es);Na=a(Ji,"transformers"),Ji.forEach(t),Sa=a(w," utilisent un "),ws=o(w,"EM",{});var Qi=l(ws);Ta=a(Qi,"algorithme de tokenisation des sous-mots"),Qi.forEach(t),Ia=a(w,". Pour identifier les sous-mots qui sont int\xE9ressants et qui apparaissent le plus fr\xE9quemment dans le corpus en question, le "),ys=o(w,"EM",{});var Zi=l(ys);Ga=a(Zi,"tokenizer"),Zi.forEach(t),Ra=a(w," doit examiner attentivement tous les textes du corpus \u2014 un processus que nous appelons "),Ps=o(w,"EM",{});var Wi=l(Ps);Ua=a(Wi,"entra\xEEnement"),Wi.forEach(t),Ha=a(w,". Les r\xE8gles exactes qui r\xE9gissent cet apprentissage d\xE9pendent du type de "),Cs=o(w,"EM",{});var eu=l(Cs);Va=a(eu,"tokenizer"),eu.forEach(t),Fa=a(w," utilis\xE9, et nous passerons en revue les trois principaux algorithmes plus loin dans ce chapitre."),w.forEach(t),cn=c(e),d(ke.$$.fragment,e),mn=c(e),d(pe.$$.fragment,e),dn=c(e),oe=o(e,"H2",{class:!0});var ba=l(oe);ce=o(ba,"A",{id:!0,class:!0,href:!0});var su=l(ce);Ms=o(su,"SPAN",{});var tu=l(Ms);d($e.$$.fragment,tu),tu.forEach(t),su.forEach(t),Ka=c(ba),As=o(ba,"SPAN",{});var nu=l(As);Ya=a(nu,"Assemblage d'un corpus"),nu.forEach(t),ba.forEach(t),fn=c(e),R=o(e,"P",{});var _e=l(R);Ba=a(_e,"Il y a une API tr\xE8s simple dans \u{1F917} "),Ds=o(_e,"EM",{});var au=l(Ds);Xa=a(au,"Transformers"),au.forEach(t),Ja=a(_e," que vous pouvez utiliser pour entra\xEEner un nouveau "),Ls=o(_e,"EM",{});var ru=l(Ls);Qa=a(ru,"tokenizer"),ru.forEach(t),Za=a(_e," avec les m\xEAmes caract\xE9ristiques qu\u2019un existant : "),Os=o(_e,"CODE",{});var ou=l(Os);Wa=a(ou,"AutoTokenizer.train_new_from_iterator()"),ou.forEach(t),er=a(_e,". Pour voir cela en action, disons que nous voulons entra\xEEner GPT-2 \xE0 partir de z\xE9ro, mais dans une langue autre que l\u2019anglais. Notre premi\xE8re t\xE2che sera de rassembler des batchs de donn\xE9es dans cette langue dans un corpus d\u2019entra\xEEnement. Pour fournir des exemples que tout le monde pourra comprendre, nous n\u2019utiliserons pas ici une langue comme le russe ou le chinois, mais plut\xF4t une langue anglaise sp\xE9cialis\xE9e : le code Python."),_e.forEach(t),hn=c(e),L=o(e,"P",{});var ee=l(L);sr=a(ee,"La biblioth\xE8que "),me=o(ee,"A",{href:!0,rel:!0});var Ui=l(me);tr=a(Ui,"\u{1F917} "),Ns=o(Ui,"EM",{});var lu=l(Ns);nr=a(lu,"Datasets"),lu.forEach(t),Ui.forEach(t),ar=a(ee," peut nous aider \xE0 assembler un corpus de code source Python. Nous allons utiliser la fonction habituelle "),Ss=o(ee,"CODE",{});var iu=l(Ss);rr=a(iu,"load_dataset()"),iu.forEach(t),or=a(ee," pour t\xE9l\xE9charger et mettre en cache le jeu de donn\xE9es "),je=o(ee,"A",{href:!0,rel:!0});var uu=l(je);lr=a(uu,"CodeSearchNet"),uu.forEach(t),ir=a(ee,". Ce jeu de donn\xE9es a \xE9t\xE9 cr\xE9\xE9 pour le "),ze=o(ee,"A",{href:!0,rel:!0});var pu=l(ze);ur=a(pu,"CodeSearchNet challenge"),pu.forEach(t),pr=a(ee," et contient des millions de fonctions provenant de biblioth\xE8ques open source sur GitHub dans plusieurs langages de programmation. Ici, nous allons charger la partie Python de ce jeu de donn\xE9es :"),ee.forEach(t),vn=c(e),d(Ee.$$.fragment,e),_n=c(e),as=o(e,"P",{});var cu=l(as);cr=a(cu,"Nous pouvons jeter un coup d\u2019\u0153il \xE0 la r\xE9partition dans le jeu d\u2019entra\xEEnement pour voir \xE0 quelles colonnes nous avons acc\xE8s :"),cu.forEach(t),xn=c(e),d(we.$$.fragment,e),gn=c(e),d(ye.$$.fragment,e),qn=c(e),U=o(e,"P",{});var xe=l(U);mr=a(xe,"Nous pouvons voir que le jeu de donn\xE9es s\xE9pare les cha\xEEnes de documents du code et sugg\xE8re une tokenization des deux. Ici, nous utiliserons simplement la colonne "),Ts=o(xe,"CODE",{});var mu=l(Ts);dr=a(mu,"whole_func_string"),mu.forEach(t),fr=a(xe," pour entra\xEEner notre "),Is=o(xe,"EM",{});var du=l(Is);hr=a(du,"tokenizer"),du.forEach(t),vr=a(xe,". Nous pouvons regarder un exemple d\u2019une de ces fonctions en indexant dans le split "),Gs=o(xe,"CODE",{});var fu=l(Gs);_r=a(fu,"train"),fu.forEach(t),xr=a(xe," :"),xe.forEach(t),bn=c(e),d(Pe.$$.fragment,e),kn=c(e),rs=o(e,"P",{});var hu=l(rs);gr=a(hu,"qui devrait afficher ce qui suit :"),hu.forEach(t),$n=c(e),d(Ce.$$.fragment,e),jn=c(e),H=o(e,"P",{});var ge=l(H);qr=a(ge,"La premi\xE8re chose \xE0 faire est de transformer l\u2019ensemble de donn\xE9es en un "),Rs=o(ge,"EM",{});var vu=l(Rs);br=a(vu,"it\xE9rateur"),vu.forEach(t),kr=a(ge," de listes de textes \u2014 par exemple, une liste de listes de textes. L\u2019utilisation de listes de textes permettra \xE0 notre "),Us=o(ge,"EM",{});var _u=l(Us);$r=a(_u,"tokenizer"),_u.forEach(t),jr=a(ge," d\u2019aller plus vite (en s\u2019entra\xEEnant sur des lots de textes au lieu de traiter des textes individuels un par un), et il doit s\u2019agir d\u2019un it\xE9rateur si nous voulons \xE9viter d\u2019avoir tout en m\xE9moire en m\xEAme temps. Si votre corpus est \xE9norme, vous voudrez profiter du fait que \u{1F917} "),Hs=o(ge,"EM",{});var xu=l(Hs);zr=a(xu,"Datasets"),xu.forEach(t),Er=a(ge," ne charge pas tout en RAM mais stocke les \xE9l\xE9ments du jeu de donn\xE9es sur le disque."),ge.forEach(t),zn=c(e),os=o(e,"P",{});var gu=l(os);wr=a(gu,"Faire ce qui suit cr\xE9erait une liste de listes de 1 000 textes chacune, mais chargerait tout en m\xE9moire :"),gu.forEach(t),En=c(e),d(Me.$$.fragment,e),wn=c(e),ls=o(e,"P",{});var qu=l(ls);yr=a(qu,"En utilisant un g\xE9n\xE9rateur Python, nous pouvons \xE9viter que Python ne charge quoi que ce soit en m\xE9moire jusqu\u2019\xE0 ce que cela soit r\xE9ellement n\xE9cessaire. Pour cr\xE9er un tel g\xE9n\xE9rateur, il suffit de remplacer les crochets par des parenth\xE8ses :"),qu.forEach(t),yn=c(e),d(Ae.$$.fragment,e),Pn=c(e),B=o(e,"P",{});var _s=l(B);Pr=a(_s,"Cette ligne de code ne r\xE9cup\xE8re aucun \xE9l\xE9ment de l\u2019ensemble de donn\xE9es ; elle cr\xE9e simplement un objet que vous pouvez utiliser dans une boucle "),Vs=o(_s,"CODE",{});var bu=l(Vs);Cr=a(bu,"for"),bu.forEach(t),Mr=a(_s," Python. Les textes ne seront charg\xE9s que lorsque vous en aurez besoin (c\u2019est-\xE0-dire lorsque vous serez \xE0 l\u2019\xE9tape de la boucle "),Fs=o(_s,"CODE",{});var ku=l(Fs);Ar=a(ku,"for"),ku.forEach(t),Dr=a(_s," qui les requiert), et seulement 1 000 textes \xE0 la fois seront charg\xE9s. De cette fa\xE7on, vous n\u2019\xE9puiserez pas toute votre m\xE9moire, m\xEAme si vous traitez un \xE9norme ensemble de donn\xE9es."),_s.forEach(t),Cn=c(e),is=o(e,"P",{});var $u=l(is);Lr=a($u,"Le probl\xE8me avec un objet g\xE9n\xE9rateur est qu\u2019il ne peut \xEAtre utilis\xE9 qu\u2019une seule fois. Ainsi, au lieu que cet objet nous donne deux fois la liste des 10 premiers chiffres :"),$u.forEach(t),Mn=c(e),d(De.$$.fragment,e),An=c(e),us=o(e,"P",{});var ju=l(us);Or=a(ju,"on les re\xE7oit une fois et ensuite une liste vide :"),ju.forEach(t),Dn=c(e),d(Le.$$.fragment,e),Ln=c(e),ps=o(e,"P",{});var zu=l(ps);Nr=a(zu,"C\u2019est pourquoi nous d\xE9finissons une fonction qui renvoie un g\xE9n\xE9rateur \xE0 la place :"),zu.forEach(t),On=c(e),d(Oe.$$.fragment,e),Nn=c(e),X=o(e,"P",{});var xs=l(X);Sr=a(xs,"Vous pouvez \xE9galement d\xE9finir votre g\xE9n\xE9rateur \xE0 l\u2019int\xE9rieur d\u2019une boucle "),Ks=o(xs,"CODE",{});var Eu=l(Ks);Tr=a(Eu,"for"),Eu.forEach(t),Ir=a(xs," en utilisant l\u2019instruction "),Ys=o(xs,"CODE",{});var wu=l(Ys);Gr=a(wu,"yield"),wu.forEach(t),Rr=a(xs," :"),xs.forEach(t),Sn=c(e),d(Ne.$$.fragment,e),Tn=c(e),cs=o(e,"P",{});var yu=l(cs);Ur=a(yu,"qui produira exactement le m\xEAme g\xE9n\xE9rateur que pr\xE9c\xE9demment, mais vous permet d\u2019utiliser une logique plus complexe que celle que vous pouvez utiliser dans une compr\xE9hension de liste."),yu.forEach(t),In=c(e),le=o(e,"H2",{class:!0});var ka=l(le);de=o(ka,"A",{id:!0,class:!0,href:!0});var Pu=l(de);Bs=o(Pu,"SPAN",{});var Cu=l(Bs);d(Se.$$.fragment,Cu),Cu.forEach(t),Pu.forEach(t),Hr=c(ka),Xs=o(ka,"SPAN",{});var Mu=l(Xs);Vr=a(Mu,"Entra\xEEnement d'un nouveau *tokenizer*."),Mu.forEach(t),ka.forEach(t),Gn=c(e),J=o(e,"P",{});var gs=l(J);Fr=a(gs,"Maintenant que nous avons notre corpus sous la forme d\u2019un it\xE9rateur de lots de textes, nous sommes pr\xEAts \xE0 entra\xEEner un nouveau "),Js=o(gs,"EM",{});var Au=l(Js);Kr=a(Au,"tokenizer"),Au.forEach(t),Yr=a(gs,". Pour ce faire, nous devons d\u2019abord charger le "),Qs=o(gs,"EM",{});var Du=l(Qs);Br=a(Du,"tokenizer"),Du.forEach(t),Xr=a(gs," que nous voulons coupler avec notre mod\xE8le (ici, GPT-2) :"),gs.forEach(t),Rn=c(e),d(Te.$$.fragment,e),Un=c(e),Q=o(e,"P",{});var qs=l(Q);Jr=a(qs,"M\xEAme si nous allons entra\xEEner un nouveau "),Zs=o(qs,"EM",{});var Lu=l(Zs);Qr=a(Lu,"tokenizer"),Lu.forEach(t),Zr=a(qs,", c\u2019est une bonne id\xE9e de le faire pour \xE9viter de partir enti\xE8rement de z\xE9ro. De cette fa\xE7on, nous n\u2019aurons pas \xE0 sp\xE9cifier l\u2019algorithme de tok\xE9nisation ou les jetons sp\xE9ciaux que nous voulons utiliser ; notre nouveau "),Ws=o(qs,"EM",{});var Ou=l(Ws);Wr=a(Ou,"tokenizer"),Ou.forEach(t),eo=a(qs," sera exactement le m\xEAme que GPT-2, et la seule chose qui changera sera le vocabulaire, qui sera d\xE9termin\xE9 par l\u2019Entra\xEEnement sur notre corpus."),qs.forEach(t),Hn=c(e),fe=o(e,"P",{});var $a=l(fe);so=a($a,"Voyons d\u2019abord comment ce "),et=o($a,"EM",{});var Nu=l(et);to=a(Nu,"tokenizer"),Nu.forEach(t),no=a($a," traiterait un exemple de fonction :"),$a.forEach(t),Vn=c(e),d(Ie.$$.fragment,e),Fn=c(e),d(Ge.$$.fragment,e),Kn=c(e),C=o(e,"P",{});var V=l(C);ao=a(V,"Ce "),st=o(V,"EM",{});var Su=l(st);ro=a(Su,"tokenizer"),Su.forEach(t),oo=a(V," poss\xE8de quelques symboles sp\xE9ciaux, comme "),tt=o(V,"CODE",{});var Tu=l(tt);lo=a(Tu,"\u0120"),Tu.forEach(t),io=a(V," et "),nt=o(V,"CODE",{});var Iu=l(nt);uo=a(Iu,"\u010A"),Iu.forEach(t),po=a(V,", qui d\xE9signent respectivement les espaces et les retours \xE0 la ligne. Comme on peut le voir, ce n\u2019est pas tr\xE8s efficace : le "),at=o(V,"EM",{});var Gu=l(at);co=a(Gu,"tokenizer"),Gu.forEach(t),mo=a(V," renvoie des jetons individuels pour chaque espace, alors qu\u2019il pourrait regrouper les niveaux d\u2019indentation (puisqu\u2019avoir des ensembles de quatre ou huit espaces va \xEAtre tr\xE8s courant dans le code). Il divise \xE9galement le nom de la fonction de fa\xE7on un peu bizarre, n\u2019\xE9tant pas habitu\xE9 \xE0 voir des mots avec le caract\xE8re "),rt=o(V,"CODE",{});var Ru=l(rt);fo=a(Ru,"_"),Ru.forEach(t),ho=a(V,"."),V.forEach(t),Yn=c(e),Z=o(e,"P",{});var bs=l(Z);vo=a(bs,"Entra\xEEnons un nouveau "),ot=o(bs,"EM",{});var Uu=l(ot);_o=a(Uu,"tokenizer"),Uu.forEach(t),xo=a(bs," et voyons s\u2019il r\xE9sout ces probl\xE8mes. Pour cela, nous allons utiliser la m\xE9thode "),lt=o(bs,"CODE",{});var Hu=l(lt);go=a(Hu,"train_new_from_iterator()"),Hu.forEach(t),qo=a(bs," :"),bs.forEach(t),Bn=c(e),d(Re.$$.fragment,e),Xn=c(e),ms=o(e,"P",{});var Vu=l(ms);bo=a(Vu,"Cette commande peut prendre un peu de temps si votre corpus est tr\xE8s grand, mais pour ce jeu de donn\xE9es de 1,6 Go de textes, elle est tr\xE8s rapide (1 minute 16 secondes sur un CPU AMD Ryzen 9 3900X avec 12 c\u0153urs)."),Vu.forEach(t),Jn=c(e),M=o(e,"P",{});var F=l(M);ko=a(F,"Notez que "),it=o(F,"CODE",{});var Fu=l(it);$o=a(Fu,"AutoTokenizer.train_new_from_iterator()"),Fu.forEach(t),jo=a(F," ne fonctionne que si le tokenizer que vous utilisez est un tokenizer \xAB rapide \xBB. Comme vous le verrez dans la section suivante, la biblioth\xE8que \u{1F917} "),ut=o(F,"EM",{});var Ku=l(ut);zo=a(Ku,"Transformers"),Ku.forEach(t),Eo=a(F," contient deux types de "),pt=o(F,"EM",{});var Yu=l(pt);wo=a(Yu,"tokenizers"),Yu.forEach(t),yo=a(F," : certains sont \xE9crits purement en Python et d\u2019autres (les rapides) sont soutenus par la biblioth\xE8que \u{1F917} "),ct=o(F,"EM",{});var Bu=l(ct);Po=a(Bu,"Tokenizers"),Bu.forEach(t),Co=a(F,", qui est \xE9crite dans le langage de programmation "),Ue=o(F,"A",{href:!0,rel:!0});var Xu=l(Ue);Mo=a(Xu,"Rust"),Xu.forEach(t),Ao=a(F,". Python est le langage le plus souvent utilis\xE9 pour les applications de science des donn\xE9es et d\u2019apprentissage profond, mais lorsque quelque chose doit \xEAtre parall\xE9lis\xE9 pour \xEAtre rapide, il doit \xEAtre \xE9crit dans un autre langage. Par exemple, les multiplications matricielles qui sont au c\u0153ur du calcul du mod\xE8le sont \xE9crites en CUDA, une biblioth\xE8que C optimis\xE9e pour les GPU."),F.forEach(t),Qn=c(e),y=o(e,"P",{});var S=l(y);Do=a(S,"Entra\xEEner un tout nouveau "),mt=o(S,"EM",{});var Ju=l(mt);Lo=a(Ju,"tokenizer"),Ju.forEach(t),Oo=a(S," en Python pur serait atrocement lent, c\u2019est pourquoi nous avons d\xE9velopp\xE9 la biblioth\xE8que \u{1F917} "),dt=o(S,"EM",{});var Qu=l(dt);No=a(Qu,"Tokenizers"),Qu.forEach(t),So=a(S,". Notez que, tout comme vous n\u2019avez pas eu \xE0 apprendre le langage CUDA pour pouvoir ex\xE9cuter votre mod\xE8le sur un lot d\u2019entr\xE9es sur un GPU, vous n\u2019aurez pas besoin d\u2019apprendre Rust pour utiliser un "),ft=o(S,"EM",{});var Zu=l(ft);To=a(Zu,"tokenizer"),Zu.forEach(t),Io=a(S," rapide. La biblioth\xE8que \u{1F917} "),ht=o(S,"EM",{});var Wu=l(ht);Go=a(Wu,"Tokenizers"),Wu.forEach(t),Ro=a(S," fournit des liaisons Python pour de nombreuses m\xE9thodes qui appellent en interne un morceau de code en Rust ; par exemple, pour parall\xE9liser l\u2019entra\xEEnement de votre nouveau "),vt=o(S,"EM",{});var ep=l(vt);Uo=a(ep,"tokenizer"),ep.forEach(t),Ho=a(S," ou, comme nous l\u2019avons vu dans le "),ds=o(S,"A",{href:!0});var sp=l(ds);Vo=a(sp,"Chapitre 3"),sp.forEach(t),Fo=a(S,", la tokenisation d\u2019un lot d\u2019entr\xE9es."),S.forEach(t),Zn=c(e),j=o(e,"P",{});var P=l(j);Ko=a(P,"La plupart des "),_t=o(P,"EM",{});var tp=l(_t);Yo=a(tp,"transformers"),tp.forEach(t),Bo=a(P," ont un "),xt=o(P,"EM",{});var np=l(xt);Xo=a(np,"tokenizer"),np.forEach(t),Jo=a(P," rapide disponible (il y a quelques exceptions que vous pouvez v\xE9rifier "),He=o(P,"A",{href:!0,rel:!0});var ap=l(He);Qo=a(ap,"ici"),ap.forEach(t),Zo=a(P,"), et l\u2019API "),gt=o(P,"CODE",{});var rp=l(gt);Wo=a(rp,"AutoTokenizer"),rp.forEach(t),el=a(P," s\xE9lectionne toujours le "),qt=o(P,"EM",{});var op=l(qt);sl=a(op,"tokenizer"),op.forEach(t),tl=a(P," rapide pour vous s\u2019il est disponible. Dans la prochaine section, nous allons jeter un coup d\u2019oeil \xE0 certaines des autres caract\xE9ristiques sp\xE9ciales des "),bt=o(P,"EM",{});var lp=l(bt);nl=a(lp,"tokenizers"),lp.forEach(t),al=a(P," rapides, qui seront vraiment utiles pour des t\xE2ches comme la classification des "),kt=o(P,"EM",{});var ip=l(kt);rl=a(ip,"tokens"),ip.forEach(t),ol=a(P," et la r\xE9ponse aux questions. Mais avant cela, essayons notre tout nouveau "),$t=o(P,"EM",{});var up=l($t);ll=a(up,"tokenizer"),up.forEach(t),il=a(P," sur l\u2019exemple pr\xE9c\xE9dent :"),P.forEach(t),Wn=c(e),d(Ve.$$.fragment,e),ea=c(e),d(Fe.$$.fragment,e),sa=c(e),q=o(e,"P",{});var z=l(q);ul=a(z,"Ici, nous voyons \xE0 nouveau les symboles sp\xE9ciaux "),jt=o(z,"CODE",{});var pp=l(jt);pl=a(pp,"\u0120"),pp.forEach(t),cl=a(z," et "),zt=o(z,"CODE",{});var cp=l(zt);ml=a(cp,"\u010A"),cp.forEach(t),dl=a(z," qui indiquent les espaces et les retours \xE0 la ligne, mais nous pouvons \xE9galement voir que notre "),Et=o(z,"EM",{});var mp=l(Et);fl=a(mp,"tokenizer"),mp.forEach(t),hl=a(z," a appris certains "),wt=o(z,"EM",{});var dp=l(wt);vl=a(dp,"tokens"),dp.forEach(t),_l=a(z," qui sont tr\xE8s sp\xE9cifiques \xE0 un corpus de fonctions Python : par exemple, il y a un token "),yt=o(z,"CODE",{});var fp=l(yt);xl=a(fp,"\u010A\u0120\u0120\u0120"),fp.forEach(t),gl=a(z," qui repr\xE9sente une indentation, et un "),Pt=o(z,"EM",{});var hp=l(Pt);ql=a(hp,"token"),hp.forEach(t),bl=c(z),Ct=o(z,"CODE",{});var vp=l(Ct);kl=a(vp,'\u0120"""'),vp.forEach(t),$l=a(z," qui repr\xE9sente les trois guillemets qui commencent une docstring. Le "),Mt=o(z,"EM",{});var _p=l(Mt);jl=a(_p,"tokenizer"),_p.forEach(t),zl=a(z," divise \xE9galement correctement le nom de la fonction sur "),At=o(z,"CODE",{});var xp=l(At);El=a(xp,"_"),xp.forEach(t),wl=a(z,". Il s\u2019agit d\u2019une repr\xE9sentation assez compacte ; en comparaison, l\u2019utilisation du "),Dt=o(z,"EM",{});var gp=l(Dt);yl=a(gp,"tokenizer"),gp.forEach(t),Pl=a(z," en anglais simple sur le m\xEAme exemple nous donnera une phrase plus longue :"),z.forEach(t),ta=c(e),d(Ke.$$.fragment,e),na=c(e),d(Ye.$$.fragment,e),aa=c(e),fs=o(e,"P",{});var qp=l(fs);Cl=a(qp,"Prenons un autre exemple :"),qp.forEach(t),ra=c(e),d(Be.$$.fragment,e),oa=c(e),d(Xe.$$.fragment,e),la=c(e),g=o(e,"P",{});var b=l(g);Ml=a(b,"En plus du "),Lt=o(b,"EM",{});var bp=l(Lt);Al=a(bp,"token"),bp.forEach(t),Dl=a(b," correspondant \xE0 une indentation, on peut \xE9galement voir ici un "),Ot=o(b,"EM",{});var kp=l(Ot);Ll=a(kp,"token"),kp.forEach(t),Ol=a(b," pour une double indentation : "),Nt=o(b,"CODE",{});var $p=l(Nt);Nl=a($p,"\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),$p.forEach(t),Sl=a(b,". Les mots sp\xE9ciaux de Python comme "),St=o(b,"CODE",{});var jp=l(St);Tl=a(jp,"class"),jp.forEach(t),Il=a(b,", "),Tt=o(b,"CODE",{});var zp=l(Tt);Gl=a(zp,"init"),zp.forEach(t),Rl=a(b,", "),It=o(b,"CODE",{});var Ep=l(It);Ul=a(Ep,"call"),Ep.forEach(t),Hl=a(b,", "),Gt=o(b,"CODE",{});var wp=l(Gt);Vl=a(wp,"self"),wp.forEach(t),Fl=a(b,", et "),Rt=o(b,"CODE",{});var yp=l(Rt);Kl=a(yp,"return"),yp.forEach(t),Yl=a(b," sont tous tokeniz\xE9s comme un seul token, et nous pouvons voir qu\u2019en plus de s\xE9parer sur "),Ut=o(b,"CODE",{});var Pp=l(Ut);Bl=a(Pp,"_"),Pp.forEach(t),Xl=a(b," et "),Ht=o(b,"CODE",{});var Cp=l(Ht);Jl=a(Cp,"."),Cp.forEach(t),Ql=a(b," le tokenizer s\xE9pare correctement m\xEAme les noms en minuscules : "),Vt=o(b,"CODE",{});var Mp=l(Vt);Zl=a(Mp,"LinearLayer"),Mp.forEach(t),Wl=a(b," est tokeniz\xE9 comme "),Ft=o(b,"CODE",{});var Ap=l(Ft);ei=a(Ap,'["\u0120Linear", "Layer"]'),Ap.forEach(t),si=a(b,"."),b.forEach(t),ia=c(e),ie=o(e,"H2",{class:!0});var ja=l(ie);he=o(ja,"A",{id:!0,class:!0,href:!0});var Dp=l(he);Kt=o(Dp,"SPAN",{});var Lp=l(Kt);d(Je.$$.fragment,Lp),Lp.forEach(t),Dp.forEach(t),ti=c(ja),Yt=o(ja,"SPAN",{});var Op=l(Yt);ni=a(Op,"Sauvegarde du *tokenizer*."),Op.forEach(t),ja.forEach(t),ua=c(e),W=o(e,"P",{});var ks=l(W);ai=a(ks,"Pour \xEAtre s\xFBr de pouvoir l\u2019utiliser plus tard, nous devons sauvegarder notre nouveau "),Bt=o(ks,"EM",{});var Np=l(Bt);ri=a(Np,"tokenizer"),Np.forEach(t),oi=a(ks,". Comme pour les mod\xE8les, ceci est fait avec la m\xE9thode "),Xt=o(ks,"CODE",{});var Sp=l(Xt);li=a(Sp,"save_pretrained()"),Sp.forEach(t),ii=a(ks," :"),ks.forEach(t),pa=c(e),d(Qe.$$.fragment,e),ca=c(e),A=o(e,"P",{});var K=l(A);ui=a(K,"Cela cr\xE9era un nouveau dossier nomm\xE9 "),Jt=o(K,"EM",{});var Tp=l(Jt);pi=a(Tp,"code-search-net-tokenizer"),Tp.forEach(t),ci=a(K,", qui contiendra tous les fichiers dont le "),Qt=o(K,"EM",{});var Ip=l(Qt);mi=a(Ip,"tokenizer"),Ip.forEach(t),di=a(K," a besoin pour \xEAtre recharg\xE9. Si vous souhaitez partager ce "),Zt=o(K,"EM",{});var Gp=l(Zt);fi=a(Gp,"tokenizer"),Gp.forEach(t),hi=a(K," avec vos coll\xE8gues et amis, vous pouvez le t\xE9l\xE9charger sur le "),Wt=o(K,"EM",{});var Rp=l(Wt);vi=a(Rp,"Hub"),Rp.forEach(t),_i=a(K," en vous connectant \xE0 votre compte. Si vous travaillez dans un "),en=o(K,"EM",{});var Up=l(en);xi=a(Up,"notebook"),Up.forEach(t),gi=a(K,", il existe une fonction pratique pour vous aider \xE0 le faire :"),K.forEach(t),ma=c(e),d(Ze.$$.fragment,e),da=c(e),hs=o(e,"P",{});var Hp=l(hs);qi=a(Hp,"Cela affichera un widget o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face. Si vous ne travaillez pas dans un ordinateur portable, tapez simplement la ligne suivante dans votre terminal :"),Hp.forEach(t),fa=c(e),d(We.$$.fragment,e),ha=c(e),ve=o(e,"P",{});var za=l(ve);bi=a(za,"Une fois que vous vous \xEAtes connect\xE9, vous pouvez pousser votre "),sn=o(za,"EM",{});var Vp=l(sn);ki=a(Vp,"tokenizer"),Vp.forEach(t),$i=a(za," en ex\xE9cutant la commande suivante :"),za.forEach(t),va=c(e),d(es.$$.fragment,e),_a=c(e),O=o(e,"P",{});var se=l(O);ji=a(se,"Cela cr\xE9era un nouveau d\xE9p\xF4t dans votre espace de noms avec le nom "),tn=o(se,"CODE",{});var Fp=l(tn);zi=a(Fp,"code-search-net-tokenizer"),Fp.forEach(t),Ei=a(se,", contenant le fichier "),nn=o(se,"EM",{});var Kp=l(nn);wi=a(Kp,"tokenizer"),Kp.forEach(t),yi=a(se,". Vous pouvez ensuite charger le "),an=o(se,"EM",{});var Yp=l(an);Pi=a(Yp,"tokenizer"),Yp.forEach(t),Ci=a(se," de n\u2019importe o\xF9 avec la m\xE9thode "),rn=o(se,"CODE",{});var Bp=l(rn);Mi=a(Bp,"from_pretrained()"),Bp.forEach(t),Ai=a(se," :"),se.forEach(t),xa=c(e),d(ss.$$.fragment,e),ga=c(e),N=o(e,"P",{});var te=l(N);Di=a(te,"Vous \xEAtes maintenant pr\xEAt \xE0 entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro et \xE0 le "),on=o(te,"EM",{});var Xp=l(on);Li=a(Xp,"finetuner"),Xp.forEach(t),Oi=a(te," sur votre t\xE2che ! Nous y reviendrons au "),vs=o(te,"A",{href:!0});var Jp=l(vs);Ni=a(Jp,"Chapitre 7"),Jp.forEach(t),Si=a(te,", mais d\u2019abord, dans le reste de ce chapitre, nous allons examiner de plus pr\xE8s les "),ln=o(te,"EM",{});var Qp=l(ln);Ti=a(Qp,"tokenizers"),Qp.forEach(t),Ii=a(te," rapides et explorer en d\xE9tail ce qui se passe r\xE9ellement lorsque nous appelons la m\xE9thode "),un=o(te,"CODE",{});var Zp=l(un);Gi=a(Zp,"train_new_from_iterator()"),Zp.forEach(t),Ri=a(te,"."),te.forEach(t),this.h()},h(){x(E,"name","hf:doc:metadata"),x(E,"content",JSON.stringify(uc)),x(G,"id","entraner-un-nouveau-tokenizer-partir-dun-ancien"),x(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),x(G,"href","#entraner-un-nouveau-tokenizer-partir-dun-ancien"),x(D,"class","relative group"),x(ns,"href","/course/fr/chapter2"),x(ce,"id","assemblage-dun-corpus"),x(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),x(ce,"href","#assemblage-dun-corpus"),x(oe,"class","relative group"),x(me,"href","https://github.com/huggingface/datasets"),x(me,"rel","nofollow"),x(je,"href","https://huggingface.co/datasets/code_search_net"),x(je,"rel","nofollow"),x(ze,"href","https://wandb.ai/github/CodeSearchNet/benchmark"),x(ze,"rel","nofollow"),x(de,"id","entranement-dun-nouveau-tokenizer"),x(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),x(de,"href","#entranement-dun-nouveau-tokenizer"),x(le,"class","relative group"),x(Ue,"href","https://www.rust-lang.org"),x(Ue,"rel","nofollow"),x(ds,"href","/course/fr/chapter3"),x(He,"href","https://huggingface.co/transformers/#supported-frameworks"),x(He,"rel","nofollow"),x(he,"id","sauvegarde-du-tokenizer"),x(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),x(he,"href","#sauvegarde-du-tokenizer"),x(ie,"class","relative group"),x(vs,"href","/course/fr/chapter7")},m(e,i){s(document.head,E),u(e,ue,i),u(e,D,i),s(D,G),s(G,ne),f(T,ne,null),s(D,qe),s(D,ae),s(ae,re),u(e,I,i),f(Y,e,i),u(e,be,i),u(e,$,i),s($,wa),s($,$s),s($s,ya),s($,Pa),s($,js),s(js,Ca),s($,Ma),s($,zs),s(zs,Aa),s($,Da),s($,ns),s(ns,La),s($,Oa),s($,Es),s(Es,Na),s($,Sa),s($,ws),s(ws,Ta),s($,Ia),s($,ys),s(ys,Ga),s($,Ra),s($,Ps),s(Ps,Ua),s($,Ha),s($,Cs),s(Cs,Va),s($,Fa),u(e,cn,i),f(ke,e,i),u(e,mn,i),f(pe,e,i),u(e,dn,i),u(e,oe,i),s(oe,ce),s(ce,Ms),f($e,Ms,null),s(oe,Ka),s(oe,As),s(As,Ya),u(e,fn,i),u(e,R,i),s(R,Ba),s(R,Ds),s(Ds,Xa),s(R,Ja),s(R,Ls),s(Ls,Qa),s(R,Za),s(R,Os),s(Os,Wa),s(R,er),u(e,hn,i),u(e,L,i),s(L,sr),s(L,me),s(me,tr),s(me,Ns),s(Ns,nr),s(L,ar),s(L,Ss),s(Ss,rr),s(L,or),s(L,je),s(je,lr),s(L,ir),s(L,ze),s(ze,ur),s(L,pr),u(e,vn,i),f(Ee,e,i),u(e,_n,i),u(e,as,i),s(as,cr),u(e,xn,i),f(we,e,i),u(e,gn,i),f(ye,e,i),u(e,qn,i),u(e,U,i),s(U,mr),s(U,Ts),s(Ts,dr),s(U,fr),s(U,Is),s(Is,hr),s(U,vr),s(U,Gs),s(Gs,_r),s(U,xr),u(e,bn,i),f(Pe,e,i),u(e,kn,i),u(e,rs,i),s(rs,gr),u(e,$n,i),f(Ce,e,i),u(e,jn,i),u(e,H,i),s(H,qr),s(H,Rs),s(Rs,br),s(H,kr),s(H,Us),s(Us,$r),s(H,jr),s(H,Hs),s(Hs,zr),s(H,Er),u(e,zn,i),u(e,os,i),s(os,wr),u(e,En,i),f(Me,e,i),u(e,wn,i),u(e,ls,i),s(ls,yr),u(e,yn,i),f(Ae,e,i),u(e,Pn,i),u(e,B,i),s(B,Pr),s(B,Vs),s(Vs,Cr),s(B,Mr),s(B,Fs),s(Fs,Ar),s(B,Dr),u(e,Cn,i),u(e,is,i),s(is,Lr),u(e,Mn,i),f(De,e,i),u(e,An,i),u(e,us,i),s(us,Or),u(e,Dn,i),f(Le,e,i),u(e,Ln,i),u(e,ps,i),s(ps,Nr),u(e,On,i),f(Oe,e,i),u(e,Nn,i),u(e,X,i),s(X,Sr),s(X,Ks),s(Ks,Tr),s(X,Ir),s(X,Ys),s(Ys,Gr),s(X,Rr),u(e,Sn,i),f(Ne,e,i),u(e,Tn,i),u(e,cs,i),s(cs,Ur),u(e,In,i),u(e,le,i),s(le,de),s(de,Bs),f(Se,Bs,null),s(le,Hr),s(le,Xs),s(Xs,Vr),u(e,Gn,i),u(e,J,i),s(J,Fr),s(J,Js),s(Js,Kr),s(J,Yr),s(J,Qs),s(Qs,Br),s(J,Xr),u(e,Rn,i),f(Te,e,i),u(e,Un,i),u(e,Q,i),s(Q,Jr),s(Q,Zs),s(Zs,Qr),s(Q,Zr),s(Q,Ws),s(Ws,Wr),s(Q,eo),u(e,Hn,i),u(e,fe,i),s(fe,so),s(fe,et),s(et,to),s(fe,no),u(e,Vn,i),f(Ie,e,i),u(e,Fn,i),f(Ge,e,i),u(e,Kn,i),u(e,C,i),s(C,ao),s(C,st),s(st,ro),s(C,oo),s(C,tt),s(tt,lo),s(C,io),s(C,nt),s(nt,uo),s(C,po),s(C,at),s(at,co),s(C,mo),s(C,rt),s(rt,fo),s(C,ho),u(e,Yn,i),u(e,Z,i),s(Z,vo),s(Z,ot),s(ot,_o),s(Z,xo),s(Z,lt),s(lt,go),s(Z,qo),u(e,Bn,i),f(Re,e,i),u(e,Xn,i),u(e,ms,i),s(ms,bo),u(e,Jn,i),u(e,M,i),s(M,ko),s(M,it),s(it,$o),s(M,jo),s(M,ut),s(ut,zo),s(M,Eo),s(M,pt),s(pt,wo),s(M,yo),s(M,ct),s(ct,Po),s(M,Co),s(M,Ue),s(Ue,Mo),s(M,Ao),u(e,Qn,i),u(e,y,i),s(y,Do),s(y,mt),s(mt,Lo),s(y,Oo),s(y,dt),s(dt,No),s(y,So),s(y,ft),s(ft,To),s(y,Io),s(y,ht),s(ht,Go),s(y,Ro),s(y,vt),s(vt,Uo),s(y,Ho),s(y,ds),s(ds,Vo),s(y,Fo),u(e,Zn,i),u(e,j,i),s(j,Ko),s(j,_t),s(_t,Yo),s(j,Bo),s(j,xt),s(xt,Xo),s(j,Jo),s(j,He),s(He,Qo),s(j,Zo),s(j,gt),s(gt,Wo),s(j,el),s(j,qt),s(qt,sl),s(j,tl),s(j,bt),s(bt,nl),s(j,al),s(j,kt),s(kt,rl),s(j,ol),s(j,$t),s($t,ll),s(j,il),u(e,Wn,i),f(Ve,e,i),u(e,ea,i),f(Fe,e,i),u(e,sa,i),u(e,q,i),s(q,ul),s(q,jt),s(jt,pl),s(q,cl),s(q,zt),s(zt,ml),s(q,dl),s(q,Et),s(Et,fl),s(q,hl),s(q,wt),s(wt,vl),s(q,_l),s(q,yt),s(yt,xl),s(q,gl),s(q,Pt),s(Pt,ql),s(q,bl),s(q,Ct),s(Ct,kl),s(q,$l),s(q,Mt),s(Mt,jl),s(q,zl),s(q,At),s(At,El),s(q,wl),s(q,Dt),s(Dt,yl),s(q,Pl),u(e,ta,i),f(Ke,e,i),u(e,na,i),f(Ye,e,i),u(e,aa,i),u(e,fs,i),s(fs,Cl),u(e,ra,i),f(Be,e,i),u(e,oa,i),f(Xe,e,i),u(e,la,i),u(e,g,i),s(g,Ml),s(g,Lt),s(Lt,Al),s(g,Dl),s(g,Ot),s(Ot,Ll),s(g,Ol),s(g,Nt),s(Nt,Nl),s(g,Sl),s(g,St),s(St,Tl),s(g,Il),s(g,Tt),s(Tt,Gl),s(g,Rl),s(g,It),s(It,Ul),s(g,Hl),s(g,Gt),s(Gt,Vl),s(g,Fl),s(g,Rt),s(Rt,Kl),s(g,Yl),s(g,Ut),s(Ut,Bl),s(g,Xl),s(g,Ht),s(Ht,Jl),s(g,Ql),s(g,Vt),s(Vt,Zl),s(g,Wl),s(g,Ft),s(Ft,ei),s(g,si),u(e,ia,i),u(e,ie,i),s(ie,he),s(he,Kt),f(Je,Kt,null),s(ie,ti),s(ie,Yt),s(Yt,ni),u(e,ua,i),u(e,W,i),s(W,ai),s(W,Bt),s(Bt,ri),s(W,oi),s(W,Xt),s(Xt,li),s(W,ii),u(e,pa,i),f(Qe,e,i),u(e,ca,i),u(e,A,i),s(A,ui),s(A,Jt),s(Jt,pi),s(A,ci),s(A,Qt),s(Qt,mi),s(A,di),s(A,Zt),s(Zt,fi),s(A,hi),s(A,Wt),s(Wt,vi),s(A,_i),s(A,en),s(en,xi),s(A,gi),u(e,ma,i),f(Ze,e,i),u(e,da,i),u(e,hs,i),s(hs,qi),u(e,fa,i),f(We,e,i),u(e,ha,i),u(e,ve,i),s(ve,bi),s(ve,sn),s(sn,ki),s(ve,$i),u(e,va,i),f(es,e,i),u(e,_a,i),u(e,O,i),s(O,ji),s(O,tn),s(tn,zi),s(O,Ei),s(O,nn),s(nn,wi),s(O,yi),s(O,an),s(an,Pi),s(O,Ci),s(O,rn),s(rn,Mi),s(O,Ai),u(e,xa,i),f(ss,e,i),u(e,ga,i),u(e,N,i),s(N,Di),s(N,on),s(on,Li),s(N,Oi),s(N,vs),s(vs,Ni),s(N,Si),s(N,ln),s(ln,Ti),s(N,Ii),s(N,un),s(un,Gi),s(N,Ri),qa=!0},p(e,[i]){const ts={};i&2&&(ts.$$scope={dirty:i,ctx:e}),pe.$set(ts)},i(e){qa||(h(T.$$.fragment,e),h(Y.$$.fragment,e),h(ke.$$.fragment,e),h(pe.$$.fragment,e),h($e.$$.fragment,e),h(Ee.$$.fragment,e),h(we.$$.fragment,e),h(ye.$$.fragment,e),h(Pe.$$.fragment,e),h(Ce.$$.fragment,e),h(Me.$$.fragment,e),h(Ae.$$.fragment,e),h(De.$$.fragment,e),h(Le.$$.fragment,e),h(Oe.$$.fragment,e),h(Ne.$$.fragment,e),h(Se.$$.fragment,e),h(Te.$$.fragment,e),h(Ie.$$.fragment,e),h(Ge.$$.fragment,e),h(Re.$$.fragment,e),h(Ve.$$.fragment,e),h(Fe.$$.fragment,e),h(Ke.$$.fragment,e),h(Ye.$$.fragment,e),h(Be.$$.fragment,e),h(Xe.$$.fragment,e),h(Je.$$.fragment,e),h(Qe.$$.fragment,e),h(Ze.$$.fragment,e),h(We.$$.fragment,e),h(es.$$.fragment,e),h(ss.$$.fragment,e),qa=!0)},o(e){v(T.$$.fragment,e),v(Y.$$.fragment,e),v(ke.$$.fragment,e),v(pe.$$.fragment,e),v($e.$$.fragment,e),v(Ee.$$.fragment,e),v(we.$$.fragment,e),v(ye.$$.fragment,e),v(Pe.$$.fragment,e),v(Ce.$$.fragment,e),v(Me.$$.fragment,e),v(Ae.$$.fragment,e),v(De.$$.fragment,e),v(Le.$$.fragment,e),v(Oe.$$.fragment,e),v(Ne.$$.fragment,e),v(Se.$$.fragment,e),v(Te.$$.fragment,e),v(Ie.$$.fragment,e),v(Ge.$$.fragment,e),v(Re.$$.fragment,e),v(Ve.$$.fragment,e),v(Fe.$$.fragment,e),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(Be.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Qe.$$.fragment,e),v(Ze.$$.fragment,e),v(We.$$.fragment,e),v(es.$$.fragment,e),v(ss.$$.fragment,e),qa=!1},d(e){t(E),e&&t(ue),e&&t(D),_(T),e&&t(I),_(Y,e),e&&t(be),e&&t($),e&&t(cn),_(ke,e),e&&t(mn),_(pe,e),e&&t(dn),e&&t(oe),_($e),e&&t(fn),e&&t(R),e&&t(hn),e&&t(L),e&&t(vn),_(Ee,e),e&&t(_n),e&&t(as),e&&t(xn),_(we,e),e&&t(gn),_(ye,e),e&&t(qn),e&&t(U),e&&t(bn),_(Pe,e),e&&t(kn),e&&t(rs),e&&t($n),_(Ce,e),e&&t(jn),e&&t(H),e&&t(zn),e&&t(os),e&&t(En),_(Me,e),e&&t(wn),e&&t(ls),e&&t(yn),_(Ae,e),e&&t(Pn),e&&t(B),e&&t(Cn),e&&t(is),e&&t(Mn),_(De,e),e&&t(An),e&&t(us),e&&t(Dn),_(Le,e),e&&t(Ln),e&&t(ps),e&&t(On),_(Oe,e),e&&t(Nn),e&&t(X),e&&t(Sn),_(Ne,e),e&&t(Tn),e&&t(cs),e&&t(In),e&&t(le),_(Se),e&&t(Gn),e&&t(J),e&&t(Rn),_(Te,e),e&&t(Un),e&&t(Q),e&&t(Hn),e&&t(fe),e&&t(Vn),_(Ie,e),e&&t(Fn),_(Ge,e),e&&t(Kn),e&&t(C),e&&t(Yn),e&&t(Z),e&&t(Bn),_(Re,e),e&&t(Xn),e&&t(ms),e&&t(Jn),e&&t(M),e&&t(Qn),e&&t(y),e&&t(Zn),e&&t(j),e&&t(Wn),_(Ve,e),e&&t(ea),_(Fe,e),e&&t(sa),e&&t(q),e&&t(ta),_(Ke,e),e&&t(na),_(Ye,e),e&&t(aa),e&&t(fs),e&&t(ra),_(Be,e),e&&t(oa),_(Xe,e),e&&t(la),e&&t(g),e&&t(ia),e&&t(ie),_(Je),e&&t(ua),e&&t(W),e&&t(pa),_(Qe,e),e&&t(ca),e&&t(A),e&&t(ma),_(Ze,e),e&&t(da),e&&t(hs),e&&t(fa),_(We,e),e&&t(ha),e&&t(ve),e&&t(va),_(es,e),e&&t(_a),e&&t(O),e&&t(xa),_(ss,e),e&&t(ga),e&&t(N)}}}const uc={local:"entraner-un-nouveau-tokenizer-partir-dun-ancien",sections:[{local:"assemblage-dun-corpus",title:"Assemblage d'un corpus"},{local:"entranement-dun-nouveau-tokenizer",title:"Entra\xEEnement d'un nouveau *tokenizer*."},{local:"sauvegarde-du-tokenizer",title:"Sauvegarde du *tokenizer*."}],title:"Entra\xEEner un nouveau *tokenizer* \xE0 partir d'un ancien"};function pc(pn){return nc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _c extends Wp{constructor(E){super();ec(this,E,pc,ic,sc,{})}}export{_c as default,uc as metadata};
