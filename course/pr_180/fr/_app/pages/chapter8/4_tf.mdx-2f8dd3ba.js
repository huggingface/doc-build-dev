import{S as Y0,i as W0,s as Z0,e as t,k as i,w as b,t as a,M as sd,c as p,d as n,m as c,x as h,a as u,h as l,b as m,F as e,g as o,y as f,q as v,o as j,B as q,v as ed}from"../../chunks/vendor-1e8b365d.js";import{T as Nl}from"../../chunks/Tip-62b14c6e.js";import{Y as nd}from"../../chunks/Youtube-c2a8cc39.js";import{I as B}from"../../chunks/IconCopyLink-483c28ba.js";import{C as w}from"../../chunks/CodeBlock-e5764662.js";import{D as ad}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as ld}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function rd(M){let d,y,$,g,k,z,P,A,O,D,ss,F,J,Ps,S,V,fs,vs,oe,es,xs,ns,js;return{c(){d=t("p"),y=a("\u270F\uFE0F "),$=t("em"),g=a("A votre tour !"),k=a(" Comme d\xE9fi optionnel apr\xE8s avoir r\xE9solu les autres probl\xE8mes, vous pouvez essayer de revenir \xE0 cette \xE9tape et faire fonctionner le mod\xE8le avec la perte originale calcul\xE9e par Keras au lieu de la perte interne. Vous devrez ajouter "),z=t("code"),P=a('"labels"'),A=a(" \xE0 l\u2019argument "),O=t("code"),D=a("label_cols"),ss=a(" de "),F=t("code"),J=a("to_tf_dataset()"),Ps=a(" pour vous assurer que les labels sont correctement sortis, ce qui vous donnera des gradients \u2014 mais il y a un autre probl\xE8me avec la perte que nous avons sp\xE9cifi\xE9e. L\u2019Entra\xEEnement fonctionnera toujours avec ce probl\xE8me, mais l\u2019apprentissage sera tr\xE8s lent et se stabilisera \xE0 une perte d\u2019entra\xEEnement \xE9lev\xE9e. Pouvez-vous trouver ce que c\u2019est ?"),S=i(),V=t("p"),fs=a("Un indice cod\xE9 en ROT13, si vous \xEAtes coinc\xE9 : Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),vs=t("code"),oe=a("ybtvgf"),es=a(". Jung ner ybtvgf ?"),xs=i(),ns=t("p"),js=a("Et un deuxi\xE8me indice : Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf ny gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf ?")},l(C){d=p(C,"P",{});var x=u(d);y=l(x,"\u270F\uFE0F "),$=p(x,"EM",{});var ie=u($);g=l(ie,"A votre tour !"),ie.forEach(n),k=l(x," Comme d\xE9fi optionnel apr\xE8s avoir r\xE9solu les autres probl\xE8mes, vous pouvez essayer de revenir \xE0 cette \xE9tape et faire fonctionner le mod\xE8le avec la perte originale calcul\xE9e par Keras au lieu de la perte interne. Vous devrez ajouter "),z=p(x,"CODE",{});var U=u(z);P=l(U,'"labels"'),U.forEach(n),A=l(x," \xE0 l\u2019argument "),O=p(x,"CODE",{});var X=u(O);D=l(X,"label_cols"),X.forEach(n),ss=l(x," de "),F=p(x,"CODE",{});var As=u(F);J=l(As,"to_tf_dataset()"),As.forEach(n),Ps=l(x," pour vous assurer que les labels sont correctement sortis, ce qui vous donnera des gradients \u2014 mais il y a un autre probl\xE8me avec la perte que nous avons sp\xE9cifi\xE9e. L\u2019Entra\xEEnement fonctionnera toujours avec ce probl\xE8me, mais l\u2019apprentissage sera tr\xE8s lent et se stabilisera \xE0 une perte d\u2019entra\xEEnement \xE9lev\xE9e. Pouvez-vous trouver ce que c\u2019est ?"),x.forEach(n),S=c(C),V=p(C,"P",{});var G=u(V);fs=l(G,"Un indice cod\xE9 en ROT13, si vous \xEAtes coinc\xE9 : Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),vs=p(G,"CODE",{});var tn=u(vs);oe=l(tn,"ybtvgf"),tn.forEach(n),es=l(G,". Jung ner ybtvgf ?"),G.forEach(n),xs=c(C),ns=p(C,"P",{});var Ds=u(ns);js=l(Ds,"Et un deuxi\xE8me indice : Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf ny gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf ?"),Ds.forEach(n)},m(C,x){o(C,d,x),e(d,y),e(d,$),e($,g),e(d,k),e(d,z),e(z,P),e(d,A),e(d,O),e(O,D),e(d,ss),e(d,F),e(F,J),e(d,Ps),o(C,S,x),o(C,V,x),e(V,fs),e(V,vs),e(vs,oe),e(V,es),o(C,xs,x),o(C,ns,x),e(ns,js)},d(C){C&&n(d),C&&n(S),C&&n(V),C&&n(xs),C&&n(ns)}}}function td(M){let d,y,$,g,k,z,P,A;return{c(){d=t("p"),y=a("\u{1F4A1} Vous pouvez \xE9galement importer la fonction "),$=t("code"),g=a("create_optimizer()"),k=a(" de \u{1F917} "),z=t("em"),P=a("Transformers"),A=a(", qui vous donnera un optimiseur AdamW avec une d\xE9croissance de poids correcte ainsi qu\u2019un r\xE9chauffement et une d\xE9croissance du taux d\u2019apprentissage. Cet optimiseur produira souvent des r\xE9sultats l\xE9g\xE8rement meilleurs que ceux que vous obtenez avec l\u2019optimiseur Adam par d\xE9faut.")},l(O){d=p(O,"P",{});var D=u(d);y=l(D,"\u{1F4A1} Vous pouvez \xE9galement importer la fonction "),$=p(D,"CODE",{});var ss=u($);g=l(ss,"create_optimizer()"),ss.forEach(n),k=l(D," de \u{1F917} "),z=p(D,"EM",{});var F=u(z);P=l(F,"Transformers"),F.forEach(n),A=l(D,", qui vous donnera un optimiseur AdamW avec une d\xE9croissance de poids correcte ainsi qu\u2019un r\xE9chauffement et une d\xE9croissance du taux d\u2019apprentissage. Cet optimiseur produira souvent des r\xE9sultats l\xE9g\xE8rement meilleurs que ceux que vous obtenez avec l\u2019optimiseur Adam par d\xE9faut."),D.forEach(n)},m(O,D){o(O,d,D),e(d,y),e(d,$),e($,g),e(d,k),e(d,z),e(z,P),e(d,A)},d(O){O&&n(d)}}}function pd(M){let d,y;return{c(){d=t("p"),y=a("Dans la prochaine partie du cours, nous examinerons des techniques plus avanc\xE9es qui peuvent vous aider \xE0 r\xE9duire votre empreinte m\xE9moire et vous permettre d\u2019affiner les plus grands mod\xE8les.")},l($){d=p($,"P",{});var g=u(d);y=l(g,"Dans la prochaine partie du cours, nous examinerons des techniques plus avanc\xE9es qui peuvent vous aider \xE0 r\xE9duire votre empreinte m\xE9moire et vous permettre d\u2019affiner les plus grands mod\xE8les."),g.forEach(n)},m($,g){o($,d,g),e(d,y)},d($){$&&n(d)}}}function ud(M){let d,y;return{c(){d=t("p"),y=a("\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes.")},l($){d=p($,"P",{});var g=u(d);y=l(g,"\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes."),g.forEach(n)},m($,g){o($,d,g),e(d,y)},d($){$&&n(d)}}}function od(M){let d,y,$,g,k;return{c(){d=t("p"),y=a("\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),$=t("code"),g=a("Trainer"),k=a(" apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet.")},l(z){d=p(z,"P",{});var P=u(d);y=l(P,"\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),$=p(P,"CODE",{});var A=u($);g=l(A,"Trainer"),A.forEach(n),k=l(P," apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet."),P.forEach(n)},m(z,P){o(z,d,P),e(d,y),e(d,$),e($,g),e(d,k)},d(z){z&&n(d)}}}function id(M){let d,y,$,g,k,z,P,A,O,D,ss,F,J,Ps,S,V,fs,vs,oe,es,xs,ns,js,C,x,ie,U,X,As,G,tn,Ds,Xt,Ll,ce,Ml,as,Qt,Hn,Yt,Wt,Rn,Zt,sp,Fl,Ts,ep,Bn,np,ap,Vl,Ss,lp,me,rp,tp,Ul,de,Gl,Os,pp,Jn,up,op,Il,pn,ip,Kl,be,Hl,un,cp,Rl,qs,Ns,Xn,he,mp,Qn,dp,Bl,on,bp,Jl,T,hp,Yn,fp,vp,Wn,jp,qp,Zn,$p,_p,sa,gp,Ep,ea,yp,zp,na,wp,kp,Xl,fe,Ql,Q,aa,Cp,Pp,la,xp,Ap,ra,Dp,Tp,Yl,ve,Wl,I,Sp,ta,Op,Np,pa,Lp,Mp,ua,Fp,Vp,Zl,Ls,Up,oa,Gp,Ip,sr,ls,Kp,ia,Hp,Rp,ca,Bp,Jp,er,je,nr,cn,Xp,ar,Ms,lr,Fs,Qp,ma,Yp,Wp,rr,qe,tr,mn,Zp,pr,$e,da,su,eu,ur,$s,Vs,ba,_e,nu,ha,au,or,Y,fa,lu,ru,va,tu,pu,ja,uu,ou,ir,rs,iu,qa,cu,mu,$a,du,bu,cr,ge,mr,Ee,dr,_,hu,_a,fu,vu,ga,ju,qu,Ea,$u,_u,ya,gu,Eu,za,yu,zu,wa,wu,ku,ka,Cu,Pu,Ca,xu,Au,Pa,Du,Tu,xa,Su,Ou,Aa,Nu,Lu,Da,Mu,Fu,Ta,Vu,Uu,br,K,Gu,Sa,Iu,Ku,Oa,Hu,Ru,Na,Bu,Ju,hr,ye,fr,dn,Xu,vr,ze,jr,W,La,Qu,Yu,Ma,Wu,Zu,Fa,so,eo,qr,we,$r,ke,_r,bn,no,gr,Ce,Er,Pe,yr,hn,ao,zr,xe,wr,Ae,kr,ts,lo,Va,ro,to,Ua,po,uo,Cr,De,Pr,Te,xr,Us,oo,Ga,io,co,Ar,Se,Dr,Oe,Tr,Gs,mo,Ia,bo,ho,Sr,_s,Is,Ka,Ne,fo,Ha,vo,Or,ps,jo,Ra,qo,$o,Ba,_o,go,Nr,Ks,Eo,Le,yo,zo,Lr,us,wo,Ja,ko,Co,Xa,Po,xo,Mr,Me,Fr,Hs,Vr,fn,Ao,Ur,Fe,Gr,Ve,Ir,vn,Do,Kr,gs,Rs,Qa,Ue,To,Ya,So,Hr,jn,Oo,Rr,Es,Bs,Wa,Ge,No,Za,Lo,Br,os,Mo,sl,Fo,Vo,el,Uo,Go,Jr,Js,Xr,ys,Xs,nl,Ie,Io,al,Ko,Qr,is,Ho,ll,Ro,Bo,rl,Jo,Xo,Yr,cs,Qo,tl,Yo,Wo,pl,Zo,si,Wr,ms,ei,ul,ni,ai,ol,li,ri,Zr,zs,Qs,il,Ke,ti,cl,pi,st,N,ui,ml,oi,ii,dl,ci,mi,bl,di,bi,hl,hi,fi,et,He,nt,qn,vi,at,Re,lt,$n,ji,rt,H,fl,qi,$i,vl,_i,gi,jl,Ei,yi,ql,zi,tt,_n,wi,pt,gn,ki,ut,En,Ci,ot,ws,Ys,$l,Be,Pi,_l,xi,it,yn,Ai,ct,ds,Di,gl,Ti,Si,El,Oi,Ni,mt,Je,dt,Ws,bt,Zs,Li,yl,Mi,Fi,ht,zn,Vi,ft,se,vt,ks,ee,zl,Xe,Ui,wl,Gi,jt,ne,Ii,kl,Ki,Hi,qt,wn,Ri,$t,kn,Bi,_t,Cs,ae,Cl,Qe,Ji,Pl,Xi,gt,le,Qi,Ye,Yi,Wi,Et,Cn,Zi,yt,R,Pn,We,sc,ec,nc,xn,Ze,ac,lc,rc,An,sn,tc,pc,uc,Dn,en,oc,ic,zt,bs,cc,xl,mc,dc,Al,bc,hc,wt;return $=new ld({props:{fw:M[0]}}),A=new B({}),J=new ad({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"}]}}),G=new B({}),ce=new nd({props:{id:"N9kO52itd0Q"}}),de=new w({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)

train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=<span class="hljs-string">&quot;adam&quot;</span>)

model.fit(train_dataset)`}}),be=new w({props:{code:"ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']",highlighted:'ValueError: No gradients provided <span class="hljs-keyword">for</span> <span class="hljs-built_in">any</span> variable: [<span class="hljs-string">&#x27;tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>]'}}),he=new B({}),fe=new w({props:{code:`for batch in train_dataset:
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>`}}),ve=new w({props:{code:`{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        ...,
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[ <span class="hljs-number">101</span>, <span class="hljs-number">2174</span>, <span class="hljs-number">1010</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3174</span>, <span class="hljs-number">2420</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">2044</span>, <span class="hljs-number">2048</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        ...,
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3398</span>, <span class="hljs-number">3398</span>, ..., <span class="hljs-number">2051</span>, <span class="hljs-number">2894</span>,  <span class="hljs-number">102</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">4124</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">2070</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>]])&gt;}`}}),je=new w({props:{code:'model.compile(optimizer="adam")',highlighted:'model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>)'}}),Ms=new Nl({props:{$$slots:{default:[rd]},$$scope:{ctx:M}}}),qe=new w({props:{code:"  246/24543 [..............................] - ETA: 15:52 - loss: nan",highlighted:'  <span class="hljs-number">246</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">52</span> - loss: nan'}}),_e=new B({}),ge=new w({props:{code:"model(batch)",highlighted:"model(batch)"}}),Ee=new w({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),ye=new w({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`}}),ze=new w({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([<span class="hljs-number">0.6844486</span> ,        nan,        nan, <span class="hljs-number">0.67127866</span>, <span class="hljs-number">0.7068601</span> ,
              nan, <span class="hljs-number">0.69309855</span>,        nan, <span class="hljs-number">0.65531296</span>,        nan,
              nan,        nan, <span class="hljs-number">0.675402</span>  ,        nan,        nan,
       <span class="hljs-number">0.69831556</span>], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">0.04761693</span>, -<span class="hljs-number">0.06509043</span>],
       [-<span class="hljs-number">0.0481936</span> , -<span class="hljs-number">0.04556257</span>],
       [-<span class="hljs-number">0.0040929</span> , -<span class="hljs-number">0.05848458</span>],
       [-<span class="hljs-number">0.02417453</span>, -<span class="hljs-number">0.0684005</span> ],
       [-<span class="hljs-number">0.02517801</span>, -<span class="hljs-number">0.05241832</span>],
       [-<span class="hljs-number">0.04514256</span>, -<span class="hljs-number">0.0757378</span> ],
       [-<span class="hljs-number">0.02656011</span>, -<span class="hljs-number">0.02646275</span>],
       [ <span class="hljs-number">0.00766164</span>, -<span class="hljs-number">0.04350497</span>],
       [ <span class="hljs-number">0.02060014</span>, -<span class="hljs-number">0.05655622</span>],
       [-<span class="hljs-number">0.02615328</span>, -<span class="hljs-number">0.0447021</span> ],
       [-<span class="hljs-number">0.05119278</span>, -<span class="hljs-number">0.06928903</span>],
       [-<span class="hljs-number">0.02859691</span>, -<span class="hljs-number">0.04879177</span>],
       [-<span class="hljs-number">0.02210129</span>, -<span class="hljs-number">0.05791225</span>],
       [-<span class="hljs-number">0.02363213</span>, -<span class="hljs-number">0.05962167</span>],
       [-<span class="hljs-number">0.05352269</span>, -<span class="hljs-number">0.0481673</span> ],
       [-<span class="hljs-number">0.08141848</span>, -<span class="hljs-number">0.07110836</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),we=new w({props:{code:`import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`}}),ke=new w({props:{code:"array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])",highlighted:'array([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>])'}}),Ce=new w({props:{code:`input_ids = batch["input_ids"].numpy()
input_ids[indices]`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
input_ids[indices]`}}),Pe=new w({props:{code:`array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])`,highlighted:`array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2032</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">16480</span>,  <span class="hljs-number">3917</span>,  <span class="hljs-number">2594</span>,  <span class="hljs-number">4135</span>,
        <span class="hljs-number">23212</span>,  <span class="hljs-number">3070</span>,  <span class="hljs-number">2214</span>, <span class="hljs-number">10170</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2012</span>,  <span class="hljs-number">4356</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">3183</span>,
         <span class="hljs-number">6838</span>, <span class="hljs-number">12953</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">6147</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2606</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">6838</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3294</span>,  <span class="hljs-number">6625</span>,  <span class="hljs-number">3773</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2214</span>,
         <span class="hljs-number">2158</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">6814</span>,  <span class="hljs-number">2016</span>,  <span class="hljs-number">2234</span>,  <span class="hljs-number">2461</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1998</span>, <span class="hljs-number">13322</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">2053</span>,  <span class="hljs-number">3382</span>,  <span class="hljs-number">2008</span>,
         <span class="hljs-number">2016</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2222</span>,  <span class="hljs-number">3046</span>,  <span class="hljs-number">8103</span>,  <span class="hljs-number">2075</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1012</span>,
          <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">3712</span>,  <span class="hljs-number">4634</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2057</span>,  <span class="hljs-number">8108</span>,
         <span class="hljs-number">2025</span>,  <span class="hljs-number">3404</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1012</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2616</span>, <span class="hljs-number">18449</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">1999</span>,
         <span class="hljs-number">1037</span>,  <span class="hljs-number">9666</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4100</span>,  <span class="hljs-number">8663</span>, <span class="hljs-number">11020</span>,  <span class="hljs-number">6313</span>,  <span class="hljs-number">2791</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2431</span>,  <span class="hljs-number">1011</span>,  <span class="hljs-number">4301</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">5177</span>,
         <span class="hljs-number">2110</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">3977</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">2832</span>,  <span class="hljs-number">2106</span>,  <span class="hljs-number">2025</span>,  <span class="hljs-number">2689</span>,  <span class="hljs-number">2104</span>,
         <span class="hljs-number">2122</span>,  <span class="hljs-number">6214</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">13090</span>,  <span class="hljs-number">5948</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2048</span>,
         <span class="hljs-number">2308</span>,  <span class="hljs-number">2006</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">5001</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2171</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">2170</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3564</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2277</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2195</span>,  <span class="hljs-number">4279</span>,  <span class="hljs-number">2191</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2181</span>,  <span class="hljs-number">2124</span>,  <span class="hljs-number">2004</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2069</span>,  <span class="hljs-number">2028</span>,
         <span class="hljs-number">2451</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2008</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2123</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1056</span>,  <span class="hljs-number">2113</span>,  <span class="hljs-number">2065</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">2428</span>, <span class="hljs-number">10654</span>,  <span class="hljs-number">7347</span>,  <span class="hljs-number">2030</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">7126</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,
         <span class="hljs-number">2291</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">5094</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,  <span class="hljs-number">2291</span>,  <span class="hljs-number">2035</span>,
         <span class="hljs-number">2105</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2029</span>,  <span class="hljs-number">3216</span>,  <span class="hljs-number">2019</span>,  <span class="hljs-number">2503</span>,  <span class="hljs-number">3444</span>,  <span class="hljs-number">1010</span>,
         <span class="hljs-number">6732</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2038</span>, <span class="hljs-number">19840</span>,  <span class="hljs-number">2098</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">9906</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2003</span>,  <span class="hljs-number">2770</span>,  <span class="hljs-number">2041</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4784</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">6732</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">9525</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">4569</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1996</span>, <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2162</span>,
         <span class="hljs-number">2252</span>,  <span class="hljs-number">5689</span>,  <span class="hljs-number">2013</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">7223</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">1996</span>,
        <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2252</span>,  <span class="hljs-number">3062</span>,  <span class="hljs-number">2000</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2598</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>, <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2049</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2025</span>,
        <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]])`}}),xe=new w({props:{code:`labels = batch['labels'].numpy()
labels[indices]`,highlighted:`labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].numpy()
labels[indices]`}}),Ae=new w({props:{code:"array([2, 2, 2, 2, 2, 2, 2, 2, 2])",highlighted:'array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),De=new w({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),Te=new w({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),Se=new w({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, <span class="hljs-attribute">num_labels</span>=3)
model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">&#x27;adam&#x27;</span>)
model.fit(train_dataset)`}}),Oe=new w({props:{code:"  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032",highlighted:'  <span class="hljs-number">869</span>/<span class="hljs-number">24543</span> [&gt;.............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">29</span> - loss: <span class="hljs-number">1.1032</span>'}}),Ne=new B({}),Me=new w({props:{code:`from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">5e-5</span>))`}}),Hs=new Nl({props:{$$slots:{default:[td]},$$scope:{ctx:M}}}),Fe=new w({props:{code:"model.fit(train_dataset)",highlighted:"model.fit(train_dataset)"}}),Ve=new w({props:{code:"319/24543 [..............................] - ETA: 16:07 - loss: 0.9718",highlighted:'<span class="hljs-number">319</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">16</span>:07 - loss: <span class="hljs-number">0.9718</span>'}}),Ue=new B({}),Ge=new B({}),Js=new Nl({props:{$$slots:{default:[pd]},$$scope:{ctx:M}}}),Ie=new B({}),Ke=new B({}),He=new w({props:{code:`input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
tokenizer.decode(input_ids[<span class="hljs-number">0</span>])`}}),Re=new w({props:{code:`labels = batch["labels"].numpy()
label = labels[0]`,highlighted:`labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
label = labels[<span class="hljs-number">0</span>]`}}),Be=new B({}),Je=new w({props:{code:`for batch in train_dataset:
    break

# Make sure you have run model.compile() and set your optimizer,
# and your loss/metrics if you're using them

model.fit(batch, epochs=20)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>

<span class="hljs-comment"># Make sure you have run model.compile() and set your optimizer,</span>
<span class="hljs-comment"># and your loss/metrics if you&#x27;re using them</span>

model.fit(batch, epochs=<span class="hljs-number">20</span>)`}}),Ws=new Nl({props:{$$slots:{default:[ud]},$$scope:{ctx:M}}}),se=new Nl({props:{warning:!0,$$slots:{default:[od]},$$scope:{ctx:M}}}),Xe=new B({}),Qe=new B({}),{c(){d=t("meta"),y=i(),b($.$$.fragment),g=i(),k=t("h1"),z=t("a"),P=t("span"),b(A.$$.fragment),O=i(),D=t("span"),ss=a("D\xE9bogage du pipeline d'entra\xEEnement"),F=i(),b(J.$$.fragment),Ps=i(),S=t("p"),V=a("Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),fs=t("em"),vs=a("finetuner"),oe=a(" un mod\xE8le sur une t\xE2che donn\xE9e, en suivant consciencieusement les conseils du "),es=t("a"),xs=a("Chapitre 7"),ns=a(". Mais lorsque vous lancez la commande "),js=t("code"),C=a("model.fit()"),x=a(", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur, mais le mod\xE8le r\xE9sultant est merdique. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),ie=i(),U=t("h2"),X=t("a"),As=t("span"),b(G.$$.fragment),tn=i(),Ds=t("span"),Xt=a("D\xE9boguer le pipeline d'entra\xEEnement"),Ll=i(),b(ce.$$.fragment),Ml=i(),as=t("p"),Qt=a("Le probl\xE8me lorsque vous rencontrez une erreur dans "),Hn=t("code"),Yt=a("trainer.train()"),Wt=a(" est qu\u2019elle peut provenir de plusieurs sources, car le "),Rn=t("code"),Zt=a("Trainer"),sp=a(" assemble g\xE9n\xE9ralement des batchs de choses. Il convertit les jeux de donn\xE9es en chargeurs de donn\xE9es, donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, il prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, il calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),Fl=i(),Ts=t("p"),ep=a("La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Bn=t("code"),np=a("trainer.train()"),ap=a(" est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),Vl=i(),Ss=t("p"),lp=a("Pour le d\xE9montrer, nous utiliserons le script suivant qui tente d\u2019ajuster un mod\xE8le DistilBERT sur le "),me=t("a"),rp=a("jeu de donn\xE9es MNLI"),tp=a(" :"),Ul=i(),b(de.$$.fragment),Gl=i(),Os=t("p"),pp=a("Si vous essayez de l\u2019ex\xE9cuter, il se peut que vous obteniez des "),Jn=t("code"),up=a("VisibleDeprecationWarning"),op=a("s lors de la conversion du jeu de donn\xE9es. Il s\u2019agit d\u2019un probl\xE8me UX connu que nous avons, donc veuillez l\u2019ignorer. Si vous lisez le cours apr\xE8s, disons, novembre 2021 et que cela se produit encore, envoyez des tweets de rage \xE0 @carrigmat jusqu\u2019\xE0 ce qu\u2019il le corrige."),Il=i(),pn=t("p"),ip=a("Le probl\xE8me le plus grave, cependant, c\u2019est que nous avons une erreur flagrante. Et c\u2019est vraiment, terriblement long :"),Kl=i(),b(be.$$.fragment),Hl=i(),un=t("p"),cp=a("Qu\u2019est-ce que cela signifie ? Nous avons essay\xE9 de nous entra\xEEner sur nos donn\xE9es, mais nous n\u2019avons pas obtenu de gradient ? C\u2019est assez d\xE9concertant ; comment commencer \xE0 d\xE9boguer quelque chose comme \xE7a ? Lorsque l\u2019erreur que vous obtenez ne sugg\xE8re pas imm\xE9diatement l\u2019origine du probl\xE8me, la meilleure solution consiste souvent \xE0 proc\xE9der par \xE9tapes, en s\u2019assurant \xE0 chaque fois que tout semble correct. Et bien s\xFBr, il faut toujours commencer par\u2026"),Rl=i(),qs=t("h3"),Ns=t("a"),Xn=t("span"),b(he.$$.fragment),mp=i(),Qn=t("span"),dp=a("V\xE9rifier vos donn\xE9es"),Bl=i(),on=t("p"),bp=a("Cela va sans dire, mais si vos donn\xE9es sont corrompues, Keras ne sera pas en mesure de les r\xE9parer pour vous. Avant toute chose, vous devez donc jeter un coup d\u2019\u0153il \xE0 ce que contient votre ensemble d\u2019entra\xEEnement."),Jl=i(),T=t("p"),hp=a("Bien qu\u2019il soit tentant de regarder dans les "),Yn=t("code"),fp=a("raw_datasets"),vp=a(" et les "),Wn=t("code"),jp=a("tokenized_datasets"),qp=a(", nous vous recommandons fortement d\u2019aller voir les donn\xE9es au moment o\xF9 elles vont entrer dans le mod\xE8le. Cela signifie lire une sortie du "),Zn=t("code"),$p=a("tf.data.Dataset"),_p=a(" que vous avez cr\xE9\xE9 avec la fonction "),sa=t("code"),gp=a("to_tf_dataset()"),Ep=a(" ! Alors comment faire ? Les objets "),ea=t("code"),yp=a("tf.data.Dataset"),zp=a(" nous donnent des batchs entiers \xE0 la fois et ne supportent pas l\u2019indexation, donc nous ne pouvons pas simplement demander "),na=t("code"),wp=a("train_dataset[0]"),kp=a(". Nous pouvons, cependant, lui demander poliment un batch :"),Xl=i(),b(fe.$$.fragment),Ql=i(),Q=t("p"),aa=t("code"),Cp=a("break"),Pp=a(" ends the loop after one iteration, so this grabs the first batch that comes out of "),la=t("code"),xp=a("train_dataset"),Ap=a(" and saves it as "),ra=t("code"),Dp=a("batch"),Tp=a(". Now, let\u2019s take a look at what\u2019s inside:"),Yl=i(),b(ve.$$.fragment),Wl=i(),I=t("p"),Sp=a("Cela semble correct, n\u2019est-ce pas ? Nous passons les "),ta=t("code"),Op=a("labels"),Np=a(", "),pa=t("code"),Lp=a("attention_mask"),Mp=a(", et "),ua=t("code"),Fp=a("input_ids"),Vp=a(" au mod\xE8le, ce qui devrait \xEAtre tout ce dont il a besoin pour calculer les sorties et la perte. Alors pourquoi n\u2019avons-nous pas de gradient ? Regardez de plus pr\xE8s : nous passons un seul dictionnaire en entr\xE9e, mais un batch d\u2019entra\xEEnement est g\xE9n\xE9ralement un tenseur ou un dictionnaire d\u2019entr\xE9e, plus un tenseur d\u2019\xE9tiquettes. Nos \xE9tiquettes sont juste une cl\xE9 dans notre dictionnaire d\u2019entr\xE9e."),Zl=i(),Ls=t("p"),Up=a("Est-ce un probl\xE8me ? Pas toujours, en fait ! Mais c\u2019est l\u2019un des probl\xE8mes les plus courants que vous rencontrerez lorsque vous entra\xEEnerez des mod\xE8les Transformer avec TensorFlow. Nos mod\xE8les peuvent tous calculer la perte en interne, mais pour ce faire, les \xE9tiquettes doivent \xEAtre transmises dans le dictionnaire d\u2019entr\xE9e. C\u2019est la perte qui est utilis\xE9e lorsque nous ne sp\xE9cifions pas de valeur de perte \xE0 "),oa=t("code"),Gp=a("compile()"),Ip=a(". Keras, d\u2019autre part, s\u2019attend g\xE9n\xE9ralement \xE0 ce que les \xE9tiquettes soient pass\xE9es s\xE9par\xE9ment du dictionnaire d\u2019entr\xE9e, et les calculs de perte \xE9choueront g\xE9n\xE9ralement si vous ne le faites pas."),sr=i(),ls=t("p"),Kp=a("Le probl\xE8me est maintenant devenu plus clair : nous avons pass\xE9 un argument "),ia=t("code"),Hp=a("loss"),Rp=a(", ce qui signifie que nous demandons \xE0 Keras de calculer les pertes pour nous, mais nous avons pass\xE9 nos \xE9tiquettes comme entr\xE9es au mod\xE8le, et non comme \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend ! Nous devons choisir l\u2019un ou l\u2019autre : soit nous utilisons la perte interne du mod\xE8le et gardons les \xE9tiquettes o\xF9 elles sont, soit nous continuons \xE0 utiliser les pertes de Keras, mais nous d\xE9pla\xE7ons les \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend. Pour simplifier, prenons la premi\xE8re approche. Changez l\u2019appel \xE0 "),ca=t("code"),Bp=a("compile()"),Jp=a(" pour lire :"),er=i(),b(je.$$.fragment),nr=i(),cn=t("p"),Xp=a("Maintenant, nous allons utiliser la perte interne du mod\xE8le, et ce probl\xE8me devrait \xEAtre r\xE9solu !"),ar=i(),b(Ms.$$.fragment),lr=i(),Fs=t("p"),Qp=a("Maintenant, essayons de nous entra\xEEner. Nous devrions obtenir des gradients maintenant, donc avec un peu de chance (la musique de mauvais augure joue ici) nous pouvons juste appeler "),ma=t("code"),Yp=a("model.fit()"),Wp=a(" et tout fonctionnera bien !"),rr=i(),b(qe.$$.fragment),tr=i(),mn=t("p"),Zp=a("Oh non."),pr=i(),$e=t("p"),da=t("code"),su=a("nan"),eu=a(" n\u2019est pas une valeur de perte tr\xE8s encourageante. Pourtant, nous avons v\xE9rifi\xE9 nos donn\xE9es, et elles semblent plut\xF4t bonnes. Si ce n\u2019est pas le probl\xE8me, quelle est la prochaine \xE9tape ? La prochaine \xE9tape \xE9vidente est de\u2026"),ur=i(),$s=t("h3"),Vs=t("a"),ba=t("span"),b(_e.$$.fragment),nu=i(),ha=t("span"),au=a("V\xE9rifier votre mod\xE8le"),or=i(),Y=t("p"),fa=t("code"),lu=a("model.fit()"),ru=a(" est une fonction tr\xE8s pratique dans Keras, mais elle fait beaucoup de choses pour vous, et cela peut rendre plus difficile de trouver exactement o\xF9 un probl\xE8me est survenu. Si vous d\xE9boguez votre mod\xE8le, une strat\xE9gie qui peut vraiment vous aider est de passer un seul batch au mod\xE8le et d\u2019examiner les sorties de ce batch en d\xE9tail. Une autre astuce vraiment utile si le mod\xE8le jette des erreurs est de "),va=t("code"),tu=a("compiler()"),pu=a(" le mod\xE8le avec "),ja=t("code"),uu=a("run_eagerly=True"),ou=a(". Cela le rendra beaucoup plus lent, mais les messages d\u2019erreur seront beaucoup plus compr\xE9hensibles, car ils indiqueront exactement o\xF9 le probl\xE8me est survenu dans le code de votre mod\xE8le."),ir=i(),rs=t("p"),iu=a("Pour l\u2019instant, cependant, nous n\u2019avons pas besoin de "),qa=t("code"),cu=a("run_eagerly"),mu=a(". Ex\xE9cutons le "),$a=t("code"),du=a("batch"),bu=a(" que nous avons obtenu pr\xE9c\xE9demment \xE0 travers le mod\xE8le et voyons \xE0 quoi ressemblent les r\xE9sultats :"),cr=i(),b(ge.$$.fragment),mr=i(),b(Ee.$$.fragment),dr=i(),_=t("p"),hu=a("Eh bien, c\u2019est d\xE9licat. Tout est \u201Cnan\u201D ! Mais c\u2019est \xE9trange, n\u2019est-ce pas ? Comment tous nos logits pourraient-ils devenir "),_a=t("code"),fu=a("nan"),vu=a(" ? \u201CNAN\u201D signifie \u201Cnot a number\u201D. Les valeurs "),ga=t("code"),ju=a("nan"),qu=a(" apparaissent souvent quand on effectue une op\xE9ration interdite, comme la division par z\xE9ro. Mais une chose tr\xE8s importante \xE0 savoir sur "),Ea=t("code"),$u=a("nan"),_u=a(" en apprentissage automatique est que cette valeur a tendance \xE0 "),ya=t("em"),gu=a("se propager"),Eu=a(". Si vous multipliez un nombre par "),za=t("code"),yu=a("nan"),zu=a(", le r\xE9sultat sera \xE9galement "),wa=t("code"),wu=a("nan"),ku=a(". Et si vous obtenez une valeur "),ka=t("code"),Cu=a("nan"),Pu=a(" n\u2019importe o\xF9 dans votre sortie, votre perte ou votre gradient, alors elle se propagera rapidement \xE0 travers tout votre mod\xE8le. Ceci parce que lorsque cette valeur "),Ca=t("code"),xu=a("nan"),Au=a(" est propag\xE9e \xE0 travers votre r\xE9seau, vous obtiendrez des gradients "),Pa=t("code"),Du=a("nan"),Tu=a(", et lorsque les mises \xE0 jour des poids sont calcul\xE9es avec ces gradients, vous obtiendrez des poids "),xa=t("code"),Su=a("nan"),Ou=a(", et ces poids calculeront encore plus de sorties "),Aa=t("code"),Nu=a("nan"),Lu=a(" ! Tr\xE8s vite, le r\xE9seau entier ne sera plus qu\u2019un gros bloc de "),Da=t("code"),Mu=a("nan"),Fu=a(". Une fois que cela arrive, il est assez difficile de voir o\xF9 le probl\xE8me a commenc\xE9. Comment peut-on isoler l\u2019endroit o\xF9 les "),Ta=t("code"),Vu=a("nan"),Uu=a(" se sont introduits en premier ?"),br=i(),K=t("p"),Gu=a("La r\xE9ponse est d\u2019essayer de "),Sa=t("em"),Iu=a("reinitialiser"),Ku=a(" notre mod\xE8le. Une fois que nous avons commenc\xE9 l\u2019entra\xEEnement, nous avons eu un "),Oa=t("code"),Hu=a("nan"),Ru=a(" quelque part et il s\u2019est rapidement propag\xE9 \xE0 travers tout le mod\xE8le. Donc, chargeons le mod\xE8le \xE0 partir d\u2019un checkpoint et ne faisons aucune mise \xE0 jour de poids, et voyons o\xF9 nous obtenons une valeur "),Na=t("code"),Bu=a("nan"),Ju=a(" :"),hr=i(),b(ye.$$.fragment),fr=i(),dn=t("p"),Xu=a("Quand on fait \xE7a, on obtient :"),vr=i(),b(ze.$$.fragment),jr=i(),W=t("p"),La=t("em"),Qu=a("Maintenant"),Yu=a(" on arrive \xE0 quelque chose ! Il n\u2019y a pas de valeurs "),Ma=t("code"),Wu=a("nan"),Zu=a(" dans nos logits, ce qui est rassurant. Mais nous voyons quelques valeurs "),Fa=t("code"),so=a("nan"),eo=a(" dans notre perte ! Y a-t-il quelque chose dans ces \xE9chantillons en particulier qui cause ce probl\xE8me ? Voyons de quels \xE9chantillons il s\u2019agit (notez que si vous ex\xE9cutez ce code vous-m\xEAme, vous pouvez obtenir des indices diff\xE9rents parce que l\u2019ensemble de donn\xE9es a \xE9t\xE9 m\xE9lang\xE9) :"),qr=i(),b(we.$$.fragment),$r=i(),b(ke.$$.fragment),_r=i(),bn=t("p"),no=a("Let\u2019s look at the samples these indices came from:"),gr=i(),b(Ce.$$.fragment),Er=i(),b(Pe.$$.fragment),yr=i(),hn=t("p"),ao=a("Il y a beaucoup de batchs ici, mais rien d\u2019inhabituel. Regardons les \xE9tiquettes :"),zr=i(),b(xe.$$.fragment),wr=i(),b(Ae.$$.fragment),kr=i(),ts=t("p"),lo=a("Ah ! Les \xE9chantillons "),Va=t("code"),ro=a("nan"),to=a(" ont tous le m\xEAme label, et c\u2019est le label 2. C\u2019est un indice tr\xE8s fort. Le fait que nous n\u2019obtenions une perte de "),Ua=t("code"),po=a("nan"),uo=a(" que lorsque notre \xE9tiquette est 2 sugg\xE8re que c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre d\u2019\xE9tiquettes dans notre mod\xE8le :"),Cr=i(),b(De.$$.fragment),Pr=i(),b(Te.$$.fragment),xr=i(),Us=t("p"),oo=a("Nous voyons maintenant le probl\xE8me : le mod\xE8le pense qu\u2019il n\u2019y a que deux classes, mais les \xE9tiquettes vont jusqu\u2019\xE0 2, ce qui signifie qu\u2019il y a en fait trois classes (car 0 est aussi une classe). C\u2019est ainsi que nous avons obtenu un "),Ga=t("code"),io=a("nan"),co=a(" - en essayant de calculer la perte pour une classe inexistante ! Essayons de changer cela et de r\xE9ajuster le mod\xE8le :"),Ar=i(),b(Se.$$.fragment),Dr=i(),b(Oe.$$.fragment),Tr=i(),Gs=t("p"),mo=a("On s\u2019entra\xEEne ! Plus de "),Ia=t("code"),bo=a("nan"),ho=a(", et nos pertes diminuent\u2026 en quelque sorte. Si vous le regardez pendant un certain temps, vous pouvez commencer \xE0 vous impatienter, car la valeur des pertes reste obstin\xE9ment \xE9lev\xE9e. Arr\xEAtons l\u2019entra\xEEnement ici et essayons de r\xE9fl\xE9chir \xE0 ce qui pourrait causer ce probl\xE8me. \xC0 ce stade, nous sommes pratiquement s\xFBrs que les donn\xE9es et le mod\xE8le sont corrects, mais notre mod\xE8le n\u2019apprend pas bien. Que reste-t-il d\u2019autre ? Il est temps de\u2026"),Sr=i(),_s=t("h3"),Is=t("a"),Ka=t("span"),b(Ne.$$.fragment),fo=i(),Ha=t("span"),vo=a("V\xE9rifier vos hyperparam\xE8tres"),Or=i(),ps=t("p"),jo=a("Si vous regardez le code ci-dessus, vous ne verrez peut-\xEAtre aucun hyperparam\xE8tre, sauf peut-\xEAtre le "),Ra=t("code"),qo=a("batch_size"),$o=a(", et cela ne semble pas \xEAtre un coupable probable. Ne soyez pas dupe, cependant ; il y a toujours des hyperparam\xE8tres, et si vous ne pouvez pas les voir, cela signifie simplement que vous ne savez pas \xE0 quoi ils sont r\xE9gl\xE9s. En particulier, souvenez-vous d\u2019une chose essentielle \xE0 propos de Keras : si vous d\xE9finissez une fonction de perte, d\u2019optimisation ou d\u2019activation avec une cha\xEEne, "),Ba=t("em"),_o=a("tous ses arguments seront d\xE9finis sur leurs valeurs par d\xE9faut"),go=a(". Cela signifie que, m\xEAme si l\u2019utilisation de cha\xEEnes de caract\xE8res est tr\xE8s pratique, vous devez \xEAtre tr\xE8s prudent, car cela peut facilement vous cacher des \xE9l\xE9ments critiques. (Toute personne essayant le d\xE9fi optionnel ci-dessus devrait prendre bonne note de ce fait)."),Nr=i(),Ks=t("p"),Eo=a("Dans ce cas, o\xF9 avons-nous d\xE9fini un argument avec une cha\xEEne ? Au d\xE9part, nous d\xE9finissions la perte avec une cha\xEEne, mais nous ne le faisons plus. Cependant, nous d\xE9finissons l\u2019optimiseur avec une cha\xEEne de caract\xE8res. Cela pourrait-il nous cacher quelque chose ? Jetons un coup d\u2019\u0153il \xE0 "),Le=t("a"),yo=a("ses arguments"),zo=a("."),Lr=i(),us=t("p"),wo=a("Y a-t-il quelque chose qui ressort ? C\u2019est exact : le taux d\u2019apprentissage ! Lorsque nous utilisons simplement la cha\xEEne "),Ja=t("code"),ko=a("'adam'"),Co=a(", nous allons obtenir le taux d\u2019apprentissage par d\xE9faut, qui est de 0.001, ou 1e-3. C\u2019est beaucoup trop \xE9lev\xE9 pour un mod\xE8le Transformer ! En g\xE9n\xE9ral, nous recommandons d\u2019essayer des taux d\u2019apprentissage entre 1e-5 et 1e-4 pour vos mod\xE8les ; c\u2019est quelque part entre 10X et 100X plus petit que la valeur que nous utilisons ici. Cela semble \xEAtre un probl\xE8me majeur, alors essayons de le r\xE9duire. Pour ce faire, nous devons importer l\u2019objet "),Xa=t("code"),Po=a("optimizer"),xo=a(". Pendant que nous y sommes, r\xE9initialisons le mod\xE8le \xE0 partir du point de contr\xF4le, au cas o\xF9 l\u2019entra\xEEnement avec un taux d\u2019apprentissage \xE9lev\xE9 aurait endommag\xE9 ses poids :"),Mr=i(),b(Me.$$.fragment),Fr=i(),b(Hs.$$.fragment),Vr=i(),fn=t("p"),Ao=a("Maintenant, nous pouvons essayer d\u2019ajuster le mod\xE8le avec le nouveau taux d\u2019apprentissage am\xE9lior\xE9 :"),Ur=i(),b(Fe.$$.fragment),Gr=i(),b(Ve.$$.fragment),Ir=i(),vn=t("p"),Do=a("Maintenant notre perte va vraiment aller quelque part ! L\u2019entra\xEEnement semble enfin fonctionner. Il y a une le\xE7on \xE0 tirer ici : lorsque votre mod\xE8le fonctionne mais que la perte ne diminue pas, et que vous \xEAtes s\xFBr que vos donn\xE9es sont correctes, c\u2019est une bonne id\xE9e de v\xE9rifier les hyperparam\xE8tres comme le taux d\u2019apprentissage et la d\xE9croissance du poids. Un r\xE9glage trop \xE9lev\xE9 de l\u2019un ou l\u2019autre de ces param\xE8tres risque fort de faire \u201D caler \u201D l\u2019entra\xEEnement \xE0 une valeur de perte \xE9lev\xE9e."),Kr=i(),gs=t("h2"),Rs=t("a"),Qa=t("span"),b(Ue.$$.fragment),To=i(),Ya=t("span"),So=a("Autres probl\xE8mes potentiels"),Hr=i(),jn=t("p"),Oo=a("Nous avons couvert les probl\xE8mes dans le script ci-dessus, mais il existe plusieurs autres erreurs courantes auxquelles vous pouvez \xEAtre confront\xE9. Jetons un coup d\u2019oeil \xE0 une liste (tr\xE8s incompl\xE8te)."),Rr=i(),Es=t("h3"),Bs=t("a"),Wa=t("span"),b(Ge.$$.fragment),No=i(),Za=t("span"),Lo=a("G\xE9rer les erreurs de manque de m\xE9moire"),Br=i(),os=t("p"),Mo=a("Le signe r\xE9v\xE9lateur d\u2019un manque de m\xE9moire est une erreur du type \u201COOM when allocating tensor\u201D \u2014 OOM est l\u2019abr\xE9viation de \u201Cout of memory\u201D. Il s\u2019agit d\u2019un risque tr\xE8s courant lorsque l\u2019on traite de grands mod\xE8les de langage. Si vous rencontrez ce probl\xE8me, une bonne strat\xE9gie consiste \xE0 diviser par deux la taille de votre batch et \xE0 r\xE9essayer. Gardez \xE0 l\u2019esprit, cependant, que certains mod\xE8les sont "),sl=t("em"),Fo=a("tr\xE8s"),Vo=a(" grands. Par exemple, le mod\xE8le GPT-2 complet poss\xE8de 1,5 Go de param\xE8tres, ce qui signifie que vous aurez besoin de 6 Go de m\xE9moire rien que pour stocker le mod\xE8le, et 6 autres Go pour ses gradients ! Entra\xEEner le mod\xE8le GPT-2 complet n\xE9cessite g\xE9n\xE9ralement plus de 20 Go de VRAM, quelle que soit la taille du batch utilis\xE9, ce dont seuls quelques GPU sont dot\xE9s. Des mod\xE8les plus l\xE9gers comme "),el=t("code"),Uo=a("distilbert-base-cased"),Go=a(" sont beaucoup plus faciles \xE0 ex\xE9cuter, et s\u2019entra\xEEnent aussi beaucoup plus rapidement."),Jr=i(),b(Js.$$.fragment),Xr=i(),ys=t("h3"),Xs=t("a"),nl=t("span"),b(Ie.$$.fragment),Io=i(),al=t("span"),Ko=a("Hungry Hungry TensorFlow \u{1F99B}"),Qr=i(),is=t("p"),Ho=a("Une bizarrerie particuli\xE8re de TensorFlow dont vous devez \xEAtre conscient est qu\u2019il s\u2019alloue "),ll=t("em"),Ro=a("toute"),Bo=a(" la m\xE9moire de votre GPU d\xE8s que vous chargez un mod\xE8le ou que vous effectuez un entra\xEEnement, puis il divise cette m\xE9moire selon les besoins. Ce comportement est diff\xE9rent de celui d\u2019autres frameworks, comme PyTorch, qui alloue la m\xE9moire selon les besoins avec CUDA plut\xF4t que de le faire en interne. L\u2019un des avantages de l\u2019approche de TensorFlow est qu\u2019elle peut souvent donner des erreurs utiles lorsque vous manquez de m\xE9moire, et qu\u2019elle peut r\xE9cup\xE9rer de cet \xE9tat sans planter tout le noyau CUDA. Mais il y a aussi un inconv\xE9nient important : si vous ex\xE9cutez deux processus TensorFlow en m\xEAme temps, alors "),rl=t("strong"),Jo=a("vous allez passer un mauvais moment"),Xo=a("."),Yr=i(),cs=t("p"),Qo=a("Si vous travaillez sur Colab, vous n\u2019avez pas \xE0 vous soucier de cela, mais si vous travaillez localement, vous devez absolument faire attention. En particulier, sachez que la fermeture d\u2019un onglet de notebook n\u2019entra\xEEne pas n\xE9cessairement la fermeture de ce "),tl=t("em"),Yo=a("notebook"),Wo=a(" ! Vous devrez peut-\xEAtre s\xE9lectionner les blocs-notes en cours d\u2019ex\xE9cution (ceux qui ont une ic\xF4ne verte) et les fermer manuellement dans la liste des r\xE9pertoires. Tout "),pl=t("em"),Zo=a("notebook"),si=a(" en cours d\u2019ex\xE9cution qui utilisait TensorFlow peut encore utiliser une grande partie de la m\xE9moire de votre GPU, ce qui signifie que tout nouveau notebook que vous d\xE9marrez peut rencontrer des probl\xE8mes tr\xE8s \xE9tranges."),Wr=i(),ms=t("p"),ei=a("Si vous commencez \xE0 obtenir des erreurs concernant CUDA, BLAS ou cuBLAS dans du code qui fonctionnait auparavant, c\u2019est tr\xE8s souvent le coupable. Vous pouvez utiliser une commande comme "),ul=t("code"),ni=a("nvidia-smi"),ai=a(" pour v\xE9rifier - quand vous \xE9teignez ou red\xE9marrez votre "),ol=t("em"),li=a("notebook"),ri=a(" actuel, est-ce que la plupart de votre m\xE9moire est libre, ou est-elle toujours utilis\xE9e ? Si elle est toujours utilis\xE9e, c\u2019est que quelque chose d\u2019autre s\u2019y accroche !"),Zr=i(),zs=t("h3"),Qs=t("a"),il=t("span"),b(Ke.$$.fragment),ti=i(),cl=t("span"),pi=a("V\xE9rifiez vos donn\xE9es (encore !)"),st=i(),N=t("p"),ui=a("Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. S\u2019il y a un bug qui corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre jeu de donn\xE9es. Un outil utile ici est "),ml=t("code"),oi=a("tokenizer.decode()"),ii=a(". Cela transformera les "),dl=t("code"),ci=a("input_ids"),mi=a(" en cha\xEEnes de caract\xE8res, afin que vous puissiez visualiser les donn\xE9es et voir si vos donn\xE9es d\u2019entra\xEEnement enseignent ce que vous voulez qu\u2019elles enseignent. Par exemple, apr\xE8s avoir obtenu un "),bl=t("code"),di=a("batch"),bi=a(" de votre "),hl=t("code"),hi=a("tf.data.Dataset"),fi=a(" comme nous l\u2019avons fait ci-dessus, vous pouvez d\xE9coder le premier \xE9l\xE9ment comme suit :"),et=i(),b(He.$$.fragment),nt=i(),qn=t("p"),vi=a("Vous pouvez ensuite la comparer avec la premi\xE8re \xE9tiquette, comme suit :"),at=i(),b(Re.$$.fragment),lt=i(),$n=t("p"),ji=a("Une fois que vous pouvez visualiser vos donn\xE9es de cette mani\xE8re, vous pouvez vous poser les questions suivantes :"),rt=i(),H=t("ul"),fl=t("li"),qi=a("les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),$i=i(),vl=t("li"),_i=a("\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),gi=i(),jl=t("li"),Ei=a("y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),yi=i(),ql=t("li"),zi=a("quelle devrait \xEAtre la perte/m\xE9trie si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),tt=i(),_n=t("p"),wi=a("Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le - si votre mod\xE8le produit des tokens, essayez aussi de les d\xE9coder ! Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, cela peut \xEAtre d\xFB au fait que votre ensemble de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification), des techniques telles que le sur\xE9chantillonnage des classes rares peuvent aider. Des techniques telles que le sur\xE9chantillonnage des classes rares peuvent donc \xEAtre utiles. D\u2019autre part, cela peut \xE9galement \xEAtre d\xFB \xE0 des probl\xE8mes d\u2019entra\xEEnement tels que de mauvais r\xE9glages des hyperparam\xE8tres."),pt=i(),gn=t("p"),ki=a("Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial avant tout entra\xEEnement est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e, car il y a probablement un bug. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),ut=i(),En=t("p"),Ci=a("Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),ot=i(),ws=t("h3"),Ys=t("a"),$l=t("span"),b(Be.$$.fragment),Pi=i(),_l=t("span"),xi=a("Surentra\xEEnement du mod\xE8le sur un seul batch"),it=i(),yn=t("p"),Ai=a("Le surentr\xE2inement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement, car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse, mais qu\u2019il se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),ct=i(),ds=t("p"),Di=a("Une fois que vous avez d\xE9fini votre "),gl=t("code"),Ti=a("mod\xE8le"),Si=a(", c\u2019est tr\xE8s facile ; il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis de traiter ce "),El=t("code"),Oi=a("batch"),Ni=a(" comme votre ensemble de donn\xE9es entier, en l\u2019ajustant sur un grand nombre d\u2019\xE9poques :"),mt=i(),b(Je.$$.fragment),dt=i(),b(Ws.$$.fragment),bt=i(),Zs=t("p"),Li=a("Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le "),yl=t("code"),Mi=a("batch"),Fi=a(", avec une perte diminuant rapidement vers 0 (ou la valeur minimale pour la perte que vous utilisez)."),ht=i(),zn=t("p"),Vi=a("Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es, et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),ft=i(),b(se.$$.fragment),vt=i(),ks=t("h3"),ee=t("a"),zl=t("span"),b(Xe.$$.fragment),Ui=i(),wl=t("span"),Gi=a("Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"),jt=i(),ne=t("p"),Ii=a("Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique, mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),kl=t("code"),Ki=a("Trainer"),Hi=a(" fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats, donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),qt=i(),wn=t("p"),Ri=a("Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 l\u2019affiner un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres, mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),$t=i(),kn=t("p"),Bi=a("Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),_t=i(),Cs=t("h3"),ae=t("a"),Cl=t("span"),b(Qe.$$.fragment),Ji=i(),Pl=t("span"),Xi=a("Demander de l'aide"),gt=i(),le=t("p"),Qi=a("Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me, mais si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),Ye=t("a"),Yi=a("forum"),Wi=a("."),Et=i(),Cn=t("p"),Zi=a("Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),yt=i(),R=t("ul"),Pn=t("li"),We=t("a"),sc=a("\u201CLa reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie\u201D"),ec=a(" par Joel Grus"),nc=i(),xn=t("li"),Ze=t("a"),ac=a("\u201CListe de contr\xF4le pour le d\xE9bogage des r\xE9seaux neuronaux\u201D"),lc=a(" par Cecelia Shao"),rc=i(),An=t("li"),sn=t("a"),tc=a("\u201CComment tester unitairement le code d\u2019apprentissage automatique\u201D"),pc=a(" par Chase Roberts"),uc=i(),Dn=t("li"),en=t("a"),oc=a("\u201CUne recette pour Entra\xEEner les r\xE9seaux neuronaux\u201D"),ic=a(" par Andrej Karpathy"),zt=i(),bs=t("p"),cc=a("Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019Entra\xEEnement des r\xE9seaux neuronaux ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),xl=t("em"),mc=a("Transformers"),dc=a(" ou \u{1F917} "),Al=t("em"),bc=a("Datasets"),hc=a(" qui ne semble pas correct, vous avez peut-\xEAtre rencontr\xE9 un bogue. Vous devez absolument nous en parler, et dans la section suivante, nous allons vous expliquer exactement comment faire."),this.h()},l(s){const r=sd('[data-svelte="svelte-1phssyn"]',document.head);d=p(r,"META",{name:!0,content:!0}),r.forEach(n),y=c(s),h($.$$.fragment,s),g=c(s),k=p(s,"H1",{class:!0});var nn=u(k);z=p(nn,"A",{id:!0,class:!0,href:!0});var Dl=u(z);P=p(Dl,"SPAN",{});var Tl=u(P);h(A.$$.fragment,Tl),Tl.forEach(n),Dl.forEach(n),O=c(nn),D=p(nn,"SPAN",{});var Sl=u(D);ss=l(Sl,"D\xE9bogage du pipeline d'entra\xEEnement"),Sl.forEach(n),nn.forEach(n),F=c(s),h(J.$$.fragment,s),Ps=c(s),S=p(s,"P",{});var Z=u(S);V=l(Z,"Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),fs=p(Z,"EM",{});var Ol=u(fs);vs=l(Ol,"finetuner"),Ol.forEach(n),oe=l(Z," un mod\xE8le sur une t\xE2che donn\xE9e, en suivant consciencieusement les conseils du "),es=p(Z,"A",{href:!0});var _c=u(es);xs=l(_c,"Chapitre 7"),_c.forEach(n),ns=l(Z,". Mais lorsque vous lancez la commande "),js=p(Z,"CODE",{});var gc=u(js);C=l(gc,"model.fit()"),gc.forEach(n),x=l(Z,", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur, mais le mod\xE8le r\xE9sultant est merdique. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),Z.forEach(n),ie=c(s),U=p(s,"H2",{class:!0});var kt=u(U);X=p(kt,"A",{id:!0,class:!0,href:!0});var Ec=u(X);As=p(Ec,"SPAN",{});var yc=u(As);h(G.$$.fragment,yc),yc.forEach(n),Ec.forEach(n),tn=c(kt),Ds=p(kt,"SPAN",{});var zc=u(Ds);Xt=l(zc,"D\xE9boguer le pipeline d'entra\xEEnement"),zc.forEach(n),kt.forEach(n),Ll=c(s),h(ce.$$.fragment,s),Ml=c(s),as=p(s,"P",{});var Tn=u(as);Qt=l(Tn,"Le probl\xE8me lorsque vous rencontrez une erreur dans "),Hn=p(Tn,"CODE",{});var wc=u(Hn);Yt=l(wc,"trainer.train()"),wc.forEach(n),Wt=l(Tn," est qu\u2019elle peut provenir de plusieurs sources, car le "),Rn=p(Tn,"CODE",{});var kc=u(Rn);Zt=l(kc,"Trainer"),kc.forEach(n),sp=l(Tn," assemble g\xE9n\xE9ralement des batchs de choses. Il convertit les jeux de donn\xE9es en chargeurs de donn\xE9es, donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, il prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, il calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),Tn.forEach(n),Fl=c(s),Ts=p(s,"P",{});var Ct=u(Ts);ep=l(Ct,"La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Bn=p(Ct,"CODE",{});var Cc=u(Bn);np=l(Cc,"trainer.train()"),Cc.forEach(n),ap=l(Ct," est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),Ct.forEach(n),Vl=c(s),Ss=p(s,"P",{});var Pt=u(Ss);lp=l(Pt,"Pour le d\xE9montrer, nous utiliserons le script suivant qui tente d\u2019ajuster un mod\xE8le DistilBERT sur le "),me=p(Pt,"A",{href:!0,rel:!0});var Pc=u(me);rp=l(Pc,"jeu de donn\xE9es MNLI"),Pc.forEach(n),tp=l(Pt," :"),Pt.forEach(n),Ul=c(s),h(de.$$.fragment,s),Gl=c(s),Os=p(s,"P",{});var xt=u(Os);pp=l(xt,"Si vous essayez de l\u2019ex\xE9cuter, il se peut que vous obteniez des "),Jn=p(xt,"CODE",{});var xc=u(Jn);up=l(xc,"VisibleDeprecationWarning"),xc.forEach(n),op=l(xt,"s lors de la conversion du jeu de donn\xE9es. Il s\u2019agit d\u2019un probl\xE8me UX connu que nous avons, donc veuillez l\u2019ignorer. Si vous lisez le cours apr\xE8s, disons, novembre 2021 et que cela se produit encore, envoyez des tweets de rage \xE0 @carrigmat jusqu\u2019\xE0 ce qu\u2019il le corrige."),xt.forEach(n),Il=c(s),pn=p(s,"P",{});var Ac=u(pn);ip=l(Ac,"Le probl\xE8me le plus grave, cependant, c\u2019est que nous avons une erreur flagrante. Et c\u2019est vraiment, terriblement long :"),Ac.forEach(n),Kl=c(s),h(be.$$.fragment,s),Hl=c(s),un=p(s,"P",{});var Dc=u(un);cp=l(Dc,"Qu\u2019est-ce que cela signifie ? Nous avons essay\xE9 de nous entra\xEEner sur nos donn\xE9es, mais nous n\u2019avons pas obtenu de gradient ? C\u2019est assez d\xE9concertant ; comment commencer \xE0 d\xE9boguer quelque chose comme \xE7a ? Lorsque l\u2019erreur que vous obtenez ne sugg\xE8re pas imm\xE9diatement l\u2019origine du probl\xE8me, la meilleure solution consiste souvent \xE0 proc\xE9der par \xE9tapes, en s\u2019assurant \xE0 chaque fois que tout semble correct. Et bien s\xFBr, il faut toujours commencer par\u2026"),Dc.forEach(n),Rl=c(s),qs=p(s,"H3",{class:!0});var At=u(qs);Ns=p(At,"A",{id:!0,class:!0,href:!0});var Tc=u(Ns);Xn=p(Tc,"SPAN",{});var Sc=u(Xn);h(he.$$.fragment,Sc),Sc.forEach(n),Tc.forEach(n),mp=c(At),Qn=p(At,"SPAN",{});var Oc=u(Qn);dp=l(Oc,"V\xE9rifier vos donn\xE9es"),Oc.forEach(n),At.forEach(n),Bl=c(s),on=p(s,"P",{});var Nc=u(on);bp=l(Nc,"Cela va sans dire, mais si vos donn\xE9es sont corrompues, Keras ne sera pas en mesure de les r\xE9parer pour vous. Avant toute chose, vous devez donc jeter un coup d\u2019\u0153il \xE0 ce que contient votre ensemble d\u2019entra\xEEnement."),Nc.forEach(n),Jl=c(s),T=p(s,"P",{});var L=u(T);hp=l(L,"Bien qu\u2019il soit tentant de regarder dans les "),Yn=p(L,"CODE",{});var Lc=u(Yn);fp=l(Lc,"raw_datasets"),Lc.forEach(n),vp=l(L," et les "),Wn=p(L,"CODE",{});var Mc=u(Wn);jp=l(Mc,"tokenized_datasets"),Mc.forEach(n),qp=l(L,", nous vous recommandons fortement d\u2019aller voir les donn\xE9es au moment o\xF9 elles vont entrer dans le mod\xE8le. Cela signifie lire une sortie du "),Zn=p(L,"CODE",{});var Fc=u(Zn);$p=l(Fc,"tf.data.Dataset"),Fc.forEach(n),_p=l(L," que vous avez cr\xE9\xE9 avec la fonction "),sa=p(L,"CODE",{});var Vc=u(sa);gp=l(Vc,"to_tf_dataset()"),Vc.forEach(n),Ep=l(L," ! Alors comment faire ? Les objets "),ea=p(L,"CODE",{});var Uc=u(ea);yp=l(Uc,"tf.data.Dataset"),Uc.forEach(n),zp=l(L," nous donnent des batchs entiers \xE0 la fois et ne supportent pas l\u2019indexation, donc nous ne pouvons pas simplement demander "),na=p(L,"CODE",{});var Gc=u(na);wp=l(Gc,"train_dataset[0]"),Gc.forEach(n),kp=l(L,". Nous pouvons, cependant, lui demander poliment un batch :"),L.forEach(n),Xl=c(s),h(fe.$$.fragment,s),Ql=c(s),Q=p(s,"P",{});var an=u(Q);aa=p(an,"CODE",{});var Ic=u(aa);Cp=l(Ic,"break"),Ic.forEach(n),Pp=l(an," ends the loop after one iteration, so this grabs the first batch that comes out of "),la=p(an,"CODE",{});var Kc=u(la);xp=l(Kc,"train_dataset"),Kc.forEach(n),Ap=l(an," and saves it as "),ra=p(an,"CODE",{});var Hc=u(ra);Dp=l(Hc,"batch"),Hc.forEach(n),Tp=l(an,". Now, let\u2019s take a look at what\u2019s inside:"),an.forEach(n),Yl=c(s),h(ve.$$.fragment,s),Wl=c(s),I=p(s,"P",{});var re=u(I);Sp=l(re,"Cela semble correct, n\u2019est-ce pas ? Nous passons les "),ta=p(re,"CODE",{});var Rc=u(ta);Op=l(Rc,"labels"),Rc.forEach(n),Np=l(re,", "),pa=p(re,"CODE",{});var Bc=u(pa);Lp=l(Bc,"attention_mask"),Bc.forEach(n),Mp=l(re,", et "),ua=p(re,"CODE",{});var Jc=u(ua);Fp=l(Jc,"input_ids"),Jc.forEach(n),Vp=l(re," au mod\xE8le, ce qui devrait \xEAtre tout ce dont il a besoin pour calculer les sorties et la perte. Alors pourquoi n\u2019avons-nous pas de gradient ? Regardez de plus pr\xE8s : nous passons un seul dictionnaire en entr\xE9e, mais un batch d\u2019entra\xEEnement est g\xE9n\xE9ralement un tenseur ou un dictionnaire d\u2019entr\xE9e, plus un tenseur d\u2019\xE9tiquettes. Nos \xE9tiquettes sont juste une cl\xE9 dans notre dictionnaire d\u2019entr\xE9e."),re.forEach(n),Zl=c(s),Ls=p(s,"P",{});var Dt=u(Ls);Up=l(Dt,"Est-ce un probl\xE8me ? Pas toujours, en fait ! Mais c\u2019est l\u2019un des probl\xE8mes les plus courants que vous rencontrerez lorsque vous entra\xEEnerez des mod\xE8les Transformer avec TensorFlow. Nos mod\xE8les peuvent tous calculer la perte en interne, mais pour ce faire, les \xE9tiquettes doivent \xEAtre transmises dans le dictionnaire d\u2019entr\xE9e. C\u2019est la perte qui est utilis\xE9e lorsque nous ne sp\xE9cifions pas de valeur de perte \xE0 "),oa=p(Dt,"CODE",{});var Xc=u(oa);Gp=l(Xc,"compile()"),Xc.forEach(n),Ip=l(Dt,". Keras, d\u2019autre part, s\u2019attend g\xE9n\xE9ralement \xE0 ce que les \xE9tiquettes soient pass\xE9es s\xE9par\xE9ment du dictionnaire d\u2019entr\xE9e, et les calculs de perte \xE9choueront g\xE9n\xE9ralement si vous ne le faites pas."),Dt.forEach(n),sr=c(s),ls=p(s,"P",{});var Sn=u(ls);Kp=l(Sn,"Le probl\xE8me est maintenant devenu plus clair : nous avons pass\xE9 un argument "),ia=p(Sn,"CODE",{});var Qc=u(ia);Hp=l(Qc,"loss"),Qc.forEach(n),Rp=l(Sn,", ce qui signifie que nous demandons \xE0 Keras de calculer les pertes pour nous, mais nous avons pass\xE9 nos \xE9tiquettes comme entr\xE9es au mod\xE8le, et non comme \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend ! Nous devons choisir l\u2019un ou l\u2019autre : soit nous utilisons la perte interne du mod\xE8le et gardons les \xE9tiquettes o\xF9 elles sont, soit nous continuons \xE0 utiliser les pertes de Keras, mais nous d\xE9pla\xE7ons les \xE9tiquettes \xE0 l\u2019endroit o\xF9 Keras les attend. Pour simplifier, prenons la premi\xE8re approche. Changez l\u2019appel \xE0 "),ca=p(Sn,"CODE",{});var Yc=u(ca);Bp=l(Yc,"compile()"),Yc.forEach(n),Jp=l(Sn," pour lire :"),Sn.forEach(n),er=c(s),h(je.$$.fragment,s),nr=c(s),cn=p(s,"P",{});var Wc=u(cn);Xp=l(Wc,"Maintenant, nous allons utiliser la perte interne du mod\xE8le, et ce probl\xE8me devrait \xEAtre r\xE9solu !"),Wc.forEach(n),ar=c(s),h(Ms.$$.fragment,s),lr=c(s),Fs=p(s,"P",{});var Tt=u(Fs);Qp=l(Tt,"Maintenant, essayons de nous entra\xEEner. Nous devrions obtenir des gradients maintenant, donc avec un peu de chance (la musique de mauvais augure joue ici) nous pouvons juste appeler "),ma=p(Tt,"CODE",{});var Zc=u(ma);Yp=l(Zc,"model.fit()"),Zc.forEach(n),Wp=l(Tt," et tout fonctionnera bien !"),Tt.forEach(n),rr=c(s),h(qe.$$.fragment,s),tr=c(s),mn=p(s,"P",{});var sm=u(mn);Zp=l(sm,"Oh non."),sm.forEach(n),pr=c(s),$e=p(s,"P",{});var fc=u($e);da=p(fc,"CODE",{});var em=u(da);su=l(em,"nan"),em.forEach(n),eu=l(fc," n\u2019est pas une valeur de perte tr\xE8s encourageante. Pourtant, nous avons v\xE9rifi\xE9 nos donn\xE9es, et elles semblent plut\xF4t bonnes. Si ce n\u2019est pas le probl\xE8me, quelle est la prochaine \xE9tape ? La prochaine \xE9tape \xE9vidente est de\u2026"),fc.forEach(n),ur=c(s),$s=p(s,"H3",{class:!0});var St=u($s);Vs=p(St,"A",{id:!0,class:!0,href:!0});var nm=u(Vs);ba=p(nm,"SPAN",{});var am=u(ba);h(_e.$$.fragment,am),am.forEach(n),nm.forEach(n),nu=c(St),ha=p(St,"SPAN",{});var lm=u(ha);au=l(lm,"V\xE9rifier votre mod\xE8le"),lm.forEach(n),St.forEach(n),or=c(s),Y=p(s,"P",{});var ln=u(Y);fa=p(ln,"CODE",{});var rm=u(fa);lu=l(rm,"model.fit()"),rm.forEach(n),ru=l(ln," est une fonction tr\xE8s pratique dans Keras, mais elle fait beaucoup de choses pour vous, et cela peut rendre plus difficile de trouver exactement o\xF9 un probl\xE8me est survenu. Si vous d\xE9boguez votre mod\xE8le, une strat\xE9gie qui peut vraiment vous aider est de passer un seul batch au mod\xE8le et d\u2019examiner les sorties de ce batch en d\xE9tail. Une autre astuce vraiment utile si le mod\xE8le jette des erreurs est de "),va=p(ln,"CODE",{});var tm=u(va);tu=l(tm,"compiler()"),tm.forEach(n),pu=l(ln," le mod\xE8le avec "),ja=p(ln,"CODE",{});var pm=u(ja);uu=l(pm,"run_eagerly=True"),pm.forEach(n),ou=l(ln,". Cela le rendra beaucoup plus lent, mais les messages d\u2019erreur seront beaucoup plus compr\xE9hensibles, car ils indiqueront exactement o\xF9 le probl\xE8me est survenu dans le code de votre mod\xE8le."),ln.forEach(n),ir=c(s),rs=p(s,"P",{});var On=u(rs);iu=l(On,"Pour l\u2019instant, cependant, nous n\u2019avons pas besoin de "),qa=p(On,"CODE",{});var um=u(qa);cu=l(um,"run_eagerly"),um.forEach(n),mu=l(On,". Ex\xE9cutons le "),$a=p(On,"CODE",{});var om=u($a);du=l(om,"batch"),om.forEach(n),bu=l(On," que nous avons obtenu pr\xE9c\xE9demment \xE0 travers le mod\xE8le et voyons \xE0 quoi ressemblent les r\xE9sultats :"),On.forEach(n),cr=c(s),h(ge.$$.fragment,s),mr=c(s),h(Ee.$$.fragment,s),dr=c(s),_=p(s,"P",{});var E=u(_);hu=l(E,"Eh bien, c\u2019est d\xE9licat. Tout est \u201Cnan\u201D ! Mais c\u2019est \xE9trange, n\u2019est-ce pas ? Comment tous nos logits pourraient-ils devenir "),_a=p(E,"CODE",{});var im=u(_a);fu=l(im,"nan"),im.forEach(n),vu=l(E," ? \u201CNAN\u201D signifie \u201Cnot a number\u201D. Les valeurs "),ga=p(E,"CODE",{});var cm=u(ga);ju=l(cm,"nan"),cm.forEach(n),qu=l(E," apparaissent souvent quand on effectue une op\xE9ration interdite, comme la division par z\xE9ro. Mais une chose tr\xE8s importante \xE0 savoir sur "),Ea=p(E,"CODE",{});var mm=u(Ea);$u=l(mm,"nan"),mm.forEach(n),_u=l(E," en apprentissage automatique est que cette valeur a tendance \xE0 "),ya=p(E,"EM",{});var dm=u(ya);gu=l(dm,"se propager"),dm.forEach(n),Eu=l(E,". Si vous multipliez un nombre par "),za=p(E,"CODE",{});var bm=u(za);yu=l(bm,"nan"),bm.forEach(n),zu=l(E,", le r\xE9sultat sera \xE9galement "),wa=p(E,"CODE",{});var hm=u(wa);wu=l(hm,"nan"),hm.forEach(n),ku=l(E,". Et si vous obtenez une valeur "),ka=p(E,"CODE",{});var fm=u(ka);Cu=l(fm,"nan"),fm.forEach(n),Pu=l(E," n\u2019importe o\xF9 dans votre sortie, votre perte ou votre gradient, alors elle se propagera rapidement \xE0 travers tout votre mod\xE8le. Ceci parce que lorsque cette valeur "),Ca=p(E,"CODE",{});var vm=u(Ca);xu=l(vm,"nan"),vm.forEach(n),Au=l(E," est propag\xE9e \xE0 travers votre r\xE9seau, vous obtiendrez des gradients "),Pa=p(E,"CODE",{});var jm=u(Pa);Du=l(jm,"nan"),jm.forEach(n),Tu=l(E,", et lorsque les mises \xE0 jour des poids sont calcul\xE9es avec ces gradients, vous obtiendrez des poids "),xa=p(E,"CODE",{});var qm=u(xa);Su=l(qm,"nan"),qm.forEach(n),Ou=l(E,", et ces poids calculeront encore plus de sorties "),Aa=p(E,"CODE",{});var $m=u(Aa);Nu=l($m,"nan"),$m.forEach(n),Lu=l(E," ! Tr\xE8s vite, le r\xE9seau entier ne sera plus qu\u2019un gros bloc de "),Da=p(E,"CODE",{});var _m=u(Da);Mu=l(_m,"nan"),_m.forEach(n),Fu=l(E,". Une fois que cela arrive, il est assez difficile de voir o\xF9 le probl\xE8me a commenc\xE9. Comment peut-on isoler l\u2019endroit o\xF9 les "),Ta=p(E,"CODE",{});var gm=u(Ta);Vu=l(gm,"nan"),gm.forEach(n),Uu=l(E," se sont introduits en premier ?"),E.forEach(n),br=c(s),K=p(s,"P",{});var te=u(K);Gu=l(te,"La r\xE9ponse est d\u2019essayer de "),Sa=p(te,"EM",{});var Em=u(Sa);Iu=l(Em,"reinitialiser"),Em.forEach(n),Ku=l(te," notre mod\xE8le. Une fois que nous avons commenc\xE9 l\u2019entra\xEEnement, nous avons eu un "),Oa=p(te,"CODE",{});var ym=u(Oa);Hu=l(ym,"nan"),ym.forEach(n),Ru=l(te," quelque part et il s\u2019est rapidement propag\xE9 \xE0 travers tout le mod\xE8le. Donc, chargeons le mod\xE8le \xE0 partir d\u2019un checkpoint et ne faisons aucune mise \xE0 jour de poids, et voyons o\xF9 nous obtenons une valeur "),Na=p(te,"CODE",{});var zm=u(Na);Bu=l(zm,"nan"),zm.forEach(n),Ju=l(te," :"),te.forEach(n),hr=c(s),h(ye.$$.fragment,s),fr=c(s),dn=p(s,"P",{});var wm=u(dn);Xu=l(wm,"Quand on fait \xE7a, on obtient :"),wm.forEach(n),vr=c(s),h(ze.$$.fragment,s),jr=c(s),W=p(s,"P",{});var rn=u(W);La=p(rn,"EM",{});var km=u(La);Qu=l(km,"Maintenant"),km.forEach(n),Yu=l(rn," on arrive \xE0 quelque chose ! Il n\u2019y a pas de valeurs "),Ma=p(rn,"CODE",{});var Cm=u(Ma);Wu=l(Cm,"nan"),Cm.forEach(n),Zu=l(rn," dans nos logits, ce qui est rassurant. Mais nous voyons quelques valeurs "),Fa=p(rn,"CODE",{});var Pm=u(Fa);so=l(Pm,"nan"),Pm.forEach(n),eo=l(rn," dans notre perte ! Y a-t-il quelque chose dans ces \xE9chantillons en particulier qui cause ce probl\xE8me ? Voyons de quels \xE9chantillons il s\u2019agit (notez que si vous ex\xE9cutez ce code vous-m\xEAme, vous pouvez obtenir des indices diff\xE9rents parce que l\u2019ensemble de donn\xE9es a \xE9t\xE9 m\xE9lang\xE9) :"),rn.forEach(n),qr=c(s),h(we.$$.fragment,s),$r=c(s),h(ke.$$.fragment,s),_r=c(s),bn=p(s,"P",{});var xm=u(bn);no=l(xm,"Let\u2019s look at the samples these indices came from:"),xm.forEach(n),gr=c(s),h(Ce.$$.fragment,s),Er=c(s),h(Pe.$$.fragment,s),yr=c(s),hn=p(s,"P",{});var Am=u(hn);ao=l(Am,"Il y a beaucoup de batchs ici, mais rien d\u2019inhabituel. Regardons les \xE9tiquettes :"),Am.forEach(n),zr=c(s),h(xe.$$.fragment,s),wr=c(s),h(Ae.$$.fragment,s),kr=c(s),ts=p(s,"P",{});var Nn=u(ts);lo=l(Nn,"Ah ! Les \xE9chantillons "),Va=p(Nn,"CODE",{});var Dm=u(Va);ro=l(Dm,"nan"),Dm.forEach(n),to=l(Nn," ont tous le m\xEAme label, et c\u2019est le label 2. C\u2019est un indice tr\xE8s fort. Le fait que nous n\u2019obtenions une perte de "),Ua=p(Nn,"CODE",{});var Tm=u(Ua);po=l(Tm,"nan"),Tm.forEach(n),uo=l(Nn," que lorsque notre \xE9tiquette est 2 sugg\xE8re que c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre d\u2019\xE9tiquettes dans notre mod\xE8le :"),Nn.forEach(n),Cr=c(s),h(De.$$.fragment,s),Pr=c(s),h(Te.$$.fragment,s),xr=c(s),Us=p(s,"P",{});var Ot=u(Us);oo=l(Ot,"Nous voyons maintenant le probl\xE8me : le mod\xE8le pense qu\u2019il n\u2019y a que deux classes, mais les \xE9tiquettes vont jusqu\u2019\xE0 2, ce qui signifie qu\u2019il y a en fait trois classes (car 0 est aussi une classe). C\u2019est ainsi que nous avons obtenu un "),Ga=p(Ot,"CODE",{});var Sm=u(Ga);io=l(Sm,"nan"),Sm.forEach(n),co=l(Ot," - en essayant de calculer la perte pour une classe inexistante ! Essayons de changer cela et de r\xE9ajuster le mod\xE8le :"),Ot.forEach(n),Ar=c(s),h(Se.$$.fragment,s),Dr=c(s),h(Oe.$$.fragment,s),Tr=c(s),Gs=p(s,"P",{});var Nt=u(Gs);mo=l(Nt,"On s\u2019entra\xEEne ! Plus de "),Ia=p(Nt,"CODE",{});var Om=u(Ia);bo=l(Om,"nan"),Om.forEach(n),ho=l(Nt,", et nos pertes diminuent\u2026 en quelque sorte. Si vous le regardez pendant un certain temps, vous pouvez commencer \xE0 vous impatienter, car la valeur des pertes reste obstin\xE9ment \xE9lev\xE9e. Arr\xEAtons l\u2019entra\xEEnement ici et essayons de r\xE9fl\xE9chir \xE0 ce qui pourrait causer ce probl\xE8me. \xC0 ce stade, nous sommes pratiquement s\xFBrs que les donn\xE9es et le mod\xE8le sont corrects, mais notre mod\xE8le n\u2019apprend pas bien. Que reste-t-il d\u2019autre ? Il est temps de\u2026"),Nt.forEach(n),Sr=c(s),_s=p(s,"H3",{class:!0});var Lt=u(_s);Is=p(Lt,"A",{id:!0,class:!0,href:!0});var Nm=u(Is);Ka=p(Nm,"SPAN",{});var Lm=u(Ka);h(Ne.$$.fragment,Lm),Lm.forEach(n),Nm.forEach(n),fo=c(Lt),Ha=p(Lt,"SPAN",{});var Mm=u(Ha);vo=l(Mm,"V\xE9rifier vos hyperparam\xE8tres"),Mm.forEach(n),Lt.forEach(n),Or=c(s),ps=p(s,"P",{});var Ln=u(ps);jo=l(Ln,"Si vous regardez le code ci-dessus, vous ne verrez peut-\xEAtre aucun hyperparam\xE8tre, sauf peut-\xEAtre le "),Ra=p(Ln,"CODE",{});var Fm=u(Ra);qo=l(Fm,"batch_size"),Fm.forEach(n),$o=l(Ln,", et cela ne semble pas \xEAtre un coupable probable. Ne soyez pas dupe, cependant ; il y a toujours des hyperparam\xE8tres, et si vous ne pouvez pas les voir, cela signifie simplement que vous ne savez pas \xE0 quoi ils sont r\xE9gl\xE9s. En particulier, souvenez-vous d\u2019une chose essentielle \xE0 propos de Keras : si vous d\xE9finissez une fonction de perte, d\u2019optimisation ou d\u2019activation avec une cha\xEEne, "),Ba=p(Ln,"EM",{});var Vm=u(Ba);_o=l(Vm,"tous ses arguments seront d\xE9finis sur leurs valeurs par d\xE9faut"),Vm.forEach(n),go=l(Ln,". Cela signifie que, m\xEAme si l\u2019utilisation de cha\xEEnes de caract\xE8res est tr\xE8s pratique, vous devez \xEAtre tr\xE8s prudent, car cela peut facilement vous cacher des \xE9l\xE9ments critiques. (Toute personne essayant le d\xE9fi optionnel ci-dessus devrait prendre bonne note de ce fait)."),Ln.forEach(n),Nr=c(s),Ks=p(s,"P",{});var Mt=u(Ks);Eo=l(Mt,"Dans ce cas, o\xF9 avons-nous d\xE9fini un argument avec une cha\xEEne ? Au d\xE9part, nous d\xE9finissions la perte avec une cha\xEEne, mais nous ne le faisons plus. Cependant, nous d\xE9finissons l\u2019optimiseur avec une cha\xEEne de caract\xE8res. Cela pourrait-il nous cacher quelque chose ? Jetons un coup d\u2019\u0153il \xE0 "),Le=p(Mt,"A",{href:!0,rel:!0});var Um=u(Le);yo=l(Um,"ses arguments"),Um.forEach(n),zo=l(Mt,"."),Mt.forEach(n),Lr=c(s),us=p(s,"P",{});var Mn=u(us);wo=l(Mn,"Y a-t-il quelque chose qui ressort ? C\u2019est exact : le taux d\u2019apprentissage ! Lorsque nous utilisons simplement la cha\xEEne "),Ja=p(Mn,"CODE",{});var Gm=u(Ja);ko=l(Gm,"'adam'"),Gm.forEach(n),Co=l(Mn,", nous allons obtenir le taux d\u2019apprentissage par d\xE9faut, qui est de 0.001, ou 1e-3. C\u2019est beaucoup trop \xE9lev\xE9 pour un mod\xE8le Transformer ! En g\xE9n\xE9ral, nous recommandons d\u2019essayer des taux d\u2019apprentissage entre 1e-5 et 1e-4 pour vos mod\xE8les ; c\u2019est quelque part entre 10X et 100X plus petit que la valeur que nous utilisons ici. Cela semble \xEAtre un probl\xE8me majeur, alors essayons de le r\xE9duire. Pour ce faire, nous devons importer l\u2019objet "),Xa=p(Mn,"CODE",{});var Im=u(Xa);Po=l(Im,"optimizer"),Im.forEach(n),xo=l(Mn,". Pendant que nous y sommes, r\xE9initialisons le mod\xE8le \xE0 partir du point de contr\xF4le, au cas o\xF9 l\u2019entra\xEEnement avec un taux d\u2019apprentissage \xE9lev\xE9 aurait endommag\xE9 ses poids :"),Mn.forEach(n),Mr=c(s),h(Me.$$.fragment,s),Fr=c(s),h(Hs.$$.fragment,s),Vr=c(s),fn=p(s,"P",{});var Km=u(fn);Ao=l(Km,"Maintenant, nous pouvons essayer d\u2019ajuster le mod\xE8le avec le nouveau taux d\u2019apprentissage am\xE9lior\xE9 :"),Km.forEach(n),Ur=c(s),h(Fe.$$.fragment,s),Gr=c(s),h(Ve.$$.fragment,s),Ir=c(s),vn=p(s,"P",{});var Hm=u(vn);Do=l(Hm,"Maintenant notre perte va vraiment aller quelque part ! L\u2019entra\xEEnement semble enfin fonctionner. Il y a une le\xE7on \xE0 tirer ici : lorsque votre mod\xE8le fonctionne mais que la perte ne diminue pas, et que vous \xEAtes s\xFBr que vos donn\xE9es sont correctes, c\u2019est une bonne id\xE9e de v\xE9rifier les hyperparam\xE8tres comme le taux d\u2019apprentissage et la d\xE9croissance du poids. Un r\xE9glage trop \xE9lev\xE9 de l\u2019un ou l\u2019autre de ces param\xE8tres risque fort de faire \u201D caler \u201D l\u2019entra\xEEnement \xE0 une valeur de perte \xE9lev\xE9e."),Hm.forEach(n),Kr=c(s),gs=p(s,"H2",{class:!0});var Ft=u(gs);Rs=p(Ft,"A",{id:!0,class:!0,href:!0});var Rm=u(Rs);Qa=p(Rm,"SPAN",{});var Bm=u(Qa);h(Ue.$$.fragment,Bm),Bm.forEach(n),Rm.forEach(n),To=c(Ft),Ya=p(Ft,"SPAN",{});var Jm=u(Ya);So=l(Jm,"Autres probl\xE8mes potentiels"),Jm.forEach(n),Ft.forEach(n),Hr=c(s),jn=p(s,"P",{});var Xm=u(jn);Oo=l(Xm,"Nous avons couvert les probl\xE8mes dans le script ci-dessus, mais il existe plusieurs autres erreurs courantes auxquelles vous pouvez \xEAtre confront\xE9. Jetons un coup d\u2019oeil \xE0 une liste (tr\xE8s incompl\xE8te)."),Xm.forEach(n),Rr=c(s),Es=p(s,"H3",{class:!0});var Vt=u(Es);Bs=p(Vt,"A",{id:!0,class:!0,href:!0});var Qm=u(Bs);Wa=p(Qm,"SPAN",{});var Ym=u(Wa);h(Ge.$$.fragment,Ym),Ym.forEach(n),Qm.forEach(n),No=c(Vt),Za=p(Vt,"SPAN",{});var Wm=u(Za);Lo=l(Wm,"G\xE9rer les erreurs de manque de m\xE9moire"),Wm.forEach(n),Vt.forEach(n),Br=c(s),os=p(s,"P",{});var Fn=u(os);Mo=l(Fn,"Le signe r\xE9v\xE9lateur d\u2019un manque de m\xE9moire est une erreur du type \u201COOM when allocating tensor\u201D \u2014 OOM est l\u2019abr\xE9viation de \u201Cout of memory\u201D. Il s\u2019agit d\u2019un risque tr\xE8s courant lorsque l\u2019on traite de grands mod\xE8les de langage. Si vous rencontrez ce probl\xE8me, une bonne strat\xE9gie consiste \xE0 diviser par deux la taille de votre batch et \xE0 r\xE9essayer. Gardez \xE0 l\u2019esprit, cependant, que certains mod\xE8les sont "),sl=p(Fn,"EM",{});var Zm=u(sl);Fo=l(Zm,"tr\xE8s"),Zm.forEach(n),Vo=l(Fn," grands. Par exemple, le mod\xE8le GPT-2 complet poss\xE8de 1,5 Go de param\xE8tres, ce qui signifie que vous aurez besoin de 6 Go de m\xE9moire rien que pour stocker le mod\xE8le, et 6 autres Go pour ses gradients ! Entra\xEEner le mod\xE8le GPT-2 complet n\xE9cessite g\xE9n\xE9ralement plus de 20 Go de VRAM, quelle que soit la taille du batch utilis\xE9, ce dont seuls quelques GPU sont dot\xE9s. Des mod\xE8les plus l\xE9gers comme "),el=p(Fn,"CODE",{});var s0=u(el);Uo=l(s0,"distilbert-base-cased"),s0.forEach(n),Go=l(Fn," sont beaucoup plus faciles \xE0 ex\xE9cuter, et s\u2019entra\xEEnent aussi beaucoup plus rapidement."),Fn.forEach(n),Jr=c(s),h(Js.$$.fragment,s),Xr=c(s),ys=p(s,"H3",{class:!0});var Ut=u(ys);Xs=p(Ut,"A",{id:!0,class:!0,href:!0});var e0=u(Xs);nl=p(e0,"SPAN",{});var n0=u(nl);h(Ie.$$.fragment,n0),n0.forEach(n),e0.forEach(n),Io=c(Ut),al=p(Ut,"SPAN",{});var a0=u(al);Ko=l(a0,"Hungry Hungry TensorFlow \u{1F99B}"),a0.forEach(n),Ut.forEach(n),Qr=c(s),is=p(s,"P",{});var Vn=u(is);Ho=l(Vn,"Une bizarrerie particuli\xE8re de TensorFlow dont vous devez \xEAtre conscient est qu\u2019il s\u2019alloue "),ll=p(Vn,"EM",{});var l0=u(ll);Ro=l(l0,"toute"),l0.forEach(n),Bo=l(Vn," la m\xE9moire de votre GPU d\xE8s que vous chargez un mod\xE8le ou que vous effectuez un entra\xEEnement, puis il divise cette m\xE9moire selon les besoins. Ce comportement est diff\xE9rent de celui d\u2019autres frameworks, comme PyTorch, qui alloue la m\xE9moire selon les besoins avec CUDA plut\xF4t que de le faire en interne. L\u2019un des avantages de l\u2019approche de TensorFlow est qu\u2019elle peut souvent donner des erreurs utiles lorsque vous manquez de m\xE9moire, et qu\u2019elle peut r\xE9cup\xE9rer de cet \xE9tat sans planter tout le noyau CUDA. Mais il y a aussi un inconv\xE9nient important : si vous ex\xE9cutez deux processus TensorFlow en m\xEAme temps, alors "),rl=p(Vn,"STRONG",{});var r0=u(rl);Jo=l(r0,"vous allez passer un mauvais moment"),r0.forEach(n),Xo=l(Vn,"."),Vn.forEach(n),Yr=c(s),cs=p(s,"P",{});var Un=u(cs);Qo=l(Un,"Si vous travaillez sur Colab, vous n\u2019avez pas \xE0 vous soucier de cela, mais si vous travaillez localement, vous devez absolument faire attention. En particulier, sachez que la fermeture d\u2019un onglet de notebook n\u2019entra\xEEne pas n\xE9cessairement la fermeture de ce "),tl=p(Un,"EM",{});var t0=u(tl);Yo=l(t0,"notebook"),t0.forEach(n),Wo=l(Un," ! Vous devrez peut-\xEAtre s\xE9lectionner les blocs-notes en cours d\u2019ex\xE9cution (ceux qui ont une ic\xF4ne verte) et les fermer manuellement dans la liste des r\xE9pertoires. Tout "),pl=p(Un,"EM",{});var p0=u(pl);Zo=l(p0,"notebook"),p0.forEach(n),si=l(Un," en cours d\u2019ex\xE9cution qui utilisait TensorFlow peut encore utiliser une grande partie de la m\xE9moire de votre GPU, ce qui signifie que tout nouveau notebook que vous d\xE9marrez peut rencontrer des probl\xE8mes tr\xE8s \xE9tranges."),Un.forEach(n),Wr=c(s),ms=p(s,"P",{});var Gn=u(ms);ei=l(Gn,"Si vous commencez \xE0 obtenir des erreurs concernant CUDA, BLAS ou cuBLAS dans du code qui fonctionnait auparavant, c\u2019est tr\xE8s souvent le coupable. Vous pouvez utiliser une commande comme "),ul=p(Gn,"CODE",{});var u0=u(ul);ni=l(u0,"nvidia-smi"),u0.forEach(n),ai=l(Gn," pour v\xE9rifier - quand vous \xE9teignez ou red\xE9marrez votre "),ol=p(Gn,"EM",{});var o0=u(ol);li=l(o0,"notebook"),o0.forEach(n),ri=l(Gn," actuel, est-ce que la plupart de votre m\xE9moire est libre, ou est-elle toujours utilis\xE9e ? Si elle est toujours utilis\xE9e, c\u2019est que quelque chose d\u2019autre s\u2019y accroche !"),Gn.forEach(n),Zr=c(s),zs=p(s,"H3",{class:!0});var Gt=u(zs);Qs=p(Gt,"A",{id:!0,class:!0,href:!0});var i0=u(Qs);il=p(i0,"SPAN",{});var c0=u(il);h(Ke.$$.fragment,c0),c0.forEach(n),i0.forEach(n),ti=c(Gt),cl=p(Gt,"SPAN",{});var m0=u(cl);pi=l(m0,"V\xE9rifiez vos donn\xE9es (encore !)"),m0.forEach(n),Gt.forEach(n),st=c(s),N=p(s,"P",{});var hs=u(N);ui=l(hs,"Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. S\u2019il y a un bug qui corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre jeu de donn\xE9es. Un outil utile ici est "),ml=p(hs,"CODE",{});var d0=u(ml);oi=l(d0,"tokenizer.decode()"),d0.forEach(n),ii=l(hs,". Cela transformera les "),dl=p(hs,"CODE",{});var b0=u(dl);ci=l(b0,"input_ids"),b0.forEach(n),mi=l(hs," en cha\xEEnes de caract\xE8res, afin que vous puissiez visualiser les donn\xE9es et voir si vos donn\xE9es d\u2019entra\xEEnement enseignent ce que vous voulez qu\u2019elles enseignent. Par exemple, apr\xE8s avoir obtenu un "),bl=p(hs,"CODE",{});var h0=u(bl);di=l(h0,"batch"),h0.forEach(n),bi=l(hs," de votre "),hl=p(hs,"CODE",{});var f0=u(hl);hi=l(f0,"tf.data.Dataset"),f0.forEach(n),fi=l(hs," comme nous l\u2019avons fait ci-dessus, vous pouvez d\xE9coder le premier \xE9l\xE9ment comme suit :"),hs.forEach(n),et=c(s),h(He.$$.fragment,s),nt=c(s),qn=p(s,"P",{});var v0=u(qn);vi=l(v0,"Vous pouvez ensuite la comparer avec la premi\xE8re \xE9tiquette, comme suit :"),v0.forEach(n),at=c(s),h(Re.$$.fragment,s),lt=c(s),$n=p(s,"P",{});var j0=u($n);ji=l(j0,"Une fois que vous pouvez visualiser vos donn\xE9es de cette mani\xE8re, vous pouvez vous poser les questions suivantes :"),j0.forEach(n),rt=c(s),H=p(s,"UL",{});var pe=u(H);fl=p(pe,"LI",{});var q0=u(fl);qi=l(q0,"les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),q0.forEach(n),$i=c(pe),vl=p(pe,"LI",{});var $0=u(vl);_i=l($0,"\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),$0.forEach(n),gi=c(pe),jl=p(pe,"LI",{});var _0=u(jl);Ei=l(_0,"y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),_0.forEach(n),yi=c(pe),ql=p(pe,"LI",{});var g0=u(ql);zi=l(g0,"quelle devrait \xEAtre la perte/m\xE9trie si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),g0.forEach(n),pe.forEach(n),tt=c(s),_n=p(s,"P",{});var E0=u(_n);wi=l(E0,"Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le - si votre mod\xE8le produit des tokens, essayez aussi de les d\xE9coder ! Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, cela peut \xEAtre d\xFB au fait que votre ensemble de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification), des techniques telles que le sur\xE9chantillonnage des classes rares peuvent aider. Des techniques telles que le sur\xE9chantillonnage des classes rares peuvent donc \xEAtre utiles. D\u2019autre part, cela peut \xE9galement \xEAtre d\xFB \xE0 des probl\xE8mes d\u2019entra\xEEnement tels que de mauvais r\xE9glages des hyperparam\xE8tres."),E0.forEach(n),pt=c(s),gn=p(s,"P",{});var y0=u(gn);ki=l(y0,"Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial avant tout entra\xEEnement est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e, car il y a probablement un bug. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),y0.forEach(n),ut=c(s),En=p(s,"P",{});var z0=u(En);Ci=l(z0,"Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),z0.forEach(n),ot=c(s),ws=p(s,"H3",{class:!0});var It=u(ws);Ys=p(It,"A",{id:!0,class:!0,href:!0});var w0=u(Ys);$l=p(w0,"SPAN",{});var k0=u($l);h(Be.$$.fragment,k0),k0.forEach(n),w0.forEach(n),Pi=c(It),_l=p(It,"SPAN",{});var C0=u(_l);xi=l(C0,"Surentra\xEEnement du mod\xE8le sur un seul batch"),C0.forEach(n),It.forEach(n),it=c(s),yn=p(s,"P",{});var P0=u(yn);Ai=l(P0,"Le surentr\xE2inement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement, car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse, mais qu\u2019il se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),P0.forEach(n),ct=c(s),ds=p(s,"P",{});var In=u(ds);Di=l(In,"Une fois que vous avez d\xE9fini votre "),gl=p(In,"CODE",{});var x0=u(gl);Ti=l(x0,"mod\xE8le"),x0.forEach(n),Si=l(In,", c\u2019est tr\xE8s facile ; il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis de traiter ce "),El=p(In,"CODE",{});var A0=u(El);Oi=l(A0,"batch"),A0.forEach(n),Ni=l(In," comme votre ensemble de donn\xE9es entier, en l\u2019ajustant sur un grand nombre d\u2019\xE9poques :"),In.forEach(n),mt=c(s),h(Je.$$.fragment,s),dt=c(s),h(Ws.$$.fragment,s),bt=c(s),Zs=p(s,"P",{});var Kt=u(Zs);Li=l(Kt,"Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le "),yl=p(Kt,"CODE",{});var D0=u(yl);Mi=l(D0,"batch"),D0.forEach(n),Fi=l(Kt,", avec une perte diminuant rapidement vers 0 (ou la valeur minimale pour la perte que vous utilisez)."),Kt.forEach(n),ht=c(s),zn=p(s,"P",{});var T0=u(zn);Vi=l(T0,"Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es, et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),T0.forEach(n),ft=c(s),h(se.$$.fragment,s),vt=c(s),ks=p(s,"H3",{class:!0});var Ht=u(ks);ee=p(Ht,"A",{id:!0,class:!0,href:!0});var S0=u(ee);zl=p(S0,"SPAN",{});var O0=u(zl);h(Xe.$$.fragment,O0),O0.forEach(n),S0.forEach(n),Ui=c(Ht),wl=p(Ht,"SPAN",{});var N0=u(wl);Gi=l(N0,"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"),N0.forEach(n),Ht.forEach(n),jt=c(s),ne=p(s,"P",{});var Rt=u(ne);Ii=l(Rt,"Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique, mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),kl=p(Rt,"CODE",{});var L0=u(kl);Ki=l(L0,"Trainer"),L0.forEach(n),Hi=l(Rt," fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats, donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),Rt.forEach(n),qt=c(s),wn=p(s,"P",{});var M0=u(wn);Ri=l(M0,"Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 l\u2019affiner un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres, mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),M0.forEach(n),$t=c(s),kn=p(s,"P",{});var F0=u(kn);Bi=l(F0,"Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),F0.forEach(n),_t=c(s),Cs=p(s,"H3",{class:!0});var Bt=u(Cs);ae=p(Bt,"A",{id:!0,class:!0,href:!0});var V0=u(ae);Cl=p(V0,"SPAN",{});var U0=u(Cl);h(Qe.$$.fragment,U0),U0.forEach(n),V0.forEach(n),Ji=c(Bt),Pl=p(Bt,"SPAN",{});var G0=u(Pl);Xi=l(G0,"Demander de l'aide"),G0.forEach(n),Bt.forEach(n),gt=c(s),le=p(s,"P",{});var Jt=u(le);Qi=l(Jt,"Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me, mais si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),Ye=p(Jt,"A",{href:!0,rel:!0});var I0=u(Ye);Yi=l(I0,"forum"),I0.forEach(n),Wi=l(Jt,"."),Jt.forEach(n),Et=c(s),Cn=p(s,"P",{});var K0=u(Cn);Zi=l(K0,"Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),K0.forEach(n),yt=c(s),R=p(s,"UL",{});var ue=u(R);Pn=p(ue,"LI",{});var vc=u(Pn);We=p(vc,"A",{href:!0,rel:!0});var H0=u(We);sc=l(H0,"\u201CLa reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie\u201D"),H0.forEach(n),ec=l(vc," par Joel Grus"),vc.forEach(n),nc=c(ue),xn=p(ue,"LI",{});var jc=u(xn);Ze=p(jc,"A",{href:!0,rel:!0});var R0=u(Ze);ac=l(R0,"\u201CListe de contr\xF4le pour le d\xE9bogage des r\xE9seaux neuronaux\u201D"),R0.forEach(n),lc=l(jc," par Cecelia Shao"),jc.forEach(n),rc=c(ue),An=p(ue,"LI",{});var qc=u(An);sn=p(qc,"A",{href:!0,rel:!0});var B0=u(sn);tc=l(B0,"\u201CComment tester unitairement le code d\u2019apprentissage automatique\u201D"),B0.forEach(n),pc=l(qc," par Chase Roberts"),qc.forEach(n),uc=c(ue),Dn=p(ue,"LI",{});var $c=u(Dn);en=p($c,"A",{href:!0,rel:!0});var J0=u(en);oc=l(J0,"\u201CUne recette pour Entra\xEEner les r\xE9seaux neuronaux\u201D"),J0.forEach(n),ic=l($c," par Andrej Karpathy"),$c.forEach(n),ue.forEach(n),zt=c(s),bs=p(s,"P",{});var Kn=u(bs);cc=l(Kn,"Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019Entra\xEEnement des r\xE9seaux neuronaux ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),xl=p(Kn,"EM",{});var X0=u(xl);mc=l(X0,"Transformers"),X0.forEach(n),dc=l(Kn," ou \u{1F917} "),Al=p(Kn,"EM",{});var Q0=u(Al);bc=l(Q0,"Datasets"),Q0.forEach(n),hc=l(Kn," qui ne semble pas correct, vous avez peut-\xEAtre rencontr\xE9 un bogue. Vous devez absolument nous en parler, et dans la section suivante, nous allons vous expliquer exactement comment faire."),Kn.forEach(n),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(cd)),m(z,"id","dbogage-du-pipeline-dentranement"),m(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(z,"href","#dbogage-du-pipeline-dentranement"),m(k,"class","relative group"),m(es,"href","/course/fr/chapter7"),m(X,"id","dboguer-le-pipeline-dentranement"),m(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(X,"href","#dboguer-le-pipeline-dentranement"),m(U,"class","relative group"),m(me,"href","https://huggingface.co/datasets/glue"),m(me,"rel","nofollow"),m(Ns,"id","vrifier-vos-donnes"),m(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ns,"href","#vrifier-vos-donnes"),m(qs,"class","relative group"),m(Vs,"id","vrifier-votre-modle"),m(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Vs,"href","#vrifier-votre-modle"),m($s,"class","relative group"),m(Is,"id","vrifier-vos-hyperparamtres"),m(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Is,"href","#vrifier-vos-hyperparamtres"),m(_s,"class","relative group"),m(Le,"href","https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"),m(Le,"rel","nofollow"),m(Rs,"id","autres-problmes-potentiels"),m(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Rs,"href","#autres-problmes-potentiels"),m(gs,"class","relative group"),m(Bs,"id","grer-les-erreurs-de-manque-de-mmoire"),m(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Bs,"href","#grer-les-erreurs-de-manque-de-mmoire"),m(Es,"class","relative group"),m(Xs,"id","hungry-hungry-tensorflow"),m(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Xs,"href","#hungry-hungry-tensorflow"),m(ys,"class","relative group"),m(Qs,"id","vrifiez-vos-donnes-encore"),m(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Qs,"href","#vrifiez-vos-donnes-encore"),m(zs,"class","relative group"),m(Ys,"id","surentranement-du-modle-sur-un-seul-batch"),m(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ys,"href","#surentranement-du-modle-sur-un-seul-batch"),m(ws,"class","relative group"),m(ee,"id","ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),m(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ee,"href","#ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),m(ks,"class","relative group"),m(ae,"id","demander-de-laide"),m(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ae,"href","#demander-de-laide"),m(Cs,"class","relative group"),m(Ye,"href","https://discuss.huggingface.co/"),m(Ye,"rel","nofollow"),m(We,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),m(We,"rel","nofollow"),m(Ze,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),m(Ze,"rel","nofollow"),m(sn,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),m(sn,"rel","nofollow"),m(en,"href","http://karpathy.github.io/2019/04/25/recipe/"),m(en,"rel","nofollow")},m(s,r){e(document.head,d),o(s,y,r),f($,s,r),o(s,g,r),o(s,k,r),e(k,z),e(z,P),f(A,P,null),e(k,O),e(k,D),e(D,ss),o(s,F,r),f(J,s,r),o(s,Ps,r),o(s,S,r),e(S,V),e(S,fs),e(fs,vs),e(S,oe),e(S,es),e(es,xs),e(S,ns),e(S,js),e(js,C),e(S,x),o(s,ie,r),o(s,U,r),e(U,X),e(X,As),f(G,As,null),e(U,tn),e(U,Ds),e(Ds,Xt),o(s,Ll,r),f(ce,s,r),o(s,Ml,r),o(s,as,r),e(as,Qt),e(as,Hn),e(Hn,Yt),e(as,Wt),e(as,Rn),e(Rn,Zt),e(as,sp),o(s,Fl,r),o(s,Ts,r),e(Ts,ep),e(Ts,Bn),e(Bn,np),e(Ts,ap),o(s,Vl,r),o(s,Ss,r),e(Ss,lp),e(Ss,me),e(me,rp),e(Ss,tp),o(s,Ul,r),f(de,s,r),o(s,Gl,r),o(s,Os,r),e(Os,pp),e(Os,Jn),e(Jn,up),e(Os,op),o(s,Il,r),o(s,pn,r),e(pn,ip),o(s,Kl,r),f(be,s,r),o(s,Hl,r),o(s,un,r),e(un,cp),o(s,Rl,r),o(s,qs,r),e(qs,Ns),e(Ns,Xn),f(he,Xn,null),e(qs,mp),e(qs,Qn),e(Qn,dp),o(s,Bl,r),o(s,on,r),e(on,bp),o(s,Jl,r),o(s,T,r),e(T,hp),e(T,Yn),e(Yn,fp),e(T,vp),e(T,Wn),e(Wn,jp),e(T,qp),e(T,Zn),e(Zn,$p),e(T,_p),e(T,sa),e(sa,gp),e(T,Ep),e(T,ea),e(ea,yp),e(T,zp),e(T,na),e(na,wp),e(T,kp),o(s,Xl,r),f(fe,s,r),o(s,Ql,r),o(s,Q,r),e(Q,aa),e(aa,Cp),e(Q,Pp),e(Q,la),e(la,xp),e(Q,Ap),e(Q,ra),e(ra,Dp),e(Q,Tp),o(s,Yl,r),f(ve,s,r),o(s,Wl,r),o(s,I,r),e(I,Sp),e(I,ta),e(ta,Op),e(I,Np),e(I,pa),e(pa,Lp),e(I,Mp),e(I,ua),e(ua,Fp),e(I,Vp),o(s,Zl,r),o(s,Ls,r),e(Ls,Up),e(Ls,oa),e(oa,Gp),e(Ls,Ip),o(s,sr,r),o(s,ls,r),e(ls,Kp),e(ls,ia),e(ia,Hp),e(ls,Rp),e(ls,ca),e(ca,Bp),e(ls,Jp),o(s,er,r),f(je,s,r),o(s,nr,r),o(s,cn,r),e(cn,Xp),o(s,ar,r),f(Ms,s,r),o(s,lr,r),o(s,Fs,r),e(Fs,Qp),e(Fs,ma),e(ma,Yp),e(Fs,Wp),o(s,rr,r),f(qe,s,r),o(s,tr,r),o(s,mn,r),e(mn,Zp),o(s,pr,r),o(s,$e,r),e($e,da),e(da,su),e($e,eu),o(s,ur,r),o(s,$s,r),e($s,Vs),e(Vs,ba),f(_e,ba,null),e($s,nu),e($s,ha),e(ha,au),o(s,or,r),o(s,Y,r),e(Y,fa),e(fa,lu),e(Y,ru),e(Y,va),e(va,tu),e(Y,pu),e(Y,ja),e(ja,uu),e(Y,ou),o(s,ir,r),o(s,rs,r),e(rs,iu),e(rs,qa),e(qa,cu),e(rs,mu),e(rs,$a),e($a,du),e(rs,bu),o(s,cr,r),f(ge,s,r),o(s,mr,r),f(Ee,s,r),o(s,dr,r),o(s,_,r),e(_,hu),e(_,_a),e(_a,fu),e(_,vu),e(_,ga),e(ga,ju),e(_,qu),e(_,Ea),e(Ea,$u),e(_,_u),e(_,ya),e(ya,gu),e(_,Eu),e(_,za),e(za,yu),e(_,zu),e(_,wa),e(wa,wu),e(_,ku),e(_,ka),e(ka,Cu),e(_,Pu),e(_,Ca),e(Ca,xu),e(_,Au),e(_,Pa),e(Pa,Du),e(_,Tu),e(_,xa),e(xa,Su),e(_,Ou),e(_,Aa),e(Aa,Nu),e(_,Lu),e(_,Da),e(Da,Mu),e(_,Fu),e(_,Ta),e(Ta,Vu),e(_,Uu),o(s,br,r),o(s,K,r),e(K,Gu),e(K,Sa),e(Sa,Iu),e(K,Ku),e(K,Oa),e(Oa,Hu),e(K,Ru),e(K,Na),e(Na,Bu),e(K,Ju),o(s,hr,r),f(ye,s,r),o(s,fr,r),o(s,dn,r),e(dn,Xu),o(s,vr,r),f(ze,s,r),o(s,jr,r),o(s,W,r),e(W,La),e(La,Qu),e(W,Yu),e(W,Ma),e(Ma,Wu),e(W,Zu),e(W,Fa),e(Fa,so),e(W,eo),o(s,qr,r),f(we,s,r),o(s,$r,r),f(ke,s,r),o(s,_r,r),o(s,bn,r),e(bn,no),o(s,gr,r),f(Ce,s,r),o(s,Er,r),f(Pe,s,r),o(s,yr,r),o(s,hn,r),e(hn,ao),o(s,zr,r),f(xe,s,r),o(s,wr,r),f(Ae,s,r),o(s,kr,r),o(s,ts,r),e(ts,lo),e(ts,Va),e(Va,ro),e(ts,to),e(ts,Ua),e(Ua,po),e(ts,uo),o(s,Cr,r),f(De,s,r),o(s,Pr,r),f(Te,s,r),o(s,xr,r),o(s,Us,r),e(Us,oo),e(Us,Ga),e(Ga,io),e(Us,co),o(s,Ar,r),f(Se,s,r),o(s,Dr,r),f(Oe,s,r),o(s,Tr,r),o(s,Gs,r),e(Gs,mo),e(Gs,Ia),e(Ia,bo),e(Gs,ho),o(s,Sr,r),o(s,_s,r),e(_s,Is),e(Is,Ka),f(Ne,Ka,null),e(_s,fo),e(_s,Ha),e(Ha,vo),o(s,Or,r),o(s,ps,r),e(ps,jo),e(ps,Ra),e(Ra,qo),e(ps,$o),e(ps,Ba),e(Ba,_o),e(ps,go),o(s,Nr,r),o(s,Ks,r),e(Ks,Eo),e(Ks,Le),e(Le,yo),e(Ks,zo),o(s,Lr,r),o(s,us,r),e(us,wo),e(us,Ja),e(Ja,ko),e(us,Co),e(us,Xa),e(Xa,Po),e(us,xo),o(s,Mr,r),f(Me,s,r),o(s,Fr,r),f(Hs,s,r),o(s,Vr,r),o(s,fn,r),e(fn,Ao),o(s,Ur,r),f(Fe,s,r),o(s,Gr,r),f(Ve,s,r),o(s,Ir,r),o(s,vn,r),e(vn,Do),o(s,Kr,r),o(s,gs,r),e(gs,Rs),e(Rs,Qa),f(Ue,Qa,null),e(gs,To),e(gs,Ya),e(Ya,So),o(s,Hr,r),o(s,jn,r),e(jn,Oo),o(s,Rr,r),o(s,Es,r),e(Es,Bs),e(Bs,Wa),f(Ge,Wa,null),e(Es,No),e(Es,Za),e(Za,Lo),o(s,Br,r),o(s,os,r),e(os,Mo),e(os,sl),e(sl,Fo),e(os,Vo),e(os,el),e(el,Uo),e(os,Go),o(s,Jr,r),f(Js,s,r),o(s,Xr,r),o(s,ys,r),e(ys,Xs),e(Xs,nl),f(Ie,nl,null),e(ys,Io),e(ys,al),e(al,Ko),o(s,Qr,r),o(s,is,r),e(is,Ho),e(is,ll),e(ll,Ro),e(is,Bo),e(is,rl),e(rl,Jo),e(is,Xo),o(s,Yr,r),o(s,cs,r),e(cs,Qo),e(cs,tl),e(tl,Yo),e(cs,Wo),e(cs,pl),e(pl,Zo),e(cs,si),o(s,Wr,r),o(s,ms,r),e(ms,ei),e(ms,ul),e(ul,ni),e(ms,ai),e(ms,ol),e(ol,li),e(ms,ri),o(s,Zr,r),o(s,zs,r),e(zs,Qs),e(Qs,il),f(Ke,il,null),e(zs,ti),e(zs,cl),e(cl,pi),o(s,st,r),o(s,N,r),e(N,ui),e(N,ml),e(ml,oi),e(N,ii),e(N,dl),e(dl,ci),e(N,mi),e(N,bl),e(bl,di),e(N,bi),e(N,hl),e(hl,hi),e(N,fi),o(s,et,r),f(He,s,r),o(s,nt,r),o(s,qn,r),e(qn,vi),o(s,at,r),f(Re,s,r),o(s,lt,r),o(s,$n,r),e($n,ji),o(s,rt,r),o(s,H,r),e(H,fl),e(fl,qi),e(H,$i),e(H,vl),e(vl,_i),e(H,gi),e(H,jl),e(jl,Ei),e(H,yi),e(H,ql),e(ql,zi),o(s,tt,r),o(s,_n,r),e(_n,wi),o(s,pt,r),o(s,gn,r),e(gn,ki),o(s,ut,r),o(s,En,r),e(En,Ci),o(s,ot,r),o(s,ws,r),e(ws,Ys),e(Ys,$l),f(Be,$l,null),e(ws,Pi),e(ws,_l),e(_l,xi),o(s,it,r),o(s,yn,r),e(yn,Ai),o(s,ct,r),o(s,ds,r),e(ds,Di),e(ds,gl),e(gl,Ti),e(ds,Si),e(ds,El),e(El,Oi),e(ds,Ni),o(s,mt,r),f(Je,s,r),o(s,dt,r),f(Ws,s,r),o(s,bt,r),o(s,Zs,r),e(Zs,Li),e(Zs,yl),e(yl,Mi),e(Zs,Fi),o(s,ht,r),o(s,zn,r),e(zn,Vi),o(s,ft,r),f(se,s,r),o(s,vt,r),o(s,ks,r),e(ks,ee),e(ee,zl),f(Xe,zl,null),e(ks,Ui),e(ks,wl),e(wl,Gi),o(s,jt,r),o(s,ne,r),e(ne,Ii),e(ne,kl),e(kl,Ki),e(ne,Hi),o(s,qt,r),o(s,wn,r),e(wn,Ri),o(s,$t,r),o(s,kn,r),e(kn,Bi),o(s,_t,r),o(s,Cs,r),e(Cs,ae),e(ae,Cl),f(Qe,Cl,null),e(Cs,Ji),e(Cs,Pl),e(Pl,Xi),o(s,gt,r),o(s,le,r),e(le,Qi),e(le,Ye),e(Ye,Yi),e(le,Wi),o(s,Et,r),o(s,Cn,r),e(Cn,Zi),o(s,yt,r),o(s,R,r),e(R,Pn),e(Pn,We),e(We,sc),e(Pn,ec),e(R,nc),e(R,xn),e(xn,Ze),e(Ze,ac),e(xn,lc),e(R,rc),e(R,An),e(An,sn),e(sn,tc),e(An,pc),e(R,uc),e(R,Dn),e(Dn,en),e(en,oc),e(Dn,ic),o(s,zt,r),o(s,bs,r),e(bs,cc),e(bs,xl),e(xl,mc),e(bs,dc),e(bs,Al),e(Al,bc),e(bs,hc),wt=!0},p(s,[r]){const nn={};r&1&&(nn.fw=s[0]),$.$set(nn);const Dl={};r&2&&(Dl.$$scope={dirty:r,ctx:s}),Ms.$set(Dl);const Tl={};r&2&&(Tl.$$scope={dirty:r,ctx:s}),Hs.$set(Tl);const Sl={};r&2&&(Sl.$$scope={dirty:r,ctx:s}),Js.$set(Sl);const Z={};r&2&&(Z.$$scope={dirty:r,ctx:s}),Ws.$set(Z);const Ol={};r&2&&(Ol.$$scope={dirty:r,ctx:s}),se.$set(Ol)},i(s){wt||(v($.$$.fragment,s),v(A.$$.fragment,s),v(J.$$.fragment,s),v(G.$$.fragment,s),v(ce.$$.fragment,s),v(de.$$.fragment,s),v(be.$$.fragment,s),v(he.$$.fragment,s),v(fe.$$.fragment,s),v(ve.$$.fragment,s),v(je.$$.fragment,s),v(Ms.$$.fragment,s),v(qe.$$.fragment,s),v(_e.$$.fragment,s),v(ge.$$.fragment,s),v(Ee.$$.fragment,s),v(ye.$$.fragment,s),v(ze.$$.fragment,s),v(we.$$.fragment,s),v(ke.$$.fragment,s),v(Ce.$$.fragment,s),v(Pe.$$.fragment,s),v(xe.$$.fragment,s),v(Ae.$$.fragment,s),v(De.$$.fragment,s),v(Te.$$.fragment,s),v(Se.$$.fragment,s),v(Oe.$$.fragment,s),v(Ne.$$.fragment,s),v(Me.$$.fragment,s),v(Hs.$$.fragment,s),v(Fe.$$.fragment,s),v(Ve.$$.fragment,s),v(Ue.$$.fragment,s),v(Ge.$$.fragment,s),v(Js.$$.fragment,s),v(Ie.$$.fragment,s),v(Ke.$$.fragment,s),v(He.$$.fragment,s),v(Re.$$.fragment,s),v(Be.$$.fragment,s),v(Je.$$.fragment,s),v(Ws.$$.fragment,s),v(se.$$.fragment,s),v(Xe.$$.fragment,s),v(Qe.$$.fragment,s),wt=!0)},o(s){j($.$$.fragment,s),j(A.$$.fragment,s),j(J.$$.fragment,s),j(G.$$.fragment,s),j(ce.$$.fragment,s),j(de.$$.fragment,s),j(be.$$.fragment,s),j(he.$$.fragment,s),j(fe.$$.fragment,s),j(ve.$$.fragment,s),j(je.$$.fragment,s),j(Ms.$$.fragment,s),j(qe.$$.fragment,s),j(_e.$$.fragment,s),j(ge.$$.fragment,s),j(Ee.$$.fragment,s),j(ye.$$.fragment,s),j(ze.$$.fragment,s),j(we.$$.fragment,s),j(ke.$$.fragment,s),j(Ce.$$.fragment,s),j(Pe.$$.fragment,s),j(xe.$$.fragment,s),j(Ae.$$.fragment,s),j(De.$$.fragment,s),j(Te.$$.fragment,s),j(Se.$$.fragment,s),j(Oe.$$.fragment,s),j(Ne.$$.fragment,s),j(Me.$$.fragment,s),j(Hs.$$.fragment,s),j(Fe.$$.fragment,s),j(Ve.$$.fragment,s),j(Ue.$$.fragment,s),j(Ge.$$.fragment,s),j(Js.$$.fragment,s),j(Ie.$$.fragment,s),j(Ke.$$.fragment,s),j(He.$$.fragment,s),j(Re.$$.fragment,s),j(Be.$$.fragment,s),j(Je.$$.fragment,s),j(Ws.$$.fragment,s),j(se.$$.fragment,s),j(Xe.$$.fragment,s),j(Qe.$$.fragment,s),wt=!1},d(s){n(d),s&&n(y),q($,s),s&&n(g),s&&n(k),q(A),s&&n(F),q(J,s),s&&n(Ps),s&&n(S),s&&n(ie),s&&n(U),q(G),s&&n(Ll),q(ce,s),s&&n(Ml),s&&n(as),s&&n(Fl),s&&n(Ts),s&&n(Vl),s&&n(Ss),s&&n(Ul),q(de,s),s&&n(Gl),s&&n(Os),s&&n(Il),s&&n(pn),s&&n(Kl),q(be,s),s&&n(Hl),s&&n(un),s&&n(Rl),s&&n(qs),q(he),s&&n(Bl),s&&n(on),s&&n(Jl),s&&n(T),s&&n(Xl),q(fe,s),s&&n(Ql),s&&n(Q),s&&n(Yl),q(ve,s),s&&n(Wl),s&&n(I),s&&n(Zl),s&&n(Ls),s&&n(sr),s&&n(ls),s&&n(er),q(je,s),s&&n(nr),s&&n(cn),s&&n(ar),q(Ms,s),s&&n(lr),s&&n(Fs),s&&n(rr),q(qe,s),s&&n(tr),s&&n(mn),s&&n(pr),s&&n($e),s&&n(ur),s&&n($s),q(_e),s&&n(or),s&&n(Y),s&&n(ir),s&&n(rs),s&&n(cr),q(ge,s),s&&n(mr),q(Ee,s),s&&n(dr),s&&n(_),s&&n(br),s&&n(K),s&&n(hr),q(ye,s),s&&n(fr),s&&n(dn),s&&n(vr),q(ze,s),s&&n(jr),s&&n(W),s&&n(qr),q(we,s),s&&n($r),q(ke,s),s&&n(_r),s&&n(bn),s&&n(gr),q(Ce,s),s&&n(Er),q(Pe,s),s&&n(yr),s&&n(hn),s&&n(zr),q(xe,s),s&&n(wr),q(Ae,s),s&&n(kr),s&&n(ts),s&&n(Cr),q(De,s),s&&n(Pr),q(Te,s),s&&n(xr),s&&n(Us),s&&n(Ar),q(Se,s),s&&n(Dr),q(Oe,s),s&&n(Tr),s&&n(Gs),s&&n(Sr),s&&n(_s),q(Ne),s&&n(Or),s&&n(ps),s&&n(Nr),s&&n(Ks),s&&n(Lr),s&&n(us),s&&n(Mr),q(Me,s),s&&n(Fr),q(Hs,s),s&&n(Vr),s&&n(fn),s&&n(Ur),q(Fe,s),s&&n(Gr),q(Ve,s),s&&n(Ir),s&&n(vn),s&&n(Kr),s&&n(gs),q(Ue),s&&n(Hr),s&&n(jn),s&&n(Rr),s&&n(Es),q(Ge),s&&n(Br),s&&n(os),s&&n(Jr),q(Js,s),s&&n(Xr),s&&n(ys),q(Ie),s&&n(Qr),s&&n(is),s&&n(Yr),s&&n(cs),s&&n(Wr),s&&n(ms),s&&n(Zr),s&&n(zs),q(Ke),s&&n(st),s&&n(N),s&&n(et),q(He,s),s&&n(nt),s&&n(qn),s&&n(at),q(Re,s),s&&n(lt),s&&n($n),s&&n(rt),s&&n(H),s&&n(tt),s&&n(_n),s&&n(pt),s&&n(gn),s&&n(ut),s&&n(En),s&&n(ot),s&&n(ws),q(Be),s&&n(it),s&&n(yn),s&&n(ct),s&&n(ds),s&&n(mt),q(Je,s),s&&n(dt),q(Ws,s),s&&n(bt),s&&n(Zs),s&&n(ht),s&&n(zn),s&&n(ft),q(se,s),s&&n(vt),s&&n(ks),q(Xe),s&&n(jt),s&&n(ne),s&&n(qt),s&&n(wn),s&&n($t),s&&n(kn),s&&n(_t),s&&n(Cs),q(Qe),s&&n(gt),s&&n(le),s&&n(Et),s&&n(Cn),s&&n(yt),s&&n(R),s&&n(zt),s&&n(bs)}}}const cd={local:"dbogage-du-pipeline-dentranement",sections:[{local:"dboguer-le-pipeline-dentranement",sections:[{local:"vrifier-vos-donnes",title:"V\xE9rifier vos donn\xE9es"},{local:"vrifier-votre-modle",title:"V\xE9rifier votre mod\xE8le"},{local:"vrifier-vos-hyperparamtres",title:"V\xE9rifier vos hyperparam\xE8tres"}],title:"D\xE9boguer le pipeline d'entra\xEEnement"},{local:"autres-problmes-potentiels",sections:[{local:"grer-les-erreurs-de-manque-de-mmoire",title:"G\xE9rer les erreurs de manque de m\xE9moire"},{local:"hungry-hungry-tensorflow",title:"Hungry Hungry TensorFlow \u{1F99B}"},{local:"vrifiez-vos-donnes-encore",title:"V\xE9rifiez vos donn\xE9es (encore !)"},{local:"surentranement-du-modle-sur-un-seul-batch",title:"Surentra\xEEnement du mod\xE8le sur un seul batch"},{local:"ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base",title:"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base"},{local:"demander-de-laide",title:"Demander de l'aide"}],title:"Autres probl\xE8mes potentiels "}],title:"D\xE9bogage du pipeline d'entra\xEEnement"};function md(M,d,y){let $="pt";return ed(()=>{const g=new URLSearchParams(window.location.search);y(0,$=g.get("fw")||"pt")}),[$]}class $d extends Y0{constructor(d){super();W0(this,d,md,id,Z0,{})}}export{$d as default,cd as metadata};
