import{S as Eo,i as yo,s as Po,e as l,k as m,w as $,t as i,l as _o,M as To,c as p,d as s,m as d,x as q,a as u,h as c,b as j,G as a,g as n,y as v,o as h,p as jo,q as b,B as g,v as xo,n as wo}from"../../chunks/vendor-hf-doc-builder.js";import{I as Qs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as zo}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Io}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Ao(z){let r,f;return r=new zo({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"}]}}),{c(){$(r.$$.fragment)},l(t){q(r.$$.fragment,t)},m(t,k){v(r,t,k),f=!0},i(t){f||(b(r.$$.fragment,t),f=!0)},o(t){h(r.$$.fragment,t),f=!1},d(t){g(r,t)}}}function So(z){let r,f;return r=new zo({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"}]}}),{c(){$(r.$$.fragment)},l(t){q(r.$$.fragment,t)},m(t,k){v(r,t,k),f=!0},i(t){f||(b(r.$$.fragment,t),f=!0)},o(t){h(r.$$.fragment,t),f=!1},d(t){g(r,t)}}}function Co(z){let r,f;return r=new w({props:{code:`
`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
output = model(**tokens)`}}),{c(){$(r.$$.fragment)},l(t){q(r.$$.fragment,t)},m(t,k){v(r,t,k),f=!0},i(t){f||(b(r.$$.fragment,t),f=!0)},o(t){h(r.$$.fragment,t),f=!1},d(t){g(r,t)}}}function Do(z){let r,f;return r=new w({props:{code:`
`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
output = model(**tokens)`}}),{c(){$(r.$$.fragment)},l(t){q(r.$$.fragment,t)},m(t,k){v(r,t,k),f=!0},i(t){f||(b(r.$$.fragment,t),f=!0)},o(t){h(r.$$.fragment,t),f=!1},d(t){g(r,t)}}}function Fo(z){let r,f,t,k,S,F,be,B,is,$e,ps,Ae,E,y,ae,te,cs,Se,N,us,qe,ms,ds,Ce,M,De,x,fs,ve,hs,bs,ge,$s,qs,Fe,ne,vs,Ne,L,Oe,re,gs,He,G,Re,le,ks,Be,U,Me,ie,_s,Le,Q,Ge,_,js,ke,ws,zs,_e,Es,ys,je,Ps,Ts,we,xs,Is,Ue,V,Qe,C,O,ze,J,As,Ee,Ss,Ve,pe,Cs,Je,K,Ke,W,We,ce,Ds,Xe,X,Ye,Y,Ze,I,Fs,ye,Ns,Os,Pe,Hs,Rs,es,D,H,Te,Z,Bs,xe,Ms,ss,R,Ls,Ie,Gs,Us,os,P,T,ue,as;t=new Io({props:{fw:z[0]}}),B=new Qs({});const Vs=[So,Ao],ee=[];function Js(e,o){return e[0]==="pt"?0:1}E=Js(z),y=ee[E]=Vs[E](z),M=new w({props:{code:`

`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),L=new w({props:{code:"",highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),G=new w({props:{code:"",highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

model_inputs = tokenizer(sequences)`}}),U=new w({props:{code:`
`,highlighted:`<span class="hljs-comment"># Ir\xE1 preencher as sequ\xEAncias at\xE9 o comprimento m\xE1ximo da sequ\xEAncia</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;longest&quot;</span>)

<span class="hljs-comment"># Ir\xE1 preencher as sequ\xEAncias at\xE9 o comprimento m\xE1ximo do modelo</span>
<span class="hljs-comment"># (512 para o modelo BERT ou DistilBERT)</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>)

<span class="hljs-comment"># Ir\xE1 preencher as sequ\xEAncias at\xE9 o comprimento m\xE1ximo especificado</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">8</span>)`}}),Q=new w({props:{code:`
`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># Ir\xE1 preencher as sequ\xEAncias at\xE9 o comprimento m\xE1ximo do modelo</span>
<span class="hljs-comment"># (512 para o modelo BERT ou DistilBERT)</span>
model_inputs = tokenizer(sequences, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Truncar\xE1 as sequ\xEAncias que s\xE3o mais longas do que o comprimento m\xE1ximo especificado</span>
model_inputs = tokenizer(sequences, max_length=<span class="hljs-number">8</span>, truncation=<span class="hljs-literal">True</span>)`}}),V=new w({props:{code:`

`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># Retorna tensores PyTorch</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Retorna tensores TensorFlow</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-comment"># Retorna NumPy arrays</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)`}}),J=new Qs({}),K=new w({props:{code:`
`,highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)
<span class="hljs-built_in">print</span>(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
<span class="hljs-built_in">print</span>(ids)`}}),W=new w({props:{code:`[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]`,highlighted:`[<span class="hljs-number">101</span>, <span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>]
[<span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>]`}}),X=new w({props:{code:`print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))`,highlighted:`<span class="hljs-built_in">print</span>(tokenizer.decode(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))
<span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),Y=new w({props:{code:`"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."`,highlighted:`<span class="hljs-string">&quot;[CLS] i&#x27;ve been waiting for a huggingface course my whole life. [SEP]&quot;</span>
<span class="hljs-string">&quot;i&#x27;ve been waiting for a huggingface course my whole life.&quot;</span>`}}),Z=new Qs({});const Ks=[Do,Co],se=[];function Ws(e,o){return e[0]==="pt"?0:1}return P=Ws(z),T=se[P]=Ks[P](z),{c(){r=l("meta"),f=m(),$(t.$$.fragment),k=m(),S=l("h1"),F=l("a"),be=l("span"),$(B.$$.fragment),is=m(),$e=l("span"),ps=i("Colocando tudo junto"),Ae=m(),y.c(),ae=m(),te=l("p"),cs=i("Nas \xFAltimas se\xE7\xF5es, temos feito o nosso melhor para fazer a maior parte do trabalho \xE0 m\xE3o. Exploramos como funcionam os tokenizers e analisamos a tokeniza\xE7\xE3o, convers\xE3o para IDs de entrada, padding, truncagem e m\xE1scaras de aten\xE7\xE3o."),Se=m(),N=l("p"),us=i("Entretanto, como vimos na se\xE7\xE3o 2, a API dos \u{1F917} Transformers pode tratar de tudo isso para n\xF3s com uma fun\xE7\xE3o de alto n\xEDvel, na qual mergulharemos aqui. Quando voc\xEA chama seu "),qe=l("code"),ms=i("tokenizer"),ds=i(" diretamente na frase, voc\xEA recebe de volta entradas que est\xE3o prontas para passar pelo seu modelo:"),Ce=m(),$(M.$$.fragment),De=m(),x=l("p"),fs=i("Aqui, a vari\xE1vel "),ve=l("code"),hs=i("model_inputs"),bs=i(" cont\xE9m tudo o que \xE9 necess\xE1rio para que um modelo funcione bem. Para DistilBERT, isso inclui os IDs de entrada, bem como a m\xE1scara de aten\xE7\xE3o. Outros modelos que aceitam entradas adicionais tamb\xE9m ter\xE3o essas sa\xEDdas pelo objeto "),ge=l("code"),$s=i("tokenizer"),qs=i("."),Fe=m(),ne=l("p"),vs=i("Como veremos em alguns exemplos abaixo, este m\xE9todo \xE9 muito poderoso. Primeiro, ele pode simbolizar uma \xFAnica sequ\xEAncia:"),Ne=m(),$(L.$$.fragment),Oe=m(),re=l("p"),gs=i("Tamb\xE9m lida com v\xE1rias sequ\xEAncias de cada vez, sem nenhuma mudan\xE7a na API:"),He=m(),$(G.$$.fragment),Re=m(),le=l("p"),ks=i("Ela pode ser aplicada de acordo com v\xE1rios objetivos:"),Be=m(),$(U.$$.fragment),Me=m(),ie=l("p"),_s=i("Tamb\xE9m pode truncar sequ\xEAncias:"),Le=m(),$(Q.$$.fragment),Ge=m(),_=l("p"),js=i("O objeto "),ke=l("code"),ws=i("tokenizer"),zs=i(" pode lidar com a convers\xE3o para tensores de estrutura espec\xEDficos, que podem ent\xE3o ser enviados diretamente para o modelo. Por exemplo, na seguinte amostra de c\xF3digo, estamos solicitando que o tokenizer retorne tensores de diferentes estruturas - "),_e=l("code"),Es=i('"pt"'),ys=i(" retorna tensores PyTorch, "),je=l("code"),Ps=i('"tf"'),Ts=i(" retorna tensores TensorFlow, e "),we=l("code"),xs=i('"np"'),Is=i(" retorna arrays NumPy:"),Ue=m(),$(V.$$.fragment),Qe=m(),C=l("h2"),O=l("a"),ze=l("span"),$(J.$$.fragment),As=m(),Ee=l("span"),Ss=i("Tokens especiais"),Ve=m(),pe=l("p"),Cs=i("Se dermos uma olhada nos IDs de entrada devolvidos pelo tokenizer, veremos que eles s\xE3o um pouco diferentes do que t\xEDnhamos anteriormente:"),Je=m(),$(K.$$.fragment),Ke=m(),$(W.$$.fragment),We=m(),ce=l("p"),Ds=i("Um token ID foi adicionada no in\xEDcio e uma no final. Vamos decodificar as duas sequ\xEAncias de IDs acima para ver do que se trata:"),Xe=m(),$(X.$$.fragment),Ye=m(),$(Y.$$.fragment),Ze=m(),I=l("p"),Fs=i("O tokenizer acrescentou a palavra especial "),ye=l("code"),Ns=i("[CLS]"),Os=i(" no in\xEDcio e a palavra especial "),Pe=l("code"),Hs=i("[SEP]"),Rs=i(" no final. Isto porque o modelo foi pr\xE9-treinado com esses, ent\xE3o para obter os mesmos resultados para infer\xEAncia, precisamos adicion\xE1-los tamb\xE9m. Note que alguns modelos n\xE3o acrescentam palavras especiais, ou acrescentam palavras diferentes; os modelos tamb\xE9m podem acrescentar estas palavras especiais apenas no in\xEDcio, ou apenas no final. Em qualquer caso, o tokenizer sabe quais s\xE3o as palavras que s\xE3o esperadas e tratar\xE1 disso para voc\xEA."),es=m(),D=l("h2"),H=l("a"),Te=l("span"),$(Z.$$.fragment),Bs=m(),xe=l("span"),Ms=i("Do tokenizer ao modelo"),ss=m(),R=l("p"),Ls=i("Agora que j\xE1 vimos todos os passos individuais que o objeto "),Ie=l("code"),Gs=i("tokenizer"),Us=i(" utiliza quando aplicado em textos, vamos ver uma \xFAltima vez como ele pode lidar com m\xFAltiplas sequ\xEAncias (padding!), sequ\xEAncias muito longas (truncagem!), e m\xFAltiplos tipos de tensores com seu API principal:"),os=m(),T.c(),ue=_o(),this.h()},l(e){const o=To('[data-svelte="svelte-1phssyn"]',document.head);r=p(o,"META",{name:!0,content:!0}),o.forEach(s),f=d(e),q(t.$$.fragment,e),k=d(e),S=p(e,"H1",{class:!0});var oe=u(S);F=p(oe,"A",{id:!0,class:!0,href:!0});var me=u(F);be=p(me,"SPAN",{});var de=u(be);q(B.$$.fragment,de),de.forEach(s),me.forEach(s),is=d(oe),$e=p(oe,"SPAN",{});var Xs=u($e);ps=c(Xs,"Colocando tudo junto"),Xs.forEach(s),oe.forEach(s),Ae=d(e),y.l(e),ae=d(e),te=p(e,"P",{});var Ys=u(te);cs=c(Ys,"Nas \xFAltimas se\xE7\xF5es, temos feito o nosso melhor para fazer a maior parte do trabalho \xE0 m\xE3o. Exploramos como funcionam os tokenizers e analisamos a tokeniza\xE7\xE3o, convers\xE3o para IDs de entrada, padding, truncagem e m\xE1scaras de aten\xE7\xE3o."),Ys.forEach(s),Se=d(e),N=p(e,"P",{});var ts=u(N);us=c(ts,"Entretanto, como vimos na se\xE7\xE3o 2, a API dos \u{1F917} Transformers pode tratar de tudo isso para n\xF3s com uma fun\xE7\xE3o de alto n\xEDvel, na qual mergulharemos aqui. Quando voc\xEA chama seu "),qe=p(ts,"CODE",{});var Zs=u(qe);ms=c(Zs,"tokenizer"),Zs.forEach(s),ds=c(ts," diretamente na frase, voc\xEA recebe de volta entradas que est\xE3o prontas para passar pelo seu modelo:"),ts.forEach(s),Ce=d(e),q(M.$$.fragment,e),De=d(e),x=p(e,"P",{});var fe=u(x);fs=c(fe,"Aqui, a vari\xE1vel "),ve=p(fe,"CODE",{});var eo=u(ve);hs=c(eo,"model_inputs"),eo.forEach(s),bs=c(fe," cont\xE9m tudo o que \xE9 necess\xE1rio para que um modelo funcione bem. Para DistilBERT, isso inclui os IDs de entrada, bem como a m\xE1scara de aten\xE7\xE3o. Outros modelos que aceitam entradas adicionais tamb\xE9m ter\xE3o essas sa\xEDdas pelo objeto "),ge=p(fe,"CODE",{});var so=u(ge);$s=c(so,"tokenizer"),so.forEach(s),qs=c(fe,"."),fe.forEach(s),Fe=d(e),ne=p(e,"P",{});var oo=u(ne);vs=c(oo,"Como veremos em alguns exemplos abaixo, este m\xE9todo \xE9 muito poderoso. Primeiro, ele pode simbolizar uma \xFAnica sequ\xEAncia:"),oo.forEach(s),Ne=d(e),q(L.$$.fragment,e),Oe=d(e),re=p(e,"P",{});var ao=u(re);gs=c(ao,"Tamb\xE9m lida com v\xE1rias sequ\xEAncias de cada vez, sem nenhuma mudan\xE7a na API:"),ao.forEach(s),He=d(e),q(G.$$.fragment,e),Re=d(e),le=p(e,"P",{});var to=u(le);ks=c(to,"Ela pode ser aplicada de acordo com v\xE1rios objetivos:"),to.forEach(s),Be=d(e),q(U.$$.fragment,e),Me=d(e),ie=p(e,"P",{});var no=u(ie);_s=c(no,"Tamb\xE9m pode truncar sequ\xEAncias:"),no.forEach(s),Le=d(e),q(Q.$$.fragment,e),Ge=d(e),_=p(e,"P",{});var A=u(_);js=c(A,"O objeto "),ke=p(A,"CODE",{});var ro=u(ke);ws=c(ro,"tokenizer"),ro.forEach(s),zs=c(A," pode lidar com a convers\xE3o para tensores de estrutura espec\xEDficos, que podem ent\xE3o ser enviados diretamente para o modelo. Por exemplo, na seguinte amostra de c\xF3digo, estamos solicitando que o tokenizer retorne tensores de diferentes estruturas - "),_e=p(A,"CODE",{});var lo=u(_e);Es=c(lo,'"pt"'),lo.forEach(s),ys=c(A," retorna tensores PyTorch, "),je=p(A,"CODE",{});var io=u(je);Ps=c(io,'"tf"'),io.forEach(s),Ts=c(A," retorna tensores TensorFlow, e "),we=p(A,"CODE",{});var po=u(we);xs=c(po,'"np"'),po.forEach(s),Is=c(A," retorna arrays NumPy:"),A.forEach(s),Ue=d(e),q(V.$$.fragment,e),Qe=d(e),C=p(e,"H2",{class:!0});var ns=u(C);O=p(ns,"A",{id:!0,class:!0,href:!0});var co=u(O);ze=p(co,"SPAN",{});var uo=u(ze);q(J.$$.fragment,uo),uo.forEach(s),co.forEach(s),As=d(ns),Ee=p(ns,"SPAN",{});var mo=u(Ee);Ss=c(mo,"Tokens especiais"),mo.forEach(s),ns.forEach(s),Ve=d(e),pe=p(e,"P",{});var fo=u(pe);Cs=c(fo,"Se dermos uma olhada nos IDs de entrada devolvidos pelo tokenizer, veremos que eles s\xE3o um pouco diferentes do que t\xEDnhamos anteriormente:"),fo.forEach(s),Je=d(e),q(K.$$.fragment,e),Ke=d(e),q(W.$$.fragment,e),We=d(e),ce=p(e,"P",{});var ho=u(ce);Ds=c(ho,"Um token ID foi adicionada no in\xEDcio e uma no final. Vamos decodificar as duas sequ\xEAncias de IDs acima para ver do que se trata:"),ho.forEach(s),Xe=d(e),q(X.$$.fragment,e),Ye=d(e),q(Y.$$.fragment,e),Ze=d(e),I=p(e,"P",{});var he=u(I);Fs=c(he,"O tokenizer acrescentou a palavra especial "),ye=p(he,"CODE",{});var bo=u(ye);Ns=c(bo,"[CLS]"),bo.forEach(s),Os=c(he," no in\xEDcio e a palavra especial "),Pe=p(he,"CODE",{});var $o=u(Pe);Hs=c($o,"[SEP]"),$o.forEach(s),Rs=c(he," no final. Isto porque o modelo foi pr\xE9-treinado com esses, ent\xE3o para obter os mesmos resultados para infer\xEAncia, precisamos adicion\xE1-los tamb\xE9m. Note que alguns modelos n\xE3o acrescentam palavras especiais, ou acrescentam palavras diferentes; os modelos tamb\xE9m podem acrescentar estas palavras especiais apenas no in\xEDcio, ou apenas no final. Em qualquer caso, o tokenizer sabe quais s\xE3o as palavras que s\xE3o esperadas e tratar\xE1 disso para voc\xEA."),he.forEach(s),es=d(e),D=p(e,"H2",{class:!0});var rs=u(D);H=p(rs,"A",{id:!0,class:!0,href:!0});var qo=u(H);Te=p(qo,"SPAN",{});var vo=u(Te);q(Z.$$.fragment,vo),vo.forEach(s),qo.forEach(s),Bs=d(rs),xe=p(rs,"SPAN",{});var go=u(xe);Ms=c(go,"Do tokenizer ao modelo"),go.forEach(s),rs.forEach(s),ss=d(e),R=p(e,"P",{});var ls=u(R);Ls=c(ls,"Agora que j\xE1 vimos todos os passos individuais que o objeto "),Ie=p(ls,"CODE",{});var ko=u(Ie);Gs=c(ko,"tokenizer"),ko.forEach(s),Us=c(ls," utiliza quando aplicado em textos, vamos ver uma \xFAltima vez como ele pode lidar com m\xFAltiplas sequ\xEAncias (padding!), sequ\xEAncias muito longas (truncagem!), e m\xFAltiplos tipos de tensores com seu API principal:"),ls.forEach(s),os=d(e),T.l(e),ue=_o(),this.h()},h(){j(r,"name","hf:doc:metadata"),j(r,"content",JSON.stringify(No)),j(F,"id","colocando-tudo-junto"),j(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(F,"href","#colocando-tudo-junto"),j(S,"class","relative group"),j(O,"id","tokens-especiais"),j(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(O,"href","#tokens-especiais"),j(C,"class","relative group"),j(H,"id","do-tokenizer-ao-modelo"),j(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(H,"href","#do-tokenizer-ao-modelo"),j(D,"class","relative group")},m(e,o){a(document.head,r),n(e,f,o),v(t,e,o),n(e,k,o),n(e,S,o),a(S,F),a(F,be),v(B,be,null),a(S,is),a(S,$e),a($e,ps),n(e,Ae,o),ee[E].m(e,o),n(e,ae,o),n(e,te,o),a(te,cs),n(e,Se,o),n(e,N,o),a(N,us),a(N,qe),a(qe,ms),a(N,ds),n(e,Ce,o),v(M,e,o),n(e,De,o),n(e,x,o),a(x,fs),a(x,ve),a(ve,hs),a(x,bs),a(x,ge),a(ge,$s),a(x,qs),n(e,Fe,o),n(e,ne,o),a(ne,vs),n(e,Ne,o),v(L,e,o),n(e,Oe,o),n(e,re,o),a(re,gs),n(e,He,o),v(G,e,o),n(e,Re,o),n(e,le,o),a(le,ks),n(e,Be,o),v(U,e,o),n(e,Me,o),n(e,ie,o),a(ie,_s),n(e,Le,o),v(Q,e,o),n(e,Ge,o),n(e,_,o),a(_,js),a(_,ke),a(ke,ws),a(_,zs),a(_,_e),a(_e,Es),a(_,ys),a(_,je),a(je,Ps),a(_,Ts),a(_,we),a(we,xs),a(_,Is),n(e,Ue,o),v(V,e,o),n(e,Qe,o),n(e,C,o),a(C,O),a(O,ze),v(J,ze,null),a(C,As),a(C,Ee),a(Ee,Ss),n(e,Ve,o),n(e,pe,o),a(pe,Cs),n(e,Je,o),v(K,e,o),n(e,Ke,o),v(W,e,o),n(e,We,o),n(e,ce,o),a(ce,Ds),n(e,Xe,o),v(X,e,o),n(e,Ye,o),v(Y,e,o),n(e,Ze,o),n(e,I,o),a(I,Fs),a(I,ye),a(ye,Ns),a(I,Os),a(I,Pe),a(Pe,Hs),a(I,Rs),n(e,es,o),n(e,D,o),a(D,H),a(H,Te),v(Z,Te,null),a(D,Bs),a(D,xe),a(xe,Ms),n(e,ss,o),n(e,R,o),a(R,Ls),a(R,Ie),a(Ie,Gs),a(R,Us),n(e,os,o),se[P].m(e,o),n(e,ue,o),as=!0},p(e,[o]){const oe={};o&1&&(oe.fw=e[0]),t.$set(oe);let me=E;E=Js(e),E!==me&&(wo(),h(ee[me],1,1,()=>{ee[me]=null}),jo(),y=ee[E],y||(y=ee[E]=Vs[E](e),y.c()),b(y,1),y.m(ae.parentNode,ae));let de=P;P=Ws(e),P!==de&&(wo(),h(se[de],1,1,()=>{se[de]=null}),jo(),T=se[P],T||(T=se[P]=Ks[P](e),T.c()),b(T,1),T.m(ue.parentNode,ue))},i(e){as||(b(t.$$.fragment,e),b(B.$$.fragment,e),b(y),b(M.$$.fragment,e),b(L.$$.fragment,e),b(G.$$.fragment,e),b(U.$$.fragment,e),b(Q.$$.fragment,e),b(V.$$.fragment,e),b(J.$$.fragment,e),b(K.$$.fragment,e),b(W.$$.fragment,e),b(X.$$.fragment,e),b(Y.$$.fragment,e),b(Z.$$.fragment,e),b(T),as=!0)},o(e){h(t.$$.fragment,e),h(B.$$.fragment,e),h(y),h(M.$$.fragment,e),h(L.$$.fragment,e),h(G.$$.fragment,e),h(U.$$.fragment,e),h(Q.$$.fragment,e),h(V.$$.fragment,e),h(J.$$.fragment,e),h(K.$$.fragment,e),h(W.$$.fragment,e),h(X.$$.fragment,e),h(Y.$$.fragment,e),h(Z.$$.fragment,e),h(T),as=!1},d(e){s(r),e&&s(f),g(t,e),e&&s(k),e&&s(S),g(B),e&&s(Ae),ee[E].d(e),e&&s(ae),e&&s(te),e&&s(Se),e&&s(N),e&&s(Ce),g(M,e),e&&s(De),e&&s(x),e&&s(Fe),e&&s(ne),e&&s(Ne),g(L,e),e&&s(Oe),e&&s(re),e&&s(He),g(G,e),e&&s(Re),e&&s(le),e&&s(Be),g(U,e),e&&s(Me),e&&s(ie),e&&s(Le),g(Q,e),e&&s(Ge),e&&s(_),e&&s(Ue),g(V,e),e&&s(Qe),e&&s(C),g(J),e&&s(Ve),e&&s(pe),e&&s(Je),g(K,e),e&&s(Ke),g(W,e),e&&s(We),e&&s(ce),e&&s(Xe),g(X,e),e&&s(Ye),g(Y,e),e&&s(Ze),e&&s(I),e&&s(es),e&&s(D),g(Z),e&&s(ss),e&&s(R),e&&s(os),se[P].d(e),e&&s(ue)}}}const No={local:"colocando-tudo-junto",sections:[{local:"tokens-especiais",title:"Tokens especiais"},{local:"do-tokenizer-ao-modelo",title:"Do tokenizer ao modelo"}],title:"Colocando tudo junto"};function Oo(z,r,f){let t="pt";return xo(()=>{const k=new URLSearchParams(window.location.search);f(0,t=k.get("fw")||"pt")}),[t]}class Go extends Eo{constructor(r){super();yo(this,r,Oo,Fo,Po,{})}}export{Go as default,No as metadata};
