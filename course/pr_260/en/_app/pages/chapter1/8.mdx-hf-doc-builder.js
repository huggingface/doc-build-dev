import{S as fe,i as we,s as ge,e as o,k as d,w as F,t as r,M as be,c as i,d as t,m as f,a as l,x as G,h as p,b as c,G as a,g as n,y as K,L as ke,q as O,o as z,B as H,v as ye}from"../../chunks/vendor-hf-doc-builder.js";import{I as ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as de}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as $e}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function _e(oe){let u,A,m,w,q,b,J,T,U,P,k,S,x,Q,C,g,V,B,X,Y,W,y,M,v,N,h,Z,$,ee,te,_,se,ae,R,j,ne,D;return b=new ve({}),k=new $e({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"}]}}),y=new de({props:{code:`
`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

unmasker = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
result = unmasker(<span class="hljs-string">&quot;This man works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])

result = unmasker(<span class="hljs-string">&quot;This woman works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])`}}),v=new de({props:{code:`['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']`,highlighted:`[<span class="hljs-string">&#x27;lawyer&#x27;</span>, <span class="hljs-string">&#x27;carpenter&#x27;</span>, <span class="hljs-string">&#x27;doctor&#x27;</span>, <span class="hljs-string">&#x27;waiter&#x27;</span>, <span class="hljs-string">&#x27;mechanic&#x27;</span>]
[<span class="hljs-string">&#x27;nurse&#x27;</span>, <span class="hljs-string">&#x27;waitress&#x27;</span>, <span class="hljs-string">&#x27;teacher&#x27;</span>, <span class="hljs-string">&#x27;maid&#x27;</span>, <span class="hljs-string">&#x27;prostitute&#x27;</span>]`}}),{c(){u=o("meta"),A=d(),m=o("h1"),w=o("a"),q=o("span"),F(b.$$.fragment),J=d(),T=o("span"),U=r("Bias and limitations"),P=d(),F(k.$$.fragment),S=d(),x=o("p"),Q=r("If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet."),C=d(),g=o("p"),V=r("To give a quick illustration, let\u2019s go back the example of a "),B=o("code"),X=r("fill-mask"),Y=r(" pipeline with the BERT model:"),W=d(),F(y.$$.fragment),M=d(),F(v.$$.fragment),N=d(),h=o("p"),Z=r("When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender \u2014 and yes, prostitute ended up in the top 5 possibilities the model associates with \u201Cwoman\u201D and \u201Cwork.\u201D This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it\u2019s trained on the "),$=o("a"),ee=r("English Wikipedia"),te=r(" and "),_=o("a"),se=r("BookCorpus"),ae=r(" datasets)."),R=d(),j=o("p"),ne=r("When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won\u2019t make this intrinsic bias disappear."),this.h()},l(e){const s=be('[data-svelte="svelte-1phssyn"]',document.head);u=i(s,"META",{name:!0,content:!0}),s.forEach(t),A=f(e),m=i(e,"H1",{class:!0});var I=l(m);w=i(I,"A",{id:!0,class:!0,href:!0});var ie=l(w);q=i(ie,"SPAN",{});var re=l(q);G(b.$$.fragment,re),re.forEach(t),ie.forEach(t),J=f(I),T=i(I,"SPAN",{});var le=l(T);U=p(le,"Bias and limitations"),le.forEach(t),I.forEach(t),P=f(e),G(k.$$.fragment,e),S=f(e),x=i(e,"P",{});var pe=l(x);Q=p(pe,"If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet."),pe.forEach(t),C=f(e),g=i(e,"P",{});var L=l(g);V=p(L,"To give a quick illustration, let\u2019s go back the example of a "),B=i(L,"CODE",{});var he=l(B);X=p(he,"fill-mask"),he.forEach(t),Y=p(L," pipeline with the BERT model:"),L.forEach(t),W=f(e),G(y.$$.fragment,e),M=f(e),G(v.$$.fragment,e),N=f(e),h=i(e,"P",{});var E=l(h);Z=p(E,"When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender \u2014 and yes, prostitute ended up in the top 5 possibilities the model associates with \u201Cwoman\u201D and \u201Cwork.\u201D This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it\u2019s trained on the "),$=i(E,"A",{href:!0,rel:!0});var ce=l($);ee=p(ce,"English Wikipedia"),ce.forEach(t),te=p(E," and "),_=i(E,"A",{href:!0,rel:!0});var ue=l(_);se=p(ue,"BookCorpus"),ue.forEach(t),ae=p(E," datasets)."),E.forEach(t),R=f(e),j=i(e,"P",{});var me=l(j);ne=p(me,"When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won\u2019t make this intrinsic bias disappear."),me.forEach(t),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(xe)),c(w,"id","bias-and-limitations"),c(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(w,"href","#bias-and-limitations"),c(m,"class","relative group"),c($,"href","https://huggingface.co/datasets/wikipedia"),c($,"rel","nofollow"),c(_,"href","https://huggingface.co/datasets/bookcorpus"),c(_,"rel","nofollow")},m(e,s){a(document.head,u),n(e,A,s),n(e,m,s),a(m,w),a(w,q),K(b,q,null),a(m,J),a(m,T),a(T,U),n(e,P,s),K(k,e,s),n(e,S,s),n(e,x,s),a(x,Q),n(e,C,s),n(e,g,s),a(g,V),a(g,B),a(B,X),a(g,Y),n(e,W,s),K(y,e,s),n(e,M,s),K(v,e,s),n(e,N,s),n(e,h,s),a(h,Z),a(h,$),a($,ee),a(h,te),a(h,_),a(_,se),a(h,ae),n(e,R,s),n(e,j,s),a(j,ne),D=!0},p:ke,i(e){D||(O(b.$$.fragment,e),O(k.$$.fragment,e),O(y.$$.fragment,e),O(v.$$.fragment,e),D=!0)},o(e){z(b.$$.fragment,e),z(k.$$.fragment,e),z(y.$$.fragment,e),z(v.$$.fragment,e),D=!1},d(e){t(u),e&&t(A),e&&t(m),H(b),e&&t(P),H(k,e),e&&t(S),e&&t(x),e&&t(C),e&&t(g),e&&t(W),H(y,e),e&&t(M),H(v,e),e&&t(N),e&&t(h),e&&t(R),e&&t(j)}}}const xe={local:"bias-and-limitations",title:"Bias and limitations"};function je(oe){return ye(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ae extends fe{constructor(u){super();we(this,u,je,_e,ge,{})}}export{Ae as default,xe as metadata};
