import{S as Fq,i as Wq,s as Rq,e as r,k as u,w as m,t as n,M as Iq,c as l,d as t,m as c,a,x as d,h as o,b as E,N as Uq,G as s,g as p,y as f,q as v,o as k,B as _,v as Gq}from"../../chunks/vendor-hf-doc-builder.js";import{T as Vq}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Xq}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ii}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as h}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Kq}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Hq(pi){let $,ge,oe,pe,qe,je,As,re,Us,Xe,Ke,Fs,ss,le,ts,Pe,He,ns,W,xe,Ws,Rs,be,Is,Gs;return{c(){$=r("p"),ge=r("strong"),oe=n("Pour aller plus loin"),pe=n(" Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=r("code"),je=n('u"\\u0085"'),As=n(` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),re=r("code"),Us=n("normalizers.Sequence"),Xe=n(", nous n\u2019avons pas inclus les Regex que le "),Ke=r("code"),Fs=n("BertNormalizer"),ss=n(" requiert quand l\u2019argument "),le=r("code"),ts=n("clean_text"),Pe=n(" est mis \xE0 "),He=r("code"),ns=n("True"),W=n(" ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),xe=r("code"),Ws=n("BertNormalizer"),Rs=n(" en ajoutant deux "),be=r("code"),Is=n("normalizers.Replace"),Gs=n(" \xE0 la s\xE9quence de normalisation.")},l(ae){$=l(ae,"P",{});var x=a($);ge=l(x,"STRONG",{});var os=a(ge);oe=o(os,"Pour aller plus loin"),os.forEach(t),pe=o(x," Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=l(x,"CODE",{});var Tn=a(qe);je=o(Tn,'u"\\u0085"'),Tn.forEach(t),As=o(x,` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),re=l(x,"CODE",{});var Dn=a(re);Us=o(Dn,"normalizers.Sequence"),Dn.forEach(t),Xe=o(x,", nous n\u2019avons pas inclus les Regex que le "),Ke=l(x,"CODE",{});var Ln=a(Ke);Fs=o(Ln,"BertNormalizer"),Ln.forEach(t),ss=o(x," requiert quand l\u2019argument "),le=l(x,"CODE",{});var J=a(le);ts=o(J,"clean_text"),J.forEach(t),Pe=o(x," est mis \xE0 "),He=l(x,"CODE",{});var On=a(He);ns=o(On,"True"),On.forEach(t),W=o(x," ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),xe=l(x,"CODE",{});var rs=a(xe);Ws=o(rs,"BertNormalizer"),rs.forEach(t),Rs=o(x," en ajoutant deux "),be=l(x,"CODE",{});var Mn=a(be);Is=o(Mn,"normalizers.Replace"),Mn.forEach(t),Gs=o(x," \xE0 la s\xE9quence de normalisation."),x.forEach(t)},m(ae,x){p(ae,$,x),s($,ge),s(ge,oe),s($,pe),s($,qe),s(qe,je),s($,As),s($,re),s(re,Us),s($,Xe),s($,Ke),s(Ke,Fs),s($,ss),s($,le),s(le,ts),s($,Pe),s($,He),s(He,ns),s($,W),s($,xe),s(xe,Ws),s($,Rs),s($,be),s(be,Is),s($,Gs)},d(ae){ae&&t($)}}}function Jq(pi){let $,ge,oe,pe,qe,je,As,re,Us,Xe,Ke,Fs,ss,le,ts,Pe,He,ns,W,xe,Ws,Rs,be,Is,Gs,ae,x,os,Tn,Dn,Ln,J,On,rs,Mn,xc,mo,bc,gc,fo,Pc,wc,ui,yn,Cc,ci,Je,Vs,o$,Tc,Xs,r$,mi,R,Dc,vo,Lc,Oc,ko,Mc,yc,_o,Sc,Nc,Sn,Bc,Ac,ho,Uc,Fc,di,Ks,fi,ls,Wc,Eo,Rc,Ic,vi,I,we,$o,Gc,Vc,zo,Xc,Kc,Hs,Hc,Jc,Yc,Ce,qo,Zc,Qc,jo,em,sm,Js,tm,nm,om,G,xo,rm,lm,bo,am,im,go,pm,um,Po,cm,mm,wo,dm,fm,Ys,vm,km,_m,Te,Co,hm,Em,To,$m,zm,Zs,qm,jm,xm,De,Do,bm,gm,Lo,Pm,wm,Qs,Cm,Tm,Dm,Le,Oo,Lm,Om,Mo,Mm,ym,et,Sm,Nm,ki,as,Bm,st,Am,Um,_i,Ye,is,yo,tt,Fm,So,Wm,hi,ue,Rm,No,Im,Gm,Nn,Vm,Xm,nt,Km,Hm,Ei,ot,$i,Oe,Jm,Bo,Ym,Zm,Ao,Qm,ed,zi,ps,sd,Uo,td,nd,qi,rt,ji,ce,od,Fo,rd,ld,Wo,ad,id,Ro,pd,ud,xi,Ze,us,Io,lt,cd,at,md,Go,dd,fd,bi,b,vd,Vo,kd,_d,Xo,hd,Ed,Ko,$d,zd,Ho,qd,jd,Jo,xd,bd,Yo,gd,Pd,Zo,wd,Cd,Qo,Td,Dd,gi,Me,Ld,er,Od,Md,sr,yd,Sd,Pi,it,wi,me,Nd,tr,Bd,Ad,nr,Ud,Fd,or,Wd,Rd,Ci,g,Id,rr,Gd,Vd,lr,Xd,Kd,ar,Hd,Jd,ir,Yd,Zd,pr,Qd,ef,ur,sf,tf,cr,nf,of,mr,rf,lf,Ti,pt,Di,V,af,dr,pf,uf,fr,cf,mf,vr,df,ff,kr,vf,kf,_r,_f,hf,Li,ut,Oi,ye,Ef,hr,$f,zf,Er,qf,jf,Mi,Se,xf,$r,bf,gf,zr,Pf,wf,yi,ct,Si,mt,Ni,cs,Bi,ms,Cf,qr,Tf,Df,Ai,dt,Ui,Bn,Lf,Fi,ft,Wi,ds,Of,jr,Mf,yf,Ri,vt,Ii,kt,Gi,fs,Sf,xr,Nf,Bf,Vi,_t,Xi,ht,Ki,vs,Af,br,Uf,Ff,Hi,Et,Ji,$t,Yi,de,Wf,gr,Rf,If,Pr,Gf,Vf,wr,Xf,Kf,Zi,zt,Qi,O,Hf,Cr,Jf,Yf,Tr,Zf,Qf,Dr,ev,sv,Lr,tv,nv,Or,ov,rv,Mr,lv,av,ep,An,iv,sp,qt,tp,Ne,pv,yr,uv,cv,Sr,mv,dv,np,jt,op,Be,fv,Nr,vv,kv,Br,_v,hv,rp,xt,lp,bt,ap,j,Ev,Ar,$v,zv,Ur,qv,jv,Fr,xv,bv,Wr,gv,Pv,Rr,wv,Cv,Ir,Tv,Dv,Gr,Lv,Ov,Vr,Mv,yv,Xr,Sv,Nv,ip,P,Bv,Kr,Av,Uv,Hr,Fv,Wv,Jr,Rv,Iv,Yr,Gv,Vv,Zr,Xv,Kv,Qr,Hv,Jv,el,Yv,Zv,sl,Qv,ek,pp,gt,up,Pt,cp,M,sk,tl,tk,nk,nl,ok,rk,ol,lk,ak,rl,ik,pk,ll,uk,ck,al,mk,dk,mp,Un,fk,dp,wt,fp,Ae,vk,il,kk,_k,pl,hk,Ek,vp,Fn,$k,kp,Ct,_p,Tt,hp,Wn,zk,Ep,Dt,$p,Lt,zp,ks,qk,ul,jk,xk,qp,Ot,jp,_s,bk,cl,gk,Pk,xp,Mt,bp,yt,gp,hs,wk,ml,Ck,Tk,Pp,St,wp,Ue,Dk,dl,Lk,Ok,fl,Mk,yk,Cp,Nt,Tp,y,Sk,vl,Nk,Bk,kl,Ak,Uk,_l,Fk,Wk,hl,Rk,Ik,El,Gk,Vk,$l,Xk,Kk,Dp,z,Hk,zl,Jk,Yk,ql,Zk,Qk,jl,e2,s2,xl,t2,n2,bl,o2,r2,gl,l2,a2,Pl,i2,p2,wl,u2,c2,Cl,m2,d2,Tl,f2,v2,Dl,k2,Ll,_2,h2,Lp,Bt,Op,fe,E2,Ol,$2,z2,Ml,q2,j2,yl,x2,b2,Mp,At,yp,S,g2,Sl,P2,w2,Nl,C2,T2,Bl,D2,L2,Al,O2,M2,Ul,y2,S2,Fl,N2,B2,Sp,Fe,A2,Wl,U2,F2,Rl,W2,R2,Np,Qe,Es,Il,Ut,I2,Ft,G2,Gl,V2,X2,Bp,ve,K2,Vl,H2,J2,Xl,Y2,Z2,Kl,Q2,e_,Ap,Wt,Up,ke,s_,Hl,t_,n_,Jl,o_,r_,Yl,l_,a_,Fp,Rn,i_,Wp,Rt,Rp,$s,p_,Zl,u_,c_,Ip,It,Gp,Gt,Vp,We,m_,Ql,d_,f_,ea,v_,k_,Xp,Vt,Kp,N,__,sa,h_,E_,ta,$_,z_,na,q_,j_,oa,x_,b_,ra,g_,P_,la,w_,C_,Hp,zs,T_,aa,D_,L_,Jp,Xt,Yp,In,O_,Zp,Kt,Qp,Ht,eu,qs,M_,ia,y_,S_,su,Jt,tu,C,N_,pa,B_,A_,ua,U_,F_,ca,W_,R_,ma,I_,G_,da,V_,X_,fa,K_,H_,va,J_,Y_,nu,Yt,ou,Zt,ru,Gn,Z_,lu,Qt,au,Vn,Q_,iu,en,pu,sn,uu,_e,eh,ka,sh,th,_a,nh,oh,ha,rh,lh,cu,tn,mu,Xn,ah,du,nn,fu,Re,ih,Ea,ph,uh,$a,ch,mh,vu,es,js,za,on,dh,rn,fh,qa,vh,kh,ku,Y,_h,ja,hh,Eh,xa,$h,zh,ba,qh,jh,ga,xh,bh,_u,ln,hu,Kn,gh,Eu,xs,Ph,Pa,wh,Ch,$u,an,zu,he,Th,wa,Dh,Lh,Ca,Oh,Mh,Ta,yh,Sh,qu,Ie,Nh,Da,Bh,Ah,La,Uh,Fh,ju,pn,xu,Hn,Wh,bu,un,gu,cn,Pu,bs,Rh,Oa,Ih,Gh,wu,mn,Cu,T,Vh,Ma,Xh,Kh,ya,Hh,Jh,Sa,Yh,Zh,Na,Qh,eE,Ba,sE,tE,Aa,nE,oE,Ua,rE,lE,Tu,gs,aE,Fa,iE,pE,Du,dn,Lu,Jn,uE,Ou,fn,Mu,vn,yu,w,cE,Wa,mE,dE,Ra,fE,vE,Ia,kE,_E,Ga,hE,EE,Va,$E,zE,Xa,qE,jE,Ka,xE,bE,Ha,gE,PE,Su,kn,Nu,_n,Bu,Yn,wE,Au,hn,Uu,Zn,CE,Fu,En,Wu,$n,Ru,Ps,TE,Ja,DE,LE,Iu,zn,Gu,D,OE,Ya,ME,yE,Za,SE,NE,Qa,BE,AE,ei,UE,FE,si,WE,RE,ti,IE,GE,ni,VE,XE,Vu,qn,Xu,Qn,KE,Ku,jn,Hu,Z,HE,oi,JE,YE,ri,ZE,QE,li,e$,s$,ai,t$,n$,Ju;return je=new ii({}),le=new Kq({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"}]}}),Ks=new Xq({props:{id:"MR8tZm5ViWU"}}),tt=new ii({}),ot=new h({props:{code:`

`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, name=<span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        <span class="hljs-keyword">yield</span> dataset[i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;text&quot;</span>]`}}),rt=new h({props:{code:`with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\\n")`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset)):
        f.write(dataset[i][<span class="hljs-string">&quot;text&quot;</span>] + <span class="hljs-string">&quot;\\n&quot;</span>)`}}),lt=new ii({}),it=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),pt=new h({props:{code:"tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)",highlighted:'tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="hljs-literal">True</span>)'}}),ut=new h({props:{code:`tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`,highlighted:`tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`}}),ct=new h({props:{code:'print(tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),mt=new h({props:{code:"hello how are u?",highlighted:"hello how are u?"}}),cs=new Vq({props:{$$slots:{default:[Hq]},$$scope:{ctx:pi}}}),dt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"}}),ft=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"}}),vt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)'}}),kt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),_t=new h({props:{code:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),ht=new h({props:{code:`[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]`,highlighted:'[(<span class="hljs-string">&quot;Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">28</span>))]'}}),Et=new h({props:{code:`pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),$t=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),zt=new h({props:{code:`special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
trainer = trainers.WordPieceTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens)`}}),qt=new h({props:{code:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)",highlighted:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"}}),jt=new h({props:{code:`tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>)
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),xt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),bt=new h({props:{code:`['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']`,highlighted:'[<span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),gt=new h({props:{code:`cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Pt=new h({props:{code:"(2, 3)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)'}}),wt=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,
    pair=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="hljs-string">&quot;[SEP]&quot;</span>, sep_token_id)],
)`}}),Ct=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Tt=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']`,highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),Dt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),Lt=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;pair&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;sentences&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Ot=new h({props:{code:'tokenizer.decoder = decoders.WordPiece(prefix="##")',highlighted:'tokenizer.decoder = decoders.WordPiece(prefix=<span class="hljs-string">&quot;##&quot;</span>)'}}),Mt=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),yt=new h({props:{code:`"let's test this tokenizer... on a pair of sentences." # Testons ce tokenizer... sur une paire de phrases.`,highlighted:'<span class="hljs-string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer... sur une paire de phrases.</span>'}}),St=new h({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Nt=new h({props:{code:'new_tokenizer = Tokenizer.from_file("tokenizer.json")',highlighted:'new_tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Bt=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    <span class="hljs-comment"># tokenizer_file=&quot;tokenizer.json&quot;, # Vous pouvez charger \xE0 partir du fichier du tokenizer, alternativement</span>
    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,
    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,
    cls_token=<span class="hljs-string">&quot;[CLS]&quot;</span>,
    sep_token=<span class="hljs-string">&quot;[SEP]&quot;</span>,
    mask_token=<span class="hljs-string">&quot;[MASK]&quot;</span>,
)`}}),At=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`}}),Ut=new ii({}),Wt=new h({props:{code:"tokenizer = Tokenizer(models.BPE())",highlighted:"tokenizer = Tokenizer(models.BPE())"}}),Rt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)",highlighted:'tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>)'}}),It=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)'}}),Gt=new h({props:{code:`[('Let', (0, 3)), ("'s", (3, 5)), ('\u0120test', (5, 10)), ('\u0120pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;s&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u0120test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120pre&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
 (<span class="hljs-string">&#x27;tokenization&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;!&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Vt=new h({props:{code:`trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`trainer = trainers.BpeTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),Xt=new h({props:{code:`tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.BPE()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Kt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ht=new h({props:{code:`['L', 'et', "'", 's', '\u0120test', '\u0120this', '\u0120to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-string">&#x27;et&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u0120test&#x27;</span>, <span class="hljs-string">&#x27;\u0120this&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Jt=new h({props:{code:"tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)",highlighted:'tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="hljs-literal">False</span>)'}}),Yt=new h({props:{code:`sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]`,highlighted:`sentence = <span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[<span class="hljs-number">4</span>]
sentence[start:end]`}}),Zt=new h({props:{code:"' test'",highlighted:'<span class="hljs-string">&#x27; test&#x27;</span>'}}),Qt=new h({props:{code:"tokenizer.decoder = decoders.ByteLevel()",highlighted:"tokenizer.decoder = decoders.ByteLevel()"}}),en=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),sn=new h({props:{code:`"Let's test this tokenizer." # Testons ce tokenizer`,highlighted:'<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer</span>'}}),tn=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
)`}}),nn=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`}}),on=new ii({}),ln=new h({props:{code:"tokenizer = Tokenizer(models.Unigram())",highlighted:"tokenizer = Tokenizer(models.Unigram())"}}),an=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Regex

tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [
        normalizers.Replace(<span class="hljs-string">&quot;\`\`&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.Replace(<span class="hljs-string">&quot;&#x27;&#x27;&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(<span class="hljs-string">&quot; {2,}&quot;</span>), <span class="hljs-string">&quot; &quot;</span>),
    ]
)`}}),pn=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"}}),un=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)'}}),cn=new h({props:{code:`[("\u2581Let's", (0, 5)), ('\u2581test', (5, 10)), ('\u2581the', (10, 14)), ('\u2581pre-tokenizer!', (14, 29))]`,highlighted:'[(<span class="hljs-string">&quot;\u2581Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u2581test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581the&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581pre-tokenizer!&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">29</span>))]'}}),mn=new h({props:{code:`special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>]
trainer = trainers.UnigramTrainer(
    vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens, unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),dn=new h({props:{code:`tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.Unigram()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),fn=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),vn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),kn=new h({props:{code:`cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),_n=new h({props:{code:"0 1",highlighted:'<span class="hljs-number">0</span> <span class="hljs-number">1</span>'}}),hn=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,
    pair=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],
)`}}),En=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences!&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),$n=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.', '.', '.', '<sep>', '\u2581', 'on', '\u2581', 'a', '\u2581pair', 
  '\u2581of', '\u2581sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]`,highlighted:`[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;\u2581pair&#x27;</span>, 
  <span class="hljs-string">&#x27;\u2581of&#x27;</span>, <span class="hljs-string">&#x27;\u2581sentence&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;cls&gt;&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]`}}),zn=new h({props:{code:"tokenizer.decoder = decoders.Metaspace()",highlighted:"tokenizer.decoder = decoders.Metaspace()"}}),qn=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>,
    unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>,
    pad_token=<span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>,
    cls_token=<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>,
    sep_token=<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>,
    mask_token=<span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>,
    padding_side=<span class="hljs-string">&quot;left&quot;</span>,
)`}}),jn=new h({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`}}),{c(){$=r("meta"),ge=u(),oe=r("h1"),pe=r("a"),qe=r("span"),m(je.$$.fragment),As=u(),re=r("span"),Us=n("Construction d'un "),Xe=r("i"),Ke=n("tokenizer"),Fs=n(", bloc par bloc"),ss=u(),m(le.$$.fragment),ts=u(),Pe=r("p"),He=n("Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),ns=u(),W=r("ul"),xe=r("li"),Ws=n("normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),Rs=u(),be=r("li"),Is=n("pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),Gs=u(),ae=r("li"),x=n("passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),os=r("em"),Tn=n("tokens"),Dn=n("),"),Ln=u(),J=r("li"),On=n("post-traitement (ajout des "),rs=r("em"),Mn=n("tokens"),xc=n(" sp\xE9ciaux du "),mo=r("em"),bc=n("tokenizer"),gc=n(", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),fo=r("em"),Pc=n("token"),wc=n(")."),ui=u(),yn=r("p"),Cc=n("Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),ci=u(),Je=r("div"),Vs=r("img"),Tc=u(),Xs=r("img"),mi=u(),R=r("p"),Dc=n("La biblioth\xE8que \u{1F917} "),vo=r("em"),Lc=n("Tokenizers"),Oc=n(" a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),ko=r("em"),Mc=n("tokenizer"),yc=n(" \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),_o=r("em"),Sc=n("tokenizer"),Nc=n(" \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=r("a"),Bc=n("section 2"),Ac=n(". Vous serez alors en mesure de construire n\u2019importe quel type de "),ho=r("em"),Uc=n("tokenizer"),Fc=n(" auquel vous pouvez penser !"),di=u(),m(Ks.$$.fragment),fi=u(),ls=r("p"),Wc=n("Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),Eo=r("code"),Rc=n("Tokenizer"),Ic=n(" avec les blocs de construction regroup\xE9s en sous-modules :"),vi=u(),I=r("ul"),we=r("li"),$o=r("code"),Gc=n("normalizers"),Vc=n(" contient tous les types de "),zo=r("code"),Xc=n("Normalizer"),Kc=n(" que vous pouvez utiliser (liste compl\xE8te "),Hs=r("a"),Hc=n("ici"),Jc=n("),"),Yc=u(),Ce=r("li"),qo=r("code"),Zc=n("pre_tokenizers"),Qc=n(" contient tous les types de "),jo=r("code"),em=n("PreTokenizer"),sm=n(" que vous pouvez utiliser (liste compl\xE8te "),Js=r("a"),tm=n("ici"),nm=n("),"),om=u(),G=r("li"),xo=r("code"),rm=n("models"),lm=n(" contient les diff\xE9rents types de "),bo=r("code"),am=n("Model"),im=n(" que vous pouvez utiliser, comme "),go=r("code"),pm=n("BPE"),um=n(", "),Po=r("code"),cm=n("WordPiece"),mm=n(", et "),wo=r("code"),dm=n("Unigram"),fm=n(" (liste compl\xE8te "),Ys=r("a"),vm=n("ici"),km=n("),"),_m=u(),Te=r("li"),Co=r("code"),hm=n("trainers"),Em=n(" contient tous les diff\xE9rents types de "),To=r("code"),$m=n("Trainer"),zm=n(" que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=r("a"),qm=n("ici"),jm=n("),"),xm=u(),De=r("li"),Do=r("code"),bm=n("post_processors"),gm=n(" contient les diff\xE9rents types de "),Lo=r("code"),Pm=n("PostProcessor"),wm=n(" que vous pouvez utiliser (liste compl\xE8te "),Qs=r("a"),Cm=n("ici"),Tm=n("),"),Dm=u(),Le=r("li"),Oo=r("code"),Lm=n("decoders"),Om=n(" contient les diff\xE9rents types de "),Mo=r("code"),Mm=n("Decoder"),ym=n(" que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=r("a"),Sm=n("ici"),Nm=n(")."),ki=u(),as=r("p"),Bm=n("Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=r("a"),Am=n("ici"),Um=n("."),_i=u(),Ye=r("h2"),is=r("a"),yo=r("span"),m(tt.$$.fragment),Fm=u(),So=r("span"),Wm=n("Acquisition d'un corpus"),hi=u(),ue=r("p"),Rm=n("Pour entra\xEEner notre nouveau "),No=r("em"),Im=n("tokenizer"),Gm=n(", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=r("a"),Vm=n("d\xE9but du chapitre"),Xm=n(", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=r("a"),Km=n("WikiText-2"),Hm=n(" :"),Ei=u(),m(ot.$$.fragment),$i=u(),Oe=r("p"),Jm=n("La fonction "),Bo=r("code"),Ym=n("get_training_corpus()"),Zm=n(" est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Ao=r("em"),Qm=n("tokenizer"),ed=n("."),zi=u(),ps=r("p"),sd=n("\u{1F917} "),Uo=r("em"),td=n("Tokenizers"),nd=n(" peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),qi=u(),m(rt.$$.fragment),ji=u(),ce=r("p"),od=n("Ensuite, nous vous montrerons comment construire vos propres "),Fo=r("em"),rd=n("tokenizers"),ld=n(" pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Wo=r("em"),ad=n("WordPiece"),id=n(", BPE et "),Ro=r("em"),pd=n("Unigram"),ud=n(". Commen\xE7ons par BERT !"),xi=u(),Ze=r("h2"),us=r("a"),Io=r("span"),m(lt.$$.fragment),cd=u(),at=r("span"),md=n("Construire un "),Go=r("i"),dd=n("tokenizer WordPiece"),fd=n(" \xE0 partir de z\xE9ro"),bi=u(),b=r("p"),vd=n("Pour construire un "),Vo=r("em"),kd=n("tokenizer"),_d=n(" avec la biblioth\xE8que \u{1F917} "),Xo=r("em"),hd=n("Tokenizers"),Ed=n(", nous commen\xE7ons par instancier un objet "),Ko=r("code"),$d=n("Tokenizer"),zd=n(" avec un "),Ho=r("code"),qd=n("model"),jd=n(". Puis nous d\xE9finissons ses attributs "),Jo=r("code"),xd=n("normalizer"),bd=n(", "),Yo=r("code"),gd=n("pre_tokenizer"),Pd=n(", "),Zo=r("code"),wd=n("post_processor"),Cd=n(" et "),Qo=r("code"),Td=n("decoder"),Dd=n(" aux valeurs que nous voulons."),gi=u(),Me=r("p"),Ld=n("Pour cet exemple, nous allons cr\xE9er un "),er=r("code"),Od=n("Tokenizer"),Md=n(" avec un mod\xE8le "),sr=r("em"),yd=n("WordPiece"),Sd=n(" :"),Pi=u(),m(it.$$.fragment),wi=u(),me=r("p"),Nd=n("Nous devons sp\xE9cifier le "),tr=r("code"),Bd=n("unk_token"),Ad=n(" pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),nr=r("code"),Ud=n("vocab"),Fd=n(" de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),or=r("code"),Wd=n("max_input_chars_per_word"),Rd=n(", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ci=u(),g=r("p"),Id=n("La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),rr=r("code"),Gd=n("BertNormalizer"),Vd=n(" a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),lr=r("code"),Xd=n("lowercase"),Kd=n(" pour mettre le texte en minuscule, "),ar=r("code"),Hd=n("strip_accents"),Jd=n(" qui enl\xE8ve les accents, "),ir=r("code"),Yd=n("clean_text"),Zd=n(" pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),pr=r("code"),Qd=n("handle_chinese_chars"),ef=n(" qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),ur=r("em"),sf=n("tokenizer"),tf=u(),cr=r("code"),nf=n("bert-base-uncased"),of=n(", nous pouvons simplement d\xE9finir ce "),mr=r("em"),rf=n("normalizer"),lf=n(" :"),Ti=u(),m(pt.$$.fragment),Di=u(),V=r("p"),af=n("Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),dr=r("em"),pf=n("tokenizer"),uf=n(", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),fr=r("em"),cf=n("Tokenizers"),mf=n(". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),vr=r("code"),df=n("Lowercase"),ff=n(" et un normaliseur "),kr=r("code"),vf=n("StripAccents"),kf=n(". Il est possible de composer plusieurs normaliseurs en utilisant une "),_r=r("code"),_f=n("Sequence"),hf=n(" :"),Li=u(),m(ut.$$.fragment),Oi=u(),ye=r("p"),Ef=n("Nous utilisons \xE9galement un normaliseur Unicode "),hr=r("code"),$f=n("NFD"),zf=n(", car sinon "),Er=r("code"),qf=n("StripAccents"),jf=n(" ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),Mi=u(),Se=r("p"),xf=n("Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),$r=r("code"),bf=n("normalize_str()"),gf=n(" du "),zr=r("code"),Pf=n("normalizer"),wf=n(" pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),yi=u(),m(ct.$$.fragment),Si=u(),m(mt.$$.fragment),Ni=u(),m(cs.$$.fragment),Bi=u(),ms=r("p"),Cf=n("L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),qr=r("code"),Tf=n("BertPreTokenizer"),Df=n(" pr\xE9construit que nous pouvons utiliser :"),Ai=u(),m(dt.$$.fragment),Ui=u(),Bn=r("p"),Lf=n("Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),Fi=u(),m(ft.$$.fragment),Wi=u(),ds=r("p"),Of=n("Notez que le "),jr=r("code"),Mf=n("Whitespace"),yf=n(" divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),Ri=u(),m(vt.$$.fragment),Ii=u(),m(kt.$$.fragment),Gi=u(),fs=r("p"),Sf=n("Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),xr=r("code"),Nf=n("WhitespaceSplit"),Bf=n(" \xE0 la place :"),Vi=u(),m(_t.$$.fragment),Xi=u(),m(ht.$$.fragment),Ki=u(),vs=r("p"),Af=n("Comme pour les normaliseurs, vous pouvez utiliser une "),br=r("code"),Uf=n("Sequence"),Ff=n(" pour composer plusieurs pr\xE9tokenizers :"),Hi=u(),m(Et.$$.fragment),Ji=u(),m($t.$$.fragment),Yi=u(),de=r("p"),Wf=n("L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),gr=r("code"),Rf=n("WordPieceTrainer"),If=n(". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Pr=r("em"),Gf=n("Tokenizers"),Vf=n(" est que vous devez lui passer tous les "),wr=r("em"),Xf=n("tokens"),Kf=n(" sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Zi=u(),m(zt.$$.fragment),Qi=u(),O=r("p"),Hf=n("En plus de sp\xE9cifier la "),Cr=r("code"),Jf=n("vocab_size"),Yf=n(" et les "),Tr=r("code"),Zf=n("special_tokens"),Qf=n(", nous pouvons d\xE9finir la "),Dr=r("code"),ev=n("min_frequency"),sv=n(" (le nombre de fois qu\u2019un "),Lr=r("em"),tv=n("token"),nv=n(" doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Or=r("code"),ov=n("continuing_subword_prefix"),rv=n(" (si nous voulons utiliser quelque chose de diff\xE9rent de "),Mr=r("code"),lv=n("##"),av=n(")."),ep=u(),An=r("p"),iv=n("Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),sp=u(),m(qt.$$.fragment),tp=u(),Ne=r("p"),pv=n("Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),yr=r("em"),uv=n("tokenizer"),cv=n(" qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Sr=r("code"),mv=n("WordPiece"),dv=n(" vide au pr\xE9alable) :"),np=u(),m(jt.$$.fragment),op=u(),Be=r("p"),fv=n("Dans les deux cas, nous pouvons ensuite tester le "),Nr=r("em"),vv=n("tokenizer"),kv=n(" sur un texte en appelant la m\xE9thode "),Br=r("code"),_v=n("encode()"),hv=n(" :"),rp=u(),m(xt.$$.fragment),lp=u(),m(bt.$$.fragment),ap=u(),j=r("p"),Ev=n("L\u2019encodage obtenu est un "),Ar=r("code"),$v=n("Encoding"),zv=n(" contenant toutes les sorties n\xE9cessaires du "),Ur=r("em"),qv=n("tokenizer"),jv=n(" dans ses diff\xE9rents attributs : "),Fr=r("code"),xv=n("ids"),bv=n(", "),Wr=r("code"),gv=n("type_ids"),Pv=n(", "),Rr=r("code"),wv=n("tokens"),Cv=n(", "),Ir=r("code"),Tv=n("offsets"),Dv=n(", "),Gr=r("code"),Lv=n("attention_mask"),Ov=n(", "),Vr=r("code"),Mv=n("special_tokens_mask"),yv=n(" et "),Xr=r("code"),Sv=n("overflowing"),Nv=n("."),ip=u(),P=r("p"),Bv=n("La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Kr=r("em"),Av=n("token"),Uv=u(),Hr=r("code"),Fv=n("[CLS]"),Wv=n(" au d\xE9but et le "),Jr=r("em"),Rv=n("token"),Iv=u(),Yr=r("code"),Gv=n("[SEP]"),Vv=n(" \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Zr=r("code"),Xv=n("TemplateProcessor"),Kv=n(" pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Qr=r("em"),Hv=n("tokens"),Jv=u(),el=r("code"),Yv=n("[CLS]"),Zv=n(" et "),sl=r("code"),Qv=n("[SEP]"),ek=n(" dans le vocabulaire :"),pp=u(),m(gt.$$.fragment),up=u(),m(Pt.$$.fragment),cp=u(),M=r("p"),sk=n("Pour \xE9crire le gabarit pour "),tl=r("code"),tk=n("TemplateProcessor"),nk=n(", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),nl=r("em"),ok=n("tokens"),rk=n(" sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),ol=r("code"),lk=n("$A"),ak=n(", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),rl=r("code"),ik=n("$B"),pk=n(". Pour chacun de ces \xE9l\xE9ments ("),ll=r("em"),uk=n("tokens"),ck=n(" sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),al=r("em"),mk=n("token"),dk=n(" correspondant apr\xE8s un deux-points."),mp=u(),Un=r("p"),fk=n("Le gabarit classique de BERT est donc d\xE9fini comme suit :"),dp=u(),m(wt.$$.fragment),fp=u(),Ae=r("p"),vk=n("Notez que nous devons transmettre les identifiants des "),il=r("em"),kk=n("tokens"),_k=n(" sp\xE9ciaux afin que le "),pl=r("em"),hk=n("tokenizer"),Ek=n(" puisse les convertir correctement."),vp=u(),Fn=r("p"),$k=n("Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),kp=u(),m(Ct.$$.fragment),_p=u(),m(Tt.$$.fragment),hp=u(),Wn=r("p"),zk=n("Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),Ep=u(),m(Dt.$$.fragment),$p=u(),m(Lt.$$.fragment),zp=u(),ks=r("p"),qk=n("Nous avons presque fini de construire ce "),ul=r("em"),jk=n("tokenizer"),xk=n(" \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),qp=u(),m(Ot.$$.fragment),jp=u(),_s=r("p"),bk=n("Testons-le sur notre pr\xE9c\xE9dent "),cl=r("code"),gk=n("encoding"),Pk=n(" :"),xp=u(),m(Mt.$$.fragment),bp=u(),m(yt.$$.fragment),gp=u(),hs=r("p"),wk=n("G\xE9nial ! Nous pouvons enregistrer notre "),ml=r("em"),Ck=n("tokenizer"),Tk=n(" dans un seul fichier JSON comme ceci :"),Pp=u(),m(St.$$.fragment),wp=u(),Ue=r("p"),Dk=n("Nous pouvons alors recharger ce fichier dans un objet "),dl=r("code"),Lk=n("Tokenizer"),Ok=n(" avec la m\xE9thode "),fl=r("code"),Mk=n("from_file()"),yk=n(" :"),Cp=u(),m(Nt.$$.fragment),Tp=u(),y=r("p"),Sk=n("Pour utiliser ce "),vl=r("em"),Nk=n("tokenizer"),Bk=n(" dans \u{1F917} "),kl=r("em"),Ak=n("Transformers"),Uk=n(", nous devons l\u2019envelopper dans un "),_l=r("code"),Fk=n("PreTrainedTokenizerFast"),Wk=n(". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),hl=r("em"),Rk=n("tokenizer"),Ik=n(" correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),El=r("code"),Gk=n("BertTokenizerFast"),Vk=n("). Si vous appliquez cette logique pour construire un tout nouveau "),$l=r("em"),Xk=n("tokenizer"),Kk=n(", vous devrez utiliser la premi\xE8re option."),Dp=u(),z=r("p"),Hk=n("Pour envelopper le "),zl=r("em"),Jk=n("tokenizer"),Yk=n(" dans un "),ql=r("code"),Zk=n("PreTrainedTokenizerFast"),Qk=n(", nous pouvons soit passer le "),jl=r("em"),e2=n("tokenizer"),s2=n(" que nous avons construit comme un "),xl=r("code"),t2=n("tokenizer_object"),n2=n(", soit passer le fichier de "),bl=r("em"),o2=n("tokenizer"),r2=n(" que nous avons sauvegard\xE9 comme "),gl=r("code"),l2=n("tokenizer_file"),a2=n(". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),Pl=r("em"),i2=n("tokens"),p2=n(" sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),wl=r("code"),u2=n("tokenizer"),c2=n(" quel "),Cl=r("em"),m2=n("token"),d2=n(" est le "),Tl=r("em"),f2=n("token"),v2=n(" de masque, quel est le "),Dl=r("em"),k2=n("token"),Ll=r("code"),_2=n("[CLS]"),h2=n(", etc :"),Lp=u(),m(Bt.$$.fragment),Op=u(),fe=r("p"),E2=n("Si vous utilisez une classe de "),Ol=r("em"),$2=n("tokenizer"),z2=n(" sp\xE9cifique (comme "),Ml=r("code"),q2=n("BertTokenizerFast"),j2=n("), vous aurez seulement besoin de sp\xE9cifier les "),yl=r("em"),x2=n("tokens"),b2=n(" sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Mp=u(),m(At.$$.fragment),yp=u(),S=r("p"),g2=n("Vous pouvez ensuite utiliser ce "),Sl=r("em"),P2=n("tokenizer"),w2=n(" comme n\u2019importe quel autre "),Nl=r("em"),C2=n("tokenizer"),T2=n(" de \u{1F917} "),Bl=r("em"),D2=n("Transformers"),L2=n(". Vous pouvez le sauvegarder avec la m\xE9thode "),Al=r("code"),O2=n("save_pretrained()"),M2=n(" ou le t\xE9l\xE9charger sur le "),Ul=r("em"),y2=n("Hub"),S2=n(" avec la m\xE9thode "),Fl=r("code"),N2=n("push_to_hub()"),B2=n("."),Sp=u(),Fe=r("p"),A2=n("Maintenant que nous avons vu comment construire un "),Wl=r("em"),U2=n("tokenizer WordPiece"),F2=n(", faisons de m\xEAme pour un "),Rl=r("em"),W2=n("tokenizer"),R2=n(" BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),Np=u(),Qe=r("h2"),Es=r("a"),Il=r("span"),m(Ut.$$.fragment),I2=u(),Ft=r("span"),G2=n("Construire un "),Gl=r("i"),V2=n("tokenizer"),X2=n(" BPE \xE0 partir de z\xE9ro"),Bp=u(),ve=r("p"),K2=n("Construisons maintenant un "),Vl=r("em"),H2=n("tokenizer"),J2=n(" BPE. Comme pour le "),Xl=r("em"),Y2=n("tokenizer"),Z2=n(" BERT, nous commen\xE7ons par initialiser un "),Kl=r("code"),Q2=n("Tokenizer"),e_=n(" avec un mod\xE8le BPE :"),Ap=u(),m(Wt.$$.fragment),Up=u(),ke=r("p"),s_=n("Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Hl=r("code"),t_=n("vocab"),n_=n(" et le "),Jl=r("code"),o_=n("merges"),r_=n(" dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Yl=r("code"),l_=n("unk_token"),a_=n(" parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Fp=u(),Rn=r("p"),i_=n("GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),Wp=u(),m(Rt.$$.fragment),Rp=u(),$s=r("p"),p_=n("L\u2019option que nous avons ajout\xE9e \xE0 "),Zl=r("code"),u_=n("ByteLevel"),c_=n(" ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),Ip=u(),m(It.$$.fragment),Gp=u(),m(Gt.$$.fragment),Vp=u(),We=r("p"),m_=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Ql=r("em"),d_=n("token"),f_=n(" sp\xE9cial est le "),ea=r("em"),v_=n("token"),k_=n(" de fin de texte :"),Xp=u(),m(Vt.$$.fragment),Kp=u(),N=r("p"),__=n("Comme avec le "),sa=r("code"),h_=n("WordPieceTrainer"),E_=n(", ainsi que le "),ta=r("code"),$_=n("vocab_size"),z_=n(" et le "),na=r("code"),q_=n("special_tokens"),j_=n(", nous pouvons sp\xE9cifier la "),oa=r("code"),x_=n("min_frequency"),b_=n(" si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ra=r("code"),g_=n("</w>"),P_=n("), nous pouvons le d\xE9finir avec "),la=r("code"),w_=n("end_of_word_suffix"),C_=n("."),Hp=u(),zs=r("p"),T_=n("Ce "),aa=r("em"),D_=n("tokenizer"),L_=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Jp=u(),m(Xt.$$.fragment),Yp=u(),In=r("p"),O_=n("Regardons la tokenisation d\u2019un exemple de texte :"),Zp=u(),m(Kt.$$.fragment),Qp=u(),m(Ht.$$.fragment),eu=u(),qs=r("p"),M_=n("Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),ia=r("em"),y_=n("tokenizer"),S_=n(" du GPT-2 comme suit :"),su=u(),m(Jt.$$.fragment),tu=u(),C=r("p"),N_=n("L\u2019option "),pa=r("code"),B_=n("trim_offsets = False"),A_=n(" indique au post-processeur que nous devons laisser les "),ua=r("em"),U_=n("offsets"),F_=n(" des "),ca=r("em"),W_=n("tokens"),R_=n(" qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ma=r("em"),I_=n("offsets"),G_=n(" pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),da=r("em"),V_=n("token"),X_=n("). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),fa=r("code"),K_=n("'\u0120test'"),H_=n(" est le "),va=r("em"),J_=n("token"),Y_=n(" \xE0 l\u2019index 4 :"),nu=u(),m(Yt.$$.fragment),ou=u(),m(Zt.$$.fragment),ru=u(),Gn=r("p"),Z_=n("Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),lu=u(),m(Qt.$$.fragment),au=u(),Vn=r("p"),Q_=n("et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),iu=u(),m(en.$$.fragment),pu=u(),m(sn.$$.fragment),uu=u(),_e=r("p"),eh=n("Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),ka=r("code"),sh=n("PreTrainedTokenizerFast"),th=n(" ou un "),_a=r("code"),nh=n("GPT2TokenizerFast"),oh=n(" si nous voulons l\u2019utiliser dans \u{1F917} "),ha=r("em"),rh=n("Transformers"),lh=n(" :"),cu=u(),m(tn.$$.fragment),mu=u(),Xn=r("p"),ah=n("ou :"),du=u(),m(nn.$$.fragment),fu=u(),Re=r("p"),ih=n("Comme dernier exemple, nous allons vous montrer comment construire un "),Ea=r("em"),ph=n("tokenizer"),uh=u(),$a=r("em"),ch=n("Unigram"),mh=n(" \xE0 partir de z\xE9ro."),vu=u(),es=r("h2"),js=r("a"),za=r("span"),m(on.$$.fragment),dh=u(),rn=r("span"),fh=n("Construire un "),qa=r("i"),vh=n("tokenizer Unigram"),kh=n(" \xE0 partir de z\xE9ro"),ku=u(),Y=r("p"),_h=n("Construisons maintenant un "),ja=r("em"),hh=n("tokenizer"),Eh=n(" XLNet. Comme pour les "),xa=r("em"),$h=n("tokenizers"),zh=n(" pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ba=r("code"),qh=n("Tokenizer"),jh=n(" avec un mod\xE8le "),ga=r("em"),xh=n("Unigram"),bh=n(" :"),_u=u(),m(ln.$$.fragment),hu=u(),Kn=r("p"),gh=n("Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),Eu=u(),xs=r("p"),Ph=n("Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),Pa=r("em"),wh=n("SentencePiece"),Ch=n(") :"),$u=u(),m(an.$$.fragment),zu=u(),he=r("p"),Th=n("Il remplace "),wa=r("code"),Dh=n("\u201C"),Lh=n(" et "),Ca=r("code"),Oh=n("\u201D"),Mh=n(" par "),Ta=r("code"),yh=n("\u201D"),Sh=n(" et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),qu=u(),Ie=r("p"),Nh=n("Le pr\xE9tokenizer \xE0 utiliser pour tout "),Da=r("em"),Bh=n("tokenizer SentencePiece"),Ah=n(" est "),La=r("code"),Uh=n("Metaspace"),Fh=n(" :"),ju=u(),m(pn.$$.fragment),xu=u(),Hn=r("p"),Wh=n("Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),bu=u(),m(un.$$.fragment),gu=u(),m(cn.$$.fragment),Pu=u(),bs=r("p"),Rh=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Oa=r("em"),Ih=n("tokens"),Gh=n(" sp\xE9ciaux :"),wu=u(),m(mn.$$.fragment),Cu=u(),T=r("p"),Vh=n("Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Ma=r("code"),Xh=n("UnigramTrainer"),Kh=n(" est le "),ya=r("code"),Hh=n("unk_token"),Jh=n(". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Sa=r("em"),Yh=n("Unigram"),Zh=n(", comme le "),Na=r("code"),Qh=n("shrinking_factor"),eE=n(" pour chaque \xE9tape o\xF9 nous enlevons des "),Ba=r("em"),sE=n("tokens"),tE=n(" (par d\xE9faut 0.75) ou le "),Aa=r("code"),nE=n("max_piece_length"),oE=n(" pour sp\xE9cifier la longueur maximale d\u2019un "),Ua=r("em"),rE=n("token"),lE=n(" donn\xE9 (par d\xE9faut 16)."),Tu=u(),gs=r("p"),aE=n("Ce "),Fa=r("em"),iE=n("tokenizer"),pE=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Du=u(),m(dn.$$.fragment),Lu=u(),Jn=r("p"),uE=n("Regardons la tokenisation de notre exemple :"),Ou=u(),m(fn.$$.fragment),Mu=u(),m(vn.$$.fragment),yu=u(),w=r("p"),cE=n("Une particularit\xE9 de XLNet est qu\u2019il place le "),Wa=r("em"),mE=n("token"),dE=u(),Ra=r("code"),fE=n("<cls>"),vE=n(" \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Ia=r("em"),kE=n("tokens"),_E=n("). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ga=r("em"),hE=n("tokens"),EE=n(" sp\xE9ciaux et les types d\u2019identifiant de "),Va=r("em"),$E=n("token"),zE=n(" avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Xa=r("em"),qE=n("tokens"),jE=u(),Ka=r("code"),xE=n("<cls>"),bE=n(" et "),Ha=r("code"),gE=n("<sep>"),PE=n(" :"),Su=u(),m(kn.$$.fragment),Nu=u(),m(_n.$$.fragment),Bu=u(),Yn=r("p"),wE=n("Le mod\xE8le ressemble \xE0 ceci :"),Au=u(),m(hn.$$.fragment),Uu=u(),Zn=r("p"),CE=n("Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),Fu=u(),m(En.$$.fragment),Wu=u(),m($n.$$.fragment),Ru=u(),Ps=r("p"),TE=n("Enfin, nous ajoutons un d\xE9codeur "),Ja=r("code"),DE=n("Metaspace"),LE=n(" :"),Iu=u(),m(zn.$$.fragment),Gu=u(),D=r("p"),OE=n("et on en a fini avec ce "),Ya=r("em"),ME=n("tokenizer"),yE=n(" ! On peut le sauvegarder et l\u2019envelopper dans un "),Za=r("code"),SE=n("PreTrainedTokenizerFast"),NE=n(" ou "),Qa=r("code"),BE=n("XLNetTokenizerFast"),AE=n(" si on veut l\u2019utiliser dans \u{1F917} "),ei=r("em"),UE=n("Transformers"),FE=n(". Une chose \xE0 noter lors de l\u2019utilisation de "),si=r("code"),WE=n("PreTrainedTokenizerFast"),RE=n(" est qu\u2019en plus des "),ti=r("em"),IE=n("tokens"),GE=n(" sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ni=r("em"),VE=n("Transformers"),XE=n(" de rembourrer \xE0 gauche :"),Vu=u(),m(qn.$$.fragment),Xu=u(),Qn=r("p"),KE=n("Ou alternativement :"),Ku=u(),m(jn.$$.fragment),Hu=u(),Z=r("p"),HE=n("Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),oi=r("em"),JE=n("tokenizers"),YE=n(" existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),ri=r("em"),ZE=n("tokenizer"),QE=n(" que vous voulez avec la biblioth\xE8que \u{1F917} "),li=r("em"),e$=n("Tokenizers"),s$=n(" et pouvoir l\u2019utiliser dans \u{1F917} "),ai=r("em"),t$=n("Transformers"),n$=n("."),this.h()},l(e){const i=Iq('[data-svelte="svelte-1phssyn"]',document.head);$=l(i,"META",{name:!0,content:!0}),i.forEach(t),ge=c(e),oe=l(e,"H1",{class:!0});var xn=a(oe);pe=l(xn,"A",{id:!0,class:!0,href:!0});var l$=a(pe);qe=l(l$,"SPAN",{});var a$=a(qe);d(je.$$.fragment,a$),a$.forEach(t),l$.forEach(t),As=c(xn),re=l(xn,"SPAN",{});var Yu=a(re);Us=o(Yu,"Construction d'un "),Xe=l(Yu,"I",{});var i$=a(Xe);Ke=o(i$,"tokenizer"),i$.forEach(t),Fs=o(Yu,", bloc par bloc"),Yu.forEach(t),xn.forEach(t),ss=c(e),d(le.$$.fragment,e),ts=c(e),Pe=l(e,"P",{});var p$=a(Pe);He=o(p$,"Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),p$.forEach(t),ns=c(e),W=l(e,"UL",{});var ws=a(W);xe=l(ws,"LI",{});var u$=a(xe);Ws=o(u$,"normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),u$.forEach(t),Rs=c(ws),be=l(ws,"LI",{});var c$=a(be);Is=o(c$,"pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),c$.forEach(t),Gs=c(ws),ae=l(ws,"LI",{});var Zu=a(ae);x=o(Zu,"passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),os=l(Zu,"EM",{});var m$=a(os);Tn=o(m$,"tokens"),m$.forEach(t),Dn=o(Zu,"),"),Zu.forEach(t),Ln=c(ws),J=l(ws,"LI",{});var Cs=a(J);On=o(Cs,"post-traitement (ajout des "),rs=l(Cs,"EM",{});var d$=a(rs);Mn=o(d$,"tokens"),d$.forEach(t),xc=o(Cs," sp\xE9ciaux du "),mo=l(Cs,"EM",{});var f$=a(mo);bc=o(f$,"tokenizer"),f$.forEach(t),gc=o(Cs,", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),fo=l(Cs,"EM",{});var v$=a(fo);Pc=o(v$,"token"),v$.forEach(t),wc=o(Cs,")."),Cs.forEach(t),ws.forEach(t),ui=c(e),yn=l(e,"P",{});var k$=a(yn);Cc=o(k$,"Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),k$.forEach(t),ci=c(e),Je=l(e,"DIV",{class:!0});var Qu=a(Je);Vs=l(Qu,"IMG",{class:!0,src:!0,alt:!0}),Tc=c(Qu),Xs=l(Qu,"IMG",{class:!0,src:!0,alt:!0}),Qu.forEach(t),mi=c(e),R=l(e,"P",{});var Ee=a(R);Dc=o(Ee,"La biblioth\xE8que \u{1F917} "),vo=l(Ee,"EM",{});var _$=a(vo);Lc=o(_$,"Tokenizers"),_$.forEach(t),Oc=o(Ee," a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),ko=l(Ee,"EM",{});var h$=a(ko);Mc=o(h$,"tokenizer"),h$.forEach(t),yc=o(Ee," \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),_o=l(Ee,"EM",{});var E$=a(_o);Sc=o(E$,"tokenizer"),E$.forEach(t),Nc=o(Ee," \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=l(Ee,"A",{href:!0});var $$=a(Sn);Bc=o($$,"section 2"),$$.forEach(t),Ac=o(Ee,". Vous serez alors en mesure de construire n\u2019importe quel type de "),ho=l(Ee,"EM",{});var z$=a(ho);Uc=o(z$,"tokenizer"),z$.forEach(t),Fc=o(Ee," auquel vous pouvez penser !"),Ee.forEach(t),di=c(e),d(Ks.$$.fragment,e),fi=c(e),ls=l(e,"P",{});var ec=a(ls);Wc=o(ec,"Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),Eo=l(ec,"CODE",{});var q$=a(Eo);Rc=o(q$,"Tokenizer"),q$.forEach(t),Ic=o(ec," avec les blocs de construction regroup\xE9s en sous-modules :"),ec.forEach(t),vi=c(e),I=l(e,"UL",{});var $e=a(I);we=l($e,"LI",{});var bn=a(we);$o=l(bn,"CODE",{});var j$=a($o);Gc=o(j$,"normalizers"),j$.forEach(t),Vc=o(bn," contient tous les types de "),zo=l(bn,"CODE",{});var x$=a(zo);Xc=o(x$,"Normalizer"),x$.forEach(t),Kc=o(bn," que vous pouvez utiliser (liste compl\xE8te "),Hs=l(bn,"A",{href:!0,rel:!0});var b$=a(Hs);Hc=o(b$,"ici"),b$.forEach(t),Jc=o(bn,"),"),bn.forEach(t),Yc=c($e),Ce=l($e,"LI",{});var gn=a(Ce);qo=l(gn,"CODE",{});var g$=a(qo);Zc=o(g$,"pre_tokenizers"),g$.forEach(t),Qc=o(gn," contient tous les types de "),jo=l(gn,"CODE",{});var P$=a(jo);em=o(P$,"PreTokenizer"),P$.forEach(t),sm=o(gn," que vous pouvez utiliser (liste compl\xE8te "),Js=l(gn,"A",{href:!0,rel:!0});var w$=a(Js);tm=o(w$,"ici"),w$.forEach(t),nm=o(gn,"),"),gn.forEach(t),om=c($e),G=l($e,"LI",{});var ie=a(G);xo=l(ie,"CODE",{});var C$=a(xo);rm=o(C$,"models"),C$.forEach(t),lm=o(ie," contient les diff\xE9rents types de "),bo=l(ie,"CODE",{});var T$=a(bo);am=o(T$,"Model"),T$.forEach(t),im=o(ie," que vous pouvez utiliser, comme "),go=l(ie,"CODE",{});var D$=a(go);pm=o(D$,"BPE"),D$.forEach(t),um=o(ie,", "),Po=l(ie,"CODE",{});var L$=a(Po);cm=o(L$,"WordPiece"),L$.forEach(t),mm=o(ie,", et "),wo=l(ie,"CODE",{});var O$=a(wo);dm=o(O$,"Unigram"),O$.forEach(t),fm=o(ie," (liste compl\xE8te "),Ys=l(ie,"A",{href:!0,rel:!0});var M$=a(Ys);vm=o(M$,"ici"),M$.forEach(t),km=o(ie,"),"),ie.forEach(t),_m=c($e),Te=l($e,"LI",{});var Pn=a(Te);Co=l(Pn,"CODE",{});var y$=a(Co);hm=o(y$,"trainers"),y$.forEach(t),Em=o(Pn," contient tous les diff\xE9rents types de "),To=l(Pn,"CODE",{});var S$=a(To);$m=o(S$,"Trainer"),S$.forEach(t),zm=o(Pn," que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=l(Pn,"A",{href:!0,rel:!0});var N$=a(Zs);qm=o(N$,"ici"),N$.forEach(t),jm=o(Pn,"),"),Pn.forEach(t),xm=c($e),De=l($e,"LI",{});var wn=a(De);Do=l(wn,"CODE",{});var B$=a(Do);bm=o(B$,"post_processors"),B$.forEach(t),gm=o(wn," contient les diff\xE9rents types de "),Lo=l(wn,"CODE",{});var A$=a(Lo);Pm=o(A$,"PostProcessor"),A$.forEach(t),wm=o(wn," que vous pouvez utiliser (liste compl\xE8te "),Qs=l(wn,"A",{href:!0,rel:!0});var U$=a(Qs);Cm=o(U$,"ici"),U$.forEach(t),Tm=o(wn,"),"),wn.forEach(t),Dm=c($e),Le=l($e,"LI",{});var Cn=a(Le);Oo=l(Cn,"CODE",{});var F$=a(Oo);Lm=o(F$,"decoders"),F$.forEach(t),Om=o(Cn," contient les diff\xE9rents types de "),Mo=l(Cn,"CODE",{});var W$=a(Mo);Mm=o(W$,"Decoder"),W$.forEach(t),ym=o(Cn," que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=l(Cn,"A",{href:!0,rel:!0});var R$=a(et);Sm=o(R$,"ici"),R$.forEach(t),Nm=o(Cn,")."),Cn.forEach(t),$e.forEach(t),ki=c(e),as=l(e,"P",{});var sc=a(as);Bm=o(sc,"Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=l(sc,"A",{href:!0,rel:!0});var I$=a(st);Am=o(I$,"ici"),I$.forEach(t),Um=o(sc,"."),sc.forEach(t),_i=c(e),Ye=l(e,"H2",{class:!0});var tc=a(Ye);is=l(tc,"A",{id:!0,class:!0,href:!0});var G$=a(is);yo=l(G$,"SPAN",{});var V$=a(yo);d(tt.$$.fragment,V$),V$.forEach(t),G$.forEach(t),Fm=c(tc),So=l(tc,"SPAN",{});var X$=a(So);Wm=o(X$,"Acquisition d'un corpus"),X$.forEach(t),tc.forEach(t),hi=c(e),ue=l(e,"P",{});var Ts=a(ue);Rm=o(Ts,"Pour entra\xEEner notre nouveau "),No=l(Ts,"EM",{});var K$=a(No);Im=o(K$,"tokenizer"),K$.forEach(t),Gm=o(Ts,", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=l(Ts,"A",{href:!0});var H$=a(Nn);Vm=o(H$,"d\xE9but du chapitre"),H$.forEach(t),Xm=o(Ts,", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=l(Ts,"A",{href:!0,rel:!0});var J$=a(nt);Km=o(J$,"WikiText-2"),J$.forEach(t),Hm=o(Ts," :"),Ts.forEach(t),Ei=c(e),d(ot.$$.fragment,e),$i=c(e),Oe=l(e,"P",{});var eo=a(Oe);Jm=o(eo,"La fonction "),Bo=l(eo,"CODE",{});var Y$=a(Bo);Ym=o(Y$,"get_training_corpus()"),Y$.forEach(t),Zm=o(eo," est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Ao=l(eo,"EM",{});var Z$=a(Ao);Qm=o(Z$,"tokenizer"),Z$.forEach(t),ed=o(eo,"."),eo.forEach(t),zi=c(e),ps=l(e,"P",{});var nc=a(ps);sd=o(nc,"\u{1F917} "),Uo=l(nc,"EM",{});var Q$=a(Uo);td=o(Q$,"Tokenizers"),Q$.forEach(t),nd=o(nc," peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),nc.forEach(t),qi=c(e),d(rt.$$.fragment,e),ji=c(e),ce=l(e,"P",{});var Ds=a(ce);od=o(Ds,"Ensuite, nous vous montrerons comment construire vos propres "),Fo=l(Ds,"EM",{});var e1=a(Fo);rd=o(e1,"tokenizers"),e1.forEach(t),ld=o(Ds," pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Wo=l(Ds,"EM",{});var s1=a(Wo);ad=o(s1,"WordPiece"),s1.forEach(t),id=o(Ds,", BPE et "),Ro=l(Ds,"EM",{});var t1=a(Ro);pd=o(t1,"Unigram"),t1.forEach(t),ud=o(Ds,". Commen\xE7ons par BERT !"),Ds.forEach(t),xi=c(e),Ze=l(e,"H2",{class:!0});var oc=a(Ze);us=l(oc,"A",{id:!0,class:!0,href:!0});var n1=a(us);Io=l(n1,"SPAN",{});var o1=a(Io);d(lt.$$.fragment,o1),o1.forEach(t),n1.forEach(t),cd=c(oc),at=l(oc,"SPAN",{});var rc=a(at);md=o(rc,"Construire un "),Go=l(rc,"I",{});var r1=a(Go);dd=o(r1,"tokenizer WordPiece"),r1.forEach(t),fd=o(rc," \xE0 partir de z\xE9ro"),rc.forEach(t),oc.forEach(t),bi=c(e),b=l(e,"P",{});var B=a(b);vd=o(B,"Pour construire un "),Vo=l(B,"EM",{});var l1=a(Vo);kd=o(l1,"tokenizer"),l1.forEach(t),_d=o(B," avec la biblioth\xE8que \u{1F917} "),Xo=l(B,"EM",{});var a1=a(Xo);hd=o(a1,"Tokenizers"),a1.forEach(t),Ed=o(B,", nous commen\xE7ons par instancier un objet "),Ko=l(B,"CODE",{});var i1=a(Ko);$d=o(i1,"Tokenizer"),i1.forEach(t),zd=o(B," avec un "),Ho=l(B,"CODE",{});var p1=a(Ho);qd=o(p1,"model"),p1.forEach(t),jd=o(B,". Puis nous d\xE9finissons ses attributs "),Jo=l(B,"CODE",{});var u1=a(Jo);xd=o(u1,"normalizer"),u1.forEach(t),bd=o(B,", "),Yo=l(B,"CODE",{});var c1=a(Yo);gd=o(c1,"pre_tokenizer"),c1.forEach(t),Pd=o(B,", "),Zo=l(B,"CODE",{});var m1=a(Zo);wd=o(m1,"post_processor"),m1.forEach(t),Cd=o(B," et "),Qo=l(B,"CODE",{});var d1=a(Qo);Td=o(d1,"decoder"),d1.forEach(t),Dd=o(B," aux valeurs que nous voulons."),B.forEach(t),gi=c(e),Me=l(e,"P",{});var so=a(Me);Ld=o(so,"Pour cet exemple, nous allons cr\xE9er un "),er=l(so,"CODE",{});var f1=a(er);Od=o(f1,"Tokenizer"),f1.forEach(t),Md=o(so," avec un mod\xE8le "),sr=l(so,"EM",{});var v1=a(sr);yd=o(v1,"WordPiece"),v1.forEach(t),Sd=o(so," :"),so.forEach(t),Pi=c(e),d(it.$$.fragment,e),wi=c(e),me=l(e,"P",{});var Ls=a(me);Nd=o(Ls,"Nous devons sp\xE9cifier le "),tr=l(Ls,"CODE",{});var k1=a(tr);Bd=o(k1,"unk_token"),k1.forEach(t),Ad=o(Ls," pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),nr=l(Ls,"CODE",{});var _1=a(nr);Ud=o(_1,"vocab"),_1.forEach(t),Fd=o(Ls," de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),or=l(Ls,"CODE",{});var h1=a(or);Wd=o(h1,"max_input_chars_per_word"),h1.forEach(t),Rd=o(Ls,", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ls.forEach(t),Ci=c(e),g=l(e,"P",{});var A=a(g);Id=o(A,"La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),rr=l(A,"CODE",{});var E1=a(rr);Gd=o(E1,"BertNormalizer"),E1.forEach(t),Vd=o(A," a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),lr=l(A,"CODE",{});var $1=a(lr);Xd=o($1,"lowercase"),$1.forEach(t),Kd=o(A," pour mettre le texte en minuscule, "),ar=l(A,"CODE",{});var z1=a(ar);Hd=o(z1,"strip_accents"),z1.forEach(t),Jd=o(A," qui enl\xE8ve les accents, "),ir=l(A,"CODE",{});var q1=a(ir);Yd=o(q1,"clean_text"),q1.forEach(t),Zd=o(A," pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),pr=l(A,"CODE",{});var j1=a(pr);Qd=o(j1,"handle_chinese_chars"),j1.forEach(t),ef=o(A," qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),ur=l(A,"EM",{});var x1=a(ur);sf=o(x1,"tokenizer"),x1.forEach(t),tf=c(A),cr=l(A,"CODE",{});var b1=a(cr);nf=o(b1,"bert-base-uncased"),b1.forEach(t),of=o(A,", nous pouvons simplement d\xE9finir ce "),mr=l(A,"EM",{});var g1=a(mr);rf=o(g1,"normalizer"),g1.forEach(t),lf=o(A," :"),A.forEach(t),Ti=c(e),d(pt.$$.fragment,e),Di=c(e),V=l(e,"P",{});var ze=a(V);af=o(ze,"Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),dr=l(ze,"EM",{});var P1=a(dr);pf=o(P1,"tokenizer"),P1.forEach(t),uf=o(ze,", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),fr=l(ze,"EM",{});var w1=a(fr);cf=o(w1,"Tokenizers"),w1.forEach(t),mf=o(ze,". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),vr=l(ze,"CODE",{});var C1=a(vr);df=o(C1,"Lowercase"),C1.forEach(t),ff=o(ze," et un normaliseur "),kr=l(ze,"CODE",{});var T1=a(kr);vf=o(T1,"StripAccents"),T1.forEach(t),kf=o(ze,". Il est possible de composer plusieurs normaliseurs en utilisant une "),_r=l(ze,"CODE",{});var D1=a(_r);_f=o(D1,"Sequence"),D1.forEach(t),hf=o(ze," :"),ze.forEach(t),Li=c(e),d(ut.$$.fragment,e),Oi=c(e),ye=l(e,"P",{});var to=a(ye);Ef=o(to,"Nous utilisons \xE9galement un normaliseur Unicode "),hr=l(to,"CODE",{});var L1=a(hr);$f=o(L1,"NFD"),L1.forEach(t),zf=o(to,", car sinon "),Er=l(to,"CODE",{});var O1=a(Er);qf=o(O1,"StripAccents"),O1.forEach(t),jf=o(to," ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),to.forEach(t),Mi=c(e),Se=l(e,"P",{});var no=a(Se);xf=o(no,"Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),$r=l(no,"CODE",{});var M1=a($r);bf=o(M1,"normalize_str()"),M1.forEach(t),gf=o(no," du "),zr=l(no,"CODE",{});var y1=a(zr);Pf=o(y1,"normalizer"),y1.forEach(t),wf=o(no," pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),no.forEach(t),yi=c(e),d(ct.$$.fragment,e),Si=c(e),d(mt.$$.fragment,e),Ni=c(e),d(cs.$$.fragment,e),Bi=c(e),ms=l(e,"P",{});var lc=a(ms);Cf=o(lc,"L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),qr=l(lc,"CODE",{});var S1=a(qr);Tf=o(S1,"BertPreTokenizer"),S1.forEach(t),Df=o(lc," pr\xE9construit que nous pouvons utiliser :"),lc.forEach(t),Ai=c(e),d(dt.$$.fragment,e),Ui=c(e),Bn=l(e,"P",{});var N1=a(Bn);Lf=o(N1,"Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),N1.forEach(t),Fi=c(e),d(ft.$$.fragment,e),Wi=c(e),ds=l(e,"P",{});var ac=a(ds);Of=o(ac,"Notez que le "),jr=l(ac,"CODE",{});var B1=a(jr);Mf=o(B1,"Whitespace"),B1.forEach(t),yf=o(ac," divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),ac.forEach(t),Ri=c(e),d(vt.$$.fragment,e),Ii=c(e),d(kt.$$.fragment,e),Gi=c(e),fs=l(e,"P",{});var ic=a(fs);Sf=o(ic,"Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),xr=l(ic,"CODE",{});var A1=a(xr);Nf=o(A1,"WhitespaceSplit"),A1.forEach(t),Bf=o(ic," \xE0 la place :"),ic.forEach(t),Vi=c(e),d(_t.$$.fragment,e),Xi=c(e),d(ht.$$.fragment,e),Ki=c(e),vs=l(e,"P",{});var pc=a(vs);Af=o(pc,"Comme pour les normaliseurs, vous pouvez utiliser une "),br=l(pc,"CODE",{});var U1=a(br);Uf=o(U1,"Sequence"),U1.forEach(t),Ff=o(pc," pour composer plusieurs pr\xE9tokenizers :"),pc.forEach(t),Hi=c(e),d(Et.$$.fragment,e),Ji=c(e),d($t.$$.fragment,e),Yi=c(e),de=l(e,"P",{});var Os=a(de);Wf=o(Os,"L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),gr=l(Os,"CODE",{});var F1=a(gr);Rf=o(F1,"WordPieceTrainer"),F1.forEach(t),If=o(Os,". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Pr=l(Os,"EM",{});var W1=a(Pr);Gf=o(W1,"Tokenizers"),W1.forEach(t),Vf=o(Os," est que vous devez lui passer tous les "),wr=l(Os,"EM",{});var R1=a(wr);Xf=o(R1,"tokens"),R1.forEach(t),Kf=o(Os," sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Os.forEach(t),Zi=c(e),d(zt.$$.fragment,e),Qi=c(e),O=l(e,"P",{});var Q=a(O);Hf=o(Q,"En plus de sp\xE9cifier la "),Cr=l(Q,"CODE",{});var I1=a(Cr);Jf=o(I1,"vocab_size"),I1.forEach(t),Yf=o(Q," et les "),Tr=l(Q,"CODE",{});var G1=a(Tr);Zf=o(G1,"special_tokens"),G1.forEach(t),Qf=o(Q,", nous pouvons d\xE9finir la "),Dr=l(Q,"CODE",{});var V1=a(Dr);ev=o(V1,"min_frequency"),V1.forEach(t),sv=o(Q," (le nombre de fois qu\u2019un "),Lr=l(Q,"EM",{});var X1=a(Lr);tv=o(X1,"token"),X1.forEach(t),nv=o(Q," doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Or=l(Q,"CODE",{});var K1=a(Or);ov=o(K1,"continuing_subword_prefix"),K1.forEach(t),rv=o(Q," (si nous voulons utiliser quelque chose de diff\xE9rent de "),Mr=l(Q,"CODE",{});var H1=a(Mr);lv=o(H1,"##"),H1.forEach(t),av=o(Q,")."),Q.forEach(t),ep=c(e),An=l(e,"P",{});var J1=a(An);iv=o(J1,"Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),J1.forEach(t),sp=c(e),d(qt.$$.fragment,e),tp=c(e),Ne=l(e,"P",{});var oo=a(Ne);pv=o(oo,"Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),yr=l(oo,"EM",{});var Y1=a(yr);uv=o(Y1,"tokenizer"),Y1.forEach(t),cv=o(oo," qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Sr=l(oo,"CODE",{});var Z1=a(Sr);mv=o(Z1,"WordPiece"),Z1.forEach(t),dv=o(oo," vide au pr\xE9alable) :"),oo.forEach(t),np=c(e),d(jt.$$.fragment,e),op=c(e),Be=l(e,"P",{});var ro=a(Be);fv=o(ro,"Dans les deux cas, nous pouvons ensuite tester le "),Nr=l(ro,"EM",{});var Q1=a(Nr);vv=o(Q1,"tokenizer"),Q1.forEach(t),kv=o(ro," sur un texte en appelant la m\xE9thode "),Br=l(ro,"CODE",{});var ez=a(Br);_v=o(ez,"encode()"),ez.forEach(t),hv=o(ro," :"),ro.forEach(t),rp=c(e),d(xt.$$.fragment,e),lp=c(e),d(bt.$$.fragment,e),ap=c(e),j=l(e,"P",{});var L=a(j);Ev=o(L,"L\u2019encodage obtenu est un "),Ar=l(L,"CODE",{});var sz=a(Ar);$v=o(sz,"Encoding"),sz.forEach(t),zv=o(L," contenant toutes les sorties n\xE9cessaires du "),Ur=l(L,"EM",{});var tz=a(Ur);qv=o(tz,"tokenizer"),tz.forEach(t),jv=o(L," dans ses diff\xE9rents attributs : "),Fr=l(L,"CODE",{});var nz=a(Fr);xv=o(nz,"ids"),nz.forEach(t),bv=o(L,", "),Wr=l(L,"CODE",{});var oz=a(Wr);gv=o(oz,"type_ids"),oz.forEach(t),Pv=o(L,", "),Rr=l(L,"CODE",{});var rz=a(Rr);wv=o(rz,"tokens"),rz.forEach(t),Cv=o(L,", "),Ir=l(L,"CODE",{});var lz=a(Ir);Tv=o(lz,"offsets"),lz.forEach(t),Dv=o(L,", "),Gr=l(L,"CODE",{});var az=a(Gr);Lv=o(az,"attention_mask"),az.forEach(t),Ov=o(L,", "),Vr=l(L,"CODE",{});var iz=a(Vr);Mv=o(iz,"special_tokens_mask"),iz.forEach(t),yv=o(L," et "),Xr=l(L,"CODE",{});var pz=a(Xr);Sv=o(pz,"overflowing"),pz.forEach(t),Nv=o(L,"."),L.forEach(t),ip=c(e),P=l(e,"P",{});var U=a(P);Bv=o(U,"La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Kr=l(U,"EM",{});var uz=a(Kr);Av=o(uz,"token"),uz.forEach(t),Uv=c(U),Hr=l(U,"CODE",{});var cz=a(Hr);Fv=o(cz,"[CLS]"),cz.forEach(t),Wv=o(U," au d\xE9but et le "),Jr=l(U,"EM",{});var mz=a(Jr);Rv=o(mz,"token"),mz.forEach(t),Iv=c(U),Yr=l(U,"CODE",{});var dz=a(Yr);Gv=o(dz,"[SEP]"),dz.forEach(t),Vv=o(U," \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Zr=l(U,"CODE",{});var fz=a(Zr);Xv=o(fz,"TemplateProcessor"),fz.forEach(t),Kv=o(U," pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Qr=l(U,"EM",{});var vz=a(Qr);Hv=o(vz,"tokens"),vz.forEach(t),Jv=c(U),el=l(U,"CODE",{});var kz=a(el);Yv=o(kz,"[CLS]"),kz.forEach(t),Zv=o(U," et "),sl=l(U,"CODE",{});var _z=a(sl);Qv=o(_z,"[SEP]"),_z.forEach(t),ek=o(U," dans le vocabulaire :"),U.forEach(t),pp=c(e),d(gt.$$.fragment,e),up=c(e),d(Pt.$$.fragment,e),cp=c(e),M=l(e,"P",{});var ee=a(M);sk=o(ee,"Pour \xE9crire le gabarit pour "),tl=l(ee,"CODE",{});var hz=a(tl);tk=o(hz,"TemplateProcessor"),hz.forEach(t),nk=o(ee,", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),nl=l(ee,"EM",{});var Ez=a(nl);ok=o(Ez,"tokens"),Ez.forEach(t),rk=o(ee," sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),ol=l(ee,"CODE",{});var $z=a(ol);lk=o($z,"$A"),$z.forEach(t),ak=o(ee,", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),rl=l(ee,"CODE",{});var zz=a(rl);ik=o(zz,"$B"),zz.forEach(t),pk=o(ee,". Pour chacun de ces \xE9l\xE9ments ("),ll=l(ee,"EM",{});var qz=a(ll);uk=o(qz,"tokens"),qz.forEach(t),ck=o(ee," sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),al=l(ee,"EM",{});var jz=a(al);mk=o(jz,"token"),jz.forEach(t),dk=o(ee," correspondant apr\xE8s un deux-points."),ee.forEach(t),mp=c(e),Un=l(e,"P",{});var xz=a(Un);fk=o(xz,"Le gabarit classique de BERT est donc d\xE9fini comme suit :"),xz.forEach(t),dp=c(e),d(wt.$$.fragment,e),fp=c(e),Ae=l(e,"P",{});var lo=a(Ae);vk=o(lo,"Notez que nous devons transmettre les identifiants des "),il=l(lo,"EM",{});var bz=a(il);kk=o(bz,"tokens"),bz.forEach(t),_k=o(lo," sp\xE9ciaux afin que le "),pl=l(lo,"EM",{});var gz=a(pl);hk=o(gz,"tokenizer"),gz.forEach(t),Ek=o(lo," puisse les convertir correctement."),lo.forEach(t),vp=c(e),Fn=l(e,"P",{});var Pz=a(Fn);$k=o(Pz,"Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),Pz.forEach(t),kp=c(e),d(Ct.$$.fragment,e),_p=c(e),d(Tt.$$.fragment,e),hp=c(e),Wn=l(e,"P",{});var wz=a(Wn);zk=o(wz,"Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),wz.forEach(t),Ep=c(e),d(Dt.$$.fragment,e),$p=c(e),d(Lt.$$.fragment,e),zp=c(e),ks=l(e,"P",{});var uc=a(ks);qk=o(uc,"Nous avons presque fini de construire ce "),ul=l(uc,"EM",{});var Cz=a(ul);jk=o(Cz,"tokenizer"),Cz.forEach(t),xk=o(uc," \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),uc.forEach(t),qp=c(e),d(Ot.$$.fragment,e),jp=c(e),_s=l(e,"P",{});var cc=a(_s);bk=o(cc,"Testons-le sur notre pr\xE9c\xE9dent "),cl=l(cc,"CODE",{});var Tz=a(cl);gk=o(Tz,"encoding"),Tz.forEach(t),Pk=o(cc," :"),cc.forEach(t),xp=c(e),d(Mt.$$.fragment,e),bp=c(e),d(yt.$$.fragment,e),gp=c(e),hs=l(e,"P",{});var mc=a(hs);wk=o(mc,"G\xE9nial ! Nous pouvons enregistrer notre "),ml=l(mc,"EM",{});var Dz=a(ml);Ck=o(Dz,"tokenizer"),Dz.forEach(t),Tk=o(mc," dans un seul fichier JSON comme ceci :"),mc.forEach(t),Pp=c(e),d(St.$$.fragment,e),wp=c(e),Ue=l(e,"P",{});var ao=a(Ue);Dk=o(ao,"Nous pouvons alors recharger ce fichier dans un objet "),dl=l(ao,"CODE",{});var Lz=a(dl);Lk=o(Lz,"Tokenizer"),Lz.forEach(t),Ok=o(ao," avec la m\xE9thode "),fl=l(ao,"CODE",{});var Oz=a(fl);Mk=o(Oz,"from_file()"),Oz.forEach(t),yk=o(ao," :"),ao.forEach(t),Cp=c(e),d(Nt.$$.fragment,e),Tp=c(e),y=l(e,"P",{});var se=a(y);Sk=o(se,"Pour utiliser ce "),vl=l(se,"EM",{});var Mz=a(vl);Nk=o(Mz,"tokenizer"),Mz.forEach(t),Bk=o(se," dans \u{1F917} "),kl=l(se,"EM",{});var yz=a(kl);Ak=o(yz,"Transformers"),yz.forEach(t),Uk=o(se,", nous devons l\u2019envelopper dans un "),_l=l(se,"CODE",{});var Sz=a(_l);Fk=o(Sz,"PreTrainedTokenizerFast"),Sz.forEach(t),Wk=o(se,". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),hl=l(se,"EM",{});var Nz=a(hl);Rk=o(Nz,"tokenizer"),Nz.forEach(t),Ik=o(se," correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),El=l(se,"CODE",{});var Bz=a(El);Gk=o(Bz,"BertTokenizerFast"),Bz.forEach(t),Vk=o(se,"). Si vous appliquez cette logique pour construire un tout nouveau "),$l=l(se,"EM",{});var Az=a($l);Xk=o(Az,"tokenizer"),Az.forEach(t),Kk=o(se,", vous devrez utiliser la premi\xE8re option."),se.forEach(t),Dp=c(e),z=l(e,"P",{});var q=a(z);Hk=o(q,"Pour envelopper le "),zl=l(q,"EM",{});var Uz=a(zl);Jk=o(Uz,"tokenizer"),Uz.forEach(t),Yk=o(q," dans un "),ql=l(q,"CODE",{});var Fz=a(ql);Zk=o(Fz,"PreTrainedTokenizerFast"),Fz.forEach(t),Qk=o(q,", nous pouvons soit passer le "),jl=l(q,"EM",{});var Wz=a(jl);e2=o(Wz,"tokenizer"),Wz.forEach(t),s2=o(q," que nous avons construit comme un "),xl=l(q,"CODE",{});var Rz=a(xl);t2=o(Rz,"tokenizer_object"),Rz.forEach(t),n2=o(q,", soit passer le fichier de "),bl=l(q,"EM",{});var Iz=a(bl);o2=o(Iz,"tokenizer"),Iz.forEach(t),r2=o(q," que nous avons sauvegard\xE9 comme "),gl=l(q,"CODE",{});var Gz=a(gl);l2=o(Gz,"tokenizer_file"),Gz.forEach(t),a2=o(q,". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),Pl=l(q,"EM",{});var Vz=a(Pl);i2=o(Vz,"tokens"),Vz.forEach(t),p2=o(q," sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),wl=l(q,"CODE",{});var Xz=a(wl);u2=o(Xz,"tokenizer"),Xz.forEach(t),c2=o(q," quel "),Cl=l(q,"EM",{});var Kz=a(Cl);m2=o(Kz,"token"),Kz.forEach(t),d2=o(q," est le "),Tl=l(q,"EM",{});var Hz=a(Tl);f2=o(Hz,"token"),Hz.forEach(t),v2=o(q," de masque, quel est le "),Dl=l(q,"EM",{});var Jz=a(Dl);k2=o(Jz,"token"),Jz.forEach(t),Ll=l(q,"CODE",{});var Yz=a(Ll);_2=o(Yz,"[CLS]"),Yz.forEach(t),h2=o(q,", etc :"),q.forEach(t),Lp=c(e),d(Bt.$$.fragment,e),Op=c(e),fe=l(e,"P",{});var Ms=a(fe);E2=o(Ms,"Si vous utilisez une classe de "),Ol=l(Ms,"EM",{});var Zz=a(Ol);$2=o(Zz,"tokenizer"),Zz.forEach(t),z2=o(Ms," sp\xE9cifique (comme "),Ml=l(Ms,"CODE",{});var Qz=a(Ml);q2=o(Qz,"BertTokenizerFast"),Qz.forEach(t),j2=o(Ms,"), vous aurez seulement besoin de sp\xE9cifier les "),yl=l(Ms,"EM",{});var e7=a(yl);x2=o(e7,"tokens"),e7.forEach(t),b2=o(Ms," sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Ms.forEach(t),Mp=c(e),d(At.$$.fragment,e),yp=c(e),S=l(e,"P",{});var te=a(S);g2=o(te,"Vous pouvez ensuite utiliser ce "),Sl=l(te,"EM",{});var s7=a(Sl);P2=o(s7,"tokenizer"),s7.forEach(t),w2=o(te," comme n\u2019importe quel autre "),Nl=l(te,"EM",{});var t7=a(Nl);C2=o(t7,"tokenizer"),t7.forEach(t),T2=o(te," de \u{1F917} "),Bl=l(te,"EM",{});var n7=a(Bl);D2=o(n7,"Transformers"),n7.forEach(t),L2=o(te,". Vous pouvez le sauvegarder avec la m\xE9thode "),Al=l(te,"CODE",{});var o7=a(Al);O2=o(o7,"save_pretrained()"),o7.forEach(t),M2=o(te," ou le t\xE9l\xE9charger sur le "),Ul=l(te,"EM",{});var r7=a(Ul);y2=o(r7,"Hub"),r7.forEach(t),S2=o(te," avec la m\xE9thode "),Fl=l(te,"CODE",{});var l7=a(Fl);N2=o(l7,"push_to_hub()"),l7.forEach(t),B2=o(te,"."),te.forEach(t),Sp=c(e),Fe=l(e,"P",{});var io=a(Fe);A2=o(io,"Maintenant que nous avons vu comment construire un "),Wl=l(io,"EM",{});var a7=a(Wl);U2=o(a7,"tokenizer WordPiece"),a7.forEach(t),F2=o(io,", faisons de m\xEAme pour un "),Rl=l(io,"EM",{});var i7=a(Rl);W2=o(i7,"tokenizer"),i7.forEach(t),R2=o(io," BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),io.forEach(t),Np=c(e),Qe=l(e,"H2",{class:!0});var dc=a(Qe);Es=l(dc,"A",{id:!0,class:!0,href:!0});var p7=a(Es);Il=l(p7,"SPAN",{});var u7=a(Il);d(Ut.$$.fragment,u7),u7.forEach(t),p7.forEach(t),I2=c(dc),Ft=l(dc,"SPAN",{});var fc=a(Ft);G2=o(fc,"Construire un "),Gl=l(fc,"I",{});var c7=a(Gl);V2=o(c7,"tokenizer"),c7.forEach(t),X2=o(fc," BPE \xE0 partir de z\xE9ro"),fc.forEach(t),dc.forEach(t),Bp=c(e),ve=l(e,"P",{});var ys=a(ve);K2=o(ys,"Construisons maintenant un "),Vl=l(ys,"EM",{});var m7=a(Vl);H2=o(m7,"tokenizer"),m7.forEach(t),J2=o(ys," BPE. Comme pour le "),Xl=l(ys,"EM",{});var d7=a(Xl);Y2=o(d7,"tokenizer"),d7.forEach(t),Z2=o(ys," BERT, nous commen\xE7ons par initialiser un "),Kl=l(ys,"CODE",{});var f7=a(Kl);Q2=o(f7,"Tokenizer"),f7.forEach(t),e_=o(ys," avec un mod\xE8le BPE :"),ys.forEach(t),Ap=c(e),d(Wt.$$.fragment,e),Up=c(e),ke=l(e,"P",{});var Ss=a(ke);s_=o(Ss,"Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Hl=l(Ss,"CODE",{});var v7=a(Hl);t_=o(v7,"vocab"),v7.forEach(t),n_=o(Ss," et le "),Jl=l(Ss,"CODE",{});var k7=a(Jl);o_=o(k7,"merges"),k7.forEach(t),r_=o(Ss," dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Yl=l(Ss,"CODE",{});var _7=a(Yl);l_=o(_7,"unk_token"),_7.forEach(t),a_=o(Ss," parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Ss.forEach(t),Fp=c(e),Rn=l(e,"P",{});var h7=a(Rn);i_=o(h7,"GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),h7.forEach(t),Wp=c(e),d(Rt.$$.fragment,e),Rp=c(e),$s=l(e,"P",{});var vc=a($s);p_=o(vc,"L\u2019option que nous avons ajout\xE9e \xE0 "),Zl=l(vc,"CODE",{});var E7=a(Zl);u_=o(E7,"ByteLevel"),E7.forEach(t),c_=o(vc," ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),vc.forEach(t),Ip=c(e),d(It.$$.fragment,e),Gp=c(e),d(Gt.$$.fragment,e),Vp=c(e),We=l(e,"P",{});var po=a(We);m_=o(po,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Ql=l(po,"EM",{});var $7=a(Ql);d_=o($7,"token"),$7.forEach(t),f_=o(po," sp\xE9cial est le "),ea=l(po,"EM",{});var z7=a(ea);v_=o(z7,"token"),z7.forEach(t),k_=o(po," de fin de texte :"),po.forEach(t),Xp=c(e),d(Vt.$$.fragment,e),Kp=c(e),N=l(e,"P",{});var ne=a(N);__=o(ne,"Comme avec le "),sa=l(ne,"CODE",{});var q7=a(sa);h_=o(q7,"WordPieceTrainer"),q7.forEach(t),E_=o(ne,", ainsi que le "),ta=l(ne,"CODE",{});var j7=a(ta);$_=o(j7,"vocab_size"),j7.forEach(t),z_=o(ne," et le "),na=l(ne,"CODE",{});var x7=a(na);q_=o(x7,"special_tokens"),x7.forEach(t),j_=o(ne,", nous pouvons sp\xE9cifier la "),oa=l(ne,"CODE",{});var b7=a(oa);x_=o(b7,"min_frequency"),b7.forEach(t),b_=o(ne," si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ra=l(ne,"CODE",{});var g7=a(ra);g_=o(g7,"</w>"),g7.forEach(t),P_=o(ne,"), nous pouvons le d\xE9finir avec "),la=l(ne,"CODE",{});var P7=a(la);w_=o(P7,"end_of_word_suffix"),P7.forEach(t),C_=o(ne,"."),ne.forEach(t),Hp=c(e),zs=l(e,"P",{});var kc=a(zs);T_=o(kc,"Ce "),aa=l(kc,"EM",{});var w7=a(aa);D_=o(w7,"tokenizer"),w7.forEach(t),L_=o(kc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),kc.forEach(t),Jp=c(e),d(Xt.$$.fragment,e),Yp=c(e),In=l(e,"P",{});var C7=a(In);O_=o(C7,"Regardons la tokenisation d\u2019un exemple de texte :"),C7.forEach(t),Zp=c(e),d(Kt.$$.fragment,e),Qp=c(e),d(Ht.$$.fragment,e),eu=c(e),qs=l(e,"P",{});var _c=a(qs);M_=o(_c,"Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),ia=l(_c,"EM",{});var T7=a(ia);y_=o(T7,"tokenizer"),T7.forEach(t),S_=o(_c," du GPT-2 comme suit :"),_c.forEach(t),su=c(e),d(Jt.$$.fragment,e),tu=c(e),C=l(e,"P",{});var X=a(C);N_=o(X,"L\u2019option "),pa=l(X,"CODE",{});var D7=a(pa);B_=o(D7,"trim_offsets = False"),D7.forEach(t),A_=o(X," indique au post-processeur que nous devons laisser les "),ua=l(X,"EM",{});var L7=a(ua);U_=o(L7,"offsets"),L7.forEach(t),F_=o(X," des "),ca=l(X,"EM",{});var O7=a(ca);W_=o(O7,"tokens"),O7.forEach(t),R_=o(X," qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ma=l(X,"EM",{});var M7=a(ma);I_=o(M7,"offsets"),M7.forEach(t),G_=o(X," pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),da=l(X,"EM",{});var y7=a(da);V_=o(y7,"token"),y7.forEach(t),X_=o(X,"). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),fa=l(X,"CODE",{});var S7=a(fa);K_=o(S7,"'\u0120test'"),S7.forEach(t),H_=o(X," est le "),va=l(X,"EM",{});var N7=a(va);J_=o(N7,"token"),N7.forEach(t),Y_=o(X," \xE0 l\u2019index 4 :"),X.forEach(t),nu=c(e),d(Yt.$$.fragment,e),ou=c(e),d(Zt.$$.fragment,e),ru=c(e),Gn=l(e,"P",{});var B7=a(Gn);Z_=o(B7,"Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),B7.forEach(t),lu=c(e),d(Qt.$$.fragment,e),au=c(e),Vn=l(e,"P",{});var A7=a(Vn);Q_=o(A7,"et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),A7.forEach(t),iu=c(e),d(en.$$.fragment,e),pu=c(e),d(sn.$$.fragment,e),uu=c(e),_e=l(e,"P",{});var Ns=a(_e);eh=o(Ns,"Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),ka=l(Ns,"CODE",{});var U7=a(ka);sh=o(U7,"PreTrainedTokenizerFast"),U7.forEach(t),th=o(Ns," ou un "),_a=l(Ns,"CODE",{});var F7=a(_a);nh=o(F7,"GPT2TokenizerFast"),F7.forEach(t),oh=o(Ns," si nous voulons l\u2019utiliser dans \u{1F917} "),ha=l(Ns,"EM",{});var W7=a(ha);rh=o(W7,"Transformers"),W7.forEach(t),lh=o(Ns," :"),Ns.forEach(t),cu=c(e),d(tn.$$.fragment,e),mu=c(e),Xn=l(e,"P",{});var R7=a(Xn);ah=o(R7,"ou :"),R7.forEach(t),du=c(e),d(nn.$$.fragment,e),fu=c(e),Re=l(e,"P",{});var uo=a(Re);ih=o(uo,"Comme dernier exemple, nous allons vous montrer comment construire un "),Ea=l(uo,"EM",{});var I7=a(Ea);ph=o(I7,"tokenizer"),I7.forEach(t),uh=c(uo),$a=l(uo,"EM",{});var G7=a($a);ch=o(G7,"Unigram"),G7.forEach(t),mh=o(uo," \xE0 partir de z\xE9ro."),uo.forEach(t),vu=c(e),es=l(e,"H2",{class:!0});var hc=a(es);js=l(hc,"A",{id:!0,class:!0,href:!0});var V7=a(js);za=l(V7,"SPAN",{});var X7=a(za);d(on.$$.fragment,X7),X7.forEach(t),V7.forEach(t),dh=c(hc),rn=l(hc,"SPAN",{});var Ec=a(rn);fh=o(Ec,"Construire un "),qa=l(Ec,"I",{});var K7=a(qa);vh=o(K7,"tokenizer Unigram"),K7.forEach(t),kh=o(Ec," \xE0 partir de z\xE9ro"),Ec.forEach(t),hc.forEach(t),ku=c(e),Y=l(e,"P",{});var Ge=a(Y);_h=o(Ge,"Construisons maintenant un "),ja=l(Ge,"EM",{});var H7=a(ja);hh=o(H7,"tokenizer"),H7.forEach(t),Eh=o(Ge," XLNet. Comme pour les "),xa=l(Ge,"EM",{});var J7=a(xa);$h=o(J7,"tokenizers"),J7.forEach(t),zh=o(Ge," pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ba=l(Ge,"CODE",{});var Y7=a(ba);qh=o(Y7,"Tokenizer"),Y7.forEach(t),jh=o(Ge," avec un mod\xE8le "),ga=l(Ge,"EM",{});var Z7=a(ga);xh=o(Z7,"Unigram"),Z7.forEach(t),bh=o(Ge," :"),Ge.forEach(t),_u=c(e),d(ln.$$.fragment,e),hu=c(e),Kn=l(e,"P",{});var Q7=a(Kn);gh=o(Q7,"Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),Q7.forEach(t),Eu=c(e),xs=l(e,"P",{});var $c=a(xs);Ph=o($c,"Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),Pa=l($c,"EM",{});var eq=a(Pa);wh=o(eq,"SentencePiece"),eq.forEach(t),Ch=o($c,") :"),$c.forEach(t),$u=c(e),d(an.$$.fragment,e),zu=c(e),he=l(e,"P",{});var Bs=a(he);Th=o(Bs,"Il remplace "),wa=l(Bs,"CODE",{});var sq=a(wa);Dh=o(sq,"\u201C"),sq.forEach(t),Lh=o(Bs," et "),Ca=l(Bs,"CODE",{});var tq=a(Ca);Oh=o(tq,"\u201D"),tq.forEach(t),Mh=o(Bs," par "),Ta=l(Bs,"CODE",{});var nq=a(Ta);yh=o(nq,"\u201D"),nq.forEach(t),Sh=o(Bs," et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),Bs.forEach(t),qu=c(e),Ie=l(e,"P",{});var co=a(Ie);Nh=o(co,"Le pr\xE9tokenizer \xE0 utiliser pour tout "),Da=l(co,"EM",{});var oq=a(Da);Bh=o(oq,"tokenizer SentencePiece"),oq.forEach(t),Ah=o(co," est "),La=l(co,"CODE",{});var rq=a(La);Uh=o(rq,"Metaspace"),rq.forEach(t),Fh=o(co," :"),co.forEach(t),ju=c(e),d(pn.$$.fragment,e),xu=c(e),Hn=l(e,"P",{});var lq=a(Hn);Wh=o(lq,"Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),lq.forEach(t),bu=c(e),d(un.$$.fragment,e),gu=c(e),d(cn.$$.fragment,e),Pu=c(e),bs=l(e,"P",{});var zc=a(bs);Rh=o(zc,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Oa=l(zc,"EM",{});var aq=a(Oa);Ih=o(aq,"tokens"),aq.forEach(t),Gh=o(zc," sp\xE9ciaux :"),zc.forEach(t),wu=c(e),d(mn.$$.fragment,e),Cu=c(e),T=l(e,"P",{});var K=a(T);Vh=o(K,"Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Ma=l(K,"CODE",{});var iq=a(Ma);Xh=o(iq,"UnigramTrainer"),iq.forEach(t),Kh=o(K," est le "),ya=l(K,"CODE",{});var pq=a(ya);Hh=o(pq,"unk_token"),pq.forEach(t),Jh=o(K,". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Sa=l(K,"EM",{});var uq=a(Sa);Yh=o(uq,"Unigram"),uq.forEach(t),Zh=o(K,", comme le "),Na=l(K,"CODE",{});var cq=a(Na);Qh=o(cq,"shrinking_factor"),cq.forEach(t),eE=o(K," pour chaque \xE9tape o\xF9 nous enlevons des "),Ba=l(K,"EM",{});var mq=a(Ba);sE=o(mq,"tokens"),mq.forEach(t),tE=o(K," (par d\xE9faut 0.75) ou le "),Aa=l(K,"CODE",{});var dq=a(Aa);nE=o(dq,"max_piece_length"),dq.forEach(t),oE=o(K," pour sp\xE9cifier la longueur maximale d\u2019un "),Ua=l(K,"EM",{});var fq=a(Ua);rE=o(fq,"token"),fq.forEach(t),lE=o(K," donn\xE9 (par d\xE9faut 16)."),K.forEach(t),Tu=c(e),gs=l(e,"P",{});var qc=a(gs);aE=o(qc,"Ce "),Fa=l(qc,"EM",{});var vq=a(Fa);iE=o(vq,"tokenizer"),vq.forEach(t),pE=o(qc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),qc.forEach(t),Du=c(e),d(dn.$$.fragment,e),Lu=c(e),Jn=l(e,"P",{});var kq=a(Jn);uE=o(kq,"Regardons la tokenisation de notre exemple :"),kq.forEach(t),Ou=c(e),d(fn.$$.fragment,e),Mu=c(e),d(vn.$$.fragment,e),yu=c(e),w=l(e,"P",{});var F=a(w);cE=o(F,"Une particularit\xE9 de XLNet est qu\u2019il place le "),Wa=l(F,"EM",{});var _q=a(Wa);mE=o(_q,"token"),_q.forEach(t),dE=c(F),Ra=l(F,"CODE",{});var hq=a(Ra);fE=o(hq,"<cls>"),hq.forEach(t),vE=o(F," \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Ia=l(F,"EM",{});var Eq=a(Ia);kE=o(Eq,"tokens"),Eq.forEach(t),_E=o(F,"). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ga=l(F,"EM",{});var $q=a(Ga);hE=o($q,"tokens"),$q.forEach(t),EE=o(F," sp\xE9ciaux et les types d\u2019identifiant de "),Va=l(F,"EM",{});var zq=a(Va);$E=o(zq,"token"),zq.forEach(t),zE=o(F," avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Xa=l(F,"EM",{});var qq=a(Xa);qE=o(qq,"tokens"),qq.forEach(t),jE=c(F),Ka=l(F,"CODE",{});var jq=a(Ka);xE=o(jq,"<cls>"),jq.forEach(t),bE=o(F," et "),Ha=l(F,"CODE",{});var xq=a(Ha);gE=o(xq,"<sep>"),xq.forEach(t),PE=o(F," :"),F.forEach(t),Su=c(e),d(kn.$$.fragment,e),Nu=c(e),d(_n.$$.fragment,e),Bu=c(e),Yn=l(e,"P",{});var bq=a(Yn);wE=o(bq,"Le mod\xE8le ressemble \xE0 ceci :"),bq.forEach(t),Au=c(e),d(hn.$$.fragment,e),Uu=c(e),Zn=l(e,"P",{});var gq=a(Zn);CE=o(gq,"Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),gq.forEach(t),Fu=c(e),d(En.$$.fragment,e),Wu=c(e),d($n.$$.fragment,e),Ru=c(e),Ps=l(e,"P",{});var jc=a(Ps);TE=o(jc,"Enfin, nous ajoutons un d\xE9codeur "),Ja=l(jc,"CODE",{});var Pq=a(Ja);DE=o(Pq,"Metaspace"),Pq.forEach(t),LE=o(jc," :"),jc.forEach(t),Iu=c(e),d(zn.$$.fragment,e),Gu=c(e),D=l(e,"P",{});var H=a(D);OE=o(H,"et on en a fini avec ce "),Ya=l(H,"EM",{});var wq=a(Ya);ME=o(wq,"tokenizer"),wq.forEach(t),yE=o(H," ! On peut le sauvegarder et l\u2019envelopper dans un "),Za=l(H,"CODE",{});var Cq=a(Za);SE=o(Cq,"PreTrainedTokenizerFast"),Cq.forEach(t),NE=o(H," ou "),Qa=l(H,"CODE",{});var Tq=a(Qa);BE=o(Tq,"XLNetTokenizerFast"),Tq.forEach(t),AE=o(H," si on veut l\u2019utiliser dans \u{1F917} "),ei=l(H,"EM",{});var Dq=a(ei);UE=o(Dq,"Transformers"),Dq.forEach(t),FE=o(H,". Une chose \xE0 noter lors de l\u2019utilisation de "),si=l(H,"CODE",{});var Lq=a(si);WE=o(Lq,"PreTrainedTokenizerFast"),Lq.forEach(t),RE=o(H," est qu\u2019en plus des "),ti=l(H,"EM",{});var Oq=a(ti);IE=o(Oq,"tokens"),Oq.forEach(t),GE=o(H," sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ni=l(H,"EM",{});var Mq=a(ni);VE=o(Mq,"Transformers"),Mq.forEach(t),XE=o(H," de rembourrer \xE0 gauche :"),H.forEach(t),Vu=c(e),d(qn.$$.fragment,e),Xu=c(e),Qn=l(e,"P",{});var yq=a(Qn);KE=o(yq,"Ou alternativement :"),yq.forEach(t),Ku=c(e),d(jn.$$.fragment,e),Hu=c(e),Z=l(e,"P",{});var Ve=a(Z);HE=o(Ve,"Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),oi=l(Ve,"EM",{});var Sq=a(oi);JE=o(Sq,"tokenizers"),Sq.forEach(t),YE=o(Ve," existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),ri=l(Ve,"EM",{});var Nq=a(ri);ZE=o(Nq,"tokenizer"),Nq.forEach(t),QE=o(Ve," que vous voulez avec la biblioth\xE8que \u{1F917} "),li=l(Ve,"EM",{});var Bq=a(li);e$=o(Bq,"Tokenizers"),Bq.forEach(t),s$=o(Ve," et pouvoir l\u2019utiliser dans \u{1F917} "),ai=l(Ve,"EM",{});var Aq=a(ai);t$=o(Aq,"Transformers"),Aq.forEach(t),n$=o(Ve,"."),Ve.forEach(t),this.h()},h(){E($,"name","hf:doc:metadata"),E($,"content",JSON.stringify(Yq)),E(pe,"id","construction-dun-itokenizeri-bloc-par-bloc"),E(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(pe,"href","#construction-dun-itokenizeri-bloc-par-bloc"),E(oe,"class","relative group"),E(Vs,"class","block dark:hidden"),Uq(Vs.src,o$="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||E(Vs,"src",o$),E(Vs,"alt","The tokenization pipeline."),E(Xs,"class","hidden dark:block"),Uq(Xs.src,r$="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||E(Xs,"src",r$),E(Xs,"alt","The tokenization pipeline."),E(Je,"class","flex justify-center"),E(Sn,"href","/course/fr/chapter6/2"),E(Hs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"),E(Hs,"rel","nofollow"),E(Js,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers"),E(Js,"rel","nofollow"),E(Ys,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models"),E(Ys,"rel","nofollow"),E(Zs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers"),E(Zs,"rel","nofollow"),E(Qs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors"),E(Qs,"rel","nofollow"),E(et,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders"),E(et,"rel","nofollow"),E(st,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html"),E(st,"rel","nofollow"),E(is,"id","acquisition-dun-corpus"),E(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(is,"href","#acquisition-dun-corpus"),E(Ye,"class","relative group"),E(Nn,"href","/course/fr/chapter6/2"),E(nt,"href","https://huggingface.co/datasets/wikitext"),E(nt,"rel","nofollow"),E(us,"id","construire-un-itokenizer-wordpiecei-partir-de-zro"),E(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(us,"href","#construire-un-itokenizer-wordpiecei-partir-de-zro"),E(Ze,"class","relative group"),E(Es,"id","construire-un-itokenizeri-bpe-partir-de-zro"),E(Es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Es,"href","#construire-un-itokenizeri-bpe-partir-de-zro"),E(Qe,"class","relative group"),E(js,"id","construire-un-itokenizer-unigrami-partir-de-zro"),E(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(js,"href","#construire-un-itokenizer-unigrami-partir-de-zro"),E(es,"class","relative group")},m(e,i){s(document.head,$),p(e,ge,i),p(e,oe,i),s(oe,pe),s(pe,qe),f(je,qe,null),s(oe,As),s(oe,re),s(re,Us),s(re,Xe),s(Xe,Ke),s(re,Fs),p(e,ss,i),f(le,e,i),p(e,ts,i),p(e,Pe,i),s(Pe,He),p(e,ns,i),p(e,W,i),s(W,xe),s(xe,Ws),s(W,Rs),s(W,be),s(be,Is),s(W,Gs),s(W,ae),s(ae,x),s(ae,os),s(os,Tn),s(ae,Dn),s(W,Ln),s(W,J),s(J,On),s(J,rs),s(rs,Mn),s(J,xc),s(J,mo),s(mo,bc),s(J,gc),s(J,fo),s(fo,Pc),s(J,wc),p(e,ui,i),p(e,yn,i),s(yn,Cc),p(e,ci,i),p(e,Je,i),s(Je,Vs),s(Je,Tc),s(Je,Xs),p(e,mi,i),p(e,R,i),s(R,Dc),s(R,vo),s(vo,Lc),s(R,Oc),s(R,ko),s(ko,Mc),s(R,yc),s(R,_o),s(_o,Sc),s(R,Nc),s(R,Sn),s(Sn,Bc),s(R,Ac),s(R,ho),s(ho,Uc),s(R,Fc),p(e,di,i),f(Ks,e,i),p(e,fi,i),p(e,ls,i),s(ls,Wc),s(ls,Eo),s(Eo,Rc),s(ls,Ic),p(e,vi,i),p(e,I,i),s(I,we),s(we,$o),s($o,Gc),s(we,Vc),s(we,zo),s(zo,Xc),s(we,Kc),s(we,Hs),s(Hs,Hc),s(we,Jc),s(I,Yc),s(I,Ce),s(Ce,qo),s(qo,Zc),s(Ce,Qc),s(Ce,jo),s(jo,em),s(Ce,sm),s(Ce,Js),s(Js,tm),s(Ce,nm),s(I,om),s(I,G),s(G,xo),s(xo,rm),s(G,lm),s(G,bo),s(bo,am),s(G,im),s(G,go),s(go,pm),s(G,um),s(G,Po),s(Po,cm),s(G,mm),s(G,wo),s(wo,dm),s(G,fm),s(G,Ys),s(Ys,vm),s(G,km),s(I,_m),s(I,Te),s(Te,Co),s(Co,hm),s(Te,Em),s(Te,To),s(To,$m),s(Te,zm),s(Te,Zs),s(Zs,qm),s(Te,jm),s(I,xm),s(I,De),s(De,Do),s(Do,bm),s(De,gm),s(De,Lo),s(Lo,Pm),s(De,wm),s(De,Qs),s(Qs,Cm),s(De,Tm),s(I,Dm),s(I,Le),s(Le,Oo),s(Oo,Lm),s(Le,Om),s(Le,Mo),s(Mo,Mm),s(Le,ym),s(Le,et),s(et,Sm),s(Le,Nm),p(e,ki,i),p(e,as,i),s(as,Bm),s(as,st),s(st,Am),s(as,Um),p(e,_i,i),p(e,Ye,i),s(Ye,is),s(is,yo),f(tt,yo,null),s(Ye,Fm),s(Ye,So),s(So,Wm),p(e,hi,i),p(e,ue,i),s(ue,Rm),s(ue,No),s(No,Im),s(ue,Gm),s(ue,Nn),s(Nn,Vm),s(ue,Xm),s(ue,nt),s(nt,Km),s(ue,Hm),p(e,Ei,i),f(ot,e,i),p(e,$i,i),p(e,Oe,i),s(Oe,Jm),s(Oe,Bo),s(Bo,Ym),s(Oe,Zm),s(Oe,Ao),s(Ao,Qm),s(Oe,ed),p(e,zi,i),p(e,ps,i),s(ps,sd),s(ps,Uo),s(Uo,td),s(ps,nd),p(e,qi,i),f(rt,e,i),p(e,ji,i),p(e,ce,i),s(ce,od),s(ce,Fo),s(Fo,rd),s(ce,ld),s(ce,Wo),s(Wo,ad),s(ce,id),s(ce,Ro),s(Ro,pd),s(ce,ud),p(e,xi,i),p(e,Ze,i),s(Ze,us),s(us,Io),f(lt,Io,null),s(Ze,cd),s(Ze,at),s(at,md),s(at,Go),s(Go,dd),s(at,fd),p(e,bi,i),p(e,b,i),s(b,vd),s(b,Vo),s(Vo,kd),s(b,_d),s(b,Xo),s(Xo,hd),s(b,Ed),s(b,Ko),s(Ko,$d),s(b,zd),s(b,Ho),s(Ho,qd),s(b,jd),s(b,Jo),s(Jo,xd),s(b,bd),s(b,Yo),s(Yo,gd),s(b,Pd),s(b,Zo),s(Zo,wd),s(b,Cd),s(b,Qo),s(Qo,Td),s(b,Dd),p(e,gi,i),p(e,Me,i),s(Me,Ld),s(Me,er),s(er,Od),s(Me,Md),s(Me,sr),s(sr,yd),s(Me,Sd),p(e,Pi,i),f(it,e,i),p(e,wi,i),p(e,me,i),s(me,Nd),s(me,tr),s(tr,Bd),s(me,Ad),s(me,nr),s(nr,Ud),s(me,Fd),s(me,or),s(or,Wd),s(me,Rd),p(e,Ci,i),p(e,g,i),s(g,Id),s(g,rr),s(rr,Gd),s(g,Vd),s(g,lr),s(lr,Xd),s(g,Kd),s(g,ar),s(ar,Hd),s(g,Jd),s(g,ir),s(ir,Yd),s(g,Zd),s(g,pr),s(pr,Qd),s(g,ef),s(g,ur),s(ur,sf),s(g,tf),s(g,cr),s(cr,nf),s(g,of),s(g,mr),s(mr,rf),s(g,lf),p(e,Ti,i),f(pt,e,i),p(e,Di,i),p(e,V,i),s(V,af),s(V,dr),s(dr,pf),s(V,uf),s(V,fr),s(fr,cf),s(V,mf),s(V,vr),s(vr,df),s(V,ff),s(V,kr),s(kr,vf),s(V,kf),s(V,_r),s(_r,_f),s(V,hf),p(e,Li,i),f(ut,e,i),p(e,Oi,i),p(e,ye,i),s(ye,Ef),s(ye,hr),s(hr,$f),s(ye,zf),s(ye,Er),s(Er,qf),s(ye,jf),p(e,Mi,i),p(e,Se,i),s(Se,xf),s(Se,$r),s($r,bf),s(Se,gf),s(Se,zr),s(zr,Pf),s(Se,wf),p(e,yi,i),f(ct,e,i),p(e,Si,i),f(mt,e,i),p(e,Ni,i),f(cs,e,i),p(e,Bi,i),p(e,ms,i),s(ms,Cf),s(ms,qr),s(qr,Tf),s(ms,Df),p(e,Ai,i),f(dt,e,i),p(e,Ui,i),p(e,Bn,i),s(Bn,Lf),p(e,Fi,i),f(ft,e,i),p(e,Wi,i),p(e,ds,i),s(ds,Of),s(ds,jr),s(jr,Mf),s(ds,yf),p(e,Ri,i),f(vt,e,i),p(e,Ii,i),f(kt,e,i),p(e,Gi,i),p(e,fs,i),s(fs,Sf),s(fs,xr),s(xr,Nf),s(fs,Bf),p(e,Vi,i),f(_t,e,i),p(e,Xi,i),f(ht,e,i),p(e,Ki,i),p(e,vs,i),s(vs,Af),s(vs,br),s(br,Uf),s(vs,Ff),p(e,Hi,i),f(Et,e,i),p(e,Ji,i),f($t,e,i),p(e,Yi,i),p(e,de,i),s(de,Wf),s(de,gr),s(gr,Rf),s(de,If),s(de,Pr),s(Pr,Gf),s(de,Vf),s(de,wr),s(wr,Xf),s(de,Kf),p(e,Zi,i),f(zt,e,i),p(e,Qi,i),p(e,O,i),s(O,Hf),s(O,Cr),s(Cr,Jf),s(O,Yf),s(O,Tr),s(Tr,Zf),s(O,Qf),s(O,Dr),s(Dr,ev),s(O,sv),s(O,Lr),s(Lr,tv),s(O,nv),s(O,Or),s(Or,ov),s(O,rv),s(O,Mr),s(Mr,lv),s(O,av),p(e,ep,i),p(e,An,i),s(An,iv),p(e,sp,i),f(qt,e,i),p(e,tp,i),p(e,Ne,i),s(Ne,pv),s(Ne,yr),s(yr,uv),s(Ne,cv),s(Ne,Sr),s(Sr,mv),s(Ne,dv),p(e,np,i),f(jt,e,i),p(e,op,i),p(e,Be,i),s(Be,fv),s(Be,Nr),s(Nr,vv),s(Be,kv),s(Be,Br),s(Br,_v),s(Be,hv),p(e,rp,i),f(xt,e,i),p(e,lp,i),f(bt,e,i),p(e,ap,i),p(e,j,i),s(j,Ev),s(j,Ar),s(Ar,$v),s(j,zv),s(j,Ur),s(Ur,qv),s(j,jv),s(j,Fr),s(Fr,xv),s(j,bv),s(j,Wr),s(Wr,gv),s(j,Pv),s(j,Rr),s(Rr,wv),s(j,Cv),s(j,Ir),s(Ir,Tv),s(j,Dv),s(j,Gr),s(Gr,Lv),s(j,Ov),s(j,Vr),s(Vr,Mv),s(j,yv),s(j,Xr),s(Xr,Sv),s(j,Nv),p(e,ip,i),p(e,P,i),s(P,Bv),s(P,Kr),s(Kr,Av),s(P,Uv),s(P,Hr),s(Hr,Fv),s(P,Wv),s(P,Jr),s(Jr,Rv),s(P,Iv),s(P,Yr),s(Yr,Gv),s(P,Vv),s(P,Zr),s(Zr,Xv),s(P,Kv),s(P,Qr),s(Qr,Hv),s(P,Jv),s(P,el),s(el,Yv),s(P,Zv),s(P,sl),s(sl,Qv),s(P,ek),p(e,pp,i),f(gt,e,i),p(e,up,i),f(Pt,e,i),p(e,cp,i),p(e,M,i),s(M,sk),s(M,tl),s(tl,tk),s(M,nk),s(M,nl),s(nl,ok),s(M,rk),s(M,ol),s(ol,lk),s(M,ak),s(M,rl),s(rl,ik),s(M,pk),s(M,ll),s(ll,uk),s(M,ck),s(M,al),s(al,mk),s(M,dk),p(e,mp,i),p(e,Un,i),s(Un,fk),p(e,dp,i),f(wt,e,i),p(e,fp,i),p(e,Ae,i),s(Ae,vk),s(Ae,il),s(il,kk),s(Ae,_k),s(Ae,pl),s(pl,hk),s(Ae,Ek),p(e,vp,i),p(e,Fn,i),s(Fn,$k),p(e,kp,i),f(Ct,e,i),p(e,_p,i),f(Tt,e,i),p(e,hp,i),p(e,Wn,i),s(Wn,zk),p(e,Ep,i),f(Dt,e,i),p(e,$p,i),f(Lt,e,i),p(e,zp,i),p(e,ks,i),s(ks,qk),s(ks,ul),s(ul,jk),s(ks,xk),p(e,qp,i),f(Ot,e,i),p(e,jp,i),p(e,_s,i),s(_s,bk),s(_s,cl),s(cl,gk),s(_s,Pk),p(e,xp,i),f(Mt,e,i),p(e,bp,i),f(yt,e,i),p(e,gp,i),p(e,hs,i),s(hs,wk),s(hs,ml),s(ml,Ck),s(hs,Tk),p(e,Pp,i),f(St,e,i),p(e,wp,i),p(e,Ue,i),s(Ue,Dk),s(Ue,dl),s(dl,Lk),s(Ue,Ok),s(Ue,fl),s(fl,Mk),s(Ue,yk),p(e,Cp,i),f(Nt,e,i),p(e,Tp,i),p(e,y,i),s(y,Sk),s(y,vl),s(vl,Nk),s(y,Bk),s(y,kl),s(kl,Ak),s(y,Uk),s(y,_l),s(_l,Fk),s(y,Wk),s(y,hl),s(hl,Rk),s(y,Ik),s(y,El),s(El,Gk),s(y,Vk),s(y,$l),s($l,Xk),s(y,Kk),p(e,Dp,i),p(e,z,i),s(z,Hk),s(z,zl),s(zl,Jk),s(z,Yk),s(z,ql),s(ql,Zk),s(z,Qk),s(z,jl),s(jl,e2),s(z,s2),s(z,xl),s(xl,t2),s(z,n2),s(z,bl),s(bl,o2),s(z,r2),s(z,gl),s(gl,l2),s(z,a2),s(z,Pl),s(Pl,i2),s(z,p2),s(z,wl),s(wl,u2),s(z,c2),s(z,Cl),s(Cl,m2),s(z,d2),s(z,Tl),s(Tl,f2),s(z,v2),s(z,Dl),s(Dl,k2),s(z,Ll),s(Ll,_2),s(z,h2),p(e,Lp,i),f(Bt,e,i),p(e,Op,i),p(e,fe,i),s(fe,E2),s(fe,Ol),s(Ol,$2),s(fe,z2),s(fe,Ml),s(Ml,q2),s(fe,j2),s(fe,yl),s(yl,x2),s(fe,b2),p(e,Mp,i),f(At,e,i),p(e,yp,i),p(e,S,i),s(S,g2),s(S,Sl),s(Sl,P2),s(S,w2),s(S,Nl),s(Nl,C2),s(S,T2),s(S,Bl),s(Bl,D2),s(S,L2),s(S,Al),s(Al,O2),s(S,M2),s(S,Ul),s(Ul,y2),s(S,S2),s(S,Fl),s(Fl,N2),s(S,B2),p(e,Sp,i),p(e,Fe,i),s(Fe,A2),s(Fe,Wl),s(Wl,U2),s(Fe,F2),s(Fe,Rl),s(Rl,W2),s(Fe,R2),p(e,Np,i),p(e,Qe,i),s(Qe,Es),s(Es,Il),f(Ut,Il,null),s(Qe,I2),s(Qe,Ft),s(Ft,G2),s(Ft,Gl),s(Gl,V2),s(Ft,X2),p(e,Bp,i),p(e,ve,i),s(ve,K2),s(ve,Vl),s(Vl,H2),s(ve,J2),s(ve,Xl),s(Xl,Y2),s(ve,Z2),s(ve,Kl),s(Kl,Q2),s(ve,e_),p(e,Ap,i),f(Wt,e,i),p(e,Up,i),p(e,ke,i),s(ke,s_),s(ke,Hl),s(Hl,t_),s(ke,n_),s(ke,Jl),s(Jl,o_),s(ke,r_),s(ke,Yl),s(Yl,l_),s(ke,a_),p(e,Fp,i),p(e,Rn,i),s(Rn,i_),p(e,Wp,i),f(Rt,e,i),p(e,Rp,i),p(e,$s,i),s($s,p_),s($s,Zl),s(Zl,u_),s($s,c_),p(e,Ip,i),f(It,e,i),p(e,Gp,i),f(Gt,e,i),p(e,Vp,i),p(e,We,i),s(We,m_),s(We,Ql),s(Ql,d_),s(We,f_),s(We,ea),s(ea,v_),s(We,k_),p(e,Xp,i),f(Vt,e,i),p(e,Kp,i),p(e,N,i),s(N,__),s(N,sa),s(sa,h_),s(N,E_),s(N,ta),s(ta,$_),s(N,z_),s(N,na),s(na,q_),s(N,j_),s(N,oa),s(oa,x_),s(N,b_),s(N,ra),s(ra,g_),s(N,P_),s(N,la),s(la,w_),s(N,C_),p(e,Hp,i),p(e,zs,i),s(zs,T_),s(zs,aa),s(aa,D_),s(zs,L_),p(e,Jp,i),f(Xt,e,i),p(e,Yp,i),p(e,In,i),s(In,O_),p(e,Zp,i),f(Kt,e,i),p(e,Qp,i),f(Ht,e,i),p(e,eu,i),p(e,qs,i),s(qs,M_),s(qs,ia),s(ia,y_),s(qs,S_),p(e,su,i),f(Jt,e,i),p(e,tu,i),p(e,C,i),s(C,N_),s(C,pa),s(pa,B_),s(C,A_),s(C,ua),s(ua,U_),s(C,F_),s(C,ca),s(ca,W_),s(C,R_),s(C,ma),s(ma,I_),s(C,G_),s(C,da),s(da,V_),s(C,X_),s(C,fa),s(fa,K_),s(C,H_),s(C,va),s(va,J_),s(C,Y_),p(e,nu,i),f(Yt,e,i),p(e,ou,i),f(Zt,e,i),p(e,ru,i),p(e,Gn,i),s(Gn,Z_),p(e,lu,i),f(Qt,e,i),p(e,au,i),p(e,Vn,i),s(Vn,Q_),p(e,iu,i),f(en,e,i),p(e,pu,i),f(sn,e,i),p(e,uu,i),p(e,_e,i),s(_e,eh),s(_e,ka),s(ka,sh),s(_e,th),s(_e,_a),s(_a,nh),s(_e,oh),s(_e,ha),s(ha,rh),s(_e,lh),p(e,cu,i),f(tn,e,i),p(e,mu,i),p(e,Xn,i),s(Xn,ah),p(e,du,i),f(nn,e,i),p(e,fu,i),p(e,Re,i),s(Re,ih),s(Re,Ea),s(Ea,ph),s(Re,uh),s(Re,$a),s($a,ch),s(Re,mh),p(e,vu,i),p(e,es,i),s(es,js),s(js,za),f(on,za,null),s(es,dh),s(es,rn),s(rn,fh),s(rn,qa),s(qa,vh),s(rn,kh),p(e,ku,i),p(e,Y,i),s(Y,_h),s(Y,ja),s(ja,hh),s(Y,Eh),s(Y,xa),s(xa,$h),s(Y,zh),s(Y,ba),s(ba,qh),s(Y,jh),s(Y,ga),s(ga,xh),s(Y,bh),p(e,_u,i),f(ln,e,i),p(e,hu,i),p(e,Kn,i),s(Kn,gh),p(e,Eu,i),p(e,xs,i),s(xs,Ph),s(xs,Pa),s(Pa,wh),s(xs,Ch),p(e,$u,i),f(an,e,i),p(e,zu,i),p(e,he,i),s(he,Th),s(he,wa),s(wa,Dh),s(he,Lh),s(he,Ca),s(Ca,Oh),s(he,Mh),s(he,Ta),s(Ta,yh),s(he,Sh),p(e,qu,i),p(e,Ie,i),s(Ie,Nh),s(Ie,Da),s(Da,Bh),s(Ie,Ah),s(Ie,La),s(La,Uh),s(Ie,Fh),p(e,ju,i),f(pn,e,i),p(e,xu,i),p(e,Hn,i),s(Hn,Wh),p(e,bu,i),f(un,e,i),p(e,gu,i),f(cn,e,i),p(e,Pu,i),p(e,bs,i),s(bs,Rh),s(bs,Oa),s(Oa,Ih),s(bs,Gh),p(e,wu,i),f(mn,e,i),p(e,Cu,i),p(e,T,i),s(T,Vh),s(T,Ma),s(Ma,Xh),s(T,Kh),s(T,ya),s(ya,Hh),s(T,Jh),s(T,Sa),s(Sa,Yh),s(T,Zh),s(T,Na),s(Na,Qh),s(T,eE),s(T,Ba),s(Ba,sE),s(T,tE),s(T,Aa),s(Aa,nE),s(T,oE),s(T,Ua),s(Ua,rE),s(T,lE),p(e,Tu,i),p(e,gs,i),s(gs,aE),s(gs,Fa),s(Fa,iE),s(gs,pE),p(e,Du,i),f(dn,e,i),p(e,Lu,i),p(e,Jn,i),s(Jn,uE),p(e,Ou,i),f(fn,e,i),p(e,Mu,i),f(vn,e,i),p(e,yu,i),p(e,w,i),s(w,cE),s(w,Wa),s(Wa,mE),s(w,dE),s(w,Ra),s(Ra,fE),s(w,vE),s(w,Ia),s(Ia,kE),s(w,_E),s(w,Ga),s(Ga,hE),s(w,EE),s(w,Va),s(Va,$E),s(w,zE),s(w,Xa),s(Xa,qE),s(w,jE),s(w,Ka),s(Ka,xE),s(w,bE),s(w,Ha),s(Ha,gE),s(w,PE),p(e,Su,i),f(kn,e,i),p(e,Nu,i),f(_n,e,i),p(e,Bu,i),p(e,Yn,i),s(Yn,wE),p(e,Au,i),f(hn,e,i),p(e,Uu,i),p(e,Zn,i),s(Zn,CE),p(e,Fu,i),f(En,e,i),p(e,Wu,i),f($n,e,i),p(e,Ru,i),p(e,Ps,i),s(Ps,TE),s(Ps,Ja),s(Ja,DE),s(Ps,LE),p(e,Iu,i),f(zn,e,i),p(e,Gu,i),p(e,D,i),s(D,OE),s(D,Ya),s(Ya,ME),s(D,yE),s(D,Za),s(Za,SE),s(D,NE),s(D,Qa),s(Qa,BE),s(D,AE),s(D,ei),s(ei,UE),s(D,FE),s(D,si),s(si,WE),s(D,RE),s(D,ti),s(ti,IE),s(D,GE),s(D,ni),s(ni,VE),s(D,XE),p(e,Vu,i),f(qn,e,i),p(e,Xu,i),p(e,Qn,i),s(Qn,KE),p(e,Ku,i),f(jn,e,i),p(e,Hu,i),p(e,Z,i),s(Z,HE),s(Z,oi),s(oi,JE),s(Z,YE),s(Z,ri),s(ri,ZE),s(Z,QE),s(Z,li),s(li,e$),s(Z,s$),s(Z,ai),s(ai,t$),s(Z,n$),Ju=!0},p(e,[i]){const xn={};i&2&&(xn.$$scope={dirty:i,ctx:e}),cs.$set(xn)},i(e){Ju||(v(je.$$.fragment,e),v(le.$$.fragment,e),v(Ks.$$.fragment,e),v(tt.$$.fragment,e),v(ot.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ut.$$.fragment,e),v(ct.$$.fragment,e),v(mt.$$.fragment,e),v(cs.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(vt.$$.fragment,e),v(kt.$$.fragment,e),v(_t.$$.fragment,e),v(ht.$$.fragment,e),v(Et.$$.fragment,e),v($t.$$.fragment,e),v(zt.$$.fragment,e),v(qt.$$.fragment,e),v(jt.$$.fragment,e),v(xt.$$.fragment,e),v(bt.$$.fragment,e),v(gt.$$.fragment,e),v(Pt.$$.fragment,e),v(wt.$$.fragment,e),v(Ct.$$.fragment,e),v(Tt.$$.fragment,e),v(Dt.$$.fragment,e),v(Lt.$$.fragment,e),v(Ot.$$.fragment,e),v(Mt.$$.fragment,e),v(yt.$$.fragment,e),v(St.$$.fragment,e),v(Nt.$$.fragment,e),v(Bt.$$.fragment,e),v(At.$$.fragment,e),v(Ut.$$.fragment,e),v(Wt.$$.fragment,e),v(Rt.$$.fragment,e),v(It.$$.fragment,e),v(Gt.$$.fragment,e),v(Vt.$$.fragment,e),v(Xt.$$.fragment,e),v(Kt.$$.fragment,e),v(Ht.$$.fragment,e),v(Jt.$$.fragment,e),v(Yt.$$.fragment,e),v(Zt.$$.fragment,e),v(Qt.$$.fragment,e),v(en.$$.fragment,e),v(sn.$$.fragment,e),v(tn.$$.fragment,e),v(nn.$$.fragment,e),v(on.$$.fragment,e),v(ln.$$.fragment,e),v(an.$$.fragment,e),v(pn.$$.fragment,e),v(un.$$.fragment,e),v(cn.$$.fragment,e),v(mn.$$.fragment,e),v(dn.$$.fragment,e),v(fn.$$.fragment,e),v(vn.$$.fragment,e),v(kn.$$.fragment,e),v(_n.$$.fragment,e),v(hn.$$.fragment,e),v(En.$$.fragment,e),v($n.$$.fragment,e),v(zn.$$.fragment,e),v(qn.$$.fragment,e),v(jn.$$.fragment,e),Ju=!0)},o(e){k(je.$$.fragment,e),k(le.$$.fragment,e),k(Ks.$$.fragment,e),k(tt.$$.fragment,e),k(ot.$$.fragment,e),k(rt.$$.fragment,e),k(lt.$$.fragment,e),k(it.$$.fragment,e),k(pt.$$.fragment,e),k(ut.$$.fragment,e),k(ct.$$.fragment,e),k(mt.$$.fragment,e),k(cs.$$.fragment,e),k(dt.$$.fragment,e),k(ft.$$.fragment,e),k(vt.$$.fragment,e),k(kt.$$.fragment,e),k(_t.$$.fragment,e),k(ht.$$.fragment,e),k(Et.$$.fragment,e),k($t.$$.fragment,e),k(zt.$$.fragment,e),k(qt.$$.fragment,e),k(jt.$$.fragment,e),k(xt.$$.fragment,e),k(bt.$$.fragment,e),k(gt.$$.fragment,e),k(Pt.$$.fragment,e),k(wt.$$.fragment,e),k(Ct.$$.fragment,e),k(Tt.$$.fragment,e),k(Dt.$$.fragment,e),k(Lt.$$.fragment,e),k(Ot.$$.fragment,e),k(Mt.$$.fragment,e),k(yt.$$.fragment,e),k(St.$$.fragment,e),k(Nt.$$.fragment,e),k(Bt.$$.fragment,e),k(At.$$.fragment,e),k(Ut.$$.fragment,e),k(Wt.$$.fragment,e),k(Rt.$$.fragment,e),k(It.$$.fragment,e),k(Gt.$$.fragment,e),k(Vt.$$.fragment,e),k(Xt.$$.fragment,e),k(Kt.$$.fragment,e),k(Ht.$$.fragment,e),k(Jt.$$.fragment,e),k(Yt.$$.fragment,e),k(Zt.$$.fragment,e),k(Qt.$$.fragment,e),k(en.$$.fragment,e),k(sn.$$.fragment,e),k(tn.$$.fragment,e),k(nn.$$.fragment,e),k(on.$$.fragment,e),k(ln.$$.fragment,e),k(an.$$.fragment,e),k(pn.$$.fragment,e),k(un.$$.fragment,e),k(cn.$$.fragment,e),k(mn.$$.fragment,e),k(dn.$$.fragment,e),k(fn.$$.fragment,e),k(vn.$$.fragment,e),k(kn.$$.fragment,e),k(_n.$$.fragment,e),k(hn.$$.fragment,e),k(En.$$.fragment,e),k($n.$$.fragment,e),k(zn.$$.fragment,e),k(qn.$$.fragment,e),k(jn.$$.fragment,e),Ju=!1},d(e){t($),e&&t(ge),e&&t(oe),_(je),e&&t(ss),_(le,e),e&&t(ts),e&&t(Pe),e&&t(ns),e&&t(W),e&&t(ui),e&&t(yn),e&&t(ci),e&&t(Je),e&&t(mi),e&&t(R),e&&t(di),_(Ks,e),e&&t(fi),e&&t(ls),e&&t(vi),e&&t(I),e&&t(ki),e&&t(as),e&&t(_i),e&&t(Ye),_(tt),e&&t(hi),e&&t(ue),e&&t(Ei),_(ot,e),e&&t($i),e&&t(Oe),e&&t(zi),e&&t(ps),e&&t(qi),_(rt,e),e&&t(ji),e&&t(ce),e&&t(xi),e&&t(Ze),_(lt),e&&t(bi),e&&t(b),e&&t(gi),e&&t(Me),e&&t(Pi),_(it,e),e&&t(wi),e&&t(me),e&&t(Ci),e&&t(g),e&&t(Ti),_(pt,e),e&&t(Di),e&&t(V),e&&t(Li),_(ut,e),e&&t(Oi),e&&t(ye),e&&t(Mi),e&&t(Se),e&&t(yi),_(ct,e),e&&t(Si),_(mt,e),e&&t(Ni),_(cs,e),e&&t(Bi),e&&t(ms),e&&t(Ai),_(dt,e),e&&t(Ui),e&&t(Bn),e&&t(Fi),_(ft,e),e&&t(Wi),e&&t(ds),e&&t(Ri),_(vt,e),e&&t(Ii),_(kt,e),e&&t(Gi),e&&t(fs),e&&t(Vi),_(_t,e),e&&t(Xi),_(ht,e),e&&t(Ki),e&&t(vs),e&&t(Hi),_(Et,e),e&&t(Ji),_($t,e),e&&t(Yi),e&&t(de),e&&t(Zi),_(zt,e),e&&t(Qi),e&&t(O),e&&t(ep),e&&t(An),e&&t(sp),_(qt,e),e&&t(tp),e&&t(Ne),e&&t(np),_(jt,e),e&&t(op),e&&t(Be),e&&t(rp),_(xt,e),e&&t(lp),_(bt,e),e&&t(ap),e&&t(j),e&&t(ip),e&&t(P),e&&t(pp),_(gt,e),e&&t(up),_(Pt,e),e&&t(cp),e&&t(M),e&&t(mp),e&&t(Un),e&&t(dp),_(wt,e),e&&t(fp),e&&t(Ae),e&&t(vp),e&&t(Fn),e&&t(kp),_(Ct,e),e&&t(_p),_(Tt,e),e&&t(hp),e&&t(Wn),e&&t(Ep),_(Dt,e),e&&t($p),_(Lt,e),e&&t(zp),e&&t(ks),e&&t(qp),_(Ot,e),e&&t(jp),e&&t(_s),e&&t(xp),_(Mt,e),e&&t(bp),_(yt,e),e&&t(gp),e&&t(hs),e&&t(Pp),_(St,e),e&&t(wp),e&&t(Ue),e&&t(Cp),_(Nt,e),e&&t(Tp),e&&t(y),e&&t(Dp),e&&t(z),e&&t(Lp),_(Bt,e),e&&t(Op),e&&t(fe),e&&t(Mp),_(At,e),e&&t(yp),e&&t(S),e&&t(Sp),e&&t(Fe),e&&t(Np),e&&t(Qe),_(Ut),e&&t(Bp),e&&t(ve),e&&t(Ap),_(Wt,e),e&&t(Up),e&&t(ke),e&&t(Fp),e&&t(Rn),e&&t(Wp),_(Rt,e),e&&t(Rp),e&&t($s),e&&t(Ip),_(It,e),e&&t(Gp),_(Gt,e),e&&t(Vp),e&&t(We),e&&t(Xp),_(Vt,e),e&&t(Kp),e&&t(N),e&&t(Hp),e&&t(zs),e&&t(Jp),_(Xt,e),e&&t(Yp),e&&t(In),e&&t(Zp),_(Kt,e),e&&t(Qp),_(Ht,e),e&&t(eu),e&&t(qs),e&&t(su),_(Jt,e),e&&t(tu),e&&t(C),e&&t(nu),_(Yt,e),e&&t(ou),_(Zt,e),e&&t(ru),e&&t(Gn),e&&t(lu),_(Qt,e),e&&t(au),e&&t(Vn),e&&t(iu),_(en,e),e&&t(pu),_(sn,e),e&&t(uu),e&&t(_e),e&&t(cu),_(tn,e),e&&t(mu),e&&t(Xn),e&&t(du),_(nn,e),e&&t(fu),e&&t(Re),e&&t(vu),e&&t(es),_(on),e&&t(ku),e&&t(Y),e&&t(_u),_(ln,e),e&&t(hu),e&&t(Kn),e&&t(Eu),e&&t(xs),e&&t($u),_(an,e),e&&t(zu),e&&t(he),e&&t(qu),e&&t(Ie),e&&t(ju),_(pn,e),e&&t(xu),e&&t(Hn),e&&t(bu),_(un,e),e&&t(gu),_(cn,e),e&&t(Pu),e&&t(bs),e&&t(wu),_(mn,e),e&&t(Cu),e&&t(T),e&&t(Tu),e&&t(gs),e&&t(Du),_(dn,e),e&&t(Lu),e&&t(Jn),e&&t(Ou),_(fn,e),e&&t(Mu),_(vn,e),e&&t(yu),e&&t(w),e&&t(Su),_(kn,e),e&&t(Nu),_(_n,e),e&&t(Bu),e&&t(Yn),e&&t(Au),_(hn,e),e&&t(Uu),e&&t(Zn),e&&t(Fu),_(En,e),e&&t(Wu),_($n,e),e&&t(Ru),e&&t(Ps),e&&t(Iu),_(zn,e),e&&t(Gu),e&&t(D),e&&t(Vu),_(qn,e),e&&t(Xu),e&&t(Qn),e&&t(Ku),_(jn,e),e&&t(Hu),e&&t(Z)}}}const Yq={local:"construction-dun-itokenizeri-bloc-par-bloc",sections:[{local:"acquisition-dun-corpus",title:"Acquisition d'un corpus"},{local:"construire-un-itokenizer-wordpiecei-partir-de-zro",title:"Construire un <i>tokenizer WordPiece</i> \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizeri-bpe-partir-de-zro",title:"Construire un <i>tokenizer</i> BPE \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizer-unigrami-partir-de-zro",title:"Construire un <i>tokenizer Unigram</i> \xE0 partir de z\xE9ro"}],title:"Construction d'un <i>tokenizer</i>, bloc par bloc"};function Zq(pi){return Gq(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class r5 extends Fq{constructor($){super();Wq(this,$,Zq,Jq,Rq,{})}}export{r5 as default,Yq as metadata};
