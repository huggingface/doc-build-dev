import{S as Ae,i as ke,s as Ie,e as r,k as d,w as $e,t as c,M as Se,c as l,d as t,m,a,x as Te,h,b as n,G as o,g as i,y as xe,L as Ce,q as Pe,o as Le,B as be,v as De}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ge}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Re}from"../../chunks/IconCopyLink-hf-doc-builder.js";function qe(se){let u,R,p,v,A,y,O,k,Q,q,E,M,_,j,I,z,F,N,P,K,U,L,V,X,b,W,Y,f,S,g,Z,ee,C,$,te,oe,D,T,re,le,G,x,ae,B;return y=new Re({}),E=new Ge({props:{id:"d_ixlCubqQw"}}),{c(){u=r("meta"),R=d(),p=r("h1"),v=r("a"),A=r("span"),$e(y.$$.fragment),O=d(),k=r("span"),Q=c("Decoder models"),q=d(),$e(E.$$.fragment),M=d(),_=r("p"),j=c("Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=r("em"),z=c("auto-regressive models"),F=c("."),N=d(),P=r("p"),K=c("The pretraining of decoder models usually revolves around predicting the next word in the sentence."),U=d(),L=r("p"),V=c("These models are best suited for tasks involving text generation."),X=d(),b=r("p"),W=c("Representatives of this family of models include:"),Y=d(),f=r("ul"),S=r("li"),g=r("a"),Z=c("CTRL"),ee=d(),C=r("li"),$=r("a"),te=c("GPT"),oe=d(),D=r("li"),T=r("a"),re=c("GPT-2"),le=d(),G=r("li"),x=r("a"),ae=c("Transformer XL"),this.h()},l(e){const s=Se('[data-svelte="svelte-1phssyn"]',document.head);u=l(s,"META",{name:!0,content:!0}),s.forEach(t),R=m(e),p=l(e,"H1",{class:!0});var H=a(p);v=l(H,"A",{id:!0,class:!0,href:!0});var ne=a(v);A=l(ne,"SPAN",{});var ie=a(A);Te(y.$$.fragment,ie),ie.forEach(t),ne.forEach(t),O=m(H),k=l(H,"SPAN",{});var fe=a(k);Q=h(fe,"Decoder models"),fe.forEach(t),H.forEach(t),q=m(e),Te(E.$$.fragment,e),M=m(e),_=l(e,"P",{});var J=a(_);j=h(J,"Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=l(J,"EM",{});var de=a(I);z=h(de,"auto-regressive models"),de.forEach(t),F=h(J,"."),J.forEach(t),N=m(e),P=l(e,"P",{});var ce=a(P);K=h(ce,"The pretraining of decoder models usually revolves around predicting the next word in the sentence."),ce.forEach(t),U=m(e),L=l(e,"P",{});var me=a(L);V=h(me,"These models are best suited for tasks involving text generation."),me.forEach(t),X=m(e),b=l(e,"P",{});var he=a(b);W=h(he,"Representatives of this family of models include:"),he.forEach(t),Y=m(e),f=l(e,"UL",{});var w=a(f);S=l(w,"LI",{});var ue=a(S);g=l(ue,"A",{href:!0,rel:!0});var pe=a(g);Z=h(pe,"CTRL"),pe.forEach(t),ue.forEach(t),ee=m(w),C=l(w,"LI",{});var ve=a(C);$=l(ve,"A",{href:!0,rel:!0});var _e=a($);te=h(_e,"GPT"),_e.forEach(t),ve.forEach(t),oe=m(w),D=l(w,"LI",{});var we=a(D);T=l(we,"A",{href:!0,rel:!0});var ye=a(T);re=h(ye,"GPT-2"),ye.forEach(t),we.forEach(t),le=m(w),G=l(w,"LI",{});var Ee=a(G);x=l(Ee,"A",{href:!0,rel:!0});var ge=a(x);ae=h(ge,"Transformer XL"),ge.forEach(t),Ee.forEach(t),w.forEach(t),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Me)),n(v,"id","decoder-models"),n(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(v,"href","#decoder-models"),n(p,"class","relative group"),n(g,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),n(g,"rel","nofollow"),n($,"href","https://huggingface.co/transformers/model_doc/gpt.html"),n($,"rel","nofollow"),n(T,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),n(T,"rel","nofollow"),n(x,"href","https://huggingface.co/transformers/model_doc/transfo-xl.html"),n(x,"rel","nofollow")},m(e,s){o(document.head,u),i(e,R,s),i(e,p,s),o(p,v),o(v,A),xe(y,A,null),o(p,O),o(p,k),o(k,Q),i(e,q,s),xe(E,e,s),i(e,M,s),i(e,_,s),o(_,j),o(_,I),o(I,z),o(_,F),i(e,N,s),i(e,P,s),o(P,K),i(e,U,s),i(e,L,s),o(L,V),i(e,X,s),i(e,b,s),o(b,W),i(e,Y,s),i(e,f,s),o(f,S),o(S,g),o(g,Z),o(f,ee),o(f,C),o(C,$),o($,te),o(f,oe),o(f,D),o(D,T),o(T,re),o(f,le),o(f,G),o(G,x),o(x,ae),B=!0},p:Ce,i(e){B||(Pe(y.$$.fragment,e),Pe(E.$$.fragment,e),B=!0)},o(e){Le(y.$$.fragment,e),Le(E.$$.fragment,e),B=!1},d(e){t(u),e&&t(R),e&&t(p),be(y),e&&t(q),be(E,e),e&&t(M),e&&t(_),e&&t(N),e&&t(P),e&&t(U),e&&t(L),e&&t(X),e&&t(b),e&&t(Y),e&&t(f)}}}const Me={local:"decoder-models",title:"Decoder models"};function Ne(se){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Be extends Ae{constructor(u){super();ke(this,u,Ne,qe,Ie,{})}}export{Be as default,Me as metadata};
