import{S as Mr,i as Pr,s as Cr,e as u,k as h,w as g,t as r,M as Fr,c as p,d as n,m as _,x as q,a as m,h as o,b as w,G as s,g as d,y as j,o as v,p as ze,q as $,B as E,v as Tr,n as Ae}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ar}from"../../chunks/Tip-hf-doc-builder.js";import{Y as xr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as bn}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as z}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Ir}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Sr}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Lr(k){let l,c;return l=new Ir({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"}]}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Nr(k){let l,c;return l=new Ir({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"}]}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Dr(k){let l,c;return l=new xr({props:{id:"ROxrFOEbsQE"}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Hr(k){let l,c;return l=new xr({props:{id:"M6adb1j2jPI"}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Or(k){let l,c,t,f;return l=new z({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),t=new z({props:{code:"InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]",highlighted:'InvalidArgumentError: Input to reshape <span class="hljs-keyword">is</span> a tensor <span class="hljs-keyword">with</span> <span class="hljs-number">14</span> values, but the requested shape has <span class="hljs-number">196</span> [Op:Reshape]'}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Br(k){let l,c,t,f;return l=new z({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# Cette ligne va \xE9chouer.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
<span class="hljs-comment"># Cette ligne va \xE9chouer.</span>
model(input_ids)`}}),t=new z({props:{code:"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",highlighted:'IndexError: Dimension out of <span class="hljs-built_in">range</span> (expected to be <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> of [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], but got <span class="hljs-number">1</span>)'}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Ur(k){let l,c,t,f;return l=new z({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new z({props:{code:`<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>,
        <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]], dtype=int32)&gt;`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Vr(k){let l,c,t,f;return l=new z({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new z({props:{code:`tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])`,highlighted:`tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,
          <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]])`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Jr(k){let l,c;return l=new z({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Rr(k){let l,c;return l=new z({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Gr(k){let l,c;return l=new z({props:{code:`Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)`,highlighted:`Input IDs: tf.Tensor(
[[ <span class="hljs-number">1045</span>  <span class="hljs-number">1005</span>  <span class="hljs-number">2310</span>  <span class="hljs-number">2042</span>  <span class="hljs-number">3403</span>  <span class="hljs-number">2005</span>  <span class="hljs-number">1037</span> <span class="hljs-number">17662</span> <span class="hljs-number">12172</span>  <span class="hljs-number">2607</span>  <span class="hljs-number">2026</span>  <span class="hljs-number">2878</span>
   <span class="hljs-number">2166</span>  <span class="hljs-number">1012</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), dtype=int32)
Logits: tf.Tensor([[-<span class="hljs-number">2.7276208</span>  <span class="hljs-number">2.8789377</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Yr(k){let l,c;return l=new z({props:{code:`Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]`,highlighted:`Input IDs: [[ <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>]]
Logits: [[-<span class="hljs-number">2.7276</span>,  <span class="hljs-number">2.8789</span>]]`}}),{c(){g(l.$$.fragment)},l(t){q(l.$$.fragment,t)},m(t,f){j(l,t,f),c=!0},i(t){c||($(l.$$.fragment,t),c=!0)},o(t){v(l.$$.fragment,t),c=!1},d(t){E(l,t)}}}function Qr(k){let l,c,t,f,a,b,N,M;return{c(){l=u("p"),c=r("\u270F\uFE0F "),t=u("strong"),f=r("Essayez !"),a=r(" Convertissez cette liste "),b=u("code"),N=r("batched_ids"),M=r(" en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !")},l(P){l=p(P,"P",{});var x=m(l);c=o(x,"\u270F\uFE0F "),t=p(x,"STRONG",{});var ne=m(t);f=o(ne,"Essayez !"),ne.forEach(n),a=o(x," Convertissez cette liste "),b=p(x,"CODE",{});var D=m(b);N=o(D,"batched_ids"),D.forEach(n),M=o(x," en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !"),x.forEach(n)},m(P,x){d(P,l,x),s(l,c),s(l,t),s(t,f),s(l,a),s(l,b),s(b,N),s(l,M)},d(P){P&&n(l)}}}function Kr(k){let l,c,t,f;return l=new z({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(tf.constant(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(batched_ids)).logits)`}}),t=new z({props:{code:`tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor([[ <span class="hljs-number">1.5693678</span> -<span class="hljs-number">1.3894581</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor([[ <span class="hljs-number">0.5803005</span>  -<span class="hljs-number">0.41252428</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor(
[[ <span class="hljs-number">1.5693681</span> -<span class="hljs-number">1.3894582</span>]
 [ <span class="hljs-number">1.3373486</span> -<span class="hljs-number">1.2163193</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Wr(k){let l,c,t,f;return l=new z({props:{code:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)`}}),t=new z({props:{code:`tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">1.3373</span>, -<span class="hljs-number">1.2163</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Xr(k){let l,c,t,f;return l=new z({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new z({props:{code:`tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[ <span class="hljs-number">1.5693681</span>  -<span class="hljs-number">1.3894582</span> ]
 [ <span class="hljs-number">0.5803021</span>  -<span class="hljs-number">0.41252586</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function Zr(k){let l,c,t,f;return l=new z({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new z({props:{code:`tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(l.$$.fragment),c=h(),g(t.$$.fragment)},l(a){q(l.$$.fragment,a),c=_(a),q(t.$$.fragment,a)},m(a,b){j(l,a,b),d(a,c,b),j(t,a,b),f=!0},i(a){f||($(l.$$.fragment,a),$(t.$$.fragment,a),f=!0)},o(a){v(l.$$.fragment,a),v(t.$$.fragment,a),f=!1},d(a){E(l,a),a&&n(c),E(t,a)}}}function eo(k){let l,c,t,f,a,b,N,M,P,x,ne,D,C,F;return{c(){l=u("p"),c=r("\u270F\uFE0F "),t=u("strong"),f=r("Essayez !"),a=r(" Appliquez la tokenisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB "),b=u("i"),N=r("I\u2019ve been waiting for a HuggingFace course my whole life."),M=r(" \xBB et \xAB "),P=u("i"),x=r("I hate this so much!"),ne=r(" \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),D=u("em"),C=r("padding"),F=r(" et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !")},l(H){l=p(H,"P",{});var y=m(l);c=o(y,"\u270F\uFE0F "),t=p(y,"STRONG",{});var T=m(t);f=o(T,"Essayez !"),T.forEach(n),a=o(y," Appliquez la tokenisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB "),b=p(y,"I",{});var ie=m(b);N=o(ie,"I\u2019ve been waiting for a HuggingFace course my whole life."),ie.forEach(n),M=o(y," \xBB et \xAB "),P=p(y,"I",{});var ue=m(P);x=o(ue,"I hate this so much!"),ue.forEach(n),ne=o(y," \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),D=p(y,"EM",{});var Xe=m(D);C=o(Xe,"padding"),Xe.forEach(n),F=o(y," et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !"),y.forEach(n)},m(H,y){d(H,l,y),s(l,c),s(l,t),s(t,f),s(l,a),s(l,b),s(b,N),s(l,M),s(l,P),s(P,x),s(l,ne),s(l,D),s(D,C),s(l,F)},d(H){H&&n(l)}}}function so(k){let l,c,t,f,a,b,N,M,P,x,ne,D,C,F,H,y,T,ie,ue,Xe,vn,O,ys,ct,mt,xe,dt,zs,ft,ht,_t,As,bt,vt,xs,$t,$n,fe,kt,Is,gt,qt,kn,pe,he,Ms,Ie,jt,Ps,Et,gn,Ze,wt,qn,V,J,es,ss,yt,jn,B,zt,Cs,At,xt,Fs,It,Mt,Ts,Pt,Ct,En,R,G,ns,ts,Ft,wn,Y,Q,ls,as,Tt,yn,K,W,rs,_e,St,Ss,Lt,Nt,zn,Me,An,os,Dt,xn,be,In,S,Ht,Ls,Ot,Bt,Ns,Ut,Vt,Ds,Jt,Rt,Hs,Gt,Yt,Mn,ce,ve,Os,Pe,Qt,is,Bs,Kt,Wt,Pn,us,Xt,Cn,Ce,Fn,L,Zt,Us,el,sl,Vs,nl,tl,Js,ll,al,Rs,rl,ol,Tn,Fe,Sn,te,il,Gs,ul,pl,Ys,cl,ml,Ln,X,Z,ps,cs,dl,Nn,A,fl,Qs,hl,_l,Ks,bl,vl,Ws,$l,kl,Xs,gl,ql,Zs,jl,El,en,wl,yl,sn,zl,Al,nn,xl,Il,Dn,me,$e,tn,Te,Ml,ln,Pl,Hn,ms,Cl,On,ke,Se,Fl,an,Tl,Sl,Ll,Le,Nl,rn,Dl,Hl,Bn,ds,Ol,Un,ee,se,fs,hs,Bl,Vn,ge,Ul,on,Vl,Jl,Jn,qe,Rn,de,je,un,Ne,Rl,pn,Gl,Gn,le,Yl,cn,Ql,Kl,mn,Wl,Xl,Yn,Ee,dn,Zl,ea,fn,sa,Qn,ae,na,De,ta,la,He,aa,ra,Kn,we,oa,hn,ia,ua,Wn,Oe,Xn;t=new Sr({props:{fw:k[0]}}),M=new bn({});const ca=[Nr,Lr],Be=[];function ma(e,i){return e[0]==="pt"?0:1}C=ma(k),F=Be[C]=ca[C](k);const da=[Hr,Dr],Ue=[];function fa(e,i){return e[0]==="pt"?0:1}y=fa(k),T=Ue[y]=da[y](k),Ie=new bn({});const ha=[Br,Or],Ve=[];function _a(e,i){return e[0]==="pt"?0:1}V=_a(k),J=Ve[V]=ha[V](k);const ba=[Vr,Ur],Je=[];function va(e,i){return e[0]==="pt"?0:1}R=va(k),G=Je[R]=ba[R](k);const $a=[Rr,Jr],Re=[];function ka(e,i){return e[0]==="pt"?0:1}Y=ka(k),Q=Re[Y]=$a[Y](k);const ga=[Yr,Gr],Ge=[];function qa(e,i){return e[0]==="pt"?0:1}K=qa(k),W=Ge[K]=ga[K](k),Me=new z({props:{code:"batched_ids = [ids, ids]",highlighted:'<span class="hljs-attr">batched_ids</span> = [ids, ids]'}}),be=new Ar({props:{$$slots:{default:[Qr]},$$scope:{ctx:k}}}),Pe=new bn({}),Ce=new z({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200]
]`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]
]`}}),Fe=new z({props:{code:`padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]`,highlighted:`padding_id = <span class="hljs-number">100</span>

batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, padding_id],
]`}});const ja=[Wr,Kr],Ye=[];function Ea(e,i){return e[0]==="pt"?0:1}X=Ea(k),Z=Ye[X]=ja[X](k),Te=new bn({});const wa=[Zr,Xr],Qe=[];function ya(e,i){return e[0]==="pt"?0:1}return ee=ya(k),se=Qe[ee]=wa[ee](k),qe=new Ar({props:{$$slots:{default:[eo]},$$scope:{ctx:k}}}),Ne=new bn({}),Oe=new z({props:{code:"sequence = sequence[:max_sequence_length]",highlighted:"sequence = sequence[:max_sequence_length]"}}),{c(){l=u("meta"),c=h(),g(t.$$.fragment),f=h(),a=u("h1"),b=u("a"),N=u("span"),g(M.$$.fragment),P=h(),x=u("span"),ne=r("Manipulation de plusieurs s\xE9quences"),D=h(),F.c(),H=h(),T.c(),ie=h(),ue=u("p"),Xe=r("Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),vn=h(),O=u("ul"),ys=u("li"),ct=r("comment g\xE9rer de plusieurs s\xE9quences ?"),mt=h(),xe=u("li"),dt=r("comment g\xE9rer de plusieurs s\xE9quences "),zs=u("em"),ft=r("de longueurs diff\xE9rentes"),ht=r(" ?"),_t=h(),As=u("li"),bt=r("les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),vt=h(),xs=u("li"),$t=r("existe-t-il une s\xE9quence trop longue ?"),$n=h(),fe=u("p"),kt=r("Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),Is=u("em"),gt=r("Transformers"),qt=r("."),kn=h(),pe=u("h2"),he=u("a"),Ms=u("span"),g(Ie.$$.fragment),jt=h(),Ps=u("span"),Et=r("Les mod\xE8les attendent un batch d'entr\xE9es"),gn=h(),Ze=u("p"),wt=r(`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),qn=h(),J.c(),es=h(),ss=u("p"),yt=r("Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),jn=h(),B=u("p"),zt=r("Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Cs=u("em"),At=r("Transformers"),xt=r(" attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Fs=u("em"),It=r("tokenizer"),Mt=r(" fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ts=u("code"),Pt=r("s\xE9quence"),Ct=r(". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),En=h(),G.c(),ns=h(),ts=u("p"),Ft=r("Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),wn=h(),Q.c(),ls=h(),as=u("p"),Tt=r("Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),yn=h(),W.c(),rs=h(),_e=u("p"),St=r("Le \xAB "),Ss=u("em"),Lt=r("batching"),Nt=r(" \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),zn=h(),g(Me.$$.fragment),An=h(),os=u("p"),Dt=r("Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),xn=h(),g(be.$$.fragment),In=h(),S=u("p"),Ht=r("Utiliser des "),Ls=u("em"),Ot=r("batchs"),Bt=r(" permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Ns=u("em"),Ut=r("rembourrer"),Vt=r("/"),Ds=u("em"),Jt=r("remplir"),Rt=r(" (le "),Hs=u("em"),Gt=r("padding"),Yt=r(" en anglais) les entr\xE9es."),Mn=h(),ce=u("h2"),ve=u("a"),Os=u("span"),g(Pe.$$.fragment),Qt=h(),is=u("span"),Bs=u("i"),Kt=r("Padding"),Wt=r(" des entr\xE9es"),Pn=h(),us=u("p"),Xt=r("La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),Cn=h(),g(Ce.$$.fragment),Fn=h(),L=u("p"),Zt=r("Afin de contourner ce probl\xE8me, nous utilisons le "),Us=u("em"),el=r("padding"),sl=r(" pour que nos tenseurs aient une forme rectangulaire. Le "),Vs=u("em"),nl=r("padding"),tl=r(" permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Js=u("em"),ll=r("padding token"),al=r(" aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Rs=u("em"),rl=r("padding"),ol=r(" fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),Tn=h(),g(Fe.$$.fragment),Sn=h(),te=u("p"),il=r("L\u2019identifiant du jeton de "),Gs=u("em"),ul=r("padding"),pl=r(" peut \xEAtre trouv\xE9 dans "),Ys=u("code"),cl=r("tokenizer.pad_token_id"),ml=r(". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),Ln=h(),Z.c(),ps=h(),cs=u("p"),dl=r("Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),Nn=h(),A=u("p"),fl=r("C\u2019est parce que dans un "),Qs=u("em"),hl=r("transformer"),_l=r(" les couches d\u2019attention "),Ks=u("em"),bl=r("contextualisent"),vl=r(" chaque "),Ws=u("em"),$l=r("token"),kl=r(". Celles-ci prennent en compte les "),Xs=u("em"),gl=r("tokens"),ql=r(" de "),Zs=u("em"),jl=r("padding"),El=r(" puisqu\u2019elles analysent tous les "),en=u("em"),wl=r("tokens"),yl=r(" d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),sn=u("em"),zl=r("padding"),Al=r(", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),nn=u("em"),xl=r("padding"),Il=r(". Ceci est fait en utilisant un masque d\u2019attention."),Dn=h(),me=u("h2"),$e=u("a"),tn=u("span"),g(Te.$$.fragment),Ml=h(),ln=u("span"),Pl=r("Masques d'attention"),Hn=h(),ms=u("p"),Cl=r("Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),On=h(),ke=u("ul"),Se=u("li"),Fl=r("1 indique que les "),an=u("em"),Tl=r("tokens"),Sl=r(" correspondants doivent \xEAtre analys\xE9s"),Ll=h(),Le=u("li"),Nl=r("0 indique que les "),rn=u("em"),Dl=r("tokens"),Hl=r(" correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),Bn=h(),ds=u("p"),Ol=r("Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),Un=h(),se.c(),fs=h(),hs=u("p"),Bl=r("Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),Vn=h(),ge=u("p"),Ul=r("Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),on=u("em"),Vl=r("padding"),Jl=r(" valant 0 dans le masque d\u2019attention."),Jn=h(),g(qe.$$.fragment),Rn=h(),de=u("h2"),je=u("a"),un=u("span"),g(Ne.$$.fragment),Rl=h(),pn=u("span"),Gl=r("S\xE9quences plus longues"),Gn=h(),le=u("p"),Yl=r("Les "),cn=u("em"),Ql=r("transformers"),Kl=r(" acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),mn=u("em"),Wl=r("tokens"),Xl=r(" et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),Yn=h(),Ee=u("ul"),dn=u("li"),Zl=r("utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),ea=h(),fn=u("li"),sa=r("tronquer les s\xE9quences."),Qn=h(),ae=u("p"),na=r("Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),De=u("a"),ta=r("Longformer"),la=r(" ou le "),He=u("a"),aa=r("LED"),ra=r(". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),Kn=h(),we=u("p"),oa=r("Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),hn=u("code"),ia=r("max_sequence_length"),ua=r(" :"),Wn=h(),g(Oe.$$.fragment),this.h()},l(e){const i=Fr('[data-svelte="svelte-1phssyn"]',document.head);l=p(i,"META",{name:!0,content:!0}),i.forEach(n),c=_(e),q(t.$$.fragment,e),f=_(e),a=p(e,"H1",{class:!0});var Ke=m(a);b=p(Ke,"A",{id:!0,class:!0,href:!0});var _s=m(b);N=p(_s,"SPAN",{});var bs=m(N);q(M.$$.fragment,bs),bs.forEach(n),_s.forEach(n),P=_(Ke),x=p(Ke,"SPAN",{});var vs=m(x);ne=o(vs,"Manipulation de plusieurs s\xE9quences"),vs.forEach(n),Ke.forEach(n),D=_(e),F.l(e),H=_(e),T.l(e),ie=_(e),ue=p(e,"P",{});var $s=m(ue);Xe=o($s,"Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),$s.forEach(n),vn=_(e),O=p(e,"UL",{});var U=m(O);ys=p(U,"LI",{});var ks=m(ys);ct=o(ks,"comment g\xE9rer de plusieurs s\xE9quences ?"),ks.forEach(n),mt=_(U),xe=p(U,"LI",{});var We=m(xe);dt=o(We,"comment g\xE9rer de plusieurs s\xE9quences "),zs=p(We,"EM",{});var gs=m(zs);ft=o(gs,"de longueurs diff\xE9rentes"),gs.forEach(n),ht=o(We," ?"),We.forEach(n),_t=_(U),As=p(U,"LI",{});var qs=m(As);bt=o(qs,"les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),qs.forEach(n),vt=_(U),xs=p(U,"LI",{});var _n=m(xs);$t=o(_n,"existe-t-il une s\xE9quence trop longue ?"),_n.forEach(n),U.forEach(n),$n=_(e),fe=p(e,"P",{});var Zn=m(fe);kt=o(Zn,"Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),Is=p(Zn,"EM",{});var za=m(Is);gt=o(za,"Transformers"),za.forEach(n),qt=o(Zn,"."),Zn.forEach(n),kn=_(e),pe=p(e,"H2",{class:!0});var et=m(pe);he=p(et,"A",{id:!0,class:!0,href:!0});var Aa=m(he);Ms=p(Aa,"SPAN",{});var xa=m(Ms);q(Ie.$$.fragment,xa),xa.forEach(n),Aa.forEach(n),jt=_(et),Ps=p(et,"SPAN",{});var Ia=m(Ps);Et=o(Ia,"Les mod\xE8les attendent un batch d'entr\xE9es"),Ia.forEach(n),et.forEach(n),gn=_(e),Ze=p(e,"P",{});var Ma=m(Ze);wt=o(Ma,`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),Ma.forEach(n),qn=_(e),J.l(e),es=_(e),ss=p(e,"P",{});var Pa=m(ss);yt=o(Pa,"Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),Pa.forEach(n),jn=_(e),B=p(e,"P",{});var ye=m(B);zt=o(ye,"Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Cs=p(ye,"EM",{});var Ca=m(Cs);At=o(Ca,"Transformers"),Ca.forEach(n),xt=o(ye," attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Fs=p(ye,"EM",{});var Fa=m(Fs);It=o(Fa,"tokenizer"),Fa.forEach(n),Mt=o(ye," fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ts=p(ye,"CODE",{});var Ta=m(Ts);Pt=o(Ta,"s\xE9quence"),Ta.forEach(n),Ct=o(ye,". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),ye.forEach(n),En=_(e),G.l(e),ns=_(e),ts=p(e,"P",{});var Sa=m(ts);Ft=o(Sa,"Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),Sa.forEach(n),wn=_(e),Q.l(e),ls=_(e),as=p(e,"P",{});var La=m(as);Tt=o(La,"Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),La.forEach(n),yn=_(e),W.l(e),rs=_(e),_e=p(e,"P",{});var st=m(_e);St=o(st,"Le \xAB "),Ss=p(st,"EM",{});var Na=m(Ss);Lt=o(Na,"batching"),Na.forEach(n),Nt=o(st," \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),st.forEach(n),zn=_(e),q(Me.$$.fragment,e),An=_(e),os=p(e,"P",{});var Da=m(os);Dt=o(Da,"Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),Da.forEach(n),xn=_(e),q(be.$$.fragment,e),In=_(e),S=p(e,"P",{});var re=m(S);Ht=o(re,"Utiliser des "),Ls=p(re,"EM",{});var Ha=m(Ls);Ot=o(Ha,"batchs"),Ha.forEach(n),Bt=o(re," permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Ns=p(re,"EM",{});var Oa=m(Ns);Ut=o(Oa,"rembourrer"),Oa.forEach(n),Vt=o(re,"/"),Ds=p(re,"EM",{});var Ba=m(Ds);Jt=o(Ba,"remplir"),Ba.forEach(n),Rt=o(re," (le "),Hs=p(re,"EM",{});var Ua=m(Hs);Gt=o(Ua,"padding"),Ua.forEach(n),Yt=o(re," en anglais) les entr\xE9es."),re.forEach(n),Mn=_(e),ce=p(e,"H2",{class:!0});var nt=m(ce);ve=p(nt,"A",{id:!0,class:!0,href:!0});var Va=m(ve);Os=p(Va,"SPAN",{});var Ja=m(Os);q(Pe.$$.fragment,Ja),Ja.forEach(n),Va.forEach(n),Qt=_(nt),is=p(nt,"SPAN",{});var pa=m(is);Bs=p(pa,"I",{});var Ra=m(Bs);Kt=o(Ra,"Padding"),Ra.forEach(n),Wt=o(pa," des entr\xE9es"),pa.forEach(n),nt.forEach(n),Pn=_(e),us=p(e,"P",{});var Ga=m(us);Xt=o(Ga,"La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),Ga.forEach(n),Cn=_(e),q(Ce.$$.fragment,e),Fn=_(e),L=p(e,"P",{});var oe=m(L);Zt=o(oe,"Afin de contourner ce probl\xE8me, nous utilisons le "),Us=p(oe,"EM",{});var Ya=m(Us);el=o(Ya,"padding"),Ya.forEach(n),sl=o(oe," pour que nos tenseurs aient une forme rectangulaire. Le "),Vs=p(oe,"EM",{});var Qa=m(Vs);nl=o(Qa,"padding"),Qa.forEach(n),tl=o(oe," permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Js=p(oe,"EM",{});var Ka=m(Js);ll=o(Ka,"padding token"),Ka.forEach(n),al=o(oe," aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Rs=p(oe,"EM",{});var Wa=m(Rs);rl=o(Wa,"padding"),Wa.forEach(n),ol=o(oe," fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),oe.forEach(n),Tn=_(e),q(Fe.$$.fragment,e),Sn=_(e),te=p(e,"P",{});var js=m(te);il=o(js,"L\u2019identifiant du jeton de "),Gs=p(js,"EM",{});var Xa=m(Gs);ul=o(Xa,"padding"),Xa.forEach(n),pl=o(js," peut \xEAtre trouv\xE9 dans "),Ys=p(js,"CODE",{});var Za=m(Ys);cl=o(Za,"tokenizer.pad_token_id"),Za.forEach(n),ml=o(js,". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),js.forEach(n),Ln=_(e),Z.l(e),ps=_(e),cs=p(e,"P",{});var er=m(cs);dl=o(er,"Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),er.forEach(n),Nn=_(e),A=p(e,"P",{});var I=m(A);fl=o(I,"C\u2019est parce que dans un "),Qs=p(I,"EM",{});var sr=m(Qs);hl=o(sr,"transformer"),sr.forEach(n),_l=o(I," les couches d\u2019attention "),Ks=p(I,"EM",{});var nr=m(Ks);bl=o(nr,"contextualisent"),nr.forEach(n),vl=o(I," chaque "),Ws=p(I,"EM",{});var tr=m(Ws);$l=o(tr,"token"),tr.forEach(n),kl=o(I,". Celles-ci prennent en compte les "),Xs=p(I,"EM",{});var lr=m(Xs);gl=o(lr,"tokens"),lr.forEach(n),ql=o(I," de "),Zs=p(I,"EM",{});var ar=m(Zs);jl=o(ar,"padding"),ar.forEach(n),El=o(I," puisqu\u2019elles analysent tous les "),en=p(I,"EM",{});var rr=m(en);wl=o(rr,"tokens"),rr.forEach(n),yl=o(I," d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),sn=p(I,"EM",{});var or=m(sn);zl=o(or,"padding"),or.forEach(n),Al=o(I,", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),nn=p(I,"EM",{});var ir=m(nn);xl=o(ir,"padding"),ir.forEach(n),Il=o(I,". Ceci est fait en utilisant un masque d\u2019attention."),I.forEach(n),Dn=_(e),me=p(e,"H2",{class:!0});var tt=m(me);$e=p(tt,"A",{id:!0,class:!0,href:!0});var ur=m($e);tn=p(ur,"SPAN",{});var pr=m(tn);q(Te.$$.fragment,pr),pr.forEach(n),ur.forEach(n),Ml=_(tt),ln=p(tt,"SPAN",{});var cr=m(ln);Pl=o(cr,"Masques d'attention"),cr.forEach(n),tt.forEach(n),Hn=_(e),ms=p(e,"P",{});var mr=m(ms);Cl=o(mr,"Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),mr.forEach(n),On=_(e),ke=p(e,"UL",{});var lt=m(ke);Se=p(lt,"LI",{});var at=m(Se);Fl=o(at,"1 indique que les "),an=p(at,"EM",{});var dr=m(an);Tl=o(dr,"tokens"),dr.forEach(n),Sl=o(at," correspondants doivent \xEAtre analys\xE9s"),at.forEach(n),Ll=_(lt),Le=p(lt,"LI",{});var rt=m(Le);Nl=o(rt,"0 indique que les "),rn=p(rt,"EM",{});var fr=m(rn);Dl=o(fr,"tokens"),fr.forEach(n),Hl=o(rt," correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),rt.forEach(n),lt.forEach(n),Bn=_(e),ds=p(e,"P",{});var hr=m(ds);Ol=o(hr,"Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),hr.forEach(n),Un=_(e),se.l(e),fs=_(e),hs=p(e,"P",{});var _r=m(hs);Bl=o(_r,"Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),_r.forEach(n),Vn=_(e),ge=p(e,"P",{});var ot=m(ge);Ul=o(ot,"Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),on=p(ot,"EM",{});var br=m(on);Vl=o(br,"padding"),br.forEach(n),Jl=o(ot," valant 0 dans le masque d\u2019attention."),ot.forEach(n),Jn=_(e),q(qe.$$.fragment,e),Rn=_(e),de=p(e,"H2",{class:!0});var it=m(de);je=p(it,"A",{id:!0,class:!0,href:!0});var vr=m(je);un=p(vr,"SPAN",{});var $r=m(un);q(Ne.$$.fragment,$r),$r.forEach(n),vr.forEach(n),Rl=_(it),pn=p(it,"SPAN",{});var kr=m(pn);Gl=o(kr,"S\xE9quences plus longues"),kr.forEach(n),it.forEach(n),Gn=_(e),le=p(e,"P",{});var Es=m(le);Yl=o(Es,"Les "),cn=p(Es,"EM",{});var gr=m(cn);Ql=o(gr,"transformers"),gr.forEach(n),Kl=o(Es," acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),mn=p(Es,"EM",{});var qr=m(mn);Wl=o(qr,"tokens"),qr.forEach(n),Xl=o(Es," et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),Es.forEach(n),Yn=_(e),Ee=p(e,"UL",{});var ut=m(Ee);dn=p(ut,"LI",{});var jr=m(dn);Zl=o(jr,"utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),jr.forEach(n),ea=_(ut),fn=p(ut,"LI",{});var Er=m(fn);sa=o(Er,"tronquer les s\xE9quences."),Er.forEach(n),ut.forEach(n),Qn=_(e),ae=p(e,"P",{});var ws=m(ae);na=o(ws,"Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),De=p(ws,"A",{href:!0,rel:!0});var wr=m(De);ta=o(wr,"Longformer"),wr.forEach(n),la=o(ws," ou le "),He=p(ws,"A",{href:!0,rel:!0});var yr=m(He);aa=o(yr,"LED"),yr.forEach(n),ra=o(ws,". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),ws.forEach(n),Kn=_(e),we=p(e,"P",{});var pt=m(we);oa=o(pt,"Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),hn=p(pt,"CODE",{});var zr=m(hn);ia=o(zr,"max_sequence_length"),zr.forEach(n),ua=o(pt," :"),pt.forEach(n),Wn=_(e),q(Oe.$$.fragment,e),this.h()},h(){w(l,"name","hf:doc:metadata"),w(l,"content",JSON.stringify(no)),w(b,"id","manipulation-de-plusieurs-squences"),w(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(b,"href","#manipulation-de-plusieurs-squences"),w(a,"class","relative group"),w(he,"id","les-modles-attendent-un-batch-dentres"),w(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(he,"href","#les-modles-attendent-un-batch-dentres"),w(pe,"class","relative group"),w(ve,"id","ipaddingi-des-entres"),w(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(ve,"href","#ipaddingi-des-entres"),w(ce,"class","relative group"),w($e,"id","masques-dattention"),w($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w($e,"href","#masques-dattention"),w(me,"class","relative group"),w(je,"id","squences-plus-longues"),w(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(je,"href","#squences-plus-longues"),w(de,"class","relative group"),w(De,"href","https://huggingface.co/transformers/model_doc/longformer.html"),w(De,"rel","nofollow"),w(He,"href","https://huggingface.co/transformers/model_doc/led.html"),w(He,"rel","nofollow")},m(e,i){s(document.head,l),d(e,c,i),j(t,e,i),d(e,f,i),d(e,a,i),s(a,b),s(b,N),j(M,N,null),s(a,P),s(a,x),s(x,ne),d(e,D,i),Be[C].m(e,i),d(e,H,i),Ue[y].m(e,i),d(e,ie,i),d(e,ue,i),s(ue,Xe),d(e,vn,i),d(e,O,i),s(O,ys),s(ys,ct),s(O,mt),s(O,xe),s(xe,dt),s(xe,zs),s(zs,ft),s(xe,ht),s(O,_t),s(O,As),s(As,bt),s(O,vt),s(O,xs),s(xs,$t),d(e,$n,i),d(e,fe,i),s(fe,kt),s(fe,Is),s(Is,gt),s(fe,qt),d(e,kn,i),d(e,pe,i),s(pe,he),s(he,Ms),j(Ie,Ms,null),s(pe,jt),s(pe,Ps),s(Ps,Et),d(e,gn,i),d(e,Ze,i),s(Ze,wt),d(e,qn,i),Ve[V].m(e,i),d(e,es,i),d(e,ss,i),s(ss,yt),d(e,jn,i),d(e,B,i),s(B,zt),s(B,Cs),s(Cs,At),s(B,xt),s(B,Fs),s(Fs,It),s(B,Mt),s(B,Ts),s(Ts,Pt),s(B,Ct),d(e,En,i),Je[R].m(e,i),d(e,ns,i),d(e,ts,i),s(ts,Ft),d(e,wn,i),Re[Y].m(e,i),d(e,ls,i),d(e,as,i),s(as,Tt),d(e,yn,i),Ge[K].m(e,i),d(e,rs,i),d(e,_e,i),s(_e,St),s(_e,Ss),s(Ss,Lt),s(_e,Nt),d(e,zn,i),j(Me,e,i),d(e,An,i),d(e,os,i),s(os,Dt),d(e,xn,i),j(be,e,i),d(e,In,i),d(e,S,i),s(S,Ht),s(S,Ls),s(Ls,Ot),s(S,Bt),s(S,Ns),s(Ns,Ut),s(S,Vt),s(S,Ds),s(Ds,Jt),s(S,Rt),s(S,Hs),s(Hs,Gt),s(S,Yt),d(e,Mn,i),d(e,ce,i),s(ce,ve),s(ve,Os),j(Pe,Os,null),s(ce,Qt),s(ce,is),s(is,Bs),s(Bs,Kt),s(is,Wt),d(e,Pn,i),d(e,us,i),s(us,Xt),d(e,Cn,i),j(Ce,e,i),d(e,Fn,i),d(e,L,i),s(L,Zt),s(L,Us),s(Us,el),s(L,sl),s(L,Vs),s(Vs,nl),s(L,tl),s(L,Js),s(Js,ll),s(L,al),s(L,Rs),s(Rs,rl),s(L,ol),d(e,Tn,i),j(Fe,e,i),d(e,Sn,i),d(e,te,i),s(te,il),s(te,Gs),s(Gs,ul),s(te,pl),s(te,Ys),s(Ys,cl),s(te,ml),d(e,Ln,i),Ye[X].m(e,i),d(e,ps,i),d(e,cs,i),s(cs,dl),d(e,Nn,i),d(e,A,i),s(A,fl),s(A,Qs),s(Qs,hl),s(A,_l),s(A,Ks),s(Ks,bl),s(A,vl),s(A,Ws),s(Ws,$l),s(A,kl),s(A,Xs),s(Xs,gl),s(A,ql),s(A,Zs),s(Zs,jl),s(A,El),s(A,en),s(en,wl),s(A,yl),s(A,sn),s(sn,zl),s(A,Al),s(A,nn),s(nn,xl),s(A,Il),d(e,Dn,i),d(e,me,i),s(me,$e),s($e,tn),j(Te,tn,null),s(me,Ml),s(me,ln),s(ln,Pl),d(e,Hn,i),d(e,ms,i),s(ms,Cl),d(e,On,i),d(e,ke,i),s(ke,Se),s(Se,Fl),s(Se,an),s(an,Tl),s(Se,Sl),s(ke,Ll),s(ke,Le),s(Le,Nl),s(Le,rn),s(rn,Dl),s(Le,Hl),d(e,Bn,i),d(e,ds,i),s(ds,Ol),d(e,Un,i),Qe[ee].m(e,i),d(e,fs,i),d(e,hs,i),s(hs,Bl),d(e,Vn,i),d(e,ge,i),s(ge,Ul),s(ge,on),s(on,Vl),s(ge,Jl),d(e,Jn,i),j(qe,e,i),d(e,Rn,i),d(e,de,i),s(de,je),s(je,un),j(Ne,un,null),s(de,Rl),s(de,pn),s(pn,Gl),d(e,Gn,i),d(e,le,i),s(le,Yl),s(le,cn),s(cn,Ql),s(le,Kl),s(le,mn),s(mn,Wl),s(le,Xl),d(e,Yn,i),d(e,Ee,i),s(Ee,dn),s(dn,Zl),s(Ee,ea),s(Ee,fn),s(fn,sa),d(e,Qn,i),d(e,ae,i),s(ae,na),s(ae,De),s(De,ta),s(ae,la),s(ae,He),s(He,aa),s(ae,ra),d(e,Kn,i),d(e,we,i),s(we,oa),s(we,hn),s(hn,ia),s(we,ua),d(e,Wn,i),j(Oe,e,i),Xn=!0},p(e,[i]){const Ke={};i&1&&(Ke.fw=e[0]),t.$set(Ke);let _s=C;C=ma(e),C!==_s&&(Ae(),v(Be[_s],1,1,()=>{Be[_s]=null}),ze(),F=Be[C],F||(F=Be[C]=ca[C](e),F.c()),$(F,1),F.m(H.parentNode,H));let bs=y;y=fa(e),y!==bs&&(Ae(),v(Ue[bs],1,1,()=>{Ue[bs]=null}),ze(),T=Ue[y],T||(T=Ue[y]=da[y](e),T.c()),$(T,1),T.m(ie.parentNode,ie));let vs=V;V=_a(e),V!==vs&&(Ae(),v(Ve[vs],1,1,()=>{Ve[vs]=null}),ze(),J=Ve[V],J||(J=Ve[V]=ha[V](e),J.c()),$(J,1),J.m(es.parentNode,es));let $s=R;R=va(e),R!==$s&&(Ae(),v(Je[$s],1,1,()=>{Je[$s]=null}),ze(),G=Je[R],G||(G=Je[R]=ba[R](e),G.c()),$(G,1),G.m(ns.parentNode,ns));let U=Y;Y=ka(e),Y!==U&&(Ae(),v(Re[U],1,1,()=>{Re[U]=null}),ze(),Q=Re[Y],Q||(Q=Re[Y]=$a[Y](e),Q.c()),$(Q,1),Q.m(ls.parentNode,ls));let ks=K;K=qa(e),K!==ks&&(Ae(),v(Ge[ks],1,1,()=>{Ge[ks]=null}),ze(),W=Ge[K],W||(W=Ge[K]=ga[K](e),W.c()),$(W,1),W.m(rs.parentNode,rs));const We={};i&2&&(We.$$scope={dirty:i,ctx:e}),be.$set(We);let gs=X;X=Ea(e),X!==gs&&(Ae(),v(Ye[gs],1,1,()=>{Ye[gs]=null}),ze(),Z=Ye[X],Z||(Z=Ye[X]=ja[X](e),Z.c()),$(Z,1),Z.m(ps.parentNode,ps));let qs=ee;ee=ya(e),ee!==qs&&(Ae(),v(Qe[qs],1,1,()=>{Qe[qs]=null}),ze(),se=Qe[ee],se||(se=Qe[ee]=wa[ee](e),se.c()),$(se,1),se.m(fs.parentNode,fs));const _n={};i&2&&(_n.$$scope={dirty:i,ctx:e}),qe.$set(_n)},i(e){Xn||($(t.$$.fragment,e),$(M.$$.fragment,e),$(F),$(T),$(Ie.$$.fragment,e),$(J),$(G),$(Q),$(W),$(Me.$$.fragment,e),$(be.$$.fragment,e),$(Pe.$$.fragment,e),$(Ce.$$.fragment,e),$(Fe.$$.fragment,e),$(Z),$(Te.$$.fragment,e),$(se),$(qe.$$.fragment,e),$(Ne.$$.fragment,e),$(Oe.$$.fragment,e),Xn=!0)},o(e){v(t.$$.fragment,e),v(M.$$.fragment,e),v(F),v(T),v(Ie.$$.fragment,e),v(J),v(G),v(Q),v(W),v(Me.$$.fragment,e),v(be.$$.fragment,e),v(Pe.$$.fragment,e),v(Ce.$$.fragment,e),v(Fe.$$.fragment,e),v(Z),v(Te.$$.fragment,e),v(se),v(qe.$$.fragment,e),v(Ne.$$.fragment,e),v(Oe.$$.fragment,e),Xn=!1},d(e){n(l),e&&n(c),E(t,e),e&&n(f),e&&n(a),E(M),e&&n(D),Be[C].d(e),e&&n(H),Ue[y].d(e),e&&n(ie),e&&n(ue),e&&n(vn),e&&n(O),e&&n($n),e&&n(fe),e&&n(kn),e&&n(pe),E(Ie),e&&n(gn),e&&n(Ze),e&&n(qn),Ve[V].d(e),e&&n(es),e&&n(ss),e&&n(jn),e&&n(B),e&&n(En),Je[R].d(e),e&&n(ns),e&&n(ts),e&&n(wn),Re[Y].d(e),e&&n(ls),e&&n(as),e&&n(yn),Ge[K].d(e),e&&n(rs),e&&n(_e),e&&n(zn),E(Me,e),e&&n(An),e&&n(os),e&&n(xn),E(be,e),e&&n(In),e&&n(S),e&&n(Mn),e&&n(ce),E(Pe),e&&n(Pn),e&&n(us),e&&n(Cn),E(Ce,e),e&&n(Fn),e&&n(L),e&&n(Tn),E(Fe,e),e&&n(Sn),e&&n(te),e&&n(Ln),Ye[X].d(e),e&&n(ps),e&&n(cs),e&&n(Nn),e&&n(A),e&&n(Dn),e&&n(me),E(Te),e&&n(Hn),e&&n(ms),e&&n(On),e&&n(ke),e&&n(Bn),e&&n(ds),e&&n(Un),Qe[ee].d(e),e&&n(fs),e&&n(hs),e&&n(Vn),e&&n(ge),e&&n(Jn),E(qe,e),e&&n(Rn),e&&n(de),E(Ne),e&&n(Gn),e&&n(le),e&&n(Yn),e&&n(Ee),e&&n(Qn),e&&n(ae),e&&n(Kn),e&&n(we),e&&n(Wn),E(Oe,e)}}}const no={local:"manipulation-de-plusieurs-squences",sections:[{local:"les-modles-attendent-un-batch-dentres",title:"Les mod\xE8les attendent un batch d'entr\xE9es"},{local:"ipaddingi-des-entres",title:"<i>Padding</i> des entr\xE9es"},{local:"masques-dattention",title:"Masques d'attention"},{local:"squences-plus-longues",title:"S\xE9quences plus longues"}],title:"Manipulation de plusieurs s\xE9quences"};function to(k,l,c){let t="pt";return Tr(()=>{const f=new URLSearchParams(window.location.search);c(0,t=f.get("fw")||"pt")}),[t]}class co extends Mr{constructor(l){super();Pr(this,l,to,so,Cr,{})}}export{co as default,no as metadata};
