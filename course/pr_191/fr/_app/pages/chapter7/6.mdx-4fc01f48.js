import{S as ap,i as op,s as lp,e as r,t as n,k as m,w as y,c as i,a as u,h as a,d as t,m as f,x as w,g as p,F as s,y as E,q as v,o as b,B as x,b as D,l as dr,M as rp,N as pr,p as ea,v as ip,n as sa}from"../../chunks/vendor-1e8b365d.js";import{T as ta}from"../../chunks/Tip-62b14c6e.js";import{Y as hi}from"../../chunks/Youtube-c2a8cc39.js";import{I as Bo}from"../../chunks/IconCopyLink-483c28ba.js";import{C as M}from"../../chunks/CodeBlock-e5764662.js";import{D as np}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as up}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function pp(A){let l,_;return l=new np({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"}]}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function dp(A){let l,_;return l=new np({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"}]}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function cp(A){let l,_;return{c(){l=r("p"),_=n("Le pr\xE9-entra\xEEnement du mod\xE8le de langue prendra un certain temps. Nous vous sugg\xE9rons d\u2019ex\xE9cuter d\u2019abord la boucle d\u2019entra\xEEnement sur un \xE9chantillon des donn\xE9es en d\xE9commentant les deux lignes partielles ci-dessus, et de vous assurer que l\u2019entra\xEEnement se termine avec succ\xE8s et que les mod\xE8les sont stock\xE9s. Rien n\u2019est plus frustrant qu\u2019un entra\xEEnement qui \xE9choue \xE0 la derni\xE8re \xE9tape parce que vous avez oubli\xE9 de cr\xE9er un dossier ou parce qu\u2019il y a une faute de frappe \xE0 la fin de la boucle d\u2019entra\xEEnement !")},l(d){l=i(d,"P",{});var h=u(l);_=a(h,"Le pr\xE9-entra\xEEnement du mod\xE8le de langue prendra un certain temps. Nous vous sugg\xE9rons d\u2019ex\xE9cuter d\u2019abord la boucle d\u2019entra\xEEnement sur un \xE9chantillon des donn\xE9es en d\xE9commentant les deux lignes partielles ci-dessus, et de vous assurer que l\u2019entra\xEEnement se termine avec succ\xE8s et que les mod\xE8les sont stock\xE9s. Rien n\u2019est plus frustrant qu\u2019un entra\xEEnement qui \xE9choue \xE0 la derni\xE8re \xE9tape parce que vous avez oubli\xE9 de cr\xE9er un dossier ou parce qu\u2019il y a une faute de frappe \xE0 la fin de la boucle d\u2019entra\xEEnement !"),h.forEach(t)},m(d,h){p(d,l,h),s(l,_)},d(d){d&&t(l)}}}function mp(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O,K,I,Z;return{c(){l=r("p"),_=n("\u270F\uFE0F "),d=r("strong"),h=n("Essayez"),k=n(" Se d\xE9barrasser de tous les morceaux qui sont plus petits que la taille du contexte n\u2019\xE9tait pas un gros probl\xE8me ici parce que nous utilisons de petites fen\xEAtres de contexte. Si vous augmentez la taille du contexte (ou si vous avez un corpus de documents courts), la fraction des morceaux qui sont jet\xE9s augmentera \xE9galement. Une fa\xE7on plus efficace de pr\xE9parer les donn\xE9es est de joindre tous les \xE9chantillons dans un batch avec un "),$=r("em"),z=n("token"),C=m(),j=r("code"),P=n("eos_token_id"),S=n(" entre les deux, puis d\u2019effectuer le chunking sur les s\xE9quences concat\xE9n\xE9es. Comme exercice, modifiez la fonction "),L=r("code"),g=n("tokenize()"),T=n(" pour utiliser cette approche. Notez que vous devrez mettre "),H=r("code"),B=n("truncation=False"),W=n(" et enlever les autres arguments du "),R=r("em"),Q=n("tokenizer"),O=n(" pour obtenir la s\xE9quence compl\xE8te des IDs des "),K=r("em"),I=n("tokens"),Z=n(".")},l(X){l=i(X,"P",{});var F=u(l);_=a(F,"\u270F\uFE0F "),d=i(F,"STRONG",{});var G=u(d);h=a(G,"Essayez"),G.forEach(t),k=a(F," Se d\xE9barrasser de tous les morceaux qui sont plus petits que la taille du contexte n\u2019\xE9tait pas un gros probl\xE8me ici parce que nous utilisons de petites fen\xEAtres de contexte. Si vous augmentez la taille du contexte (ou si vous avez un corpus de documents courts), la fraction des morceaux qui sont jet\xE9s augmentera \xE9galement. Une fa\xE7on plus efficace de pr\xE9parer les donn\xE9es est de joindre tous les \xE9chantillons dans un batch avec un "),$=i(F,"EM",{});var U=u($);z=a(U,"token"),U.forEach(t),C=f(F),j=i(F,"CODE",{});var V=u(j);P=a(V,"eos_token_id"),V.forEach(t),S=a(F," entre les deux, puis d\u2019effectuer le chunking sur les s\xE9quences concat\xE9n\xE9es. Comme exercice, modifiez la fonction "),L=i(F,"CODE",{});var fe=u(L);g=a(fe,"tokenize()"),fe.forEach(t),T=a(F," pour utiliser cette approche. Notez que vous devrez mettre "),H=i(F,"CODE",{});var ee=u(H);B=a(ee,"truncation=False"),ee.forEach(t),W=a(F," et enlever les autres arguments du "),R=i(F,"EM",{});var te=u(R);Q=a(te,"tokenizer"),te.forEach(t),O=a(F," pour obtenir la s\xE9quence compl\xE8te des IDs des "),K=i(F,"EM",{});var pe=u(K);I=a(pe,"tokens"),pe.forEach(t),Z=a(F,"."),F.forEach(t)},m(X,F){p(X,l,F),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C),s(l,j),s(j,P),s(l,S),s(l,L),s(L,g),s(l,T),s(l,H),s(H,B),s(l,W),s(l,R),s(R,Q),s(l,O),s(l,K),s(K,I),s(l,Z)},d(X){X&&t(l)}}}function fp(A){let l,_,d,h,k,$,z,C,j,P,S,L;return l=new M({props:{code:`from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),j=new M({props:{code:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()`,highlighted:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  <span class="hljs-comment"># Builds the model</span>
model.summary()`}}),S=new M({props:{code:`_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________`,highlighted:`_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#   </span>
=================================================================
transformer (TFGPT2MainLayer multiple                  <span class="hljs-number">124242432</span> 
=================================================================
Total params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Trainable params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){y(l.$$.fragment),_=m(),d=r("p"),h=n("Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=r("code"),$=n("from_pretrained()"),z=n(", puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),C=m(),y(j.$$.fragment),P=m(),y(S.$$.fragment)},l(g){w(l.$$.fragment,g),_=f(g),d=i(g,"P",{});var T=u(d);h=a(T,"Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=i(T,"CODE",{});var H=u(k);$=a(H,"from_pretrained()"),H.forEach(t),z=a(T,", puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),T.forEach(t),C=f(g),w(j.$$.fragment,g),P=f(g),w(S.$$.fragment,g)},m(g,T){E(l,g,T),p(g,_,T),p(g,d,T),s(d,h),s(d,k),s(k,$),s(d,z),p(g,C,T),E(j,g,T),p(g,P,T),E(S,g,T),L=!0},i(g){L||(v(l.$$.fragment,g),v(j.$$.fragment,g),v(S.$$.fragment,g),L=!0)},o(g){b(l.$$.fragment,g),b(j.$$.fragment,g),b(S.$$.fragment,g),L=!1},d(g){x(l,g),g&&t(_),g&&t(d),g&&t(C),x(j,g),g&&t(P),x(S,g)}}}function _p(A){let l,_,d,h,k,$,z,C,j,P,S,L;return l=new M({props:{code:`from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),j=new M({props:{code:`model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")`,highlighted:`model = GPT2LMHeadModel(config)
model_size = <span class="hljs-built_in">sum</span>(t.numel() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> model.parameters())
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;GPT-2 size: <span class="hljs-subst">{model_size/<span class="hljs-number">1000</span>**<span class="hljs-number">2</span>:<span class="hljs-number">.1</span>f}</span>M parameters&quot;</span>)`}}),S=new M({props:{code:"GPT-2 size: 124.2M parameters",highlighted:'GPT-<span class="hljs-number">2</span> size: <span class="hljs-number">124.2</span>M parameters'}}),{c(){y(l.$$.fragment),_=m(),d=r("p"),h=n("Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=r("code"),$=n("from_pretrained()"),z=n(", puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),C=m(),y(j.$$.fragment),P=m(),y(S.$$.fragment)},l(g){w(l.$$.fragment,g),_=f(g),d=i(g,"P",{});var T=u(d);h=a(T,"Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=i(T,"CODE",{});var H=u(k);$=a(H,"from_pretrained()"),H.forEach(t),z=a(T,", puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),T.forEach(t),C=f(g),w(j.$$.fragment,g),P=f(g),w(S.$$.fragment,g)},m(g,T){E(l,g,T),p(g,_,T),p(g,d,T),s(d,h),s(d,k),s(k,$),s(d,z),p(g,C,T),E(j,g,T),p(g,P,T),E(S,g,T),L=!0},i(g){L||(v(l.$$.fragment,g),v(j.$$.fragment,g),v(S.$$.fragment,g),L=!0)},o(g){b(l.$$.fragment,g),b(j.$$.fragment,g),b(S.$$.fragment,g),L=!1},d(g){x(l,g),g&&t(_),g&&t(d),g&&t(C),x(j,g),g&&t(P),x(S,g)}}}function hp(A){let l,_;return l=new M({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function vp(A){let l,_;return l=new M({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>)`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function gp(A){let l,_;return l=new M({props:{code:`input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)`,highlighted:`input_ids shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
attention_mask shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
labels shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function bp(A){let l,_;return l=new M({props:{code:`input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])`,highlighted:`input_ids shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
attention_mask shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
labels shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function sp(A){let l,_,d,h,k,$,z,C;return z=new M({props:{code:`tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)
tf_eval_dataset = tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),{c(){l=r("p"),_=n("Maintenant nous pouvons utiliser la m\xE9thode "),d=r("code"),h=n("to_tf_dataset()"),k=n(" pour convertir nos jeux de donn\xE9es en jeux de donn\xE9es TensorFlow avec le collateur de donn\xE9es que nous avons cr\xE9\xE9 ci-dessus :"),$=m(),y(z.$$.fragment)},l(j){l=i(j,"P",{});var P=u(l);_=a(P,"Maintenant nous pouvons utiliser la m\xE9thode "),d=i(P,"CODE",{});var S=u(d);h=a(S,"to_tf_dataset()"),S.forEach(t),k=a(P," pour convertir nos jeux de donn\xE9es en jeux de donn\xE9es TensorFlow avec le collateur de donn\xE9es que nous avons cr\xE9\xE9 ci-dessus :"),P.forEach(t),$=f(j),w(z.$$.fragment,j)},m(j,P){p(j,l,P),s(l,_),s(l,d),s(d,h),s(l,k),p(j,$,P),E(z,j,P),C=!0},i(j){C||(v(z.$$.fragment,j),C=!0)},o(j){b(z.$$.fragment,j),C=!1},d(j){j&&t(l),j&&t($),x(z,j)}}}function $p(A){let l,_;return{c(){l=r("p"),_=n("\u26A0\uFE0F Le d\xE9placement des entr\xE9es et des \xE9tiquettes pour les aligner se fait \xE0 l\u2019int\xE9rieur du mod\xE8le, de sorte que le collecteur de donn\xE9es ne fait que copier les entr\xE9es pour cr\xE9er les \xE9tiquettes.")},l(d){l=i(d,"P",{});var h=u(l);_=a(h,"\u26A0\uFE0F Le d\xE9placement des entr\xE9es et des \xE9tiquettes pour les aligner se fait \xE0 l\u2019int\xE9rieur du mod\xE8le, de sorte que le collecteur de donn\xE9es ne fait que copier les entr\xE9es pour cr\xE9er les \xE9tiquettes."),h.forEach(t)},m(d,h){p(d,l,h),s(l,_)},d(d){d&&t(l)}}}function qp(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O,K,I,Z,X,F;return P=new M({props:{code:`from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),X=new M({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>, tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`}}),{c(){l=r("p"),_=n("Tout ce qu\u2019il reste \xE0 faire est de configurer les hyperparam\xE8tres d\u2019entra\xEEnement et d\u2019appeler "),d=r("code"),h=n("compile()"),k=n(" et "),$=r("code"),z=n("fit()"),C=n(". Nous utiliserons un programme de taux d\u2019apprentissage avec un certain \xE9chauffement pour am\xE9liorer la stabilit\xE9 de l\u2019entra\xEEnement :"),j=m(),y(P.$$.fragment),S=m(),L=r("p"),g=n("Maintenant, nous pouvons simplement appeler "),T=r("code"),H=n("model.fit()"),B=n(" et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019ensemble d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures, alors prenez quelques caf\xE9s et un bon livre \xE0 lire ! Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),W=r("em"),R=n("tokenizer"),Q=n(" vers le "),O=r("em"),K=n("Hub"),I=n(" :"),Z=m(),y(X.$$.fragment)},l(G){l=i(G,"P",{});var U=u(l);_=a(U,"Tout ce qu\u2019il reste \xE0 faire est de configurer les hyperparam\xE8tres d\u2019entra\xEEnement et d\u2019appeler "),d=i(U,"CODE",{});var V=u(d);h=a(V,"compile()"),V.forEach(t),k=a(U," et "),$=i(U,"CODE",{});var fe=u($);z=a(fe,"fit()"),fe.forEach(t),C=a(U,". Nous utiliserons un programme de taux d\u2019apprentissage avec un certain \xE9chauffement pour am\xE9liorer la stabilit\xE9 de l\u2019entra\xEEnement :"),U.forEach(t),j=f(G),w(P.$$.fragment,G),S=f(G),L=i(G,"P",{});var ee=u(L);g=a(ee,"Maintenant, nous pouvons simplement appeler "),T=i(ee,"CODE",{});var te=u(T);H=a(te,"model.fit()"),te.forEach(t),B=a(ee," et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019ensemble d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures, alors prenez quelques caf\xE9s et un bon livre \xE0 lire ! Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),W=i(ee,"EM",{});var pe=u(W);R=a(pe,"tokenizer"),pe.forEach(t),Q=a(ee," vers le "),O=i(ee,"EM",{});var N=u(O);K=a(N,"Hub"),N.forEach(t),I=a(ee," :"),ee.forEach(t),Z=f(G),w(X.$$.fragment,G)},m(G,U){p(G,l,U),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C),p(G,j,U),E(P,G,U),p(G,S,U),p(G,L,U),s(L,g),s(L,T),s(T,H),s(L,B),s(L,W),s(W,R),s(L,Q),s(L,O),s(O,K),s(L,I),p(G,Z,U),E(X,G,U),F=!0},i(G){F||(v(P.$$.fragment,G),v(X.$$.fragment,G),F=!0)},o(G){b(P.$$.fragment,G),b(X.$$.fragment,G),F=!1},d(G){G&&t(l),G&&t(j),x(P,G),G&&t(S),G&&t(L),G&&t(Z),x(X,G)}}}function kp(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O,K,I,Z,X,F,G,U,V,fe,ee,te,pe;return H=new M({props:{code:`from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>,
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
    eval_steps=<span class="hljs-number">5_000</span>,
    logging_steps=<span class="hljs-number">5_000</span>,
    gradient_accumulation_steps=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">1</span>,
    weight_decay=<span class="hljs-number">0.1</span>,
    warmup_steps=<span class="hljs-number">1_000</span>,
    lr_scheduler_type=<span class="hljs-string">&quot;cosine&quot;</span>,
    learning_rate=<span class="hljs-number">5e-4</span>,
    save_steps=<span class="hljs-number">5_000</span>,
    fp16=<span class="hljs-literal">True</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;valid&quot;</span>],
)`}}),Z=new M({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),te=new M({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){l=r("p"),_=n("Tout ce qu\u2019il reste \xE0 faire est de configurer les arguments d\u2019entra\xEEnement et de lancer le "),d=r("code"),h=n("Trainer"),k=n(". Nous utiliserons un programme de taux d\u2019apprentissage en cosinus avec un certain r\xE9chauffement et une taille de lot effective de 256 ("),$=r("code"),z=n("per_device_train_batch_size"),C=m(),j=r("em"),P=r("code"),S=n("gradient_accumulation_steps"),L=n("). L\u2019accumulation du gradient est utilis\xE9e lorsqu\u2019un seul lot ne tient pas en m\xE9moire, et construit le gradient de mani\xE8re incr\xE9mentale \xE0 travers plusieurs passages avant/arri\xE8re. Nous verrons cela en action lorsque nous cr\xE9erons la boucle d\u2019entra\xEEnement avec \u{1F917} "),g=n("Accelerate*."),T=m(),y(H.$$.fragment),B=m(),W=r("p"),R=n("Maintenant, nous pouvons simplement lancer le "),Q=r("code"),O=n("Trainer"),K=n(" et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019ensemble d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures, alors prenez quelques caf\xE9s et un bon livre \xE0 lire !"),I=m(),y(Z.$$.fragment),X=m(),F=r("p"),G=n("Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),U=r("em"),V=n("tokenizer"),fe=n(" vers le Hub :"),ee=m(),y(te.$$.fragment)},l(N){l=i(N,"P",{});var J=u(l);_=a(J,"Tout ce qu\u2019il reste \xE0 faire est de configurer les arguments d\u2019entra\xEEnement et de lancer le "),d=i(J,"CODE",{});var et=u(d);h=a(et,"Trainer"),et.forEach(t),k=a(J,". Nous utiliserons un programme de taux d\u2019apprentissage en cosinus avec un certain r\xE9chauffement et une taille de lot effective de 256 ("),$=i(J,"CODE",{});var Ze=u($);z=a(Ze,"per_device_train_batch_size"),Ze.forEach(t),C=f(J),j=i(J,"EM",{});var Oe=u(j);P=i(Oe,"CODE",{});var st=u(P);S=a(st,"gradient_accumulation_steps"),st.forEach(t),L=a(Oe,"). L\u2019accumulation du gradient est utilis\xE9e lorsqu\u2019un seul lot ne tient pas en m\xE9moire, et construit le gradient de mani\xE8re incr\xE9mentale \xE0 travers plusieurs passages avant/arri\xE8re. Nous verrons cela en action lorsque nous cr\xE9erons la boucle d\u2019entra\xEEnement avec \u{1F917} "),Oe.forEach(t),g=a(J,"Accelerate*."),J.forEach(t),T=f(N),w(H.$$.fragment,N),B=f(N),W=i(N,"P",{});var Ee=u(W);R=a(Ee,"Maintenant, nous pouvons simplement lancer le "),Q=i(Ee,"CODE",{});var Qe=u(Q);O=a(Qe,"Trainer"),Qe.forEach(t),K=a(Ee," et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019ensemble d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures, alors prenez quelques caf\xE9s et un bon livre \xE0 lire !"),Ee.forEach(t),I=f(N),w(Z.$$.fragment,N),X=f(N),F=i(N,"P",{});var es=u(F);G=a(es,"Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),U=i(es,"EM",{});var ss=u(U);V=a(ss,"tokenizer"),ss.forEach(t),fe=a(es," vers le Hub :"),es.forEach(t),ee=f(N),w(te.$$.fragment,N)},m(N,J){p(N,l,J),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C),s(l,j),s(j,P),s(P,S),s(j,L),s(l,g),p(N,T,J),E(H,N,J),p(N,B,J),p(N,W,J),s(W,R),s(W,Q),s(Q,O),s(W,K),p(N,I,J),E(Z,N,J),p(N,X,J),p(N,F,J),s(F,G),s(F,U),s(U,V),s(F,fe),p(N,ee,J),E(te,N,J),pe=!0},i(N){pe||(v(H.$$.fragment,N),v(Z.$$.fragment,N),v(te.$$.fragment,N),pe=!0)},o(N){b(H.$$.fragment,N),b(Z.$$.fragment,N),b(te.$$.fragment,N),pe=!1},d(N){N&&t(l),N&&t(T),x(H,N),N&&t(B),N&&t(W),N&&t(I),x(Z,N),N&&t(X),N&&t(F),N&&t(ee),x(te,N)}}}function jp(A){let l,_,d,h,k,$,z,C;return{c(){l=r("p"),_=n("\u270F\uFE0F "),d=r("strong"),h=n("Essayez"),k=n(" Il ne nous a fallu qu\u2019une trentaine de lignes de code en plus des "),$=r("code"),z=n("TrainingArguments"),C=n(" pour passer des textes bruts \xE0 l\u2019entra\xEEnement de GPT-2. Essayez-le avec votre propre jeu de donn\xE9es et voyez si vous pouvez obtenir de bons r\xE9sultats !")},l(j){l=i(j,"P",{});var P=u(l);_=a(P,"\u270F\uFE0F "),d=i(P,"STRONG",{});var S=u(d);h=a(S,"Essayez"),S.forEach(t),k=a(P," Il ne nous a fallu qu\u2019une trentaine de lignes de code en plus des "),$=i(P,"CODE",{});var L=u($);z=a(L,"TrainingArguments"),L.forEach(t),C=a(P," pour passer des textes bruts \xE0 l\u2019entra\xEEnement de GPT-2. Essayez-le avec votre propre jeu de donn\xE9es et voyez si vous pouvez obtenir de bons r\xE9sultats !"),P.forEach(t)},m(j,P){p(j,l,P),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C)},d(j){j&&t(l)}}}function yp(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O;return{c(){l=r("p"),_=n("\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPU, vous pouvez essayer d\u2019utiliser un contexte "),d=r("code"),h=n("MirroredStrategy"),k=n(" pour acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement. Vous devrez cr\xE9er un objet "),$=r("code"),z=n("tf.distribute.MirroredStrategy"),C=n(", et vous assurer que les commandes "),j=r("code"),P=n("to_tf_dataset"),S=n(" ainsi que la cr\xE9ation du mod\xE8le et l\u2019appel \xE0 "),L=r("code"),g=n("fit()"),T=n(" sont tous ex\xE9cut\xE9s dans son contexte "),H=r("code"),B=n("scope()"),W=n(". Vous pouvez consulter la documentation \xE0 ce sujet "),R=r("a"),Q=n("ici"),O=n("."),this.h()},l(K){l=i(K,"P",{});var I=u(l);_=a(I,"\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPU, vous pouvez essayer d\u2019utiliser un contexte "),d=i(I,"CODE",{});var Z=u(d);h=a(Z,"MirroredStrategy"),Z.forEach(t),k=a(I," pour acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement. Vous devrez cr\xE9er un objet "),$=i(I,"CODE",{});var X=u($);z=a(X,"tf.distribute.MirroredStrategy"),X.forEach(t),C=a(I,", et vous assurer que les commandes "),j=i(I,"CODE",{});var F=u(j);P=a(F,"to_tf_dataset"),F.forEach(t),S=a(I," ainsi que la cr\xE9ation du mod\xE8le et l\u2019appel \xE0 "),L=i(I,"CODE",{});var G=u(L);g=a(G,"fit()"),G.forEach(t),T=a(I," sont tous ex\xE9cut\xE9s dans son contexte "),H=i(I,"CODE",{});var U=u(H);B=a(U,"scope()"),U.forEach(t),W=a(I,". Vous pouvez consulter la documentation \xE0 ce sujet "),R=i(I,"A",{href:!0,rel:!0});var V=u(R);Q=a(V,"ici"),V.forEach(t),O=a(I,"."),I.forEach(t),this.h()},h(){D(R,"href","https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit"),D(R,"rel","nofollow")},m(K,I){p(K,l,I),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C),s(l,j),s(j,P),s(l,S),s(l,L),s(L,g),s(l,T),s(l,H),s(H,B),s(l,W),s(l,R),s(R,Q),s(l,O)},d(K){K&&t(l)}}}function wp(A){let l,_,d,h,k;return{c(){l=r("p"),_=n("\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, essayez d\u2019y ex\xE9cuter le code. Le "),d=r("code"),h=n("Trainer"),k=n(" g\xE8re automatiquement plusieurs machines, et cela peut acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement.")},l($){l=i($,"P",{});var z=u(l);_=a(z,"\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, essayez d\u2019y ex\xE9cuter le code. Le "),d=i(z,"CODE",{});var C=u(d);h=a(C,"Trainer"),C.forEach(t),k=a(z," g\xE8re automatiquement plusieurs machines, et cela peut acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement."),z.forEach(t)},m($,z){p($,l,z),s(l,_),s(l,d),s(d,h),s(l,k)},d($){$&&t(l)}}}function Ep(A){let l;function _(k,$){return k[0]==="pt"?wp:yp}let d=_(A),h=d(A);return{c(){h.c(),l=dr()},l(k){h.l(k),l=dr()},m(k,$){h.m(k,$),p(k,l,$)},p(k,$){d!==(d=_(k))&&(h.d(1),h=d(k),h&&(h.c(),h.m(l.parentNode,l)))},d(k){h.d(k),k&&t(l)}}}function xp(A){let l,_;return l=new M({props:{code:`from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

course_model = TFGPT2LMHeadModel.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
course_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=course_model, tokenizer=course_tokenizer, device=<span class="hljs-number">0</span>
)`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function zp(A){let l,_;return l=new M({props:{code:`import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>, device=device
)`}}),{c(){y(l.$$.fragment)},l(d){w(l.$$.fragment,d)},m(d,h){E(l,d,h),_=!0},i(d){_||(v(l.$$.fragment,d),_=!0)},o(d){b(l.$$.fragment,d),_=!1},d(d){x(l,d)}}}function Pp(A){let l,_,d,h,k,$,z,C;return{c(){l=r("p"),_=n("Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe de la pile de science des donn\xE9es Python (bien s\xFBr, nous devrions l\u2019\xE9valuer de mani\xE8re plus approfondie avant de d\xE9ployer le mod\xE8le dans le monde r\xE9el). Cependant, il est parfois n\xE9cessaire de personnaliser davantage l\u2019entra\xEEnement du mod\xE8le afin d\u2019obtenir les performances n\xE9cessaires pour un cas d\u2019utilisation donn\xE9. Par exemple, que se passe-t-il si l\u2019on souhaite mettre \xE0 jour dynamiquement la taille du lot ou si l\u2019on dispose d\u2019une boucle d\u2019entra\xEEnement conditionnelle qui ignore les mauvais exemples \xE0 la vol\xE9e ? Une option serait de sous-classer le "),d=r("code"),h=n("Trainer"),k=n(" et d\u2019ajouter les changements n\xE9cessaires, mais parfois il est plus simple d\u2019\xE9crire la boucle d\u2019entra\xEEnement \xE0 partir de z\xE9ro. C\u2019est l\xE0 qu\u2019intervient \u{1F917} "),$=r("em"),z=n("Accelerate"),C=n(".")},l(j){l=i(j,"P",{});var P=u(l);_=a(P,"Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe de la pile de science des donn\xE9es Python (bien s\xFBr, nous devrions l\u2019\xE9valuer de mani\xE8re plus approfondie avant de d\xE9ployer le mod\xE8le dans le monde r\xE9el). Cependant, il est parfois n\xE9cessaire de personnaliser davantage l\u2019entra\xEEnement du mod\xE8le afin d\u2019obtenir les performances n\xE9cessaires pour un cas d\u2019utilisation donn\xE9. Par exemple, que se passe-t-il si l\u2019on souhaite mettre \xE0 jour dynamiquement la taille du lot ou si l\u2019on dispose d\u2019une boucle d\u2019entra\xEEnement conditionnelle qui ignore les mauvais exemples \xE0 la vol\xE9e ? Une option serait de sous-classer le "),d=i(P,"CODE",{});var S=u(d);h=a(S,"Trainer"),S.forEach(t),k=a(P," et d\u2019ajouter les changements n\xE9cessaires, mais parfois il est plus simple d\u2019\xE9crire la boucle d\u2019entra\xEEnement \xE0 partir de z\xE9ro. C\u2019est l\xE0 qu\u2019intervient \u{1F917} "),$=i(P,"EM",{});var L=u($);z=a(L,"Accelerate"),L.forEach(t),C=a(P,"."),P.forEach(t)},m(j,P){p(j,l,P),s(l,_),s(l,d),s(d,h),s(l,k),s(l,$),s($,z),s(l,C)},d(j){j&&t(l)}}}function Cp(A){let l,_;return{c(){l=r("p"),_=n("Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe de la pile Python pour la science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant.")},l(d){l=i(d,"P",{});var h=u(l);_=a(h,"Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe de la pile Python pour la science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant."),h.forEach(t)},m(d,h){p(d,l,h),s(l,_)},d(d){d&&t(l)}}}function tp(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O,K,I,Z,X,F,G,U,V,fe,ee,te,pe,N,J,et,Ze,Oe,st,Ee,Qe,es,ss,Mt,na,un,Se,pn,le,At,tt,aa,Nt,Lt,oa,Ot,St,la,dn,Ge,cn,$s,nt,ae,ra,at,_e,so,ot,he,to,lt,xe,ia,qs,ua,pa,Fe,da,ca,rt,He,ts,it,be,mn,ns,ma,Gt,ne,Ft,ks,fa,Ht,It,_a,Rt,js,ut,ze,ha,pt,as,va,ys,ga,ba,Ut,$e,Vt,Ie,$a,Bt,qe,Wt,Pe,fn,ws,Es,Xt,de,_n,os,qa,hn,Ce,vn,xs,Kt,Te,gn,zs,ls,bn,Re,$n,dt,qn,rs,is,us,ps,Ue,ke,ct,Jt,kn,Ps,mt,Cs,jn,Ve,Ts,ft,ce,ka,_t,Be,ds,Ds,Ms,ja,As,ya,yn,We,wn,cs,En,Xe,xn,Ns,Yt,oe,wa,ht,Ls,zn,ve,Ea,Zt,vt,xa,Qt,en,za,Pn,Os,Ke,ms,gt,Cn,je,Pa,Je,bt,sn,Tn,Ss,tn,ye,nn,se,Ca,$t,Gs,Dn,Ye,Mn;return h=new Bo({}),R=new hi({props:{id:"Hm8_PgVTFuc"}}),Ge=new M({props:{code:`keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")`,highlighted:`keytoken_ids = []
<span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> [
    <span class="hljs-string">&quot;plt&quot;</span>,
    <span class="hljs-string">&quot;pd&quot;</span>,
    <span class="hljs-string">&quot;sk&quot;</span>,
    <span class="hljs-string">&quot;fit&quot;</span>,
    <span class="hljs-string">&quot;predict&quot;</span>,
    <span class="hljs-string">&quot; plt&quot;</span>,
    <span class="hljs-string">&quot; pd&quot;</span>,
    <span class="hljs-string">&quot; sk&quot;</span>,
    <span class="hljs-string">&quot; fit&quot;</span>,
    <span class="hljs-string">&quot; predict&quot;</span>,
    <span class="hljs-string">&quot;testtest&quot;</span>,
]:
    ids = tokenizer([keyword]).input_ids[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ids) == <span class="hljs-number">1</span>:
        keytoken_ids.append(ids[<span class="hljs-number">0</span>])
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keyword has not single token: <span class="hljs-subst">{keyword}</span>&quot;</span>)`}}),$s=new M({props:{code:"'Keyword has not single token: testtest'",highlighted:'<span class="hljs-string">&#x27;Keyword has not single token: testtest&#x27;</span>'}}),be=new M({props:{code:`from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss`,highlighted:`<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> CrossEntropyLoss
<span class="hljs-keyword">import</span> torch


<span class="hljs-keyword">def</span> <span class="hljs-title function_">keytoken_weighted_loss</span>(<span class="hljs-params">inputs, logits, keytoken_ids, alpha=<span class="hljs-number">1.0</span></span>):
    <span class="hljs-comment"># Shift so that tokens &lt; n predict n</span>
    shift_labels = inputs[..., <span class="hljs-number">1</span>:].contiguous()
    shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()
    <span class="hljs-comment"># Calculate per-token loss</span>
    loss_fct = CrossEntropyLoss(reduce=<span class="hljs-literal">False</span>)
    loss = loss_fct(shift_logits.view(-<span class="hljs-number">1</span>, shift_logits.size(-<span class="hljs-number">1</span>)), shift_labels.view(-<span class="hljs-number">1</span>))
    <span class="hljs-comment"># Resize and average loss per sample</span>
    loss_per_sample = loss.view(shift_logits.size(<span class="hljs-number">0</span>), shift_logits.size(<span class="hljs-number">1</span>)).mean(axis=<span class="hljs-number">1</span>)
    <span class="hljs-comment"># Calculate and scale weighting</span>
    weights = torch.stack([(inputs == kt).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">for</span> kt <span class="hljs-keyword">in</span> keytoken_ids]).<span class="hljs-built_in">sum</span>(
        axis=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]
    )
    weights = alpha * (<span class="hljs-number">1.0</span> + weights)
    <span class="hljs-comment"># Calculate weighted average</span>
    weighted_loss = (loss_per_sample * weights).mean()
    <span class="hljs-keyword">return</span> weighted_loss`}}),$e=new M({props:{code:`from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader

tokenized_dataset.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
train_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>], batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)
eval_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>], batch_size=<span class="hljs-number">32</span>)`}}),qe=new M({props:{code:`weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]`,highlighted:`weight_decay = <span class="hljs-number">0.1</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_grouped_params</span>(<span class="hljs-params">model, no_decay=[<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-string">&quot;LayerNorm.weight&quot;</span>]</span>):
    params_with_wd, params_without_wd = [], []
    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay):
            params_without_wd.append(p)
        <span class="hljs-keyword">else</span>:
            params_with_wd.append(p)
    <span class="hljs-keyword">return</span> [
        {<span class="hljs-string">&quot;params&quot;</span>: params_with_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: weight_decay},
        {<span class="hljs-string">&quot;params&quot;</span>: params_without_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: <span class="hljs-number">0.0</span>},
    ]`}}),Es=new M({props:{code:`def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], labels=batch[<span class="hljs-string">&quot;input_ids&quot;</span>])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    <span class="hljs-keyword">try</span>:
        perplexity = torch.exp(loss)
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)
    <span class="hljs-keyword">return</span> loss.item(), perplexity.item()`}}),Te=new M({props:{code:"model = GPT2LMHeadModel(config)",highlighted:"model = GPT2LMHeadModel(config)"}}),Re=new M({props:{code:`from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(get_grouped_params(model), lr=<span class="hljs-number">5e-4</span>)`}}),is=new M({props:{code:`from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator(fp16=<span class="hljs-literal">True</span>)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),ps=new ta({props:{$$slots:{default:[Tp]},$$scope:{ctx:A}}}),Ts=new M({props:{code:`num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)`,highlighted:`num_train_epochs = <span class="hljs-number">1</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name=<span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_training_steps=num_training_steps,
)`}}),Xe=new M({props:{code:`from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, get_full_repo_name

model_name = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),Ns=new M({props:{code:"'sgugger/codeparrot-ds-accelerate'",highlighted:'<span class="hljs-string">&#x27;sgugger/codeparrot-ds-accelerate&#x27;</span>'}}),Ls=new M({props:{code:`output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`output_dir = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),gt=new M({props:{code:"evaluate()",highlighted:"evaluate()"}}),je=new M({props:{code:"(10.934126853942871, 56057.14453125)",highlighted:'(<span class="hljs-number">10.934126853942871</span>, <span class="hljs-number">56057.14453125</span>)'}}),ye=new M({props:{code:`from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=len(train_dataloader)
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm

gradient_accumulation_steps = <span class="hljs-number">8</span>
eval_steps = <span class="hljs-number">5_000</span>

model.train()
completed_steps = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> tqdm(
        <span class="hljs-built_in">enumerate</span>(train_dataloader, start=<span class="hljs-number">1</span>), total=<span class="hljs-built_in">len</span>(train_dataloader)
    ):
        logits = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>]).logits
        loss = keytoken_weighted_loss(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], logits, keytoken_ids)
        <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            accelerator.<span class="hljs-built_in">print</span>(
                {
                    <span class="hljs-string">&quot;lr&quot;</span>: get_lr(),
                    <span class="hljs-string">&quot;samples&quot;</span>: step * samples_per_step,
                    <span class="hljs-string">&quot;steps&quot;</span>: completed_steps,
                    <span class="hljs-string">&quot;loss/train&quot;</span>: loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        <span class="hljs-keyword">if</span> step % gradient_accumulation_steps == <span class="hljs-number">0</span>:
            accelerator.clip_grad_norm_(model.parameters(), <span class="hljs-number">1.0</span>)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> (step % (eval_steps * gradient_accumulation_steps)) == <span class="hljs-number">0</span>:
            eval_loss, perplexity = evaluate()
            accelerator.<span class="hljs-built_in">print</span>({<span class="hljs-string">&quot;loss/eval&quot;</span>: eval_loss, <span class="hljs-string">&quot;perplexity&quot;</span>: perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            <span class="hljs-keyword">if</span> accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=<span class="hljs-string">f&quot;Training in progress step <span class="hljs-subst">{step}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
                )`}}),Gs=new ta({props:{$$slots:{default:[Dp]},$$scope:{ctx:A}}}),Ye=new ta({props:{$$slots:{default:[Mp]},$$scope:{ctx:A}}}),{c(){l=r("h2"),_=r("a"),d=r("span"),y(h.$$.fragment),k=m(),$=r("span"),z=n("Entra\xEEner avec \u{1F917} *Accelerate*"),C=m(),j=r("p"),P=n("Nous avons vu comment entra\xEEner un mod\xE8le avec le "),S=r("code"),L=n("Trainer"),g=n(", qui peut permettre une certaine personnalisation. Cependant, parfois nous voulons un contr\xF4le total sur la boucle d\u2019entra\xEEnement, ou nous voulons faire quelques changements exotiques. Dans ce cas, \u{1F917} "),T=r("em"),H=n("Accelerate"),B=n(" est un excellent choix, et dans cette section, nous allons suivre les \xE9tapes pour l\u2019utiliser pour entra\xEEner notre mod\xE8le. Pour rendre les choses plus int\xE9ressantes, nous allons \xE9galement ajouter une touche \xE0 la boucle d\u2019entra\xEEnement."),W=m(),y(R.$$.fragment),Q=m(),O=r("p"),K=n("Puisque nous sommes principalement int\xE9ress\xE9s par l\u2019autocompl\xE9tion sensible pour les biblioth\xE8ques de science des donn\xE9es, il est logique de donner plus de poids aux \xE9chantillons d\u2019entra\xEEnement qui utilisent davantage ces biblioth\xE8ques. Nous pouvons facilement identifier ces exemples gr\xE2ce \xE0 l\u2019utilisation de mots-cl\xE9s tels que "),I=r("code"),Z=n("plt"),X=n(", "),F=r("code"),G=n("pd"),U=n(", "),V=r("code"),fe=n("sk"),ee=n(", "),te=r("code"),pe=n("fit"),N=n(", et "),J=r("code"),et=n("predict"),Ze=n(", qui sont les noms d\u2019importation les plus fr\xE9quents pour "),Oe=r("code"),st=n("matplotlib.pyplot"),Ee=n(", "),Qe=r("code"),es=n("pandas"),ss=n(", et "),Mt=r("code"),na=n("sklearn"),un=n(" ainsi que le mod\xE8le fit/predict de ce dernier. Si chacun d\u2019entre eux est repr\xE9sent\xE9 par un seul token, nous pouvons facilement v\xE9rifier s\u2019ils apparaissent dans la s\xE9quence d\u2019entr\xE9e. Les "),Se=r("em"),pn=n("tokens"),le=n(" peuvent avoir un pr\xE9fixe d\u2019espacement, donc nous v\xE9rifierons aussi ces versions dans le vocabulaire du "),At=r("em"),tt=n("tokenizer"),aa=n(". Pour v\xE9rifier que cela fonctionne, nous ajouterons un "),Nt=r("em"),Lt=n("token"),oa=n(" de test qui devrait \xEAtre divis\xE9 en plusieurs "),Ot=r("em"),St=n("tokens"),la=n(" :"),dn=m(),y(Ge.$$.fragment),cn=m(),y($s.$$.fragment),nt=m(),ae=r("p"),ra=n("Super, \xE7a a l\u2019air de bien fonctionner ! Nous pouvons maintenant \xE9crire une fonction de perte personnalis\xE9e qui prend la s\xE9quence d\u2019entr\xE9e, les logits et les "),at=r("em"),_e=n("tokens"),so=n(" cl\xE9s que nous venons de s\xE9lectionner comme entr\xE9es. Tout d\u2019abord, nous devons aligner les logits et les entr\xE9es : la s\xE9quence d\u2019entr\xE9e d\xE9cal\xE9e d\u2019une unit\xE9 vers la droite forme les \xE9tiquettes, puisque le "),ot=r("em"),he=n("token"),to=n(" suivant est l\u2019\xE9tiquette du "),lt=r("em"),xe=n("token"),ia=n(" actuel. Nous pouvons y parvenir en commen\xE7ant les \xE9tiquettes \xE0 partir du deuxi\xE8me "),qs=r("em"),ua=n("token"),pa=n(" de la s\xE9quence d\u2019entr\xE9e, puisque le mod\xE8le ne fait pas de pr\xE9diction pour le premier "),Fe=r("em"),da=n("token"),ca=n(" de toute fa\xE7on. Ensuite, nous coupons le dernier logit, car nous n\u2019avons pas d\u2019\xE9tiquette pour le "),rt=r("em"),He=n("token"),ts=n(" qui suit la s\xE9quence d\u2019entr\xE9e compl\xE8te. Avec cela, nous pouvons calculer la perte par \xE9chantillon et compter les occurrences de tous les mots-cl\xE9s dans chaque \xE9chantillon. Enfin, nous calculons la moyenne pond\xE9r\xE9e sur tous les \xE9chantillons en utilisant les occurrences comme poids. Comme nous ne voulons pas rejeter tous les \xE9chantillons qui ne contiennent pas de mots-cl\xE9s, nous ajoutons 1 aux poids :"),it=m(),y(be.$$.fragment),mn=m(),ns=r("p"),ma=n("Avant de commencer \xE0 s\u2019entra\xEEner avec cette nouvelle fonction de perte g\xE9niale, nous devons pr\xE9parer quelques \xE9l\xE9ments :"),Gt=m(),ne=r("ul"),Ft=r("li"),ks=n("nous avons besoin de chargeurs de donn\xE9es pour charger les donn\xE9es par lots."),fa=m(),Ht=r("li"),It=n("nous devons d\xE9finir les param\xE8tres de d\xE9croissance du poids."),_a=m(),Rt=r("li"),js=n("de temps en temps, nous voulons \xE9valuer, il est donc logique d\u2019envelopper le code d\u2019\xE9valuation dans une fonction."),ut=m(),ze=r("p"),ha=n("Commen\xE7ons par les chargeurs de donn\xE9es. Nous avons seulement besoin de d\xE9finir le format du jeu de donn\xE9es \xE0 "),pt=r("code"),as=n('"torch"'),va=n(", et ensuite nous pouvons le passer \xE0 un PyTorch "),ys=r("code"),ga=n("DataLoader"),ba=n(" avec la taille de lot appropri\xE9e :"),Ut=m(),y($e.$$.fragment),Vt=m(),Ie=r("p"),$a=n("Ensuite, nous regroupons les param\xE8tres de fa\xE7on \xE0 ce que l\u2019optimiseur sache lesquels b\xE9n\xE9ficieront d\u2019une d\xE9croissance de poids suppl\xE9mentaire. Habituellement, tous les termes de biais et de poids LayerNorm en sont exempt\xE9s ; voici comment nous pouvons le faire :"),Bt=m(),y(qe.$$.fragment),Wt=m(),Pe=r("p"),fn=n("Puisque nous voulons \xE9valuer le mod\xE8le r\xE9guli\xE8rement sur l\u2019ensemble de validation pendant l\u2019entra\xEEnement, \xE9crivons une fonction pour cela aussi. Elle passe simplement par le dataloader d\u2019\xE9valuation et rassemble toutes les pertes \xE0 travers les processus :"),ws=m(),y(Es.$$.fragment),Xt=m(),de=r("p"),_n=n("Avec la fonction "),os=r("code"),qa=n("evaluate()"),hn=n(" nous pouvons rapporter la perte et la "),Ce=r("a"),vn=n("perplexit\xE9"),xs=n(" \xE0 intervalles r\xE9guliers. Ensuite, nous red\xE9finissons notre mod\xE8le pour nous assurer que nous nous entra\xEEnons \xE0 nouveau \xE0 partir de z\xE9ro :"),Kt=m(),y(Te.$$.fragment),gn=m(),zs=r("p"),ls=n("Nous pouvons ensuite d\xE9finir notre optimiseur, en utilisant la fonction pr\xE9c\xE9dente pour diviser les param\xE8tres de la d\xE9croissance du poids :"),bn=m(),y(Re.$$.fragment),$n=m(),dt=r("p"),qn=n("Pr\xE9parons maintenant le mod\xE8le, l\u2019optimiseur et les chargeurs de donn\xE9es pour pouvoir commencer l\u2019entra\xEEnement :"),rs=m(),y(is.$$.fragment),us=m(),y(ps.$$.fragment),Ue=m(),ke=r("p"),ct=n("Maintenant que nous avons envoy\xE9 notre "),Jt=r("code"),kn=n("train_dataloader"),Ps=n(" \xE0 "),mt=r("code"),Cs=n("accelerator.prepare()"),jn=n(", nous pouvons utiliser sa longueur pour calculer le nombre d\u2019\xE9tapes d\u2019entra\xEEnement. Rappelez-vous que nous devons toujours faire cela apr\xE8s avoir pr\xE9par\xE9 le dataloader, car cette m\xE9thode modifiera sa longueur. Nous utilisons un programme lin\xE9aire classique du taux d\u2019apprentissage \xE0 0 :"),Ve=m(),y(Ts.$$.fragment),ft=m(),ce=r("p"),ka=n("Enfin, pour pousser notre mod\xE8le vers le Hub, nous aurons besoin de cr\xE9er un objet "),_t=r("code"),Be=n("Repository"),ds=n(" dans un dossier de travail. Tout d\u2019abord, connectez-vous au "),Ds=r("em"),Ms=n("Hub"),ja=n(", si vous n\u2019\xEAtes pas d\xE9j\xE0 connect\xE9. Nous d\xE9terminerons le nom du d\xE9p\xF4t \xE0 partir de l\u2019ID du mod\xE8le que nous voulons donner \xE0 notre mod\xE8le (n\u2019h\xE9sitez pas \xE0 remplacer le "),As=r("code"),ya=n("repo_name"),yn=n(" par votre propre choix ; il doit juste contenir votre nom d\u2019utilisateur, ce que fait la fonction "),We=r("code"),wn=n("get_full_repo_name()"),cs=n(") :"),En=m(),y(Xe.$$.fragment),xn=m(),y(Ns.$$.fragment),Yt=m(),oe=r("p"),wa=n("Ensuite, nous pouvons cloner ce r\xE9f\xE9rentiel dans un dossier local. S\u2019il existe d\xE9j\xE0, ce dossier local doit \xEAtre un clone existant du r\xE9f\xE9rentiel avec lequel nous travaillons :"),ht=m(),y(Ls.$$.fragment),zn=m(),ve=r("p"),Ea=n("Nous pouvons maintenant t\xE9l\xE9charger tout ce que nous sauvegardons dans "),Zt=r("code"),vt=n("output_dir"),xa=n(" en appelant la m\xE9thode "),Qt=r("code"),en=n("repo.push_to_hub()"),za=n(". Cela nous aidera \xE0 t\xE9l\xE9charger les mod\xE8les interm\xE9diaires \xE0 la fin de chaque \xE9poque."),Pn=m(),Os=r("p"),Ke=n("Avant de nous entra\xEEner, ex\xE9cutons un test rapide pour voir si la fonction d\u2019\xE9valuation fonctionne correctement :"),ms=m(),y(gt.$$.fragment),Cn=m(),y(je.$$.fragment),Pa=m(),Je=r("p"),bt=n("Ce sont des valeurs tr\xE8s \xE9lev\xE9es pour la perte et la perplexit\xE9, mais ce n\u2019est pas surprenant puisque nous n\u2019avons pas encore entra\xEEn\xE9 le mod\xE8le. Avec cela, nous avons tout pr\xE9par\xE9 pour \xE9crire la partie principale du script d\u2019entra\xEEnement : la boucle d\u2019entra\xEEnement. Dans la boucle d\u2019entra\xEEnement, nous it\xE9rons sur le chargeur de donn\xE9es et transmettons les lots au mod\xE8le. Avec les logits, nous pouvons alors \xE9valuer notre fonction de perte personnalis\xE9e. Nous mettons \xE0 l\u2019\xE9chelle la perte par le nombre d\u2019\xE9tapes d\u2019accumulation du gradient afin de ne pas cr\xE9er de plus grandes pertes en agr\xE9geant plus d\u2019\xE9tapes. Avant de proc\xE9der \xE0 l\u2019optimisation, nous d\xE9coupons \xE9galement les gradients pour une meilleure convergence. Enfin, tous les quelques pas, nous \xE9valuons le mod\xE8le sur l\u2019ensemble d\u2019\xE9valuation avec notre nouvelle fonction "),sn=r("code"),Tn=n("evaluate()"),Ss=n(" :"),tn=m(),y(ye.$$.fragment),nn=m(),se=r("p"),Ca=n("Et voil\xE0, vous disposez maintenant de votre propre boucle d\u2019entra\xEEnement personnalis\xE9e pour les mod\xE8les de langage causal tels que GPT-2, que vous pouvez encore adapter \xE0 vos besoins."),$t=m(),y(Gs.$$.fragment),Dn=m(),y(Ye.$$.fragment),this.h()},l(o){l=i(o,"H2",{class:!0});var q=u(l);_=i(q,"A",{id:!0,class:!0,href:!0});var no=u(_);d=i(no,"SPAN",{});var ao=u(d);w(h.$$.fragment,ao),ao.forEach(t),no.forEach(t),k=f(q),$=i(q,"SPAN",{});var An=u($);z=a(An,"Entra\xEEner avec \u{1F917} *Accelerate*"),An.forEach(t),q.forEach(t),C=f(o),j=i(o,"P",{});var qt=u(j);P=a(qt,"Nous avons vu comment entra\xEEner un mod\xE8le avec le "),S=i(qt,"CODE",{});var oo=u(S);L=a(oo,"Trainer"),oo.forEach(t),g=a(qt,", qui peut permettre une certaine personnalisation. Cependant, parfois nous voulons un contr\xF4le total sur la boucle d\u2019entra\xEEnement, ou nous voulons faire quelques changements exotiques. Dans ce cas, \u{1F917} "),T=i(qt,"EM",{});var Nn=u(T);H=a(Nn,"Accelerate"),Nn.forEach(t),B=a(qt," est un excellent choix, et dans cette section, nous allons suivre les \xE9tapes pour l\u2019utiliser pour entra\xEEner notre mod\xE8le. Pour rendre les choses plus int\xE9ressantes, nous allons \xE9galement ajouter une touche \xE0 la boucle d\u2019entra\xEEnement."),qt.forEach(t),W=f(o),w(R.$$.fragment,o),Q=f(o),O=i(o,"P",{});var Y=u(O);K=a(Y,"Puisque nous sommes principalement int\xE9ress\xE9s par l\u2019autocompl\xE9tion sensible pour les biblioth\xE8ques de science des donn\xE9es, il est logique de donner plus de poids aux \xE9chantillons d\u2019entra\xEEnement qui utilisent davantage ces biblioth\xE8ques. Nous pouvons facilement identifier ces exemples gr\xE2ce \xE0 l\u2019utilisation de mots-cl\xE9s tels que "),I=i(Y,"CODE",{});var lo=u(I);Z=a(lo,"plt"),lo.forEach(t),X=a(Y,", "),F=i(Y,"CODE",{});var Ln=u(F);G=a(Ln,"pd"),Ln.forEach(t),U=a(Y,", "),V=i(Y,"CODE",{});var ro=u(V);fe=a(ro,"sk"),ro.forEach(t),ee=a(Y,", "),te=i(Y,"CODE",{});var io=u(te);pe=a(io,"fit"),io.forEach(t),N=a(Y,", et "),J=i(Y,"CODE",{});var Ta=u(J);et=a(Ta,"predict"),Ta.forEach(t),Ze=a(Y,", qui sont les noms d\u2019importation les plus fr\xE9quents pour "),Oe=i(Y,"CODE",{});var re=u(Oe);st=a(re,"matplotlib.pyplot"),re.forEach(t),Ee=a(Y,", "),Qe=i(Y,"CODE",{});var uo=u(Qe);es=a(uo,"pandas"),uo.forEach(t),ss=a(Y,", et "),Mt=i(Y,"CODE",{});var On=u(Mt);na=a(On,"sklearn"),On.forEach(t),un=a(Y," ainsi que le mod\xE8le fit/predict de ce dernier. Si chacun d\u2019entre eux est repr\xE9sent\xE9 par un seul token, nous pouvons facilement v\xE9rifier s\u2019ils apparaissent dans la s\xE9quence d\u2019entr\xE9e. Les "),Se=i(Y,"EM",{});var po=u(Se);pn=a(po,"tokens"),po.forEach(t),le=a(Y," peuvent avoir un pr\xE9fixe d\u2019espacement, donc nous v\xE9rifierons aussi ces versions dans le vocabulaire du "),At=i(Y,"EM",{});var co=u(At);tt=a(co,"tokenizer"),co.forEach(t),aa=a(Y,". Pour v\xE9rifier que cela fonctionne, nous ajouterons un "),Nt=i(Y,"EM",{});var Sn=u(Nt);Lt=a(Sn,"token"),Sn.forEach(t),oa=a(Y," de test qui devrait \xEAtre divis\xE9 en plusieurs "),Ot=i(Y,"EM",{});var mo=u(Ot);St=a(mo,"tokens"),mo.forEach(t),la=a(Y," :"),Y.forEach(t),dn=f(o),w(Ge.$$.fragment,o),cn=f(o),w($s.$$.fragment,o),nt=f(o),ae=i(o,"P",{});var me=u(ae);ra=a(me,"Super, \xE7a a l\u2019air de bien fonctionner ! Nous pouvons maintenant \xE9crire une fonction de perte personnalis\xE9e qui prend la s\xE9quence d\u2019entr\xE9e, les logits et les "),at=i(me,"EM",{});var an=u(at);_e=a(an,"tokens"),an.forEach(t),so=a(me," cl\xE9s que nous venons de s\xE9lectionner comme entr\xE9es. Tout d\u2019abord, nous devons aligner les logits et les entr\xE9es : la s\xE9quence d\u2019entr\xE9e d\xE9cal\xE9e d\u2019une unit\xE9 vers la droite forme les \xE9tiquettes, puisque le "),ot=i(me,"EM",{});var fo=u(ot);he=a(fo,"token"),fo.forEach(t),to=a(me," suivant est l\u2019\xE9tiquette du "),lt=i(me,"EM",{});var _o=u(lt);xe=a(_o,"token"),_o.forEach(t),ia=a(me," actuel. Nous pouvons y parvenir en commen\xE7ant les \xE9tiquettes \xE0 partir du deuxi\xE8me "),qs=i(me,"EM",{});var Gn=u(qs);ua=a(Gn,"token"),Gn.forEach(t),pa=a(me," de la s\xE9quence d\u2019entr\xE9e, puisque le mod\xE8le ne fait pas de pr\xE9diction pour le premier "),Fe=i(me,"EM",{});var ho=u(Fe);da=a(ho,"token"),ho.forEach(t),ca=a(me," de toute fa\xE7on. Ensuite, nous coupons le dernier logit, car nous n\u2019avons pas d\u2019\xE9tiquette pour le "),rt=i(me,"EM",{});var vo=u(rt);He=a(vo,"token"),vo.forEach(t),ts=a(me," qui suit la s\xE9quence d\u2019entr\xE9e compl\xE8te. Avec cela, nous pouvons calculer la perte par \xE9chantillon et compter les occurrences de tous les mots-cl\xE9s dans chaque \xE9chantillon. Enfin, nous calculons la moyenne pond\xE9r\xE9e sur tous les \xE9chantillons en utilisant les occurrences comme poids. Comme nous ne voulons pas rejeter tous les \xE9chantillons qui ne contiennent pas de mots-cl\xE9s, nous ajoutons 1 aux poids :"),me.forEach(t),it=f(o),w(be.$$.fragment,o),mn=f(o),ns=i(o,"P",{});var Fn=u(ns);ma=a(Fn,"Avant de commencer \xE0 s\u2019entra\xEEner avec cette nouvelle fonction de perte g\xE9niale, nous devons pr\xE9parer quelques \xE9l\xE9ments :"),Fn.forEach(t),Gt=f(o),ne=i(o,"UL",{});var kt=u(ne);Ft=i(kt,"LI",{});var go=u(Ft);ks=a(go,"nous avons besoin de chargeurs de donn\xE9es pour charger les donn\xE9es par lots."),go.forEach(t),fa=f(kt),Ht=i(kt,"LI",{});var Da=u(Ht);It=a(Da,"nous devons d\xE9finir les param\xE8tres de d\xE9croissance du poids."),Da.forEach(t),_a=f(kt),Rt=i(kt,"LI",{});var jt=u(Rt);js=a(jt,"de temps en temps, nous voulons \xE9valuer, il est donc logique d\u2019envelopper le code d\u2019\xE9valuation dans une fonction."),jt.forEach(t),kt.forEach(t),ut=f(o),ze=i(o,"P",{});var Fs=u(ze);ha=a(Fs,"Commen\xE7ons par les chargeurs de donn\xE9es. Nous avons seulement besoin de d\xE9finir le format du jeu de donn\xE9es \xE0 "),pt=i(Fs,"CODE",{});var yt=u(pt);as=a(yt,'"torch"'),yt.forEach(t),va=a(Fs,", et ensuite nous pouvons le passer \xE0 un PyTorch "),ys=i(Fs,"CODE",{});var Ma=u(ys);ga=a(Ma,"DataLoader"),Ma.forEach(t),ba=a(Fs," avec la taille de lot appropri\xE9e :"),Fs.forEach(t),Ut=f(o),w($e.$$.fragment,o),Vt=f(o),Ie=i(o,"P",{});var we=u(Ie);$a=a(we,"Ensuite, nous regroupons les param\xE8tres de fa\xE7on \xE0 ce que l\u2019optimiseur sache lesquels b\xE9n\xE9ficieront d\u2019une d\xE9croissance de poids suppl\xE9mentaire. Habituellement, tous les termes de biais et de poids LayerNorm en sont exempt\xE9s ; voici comment nous pouvons le faire :"),we.forEach(t),Bt=f(o),w(qe.$$.fragment,o),Wt=f(o),Pe=i(o,"P",{});var bo=u(Pe);fn=a(bo,"Puisque nous voulons \xE9valuer le mod\xE8le r\xE9guli\xE8rement sur l\u2019ensemble de validation pendant l\u2019entra\xEEnement, \xE9crivons une fonction pour cela aussi. Elle passe simplement par le dataloader d\u2019\xE9valuation et rassemble toutes les pertes \xE0 travers les processus :"),bo.forEach(t),ws=f(o),w(Es.$$.fragment,o),Xt=f(o),de=i(o,"P",{});var fs=u(de);_n=a(fs,"Avec la fonction "),os=i(fs,"CODE",{});var $o=u(os);qa=a($o,"evaluate()"),$o.forEach(t),hn=a(fs," nous pouvons rapporter la perte et la "),Ce=i(fs,"A",{href:!0});var qo=u(Ce);vn=a(qo,"perplexit\xE9"),qo.forEach(t),xs=a(fs," \xE0 intervalles r\xE9guliers. Ensuite, nous red\xE9finissons notre mod\xE8le pour nous assurer que nous nous entra\xEEnons \xE0 nouveau \xE0 partir de z\xE9ro :"),fs.forEach(t),Kt=f(o),w(Te.$$.fragment,o),gn=f(o),zs=i(o,"P",{});var Hn=u(zs);ls=a(Hn,"Nous pouvons ensuite d\xE9finir notre optimiseur, en utilisant la fonction pr\xE9c\xE9dente pour diviser les param\xE8tres de la d\xE9croissance du poids :"),Hn.forEach(t),bn=f(o),w(Re.$$.fragment,o),$n=f(o),dt=i(o,"P",{});var ko=u(dt);qn=a(ko,"Pr\xE9parons maintenant le mod\xE8le, l\u2019optimiseur et les chargeurs de donn\xE9es pour pouvoir commencer l\u2019entra\xEEnement :"),ko.forEach(t),rs=f(o),w(is.$$.fragment,o),us=f(o),w(ps.$$.fragment,o),Ue=f(o),ke=i(o,"P",{});var wt=u(ke);ct=a(wt,"Maintenant que nous avons envoy\xE9 notre "),Jt=i(wt,"CODE",{});var In=u(Jt);kn=a(In,"train_dataloader"),In.forEach(t),Ps=a(wt," \xE0 "),mt=i(wt,"CODE",{});var jo=u(mt);Cs=a(jo,"accelerator.prepare()"),jo.forEach(t),jn=a(wt,", nous pouvons utiliser sa longueur pour calculer le nombre d\u2019\xE9tapes d\u2019entra\xEEnement. Rappelez-vous que nous devons toujours faire cela apr\xE8s avoir pr\xE9par\xE9 le dataloader, car cette m\xE9thode modifiera sa longueur. Nous utilisons un programme lin\xE9aire classique du taux d\u2019apprentissage \xE0 0 :"),wt.forEach(t),Ve=f(o),w(Ts.$$.fragment,o),ft=f(o),ce=i(o,"P",{});var De=u(ce);ka=a(De,"Enfin, pour pousser notre mod\xE8le vers le Hub, nous aurons besoin de cr\xE9er un objet "),_t=i(De,"CODE",{});var Aa=u(_t);Be=a(Aa,"Repository"),Aa.forEach(t),ds=a(De," dans un dossier de travail. Tout d\u2019abord, connectez-vous au "),Ds=i(De,"EM",{});var on=u(Ds);Ms=a(on,"Hub"),on.forEach(t),ja=a(De,", si vous n\u2019\xEAtes pas d\xE9j\xE0 connect\xE9. Nous d\xE9terminerons le nom du d\xE9p\xF4t \xE0 partir de l\u2019ID du mod\xE8le que nous voulons donner \xE0 notre mod\xE8le (n\u2019h\xE9sitez pas \xE0 remplacer le "),As=i(De,"CODE",{});var yo=u(As);ya=a(yo,"repo_name"),yo.forEach(t),yn=a(De," par votre propre choix ; il doit juste contenir votre nom d\u2019utilisateur, ce que fait la fonction "),We=i(De,"CODE",{});var Na=u(We);wn=a(Na,"get_full_repo_name()"),Na.forEach(t),cs=a(De,") :"),De.forEach(t),En=f(o),w(Xe.$$.fragment,o),xn=f(o),w(Ns.$$.fragment,o),Yt=f(o),oe=i(o,"P",{});var Hs=u(oe);wa=a(Hs,"Ensuite, nous pouvons cloner ce r\xE9f\xE9rentiel dans un dossier local. S\u2019il existe d\xE9j\xE0, ce dossier local doit \xEAtre un clone existant du r\xE9f\xE9rentiel avec lequel nous travaillons :"),Hs.forEach(t),ht=f(o),w(Ls.$$.fragment,o),zn=f(o),ve=i(o,"P",{});var Is=u(ve);Ea=a(Is,"Nous pouvons maintenant t\xE9l\xE9charger tout ce que nous sauvegardons dans "),Zt=i(Is,"CODE",{});var _s=u(Zt);vt=a(_s,"output_dir"),_s.forEach(t),xa=a(Is," en appelant la m\xE9thode "),Qt=i(Is,"CODE",{});var Rs=u(Qt);en=a(Rs,"repo.push_to_hub()"),Rs.forEach(t),za=a(Is,". Cela nous aidera \xE0 t\xE9l\xE9charger les mod\xE8les interm\xE9diaires \xE0 la fin de chaque \xE9poque."),Is.forEach(t),Pn=f(o),Os=i(o,"P",{});var Rn=u(Os);Ke=a(Rn,"Avant de nous entra\xEEner, ex\xE9cutons un test rapide pour voir si la fonction d\u2019\xE9valuation fonctionne correctement :"),Rn.forEach(t),ms=f(o),w(gt.$$.fragment,o),Cn=f(o),w(je.$$.fragment,o),Pa=f(o),Je=i(o,"P",{});var hs=u(Je);bt=a(hs,"Ce sont des valeurs tr\xE8s \xE9lev\xE9es pour la perte et la perplexit\xE9, mais ce n\u2019est pas surprenant puisque nous n\u2019avons pas encore entra\xEEn\xE9 le mod\xE8le. Avec cela, nous avons tout pr\xE9par\xE9 pour \xE9crire la partie principale du script d\u2019entra\xEEnement : la boucle d\u2019entra\xEEnement. Dans la boucle d\u2019entra\xEEnement, nous it\xE9rons sur le chargeur de donn\xE9es et transmettons les lots au mod\xE8le. Avec les logits, nous pouvons alors \xE9valuer notre fonction de perte personnalis\xE9e. Nous mettons \xE0 l\u2019\xE9chelle la perte par le nombre d\u2019\xE9tapes d\u2019accumulation du gradient afin de ne pas cr\xE9er de plus grandes pertes en agr\xE9geant plus d\u2019\xE9tapes. Avant de proc\xE9der \xE0 l\u2019optimisation, nous d\xE9coupons \xE9galement les gradients pour une meilleure convergence. Enfin, tous les quelques pas, nous \xE9valuons le mod\xE8le sur l\u2019ensemble d\u2019\xE9valuation avec notre nouvelle fonction "),sn=i(hs,"CODE",{});var wo=u(sn);Tn=a(wo,"evaluate()"),wo.forEach(t),Ss=a(hs," :"),hs.forEach(t),tn=f(o),w(ye.$$.fragment,o),nn=f(o),se=i(o,"P",{});var Un=u(se);Ca=a(Un,"Et voil\xE0, vous disposez maintenant de votre propre boucle d\u2019entra\xEEnement personnalis\xE9e pour les mod\xE8les de langage causal tels que GPT-2, que vous pouvez encore adapter \xE0 vos besoins."),Un.forEach(t),$t=f(o),w(Gs.$$.fragment,o),Dn=f(o),w(Ye.$$.fragment,o),this.h()},h(){D(_,"id","entraner-avec-accelerate"),D(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(_,"href","#entraner-avec-accelerate"),D(l,"class","relative group"),D(Ce,"href","/course/fr/chapter7/3")},m(o,q){p(o,l,q),s(l,_),s(_,d),E(h,d,null),s(l,k),s(l,$),s($,z),p(o,C,q),p(o,j,q),s(j,P),s(j,S),s(S,L),s(j,g),s(j,T),s(T,H),s(j,B),p(o,W,q),E(R,o,q),p(o,Q,q),p(o,O,q),s(O,K),s(O,I),s(I,Z),s(O,X),s(O,F),s(F,G),s(O,U),s(O,V),s(V,fe),s(O,ee),s(O,te),s(te,pe),s(O,N),s(O,J),s(J,et),s(O,Ze),s(O,Oe),s(Oe,st),s(O,Ee),s(O,Qe),s(Qe,es),s(O,ss),s(O,Mt),s(Mt,na),s(O,un),s(O,Se),s(Se,pn),s(O,le),s(O,At),s(At,tt),s(O,aa),s(O,Nt),s(Nt,Lt),s(O,oa),s(O,Ot),s(Ot,St),s(O,la),p(o,dn,q),E(Ge,o,q),p(o,cn,q),E($s,o,q),p(o,nt,q),p(o,ae,q),s(ae,ra),s(ae,at),s(at,_e),s(ae,so),s(ae,ot),s(ot,he),s(ae,to),s(ae,lt),s(lt,xe),s(ae,ia),s(ae,qs),s(qs,ua),s(ae,pa),s(ae,Fe),s(Fe,da),s(ae,ca),s(ae,rt),s(rt,He),s(ae,ts),p(o,it,q),E(be,o,q),p(o,mn,q),p(o,ns,q),s(ns,ma),p(o,Gt,q),p(o,ne,q),s(ne,Ft),s(Ft,ks),s(ne,fa),s(ne,Ht),s(Ht,It),s(ne,_a),s(ne,Rt),s(Rt,js),p(o,ut,q),p(o,ze,q),s(ze,ha),s(ze,pt),s(pt,as),s(ze,va),s(ze,ys),s(ys,ga),s(ze,ba),p(o,Ut,q),E($e,o,q),p(o,Vt,q),p(o,Ie,q),s(Ie,$a),p(o,Bt,q),E(qe,o,q),p(o,Wt,q),p(o,Pe,q),s(Pe,fn),p(o,ws,q),E(Es,o,q),p(o,Xt,q),p(o,de,q),s(de,_n),s(de,os),s(os,qa),s(de,hn),s(de,Ce),s(Ce,vn),s(de,xs),p(o,Kt,q),E(Te,o,q),p(o,gn,q),p(o,zs,q),s(zs,ls),p(o,bn,q),E(Re,o,q),p(o,$n,q),p(o,dt,q),s(dt,qn),p(o,rs,q),E(is,o,q),p(o,us,q),E(ps,o,q),p(o,Ue,q),p(o,ke,q),s(ke,ct),s(ke,Jt),s(Jt,kn),s(ke,Ps),s(ke,mt),s(mt,Cs),s(ke,jn),p(o,Ve,q),E(Ts,o,q),p(o,ft,q),p(o,ce,q),s(ce,ka),s(ce,_t),s(_t,Be),s(ce,ds),s(ce,Ds),s(Ds,Ms),s(ce,ja),s(ce,As),s(As,ya),s(ce,yn),s(ce,We),s(We,wn),s(ce,cs),p(o,En,q),E(Xe,o,q),p(o,xn,q),E(Ns,o,q),p(o,Yt,q),p(o,oe,q),s(oe,wa),p(o,ht,q),E(Ls,o,q),p(o,zn,q),p(o,ve,q),s(ve,Ea),s(ve,Zt),s(Zt,vt),s(ve,xa),s(ve,Qt),s(Qt,en),s(ve,za),p(o,Pn,q),p(o,Os,q),s(Os,Ke),p(o,ms,q),E(gt,o,q),p(o,Cn,q),E(je,o,q),p(o,Pa,q),p(o,Je,q),s(Je,bt),s(Je,sn),s(sn,Tn),s(Je,Ss),p(o,tn,q),E(ye,o,q),p(o,nn,q),p(o,se,q),s(se,Ca),p(o,$t,q),E(Gs,o,q),p(o,Dn,q),E(Ye,o,q),Mn=!0},i(o){Mn||(v(h.$$.fragment,o),v(R.$$.fragment,o),v(Ge.$$.fragment,o),v($s.$$.fragment,o),v(be.$$.fragment,o),v($e.$$.fragment,o),v(qe.$$.fragment,o),v(Es.$$.fragment,o),v(Te.$$.fragment,o),v(Re.$$.fragment,o),v(is.$$.fragment,o),v(ps.$$.fragment,o),v(Ts.$$.fragment,o),v(Xe.$$.fragment,o),v(Ns.$$.fragment,o),v(Ls.$$.fragment,o),v(gt.$$.fragment,o),v(je.$$.fragment,o),v(ye.$$.fragment,o),v(Gs.$$.fragment,o),v(Ye.$$.fragment,o),Mn=!0)},o(o){b(h.$$.fragment,o),b(R.$$.fragment,o),b(Ge.$$.fragment,o),b($s.$$.fragment,o),b(be.$$.fragment,o),b($e.$$.fragment,o),b(qe.$$.fragment,o),b(Es.$$.fragment,o),b(Te.$$.fragment,o),b(Re.$$.fragment,o),b(is.$$.fragment,o),b(ps.$$.fragment,o),b(Ts.$$.fragment,o),b(Xe.$$.fragment,o),b(Ns.$$.fragment,o),b(Ls.$$.fragment,o),b(gt.$$.fragment,o),b(je.$$.fragment,o),b(ye.$$.fragment,o),b(Gs.$$.fragment,o),b(Ye.$$.fragment,o),Mn=!1},d(o){o&&t(l),x(h),o&&t(C),o&&t(j),o&&t(W),x(R,o),o&&t(Q),o&&t(O),o&&t(dn),x(Ge,o),o&&t(cn),x($s,o),o&&t(nt),o&&t(ae),o&&t(it),x(be,o),o&&t(mn),o&&t(ns),o&&t(Gt),o&&t(ne),o&&t(ut),o&&t(ze),o&&t(Ut),x($e,o),o&&t(Vt),o&&t(Ie),o&&t(Bt),x(qe,o),o&&t(Wt),o&&t(Pe),o&&t(ws),x(Es,o),o&&t(Xt),o&&t(de),o&&t(Kt),x(Te,o),o&&t(gn),o&&t(zs),o&&t(bn),x(Re,o),o&&t($n),o&&t(dt),o&&t(rs),x(is,o),o&&t(us),x(ps,o),o&&t(Ue),o&&t(ke),o&&t(Ve),x(Ts,o),o&&t(ft),o&&t(ce),o&&t(En),x(Xe,o),o&&t(xn),x(Ns,o),o&&t(Yt),o&&t(oe),o&&t(ht),x(Ls,o),o&&t(zn),o&&t(ve),o&&t(Pn),o&&t(Os),o&&t(ms),x(gt,o),o&&t(Cn),x(je,o),o&&t(Pa),o&&t(Je),o&&t(tn),x(ye,o),o&&t(nn),o&&t(se),o&&t($t),x(Gs,o),o&&t(Dn),x(Ye,o)}}}function Tp(A){let l,_,d,h,k;return{c(){l=r("p"),_=n("\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code commen\xE7ant \xE0 la cellule ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),d=r("a"),h=n("Chapitre 3"),k=n(" pour plus de d\xE9tails."),this.h()},l($){l=i($,"P",{});var z=u(l);_=a(z,"\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code commen\xE7ant \xE0 la cellule ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),d=i(z,"A",{href:!0});var C=u(d);h=a(C,"Chapitre 3"),C.forEach(t),k=a(z," pour plus de d\xE9tails."),z.forEach(t),this.h()},h(){D(d,"href","/course/fr/chapter3")},m($,z){p($,l,z),s(l,_),s(l,d),s(d,h),s(l,k)},d($){$&&t(l)}}}function Dp(A){let l,_,d,h,k;return{c(){l=r("p"),_=n("\u270F\uFE0F "),d=r("strong"),h=n("Essayez"),k=n(" Vous pouvez cr\xE9er votre propre fonction de perte personnalis\xE9e, adapt\xE9e \xE0 votre cas d\u2019utilisation, ou ajouter une autre \xE9tape personnalis\xE9e dans la boucle d\u2019entra\xEEnement.")},l($){l=i($,"P",{});var z=u(l);_=a(z,"\u270F\uFE0F "),d=i(z,"STRONG",{});var C=u(d);h=a(C,"Essayez"),C.forEach(t),k=a(z," Vous pouvez cr\xE9er votre propre fonction de perte personnalis\xE9e, adapt\xE9e \xE0 votre cas d\u2019utilisation, ou ajouter une autre \xE9tape personnalis\xE9e dans la boucle d\u2019entra\xEEnement."),z.forEach(t)},m($,z){p($,l,z),s(l,_),s(l,d),s(d,h),s(l,k)},d($){$&&t(l)}}}function Mp(A){let l,_,d,h,k;return{c(){l=r("p"),_=n("\u270F\uFE0F "),d=r("strong"),h=n("Essayez"),k=n(" Lorsque vous effectuez de longues exp\xE9riences d\u2019entra\xEEnement, il est bon d\u2019enregistrer les mesures importantes \xE0 l\u2019aide d\u2019outils tels que TensorBoard ou Weights & Biases. Ajoutez une journalisation appropri\xE9e \xE0 la boucle d\u2019entra\xEEnement afin de pouvoir toujours v\xE9rifier comment se d\xE9roule l\u2019entra\xEEnement.")},l($){l=i($,"P",{});var z=u(l);_=a(z,"\u270F\uFE0F "),d=i(z,"STRONG",{});var C=u(d);h=a(C,"Essayez"),C.forEach(t),k=a(z," Lorsque vous effectuez de longues exp\xE9riences d\u2019entra\xEEnement, il est bon d\u2019enregistrer les mesures importantes \xE0 l\u2019aide d\u2019outils tels que TensorBoard ou Weights & Biases. Ajoutez une journalisation appropri\xE9e \xE0 la boucle d\u2019entra\xEEnement afin de pouvoir toujours v\xE9rifier comment se d\xE9roule l\u2019entra\xEEnement."),z.forEach(t)},m($,z){p($,l,z),s(l,_),s(l,d),s(d,h),s(l,k)},d($){$&&t(l)}}}function Ap(A){let l,_,d,h,k,$,z,C,j,P,S,L,g,T,H,B,W,R,Q,O,K,I,Z,X,F,G,U,V,fe,ee,te,pe,N,J,et,Ze,Oe,st,Ee,Qe,es,ss,Mt,na,un,Se,pn,le,At,tt,aa,Nt,Lt,oa,Ot,St,la,dn,Ge,cn,$s,nt,ae,ra,at,_e,so,ot,he,to,lt,xe,ia,qs,ua,pa,Fe,da,ca,rt,He,ts,it,be,mn,ns,ma,Gt,ne,Ft,ks,fa,Ht,It,_a,Rt,js,ut,ze,ha,pt,as,va,ys,ga,ba,Ut,$e,Vt,Ie,$a,Bt,qe,Wt,Pe,fn,ws,Es,Xt,de,_n,os,qa,hn,Ce,vn,xs,Kt,Te,gn,zs,ls,bn,Re,$n,dt,qn,rs,is,us,ps,Ue,ke,ct,Jt,kn,Ps,mt,Cs,jn,Ve,Ts,ft,ce,ka,_t,Be,ds,Ds,Ms,ja,As,ya,yn,We,wn,cs,En,Xe,xn,Ns,Yt,oe,wa,ht,Ls,zn,ve,Ea,Zt,vt,xa,Qt,en,za,Pn,Os,Ke,ms,gt,Cn,je,Pa,Je,bt,sn,Tn,Ss,tn,ye,nn,se,Ca,$t,Gs,Dn,Ye,Mn,o,q,no,ao,An,qt,oo,Nn,Y,lo,Ln,ro,io,Ta,re,uo,On,po,co,Sn,mo,me,an,fo,_o,Gn,ho,vo,Fn,kt,go,Da,jt,Fs,yt,Ma,we,bo,fs,$o,qo,Hn,ko,wt,In,jo,De,Aa,on,yo,Na,Hs,Is,_s,Rs,Rn,hs,wo,Un,cr,ql,Me,mr,Wo,fr,_r,Xo,hr,vr,Ko,gr,br,Jo,$r,qr,kl,Us,Vs,Eo,Et,kr,Yo,jr,yr,Zo,wr,Er,jl,Ae,xr,Qo,zr,Pr,el,Cr,Tr,sl,Dr,Mr,tl,Ar,Nr,yl,Bs,Ws,xo,zo,Lr,wl,La,El,Xs,Ks,Po,Co,Or,xl,To,Vn,zl,Bn,Sr,nl,Gr,Fr,Pl,Oa,Cl,Do,Hr,Tl,Wn,Ir,al,Rr,Ur,Dl,Sa,Ml,Js,Ys,Mo,Xn,Al,Kn,Nl,ln,Jn,ol,Ga,Vr,ll,Br,Ll,Yn,Wr,rl,Xr,Kr,Ol,Zs,Qs,Ao,No,Jr,Sl,Fa,Gl,Ha,Fl,xt,Yr,il,Zr,Qr,ul,ei,si,Hl,Ia,Il,Ra,Rl,Ne,ti,pl,ni,ai,dl,oi,li,cl,ri,ii,ml,ui,pi,Ul,Ua,Vl,Va,Bl,zt,di,fl,ci,mi,_l,fi,_i,Wl,Ba,Xl,Wa,Kl,Lo,Oo,Jl;d=new up({props:{fw:A[0]}}),C=new Bo({});const vi=[dp,pp],Xa=[];function gi(e,c){return e[0]==="pt"?0:1}g=gi(A),T=Xa[g]=vi[g](A),Se=new hi({props:{id:"Vpjb1lu0MDk"}}),be=new Bo({}),$e=new M({props:{code:`def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">any_keyword_in_string</span>(<span class="hljs-params">string, keywords</span>):
    <span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> keywords:
        <span class="hljs-keyword">if</span> keyword <span class="hljs-keyword">in</span> string:
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>`}}),qe=new M({props:{code:`filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`,highlighted:`filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]
example_1 = <span class="hljs-string">&quot;import numpy as np&quot;</span>
example_2 = <span class="hljs-string">&quot;import pandas as pd&quot;</span>

<span class="hljs-built_in">print</span>(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`}}),Pe=new M({props:{code:"False True",highlighted:'<span class="hljs-literal">False</span> <span class="hljs-literal">True</span>'}}),de=new M({props:{code:`def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_streaming_dataset</span>(<span class="hljs-params">dataset, filters</span>):
    filtered_dict = defaultdict(<span class="hljs-built_in">list</span>)
    total = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">iter</span>(dataset)):
        total += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> any_keyword_in_string(sample[<span class="hljs-string">&quot;content&quot;</span>], filters):
            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sample.items():
                filtered_dict[k].append(v)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">len</span>(filtered_dict[<span class="hljs-string">&#x27;content&#x27;</span>])/total:<span class="hljs-number">.2</span>%}</span> of data after filtering.&quot;</span>)
    <span class="hljs-keyword">return</span> Dataset.from_dict(filtered_dict)`}}),Ce=new M({props:{code:`# Cette cellule prendra beaucoup de temps \xE0 s'ex\xE9cuter, donc vous devriez la sauter et aller \xE0 la suivante !
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)`,highlighted:`<span class="hljs-comment"># Cette cellule prendra beaucoup de temps \xE0 s&#x27;ex\xE9cuter, donc vous devriez la sauter et aller \xE0 la suivante !</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

split = <span class="hljs-string">&quot;train&quot;</span>  <span class="hljs-comment"># &quot;valid&quot;</span>
filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]

data = load_dataset(<span class="hljs-string">f&quot;transformersbook/codeparrot-<span class="hljs-subst">{split}</span>&quot;</span>, split=split, streaming=<span class="hljs-literal">True</span>)
filtered_data = filter_streaming_dataset(data, filters)`}}),xs=new M({props:{code:"3.26% of data after filtering.",highlighted:'<span class="hljs-number">3.26</span>% of data after filtering.'}}),rs=new M({props:{code:`from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="train")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DatasetDict

ds_train = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-train&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
ds_valid = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-valid&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)

raw_datasets = DatasetDict(
    {
        <span class="hljs-string">&quot;train&quot;</span>: ds_train,  <span class="hljs-comment"># .shuffle().select(range(50000)),</span>
        <span class="hljs-string">&quot;valid&quot;</span>: ds_valid,  <span class="hljs-comment"># .shuffle().select(range(500))</span>
    }
)

raw_datasets`}}),us=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">606720</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">3322</span>
    })
})`}}),Ue=new ta({props:{$$slots:{default:[cp]},$$scope:{ctx:A}}}),Ps=new M({props:{code:`for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")`,highlighted:`<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key.upper()}</span>: <span class="hljs-subst">{raw_datasets[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>][key][:<span class="hljs-number">200</span>]}</span>&quot;</span>)`}}),Cs=new M({props:{code:`'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:\`sklearn.utils\` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''`,highlighted:`<span class="hljs-string">&#x27;REPO_NAME: kmike/scikit-learn&#x27;</span>
<span class="hljs-string">&#x27;PATH: sklearn/utils/__init__.py&#x27;</span>
<span class="hljs-string">&#x27;COPIES: 3&#x27;</span>
<span class="hljs-string">&#x27;SIZE: 10094&#x27;</span>
<span class="hljs-string">&#x27;&#x27;&#x27;CONTENT: &quot;&quot;&quot;
The :mod:\`sklearn.utils\` module includes various utilites.
&quot;&quot;&quot;

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause&#x27;&#x27;&#x27;</span>`}}),Ms=new Bo({}),We=new hi({props:{id:"ma1TrR7gE7I"}}),Ss=new M({props:{code:`from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

context_length = <span class="hljs-number">128</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)

outputs = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">2</span>][<span class="hljs-string">&quot;content&quot;</span>],
    truncation=<span class="hljs-literal">True</span>,
    max_length=context_length,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_length=<span class="hljs-literal">True</span>,
)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input IDs length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(outputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input chunk lengths: <span class="hljs-subst">{(outputs[<span class="hljs-string">&#x27;length&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Chunk mapping: <span class="hljs-subst">{outputs[<span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>]}</span>&quot;</span>)`}}),ye=new M({props:{code:`Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`Input IDs length: <span class="hljs-number">34</span>
Input chunk lengths: [<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">117</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">41</span>]
Chunk mapping: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),jt=new M({props:{code:`def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">element</span>):
    outputs = tokenizer(
        element[<span class="hljs-string">&quot;content&quot;</span>],
        truncation=<span class="hljs-literal">True</span>,
        max_length=context_length,
        return_overflowing_tokens=<span class="hljs-literal">True</span>,
        return_length=<span class="hljs-literal">True</span>,
    )
    input_batch = []
    <span class="hljs-keyword">for</span> length, input_ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(outputs[<span class="hljs-string">&quot;length&quot;</span>], outputs[<span class="hljs-string">&quot;input_ids&quot;</span>]):
        <span class="hljs-keyword">if</span> length == context_length:
            input_batch.append(input_ids)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;input_ids&quot;</span>: input_batch}


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(
    tokenize, batched=<span class="hljs-literal">True</span>, remove_columns=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names
)
tokenized_datasets`}}),yt=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">16702061</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">93164</span>
    })
})`}}),Hs=new ta({props:{$$slots:{default:[mp]},$$scope:{ctx:A}}}),hs=new Bo({});const bi=[_p,fp],Ka=[];function $i(e,c){return e[0]==="pt"?0:1}Us=$i(A),Vs=Ka[Us]=bi[Us](A);const qi=[vp,hp],Ja=[];function ki(e,c){return e[0]==="pt"?0:1}Bs=ki(A),Ws=Ja[Bs]=qi[Bs](A),La=new M({props:{code:`out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")`,highlighted:`out = data_collator([tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> out:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span> shape: <span class="hljs-subst">{out[key].shape}</span>&quot;</span>)`}});const ji=[bp,gp],Ya=[];function yi(e,c){return e[0]==="pt"?0:1}Xs=yi(A),Ks=Ya[Xs]=ji[Xs](A);let ie=A[0]==="tf"&&sp();Vn=new ta({props:{warning:!0,$$slots:{default:[$p]},$$scope:{ctx:A}}}),Oa=new M({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),Sa=new M({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}});const wi=[kp,qp],Za=[];function Ei(e,c){return e[0]==="pt"?0:1}Js=Ei(A),Ys=Za[Js]=wi[Js](A),Xn=new ta({props:{$$slots:{default:[jp]},$$scope:{ctx:A}}}),Kn=new ta({props:{$$slots:{default:[Ep]},$$scope:{ctx:A}}}),Ga=new Bo({});const xi=[zp,xp],Qa=[];function zi(e,c){return e[0]==="pt"?0:1}Zs=zi(A),Qs=Qa[Zs]=xi[Zs](A),Fa=new M({props:{code:`txt = """\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Ha=new M({props:{code:`# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
plt.scatter(x, y)`,highlighted:`<span class="hljs-comment"># cr\xE9er des donn\xE9es</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># cr\xE9er un nuage de points avec x, y</span>
plt.scatter(x, y)`}}),Ia=new M({props:{code:`txt = """\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Ra=new M({props:{code:`# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for`,highlighted:`<span class="hljs-comment"># cr\xE9er des donn\xE9es</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;x&#x27;</span>: x, <span class="hljs-string">&#x27;y&#x27;</span>: y})
df.insert(<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;x&#x27;</span>, x)
<span class="hljs-keyword">for</span>`}}),Ua=new M({props:{code:`txt = """\\
# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({&#x27;profession&#x27;: x, &#x27;income&#x27;:y, &#x27;name&#x27;: z})

# calculer le revenu moyen par profession&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Va=new M({props:{code:`# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession
profession = df.groupby(['profession']).mean()`,highlighted:`<span class="hljs-comment"># tableau de donn\xE9es avec profession, revenu et nom</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;profession&#x27;</span>: x, <span class="hljs-string">&#x27;income&#x27;</span>:y, <span class="hljs-string">&#x27;name&#x27;</span>: z})

<span class="hljs-comment"># calculer le revenu moyen par profession</span>
profession = df.groupby([<span class="hljs-string">&#x27;profession&#x27;</span>]).mean()`}}),Ba=new M({props:{code:`txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Wa=new M({props:{code:`# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf`,highlighted:`<span class="hljs-comment"># import random forest regressor from scikit-learn</span>
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor

<span class="hljs-comment"># entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :</span>
rf = RandomForestRegressor(n_estimators=<span class="hljs-number">300</span>, random_state=random_state, max_depth=<span class="hljs-number">3</span>)
rf.fit(X, y)
rf`}});function Pi(e,c){return e[0]==="tf"?Cp:Pp}let Yl=Pi(A),rn=Yl(A),ue=A[0]==="pt"&&tp(A);return{c(){l=r("meta"),_=m(),y(d.$$.fragment),h=m(),k=r("h1"),$=r("a"),z=r("span"),y(C.$$.fragment),j=m(),P=r("span"),S=n("Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"),L=m(),T.c(),H=m(),B=r("p"),W=n("Jusqu\u2019\xE0 pr\xE9sent, nous avons surtout utilis\xE9 des mod\xE8les pr\xE9-entra\xEEn\xE9s et les avons "),R=r("em"),Q=n("finetun\xE9s"),O=n(" pour de nouveaux cas d\u2019utilisation en r\xE9utilisant les poids du pr\xE9-entra\xEEnement. Comme nous l\u2019avons vu dans le "),K=r("a"),I=n("Chapitre 1"),Z=n(", ceci est commun\xE9ment appel\xE9 "),X=r("em"),F=n("apprentissage par transfert"),G=n(", et c\u2019est une strat\xE9gie tr\xE8s efficace pour appliquer les mod\xE8les Transformer \xE0 la plupart des cas d\u2019utilisation du monde r\xE9el o\xF9 les donn\xE9es \xE9tiquet\xE9es sont rares. Dans ce chapitre, nous allons adopter une approche diff\xE9rente et entra\xEEner un mod\xE8le compl\xE8tement nouveau \xE0 partir de z\xE9ro. C\u2019est une bonne approche \xE0 adopter si vous avez beaucoup de donn\xE9es et qu\u2019elle est tr\xE8s diff\xE9rente des donn\xE9es de pr\xE9-entra\xEEnement utilis\xE9es pour les mod\xE8les disponibles. Cependant, le pr\xE9-entra\xEEnement d\u2019un mod\xE8le de langue n\xE9cessite beaucoup plus de ressources informatiques que le simple affinage d\u2019un mod\xE8le existant. Parmi les exemples o\xF9 il peut \xEAtre utile d\u2019entra\xEEner un nouveau mod\xE8le, citons les ensembles de donn\xE9es constitu\xE9s de notes de musique, de s\xE9quences mol\xE9culaires telles que l\u2019ADN ou de langages de programmation. Ces derniers ont r\xE9cemment gagn\xE9 en popularit\xE9 gr\xE2ce \xE0 des outils tels que TabNine et Copilot de GitHub, aliment\xE9s par le mod\xE8le Codex d\u2019OpenAI, qui peuvent g\xE9n\xE9rer de longues s\xE9quences de code. Cette t\xE2che de g\xE9n\xE9ration de texte est mieux abord\xE9e avec des mod\xE8les de langage autor\xE9gressifs ou causaux tels que GPT-2."),U=m(),V=r("p"),fe=n("Dans cette section, nous allons construire une version r\xE9duite d\u2019un mod\xE8le de g\xE9n\xE9ration de code : nous nous concentrerons sur les compl\xE9ments d\u2019une ligne au lieu des fonctions ou des classes compl\xE8tes, en utilisant un sous-ensemble de code Python. Lorsque vous travaillez avec des donn\xE9es en Python, vous \xEAtes souvent en contact avec la pile de donn\xE9es scientifiques Python, compos\xE9e des biblioth\xE8ques "),ee=r("code"),te=n("matplotlib"),pe=n(", "),N=r("code"),J=n("seaborn"),et=n(", "),Ze=r("code"),Oe=n("pandas"),st=n(" et "),Ee=r("code"),Qe=n("scikit-learn"),es=n(". Lors de l\u2019utilisation de ces "),ss=r("em"),Mt=n("frameworks"),na=n(", il est fr\xE9quent d\u2019avoir besoin de rechercher des commandes sp\xE9cifiques, il serait donc bien d\u2019utiliser un mod\xE8le pour compl\xE9ter ces appels pour nous."),un=m(),y(Se.$$.fragment),pn=m(),le=r("p"),At=n("Dans le "),tt=r("a"),aa=n("Chapitre 6"),Nt=n(", nous avons cr\xE9\xE9 un "),Lt=r("em"),oa=n("tokenizer"),Ot=n(" efficace pour traiter le code source Python, mais nous avons toujours besoin d\u2019un ensemble de donn\xE9es \xE0 grande \xE9chelle pour pr\xE9-entra\xEEner un mod\xE8le. Ici, nous allons appliquer notre "),St=r("em"),la=n("tokenizer"),dn=n(" \xE0 un corpus de code Python provenant des d\xE9p\xF4ts GitHub. Nous utiliserons ensuite l\u2019API "),Ge=r("code"),cn=n("Trainer"),$s=n(" et \u{1F917} "),nt=r("em"),ae=n("Accelerate"),ra=n(" pour entra\xEEner le mod\xE8le. C\u2019est parti !"),at=m(),_e=r("iframe"),ot=m(),he=r("iframe"),lt=m(),xe=r("p"),ia=n("Il s\u2019agit en fait de la pr\xE9sentation du mod\xE8le qui a \xE9t\xE9 entra\xEEn\xE9 et t\xE9l\xE9charg\xE9 sur le "),qs=r("em"),ua=n("Hub"),pa=n(" \xE0 l\u2019aide du code pr\xE9sent\xE9 dans cette section. Vous pouvez le trouver "),Fe=r("a"),da=n("ici"),ca=n(". Notez qu\u2019\xE9tant donn\xE9 qu\u2019il y a une certaine randomisation dans la g\xE9n\xE9ration du texte, vous obtiendrez probablement un r\xE9sultat l\xE9g\xE8rement diff\xE9rent."),rt=m(),He=r("h2"),ts=r("a"),it=r("span"),y(be.$$.fragment),mn=m(),ns=r("span"),ma=n("Collecte des donn\xE9es"),Gt=m(),ne=r("p"),Ft=n("Le code Python est disponible en abondance dans les d\xE9p\xF4ts de code tels que GitHub, que nous pouvons utiliser pour cr\xE9er un ensemble de donn\xE9es en r\xE9cup\xE9rant chaque d\xE9p\xF4t Python. C\u2019est l\u2019approche adopt\xE9e dans le "),ks=r("a"),fa=n("manuel Transformers"),Ht=n(" pour pr\xE9-entra\xEEner un grand mod\xE8le GPT-2. En utilisant un d\xE9p\xF4t GitHub d\u2019environ 180 Go contenant approximativement 20 millions de fichiers Python appel\xE9 "),It=r("code"),_a=n("codeparrot"),Rt=n(", les auteurs ont construit un ensemble de donn\xE9es qu\u2019ils ont ensuite partag\xE9 sur le "),js=r("a"),ut=r("em"),ze=n("Hub"),ha=n("."),pt=m(),as=r("p"),va=n("Cependant, s\u2019entra\xEEner sur l\u2019ensemble du corpus prend beaucoup de temps et demande beaucoup de calculs, et nous n\u2019avons besoin que du sous-ensemble du jeu de donn\xE9es concern\xE9 par la pile Python pour la science des donn\xE9es. Commen\xE7ons donc par filtrer l\u2019ensemble de donn\xE9es "),ys=r("code"),ga=n("codeparrot"),ba=n(" pour tous les fichiers qui incluent l\u2019une des biblioth\xE8ques de cette pile. En raison de la taille de l\u2019ensemble de donn\xE9es, nous voulons \xE9viter de le t\xE9l\xE9charger ; \xE0 la place, nous utiliserons la fonctionnalit\xE9 de streaming pour le filtrer \xE0 la vol\xE9e. Pour nous aider \xE0 filtrer les \xE9chantillons de code utilisant les biblioth\xE8ques que nous avons mentionn\xE9es pr\xE9c\xE9demment, nous utiliserons la fonction suivante :"),Ut=m(),y($e.$$.fragment),Vt=m(),Ie=r("p"),$a=n("Testons-le sur deux exemples :"),Bt=m(),y(qe.$$.fragment),Wt=m(),y(Pe.$$.fragment),fn=m(),ws=r("p"),Es=n("Nous pouvons l\u2019utiliser pour cr\xE9er une fonction qui diffusera l\u2019ensemble de donn\xE9es et filtrera les \xE9l\xE9ments que nous voulons :"),Xt=m(),y(de.$$.fragment),_n=m(),os=r("p"),qa=n("Ensuite, nous pouvons simplement appliquer cette fonction \xE0 l\u2019ensemble de donn\xE9es en continu :"),hn=m(),y(Ce.$$.fragment),vn=m(),y(xs.$$.fragment),Kt=m(),Te=r("p"),gn=n("Cela nous laisse avec environ 3 % de l\u2019ensemble de donn\xE9es original, ce qui est tout de m\xEAme assez important. L\u2019ensemble de donn\xE9es r\xE9sultant est de 6 Go et se compose de 600 000 scripts Python !"),zs=m(),ls=r("p"),bn=n("Le filtrage de l\u2019ensemble complet de donn\xE9es peut prendre de 2 \xE0 3 heures, selon votre machine et votre bande passante. Si vous ne voulez pas passer par ce long processus vous-m\xEAme, nous fournissons l\u2019ensemble de donn\xE9es filtr\xE9 sur le "),Re=r("em"),$n=n("Hub"),dt=n(" pour que vous puissiez le t\xE9l\xE9charger :"),qn=m(),y(rs.$$.fragment),is=m(),y(us.$$.fragment),ps=m(),y(Ue.$$.fragment),ke=m(),ct=r("p"),Jt=n("Examinons un exemple tir\xE9 de l\u2019ensemble de donn\xE9es. Nous ne montrerons que les 200 premiers caract\xE8res de chaque champ :"),kn=m(),y(Ps.$$.fragment),mt=m(),y(Cs.$$.fragment),jn=m(),Ve=r("p"),Ts=n("Nous pouvons voir que le champ "),ft=r("code"),ce=n("content"),ka=n(" contient le code sur lequel nous voulons que notre mod\xE8le s\u2019entra\xEEne. Maintenant que nous avons un jeu de donn\xE9es, nous devons pr\xE9parer les textes afin qu\u2019ils soient dans un format appropri\xE9 pour le pr\xE9-entra\xEEnement."),_t=m(),Be=r("h2"),ds=r("a"),Ds=r("span"),y(Ms.$$.fragment),ja=m(),As=r("span"),ya=n("Pr\xE9paration du jeu de donn\xE9es"),yn=m(),y(We.$$.fragment),wn=m(),cs=r("p"),En=n("La premi\xE8re \xE9tape sera de tokeniser les donn\xE9es, afin de pouvoir les utiliser pour l\u2019entra\xEEnement. Puisque notre objectif est principalement d\u2019autocompl\xE9ter des appels de fonction courts, nous pouvons garder la taille du contexte relativement petite. L\u2019avantage est que nous pouvons entra\xEEner le mod\xE8le beaucoup plus rapidement et qu\u2019il n\xE9cessite beaucoup moins de m\xE9moire. S\u2019il est important pour votre application d\u2019avoir plus de contexte (par exemple, si vous voulez que le mod\xE8le \xE9crive des tests unitaires bas\xE9s sur un fichier avec la d\xE9finition de la fonction), assurez-vous d\u2019augmenter ce nombre, mais gardez \xE9galement \xE0 l\u2019esprit que cela s\u2019accompagne d\u2019une plus grande empreinte m\xE9moire du GPU. Pour l\u2019instant, fixons la taille du contexte \xE0 128 "),Xe=r("em"),xn=n("tokens"),Ns=n(", par opposition aux 1 024 ou 2 048 utilis\xE9s dans GPT-2 ou GPT-3, respectivement."),Yt=m(),oe=r("p"),wa=n("La plupart des documents contiennent beaucoup plus de 128 "),ht=r("em"),Ls=n("tokens"),zn=n(", donc le fait de tronquer les entr\xE9es \xE0 la longueur maximale \xE9liminerait une grande partie de notre jeu de donn\xE9es. A la place, nous utiliserons l\u2019option "),ve=r("code"),Ea=n("return_overflowing_tokens"),Zt=n(" pour tokeniser l\u2019entr\xE9e enti\xE8re et la diviser en plusieurs morceaux, comme nous l\u2019avons fait dans "),vt=r("a"),xa=n("Chapter 6"),Qt=n(". Nous utiliserons \xE9galement l\u2019option "),en=r("code"),za=n("return_length"),Pn=n(" pour retourner automatiquement la longueur de chaque morceau cr\xE9\xE9. Souvent, le dernier morceau sera plus petit que la taille du contexte, et nous nous d\xE9barrasserons de ces morceaux pour \xE9viter les probl\xE8mes de remplissage ; nous n\u2019en avons pas vraiment besoin puisque nous avons beaucoup de donn\xE9es de toute fa\xE7on."),Os=m(),Ke=r("div"),ms=r("img"),Cn=m(),je=r("img"),Je=m(),bt=r("p"),sn=n("Voyons exactement comment cela fonctionne en examinant les deux premiers exemples :"),Tn=m(),y(Ss.$$.fragment),tn=m(),y(ye.$$.fragment),nn=m(),se=r("p"),Ca=n("Nous pouvons voir que nous obtenons 34 segments au total \xE0 partir de ces deux exemples. En regardant les longueurs des "),$t=r("em"),Gs=n("chunks"),Dn=n(", nous pouvons voir que les "),Ye=r("em"),Mn=n("chunks"),o=n(" \xE0 la fin des deux documents ont moins de 128 "),q=r("em"),no=n("tokens"),ao=n(" (117 et 41, respectivement). Ils ne repr\xE9sentent qu\u2019une petite fraction du total des "),An=r("em"),qt=n("chunks"),oo=n(" que nous avons, donc nous pouvons les jeter sans risque. Avec le champ "),Nn=r("code"),Y=n("overflow_to_sample_mapping"),lo=n(", nous pouvons aussi reconstruire quels "),Ln=r("em"),ro=n("chunks"),io=n(" appartenaient \xE0 quels \xE9chantillons d\u2019entr\xE9e."),Ta=m(),re=r("p"),uo=n("Avec cette op\xE9ration, nous utilisons une fonctionnalit\xE9 pratique de la fonction "),On=r("code"),po=n("Dataset.map()"),co=n(" dans \u{1F917} "),Sn=r("em"),mo=n("Datasets"),me=n(", qui est qu\u2019elle ne n\xE9cessite pas de mappage un \xE0 un ; comme nous l\u2019avons vu dans la "),an=r("a"),fo=n("section 3"),_o=n(", nous pouvons cr\xE9er des batchs avec plus ou moins d\u2019\xE9l\xE9ments que le batchd\u2019entr\xE9e. Ceci est utile lorsque l\u2019on effectue des op\xE9rations telles que l\u2019augmentation ou le filtrage des donn\xE9es qui modifient le nombre d\u2019\xE9l\xE9ments. Dans notre cas, lors de la tokenisation de chaque \xE9l\xE9ment en "),Gn=r("em"),ho=n("chunks"),vo=n(" de la taille de contexte sp\xE9cifi\xE9e, nous cr\xE9ons de nombreux \xE9chantillons de chaque document. Nous devons juste nous assurer de supprimer les colonnes existantes, car elles ont une taille conflictuelle. Si nous voulions les garder, nous pourrions les r\xE9p\xE9ter de mani\xE8re appropri\xE9e et les retourner dans l\u2019appel "),Fn=r("code"),kt=n("Dataset.map()"),go=n(" :"),Da=m(),y(jt.$$.fragment),Fs=m(),y(yt.$$.fragment),Ma=m(),we=r("p"),bo=n("Nous avons maintenant 16,7 millions d\u2019exemples avec 128 "),fs=r("em"),$o=n("tokens"),qo=n(" chacun, ce qui correspond \xE0 environ 2,1 milliards de "),Hn=r("em"),ko=n("tokens"),wt=n(" au total. Pour r\xE9f\xE9rence, les mod\xE8les GPT-3 et Codex d\u2019OpenAI sont entra\xEEn\xE9s sur 300 et 100 milliards de "),In=r("em"),jo=n("tokens"),De=n(", respectivement, o\xF9 les mod\xE8les Codex sont initialis\xE9s \xE0 partir des points de contr\xF4le GPT-3. Notre objectif dans cette section n\u2019est pas de rivaliser avec ces mod\xE8les, qui peuvent g\xE9n\xE9rer des textes longs et coh\xE9rents, mais de cr\xE9er une version r\xE9duite fournissant une fonction d\u2019autocompl\xE9tion rapide pour les scientifiques des donn\xE9es."),Aa=m(),on=r("p"),yo=n("Maintenant que l\u2019ensemble de donn\xE9es est pr\xEAt, configurons le mod\xE8le !"),Na=m(),y(Hs.$$.fragment),Is=m(),_s=r("h2"),Rs=r("a"),Rn=r("span"),y(hs.$$.fragment),wo=m(),Un=r("span"),cr=n("Initialisation d'un nouveau mod\xE8le"),ql=m(),Me=r("p"),mr=n("Notre premi\xE8re \xE9tape consiste \xE0 initialiser fra\xEEchement un mod\xE8le GPT-2. Nous utiliserons la m\xEAme configuration pour notre mod\xE8le que pour le petit mod\xE8le GPT-2, donc nous chargeons la configuration pr\xE9-entra\xEEn\xE9e, nous nous assurons que la taille du "),Wo=r("em"),fr=n("tokenizer"),_r=n(" correspond \xE0 la taille du vocabulaire du mod\xE8le et nous passons les identifiants des "),Xo=r("em"),hr=n("tokens"),vr=m(),Ko=r("code"),gr=n("bos"),br=n(" et "),Jo=r("code"),$r=n("eos"),qr=n(" (d\xE9but et fin de s\xE9quence) :"),kl=m(),Vs.c(),Eo=m(),Et=r("p"),kr=n("Notre mod\xE8le comporte 124 millions de param\xE8tres que nous devrons r\xE9gler. Avant de commencer l\u2019entra\xEEnement, nous devons configurer un collateur de donn\xE9es qui se chargera de cr\xE9er les lots. Nous pouvons utiliser le collateur "),Yo=r("code"),jr=n("DataCollatorForLanguageModeling"),yr=n(", qui est con\xE7u sp\xE9cifiquement pour la mod\xE9lisation du langage (comme son nom le sugg\xE8re subtilement). En plus de l\u2019empilage et du remplissage des lots, il s\u2019occupe aussi de la cr\xE9ation des \xE9tiquettes du mod\xE8le de langage \u2014 dans la mod\xE9lisation causale du langage, les entr\xE9es servent aussi d\u2019\xE9tiquettes (juste d\xE9cal\xE9es d\u2019un \xE9l\xE9ment), et ce collateur de donn\xE9es les cr\xE9e \xE0 la vol\xE9e pendant l\u2019entra\xEEnement pour ne pas avoir \xE0 dupliquer les "),Zo=r("code"),wr=n("input_ids"),Er=n("."),jl=m(),Ae=r("p"),xr=n("Notez que "),Qo=r("code"),zr=n("DataCollatorForLanguageModeling"),Pr=n(" supporte \xE0 la fois le "),el=r("em"),Cr=n("masked language modeling"),Tr=n(" (MLM) et le "),sl=r("em"),Dr=n("causal language modeling"),Mr=n(" (CLM). Par d\xE9faut, il pr\xE9pare les donn\xE9es pour MLM, mais nous pouvons passer \xE0 CLM en d\xE9finissant l\u2019argument "),tl=r("code"),Ar=n("mlm=False"),Nr=n(" :"),yl=m(),Ws.c(),xo=m(),zo=r("p"),Lr=n("Prenons un exemple :"),wl=m(),y(La.$$.fragment),El=m(),Ks.c(),Po=m(),Co=r("p"),Or=n("Nous pouvons voir que les exemples ont \xE9t\xE9 empil\xE9s et que tous les tenseurs ont la m\xEAme forme."),xl=m(),ie&&ie.c(),To=m(),y(Vn.$$.fragment),zl=m(),Bn=r("p"),Sr=n("Nous avons maintenant tout ce qu\u2019il faut pour former notre mod\xE8le - ce n\u2019\xE9tait pas si compliqu\xE9 apr\xE8s tout ! Avant de commencer l\u2019entra\xEEnement, nous devons nous connecter \xE0 Hugging Face. Si vous travaillez dans un "),nl=r("em"),Gr=n("notebook"),Fr=n(", vous pouvez le faire avec la fonction utilitaire suivante :"),Pl=m(),y(Oa.$$.fragment),Cl=m(),Do=r("p"),Hr=n("Cela affichera un widget o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face."),Tl=m(),Wn=r("p"),Ir=n("Si vous ne travaillez pas dans un "),al=r("em"),Rr=n("notebook"),Ur=n(", tapez simplement la ligne suivante dans votre terminal :"),Dl=m(),y(Sa.$$.fragment),Ml=m(),Ys.c(),Mo=m(),y(Xn.$$.fragment),Al=m(),y(Kn.$$.fragment),Nl=m(),ln=r("h2"),Jn=r("a"),ol=r("span"),y(Ga.$$.fragment),Vr=m(),ll=r("span"),Br=n("G\xE9n\xE9ration de code avec un pipeline"),Ll=m(),Yn=r("p"),Wr=n("C\u2019est maintenant le moment de v\xE9rit\xE9 : voyons comment le mod\xE8le entra\xEEn\xE9 fonctionne r\xE9ellement ! Nous pouvons voir dans les logs que la perte a diminu\xE9 r\xE9guli\xE8rement, mais pour mettre le mod\xE8le \xE0 l\u2019\xE9preuve, regardons comment il fonctionne sur certains messages. Pour ce faire, nous allons envelopper le mod\xE8le dans une "),rl=r("code"),Xr=n("pipeline"),Kr=n(" de g\xE9n\xE9ration de texte, et nous allons le mettre sur le GPU pour des g\xE9n\xE9rations rapides s\u2019il y en a un de disponible :"),Ol=m(),Qs.c(),Ao=m(),No=r("p"),Jr=n("Let\u2019s start with the simple task of creating a scatter plot:"),Sl=m(),y(Fa.$$.fragment),Gl=m(),y(Ha.$$.fragment),Fl=m(),xt=r("p"),Yr=n("Le r\xE9sultat semble correct. Est-ce que cela fonctionne aussi pour une op\xE9ration "),il=r("code"),Zr=n("pandas"),Qr=n(" ? Voyons si nous pouvons cr\xE9er un "),ul=r("code"),ei=n("DataFrame"),si=n(" \xE0 partir de deux tableaux :"),Hl=m(),y(Ia.$$.fragment),Il=m(),y(Ra.$$.fragment),Rl=m(),Ne=r("p"),ti=n("Bien, c\u2019est la bonne r\xE9ponse. Bien qu\u2019il ins\xE8re ensuite la colonne "),pl=r("code"),ni=n("x"),ai=n(" \xE0 nouveau. Comme le nombre de "),dl=r("em"),oi=n("tokens"),li=n(" g\xE9n\xE9r\xE9s est limit\xE9, la boucle "),cl=r("code"),ri=n("for"),ii=n(" suivante est coup\xE9e. Voyons si nous pouvons faire quelque chose d\u2019un peu plus complexe et faire en sorte que le mod\xE8le nous aide \xE0 utiliser l\u2019op\xE9ration "),ml=r("code"),ui=n("groupby"),pi=n(" :"),Ul=m(),y(Ua.$$.fragment),Vl=m(),y(Va.$$.fragment),Bl=m(),zt=r("p"),di=n("Pas mal, c\u2019est la bonne fa\xE7on de faire. Enfin, voyons si nous pouvons aussi l\u2019utiliser pour "),fl=r("code"),ci=n("scikit-learn"),mi=n(" et mettre en place un mod\xE8le "),_l=r("em"),fi=n("Random Forest"),_i=n(" :"),Wl=m(),y(Ba.$$.fragment),Xl=m(),y(Wa.$$.fragment),Kl=m(),rn.c(),Lo=m(),ue&&ue.c(),Oo=dr(),this.h()},l(e){const c=rp('[data-svelte="svelte-1phssyn"]',document.head);l=i(c,"META",{name:!0,content:!0}),c.forEach(t),_=f(e),w(d.$$.fragment,e),h=f(e),k=i(e,"H1",{class:!0});var eo=u(k);$=i(eo,"A",{id:!0,class:!0,href:!0});var So=u($);z=i(So,"SPAN",{});var hl=u(z);w(C.$$.fragment,hl),hl.forEach(t),So.forEach(t),j=f(eo),P=i(eo,"SPAN",{});var vl=u(P);S=a(vl,"Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"),vl.forEach(t),eo.forEach(t),L=f(e),T.l(e),H=f(e),B=i(e,"P",{});var vs=u(B);W=a(vs,"Jusqu\u2019\xE0 pr\xE9sent, nous avons surtout utilis\xE9 des mod\xE8les pr\xE9-entra\xEEn\xE9s et les avons "),R=i(vs,"EM",{});var Go=u(R);Q=a(Go,"finetun\xE9s"),Go.forEach(t),O=a(vs," pour de nouveaux cas d\u2019utilisation en r\xE9utilisant les poids du pr\xE9-entra\xEEnement. Comme nous l\u2019avons vu dans le "),K=i(vs,"A",{href:!0});var Fo=u(K);I=a(Fo,"Chapitre 1"),Fo.forEach(t),Z=a(vs,", ceci est commun\xE9ment appel\xE9 "),X=i(vs,"EM",{});var gl=u(X);F=a(gl,"apprentissage par transfert"),gl.forEach(t),G=a(vs,", et c\u2019est une strat\xE9gie tr\xE8s efficace pour appliquer les mod\xE8les Transformer \xE0 la plupart des cas d\u2019utilisation du monde r\xE9el o\xF9 les donn\xE9es \xE9tiquet\xE9es sont rares. Dans ce chapitre, nous allons adopter une approche diff\xE9rente et entra\xEEner un mod\xE8le compl\xE8tement nouveau \xE0 partir de z\xE9ro. C\u2019est une bonne approche \xE0 adopter si vous avez beaucoup de donn\xE9es et qu\u2019elle est tr\xE8s diff\xE9rente des donn\xE9es de pr\xE9-entra\xEEnement utilis\xE9es pour les mod\xE8les disponibles. Cependant, le pr\xE9-entra\xEEnement d\u2019un mod\xE8le de langue n\xE9cessite beaucoup plus de ressources informatiques que le simple affinage d\u2019un mod\xE8le existant. Parmi les exemples o\xF9 il peut \xEAtre utile d\u2019entra\xEEner un nouveau mod\xE8le, citons les ensembles de donn\xE9es constitu\xE9s de notes de musique, de s\xE9quences mol\xE9culaires telles que l\u2019ADN ou de langages de programmation. Ces derniers ont r\xE9cemment gagn\xE9 en popularit\xE9 gr\xE2ce \xE0 des outils tels que TabNine et Copilot de GitHub, aliment\xE9s par le mod\xE8le Codex d\u2019OpenAI, qui peuvent g\xE9n\xE9rer de longues s\xE9quences de code. Cette t\xE2che de g\xE9n\xE9ration de texte est mieux abord\xE9e avec des mod\xE8les de langage autor\xE9gressifs ou causaux tels que GPT-2."),vs.forEach(t),U=f(e),V=i(e,"P",{});var ge=u(V);fe=a(ge,"Dans cette section, nous allons construire une version r\xE9duite d\u2019un mod\xE8le de g\xE9n\xE9ration de code : nous nous concentrerons sur les compl\xE9ments d\u2019une ligne au lieu des fonctions ou des classes compl\xE8tes, en utilisant un sous-ensemble de code Python. Lorsque vous travaillez avec des donn\xE9es en Python, vous \xEAtes souvent en contact avec la pile de donn\xE9es scientifiques Python, compos\xE9e des biblioth\xE8ques "),ee=i(ge,"CODE",{});var bl=u(ee);te=a(bl,"matplotlib"),bl.forEach(t),pe=a(ge,", "),N=i(ge,"CODE",{});var $l=u(N);J=a($l,"seaborn"),$l.forEach(t),et=a(ge,", "),Ze=i(ge,"CODE",{});var Ho=u(Ze);Oe=a(Ho,"pandas"),Ho.forEach(t),st=a(ge," et "),Ee=i(ge,"CODE",{});var Ci=u(Ee);Qe=a(Ci,"scikit-learn"),Ci.forEach(t),es=a(ge,". Lors de l\u2019utilisation de ces "),ss=i(ge,"EM",{});var Ti=u(ss);Mt=a(Ti,"frameworks"),Ti.forEach(t),na=a(ge,", il est fr\xE9quent d\u2019avoir besoin de rechercher des commandes sp\xE9cifiques, il serait donc bien d\u2019utiliser un mod\xE8le pour compl\xE9ter ces appels pour nous."),ge.forEach(t),un=f(e),w(Se.$$.fragment,e),pn=f(e),le=i(e,"P",{});var gs=u(le);At=a(gs,"Dans le "),tt=i(gs,"A",{href:!0});var Di=u(tt);aa=a(Di,"Chapitre 6"),Di.forEach(t),Nt=a(gs,", nous avons cr\xE9\xE9 un "),Lt=i(gs,"EM",{});var Mi=u(Lt);oa=a(Mi,"tokenizer"),Mi.forEach(t),Ot=a(gs," efficace pour traiter le code source Python, mais nous avons toujours besoin d\u2019un ensemble de donn\xE9es \xE0 grande \xE9chelle pour pr\xE9-entra\xEEner un mod\xE8le. Ici, nous allons appliquer notre "),St=i(gs,"EM",{});var Ai=u(St);la=a(Ai,"tokenizer"),Ai.forEach(t),dn=a(gs," \xE0 un corpus de code Python provenant des d\xE9p\xF4ts GitHub. Nous utiliserons ensuite l\u2019API "),Ge=i(gs,"CODE",{});var Ni=u(Ge);cn=a(Ni,"Trainer"),Ni.forEach(t),$s=a(gs," et \u{1F917} "),nt=i(gs,"EM",{});var Li=u(nt);ae=a(Li,"Accelerate"),Li.forEach(t),ra=a(gs," pour entra\xEEner le mod\xE8le. C\u2019est parti !"),gs.forEach(t),at=f(e),_e=i(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),u(_e).forEach(t),ot=f(e),he=i(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),u(he).forEach(t),lt=f(e),xe=i(e,"P",{});var Io=u(xe);ia=a(Io,"Il s\u2019agit en fait de la pr\xE9sentation du mod\xE8le qui a \xE9t\xE9 entra\xEEn\xE9 et t\xE9l\xE9charg\xE9 sur le "),qs=i(Io,"EM",{});var Oi=u(qs);ua=a(Oi,"Hub"),Oi.forEach(t),pa=a(Io," \xE0 l\u2019aide du code pr\xE9sent\xE9 dans cette section. Vous pouvez le trouver "),Fe=i(Io,"A",{href:!0,rel:!0});var Si=u(Fe);da=a(Si,"ici"),Si.forEach(t),ca=a(Io,". Notez qu\u2019\xE9tant donn\xE9 qu\u2019il y a une certaine randomisation dans la g\xE9n\xE9ration du texte, vous obtiendrez probablement un r\xE9sultat l\xE9g\xE8rement diff\xE9rent."),Io.forEach(t),rt=f(e),He=i(e,"H2",{class:!0});var Zl=u(He);ts=i(Zl,"A",{id:!0,class:!0,href:!0});var Gi=u(ts);it=i(Gi,"SPAN",{});var Fi=u(it);w(be.$$.fragment,Fi),Fi.forEach(t),Gi.forEach(t),mn=f(Zl),ns=i(Zl,"SPAN",{});var Hi=u(ns);ma=a(Hi,"Collecte des donn\xE9es"),Hi.forEach(t),Zl.forEach(t),Gt=f(e),ne=i(e,"P",{});var Zn=u(ne);Ft=a(Zn,"Le code Python est disponible en abondance dans les d\xE9p\xF4ts de code tels que GitHub, que nous pouvons utiliser pour cr\xE9er un ensemble de donn\xE9es en r\xE9cup\xE9rant chaque d\xE9p\xF4t Python. C\u2019est l\u2019approche adopt\xE9e dans le "),ks=i(Zn,"A",{href:!0,rel:!0});var Ii=u(ks);fa=a(Ii,"manuel Transformers"),Ii.forEach(t),Ht=a(Zn," pour pr\xE9-entra\xEEner un grand mod\xE8le GPT-2. En utilisant un d\xE9p\xF4t GitHub d\u2019environ 180 Go contenant approximativement 20 millions de fichiers Python appel\xE9 "),It=i(Zn,"CODE",{});var Ri=u(It);_a=a(Ri,"codeparrot"),Ri.forEach(t),Rt=a(Zn,", les auteurs ont construit un ensemble de donn\xE9es qu\u2019ils ont ensuite partag\xE9 sur le "),js=i(Zn,"A",{href:!0,rel:!0});var Ui=u(js);ut=i(Ui,"EM",{});var Vi=u(ut);ze=a(Vi,"Hub"),Vi.forEach(t),Ui.forEach(t),ha=a(Zn,"."),Zn.forEach(t),pt=f(e),as=i(e,"P",{});var Ql=u(as);va=a(Ql,"Cependant, s\u2019entra\xEEner sur l\u2019ensemble du corpus prend beaucoup de temps et demande beaucoup de calculs, et nous n\u2019avons besoin que du sous-ensemble du jeu de donn\xE9es concern\xE9 par la pile Python pour la science des donn\xE9es. Commen\xE7ons donc par filtrer l\u2019ensemble de donn\xE9es "),ys=i(Ql,"CODE",{});var Bi=u(ys);ga=a(Bi,"codeparrot"),Bi.forEach(t),ba=a(Ql," pour tous les fichiers qui incluent l\u2019une des biblioth\xE8ques de cette pile. En raison de la taille de l\u2019ensemble de donn\xE9es, nous voulons \xE9viter de le t\xE9l\xE9charger ; \xE0 la place, nous utiliserons la fonctionnalit\xE9 de streaming pour le filtrer \xE0 la vol\xE9e. Pour nous aider \xE0 filtrer les \xE9chantillons de code utilisant les biblioth\xE8ques que nous avons mentionn\xE9es pr\xE9c\xE9demment, nous utiliserons la fonction suivante :"),Ql.forEach(t),Ut=f(e),w($e.$$.fragment,e),Vt=f(e),Ie=i(e,"P",{});var Wi=u(Ie);$a=a(Wi,"Testons-le sur deux exemples :"),Wi.forEach(t),Bt=f(e),w(qe.$$.fragment,e),Wt=f(e),w(Pe.$$.fragment,e),fn=f(e),ws=i(e,"P",{});var Xi=u(ws);Es=a(Xi,"Nous pouvons l\u2019utiliser pour cr\xE9er une fonction qui diffusera l\u2019ensemble de donn\xE9es et filtrera les \xE9l\xE9ments que nous voulons :"),Xi.forEach(t),Xt=f(e),w(de.$$.fragment,e),_n=f(e),os=i(e,"P",{});var Ki=u(os);qa=a(Ki,"Ensuite, nous pouvons simplement appliquer cette fonction \xE0 l\u2019ensemble de donn\xE9es en continu :"),Ki.forEach(t),hn=f(e),w(Ce.$$.fragment,e),vn=f(e),w(xs.$$.fragment,e),Kt=f(e),Te=i(e,"P",{});var Ji=u(Te);gn=a(Ji,"Cela nous laisse avec environ 3 % de l\u2019ensemble de donn\xE9es original, ce qui est tout de m\xEAme assez important. L\u2019ensemble de donn\xE9es r\xE9sultant est de 6 Go et se compose de 600 000 scripts Python !"),Ji.forEach(t),zs=f(e),ls=i(e,"P",{});var er=u(ls);bn=a(er,"Le filtrage de l\u2019ensemble complet de donn\xE9es peut prendre de 2 \xE0 3 heures, selon votre machine et votre bande passante. Si vous ne voulez pas passer par ce long processus vous-m\xEAme, nous fournissons l\u2019ensemble de donn\xE9es filtr\xE9 sur le "),Re=i(er,"EM",{});var Yi=u(Re);$n=a(Yi,"Hub"),Yi.forEach(t),dt=a(er," pour que vous puissiez le t\xE9l\xE9charger :"),er.forEach(t),qn=f(e),w(rs.$$.fragment,e),is=f(e),w(us.$$.fragment,e),ps=f(e),w(Ue.$$.fragment,e),ke=f(e),ct=i(e,"P",{});var Zi=u(ct);Jt=a(Zi,"Examinons un exemple tir\xE9 de l\u2019ensemble de donn\xE9es. Nous ne montrerons que les 200 premiers caract\xE8res de chaque champ :"),Zi.forEach(t),kn=f(e),w(Ps.$$.fragment,e),mt=f(e),w(Cs.$$.fragment,e),jn=f(e),Ve=i(e,"P",{});var sr=u(Ve);Ts=a(sr,"Nous pouvons voir que le champ "),ft=i(sr,"CODE",{});var Qi=u(ft);ce=a(Qi,"content"),Qi.forEach(t),ka=a(sr," contient le code sur lequel nous voulons que notre mod\xE8le s\u2019entra\xEEne. Maintenant que nous avons un jeu de donn\xE9es, nous devons pr\xE9parer les textes afin qu\u2019ils soient dans un format appropri\xE9 pour le pr\xE9-entra\xEEnement."),sr.forEach(t),_t=f(e),Be=i(e,"H2",{class:!0});var tr=u(Be);ds=i(tr,"A",{id:!0,class:!0,href:!0});var eu=u(ds);Ds=i(eu,"SPAN",{});var su=u(Ds);w(Ms.$$.fragment,su),su.forEach(t),eu.forEach(t),ja=f(tr),As=i(tr,"SPAN",{});var tu=u(As);ya=a(tu,"Pr\xE9paration du jeu de donn\xE9es"),tu.forEach(t),tr.forEach(t),yn=f(e),w(We.$$.fragment,e),wn=f(e),cs=i(e,"P",{});var nr=u(cs);En=a(nr,"La premi\xE8re \xE9tape sera de tokeniser les donn\xE9es, afin de pouvoir les utiliser pour l\u2019entra\xEEnement. Puisque notre objectif est principalement d\u2019autocompl\xE9ter des appels de fonction courts, nous pouvons garder la taille du contexte relativement petite. L\u2019avantage est que nous pouvons entra\xEEner le mod\xE8le beaucoup plus rapidement et qu\u2019il n\xE9cessite beaucoup moins de m\xE9moire. S\u2019il est important pour votre application d\u2019avoir plus de contexte (par exemple, si vous voulez que le mod\xE8le \xE9crive des tests unitaires bas\xE9s sur un fichier avec la d\xE9finition de la fonction), assurez-vous d\u2019augmenter ce nombre, mais gardez \xE9galement \xE0 l\u2019esprit que cela s\u2019accompagne d\u2019une plus grande empreinte m\xE9moire du GPU. Pour l\u2019instant, fixons la taille du contexte \xE0 128 "),Xe=i(nr,"EM",{});var nu=u(Xe);xn=a(nu,"tokens"),nu.forEach(t),Ns=a(nr,", par opposition aux 1 024 ou 2 048 utilis\xE9s dans GPT-2 ou GPT-3, respectivement."),nr.forEach(t),Yt=f(e),oe=i(e,"P",{});var Pt=u(oe);wa=a(Pt,"La plupart des documents contiennent beaucoup plus de 128 "),ht=i(Pt,"EM",{});var au=u(ht);Ls=a(au,"tokens"),au.forEach(t),zn=a(Pt,", donc le fait de tronquer les entr\xE9es \xE0 la longueur maximale \xE9liminerait une grande partie de notre jeu de donn\xE9es. A la place, nous utiliserons l\u2019option "),ve=i(Pt,"CODE",{});var ou=u(ve);Ea=a(ou,"return_overflowing_tokens"),ou.forEach(t),Zt=a(Pt," pour tokeniser l\u2019entr\xE9e enti\xE8re et la diviser en plusieurs morceaux, comme nous l\u2019avons fait dans "),vt=i(Pt,"A",{href:!0});var lu=u(vt);xa=a(lu,"Chapter 6"),lu.forEach(t),Qt=a(Pt,". Nous utiliserons \xE9galement l\u2019option "),en=i(Pt,"CODE",{});var ru=u(en);za=a(ru,"return_length"),ru.forEach(t),Pn=a(Pt," pour retourner automatiquement la longueur de chaque morceau cr\xE9\xE9. Souvent, le dernier morceau sera plus petit que la taille du contexte, et nous nous d\xE9barrasserons de ces morceaux pour \xE9viter les probl\xE8mes de remplissage ; nous n\u2019en avons pas vraiment besoin puisque nous avons beaucoup de donn\xE9es de toute fa\xE7on."),Pt.forEach(t),Os=f(e),Ke=i(e,"DIV",{class:!0});var ar=u(Ke);ms=i(ar,"IMG",{class:!0,src:!0,alt:!0}),Cn=f(ar),je=i(ar,"IMG",{class:!0,src:!0,alt:!0}),ar.forEach(t),Je=f(e),bt=i(e,"P",{});var iu=u(bt);sn=a(iu,"Voyons exactement comment cela fonctionne en examinant les deux premiers exemples :"),iu.forEach(t),Tn=f(e),w(Ss.$$.fragment,e),tn=f(e),w(ye.$$.fragment,e),nn=f(e),se=i(e,"P",{});var Le=u(se);Ca=a(Le,"Nous pouvons voir que nous obtenons 34 segments au total \xE0 partir de ces deux exemples. En regardant les longueurs des "),$t=i(Le,"EM",{});var uu=u($t);Gs=a(uu,"chunks"),uu.forEach(t),Dn=a(Le,", nous pouvons voir que les "),Ye=i(Le,"EM",{});var pu=u(Ye);Mn=a(pu,"chunks"),pu.forEach(t),o=a(Le," \xE0 la fin des deux documents ont moins de 128 "),q=i(Le,"EM",{});var du=u(q);no=a(du,"tokens"),du.forEach(t),ao=a(Le," (117 et 41, respectivement). Ils ne repr\xE9sentent qu\u2019une petite fraction du total des "),An=i(Le,"EM",{});var cu=u(An);qt=a(cu,"chunks"),cu.forEach(t),oo=a(Le," que nous avons, donc nous pouvons les jeter sans risque. Avec le champ "),Nn=i(Le,"CODE",{});var mu=u(Nn);Y=a(mu,"overflow_to_sample_mapping"),mu.forEach(t),lo=a(Le,", nous pouvons aussi reconstruire quels "),Ln=i(Le,"EM",{});var fu=u(Ln);ro=a(fu,"chunks"),fu.forEach(t),io=a(Le," appartenaient \xE0 quels \xE9chantillons d\u2019entr\xE9e."),Le.forEach(t),Ta=f(e),re=i(e,"P",{});var bs=u(re);uo=a(bs,"Avec cette op\xE9ration, nous utilisons une fonctionnalit\xE9 pratique de la fonction "),On=i(bs,"CODE",{});var _u=u(On);po=a(_u,"Dataset.map()"),_u.forEach(t),co=a(bs," dans \u{1F917} "),Sn=i(bs,"EM",{});var hu=u(Sn);mo=a(hu,"Datasets"),hu.forEach(t),me=a(bs,", qui est qu\u2019elle ne n\xE9cessite pas de mappage un \xE0 un ; comme nous l\u2019avons vu dans la "),an=i(bs,"A",{href:!0});var vu=u(an);fo=a(vu,"section 3"),vu.forEach(t),_o=a(bs,", nous pouvons cr\xE9er des batchs avec plus ou moins d\u2019\xE9l\xE9ments que le batchd\u2019entr\xE9e. Ceci est utile lorsque l\u2019on effectue des op\xE9rations telles que l\u2019augmentation ou le filtrage des donn\xE9es qui modifient le nombre d\u2019\xE9l\xE9ments. Dans notre cas, lors de la tokenisation de chaque \xE9l\xE9ment en "),Gn=i(bs,"EM",{});var gu=u(Gn);ho=a(gu,"chunks"),gu.forEach(t),vo=a(bs," de la taille de contexte sp\xE9cifi\xE9e, nous cr\xE9ons de nombreux \xE9chantillons de chaque document. Nous devons juste nous assurer de supprimer les colonnes existantes, car elles ont une taille conflictuelle. Si nous voulions les garder, nous pourrions les r\xE9p\xE9ter de mani\xE8re appropri\xE9e et les retourner dans l\u2019appel "),Fn=i(bs,"CODE",{});var bu=u(Fn);kt=a(bu,"Dataset.map()"),bu.forEach(t),go=a(bs," :"),bs.forEach(t),Da=f(e),w(jt.$$.fragment,e),Fs=f(e),w(yt.$$.fragment,e),Ma=f(e),we=i(e,"P",{});var Qn=u(we);bo=a(Qn,"Nous avons maintenant 16,7 millions d\u2019exemples avec 128 "),fs=i(Qn,"EM",{});var $u=u(fs);$o=a($u,"tokens"),$u.forEach(t),qo=a(Qn," chacun, ce qui correspond \xE0 environ 2,1 milliards de "),Hn=i(Qn,"EM",{});var qu=u(Hn);ko=a(qu,"tokens"),qu.forEach(t),wt=a(Qn," au total. Pour r\xE9f\xE9rence, les mod\xE8les GPT-3 et Codex d\u2019OpenAI sont entra\xEEn\xE9s sur 300 et 100 milliards de "),In=i(Qn,"EM",{});var ku=u(In);jo=a(ku,"tokens"),ku.forEach(t),De=a(Qn,", respectivement, o\xF9 les mod\xE8les Codex sont initialis\xE9s \xE0 partir des points de contr\xF4le GPT-3. Notre objectif dans cette section n\u2019est pas de rivaliser avec ces mod\xE8les, qui peuvent g\xE9n\xE9rer des textes longs et coh\xE9rents, mais de cr\xE9er une version r\xE9duite fournissant une fonction d\u2019autocompl\xE9tion rapide pour les scientifiques des donn\xE9es."),Qn.forEach(t),Aa=f(e),on=i(e,"P",{});var ju=u(on);yo=a(ju,"Maintenant que l\u2019ensemble de donn\xE9es est pr\xEAt, configurons le mod\xE8le !"),ju.forEach(t),Na=f(e),w(Hs.$$.fragment,e),Is=f(e),_s=i(e,"H2",{class:!0});var or=u(_s);Rs=i(or,"A",{id:!0,class:!0,href:!0});var yu=u(Rs);Rn=i(yu,"SPAN",{});var wu=u(Rn);w(hs.$$.fragment,wu),wu.forEach(t),yu.forEach(t),wo=f(or),Un=i(or,"SPAN",{});var Eu=u(Un);cr=a(Eu,"Initialisation d'un nouveau mod\xE8le"),Eu.forEach(t),or.forEach(t),ql=f(e),Me=i(e,"P",{});var Ct=u(Me);mr=a(Ct,"Notre premi\xE8re \xE9tape consiste \xE0 initialiser fra\xEEchement un mod\xE8le GPT-2. Nous utiliserons la m\xEAme configuration pour notre mod\xE8le que pour le petit mod\xE8le GPT-2, donc nous chargeons la configuration pr\xE9-entra\xEEn\xE9e, nous nous assurons que la taille du "),Wo=i(Ct,"EM",{});var xu=u(Wo);fr=a(xu,"tokenizer"),xu.forEach(t),_r=a(Ct," correspond \xE0 la taille du vocabulaire du mod\xE8le et nous passons les identifiants des "),Xo=i(Ct,"EM",{});var zu=u(Xo);hr=a(zu,"tokens"),zu.forEach(t),vr=f(Ct),Ko=i(Ct,"CODE",{});var Pu=u(Ko);gr=a(Pu,"bos"),Pu.forEach(t),br=a(Ct," et "),Jo=i(Ct,"CODE",{});var Cu=u(Jo);$r=a(Cu,"eos"),Cu.forEach(t),qr=a(Ct," (d\xE9but et fin de s\xE9quence) :"),Ct.forEach(t),kl=f(e),Vs.l(e),Eo=f(e),Et=i(e,"P",{});var Ro=u(Et);kr=a(Ro,"Notre mod\xE8le comporte 124 millions de param\xE8tres que nous devrons r\xE9gler. Avant de commencer l\u2019entra\xEEnement, nous devons configurer un collateur de donn\xE9es qui se chargera de cr\xE9er les lots. Nous pouvons utiliser le collateur "),Yo=i(Ro,"CODE",{});var Tu=u(Yo);jr=a(Tu,"DataCollatorForLanguageModeling"),Tu.forEach(t),yr=a(Ro,", qui est con\xE7u sp\xE9cifiquement pour la mod\xE9lisation du langage (comme son nom le sugg\xE8re subtilement). En plus de l\u2019empilage et du remplissage des lots, il s\u2019occupe aussi de la cr\xE9ation des \xE9tiquettes du mod\xE8le de langage \u2014 dans la mod\xE9lisation causale du langage, les entr\xE9es servent aussi d\u2019\xE9tiquettes (juste d\xE9cal\xE9es d\u2019un \xE9l\xE9ment), et ce collateur de donn\xE9es les cr\xE9e \xE0 la vol\xE9e pendant l\u2019entra\xEEnement pour ne pas avoir \xE0 dupliquer les "),Zo=i(Ro,"CODE",{});var Du=u(Zo);wr=a(Du,"input_ids"),Du.forEach(t),Er=a(Ro,"."),Ro.forEach(t),jl=f(e),Ae=i(e,"P",{});var Tt=u(Ae);xr=a(Tt,"Notez que "),Qo=i(Tt,"CODE",{});var Mu=u(Qo);zr=a(Mu,"DataCollatorForLanguageModeling"),Mu.forEach(t),Pr=a(Tt," supporte \xE0 la fois le "),el=i(Tt,"EM",{});var Au=u(el);Cr=a(Au,"masked language modeling"),Au.forEach(t),Tr=a(Tt," (MLM) et le "),sl=i(Tt,"EM",{});var Nu=u(sl);Dr=a(Nu,"causal language modeling"),Nu.forEach(t),Mr=a(Tt," (CLM). Par d\xE9faut, il pr\xE9pare les donn\xE9es pour MLM, mais nous pouvons passer \xE0 CLM en d\xE9finissant l\u2019argument "),tl=i(Tt,"CODE",{});var Lu=u(tl);Ar=a(Lu,"mlm=False"),Lu.forEach(t),Nr=a(Tt," :"),Tt.forEach(t),yl=f(e),Ws.l(e),xo=f(e),zo=i(e,"P",{});var Ou=u(zo);Lr=a(Ou,"Prenons un exemple :"),Ou.forEach(t),wl=f(e),w(La.$$.fragment,e),El=f(e),Ks.l(e),Po=f(e),Co=i(e,"P",{});var Su=u(Co);Or=a(Su,"Nous pouvons voir que les exemples ont \xE9t\xE9 empil\xE9s et que tous les tenseurs ont la m\xEAme forme."),Su.forEach(t),xl=f(e),ie&&ie.l(e),To=f(e),w(Vn.$$.fragment,e),zl=f(e),Bn=i(e,"P",{});var lr=u(Bn);Sr=a(lr,"Nous avons maintenant tout ce qu\u2019il faut pour former notre mod\xE8le - ce n\u2019\xE9tait pas si compliqu\xE9 apr\xE8s tout ! Avant de commencer l\u2019entra\xEEnement, nous devons nous connecter \xE0 Hugging Face. Si vous travaillez dans un "),nl=i(lr,"EM",{});var Gu=u(nl);Gr=a(Gu,"notebook"),Gu.forEach(t),Fr=a(lr,", vous pouvez le faire avec la fonction utilitaire suivante :"),lr.forEach(t),Pl=f(e),w(Oa.$$.fragment,e),Cl=f(e),Do=i(e,"P",{});var Fu=u(Do);Hr=a(Fu,"Cela affichera un widget o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face."),Fu.forEach(t),Tl=f(e),Wn=i(e,"P",{});var rr=u(Wn);Ir=a(rr,"Si vous ne travaillez pas dans un "),al=i(rr,"EM",{});var Hu=u(al);Rr=a(Hu,"notebook"),Hu.forEach(t),Ur=a(rr,", tapez simplement la ligne suivante dans votre terminal :"),rr.forEach(t),Dl=f(e),w(Sa.$$.fragment,e),Ml=f(e),Ys.l(e),Mo=f(e),w(Xn.$$.fragment,e),Al=f(e),w(Kn.$$.fragment,e),Nl=f(e),ln=i(e,"H2",{class:!0});var ir=u(ln);Jn=i(ir,"A",{id:!0,class:!0,href:!0});var Iu=u(Jn);ol=i(Iu,"SPAN",{});var Ru=u(ol);w(Ga.$$.fragment,Ru),Ru.forEach(t),Iu.forEach(t),Vr=f(ir),ll=i(ir,"SPAN",{});var Uu=u(ll);Br=a(Uu,"G\xE9n\xE9ration de code avec un pipeline"),Uu.forEach(t),ir.forEach(t),Ll=f(e),Yn=i(e,"P",{});var ur=u(Yn);Wr=a(ur,"C\u2019est maintenant le moment de v\xE9rit\xE9 : voyons comment le mod\xE8le entra\xEEn\xE9 fonctionne r\xE9ellement ! Nous pouvons voir dans les logs que la perte a diminu\xE9 r\xE9guli\xE8rement, mais pour mettre le mod\xE8le \xE0 l\u2019\xE9preuve, regardons comment il fonctionne sur certains messages. Pour ce faire, nous allons envelopper le mod\xE8le dans une "),rl=i(ur,"CODE",{});var Vu=u(rl);Xr=a(Vu,"pipeline"),Vu.forEach(t),Kr=a(ur," de g\xE9n\xE9ration de texte, et nous allons le mettre sur le GPU pour des g\xE9n\xE9rations rapides s\u2019il y en a un de disponible :"),ur.forEach(t),Ol=f(e),Qs.l(e),Ao=f(e),No=i(e,"P",{});var Bu=u(No);Jr=a(Bu,"Let\u2019s start with the simple task of creating a scatter plot:"),Bu.forEach(t),Sl=f(e),w(Fa.$$.fragment,e),Gl=f(e),w(Ha.$$.fragment,e),Fl=f(e),xt=i(e,"P",{});var Uo=u(xt);Yr=a(Uo,"Le r\xE9sultat semble correct. Est-ce que cela fonctionne aussi pour une op\xE9ration "),il=i(Uo,"CODE",{});var Wu=u(il);Zr=a(Wu,"pandas"),Wu.forEach(t),Qr=a(Uo," ? Voyons si nous pouvons cr\xE9er un "),ul=i(Uo,"CODE",{});var Xu=u(ul);ei=a(Xu,"DataFrame"),Xu.forEach(t),si=a(Uo," \xE0 partir de deux tableaux :"),Uo.forEach(t),Hl=f(e),w(Ia.$$.fragment,e),Il=f(e),w(Ra.$$.fragment,e),Rl=f(e),Ne=i(e,"P",{});var Dt=u(Ne);ti=a(Dt,"Bien, c\u2019est la bonne r\xE9ponse. Bien qu\u2019il ins\xE8re ensuite la colonne "),pl=i(Dt,"CODE",{});var Ku=u(pl);ni=a(Ku,"x"),Ku.forEach(t),ai=a(Dt," \xE0 nouveau. Comme le nombre de "),dl=i(Dt,"EM",{});var Ju=u(dl);oi=a(Ju,"tokens"),Ju.forEach(t),li=a(Dt," g\xE9n\xE9r\xE9s est limit\xE9, la boucle "),cl=i(Dt,"CODE",{});var Yu=u(cl);ri=a(Yu,"for"),Yu.forEach(t),ii=a(Dt," suivante est coup\xE9e. Voyons si nous pouvons faire quelque chose d\u2019un peu plus complexe et faire en sorte que le mod\xE8le nous aide \xE0 utiliser l\u2019op\xE9ration "),ml=i(Dt,"CODE",{});var Zu=u(ml);ui=a(Zu,"groupby"),Zu.forEach(t),pi=a(Dt," :"),Dt.forEach(t),Ul=f(e),w(Ua.$$.fragment,e),Vl=f(e),w(Va.$$.fragment,e),Bl=f(e),zt=i(e,"P",{});var Vo=u(zt);di=a(Vo,"Pas mal, c\u2019est la bonne fa\xE7on de faire. Enfin, voyons si nous pouvons aussi l\u2019utiliser pour "),fl=i(Vo,"CODE",{});var Qu=u(fl);ci=a(Qu,"scikit-learn"),Qu.forEach(t),mi=a(Vo," et mettre en place un mod\xE8le "),_l=i(Vo,"EM",{});var ep=u(_l);fi=a(ep,"Random Forest"),ep.forEach(t),_i=a(Vo," :"),Vo.forEach(t),Wl=f(e),w(Ba.$$.fragment,e),Xl=f(e),w(Wa.$$.fragment,e),Kl=f(e),rn.l(e),Lo=f(e),ue&&ue.l(e),Oo=dr(),this.h()},h(){D(l,"name","hf:doc:metadata"),D(l,"content",JSON.stringify(Np)),D($,"id","entraner-un-modle-de-langage-causal-partir-de-zro"),D($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D($,"href","#entraner-un-modle-de-langage-causal-partir-de-zro"),D(k,"class","relative group"),D(K,"href","/course/fr/chapter1"),D(tt,"href","/course/fr/chapter6"),pr(_e.src,so="https://hf.space/gradioiframe/course-demos/codeparrot-ds/+")||D(_e,"src",so),D(_e,"frameborder","0"),D(_e,"height","300"),D(_e,"title","Gradio app"),D(_e,"class","block dark:hidden container p-0 flex-grow space-iframe"),D(_e,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),D(_e,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),pr(he.src,to="https://hf.space/gradioiframe/course-demos/codeparrot-ds-darkmode/+")||D(he,"src",to),D(he,"frameborder","0"),D(he,"height","300"),D(he,"title","Gradio app"),D(he,"class","hidden dark:block container p-0 flex-grow space-iframe"),D(he,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),D(he,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),D(Fe,"href","https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28"),D(Fe,"rel","nofollow"),D(ts,"id","collecte-des-donnes"),D(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ts,"href","#collecte-des-donnes"),D(He,"class","relative group"),D(ks,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/"),D(ks,"rel","nofollow"),D(js,"href","https://huggingface.co/datasets/transformersbook/codeparrot"),D(js,"rel","nofollow"),D(ds,"id","prparation-du-jeu-de-donnes"),D(ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ds,"href","#prparation-du-jeu-de-donnes"),D(Be,"class","relative group"),D(vt,"href","/course/chapter6/4"),D(ms,"class","block dark:hidden"),pr(ms.src,gt="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg")||D(ms,"src",gt),D(ms,"alt","Chunking a large texts in several pieces."),D(je,"class","hidden dark:block"),pr(je.src,Pa="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg")||D(je,"src",Pa),D(je,"alt","Chunking a large texts in several pieces."),D(Ke,"class","flex justify-center"),D(an,"href","/course/fr/chapter7/3"),D(Rs,"id","initialisation-dun-nouveau-modle"),D(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Rs,"href","#initialisation-dun-nouveau-modle"),D(_s,"class","relative group"),D(Jn,"id","gnration-de-code-avec-un-pipeline"),D(Jn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Jn,"href","#gnration-de-code-avec-un-pipeline"),D(ln,"class","relative group")},m(e,c){s(document.head,l),p(e,_,c),E(d,e,c),p(e,h,c),p(e,k,c),s(k,$),s($,z),E(C,z,null),s(k,j),s(k,P),s(P,S),p(e,L,c),Xa[g].m(e,c),p(e,H,c),p(e,B,c),s(B,W),s(B,R),s(R,Q),s(B,O),s(B,K),s(K,I),s(B,Z),s(B,X),s(X,F),s(B,G),p(e,U,c),p(e,V,c),s(V,fe),s(V,ee),s(ee,te),s(V,pe),s(V,N),s(N,J),s(V,et),s(V,Ze),s(Ze,Oe),s(V,st),s(V,Ee),s(Ee,Qe),s(V,es),s(V,ss),s(ss,Mt),s(V,na),p(e,un,c),E(Se,e,c),p(e,pn,c),p(e,le,c),s(le,At),s(le,tt),s(tt,aa),s(le,Nt),s(le,Lt),s(Lt,oa),s(le,Ot),s(le,St),s(St,la),s(le,dn),s(le,Ge),s(Ge,cn),s(le,$s),s(le,nt),s(nt,ae),s(le,ra),p(e,at,c),p(e,_e,c),p(e,ot,c),p(e,he,c),p(e,lt,c),p(e,xe,c),s(xe,ia),s(xe,qs),s(qs,ua),s(xe,pa),s(xe,Fe),s(Fe,da),s(xe,ca),p(e,rt,c),p(e,He,c),s(He,ts),s(ts,it),E(be,it,null),s(He,mn),s(He,ns),s(ns,ma),p(e,Gt,c),p(e,ne,c),s(ne,Ft),s(ne,ks),s(ks,fa),s(ne,Ht),s(ne,It),s(It,_a),s(ne,Rt),s(ne,js),s(js,ut),s(ut,ze),s(ne,ha),p(e,pt,c),p(e,as,c),s(as,va),s(as,ys),s(ys,ga),s(as,ba),p(e,Ut,c),E($e,e,c),p(e,Vt,c),p(e,Ie,c),s(Ie,$a),p(e,Bt,c),E(qe,e,c),p(e,Wt,c),E(Pe,e,c),p(e,fn,c),p(e,ws,c),s(ws,Es),p(e,Xt,c),E(de,e,c),p(e,_n,c),p(e,os,c),s(os,qa),p(e,hn,c),E(Ce,e,c),p(e,vn,c),E(xs,e,c),p(e,Kt,c),p(e,Te,c),s(Te,gn),p(e,zs,c),p(e,ls,c),s(ls,bn),s(ls,Re),s(Re,$n),s(ls,dt),p(e,qn,c),E(rs,e,c),p(e,is,c),E(us,e,c),p(e,ps,c),E(Ue,e,c),p(e,ke,c),p(e,ct,c),s(ct,Jt),p(e,kn,c),E(Ps,e,c),p(e,mt,c),E(Cs,e,c),p(e,jn,c),p(e,Ve,c),s(Ve,Ts),s(Ve,ft),s(ft,ce),s(Ve,ka),p(e,_t,c),p(e,Be,c),s(Be,ds),s(ds,Ds),E(Ms,Ds,null),s(Be,ja),s(Be,As),s(As,ya),p(e,yn,c),E(We,e,c),p(e,wn,c),p(e,cs,c),s(cs,En),s(cs,Xe),s(Xe,xn),s(cs,Ns),p(e,Yt,c),p(e,oe,c),s(oe,wa),s(oe,ht),s(ht,Ls),s(oe,zn),s(oe,ve),s(ve,Ea),s(oe,Zt),s(oe,vt),s(vt,xa),s(oe,Qt),s(oe,en),s(en,za),s(oe,Pn),p(e,Os,c),p(e,Ke,c),s(Ke,ms),s(Ke,Cn),s(Ke,je),p(e,Je,c),p(e,bt,c),s(bt,sn),p(e,Tn,c),E(Ss,e,c),p(e,tn,c),E(ye,e,c),p(e,nn,c),p(e,se,c),s(se,Ca),s(se,$t),s($t,Gs),s(se,Dn),s(se,Ye),s(Ye,Mn),s(se,o),s(se,q),s(q,no),s(se,ao),s(se,An),s(An,qt),s(se,oo),s(se,Nn),s(Nn,Y),s(se,lo),s(se,Ln),s(Ln,ro),s(se,io),p(e,Ta,c),p(e,re,c),s(re,uo),s(re,On),s(On,po),s(re,co),s(re,Sn),s(Sn,mo),s(re,me),s(re,an),s(an,fo),s(re,_o),s(re,Gn),s(Gn,ho),s(re,vo),s(re,Fn),s(Fn,kt),s(re,go),p(e,Da,c),E(jt,e,c),p(e,Fs,c),E(yt,e,c),p(e,Ma,c),p(e,we,c),s(we,bo),s(we,fs),s(fs,$o),s(we,qo),s(we,Hn),s(Hn,ko),s(we,wt),s(we,In),s(In,jo),s(we,De),p(e,Aa,c),p(e,on,c),s(on,yo),p(e,Na,c),E(Hs,e,c),p(e,Is,c),p(e,_s,c),s(_s,Rs),s(Rs,Rn),E(hs,Rn,null),s(_s,wo),s(_s,Un),s(Un,cr),p(e,ql,c),p(e,Me,c),s(Me,mr),s(Me,Wo),s(Wo,fr),s(Me,_r),s(Me,Xo),s(Xo,hr),s(Me,vr),s(Me,Ko),s(Ko,gr),s(Me,br),s(Me,Jo),s(Jo,$r),s(Me,qr),p(e,kl,c),Ka[Us].m(e,c),p(e,Eo,c),p(e,Et,c),s(Et,kr),s(Et,Yo),s(Yo,jr),s(Et,yr),s(Et,Zo),s(Zo,wr),s(Et,Er),p(e,jl,c),p(e,Ae,c),s(Ae,xr),s(Ae,Qo),s(Qo,zr),s(Ae,Pr),s(Ae,el),s(el,Cr),s(Ae,Tr),s(Ae,sl),s(sl,Dr),s(Ae,Mr),s(Ae,tl),s(tl,Ar),s(Ae,Nr),p(e,yl,c),Ja[Bs].m(e,c),p(e,xo,c),p(e,zo,c),s(zo,Lr),p(e,wl,c),E(La,e,c),p(e,El,c),Ya[Xs].m(e,c),p(e,Po,c),p(e,Co,c),s(Co,Or),p(e,xl,c),ie&&ie.m(e,c),p(e,To,c),E(Vn,e,c),p(e,zl,c),p(e,Bn,c),s(Bn,Sr),s(Bn,nl),s(nl,Gr),s(Bn,Fr),p(e,Pl,c),E(Oa,e,c),p(e,Cl,c),p(e,Do,c),s(Do,Hr),p(e,Tl,c),p(e,Wn,c),s(Wn,Ir),s(Wn,al),s(al,Rr),s(Wn,Ur),p(e,Dl,c),E(Sa,e,c),p(e,Ml,c),Za[Js].m(e,c),p(e,Mo,c),E(Xn,e,c),p(e,Al,c),E(Kn,e,c),p(e,Nl,c),p(e,ln,c),s(ln,Jn),s(Jn,ol),E(Ga,ol,null),s(ln,Vr),s(ln,ll),s(ll,Br),p(e,Ll,c),p(e,Yn,c),s(Yn,Wr),s(Yn,rl),s(rl,Xr),s(Yn,Kr),p(e,Ol,c),Qa[Zs].m(e,c),p(e,Ao,c),p(e,No,c),s(No,Jr),p(e,Sl,c),E(Fa,e,c),p(e,Gl,c),E(Ha,e,c),p(e,Fl,c),p(e,xt,c),s(xt,Yr),s(xt,il),s(il,Zr),s(xt,Qr),s(xt,ul),s(ul,ei),s(xt,si),p(e,Hl,c),E(Ia,e,c),p(e,Il,c),E(Ra,e,c),p(e,Rl,c),p(e,Ne,c),s(Ne,ti),s(Ne,pl),s(pl,ni),s(Ne,ai),s(Ne,dl),s(dl,oi),s(Ne,li),s(Ne,cl),s(cl,ri),s(Ne,ii),s(Ne,ml),s(ml,ui),s(Ne,pi),p(e,Ul,c),E(Ua,e,c),p(e,Vl,c),E(Va,e,c),p(e,Bl,c),p(e,zt,c),s(zt,di),s(zt,fl),s(fl,ci),s(zt,mi),s(zt,_l),s(_l,fi),s(zt,_i),p(e,Wl,c),E(Ba,e,c),p(e,Xl,c),E(Wa,e,c),p(e,Kl,c),rn.m(e,c),p(e,Lo,c),ue&&ue.m(e,c),p(e,Oo,c),Jl=!0},p(e,[c]){const eo={};c&1&&(eo.fw=e[0]),d.$set(eo);let So=g;g=gi(e),g!==So&&(sa(),b(Xa[So],1,1,()=>{Xa[So]=null}),ea(),T=Xa[g],T||(T=Xa[g]=vi[g](e),T.c()),v(T,1),T.m(H.parentNode,H));const hl={};c&2&&(hl.$$scope={dirty:c,ctx:e}),Ue.$set(hl);const vl={};c&2&&(vl.$$scope={dirty:c,ctx:e}),Hs.$set(vl);let vs=Us;Us=$i(e),Us!==vs&&(sa(),b(Ka[vs],1,1,()=>{Ka[vs]=null}),ea(),Vs=Ka[Us],Vs||(Vs=Ka[Us]=bi[Us](e),Vs.c()),v(Vs,1),Vs.m(Eo.parentNode,Eo));let Go=Bs;Bs=ki(e),Bs!==Go&&(sa(),b(Ja[Go],1,1,()=>{Ja[Go]=null}),ea(),Ws=Ja[Bs],Ws||(Ws=Ja[Bs]=qi[Bs](e),Ws.c()),v(Ws,1),Ws.m(xo.parentNode,xo));let Fo=Xs;Xs=yi(e),Xs!==Fo&&(sa(),b(Ya[Fo],1,1,()=>{Ya[Fo]=null}),ea(),Ks=Ya[Xs],Ks||(Ks=Ya[Xs]=ji[Xs](e),Ks.c()),v(Ks,1),Ks.m(Po.parentNode,Po)),e[0]==="tf"?ie?c&1&&v(ie,1):(ie=sp(),ie.c(),v(ie,1),ie.m(To.parentNode,To)):ie&&(sa(),b(ie,1,1,()=>{ie=null}),ea());const gl={};c&2&&(gl.$$scope={dirty:c,ctx:e}),Vn.$set(gl);let ge=Js;Js=Ei(e),Js!==ge&&(sa(),b(Za[ge],1,1,()=>{Za[ge]=null}),ea(),Ys=Za[Js],Ys||(Ys=Za[Js]=wi[Js](e),Ys.c()),v(Ys,1),Ys.m(Mo.parentNode,Mo));const bl={};c&2&&(bl.$$scope={dirty:c,ctx:e}),Xn.$set(bl);const $l={};c&3&&($l.$$scope={dirty:c,ctx:e}),Kn.$set($l);let Ho=Zs;Zs=zi(e),Zs!==Ho&&(sa(),b(Qa[Ho],1,1,()=>{Qa[Ho]=null}),ea(),Qs=Qa[Zs],Qs||(Qs=Qa[Zs]=xi[Zs](e),Qs.c()),v(Qs,1),Qs.m(Ao.parentNode,Ao)),Yl!==(Yl=Pi(e))&&(rn.d(1),rn=Yl(e),rn&&(rn.c(),rn.m(Lo.parentNode,Lo))),e[0]==="pt"?ue?c&1&&v(ue,1):(ue=tp(e),ue.c(),v(ue,1),ue.m(Oo.parentNode,Oo)):ue&&(sa(),b(ue,1,1,()=>{ue=null}),ea())},i(e){Jl||(v(d.$$.fragment,e),v(C.$$.fragment,e),v(T),v(Se.$$.fragment,e),v(be.$$.fragment,e),v($e.$$.fragment,e),v(qe.$$.fragment,e),v(Pe.$$.fragment,e),v(de.$$.fragment,e),v(Ce.$$.fragment,e),v(xs.$$.fragment,e),v(rs.$$.fragment,e),v(us.$$.fragment,e),v(Ue.$$.fragment,e),v(Ps.$$.fragment,e),v(Cs.$$.fragment,e),v(Ms.$$.fragment,e),v(We.$$.fragment,e),v(Ss.$$.fragment,e),v(ye.$$.fragment,e),v(jt.$$.fragment,e),v(yt.$$.fragment,e),v(Hs.$$.fragment,e),v(hs.$$.fragment,e),v(Vs),v(Ws),v(La.$$.fragment,e),v(Ks),v(ie),v(Vn.$$.fragment,e),v(Oa.$$.fragment,e),v(Sa.$$.fragment,e),v(Ys),v(Xn.$$.fragment,e),v(Kn.$$.fragment,e),v(Ga.$$.fragment,e),v(Qs),v(Fa.$$.fragment,e),v(Ha.$$.fragment,e),v(Ia.$$.fragment,e),v(Ra.$$.fragment,e),v(Ua.$$.fragment,e),v(Va.$$.fragment,e),v(Ba.$$.fragment,e),v(Wa.$$.fragment,e),v(ue),Jl=!0)},o(e){b(d.$$.fragment,e),b(C.$$.fragment,e),b(T),b(Se.$$.fragment,e),b(be.$$.fragment,e),b($e.$$.fragment,e),b(qe.$$.fragment,e),b(Pe.$$.fragment,e),b(de.$$.fragment,e),b(Ce.$$.fragment,e),b(xs.$$.fragment,e),b(rs.$$.fragment,e),b(us.$$.fragment,e),b(Ue.$$.fragment,e),b(Ps.$$.fragment,e),b(Cs.$$.fragment,e),b(Ms.$$.fragment,e),b(We.$$.fragment,e),b(Ss.$$.fragment,e),b(ye.$$.fragment,e),b(jt.$$.fragment,e),b(yt.$$.fragment,e),b(Hs.$$.fragment,e),b(hs.$$.fragment,e),b(Vs),b(Ws),b(La.$$.fragment,e),b(Ks),b(ie),b(Vn.$$.fragment,e),b(Oa.$$.fragment,e),b(Sa.$$.fragment,e),b(Ys),b(Xn.$$.fragment,e),b(Kn.$$.fragment,e),b(Ga.$$.fragment,e),b(Qs),b(Fa.$$.fragment,e),b(Ha.$$.fragment,e),b(Ia.$$.fragment,e),b(Ra.$$.fragment,e),b(Ua.$$.fragment,e),b(Va.$$.fragment,e),b(Ba.$$.fragment,e),b(Wa.$$.fragment,e),b(ue),Jl=!1},d(e){t(l),e&&t(_),x(d,e),e&&t(h),e&&t(k),x(C),e&&t(L),Xa[g].d(e),e&&t(H),e&&t(B),e&&t(U),e&&t(V),e&&t(un),x(Se,e),e&&t(pn),e&&t(le),e&&t(at),e&&t(_e),e&&t(ot),e&&t(he),e&&t(lt),e&&t(xe),e&&t(rt),e&&t(He),x(be),e&&t(Gt),e&&t(ne),e&&t(pt),e&&t(as),e&&t(Ut),x($e,e),e&&t(Vt),e&&t(Ie),e&&t(Bt),x(qe,e),e&&t(Wt),x(Pe,e),e&&t(fn),e&&t(ws),e&&t(Xt),x(de,e),e&&t(_n),e&&t(os),e&&t(hn),x(Ce,e),e&&t(vn),x(xs,e),e&&t(Kt),e&&t(Te),e&&t(zs),e&&t(ls),e&&t(qn),x(rs,e),e&&t(is),x(us,e),e&&t(ps),x(Ue,e),e&&t(ke),e&&t(ct),e&&t(kn),x(Ps,e),e&&t(mt),x(Cs,e),e&&t(jn),e&&t(Ve),e&&t(_t),e&&t(Be),x(Ms),e&&t(yn),x(We,e),e&&t(wn),e&&t(cs),e&&t(Yt),e&&t(oe),e&&t(Os),e&&t(Ke),e&&t(Je),e&&t(bt),e&&t(Tn),x(Ss,e),e&&t(tn),x(ye,e),e&&t(nn),e&&t(se),e&&t(Ta),e&&t(re),e&&t(Da),x(jt,e),e&&t(Fs),x(yt,e),e&&t(Ma),e&&t(we),e&&t(Aa),e&&t(on),e&&t(Na),x(Hs,e),e&&t(Is),e&&t(_s),x(hs),e&&t(ql),e&&t(Me),e&&t(kl),Ka[Us].d(e),e&&t(Eo),e&&t(Et),e&&t(jl),e&&t(Ae),e&&t(yl),Ja[Bs].d(e),e&&t(xo),e&&t(zo),e&&t(wl),x(La,e),e&&t(El),Ya[Xs].d(e),e&&t(Po),e&&t(Co),e&&t(xl),ie&&ie.d(e),e&&t(To),x(Vn,e),e&&t(zl),e&&t(Bn),e&&t(Pl),x(Oa,e),e&&t(Cl),e&&t(Do),e&&t(Tl),e&&t(Wn),e&&t(Dl),x(Sa,e),e&&t(Ml),Za[Js].d(e),e&&t(Mo),x(Xn,e),e&&t(Al),x(Kn,e),e&&t(Nl),e&&t(ln),x(Ga),e&&t(Ll),e&&t(Yn),e&&t(Ol),Qa[Zs].d(e),e&&t(Ao),e&&t(No),e&&t(Sl),x(Fa,e),e&&t(Gl),x(Ha,e),e&&t(Fl),e&&t(xt),e&&t(Hl),x(Ia,e),e&&t(Il),x(Ra,e),e&&t(Rl),e&&t(Ne),e&&t(Ul),x(Ua,e),e&&t(Vl),x(Va,e),e&&t(Bl),e&&t(zt),e&&t(Wl),x(Ba,e),e&&t(Xl),x(Wa,e),e&&t(Kl),rn.d(e),e&&t(Lo),ue&&ue.d(e),e&&t(Oo)}}}const Np={local:"entraner-un-modle-de-langage-causal-partir-de-zro",sections:[{local:"collecte-des-donnes",title:"Collecte des donn\xE9es"},{local:"prparation-du-jeu-de-donnes",title:"Pr\xE9paration du jeu de donn\xE9es"},{local:"initialisation-dun-nouveau-modle",title:"Initialisation d'un nouveau mod\xE8le"},{local:"gnration-de-code-avec-un-pipeline",title:"G\xE9n\xE9ration de code avec un pipeline"},{local:"entraner-avec-accelerate",title:"Entra\xEEner avec \u{1F917} *Accelerate*"}],title:"Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"};function Lp(A,l,_){let d="pt";return ip(()=>{const h=new URLSearchParams(window.location.search);_(0,d=h.get("fw")||"pt")}),[d]}class Up extends ap{constructor(l){super();op(this,l,Lp,Ap,lp,{})}}export{Up as default,Np as metadata};
