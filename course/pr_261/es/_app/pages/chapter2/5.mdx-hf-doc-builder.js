import{S as Rt,i as Gt,s as Vt,e as u,k as b,w as j,t as d,M as Yt,c,d as t,m as h,x as v,a as m,h as f,b as w,G as l,g as i,y as q,o as $,p as de,q as g,B as y,v as Jt,n as fe}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ot}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Bt}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Os}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as E}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Ut}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Qt}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Kt(k){let n,r;return n=new Ut({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"}]}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function Wt(k){let n,r;return n=new Ut({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"}]}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function Xt(k){let n,r;return n=new Bt({props:{id:"ROxrFOEbsQE"}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function Zt(k){let n,r;return n=new Bt({props:{id:"M6adb1j2jPI"}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function el(k){let n,r,s,p;return n=new E({props:{code:`

`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
<span class="hljs-comment"># Esta l\xEDnea va a fallar:</span>
model(input_ids)`}}),s=new E({props:{code:"InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]",highlighted:'InvalidArgumentError: Input to reshape <span class="hljs-keyword">is</span> a tensor <span class="hljs-keyword">with</span> <span class="hljs-number">14</span> values, but the requested shape has <span class="hljs-number">196</span> [Op:Reshape]'}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function sl(k){let n,r,s,p;return n=new E({props:{code:`

`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
<span class="hljs-comment"># Esta l\xEDnea va a fallar:</span>
model(input_ids)`}}),s=new E({props:{code:"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",highlighted:'IndexError: Dimension out of <span class="hljs-built_in">range</span> (expected to be <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> of [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], but got <span class="hljs-number">1</span>)'}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function al(k){let n,r,s,p;return n=new E({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),s=new E({props:{code:`<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>,
        <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]], dtype=int32)&gt;`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function nl(k){let n,r,s,p;return n=new E({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),s=new E({props:{code:`tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])`,highlighted:`tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,
          <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]])`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function tl(k){let n,r;return n=new E({props:{code:`



`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function ll(k){let n,r;return n=new E({props:{code:`



`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function ol(k){let n,r;return n=new E({props:{code:`Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)`,highlighted:`Input IDs: tf.Tensor(
[[ <span class="hljs-number">1045</span>  <span class="hljs-number">1005</span>  <span class="hljs-number">2310</span>  <span class="hljs-number">2042</span>  <span class="hljs-number">3403</span>  <span class="hljs-number">2005</span>  <span class="hljs-number">1037</span> <span class="hljs-number">17662</span> <span class="hljs-number">12172</span>  <span class="hljs-number">2607</span>  <span class="hljs-number">2026</span>  <span class="hljs-number">2878</span>
   <span class="hljs-number">2166</span>  <span class="hljs-number">1012</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), dtype=int32)
Logits: tf.Tensor([[-<span class="hljs-number">2.7276208</span>  <span class="hljs-number">2.8789377</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function rl(k){let n,r;return n=new E({props:{code:`Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]`,highlighted:`Input IDs: [[ <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>]]
Logits: [[-<span class="hljs-number">2.7276</span>,  <span class="hljs-number">2.8789</span>]]`}}),{c(){j(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,p){q(n,s,p),r=!0},i(s){r||(g(n.$$.fragment,s),r=!0)},o(s){$(n.$$.fragment,s),r=!1},d(s){y(n,s)}}}function il(k){let n,r,s,p,a,_,z,A;return{c(){n=u("p"),r=d("\u270F\uFE0F "),s=u("strong"),p=d("Try it out!"),a=d(" Convierte esta lista "),_=u("code"),z=d("batched_ids"),A=d(" en un tensor y p\xE1salo por tu modelo. Comprueba que obtienes los mismos logits que antes (\xA1pero dos veces!).")},l(W){n=c(W,"P",{});var P=m(n);r=f(P,"\u270F\uFE0F "),s=c(P,"STRONG",{});var Ne=m(s);p=f(Ne,"Try it out!"),Ne.forEach(t),a=f(P," Convierte esta lista "),_=c(P,"CODE",{});var be=m(_);z=f(be,"batched_ids"),be.forEach(t),A=f(P," en un tensor y p\xE1salo por tu modelo. Comprueba que obtienes los mismos logits que antes (\xA1pero dos veces!)."),P.forEach(t)},m(W,P){i(W,n,P),l(n,r),l(n,s),l(s,p),l(n,a),l(n,_),l(_,z),l(n,A)},d(W){W&&t(n)}}}function pl(k){let n,r,s,p;return n=new E({props:{code:`
`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(tf.constant(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(batched_ids)).logits)`}}),s=new E({props:{code:`tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor([[ <span class="hljs-number">1.5693678</span> -<span class="hljs-number">1.3894581</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor([[ <span class="hljs-number">0.5803005</span>  -<span class="hljs-number">0.41252428</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor(
[[ <span class="hljs-number">1.5693681</span> -<span class="hljs-number">1.3894582</span>]
 [ <span class="hljs-number">1.3373486</span> -<span class="hljs-number">1.2163193</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function ul(k){let n,r,s,p;return n=new E({props:{code:`
`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)`}}),s=new E({props:{code:`tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">1.3373</span>, -<span class="hljs-number">1.2163</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function cl(k){let n,r,s,p;return n=new E({props:{code:`
`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),s=new E({props:{code:`tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[ <span class="hljs-number">1.5693681</span>  -<span class="hljs-number">1.3894582</span> ]
 [ <span class="hljs-number">0.5803021</span>  -<span class="hljs-number">0.41252586</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function ml(k){let n,r,s,p;return n=new E({props:{code:`
`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),s=new E({props:{code:`tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){j(n.$$.fragment),r=b(),j(s.$$.fragment)},l(a){v(n.$$.fragment,a),r=h(a),v(s.$$.fragment,a)},m(a,_){q(n,a,_),i(a,r,_),q(s,a,_),p=!0},i(a){p||(g(n.$$.fragment,a),g(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){y(n,a),a&&t(r),y(s,a)}}}function dl(k){let n,r,s,p,a;return{c(){n=u("p"),r=d("\u270F\uFE0F "),s=u("strong"),p=d("Try it out!"),a=d(" Aplique la tokenizaci\xF3n manualmente a las dos frases utilizadas en la secci\xF3n 2 (\u201CLlevo toda la vida esperando un curso de HuggingFace\u201D y \u201C\xA1Odio tanto esto!\u201D). P\xE1selas por el modelo y compruebe que obtiene los mismos logits que en la secci\xF3n 2. Ahora j\xFAntalos usando el token de relleno, y luego crea la m\xE1scara de atenci\xF3n adecuada. Comprueba que obtienes los mismos resultados al pasarlos por el modelo.")},l(_){n=c(_,"P",{});var z=m(n);r=f(z,"\u270F\uFE0F "),s=c(z,"STRONG",{});var A=m(s);p=f(A,"Try it out!"),A.forEach(t),a=f(z," Aplique la tokenizaci\xF3n manualmente a las dos frases utilizadas en la secci\xF3n 2 (\u201CLlevo toda la vida esperando un curso de HuggingFace\u201D y \u201C\xA1Odio tanto esto!\u201D). P\xE1selas por el modelo y compruebe que obtiene los mismos logits que en la secci\xF3n 2. Ahora j\xFAntalos usando el token de relleno, y luego crea la m\xE1scara de atenci\xF3n adecuada. Comprueba que obtienes los mismos resultados al pasarlos por el modelo."),z.forEach(t)},m(_,z){i(_,n,z),l(n,r),l(n,s),l(s,p),l(n,a)},d(_){_&&t(n)}}}function fl(k){let n,r,s,p,a,_,z,A,W,P,Ne,be,T,C,He,L,M,Oe,Be,Ia,Bs,I,hs,Sa,Ta,he,Ca,_s,La,Ma,Da,$s,xa,Fa,gs,Na,Us,Ue,Ha,Rs,X,ae,ks,_e,Oa,js,Ba,Gs,Re,Ua,Vs,D,x,Ge,Ve,Ra,Ys,ne,Ga,vs,Va,Ya,Js,F,N,Ye,Je,Ja,Qs,H,O,Qe,Ke,Qa,Ks,B,U,We,$e,qs,Ka,Wa,Ws,ge,Xs,Xe,Xa,Zs,te,ea,le,Za,ys,en,sn,sa,Z,oe,ws,ke,an,Es,nn,aa,Ze,tn,na,je,ta,J,ln,zs,on,rn,As,pn,un,la,ve,oa,Q,cn,Ps,mn,dn,Is,fn,bn,ra,R,G,es,ss,hn,ia,re,_n,Ss,$n,gn,pa,ee,ie,Ts,qe,kn,Cs,jn,ua,ye,Ls,vn,qn,ca,V,Y,as,ns,yn,ma,ts,wn,da,pe,fa,se,ue,Ms,we,En,Ds,zn,ba,ls,An,ha,ce,xs,Pn,In,Fs,Sn,_a,K,Tn,Ee,Cn,Ln,ze,Mn,Dn,$a,me,xn,Ns,Fn,Nn,ga,Ae,ka;s=new Qt({props:{fw:k[0]}}),A=new Os({});const Bn=[Wt,Kt],Pe=[];function Un(e,o){return e[0]==="pt"?0:1}T=Un(k),C=Pe[T]=Bn[T](k);const Rn=[Zt,Xt],Ie=[];function Gn(e,o){return e[0]==="pt"?0:1}L=Gn(k),M=Ie[L]=Rn[L](k),_e=new Os({});const Vn=[sl,el],Se=[];function Yn(e,o){return e[0]==="pt"?0:1}D=Yn(k),x=Se[D]=Vn[D](k);const Jn=[nl,al],Te=[];function Qn(e,o){return e[0]==="pt"?0:1}F=Qn(k),N=Te[F]=Jn[F](k);const Kn=[ll,tl],Ce=[];function Wn(e,o){return e[0]==="pt"?0:1}H=Wn(k),O=Ce[H]=Kn[H](k);const Xn=[rl,ol],Le=[];function Zn(e,o){return e[0]==="pt"?0:1}B=Zn(k),U=Le[B]=Xn[B](k),ge=new E({props:{code:"batched_ids = [ids, ids]",highlighted:'<span class="hljs-attr">batched_ids</span> = [ids, ids]'}}),te=new Ot({props:{$$slots:{default:[il]},$$scope:{ctx:k}}}),ke=new Os({}),je=new E({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200]
]`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]
]`}}),ve=new E({props:{code:"",highlighted:`padding_id = <span class="hljs-number">100</span>

batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, padding_id],
]`}});const et=[ul,pl],Me=[];function st(e,o){return e[0]==="pt"?0:1}R=st(k),G=Me[R]=et[R](k),qe=new Os({});const at=[ml,cl],De=[];function nt(e,o){return e[0]==="pt"?0:1}return V=nt(k),Y=De[V]=at[V](k),pe=new Ot({props:{$$slots:{default:[dl]},$$scope:{ctx:k}}}),we=new Os({}),Ae=new E({props:{code:"sequence = sequence[:max_sequence_length]",highlighted:"sequence = sequence[:max_sequence_length]"}}),{c(){n=u("meta"),r=b(),j(s.$$.fragment),p=b(),a=u("h1"),_=u("a"),z=u("span"),j(A.$$.fragment),W=b(),P=u("span"),Ne=d("Manejando Secuencias M\xFAltiples"),be=b(),C.c(),He=b(),M.c(),Oe=b(),Be=u("p"),Ia=d("En la secci\xF3n anterior, hemos explorado el caso de uso m\xE1s sencillo: hacer inferencia sobre una \xFAnica secuencia de poca longitud. Sin embargo, surgen algunas preguntas:"),Bs=b(),I=u("ul"),hs=u("li"),Sa=d("\xBFC\xF3mo manejamos las secuencias m\xFAltiples?"),Ta=b(),he=u("li"),Ca=d("\xBFC\xF3mo manejamos las secuencias m\xFAltiples "),_s=u("em"),La=d("de diferentes longitudes"),Ma=d("?"),Da=b(),$s=u("li"),xa=d("\xBFSon los \xEDndices de vocabulario las \xFAnicas entradas que permiten que un modelo funcione bien?"),Fa=b(),gs=u("li"),Na=d("\xBFExiste una secuencia demasiado larga?"),Us=b(),Ue=u("p"),Ha=d("Veamos qu\xE9 tipo de problemas plantean estas preguntas, y c\xF3mo podemos resolverlos utilizando la API de Transformers \u{1F917}."),Rs=b(),X=u("h2"),ae=u("a"),ks=u("span"),j(_e.$$.fragment),Oa=b(),js=u("span"),Ba=d("Los modelos esperan Baches de entrada"),Gs=b(),Re=u("p"),Ua=d("En el ejercicio anterior has visto c\xF3mo las secuencias se traducen en listas de n\xFAmeros. Convirtamos esta lista de n\xFAmeros en un tensor y envi\xE9moslo al modelo:"),Vs=b(),x.c(),Ge=b(),Ve=u("p"),Ra=d("\xA1Oh, no! \xBFPor qu\xE9 ha fallado esto? \u201CHemos seguido los pasos de la tuber\xEDa en la secci\xF3n 2."),Ys=b(),ne=u("p"),Ga=d("El problema es que enviamos una sola secuencia al modelo, mientras que los modelos de \u{1F917} Transformers esperan m\xFAltiples frases por defecto. Aqu\xED tratamos de hacer todo lo que el tokenizador hizo detr\xE1s de escena cuando lo aplicamos a una "),vs=u("code"),Va=d("secuencia"),Ya=d(", pero si te fijas bien, ver\xE1s que no s\xF3lo convirti\xF3 la lista de IDs de entrada en un tensor, sino que le agreg\xF3 una dimensi\xF3n encima:"),Js=b(),N.c(),Ye=b(),Je=u("p"),Ja=d("Intent\xE9moslo de nuevo y a\xF1adamos una nueva dimensi\xF3n encima:"),Qs=b(),O.c(),Qe=b(),Ke=u("p"),Qa=d("Imprimimos los IDs de entrada as\xED como los logits resultantes - aqu\xED est\xE1 la salida:"),Ks=b(),U.c(),We=b(),$e=u("p"),qs=u("em"),Ka=d("El \u201Cbatching\u201D"),Wa=d(" es el acto de enviar varias frases a trav\xE9s del modelo, todas a la vez. Si s\xF3lo tienes una frase, puedes construir un lote con una sola secuencia:"),Ws=b(),j(ge.$$.fragment),Xs=b(),Xe=u("p"),Xa=d("Se trata de un lote de dos secuencias id\xE9nticas."),Zs=b(),j(te.$$.fragment),ea=b(),le=u("p"),Za=d("La creaci\xF3n de lotes permite que el modelo funcione cuando lo alimentas con m\xFAltiples sentencias. Utilizar varias secuencias es tan sencillo como crear un lote con una sola secuencia. Sin embargo, hay un segundo problema. Cuando se trata de agrupar dos (o m\xE1s) frases, \xE9stas pueden ser de diferente longitud. Si alguna vez ha trabajado con tensores, sabr\xE1 que deben tener forma rectangular, por lo que no podr\xE1 convertir la lista de IDs de entrada en un tensor directamente. Para evitar este problema, usamos el "),ys=u("em"),en=d("padding"),sn=d(" para las entradas."),sa=b(),Z=u("h2"),oe=u("a"),ws=u("span"),j(ke.$$.fragment),an=b(),Es=u("span"),nn=d("Padding a las entradas"),aa=b(),Ze=u("p"),tn=d("La siguiente lista de listas no se puede convertir en un tensor:"),na=b(),j(je.$$.fragment),ta=b(),J=u("p"),ln=d("Para solucionar esto, utilizaremos "),zs=u("em"),on=d("padding"),rn=d(" para que nuestros tensores tengan una forma rectangular. El acolchado asegura que todas nuestras sentencias tengan la misma longitud a\xF1adiendo una palabra especial llamada "),As=u("em"),pn=d("padding token"),un=d(" a las sentencias con menos valores. Por ejemplo, si tienes 10 frases con 10 palabras y 1 frase con 20 palabras, el relleno asegurar\xE1 que todas las frases tengan 20 palabras. En nuestro ejemplo, el tensor resultante tiene este aspecto:"),la=b(),j(ve.$$.fragment),oa=b(),Q=u("p"),cn=d("El ID del "),Ps=u("em"),mn=d("padding token"),dn=d(" se puede encontrar en "),Is=u("code"),fn=d("tokenizer.pad_token_id"),bn=d(". Us\xE9moslo y enviemos nuestras dos sentencias a trav\xE9s del modelo de forma individual y por lotes:"),ra=b(),G.c(),es=b(),ss=u("p"),hn=d("Hay un problema con los logits en nuestras predicciones por lotes: la segunda fila deber\xEDa ser la misma que los logits de la segunda frase, \xA1pero tenemos valores completamente diferentes!"),ia=b(),re=u("p"),_n=d("Esto se debe a que la caracter\xEDstica clave de los modelos Transformer son las capas de atenci\xF3n que "),Ss=u("em"),$n=d("contextualizan"),gn=d(" cada token. \xC9stas tendr\xE1n en cuenta los tokens de relleno, ya que atienden a todos los tokens de una secuencia. Para obtener el mismo resultado al pasar oraciones individuales de diferente longitud por el modelo o al pasar un lote con las mismas oraciones y el padding aplicado, tenemos que decirles a esas capas de atenci\xF3n que ignoren los tokens de padding. Esto se hace utilizando una m\xE1scara de atenci\xF3n."),pa=b(),ee=u("h2"),ie=u("a"),Ts=u("span"),j(qe.$$.fragment),kn=b(),Cs=u("span"),jn=d("M\xE1scaras de atenci\xF3n"),ua=b(),ye=u("p"),Ls=u("em"),vn=d("Las m\xE1scaras de atenci\xF3n"),qn=d(" son tensores con la misma forma que el tensor de IDs de entrada, rellenados con 0s y 1s: los 1s indican que los tokens correspondientes deben ser atendidos, y los 0s indican que los tokens correspondientes no deben ser atendidos (es decir, deben ser ignorados por las capas de atenci\xF3n del modelo)."),ca=b(),Y.c(),as=b(),ns=u("p"),yn=d("Ahora obtenemos los mismos logits para la segunda frase del lote."),ma=b(),ts=u("p"),wn=d("Podemos ver que el \xFAltimo valor de la segunda secuencia es un ID de relleno, que es un valor 0 en la m\xE1scara de atenci\xF3n."),da=b(),j(pe.$$.fragment),fa=b(),se=u("h2"),ue=u("a"),Ms=u("span"),j(we.$$.fragment),En=b(),Ds=u("span"),zn=d("Secuencias largas"),ba=b(),ls=u("p"),An=d("Con los modelos Transformer, hay un l\xEDmite en la longitud de las secuencias que podemos pasar a los modelos. La mayor\xEDa de los modelos manejan secuencias de hasta 512 o 1024 tokens, y se bloquean cuando se les pide que procesen secuencias m\xE1s largas. Hay dos soluciones a este problema:"),ha=b(),ce=u("ul"),xs=u("li"),Pn=d("Usar un modelo que soporte secuencias largas"),In=b(),Fs=u("li"),Sn=d("Truncar tus secuencias"),_a=b(),K=u("p"),Tn=d("Los modelos tienen diferentes longitudes de secuencia soportadas, y algunos se especializan en el manejo de secuencias muy largas. Un ejemplo es "),Ee=u("a"),Cn=d("Longformer"),Ln=d(" y otro es "),ze=u("a"),Mn=d("LED"),Dn=d(". Si est\xE1s trabajando en una tarea que requiere secuencias muy largas, te recomendamos que eches un vistazo a esos modelos."),$a=b(),me=u("p"),xn=d("En caso contrario, le recomendamos que trunque sus secuencias especificando el par\xE1metro "),Ns=u("code"),Fn=d("max_sequence_length"),Nn=d(":"),ga=b(),j(Ae.$$.fragment),this.h()},l(e){const o=Yt('[data-svelte="svelte-1phssyn"]',document.head);n=c(o,"META",{name:!0,content:!0}),o.forEach(t),r=h(e),v(s.$$.fragment,e),p=h(e),a=c(e,"H1",{class:!0});var xe=m(a);_=c(xe,"A",{id:!0,class:!0,href:!0});var os=m(_);z=c(os,"SPAN",{});var rs=m(z);v(A.$$.fragment,rs),rs.forEach(t),os.forEach(t),W=h(xe),P=c(xe,"SPAN",{});var is=m(P);Ne=f(is,"Manejando Secuencias M\xFAltiples"),is.forEach(t),xe.forEach(t),be=h(e),C.l(e),He=h(e),M.l(e),Oe=h(e),Be=c(e,"P",{});var ps=m(Be);Ia=f(ps,"En la secci\xF3n anterior, hemos explorado el caso de uso m\xE1s sencillo: hacer inferencia sobre una \xFAnica secuencia de poca longitud. Sin embargo, surgen algunas preguntas:"),ps.forEach(t),Bs=h(e),I=c(e,"UL",{});var S=m(I);hs=c(S,"LI",{});var us=m(hs);Sa=f(us,"\xBFC\xF3mo manejamos las secuencias m\xFAltiples?"),us.forEach(t),Ta=h(S),he=c(S,"LI",{});var Fe=m(he);Ca=f(Fe,"\xBFC\xF3mo manejamos las secuencias m\xFAltiples "),_s=c(Fe,"EM",{});var cs=m(_s);La=f(cs,"de diferentes longitudes"),cs.forEach(t),Ma=f(Fe,"?"),Fe.forEach(t),Da=h(S),$s=c(S,"LI",{});var ms=m($s);xa=f(ms,"\xBFSon los \xEDndices de vocabulario las \xFAnicas entradas que permiten que un modelo funcione bien?"),ms.forEach(t),Fa=h(S),gs=c(S,"LI",{});var Hs=m(gs);Na=f(Hs,"\xBFExiste una secuencia demasiado larga?"),Hs.forEach(t),S.forEach(t),Us=h(e),Ue=c(e,"P",{});var tt=m(Ue);Ha=f(tt,"Veamos qu\xE9 tipo de problemas plantean estas preguntas, y c\xF3mo podemos resolverlos utilizando la API de Transformers \u{1F917}."),tt.forEach(t),Rs=h(e),X=c(e,"H2",{class:!0});var ja=m(X);ae=c(ja,"A",{id:!0,class:!0,href:!0});var lt=m(ae);ks=c(lt,"SPAN",{});var ot=m(ks);v(_e.$$.fragment,ot),ot.forEach(t),lt.forEach(t),Oa=h(ja),js=c(ja,"SPAN",{});var rt=m(js);Ba=f(rt,"Los modelos esperan Baches de entrada"),rt.forEach(t),ja.forEach(t),Gs=h(e),Re=c(e,"P",{});var it=m(Re);Ua=f(it,"En el ejercicio anterior has visto c\xF3mo las secuencias se traducen en listas de n\xFAmeros. Convirtamos esta lista de n\xFAmeros en un tensor y envi\xE9moslo al modelo:"),it.forEach(t),Vs=h(e),x.l(e),Ge=h(e),Ve=c(e,"P",{});var pt=m(Ve);Ra=f(pt,"\xA1Oh, no! \xBFPor qu\xE9 ha fallado esto? \u201CHemos seguido los pasos de la tuber\xEDa en la secci\xF3n 2."),pt.forEach(t),Ys=h(e),ne=c(e,"P",{});var va=m(ne);Ga=f(va,"El problema es que enviamos una sola secuencia al modelo, mientras que los modelos de \u{1F917} Transformers esperan m\xFAltiples frases por defecto. Aqu\xED tratamos de hacer todo lo que el tokenizador hizo detr\xE1s de escena cuando lo aplicamos a una "),vs=c(va,"CODE",{});var ut=m(vs);Va=f(ut,"secuencia"),ut.forEach(t),Ya=f(va,", pero si te fijas bien, ver\xE1s que no s\xF3lo convirti\xF3 la lista de IDs de entrada en un tensor, sino que le agreg\xF3 una dimensi\xF3n encima:"),va.forEach(t),Js=h(e),N.l(e),Ye=h(e),Je=c(e,"P",{});var ct=m(Je);Ja=f(ct,"Intent\xE9moslo de nuevo y a\xF1adamos una nueva dimensi\xF3n encima:"),ct.forEach(t),Qs=h(e),O.l(e),Qe=h(e),Ke=c(e,"P",{});var mt=m(Ke);Qa=f(mt,"Imprimimos los IDs de entrada as\xED como los logits resultantes - aqu\xED est\xE1 la salida:"),mt.forEach(t),Ks=h(e),U.l(e),We=h(e),$e=c(e,"P",{});var Hn=m($e);qs=c(Hn,"EM",{});var dt=m(qs);Ka=f(dt,"El \u201Cbatching\u201D"),dt.forEach(t),Wa=f(Hn," es el acto de enviar varias frases a trav\xE9s del modelo, todas a la vez. Si s\xF3lo tienes una frase, puedes construir un lote con una sola secuencia:"),Hn.forEach(t),Ws=h(e),v(ge.$$.fragment,e),Xs=h(e),Xe=c(e,"P",{});var ft=m(Xe);Xa=f(ft,"Se trata de un lote de dos secuencias id\xE9nticas."),ft.forEach(t),Zs=h(e),v(te.$$.fragment,e),ea=h(e),le=c(e,"P",{});var qa=m(le);Za=f(qa,"La creaci\xF3n de lotes permite que el modelo funcione cuando lo alimentas con m\xFAltiples sentencias. Utilizar varias secuencias es tan sencillo como crear un lote con una sola secuencia. Sin embargo, hay un segundo problema. Cuando se trata de agrupar dos (o m\xE1s) frases, \xE9stas pueden ser de diferente longitud. Si alguna vez ha trabajado con tensores, sabr\xE1 que deben tener forma rectangular, por lo que no podr\xE1 convertir la lista de IDs de entrada en un tensor directamente. Para evitar este problema, usamos el "),ys=c(qa,"EM",{});var bt=m(ys);en=f(bt,"padding"),bt.forEach(t),sn=f(qa," para las entradas."),qa.forEach(t),sa=h(e),Z=c(e,"H2",{class:!0});var ya=m(Z);oe=c(ya,"A",{id:!0,class:!0,href:!0});var ht=m(oe);ws=c(ht,"SPAN",{});var _t=m(ws);v(ke.$$.fragment,_t),_t.forEach(t),ht.forEach(t),an=h(ya),Es=c(ya,"SPAN",{});var $t=m(Es);nn=f($t,"Padding a las entradas"),$t.forEach(t),ya.forEach(t),aa=h(e),Ze=c(e,"P",{});var gt=m(Ze);tn=f(gt,"La siguiente lista de listas no se puede convertir en un tensor:"),gt.forEach(t),na=h(e),v(je.$$.fragment,e),ta=h(e),J=c(e,"P",{});var ds=m(J);ln=f(ds,"Para solucionar esto, utilizaremos "),zs=c(ds,"EM",{});var kt=m(zs);on=f(kt,"padding"),kt.forEach(t),rn=f(ds," para que nuestros tensores tengan una forma rectangular. El acolchado asegura que todas nuestras sentencias tengan la misma longitud a\xF1adiendo una palabra especial llamada "),As=c(ds,"EM",{});var jt=m(As);pn=f(jt,"padding token"),jt.forEach(t),un=f(ds," a las sentencias con menos valores. Por ejemplo, si tienes 10 frases con 10 palabras y 1 frase con 20 palabras, el relleno asegurar\xE1 que todas las frases tengan 20 palabras. En nuestro ejemplo, el tensor resultante tiene este aspecto:"),ds.forEach(t),la=h(e),v(ve.$$.fragment,e),oa=h(e),Q=c(e,"P",{});var fs=m(Q);cn=f(fs,"El ID del "),Ps=c(fs,"EM",{});var vt=m(Ps);mn=f(vt,"padding token"),vt.forEach(t),dn=f(fs," se puede encontrar en "),Is=c(fs,"CODE",{});var qt=m(Is);fn=f(qt,"tokenizer.pad_token_id"),qt.forEach(t),bn=f(fs,". Us\xE9moslo y enviemos nuestras dos sentencias a trav\xE9s del modelo de forma individual y por lotes:"),fs.forEach(t),ra=h(e),G.l(e),es=h(e),ss=c(e,"P",{});var yt=m(ss);hn=f(yt,"Hay un problema con los logits en nuestras predicciones por lotes: la segunda fila deber\xEDa ser la misma que los logits de la segunda frase, \xA1pero tenemos valores completamente diferentes!"),yt.forEach(t),ia=h(e),re=c(e,"P",{});var wa=m(re);_n=f(wa,"Esto se debe a que la caracter\xEDstica clave de los modelos Transformer son las capas de atenci\xF3n que "),Ss=c(wa,"EM",{});var wt=m(Ss);$n=f(wt,"contextualizan"),wt.forEach(t),gn=f(wa," cada token. \xC9stas tendr\xE1n en cuenta los tokens de relleno, ya que atienden a todos los tokens de una secuencia. Para obtener el mismo resultado al pasar oraciones individuales de diferente longitud por el modelo o al pasar un lote con las mismas oraciones y el padding aplicado, tenemos que decirles a esas capas de atenci\xF3n que ignoren los tokens de padding. Esto se hace utilizando una m\xE1scara de atenci\xF3n."),wa.forEach(t),pa=h(e),ee=c(e,"H2",{class:!0});var Ea=m(ee);ie=c(Ea,"A",{id:!0,class:!0,href:!0});var Et=m(ie);Ts=c(Et,"SPAN",{});var zt=m(Ts);v(qe.$$.fragment,zt),zt.forEach(t),Et.forEach(t),kn=h(Ea),Cs=c(Ea,"SPAN",{});var At=m(Cs);jn=f(At,"M\xE1scaras de atenci\xF3n"),At.forEach(t),Ea.forEach(t),ua=h(e),ye=c(e,"P",{});var On=m(ye);Ls=c(On,"EM",{});var Pt=m(Ls);vn=f(Pt,"Las m\xE1scaras de atenci\xF3n"),Pt.forEach(t),qn=f(On," son tensores con la misma forma que el tensor de IDs de entrada, rellenados con 0s y 1s: los 1s indican que los tokens correspondientes deben ser atendidos, y los 0s indican que los tokens correspondientes no deben ser atendidos (es decir, deben ser ignorados por las capas de atenci\xF3n del modelo)."),On.forEach(t),ca=h(e),Y.l(e),as=h(e),ns=c(e,"P",{});var It=m(ns);yn=f(It,"Ahora obtenemos los mismos logits para la segunda frase del lote."),It.forEach(t),ma=h(e),ts=c(e,"P",{});var St=m(ts);wn=f(St,"Podemos ver que el \xFAltimo valor de la segunda secuencia es un ID de relleno, que es un valor 0 en la m\xE1scara de atenci\xF3n."),St.forEach(t),da=h(e),v(pe.$$.fragment,e),fa=h(e),se=c(e,"H2",{class:!0});var za=m(se);ue=c(za,"A",{id:!0,class:!0,href:!0});var Tt=m(ue);Ms=c(Tt,"SPAN",{});var Ct=m(Ms);v(we.$$.fragment,Ct),Ct.forEach(t),Tt.forEach(t),En=h(za),Ds=c(za,"SPAN",{});var Lt=m(Ds);zn=f(Lt,"Secuencias largas"),Lt.forEach(t),za.forEach(t),ba=h(e),ls=c(e,"P",{});var Mt=m(ls);An=f(Mt,"Con los modelos Transformer, hay un l\xEDmite en la longitud de las secuencias que podemos pasar a los modelos. La mayor\xEDa de los modelos manejan secuencias de hasta 512 o 1024 tokens, y se bloquean cuando se les pide que procesen secuencias m\xE1s largas. Hay dos soluciones a este problema:"),Mt.forEach(t),ha=h(e),ce=c(e,"UL",{});var Aa=m(ce);xs=c(Aa,"LI",{});var Dt=m(xs);Pn=f(Dt,"Usar un modelo que soporte secuencias largas"),Dt.forEach(t),In=h(Aa),Fs=c(Aa,"LI",{});var xt=m(Fs);Sn=f(xt,"Truncar tus secuencias"),xt.forEach(t),Aa.forEach(t),_a=h(e),K=c(e,"P",{});var bs=m(K);Tn=f(bs,"Los modelos tienen diferentes longitudes de secuencia soportadas, y algunos se especializan en el manejo de secuencias muy largas. Un ejemplo es "),Ee=c(bs,"A",{href:!0,rel:!0});var Ft=m(Ee);Cn=f(Ft,"Longformer"),Ft.forEach(t),Ln=f(bs," y otro es "),ze=c(bs,"A",{href:!0,rel:!0});var Nt=m(ze);Mn=f(Nt,"LED"),Nt.forEach(t),Dn=f(bs,". Si est\xE1s trabajando en una tarea que requiere secuencias muy largas, te recomendamos que eches un vistazo a esos modelos."),bs.forEach(t),$a=h(e),me=c(e,"P",{});var Pa=m(me);xn=f(Pa,"En caso contrario, le recomendamos que trunque sus secuencias especificando el par\xE1metro "),Ns=c(Pa,"CODE",{});var Ht=m(Ns);Fn=f(Ht,"max_sequence_length"),Ht.forEach(t),Nn=f(Pa,":"),Pa.forEach(t),ga=h(e),v(Ae.$$.fragment,e),this.h()},h(){w(n,"name","hf:doc:metadata"),w(n,"content",JSON.stringify(bl)),w(_,"id","manejando-secuencias-mltiples"),w(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(_,"href","#manejando-secuencias-mltiples"),w(a,"class","relative group"),w(ae,"id","los-modelos-esperan-baches-de-entrada"),w(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(ae,"href","#los-modelos-esperan-baches-de-entrada"),w(X,"class","relative group"),w(oe,"id","padding-a-las-entradas"),w(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(oe,"href","#padding-a-las-entradas"),w(Z,"class","relative group"),w(ie,"id","mscaras-de-atencin"),w(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(ie,"href","#mscaras-de-atencin"),w(ee,"class","relative group"),w(ue,"id","secuencias-largas"),w(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(ue,"href","#secuencias-largas"),w(se,"class","relative group"),w(Ee,"href","https://huggingface.co/transformers/model_doc/longformer.html"),w(Ee,"rel","nofollow"),w(ze,"href","https://huggingface.co/transformers/model_doc/led.html"),w(ze,"rel","nofollow")},m(e,o){l(document.head,n),i(e,r,o),q(s,e,o),i(e,p,o),i(e,a,o),l(a,_),l(_,z),q(A,z,null),l(a,W),l(a,P),l(P,Ne),i(e,be,o),Pe[T].m(e,o),i(e,He,o),Ie[L].m(e,o),i(e,Oe,o),i(e,Be,o),l(Be,Ia),i(e,Bs,o),i(e,I,o),l(I,hs),l(hs,Sa),l(I,Ta),l(I,he),l(he,Ca),l(he,_s),l(_s,La),l(he,Ma),l(I,Da),l(I,$s),l($s,xa),l(I,Fa),l(I,gs),l(gs,Na),i(e,Us,o),i(e,Ue,o),l(Ue,Ha),i(e,Rs,o),i(e,X,o),l(X,ae),l(ae,ks),q(_e,ks,null),l(X,Oa),l(X,js),l(js,Ba),i(e,Gs,o),i(e,Re,o),l(Re,Ua),i(e,Vs,o),Se[D].m(e,o),i(e,Ge,o),i(e,Ve,o),l(Ve,Ra),i(e,Ys,o),i(e,ne,o),l(ne,Ga),l(ne,vs),l(vs,Va),l(ne,Ya),i(e,Js,o),Te[F].m(e,o),i(e,Ye,o),i(e,Je,o),l(Je,Ja),i(e,Qs,o),Ce[H].m(e,o),i(e,Qe,o),i(e,Ke,o),l(Ke,Qa),i(e,Ks,o),Le[B].m(e,o),i(e,We,o),i(e,$e,o),l($e,qs),l(qs,Ka),l($e,Wa),i(e,Ws,o),q(ge,e,o),i(e,Xs,o),i(e,Xe,o),l(Xe,Xa),i(e,Zs,o),q(te,e,o),i(e,ea,o),i(e,le,o),l(le,Za),l(le,ys),l(ys,en),l(le,sn),i(e,sa,o),i(e,Z,o),l(Z,oe),l(oe,ws),q(ke,ws,null),l(Z,an),l(Z,Es),l(Es,nn),i(e,aa,o),i(e,Ze,o),l(Ze,tn),i(e,na,o),q(je,e,o),i(e,ta,o),i(e,J,o),l(J,ln),l(J,zs),l(zs,on),l(J,rn),l(J,As),l(As,pn),l(J,un),i(e,la,o),q(ve,e,o),i(e,oa,o),i(e,Q,o),l(Q,cn),l(Q,Ps),l(Ps,mn),l(Q,dn),l(Q,Is),l(Is,fn),l(Q,bn),i(e,ra,o),Me[R].m(e,o),i(e,es,o),i(e,ss,o),l(ss,hn),i(e,ia,o),i(e,re,o),l(re,_n),l(re,Ss),l(Ss,$n),l(re,gn),i(e,pa,o),i(e,ee,o),l(ee,ie),l(ie,Ts),q(qe,Ts,null),l(ee,kn),l(ee,Cs),l(Cs,jn),i(e,ua,o),i(e,ye,o),l(ye,Ls),l(Ls,vn),l(ye,qn),i(e,ca,o),De[V].m(e,o),i(e,as,o),i(e,ns,o),l(ns,yn),i(e,ma,o),i(e,ts,o),l(ts,wn),i(e,da,o),q(pe,e,o),i(e,fa,o),i(e,se,o),l(se,ue),l(ue,Ms),q(we,Ms,null),l(se,En),l(se,Ds),l(Ds,zn),i(e,ba,o),i(e,ls,o),l(ls,An),i(e,ha,o),i(e,ce,o),l(ce,xs),l(xs,Pn),l(ce,In),l(ce,Fs),l(Fs,Sn),i(e,_a,o),i(e,K,o),l(K,Tn),l(K,Ee),l(Ee,Cn),l(K,Ln),l(K,ze),l(ze,Mn),l(K,Dn),i(e,$a,o),i(e,me,o),l(me,xn),l(me,Ns),l(Ns,Fn),l(me,Nn),i(e,ga,o),q(Ae,e,o),ka=!0},p(e,[o]){const xe={};o&1&&(xe.fw=e[0]),s.$set(xe);let os=T;T=Un(e),T!==os&&(fe(),$(Pe[os],1,1,()=>{Pe[os]=null}),de(),C=Pe[T],C||(C=Pe[T]=Bn[T](e),C.c()),g(C,1),C.m(He.parentNode,He));let rs=L;L=Gn(e),L!==rs&&(fe(),$(Ie[rs],1,1,()=>{Ie[rs]=null}),de(),M=Ie[L],M||(M=Ie[L]=Rn[L](e),M.c()),g(M,1),M.m(Oe.parentNode,Oe));let is=D;D=Yn(e),D!==is&&(fe(),$(Se[is],1,1,()=>{Se[is]=null}),de(),x=Se[D],x||(x=Se[D]=Vn[D](e),x.c()),g(x,1),x.m(Ge.parentNode,Ge));let ps=F;F=Qn(e),F!==ps&&(fe(),$(Te[ps],1,1,()=>{Te[ps]=null}),de(),N=Te[F],N||(N=Te[F]=Jn[F](e),N.c()),g(N,1),N.m(Ye.parentNode,Ye));let S=H;H=Wn(e),H!==S&&(fe(),$(Ce[S],1,1,()=>{Ce[S]=null}),de(),O=Ce[H],O||(O=Ce[H]=Kn[H](e),O.c()),g(O,1),O.m(Qe.parentNode,Qe));let us=B;B=Zn(e),B!==us&&(fe(),$(Le[us],1,1,()=>{Le[us]=null}),de(),U=Le[B],U||(U=Le[B]=Xn[B](e),U.c()),g(U,1),U.m(We.parentNode,We));const Fe={};o&2&&(Fe.$$scope={dirty:o,ctx:e}),te.$set(Fe);let cs=R;R=st(e),R!==cs&&(fe(),$(Me[cs],1,1,()=>{Me[cs]=null}),de(),G=Me[R],G||(G=Me[R]=et[R](e),G.c()),g(G,1),G.m(es.parentNode,es));let ms=V;V=nt(e),V!==ms&&(fe(),$(De[ms],1,1,()=>{De[ms]=null}),de(),Y=De[V],Y||(Y=De[V]=at[V](e),Y.c()),g(Y,1),Y.m(as.parentNode,as));const Hs={};o&2&&(Hs.$$scope={dirty:o,ctx:e}),pe.$set(Hs)},i(e){ka||(g(s.$$.fragment,e),g(A.$$.fragment,e),g(C),g(M),g(_e.$$.fragment,e),g(x),g(N),g(O),g(U),g(ge.$$.fragment,e),g(te.$$.fragment,e),g(ke.$$.fragment,e),g(je.$$.fragment,e),g(ve.$$.fragment,e),g(G),g(qe.$$.fragment,e),g(Y),g(pe.$$.fragment,e),g(we.$$.fragment,e),g(Ae.$$.fragment,e),ka=!0)},o(e){$(s.$$.fragment,e),$(A.$$.fragment,e),$(C),$(M),$(_e.$$.fragment,e),$(x),$(N),$(O),$(U),$(ge.$$.fragment,e),$(te.$$.fragment,e),$(ke.$$.fragment,e),$(je.$$.fragment,e),$(ve.$$.fragment,e),$(G),$(qe.$$.fragment,e),$(Y),$(pe.$$.fragment,e),$(we.$$.fragment,e),$(Ae.$$.fragment,e),ka=!1},d(e){t(n),e&&t(r),y(s,e),e&&t(p),e&&t(a),y(A),e&&t(be),Pe[T].d(e),e&&t(He),Ie[L].d(e),e&&t(Oe),e&&t(Be),e&&t(Bs),e&&t(I),e&&t(Us),e&&t(Ue),e&&t(Rs),e&&t(X),y(_e),e&&t(Gs),e&&t(Re),e&&t(Vs),Se[D].d(e),e&&t(Ge),e&&t(Ve),e&&t(Ys),e&&t(ne),e&&t(Js),Te[F].d(e),e&&t(Ye),e&&t(Je),e&&t(Qs),Ce[H].d(e),e&&t(Qe),e&&t(Ke),e&&t(Ks),Le[B].d(e),e&&t(We),e&&t($e),e&&t(Ws),y(ge,e),e&&t(Xs),e&&t(Xe),e&&t(Zs),y(te,e),e&&t(ea),e&&t(le),e&&t(sa),e&&t(Z),y(ke),e&&t(aa),e&&t(Ze),e&&t(na),y(je,e),e&&t(ta),e&&t(J),e&&t(la),y(ve,e),e&&t(oa),e&&t(Q),e&&t(ra),Me[R].d(e),e&&t(es),e&&t(ss),e&&t(ia),e&&t(re),e&&t(pa),e&&t(ee),y(qe),e&&t(ua),e&&t(ye),e&&t(ca),De[V].d(e),e&&t(as),e&&t(ns),e&&t(ma),e&&t(ts),e&&t(da),y(pe,e),e&&t(fa),e&&t(se),y(we),e&&t(ba),e&&t(ls),e&&t(ha),e&&t(ce),e&&t(_a),e&&t(K),e&&t($a),e&&t(me),e&&t(ga),y(Ae,e)}}}const bl={local:"manejando-secuencias-mltiples",sections:[{local:"los-modelos-esperan-baches-de-entrada",title:"Los modelos esperan Baches de entrada "},{local:"padding-a-las-entradas",title:"Padding a las entradas"},{local:"mscaras-de-atencin",title:"M\xE1scaras de atenci\xF3n"},{local:"secuencias-largas",title:"Secuencias largas"}],title:"Manejando Secuencias M\xFAltiples"};function hl(k,n,r){let s="pt";return Jt(()=>{const p=new URLSearchParams(window.location.search);r(0,s=p.get("fw")||"pt")}),[s]}class yl extends Rt{constructor(n){super();Gt(this,n,hl,fl,Vt,{})}}export{yl as default,bl as metadata};
