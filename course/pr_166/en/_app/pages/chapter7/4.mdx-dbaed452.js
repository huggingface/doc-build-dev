import{S as wd,i as bd,s as $d,e as l,t as a,k as c,c as r,a as i,h as n,d as t,m as d,b as _,g as o,F as s,w as y,x as j,y as E,q as w,o as b,B as x,M as kd,N as Cl,p as ao,v as vd,n as no,L as fd}from"../../chunks/vendor-1e8b365d.js";import{T as Xa}from"../../chunks/Tip-62b14c6e.js";import{Y as mi}from"../../chunks/Youtube-c2a8cc39.js";import{I as hs}from"../../chunks/IconCopyLink-483c28ba.js";import{C}from"../../chunks/CodeBlock-e5764662.js";import{D as gd}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as yd}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function jd(B){let h,$;return h=new gd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"}]}}),{c(){y(h.$$.fragment)},l(m){j(h.$$.fragment,m)},m(m,q){E(h,m,q),$=!0},i(m){$||(w(h.$$.fragment,m),$=!0)},o(m){b(h.$$.fragment,m),$=!1},d(m){x(h,m)}}}function Ed(B){let h,$;return h=new gd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"}]}}),{c(){y(h.$$.fragment)},l(m){j(h.$$.fragment,m)},m(m,q){E(h,m,q),$=!0},i(m){$||(w(h.$$.fragment,m),$=!0)},o(m){b(h.$$.fragment,m),$=!1},d(m){x(h,m)}}}function xd(B){let h,$,m,q,A;return{c(){h=l("p"),$=a("\u270F\uFE0F "),m=l("strong"),q=a("Your turn!"),A=a(" Another English word that is often used in French is \u201Cemail.\u201D Find the first sample in the training dataset that uses this word. How is it translated? How does the pretrained model translate the same English sentence?")},l(g){h=r(g,"P",{});var z=i(h);$=n(z,"\u270F\uFE0F "),m=r(z,"STRONG",{});var P=i(m);q=n(P,"Your turn!"),P.forEach(t),A=n(z," Another English word that is often used in French is \u201Cemail.\u201D Find the first sample in the training dataset that uses this word. How is it translated? How does the pretrained model translate the same English sentence?"),z.forEach(t)},m(g,z){o(g,h,z),s(h,$),s(h,m),s(m,q),s(h,A)},d(g){g&&t(h)}}}function qd(B){let h,$,m,q,A,g,z,P;return{c(){h=l("p"),$=a("\u{1F4A1} If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting "),m=l("code"),q=a("tokenizer.src_lang"),A=a(" and "),g=l("code"),z=a("tokenizer.tgt_lang"),P=a(" to the right values.")},l(D){h=r(D,"P",{});var T=i(h);$=n(T,"\u{1F4A1} If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting "),m=r(T,"CODE",{});var N=i(m);q=n(N,"tokenizer.src_lang"),N.forEach(t),A=n(T," and "),g=r(T,"CODE",{});var v=i(g);z=n(v,"tokenizer.tgt_lang"),v.forEach(t),P=n(T," to the right values."),T.forEach(t)},m(D,T){o(D,h,T),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P)},d(D){D&&t(h)}}}function Td(B){let h,$,m,q,A,g,z,P;return{c(){h=l("p"),$=a("\u{1F4A1} If you are using a T5 model (more specifically, one of the "),m=l("code"),q=a("t5-xxx"),A=a(" checkpoints), the model will expect the text inputs to have a prefix indicating the task at hand, such as "),g=l("code"),z=a("translate: English to French:"),P=a(".")},l(D){h=r(D,"P",{});var T=i(h);$=n(T,"\u{1F4A1} If you are using a T5 model (more specifically, one of the "),m=r(T,"CODE",{});var N=i(m);q=n(N,"t5-xxx"),N.forEach(t),A=n(T," checkpoints), the model will expect the text inputs to have a prefix indicating the task at hand, such as "),g=r(T,"CODE",{});var v=i(g);z=n(v,"translate: English to French:"),v.forEach(t),P=n(T,"."),T.forEach(t)},m(D,T){o(D,h,T),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P)},d(D){D&&t(h)}}}function zd(B){let h,$,m,q,A,g,z,P;return{c(){h=l("p"),$=a("\u26A0\uFE0F We don\u2019t pay attention to the attention mask of the targets, as the model won\u2019t expect it. Instead, the labels corresponding to a padding token should be set to "),m=l("code"),q=a("-100"),A=a(" so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to "),g=l("code"),z=a("-100"),P=a(".")},l(D){h=r(D,"P",{});var T=i(h);$=n(T,"\u26A0\uFE0F We don\u2019t pay attention to the attention mask of the targets, as the model won\u2019t expect it. Instead, the labels corresponding to a padding token should be set to "),m=r(T,"CODE",{});var N=i(m);q=n(N,"-100"),N.forEach(t),A=n(T," so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to "),g=r(T,"CODE",{});var v=i(g);z=n(v,"-100"),v.forEach(t),P=n(T,"."),T.forEach(t)},m(D,T){o(D,h,T),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P)},d(D){D&&t(h)}}}function Dd(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M;return q=new hs({}),U=new C({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=<span class="hljs-literal">True</span>)`}}),X=new Xa({props:{warning:!1,$$slots:{default:[Ad]},$$scope:{ctx:B}}}),{c(){h=l("h2"),$=l("a"),m=l("span"),y(q.$$.fragment),A=c(),g=l("span"),z=a("Fine-tuning the model with Keras"),P=c(),D=l("p"),T=a("First things first, we need an actual model to fine-tune. We\u2019ll use the usual "),N=l("code"),v=a("AutoModel"),S=a(" API:"),H=c(),y(U.$$.fragment),F=c(),y(X.$$.fragment),this.h()},l(I){h=r(I,"H2",{class:!0});var Y=i(h);$=r(Y,"A",{id:!0,class:!0,href:!0});var Z=i($);m=r(Z,"SPAN",{});var re=i(m);j(q.$$.fragment,re),re.forEach(t),Z.forEach(t),A=d(Y),g=r(Y,"SPAN",{});var he=i(g);z=n(he,"Fine-tuning the model with Keras"),he.forEach(t),Y.forEach(t),P=d(I),D=r(I,"P",{});var Q=i(D);T=n(Q,"First things first, we need an actual model to fine-tune. We\u2019ll use the usual "),N=r(Q,"CODE",{});var ee=i(N);v=n(ee,"AutoModel"),ee.forEach(t),S=n(Q," API:"),Q.forEach(t),H=d(I),j(U.$$.fragment,I),F=d(I),j(X.$$.fragment,I),this.h()},h(){_($,"id","finetuning-the-model-with-keras"),_($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_($,"href","#finetuning-the-model-with-keras"),_(h,"class","relative group")},m(I,Y){o(I,h,Y),s(h,$),s($,m),E(q,m,null),s(h,A),s(h,g),s(g,z),o(I,P,Y),o(I,D,Y),s(D,T),s(D,N),s(N,v),s(D,S),o(I,H,Y),E(U,I,Y),o(I,F,Y),E(X,I,Y),M=!0},i(I){M||(w(q.$$.fragment,I),w(U.$$.fragment,I),w(X.$$.fragment,I),M=!0)},o(I){b(q.$$.fragment,I),b(U.$$.fragment,I),b(X.$$.fragment,I),M=!1},d(I){I&&t(h),x(q),I&&t(P),I&&t(D),I&&t(H),x(U,I),I&&t(F),x(X,I)}}}function Pd(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we,V,se,de,be,oe,ie;return q=new hs({}),oe=new C({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){h=l("h2"),$=l("a"),m=l("span"),y(q.$$.fragment),A=c(),g=l("span"),z=a("Fine-tuning the model with the "),P=l("code"),D=a("Trainer"),T=a(" API"),N=c(),v=l("p"),S=a("The actual code using the "),H=l("code"),U=a("Trainer"),F=a(" will be the same as before, with just one little change: we use a "),X=l("a"),M=l("code"),I=a("Seq2SeqTrainer"),Y=a(" here, which is a subclass of "),Z=l("code"),re=a("Trainer"),he=a(" that will allow us to properly deal with the evaluation, using the "),Q=l("code"),ee=a("generate()"),te=a(" method to predict outputs from the inputs. We\u2019ll dive into that in more detail when we talk about the metric computation."),G=c(),ne=l("p"),we=a("First things first, we need an actual model to fine-tune. We\u2019ll use the usual "),V=l("code"),se=a("AutoModel"),de=a(" API:"),be=c(),y(oe.$$.fragment),this.h()},l(W){h=r(W,"H2",{class:!0});var ae=i(h);$=r(ae,"A",{id:!0,class:!0,href:!0});var Te=i($);m=r(Te,"SPAN",{});var ue=i(m);j(q.$$.fragment,ue),ue.forEach(t),Te.forEach(t),A=d(ae),g=r(ae,"SPAN",{});var K=i(g);z=n(K,"Fine-tuning the model with the "),P=r(K,"CODE",{});var ve=i(P);D=n(ve,"Trainer"),ve.forEach(t),T=n(K," API"),K.forEach(t),ae.forEach(t),N=d(W),v=r(W,"P",{});var le=i(v);S=n(le,"The actual code using the "),H=r(le,"CODE",{});var Ee=i(H);U=n(Ee,"Trainer"),Ee.forEach(t),F=n(le," will be the same as before, with just one little change: we use a "),X=r(le,"A",{href:!0,rel:!0});var Ye=i(X);M=r(Ye,"CODE",{});var fe=i(M);I=n(fe,"Seq2SeqTrainer"),fe.forEach(t),Ye.forEach(t),Y=n(le," here, which is a subclass of "),Z=r(le,"CODE",{});var xe=i(Z);re=n(xe,"Trainer"),xe.forEach(t),he=n(le," that will allow us to properly deal with the evaluation, using the "),Q=r(le,"CODE",{});var J=i(Q);ee=n(J,"generate()"),J.forEach(t),te=n(le," method to predict outputs from the inputs. We\u2019ll dive into that in more detail when we talk about the metric computation."),le.forEach(t),G=d(W),ne=r(W,"P",{});var Ae=i(ne);we=n(Ae,"First things first, we need an actual model to fine-tune. We\u2019ll use the usual "),V=r(Ae,"CODE",{});var me=i(V);se=n(me,"AutoModel"),me.forEach(t),de=n(Ae," API:"),Ae.forEach(t),be=d(W),j(oe.$$.fragment,W),this.h()},h(){_($,"id","finetuning-the-model-with-the-trainer-api"),_($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_($,"href","#finetuning-the-model-with-the-trainer-api"),_(h,"class","relative group"),_(X,"href","https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer"),_(X,"rel","nofollow")},m(W,ae){o(W,h,ae),s(h,$),s($,m),E(q,m,null),s(h,A),s(h,g),s(g,z),s(g,P),s(P,D),s(g,T),o(W,N,ae),o(W,v,ae),s(v,S),s(v,H),s(H,U),s(v,F),s(v,X),s(X,M),s(M,I),s(v,Y),s(v,Z),s(Z,re),s(v,he),s(v,Q),s(Q,ee),s(v,te),o(W,G,ae),o(W,ne,ae),s(ne,we),s(ne,V),s(V,se),s(ne,de),o(W,be,ae),E(oe,W,ae),ie=!0},i(W){ie||(w(q.$$.fragment,W),w(oe.$$.fragment,W),ie=!0)},o(W){b(q.$$.fragment,W),b(oe.$$.fragment,W),ie=!1},d(W){W&&t(h),x(q),W&&t(N),W&&t(v),W&&t(G),W&&t(ne),W&&t(be),x(oe,W)}}}function Ad(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H;return{c(){h=l("p"),$=a("\u{1F4A1} The "),m=l("code"),q=a("Helsinki-NLP/opus-mt-en-fr"),A=a(` checkpoint only has PyTorch weights, so
you\u2019ll get an error if you try to load the model without using the
`),g=l("code"),z=a("from_pt=True"),P=a(" argument in the "),D=l("code"),T=a("from_pretrained()"),N=a(` method. When you specify
`),v=l("code"),S=a("from_pt=True"),H=a(`, the library will automatically download and convert the
PyTorch weights for you. As you can see, it is very simple to switch between
frameworks in \u{1F917} Transformers!`)},l(U){h=r(U,"P",{});var F=i(h);$=n(F,"\u{1F4A1} The "),m=r(F,"CODE",{});var X=i(m);q=n(X,"Helsinki-NLP/opus-mt-en-fr"),X.forEach(t),A=n(F,` checkpoint only has PyTorch weights, so
you\u2019ll get an error if you try to load the model without using the
`),g=r(F,"CODE",{});var M=i(g);z=n(M,"from_pt=True"),M.forEach(t),P=n(F," argument in the "),D=r(F,"CODE",{});var I=i(D);T=n(I,"from_pretrained()"),I.forEach(t),N=n(F,` method. When you specify
`),v=r(F,"CODE",{});var Y=i(v);S=n(Y,"from_pt=True"),Y.forEach(t),H=n(F,`, the library will automatically download and convert the
PyTorch weights for you. As you can see, it is very simple to switch between
frameworks in \u{1F917} Transformers!`),F.forEach(t)},m(U,F){o(U,h,F),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),s(h,D),s(D,T),s(h,N),s(h,v),s(v,S),s(h,H)},d(U){U&&t(h)}}}function Sd(B){let h,$;return h=new C({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){y(h.$$.fragment)},l(m){j(h.$$.fragment,m)},m(m,q){E(h,m,q),$=!0},i(m){$||(w(h.$$.fragment,m),$=!0)},o(m){b(h.$$.fragment,m),$=!1},d(m){x(h,m)}}}function Cd(B){let h,$;return h=new C({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`}}),{c(){y(h.$$.fragment)},l(m){j(h.$$.fragment,m)},m(m,q){E(h,m,q),$=!0},i(m){$||(w(h.$$.fragment,m),$=!0)},o(m){b(h.$$.fragment,m),$=!1},d(m){x(h,m)}}}function Od(B){let h,$,m,q,A,g,z,P,D,T,N;return T=new C({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)
tf_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
)`}}),{c(){h=l("p"),$=a("We can now use this "),m=l("code"),q=a("data_collator"),A=a(" to convert each of our datasets to a "),g=l("code"),z=a("tf.data.Dataset"),P=a(", ready for training:"),D=c(),y(T.$$.fragment)},l(v){h=r(v,"P",{});var S=i(h);$=n(S,"We can now use this "),m=r(S,"CODE",{});var H=i(m);q=n(H,"data_collator"),H.forEach(t),A=n(S," to convert each of our datasets to a "),g=r(S,"CODE",{});var U=i(g);z=n(U,"tf.data.Dataset"),U.forEach(t),P=n(S,", ready for training:"),S.forEach(t),D=d(v),j(T.$$.fragment,v)},m(v,S){o(v,h,S),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),o(v,D,S),E(T,v,S),N=!0},i(v){N||(w(T.$$.fragment,v),N=!0)},o(v){b(T.$$.fragment,v),N=!1},d(v){v&&t(h),v&&t(D),x(T,v)}}}function Fd(B){let h,$,m,q,A,g,z,P;return{c(){h=l("p"),$=a("We will pass this "),m=l("code"),q=a("data_collator"),A=a(" along to the "),g=l("code"),z=a("Seq2SeqTrainer"),P=a(". Next, let\u2019s have a look at the metric.")},l(D){h=r(D,"P",{});var T=i(h);$=n(T,"We will pass this "),m=r(T,"CODE",{});var N=i(m);q=n(N,"data_collator"),N.forEach(t),A=n(T," along to the "),g=r(T,"CODE",{});var v=i(g);z=n(v,"Seq2SeqTrainer"),v.forEach(t),P=n(T,". Next, let\u2019s have a look at the metric."),T.forEach(t)},m(D,T){o(D,h,T),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P)},i:fd,o:fd,d(D){D&&t(h)}}}function md(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we;return{c(){h=l("p"),$=a("The feature that "),m=l("code"),q=a("Seq2SeqTrainer"),A=a(" adds to its superclass "),g=l("code"),z=a("Trainer"),P=a(" is the ability to use the "),D=l("code"),T=a("generate()"),N=a(" method during evaluation or prediction. During training, the model will use the "),v=l("code"),S=a("decoder_input_ids"),H=a(" with an attention mask ensuring it does not use the tokens after the token it\u2019s trying to predict, to speed up training. During inference we won\u2019t be able to use those since we won\u2019t have labels, so it\u2019s a good idea to evaluate our model with the same setup."),U=c(),F=l("p"),X=a("As we saw in "),M=l("a"),I=a("Chapter 1"),Y=a(", the decoder performs inference by predicting tokens one by one \u2014 something that\u2019s implemented behind the scenes in \u{1F917} Transformers by the "),Z=l("code"),re=a("generate()"),he=a(" method. The "),Q=l("code"),ee=a("Seq2SeqTrainer"),te=a(" will let us use that method for evaluation if we set "),G=l("code"),ne=a("predict_with_generate=True"),we=a("."),this.h()},l(V){h=r(V,"P",{});var se=i(h);$=n(se,"The feature that "),m=r(se,"CODE",{});var de=i(m);q=n(de,"Seq2SeqTrainer"),de.forEach(t),A=n(se," adds to its superclass "),g=r(se,"CODE",{});var be=i(g);z=n(be,"Trainer"),be.forEach(t),P=n(se," is the ability to use the "),D=r(se,"CODE",{});var oe=i(D);T=n(oe,"generate()"),oe.forEach(t),N=n(se," method during evaluation or prediction. During training, the model will use the "),v=r(se,"CODE",{});var ie=i(v);S=n(ie,"decoder_input_ids"),ie.forEach(t),H=n(se," with an attention mask ensuring it does not use the tokens after the token it\u2019s trying to predict, to speed up training. During inference we won\u2019t be able to use those since we won\u2019t have labels, so it\u2019s a good idea to evaluate our model with the same setup."),se.forEach(t),U=d(V),F=r(V,"P",{});var W=i(F);X=n(W,"As we saw in "),M=r(W,"A",{href:!0});var ae=i(M);I=n(ae,"Chapter 1"),ae.forEach(t),Y=n(W,", the decoder performs inference by predicting tokens one by one \u2014 something that\u2019s implemented behind the scenes in \u{1F917} Transformers by the "),Z=r(W,"CODE",{});var Te=i(Z);re=n(Te,"generate()"),Te.forEach(t),he=n(W," method. The "),Q=r(W,"CODE",{});var ue=i(Q);ee=n(ue,"Seq2SeqTrainer"),ue.forEach(t),te=n(W," will let us use that method for evaluation if we set "),G=r(W,"CODE",{});var K=i(G);ne=n(K,"predict_with_generate=True"),K.forEach(t),we=n(W,"."),W.forEach(t),this.h()},h(){_(M,"href","/course/chapter1/6")},m(V,se){o(V,h,se),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),s(h,D),s(D,T),s(h,N),s(h,v),s(v,S),s(h,H),o(V,U,se),o(V,F,se),s(F,X),s(F,M),s(M,I),s(F,Y),s(F,Z),s(Z,re),s(F,he),s(F,Q),s(Q,ee),s(F,te),s(F,G),s(G,ne),s(F,we)},d(V){V&&t(h),V&&t(U),V&&t(F)}}}function Id(B){let h,$,m,q,A,g,z,P,D,T,N;return T=new C({props:{code:`import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):
    preds, labels = eval_preds
    <span class="hljs-comment"># In case the model returns more than the prediction logits</span>
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(preds, <span class="hljs-built_in">tuple</span>):
        preds = preds[<span class="hljs-number">0</span>]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Replace -100s in the labels as we can&#x27;t decode them</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Some simple post-processing</span>
    decoded_preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [[label.strip()] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;bleu&quot;</span>: result[<span class="hljs-string">&quot;score&quot;</span>]}`}}),{c(){h=l("p"),$=a("To get from the model outputs to texts the metric can use, we will use the "),m=l("code"),q=a("tokenizer.batch_decode()"),A=a(" method. We just have to clean up all the "),g=l("code"),z=a("-100"),P=a("s in the labels (the tokenizer will automatically do the same for the padding token):"),D=c(),y(T.$$.fragment)},l(v){h=r(v,"P",{});var S=i(h);$=n(S,"To get from the model outputs to texts the metric can use, we will use the "),m=r(S,"CODE",{});var H=i(m);q=n(H,"tokenizer.batch_decode()"),H.forEach(t),A=n(S," method. We just have to clean up all the "),g=r(S,"CODE",{});var U=i(g);z=n(U,"-100"),U.forEach(t),P=n(S,"s in the labels (the tokenizer will automatically do the same for the padding token):"),S.forEach(t),D=d(v),j(T.$$.fragment,v)},m(v,S){o(v,h,S),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),o(v,D,S),E(T,v,S),N=!0},i(v){N||(w(T.$$.fragment,v),N=!0)},o(v){b(T.$$.fragment,v),N=!1},d(v){v&&t(h),v&&t(D),x(T,v)}}}function Ld(B){let h,$,m,q,A,g,z,P,D,T,N;return T=new C({props:{code:`import numpy as np


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch in tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = batch["labels"].numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].shuffle().select(<span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
        collate_fn=data_collator,
        shuffle=<span class="hljs-literal">False</span>,
        batch_size=<span class="hljs-number">4</span>,
    )
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch[<span class="hljs-string">&quot;input_ids&quot;</span>], attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
        labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
        labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
        decoded_preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
        decoded_labels = [[label.strip()] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;bleu&quot;</span>: result[<span class="hljs-string">&quot;score&quot;</span>]}`}}),{c(){h=l("p"),$=a("To get from the model outputs to texts the metric can use, we will use the "),m=l("code"),q=a("tokenizer.batch_decode()"),A=a(" method. We just have to clean up all the "),g=l("code"),z=a("-100"),P=a("s in the labels; the tokenizer will automatically do the same for the padding token. Let\u2019s define a function that takes our model and a dataset and computes metrics on it. Because generation of long sequences can be slow, we subsample the validation set to make sure this doesn\u2019t take forever:"),D=c(),y(T.$$.fragment)},l(v){h=r(v,"P",{});var S=i(h);$=n(S,"To get from the model outputs to texts the metric can use, we will use the "),m=r(S,"CODE",{});var H=i(m);q=n(H,"tokenizer.batch_decode()"),H.forEach(t),A=n(S," method. We just have to clean up all the "),g=r(S,"CODE",{});var U=i(g);z=n(U,"-100"),U.forEach(t),P=n(S,"s in the labels; the tokenizer will automatically do the same for the padding token. Let\u2019s define a function that takes our model and a dataset and computes metrics on it. Because generation of long sequences can be slow, we subsample the validation set to make sure this doesn\u2019t take forever:"),S.forEach(t),D=d(v),j(T.$$.fragment,v)},m(v,S){o(v,h,S),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),o(v,D,S),E(T,v,S),N=!0},i(v){N||(w(T.$$.fragment,v),N=!0)},o(v){b(T.$$.fragment,v),N=!1},d(v){v&&t(h),v&&t(D),x(T,v)}}}function Nd(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we,V,se,de,be,oe,ie,W,ae,Te,ue,K,ve,le,Ee,Ye,fe,xe,J,Ae,me,pt,kt,$e,et,Ge,Se,ye,vt,_e,k,R,tt,pe,Xe,ht,je,ke,yt,ge,Oe,Fe,Tt,ce,ct,st,Ie,cs,Le,$s,ds,dt,ut,ft,at,Ne,zt,mt,Vt,nt,He,Qt,qe,es,ze,Dt,jt,ks,It,We,vs,Lt,ot,De,Me,_t,Pt,Nt,gt,ts,Ue,lt,Ht,wt,us,At,bt,ss,Ze,fs,Be;return S=new C({props:{code:`from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    <span class="hljs-string">f&quot;marian-finetuned-kde4-en-to-fr&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;no&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">64</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    predict_with_generate=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)`}}),R=new Xa({props:{$$slots:{default:[Wd]},$$scope:{ctx:B}}}),ge=new C({props:{code:`from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`}}),ct=new C({props:{code:"trainer.evaluate(max_length=max_target_length)",highlighted:"trainer.evaluate(max_length=max_target_length)"}}),Ie=new C({props:{code:`{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}`,highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">1.6964408159255981</span>,
 <span class="hljs-string">&#x27;eval_bleu&#x27;</span>: <span class="hljs-number">39.26865061007616</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">965.8884</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">21.76</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">0.341</span>}`}}),at=new C({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),qe=new C({props:{code:"trainer.evaluate(max_length=max_target_length)",highlighted:"trainer.evaluate(max_length=max_target_length)"}}),ze=new C({props:{code:`{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}`,highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">0.8558505773544312</span>,
 <span class="hljs-string">&#x27;eval_bleu&#x27;</span>: <span class="hljs-number">52.94161337775576</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">714.2576</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">29.426</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">0.461</span>,
 <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-number">3.0</span>}`}}),gt=new C({props:{code:'trainer.push_to_hub(tags="translation", commit_message="Training complete")',highlighted:'trainer.push_to_hub(tags=<span class="hljs-string">&quot;translation&quot;</span>, commit_message=<span class="hljs-string">&quot;Training complete&quot;</span>)'}}),wt=new C({props:{code:"'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3&#x27;</span>'}}),{c(){h=l("p"),$=a("Once this is done, we can define our "),m=l("code"),q=a("Seq2SeqTrainingArguments"),A=a(". Like for the "),g=l("code"),z=a("Trainer"),P=a(", we use a subclass of "),D=l("code"),T=a("TrainingArguments"),N=a(" that contains a few more fields:"),v=c(),y(S.$$.fragment),H=c(),U=l("p"),F=a("Apart from the usual hyperparameters (like learning rate, number of epochs, batch size, and some weight decay), here are a few changes compared to what we saw in the previous sections:"),X=c(),M=l("ul"),I=l("li"),Y=a("We don\u2019t set any regular evaluation, as evaluation takes a while; we will just evaluate our model once before training and after."),Z=c(),re=l("li"),he=a("We set "),Q=l("code"),ee=a("fp16=True"),te=a(", which speeds up training on modern GPUs."),G=c(),ne=l("li"),we=a("We set "),V=l("code"),se=a("predict_with_generate=True"),de=a(", as discussed above."),be=c(),oe=l("li"),ie=a("We use "),W=l("code"),ae=a("push_to_hub=True"),Te=a(" to upload the model to the Hub at the end of each epoch."),ue=c(),K=l("p"),ve=a("Note that you can specify the full name of the repository you want to push to with the "),le=l("code"),Ee=a("hub_model_id"),Ye=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),fe=l("a"),xe=l("code"),J=a("huggingface-course"),Ae=a(" organization"),me=a(", we added "),pt=l("code"),kt=a('hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"'),$e=a(" to "),et=l("code"),Ge=a("Seq2SeqTrainingArguments"),Se=a(". By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be "),ye=l("code"),vt=a('"sgugger/marian-finetuned-kde4-en-to-fr"'),_e=a(" (which is the model we linked to at the beginning of this section)."),k=c(),y(R.$$.fragment),tt=c(),pe=l("p"),Xe=a("Finally, we just pass everything to the "),ht=l("code"),je=a("Seq2SeqTrainer"),ke=a(":"),yt=c(),y(ge.$$.fragment),Oe=c(),Fe=l("p"),Tt=a("Before training, we\u2019ll first look at the score our model gets, to double-check that we\u2019re not making things worse with our fine-tuning. This command will take a bit of time, so you can grab a coffee while it executes:"),ce=c(),y(ct.$$.fragment),st=c(),y(Ie.$$.fragment),cs=c(),Le=l("p"),$s=a("A BLEU score of 39 is not too bad, which reflects the fact that our model is already good at translating English sentences to French ones."),ds=c(),dt=l("p"),ut=a("Next is the training, which will also take a bit of time:"),ft=c(),y(at.$$.fragment),Ne=c(),zt=l("p"),mt=a("Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),Vt=c(),nt=l("p"),He=a("Once training is done, we evaluate our model again \u2014 hopefully we will see some amelioration in the BLEU score!"),Qt=c(),y(qe.$$.fragment),es=c(),y(ze.$$.fragment),Dt=c(),jt=l("p"),ks=a("That\u2019s a nearly 14-point improvement, which is great."),It=c(),We=l("p"),vs=a("Finally, we use the "),Lt=l("code"),ot=a("push_to_hub()"),De=a(" method to make sure we upload the latest version of the model. The "),Me=l("code"),_t=a("Trainer"),Pt=a(" also drafts a model card with all the evaluation results and uploads it. This model card contains metadata that helps the Model Hub pick the widget for the inference demo. Usually, there is no need to say anything as it can infer the right widget from the model class, but in this case, the same model class can be used for all kinds of sequence-to-sequence problems, so we specify it\u2019s a translation model:"),Nt=c(),y(gt.$$.fragment),ts=c(),Ue=l("p"),lt=a("This command returns the URL of the commit it just did, if you want to inspect it:"),Ht=c(),y(wt.$$.fragment),us=c(),At=l("p"),bt=a("At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task \u2014 congratulations!"),ss=c(),Ze=l("p"),fs=a("If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using \u{1F917} Accelerate."),this.h()},l(f){h=r(f,"P",{});var O=i(h);$=n(O,"Once this is done, we can define our "),m=r(O,"CODE",{});var ys=i(m);q=n(ys,"Seq2SeqTrainingArguments"),ys.forEach(t),A=n(O,". Like for the "),g=r(O,"CODE",{});var rt=i(g);z=n(rt,"Trainer"),rt.forEach(t),P=n(O,", we use a subclass of "),D=r(O,"CODE",{});var da=i(D);T=n(da,"TrainingArguments"),da.forEach(t),N=n(O," that contains a few more fields:"),O.forEach(t),v=d(f),j(S.$$.fragment,f),H=d(f),U=r(f,"P",{});var js=i(U);F=n(js,"Apart from the usual hyperparameters (like learning rate, number of epochs, batch size, and some weight decay), here are a few changes compared to what we saw in the previous sections:"),js.forEach(t),X=d(f),M=r(f,"UL",{});var Pe=i(M);I=r(Pe,"LI",{});var Ks=i(I);Y=n(Ks,"We don\u2019t set any regular evaluation, as evaluation takes a while; we will just evaluate our model once before training and after."),Ks.forEach(t),Z=d(Pe),re=r(Pe,"LI",{});var as=i(re);he=n(as,"We set "),Q=r(as,"CODE",{});var St=i(Q);ee=n(St,"fp16=True"),St.forEach(t),te=n(as,", which speeds up training on modern GPUs."),as.forEach(t),G=d(Pe),ne=r(Pe,"LI",{});var Re=i(ne);we=n(Re,"We set "),V=r(Re,"CODE",{});var Wt=i(V);se=n(Wt,"predict_with_generate=True"),Wt.forEach(t),de=n(Re,", as discussed above."),Re.forEach(t),be=d(Pe),oe=r(Pe,"LI",{});var Ct=i(oe);ie=n(Ct,"We use "),W=r(Ct,"CODE",{});var Je=i(W);ae=n(Je,"push_to_hub=True"),Je.forEach(t),Te=n(Ct," to upload the model to the Hub at the end of each epoch."),Ct.forEach(t),Pe.forEach(t),ue=d(f),K=r(f,"P",{});var Ke=i(K);ve=n(Ke,"Note that you can specify the full name of the repository you want to push to with the "),le=r(Ke,"CODE",{});var Ot=i(le);Ee=n(Ot,"hub_model_id"),Ot.forEach(t),Ye=n(Ke," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),fe=r(Ke,"A",{href:!0,rel:!0});var Es=i(fe);xe=r(Es,"CODE",{});var Ys=i(xe);J=n(Ys,"huggingface-course"),Ys.forEach(t),Ae=n(Es," organization"),Es.forEach(t),me=n(Ke,", we added "),pt=r(Ke,"CODE",{});var ns=i(pt);kt=n(ns,'hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"'),ns.forEach(t),$e=n(Ke," to "),et=r(Ke,"CODE",{});var Ve=i(et);Ge=n(Ve,"Seq2SeqTrainingArguments"),Ve.forEach(t),Se=n(Ke,". By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be "),ye=r(Ke,"CODE",{});var Gs=i(ye);vt=n(Gs,'"sgugger/marian-finetuned-kde4-en-to-fr"'),Gs.forEach(t),_e=n(Ke," (which is the model we linked to at the beginning of this section)."),Ke.forEach(t),k=d(f),j(R.$$.fragment,f),tt=d(f),pe=r(f,"P",{});var $t=i(pe);Xe=n($t,"Finally, we just pass everything to the "),ht=r($t,"CODE",{});var Xs=i(ht);je=n(Xs,"Seq2SeqTrainer"),Xs.forEach(t),ke=n($t,":"),$t.forEach(t),yt=d(f),j(ge.$$.fragment,f),Oe=d(f),Fe=r(f,"P",{});var Zs=i(Fe);Tt=n(Zs,"Before training, we\u2019ll first look at the score our model gets, to double-check that we\u2019re not making things worse with our fine-tuning. This command will take a bit of time, so you can grab a coffee while it executes:"),Zs.forEach(t),ce=d(f),j(ct.$$.fragment,f),st=d(f),j(Ie.$$.fragment,f),cs=d(f),Le=r(f,"P",{});var os=i(Le);$s=n(os,"A BLEU score of 39 is not too bad, which reflects the fact that our model is already good at translating English sentences to French ones."),os.forEach(t),ds=d(f),dt=r(f,"P",{});var Mt=i(dt);ut=n(Mt,"Next is the training, which will also take a bit of time:"),Mt.forEach(t),ft=d(f),j(at.$$.fragment,f),Ne=d(f),zt=r(f,"P",{});var xs=i(zt);mt=n(xs,"Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),xs.forEach(t),Vt=d(f),nt=r(f,"P",{});var Et=i(nt);He=n(Et,"Once training is done, we evaluate our model again \u2014 hopefully we will see some amelioration in the BLEU score!"),Et.forEach(t),Qt=d(f),j(qe.$$.fragment,f),es=d(f),j(ze.$$.fragment,f),Dt=d(f),jt=r(f,"P",{});var qs=i(jt);ks=n(qs,"That\u2019s a nearly 14-point improvement, which is great."),qs.forEach(t),It=d(f),We=r(f,"P",{});var Ce=i(We);vs=n(Ce,"Finally, we use the "),Lt=r(Ce,"CODE",{});var Ts=i(Lt);ot=n(Ts,"push_to_hub()"),Ts.forEach(t),De=n(Ce," method to make sure we upload the latest version of the model. The "),Me=r(Ce,"CODE",{});var xt=i(Me);_t=n(xt,"Trainer"),xt.forEach(t),Pt=n(Ce," also drafts a model card with all the evaluation results and uploads it. This model card contains metadata that helps the Model Hub pick the widget for the inference demo. Usually, there is no need to say anything as it can infer the right widget from the model class, but in this case, the same model class can be used for all kinds of sequence-to-sequence problems, so we specify it\u2019s a translation model:"),Ce.forEach(t),Nt=d(f),j(gt.$$.fragment,f),ts=d(f),Ue=r(f,"P",{});var zs=i(Ue);lt=n(zs,"This command returns the URL of the commit it just did, if you want to inspect it:"),zs.forEach(t),Ht=d(f),j(wt.$$.fragment,f),us=d(f),At=r(f,"P",{});var Ut=i(At);bt=n(Ut,"At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task \u2014 congratulations!"),Ut.forEach(t),ss=d(f),Ze=r(f,"P",{});var ls=i(Ze);fs=n(ls,"If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using \u{1F917} Accelerate."),ls.forEach(t),this.h()},h(){_(fe,"href","https://huggingface.co/huggingface-course"),_(fe,"rel","nofollow")},m(f,O){o(f,h,O),s(h,$),s(h,m),s(m,q),s(h,A),s(h,g),s(g,z),s(h,P),s(h,D),s(D,T),s(h,N),o(f,v,O),E(S,f,O),o(f,H,O),o(f,U,O),s(U,F),o(f,X,O),o(f,M,O),s(M,I),s(I,Y),s(M,Z),s(M,re),s(re,he),s(re,Q),s(Q,ee),s(re,te),s(M,G),s(M,ne),s(ne,we),s(ne,V),s(V,se),s(ne,de),s(M,be),s(M,oe),s(oe,ie),s(oe,W),s(W,ae),s(oe,Te),o(f,ue,O),o(f,K,O),s(K,ve),s(K,le),s(le,Ee),s(K,Ye),s(K,fe),s(fe,xe),s(xe,J),s(fe,Ae),s(K,me),s(K,pt),s(pt,kt),s(K,$e),s(K,et),s(et,Ge),s(K,Se),s(K,ye),s(ye,vt),s(K,_e),o(f,k,O),E(R,f,O),o(f,tt,O),o(f,pe,O),s(pe,Xe),s(pe,ht),s(ht,je),s(pe,ke),o(f,yt,O),E(ge,f,O),o(f,Oe,O),o(f,Fe,O),s(Fe,Tt),o(f,ce,O),E(ct,f,O),o(f,st,O),E(Ie,f,O),o(f,cs,O),o(f,Le,O),s(Le,$s),o(f,ds,O),o(f,dt,O),s(dt,ut),o(f,ft,O),E(at,f,O),o(f,Ne,O),o(f,zt,O),s(zt,mt),o(f,Vt,O),o(f,nt,O),s(nt,He),o(f,Qt,O),E(qe,f,O),o(f,es,O),E(ze,f,O),o(f,Dt,O),o(f,jt,O),s(jt,ks),o(f,It,O),o(f,We,O),s(We,vs),s(We,Lt),s(Lt,ot),s(We,De),s(We,Me),s(Me,_t),s(We,Pt),o(f,Nt,O),E(gt,f,O),o(f,ts,O),o(f,Ue,O),s(Ue,lt),o(f,Ht,O),E(wt,f,O),o(f,us,O),o(f,At,O),s(At,bt),o(f,ss,O),o(f,Ze,O),s(Ze,fs),Be=!0},i(f){Be||(w(S.$$.fragment,f),w(R.$$.fragment,f),w(ge.$$.fragment,f),w(ct.$$.fragment,f),w(Ie.$$.fragment,f),w(at.$$.fragment,f),w(qe.$$.fragment,f),w(ze.$$.fragment,f),w(gt.$$.fragment,f),w(wt.$$.fragment,f),Be=!0)},o(f){b(S.$$.fragment,f),b(R.$$.fragment,f),b(ge.$$.fragment,f),b(ct.$$.fragment,f),b(Ie.$$.fragment,f),b(at.$$.fragment,f),b(qe.$$.fragment,f),b(ze.$$.fragment,f),b(gt.$$.fragment,f),b(wt.$$.fragment,f),Be=!1},d(f){f&&t(h),f&&t(v),x(S,f),f&&t(H),f&&t(U),f&&t(X),f&&t(M),f&&t(ue),f&&t(K),f&&t(k),x(R,f),f&&t(tt),f&&t(pe),f&&t(yt),x(ge,f),f&&t(Oe),f&&t(Fe),f&&t(ce),x(ct,f),f&&t(st),x(Ie,f),f&&t(cs),f&&t(Le),f&&t(ds),f&&t(dt),f&&t(ft),x(at,f),f&&t(Ne),f&&t(zt),f&&t(Vt),f&&t(nt),f&&t(Qt),x(qe,f),f&&t(es),x(ze,f),f&&t(Dt),f&&t(jt),f&&t(It),f&&t(We),f&&t(Nt),x(gt,f),f&&t(ts),f&&t(Ue),f&&t(Ht),x(wt,f),f&&t(us),f&&t(At),f&&t(ss),f&&t(Ze)}}}function Hd(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we,V,se,de,be,oe,ie,W,ae,Te,ue,K,ve,le,Ee,Ye,fe,xe,J,Ae,me,pt,kt,$e,et,Ge,Se,ye,vt,_e;return q=new C({props:{code:"print(compute_metrics())",highlighted:'<span class="hljs-built_in">print</span>(compute_metrics())'}}),g=new C({props:{code:"{'bleu': 33.26983701454733}",highlighted:'{&#x27;bleu&#x27;: <span class="hljs-number">33.26983701454733</span>}'}}),H=new C({props:{code:`from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_epochs = <span class="hljs-number">3</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),ee=new C({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">&quot;marian-finetuned-kde4-en-to-fr&quot;</span>, tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)`}}),J=new Xa({props:{$$slots:{default:[Md]},$$scope:{ctx:B}}}),$e=new C({props:{code:"print(compute_metrics())",highlighted:'<span class="hljs-built_in">print</span>(compute_metrics())'}}),Ge=new C({props:{code:"{'bleu': 57.334066271545865}",highlighted:'{&#x27;bleu&#x27;: <span class="hljs-number">57.334066271545865</span>}'}}),{c(){h=l("p"),$=a("Before we start, let\u2019s see what kind of results we get from our model without any training:"),m=c(),y(q.$$.fragment),A=c(),y(g.$$.fragment),z=c(),P=l("p"),D=a("Once this is done, we can prepare everything we need to compile and train our model. Note the use of "),T=l("code"),N=a('tf.keras.mixed_precision.set_global_policy("mixed_float16")'),v=a(" \u2014 this will tell Keras to train using float16, which can give a significant speedup on GPUs that support it (Nvidia 20xx/V100 or newer)."),S=c(),y(H.$$.fragment),U=c(),F=l("p"),X=a("Next, we define a "),M=l("code"),I=a("PushToHubCallback"),Y=a(" to upload our model to the Hub during training, as we saw in "),Z=l("a"),re=a("section 2"),he=a(", and then we simply fit the model with that callback:"),Q=c(),y(ee.$$.fragment),te=c(),G=l("p"),ne=a("Note that you can specify the name of the repository you want to push to with the "),we=l("code"),V=a("hub_model_id"),se=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),de=l("a"),be=l("code"),oe=a("huggingface-course"),ie=a(" organization"),W=a(", we added "),ae=l("code"),Te=a('hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"'),ue=a(" to "),K=l("code"),ve=a("Seq2SeqTrainingArguments"),le=a(". By default, the repository used will be in your namespace and named after the output directory you set, so here it will be "),Ee=l("code"),Ye=a('"sgugger/marian-finetuned-kde4-en-to-fr"'),fe=a(" (which is the model we linked to at the beginning of this section)."),xe=c(),y(J.$$.fragment),Ae=c(),me=l("p"),pt=a("Finally, let\u2019s see what our metrics look like now that training has finished:"),kt=c(),y($e.$$.fragment),et=c(),y(Ge.$$.fragment),Se=c(),ye=l("p"),vt=a("At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task \u2014 congratulations!"),this.h()},l(k){h=r(k,"P",{});var R=i(h);$=n(R,"Before we start, let\u2019s see what kind of results we get from our model without any training:"),R.forEach(t),m=d(k),j(q.$$.fragment,k),A=d(k),j(g.$$.fragment,k),z=d(k),P=r(k,"P",{});var tt=i(P);D=n(tt,"Once this is done, we can prepare everything we need to compile and train our model. Note the use of "),T=r(tt,"CODE",{});var pe=i(T);N=n(pe,'tf.keras.mixed_precision.set_global_policy("mixed_float16")'),pe.forEach(t),v=n(tt," \u2014 this will tell Keras to train using float16, which can give a significant speedup on GPUs that support it (Nvidia 20xx/V100 or newer)."),tt.forEach(t),S=d(k),j(H.$$.fragment,k),U=d(k),F=r(k,"P",{});var Xe=i(F);X=n(Xe,"Next, we define a "),M=r(Xe,"CODE",{});var ht=i(M);I=n(ht,"PushToHubCallback"),ht.forEach(t),Y=n(Xe," to upload our model to the Hub during training, as we saw in "),Z=r(Xe,"A",{href:!0});var je=i(Z);re=n(je,"section 2"),je.forEach(t),he=n(Xe,", and then we simply fit the model with that callback:"),Xe.forEach(t),Q=d(k),j(ee.$$.fragment,k),te=d(k),G=r(k,"P",{});var ke=i(G);ne=n(ke,"Note that you can specify the name of the repository you want to push to with the "),we=r(ke,"CODE",{});var yt=i(we);V=n(yt,"hub_model_id"),yt.forEach(t),se=n(ke," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),de=r(ke,"A",{href:!0,rel:!0});var ge=i(de);be=r(ge,"CODE",{});var Oe=i(be);oe=n(Oe,"huggingface-course"),Oe.forEach(t),ie=n(ge," organization"),ge.forEach(t),W=n(ke,", we added "),ae=r(ke,"CODE",{});var Fe=i(ae);Te=n(Fe,'hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"'),Fe.forEach(t),ue=n(ke," to "),K=r(ke,"CODE",{});var Tt=i(K);ve=n(Tt,"Seq2SeqTrainingArguments"),Tt.forEach(t),le=n(ke,". By default, the repository used will be in your namespace and named after the output directory you set, so here it will be "),Ee=r(ke,"CODE",{});var ce=i(Ee);Ye=n(ce,'"sgugger/marian-finetuned-kde4-en-to-fr"'),ce.forEach(t),fe=n(ke," (which is the model we linked to at the beginning of this section)."),ke.forEach(t),xe=d(k),j(J.$$.fragment,k),Ae=d(k),me=r(k,"P",{});var ct=i(me);pt=n(ct,"Finally, let\u2019s see what our metrics look like now that training has finished:"),ct.forEach(t),kt=d(k),j($e.$$.fragment,k),et=d(k),j(Ge.$$.fragment,k),Se=d(k),ye=r(k,"P",{});var st=i(ye);vt=n(st,"At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task \u2014 congratulations!"),st.forEach(t),this.h()},h(){_(Z,"href","(/course/chapter7/2)"),_(de,"href","https://huggingface.co/huggingface-course"),_(de,"rel","nofollow")},m(k,R){o(k,h,R),s(h,$),o(k,m,R),E(q,k,R),o(k,A,R),E(g,k,R),o(k,z,R),o(k,P,R),s(P,D),s(P,T),s(T,N),s(P,v),o(k,S,R),E(H,k,R),o(k,U,R),o(k,F,R),s(F,X),s(F,M),s(M,I),s(F,Y),s(F,Z),s(Z,re),s(F,he),o(k,Q,R),E(ee,k,R),o(k,te,R),o(k,G,R),s(G,ne),s(G,we),s(we,V),s(G,se),s(G,de),s(de,be),s(be,oe),s(de,ie),s(G,W),s(G,ae),s(ae,Te),s(G,ue),s(G,K),s(K,ve),s(G,le),s(G,Ee),s(Ee,Ye),s(G,fe),o(k,xe,R),E(J,k,R),o(k,Ae,R),o(k,me,R),s(me,pt),o(k,kt,R),E($e,k,R),o(k,et,R),E(Ge,k,R),o(k,Se,R),o(k,ye,R),s(ye,vt),_e=!0},i(k){_e||(w(q.$$.fragment,k),w(g.$$.fragment,k),w(H.$$.fragment,k),w(ee.$$.fragment,k),w(J.$$.fragment,k),w($e.$$.fragment,k),w(Ge.$$.fragment,k),_e=!0)},o(k){b(q.$$.fragment,k),b(g.$$.fragment,k),b(H.$$.fragment,k),b(ee.$$.fragment,k),b(J.$$.fragment,k),b($e.$$.fragment,k),b(Ge.$$.fragment,k),_e=!1},d(k){k&&t(h),k&&t(m),x(q,k),k&&t(A),x(g,k),k&&t(z),k&&t(P),k&&t(S),x(H,k),k&&t(U),k&&t(F),k&&t(Q),x(ee,k),k&&t(te),k&&t(G),k&&t(xe),x(J,k),k&&t(Ae),k&&t(me),k&&t(kt),x($e,k),k&&t(et),x(Ge,k),k&&t(Se),k&&t(ye)}}}function Wd(B){let h,$,m,q,A;return{c(){h=l("p"),$=a("\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when defining your "),m=l("code"),q=a("Seq2SeqTrainer"),A=a(" and will need to set a new name.")},l(g){h=r(g,"P",{});var z=i(h);$=n(z,"\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when defining your "),m=r(z,"CODE",{});var P=i(m);q=n(P,"Seq2SeqTrainer"),P.forEach(t),A=n(z," and will need to set a new name."),z.forEach(t)},m(g,z){o(g,h,z),s(h,$),s(h,m),s(m,q),s(h,A)},d(g){g&&t(h)}}}function Md(B){let h,$,m,q,A;return{c(){h=l("p"),$=a("\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when calling "),m=l("code"),q=a("model.fit()"),A=a(" and will need to set a new name.")},l(g){h=r(g,"P",{});var z=i(h);$=n(z,"\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when calling "),m=r(z,"CODE",{});var P=i(m);q=n(P,"model.fit()"),P.forEach(t),A=n(z," and will need to set a new name."),z.forEach(t)},m(g,z){o(g,h,z),s(h,$),s(h,m),s(m,q),s(h,A)},d(g){g&&t(h)}}}function _d(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we,V,se,de,be,oe,ie,W,ae,Te,ue,K,ve,le,Ee,Ye,fe,xe,J,Ae,me,pt,kt,$e,et,Ge,Se,ye,vt,_e,k,R,tt,pe,Xe,ht,je,ke,yt,ge,Oe,Fe,Tt,ce,ct,st,Ie,cs,Le,$s,ds,dt,ut,ft,at,Ne,zt,mt,Vt,nt,He,Qt,qe,es,ze,Dt,jt,ks,It,We,vs,Lt,ot,De,Me,_t,Pt,Nt,gt,ts,Ue,lt,Ht,wt,us,At,bt,ss,Ze,fs,Be,f,O,ys,rt,da,js,Pe,Ks,as,St,Re,Wt,Ct,Je,Ke,Ot,Es,Ys,ns,Ve,Gs,$t,Xs,Zs,os,Mt,xs,Et,qs,Ce,Ts,xt,zs,Ut,ls,Bt,Za,Js,Ds,xa,Rt,ua,qa,Ps,fa;return q=new hs({}),Z=new hs({}),ie=new C({props:{code:`from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=<span class="hljs-number">8</span>
)`}}),K=new C({props:{code:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",highlighted:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"}}),fe=new C({props:{code:`from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`}}),ye=new C({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),Fe=new C({props:{code:`from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">3</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),Ne=new C({props:{code:`from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, get_full_repo_name

model_name = <span class="hljs-string">&quot;marian-finetuned-kde4-en-to-fr-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),mt=new C({props:{code:"'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'",highlighted:'<span class="hljs-string">&#x27;sgugger/marian-finetuned-kde4-en-to-fr-accelerate&#x27;</span>'}}),qe=new C({props:{code:`output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`output_dir = <span class="hljs-string">&quot;marian-finetuned-kde4-en-to-fr-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),Pt=new hs({}),Be=new C({props:{code:`def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess</span>(<span class="hljs-params">predictions, labels</span>):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them.</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Some simple post-processing</span>
    decoded_preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [[label.strip()] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    <span class="hljs-keyword">return</span> decoded_preds, decoded_labels`}}),xt=new C({props:{code:`from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Training</span>
    model.train()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
                attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
                max_length=<span class="hljs-number">128</span>,
            )
        labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

        <span class="hljs-comment"># Necessary to pad predictions and labels for being gathered</span>
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=<span class="hljs-number">1</span>, pad_index=-<span class="hljs-number">100</span>)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;epoch <span class="hljs-subst">{epoch}</span>, BLEU score: <span class="hljs-subst">{results[<span class="hljs-string">&#x27;score&#x27;</span>]:<span class="hljs-number">.2</span>f}</span>&quot;</span>)

    <span class="hljs-comment"># Save and upload</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),Ut=new C({props:{code:`epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44`,highlighted:`epoch <span class="hljs-number">0</span>, BLEU score: <span class="hljs-number">53.47</span>
epoch <span class="hljs-number">1</span>, BLEU score: <span class="hljs-number">54.24</span>
epoch <span class="hljs-number">2</span>, BLEU score: <span class="hljs-number">54.44</span>`}}),{c(){h=l("h2"),$=l("a"),m=l("span"),y(q.$$.fragment),A=c(),g=l("span"),z=a("A custom training loop"),P=c(),D=l("p"),T=a("Let\u2019s now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in "),N=l("a"),v=a("section 2"),S=a(" and "),H=l("a"),U=a("Chapter 3"),F=a("."),X=c(),M=l("h3"),I=l("a"),Y=l("span"),y(Z.$$.fragment),re=c(),he=l("span"),Q=a("Preparing everything for training"),ee=c(),te=l("p"),G=a("You\u2019ve seen all of this a few times now, so we\u2019ll go through the code quite quickly. First we\u2019ll build the "),ne=l("code"),we=a("DataLoader"),V=a("s from our datasets, after setting the datasets to the "),se=l("code"),de=a('"torch"'),be=a(" format so we get PyTorch tensors:"),oe=c(),y(ie.$$.fragment),W=c(),ae=l("p"),Te=a("Next we reinstantiate our model, to make sure we\u2019re not continuing the fine-tuning from before but starting from the pretrained model again:"),ue=c(),y(K.$$.fragment),ve=c(),le=l("p"),Ee=a("Then we will need an optimizer:"),Ye=c(),y(fe.$$.fragment),xe=c(),J=l("p"),Ae=a("Once we have all those objects, we can send them to the "),me=l("code"),pt=a("accelerator.prepare()"),kt=a(" method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn\u2019t execute any cell that instantiates an "),$e=l("code"),et=a("Accelerator"),Ge=a("."),Se=c(),y(ye.$$.fragment),vt=c(),_e=l("p"),k=a("Now that we have sent our "),R=l("code"),tt=a("train_dataloader"),pe=a(" to "),Xe=l("code"),ht=a("accelerator.prepare()"),je=a(", we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the "),ke=l("code"),yt=a("DataLoader"),ge=a(". We use a classic linear schedule from the learning rate to 0:"),Oe=c(),y(Fe.$$.fragment),Tt=c(),ce=l("p"),ct=a("Lastly, to push our model to the Hub, we will need to create a "),st=l("code"),Ie=a("Repository"),cs=a(" object in a working folder. First log in to the Hugging Face Hub, if you\u2019re not logged in already. We\u2019ll determine the repository name from the model ID we want to give our model (feel free to replace the "),Le=l("code"),$s=a("repo_name"),ds=a(" with your own choice; it just needs to contain your username, which is what the function "),dt=l("code"),ut=a("get_full_repo_name()"),ft=a(" does):"),at=c(),y(Ne.$$.fragment),zt=c(),y(mt.$$.fragment),Vt=c(),nt=l("p"),He=a("Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with:"),Qt=c(),y(qe.$$.fragment),es=c(),ze=l("p"),Dt=a("We can now upload anything we save in "),jt=l("code"),ks=a("output_dir"),It=a(" by calling the "),We=l("code"),vs=a("repo.push_to_hub()"),Lt=a(" method. This will help us upload the intermediate models at the end of each epoch."),ot=c(),De=l("h3"),Me=l("a"),_t=l("span"),y(Pt.$$.fragment),Nt=c(),gt=l("span"),ts=a("Training loop"),Ue=c(),lt=l("p"),Ht=a("We are now ready to write the full training loop. To simplify its evaluation part, we define this "),wt=l("code"),us=a("postprocess()"),At=a(" function that takes predictions and labels and converts them to the lists of strings our "),bt=l("code"),ss=a("metric"),Ze=a(" object will expect:"),fs=c(),y(Be.$$.fragment),f=c(),O=l("p"),ys=a("The training loop looks a lot like the ones in "),rt=l("a"),da=a("section 2"),js=a(" and "),Pe=l("a"),Ks=a("Chapter 3"),as=a(", with a few differences in the evaluation part \u2014 so let\u2019s focus on that!"),St=c(),Re=l("p"),Wt=a("The first thing to note is that we use the "),Ct=l("code"),Je=a("generate()"),Ke=a(" method to compute predictions, but this is a method on our base model, not the wrapped model \u{1F917} Accelerate created in the "),Ot=l("code"),Es=a("prepare()"),Ys=a(" method. That\u2019s why we unwrap the model first, then call this method."),ns=c(),Ve=l("p"),Gs=a("The second thing is that, like with "),$t=l("a"),Xs=a("token classification"),Zs=a(", two processes may have padded the inputs and labels to different shapes, so we use "),os=l("code"),Mt=a("accelerator.pad_across_processes()"),xs=a(" to make the predictions and labels the same shape before calling the "),Et=l("code"),qs=a("gather()"),Ce=a(" method. If we don\u2019t do this, the evaluation will either error out or hang forever."),Ts=c(),y(xt.$$.fragment),zs=c(),y(Ut.$$.fragment),ls=c(),Bt=l("p"),Za=a("Once this is done, you should have a model that has results pretty similar to the one trained with the "),Js=l("code"),Ds=a("Seq2SeqTrainer"),xa=a(". You can check the one we trained using this code at "),Rt=l("a"),ua=l("em"),qa=a("huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate"),Ps=a(". And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!"),this.h()},l(u){h=r(u,"H2",{class:!0});var L=i(h);$=r(L,"A",{id:!0,class:!0,href:!0});var ma=i($);m=r(ma,"SPAN",{});var oo=i(m);j(q.$$.fragment,oo),oo.forEach(t),ma.forEach(t),A=d(L),g=r(L,"SPAN",{});var Ja=i(g);z=n(Ja,"A custom training loop"),Ja.forEach(t),L.forEach(t),P=d(u),D=r(u,"P",{});var Kt=i(D);T=n(Kt,"Let\u2019s now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in "),N=r(Kt,"A",{href:!0});var Va=i(N);v=n(Va,"section 2"),Va.forEach(t),S=n(Kt," and "),H=r(Kt,"A",{href:!0});var Vs=i(H);U=n(Vs,"Chapter 3"),Vs.forEach(t),F=n(Kt,"."),Kt.forEach(t),X=d(u),M=r(u,"H3",{class:!0});var _a=i(M);I=r(_a,"A",{id:!0,class:!0,href:!0});var ga=i(I);Y=r(ga,"SPAN",{});var lo=i(Y);j(Z.$$.fragment,lo),lo.forEach(t),ga.forEach(t),re=d(_a),he=r(_a,"SPAN",{});var Qa=i(he);Q=n(Qa,"Preparing everything for training"),Qa.forEach(t),_a.forEach(t),ee=d(u),te=r(u,"P",{});var Yt=i(te);G=n(Yt,"You\u2019ve seen all of this a few times now, so we\u2019ll go through the code quite quickly. First we\u2019ll build the "),ne=r(Yt,"CODE",{});var en=i(ne);we=n(en,"DataLoader"),en.forEach(t),V=n(Yt,"s from our datasets, after setting the datasets to the "),se=r(Yt,"CODE",{});var Qs=i(se);de=n(Qs,'"torch"'),Qs.forEach(t),be=n(Yt," format so we get PyTorch tensors:"),Yt.forEach(t),oe=d(u),j(ie.$$.fragment,u),W=d(u),ae=r(u,"P",{});var tn=i(ae);Te=n(tn,"Next we reinstantiate our model, to make sure we\u2019re not continuing the fine-tuning from before but starting from the pretrained model again:"),tn.forEach(t),ue=d(u),j(K.$$.fragment,u),ve=d(u),le=r(u,"P",{});var wa=i(le);Ee=n(wa,"Then we will need an optimizer:"),wa.forEach(t),Ye=d(u),j(fe.$$.fragment,u),xe=d(u),J=r(u,"P",{});var ea=i(J);Ae=n(ea,"Once we have all those objects, we can send them to the "),me=r(ea,"CODE",{});var sn=i(me);pt=n(sn,"accelerator.prepare()"),sn.forEach(t),kt=n(ea," method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn\u2019t execute any cell that instantiates an "),$e=r(ea,"CODE",{});var ta=i($e);et=n(ta,"Accelerator"),ta.forEach(t),Ge=n(ea,"."),ea.forEach(t),Se=d(u),j(ye.$$.fragment,u),vt=d(u),_e=r(u,"P",{});var rs=i(_e);k=n(rs,"Now that we have sent our "),R=r(rs,"CODE",{});var sa=i(R);tt=n(sa,"train_dataloader"),sa.forEach(t),pe=n(rs," to "),Xe=r(rs,"CODE",{});var an=i(Xe);ht=n(an,"accelerator.prepare()"),an.forEach(t),je=n(rs,", we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the "),ke=r(rs,"CODE",{});var ba=i(ke);yt=n(ba,"DataLoader"),ba.forEach(t),ge=n(rs,". We use a classic linear schedule from the learning rate to 0:"),rs.forEach(t),Oe=d(u),j(Fe.$$.fragment,u),Tt=d(u),ce=r(u,"P",{});var ms=i(ce);ct=n(ms,"Lastly, to push our model to the Hub, we will need to create a "),st=r(ms,"CODE",{});var nn=i(st);Ie=n(nn,"Repository"),nn.forEach(t),cs=n(ms," object in a working folder. First log in to the Hugging Face Hub, if you\u2019re not logged in already. We\u2019ll determine the repository name from the model ID we want to give our model (feel free to replace the "),Le=r(ms,"CODE",{});var aa=i(Le);$s=n(aa,"repo_name"),aa.forEach(t),ds=n(ms," with your own choice; it just needs to contain your username, which is what the function "),dt=r(ms,"CODE",{});var on=i(dt);ut=n(on,"get_full_repo_name()"),on.forEach(t),ft=n(ms," does):"),ms.forEach(t),at=d(u),j(Ne.$$.fragment,u),zt=d(u),j(mt.$$.fragment,u),Vt=d(u),nt=r(u,"P",{});var As=i(nt);He=n(As,"Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with:"),As.forEach(t),Qt=d(u),j(qe.$$.fragment,u),es=d(u),ze=r(u,"P",{});var Ss=i(ze);Dt=n(Ss,"We can now upload anything we save in "),jt=r(Ss,"CODE",{});var _s=i(jt);ks=n(_s,"output_dir"),_s.forEach(t),It=n(Ss," by calling the "),We=r(Ss,"CODE",{});var Cs=i(We);vs=n(Cs,"repo.push_to_hub()"),Cs.forEach(t),Lt=n(Ss," method. This will help us upload the intermediate models at the end of each epoch."),Ss.forEach(t),ot=d(u),De=r(u,"H3",{class:!0});var na=i(De);Me=r(na,"A",{id:!0,class:!0,href:!0});var oa=i(Me);_t=r(oa,"SPAN",{});var ro=i(_t);j(Pt.$$.fragment,ro),ro.forEach(t),oa.forEach(t),Nt=d(na),gt=r(na,"SPAN",{});var Ta=i(gt);ts=n(Ta,"Training loop"),Ta.forEach(t),na.forEach(t),Ue=d(u),lt=r(u,"P",{});var la=i(lt);Ht=n(la,"We are now ready to write the full training loop. To simplify its evaluation part, we define this "),wt=r(la,"CODE",{});var ln=i(wt);us=n(ln,"postprocess()"),ln.forEach(t),At=n(la," function that takes predictions and labels and converts them to the lists of strings our "),bt=r(la,"CODE",{});var ra=i(bt);ss=n(ra,"metric"),ra.forEach(t),Ze=n(la," object will expect:"),la.forEach(t),fs=d(u),j(Be.$$.fragment,u),f=d(u),O=r(u,"P",{});var Os=i(O);ys=n(Os,"The training loop looks a lot like the ones in "),rt=r(Os,"A",{href:!0});var is=i(rt);da=n(is,"section 2"),is.forEach(t),js=n(Os," and "),Pe=r(Os,"A",{href:!0});var io=i(Pe);Ks=n(io,"Chapter 3"),io.forEach(t),as=n(Os,", with a few differences in the evaluation part \u2014 so let\u2019s focus on that!"),Os.forEach(t),St=d(u),Re=r(u,"P",{});var gs=i(Re);Wt=n(gs,"The first thing to note is that we use the "),Ct=r(gs,"CODE",{});var po=i(Ct);Je=n(po,"generate()"),po.forEach(t),Ke=n(gs," method to compute predictions, but this is a method on our base model, not the wrapped model \u{1F917} Accelerate created in the "),Ot=r(gs,"CODE",{});var ho=i(Ot);Es=n(ho,"prepare()"),ho.forEach(t),Ys=n(gs," method. That\u2019s why we unwrap the model first, then call this method."),gs.forEach(t),ns=d(u),Ve=r(u,"P",{});var qt=i(Ve);Gs=n(qt,"The second thing is that, like with "),$t=r(qt,"A",{href:!0});var co=i($t);Xs=n(co,"token classification"),co.forEach(t),Zs=n(qt,", two processes may have padded the inputs and labels to different shapes, so we use "),os=r(qt,"CODE",{});var uo=i(os);Mt=n(uo,"accelerator.pad_across_processes()"),uo.forEach(t),xs=n(qt," to make the predictions and labels the same shape before calling the "),Et=r(qt,"CODE",{});var rn=i(Et);qs=n(rn,"gather()"),rn.forEach(t),Ce=n(qt," method. If we don\u2019t do this, the evaluation will either error out or hang forever."),qt.forEach(t),Ts=d(u),j(xt.$$.fragment,u),zs=d(u),j(Ut.$$.fragment,u),ls=d(u),Bt=r(u,"P",{});var Gt=i(Bt);Za=n(Gt,"Once this is done, you should have a model that has results pretty similar to the one trained with the "),Js=r(Gt,"CODE",{});var pn=i(Js);Ds=n(pn,"Seq2SeqTrainer"),pn.forEach(t),xa=n(Gt,". You can check the one we trained using this code at "),Rt=r(Gt,"A",{href:!0,rel:!0});var ps=i(Rt);ua=r(ps,"EM",{});var fo=i(ua);qa=n(fo,"huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate"),fo.forEach(t),ps.forEach(t),Ps=n(Gt,". And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!"),Gt.forEach(t),this.h()},h(){_($,"id","a-custom-training-loop"),_($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_($,"href","#a-custom-training-loop"),_(h,"class","relative group"),_(N,"href","/course/chapter7/2"),_(H,"href","/course/chapter3/4"),_(I,"id","preparing-everything-for-training"),_(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(I,"href","#preparing-everything-for-training"),_(M,"class","relative group"),_(Me,"id","training-loop"),_(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Me,"href","#training-loop"),_(De,"class","relative group"),_(rt,"href","/course/chapter7/2"),_(Pe,"href","/course/chapter3"),_($t,"href","/course/chapter7/2"),_(Rt,"href","https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate"),_(Rt,"rel","nofollow")},m(u,L){o(u,h,L),s(h,$),s($,m),E(q,m,null),s(h,A),s(h,g),s(g,z),o(u,P,L),o(u,D,L),s(D,T),s(D,N),s(N,v),s(D,S),s(D,H),s(H,U),s(D,F),o(u,X,L),o(u,M,L),s(M,I),s(I,Y),E(Z,Y,null),s(M,re),s(M,he),s(he,Q),o(u,ee,L),o(u,te,L),s(te,G),s(te,ne),s(ne,we),s(te,V),s(te,se),s(se,de),s(te,be),o(u,oe,L),E(ie,u,L),o(u,W,L),o(u,ae,L),s(ae,Te),o(u,ue,L),E(K,u,L),o(u,ve,L),o(u,le,L),s(le,Ee),o(u,Ye,L),E(fe,u,L),o(u,xe,L),o(u,J,L),s(J,Ae),s(J,me),s(me,pt),s(J,kt),s(J,$e),s($e,et),s(J,Ge),o(u,Se,L),E(ye,u,L),o(u,vt,L),o(u,_e,L),s(_e,k),s(_e,R),s(R,tt),s(_e,pe),s(_e,Xe),s(Xe,ht),s(_e,je),s(_e,ke),s(ke,yt),s(_e,ge),o(u,Oe,L),E(Fe,u,L),o(u,Tt,L),o(u,ce,L),s(ce,ct),s(ce,st),s(st,Ie),s(ce,cs),s(ce,Le),s(Le,$s),s(ce,ds),s(ce,dt),s(dt,ut),s(ce,ft),o(u,at,L),E(Ne,u,L),o(u,zt,L),E(mt,u,L),o(u,Vt,L),o(u,nt,L),s(nt,He),o(u,Qt,L),E(qe,u,L),o(u,es,L),o(u,ze,L),s(ze,Dt),s(ze,jt),s(jt,ks),s(ze,It),s(ze,We),s(We,vs),s(ze,Lt),o(u,ot,L),o(u,De,L),s(De,Me),s(Me,_t),E(Pt,_t,null),s(De,Nt),s(De,gt),s(gt,ts),o(u,Ue,L),o(u,lt,L),s(lt,Ht),s(lt,wt),s(wt,us),s(lt,At),s(lt,bt),s(bt,ss),s(lt,Ze),o(u,fs,L),E(Be,u,L),o(u,f,L),o(u,O,L),s(O,ys),s(O,rt),s(rt,da),s(O,js),s(O,Pe),s(Pe,Ks),s(O,as),o(u,St,L),o(u,Re,L),s(Re,Wt),s(Re,Ct),s(Ct,Je),s(Re,Ke),s(Re,Ot),s(Ot,Es),s(Re,Ys),o(u,ns,L),o(u,Ve,L),s(Ve,Gs),s(Ve,$t),s($t,Xs),s(Ve,Zs),s(Ve,os),s(os,Mt),s(Ve,xs),s(Ve,Et),s(Et,qs),s(Ve,Ce),o(u,Ts,L),E(xt,u,L),o(u,zs,L),E(Ut,u,L),o(u,ls,L),o(u,Bt,L),s(Bt,Za),s(Bt,Js),s(Js,Ds),s(Bt,xa),s(Bt,Rt),s(Rt,ua),s(ua,qa),s(Bt,Ps),fa=!0},i(u){fa||(w(q.$$.fragment,u),w(Z.$$.fragment,u),w(ie.$$.fragment,u),w(K.$$.fragment,u),w(fe.$$.fragment,u),w(ye.$$.fragment,u),w(Fe.$$.fragment,u),w(Ne.$$.fragment,u),w(mt.$$.fragment,u),w(qe.$$.fragment,u),w(Pt.$$.fragment,u),w(Be.$$.fragment,u),w(xt.$$.fragment,u),w(Ut.$$.fragment,u),fa=!0)},o(u){b(q.$$.fragment,u),b(Z.$$.fragment,u),b(ie.$$.fragment,u),b(K.$$.fragment,u),b(fe.$$.fragment,u),b(ye.$$.fragment,u),b(Fe.$$.fragment,u),b(Ne.$$.fragment,u),b(mt.$$.fragment,u),b(qe.$$.fragment,u),b(Pt.$$.fragment,u),b(Be.$$.fragment,u),b(xt.$$.fragment,u),b(Ut.$$.fragment,u),fa=!1},d(u){u&&t(h),x(q),u&&t(P),u&&t(D),u&&t(X),u&&t(M),x(Z),u&&t(ee),u&&t(te),u&&t(oe),x(ie,u),u&&t(W),u&&t(ae),u&&t(ue),x(K,u),u&&t(ve),u&&t(le),u&&t(Ye),x(fe,u),u&&t(xe),u&&t(J),u&&t(Se),x(ye,u),u&&t(vt),u&&t(_e),u&&t(Oe),x(Fe,u),u&&t(Tt),u&&t(ce),u&&t(at),x(Ne,u),u&&t(zt),x(mt,u),u&&t(Vt),u&&t(nt),u&&t(Qt),x(qe,u),u&&t(es),u&&t(ze),u&&t(ot),u&&t(De),x(Pt),u&&t(Ue),u&&t(lt),u&&t(fs),x(Be,u),u&&t(f),u&&t(O),u&&t(St),u&&t(Re),u&&t(ns),u&&t(Ve),u&&t(Ts),x(xt,u),u&&t(zs),x(Ut,u),u&&t(ls),u&&t(Bt)}}}function Ud(B){let h,$,m,q,A;return{c(){h=l("p"),$=a("\u270F\uFE0F "),m=l("strong"),q=a("Your turn!"),A=a(" What does the model return on the sample with the word \u201Cemail\u201D you identified earlier?")},l(g){h=r(g,"P",{});var z=i(h);$=n(z,"\u270F\uFE0F "),m=r(z,"STRONG",{});var P=i(m);q=n(P,"Your turn!"),P.forEach(t),A=n(z," What does the model return on the sample with the word \u201Cemail\u201D you identified earlier?"),z.forEach(t)},m(g,z){o(g,h,z),s(h,$),s(h,m),s(m,q),s(h,A)},d(g){g&&t(h)}}}function Bd(B){let h,$,m,q,A,g,z,P,D,T,N,v,S,H,U,F,X,M,I,Y,Z,re,he,Q,ee,te,G,ne,we,V,se,de,be,oe,ie,W,ae,Te,ue,K,ve,le,Ee,Ye,fe,xe,J,Ae,me,pt,kt,$e,et,Ge,Se,ye,vt,_e,k,R,tt,pe,Xe,ht,je,ke,yt,ge,Oe,Fe,Tt,ce,ct,st,Ie,cs,Le,$s,ds,dt,ut,ft,at,Ne,zt,mt,Vt,nt,He,Qt,qe,es,ze,Dt,jt,ks,It,We,vs,Lt,ot,De,Me,_t,Pt,Nt,gt,ts,Ue,lt,Ht,wt,us,At,bt,ss,Ze,fs,Be,f,O,ys,rt,da,js,Pe,Ks,as,St,Re,Wt,Ct,Je,Ke,Ot,Es,Ys,ns,Ve,Gs,$t,Xs,Zs,os,Mt,xs,Et,qs,Ce,Ts,xt,zs,Ut,ls,Bt,Za,Js,Ds,xa,Rt,ua,qa,Ps,fa,u,L,ma,oo,Ja,Kt,Va,Vs,_a,ga,lo,Qa,Yt,en,Qs,tn,wa,ea,sn,ta,rs,sa,an,ba,ms,nn,aa,on,As,Ss,_s,Cs,na,oa,ro,Ta,la,ln,ra,Os,is,io,gs,po,ho,qt,co,uo,rn,Gt,pn,ps,fo,Qo,_i,gi,hn,wi,bi,Ol,za,Fl,Da,$i,el,ki,vi,Il,Pa,yi,tl,ji,Ei,Ll,cn,Nl,ia,xi,sl,qi,Ti,al,zi,Di,Hl,Aa,Pi,nl,Ai,Si,Wl,mo,Ci,Ml,dn,Ul,_o,Oi,Bl,un,Rl,fn,Kl,go,Fi,Yl,ws,Ii,ol,Li,Ni,ll,Hi,Wi,rl,Mi,Ui,Gl,mn,Xl,wo,Bi,Zl,Sa,Jl,Ca,Vl,bo,Ri,Ql,_n,er,$o,Ki,tr,Fs,Is,ko,vo,Yi,sr,$a,Oa,il,gn,Gi,pl,Xi,ar,bs,Zi,hl,Ji,Vi,yo,Qi,ep,cl,tp,sp,nr,it,ap,wn,dl,np,op,ul,lp,rp,fl,ip,pp,ml,hp,cp,_l,dp,up,gl,fp,mp,or,Ls,Ns,jo,Eo,_p,lr,bn,rr,$n,ir,Fa,gp,wl,wp,bp,pr,kn,hr,vn,cr,xo,$p,dr,yn,ur,jn,fr,qo,kp,mr,En,_r,xn,gr,Hs,Ws,To,ka,Ia,bl,qn,vp,$l,yp,wr,Tn,br,zo,Xt,jp,zn,Ep,xp,Dn,qp,Tp,kl,zp,Dp,vl,Pp,Ap,$r,La,Sp,Pn,Cp,Op,kr,An,vr,pa,Fp,yl,Ip,Lp,Do,Np,Hp,yr,Sn,jr,Po,Wp,Er,Ao,Mp,xr,Cn,qr,On,Tr,Zt,Up,Fn,Bp,Rp,jl,Kp,Yp,El,Gp,Xp,In,Zp,Jp,zr,Ln,Dr,Nn,Pr,Hn,Ar,Wn,Sr,So,Vp,Cr,Ms,Us,Co,Oo,Qp,Or,va,Na,xl,Mn,eh,ql,th,Fr,Fo,sh,Ir,Un,Lr,Io,ah,Nr,Lo,nh,Hr,Bn,Wr,Bs,Rs,No,Ho,ya,Ha,Tl,Rn,oh,zl,lh,Mr,Wa,rh,Dl,ih,ph,Ur,Kn,Br,Yn,Rr,Wo,hh,Kr,Gn,Yr,Xn,Gr,Mo,ch,Xr,Ma,Zr;m=new yd({props:{fw:B[0]}}),P=new hs({});const dh=[Ed,jd],Zn=[];function uh(e,p){return e[0]==="pt"?0:1}S=uh(B),H=Zn[S]=dh[S](B),ue=new mi({props:{id:"1JvfrvZgi6c"}}),Ne=new hs({}),_t=new hs({}),bt=new C({props:{code:`from datasets import load_dataset, load_metric

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric

raw_datasets = load_dataset(<span class="hljs-string">&quot;kde4&quot;</span>, lang1=<span class="hljs-string">&quot;en&quot;</span>, lang2=<span class="hljs-string">&quot;fr&quot;</span>)`}}),St=new C({props:{code:"raw_datasets",highlighted:"raw_datasets"}}),Wt=new C({props:{code:`DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;translation&#x27;</span>],
        num_rows: <span class="hljs-number">210173</span>
    })
})`}}),Mt=new C({props:{code:`split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets`,highlighted:`split_datasets = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(train_size=<span class="hljs-number">0.9</span>, seed=<span class="hljs-number">20</span>)
split_datasets`}}),Et=new C({props:{code:`DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;translation&#x27;</span>],
        num_rows: <span class="hljs-number">189155</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;translation&#x27;</span>],
        num_rows: <span class="hljs-number">21018</span>
    })
})`}}),Ds=new C({props:{code:'split_datasets["validation"] = split_datasets.pop("test")',highlighted:'split_datasets[<span class="hljs-string">&quot;validation&quot;</span>] = split_datasets.pop(<span class="hljs-string">&quot;test&quot;</span>)'}}),Ps=new C({props:{code:'split_datasets["train"][1]["translation"]',highlighted:'split_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;translation&quot;</span>]'}}),u=new C({props:{code:`{'en': 'Default to expanded threads',
 'fr': 'Par d\xE9faut, d\xE9velopper les fils de discussion'}`,highlighted:`{<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;Default to expanded threads&#x27;</span>,
 <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Par d\xE9faut, d\xE9velopper les fils de discussion&#x27;</span>}`}}),Kt=new C({props:{code:`from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

model_checkpoint = <span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-fr&quot;</span>
translator = pipeline(<span class="hljs-string">&quot;translation&quot;</span>, model=model_checkpoint)
translator(<span class="hljs-string">&quot;Default to expanded threads&quot;</span>)`}}),Vs=new C({props:{code:"[{'translation_text': 'Par d\xE9faut pour les threads \xE9largis'}]",highlighted:'[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;Par d\xE9faut pour les threads \xE9largis&#x27;</span>}]'}}),Yt=new C({props:{code:'split_datasets["train"][172]["translation"]',highlighted:'split_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">172</span>][<span class="hljs-string">&quot;translation&quot;</span>]'}}),Qs=new C({props:{code:`{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}`,highlighted:`{<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;Unable to import %1 using the OFX importer plugin. This file is not the correct format.&#x27;</span>,
 <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&quot;Impossible d&#x27;importer %1 en utilisant le module d&#x27;extension d&#x27;importation OFX. Ce fichier n&#x27;a pas un format correct.&quot;</span>}`}}),ta=new C({props:{code:`translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)`,highlighted:`translator(
    <span class="hljs-string">&quot;Unable to import %1 using the OFX importer plugin. This file is not the correct format.&quot;</span>
)`}}),sa=new C({props:{code:`[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]`,highlighted:'[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&quot;Impossible d&#x27;importer %1 en utilisant le plugin d&#x27;importateur OFX. Ce fichier n&#x27;est pas le bon format.&quot;</span>}]'}}),aa=new mi({props:{id:"0Oxphw4Q9fo"}}),As=new Xa({props:{$$slots:{default:[xd]},$$scope:{ctx:B}}}),oa=new hs({}),ra=new mi({props:{id:"XAR8jnZZuUs"}}),Gt=new C({props:{code:`from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-fr&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),za=new Xa({props:{$$slots:{default:[qd]},$$scope:{ctx:B}}}),cn=new C({props:{code:`with open(file_path) as f:
    content = f.read()`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path) <span class="hljs-keyword">as</span> f:
    content = f.<span class="hljs-built_in">read</span>()`}}),dn=new C({props:{code:`en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence)
with tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)`,highlighted:`en_sentence = split_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;translation&quot;</span>][<span class="hljs-string">&quot;en&quot;</span>]
fr_sentence = split_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;translation&quot;</span>][<span class="hljs-string">&quot;fr&quot;</span>]

inputs = tokenizer(en_sentence)
<span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)`}}),un=new C({props:{code:`wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(targets["input_ids"]))`,highlighted:`wrong_targets = tokenizer(fr_sentence)
<span class="hljs-built_in">print</span>(tokenizer.convert_ids_to_tokens(wrong_targets[<span class="hljs-string">&quot;input_ids&quot;</span>]))
<span class="hljs-built_in">print</span>(tokenizer.convert_ids_to_tokens(targets[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),fn=new C({props:{code:`['\u2581Par', '\u2581d\xE9', 'f', 'aut', ',', '\u2581d\xE9', 've', 'lop', 'per', '\u2581les', '\u2581fil', 's', '\u2581de', '\u2581discussion', '</s>']
['\u2581Par', '\u2581d\xE9faut', ',', '\u2581d\xE9velopper', '\u2581les', '\u2581fils', '\u2581de', '\u2581discussion', '</s>']`,highlighted:`[<span class="hljs-string">&#x27;\u2581Par&#x27;</span>, <span class="hljs-string">&#x27;\u2581d\xE9&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;aut&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u2581d\xE9&#x27;</span>, <span class="hljs-string">&#x27;ve&#x27;</span>, <span class="hljs-string">&#x27;lop&#x27;</span>, <span class="hljs-string">&#x27;per&#x27;</span>, <span class="hljs-string">&#x27;\u2581les&#x27;</span>, <span class="hljs-string">&#x27;\u2581fil&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581de&#x27;</span>, <span class="hljs-string">&#x27;\u2581discussion&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]
[<span class="hljs-string">&#x27;\u2581Par&#x27;</span>, <span class="hljs-string">&#x27;\u2581d\xE9faut&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u2581d\xE9velopper&#x27;</span>, <span class="hljs-string">&#x27;\u2581les&#x27;</span>, <span class="hljs-string">&#x27;\u2581fils&#x27;</span>, <span class="hljs-string">&#x27;\u2581de&#x27;</span>, <span class="hljs-string">&#x27;\u2581discussion&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]`}}),mn=new C({props:{code:`max_input_length = 128
max_target_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`max_input_length = <span class="hljs-number">128</span>
max_target_length = <span class="hljs-number">128</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    inputs = [ex[<span class="hljs-string">&quot;en&quot;</span>] <span class="hljs-keyword">for</span> ex <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
    targets = [ex[<span class="hljs-string">&quot;fr&quot;</span>] <span class="hljs-keyword">for</span> ex <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Set up the tokenizer for targets</span>
    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=<span class="hljs-literal">True</span>)

    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
    <span class="hljs-keyword">return</span> model_inputs`}}),Sa=new Xa({props:{$$slots:{default:[Td]},$$scope:{ctx:B}}}),Ca=new Xa({props:{warning:!0,$$slots:{default:[zd]},$$scope:{ctx:B}}}),_n=new C({props:{code:`tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)`,highlighted:`tokenized_datasets = split_datasets.<span class="hljs-built_in">map</span>(
    preprocess_function,
    batched=<span class="hljs-literal">True</span>,
    remove_columns=split_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names,
)`}});const fh=[Pd,Dd],Jn=[];function mh(e,p){return e[0]==="pt"?0:1}Fs=mh(B),Is=Jn[Fs]=fh[Fs](B),gn=new hs({});const _h=[Cd,Sd],Vn=[];function gh(e,p){return e[0]==="pt"?0:1}Ls=gh(B),Ns=Vn[Ls]=_h[Ls](B),bn=new C({props:{code:`batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()`,highlighted:`batch = data_collator([tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)])
batch.keys()`}}),$n=new C({props:{code:"dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;decoder_input_ids&#x27;</span>])'}}),kn=new C({props:{code:'batch["labels"]',highlighted:'batch[<span class="hljs-string">&quot;labels&quot;</span>]'}}),vn=new C({props:{code:`tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])`,highlighted:`tensor([[  <span class="hljs-number">577</span>,  <span class="hljs-number">5891</span>,     <span class="hljs-number">2</span>,  <span class="hljs-number">3184</span>,    <span class="hljs-number">16</span>,  <span class="hljs-number">2542</span>,     <span class="hljs-number">5</span>,  <span class="hljs-number">1710</span>,     <span class="hljs-number">0</span>,  -<span class="hljs-number">100</span>,
          -<span class="hljs-number">100</span>,  -<span class="hljs-number">100</span>,  -<span class="hljs-number">100</span>,  -<span class="hljs-number">100</span>,  -<span class="hljs-number">100</span>,  -<span class="hljs-number">100</span>],
        [ <span class="hljs-number">1211</span>,     <span class="hljs-number">3</span>,    <span class="hljs-number">49</span>,  <span class="hljs-number">9409</span>,  <span class="hljs-number">1211</span>,     <span class="hljs-number">3</span>, <span class="hljs-number">29140</span>,   <span class="hljs-number">817</span>,  <span class="hljs-number">3124</span>,   <span class="hljs-number">817</span>,
           <span class="hljs-number">550</span>,  <span class="hljs-number">7032</span>,  <span class="hljs-number">5821</span>,  <span class="hljs-number">7907</span>, <span class="hljs-number">12649</span>,     <span class="hljs-number">0</span>]])`}}),yn=new C({props:{code:'batch["decoder_input_ids"]',highlighted:'batch[<span class="hljs-string">&quot;decoder_input_ids&quot;</span>]'}}),jn=new C({props:{code:`tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])`,highlighted:`tensor([[<span class="hljs-number">59513</span>,   <span class="hljs-number">577</span>,  <span class="hljs-number">5891</span>,     <span class="hljs-number">2</span>,  <span class="hljs-number">3184</span>,    <span class="hljs-number">16</span>,  <span class="hljs-number">2542</span>,     <span class="hljs-number">5</span>,  <span class="hljs-number">1710</span>,     <span class="hljs-number">0</span>,
         <span class="hljs-number">59513</span>, <span class="hljs-number">59513</span>, <span class="hljs-number">59513</span>, <span class="hljs-number">59513</span>, <span class="hljs-number">59513</span>, <span class="hljs-number">59513</span>],
        [<span class="hljs-number">59513</span>,  <span class="hljs-number">1211</span>,     <span class="hljs-number">3</span>,    <span class="hljs-number">49</span>,  <span class="hljs-number">9409</span>,  <span class="hljs-number">1211</span>,     <span class="hljs-number">3</span>, <span class="hljs-number">29140</span>,   <span class="hljs-number">817</span>,  <span class="hljs-number">3124</span>,
           <span class="hljs-number">817</span>,   <span class="hljs-number">550</span>,  <span class="hljs-number">7032</span>,  <span class="hljs-number">5821</span>,  <span class="hljs-number">7907</span>, <span class="hljs-number">12649</span>]])`}}),En=new C({props:{code:`for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])`,highlighted:`<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>):
    <span class="hljs-built_in">print</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i][<span class="hljs-string">&quot;labels&quot;</span>])`}}),xn=new C({props:{code:`[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]`,highlighted:`[<span class="hljs-number">577</span>, <span class="hljs-number">5891</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3184</span>, <span class="hljs-number">16</span>, <span class="hljs-number">2542</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1710</span>, <span class="hljs-number">0</span>]
[<span class="hljs-number">1211</span>, <span class="hljs-number">3</span>, <span class="hljs-number">49</span>, <span class="hljs-number">9409</span>, <span class="hljs-number">1211</span>, <span class="hljs-number">3</span>, <span class="hljs-number">29140</span>, <span class="hljs-number">817</span>, <span class="hljs-number">3124</span>, <span class="hljs-number">817</span>, <span class="hljs-number">550</span>, <span class="hljs-number">7032</span>, <span class="hljs-number">5821</span>, <span class="hljs-number">7907</span>, <span class="hljs-number">12649</span>, <span class="hljs-number">0</span>]`}});const wh=[Fd,Od],Qn=[];function bh(e,p){return e[0]==="pt"?0:1}Hs=bh(B),Ws=Qn[Hs]=wh[Hs](B),qn=new hs({}),Tn=new mi({props:{id:"M05L1DhFqcw"}});let Ft=B[0]==="pt"&&md();An=new C({props:{code:"!pip install sacrebleu",highlighted:"!pip install sacrebleu"}}),Sn=new C({props:{code:`from datasets import load_metric

metric = load_metric("sacrebleu")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;sacrebleu&quot;</span>)`}}),Cn=new C({props:{code:`predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)`,highlighted:`predictions = [
    <span class="hljs-string">&quot;This plugin lets you translate web pages between several languages automatically.&quot;</span>
]
references = [
    [
        <span class="hljs-string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    ]
]
metric.compute(predictions=predictions, references=references)`}}),On=new C({props:{code:`{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">46.750469682990165</span>,
 <span class="hljs-string">&#x27;counts&#x27;</span>: [<span class="hljs-number">11</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>],
 <span class="hljs-string">&#x27;totals&#x27;</span>: [<span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>],
 <span class="hljs-string">&#x27;precisions&#x27;</span>: [<span class="hljs-number">91.67</span>, <span class="hljs-number">54.54</span>, <span class="hljs-number">40.0</span>, <span class="hljs-number">33.33</span>],
 <span class="hljs-string">&#x27;bp&#x27;</span>: <span class="hljs-number">0.9200444146293233</span>,
 <span class="hljs-string">&#x27;sys_len&#x27;</span>: <span class="hljs-number">12</span>,
 <span class="hljs-string">&#x27;ref_len&#x27;</span>: <span class="hljs-number">13</span>}`}}),Ln=new C({props:{code:`predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)`,highlighted:`predictions = [<span class="hljs-string">&quot;This This This This&quot;</span>]
references = [
    [
        <span class="hljs-string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    ]
]
metric.compute(predictions=predictions, references=references)`}}),Nn=new C({props:{code:`{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">1.683602693167689</span>,
 <span class="hljs-string">&#x27;counts&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;totals&#x27;</span>: [<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
 <span class="hljs-string">&#x27;precisions&#x27;</span>: [<span class="hljs-number">25.0</span>, <span class="hljs-number">16.67</span>, <span class="hljs-number">12.5</span>, <span class="hljs-number">12.5</span>],
 <span class="hljs-string">&#x27;bp&#x27;</span>: <span class="hljs-number">0.10539922456186433</span>,
 <span class="hljs-string">&#x27;sys_len&#x27;</span>: <span class="hljs-number">4</span>,
 <span class="hljs-string">&#x27;ref_len&#x27;</span>: <span class="hljs-number">13</span>}`}}),Hn=new C({props:{code:`predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)`,highlighted:`predictions = [<span class="hljs-string">&quot;This plugin&quot;</span>]
references = [
    [
        <span class="hljs-string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span>
    ]
]
metric.compute(predictions=predictions, references=references)`}}),Wn=new C({props:{code:`{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0</span>,
 <span class="hljs-string">&#x27;counts&#x27;</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;totals&#x27;</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;precisions&#x27;</span>: [<span class="hljs-number">100.0</span>, <span class="hljs-number">100.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],
 <span class="hljs-string">&#x27;bp&#x27;</span>: <span class="hljs-number">0.004086771438464067</span>,
 <span class="hljs-string">&#x27;sys_len&#x27;</span>: <span class="hljs-number">2</span>,
 <span class="hljs-string">&#x27;ref_len&#x27;</span>: <span class="hljs-number">13</span>}`}});const $h=[Ld,Id],eo=[];function kh(e,p){return e[0]==="tf"?0:1}Ms=kh(B),Us=eo[Ms]=$h[Ms](B),Mn=new hs({}),Un=new C({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),Bn=new C({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}});const vh=[Hd,Nd],to=[];function yh(e,p){return e[0]==="tf"?0:1}Bs=yh(B),Rs=to[Bs]=vh[Bs](B);let Qe=B[0]==="pt"&&_d();return Rn=new hs({}),Kn=new C({props:{code:`from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-comment"># Replace this with your own checkpoint</span>
model_checkpoint = <span class="hljs-string">&quot;huggingface-course/marian-finetuned-kde4-en-to-fr&quot;</span>
translator = pipeline(<span class="hljs-string">&quot;translation&quot;</span>, model=model_checkpoint)
translator(<span class="hljs-string">&quot;Default to expanded threads&quot;</span>)`}}),Yn=new C({props:{code:"[{'translation_text': 'Par d\xE9faut, d\xE9velopper les fils de discussion'}]",highlighted:'[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;Par d\xE9faut, d\xE9velopper les fils de discussion&#x27;</span>}]'}}),Gn=new C({props:{code:`translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)`,highlighted:`translator(
    <span class="hljs-string">&quot;Unable to import %1 using the OFX importer plugin. This file is not the correct format.&quot;</span>
)`}}),Xn=new C({props:{code:`[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]`,highlighted:'[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&quot;Impossible d&#x27;importer %1 en utilisant le module externe d&#x27;importation OFX. Ce fichier n&#x27;est pas le bon format.&quot;</span>}]'}}),Ma=new Xa({props:{$$slots:{default:[Ud]},$$scope:{ctx:B}}}),{c(){h=l("meta"),$=c(),y(m.$$.fragment),q=c(),A=l("h1"),g=l("a"),z=l("span"),y(P.$$.fragment),D=c(),T=l("span"),N=a("Translation"),v=c(),H.c(),U=c(),F=l("p"),X=a("Let\u2019s now dive into translation. This is another "),M=l("a"),I=a("sequence-to-sequence task"),Y=a(", which means it\u2019s a problem that can be formulated as going from one sequence to another. In that sense the problem is pretty close to "),Z=l("a"),re=a("summarization"),he=a(", and you could adapt what we will see here to other sequence-to-sequence problems such as:"),Q=c(),ee=l("ul"),te=l("li"),G=l("strong"),ne=a("Style transfer"),we=a(": Creating a model that "),V=l("em"),se=a("translates"),de=a(" texts written in a certain style to another (e.g., formal to casual or Shakespearean English to modern English)"),be=c(),oe=l("li"),ie=l("strong"),W=a("Generative question answering"),ae=a(": Creating a model that generates answers to questions, given a context"),Te=c(),y(ue.$$.fragment),K=c(),ve=l("p"),le=a("If you have a big enough corpus of texts in two (or more) languages, you can train a new translation model from scratch like we will in the section on "),Ee=l("a"),Ye=a("causal language modeling"),fe=a(". It will be faster, however, to fine-tune an existing translation model, be it a multilingual one like mT5 or mBART that you want to fine-tune to a specific language pair, or even a model specialized for translation from one language to another that you want to fine-tune to your specific corpus."),xe=c(),J=l("p"),Ae=a("In this section, we will fine-tune a Marian model pretrained to translate from English to French (since a lot of Hugging Face employees speak both those languages) on the "),me=l("a"),pt=a("KDE4 dataset"),kt=a(", which is a dataset of localized files for the "),$e=l("a"),et=a("KDE apps"),Ge=a(". The model we will use has been pretrained on a large corpus of French and English texts taken from the "),Se=l("a"),ye=a("Opus dataset"),vt=a(", which actually contains the KDE4 dataset. But even if the pretrained model we use has seen that data during its pretraining, we will see that we can get a better version of it after fine-tuning."),_e=c(),k=l("p"),R=a("Once we\u2019re finished, we will have a model able to make predictions like this one:"),tt=c(),pe=l("iframe"),ht=c(),je=l("iframe"),yt=c(),ge=l("a"),Oe=l("img"),Tt=c(),ce=l("img"),st=c(),Ie=l("p"),cs=a("As in the previous sections, you can find the actual model that we\u2019ll train and upload to the Hub using the code below and double-check its predictions "),Le=l("a"),$s=a("here"),ds=a("."),dt=c(),ut=l("h2"),ft=l("a"),at=l("span"),y(Ne.$$.fragment),zt=c(),mt=l("span"),Vt=a("Preparing the data"),nt=c(),He=l("p"),Qt=a("To fine-tune or train a translation model from scratch, we will need a dataset suitable for the task. As mentioned previously, we\u2019ll use the "),qe=l("a"),es=a("KDE4 dataset"),ze=a(" in this section, but you can adapt the code to use your own data quite easily, as long as you have pairs of sentences in the two languages you want to translate from and into. Refer back to "),Dt=l("a"),jt=a("Chapter 5"),ks=a(" if you need a reminder of how to load your custom data in a "),It=l("code"),We=a("Dataset"),vs=a("."),Lt=c(),ot=l("h3"),De=l("a"),Me=l("span"),y(_t.$$.fragment),Pt=c(),Nt=l("span"),gt=a("The KDE4 dataset"),ts=c(),Ue=l("p"),lt=a("As usual, we download our dataset using the "),Ht=l("code"),wt=a("load_dataset()"),us=a(" function:"),At=c(),y(bt.$$.fragment),ss=c(),Ze=l("p"),fs=a("If you want to work with a different pair of languages, you can specify them by their codes. A total of 92 languages are available for this dataset; you can see them all by expanding the language tags on its "),Be=l("a"),f=a("dataset card"),O=a("."),ys=c(),rt=l("img"),js=c(),Pe=l("p"),Ks=a("Let\u2019s have a look at the dataset:"),as=c(),y(St.$$.fragment),Re=c(),y(Wt.$$.fragment),Ct=c(),Je=l("p"),Ke=a("We have 210,173 pairs of sentences, but in one single split, so we will need to create our own validation set. As we saw in "),Ot=l("a"),Es=a("Chapter 5"),Ys=a(", a "),ns=l("code"),Ve=a("Dataset"),Gs=a(" has a "),$t=l("code"),Xs=a("train_test_split()"),Zs=a(" method that can help us. We\u2019ll provide a seed for reproducibility:"),os=c(),y(Mt.$$.fragment),xs=c(),y(Et.$$.fragment),qs=c(),Ce=l("p"),Ts=a("We can rename the "),xt=l("code"),zs=a('"test"'),Ut=a(" key to "),ls=l("code"),Bt=a('"validation"'),Za=a(" like this:"),Js=c(),y(Ds.$$.fragment),xa=c(),Rt=l("p"),ua=a("Now let\u2019s take a look at one element of the dataset:"),qa=c(),y(Ps.$$.fragment),fa=c(),y(u.$$.fragment),L=c(),ma=l("p"),oo=a("We get a dictionary with two sentences in the pair of languages we requested. One particularity of this dataset full of technical computer science terms is that they are all fully translated in French. However, French engineers are often lazy and leave most computer science-specific words in English when they talk. Here, for instance, the word \u201Cthreads\u201D might well appear in a French sentence, especially in a technical conversation; but in this dataset it has been translated into the more correct \u201Cfils de discussion.\u201D The pretrained model we use, which has been pretrained on a larger corpus of French and English sentences, takes the easier option of leaving the word as is:"),Ja=c(),y(Kt.$$.fragment),Va=c(),y(Vs.$$.fragment),_a=c(),ga=l("p"),lo=a(`Another example of this behavior can be seen with the word \u201Cplugin,\u201D which isn\u2019t officially a French word but which most native speakers will understand and not bother to translate.
In the KDE4 dataset this word has been translated in French into the more official \u201Cmodule d\u2019extension\u201D:`),Qa=c(),y(Yt.$$.fragment),en=c(),y(Qs.$$.fragment),tn=c(),wa=l("p"),ea=a("Our pretrained model, however, sticks with the compact and familiar English word:"),sn=c(),y(ta.$$.fragment),rs=c(),y(sa.$$.fragment),an=c(),ba=l("p"),ms=a("It will be interesting to see if our fine-tuned model picks up on those particularities of the dataset (spoiler alert: it will)."),nn=c(),y(aa.$$.fragment),on=c(),y(As.$$.fragment),Ss=c(),_s=l("h3"),Cs=l("a"),na=l("span"),y(oa.$$.fragment),ro=c(),Ta=l("span"),la=a("Processing the data"),ln=c(),y(ra.$$.fragment),Os=c(),is=l("p"),io=a("You should know the drill by now: the texts all need to be converted into sets of token IDs so the model can make sense of them. For this task, we\u2019ll need to tokenize both the inputs and the targets. Our first task is to create our "),gs=l("code"),po=a("tokenizer"),ho=a(" object. As noted earlier, we\u2019ll be using a Marian English to French pretrained model. If you are trying this code with another pair of languages, make sure to adapt the model checkpoint. The "),qt=l("a"),co=a("Helsinki-NLP"),uo=a(" organization provides more than a thousand models in multiple languages."),rn=c(),y(Gt.$$.fragment),pn=c(),ps=l("p"),fo=a("You can also replace the "),Qo=l("code"),_i=a("model_checkpoint"),gi=a(" with any other model you prefer from the "),hn=l("a"),wi=a("Hub"),bi=a(", or a local folder where you\u2019ve saved a pretrained model and a tokenizer."),Ol=c(),y(za.$$.fragment),Fl=c(),Da=l("p"),$i=a("The preparation of our data is pretty straightforward. There\u2019s just one thing to remember: you process the inputs as usual, but for the targets, you need to wrap the tokenizer inside the context manager "),el=l("code"),ki=a("as_target_tokenizer()"),vi=a("."),Il=c(),Pa=l("p"),yi=a("A context manager in Python is introduced with the "),tl=l("code"),ji=a("with"),Ei=a(" statement and is useful when you have two related operations to execute as a pair. The most common example of this is when you write or read a file, which is often done inside an instruction like:"),Ll=c(),y(cn.$$.fragment),Nl=c(),ia=l("p"),xi=a("Here the two related operations that are executed as a pair are the actions of opening and closing the file. The object corresponding to the opened file "),sl=l("code"),qi=a("f"),Ti=a(" only exists inside the indented block under the "),al=l("code"),zi=a("with"),Di=a("; the opening happens before that block and the closing at the end of the block."),Hl=c(),Aa=l("p"),Pi=a("In the case at hand, the context manager "),nl=l("code"),Ai=a("as_target_tokenizer()"),Si=a(" will set the tokenizer in the output language (here, French) before the indented block is executed, then set it back in the input language (here, English)."),Wl=c(),mo=l("p"),Ci=a("So, preprocessing one sample looks like this:"),Ml=c(),y(dn.$$.fragment),Ul=c(),_o=l("p"),Oi=a("If we forget to tokenize the targets inside the context manager, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all:"),Bl=c(),y(un.$$.fragment),Rl=c(),y(fn.$$.fragment),Kl=c(),go=l("p"),Fi=a("As we can see, using the English tokenizer to preprocess a French sentence results in a lot more tokens, since the tokenizer doesn\u2019t know any French words (except those that also appear in the English language, like \u201Cdiscussion\u201D)."),Yl=c(),ws=l("p"),Ii=a("Both "),ol=l("code"),Li=a("inputs"),Ni=a(" and "),ll=l("code"),Hi=a("targets"),Wi=a(" are dictionaries with our usual keys (input IDs, attention mask, etc.), so the last step is to set a "),rl=l("code"),Mi=a('"labels"'),Ui=a(" key inside the inputs. We do this in the preprocessing function we will apply on the datasets:"),Gl=c(),y(mn.$$.fragment),Xl=c(),wo=l("p"),Bi=a("Note that we set similar maximum lengths for our inputs and outputs. Since the texts we\u2019re dealing with seem pretty short, we use 128."),Zl=c(),y(Sa.$$.fragment),Jl=c(),y(Ca.$$.fragment),Vl=c(),bo=l("p"),Ri=a("We can now apply that preprocessing in one go on all the splits of our dataset:"),Ql=c(),y(_n.$$.fragment),er=c(),$o=l("p"),Ki=a("Now that the data has been preprocessed, we are ready to fine-tune our pretrained model!"),tr=c(),Is.c(),ko=c(),vo=l("p"),Yi=a("Note that this time we are using a model that was trained on a translation task and can actually be used already, so there is no warning about missing weights or newly initialized ones."),sr=c(),$a=l("h3"),Oa=l("a"),il=l("span"),y(gn.$$.fragment),Gi=c(),pl=l("span"),Xi=a("Data collation"),ar=c(),bs=l("p"),Zi=a("We\u2019ll need a data collator to deal with the padding for dynamic batching. We can\u2019t just use a "),hl=l("code"),Ji=a("DataCollatorWithPadding"),Vi=a(" like in "),yo=l("a"),Qi=a("Chapter 3"),ep=a(" in this case, because that only pads the inputs (input IDs, attention mask, and token type IDs). Our labels should also be padded to the maximum length encountered in the labels. And, as mentioned previously, the padding value used to pad the labels should be "),cl=l("code"),tp=a("-100"),sp=a(" and not the padding token of the tokenizer, to make sure those padded values are ignored in the loss computation."),nr=c(),it=l("p"),ap=a("This is all done by a "),wn=l("a"),dl=l("code"),np=a("DataCollatorForSeq2Seq"),op=a(". Like the "),ul=l("code"),lp=a("DataCollatorWithPadding"),rp=a(", it takes the "),fl=l("code"),ip=a("tokenizer"),pp=a(" used to preprocess the inputs, but it also takes the "),ml=l("code"),hp=a("model"),cp=a(". This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labels with a special token at the beginning. Since this shift is done slightly differently for different architectures, the "),_l=l("code"),dp=a("DataCollatorForSeq2Seq"),up=a(" needs to know the "),gl=l("code"),fp=a("model"),mp=a(" object:"),or=c(),Ns.c(),jo=c(),Eo=l("p"),_p=a("To test this on a few samples, we just call it on a list of examples from our tokenized training set:"),lr=c(),y(bn.$$.fragment),rr=c(),y($n.$$.fragment),ir=c(),Fa=l("p"),gp=a("We can check our labels have been padded to the maximum length of the batch, using "),wl=l("code"),wp=a("-100"),bp=a(":"),pr=c(),y(kn.$$.fragment),hr=c(),y(vn.$$.fragment),cr=c(),xo=l("p"),$p=a("And we can also have a look at the decoder input IDs, to see that they are shifted versions of the labels:"),dr=c(),y(yn.$$.fragment),ur=c(),y(jn.$$.fragment),fr=c(),qo=l("p"),kp=a("Here are the labels for the first and second elements in our dataset:"),mr=c(),y(En.$$.fragment),_r=c(),y(xn.$$.fragment),gr=c(),Ws.c(),To=c(),ka=l("h3"),Ia=l("a"),bl=l("span"),y(qn.$$.fragment),vp=c(),$l=l("span"),yp=a("Metrics"),wr=c(),y(Tn.$$.fragment),br=c(),Ft&&Ft.c(),zo=c(),Xt=l("p"),jp=a("The traditional metric used for translation is the "),zn=l("a"),Ep=a("BLEU score"),xp=a(", introduced in "),Dn=l("a"),qp=a("a 2002 article"),Tp=a(" by Kishore Papineni et al. The BLEU score evaluates how close the translations are to their labels. It does not measure the intelligibility or grammatical correctness of the model\u2019s generated outputs, but uses statistical rules to ensure that all the words in the generated outputs also appear in the targets. In addition, there are rules that penalize repetitions of the same words if they are not also repeated in the targets (to avoid the model outputting sentences like "),kl=l("code"),zp=a('"the the the the the"'),Dp=a(") and output sentences that are shorter than those in the targets (to avoid the model outputting sentences like "),vl=l("code"),Pp=a('"the"'),Ap=a(")."),$r=c(),La=l("p"),Sp=a("One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is "),Pn=l("a"),Cp=a("SacreBLEU"),Op=a(", which addresses this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the SacreBLEU library:"),kr=c(),y(An.$$.fragment),vr=c(),pa=l("p"),Fp=a("We can then load it via "),yl=l("code"),Ip=a("load_metric()"),Lp=a(" like we did in "),Do=l("a"),Np=a("Chapter 3"),Hp=a(":"),yr=c(),y(Sn.$$.fragment),jr=c(),Po=l("p"),Wp=a("This metric will take texts as inputs and targets. It is designed to accept several acceptable targets, as there are often multiple acceptable translations of the same sentence \u2014 the dataset we\u2019re using only provides one, but it\u2019s not uncommon in NLP to find datasets that give several sentences as labels. So, the predictions should be a list of sentences, but the references should be a list of lists of sentences."),Er=c(),Ao=l("p"),Mp=a("Let\u2019s try an example:"),xr=c(),y(Cn.$$.fragment),qr=c(),y(On.$$.fragment),Tr=c(),Zt=l("p"),Up=a("This gets a BLEU score of 46.75, which is rather good \u2014 for reference, the original Transformer model in the "),Fn=l("a"),Bp=a("\u201CAttention Is All You Need\u201D paper"),Rp=a(" achieved a BLEU score of 41.8 on a similar translation task between English and French! (For more information about the individual metrics, like "),jl=l("code"),Kp=a("counts"),Yp=a(" and "),El=l("code"),Gp=a("bp"),Xp=a(", see the "),In=l("a"),Zp=a("SacreBLEU repository"),Jp=a(".) On the other hand, if we try with the two bad types of predictions (lots of repetitions or too short) that often come out of translation models, we will get rather bad BLEU scores:"),zr=c(),y(Ln.$$.fragment),Dr=c(),y(Nn.$$.fragment),Pr=c(),y(Hn.$$.fragment),Ar=c(),y(Wn.$$.fragment),Sr=c(),So=l("p"),Vp=a("The score can go from 0 to 100, and higher is better."),Cr=c(),Us.c(),Co=c(),Oo=l("p"),Qp=a("Now that this is done, we are ready to fine-tune our model!"),Or=c(),va=l("h3"),Na=l("a"),xl=l("span"),y(Mn.$$.fragment),eh=c(),ql=l("span"),th=a("Fine-tuning the model"),Fr=c(),Fo=l("p"),sh=a("The first step is to log in to Hugging Face, so you\u2019re able to upload your results to the Model Hub. There\u2019s a convenience function to help you with this in a notebook:"),Ir=c(),y(Un.$$.fragment),Lr=c(),Io=l("p"),ah=a("This will display a widget where you can enter your Hugging Face login credentials."),Nr=c(),Lo=l("p"),nh=a("If you aren\u2019t working in a notebook, just type the following line in your terminal:"),Hr=c(),y(Bn.$$.fragment),Wr=c(),Rs.c(),No=c(),Qe&&Qe.c(),Ho=c(),ya=l("h2"),Ha=l("a"),Tl=l("span"),y(Rn.$$.fragment),oh=c(),zl=l("span"),lh=a("Using the fine-tuned model"),Mr=c(),Wa=l("p"),rh=a("We\u2019ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a "),Dl=l("code"),ih=a("pipeline"),ph=a(", we just have to specify the proper model identifier:"),Ur=c(),y(Kn.$$.fragment),Br=c(),y(Yn.$$.fragment),Rr=c(),Wo=l("p"),hh=a("As expected, our pretrained model adapted its knowledge to the corpus we fine-tuned it on, and instead of leaving the English word \u201Cthreads\u201D alone, it now translates it to the French official version. It\u2019s the same for \u201Cplugin\u201D:"),Kr=c(),y(Gn.$$.fragment),Yr=c(),y(Xn.$$.fragment),Gr=c(),Mo=l("p"),ch=a("Another great example of domain adaptation!"),Xr=c(),y(Ma.$$.fragment),this.h()},l(e){const p=kd('[data-svelte="svelte-1phssyn"]',document.head);h=r(p,"META",{name:!0,content:!0}),p.forEach(t),$=d(e),j(m.$$.fragment,e),q=d(e),A=r(e,"H1",{class:!0});var so=i(A);g=r(so,"A",{id:!0,class:!0,href:!0});var Uo=i(g);z=r(Uo,"SPAN",{});var Pl=i(z);j(P.$$.fragment,Pl),Pl.forEach(t),Uo.forEach(t),D=d(so),T=r(so,"SPAN",{});var Al=i(T);N=n(Al,"Translation"),Al.forEach(t),so.forEach(t),v=d(e),H.l(e),U=d(e),F=r(e,"P",{});var ja=i(F);X=n(ja,"Let\u2019s now dive into translation. This is another "),M=r(ja,"A",{href:!0});var Sl=i(M);I=n(Sl,"sequence-to-sequence task"),Sl.forEach(t),Y=n(ja,", which means it\u2019s a problem that can be formulated as going from one sequence to another. In that sense the problem is pretty close to "),Z=r(ja,"A",{href:!0});var Bo=i(Z);re=n(Bo,"summarization"),Bo.forEach(t),he=n(ja,", and you could adapt what we will see here to other sequence-to-sequence problems such as:"),ja.forEach(t),Q=d(e),ee=r(e,"UL",{});var Ua=i(ee);te=r(Ua,"LI",{});var Ea=i(te);G=r(Ea,"STRONG",{});var Ro=i(G);ne=n(Ro,"Style transfer"),Ro.forEach(t),we=n(Ea,": Creating a model that "),V=r(Ea,"EM",{});var Ko=i(V);se=n(Ko,"translates"),Ko.forEach(t),de=n(Ea," texts written in a certain style to another (e.g., formal to casual or Shakespearean English to modern English)"),Ea.forEach(t),be=d(Ua),oe=r(Ua,"LI",{});var Yo=i(oe);ie=r(Yo,"STRONG",{});var jh=i(ie);W=n(jh,"Generative question answering"),jh.forEach(t),ae=n(Yo,": Creating a model that generates answers to questions, given a context"),Yo.forEach(t),Ua.forEach(t),Te=d(e),j(ue.$$.fragment,e),K=d(e),ve=r(e,"P",{});var Jr=i(ve);le=n(Jr,"If you have a big enough corpus of texts in two (or more) languages, you can train a new translation model from scratch like we will in the section on "),Ee=r(Jr,"A",{href:!0});var Eh=i(Ee);Ye=n(Eh,"causal language modeling"),Eh.forEach(t),fe=n(Jr,". It will be faster, however, to fine-tune an existing translation model, be it a multilingual one like mT5 or mBART that you want to fine-tune to a specific language pair, or even a model specialized for translation from one language to another that you want to fine-tune to your specific corpus."),Jr.forEach(t),xe=d(e),J=r(e,"P",{});var Ba=i(J);Ae=n(Ba,"In this section, we will fine-tune a Marian model pretrained to translate from English to French (since a lot of Hugging Face employees speak both those languages) on the "),me=r(Ba,"A",{href:!0,rel:!0});var xh=i(me);pt=n(xh,"KDE4 dataset"),xh.forEach(t),kt=n(Ba,", which is a dataset of localized files for the "),$e=r(Ba,"A",{href:!0,rel:!0});var qh=i($e);et=n(qh,"KDE apps"),qh.forEach(t),Ge=n(Ba,". The model we will use has been pretrained on a large corpus of French and English texts taken from the "),Se=r(Ba,"A",{href:!0,rel:!0});var Th=i(Se);ye=n(Th,"Opus dataset"),Th.forEach(t),vt=n(Ba,", which actually contains the KDE4 dataset. But even if the pretrained model we use has seen that data during its pretraining, we will see that we can get a better version of it after fine-tuning."),Ba.forEach(t),_e=d(e),k=r(e,"P",{});var zh=i(k);R=n(zh,"Once we\u2019re finished, we will have a model able to make predictions like this one:"),zh.forEach(t),tt=d(e),pe=r(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),i(pe).forEach(t),ht=d(e),je=r(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),i(je).forEach(t),yt=d(e),ge=r(e,"A",{class:!0,href:!0});var Vr=i(ge);Oe=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Tt=d(Vr),ce=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Vr.forEach(t),st=d(e),Ie=r(e,"P",{});var Qr=i(Ie);cs=n(Qr,"As in the previous sections, you can find the actual model that we\u2019ll train and upload to the Hub using the code below and double-check its predictions "),Le=r(Qr,"A",{href:!0,rel:!0});var Dh=i(Le);$s=n(Dh,"here"),Dh.forEach(t),ds=n(Qr,"."),Qr.forEach(t),dt=d(e),ut=r(e,"H2",{class:!0});var ei=i(ut);ft=r(ei,"A",{id:!0,class:!0,href:!0});var Ph=i(ft);at=r(Ph,"SPAN",{});var Ah=i(at);j(Ne.$$.fragment,Ah),Ah.forEach(t),Ph.forEach(t),zt=d(ei),mt=r(ei,"SPAN",{});var Sh=i(mt);Vt=n(Sh,"Preparing the data"),Sh.forEach(t),ei.forEach(t),nt=d(e),He=r(e,"P",{});var Ra=i(He);Qt=n(Ra,"To fine-tune or train a translation model from scratch, we will need a dataset suitable for the task. As mentioned previously, we\u2019ll use the "),qe=r(Ra,"A",{href:!0,rel:!0});var Ch=i(qe);es=n(Ch,"KDE4 dataset"),Ch.forEach(t),ze=n(Ra," in this section, but you can adapt the code to use your own data quite easily, as long as you have pairs of sentences in the two languages you want to translate from and into. Refer back to "),Dt=r(Ra,"A",{href:!0});var Oh=i(Dt);jt=n(Oh,"Chapter 5"),Oh.forEach(t),ks=n(Ra," if you need a reminder of how to load your custom data in a "),It=r(Ra,"CODE",{});var Fh=i(It);We=n(Fh,"Dataset"),Fh.forEach(t),vs=n(Ra,"."),Ra.forEach(t),Lt=d(e),ot=r(e,"H3",{class:!0});var ti=i(ot);De=r(ti,"A",{id:!0,class:!0,href:!0});var Ih=i(De);Me=r(Ih,"SPAN",{});var Lh=i(Me);j(_t.$$.fragment,Lh),Lh.forEach(t),Ih.forEach(t),Pt=d(ti),Nt=r(ti,"SPAN",{});var Nh=i(Nt);gt=n(Nh,"The KDE4 dataset"),Nh.forEach(t),ti.forEach(t),ts=d(e),Ue=r(e,"P",{});var si=i(Ue);lt=n(si,"As usual, we download our dataset using the "),Ht=r(si,"CODE",{});var Hh=i(Ht);wt=n(Hh,"load_dataset()"),Hh.forEach(t),us=n(si," function:"),si.forEach(t),At=d(e),j(bt.$$.fragment,e),ss=d(e),Ze=r(e,"P",{});var ai=i(Ze);fs=n(ai,"If you want to work with a different pair of languages, you can specify them by their codes. A total of 92 languages are available for this dataset; you can see them all by expanding the language tags on its "),Be=r(ai,"A",{href:!0,rel:!0});var Wh=i(Be);f=n(Wh,"dataset card"),Wh.forEach(t),O=n(ai,"."),ai.forEach(t),ys=d(e),rt=r(e,"IMG",{src:!0,alt:!0,width:!0}),js=d(e),Pe=r(e,"P",{});var Mh=i(Pe);Ks=n(Mh,"Let\u2019s have a look at the dataset:"),Mh.forEach(t),as=d(e),j(St.$$.fragment,e),Re=d(e),j(Wt.$$.fragment,e),Ct=d(e),Je=r(e,"P",{});var Ka=i(Je);Ke=n(Ka,"We have 210,173 pairs of sentences, but in one single split, so we will need to create our own validation set. As we saw in "),Ot=r(Ka,"A",{href:!0});var Uh=i(Ot);Es=n(Uh,"Chapter 5"),Uh.forEach(t),Ys=n(Ka,", a "),ns=r(Ka,"CODE",{});var Bh=i(ns);Ve=n(Bh,"Dataset"),Bh.forEach(t),Gs=n(Ka," has a "),$t=r(Ka,"CODE",{});var Rh=i($t);Xs=n(Rh,"train_test_split()"),Rh.forEach(t),Zs=n(Ka," method that can help us. We\u2019ll provide a seed for reproducibility:"),Ka.forEach(t),os=d(e),j(Mt.$$.fragment,e),xs=d(e),j(Et.$$.fragment,e),qs=d(e),Ce=r(e,"P",{});var Go=i(Ce);Ts=n(Go,"We can rename the "),xt=r(Go,"CODE",{});var Kh=i(xt);zs=n(Kh,'"test"'),Kh.forEach(t),Ut=n(Go," key to "),ls=r(Go,"CODE",{});var Yh=i(ls);Bt=n(Yh,'"validation"'),Yh.forEach(t),Za=n(Go," like this:"),Go.forEach(t),Js=d(e),j(Ds.$$.fragment,e),xa=d(e),Rt=r(e,"P",{});var Gh=i(Rt);ua=n(Gh,"Now let\u2019s take a look at one element of the dataset:"),Gh.forEach(t),qa=d(e),j(Ps.$$.fragment,e),fa=d(e),j(u.$$.fragment,e),L=d(e),ma=r(e,"P",{});var Xh=i(ma);oo=n(Xh,"We get a dictionary with two sentences in the pair of languages we requested. One particularity of this dataset full of technical computer science terms is that they are all fully translated in French. However, French engineers are often lazy and leave most computer science-specific words in English when they talk. Here, for instance, the word \u201Cthreads\u201D might well appear in a French sentence, especially in a technical conversation; but in this dataset it has been translated into the more correct \u201Cfils de discussion.\u201D The pretrained model we use, which has been pretrained on a larger corpus of French and English sentences, takes the easier option of leaving the word as is:"),Xh.forEach(t),Ja=d(e),j(Kt.$$.fragment,e),Va=d(e),j(Vs.$$.fragment,e),_a=d(e),ga=r(e,"P",{});var Zh=i(ga);lo=n(Zh,`Another example of this behavior can be seen with the word \u201Cplugin,\u201D which isn\u2019t officially a French word but which most native speakers will understand and not bother to translate.
In the KDE4 dataset this word has been translated in French into the more official \u201Cmodule d\u2019extension\u201D:`),Zh.forEach(t),Qa=d(e),j(Yt.$$.fragment,e),en=d(e),j(Qs.$$.fragment,e),tn=d(e),wa=r(e,"P",{});var Jh=i(wa);ea=n(Jh,"Our pretrained model, however, sticks with the compact and familiar English word:"),Jh.forEach(t),sn=d(e),j(ta.$$.fragment,e),rs=d(e),j(sa.$$.fragment,e),an=d(e),ba=r(e,"P",{});var Vh=i(ba);ms=n(Vh,"It will be interesting to see if our fine-tuned model picks up on those particularities of the dataset (spoiler alert: it will)."),Vh.forEach(t),nn=d(e),j(aa.$$.fragment,e),on=d(e),j(As.$$.fragment,e),Ss=d(e),_s=r(e,"H3",{class:!0});var ni=i(_s);Cs=r(ni,"A",{id:!0,class:!0,href:!0});var Qh=i(Cs);na=r(Qh,"SPAN",{});var ec=i(na);j(oa.$$.fragment,ec),ec.forEach(t),Qh.forEach(t),ro=d(ni),Ta=r(ni,"SPAN",{});var tc=i(Ta);la=n(tc,"Processing the data"),tc.forEach(t),ni.forEach(t),ln=d(e),j(ra.$$.fragment,e),Os=d(e),is=r(e,"P",{});var Xo=i(is);io=n(Xo,"You should know the drill by now: the texts all need to be converted into sets of token IDs so the model can make sense of them. For this task, we\u2019ll need to tokenize both the inputs and the targets. Our first task is to create our "),gs=r(Xo,"CODE",{});var sc=i(gs);po=n(sc,"tokenizer"),sc.forEach(t),ho=n(Xo," object. As noted earlier, we\u2019ll be using a Marian English to French pretrained model. If you are trying this code with another pair of languages, make sure to adapt the model checkpoint. The "),qt=r(Xo,"A",{href:!0,rel:!0});var ac=i(qt);co=n(ac,"Helsinki-NLP"),ac.forEach(t),uo=n(Xo," organization provides more than a thousand models in multiple languages."),Xo.forEach(t),rn=d(e),j(Gt.$$.fragment,e),pn=d(e),ps=r(e,"P",{});var Zo=i(ps);fo=n(Zo,"You can also replace the "),Qo=r(Zo,"CODE",{});var nc=i(Qo);_i=n(nc,"model_checkpoint"),nc.forEach(t),gi=n(Zo," with any other model you prefer from the "),hn=r(Zo,"A",{href:!0,rel:!0});var oc=i(hn);wi=n(oc,"Hub"),oc.forEach(t),bi=n(Zo,", or a local folder where you\u2019ve saved a pretrained model and a tokenizer."),Zo.forEach(t),Ol=d(e),j(za.$$.fragment,e),Fl=d(e),Da=r(e,"P",{});var oi=i(Da);$i=n(oi,"The preparation of our data is pretty straightforward. There\u2019s just one thing to remember: you process the inputs as usual, but for the targets, you need to wrap the tokenizer inside the context manager "),el=r(oi,"CODE",{});var lc=i(el);ki=n(lc,"as_target_tokenizer()"),lc.forEach(t),vi=n(oi,"."),oi.forEach(t),Il=d(e),Pa=r(e,"P",{});var li=i(Pa);yi=n(li,"A context manager in Python is introduced with the "),tl=r(li,"CODE",{});var rc=i(tl);ji=n(rc,"with"),rc.forEach(t),Ei=n(li," statement and is useful when you have two related operations to execute as a pair. The most common example of this is when you write or read a file, which is often done inside an instruction like:"),li.forEach(t),Ll=d(e),j(cn.$$.fragment,e),Nl=d(e),ia=r(e,"P",{});var Jo=i(ia);xi=n(Jo,"Here the two related operations that are executed as a pair are the actions of opening and closing the file. The object corresponding to the opened file "),sl=r(Jo,"CODE",{});var ic=i(sl);qi=n(ic,"f"),ic.forEach(t),Ti=n(Jo," only exists inside the indented block under the "),al=r(Jo,"CODE",{});var pc=i(al);zi=n(pc,"with"),pc.forEach(t),Di=n(Jo,"; the opening happens before that block and the closing at the end of the block."),Jo.forEach(t),Hl=d(e),Aa=r(e,"P",{});var ri=i(Aa);Pi=n(ri,"In the case at hand, the context manager "),nl=r(ri,"CODE",{});var hc=i(nl);Ai=n(hc,"as_target_tokenizer()"),hc.forEach(t),Si=n(ri," will set the tokenizer in the output language (here, French) before the indented block is executed, then set it back in the input language (here, English)."),ri.forEach(t),Wl=d(e),mo=r(e,"P",{});var cc=i(mo);Ci=n(cc,"So, preprocessing one sample looks like this:"),cc.forEach(t),Ml=d(e),j(dn.$$.fragment,e),Ul=d(e),_o=r(e,"P",{});var dc=i(_o);Oi=n(dc,"If we forget to tokenize the targets inside the context manager, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all:"),dc.forEach(t),Bl=d(e),j(un.$$.fragment,e),Rl=d(e),j(fn.$$.fragment,e),Kl=d(e),go=r(e,"P",{});var uc=i(go);Fi=n(uc,"As we can see, using the English tokenizer to preprocess a French sentence results in a lot more tokens, since the tokenizer doesn\u2019t know any French words (except those that also appear in the English language, like \u201Cdiscussion\u201D)."),uc.forEach(t),Yl=d(e),ws=r(e,"P",{});var Ya=i(ws);Ii=n(Ya,"Both "),ol=r(Ya,"CODE",{});var fc=i(ol);Li=n(fc,"inputs"),fc.forEach(t),Ni=n(Ya," and "),ll=r(Ya,"CODE",{});var mc=i(ll);Hi=n(mc,"targets"),mc.forEach(t),Wi=n(Ya," are dictionaries with our usual keys (input IDs, attention mask, etc.), so the last step is to set a "),rl=r(Ya,"CODE",{});var _c=i(rl);Mi=n(_c,'"labels"'),_c.forEach(t),Ui=n(Ya," key inside the inputs. We do this in the preprocessing function we will apply on the datasets:"),Ya.forEach(t),Gl=d(e),j(mn.$$.fragment,e),Xl=d(e),wo=r(e,"P",{});var gc=i(wo);Bi=n(gc,"Note that we set similar maximum lengths for our inputs and outputs. Since the texts we\u2019re dealing with seem pretty short, we use 128."),gc.forEach(t),Zl=d(e),j(Sa.$$.fragment,e),Jl=d(e),j(Ca.$$.fragment,e),Vl=d(e),bo=r(e,"P",{});var wc=i(bo);Ri=n(wc,"We can now apply that preprocessing in one go on all the splits of our dataset:"),wc.forEach(t),Ql=d(e),j(_n.$$.fragment,e),er=d(e),$o=r(e,"P",{});var bc=i($o);Ki=n(bc,"Now that the data has been preprocessed, we are ready to fine-tune our pretrained model!"),bc.forEach(t),tr=d(e),Is.l(e),ko=d(e),vo=r(e,"P",{});var $c=i(vo);Yi=n($c,"Note that this time we are using a model that was trained on a translation task and can actually be used already, so there is no warning about missing weights or newly initialized ones."),$c.forEach(t),sr=d(e),$a=r(e,"H3",{class:!0});var ii=i($a);Oa=r(ii,"A",{id:!0,class:!0,href:!0});var kc=i(Oa);il=r(kc,"SPAN",{});var vc=i(il);j(gn.$$.fragment,vc),vc.forEach(t),kc.forEach(t),Gi=d(ii),pl=r(ii,"SPAN",{});var yc=i(pl);Xi=n(yc,"Data collation"),yc.forEach(t),ii.forEach(t),ar=d(e),bs=r(e,"P",{});var Ga=i(bs);Zi=n(Ga,"We\u2019ll need a data collator to deal with the padding for dynamic batching. We can\u2019t just use a "),hl=r(Ga,"CODE",{});var jc=i(hl);Ji=n(jc,"DataCollatorWithPadding"),jc.forEach(t),Vi=n(Ga," like in "),yo=r(Ga,"A",{href:!0});var Ec=i(yo);Qi=n(Ec,"Chapter 3"),Ec.forEach(t),ep=n(Ga," in this case, because that only pads the inputs (input IDs, attention mask, and token type IDs). Our labels should also be padded to the maximum length encountered in the labels. And, as mentioned previously, the padding value used to pad the labels should be "),cl=r(Ga,"CODE",{});var xc=i(cl);tp=n(xc,"-100"),xc.forEach(t),sp=n(Ga," and not the padding token of the tokenizer, to make sure those padded values are ignored in the loss computation."),Ga.forEach(t),nr=d(e),it=r(e,"P",{});var Jt=i(it);ap=n(Jt,"This is all done by a "),wn=r(Jt,"A",{href:!0,rel:!0});var qc=i(wn);dl=r(qc,"CODE",{});var Tc=i(dl);np=n(Tc,"DataCollatorForSeq2Seq"),Tc.forEach(t),qc.forEach(t),op=n(Jt,". Like the "),ul=r(Jt,"CODE",{});var zc=i(ul);lp=n(zc,"DataCollatorWithPadding"),zc.forEach(t),rp=n(Jt,", it takes the "),fl=r(Jt,"CODE",{});var Dc=i(fl);ip=n(Dc,"tokenizer"),Dc.forEach(t),pp=n(Jt," used to preprocess the inputs, but it also takes the "),ml=r(Jt,"CODE",{});var Pc=i(ml);hp=n(Pc,"model"),Pc.forEach(t),cp=n(Jt,". This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labels with a special token at the beginning. Since this shift is done slightly differently for different architectures, the "),_l=r(Jt,"CODE",{});var Ac=i(_l);dp=n(Ac,"DataCollatorForSeq2Seq"),Ac.forEach(t),up=n(Jt," needs to know the "),gl=r(Jt,"CODE",{});var Sc=i(gl);fp=n(Sc,"model"),Sc.forEach(t),mp=n(Jt," object:"),Jt.forEach(t),or=d(e),Ns.l(e),jo=d(e),Eo=r(e,"P",{});var Cc=i(Eo);_p=n(Cc,"To test this on a few samples, we just call it on a list of examples from our tokenized training set:"),Cc.forEach(t),lr=d(e),j(bn.$$.fragment,e),rr=d(e),j($n.$$.fragment,e),ir=d(e),Fa=r(e,"P",{});var pi=i(Fa);gp=n(pi,"We can check our labels have been padded to the maximum length of the batch, using "),wl=r(pi,"CODE",{});var Oc=i(wl);wp=n(Oc,"-100"),Oc.forEach(t),bp=n(pi,":"),pi.forEach(t),pr=d(e),j(kn.$$.fragment,e),hr=d(e),j(vn.$$.fragment,e),cr=d(e),xo=r(e,"P",{});var Fc=i(xo);$p=n(Fc,"And we can also have a look at the decoder input IDs, to see that they are shifted versions of the labels:"),Fc.forEach(t),dr=d(e),j(yn.$$.fragment,e),ur=d(e),j(jn.$$.fragment,e),fr=d(e),qo=r(e,"P",{});var Ic=i(qo);kp=n(Ic,"Here are the labels for the first and second elements in our dataset:"),Ic.forEach(t),mr=d(e),j(En.$$.fragment,e),_r=d(e),j(xn.$$.fragment,e),gr=d(e),Ws.l(e),To=d(e),ka=r(e,"H3",{class:!0});var hi=i(ka);Ia=r(hi,"A",{id:!0,class:!0,href:!0});var Lc=i(Ia);bl=r(Lc,"SPAN",{});var Nc=i(bl);j(qn.$$.fragment,Nc),Nc.forEach(t),Lc.forEach(t),vp=d(hi),$l=r(hi,"SPAN",{});var Hc=i($l);yp=n(Hc,"Metrics"),Hc.forEach(t),hi.forEach(t),wr=d(e),j(Tn.$$.fragment,e),br=d(e),Ft&&Ft.l(e),zo=d(e),Xt=r(e,"P",{});var ha=i(Xt);jp=n(ha,"The traditional metric used for translation is the "),zn=r(ha,"A",{href:!0,rel:!0});var Wc=i(zn);Ep=n(Wc,"BLEU score"),Wc.forEach(t),xp=n(ha,", introduced in "),Dn=r(ha,"A",{href:!0,rel:!0});var Mc=i(Dn);qp=n(Mc,"a 2002 article"),Mc.forEach(t),Tp=n(ha," by Kishore Papineni et al. The BLEU score evaluates how close the translations are to their labels. It does not measure the intelligibility or grammatical correctness of the model\u2019s generated outputs, but uses statistical rules to ensure that all the words in the generated outputs also appear in the targets. In addition, there are rules that penalize repetitions of the same words if they are not also repeated in the targets (to avoid the model outputting sentences like "),kl=r(ha,"CODE",{});var Uc=i(kl);zp=n(Uc,'"the the the the the"'),Uc.forEach(t),Dp=n(ha,") and output sentences that are shorter than those in the targets (to avoid the model outputting sentences like "),vl=r(ha,"CODE",{});var Bc=i(vl);Pp=n(Bc,'"the"'),Bc.forEach(t),Ap=n(ha,")."),ha.forEach(t),$r=d(e),La=r(e,"P",{});var ci=i(La);Sp=n(ci,"One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is "),Pn=r(ci,"A",{href:!0,rel:!0});var Rc=i(Pn);Cp=n(Rc,"SacreBLEU"),Rc.forEach(t),Op=n(ci,", which addresses this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the SacreBLEU library:"),ci.forEach(t),kr=d(e),j(An.$$.fragment,e),vr=d(e),pa=r(e,"P",{});var Vo=i(pa);Fp=n(Vo,"We can then load it via "),yl=r(Vo,"CODE",{});var Kc=i(yl);Ip=n(Kc,"load_metric()"),Kc.forEach(t),Lp=n(Vo," like we did in "),Do=r(Vo,"A",{href:!0});var Yc=i(Do);Np=n(Yc,"Chapter 3"),Yc.forEach(t),Hp=n(Vo,":"),Vo.forEach(t),yr=d(e),j(Sn.$$.fragment,e),jr=d(e),Po=r(e,"P",{});var Gc=i(Po);Wp=n(Gc,"This metric will take texts as inputs and targets. It is designed to accept several acceptable targets, as there are often multiple acceptable translations of the same sentence \u2014 the dataset we\u2019re using only provides one, but it\u2019s not uncommon in NLP to find datasets that give several sentences as labels. So, the predictions should be a list of sentences, but the references should be a list of lists of sentences."),Gc.forEach(t),Er=d(e),Ao=r(e,"P",{});var Xc=i(Ao);Mp=n(Xc,"Let\u2019s try an example:"),Xc.forEach(t),xr=d(e),j(Cn.$$.fragment,e),qr=d(e),j(On.$$.fragment,e),Tr=d(e),Zt=r(e,"P",{});var ca=i(Zt);Up=n(ca,"This gets a BLEU score of 46.75, which is rather good \u2014 for reference, the original Transformer model in the "),Fn=r(ca,"A",{href:!0,rel:!0});var Zc=i(Fn);Bp=n(Zc,"\u201CAttention Is All You Need\u201D paper"),Zc.forEach(t),Rp=n(ca," achieved a BLEU score of 41.8 on a similar translation task between English and French! (For more information about the individual metrics, like "),jl=r(ca,"CODE",{});var Jc=i(jl);Kp=n(Jc,"counts"),Jc.forEach(t),Yp=n(ca," and "),El=r(ca,"CODE",{});var Vc=i(El);Gp=n(Vc,"bp"),Vc.forEach(t),Xp=n(ca,", see the "),In=r(ca,"A",{href:!0,rel:!0});var Qc=i(In);Zp=n(Qc,"SacreBLEU repository"),Qc.forEach(t),Jp=n(ca,".) On the other hand, if we try with the two bad types of predictions (lots of repetitions or too short) that often come out of translation models, we will get rather bad BLEU scores:"),ca.forEach(t),zr=d(e),j(Ln.$$.fragment,e),Dr=d(e),j(Nn.$$.fragment,e),Pr=d(e),j(Hn.$$.fragment,e),Ar=d(e),j(Wn.$$.fragment,e),Sr=d(e),So=r(e,"P",{});var ed=i(So);Vp=n(ed,"The score can go from 0 to 100, and higher is better."),ed.forEach(t),Cr=d(e),Us.l(e),Co=d(e),Oo=r(e,"P",{});var td=i(Oo);Qp=n(td,"Now that this is done, we are ready to fine-tune our model!"),td.forEach(t),Or=d(e),va=r(e,"H3",{class:!0});var di=i(va);Na=r(di,"A",{id:!0,class:!0,href:!0});var sd=i(Na);xl=r(sd,"SPAN",{});var ad=i(xl);j(Mn.$$.fragment,ad),ad.forEach(t),sd.forEach(t),eh=d(di),ql=r(di,"SPAN",{});var nd=i(ql);th=n(nd,"Fine-tuning the model"),nd.forEach(t),di.forEach(t),Fr=d(e),Fo=r(e,"P",{});var od=i(Fo);sh=n(od,"The first step is to log in to Hugging Face, so you\u2019re able to upload your results to the Model Hub. There\u2019s a convenience function to help you with this in a notebook:"),od.forEach(t),Ir=d(e),j(Un.$$.fragment,e),Lr=d(e),Io=r(e,"P",{});var ld=i(Io);ah=n(ld,"This will display a widget where you can enter your Hugging Face login credentials."),ld.forEach(t),Nr=d(e),Lo=r(e,"P",{});var rd=i(Lo);nh=n(rd,"If you aren\u2019t working in a notebook, just type the following line in your terminal:"),rd.forEach(t),Hr=d(e),j(Bn.$$.fragment,e),Wr=d(e),Rs.l(e),No=d(e),Qe&&Qe.l(e),Ho=d(e),ya=r(e,"H2",{class:!0});var ui=i(ya);Ha=r(ui,"A",{id:!0,class:!0,href:!0});var id=i(Ha);Tl=r(id,"SPAN",{});var pd=i(Tl);j(Rn.$$.fragment,pd),pd.forEach(t),id.forEach(t),oh=d(ui),zl=r(ui,"SPAN",{});var hd=i(zl);lh=n(hd,"Using the fine-tuned model"),hd.forEach(t),ui.forEach(t),Mr=d(e),Wa=r(e,"P",{});var fi=i(Wa);rh=n(fi,"We\u2019ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a "),Dl=r(fi,"CODE",{});var cd=i(Dl);ih=n(cd,"pipeline"),cd.forEach(t),ph=n(fi,", we just have to specify the proper model identifier:"),fi.forEach(t),Ur=d(e),j(Kn.$$.fragment,e),Br=d(e),j(Yn.$$.fragment,e),Rr=d(e),Wo=r(e,"P",{});var dd=i(Wo);hh=n(dd,"As expected, our pretrained model adapted its knowledge to the corpus we fine-tuned it on, and instead of leaving the English word \u201Cthreads\u201D alone, it now translates it to the French official version. It\u2019s the same for \u201Cplugin\u201D:"),dd.forEach(t),Kr=d(e),j(Gn.$$.fragment,e),Yr=d(e),j(Xn.$$.fragment,e),Gr=d(e),Mo=r(e,"P",{});var ud=i(Mo);ch=n(ud,"Another great example of domain adaptation!"),ud.forEach(t),Xr=d(e),j(Ma.$$.fragment,e),this.h()},h(){_(h,"name","hf:doc:metadata"),_(h,"content",JSON.stringify(Rd)),_(g,"id","translation"),_(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(g,"href","#translation"),_(A,"class","relative group"),_(M,"href","/course/chapter1/7"),_(Z,"href","/course/chapter7/6"),_(Ee,"href","/course/chapter7/6"),_(me,"href","https://huggingface.co/datasets/kde4"),_(me,"rel","nofollow"),_($e,"href","https://apps.kde.org/"),_($e,"rel","nofollow"),_(Se,"href","https://opus.nlpl.eu/"),_(Se,"rel","nofollow"),Cl(pe.src,Xe="https://hf.space/gradioiframe/course-demos/marian-finetuned-kde4-en-to-fr/+")||_(pe,"src",Xe),_(pe,"frameborder","0"),_(pe,"height","350"),_(pe,"title","Gradio app"),_(pe,"class","block dark:hidden container p-0 flex-grow space-iframe"),_(pe,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),_(pe,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Cl(je.src,ke="https://hf.space/gradioiframe/course-demos/marian-finetuned-kde4-en-to-fr-darkmode/+")||_(je,"src",ke),_(je,"frameborder","0"),_(je,"height","350"),_(je,"title","Gradio app"),_(je,"class","hidden dark:block container p-0 flex-grow space-iframe"),_(je,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),_(je,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),_(Oe,"class","block dark:hidden lg:w-3/5"),Cl(Oe.src,Fe="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png")||_(Oe,"src",Fe),_(Oe,"alt","One-hot encoded labels for question answering."),_(ce,"class","hidden dark:block lg:w-3/5"),Cl(ce.src,ct="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png")||_(ce,"src",ct),_(ce,"alt","One-hot encoded labels for question answering."),_(ge,"class","flex justify-center"),_(ge,"href","/huggingface-course/marian-finetuned-kde4-en-to-fr"),_(Le,"href","https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages."),_(Le,"rel","nofollow"),_(ft,"id","preparing-the-data"),_(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ft,"href","#preparing-the-data"),_(ut,"class","relative group"),_(qe,"href","https://huggingface.co/datasets/kde4"),_(qe,"rel","nofollow"),_(Dt,"href","/course/chapter5"),_(De,"id","the-kde4-dataset"),_(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(De,"href","#the-kde4-dataset"),_(ot,"class","relative group"),_(Be,"href","https://huggingface.co/datasets/kde4"),_(Be,"rel","nofollow"),Cl(rt.src,da="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png")||_(rt,"src",da),_(rt,"alt","Language available for the KDE4 dataset."),_(rt,"width","100%"),_(Ot,"href","/course/chapter5"),_(Cs,"id","processing-the-data"),_(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Cs,"href","#processing-the-data"),_(_s,"class","relative group"),_(qt,"href","https://huggingface.co/Helsinki-NLP"),_(qt,"rel","nofollow"),_(hn,"href","https://huggingface.co/models"),_(hn,"rel","nofollow"),_(Oa,"id","data-collation"),_(Oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Oa,"href","#data-collation"),_($a,"class","relative group"),_(yo,"href","/course/chapter3"),_(wn,"href","https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq"),_(wn,"rel","nofollow"),_(Ia,"id","metrics"),_(Ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Ia,"href","#metrics"),_(ka,"class","relative group"),_(zn,"href","https://en.wikipedia.org/wiki/BLEU"),_(zn,"rel","nofollow"),_(Dn,"href","https://aclanthology.org/P02-1040.pdf"),_(Dn,"rel","nofollow"),_(Pn,"href","https://github.com/mjpost/sacrebleu"),_(Pn,"rel","nofollow"),_(Do,"href","/course/chapter3"),_(Fn,"href","https://arxiv.org/pdf/1706.03762.pdf"),_(Fn,"rel","nofollow"),_(In,"href","https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74"),_(In,"rel","nofollow"),_(Na,"id","finetuning-the-model"),_(Na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Na,"href","#finetuning-the-model"),_(va,"class","relative group"),_(Ha,"id","using-the-finetuned-model"),_(Ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Ha,"href","#using-the-finetuned-model"),_(ya,"class","relative group")},m(e,p){s(document.head,h),o(e,$,p),E(m,e,p),o(e,q,p),o(e,A,p),s(A,g),s(g,z),E(P,z,null),s(A,D),s(A,T),s(T,N),o(e,v,p),Zn[S].m(e,p),o(e,U,p),o(e,F,p),s(F,X),s(F,M),s(M,I),s(F,Y),s(F,Z),s(Z,re),s(F,he),o(e,Q,p),o(e,ee,p),s(ee,te),s(te,G),s(G,ne),s(te,we),s(te,V),s(V,se),s(te,de),s(ee,be),s(ee,oe),s(oe,ie),s(ie,W),s(oe,ae),o(e,Te,p),E(ue,e,p),o(e,K,p),o(e,ve,p),s(ve,le),s(ve,Ee),s(Ee,Ye),s(ve,fe),o(e,xe,p),o(e,J,p),s(J,Ae),s(J,me),s(me,pt),s(J,kt),s(J,$e),s($e,et),s(J,Ge),s(J,Se),s(Se,ye),s(J,vt),o(e,_e,p),o(e,k,p),s(k,R),o(e,tt,p),o(e,pe,p),o(e,ht,p),o(e,je,p),o(e,yt,p),o(e,ge,p),s(ge,Oe),s(ge,Tt),s(ge,ce),o(e,st,p),o(e,Ie,p),s(Ie,cs),s(Ie,Le),s(Le,$s),s(Ie,ds),o(e,dt,p),o(e,ut,p),s(ut,ft),s(ft,at),E(Ne,at,null),s(ut,zt),s(ut,mt),s(mt,Vt),o(e,nt,p),o(e,He,p),s(He,Qt),s(He,qe),s(qe,es),s(He,ze),s(He,Dt),s(Dt,jt),s(He,ks),s(He,It),s(It,We),s(He,vs),o(e,Lt,p),o(e,ot,p),s(ot,De),s(De,Me),E(_t,Me,null),s(ot,Pt),s(ot,Nt),s(Nt,gt),o(e,ts,p),o(e,Ue,p),s(Ue,lt),s(Ue,Ht),s(Ht,wt),s(Ue,us),o(e,At,p),E(bt,e,p),o(e,ss,p),o(e,Ze,p),s(Ze,fs),s(Ze,Be),s(Be,f),s(Ze,O),o(e,ys,p),o(e,rt,p),o(e,js,p),o(e,Pe,p),s(Pe,Ks),o(e,as,p),E(St,e,p),o(e,Re,p),E(Wt,e,p),o(e,Ct,p),o(e,Je,p),s(Je,Ke),s(Je,Ot),s(Ot,Es),s(Je,Ys),s(Je,ns),s(ns,Ve),s(Je,Gs),s(Je,$t),s($t,Xs),s(Je,Zs),o(e,os,p),E(Mt,e,p),o(e,xs,p),E(Et,e,p),o(e,qs,p),o(e,Ce,p),s(Ce,Ts),s(Ce,xt),s(xt,zs),s(Ce,Ut),s(Ce,ls),s(ls,Bt),s(Ce,Za),o(e,Js,p),E(Ds,e,p),o(e,xa,p),o(e,Rt,p),s(Rt,ua),o(e,qa,p),E(Ps,e,p),o(e,fa,p),E(u,e,p),o(e,L,p),o(e,ma,p),s(ma,oo),o(e,Ja,p),E(Kt,e,p),o(e,Va,p),E(Vs,e,p),o(e,_a,p),o(e,ga,p),s(ga,lo),o(e,Qa,p),E(Yt,e,p),o(e,en,p),E(Qs,e,p),o(e,tn,p),o(e,wa,p),s(wa,ea),o(e,sn,p),E(ta,e,p),o(e,rs,p),E(sa,e,p),o(e,an,p),o(e,ba,p),s(ba,ms),o(e,nn,p),E(aa,e,p),o(e,on,p),E(As,e,p),o(e,Ss,p),o(e,_s,p),s(_s,Cs),s(Cs,na),E(oa,na,null),s(_s,ro),s(_s,Ta),s(Ta,la),o(e,ln,p),E(ra,e,p),o(e,Os,p),o(e,is,p),s(is,io),s(is,gs),s(gs,po),s(is,ho),s(is,qt),s(qt,co),s(is,uo),o(e,rn,p),E(Gt,e,p),o(e,pn,p),o(e,ps,p),s(ps,fo),s(ps,Qo),s(Qo,_i),s(ps,gi),s(ps,hn),s(hn,wi),s(ps,bi),o(e,Ol,p),E(za,e,p),o(e,Fl,p),o(e,Da,p),s(Da,$i),s(Da,el),s(el,ki),s(Da,vi),o(e,Il,p),o(e,Pa,p),s(Pa,yi),s(Pa,tl),s(tl,ji),s(Pa,Ei),o(e,Ll,p),E(cn,e,p),o(e,Nl,p),o(e,ia,p),s(ia,xi),s(ia,sl),s(sl,qi),s(ia,Ti),s(ia,al),s(al,zi),s(ia,Di),o(e,Hl,p),o(e,Aa,p),s(Aa,Pi),s(Aa,nl),s(nl,Ai),s(Aa,Si),o(e,Wl,p),o(e,mo,p),s(mo,Ci),o(e,Ml,p),E(dn,e,p),o(e,Ul,p),o(e,_o,p),s(_o,Oi),o(e,Bl,p),E(un,e,p),o(e,Rl,p),E(fn,e,p),o(e,Kl,p),o(e,go,p),s(go,Fi),o(e,Yl,p),o(e,ws,p),s(ws,Ii),s(ws,ol),s(ol,Li),s(ws,Ni),s(ws,ll),s(ll,Hi),s(ws,Wi),s(ws,rl),s(rl,Mi),s(ws,Ui),o(e,Gl,p),E(mn,e,p),o(e,Xl,p),o(e,wo,p),s(wo,Bi),o(e,Zl,p),E(Sa,e,p),o(e,Jl,p),E(Ca,e,p),o(e,Vl,p),o(e,bo,p),s(bo,Ri),o(e,Ql,p),E(_n,e,p),o(e,er,p),o(e,$o,p),s($o,Ki),o(e,tr,p),Jn[Fs].m(e,p),o(e,ko,p),o(e,vo,p),s(vo,Yi),o(e,sr,p),o(e,$a,p),s($a,Oa),s(Oa,il),E(gn,il,null),s($a,Gi),s($a,pl),s(pl,Xi),o(e,ar,p),o(e,bs,p),s(bs,Zi),s(bs,hl),s(hl,Ji),s(bs,Vi),s(bs,yo),s(yo,Qi),s(bs,ep),s(bs,cl),s(cl,tp),s(bs,sp),o(e,nr,p),o(e,it,p),s(it,ap),s(it,wn),s(wn,dl),s(dl,np),s(it,op),s(it,ul),s(ul,lp),s(it,rp),s(it,fl),s(fl,ip),s(it,pp),s(it,ml),s(ml,hp),s(it,cp),s(it,_l),s(_l,dp),s(it,up),s(it,gl),s(gl,fp),s(it,mp),o(e,or,p),Vn[Ls].m(e,p),o(e,jo,p),o(e,Eo,p),s(Eo,_p),o(e,lr,p),E(bn,e,p),o(e,rr,p),E($n,e,p),o(e,ir,p),o(e,Fa,p),s(Fa,gp),s(Fa,wl),s(wl,wp),s(Fa,bp),o(e,pr,p),E(kn,e,p),o(e,hr,p),E(vn,e,p),o(e,cr,p),o(e,xo,p),s(xo,$p),o(e,dr,p),E(yn,e,p),o(e,ur,p),E(jn,e,p),o(e,fr,p),o(e,qo,p),s(qo,kp),o(e,mr,p),E(En,e,p),o(e,_r,p),E(xn,e,p),o(e,gr,p),Qn[Hs].m(e,p),o(e,To,p),o(e,ka,p),s(ka,Ia),s(Ia,bl),E(qn,bl,null),s(ka,vp),s(ka,$l),s($l,yp),o(e,wr,p),E(Tn,e,p),o(e,br,p),Ft&&Ft.m(e,p),o(e,zo,p),o(e,Xt,p),s(Xt,jp),s(Xt,zn),s(zn,Ep),s(Xt,xp),s(Xt,Dn),s(Dn,qp),s(Xt,Tp),s(Xt,kl),s(kl,zp),s(Xt,Dp),s(Xt,vl),s(vl,Pp),s(Xt,Ap),o(e,$r,p),o(e,La,p),s(La,Sp),s(La,Pn),s(Pn,Cp),s(La,Op),o(e,kr,p),E(An,e,p),o(e,vr,p),o(e,pa,p),s(pa,Fp),s(pa,yl),s(yl,Ip),s(pa,Lp),s(pa,Do),s(Do,Np),s(pa,Hp),o(e,yr,p),E(Sn,e,p),o(e,jr,p),o(e,Po,p),s(Po,Wp),o(e,Er,p),o(e,Ao,p),s(Ao,Mp),o(e,xr,p),E(Cn,e,p),o(e,qr,p),E(On,e,p),o(e,Tr,p),o(e,Zt,p),s(Zt,Up),s(Zt,Fn),s(Fn,Bp),s(Zt,Rp),s(Zt,jl),s(jl,Kp),s(Zt,Yp),s(Zt,El),s(El,Gp),s(Zt,Xp),s(Zt,In),s(In,Zp),s(Zt,Jp),o(e,zr,p),E(Ln,e,p),o(e,Dr,p),E(Nn,e,p),o(e,Pr,p),E(Hn,e,p),o(e,Ar,p),E(Wn,e,p),o(e,Sr,p),o(e,So,p),s(So,Vp),o(e,Cr,p),eo[Ms].m(e,p),o(e,Co,p),o(e,Oo,p),s(Oo,Qp),o(e,Or,p),o(e,va,p),s(va,Na),s(Na,xl),E(Mn,xl,null),s(va,eh),s(va,ql),s(ql,th),o(e,Fr,p),o(e,Fo,p),s(Fo,sh),o(e,Ir,p),E(Un,e,p),o(e,Lr,p),o(e,Io,p),s(Io,ah),o(e,Nr,p),o(e,Lo,p),s(Lo,nh),o(e,Hr,p),E(Bn,e,p),o(e,Wr,p),to[Bs].m(e,p),o(e,No,p),Qe&&Qe.m(e,p),o(e,Ho,p),o(e,ya,p),s(ya,Ha),s(Ha,Tl),E(Rn,Tl,null),s(ya,oh),s(ya,zl),s(zl,lh),o(e,Mr,p),o(e,Wa,p),s(Wa,rh),s(Wa,Dl),s(Dl,ih),s(Wa,ph),o(e,Ur,p),E(Kn,e,p),o(e,Br,p),E(Yn,e,p),o(e,Rr,p),o(e,Wo,p),s(Wo,hh),o(e,Kr,p),E(Gn,e,p),o(e,Yr,p),E(Xn,e,p),o(e,Gr,p),o(e,Mo,p),s(Mo,ch),o(e,Xr,p),E(Ma,e,p),Zr=!0},p(e,[p]){const so={};p&1&&(so.fw=e[0]),m.$set(so);let Uo=S;S=uh(e),S!==Uo&&(no(),b(Zn[Uo],1,1,()=>{Zn[Uo]=null}),ao(),H=Zn[S],H||(H=Zn[S]=dh[S](e),H.c()),w(H,1),H.m(U.parentNode,U));const Pl={};p&2&&(Pl.$$scope={dirty:p,ctx:e}),As.$set(Pl);const Al={};p&2&&(Al.$$scope={dirty:p,ctx:e}),za.$set(Al);const ja={};p&2&&(ja.$$scope={dirty:p,ctx:e}),Sa.$set(ja);const Sl={};p&2&&(Sl.$$scope={dirty:p,ctx:e}),Ca.$set(Sl);let Bo=Fs;Fs=mh(e),Fs!==Bo&&(no(),b(Jn[Bo],1,1,()=>{Jn[Bo]=null}),ao(),Is=Jn[Fs],Is||(Is=Jn[Fs]=fh[Fs](e),Is.c()),w(Is,1),Is.m(ko.parentNode,ko));let Ua=Ls;Ls=gh(e),Ls!==Ua&&(no(),b(Vn[Ua],1,1,()=>{Vn[Ua]=null}),ao(),Ns=Vn[Ls],Ns||(Ns=Vn[Ls]=_h[Ls](e),Ns.c()),w(Ns,1),Ns.m(jo.parentNode,jo));let Ea=Hs;Hs=bh(e),Hs!==Ea&&(no(),b(Qn[Ea],1,1,()=>{Qn[Ea]=null}),ao(),Ws=Qn[Hs],Ws||(Ws=Qn[Hs]=wh[Hs](e),Ws.c()),w(Ws,1),Ws.m(To.parentNode,To)),e[0]==="pt"?Ft||(Ft=md(),Ft.c(),Ft.m(zo.parentNode,zo)):Ft&&(Ft.d(1),Ft=null);let Ro=Ms;Ms=kh(e),Ms!==Ro&&(no(),b(eo[Ro],1,1,()=>{eo[Ro]=null}),ao(),Us=eo[Ms],Us||(Us=eo[Ms]=$h[Ms](e),Us.c()),w(Us,1),Us.m(Co.parentNode,Co));let Ko=Bs;Bs=yh(e),Bs!==Ko&&(no(),b(to[Ko],1,1,()=>{to[Ko]=null}),ao(),Rs=to[Bs],Rs||(Rs=to[Bs]=vh[Bs](e),Rs.c()),w(Rs,1),Rs.m(No.parentNode,No)),e[0]==="pt"?Qe?p&1&&w(Qe,1):(Qe=_d(),Qe.c(),w(Qe,1),Qe.m(Ho.parentNode,Ho)):Qe&&(no(),b(Qe,1,1,()=>{Qe=null}),ao());const Yo={};p&2&&(Yo.$$scope={dirty:p,ctx:e}),Ma.$set(Yo)},i(e){Zr||(w(m.$$.fragment,e),w(P.$$.fragment,e),w(H),w(ue.$$.fragment,e),w(Ne.$$.fragment,e),w(_t.$$.fragment,e),w(bt.$$.fragment,e),w(St.$$.fragment,e),w(Wt.$$.fragment,e),w(Mt.$$.fragment,e),w(Et.$$.fragment,e),w(Ds.$$.fragment,e),w(Ps.$$.fragment,e),w(u.$$.fragment,e),w(Kt.$$.fragment,e),w(Vs.$$.fragment,e),w(Yt.$$.fragment,e),w(Qs.$$.fragment,e),w(ta.$$.fragment,e),w(sa.$$.fragment,e),w(aa.$$.fragment,e),w(As.$$.fragment,e),w(oa.$$.fragment,e),w(ra.$$.fragment,e),w(Gt.$$.fragment,e),w(za.$$.fragment,e),w(cn.$$.fragment,e),w(dn.$$.fragment,e),w(un.$$.fragment,e),w(fn.$$.fragment,e),w(mn.$$.fragment,e),w(Sa.$$.fragment,e),w(Ca.$$.fragment,e),w(_n.$$.fragment,e),w(Is),w(gn.$$.fragment,e),w(Ns),w(bn.$$.fragment,e),w($n.$$.fragment,e),w(kn.$$.fragment,e),w(vn.$$.fragment,e),w(yn.$$.fragment,e),w(jn.$$.fragment,e),w(En.$$.fragment,e),w(xn.$$.fragment,e),w(Ws),w(qn.$$.fragment,e),w(Tn.$$.fragment,e),w(An.$$.fragment,e),w(Sn.$$.fragment,e),w(Cn.$$.fragment,e),w(On.$$.fragment,e),w(Ln.$$.fragment,e),w(Nn.$$.fragment,e),w(Hn.$$.fragment,e),w(Wn.$$.fragment,e),w(Us),w(Mn.$$.fragment,e),w(Un.$$.fragment,e),w(Bn.$$.fragment,e),w(Rs),w(Qe),w(Rn.$$.fragment,e),w(Kn.$$.fragment,e),w(Yn.$$.fragment,e),w(Gn.$$.fragment,e),w(Xn.$$.fragment,e),w(Ma.$$.fragment,e),Zr=!0)},o(e){b(m.$$.fragment,e),b(P.$$.fragment,e),b(H),b(ue.$$.fragment,e),b(Ne.$$.fragment,e),b(_t.$$.fragment,e),b(bt.$$.fragment,e),b(St.$$.fragment,e),b(Wt.$$.fragment,e),b(Mt.$$.fragment,e),b(Et.$$.fragment,e),b(Ds.$$.fragment,e),b(Ps.$$.fragment,e),b(u.$$.fragment,e),b(Kt.$$.fragment,e),b(Vs.$$.fragment,e),b(Yt.$$.fragment,e),b(Qs.$$.fragment,e),b(ta.$$.fragment,e),b(sa.$$.fragment,e),b(aa.$$.fragment,e),b(As.$$.fragment,e),b(oa.$$.fragment,e),b(ra.$$.fragment,e),b(Gt.$$.fragment,e),b(za.$$.fragment,e),b(cn.$$.fragment,e),b(dn.$$.fragment,e),b(un.$$.fragment,e),b(fn.$$.fragment,e),b(mn.$$.fragment,e),b(Sa.$$.fragment,e),b(Ca.$$.fragment,e),b(_n.$$.fragment,e),b(Is),b(gn.$$.fragment,e),b(Ns),b(bn.$$.fragment,e),b($n.$$.fragment,e),b(kn.$$.fragment,e),b(vn.$$.fragment,e),b(yn.$$.fragment,e),b(jn.$$.fragment,e),b(En.$$.fragment,e),b(xn.$$.fragment,e),b(Ws),b(qn.$$.fragment,e),b(Tn.$$.fragment,e),b(An.$$.fragment,e),b(Sn.$$.fragment,e),b(Cn.$$.fragment,e),b(On.$$.fragment,e),b(Ln.$$.fragment,e),b(Nn.$$.fragment,e),b(Hn.$$.fragment,e),b(Wn.$$.fragment,e),b(Us),b(Mn.$$.fragment,e),b(Un.$$.fragment,e),b(Bn.$$.fragment,e),b(Rs),b(Qe),b(Rn.$$.fragment,e),b(Kn.$$.fragment,e),b(Yn.$$.fragment,e),b(Gn.$$.fragment,e),b(Xn.$$.fragment,e),b(Ma.$$.fragment,e),Zr=!1},d(e){t(h),e&&t($),x(m,e),e&&t(q),e&&t(A),x(P),e&&t(v),Zn[S].d(e),e&&t(U),e&&t(F),e&&t(Q),e&&t(ee),e&&t(Te),x(ue,e),e&&t(K),e&&t(ve),e&&t(xe),e&&t(J),e&&t(_e),e&&t(k),e&&t(tt),e&&t(pe),e&&t(ht),e&&t(je),e&&t(yt),e&&t(ge),e&&t(st),e&&t(Ie),e&&t(dt),e&&t(ut),x(Ne),e&&t(nt),e&&t(He),e&&t(Lt),e&&t(ot),x(_t),e&&t(ts),e&&t(Ue),e&&t(At),x(bt,e),e&&t(ss),e&&t(Ze),e&&t(ys),e&&t(rt),e&&t(js),e&&t(Pe),e&&t(as),x(St,e),e&&t(Re),x(Wt,e),e&&t(Ct),e&&t(Je),e&&t(os),x(Mt,e),e&&t(xs),x(Et,e),e&&t(qs),e&&t(Ce),e&&t(Js),x(Ds,e),e&&t(xa),e&&t(Rt),e&&t(qa),x(Ps,e),e&&t(fa),x(u,e),e&&t(L),e&&t(ma),e&&t(Ja),x(Kt,e),e&&t(Va),x(Vs,e),e&&t(_a),e&&t(ga),e&&t(Qa),x(Yt,e),e&&t(en),x(Qs,e),e&&t(tn),e&&t(wa),e&&t(sn),x(ta,e),e&&t(rs),x(sa,e),e&&t(an),e&&t(ba),e&&t(nn),x(aa,e),e&&t(on),x(As,e),e&&t(Ss),e&&t(_s),x(oa),e&&t(ln),x(ra,e),e&&t(Os),e&&t(is),e&&t(rn),x(Gt,e),e&&t(pn),e&&t(ps),e&&t(Ol),x(za,e),e&&t(Fl),e&&t(Da),e&&t(Il),e&&t(Pa),e&&t(Ll),x(cn,e),e&&t(Nl),e&&t(ia),e&&t(Hl),e&&t(Aa),e&&t(Wl),e&&t(mo),e&&t(Ml),x(dn,e),e&&t(Ul),e&&t(_o),e&&t(Bl),x(un,e),e&&t(Rl),x(fn,e),e&&t(Kl),e&&t(go),e&&t(Yl),e&&t(ws),e&&t(Gl),x(mn,e),e&&t(Xl),e&&t(wo),e&&t(Zl),x(Sa,e),e&&t(Jl),x(Ca,e),e&&t(Vl),e&&t(bo),e&&t(Ql),x(_n,e),e&&t(er),e&&t($o),e&&t(tr),Jn[Fs].d(e),e&&t(ko),e&&t(vo),e&&t(sr),e&&t($a),x(gn),e&&t(ar),e&&t(bs),e&&t(nr),e&&t(it),e&&t(or),Vn[Ls].d(e),e&&t(jo),e&&t(Eo),e&&t(lr),x(bn,e),e&&t(rr),x($n,e),e&&t(ir),e&&t(Fa),e&&t(pr),x(kn,e),e&&t(hr),x(vn,e),e&&t(cr),e&&t(xo),e&&t(dr),x(yn,e),e&&t(ur),x(jn,e),e&&t(fr),e&&t(qo),e&&t(mr),x(En,e),e&&t(_r),x(xn,e),e&&t(gr),Qn[Hs].d(e),e&&t(To),e&&t(ka),x(qn),e&&t(wr),x(Tn,e),e&&t(br),Ft&&Ft.d(e),e&&t(zo),e&&t(Xt),e&&t($r),e&&t(La),e&&t(kr),x(An,e),e&&t(vr),e&&t(pa),e&&t(yr),x(Sn,e),e&&t(jr),e&&t(Po),e&&t(Er),e&&t(Ao),e&&t(xr),x(Cn,e),e&&t(qr),x(On,e),e&&t(Tr),e&&t(Zt),e&&t(zr),x(Ln,e),e&&t(Dr),x(Nn,e),e&&t(Pr),x(Hn,e),e&&t(Ar),x(Wn,e),e&&t(Sr),e&&t(So),e&&t(Cr),eo[Ms].d(e),e&&t(Co),e&&t(Oo),e&&t(Or),e&&t(va),x(Mn),e&&t(Fr),e&&t(Fo),e&&t(Ir),x(Un,e),e&&t(Lr),e&&t(Io),e&&t(Nr),e&&t(Lo),e&&t(Hr),x(Bn,e),e&&t(Wr),to[Bs].d(e),e&&t(No),Qe&&Qe.d(e),e&&t(Ho),e&&t(ya),x(Rn),e&&t(Mr),e&&t(Wa),e&&t(Ur),x(Kn,e),e&&t(Br),x(Yn,e),e&&t(Rr),e&&t(Wo),e&&t(Kr),x(Gn,e),e&&t(Yr),x(Xn,e),e&&t(Gr),e&&t(Mo),e&&t(Xr),x(Ma,e)}}}const Rd={local:"translation",sections:[{local:"preparing-the-data",sections:[{local:"the-kde4-dataset",title:"The KDE4 dataset"},{local:"processing-the-data",title:"Processing the data"}],title:"Preparing the data"},{local:"finetuning-the-model-with-the-trainer-api",title:"Fine-tuning the model with the `Trainer` API"},{local:"finetuning-the-model-with-keras",sections:[{local:"data-collation",title:"Data collation"},{local:"metrics",title:"Metrics"},{local:"finetuning-the-model",title:"Fine-tuning the model"}],title:"Fine-tuning the model with Keras"},{local:"a-custom-training-loop",sections:[{local:"preparing-everything-for-training",title:"Preparing everything for training"},{local:"training-loop",title:"Training loop"}],title:"A custom training loop"},{local:"using-the-finetuned-model",title:"Using the fine-tuned model"}],title:"Translation"};function Kd(B,h,$){let m="pt";return vd(()=>{const q=new URLSearchParams(window.location.search);$(0,m=q.get("fw")||"pt")}),[m]}class eu extends wd{constructor(h){super();bd(this,h,Kd,Bd,$d,{})}}export{eu as default,Rd as metadata};
