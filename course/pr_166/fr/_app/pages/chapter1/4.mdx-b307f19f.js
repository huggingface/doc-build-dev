import{S as xm,i as Pm,s as $m,e as r,k as d,w as h,t as l,M as km,c as a,d as s,m,a as n,x as v,h as o,b as c,N as p,F as t,g as u,y as g,L as wm,q as E,o as _,B as q,v as ym}from"../../chunks/vendor-1e8b365d.js";import{Y as Uu}from"../../chunks/Youtube-c2a8cc39.js";import{I as S}from"../../chunks/IconCopyLink-483c28ba.js";function Lm(Hu){let z,Yr,B,me,is,Be,In,us,Cn,Fr,pe,Nn,cs,Gn,Rn,Jr,D,fe,ds,De,jn,ms,Sn,Xr,he,zn,ps,Bn,Dn,Qr,O,Oe,Vu,On,Ue,Yu,Wr,He,ve,Un,fs,Hn,Vn,Zr,f,hs,b,vs,Yn,Fn,Ve,Jn,Xn,gs,Qn,Wn,Es,Zn,Kn,el,_s,ge,qs,tl,sl,Ye,rl,al,nl,bs,Ee,xs,ll,ol,Fe,il,ul,cl,Ps,_e,$s,dl,ml,Je,pl,fl,hl,ks,x,ws,vl,gl,Xe,El,_l,Qe,ql,bl,ys,xl,Pl,$l,Ls,P,Ts,kl,wl,We,yl,Ll,Ms,Tl,Ml,As,Al,Il,Kr,qe,Cl,Is,Nl,Gl,ea,L,U,Rl,Cs,jl,Sl,Ns,zl,Bl,Dl,H,Ol,Gs,Ul,Hl,Rs,Vl,Yl,Fl,V,Jl,js,Xl,Ql,Ss,Wl,Zl,ta,yt,Kl,sa,Y,be,zs,Ze,eo,Bs,to,ra,T,so,Ds,ro,ao,Os,no,lo,aa,xe,oo,Us,io,uo,na,M,co,Hs,mo,po,Vs,fo,ho,la,F,Ke,Fu,vo,et,Ju,oa,Pe,go,Ys,Eo,_o,ia,J,tt,Xu,qo,st,Qu,ua,X,$e,Fs,rt,bo,Js,xo,ca,Lt,Po,da,at,nt,Wu,ma,Tt,$o,pa,Q,lt,Zu,ko,ot,Ku,fa,it,ha,Mt,wo,va,At,yo,ga,It,Lo,Ea,W,ke,Xs,ut,To,Qs,Mo,_a,ct,qa,Ct,Ao,ba,Z,dt,ec,Io,mt,tc,xa,Nt,Co,Pa,A,No,Ws,Go,Ro,Zs,jo,So,$a,I,K,zo,Ks,Bo,Do,er,Oo,Uo,Ho,pt,Vo,tr,Yo,Fo,Jo,sr,Xo,ka,C,Qo,rr,Wo,Zo,ar,Ko,ei,wa,ee,ft,sc,ti,ht,rc,ya,N,si,nr,ri,ai,lr,ni,li,La,we,oi,or,ii,ui,Ta,te,ye,ir,vt,ci,ur,di,Ma,Le,mi,cr,pi,fi,Aa,gt,Ia,se,Te,dr,Et,hi,mr,vi,Ca,Gt,gi,Na,Me,Rt,pr,Ei,_i,qi,jt,fr,bi,xi,Ga,re,_t,ac,Pi,qt,nc,Ra,St,$i,ja,G,zt,hr,ki,wi,yi,Bt,vr,Li,Ti,Mi,Ae,gr,Ai,Ii,Er,Ci,Ni,Sa,Dt,Gi,za,ae,Ie,_r,bt,Ri,qr,ji,Ba,$,Si,br,zi,Bi,xr,Di,Oi,xt,Pr,Ui,Hi,Da,Ot,Vi,Oa,Ut,Yi,Ua,Ce,Fi,$r,Ji,Xi,Ha,ne,Ne,kr,Pt,Qi,wr,Wi,Va,Ge,Zi,yr,Ki,eu,Ya,Ht,tu,Fa,Re,su,Lr,ru,au,Ja,le,$t,lc,nu,kt,oc,Xa,Vt,lu,Qa,R,ou,Tr,iu,uu,Mr,cu,du,Wa,oe,je,Ar,wt,mu,Ir,pu,Za,j,Yt,Cr,fu,hu,vu,Ft,Nr,gu,Eu,_u,k,Gr,qu,bu,Rr,xu,Pu,jr,$u,ku,Sr,wu,yu,Ka,w,Lu,zr,Tu,Mu,Br,Au,Iu,Dr,Cu,Nu,en;return Be=new S({}),De=new S({}),Ze=new S({}),rt=new S({}),it=new Uu({props:{id:"ftWlj4FBHTg"}}),ut=new S({}),ct=new Uu({props:{id:"BqqfQnyjmgg"}}),vt=new S({}),gt=new Uu({props:{id:"H39Z_720T5s"}}),Et=new S({}),bt=new S({}),Pt=new S({}),wt=new S({}),{c(){z=r("meta"),Yr=d(),B=r("h1"),me=r("a"),is=r("span"),h(Be.$$.fragment),In=d(),us=r("span"),Cn=l("Comment fonctionnent les *transformers* ?"),Fr=d(),pe=r("p"),Nn=l("Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),cs=r("em"),Gn=l("transformers"),Rn=l("."),Jr=d(),D=r("h2"),fe=r("a"),ds=r("span"),h(De.$$.fragment),jn=d(),ms=r("span"),Sn=l("Court historique des *transformers*"),Xr=d(),he=r("p"),zn=l("Voici quelques dates clefs dans la courte histoire des "),ps=r("em"),Bn=l("transformers"),Dn=l(" :"),Qr=d(),O=r("div"),Oe=r("img"),On=d(),Ue=r("img"),Wr=d(),He=r("p"),ve=r("a"),Un=l("L\u2019architecture "),fs=r("em"),Hn=l("Transformer"),Vn=l(" a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),Zr=d(),f=r("ul"),hs=r("li"),b=r("p"),vs=r("strong"),Yn=l("Juin 2018"),Fn=l(" : "),Ve=r("a"),Jn=l("GPT"),Xn=l(", le premier "),gs=r("em"),Qn=l("transformer"),Wn=l(" pr\xE9-entra\xEEn\xE9 et "),Es=r("em"),Zn=l("finetun\xE9"),Kn=l(" sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),el=d(),_s=r("li"),ge=r("p"),qs=r("strong"),tl=l("Octobre 2018"),sl=l(" : "),Ye=r("a"),rl=l("BERT"),al=l(", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),nl=d(),bs=r("li"),Ee=r("p"),xs=r("strong"),ll=l("F\xE9vrier 2019"),ol=l(" : "),Fe=r("a"),il=l("GPT-2"),ul=l(", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),cl=d(),Ps=r("li"),_e=r("p"),$s=r("strong"),dl=l("Octobre 2019"),ml=l(" : "),Je=r("a"),pl=l("DistilBERT"),fl=l(", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),hl=d(),ks=r("li"),x=r("p"),ws=r("strong"),vl=l("Octobre 2019"),gl=l(" : "),Xe=r("a"),El=l("BART"),_l=l(" et "),Qe=r("a"),ql=l("T5"),bl=l(", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),ys=r("em"),xl=l("transformer"),Pl=l(" original (les premiers \xE0 faire cela),"),$l=d(),Ls=r("li"),P=r("p"),Ts=r("strong"),kl=l("Mai 2020"),wl=l(" : "),We=r("a"),yl=l("GPT-3"),Ll=l(", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Ms=r("em"),Tl=l("finetuning"),Ml=l(" (appel\xE9 "),As=r("em"),Al=l("zero-shot learning"),Il=l(")."),Kr=d(),qe=r("p"),Cl=l("Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Is=r("em"),Nl=l("transformers"),Gl=l(". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),ea=d(),L=r("ul"),U=r("li"),Rl=l("ceux de type GPT (aussi appel\xE9s "),Cs=r("em"),jl=l("transformers"),Sl=d(),Ns=r("em"),zl=l("autor\xE9gressifs"),Bl=l(")"),Dl=d(),H=r("li"),Ol=l("ceux de type BERT (aussi appel\xE9s "),Gs=r("em"),Ul=l("transformers"),Hl=d(),Rs=r("em"),Vl=l("auto-encodeurs"),Yl=l(")"),Fl=d(),V=r("li"),Jl=l("ceux de type BART/T5 (aussi appel\xE9s "),js=r("em"),Xl=l("transformers"),Ql=d(),Ss=r("em"),Wl=l("s\xE9quence-\xE0-s\xE9quence"),Zl=l(")"),ta=d(),yt=r("p"),Kl=l("Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),sa=d(),Y=r("h2"),be=r("a"),zs=r("span"),h(Ze.$$.fragment),eo=d(),Bs=r("span"),to=l("Les *transformers* sont des mod\xE8les de langage"),ra=d(),T=r("p"),so=l("Tous les "),Ds=r("em"),ro=l("transformers"),ao=l(" mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Os=r("em"),no=l("mod\xE8les de langage"),lo=l(". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),aa=d(),xe=r("p"),oo=l("Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),Us=r("em"),io=l("finetun\xE9"),uo=l(" de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),na=d(),M=r("p"),co=l("Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),Hs=r("em"),mo=l("n"),po=l(" mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),Vs=r("em"),fo=l("mod\xE9lisation causale du langage"),ho=l(" car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),la=d(),F=r("div"),Ke=r("img"),vo=d(),et=r("img"),oa=d(),Pe=r("p"),go=l("Un autre exemple est la "),Ys=r("em"),Eo=l("mod\xE9lisation du langage masqu\xE9"),_o=l(", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),ia=d(),J=r("div"),tt=r("img"),qo=d(),st=r("img"),ua=d(),X=r("h2"),$e=r("a"),Fs=r("span"),h(rt.$$.fragment),bo=d(),Js=r("span"),xo=l("Les *transformers* sont \xE9normes"),ca=d(),Lt=r("p"),Po=l("En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),da=d(),at=r("div"),nt=r("img"),ma=d(),Tt=r("p"),$o=l("Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),pa=d(),Q=r("div"),lt=r("img"),ko=d(),ot=r("img"),fa=d(),h(it.$$.fragment),ha=d(),Mt=r("p"),wo=l("L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),va=d(),At=r("p"),yo=l("Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),ga=d(),It=r("p"),Lo=l("C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),Ea=d(),W=r("h2"),ke=r("a"),Xs=r("span"),h(ut.$$.fragment),To=d(),Qs=r("span"),Mo=l("L'apprentissage par transfert"),_a=d(),h(ct.$$.fragment),qa=d(),Ct=r("p"),Ao=l("Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),ba=d(),Z=r("div"),dt=r("img"),Io=d(),mt=r("img"),xa=d(),Nt=r("p"),Co=l("Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),Pa=d(),A=r("p"),No=l("Le "),Ws=r("em"),Go=l("finetuning"),Ro=l(", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),Zs=r("em"),jo=l("finetuning"),So=l(", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),$a=d(),I=r("ul"),K=r("li"),zo=l("Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),Ks=r("em"),Bo=l("finetuning"),Do=l(". Le processus de "),er=r("em"),Oo=l("finetuning"),Uo=l(" est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),Ho=d(),pt=r("li"),Vo=l("Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),tr=r("em"),Yo=l("finetuning"),Fo=l(" n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),Jo=d(),sr=r("li"),Xo=l("Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),ka=d(),C=r("p"),Qo=l("Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),rr=r("em"),Wo=l("finetuner"),Zo=l(" sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),ar=r("em"),Ko=l("finetuning"),ei=l(" ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),wa=d(),ee=r("div"),ft=r("img"),ti=d(),ht=r("img"),ya=d(),N=r("p"),si=l("Le "),nr=r("em"),ri=l("finetuning"),ai=l(" d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),lr=r("em"),ni=l("finetuning"),li=l(" car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),La=d(),we=r("p"),oi=l("Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),or=r("em"),ii=l("finetuner"),ui=l("."),Ta=d(),te=r("h2"),ye=r("a"),ir=r("span"),h(vt.$$.fragment),ci=d(),ur=r("span"),di=l("Architecture g\xE9n\xE9rale"),Ma=d(),Le=r("p"),mi=l("Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),cr=r("em"),pi=l("transformers"),fi=l(". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),Aa=d(),h(gt.$$.fragment),Ia=d(),se=r("h2"),Te=r("a"),dr=r("span"),h(Et.$$.fragment),hi=d(),mr=r("span"),vi=l("Introduction"),Ca=d(),Gt=r("p"),gi=l("Le mod\xE8le est principalement compos\xE9 de deux blocs :"),Na=d(),Me=r("ul"),Rt=r("li"),pr=r("strong"),Ei=l("Encodeur (\xE0 gauche)"),_i=l(" : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),qi=d(),jt=r("li"),fr=r("strong"),bi=l("D\xE9codeur (\xE0 droite)"),xi=l(" : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),Ga=d(),re=r("div"),_t=r("img"),Pi=d(),qt=r("img"),Ra=d(),St=r("p"),$i=l("Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),ja=d(),G=r("ul"),zt=r("li"),hr=r("strong"),ki=l("Mod\xE8les uniquement encodeurs"),wi=l(" : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),yi=d(),Bt=r("li"),vr=r("strong"),Li=l("Mod\xE8les uniquement d\xE9codeurs"),Ti=l(" : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),Mi=d(),Ae=r("li"),gr=r("strong"),Ai=l("Mod\xE8les encodeurs-d\xE9codeurs"),Ii=l(" (ou "),Er=r("strong"),Ci=l("mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),Ni=l(") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),Sa=d(),Dt=r("p"),Gi=l("Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),za=d(),ae=r("h2"),Ie=r("a"),_r=r("span"),h(bt.$$.fragment),Ri=d(),qr=r("span"),ji=l("Les couches d'attention"),Ba=d(),$=r("p"),Si=l("Une caract\xE9ristique cl\xE9 des "),br=r("em"),zi=l("transformers"),Bi=l(" est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),xr=r("em"),Di=l("transformer"),Oi=l(" s\u2019e nome "),xt=r("a"),Pr=r("em"),Ui=l("Attention Is All You Need"),Hi=l(" ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),Da=d(),Ot=r("p"),Vi=l("Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB You like this course \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB You \xBB pour obtenir la traduction correcte du mot \xAB like \xBB, car en fran\xE7ais le verbe \xAB like \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB this \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB course  \xBB car \xAB this \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB this \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),Oa=d(),Ut=r("p"),Yi=l("Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),Ua=d(),Ce=r("p"),Fi=l("Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),$r=r("em"),Ji=l("transformers"),Xi=l("."),Ha=d(),ne=r("h2"),Ne=r("a"),kr=r("span"),h(Pt.$$.fragment),Qi=d(),wr=r("span"),Wi=l("L'architecture originale"),Va=d(),Ge=r("p"),Zi=l("L\u2019architecture du "),yr=r("em"),Ki=l("transformer"),eu=l(" a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),Ya=d(),Ht=r("p"),tu=l("Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),Fa=d(),Re=r("p"),su=l("L\u2019architecture originale du "),Lr=r("em"),ru=l("transformer"),au=l(" ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),Ja=d(),le=r("div"),$t=r("img"),nu=d(),kt=r("img"),Xa=d(),Vt=r("p"),lu=l("Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),Qa=d(),R=r("p"),ou=l("Le "),Tr=r("em"),iu=l("masque d\u2019attention"),uu=l(" peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Mr=r("em"),cu=l("padding"),du=l(") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),Wa=d(),oe=r("h2"),je=r("a"),Ar=r("span"),h(wt.$$.fragment),mu=d(),Ir=r("span"),pu=l("Architectures contre *checkpoints*"),Za=l(`

 
En approfondissant l'\xE9tude des *transformers* dans ce cours, vous verrez des mentions d'*architectures* et de *checkpoints* ainsi que de *mod\xE8les*. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),j=r("ul"),Yt=r("li"),Cr=r("strong"),fu=l("Architecture"),hu=l(" : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),vu=d(),Ft=r("li"),Nr=r("strong"),gu=l("Checkpoints"),Eu=l(" : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),_u=d(),k=r("li"),Gr=r("strong"),qu=l("Mod\xE8le"),bu=l(" : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),Rr=r("em"),xu=l("checkpoint"),Pu=l(" \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),jr=r("em"),$u=l("architecture"),ku=l(" ou "),Sr=r("em"),wu=l("checkpoint"),yu=l(" lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),Ka=d(),w=r("p"),Lu=l("Par exemple, BERT est une architecture alors que "),zr=r("code"),Tu=l("bert-base-cased"),Mu=l(" (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),Br=r("em"),Au=l("checkpoint"),Iu=l(". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),Dr=r("code"),Cu=l("bert-base-cased"),Nu=l(" \xBB."),this.h()},l(e){const i=km('[data-svelte="svelte-1phssyn"]',document.head);z=a(i,"META",{name:!0,content:!0}),i.forEach(s),Yr=m(e),B=a(e,"H1",{class:!0});var tn=n(B);me=a(tn,"A",{id:!0,class:!0,href:!0});var ic=n(me);is=a(ic,"SPAN",{});var uc=n(is);v(Be.$$.fragment,uc),uc.forEach(s),ic.forEach(s),In=m(tn),us=a(tn,"SPAN",{});var cc=n(us);Cn=o(cc,"Comment fonctionnent les *transformers* ?"),cc.forEach(s),tn.forEach(s),Fr=m(e),pe=a(e,"P",{});var sn=n(pe);Nn=o(sn,"Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),cs=a(sn,"EM",{});var dc=n(cs);Gn=o(dc,"transformers"),dc.forEach(s),Rn=o(sn,"."),sn.forEach(s),Jr=m(e),D=a(e,"H2",{class:!0});var rn=n(D);fe=a(rn,"A",{id:!0,class:!0,href:!0});var mc=n(fe);ds=a(mc,"SPAN",{});var pc=n(ds);v(De.$$.fragment,pc),pc.forEach(s),mc.forEach(s),jn=m(rn),ms=a(rn,"SPAN",{});var fc=n(ms);Sn=o(fc,"Court historique des *transformers*"),fc.forEach(s),rn.forEach(s),Xr=m(e),he=a(e,"P",{});var an=n(he);zn=o(an,"Voici quelques dates clefs dans la courte histoire des "),ps=a(an,"EM",{});var hc=n(ps);Bn=o(hc,"transformers"),hc.forEach(s),Dn=o(an," :"),an.forEach(s),Qr=m(e),O=a(e,"DIV",{class:!0});var nn=n(O);Oe=a(nn,"IMG",{class:!0,src:!0,alt:!0}),On=m(nn),Ue=a(nn,"IMG",{class:!0,src:!0,alt:!0}),nn.forEach(s),Wr=m(e),He=a(e,"P",{});var Gu=n(He);ve=a(Gu,"A",{href:!0,rel:!0});var Ru=n(ve);Un=o(Ru,"L\u2019architecture "),fs=a(Ru,"EM",{});var vc=n(fs);Hn=o(vc,"Transformer"),vc.forEach(s),Ru.forEach(s),Vn=o(Gu," a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),Gu.forEach(s),Zr=m(e),f=a(e,"UL",{});var y=n(f);hs=a(y,"LI",{});var gc=n(hs);b=a(gc,"P",{});var ie=n(b);vs=a(ie,"STRONG",{});var Ec=n(vs);Yn=o(Ec,"Juin 2018"),Ec.forEach(s),Fn=o(ie," : "),Ve=a(ie,"A",{href:!0,rel:!0});var _c=n(Ve);Jn=o(_c,"GPT"),_c.forEach(s),Xn=o(ie,", le premier "),gs=a(ie,"EM",{});var qc=n(gs);Qn=o(qc,"transformer"),qc.forEach(s),Wn=o(ie," pr\xE9-entra\xEEn\xE9 et "),Es=a(ie,"EM",{});var bc=n(Es);Zn=o(bc,"finetun\xE9"),bc.forEach(s),Kn=o(ie," sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),ie.forEach(s),gc.forEach(s),el=m(y),_s=a(y,"LI",{});var xc=n(_s);ge=a(xc,"P",{});var Or=n(ge);qs=a(Or,"STRONG",{});var Pc=n(qs);tl=o(Pc,"Octobre 2018"),Pc.forEach(s),sl=o(Or," : "),Ye=a(Or,"A",{href:!0,rel:!0});var $c=n(Ye);rl=o($c,"BERT"),$c.forEach(s),al=o(Or,", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),Or.forEach(s),xc.forEach(s),nl=m(y),bs=a(y,"LI",{});var kc=n(bs);Ee=a(kc,"P",{});var Ur=n(Ee);xs=a(Ur,"STRONG",{});var wc=n(xs);ll=o(wc,"F\xE9vrier 2019"),wc.forEach(s),ol=o(Ur," : "),Fe=a(Ur,"A",{href:!0,rel:!0});var yc=n(Fe);il=o(yc,"GPT-2"),yc.forEach(s),ul=o(Ur,", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),Ur.forEach(s),kc.forEach(s),cl=m(y),Ps=a(y,"LI",{});var Lc=n(Ps);_e=a(Lc,"P",{});var Hr=n(_e);$s=a(Hr,"STRONG",{});var Tc=n($s);dl=o(Tc,"Octobre 2019"),Tc.forEach(s),ml=o(Hr," : "),Je=a(Hr,"A",{href:!0,rel:!0});var Mc=n(Je);pl=o(Mc,"DistilBERT"),Mc.forEach(s),fl=o(Hr,", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),Hr.forEach(s),Lc.forEach(s),hl=m(y),ks=a(y,"LI",{});var Ac=n(ks);x=a(Ac,"P",{});var ue=n(x);ws=a(ue,"STRONG",{});var Ic=n(ws);vl=o(Ic,"Octobre 2019"),Ic.forEach(s),gl=o(ue," : "),Xe=a(ue,"A",{href:!0,rel:!0});var Cc=n(Xe);El=o(Cc,"BART"),Cc.forEach(s),_l=o(ue," et "),Qe=a(ue,"A",{href:!0,rel:!0});var Nc=n(Qe);ql=o(Nc,"T5"),Nc.forEach(s),bl=o(ue,", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),ys=a(ue,"EM",{});var Gc=n(ys);xl=o(Gc,"transformer"),Gc.forEach(s),Pl=o(ue," original (les premiers \xE0 faire cela),"),ue.forEach(s),Ac.forEach(s),$l=m(y),Ls=a(y,"LI",{});var Rc=n(Ls);P=a(Rc,"P",{});var ce=n(P);Ts=a(ce,"STRONG",{});var jc=n(Ts);kl=o(jc,"Mai 2020"),jc.forEach(s),wl=o(ce," : "),We=a(ce,"A",{href:!0,rel:!0});var Sc=n(We);yl=o(Sc,"GPT-3"),Sc.forEach(s),Ll=o(ce,", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Ms=a(ce,"EM",{});var zc=n(Ms);Tl=o(zc,"finetuning"),zc.forEach(s),Ml=o(ce," (appel\xE9 "),As=a(ce,"EM",{});var Bc=n(As);Al=o(Bc,"zero-shot learning"),Bc.forEach(s),Il=o(ce,")."),ce.forEach(s),Rc.forEach(s),y.forEach(s),Kr=m(e),qe=a(e,"P",{});var ln=n(qe);Cl=o(ln,"Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Is=a(ln,"EM",{});var Dc=n(Is);Nl=o(Dc,"transformers"),Dc.forEach(s),Gl=o(ln,". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),ln.forEach(s),ea=m(e),L=a(e,"UL",{});var Jt=n(L);U=a(Jt,"LI",{});var Xt=n(U);Rl=o(Xt,"ceux de type GPT (aussi appel\xE9s "),Cs=a(Xt,"EM",{});var Oc=n(Cs);jl=o(Oc,"transformers"),Oc.forEach(s),Sl=m(Xt),Ns=a(Xt,"EM",{});var Uc=n(Ns);zl=o(Uc,"autor\xE9gressifs"),Uc.forEach(s),Bl=o(Xt,")"),Xt.forEach(s),Dl=m(Jt),H=a(Jt,"LI",{});var Qt=n(H);Ol=o(Qt,"ceux de type BERT (aussi appel\xE9s "),Gs=a(Qt,"EM",{});var Hc=n(Gs);Ul=o(Hc,"transformers"),Hc.forEach(s),Hl=m(Qt),Rs=a(Qt,"EM",{});var Vc=n(Rs);Vl=o(Vc,"auto-encodeurs"),Vc.forEach(s),Yl=o(Qt,")"),Qt.forEach(s),Fl=m(Jt),V=a(Jt,"LI",{});var Wt=n(V);Jl=o(Wt,"ceux de type BART/T5 (aussi appel\xE9s "),js=a(Wt,"EM",{});var Yc=n(js);Xl=o(Yc,"transformers"),Yc.forEach(s),Ql=m(Wt),Ss=a(Wt,"EM",{});var Fc=n(Ss);Wl=o(Fc,"s\xE9quence-\xE0-s\xE9quence"),Fc.forEach(s),Zl=o(Wt,")"),Wt.forEach(s),Jt.forEach(s),ta=m(e),yt=a(e,"P",{});var Jc=n(yt);Kl=o(Jc,"Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),Jc.forEach(s),sa=m(e),Y=a(e,"H2",{class:!0});var on=n(Y);be=a(on,"A",{id:!0,class:!0,href:!0});var Xc=n(be);zs=a(Xc,"SPAN",{});var Qc=n(zs);v(Ze.$$.fragment,Qc),Qc.forEach(s),Xc.forEach(s),eo=m(on),Bs=a(on,"SPAN",{});var Wc=n(Bs);to=o(Wc,"Les *transformers* sont des mod\xE8les de langage"),Wc.forEach(s),on.forEach(s),ra=m(e),T=a(e,"P",{});var Zt=n(T);so=o(Zt,"Tous les "),Ds=a(Zt,"EM",{});var Zc=n(Ds);ro=o(Zc,"transformers"),Zc.forEach(s),ao=o(Zt," mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Os=a(Zt,"EM",{});var Kc=n(Os);no=o(Kc,"mod\xE8les de langage"),Kc.forEach(s),lo=o(Zt,". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),Zt.forEach(s),aa=m(e),xe=a(e,"P",{});var un=n(xe);oo=o(un,"Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),Us=a(un,"EM",{});var ed=n(Us);io=o(ed,"finetun\xE9"),ed.forEach(s),uo=o(un," de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),un.forEach(s),na=m(e),M=a(e,"P",{});var Kt=n(M);co=o(Kt,"Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),Hs=a(Kt,"EM",{});var td=n(Hs);mo=o(td,"n"),td.forEach(s),po=o(Kt," mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),Vs=a(Kt,"EM",{});var sd=n(Vs);fo=o(sd,"mod\xE9lisation causale du langage"),sd.forEach(s),ho=o(Kt," car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),Kt.forEach(s),la=m(e),F=a(e,"DIV",{class:!0});var cn=n(F);Ke=a(cn,"IMG",{class:!0,src:!0,alt:!0}),vo=m(cn),et=a(cn,"IMG",{class:!0,src:!0,alt:!0}),cn.forEach(s),oa=m(e),Pe=a(e,"P",{});var dn=n(Pe);go=o(dn,"Un autre exemple est la "),Ys=a(dn,"EM",{});var rd=n(Ys);Eo=o(rd,"mod\xE9lisation du langage masqu\xE9"),rd.forEach(s),_o=o(dn,", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),dn.forEach(s),ia=m(e),J=a(e,"DIV",{class:!0});var mn=n(J);tt=a(mn,"IMG",{class:!0,src:!0,alt:!0}),qo=m(mn),st=a(mn,"IMG",{class:!0,src:!0,alt:!0}),mn.forEach(s),ua=m(e),X=a(e,"H2",{class:!0});var pn=n(X);$e=a(pn,"A",{id:!0,class:!0,href:!0});var ad=n($e);Fs=a(ad,"SPAN",{});var nd=n(Fs);v(rt.$$.fragment,nd),nd.forEach(s),ad.forEach(s),bo=m(pn),Js=a(pn,"SPAN",{});var ld=n(Js);xo=o(ld,"Les *transformers* sont \xE9normes"),ld.forEach(s),pn.forEach(s),ca=m(e),Lt=a(e,"P",{});var od=n(Lt);Po=o(od,"En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),od.forEach(s),da=m(e),at=a(e,"DIV",{class:!0});var id=n(at);nt=a(id,"IMG",{src:!0,alt:!0,width:!0}),id.forEach(s),ma=m(e),Tt=a(e,"P",{});var ud=n(Tt);$o=o(ud,"Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),ud.forEach(s),pa=m(e),Q=a(e,"DIV",{class:!0});var fn=n(Q);lt=a(fn,"IMG",{class:!0,src:!0,alt:!0}),ko=m(fn),ot=a(fn,"IMG",{class:!0,src:!0,alt:!0}),fn.forEach(s),fa=m(e),v(it.$$.fragment,e),ha=m(e),Mt=a(e,"P",{});var cd=n(Mt);wo=o(cd,"L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),cd.forEach(s),va=m(e),At=a(e,"P",{});var dd=n(At);yo=o(dd,"Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),dd.forEach(s),ga=m(e),It=a(e,"P",{});var md=n(It);Lo=o(md,"C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),md.forEach(s),Ea=m(e),W=a(e,"H2",{class:!0});var hn=n(W);ke=a(hn,"A",{id:!0,class:!0,href:!0});var pd=n(ke);Xs=a(pd,"SPAN",{});var fd=n(Xs);v(ut.$$.fragment,fd),fd.forEach(s),pd.forEach(s),To=m(hn),Qs=a(hn,"SPAN",{});var hd=n(Qs);Mo=o(hd,"L'apprentissage par transfert"),hd.forEach(s),hn.forEach(s),_a=m(e),v(ct.$$.fragment,e),qa=m(e),Ct=a(e,"P",{});var vd=n(Ct);Ao=o(vd,"Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),vd.forEach(s),ba=m(e),Z=a(e,"DIV",{class:!0});var vn=n(Z);dt=a(vn,"IMG",{class:!0,src:!0,alt:!0}),Io=m(vn),mt=a(vn,"IMG",{class:!0,src:!0,alt:!0}),vn.forEach(s),xa=m(e),Nt=a(e,"P",{});var gd=n(Nt);Co=o(gd,"Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),gd.forEach(s),Pa=m(e),A=a(e,"P",{});var es=n(A);No=o(es,"Le "),Ws=a(es,"EM",{});var Ed=n(Ws);Go=o(Ed,"finetuning"),Ed.forEach(s),Ro=o(es,", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),Zs=a(es,"EM",{});var _d=n(Zs);jo=o(_d,"finetuning"),_d.forEach(s),So=o(es,", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),es.forEach(s),$a=m(e),I=a(e,"UL",{});var ts=n(I);K=a(ts,"LI",{});var ss=n(K);zo=o(ss,"Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),Ks=a(ss,"EM",{});var qd=n(Ks);Bo=o(qd,"finetuning"),qd.forEach(s),Do=o(ss,". Le processus de "),er=a(ss,"EM",{});var bd=n(er);Oo=o(bd,"finetuning"),bd.forEach(s),Uo=o(ss," est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),ss.forEach(s),Ho=m(ts),pt=a(ts,"LI",{});var gn=n(pt);Vo=o(gn,"Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),tr=a(gn,"EM",{});var xd=n(tr);Yo=o(xd,"finetuning"),xd.forEach(s),Fo=o(gn," n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),gn.forEach(s),Jo=m(ts),sr=a(ts,"LI",{});var Pd=n(sr);Xo=o(Pd,"Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),Pd.forEach(s),ts.forEach(s),ka=m(e),C=a(e,"P",{});var rs=n(C);Qo=o(rs,"Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),rr=a(rs,"EM",{});var $d=n(rr);Wo=o($d,"finetuner"),$d.forEach(s),Zo=o(rs," sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),ar=a(rs,"EM",{});var kd=n(ar);Ko=o(kd,"finetuning"),kd.forEach(s),ei=o(rs," ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),rs.forEach(s),wa=m(e),ee=a(e,"DIV",{class:!0});var En=n(ee);ft=a(En,"IMG",{class:!0,src:!0,alt:!0}),ti=m(En),ht=a(En,"IMG",{class:!0,src:!0,alt:!0}),En.forEach(s),ya=m(e),N=a(e,"P",{});var as=n(N);si=o(as,"Le "),nr=a(as,"EM",{});var wd=n(nr);ri=o(wd,"finetuning"),wd.forEach(s),ai=o(as," d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),lr=a(as,"EM",{});var yd=n(lr);ni=o(yd,"finetuning"),yd.forEach(s),li=o(as," car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),as.forEach(s),La=m(e),we=a(e,"P",{});var _n=n(we);oi=o(_n,"Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),or=a(_n,"EM",{});var Ld=n(or);ii=o(Ld,"finetuner"),Ld.forEach(s),ui=o(_n,"."),_n.forEach(s),Ta=m(e),te=a(e,"H2",{class:!0});var qn=n(te);ye=a(qn,"A",{id:!0,class:!0,href:!0});var Td=n(ye);ir=a(Td,"SPAN",{});var Md=n(ir);v(vt.$$.fragment,Md),Md.forEach(s),Td.forEach(s),ci=m(qn),ur=a(qn,"SPAN",{});var Ad=n(ur);di=o(Ad,"Architecture g\xE9n\xE9rale"),Ad.forEach(s),qn.forEach(s),Ma=m(e),Le=a(e,"P",{});var bn=n(Le);mi=o(bn,"Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),cr=a(bn,"EM",{});var Id=n(cr);pi=o(Id,"transformers"),Id.forEach(s),fi=o(bn,". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),bn.forEach(s),Aa=m(e),v(gt.$$.fragment,e),Ia=m(e),se=a(e,"H2",{class:!0});var xn=n(se);Te=a(xn,"A",{id:!0,class:!0,href:!0});var Cd=n(Te);dr=a(Cd,"SPAN",{});var Nd=n(dr);v(Et.$$.fragment,Nd),Nd.forEach(s),Cd.forEach(s),hi=m(xn),mr=a(xn,"SPAN",{});var Gd=n(mr);vi=o(Gd,"Introduction"),Gd.forEach(s),xn.forEach(s),Ca=m(e),Gt=a(e,"P",{});var Rd=n(Gt);gi=o(Rd,"Le mod\xE8le est principalement compos\xE9 de deux blocs :"),Rd.forEach(s),Na=m(e),Me=a(e,"UL",{});var Pn=n(Me);Rt=a(Pn,"LI",{});var ju=n(Rt);pr=a(ju,"STRONG",{});var jd=n(pr);Ei=o(jd,"Encodeur (\xE0 gauche)"),jd.forEach(s),_i=o(ju," : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),ju.forEach(s),qi=m(Pn),jt=a(Pn,"LI",{});var Su=n(jt);fr=a(Su,"STRONG",{});var Sd=n(fr);bi=o(Sd,"D\xE9codeur (\xE0 droite)"),Sd.forEach(s),xi=o(Su," : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),Su.forEach(s),Pn.forEach(s),Ga=m(e),re=a(e,"DIV",{class:!0});var $n=n(re);_t=a($n,"IMG",{class:!0,src:!0,alt:!0}),Pi=m($n),qt=a($n,"IMG",{class:!0,src:!0,alt:!0}),$n.forEach(s),Ra=m(e),St=a(e,"P",{});var zd=n(St);$i=o(zd,"Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),zd.forEach(s),ja=m(e),G=a(e,"UL",{});var ns=n(G);zt=a(ns,"LI",{});var zu=n(zt);hr=a(zu,"STRONG",{});var Bd=n(hr);ki=o(Bd,"Mod\xE8les uniquement encodeurs"),Bd.forEach(s),wi=o(zu," : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),zu.forEach(s),yi=m(ns),Bt=a(ns,"LI",{});var Bu=n(Bt);vr=a(Bu,"STRONG",{});var Dd=n(vr);Li=o(Dd,"Mod\xE8les uniquement d\xE9codeurs"),Dd.forEach(s),Ti=o(Bu," : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),Bu.forEach(s),Mi=m(ns),Ae=a(ns,"LI",{});var Vr=n(Ae);gr=a(Vr,"STRONG",{});var Od=n(gr);Ai=o(Od,"Mod\xE8les encodeurs-d\xE9codeurs"),Od.forEach(s),Ii=o(Vr," (ou "),Er=a(Vr,"STRONG",{});var Ud=n(Er);Ci=o(Ud,"mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),Ud.forEach(s),Ni=o(Vr,") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),Vr.forEach(s),ns.forEach(s),Sa=m(e),Dt=a(e,"P",{});var Hd=n(Dt);Gi=o(Hd,"Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),Hd.forEach(s),za=m(e),ae=a(e,"H2",{class:!0});var kn=n(ae);Ie=a(kn,"A",{id:!0,class:!0,href:!0});var Vd=n(Ie);_r=a(Vd,"SPAN",{});var Yd=n(_r);v(bt.$$.fragment,Yd),Yd.forEach(s),Vd.forEach(s),Ri=m(kn),qr=a(kn,"SPAN",{});var Fd=n(qr);ji=o(Fd,"Les couches d'attention"),Fd.forEach(s),kn.forEach(s),Ba=m(e),$=a(e,"P",{});var Se=n($);Si=o(Se,"Une caract\xE9ristique cl\xE9 des "),br=a(Se,"EM",{});var Jd=n(br);zi=o(Jd,"transformers"),Jd.forEach(s),Bi=o(Se," est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),xr=a(Se,"EM",{});var Xd=n(xr);Di=o(Xd,"transformer"),Xd.forEach(s),Oi=o(Se," s\u2019e nome "),xt=a(Se,"A",{href:!0,rel:!0});var Qd=n(xt);Pr=a(Qd,"EM",{});var Wd=n(Pr);Ui=o(Wd,"Attention Is All You Need"),Wd.forEach(s),Qd.forEach(s),Hi=o(Se," ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),Se.forEach(s),Da=m(e),Ot=a(e,"P",{});var Zd=n(Ot);Vi=o(Zd,"Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB You like this course \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB You \xBB pour obtenir la traduction correcte du mot \xAB like \xBB, car en fran\xE7ais le verbe \xAB like \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB this \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB course  \xBB car \xAB this \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB this \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),Zd.forEach(s),Oa=m(e),Ut=a(e,"P",{});var Kd=n(Ut);Yi=o(Kd,"Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),Kd.forEach(s),Ua=m(e),Ce=a(e,"P",{});var wn=n(Ce);Fi=o(wn,"Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),$r=a(wn,"EM",{});var em=n($r);Ji=o(em,"transformers"),em.forEach(s),Xi=o(wn,"."),wn.forEach(s),Ha=m(e),ne=a(e,"H2",{class:!0});var yn=n(ne);Ne=a(yn,"A",{id:!0,class:!0,href:!0});var tm=n(Ne);kr=a(tm,"SPAN",{});var sm=n(kr);v(Pt.$$.fragment,sm),sm.forEach(s),tm.forEach(s),Qi=m(yn),wr=a(yn,"SPAN",{});var rm=n(wr);Wi=o(rm,"L'architecture originale"),rm.forEach(s),yn.forEach(s),Va=m(e),Ge=a(e,"P",{});var Ln=n(Ge);Zi=o(Ln,"L\u2019architecture du "),yr=a(Ln,"EM",{});var am=n(yr);Ki=o(am,"transformer"),am.forEach(s),eu=o(Ln," a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),Ln.forEach(s),Ya=m(e),Ht=a(e,"P",{});var nm=n(Ht);tu=o(nm,"Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),nm.forEach(s),Fa=m(e),Re=a(e,"P",{});var Tn=n(Re);su=o(Tn,"L\u2019architecture originale du "),Lr=a(Tn,"EM",{});var lm=n(Lr);ru=o(lm,"transformer"),lm.forEach(s),au=o(Tn," ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),Tn.forEach(s),Ja=m(e),le=a(e,"DIV",{class:!0});var Mn=n(le);$t=a(Mn,"IMG",{class:!0,src:!0,alt:!0}),nu=m(Mn),kt=a(Mn,"IMG",{class:!0,src:!0,alt:!0}),Mn.forEach(s),Xa=m(e),Vt=a(e,"P",{});var om=n(Vt);lu=o(om,"Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),om.forEach(s),Qa=m(e),R=a(e,"P",{});var ls=n(R);ou=o(ls,"Le "),Tr=a(ls,"EM",{});var im=n(Tr);iu=o(im,"masque d\u2019attention"),im.forEach(s),uu=o(ls," peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Mr=a(ls,"EM",{});var um=n(Mr);cu=o(um,"padding"),um.forEach(s),du=o(ls,") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),ls.forEach(s),Wa=m(e),oe=a(e,"H2",{class:!0});var An=n(oe);je=a(An,"A",{id:!0,class:!0,href:!0});var cm=n(je);Ar=a(cm,"SPAN",{});var dm=n(Ar);v(wt.$$.fragment,dm),dm.forEach(s),cm.forEach(s),mu=m(An),Ir=a(An,"SPAN",{});var mm=n(Ir);pu=o(mm,"Architectures contre *checkpoints*"),mm.forEach(s),An.forEach(s),Za=o(e,`

 
En approfondissant l'\xE9tude des *transformers* dans ce cours, vous verrez des mentions d'*architectures* et de *checkpoints* ainsi que de *mod\xE8les*. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),j=a(e,"UL",{});var os=n(j);Yt=a(os,"LI",{});var Du=n(Yt);Cr=a(Du,"STRONG",{});var pm=n(Cr);fu=o(pm,"Architecture"),pm.forEach(s),hu=o(Du," : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),Du.forEach(s),vu=m(os),Ft=a(os,"LI",{});var Ou=n(Ft);Nr=a(Ou,"STRONG",{});var fm=n(Nr);gu=o(fm,"Checkpoints"),fm.forEach(s),Eu=o(Ou," : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),Ou.forEach(s),_u=m(os),k=a(os,"LI",{});var de=n(k);Gr=a(de,"STRONG",{});var hm=n(Gr);qu=o(hm,"Mod\xE8le"),hm.forEach(s),bu=o(de," : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),Rr=a(de,"EM",{});var vm=n(Rr);xu=o(vm,"checkpoint"),vm.forEach(s),Pu=o(de," \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),jr=a(de,"EM",{});var gm=n(jr);$u=o(gm,"architecture"),gm.forEach(s),ku=o(de," ou "),Sr=a(de,"EM",{});var Em=n(Sr);wu=o(Em,"checkpoint"),Em.forEach(s),yu=o(de," lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),de.forEach(s),os.forEach(s),Ka=m(e),w=a(e,"P",{});var ze=n(w);Lu=o(ze,"Par exemple, BERT est une architecture alors que "),zr=a(ze,"CODE",{});var _m=n(zr);Tu=o(_m,"bert-base-cased"),_m.forEach(s),Mu=o(ze," (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),Br=a(ze,"EM",{});var qm=n(Br);Au=o(qm,"checkpoint"),qm.forEach(s),Iu=o(ze,". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),Dr=a(ze,"CODE",{});var bm=n(Dr);Cu=o(bm,"bert-base-cased"),bm.forEach(s),Nu=o(ze," \xBB."),ze.forEach(s),this.h()},h(){c(z,"name","hf:doc:metadata"),c(z,"content",JSON.stringify(Tm)),c(me,"id","comment-fonctionnent-les-transformers"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#comment-fonctionnent-les-transformers"),c(B,"class","relative group"),c(fe,"id","court-historique-des-transformers"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#court-historique-des-transformers"),c(D,"class","relative group"),c(Oe,"class","block dark:hidden"),p(Oe.src,Vu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(Oe,"src",Vu),c(Oe,"alt","A brief chronology of Transformers models."),c(Ue,"class","hidden dark:block"),p(Ue.src,Yu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(Ue,"src",Yu),c(Ue,"alt","A brief chronology of Transformers models."),c(O,"class","flex justify-center"),c(ve,"href","https://arxiv.org/abs/1706.03762"),c(ve,"rel","nofollow"),c(Ve,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Ve,"rel","nofollow"),c(Ye,"href","https://arxiv.org/abs/1810.04805"),c(Ye,"rel","nofollow"),c(Fe,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(Fe,"rel","nofollow"),c(Je,"href","https://arxiv.org/abs/1910.01108"),c(Je,"rel","nofollow"),c(Xe,"href","https://arxiv.org/abs/1910.13461"),c(Xe,"rel","nofollow"),c(Qe,"href","https://arxiv.org/abs/1910.10683"),c(Qe,"rel","nofollow"),c(We,"href","https://arxiv.org/abs/2005.14165"),c(We,"rel","nofollow"),c(be,"id","les-transformers-sont-des-modles-de-langage"),c(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(be,"href","#les-transformers-sont-des-modles-de-langage"),c(Y,"class","relative group"),c(Ke,"class","block dark:hidden"),p(Ke.src,Fu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(Ke,"src",Fu),c(Ke,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(et,"class","hidden dark:block"),p(et.src,Ju="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(et,"src",Ju),c(et,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(F,"class","flex justify-center"),c(tt,"class","block dark:hidden"),p(tt.src,Xu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(tt,"src",Xu),c(tt,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(st,"class","hidden dark:block"),p(st.src,Qu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(st,"src",Qu),c(st,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(J,"class","flex justify-center"),c($e,"id","les-transformers-sont-normes"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#les-transformers-sont-normes"),c(X,"class","relative group"),p(nt.src,Wu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(nt,"src",Wu),c(nt,"alt","Number of parameters of recent Transformers models"),c(nt,"width","90%"),c(at,"class","flex justify-center"),c(lt,"class","block dark:hidden"),p(lt.src,Zu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(lt,"src",Zu),c(lt,"alt","The carbon footprint of a large language model."),c(ot,"class","hidden dark:block"),p(ot.src,Ku="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(ot,"src",Ku),c(ot,"alt","The carbon footprint of a large language model."),c(Q,"class","flex justify-center"),c(ke,"id","lapprentissage-par-transfert"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#lapprentissage-par-transfert"),c(W,"class","relative group"),c(dt,"class","block dark:hidden"),p(dt.src,ec="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(dt,"src",ec),c(dt,"alt","The pretraining of a language model is costly in both time and money."),c(mt,"class","hidden dark:block"),p(mt.src,tc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(mt,"src",tc),c(mt,"alt","The pretraining of a language model is costly in both time and money."),c(Z,"class","flex justify-center"),c(ft,"class","block dark:hidden"),p(ft.src,sc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(ft,"src",sc),c(ft,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(ht,"class","hidden dark:block"),p(ht.src,rc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(ht,"src",rc),c(ht,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(ee,"class","flex justify-center"),c(ye,"id","architecture-gnrale"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#architecture-gnrale"),c(te,"class","relative group"),c(Te,"id","introduction"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#introduction"),c(se,"class","relative group"),c(_t,"class","block dark:hidden"),p(_t.src,ac="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(_t,"src",ac),c(_t,"alt","Architecture of a Transformers models"),c(qt,"class","hidden dark:block"),p(qt.src,nc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(qt,"src",nc),c(qt,"alt","Architecture of a Transformers models"),c(re,"class","flex justify-center"),c(Ie,"id","les-couches-dattention"),c(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ie,"href","#les-couches-dattention"),c(ae,"class","relative group"),c(xt,"href","https://arxiv.org/abs/1706.03762"),c(xt,"rel","nofollow"),c(Ne,"id","larchitecture-originale"),c(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ne,"href","#larchitecture-originale"),c(ne,"class","relative group"),c($t,"class","block dark:hidden"),p($t.src,lc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c($t,"src",lc),c($t,"alt","Architecture of a Transformers models"),c(kt,"class","hidden dark:block"),p(kt.src,oc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(kt,"src",oc),c(kt,"alt","Architecture of a Transformers models"),c(le,"class","flex justify-center"),c(je,"id","architectures-contre-checkpoints"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#architectures-contre-checkpoints"),c(oe,"class","relative group")},m(e,i){t(document.head,z),u(e,Yr,i),u(e,B,i),t(B,me),t(me,is),g(Be,is,null),t(B,In),t(B,us),t(us,Cn),u(e,Fr,i),u(e,pe,i),t(pe,Nn),t(pe,cs),t(cs,Gn),t(pe,Rn),u(e,Jr,i),u(e,D,i),t(D,fe),t(fe,ds),g(De,ds,null),t(D,jn),t(D,ms),t(ms,Sn),u(e,Xr,i),u(e,he,i),t(he,zn),t(he,ps),t(ps,Bn),t(he,Dn),u(e,Qr,i),u(e,O,i),t(O,Oe),t(O,On),t(O,Ue),u(e,Wr,i),u(e,He,i),t(He,ve),t(ve,Un),t(ve,fs),t(fs,Hn),t(He,Vn),u(e,Zr,i),u(e,f,i),t(f,hs),t(hs,b),t(b,vs),t(vs,Yn),t(b,Fn),t(b,Ve),t(Ve,Jn),t(b,Xn),t(b,gs),t(gs,Qn),t(b,Wn),t(b,Es),t(Es,Zn),t(b,Kn),t(f,el),t(f,_s),t(_s,ge),t(ge,qs),t(qs,tl),t(ge,sl),t(ge,Ye),t(Ye,rl),t(ge,al),t(f,nl),t(f,bs),t(bs,Ee),t(Ee,xs),t(xs,ll),t(Ee,ol),t(Ee,Fe),t(Fe,il),t(Ee,ul),t(f,cl),t(f,Ps),t(Ps,_e),t(_e,$s),t($s,dl),t(_e,ml),t(_e,Je),t(Je,pl),t(_e,fl),t(f,hl),t(f,ks),t(ks,x),t(x,ws),t(ws,vl),t(x,gl),t(x,Xe),t(Xe,El),t(x,_l),t(x,Qe),t(Qe,ql),t(x,bl),t(x,ys),t(ys,xl),t(x,Pl),t(f,$l),t(f,Ls),t(Ls,P),t(P,Ts),t(Ts,kl),t(P,wl),t(P,We),t(We,yl),t(P,Ll),t(P,Ms),t(Ms,Tl),t(P,Ml),t(P,As),t(As,Al),t(P,Il),u(e,Kr,i),u(e,qe,i),t(qe,Cl),t(qe,Is),t(Is,Nl),t(qe,Gl),u(e,ea,i),u(e,L,i),t(L,U),t(U,Rl),t(U,Cs),t(Cs,jl),t(U,Sl),t(U,Ns),t(Ns,zl),t(U,Bl),t(L,Dl),t(L,H),t(H,Ol),t(H,Gs),t(Gs,Ul),t(H,Hl),t(H,Rs),t(Rs,Vl),t(H,Yl),t(L,Fl),t(L,V),t(V,Jl),t(V,js),t(js,Xl),t(V,Ql),t(V,Ss),t(Ss,Wl),t(V,Zl),u(e,ta,i),u(e,yt,i),t(yt,Kl),u(e,sa,i),u(e,Y,i),t(Y,be),t(be,zs),g(Ze,zs,null),t(Y,eo),t(Y,Bs),t(Bs,to),u(e,ra,i),u(e,T,i),t(T,so),t(T,Ds),t(Ds,ro),t(T,ao),t(T,Os),t(Os,no),t(T,lo),u(e,aa,i),u(e,xe,i),t(xe,oo),t(xe,Us),t(Us,io),t(xe,uo),u(e,na,i),u(e,M,i),t(M,co),t(M,Hs),t(Hs,mo),t(M,po),t(M,Vs),t(Vs,fo),t(M,ho),u(e,la,i),u(e,F,i),t(F,Ke),t(F,vo),t(F,et),u(e,oa,i),u(e,Pe,i),t(Pe,go),t(Pe,Ys),t(Ys,Eo),t(Pe,_o),u(e,ia,i),u(e,J,i),t(J,tt),t(J,qo),t(J,st),u(e,ua,i),u(e,X,i),t(X,$e),t($e,Fs),g(rt,Fs,null),t(X,bo),t(X,Js),t(Js,xo),u(e,ca,i),u(e,Lt,i),t(Lt,Po),u(e,da,i),u(e,at,i),t(at,nt),u(e,ma,i),u(e,Tt,i),t(Tt,$o),u(e,pa,i),u(e,Q,i),t(Q,lt),t(Q,ko),t(Q,ot),u(e,fa,i),g(it,e,i),u(e,ha,i),u(e,Mt,i),t(Mt,wo),u(e,va,i),u(e,At,i),t(At,yo),u(e,ga,i),u(e,It,i),t(It,Lo),u(e,Ea,i),u(e,W,i),t(W,ke),t(ke,Xs),g(ut,Xs,null),t(W,To),t(W,Qs),t(Qs,Mo),u(e,_a,i),g(ct,e,i),u(e,qa,i),u(e,Ct,i),t(Ct,Ao),u(e,ba,i),u(e,Z,i),t(Z,dt),t(Z,Io),t(Z,mt),u(e,xa,i),u(e,Nt,i),t(Nt,Co),u(e,Pa,i),u(e,A,i),t(A,No),t(A,Ws),t(Ws,Go),t(A,Ro),t(A,Zs),t(Zs,jo),t(A,So),u(e,$a,i),u(e,I,i),t(I,K),t(K,zo),t(K,Ks),t(Ks,Bo),t(K,Do),t(K,er),t(er,Oo),t(K,Uo),t(I,Ho),t(I,pt),t(pt,Vo),t(pt,tr),t(tr,Yo),t(pt,Fo),t(I,Jo),t(I,sr),t(sr,Xo),u(e,ka,i),u(e,C,i),t(C,Qo),t(C,rr),t(rr,Wo),t(C,Zo),t(C,ar),t(ar,Ko),t(C,ei),u(e,wa,i),u(e,ee,i),t(ee,ft),t(ee,ti),t(ee,ht),u(e,ya,i),u(e,N,i),t(N,si),t(N,nr),t(nr,ri),t(N,ai),t(N,lr),t(lr,ni),t(N,li),u(e,La,i),u(e,we,i),t(we,oi),t(we,or),t(or,ii),t(we,ui),u(e,Ta,i),u(e,te,i),t(te,ye),t(ye,ir),g(vt,ir,null),t(te,ci),t(te,ur),t(ur,di),u(e,Ma,i),u(e,Le,i),t(Le,mi),t(Le,cr),t(cr,pi),t(Le,fi),u(e,Aa,i),g(gt,e,i),u(e,Ia,i),u(e,se,i),t(se,Te),t(Te,dr),g(Et,dr,null),t(se,hi),t(se,mr),t(mr,vi),u(e,Ca,i),u(e,Gt,i),t(Gt,gi),u(e,Na,i),u(e,Me,i),t(Me,Rt),t(Rt,pr),t(pr,Ei),t(Rt,_i),t(Me,qi),t(Me,jt),t(jt,fr),t(fr,bi),t(jt,xi),u(e,Ga,i),u(e,re,i),t(re,_t),t(re,Pi),t(re,qt),u(e,Ra,i),u(e,St,i),t(St,$i),u(e,ja,i),u(e,G,i),t(G,zt),t(zt,hr),t(hr,ki),t(zt,wi),t(G,yi),t(G,Bt),t(Bt,vr),t(vr,Li),t(Bt,Ti),t(G,Mi),t(G,Ae),t(Ae,gr),t(gr,Ai),t(Ae,Ii),t(Ae,Er),t(Er,Ci),t(Ae,Ni),u(e,Sa,i),u(e,Dt,i),t(Dt,Gi),u(e,za,i),u(e,ae,i),t(ae,Ie),t(Ie,_r),g(bt,_r,null),t(ae,Ri),t(ae,qr),t(qr,ji),u(e,Ba,i),u(e,$,i),t($,Si),t($,br),t(br,zi),t($,Bi),t($,xr),t(xr,Di),t($,Oi),t($,xt),t(xt,Pr),t(Pr,Ui),t($,Hi),u(e,Da,i),u(e,Ot,i),t(Ot,Vi),u(e,Oa,i),u(e,Ut,i),t(Ut,Yi),u(e,Ua,i),u(e,Ce,i),t(Ce,Fi),t(Ce,$r),t($r,Ji),t(Ce,Xi),u(e,Ha,i),u(e,ne,i),t(ne,Ne),t(Ne,kr),g(Pt,kr,null),t(ne,Qi),t(ne,wr),t(wr,Wi),u(e,Va,i),u(e,Ge,i),t(Ge,Zi),t(Ge,yr),t(yr,Ki),t(Ge,eu),u(e,Ya,i),u(e,Ht,i),t(Ht,tu),u(e,Fa,i),u(e,Re,i),t(Re,su),t(Re,Lr),t(Lr,ru),t(Re,au),u(e,Ja,i),u(e,le,i),t(le,$t),t(le,nu),t(le,kt),u(e,Xa,i),u(e,Vt,i),t(Vt,lu),u(e,Qa,i),u(e,R,i),t(R,ou),t(R,Tr),t(Tr,iu),t(R,uu),t(R,Mr),t(Mr,cu),t(R,du),u(e,Wa,i),u(e,oe,i),t(oe,je),t(je,Ar),g(wt,Ar,null),t(oe,mu),t(oe,Ir),t(Ir,pu),u(e,Za,i),u(e,j,i),t(j,Yt),t(Yt,Cr),t(Cr,fu),t(Yt,hu),t(j,vu),t(j,Ft),t(Ft,Nr),t(Nr,gu),t(Ft,Eu),t(j,_u),t(j,k),t(k,Gr),t(Gr,qu),t(k,bu),t(k,Rr),t(Rr,xu),t(k,Pu),t(k,jr),t(jr,$u),t(k,ku),t(k,Sr),t(Sr,wu),t(k,yu),u(e,Ka,i),u(e,w,i),t(w,Lu),t(w,zr),t(zr,Tu),t(w,Mu),t(w,Br),t(Br,Au),t(w,Iu),t(w,Dr),t(Dr,Cu),t(w,Nu),en=!0},p:wm,i(e){en||(E(Be.$$.fragment,e),E(De.$$.fragment,e),E(Ze.$$.fragment,e),E(rt.$$.fragment,e),E(it.$$.fragment,e),E(ut.$$.fragment,e),E(ct.$$.fragment,e),E(vt.$$.fragment,e),E(gt.$$.fragment,e),E(Et.$$.fragment,e),E(bt.$$.fragment,e),E(Pt.$$.fragment,e),E(wt.$$.fragment,e),en=!0)},o(e){_(Be.$$.fragment,e),_(De.$$.fragment,e),_(Ze.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(ut.$$.fragment,e),_(ct.$$.fragment,e),_(vt.$$.fragment,e),_(gt.$$.fragment,e),_(Et.$$.fragment,e),_(bt.$$.fragment,e),_(Pt.$$.fragment,e),_(wt.$$.fragment,e),en=!1},d(e){s(z),e&&s(Yr),e&&s(B),q(Be),e&&s(Fr),e&&s(pe),e&&s(Jr),e&&s(D),q(De),e&&s(Xr),e&&s(he),e&&s(Qr),e&&s(O),e&&s(Wr),e&&s(He),e&&s(Zr),e&&s(f),e&&s(Kr),e&&s(qe),e&&s(ea),e&&s(L),e&&s(ta),e&&s(yt),e&&s(sa),e&&s(Y),q(Ze),e&&s(ra),e&&s(T),e&&s(aa),e&&s(xe),e&&s(na),e&&s(M),e&&s(la),e&&s(F),e&&s(oa),e&&s(Pe),e&&s(ia),e&&s(J),e&&s(ua),e&&s(X),q(rt),e&&s(ca),e&&s(Lt),e&&s(da),e&&s(at),e&&s(ma),e&&s(Tt),e&&s(pa),e&&s(Q),e&&s(fa),q(it,e),e&&s(ha),e&&s(Mt),e&&s(va),e&&s(At),e&&s(ga),e&&s(It),e&&s(Ea),e&&s(W),q(ut),e&&s(_a),q(ct,e),e&&s(qa),e&&s(Ct),e&&s(ba),e&&s(Z),e&&s(xa),e&&s(Nt),e&&s(Pa),e&&s(A),e&&s($a),e&&s(I),e&&s(ka),e&&s(C),e&&s(wa),e&&s(ee),e&&s(ya),e&&s(N),e&&s(La),e&&s(we),e&&s(Ta),e&&s(te),q(vt),e&&s(Ma),e&&s(Le),e&&s(Aa),q(gt,e),e&&s(Ia),e&&s(se),q(Et),e&&s(Ca),e&&s(Gt),e&&s(Na),e&&s(Me),e&&s(Ga),e&&s(re),e&&s(Ra),e&&s(St),e&&s(ja),e&&s(G),e&&s(Sa),e&&s(Dt),e&&s(za),e&&s(ae),q(bt),e&&s(Ba),e&&s($),e&&s(Da),e&&s(Ot),e&&s(Oa),e&&s(Ut),e&&s(Ua),e&&s(Ce),e&&s(Ha),e&&s(ne),q(Pt),e&&s(Va),e&&s(Ge),e&&s(Ya),e&&s(Ht),e&&s(Fa),e&&s(Re),e&&s(Ja),e&&s(le),e&&s(Xa),e&&s(Vt),e&&s(Qa),e&&s(R),e&&s(Wa),e&&s(oe),q(wt),e&&s(Za),e&&s(j),e&&s(Ka),e&&s(w)}}}const Tm={local:"comment-fonctionnent-les-transformers",sections:[{local:"court-historique-des-transformers",title:"Court historique des *transformers*"},{local:"les-transformers-sont-des-modles-de-langage",title:"Les *transformers* sont des mod\xE8les de langage"},{local:"les-transformers-sont-normes",title:"Les *transformers* sont \xE9normes"},{local:"lapprentissage-par-transfert",title:"L'apprentissage par transfert"},{local:"architecture-gnrale",title:"Architecture g\xE9n\xE9rale"},{local:"introduction",title:"Introduction"},{local:"les-couches-dattention",title:"Les couches d'attention"},{local:"larchitecture-originale",title:"L'architecture originale"},{local:"architectures-contre-checkpoints",title:"Architectures contre *checkpoints*"}],title:"Comment fonctionnent les *transformers* ?"};function Mm(Hu){return ym(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nm extends xm{constructor(z){super();Pm(this,z,Mm,Lm,$m,{})}}export{Nm as default,Tm as metadata};
