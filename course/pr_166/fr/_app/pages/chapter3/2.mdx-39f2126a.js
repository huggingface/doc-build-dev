import{S as Ld,i as Fd,s as Rd,e as o,t as n,k as m,w as x,c as l,a as r,h as a,d as t,m as f,x as z,g as c,F as s,y as w,q,o as j,B as y,l as Ad,M as Hd,b as D,p as Ws,v as Id,n as Us}from"../../chunks/vendor-1e8b365d.js";import{T as fp}from"../../chunks/Tip-62b14c6e.js";import{Y as va}from"../../chunks/Youtube-c2a8cc39.js";import{I as To}from"../../chunks/IconCopyLink-483c28ba.js";import{C as M}from"../../chunks/CodeBlock-e5764662.js";import{D as Od}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Vd}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Wd(C){let i,d;return i=new Od({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Ud(C){let i,d;return i=new Od({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Bd(C){let i,d,u,_,g,v,b,$;return b=new M({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# M\xEAme chose que pr\xE9c\xE9demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Ceci est nouveau
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># M\xEAme chose que pr\xE9c\xE9demment</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Ce cours est incroyable !</span>
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># Ceci est nouveau</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){i=o("p"),d=n("En continuant avec l\u2019exemple du "),u=o("a"),_=n("chapitre pr\xE9c\xE9dent"),g=n(", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec TensorFlow :"),v=m(),x(b.$$.fragment),this.h()},l(h){i=l(h,"P",{});var k=r(i);d=a(k,"En continuant avec l\u2019exemple du "),u=l(k,"A",{href:!0});var S=r(u);_=a(S,"chapitre pr\xE9c\xE9dent"),S.forEach(t),g=a(k,", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec TensorFlow :"),k.forEach(t),v=f(h),z(b.$$.fragment,h),this.h()},h(){D(u,"href","/course/fr/chapter2")},m(h,k){c(h,i,k),s(i,d),s(i,u),s(u,_),s(i,g),c(h,v,k),w(b,h,k),$=!0},i(h){$||(q(b.$$.fragment,h),$=!0)},o(h){j(b.$$.fragment,h),$=!1},d(h){h&&t(i),h&&t(v),y(b,h)}}}function Gd(C){let i,d,u,_,g,v,b,$;return b=new M({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# M\xEAme chose que pr\xE9c\xE9demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Ceci est nouveau
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># M\xEAme chose que pr\xE9c\xE9demment</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Ce cours est incroyable !</span>
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Ceci est nouveau</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){i=o("p"),d=n("En continuant avec l\u2019exemple du "),u=o("a"),_=n("chapitre pr\xE9c\xE9dent"),g=n(", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec PyTorch :"),v=m(),x(b.$$.fragment),this.h()},l(h){i=l(h,"P",{});var k=r(i);d=a(k,"En continuant avec l\u2019exemple du "),u=l(k,"A",{href:!0});var S=r(u);_=a(S,"chapitre pr\xE9c\xE9dent"),S.forEach(t),g=a(k,", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec PyTorch :"),k.forEach(t),v=f(h),z(b.$$.fragment,h),this.h()},h(){D(u,"href","/course/fr/chapter2")},m(h,k){c(h,i,k),s(i,d),s(i,u),s(u,_),s(i,g),c(h,v,k),w(b,h,k),$=!0},i(h){$||(q(b.$$.fragment,h),$=!0)},o(h){j(b.$$.fragment,h),$=!1},d(h){h&&t(i),h&&t(v),y(b,h)}}}function Jd(C){let i,d;return i=new va({props:{id:"W_gMJF0xomE"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Qd(C){let i,d;return i=new va({props:{id:"_BZearw7f0w"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Kd(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Regardez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et l\u2019\xE9l\xE9ment 87 de l\u2019ensemble de validation. Quelles sont leurs \xE9tiquettes ?")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Regardez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et l\u2019\xE9l\xE9ment 87 de l\u2019ensemble de validation. Quelles sont leurs \xE9tiquettes ?"),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function Yd(C){let i,d;return i=new va({props:{id:"P-rZWqcB6CE"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Zd(C){let i,d;return i=new va({props:{id:"0u3ioSwev3s"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Xd(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Prenez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et tokenisez les deux phrases s\xE9par\xE9ment et par paire. Quelle est la diff\xE9rence entre les deux r\xE9sultats ?")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Prenez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et tokenisez les deux phrases s\xE9par\xE9ment et par paire. Quelle est la diff\xE9rence entre les deux r\xE9sultats ?"),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function em(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R;return{c(){i=o("p"),d=n("La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=o("em"),_=n("fonction de rassemblement"),g=n(". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=o("code"),b=n("DataLoader"),$=n(", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en type tf.Tensor et les concat\xE9ner (r\xE9cursivement si les \xE9l\xE9ments sont des listes, des "),h=o("em"),k=n("tuples"),S=n(" ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=o("em"),O=n("padding"),L=n(", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=o("em"),F=n("padding"),R=n(" suppl\xE9mentaire.")},l(H){i=l(H,"P",{});var E=r(i);d=a(E,"La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=l(E,"EM",{});var ae=r(u);_=a(ae,"fonction de rassemblement"),ae.forEach(t),g=a(E,". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=l(E,"CODE",{});var Q=r(v);b=a(Q,"DataLoader"),Q.forEach(t),$=a(E,", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en type tf.Tensor et les concat\xE9ner (r\xE9cursivement si les \xE9l\xE9ments sont des listes, des "),h=l(E,"EM",{});var W=r(h);k=a(W,"tuples"),W.forEach(t),S=a(E," ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=l(E,"EM",{});var oe=r(I);O=a(oe,"padding"),oe.forEach(t),L=a(E,", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=l(E,"EM",{});var P=r(A);F=a(P,"padding"),P.forEach(t),R=a(E," suppl\xE9mentaire."),E.forEach(t)},m(H,E){c(H,i,E),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R)},d(H){H&&t(i)}}}function sm(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R;return{c(){i=o("p"),d=n("La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=o("em"),_=n("fonction de rassemblement"),g=n(". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=o("code"),b=n("DataLoader"),$=n(", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en tenseurs PyTorch et les concat\xE9ner (r\xE9cursivement si vos \xE9l\xE9ments sont des listes, des "),h=o("em"),k=n("tuples"),S=n(" ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=o("em"),O=n("padding"),L=n(", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=o("em"),F=n("padding"),R=n(" suppl\xE9mentaire.")},l(H){i=l(H,"P",{});var E=r(i);d=a(E,"La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=l(E,"EM",{});var ae=r(u);_=a(ae,"fonction de rassemblement"),ae.forEach(t),g=a(E,". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=l(E,"CODE",{});var Q=r(v);b=a(Q,"DataLoader"),Q.forEach(t),$=a(E,", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en tenseurs PyTorch et les concat\xE9ner (r\xE9cursivement si vos \xE9l\xE9ments sont des listes, des "),h=l(E,"EM",{});var W=r(h);k=a(W,"tuples"),W.forEach(t),S=a(E," ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=l(E,"EM",{});var oe=r(I);O=a(oe,"padding"),oe.forEach(t),L=a(E,", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=l(E,"EM",{});var P=r(A);F=a(P,"padding"),P.forEach(t),R=a(E," suppl\xE9mentaire."),E.forEach(t)},m(H,E){c(H,i,E),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R)},d(H){H&&t(i)}}}function tm(C){let i,d;return i=new M({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function nm(C){let i,d;return i=new M({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function am(C){let i,d,u,_,g,v,b,$;return i=new M({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){x(i.$$.fragment),d=m(),u=o("p"),_=n("C\u2019est beau ! Maintenant que nous sommes pass\xE9s du texte brut \xE0 des batchs que notre mod\xE8le peut traiter, nous sommes pr\xEAts \xE0 le "),g=o("em"),v=n("finetuner"),b=n(" !")},l(h){z(i.$$.fragment,h),d=f(h),u=l(h,"P",{});var k=r(u);_=a(k,"C\u2019est beau ! Maintenant que nous sommes pass\xE9s du texte brut \xE0 des batchs que notre mod\xE8le peut traiter, nous sommes pr\xEAts \xE0 le "),g=l(k,"EM",{});var S=r(g);v=a(S,"finetuner"),S.forEach(t),b=a(k," !"),k.forEach(t)},m(h,k){w(i,h,k),c(h,d,k),c(h,u,k),s(u,_),s(u,g),s(g,v),s(u,b),$=!0},i(h){$||(q(i.$$.fragment,h),$=!0)},o(h){j(i.$$.fragment,h),$=!1},d(h){y(i,h),h&&t(d),h&&t(u)}}}function om(C){let i,d;return i=new M({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function lm(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Reproduisez le pr\xE9traitement sur le jeu de donn\xE9es GLUE SST-2. C\u2019est un peu diff\xE9rent puisqu\u2019il est compos\xE9 de phrases simples au lieu de paires, mais le reste de ce que nous avons fait devrait \xEAtre identique. Pour un d\xE9fi plus difficile, essayez d\u2019\xE9crire une fonction de pr\xE9traitement qui fonctionne sur toutes les t\xE2ches GLUE.")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Reproduisez le pr\xE9traitement sur le jeu de donn\xE9es GLUE SST-2. C\u2019est un peu diff\xE9rent puisqu\u2019il est compos\xE9 de phrases simples au lieu de paires, mais le reste de ce que nous avons fait devrait \xEAtre identique. Pour un d\xE9fi plus difficile, essayez d\u2019\xE9crire une fonction de pr\xE9traitement qui fonctionne sur toutes les t\xE2ches GLUE."),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function Nd(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R,H,E,ae,Q,W,oe;return E=new M({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){i=o("p"),d=n("Maintenant que nous disposons de notre jeu de donn\xE9es et d\u2019un collecteur de donn\xE9es, nous devons les assembler. Nous pourrions charger manuellement des batchs et les assembler mais c\u2019est beaucoup de travail et probablement pas tr\xE8s performant non plus. A la place, il existe une m\xE9thode simple qui offre une solution performante \xE0 ce probl\xE8me : "),u=o("code"),_=n("to_tf_dataset()"),g=n(". Cela va envelopper un "),v=o("code"),b=n("tf.data.Dataset"),$=n(" autour de votre jeu de donn\xE9es, avec une fonction de collation optionnelle. "),h=o("code"),k=n("tf.data.Dataset"),S=n(" est un format natif de TensorFlow que Keras peut utiliser pour "),I=o("code"),O=n("model.fit()"),L=n(", donc cette seule m\xE9thode convertit imm\xE9diatement un "),A=o("em"),F=n("dataset"),R=n(" en un format pr\xEAt pour l\u2019entra\xEEnement. Voyons cela en action avec notre jeu de donn\xE9es !"),H=m(),x(E.$$.fragment),ae=m(),Q=o("p"),W=n("Et c\u2019est tout ! Nous pouvons utiliser ces jeux de donn\xE9es dans le prochain cours, o\xF9 l\u2019entra\xEEnement sera agr\xE9ablement simple apr\xE8s tout le dur travail de pr\xE9traitement des donn\xE9es.")},l(P){i=l(P,"P",{});var V=r(i);d=a(V,"Maintenant que nous disposons de notre jeu de donn\xE9es et d\u2019un collecteur de donn\xE9es, nous devons les assembler. Nous pourrions charger manuellement des batchs et les assembler mais c\u2019est beaucoup de travail et probablement pas tr\xE8s performant non plus. A la place, il existe une m\xE9thode simple qui offre une solution performante \xE0 ce probl\xE8me : "),u=l(V,"CODE",{});var Bs=r(u);_=a(Bs,"to_tf_dataset()"),Bs.forEach(t),g=a(V,". Cela va envelopper un "),v=l(V,"CODE",{});var Ce=r(v);b=a(Ce,"tf.data.Dataset"),Ce.forEach(t),$=a(V," autour de votre jeu de donn\xE9es, avec une fonction de collation optionnelle. "),h=l(V,"CODE",{});var Gs=r(h);k=a(Gs,"tf.data.Dataset"),Gs.forEach(t),S=a(V," est un format natif de TensorFlow que Keras peut utiliser pour "),I=l(V,"CODE",{});var Js=r(I);O=a(Js,"model.fit()"),Js.forEach(t),L=a(V,", donc cette seule m\xE9thode convertit imm\xE9diatement un "),A=l(V,"EM",{});var us=r(A);F=a(us,"dataset"),us.forEach(t),R=a(V," en un format pr\xEAt pour l\u2019entra\xEEnement. Voyons cela en action avec notre jeu de donn\xE9es !"),V.forEach(t),H=f(P),z(E.$$.fragment,P),ae=f(P),Q=l(P,"P",{});var pe=r(Q);W=a(pe,"Et c\u2019est tout ! Nous pouvons utiliser ces jeux de donn\xE9es dans le prochain cours, o\xF9 l\u2019entra\xEEnement sera agr\xE9ablement simple apr\xE8s tout le dur travail de pr\xE9traitement des donn\xE9es."),pe.forEach(t)},m(P,V){c(P,i,V),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R),c(P,H,V),w(E,P,V),c(P,ae,V),c(P,Q,V),s(Q,W),oe=!0},i(P){oe||(q(E.$$.fragment,P),oe=!0)},o(P){j(E.$$.fragment,P),oe=!1},d(P){P&&t(i),P&&t(H),y(E,P),P&&t(ae),P&&t(Q)}}}function rm(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R,H,E,ae,Q,W,oe,P,V,Bs,Ce,Gs,Js,us,pe,We,bt,ps,Mo,qt,So,ba,ge,ke,Qs,K,Ao,jt,No,Oo,cs,Lo,Fo,ds,Ro,Ho,Ue,gt,Io,Vo,Wo,kt,Uo,Bo,qa,De,Go,Et,Jo,Qo,$t,Ko,Yo,ja,ms,ga,fs,ka,Y,Zo,xt,Xo,el,zt,sl,tl,wt,nl,al,yt,ol,ll,Ct,rl,il,Ea,Pe,ul,Dt,pl,cl,Pt,dl,ml,$a,Be,fl,Tt,hl,_l,xa,hs,za,_s,wa,Te,vl,Mt,bl,ql,St,jl,gl,ya,vs,Ca,bs,Da,U,kl,At,El,$l,Nt,xl,zl,Ot,wl,yl,Lt,Cl,Dl,Ft,Pl,Tl,Rt,Ml,Sl,Ht,Al,Nl,Pa,Ge,Ta,He,Je,It,qs,Ol,Vt,Ll,Ma,Ee,$e,Ks,ce,Fl,Ys,Rl,Hl,Wt,Il,Vl,Ut,Wl,Ul,Sa,js,Aa,Qe,Bl,Bt,Gl,Jl,Na,gs,Oa,ks,La,le,Ql,Gt,Kl,Yl,Jt,Zl,Xl,Zs,er,sr,Qt,tr,nr,Fa,Ke,Ra,Ye,ar,Kt,or,lr,Ha,Es,Ia,Xs,rr,Va,$s,Wa,Me,ir,Yt,ur,pr,Zt,cr,dr,Ua,xs,Ba,J,mr,Xt,fr,hr,en,_r,vr,sn,br,qr,tn,jr,gr,nn,kr,Er,an,$r,xr,Ga,Se,zr,on,wr,yr,ln,Cr,Dr,Ja,de,Pr,rn,Tr,Mr,et,Sr,Ar,un,Nr,Or,Qa,Ze,Lr,pn,Fr,Rr,Ka,re,Hr,cn,Ir,Vr,dn,Wr,Ur,mn,Br,Gr,fn,Jr,Qr,Ya,ie,Kr,hn,Yr,Zr,st,Xr,ei,_n,si,ti,tt,ni,ai,Za,zs,Xa,Z,oi,vn,li,ri,bn,ii,ui,qn,pi,ci,jn,di,mi,ws,fi,hi,eo,Ae,_i,ys,gn,vi,bi,kn,qi,ji,so,Cs,to,T,gi,En,ki,Ei,$n,$i,xi,xn,zi,wi,zn,yi,Ci,wn,Di,Pi,yn,Ti,Mi,Cn,Si,Ai,Dn,Ni,Oi,Pn,Li,Fi,Xe,Ri,Tn,Hi,Ii,Mn,Vi,Wi,no,me,Ui,Sn,Bi,Gi,An,Ji,Qi,Nn,Ki,Yi,ao,Ne,Zi,On,Xi,eu,Ln,su,tu,oo,Ds,lo,es,nu,Fn,au,ou,ro,Ps,io,X,lu,Rn,ru,iu,Hn,uu,pu,In,cu,du,Vn,mu,fu,Wn,hu,_u,uo,ee,vu,Un,bu,qu,Bn,ju,gu,Gn,ku,Eu,Jn,$u,xu,Qn,zu,wu,po,ss,yu,Kn,Cu,Du,co,Ie,ts,Yn,Ts,Pu,Zn,Tu,mo,Ms,fo,nt,B,Mu,Xn,Su,Au,ea,Nu,Ou,sa,Lu,Fu,ta,Ru,Hu,na,Iu,Vu,aa,Wu,Uu,oa,Bu,Gu,ho,xe,ze,at,se,Ju,la,Qu,Ku,ra,Yu,Zu,ia,Xu,ep,ua,sp,tp,pa,np,ap,_o,Ss,vo,As,bo,fe,op,ca,lp,rp,da,ip,up,ma,pp,cp,qo,Ns,jo,we,ye,ot,ns,go,lt,ko;u=new Vd({props:{fw:C[0]}}),$=new To({});const hp=[Ud,Wd],Os=[];function _p(e,p){return e[0]==="pt"?0:1}O=_p(C),L=Os[O]=hp[O](C);const vp=[Gd,Bd],Ls=[];function bp(e,p){return e[0]==="pt"?0:1}F=bp(C),R=Ls[F]=vp[F](C),ps=new To({});const qp=[Qd,Jd],Fs=[];function jp(e,p){return e[0]==="pt"?0:1}ge=jp(C),ke=Fs[ge]=qp[ge](C),ms=new M({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),fs=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),hs=new M({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),_s=new M({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', # Amrozi a accus\xE9 son fr\xE8re, qu'il a appel\xE9 \xAB le t\xE9moin \xBB, de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'} # Se r\xE9f\xE9rant \xE0 lui uniquement comme \xAB le t\xE9moin \xBB, Amrozi a accus\xE9 son fr\xE8re de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>, <span class="hljs-comment"># Amrozi a accus\xE9 son fr\xE8re, qu&#x27;il a appel\xE9 \xAB le t\xE9moin \xBB, de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.</span>
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>} <span class="hljs-comment"># Se r\xE9f\xE9rant \xE0 lui uniquement comme \xAB le t\xE9moin \xBB, Amrozi a accus\xE9 son fr\xE8re de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.</span>`}}),vs=new M({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),bs=new M({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Ge=new fp({props:{$$slots:{default:[Kd]},$$scope:{ctx:C}}}),qs=new To({});const gp=[Zd,Yd],Rs=[];function kp(e,p){return e[0]==="pt"?0:1}Ee=kp(C),$e=Rs[Ee]=gp[Ee](C),js=new M({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),gs=new M({props:{code:`inputs = tokenizer(
    "This is the first sentence.", "This is the second one."
)  # "C'est la premi\xE8re phrase.", "C'est la deuxi\xE8me."
inputs`,highlighted:`inputs = tokenizer(
    <span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>
)  <span class="hljs-comment"># &quot;C&#x27;est la premi\xE8re phrase.&quot;, &quot;C&#x27;est la deuxi\xE8me.&quot;</span>
inputs`}}),ks=new M({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Ke=new fp({props:{$$slots:{default:[Xd]},$$scope:{ctx:C}}}),Es=new M({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),$s=new M({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),xs=new M({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),zs=new M({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),Cs=new M({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),Ds=new M({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),Ps=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),Ts=new To({}),Ms=new va({props:{id:"7q5NyFT8REg"}});function Ep(e,p){return e[0]==="pt"?sm:em}let Eo=Ep(C),Ve=Eo(C);const $p=[nm,tm],Hs=[];function xp(e,p){return e[0]==="pt"?0:1}xe=xp(C),ze=Hs[xe]=$p[xe](C),Ss=new M({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),As=new M({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),Ns=new M({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const zp=[om,am],Is=[];function wp(e,p){return e[0]==="tf"?0:1}we=wp(C),ye=Is[we]=zp[we](C),ns=new fp({props:{$$slots:{default:[lm]},$$scope:{ctx:C}}});let G=C[0]==="tf"&&Nd();return{c(){i=o("meta"),d=m(),x(u.$$.fragment),_=m(),g=o("h1"),v=o("a"),b=o("span"),x($.$$.fragment),h=m(),k=o("span"),S=n("Pr\xE9parer les donn\xE9es"),I=m(),L.c(),A=m(),R.c(),H=m(),E=o("p"),ae=n("Evidemment, entra\xEEner un mod\xE8le avec seulement deux phrases ne va pas donner de bons r\xE9sultats. Pour obtenir de meilleurs r\xE9sultats, vous allez avoir \xE0 pr\xE9parer un plus grand jeu de donn\xE9es."),Q=m(),W=o("p"),oe=n("Dans cette section, nous allons utiliser comme exemple le jeu de donn\xE9es MRPC ("),P=o("em"),V=n("Microsoft Research Paraphrase Corpus"),Bs=n(") pr\xE9sent\xE9 dans un "),Ce=o("a"),Gs=n("papier"),Js=n(" par William B. Dolan et Chris Brockett. Ce jeu de donn\xE9es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m\xEAme signification). Nous l\u2019avons choisi pour ce chapitre parce que c\u2019est un petit jeu de donn\xE9es et cela rend donc simples les exp\xE9riences d\u2019entra\xEEnement sur ce jeu de donn\xE9es."),us=m(),pe=o("h3"),We=o("a"),bt=o("span"),x(ps.$$.fragment),Mo=m(),qt=o("span"),So=n("Charger un jeu de donn\xE9es depuis le *Hub*"),ba=m(),ke.c(),Qs=m(),K=o("p"),Ao=n("Le "),jt=o("em"),No=n("Hub"),Oo=n(" ne contient pas seulement des mod\xE8les mais aussi plusieurs jeux de donn\xE9es dans un tas de langues diff\xE9rentes. Vous pouvez explorer les jeux de donn\xE9es "),cs=o("a"),Lo=n("ici"),Fo=n(" et nous vous conseillons d\u2019essayer de charger un nouveau jeu de donn\xE9es une fois que vous avez \xE9tudi\xE9 cette section (voir la documentation g\xE9n\xE9rale "),ds=o("a"),Ro=n("ici"),Ho=n("). Mais pour l\u2019instant, concentrons-nous sur le jeu de donn\xE9es MRPC ! Il s\u2019agit de l\u2019un des 10 jeux de donn\xE9es qui constituent le "),Ue=o("a"),gt=o("em"),Io=n("benchmark"),Vo=n(" GLUE"),Wo=n(" qui est un "),kt=o("em"),Uo=n("benchmark"),Bo=n(" acad\xE9mique utilis\xE9 pour mesurer les performances des mod\xE8les d\u2019apprentissage automatique sur 10 diff\xE9rentes t\xE2ches de classification de textes."),qa=m(),De=o("p"),Go=n("La biblioth\xE8que \u{1F917} "),Et=o("em"),Jo=n("Datasets"),Qo=n(" propose une commande tr\xE8s simple pour t\xE9l\xE9charger et mettre en cache un jeu de donn\xE9es \xE0 partir du "),$t=o("em"),Ko=n("Hub"),Yo=n(". On peut t\xE9l\xE9charger le jeu de donn\xE9es MRPC comme ceci :"),ja=m(),x(ms.$$.fragment),ga=m(),x(fs.$$.fragment),ka=m(),Y=o("p"),Zo=n("Comme vous le voyez, on obtient un objet de type "),xt=o("code"),Xo=n("DatasetDict"),el=n(" qui contient le jeu de donn\xE9es d\u2019entra\xEEnement, celui de validation et celui de test. Chacun d\u2019eux contient plusieurs colonnes ("),zt=o("code"),sl=n("sentence1"),tl=n(", "),wt=o("code"),nl=n("sentence2"),al=n(", "),yt=o("code"),ol=n("label"),ll=n(" et "),Ct=o("code"),rl=n("idx"),il=n(") et une variable nombre de lignes qui contient le nombre d\u2019\xE9l\xE9ments dans chaque jeu de donn\xE9es (il y a donc 3.668 paires de phrases dans le jeu d\u2019entra\xEEnement, 408 dans celui de validation et 1.725 dans celui de test)."),Ea=m(),Pe=o("p"),ul=n("Cette commande t\xE9l\xE9charge et met en cache le jeu de donn\xE9es dans "),Dt=o("em"),pl=n("~/.cache/huggingface/dataset"),cl=n(". Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d\u2019environnement "),Pt=o("code"),dl=n("HF_HOME"),ml=n("."),$a=m(),Be=o("p"),fl=n("Nous pouvons acc\xE9der \xE0 chaque paire de phrase de notre objet "),Tt=o("code"),hl=n("raw_datasets"),_l=n(" par les indices, comme avec un dictionnaire :"),xa=m(),x(hs.$$.fragment),za=m(),x(_s.$$.fragment),wa=m(),Te=o("p"),vl=n("Nous pouvons voir que les \xE9tiquettes sont d\xE9j\xE0 des entiers, donc nous n\u2019aurons pas \xE0 faire de pr\xE9traitement ici. Pour savoir quel entier correspond \xE0 quel label, nous pouvons inspecter les "),Mt=o("code"),bl=n("features"),ql=n(" de notre "),St=o("code"),jl=n("raw_train_dataset"),gl=n(". Cela nous indiquera le type de chaque colonne :"),ya=m(),x(vs.$$.fragment),Ca=m(),x(bs.$$.fragment),Da=m(),U=o("p"),kl=n("En r\xE9alit\xE9, "),At=o("code"),El=n("label"),$l=n(" est de type "),Nt=o("code"),xl=n("ClassLabel"),zl=n(" et la correspondance des entiers aux noms des labels est enregistr\xE9e le dossier "),Ot=o("em"),wl=n("names"),yl=n(". "),Lt=o("code"),Cl=n("0"),Dl=n(" correspond \xE0  "),Ft=o("code"),Pl=n("not_equivalent"),Tl=n(" et "),Rt=o("code"),Ml=n("1"),Sl=n(" correspond \xE0 "),Ht=o("code"),Al=n("equivalent"),Nl=n("."),Pa=m(),x(Ge.$$.fragment),Ta=m(),He=o("h3"),Je=o("a"),It=o("span"),x(qs.$$.fragment),Ol=m(),Vt=o("span"),Ll=n("Pr\xE9traitement d'un jeu de donn\xE9es"),Ma=m(),$e.c(),Ks=m(),ce=o("p"),Fl=n("Pour pr\xE9traiter le jeu de donn\xE9es, nous devons convertir le texte en chiffres compr\xE9hensibles par le mod\xE8le. Comme vous l\u2019avez vu dans le "),Ys=o("a"),Rl=n("chapitre pr\xE9c\xE9dent"),Hl=n(", cette conversion est effectu\xE9e par un "),Wt=o("em"),Il=n("tokenizer"),Vl=n(". Nous pouvons fournir au "),Ut=o("em"),Wl=n("tokenizer"),Ul=n(" une phrase ou une liste de phrases, de sorte que nous pouvons directement tokeniser toutes les premi\xE8res phrases et toutes les secondes phrases de chaque paire comme ceci :"),Sa=m(),x(js.$$.fragment),Aa=m(),Qe=o("p"),Bl=n("Cependant, nous ne pouvons pas simplement passer deux s\xE9quences au mod\xE8le et obtenir une pr\xE9diction pour savoir si les deux phrases sont des paraphrases ou non. Nous devons traiter les deux s\xE9quences comme une paire, et appliquer le pr\xE9traitement appropri\xE9. Heureusement, le "),Bt=o("em"),Gl=n("tokenizer"),Jl=n(" peut \xE9galement prendre une paire de s\xE9quences et la pr\xE9parer de la mani\xE8re attendue par notre mod\xE8le BERT :"),Na=m(),x(gs.$$.fragment),Oa=m(),x(ks.$$.fragment),La=m(),le=o("p"),Ql=n("Nous avons discut\xE9 des cl\xE9s "),Gt=o("code"),Kl=n("input_ids"),Yl=n(" et "),Jt=o("code"),Zl=n("attention_mask"),Xl=n(" dans le "),Zs=o("a"),er=n("Chapitre 2"),sr=n(", mais nous avons laiss\xE9 de c\xF4t\xE9 les "),Qt=o("code"),tr=n("token_type_ids"),nr=n(". Dans cet exemple, c\u2019est ce qui indique au mod\xE8le quelle partie de l\u2019entr\xE9e est la premi\xE8re phrase et quelle partie est la deuxi\xE8me phrase."),Fa=m(),x(Ke.$$.fragment),Ra=m(),Ye=o("p"),ar=n("Si on d\xE9code les IDs dans "),Kt=o("code"),or=n("input_ids"),lr=n(" en mots :"),Ha=m(),x(Es.$$.fragment),Ia=m(),Xs=o("p"),rr=n("nous aurons :"),Va=m(),x($s.$$.fragment),Wa=m(),Me=o("p"),ir=n("Nous voyons donc que le mod\xE8le s\u2019attend \xE0 ce que les entr\xE9es soient de la forme "),Yt=o("code"),ur=n("[CLS] phrase1 [SEP] phrase2 [SEP]"),pr=n(" lorsqu\u2019il y a deux phrases. En alignant cela avec les "),Zt=o("code"),cr=n("token_type_ids"),dr=n(", on obtient :"),Ua=m(),x(xs.$$.fragment),Ba=m(),J=o("p"),mr=n("Comme vous pouvez le voir, les parties de l\u2019entr\xE9e correspondant \xE0 "),Xt=o("code"),fr=n("[CLS] sentence1 [SEP]"),hr=n(" ont toutes un "),en=o("em"),_r=n("token"),vr=n(" de type ID de "),sn=o("code"),br=n("0"),qr=n(", tandis que les autres parties, correspondant \xE0 "),tn=o("code"),jr=n("sentence2 [SEP]"),gr=n(", ont toutes un "),nn=o("em"),kr=n("token"),Er=n(" de type ID de "),an=o("code"),$r=n("1"),xr=n("."),Ga=m(),Se=o("p"),zr=n("Notez que si vous choisissez un autre "),on=o("em"),wr=n("checkpoint"),yr=n(", vous n\u2019aurez pas n\xE9cessairement les "),ln=o("code"),Cr=n("token_type_ids"),Dr=n(" dans vos entr\xE9es tokenis\xE9es (par exemple, ils ne sont pas retourn\xE9s si vous utilisez un mod\xE8le DistilBERT). Ils ne sont retourn\xE9s que lorsque le mod\xE8le sait quoi faire avec eux, parce qu\u2019il les a vus pendant son pr\xE9-entra\xEEnement."),Ja=m(),de=o("p"),Pr=n("Ici, BERT est pr\xE9-entra\xEEn\xE9 avec les "),rn=o("em"),Tr=n("tokens"),Mr=n(" de type ID et en plus de l\u2019objectif de mod\xE9lisation du langage masqu\xE9 dont nous avons abord\xE9 dans "),et=o("a"),Sr=n("Chapitre 1"),Ar=n(", il a un objectif suppl\xE9mentaire appel\xE9 "),un=o("em"),Nr=n("pr\xE9diction de la phrase suivante"),Or=n(". Le but de cette t\xE2che est de mod\xE9liser la relation entre des paires de phrases."),Qa=m(),Ze=o("p"),Lr=n("Avec la pr\xE9diction de la phrase suivante, on fournit au mod\xE8le des paires de phrases (avec des "),pn=o("em"),Fr=n("tokens"),Rr=n(" masqu\xE9s de mani\xE8re al\xE9atoire) et on lui demande de pr\xE9dire si la deuxi\xE8me phrase suit la premi\xE8re. Pour rendre la t\xE2che non triviale, la moiti\xE9 du temps, les phrases se suivent dans le document d\u2019origine dont elles ont \xE9t\xE9 extraites, et l\u2019autre moiti\xE9 du temps, les deux phrases proviennent de deux documents diff\xE9rents."),Ka=m(),re=o("p"),Hr=n("En g\xE9n\xE9ral, vous n\u2019avez pas besoin de vous inqui\xE9ter de savoir s\u2019il y a ou non des "),cn=o("code"),Ir=n("token_type_ids"),Vr=n(" dans vos entr\xE9es tokenis\xE9es : tant que vous utilisez le m\xEAme "),dn=o("em"),Wr=n("checkpoint"),Ur=n(" pour le "),mn=o("em"),Br=n("tokenizer"),Gr=n(" et le mod\xE8le, tout ira bien puisque le "),fn=o("em"),Jr=n("tokenizer"),Qr=n(" sait quoi fournir \xE0 son mod\xE8le."),Ya=m(),ie=o("p"),Kr=n("Maintenant que nous avons vu comment notre "),hn=o("em"),Yr=n("tokenizer"),Zr=n(" peut traiter une paire de phrases, nous pouvons l\u2019utiliser pour tokeniser l\u2019ensemble de notre jeu de donn\xE9es : comme dans le "),st=o("a"),Xr=n("chapitre pr\xE9c\xE9dent"),ei=n(", nous pouvons fournir au "),_n=o("em"),si=n("tokenizer"),ti=n(" une liste de paires de phrases en lui donnant la liste des premi\xE8res phrases, puis la liste des secondes phrases. Ceci est \xE9galement compatible avec les options de remplissage et de troncature que nous avons vues dans le "),tt=o("a"),ni=n("Chapitre 2"),ai=n(". Voici donc une fa\xE7on de pr\xE9traiter le jeu de donn\xE9es d\u2019entra\xEEnement :"),Za=m(),x(zs.$$.fragment),Xa=m(),Z=o("p"),oi=n("Cela fonctionne bien, mais a l\u2019inconv\xE9nient de retourner un dictionnaire (avec nos cl\xE9s, "),vn=o("code"),li=n("input_ids"),ri=n(", "),bn=o("code"),ii=n("attention_mask"),ui=n(", et "),qn=o("code"),pi=n("token_type_ids"),ci=n(", et des valeurs qui sont des listes de listes). Cela ne fonctionnera \xE9galement que si vous avez assez de RAM pour stocker l\u2019ensemble de votre jeu de donn\xE9es pendant la tokenisation (alors que les jeux de donn\xE9es de la biblioth\xE8que \u{1F917} "),jn=o("em"),di=n("Datasets"),mi=n(" sont des fichiers "),ws=o("a"),fi=n("Apache Arrow"),hi=n(" stock\xE9s sur le disque, vous ne gardez donc en m\xE9moire que les \xE9chantillons que vous demandez)."),eo=m(),Ae=o("p"),_i=n("Pour conserver les donn\xE9es sous forme de jeu de donn\xE9es, nous utiliserons la m\xE9thode "),ys=o("a"),gn=o("code"),vi=n("Dataset.map()"),bi=n(". Cela nous permet \xE9galement une certaine flexibilit\xE9, si nous avons besoin d\u2019un pr\xE9traitement plus pouss\xE9 que la simple tokenisation. La m\xE9thode "),kn=o("code"),qi=n("map()"),ji=n(" fonctionne en appliquant une fonction sur chaque \xE9l\xE9ment de l\u2019ensemble de donn\xE9es, donc d\xE9finissons une fonction qui tokenise nos entr\xE9es :"),so=m(),x(Cs.$$.fragment),to=m(),T=o("p"),gi=n("Cette fonction prend un dictionnaire (comme les \xE9l\xE9ments de notre jeu de donn\xE9es) et retourne un nouveau dictionnaire avec les cl\xE9s "),En=o("code"),ki=n("input_ids"),Ei=n(", "),$n=o("code"),$i=n("attention_mask"),xi=n(", et "),xn=o("code"),zi=n("token_type_ids"),wi=n(". Notez que cela fonctionne \xE9galement si le dictionnaire "),zn=o("code"),yi=n("example"),Ci=n(" contient plusieurs \xE9chantillons (chaque cl\xE9 \xE9tant une liste de phrases) puisque le "),wn=o("code"),Di=n("tokenizer"),Pi=n(" travaille sur des listes de paires de phrases, comme vu pr\xE9c\xE9demment. Cela nous permettra d\u2019utiliser l\u2019option "),yn=o("code"),Ti=n("batched=True"),Mi=n(" dans notre appel \xE0 "),Cn=o("code"),Si=n("map()"),Ai=n(", ce qui acc\xE9l\xE9rera grandement la tok\xE9nisation. Le "),Dn=o("code"),Ni=n("tokenizer"),Oi=n(" est soutenu par un "),Pn=o("em"),Li=n("tokenizer"),Fi=n(" \xE9crit en Rust \xE0 partir de la biblioth\xE8que "),Xe=o("a"),Ri=n("\u{1F917} "),Tn=o("em"),Hi=n("Tokenizers"),Ii=n(". Ce "),Mn=o("em"),Vi=n("tokenizer"),Wi=n(" peut \xEAtre tr\xE8s rapide, mais seulement si on lui donne beaucoup d\u2019entr\xE9es en m\xEAme temps."),no=m(),me=o("p"),Ui=n("Notez que nous avons laiss\xE9 l\u2019argument "),Sn=o("code"),Bi=n("padding"),Gi=n(" hors de notre fonction de "),An=o("em"),Ji=n("tokenizer"),Qi=n(" pour le moment. C\u2019est parce que le "),Nn=o("em"),Ki=n("padding"),Yi=n(" de tous les \xE9chantillons \xE0 la longueur maximale n\u2019est pas efficace : il est pr\xE9f\xE9rable de remplir les \xE9chantillons lorsque nous construisons un batch, car alors nous avons seulement besoin de remplir \xE0 la longueur maximale dans ce batch, et non la longueur maximale dans l\u2019ensemble des donn\xE9es. Cela peut permettre de gagner beaucoup de temps et de puissance de traitement lorsque les entr\xE9es ont des longueurs tr\xE8s variables !"),ao=m(),Ne=o("p"),Zi=n("Voici comment nous appliquons la fonction de tokenization sur tous nos jeux de donn\xE9es en m\xEAme temps. Nous utilisons "),On=o("code"),Xi=n("batched=True"),eu=n(" dans notre appel \xE0 "),Ln=o("code"),su=n("map"),tu=n(" pour que la fonction soit appliqu\xE9e \xE0 plusieurs \xE9l\xE9ments de notre jeu de donn\xE9es en une fois, et non \xE0 chaque \xE9l\xE9ment s\xE9par\xE9ment. Cela permet un pr\xE9traitement plus rapide."),oo=m(),x(Ds.$$.fragment),lo=m(),es=o("p"),nu=n("La fa\xE7on dont la biblioth\xE8que \u{1F917} "),Fn=o("em"),au=n("Datasets"),ou=n(" applique ce traitement consiste \xE0 ajouter de nouveaux champs aux jeux de donn\xE9es, un pour chaque cl\xE9 du dictionnaire renvoy\xE9 par la fonction de pr\xE9traitement :"),ro=m(),x(Ps.$$.fragment),io=m(),X=o("p"),lu=n("Vous pouvez m\xEAme utiliser le multitraitement lorsque vous appliquez votre fonction de pr\xE9traitement avec "),Rn=o("code"),ru=n("map()"),iu=n(" en passant un argument "),Hn=o("code"),uu=n("num_proc"),pu=n(". Nous ne l\u2019avons pas fait ici parce que la biblioth\xE8que \u{1F917} "),In=o("em"),cu=n("Tokenizers"),du=n(" utilise d\xE9j\xE0 plusieurs "),Vn=o("em"),mu=n("threads"),fu=n(" pour tokeniser nos \xE9chantillons plus rapidement, mais si vous n\u2019utilisez pas un "),Wn=o("em"),hu=n("tokenizer"),_u=n(" rapide soutenu par cette biblioth\xE8que, cela pourrait acc\xE9l\xE9rer votre pr\xE9traitement."),uo=m(),ee=o("p"),vu=n("Notre "),Un=o("code"),bu=n("tokenize_function"),qu=n(" retourne un dictionnaire avec les cl\xE9s "),Bn=o("code"),ju=n("input_ids"),gu=n(", "),Gn=o("code"),ku=n("attention_mask"),Eu=n(", et "),Jn=o("code"),$u=n("token_type_ids"),xu=n(", donc ces trois champs sont ajout\xE9s \xE0 toutes les divisions de notre jeu de donn\xE9es. Notez que nous aurions \xE9galement pu modifier des champs existants si notre fonction de pr\xE9traitement avait retourn\xE9 une nouvelle valeur pour une cl\xE9 existante dans l\u2019ensemble de donn\xE9es auquel nous avons appliqu\xE9 "),Qn=o("code"),zu=n("map()"),wu=n("."),po=m(),ss=o("p"),yu=n("La derni\xE8re chose que nous devrons faire est de remplir tous les exemples \xE0 la longueur de l\u2019\xE9l\xE9ment le plus long lorsque nous regroupons les \xE9l\xE9ments, une technique que nous appelons le "),Kn=o("em"),Cu=n("padding dynamique"),Du=n("."),co=m(),Ie=o("h3"),ts=o("a"),Yn=o("span"),x(Ts.$$.fragment),Pu=m(),Zn=o("span"),Tu=n("*Padding* dynamique"),mo=m(),x(Ms.$$.fragment),fo=m(),Ve.c(),nt=m(),B=o("p"),Mu=n("Pour faire cela en pratique, nous devons d\xE9finir une fonction de rassemblement qui appliquera la bonne quantit\xE9 de "),Xn=o("em"),Su=n("padding"),Au=n(" aux \xE9l\xE9ments du jeu de donn\xE9es que nous voulons regrouper. Heureusement, la biblioth\xE8que \u{1F917} "),ea=o("em"),Nu=n("Transformers"),Ou=n(" nous fournit une telle fonction via "),sa=o("code"),Lu=n("DataCollatorWithPadding"),Fu=n(". Elle prend un "),ta=o("em"),Ru=n("tokenizer"),Hu=n(" lorsque vous l\u2019instanciez (pour savoir quel "),na=o("em"),Iu=n("token"),Vu=n(" de "),aa=o("em"),Wu=n("padding"),Uu=n(" utiliser et si le mod\xE8le s\u2019attend \xE0 ce que le "),oa=o("em"),Bu=n("padding"),Gu=n(" soit \xE0 gauche ou \xE0 droite des entr\xE9es) et fera tout ce dont vous avez besoin :"),ho=m(),ze.c(),at=m(),se=o("p"),Ju=n("Pour tester notre nouveau jouet, prenons quelques \xE9l\xE9ments de notre jeu d\u2019entra\xEEnement avec lesquels nous allons former un batch. Ici, on supprime les colonnes "),la=o("code"),Qu=n("idx"),Ku=n(", "),ra=o("code"),Yu=n("sentence1"),Zu=n(" et "),ia=o("code"),Xu=n("sentence2"),ep=n(" puisque nous n\u2019en aurons pas besoin et qu\u2019elles contiennent des "),ua=o("em"),sp=n("strings"),tp=n(" (et nous ne pouvons pas cr\xE9er des tenseurs avec des "),pa=o("em"),np=n("strings"),ap=n(") et on regarde la longueur de chaque entr\xE9e du batch :"),_o=m(),x(Ss.$$.fragment),vo=m(),x(As.$$.fragment),bo=m(),fe=o("p"),op=n("Sans surprise, nous obtenons des \xE9chantillons de longueur variable, de 32 \xE0 67. Le "),ca=o("em"),lp=n("padding"),rp=n(" dynamique signifie que les \xE9chantillons de ce batch doivent tous \xEAtre rembourr\xE9s \xE0 une longueur de 67, la longueur maximale dans le batch. Sans le "),da=o("em"),ip=n("padding"),up=n(" dynamique, tous les \xE9chantillons devraient \xEAtre rembourr\xE9s \xE0 la longueur maximale du jeu de donn\xE9es entier, ou \xE0 la longueur maximale que le mod\xE8le peut accepter. V\xE9rifions \xE0 nouveau que notre "),ma=o("code"),pp=n("data_collator"),cp=n(" rembourre dynamiquement le batch correctement :"),qo=m(),x(Ns.$$.fragment),jo=m(),ye.c(),ot=m(),x(ns.$$.fragment),go=m(),G&&G.c(),lt=Ad(),this.h()},l(e){const p=Hd('[data-svelte="svelte-1phssyn"]',document.head);i=l(p,"META",{name:!0,content:!0}),p.forEach(t),d=f(e),z(u.$$.fragment,e),_=f(e),g=l(e,"H1",{class:!0});var Vs=r(g);v=l(Vs,"A",{id:!0,class:!0,href:!0});var rt=r(v);b=l(rt,"SPAN",{});var it=r(b);z($.$$.fragment,it),it.forEach(t),rt.forEach(t),h=f(Vs),k=l(Vs,"SPAN",{});var ut=r(k);S=a(ut,"Pr\xE9parer les donn\xE9es"),ut.forEach(t),Vs.forEach(t),I=f(e),L.l(e),A=f(e),R.l(e),H=f(e),E=l(e,"P",{});var fa=r(E);ae=a(fa,"Evidemment, entra\xEEner un mod\xE8le avec seulement deux phrases ne va pas donner de bons r\xE9sultats. Pour obtenir de meilleurs r\xE9sultats, vous allez avoir \xE0 pr\xE9parer un plus grand jeu de donn\xE9es."),fa.forEach(t),Q=f(e),W=l(e,"P",{});var Oe=r(W);oe=a(Oe,"Dans cette section, nous allons utiliser comme exemple le jeu de donn\xE9es MRPC ("),P=l(Oe,"EM",{});var ha=r(P);V=a(ha,"Microsoft Research Paraphrase Corpus"),ha.forEach(t),Bs=a(Oe,") pr\xE9sent\xE9 dans un "),Ce=l(Oe,"A",{href:!0,rel:!0});var pt=r(Ce);Gs=a(pt,"papier"),pt.forEach(t),Js=a(Oe," par William B. Dolan et Chris Brockett. Ce jeu de donn\xE9es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m\xEAme signification). Nous l\u2019avons choisi pour ce chapitre parce que c\u2019est un petit jeu de donn\xE9es et cela rend donc simples les exp\xE9riences d\u2019entra\xEEnement sur ce jeu de donn\xE9es."),Oe.forEach(t),us=f(e),pe=l(e,"H3",{class:!0});var as=r(pe);We=l(as,"A",{id:!0,class:!0,href:!0});var _a=r(We);bt=l(_a,"SPAN",{});var yp=r(bt);z(ps.$$.fragment,yp),yp.forEach(t),_a.forEach(t),Mo=f(as),qt=l(as,"SPAN",{});var Cp=r(qt);So=a(Cp,"Charger un jeu de donn\xE9es depuis le *Hub*"),Cp.forEach(t),as.forEach(t),ba=f(e),ke.l(e),Qs=f(e),K=l(e,"P",{});var he=r(K);Ao=a(he,"Le "),jt=l(he,"EM",{});var Dp=r(jt);No=a(Dp,"Hub"),Dp.forEach(t),Oo=a(he," ne contient pas seulement des mod\xE8les mais aussi plusieurs jeux de donn\xE9es dans un tas de langues diff\xE9rentes. Vous pouvez explorer les jeux de donn\xE9es "),cs=l(he,"A",{href:!0,rel:!0});var Pp=r(cs);Lo=a(Pp,"ici"),Pp.forEach(t),Fo=a(he," et nous vous conseillons d\u2019essayer de charger un nouveau jeu de donn\xE9es une fois que vous avez \xE9tudi\xE9 cette section (voir la documentation g\xE9n\xE9rale "),ds=l(he,"A",{href:!0,rel:!0});var Tp=r(ds);Ro=a(Tp,"ici"),Tp.forEach(t),Ho=a(he,"). Mais pour l\u2019instant, concentrons-nous sur le jeu de donn\xE9es MRPC ! Il s\u2019agit de l\u2019un des 10 jeux de donn\xE9es qui constituent le "),Ue=l(he,"A",{href:!0,rel:!0});var dp=r(Ue);gt=l(dp,"EM",{});var Mp=r(gt);Io=a(Mp,"benchmark"),Mp.forEach(t),Vo=a(dp," GLUE"),dp.forEach(t),Wo=a(he," qui est un "),kt=l(he,"EM",{});var Sp=r(kt);Uo=a(Sp,"benchmark"),Sp.forEach(t),Bo=a(he," acad\xE9mique utilis\xE9 pour mesurer les performances des mod\xE8les d\u2019apprentissage automatique sur 10 diff\xE9rentes t\xE2ches de classification de textes."),he.forEach(t),qa=f(e),De=l(e,"P",{});var ct=r(De);Go=a(ct,"La biblioth\xE8que \u{1F917} "),Et=l(ct,"EM",{});var Ap=r(Et);Jo=a(Ap,"Datasets"),Ap.forEach(t),Qo=a(ct," propose une commande tr\xE8s simple pour t\xE9l\xE9charger et mettre en cache un jeu de donn\xE9es \xE0 partir du "),$t=l(ct,"EM",{});var Np=r($t);Ko=a(Np,"Hub"),Np.forEach(t),Yo=a(ct,". On peut t\xE9l\xE9charger le jeu de donn\xE9es MRPC comme ceci :"),ct.forEach(t),ja=f(e),z(ms.$$.fragment,e),ga=f(e),z(fs.$$.fragment,e),ka=f(e),Y=l(e,"P",{});var _e=r(Y);Zo=a(_e,"Comme vous le voyez, on obtient un objet de type "),xt=l(_e,"CODE",{});var Op=r(xt);Xo=a(Op,"DatasetDict"),Op.forEach(t),el=a(_e," qui contient le jeu de donn\xE9es d\u2019entra\xEEnement, celui de validation et celui de test. Chacun d\u2019eux contient plusieurs colonnes ("),zt=l(_e,"CODE",{});var Lp=r(zt);sl=a(Lp,"sentence1"),Lp.forEach(t),tl=a(_e,", "),wt=l(_e,"CODE",{});var Fp=r(wt);nl=a(Fp,"sentence2"),Fp.forEach(t),al=a(_e,", "),yt=l(_e,"CODE",{});var Rp=r(yt);ol=a(Rp,"label"),Rp.forEach(t),ll=a(_e," et "),Ct=l(_e,"CODE",{});var Hp=r(Ct);rl=a(Hp,"idx"),Hp.forEach(t),il=a(_e,") et une variable nombre de lignes qui contient le nombre d\u2019\xE9l\xE9ments dans chaque jeu de donn\xE9es (il y a donc 3.668 paires de phrases dans le jeu d\u2019entra\xEEnement, 408 dans celui de validation et 1.725 dans celui de test)."),_e.forEach(t),Ea=f(e),Pe=l(e,"P",{});var dt=r(Pe);ul=a(dt,"Cette commande t\xE9l\xE9charge et met en cache le jeu de donn\xE9es dans "),Dt=l(dt,"EM",{});var Ip=r(Dt);pl=a(Ip,"~/.cache/huggingface/dataset"),Ip.forEach(t),cl=a(dt,". Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d\u2019environnement "),Pt=l(dt,"CODE",{});var Vp=r(Pt);dl=a(Vp,"HF_HOME"),Vp.forEach(t),ml=a(dt,"."),dt.forEach(t),$a=f(e),Be=l(e,"P",{});var $o=r(Be);fl=a($o,"Nous pouvons acc\xE9der \xE0 chaque paire de phrase de notre objet "),Tt=l($o,"CODE",{});var Wp=r(Tt);hl=a(Wp,"raw_datasets"),Wp.forEach(t),_l=a($o," par les indices, comme avec un dictionnaire :"),$o.forEach(t),xa=f(e),z(hs.$$.fragment,e),za=f(e),z(_s.$$.fragment,e),wa=f(e),Te=l(e,"P",{});var mt=r(Te);vl=a(mt,"Nous pouvons voir que les \xE9tiquettes sont d\xE9j\xE0 des entiers, donc nous n\u2019aurons pas \xE0 faire de pr\xE9traitement ici. Pour savoir quel entier correspond \xE0 quel label, nous pouvons inspecter les "),Mt=l(mt,"CODE",{});var Up=r(Mt);bl=a(Up,"features"),Up.forEach(t),ql=a(mt," de notre "),St=l(mt,"CODE",{});var Bp=r(St);jl=a(Bp,"raw_train_dataset"),Bp.forEach(t),gl=a(mt,". Cela nous indiquera le type de chaque colonne :"),mt.forEach(t),ya=f(e),z(vs.$$.fragment,e),Ca=f(e),z(bs.$$.fragment,e),Da=f(e),U=l(e,"P",{});var te=r(U);kl=a(te,"En r\xE9alit\xE9, "),At=l(te,"CODE",{});var Gp=r(At);El=a(Gp,"label"),Gp.forEach(t),$l=a(te," est de type "),Nt=l(te,"CODE",{});var Jp=r(Nt);xl=a(Jp,"ClassLabel"),Jp.forEach(t),zl=a(te," et la correspondance des entiers aux noms des labels est enregistr\xE9e le dossier "),Ot=l(te,"EM",{});var Qp=r(Ot);wl=a(Qp,"names"),Qp.forEach(t),yl=a(te,". "),Lt=l(te,"CODE",{});var Kp=r(Lt);Cl=a(Kp,"0"),Kp.forEach(t),Dl=a(te," correspond \xE0  "),Ft=l(te,"CODE",{});var Yp=r(Ft);Pl=a(Yp,"not_equivalent"),Yp.forEach(t),Tl=a(te," et "),Rt=l(te,"CODE",{});var Zp=r(Rt);Ml=a(Zp,"1"),Zp.forEach(t),Sl=a(te," correspond \xE0 "),Ht=l(te,"CODE",{});var Xp=r(Ht);Al=a(Xp,"equivalent"),Xp.forEach(t),Nl=a(te,"."),te.forEach(t),Pa=f(e),z(Ge.$$.fragment,e),Ta=f(e),He=l(e,"H3",{class:!0});var xo=r(He);Je=l(xo,"A",{id:!0,class:!0,href:!0});var ec=r(Je);It=l(ec,"SPAN",{});var sc=r(It);z(qs.$$.fragment,sc),sc.forEach(t),ec.forEach(t),Ol=f(xo),Vt=l(xo,"SPAN",{});var tc=r(Vt);Ll=a(tc,"Pr\xE9traitement d'un jeu de donn\xE9es"),tc.forEach(t),xo.forEach(t),Ma=f(e),$e.l(e),Ks=f(e),ce=l(e,"P",{});var os=r(ce);Fl=a(os,"Pour pr\xE9traiter le jeu de donn\xE9es, nous devons convertir le texte en chiffres compr\xE9hensibles par le mod\xE8le. Comme vous l\u2019avez vu dans le "),Ys=l(os,"A",{href:!0});var nc=r(Ys);Rl=a(nc,"chapitre pr\xE9c\xE9dent"),nc.forEach(t),Hl=a(os,", cette conversion est effectu\xE9e par un "),Wt=l(os,"EM",{});var ac=r(Wt);Il=a(ac,"tokenizer"),ac.forEach(t),Vl=a(os,". Nous pouvons fournir au "),Ut=l(os,"EM",{});var oc=r(Ut);Wl=a(oc,"tokenizer"),oc.forEach(t),Ul=a(os," une phrase ou une liste de phrases, de sorte que nous pouvons directement tokeniser toutes les premi\xE8res phrases et toutes les secondes phrases de chaque paire comme ceci :"),os.forEach(t),Sa=f(e),z(js.$$.fragment,e),Aa=f(e),Qe=l(e,"P",{});var zo=r(Qe);Bl=a(zo,"Cependant, nous ne pouvons pas simplement passer deux s\xE9quences au mod\xE8le et obtenir une pr\xE9diction pour savoir si les deux phrases sont des paraphrases ou non. Nous devons traiter les deux s\xE9quences comme une paire, et appliquer le pr\xE9traitement appropri\xE9. Heureusement, le "),Bt=l(zo,"EM",{});var lc=r(Bt);Gl=a(lc,"tokenizer"),lc.forEach(t),Jl=a(zo," peut \xE9galement prendre une paire de s\xE9quences et la pr\xE9parer de la mani\xE8re attendue par notre mod\xE8le BERT :"),zo.forEach(t),Na=f(e),z(gs.$$.fragment,e),Oa=f(e),z(ks.$$.fragment,e),La=f(e),le=l(e,"P",{});var Le=r(le);Ql=a(Le,"Nous avons discut\xE9 des cl\xE9s "),Gt=l(Le,"CODE",{});var rc=r(Gt);Kl=a(rc,"input_ids"),rc.forEach(t),Yl=a(Le," et "),Jt=l(Le,"CODE",{});var ic=r(Jt);Zl=a(ic,"attention_mask"),ic.forEach(t),Xl=a(Le," dans le "),Zs=l(Le,"A",{href:!0});var uc=r(Zs);er=a(uc,"Chapitre 2"),uc.forEach(t),sr=a(Le,", mais nous avons laiss\xE9 de c\xF4t\xE9 les "),Qt=l(Le,"CODE",{});var pc=r(Qt);tr=a(pc,"token_type_ids"),pc.forEach(t),nr=a(Le,". Dans cet exemple, c\u2019est ce qui indique au mod\xE8le quelle partie de l\u2019entr\xE9e est la premi\xE8re phrase et quelle partie est la deuxi\xE8me phrase."),Le.forEach(t),Fa=f(e),z(Ke.$$.fragment,e),Ra=f(e),Ye=l(e,"P",{});var wo=r(Ye);ar=a(wo,"Si on d\xE9code les IDs dans "),Kt=l(wo,"CODE",{});var cc=r(Kt);or=a(cc,"input_ids"),cc.forEach(t),lr=a(wo," en mots :"),wo.forEach(t),Ha=f(e),z(Es.$$.fragment,e),Ia=f(e),Xs=l(e,"P",{});var dc=r(Xs);rr=a(dc,"nous aurons :"),dc.forEach(t),Va=f(e),z($s.$$.fragment,e),Wa=f(e),Me=l(e,"P",{});var ft=r(Me);ir=a(ft,"Nous voyons donc que le mod\xE8le s\u2019attend \xE0 ce que les entr\xE9es soient de la forme "),Yt=l(ft,"CODE",{});var mc=r(Yt);ur=a(mc,"[CLS] phrase1 [SEP] phrase2 [SEP]"),mc.forEach(t),pr=a(ft," lorsqu\u2019il y a deux phrases. En alignant cela avec les "),Zt=l(ft,"CODE",{});var fc=r(Zt);cr=a(fc,"token_type_ids"),fc.forEach(t),dr=a(ft,", on obtient :"),ft.forEach(t),Ua=f(e),z(xs.$$.fragment,e),Ba=f(e),J=l(e,"P",{});var ue=r(J);mr=a(ue,"Comme vous pouvez le voir, les parties de l\u2019entr\xE9e correspondant \xE0 "),Xt=l(ue,"CODE",{});var hc=r(Xt);fr=a(hc,"[CLS] sentence1 [SEP]"),hc.forEach(t),hr=a(ue," ont toutes un "),en=l(ue,"EM",{});var _c=r(en);_r=a(_c,"token"),_c.forEach(t),vr=a(ue," de type ID de "),sn=l(ue,"CODE",{});var vc=r(sn);br=a(vc,"0"),vc.forEach(t),qr=a(ue,", tandis que les autres parties, correspondant \xE0 "),tn=l(ue,"CODE",{});var bc=r(tn);jr=a(bc,"sentence2 [SEP]"),bc.forEach(t),gr=a(ue,", ont toutes un "),nn=l(ue,"EM",{});var qc=r(nn);kr=a(qc,"token"),qc.forEach(t),Er=a(ue," de type ID de "),an=l(ue,"CODE",{});var jc=r(an);$r=a(jc,"1"),jc.forEach(t),xr=a(ue,"."),ue.forEach(t),Ga=f(e),Se=l(e,"P",{});var ht=r(Se);zr=a(ht,"Notez que si vous choisissez un autre "),on=l(ht,"EM",{});var gc=r(on);wr=a(gc,"checkpoint"),gc.forEach(t),yr=a(ht,", vous n\u2019aurez pas n\xE9cessairement les "),ln=l(ht,"CODE",{});var kc=r(ln);Cr=a(kc,"token_type_ids"),kc.forEach(t),Dr=a(ht," dans vos entr\xE9es tokenis\xE9es (par exemple, ils ne sont pas retourn\xE9s si vous utilisez un mod\xE8le DistilBERT). Ils ne sont retourn\xE9s que lorsque le mod\xE8le sait quoi faire avec eux, parce qu\u2019il les a vus pendant son pr\xE9-entra\xEEnement."),ht.forEach(t),Ja=f(e),de=l(e,"P",{});var ls=r(de);Pr=a(ls,"Ici, BERT est pr\xE9-entra\xEEn\xE9 avec les "),rn=l(ls,"EM",{});var Ec=r(rn);Tr=a(Ec,"tokens"),Ec.forEach(t),Mr=a(ls," de type ID et en plus de l\u2019objectif de mod\xE9lisation du langage masqu\xE9 dont nous avons abord\xE9 dans "),et=l(ls,"A",{href:!0});var $c=r(et);Sr=a($c,"Chapitre 1"),$c.forEach(t),Ar=a(ls,", il a un objectif suppl\xE9mentaire appel\xE9 "),un=l(ls,"EM",{});var xc=r(un);Nr=a(xc,"pr\xE9diction de la phrase suivante"),xc.forEach(t),Or=a(ls,". Le but de cette t\xE2che est de mod\xE9liser la relation entre des paires de phrases."),ls.forEach(t),Qa=f(e),Ze=l(e,"P",{});var yo=r(Ze);Lr=a(yo,"Avec la pr\xE9diction de la phrase suivante, on fournit au mod\xE8le des paires de phrases (avec des "),pn=l(yo,"EM",{});var zc=r(pn);Fr=a(zc,"tokens"),zc.forEach(t),Rr=a(yo," masqu\xE9s de mani\xE8re al\xE9atoire) et on lui demande de pr\xE9dire si la deuxi\xE8me phrase suit la premi\xE8re. Pour rendre la t\xE2che non triviale, la moiti\xE9 du temps, les phrases se suivent dans le document d\u2019origine dont elles ont \xE9t\xE9 extraites, et l\u2019autre moiti\xE9 du temps, les deux phrases proviennent de deux documents diff\xE9rents."),yo.forEach(t),Ka=f(e),re=l(e,"P",{});var Fe=r(re);Hr=a(Fe,"En g\xE9n\xE9ral, vous n\u2019avez pas besoin de vous inqui\xE9ter de savoir s\u2019il y a ou non des "),cn=l(Fe,"CODE",{});var wc=r(cn);Ir=a(wc,"token_type_ids"),wc.forEach(t),Vr=a(Fe," dans vos entr\xE9es tokenis\xE9es : tant que vous utilisez le m\xEAme "),dn=l(Fe,"EM",{});var yc=r(dn);Wr=a(yc,"checkpoint"),yc.forEach(t),Ur=a(Fe," pour le "),mn=l(Fe,"EM",{});var Cc=r(mn);Br=a(Cc,"tokenizer"),Cc.forEach(t),Gr=a(Fe," et le mod\xE8le, tout ira bien puisque le "),fn=l(Fe,"EM",{});var Dc=r(fn);Jr=a(Dc,"tokenizer"),Dc.forEach(t),Qr=a(Fe," sait quoi fournir \xE0 son mod\xE8le."),Fe.forEach(t),Ya=f(e),ie=l(e,"P",{});var Re=r(ie);Kr=a(Re,"Maintenant que nous avons vu comment notre "),hn=l(Re,"EM",{});var Pc=r(hn);Yr=a(Pc,"tokenizer"),Pc.forEach(t),Zr=a(Re," peut traiter une paire de phrases, nous pouvons l\u2019utiliser pour tokeniser l\u2019ensemble de notre jeu de donn\xE9es : comme dans le "),st=l(Re,"A",{href:!0});var Tc=r(st);Xr=a(Tc,"chapitre pr\xE9c\xE9dent"),Tc.forEach(t),ei=a(Re,", nous pouvons fournir au "),_n=l(Re,"EM",{});var Mc=r(_n);si=a(Mc,"tokenizer"),Mc.forEach(t),ti=a(Re," une liste de paires de phrases en lui donnant la liste des premi\xE8res phrases, puis la liste des secondes phrases. Ceci est \xE9galement compatible avec les options de remplissage et de troncature que nous avons vues dans le "),tt=l(Re,"A",{href:!0});var Sc=r(tt);ni=a(Sc,"Chapitre 2"),Sc.forEach(t),ai=a(Re,". Voici donc une fa\xE7on de pr\xE9traiter le jeu de donn\xE9es d\u2019entra\xEEnement :"),Re.forEach(t),Za=f(e),z(zs.$$.fragment,e),Xa=f(e),Z=l(e,"P",{});var ve=r(Z);oi=a(ve,"Cela fonctionne bien, mais a l\u2019inconv\xE9nient de retourner un dictionnaire (avec nos cl\xE9s, "),vn=l(ve,"CODE",{});var Ac=r(vn);li=a(Ac,"input_ids"),Ac.forEach(t),ri=a(ve,", "),bn=l(ve,"CODE",{});var Nc=r(bn);ii=a(Nc,"attention_mask"),Nc.forEach(t),ui=a(ve,", et "),qn=l(ve,"CODE",{});var Oc=r(qn);pi=a(Oc,"token_type_ids"),Oc.forEach(t),ci=a(ve,", et des valeurs qui sont des listes de listes). Cela ne fonctionnera \xE9galement que si vous avez assez de RAM pour stocker l\u2019ensemble de votre jeu de donn\xE9es pendant la tokenisation (alors que les jeux de donn\xE9es de la biblioth\xE8que \u{1F917} "),jn=l(ve,"EM",{});var Lc=r(jn);di=a(Lc,"Datasets"),Lc.forEach(t),mi=a(ve," sont des fichiers "),ws=l(ve,"A",{href:!0,rel:!0});var Fc=r(ws);fi=a(Fc,"Apache Arrow"),Fc.forEach(t),hi=a(ve," stock\xE9s sur le disque, vous ne gardez donc en m\xE9moire que les \xE9chantillons que vous demandez)."),ve.forEach(t),eo=f(e),Ae=l(e,"P",{});var _t=r(Ae);_i=a(_t,"Pour conserver les donn\xE9es sous forme de jeu de donn\xE9es, nous utiliserons la m\xE9thode "),ys=l(_t,"A",{href:!0,rel:!0});var Rc=r(ys);gn=l(Rc,"CODE",{});var Hc=r(gn);vi=a(Hc,"Dataset.map()"),Hc.forEach(t),Rc.forEach(t),bi=a(_t,". Cela nous permet \xE9galement une certaine flexibilit\xE9, si nous avons besoin d\u2019un pr\xE9traitement plus pouss\xE9 que la simple tokenisation. La m\xE9thode "),kn=l(_t,"CODE",{});var Ic=r(kn);qi=a(Ic,"map()"),Ic.forEach(t),ji=a(_t," fonctionne en appliquant une fonction sur chaque \xE9l\xE9ment de l\u2019ensemble de donn\xE9es, donc d\xE9finissons une fonction qui tokenise nos entr\xE9es :"),_t.forEach(t),so=f(e),z(Cs.$$.fragment,e),to=f(e),T=l(e,"P",{});var N=r(T);gi=a(N,"Cette fonction prend un dictionnaire (comme les \xE9l\xE9ments de notre jeu de donn\xE9es) et retourne un nouveau dictionnaire avec les cl\xE9s "),En=l(N,"CODE",{});var Vc=r(En);ki=a(Vc,"input_ids"),Vc.forEach(t),Ei=a(N,", "),$n=l(N,"CODE",{});var Wc=r($n);$i=a(Wc,"attention_mask"),Wc.forEach(t),xi=a(N,", et "),xn=l(N,"CODE",{});var Uc=r(xn);zi=a(Uc,"token_type_ids"),Uc.forEach(t),wi=a(N,". Notez que cela fonctionne \xE9galement si le dictionnaire "),zn=l(N,"CODE",{});var Bc=r(zn);yi=a(Bc,"example"),Bc.forEach(t),Ci=a(N," contient plusieurs \xE9chantillons (chaque cl\xE9 \xE9tant une liste de phrases) puisque le "),wn=l(N,"CODE",{});var Gc=r(wn);Di=a(Gc,"tokenizer"),Gc.forEach(t),Pi=a(N," travaille sur des listes de paires de phrases, comme vu pr\xE9c\xE9demment. Cela nous permettra d\u2019utiliser l\u2019option "),yn=l(N,"CODE",{});var Jc=r(yn);Ti=a(Jc,"batched=True"),Jc.forEach(t),Mi=a(N," dans notre appel \xE0 "),Cn=l(N,"CODE",{});var Qc=r(Cn);Si=a(Qc,"map()"),Qc.forEach(t),Ai=a(N,", ce qui acc\xE9l\xE9rera grandement la tok\xE9nisation. Le "),Dn=l(N,"CODE",{});var Kc=r(Dn);Ni=a(Kc,"tokenizer"),Kc.forEach(t),Oi=a(N," est soutenu par un "),Pn=l(N,"EM",{});var Yc=r(Pn);Li=a(Yc,"tokenizer"),Yc.forEach(t),Fi=a(N," \xE9crit en Rust \xE0 partir de la biblioth\xE8que "),Xe=l(N,"A",{href:!0,rel:!0});var mp=r(Xe);Ri=a(mp,"\u{1F917} "),Tn=l(mp,"EM",{});var Zc=r(Tn);Hi=a(Zc,"Tokenizers"),Zc.forEach(t),mp.forEach(t),Ii=a(N,". Ce "),Mn=l(N,"EM",{});var Xc=r(Mn);Vi=a(Xc,"tokenizer"),Xc.forEach(t),Wi=a(N," peut \xEAtre tr\xE8s rapide, mais seulement si on lui donne beaucoup d\u2019entr\xE9es en m\xEAme temps."),N.forEach(t),no=f(e),me=l(e,"P",{});var rs=r(me);Ui=a(rs,"Notez que nous avons laiss\xE9 l\u2019argument "),Sn=l(rs,"CODE",{});var ed=r(Sn);Bi=a(ed,"padding"),ed.forEach(t),Gi=a(rs," hors de notre fonction de "),An=l(rs,"EM",{});var sd=r(An);Ji=a(sd,"tokenizer"),sd.forEach(t),Qi=a(rs," pour le moment. C\u2019est parce que le "),Nn=l(rs,"EM",{});var td=r(Nn);Ki=a(td,"padding"),td.forEach(t),Yi=a(rs," de tous les \xE9chantillons \xE0 la longueur maximale n\u2019est pas efficace : il est pr\xE9f\xE9rable de remplir les \xE9chantillons lorsque nous construisons un batch, car alors nous avons seulement besoin de remplir \xE0 la longueur maximale dans ce batch, et non la longueur maximale dans l\u2019ensemble des donn\xE9es. Cela peut permettre de gagner beaucoup de temps et de puissance de traitement lorsque les entr\xE9es ont des longueurs tr\xE8s variables !"),rs.forEach(t),ao=f(e),Ne=l(e,"P",{});var vt=r(Ne);Zi=a(vt,"Voici comment nous appliquons la fonction de tokenization sur tous nos jeux de donn\xE9es en m\xEAme temps. Nous utilisons "),On=l(vt,"CODE",{});var nd=r(On);Xi=a(nd,"batched=True"),nd.forEach(t),eu=a(vt," dans notre appel \xE0 "),Ln=l(vt,"CODE",{});var ad=r(Ln);su=a(ad,"map"),ad.forEach(t),tu=a(vt," pour que la fonction soit appliqu\xE9e \xE0 plusieurs \xE9l\xE9ments de notre jeu de donn\xE9es en une fois, et non \xE0 chaque \xE9l\xE9ment s\xE9par\xE9ment. Cela permet un pr\xE9traitement plus rapide."),vt.forEach(t),oo=f(e),z(Ds.$$.fragment,e),lo=f(e),es=l(e,"P",{});var Co=r(es);nu=a(Co,"La fa\xE7on dont la biblioth\xE8que \u{1F917} "),Fn=l(Co,"EM",{});var od=r(Fn);au=a(od,"Datasets"),od.forEach(t),ou=a(Co," applique ce traitement consiste \xE0 ajouter de nouveaux champs aux jeux de donn\xE9es, un pour chaque cl\xE9 du dictionnaire renvoy\xE9 par la fonction de pr\xE9traitement :"),Co.forEach(t),ro=f(e),z(Ps.$$.fragment,e),io=f(e),X=l(e,"P",{});var be=r(X);lu=a(be,"Vous pouvez m\xEAme utiliser le multitraitement lorsque vous appliquez votre fonction de pr\xE9traitement avec "),Rn=l(be,"CODE",{});var ld=r(Rn);ru=a(ld,"map()"),ld.forEach(t),iu=a(be," en passant un argument "),Hn=l(be,"CODE",{});var rd=r(Hn);uu=a(rd,"num_proc"),rd.forEach(t),pu=a(be,". Nous ne l\u2019avons pas fait ici parce que la biblioth\xE8que \u{1F917} "),In=l(be,"EM",{});var id=r(In);cu=a(id,"Tokenizers"),id.forEach(t),du=a(be," utilise d\xE9j\xE0 plusieurs "),Vn=l(be,"EM",{});var ud=r(Vn);mu=a(ud,"threads"),ud.forEach(t),fu=a(be," pour tokeniser nos \xE9chantillons plus rapidement, mais si vous n\u2019utilisez pas un "),Wn=l(be,"EM",{});var pd=r(Wn);hu=a(pd,"tokenizer"),pd.forEach(t),_u=a(be," rapide soutenu par cette biblioth\xE8que, cela pourrait acc\xE9l\xE9rer votre pr\xE9traitement."),be.forEach(t),uo=f(e),ee=l(e,"P",{});var qe=r(ee);vu=a(qe,"Notre "),Un=l(qe,"CODE",{});var cd=r(Un);bu=a(cd,"tokenize_function"),cd.forEach(t),qu=a(qe," retourne un dictionnaire avec les cl\xE9s "),Bn=l(qe,"CODE",{});var dd=r(Bn);ju=a(dd,"input_ids"),dd.forEach(t),gu=a(qe,", "),Gn=l(qe,"CODE",{});var md=r(Gn);ku=a(md,"attention_mask"),md.forEach(t),Eu=a(qe,", et "),Jn=l(qe,"CODE",{});var fd=r(Jn);$u=a(fd,"token_type_ids"),fd.forEach(t),xu=a(qe,", donc ces trois champs sont ajout\xE9s \xE0 toutes les divisions de notre jeu de donn\xE9es. Notez que nous aurions \xE9galement pu modifier des champs existants si notre fonction de pr\xE9traitement avait retourn\xE9 une nouvelle valeur pour une cl\xE9 existante dans l\u2019ensemble de donn\xE9es auquel nous avons appliqu\xE9 "),Qn=l(qe,"CODE",{});var hd=r(Qn);zu=a(hd,"map()"),hd.forEach(t),wu=a(qe,"."),qe.forEach(t),po=f(e),ss=l(e,"P",{});var Do=r(ss);yu=a(Do,"La derni\xE8re chose que nous devrons faire est de remplir tous les exemples \xE0 la longueur de l\u2019\xE9l\xE9ment le plus long lorsque nous regroupons les \xE9l\xE9ments, une technique que nous appelons le "),Kn=l(Do,"EM",{});var _d=r(Kn);Cu=a(_d,"padding dynamique"),_d.forEach(t),Du=a(Do,"."),Do.forEach(t),co=f(e),Ie=l(e,"H3",{class:!0});var Po=r(Ie);ts=l(Po,"A",{id:!0,class:!0,href:!0});var vd=r(ts);Yn=l(vd,"SPAN",{});var bd=r(Yn);z(Ts.$$.fragment,bd),bd.forEach(t),vd.forEach(t),Pu=f(Po),Zn=l(Po,"SPAN",{});var qd=r(Zn);Tu=a(qd,"*Padding* dynamique"),qd.forEach(t),Po.forEach(t),mo=f(e),z(Ms.$$.fragment,e),fo=f(e),Ve.l(e),nt=f(e),B=l(e,"P",{});var ne=r(B);Mu=a(ne,"Pour faire cela en pratique, nous devons d\xE9finir une fonction de rassemblement qui appliquera la bonne quantit\xE9 de "),Xn=l(ne,"EM",{});var jd=r(Xn);Su=a(jd,"padding"),jd.forEach(t),Au=a(ne," aux \xE9l\xE9ments du jeu de donn\xE9es que nous voulons regrouper. Heureusement, la biblioth\xE8que \u{1F917} "),ea=l(ne,"EM",{});var gd=r(ea);Nu=a(gd,"Transformers"),gd.forEach(t),Ou=a(ne," nous fournit une telle fonction via "),sa=l(ne,"CODE",{});var kd=r(sa);Lu=a(kd,"DataCollatorWithPadding"),kd.forEach(t),Fu=a(ne,". Elle prend un "),ta=l(ne,"EM",{});var Ed=r(ta);Ru=a(Ed,"tokenizer"),Ed.forEach(t),Hu=a(ne," lorsque vous l\u2019instanciez (pour savoir quel "),na=l(ne,"EM",{});var $d=r(na);Iu=a($d,"token"),$d.forEach(t),Vu=a(ne," de "),aa=l(ne,"EM",{});var xd=r(aa);Wu=a(xd,"padding"),xd.forEach(t),Uu=a(ne," utiliser et si le mod\xE8le s\u2019attend \xE0 ce que le "),oa=l(ne,"EM",{});var zd=r(oa);Bu=a(zd,"padding"),zd.forEach(t),Gu=a(ne," soit \xE0 gauche ou \xE0 droite des entr\xE9es) et fera tout ce dont vous avez besoin :"),ne.forEach(t),ho=f(e),ze.l(e),at=f(e),se=l(e,"P",{});var je=r(se);Ju=a(je,"Pour tester notre nouveau jouet, prenons quelques \xE9l\xE9ments de notre jeu d\u2019entra\xEEnement avec lesquels nous allons former un batch. Ici, on supprime les colonnes "),la=l(je,"CODE",{});var wd=r(la);Qu=a(wd,"idx"),wd.forEach(t),Ku=a(je,", "),ra=l(je,"CODE",{});var yd=r(ra);Yu=a(yd,"sentence1"),yd.forEach(t),Zu=a(je," et "),ia=l(je,"CODE",{});var Cd=r(ia);Xu=a(Cd,"sentence2"),Cd.forEach(t),ep=a(je," puisque nous n\u2019en aurons pas besoin et qu\u2019elles contiennent des "),ua=l(je,"EM",{});var Dd=r(ua);sp=a(Dd,"strings"),Dd.forEach(t),tp=a(je," (et nous ne pouvons pas cr\xE9er des tenseurs avec des "),pa=l(je,"EM",{});var Pd=r(pa);np=a(Pd,"strings"),Pd.forEach(t),ap=a(je,") et on regarde la longueur de chaque entr\xE9e du batch :"),je.forEach(t),_o=f(e),z(Ss.$$.fragment,e),vo=f(e),z(As.$$.fragment,e),bo=f(e),fe=l(e,"P",{});var is=r(fe);op=a(is,"Sans surprise, nous obtenons des \xE9chantillons de longueur variable, de 32 \xE0 67. Le "),ca=l(is,"EM",{});var Td=r(ca);lp=a(Td,"padding"),Td.forEach(t),rp=a(is," dynamique signifie que les \xE9chantillons de ce batch doivent tous \xEAtre rembourr\xE9s \xE0 une longueur de 67, la longueur maximale dans le batch. Sans le "),da=l(is,"EM",{});var Md=r(da);ip=a(Md,"padding"),Md.forEach(t),up=a(is," dynamique, tous les \xE9chantillons devraient \xEAtre rembourr\xE9s \xE0 la longueur maximale du jeu de donn\xE9es entier, ou \xE0 la longueur maximale que le mod\xE8le peut accepter. V\xE9rifions \xE0 nouveau que notre "),ma=l(is,"CODE",{});var Sd=r(ma);pp=a(Sd,"data_collator"),Sd.forEach(t),cp=a(is," rembourre dynamiquement le batch correctement :"),is.forEach(t),qo=f(e),z(Ns.$$.fragment,e),jo=f(e),ye.l(e),ot=f(e),z(ns.$$.fragment,e),go=f(e),G&&G.l(e),lt=Ad(),this.h()},h(){D(i,"name","hf:doc:metadata"),D(i,"content",JSON.stringify(im)),D(v,"id","prparer-les-donnes"),D(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(v,"href","#prparer-les-donnes"),D(g,"class","relative group"),D(Ce,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),D(Ce,"rel","nofollow"),D(We,"id","charger-un-jeu-de-donnes-depuis-le-hub"),D(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(We,"href","#charger-un-jeu-de-donnes-depuis-le-hub"),D(pe,"class","relative group"),D(cs,"href","https://huggingface.co/datasets"),D(cs,"rel","nofollow"),D(ds,"href","https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub"),D(ds,"rel","nofollow"),D(Ue,"href","https://gluebenchmark.com/"),D(Ue,"rel","nofollow"),D(Je,"id","prtraitement-dun-jeu-de-donnes"),D(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Je,"href","#prtraitement-dun-jeu-de-donnes"),D(He,"class","relative group"),D(Ys,"href","/course/fr/chapter2"),D(Zs,"href","/course/fr/chapter2"),D(et,"href","/course/fr/chapter1"),D(st,"href","/course/fr/chapter2"),D(tt,"href","/course/fr/chapter2"),D(ws,"href","https://arrow.apache.org/"),D(ws,"rel","nofollow"),D(ys,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),D(ys,"rel","nofollow"),D(Xe,"href","https://github.com/huggingface/tokenizers"),D(Xe,"rel","nofollow"),D(ts,"id","padding-dynamique"),D(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ts,"href","#padding-dynamique"),D(Ie,"class","relative group")},m(e,p){s(document.head,i),c(e,d,p),w(u,e,p),c(e,_,p),c(e,g,p),s(g,v),s(v,b),w($,b,null),s(g,h),s(g,k),s(k,S),c(e,I,p),Os[O].m(e,p),c(e,A,p),Ls[F].m(e,p),c(e,H,p),c(e,E,p),s(E,ae),c(e,Q,p),c(e,W,p),s(W,oe),s(W,P),s(P,V),s(W,Bs),s(W,Ce),s(Ce,Gs),s(W,Js),c(e,us,p),c(e,pe,p),s(pe,We),s(We,bt),w(ps,bt,null),s(pe,Mo),s(pe,qt),s(qt,So),c(e,ba,p),Fs[ge].m(e,p),c(e,Qs,p),c(e,K,p),s(K,Ao),s(K,jt),s(jt,No),s(K,Oo),s(K,cs),s(cs,Lo),s(K,Fo),s(K,ds),s(ds,Ro),s(K,Ho),s(K,Ue),s(Ue,gt),s(gt,Io),s(Ue,Vo),s(K,Wo),s(K,kt),s(kt,Uo),s(K,Bo),c(e,qa,p),c(e,De,p),s(De,Go),s(De,Et),s(Et,Jo),s(De,Qo),s(De,$t),s($t,Ko),s(De,Yo),c(e,ja,p),w(ms,e,p),c(e,ga,p),w(fs,e,p),c(e,ka,p),c(e,Y,p),s(Y,Zo),s(Y,xt),s(xt,Xo),s(Y,el),s(Y,zt),s(zt,sl),s(Y,tl),s(Y,wt),s(wt,nl),s(Y,al),s(Y,yt),s(yt,ol),s(Y,ll),s(Y,Ct),s(Ct,rl),s(Y,il),c(e,Ea,p),c(e,Pe,p),s(Pe,ul),s(Pe,Dt),s(Dt,pl),s(Pe,cl),s(Pe,Pt),s(Pt,dl),s(Pe,ml),c(e,$a,p),c(e,Be,p),s(Be,fl),s(Be,Tt),s(Tt,hl),s(Be,_l),c(e,xa,p),w(hs,e,p),c(e,za,p),w(_s,e,p),c(e,wa,p),c(e,Te,p),s(Te,vl),s(Te,Mt),s(Mt,bl),s(Te,ql),s(Te,St),s(St,jl),s(Te,gl),c(e,ya,p),w(vs,e,p),c(e,Ca,p),w(bs,e,p),c(e,Da,p),c(e,U,p),s(U,kl),s(U,At),s(At,El),s(U,$l),s(U,Nt),s(Nt,xl),s(U,zl),s(U,Ot),s(Ot,wl),s(U,yl),s(U,Lt),s(Lt,Cl),s(U,Dl),s(U,Ft),s(Ft,Pl),s(U,Tl),s(U,Rt),s(Rt,Ml),s(U,Sl),s(U,Ht),s(Ht,Al),s(U,Nl),c(e,Pa,p),w(Ge,e,p),c(e,Ta,p),c(e,He,p),s(He,Je),s(Je,It),w(qs,It,null),s(He,Ol),s(He,Vt),s(Vt,Ll),c(e,Ma,p),Rs[Ee].m(e,p),c(e,Ks,p),c(e,ce,p),s(ce,Fl),s(ce,Ys),s(Ys,Rl),s(ce,Hl),s(ce,Wt),s(Wt,Il),s(ce,Vl),s(ce,Ut),s(Ut,Wl),s(ce,Ul),c(e,Sa,p),w(js,e,p),c(e,Aa,p),c(e,Qe,p),s(Qe,Bl),s(Qe,Bt),s(Bt,Gl),s(Qe,Jl),c(e,Na,p),w(gs,e,p),c(e,Oa,p),w(ks,e,p),c(e,La,p),c(e,le,p),s(le,Ql),s(le,Gt),s(Gt,Kl),s(le,Yl),s(le,Jt),s(Jt,Zl),s(le,Xl),s(le,Zs),s(Zs,er),s(le,sr),s(le,Qt),s(Qt,tr),s(le,nr),c(e,Fa,p),w(Ke,e,p),c(e,Ra,p),c(e,Ye,p),s(Ye,ar),s(Ye,Kt),s(Kt,or),s(Ye,lr),c(e,Ha,p),w(Es,e,p),c(e,Ia,p),c(e,Xs,p),s(Xs,rr),c(e,Va,p),w($s,e,p),c(e,Wa,p),c(e,Me,p),s(Me,ir),s(Me,Yt),s(Yt,ur),s(Me,pr),s(Me,Zt),s(Zt,cr),s(Me,dr),c(e,Ua,p),w(xs,e,p),c(e,Ba,p),c(e,J,p),s(J,mr),s(J,Xt),s(Xt,fr),s(J,hr),s(J,en),s(en,_r),s(J,vr),s(J,sn),s(sn,br),s(J,qr),s(J,tn),s(tn,jr),s(J,gr),s(J,nn),s(nn,kr),s(J,Er),s(J,an),s(an,$r),s(J,xr),c(e,Ga,p),c(e,Se,p),s(Se,zr),s(Se,on),s(on,wr),s(Se,yr),s(Se,ln),s(ln,Cr),s(Se,Dr),c(e,Ja,p),c(e,de,p),s(de,Pr),s(de,rn),s(rn,Tr),s(de,Mr),s(de,et),s(et,Sr),s(de,Ar),s(de,un),s(un,Nr),s(de,Or),c(e,Qa,p),c(e,Ze,p),s(Ze,Lr),s(Ze,pn),s(pn,Fr),s(Ze,Rr),c(e,Ka,p),c(e,re,p),s(re,Hr),s(re,cn),s(cn,Ir),s(re,Vr),s(re,dn),s(dn,Wr),s(re,Ur),s(re,mn),s(mn,Br),s(re,Gr),s(re,fn),s(fn,Jr),s(re,Qr),c(e,Ya,p),c(e,ie,p),s(ie,Kr),s(ie,hn),s(hn,Yr),s(ie,Zr),s(ie,st),s(st,Xr),s(ie,ei),s(ie,_n),s(_n,si),s(ie,ti),s(ie,tt),s(tt,ni),s(ie,ai),c(e,Za,p),w(zs,e,p),c(e,Xa,p),c(e,Z,p),s(Z,oi),s(Z,vn),s(vn,li),s(Z,ri),s(Z,bn),s(bn,ii),s(Z,ui),s(Z,qn),s(qn,pi),s(Z,ci),s(Z,jn),s(jn,di),s(Z,mi),s(Z,ws),s(ws,fi),s(Z,hi),c(e,eo,p),c(e,Ae,p),s(Ae,_i),s(Ae,ys),s(ys,gn),s(gn,vi),s(Ae,bi),s(Ae,kn),s(kn,qi),s(Ae,ji),c(e,so,p),w(Cs,e,p),c(e,to,p),c(e,T,p),s(T,gi),s(T,En),s(En,ki),s(T,Ei),s(T,$n),s($n,$i),s(T,xi),s(T,xn),s(xn,zi),s(T,wi),s(T,zn),s(zn,yi),s(T,Ci),s(T,wn),s(wn,Di),s(T,Pi),s(T,yn),s(yn,Ti),s(T,Mi),s(T,Cn),s(Cn,Si),s(T,Ai),s(T,Dn),s(Dn,Ni),s(T,Oi),s(T,Pn),s(Pn,Li),s(T,Fi),s(T,Xe),s(Xe,Ri),s(Xe,Tn),s(Tn,Hi),s(T,Ii),s(T,Mn),s(Mn,Vi),s(T,Wi),c(e,no,p),c(e,me,p),s(me,Ui),s(me,Sn),s(Sn,Bi),s(me,Gi),s(me,An),s(An,Ji),s(me,Qi),s(me,Nn),s(Nn,Ki),s(me,Yi),c(e,ao,p),c(e,Ne,p),s(Ne,Zi),s(Ne,On),s(On,Xi),s(Ne,eu),s(Ne,Ln),s(Ln,su),s(Ne,tu),c(e,oo,p),w(Ds,e,p),c(e,lo,p),c(e,es,p),s(es,nu),s(es,Fn),s(Fn,au),s(es,ou),c(e,ro,p),w(Ps,e,p),c(e,io,p),c(e,X,p),s(X,lu),s(X,Rn),s(Rn,ru),s(X,iu),s(X,Hn),s(Hn,uu),s(X,pu),s(X,In),s(In,cu),s(X,du),s(X,Vn),s(Vn,mu),s(X,fu),s(X,Wn),s(Wn,hu),s(X,_u),c(e,uo,p),c(e,ee,p),s(ee,vu),s(ee,Un),s(Un,bu),s(ee,qu),s(ee,Bn),s(Bn,ju),s(ee,gu),s(ee,Gn),s(Gn,ku),s(ee,Eu),s(ee,Jn),s(Jn,$u),s(ee,xu),s(ee,Qn),s(Qn,zu),s(ee,wu),c(e,po,p),c(e,ss,p),s(ss,yu),s(ss,Kn),s(Kn,Cu),s(ss,Du),c(e,co,p),c(e,Ie,p),s(Ie,ts),s(ts,Yn),w(Ts,Yn,null),s(Ie,Pu),s(Ie,Zn),s(Zn,Tu),c(e,mo,p),w(Ms,e,p),c(e,fo,p),Ve.m(e,p),c(e,nt,p),c(e,B,p),s(B,Mu),s(B,Xn),s(Xn,Su),s(B,Au),s(B,ea),s(ea,Nu),s(B,Ou),s(B,sa),s(sa,Lu),s(B,Fu),s(B,ta),s(ta,Ru),s(B,Hu),s(B,na),s(na,Iu),s(B,Vu),s(B,aa),s(aa,Wu),s(B,Uu),s(B,oa),s(oa,Bu),s(B,Gu),c(e,ho,p),Hs[xe].m(e,p),c(e,at,p),c(e,se,p),s(se,Ju),s(se,la),s(la,Qu),s(se,Ku),s(se,ra),s(ra,Yu),s(se,Zu),s(se,ia),s(ia,Xu),s(se,ep),s(se,ua),s(ua,sp),s(se,tp),s(se,pa),s(pa,np),s(se,ap),c(e,_o,p),w(Ss,e,p),c(e,vo,p),w(As,e,p),c(e,bo,p),c(e,fe,p),s(fe,op),s(fe,ca),s(ca,lp),s(fe,rp),s(fe,da),s(da,ip),s(fe,up),s(fe,ma),s(ma,pp),s(fe,cp),c(e,qo,p),w(Ns,e,p),c(e,jo,p),Is[we].m(e,p),c(e,ot,p),w(ns,e,p),c(e,go,p),G&&G.m(e,p),c(e,lt,p),ko=!0},p(e,[p]){const Vs={};p&1&&(Vs.fw=e[0]),u.$set(Vs);let rt=O;O=_p(e),O!==rt&&(Us(),j(Os[rt],1,1,()=>{Os[rt]=null}),Ws(),L=Os[O],L||(L=Os[O]=hp[O](e),L.c()),q(L,1),L.m(A.parentNode,A));let it=F;F=bp(e),F!==it&&(Us(),j(Ls[it],1,1,()=>{Ls[it]=null}),Ws(),R=Ls[F],R||(R=Ls[F]=vp[F](e),R.c()),q(R,1),R.m(H.parentNode,H));let ut=ge;ge=jp(e),ge!==ut&&(Us(),j(Fs[ut],1,1,()=>{Fs[ut]=null}),Ws(),ke=Fs[ge],ke||(ke=Fs[ge]=qp[ge](e),ke.c()),q(ke,1),ke.m(Qs.parentNode,Qs));const fa={};p&2&&(fa.$$scope={dirty:p,ctx:e}),Ge.$set(fa);let Oe=Ee;Ee=kp(e),Ee!==Oe&&(Us(),j(Rs[Oe],1,1,()=>{Rs[Oe]=null}),Ws(),$e=Rs[Ee],$e||($e=Rs[Ee]=gp[Ee](e),$e.c()),q($e,1),$e.m(Ks.parentNode,Ks));const ha={};p&2&&(ha.$$scope={dirty:p,ctx:e}),Ke.$set(ha),Eo!==(Eo=Ep(e))&&(Ve.d(1),Ve=Eo(e),Ve&&(Ve.c(),Ve.m(nt.parentNode,nt)));let pt=xe;xe=xp(e),xe!==pt&&(Us(),j(Hs[pt],1,1,()=>{Hs[pt]=null}),Ws(),ze=Hs[xe],ze||(ze=Hs[xe]=$p[xe](e),ze.c()),q(ze,1),ze.m(at.parentNode,at));let as=we;we=wp(e),we!==as&&(Us(),j(Is[as],1,1,()=>{Is[as]=null}),Ws(),ye=Is[we],ye||(ye=Is[we]=zp[we](e),ye.c()),q(ye,1),ye.m(ot.parentNode,ot));const _a={};p&2&&(_a.$$scope={dirty:p,ctx:e}),ns.$set(_a),e[0]==="tf"?G?p&1&&q(G,1):(G=Nd(),G.c(),q(G,1),G.m(lt.parentNode,lt)):G&&(Us(),j(G,1,1,()=>{G=null}),Ws())},i(e){ko||(q(u.$$.fragment,e),q($.$$.fragment,e),q(L),q(R),q(ps.$$.fragment,e),q(ke),q(ms.$$.fragment,e),q(fs.$$.fragment,e),q(hs.$$.fragment,e),q(_s.$$.fragment,e),q(vs.$$.fragment,e),q(bs.$$.fragment,e),q(Ge.$$.fragment,e),q(qs.$$.fragment,e),q($e),q(js.$$.fragment,e),q(gs.$$.fragment,e),q(ks.$$.fragment,e),q(Ke.$$.fragment,e),q(Es.$$.fragment,e),q($s.$$.fragment,e),q(xs.$$.fragment,e),q(zs.$$.fragment,e),q(Cs.$$.fragment,e),q(Ds.$$.fragment,e),q(Ps.$$.fragment,e),q(Ts.$$.fragment,e),q(Ms.$$.fragment,e),q(ze),q(Ss.$$.fragment,e),q(As.$$.fragment,e),q(Ns.$$.fragment,e),q(ye),q(ns.$$.fragment,e),q(G),ko=!0)},o(e){j(u.$$.fragment,e),j($.$$.fragment,e),j(L),j(R),j(ps.$$.fragment,e),j(ke),j(ms.$$.fragment,e),j(fs.$$.fragment,e),j(hs.$$.fragment,e),j(_s.$$.fragment,e),j(vs.$$.fragment,e),j(bs.$$.fragment,e),j(Ge.$$.fragment,e),j(qs.$$.fragment,e),j($e),j(js.$$.fragment,e),j(gs.$$.fragment,e),j(ks.$$.fragment,e),j(Ke.$$.fragment,e),j(Es.$$.fragment,e),j($s.$$.fragment,e),j(xs.$$.fragment,e),j(zs.$$.fragment,e),j(Cs.$$.fragment,e),j(Ds.$$.fragment,e),j(Ps.$$.fragment,e),j(Ts.$$.fragment,e),j(Ms.$$.fragment,e),j(ze),j(Ss.$$.fragment,e),j(As.$$.fragment,e),j(Ns.$$.fragment,e),j(ye),j(ns.$$.fragment,e),j(G),ko=!1},d(e){t(i),e&&t(d),y(u,e),e&&t(_),e&&t(g),y($),e&&t(I),Os[O].d(e),e&&t(A),Ls[F].d(e),e&&t(H),e&&t(E),e&&t(Q),e&&t(W),e&&t(us),e&&t(pe),y(ps),e&&t(ba),Fs[ge].d(e),e&&t(Qs),e&&t(K),e&&t(qa),e&&t(De),e&&t(ja),y(ms,e),e&&t(ga),y(fs,e),e&&t(ka),e&&t(Y),e&&t(Ea),e&&t(Pe),e&&t($a),e&&t(Be),e&&t(xa),y(hs,e),e&&t(za),y(_s,e),e&&t(wa),e&&t(Te),e&&t(ya),y(vs,e),e&&t(Ca),y(bs,e),e&&t(Da),e&&t(U),e&&t(Pa),y(Ge,e),e&&t(Ta),e&&t(He),y(qs),e&&t(Ma),Rs[Ee].d(e),e&&t(Ks),e&&t(ce),e&&t(Sa),y(js,e),e&&t(Aa),e&&t(Qe),e&&t(Na),y(gs,e),e&&t(Oa),y(ks,e),e&&t(La),e&&t(le),e&&t(Fa),y(Ke,e),e&&t(Ra),e&&t(Ye),e&&t(Ha),y(Es,e),e&&t(Ia),e&&t(Xs),e&&t(Va),y($s,e),e&&t(Wa),e&&t(Me),e&&t(Ua),y(xs,e),e&&t(Ba),e&&t(J),e&&t(Ga),e&&t(Se),e&&t(Ja),e&&t(de),e&&t(Qa),e&&t(Ze),e&&t(Ka),e&&t(re),e&&t(Ya),e&&t(ie),e&&t(Za),y(zs,e),e&&t(Xa),e&&t(Z),e&&t(eo),e&&t(Ae),e&&t(so),y(Cs,e),e&&t(to),e&&t(T),e&&t(no),e&&t(me),e&&t(ao),e&&t(Ne),e&&t(oo),y(Ds,e),e&&t(lo),e&&t(es),e&&t(ro),y(Ps,e),e&&t(io),e&&t(X),e&&t(uo),e&&t(ee),e&&t(po),e&&t(ss),e&&t(co),e&&t(Ie),y(Ts),e&&t(mo),y(Ms,e),e&&t(fo),Ve.d(e),e&&t(nt),e&&t(B),e&&t(ho),Hs[xe].d(e),e&&t(at),e&&t(se),e&&t(_o),y(Ss,e),e&&t(vo),y(As,e),e&&t(bo),e&&t(fe),e&&t(qo),y(Ns,e),e&&t(jo),Is[we].d(e),e&&t(ot),y(ns,e),e&&t(go),G&&G.d(e),e&&t(lt)}}}const im={local:"prparer-les-donnes",sections:[{local:"charger-un-jeu-de-donnes-depuis-le-hub",title:"Charger un jeu de donn\xE9es depuis le *Hub*"},{local:"prtraitement-dun-jeu-de-donnes",title:"Pr\xE9traitement d'un jeu de donn\xE9es"},{local:"padding-dynamique",title:"*Padding* dynamique"}],title:"Pr\xE9parer les donn\xE9es"};function um(C,i,d){let u="pt";return Id(()=>{const _=new URLSearchParams(window.location.search);d(0,u=_.get("fw")||"pt")}),[u]}class vm extends Ld{constructor(i){super();Fd(this,i,um,rm,Rd,{})}}export{vm as default,im as metadata};
