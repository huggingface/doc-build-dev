import{S as Jc,i as Wc,s as Zc,e as a,k as u,w as v,t as l,M as Kc,c as r,d as t,m as p,a as o,x as h,h as n,b as c,N as m,G as i,g as d,y as g,L as eu,q as _,o as E,B as z,v as iu}from"../../chunks/vendor-hf-doc-builder.js";import{Y as ws}from"../../chunks/Youtube-hf-doc-builder.js";import{I as S}from"../../chunks/IconCopyLink-hf-doc-builder.js";function tu(ys){let R,Ea,x,ie,et,$e,ao,it,ro,za,gi,oo,ba,D,te,tt,Ie,lo,at,no,Ta,_i,so,ka,B,Pe,As,co,we,Ls,qa,ae,uo,ye,po,mo,$a,f,rt,re,ot,fo,vo,Ae,ho,go,_o,lt,oe,nt,Eo,zo,Le,bo,To,ko,st,le,dt,qo,$o,Me,Io,Po,wo,ct,ne,ut,yo,Ao,Ne,Lo,Mo,No,pt,q,mt,Go,So,Ge,Ro,xo,Se,Do,Bo,Oo,ft,$,vt,Co,Uo,Re,Qo,Ho,ht,jo,Vo,Ia,Ei,Yo,Pa,I,xe,Xo,gt,Fo,Jo,Wo,De,Zo,_t,Ko,el,il,Be,tl,Et,al,rl,wa,zi,ol,ya,O,se,zt,Oe,ll,bt,nl,Aa,P,sl,Tt,dl,cl,kt,ul,pl,La,de,ml,qt,fl,vl,Ma,w,hl,$t,gl,_l,It,El,zl,Na,C,Ce,Ms,bl,Ue,Ns,Ga,ce,Tl,Pt,kl,ql,Sa,U,Qe,Gs,$l,He,Ss,Ra,Q,ue,wt,je,Il,yt,Pl,xa,bi,wl,Da,Ve,Ye,Rs,Ba,Ti,yl,Oa,H,Xe,xs,Al,Fe,Ds,Ca,Je,Ua,ki,Ll,Qa,qi,Ml,Ha,$i,Nl,ja,j,pe,At,We,Gl,Lt,Sl,Va,Ze,Ya,Ii,Rl,Xa,V,Ke,Bs,xl,ei,Os,Fa,Pi,Dl,Ja,y,Bl,Mt,Ol,Cl,Nt,Ul,Ql,Wa,A,Gt,Hl,jl,St,Vl,Yl,Rt,Xl,Za,me,Fl,xt,Jl,Wl,Ka,Y,ii,Cs,Zl,ti,Us,er,wi,Kl,ir,yi,en,tr,X,fe,Dt,ai,tn,Bt,an,ar,Ai,rn,rr,ri,or,F,ve,Ot,oi,on,Ct,ln,lr,Li,nn,nr,he,Mi,Ut,sn,dn,cn,Ni,Qt,un,pn,sr,J,li,Qs,mn,ni,Hs,dr,Gi,fn,cr,L,Si,Ht,vn,hn,gn,Ri,jt,_n,En,zn,ge,Vt,bn,Tn,Yt,kn,qn,ur,xi,$n,pr,W,_e,Xt,si,In,Ft,Pn,mr,M,wn,Jt,yn,An,di,Ln,Mn,fr,Di,Nn,vr,Bi,Gn,hr,Oi,Sn,gr,Z,Ee,Wt,ci,Rn,Zt,xn,_r,Ci,Dn,Er,Ui,Bn,zr,Qi,On,br,K,ui,js,Cn,pi,Vs,Tr,ze,Un,Kt,Qn,Hn,kr,be,jn,ea,Vn,Yn,qr,ee,Te,ia,mi,Xn,ta,Fn,$r,b,Jn,aa,Wn,Zn,ra,Kn,es,oa,is,ts,Ir,N,Hi,la,as,rs,os,ji,na,ls,ns,ss,G,sa,ds,cs,da,us,ps,ca,ms,fs,Pr,T,vs,ua,hs,gs,pa,_s,Es,ma,zs,bs,wr;return $e=new S({}),Ie=new S({}),Oe=new S({}),je=new S({}),Je=new ws({props:{id:"ftWlj4FBHTg"}}),We=new S({}),Ze=new ws({props:{id:"BqqfQnyjmgg"}}),ai=new S({}),ri=new ws({props:{id:"H39Z_720T5s"}}),oi=new S({}),si=new S({}),ci=new S({}),mi=new S({}),{c(){R=a("meta"),Ea=u(),x=a("h1"),ie=a("a"),et=a("span"),v($e.$$.fragment),ao=u(),it=a("span"),ro=l("Come funzionano i Transformer?"),za=u(),gi=a("p"),oo=l("In questa sezione, vedremo in maniera approfondita l\u2019architettura dei modelli Transformer."),ba=u(),D=a("h2"),te=a("a"),tt=a("span"),v(Ie.$$.fragment),lo=u(),at=a("span"),no=l("Un po' di storia dei Transformer"),Ta=u(),_i=a("p"),so=l("Ecco alcuni punti di riferimento nella (breve) storia dei modelli Transformer:"),ka=u(),B=a("div"),Pe=a("img"),co=u(),we=a("img"),qa=u(),ae=a("p"),uo=l("L\u2019"),ye=a("a"),po=l("architettura Transformer"),mo=l(" \xE8 stata introdotta in giugno 2017. Il focus della ricerca di partenza era sui compiti di traduzione. A questa segu\xEC l\u2019introduzione di numerosi modelli influenti, tra cui figurano:"),$a=u(),f=a("ul"),rt=a("li"),re=a("p"),ot=a("strong"),fo=l("giugno 2018"),vo=l(": "),Ae=a("a"),ho=l("GPT"),go=l(", il primo modello Transformer pre-addestrato, viene usato per affinare diversi compiti di NLP e ottiene risultati all\u2019avanguardia"),_o=u(),lt=a("li"),oe=a("p"),nt=a("strong"),Eo=l("ottobre 2018"),zo=l(": "),Le=a("a"),bo=l("BERT"),To=l(", un altro ampio modello pre-addestrato, questa volta progettato per produrre riassunti di frasi migliori (ne scopriremo di pi\xF9 nel prossimo capitolo!)"),ko=u(),st=a("li"),le=a("p"),dt=a("strong"),qo=l("febbraio 2019"),$o=l(": "),Me=a("a"),Io=l("GPT-2"),Po=l(", una versione (migliorata e ingrandita) di GPT che non fu distribuita immediatamente al pubblico a causa di preoccupazioni etiche"),wo=u(),ct=a("li"),ne=a("p"),ut=a("strong"),yo=l("ottobre 2019"),Ao=l(": "),Ne=a("a"),Lo=l("DistilBERT"),Mo=l(", una versione distillata di BERT che \xE8 il 60% pi\xF9 rapida e il 40% pi\xF9 leggera in memoria, pur conservando il 97% della performance di BERT"),No=u(),pt=a("li"),q=a("p"),mt=a("strong"),Go=l("ottobre 2019"),So=l(": "),Ge=a("a"),Ro=l("BART"),xo=l(" e "),Se=a("a"),Do=l("T5"),Bo=l(", due grossi modelli pre-addestrati che utilizzano la stessa architettura del modello Transformer originale (nonch\xE9 i primi a farlo)"),Oo=u(),ft=a("li"),$=a("p"),vt=a("strong"),Co=l("maggio 2020"),Uo=l(", "),Re=a("a"),Qo=l("GPT-3"),Ho=l(", una versione ancora pi\xF9 ampia di GPT-2, con buone prestazioni in vari compiti e nessun bisogno di fine-tuning (il cosiddetto "),ht=a("em"),jo=l("zero-shot learning"),Vo=l(")"),Ia=u(),Ei=a("p"),Yo=l("La lista \xE8 tutto fuorch\xE9 esaustiva ed \xE8 volta solo a mettere in evidenza alcuni dei diversi tipi di modelli Transformer. In genere, questi possono essere raggruppati in tre categorie:"),Pa=u(),I=a("ul"),xe=a("li"),Xo=l("Modelli in stile GPT (detti anche modelli Transformer "),gt=a("em"),Fo=l("auto-regressive"),Jo=l(")"),Wo=u(),De=a("li"),Zo=l("Modelli in stile BERT (detti anche modelli Transformer "),_t=a("em"),Ko=l("auto-encoding"),el=l(")"),il=u(),Be=a("li"),tl=l("Modelli in stile BART/T5 (detti anche modelli Transformer "),Et=a("em"),al=l("sequence-to-sequence"),rl=l(")"),wa=u(),zi=a("p"),ol=l("Studieremo queste famiglie pi\xF9 nel dettaglio in seguito."),ya=u(),O=a("h2"),se=a("a"),zt=a("span"),v(Oe.$$.fragment),ll=u(),bt=a("span"),nl=l("I Transformer sono modelli linguistici"),Aa=u(),P=a("p"),sl=l("Tutti i modelli Transformer menzionati qui sopra (GPT, BERT, BART, T5, ecc.) sono stati addestrati come modelli linguistici ("),Tt=a("em"),dl=l("language models"),cl=l("). Ci\xF2 significa che sono stati addestrati su grandi quantit\xE0 di testo grezzo in stile auto-supervisionato ("),kt=a("em"),ul=l("self-supervising"),pl=l("). L\u2019apprendimento auto-supervisionato \xE8 un tipo di apprendimento il cui obbiettivo viene computato direttamente dagli input del modello. Ci\xF2 significa che non \xE8 richiesto alcun intervento umano per etichettare i dati!"),La=u(),de=a("p"),ml=l("Un modello di questo tipo sviluppa una comprensione statistica della lingua alla quale \xE8 stato addestrato, ma non \xE8 molto utile in compiti pratici e precisi. Per questa ragione, il modello pre-addestrato generale viene in seguito sottoposto a un processo detto "),qt=a("em"),fl=l("transfer learning"),vl=l(". Durante questo processo, il modello viene affinato per un determinato compito in maniera supervisionata (ossia utilizzando etichette generate da umani)."),Ma=u(),w=a("p"),hl=l("Un esempio di compito \xE8 la previsione della parola seguente in una frase di cui sono state lette "),$t=a("em"),gl=l("n"),_l=l(" parole precedenti. Quest\u2019operazione si chiama "),It=a("em"),El=l("causal language modeling"),zl=l(" perch\xE9 il suo output dipende dagli input presenti e passati, ma non da quelli futuri."),Na=u(),C=a("div"),Ce=a("img"),bl=u(),Ue=a("img"),Ga=u(),ce=a("p"),Tl=l("Un altro esempio \xE8 il "),Pt=a("em"),kl=l("masked language modeling"),ql=l(", in cui il modello prevede una parola occultata della frase."),Sa=u(),U=a("div"),Qe=a("img"),$l=u(),He=a("img"),Ra=u(),Q=a("h2"),ue=a("a"),wt=a("span"),v(je.$$.fragment),Il=u(),yt=a("span"),Pl=l("I Transformers sono modelli enormi"),xa=u(),bi=a("p"),wl=l("A parte per alcune eccezioni (come DistilBERT), la strategia generale per ottenere performance migliori consiste nell\u2019aumentare la taglia dei modelli, nonch\xE9 la quantit\xE0 di dati utilizzati per il pre-addestramento."),Da=u(),Ve=a("div"),Ye=a("img"),Ba=u(),Ti=a("p"),yl=l("Sfortunatamente, l\u2019addestramento di un modello, e specialmente di un modello grosso, richiede grandi quantit\xE0 di dati. Ci\xF2 si rivela molto costoso in termini di tempo, risorse informatiche e impatto ambientale, come mostrano i grafici qui sotto."),Oa=u(),H=a("div"),Xe=a("img"),Al=u(),Fe=a("img"),Ca=u(),v(Je.$$.fragment),Ua=u(),ki=a("p"),Ll=l("Questi dati si riferiscono a un progetto per un modello (molto grande) condotto da un team che provava consciamente a ridurre l\u2019impatto ambientale del pre-addestramento. L\u2019impronta di trials volti a ottenere i miglior iperparamenti possibili sarebbe ancora pi\xF9 importante."),Qa=u(),qi=a("p"),Ml=l("Immagina cosa succederebbe se ogni volta che un gruppo di ricerca, un\u2019organizzazione studentesca o un\u2019azienda vuole addestrare un modello lo facesse da zero! I costi globali sarebbero inutilmente enormi!"),Ha=u(),$i=a("p"),Nl=l("Questo \xE8 il motivo per cui la condivisione di modelli linguistici \xE8 fondamentale: lavorare a partire da modelli gi\xE0 addestrati riduce i costi informatici complessivi e l\u2019impatto ambientale della comunit\xE0."),ja=u(),j=a("h2"),pe=a("a"),At=a("span"),v(We.$$.fragment),Gl=u(),Lt=a("span"),Sl=l("Transfer Learning"),Va=u(),v(Ze.$$.fragment),Ya=u(),Ii=a("p"),Rl=l("Il pre-addestramento \xE8 l\u2019atto di addestrare un modello da zero: i pesi sono inizializzati in maniera casuale, e l\u2019addestramento inizia senza alcuna conoscenza pregressa."),Xa=u(),V=a("div"),Ke=a("img"),xl=u(),ei=a("img"),Fa=u(),Pi=a("p"),Dl=l("Questo pre-addestramento \xE8 solitamente fatto su enormi quantit\xE0 di dati. Di conseguenza, l\u2019addestramento richiede un corpus di dati molto ampio e pu\xF2 prendere diverse settimane."),Ja=u(),y=a("p"),Bl=l("L\u2019affinamento ("),Mt=a("em"),Ol=l("fine-tuning"),Cl=l("), al contrario, \xE8 un addestramento che ha luogo "),Nt=a("strong"),Ul=l("dopo"),Ql=l(" che il modello \xE8 stato pre-addestrato. Per poter effettuare un fine-tuning, \xE8 necessario acquisire un modello linguistico pre-addestrato e addestrarlo ulteriormente con una base dati adatta al compito in questione. Ma perch\xE9 non addestrare direttamente al compito finale? Esistono alcune ragioni:"),Wa=u(),A=a("ul"),Gt=a("li"),Hl=l("Il modello pre-addestrato \xE8 gi\xE0 addestrato su basi dati che contengono similarit\xE0 con la base dati usata per il fine-tuning. Il processo di fine-tuning riesce quindi ad beneficiare della conoscenza acquisita dal modello iniziale durante il pre-addestramento (ad esempio, nei problemi di NLP, il modello pre-addestrato avr\xE0 gi\xE0 conoscenze statistiche della lingua utilizzata nel compito)."),jl=u(),St=a("li"),Vl=l("Siccome il modello pre-addestrato \xE8 stato addestrato usando moltissimi dati, il fine-tuning richiede molto meno dati per ottenere buoni risultati."),Yl=u(),Rt=a("li"),Xl=l("Per la stessa ragione, occorrono molto meno tempo e risorse per ottenere buoni risultati."),Za=u(),me=a("p"),Fl=l("Ad esempio, \xE8 possibile approfittare di un modello pre-addestrato per la lingua inglese e poi affinarlo usando un corpus arXiv, ottenendo cos\xEC un modello specifico per la scienza/ricerca. L\u2019affinamento non richieder\xE0 che una quantit\xE0 limitata di dati: le conoscenze acquisite dal modello pre-addestrato sono \u201Ctrasferite\u201D, come riflette il nome "),xt=a("em"),Jl=l("transfer learning"),Wl=l("."),Ka=u(),Y=a("div"),ii=a("img"),Zl=u(),ti=a("img"),er=u(),wi=a("p"),Kl=l("Il fine-tuning di un modello ha quindi costi ridotti in termini di dati, finanze e impatto ambientale. Iterare su diversi schemi di fine-tuning \xE8 anche pi\xF9 rapido e semplice, in quanto l\u2019addestramento \xE8 meno restrittivo di un pre-addestramento completo."),ir=u(),yi=a("p"),en=l("Questo processo permette anche di ottenere risultati migliori di un addestramento da zero (a meno di non essere in possesso di moltissimi dati), motivo per cui bisognerebbe sempre partire da un modello pre-addestrato (quanto possibile compatibile con il compito da eseguire) e affinarlo."),tr=u(),X=a("h2"),fe=a("a"),Dt=a("span"),v(ai.$$.fragment),tn=u(),Bt=a("span"),an=l("Architettura generale"),ar=u(),Ai=a("p"),rn=l("In questa sezione, vedremo l\u2019architettura generale del modello Transformer. Non preoccuparti se non capisci tutti i concetti: pi\xF9 avanti, troverai sezioni dettagliate per ogni componente."),rr=u(),v(ri.$$.fragment),or=u(),F=a("h2"),ve=a("a"),Ot=a("span"),v(oi.$$.fragment),on=u(),Ct=a("span"),ln=l("Introduzione"),lr=u(),Li=a("p"),nn=l("Il modello si compone principalmente di due blocchi:"),nr=u(),he=a("ul"),Mi=a("li"),Ut=a("strong"),sn=l("Encoder (sinistra)"),dn=l(": L\u2019encoder riceve un input e ne costruisce una rappresentazione, le features. Ci\xF2 significa che il modello \xE8 ottimizzato per la comprensione dell\u2019input."),cn=u(),Ni=a("li"),Qt=a("strong"),un=l("Decoder (destra)"),pn=l(": Il decoder utilizza la rappresentazione dell\u2019encoder (le features) assieme ad ulteriori input per generare la sequenza target. Ci\xF2 significa che il modello \xE8 ottimizzato per la generazione di output."),sr=u(),J=a("div"),li=a("img"),mn=u(),ni=a("img"),dr=u(),Gi=a("p"),fn=l("Ognuna di queste parti pu\xF2 essere utilizzata indipendentemente, in base al compito:"),cr=u(),L=a("ul"),Si=a("li"),Ht=a("strong"),vn=l("Modelli Encoder-only"),hn=l(": Ottimi per compiti che richiedono una comprensione dell\u2019input, come la classificazione frasale e il riconoscimento delle entit\xE0 nominate."),gn=u(),Ri=a("li"),jt=a("strong"),_n=l("Modelli Decoder-only"),En=l(": Ottimi per compiti generativi come la generazione testuale."),zn=u(),ge=a("li"),Vt=a("strong"),bn=l("Modelli Encoder-decoder"),Tn=l(" o "),Yt=a("strong"),kn=l("modelli sequence-to-sequence"),qn=l(": Ottimi per compiti generativi che richiedono un input, come la traduzione o il riassunto."),ur=u(),xi=a("p"),$n=l("Analizzeremo ciascuna di queste architetture indipendentemente pi\xF9 tardi nel corso."),pr=u(),W=a("h2"),_e=a("a"),Xt=a("span"),v(si.$$.fragment),In=u(),Ft=a("span"),Pn=l("Attention layers"),mr=u(),M=a("p"),wn=l("Una caratteristica chiave dei modelli Transformer \xE8 che sono basati su strati speciali detti "),Jt=a("em"),yn=l("attention layers"),An=l(". Non a caso, il titolo del paper che introdusse l\u2019architettura Transformer era "),di=a("a"),Ln=l("\u201CAttention Is All You Need\u201D"),Mn=l("! Esploreremo gli attention layer nel dettaglio pi\xF9 avanti in questo corso; per ora, tutto ci\xF2 che hai bisogno di sapere \xE8 che un layer dir\xE0 al modello di prestare particolare attenzione a certe parole nella frase input (ignorando praticamente le altre) quando si occupa della rappresentazione delle singole parole."),fr=u(),Di=a("p"),Nn=l("Come esempio concreto, pensa ad un compito di traduzione testuale dall\u2019inglese al francese. Dato l\u2019input \u201CYou like this course\u201D, un modello di traduzione dovr\xE0 fare riferimento alla parola adiacente \u201CYou\u201D per fornire la traduzione corretta della parola \u201Clike\u201D, perch\xE9 in francese la coniugazione del verbo \u201Clike\u201D cambia in base al soggetto. Diversamente, il resto della frase non \xE8 utile alla sua traduzione di quella precisa parola. In maniera simile, durante la traduzione di \u201Cthis\u201D il modello dovr\xE0 prestare attenzione alla parola \u201Ccourse\u201D, in quanto \u201Cthis\u201D ha traduzioni diverse se associato con nomi femminili o maschili. Di nuovo, il resto delle parole della frase non contribuiscono alla corretta traduzione di \u201Cthis\u201D. Con frasi pi\xF9 complesse (e regole grammaticali pi\xF9 complesse), il modello potrebbe aver bisogno di prestare particolare attenzione a parole ben pi\xF9 lontane nella frase per tradurre correttamente ogni parola."),vr=u(),Bi=a("p"),Gn=l("Lo stesso concetto si applica a qualsiasi compito che ha a che fare con il linguaggio naturale: una parola ha un senso a s\xE9 stante, ma tale senso \xE8 profondamente influenzato dal contesto, il quale \xE8 costituito da una qualsiasi parola (o parole) che precede o segue la parola sotto osservazione."),hr=u(),Oi=a("p"),Sn=l("Ora che sai cosa sono gli attention layer, guardiamo un po\u2019 pi\xF9 nel dettaglio all\u2019architettura Transformer."),gr=u(),Z=a("h2"),Ee=a("a"),Wt=a("span"),v(ci.$$.fragment),Rn=u(),Zt=a("span"),xn=l("L'architettura originale"),_r=u(),Ci=a("p"),Dn=l("All\u2019origine, l\u2019architettura Transformer fu creata per la traduzione. In fase di addestramento, l\u2019encoder riceve degli input (frasi) in una certa lingua, mentre il decoder riceve le stesse frasi nella lingua target d\u2019elezione. Nell\u2019encoder, gli attention layer sono in grado di utilizzare qualsiasi parola in una data frase (dato che, come abbiamo appena visto, la traduzione di una determinata parola pu\xF2 dipendere da ci\xF2 che la precede o segue nella frase). Diversamente, decoder procede in maniera sequenziale ed \xE8 capace di prestare attenzione solo alle parole della frase che ha gi\xE0 tradotto (ossia, solo le parole che precedono la parola che sta generando). Ad esempio, una volta predette le prime tre parole della frase target, le passiamo al decoder  che utilizza tutti gli input dell\u2019encoder per provare a predirre la quarta parola."),Er=u(),Ui=a("p"),Bn=l("Per accelerare il processo di addestramento (quando il modello ha accesso alle frasi target), l\u2019intero target viene fornito al decoder, che per\xF2 non \xE8 in grado di accedere alle parole future (se avesse accesso alla parola in seconda posizione mentre cerca di predirre la parola in seconda posizione, il problema cesserebbe di essere complesso). Ad esempio, mentre prova a predirre la quarta parola, l\u2019attention layer avr\xE0 accesso solo alle posizioni tra la prima e la terza."),zr=u(),Qi=a("p"),On=l("L\u2019architettura Transformer originale aveva la struttura qui sotto, con l\u2019encoder a sinistra e il decoder a destra:"),br=u(),K=a("div"),ui=a("img"),Cn=u(),pi=a("img"),Tr=u(),ze=a("p"),Un=l("Nota che il primo attention layer in un "),Kt=a("em"),Qn=l("decoder block"),Hn=l(" presta attenzione a tutti gli input (passati) al decoder, mentre il secondo attention layer utilizza l\u2019output del encoder. Gli \xE8 perci\xF2 possibile avere accesso a tutta la frase input per meglio prevedere la parola corrente. Questa caratteristica \xE8 molto utile in quanto lingue diverse possono avere regole grammaticali diverse piazzano le parole in ordini diversi, oppure perch\xE9 il contesto che compare pi\xF9 tardi nella frase potrebbe essere utile nella determinazione della migliore traduzione di una data parola."),kr=u(),be=a("p"),jn=l("L\u2019"),ea=a("em"),Vn=l("attention mask"),Yn=l(" pu\xF2 essere utilizzato anche nell\u2019encoder/decoder per evitare che il modello presti attenzione a certe parole speciali, come ad esempio parole riempitive utilizzate per rendere tutti gli input della stessa lunghezza."),qr=u(),ee=a("h2"),Te=a("a"),ia=a("span"),v(mi.$$.fragment),Xn=u(),ta=a("span"),Fn=l("Architetture vs. checkpoint"),$r=u(),b=a("p"),Jn=l("Durante questo viaggio nel mondo dei modelli Transformer, incontrerai menzioni di "),aa=a("em"),Wn=l("architetture"),Zn=l(" e "),ra=a("em"),Kn=l("checkpoint"),es=l(", nonch\xE9 di "),oa=a("em"),is=l("modelli"),ts=l(". Questi termini hanno significati leggermente diversi:"),Ir=u(),N=a("ul"),Hi=a("li"),la=a("strong"),as=l("Architettura"),rs=l(": Lo scheletro del modello, ossia la definizione di ogni livello e operazione che compare nel modello."),os=u(),ji=a("li"),na=a("strong"),ls=l("Checkpoint"),ns=l(": I pesi che verranno caricati in una determinata architettura."),ss=u(),G=a("li"),sa=a("strong"),ds=l("Modello"),cs=l(": Un termine generico meno preciso di \u201Carchitettura\u201D o \u201Ccheckpoint\u201D, in quanto pu\xF2 significare entrambi. In questo corso faremo la distinzione tra "),da=a("em"),us=l("architettura"),ps=l(" e "),ca=a("em"),ms=l("checkpoint"),fs=l(" quando sar\xE0 necessario ridurre le ambiguit\xE0."),Pr=u(),T=a("p"),vs=l("Ad esempio, BERT \xE8 un\u2019architettura, mentre "),ua=a("code"),hs=l("bert-base-cased"),gs=l(", un set di pesi ("),pa=a("em"),_s=l("weights"),Es=l(") addestrati dal team di Google per la prima versione di BERT, \xE8 un checkpoint. Ciononostante, \xE8 possibile dire \u201Cil modello BERT\u201D e \u201Cil modello "),ma=a("code"),zs=l("bert-base-cased"),bs=l(".\u201D"),this.h()},l(e){const s=Kc('[data-svelte="svelte-1phssyn"]',document.head);R=r(s,"META",{name:!0,content:!0}),s.forEach(t),Ea=p(e),x=r(e,"H1",{class:!0});var yr=o(x);ie=r(yr,"A",{id:!0,class:!0,href:!0});var Ys=o(ie);et=r(Ys,"SPAN",{});var Xs=o(et);h($e.$$.fragment,Xs),Xs.forEach(t),Ys.forEach(t),ao=p(yr),it=r(yr,"SPAN",{});var Fs=o(it);ro=n(Fs,"Come funzionano i Transformer?"),Fs.forEach(t),yr.forEach(t),za=p(e),gi=r(e,"P",{});var Js=o(gi);oo=n(Js,"In questa sezione, vedremo in maniera approfondita l\u2019architettura dei modelli Transformer."),Js.forEach(t),ba=p(e),D=r(e,"H2",{class:!0});var Ar=o(D);te=r(Ar,"A",{id:!0,class:!0,href:!0});var Ws=o(te);tt=r(Ws,"SPAN",{});var Zs=o(tt);h(Ie.$$.fragment,Zs),Zs.forEach(t),Ws.forEach(t),lo=p(Ar),at=r(Ar,"SPAN",{});var Ks=o(at);no=n(Ks,"Un po' di storia dei Transformer"),Ks.forEach(t),Ar.forEach(t),Ta=p(e),_i=r(e,"P",{});var ed=o(_i);so=n(ed,"Ecco alcuni punti di riferimento nella (breve) storia dei modelli Transformer:"),ed.forEach(t),ka=p(e),B=r(e,"DIV",{class:!0});var Lr=o(B);Pe=r(Lr,"IMG",{class:!0,src:!0,alt:!0}),co=p(Lr),we=r(Lr,"IMG",{class:!0,src:!0,alt:!0}),Lr.forEach(t),qa=p(e),ae=r(e,"P",{});var Mr=o(ae);uo=n(Mr,"L\u2019"),ye=r(Mr,"A",{href:!0,rel:!0});var id=o(ye);po=n(id,"architettura Transformer"),id.forEach(t),mo=n(Mr," \xE8 stata introdotta in giugno 2017. Il focus della ricerca di partenza era sui compiti di traduzione. A questa segu\xEC l\u2019introduzione di numerosi modelli influenti, tra cui figurano:"),Mr.forEach(t),$a=p(e),f=r(e,"UL",{});var k=o(f);rt=r(k,"LI",{});var td=o(rt);re=r(td,"P",{});var fa=o(re);ot=r(fa,"STRONG",{});var ad=o(ot);fo=n(ad,"giugno 2018"),ad.forEach(t),vo=n(fa,": "),Ae=r(fa,"A",{href:!0,rel:!0});var rd=o(Ae);ho=n(rd,"GPT"),rd.forEach(t),go=n(fa,", il primo modello Transformer pre-addestrato, viene usato per affinare diversi compiti di NLP e ottiene risultati all\u2019avanguardia"),fa.forEach(t),td.forEach(t),_o=p(k),lt=r(k,"LI",{});var od=o(lt);oe=r(od,"P",{});var va=o(oe);nt=r(va,"STRONG",{});var ld=o(nt);Eo=n(ld,"ottobre 2018"),ld.forEach(t),zo=n(va,": "),Le=r(va,"A",{href:!0,rel:!0});var nd=o(Le);bo=n(nd,"BERT"),nd.forEach(t),To=n(va,", un altro ampio modello pre-addestrato, questa volta progettato per produrre riassunti di frasi migliori (ne scopriremo di pi\xF9 nel prossimo capitolo!)"),va.forEach(t),od.forEach(t),ko=p(k),st=r(k,"LI",{});var sd=o(st);le=r(sd,"P",{});var ha=o(le);dt=r(ha,"STRONG",{});var dd=o(dt);qo=n(dd,"febbraio 2019"),dd.forEach(t),$o=n(ha,": "),Me=r(ha,"A",{href:!0,rel:!0});var cd=o(Me);Io=n(cd,"GPT-2"),cd.forEach(t),Po=n(ha,", una versione (migliorata e ingrandita) di GPT che non fu distribuita immediatamente al pubblico a causa di preoccupazioni etiche"),ha.forEach(t),sd.forEach(t),wo=p(k),ct=r(k,"LI",{});var ud=o(ct);ne=r(ud,"P",{});var ga=o(ne);ut=r(ga,"STRONG",{});var pd=o(ut);yo=n(pd,"ottobre 2019"),pd.forEach(t),Ao=n(ga,": "),Ne=r(ga,"A",{href:!0,rel:!0});var md=o(Ne);Lo=n(md,"DistilBERT"),md.forEach(t),Mo=n(ga,", una versione distillata di BERT che \xE8 il 60% pi\xF9 rapida e il 40% pi\xF9 leggera in memoria, pur conservando il 97% della performance di BERT"),ga.forEach(t),ud.forEach(t),No=p(k),pt=r(k,"LI",{});var fd=o(pt);q=r(fd,"P",{});var fi=o(q);mt=r(fi,"STRONG",{});var vd=o(mt);Go=n(vd,"ottobre 2019"),vd.forEach(t),So=n(fi,": "),Ge=r(fi,"A",{href:!0,rel:!0});var hd=o(Ge);Ro=n(hd,"BART"),hd.forEach(t),xo=n(fi," e "),Se=r(fi,"A",{href:!0,rel:!0});var gd=o(Se);Do=n(gd,"T5"),gd.forEach(t),Bo=n(fi,", due grossi modelli pre-addestrati che utilizzano la stessa architettura del modello Transformer originale (nonch\xE9 i primi a farlo)"),fi.forEach(t),fd.forEach(t),Oo=p(k),ft=r(k,"LI",{});var _d=o(ft);$=r(_d,"P",{});var vi=o($);vt=r(vi,"STRONG",{});var Ed=o(vt);Co=n(Ed,"maggio 2020"),Ed.forEach(t),Uo=n(vi,", "),Re=r(vi,"A",{href:!0,rel:!0});var zd=o(Re);Qo=n(zd,"GPT-3"),zd.forEach(t),Ho=n(vi,", una versione ancora pi\xF9 ampia di GPT-2, con buone prestazioni in vari compiti e nessun bisogno di fine-tuning (il cosiddetto "),ht=r(vi,"EM",{});var bd=o(ht);jo=n(bd,"zero-shot learning"),bd.forEach(t),Vo=n(vi,")"),vi.forEach(t),_d.forEach(t),k.forEach(t),Ia=p(e),Ei=r(e,"P",{});var Td=o(Ei);Yo=n(Td,"La lista \xE8 tutto fuorch\xE9 esaustiva ed \xE8 volta solo a mettere in evidenza alcuni dei diversi tipi di modelli Transformer. In genere, questi possono essere raggruppati in tre categorie:"),Td.forEach(t),Pa=p(e),I=r(e,"UL",{});var Vi=o(I);xe=r(Vi,"LI",{});var Nr=o(xe);Xo=n(Nr,"Modelli in stile GPT (detti anche modelli Transformer "),gt=r(Nr,"EM",{});var kd=o(gt);Fo=n(kd,"auto-regressive"),kd.forEach(t),Jo=n(Nr,")"),Nr.forEach(t),Wo=p(Vi),De=r(Vi,"LI",{});var Gr=o(De);Zo=n(Gr,"Modelli in stile BERT (detti anche modelli Transformer "),_t=r(Gr,"EM",{});var qd=o(_t);Ko=n(qd,"auto-encoding"),qd.forEach(t),el=n(Gr,")"),Gr.forEach(t),il=p(Vi),Be=r(Vi,"LI",{});var Sr=o(Be);tl=n(Sr,"Modelli in stile BART/T5 (detti anche modelli Transformer "),Et=r(Sr,"EM",{});var $d=o(Et);al=n($d,"sequence-to-sequence"),$d.forEach(t),rl=n(Sr,")"),Sr.forEach(t),Vi.forEach(t),wa=p(e),zi=r(e,"P",{});var Id=o(zi);ol=n(Id,"Studieremo queste famiglie pi\xF9 nel dettaglio in seguito."),Id.forEach(t),ya=p(e),O=r(e,"H2",{class:!0});var Rr=o(O);se=r(Rr,"A",{id:!0,class:!0,href:!0});var Pd=o(se);zt=r(Pd,"SPAN",{});var wd=o(zt);h(Oe.$$.fragment,wd),wd.forEach(t),Pd.forEach(t),ll=p(Rr),bt=r(Rr,"SPAN",{});var yd=o(bt);nl=n(yd,"I Transformer sono modelli linguistici"),yd.forEach(t),Rr.forEach(t),Aa=p(e),P=r(e,"P",{});var Yi=o(P);sl=n(Yi,"Tutti i modelli Transformer menzionati qui sopra (GPT, BERT, BART, T5, ecc.) sono stati addestrati come modelli linguistici ("),Tt=r(Yi,"EM",{});var Ad=o(Tt);dl=n(Ad,"language models"),Ad.forEach(t),cl=n(Yi,"). Ci\xF2 significa che sono stati addestrati su grandi quantit\xE0 di testo grezzo in stile auto-supervisionato ("),kt=r(Yi,"EM",{});var Ld=o(kt);ul=n(Ld,"self-supervising"),Ld.forEach(t),pl=n(Yi,"). L\u2019apprendimento auto-supervisionato \xE8 un tipo di apprendimento il cui obbiettivo viene computato direttamente dagli input del modello. Ci\xF2 significa che non \xE8 richiesto alcun intervento umano per etichettare i dati!"),Yi.forEach(t),La=p(e),de=r(e,"P",{});var xr=o(de);ml=n(xr,"Un modello di questo tipo sviluppa una comprensione statistica della lingua alla quale \xE8 stato addestrato, ma non \xE8 molto utile in compiti pratici e precisi. Per questa ragione, il modello pre-addestrato generale viene in seguito sottoposto a un processo detto "),qt=r(xr,"EM",{});var Md=o(qt);fl=n(Md,"transfer learning"),Md.forEach(t),vl=n(xr,". Durante questo processo, il modello viene affinato per un determinato compito in maniera supervisionata (ossia utilizzando etichette generate da umani)."),xr.forEach(t),Ma=p(e),w=r(e,"P",{});var Xi=o(w);hl=n(Xi,"Un esempio di compito \xE8 la previsione della parola seguente in una frase di cui sono state lette "),$t=r(Xi,"EM",{});var Nd=o($t);gl=n(Nd,"n"),Nd.forEach(t),_l=n(Xi," parole precedenti. Quest\u2019operazione si chiama "),It=r(Xi,"EM",{});var Gd=o(It);El=n(Gd,"causal language modeling"),Gd.forEach(t),zl=n(Xi," perch\xE9 il suo output dipende dagli input presenti e passati, ma non da quelli futuri."),Xi.forEach(t),Na=p(e),C=r(e,"DIV",{class:!0});var Dr=o(C);Ce=r(Dr,"IMG",{class:!0,src:!0,alt:!0}),bl=p(Dr),Ue=r(Dr,"IMG",{class:!0,src:!0,alt:!0}),Dr.forEach(t),Ga=p(e),ce=r(e,"P",{});var Br=o(ce);Tl=n(Br,"Un altro esempio \xE8 il "),Pt=r(Br,"EM",{});var Sd=o(Pt);kl=n(Sd,"masked language modeling"),Sd.forEach(t),ql=n(Br,", in cui il modello prevede una parola occultata della frase."),Br.forEach(t),Sa=p(e),U=r(e,"DIV",{class:!0});var Or=o(U);Qe=r(Or,"IMG",{class:!0,src:!0,alt:!0}),$l=p(Or),He=r(Or,"IMG",{class:!0,src:!0,alt:!0}),Or.forEach(t),Ra=p(e),Q=r(e,"H2",{class:!0});var Cr=o(Q);ue=r(Cr,"A",{id:!0,class:!0,href:!0});var Rd=o(ue);wt=r(Rd,"SPAN",{});var xd=o(wt);h(je.$$.fragment,xd),xd.forEach(t),Rd.forEach(t),Il=p(Cr),yt=r(Cr,"SPAN",{});var Dd=o(yt);Pl=n(Dd,"I Transformers sono modelli enormi"),Dd.forEach(t),Cr.forEach(t),xa=p(e),bi=r(e,"P",{});var Bd=o(bi);wl=n(Bd,"A parte per alcune eccezioni (come DistilBERT), la strategia generale per ottenere performance migliori consiste nell\u2019aumentare la taglia dei modelli, nonch\xE9 la quantit\xE0 di dati utilizzati per il pre-addestramento."),Bd.forEach(t),Da=p(e),Ve=r(e,"DIV",{class:!0});var Od=o(Ve);Ye=r(Od,"IMG",{src:!0,alt:!0,width:!0}),Od.forEach(t),Ba=p(e),Ti=r(e,"P",{});var Cd=o(Ti);yl=n(Cd,"Sfortunatamente, l\u2019addestramento di un modello, e specialmente di un modello grosso, richiede grandi quantit\xE0 di dati. Ci\xF2 si rivela molto costoso in termini di tempo, risorse informatiche e impatto ambientale, come mostrano i grafici qui sotto."),Cd.forEach(t),Oa=p(e),H=r(e,"DIV",{class:!0});var Ur=o(H);Xe=r(Ur,"IMG",{class:!0,src:!0,alt:!0}),Al=p(Ur),Fe=r(Ur,"IMG",{class:!0,src:!0,alt:!0}),Ur.forEach(t),Ca=p(e),h(Je.$$.fragment,e),Ua=p(e),ki=r(e,"P",{});var Ud=o(ki);Ll=n(Ud,"Questi dati si riferiscono a un progetto per un modello (molto grande) condotto da un team che provava consciamente a ridurre l\u2019impatto ambientale del pre-addestramento. L\u2019impronta di trials volti a ottenere i miglior iperparamenti possibili sarebbe ancora pi\xF9 importante."),Ud.forEach(t),Qa=p(e),qi=r(e,"P",{});var Qd=o(qi);Ml=n(Qd,"Immagina cosa succederebbe se ogni volta che un gruppo di ricerca, un\u2019organizzazione studentesca o un\u2019azienda vuole addestrare un modello lo facesse da zero! I costi globali sarebbero inutilmente enormi!"),Qd.forEach(t),Ha=p(e),$i=r(e,"P",{});var Hd=o($i);Nl=n(Hd,"Questo \xE8 il motivo per cui la condivisione di modelli linguistici \xE8 fondamentale: lavorare a partire da modelli gi\xE0 addestrati riduce i costi informatici complessivi e l\u2019impatto ambientale della comunit\xE0."),Hd.forEach(t),ja=p(e),j=r(e,"H2",{class:!0});var Qr=o(j);pe=r(Qr,"A",{id:!0,class:!0,href:!0});var jd=o(pe);At=r(jd,"SPAN",{});var Vd=o(At);h(We.$$.fragment,Vd),Vd.forEach(t),jd.forEach(t),Gl=p(Qr),Lt=r(Qr,"SPAN",{});var Yd=o(Lt);Sl=n(Yd,"Transfer Learning"),Yd.forEach(t),Qr.forEach(t),Va=p(e),h(Ze.$$.fragment,e),Ya=p(e),Ii=r(e,"P",{});var Xd=o(Ii);Rl=n(Xd,"Il pre-addestramento \xE8 l\u2019atto di addestrare un modello da zero: i pesi sono inizializzati in maniera casuale, e l\u2019addestramento inizia senza alcuna conoscenza pregressa."),Xd.forEach(t),Xa=p(e),V=r(e,"DIV",{class:!0});var Hr=o(V);Ke=r(Hr,"IMG",{class:!0,src:!0,alt:!0}),xl=p(Hr),ei=r(Hr,"IMG",{class:!0,src:!0,alt:!0}),Hr.forEach(t),Fa=p(e),Pi=r(e,"P",{});var Fd=o(Pi);Dl=n(Fd,"Questo pre-addestramento \xE8 solitamente fatto su enormi quantit\xE0 di dati. Di conseguenza, l\u2019addestramento richiede un corpus di dati molto ampio e pu\xF2 prendere diverse settimane."),Fd.forEach(t),Ja=p(e),y=r(e,"P",{});var Fi=o(y);Bl=n(Fi,"L\u2019affinamento ("),Mt=r(Fi,"EM",{});var Jd=o(Mt);Ol=n(Jd,"fine-tuning"),Jd.forEach(t),Cl=n(Fi,"), al contrario, \xE8 un addestramento che ha luogo "),Nt=r(Fi,"STRONG",{});var Wd=o(Nt);Ul=n(Wd,"dopo"),Wd.forEach(t),Ql=n(Fi," che il modello \xE8 stato pre-addestrato. Per poter effettuare un fine-tuning, \xE8 necessario acquisire un modello linguistico pre-addestrato e addestrarlo ulteriormente con una base dati adatta al compito in questione. Ma perch\xE9 non addestrare direttamente al compito finale? Esistono alcune ragioni:"),Fi.forEach(t),Wa=p(e),A=r(e,"UL",{});var Ji=o(A);Gt=r(Ji,"LI",{});var Zd=o(Gt);Hl=n(Zd,"Il modello pre-addestrato \xE8 gi\xE0 addestrato su basi dati che contengono similarit\xE0 con la base dati usata per il fine-tuning. Il processo di fine-tuning riesce quindi ad beneficiare della conoscenza acquisita dal modello iniziale durante il pre-addestramento (ad esempio, nei problemi di NLP, il modello pre-addestrato avr\xE0 gi\xE0 conoscenze statistiche della lingua utilizzata nel compito)."),Zd.forEach(t),jl=p(Ji),St=r(Ji,"LI",{});var Kd=o(St);Vl=n(Kd,"Siccome il modello pre-addestrato \xE8 stato addestrato usando moltissimi dati, il fine-tuning richiede molto meno dati per ottenere buoni risultati."),Kd.forEach(t),Yl=p(Ji),Rt=r(Ji,"LI",{});var ec=o(Rt);Xl=n(ec,"Per la stessa ragione, occorrono molto meno tempo e risorse per ottenere buoni risultati."),ec.forEach(t),Ji.forEach(t),Za=p(e),me=r(e,"P",{});var jr=o(me);Fl=n(jr,"Ad esempio, \xE8 possibile approfittare di un modello pre-addestrato per la lingua inglese e poi affinarlo usando un corpus arXiv, ottenendo cos\xEC un modello specifico per la scienza/ricerca. L\u2019affinamento non richieder\xE0 che una quantit\xE0 limitata di dati: le conoscenze acquisite dal modello pre-addestrato sono \u201Ctrasferite\u201D, come riflette il nome "),xt=r(jr,"EM",{});var ic=o(xt);Jl=n(ic,"transfer learning"),ic.forEach(t),Wl=n(jr,"."),jr.forEach(t),Ka=p(e),Y=r(e,"DIV",{class:!0});var Vr=o(Y);ii=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Zl=p(Vr),ti=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Vr.forEach(t),er=p(e),wi=r(e,"P",{});var tc=o(wi);Kl=n(tc,"Il fine-tuning di un modello ha quindi costi ridotti in termini di dati, finanze e impatto ambientale. Iterare su diversi schemi di fine-tuning \xE8 anche pi\xF9 rapido e semplice, in quanto l\u2019addestramento \xE8 meno restrittivo di un pre-addestramento completo."),tc.forEach(t),ir=p(e),yi=r(e,"P",{});var ac=o(yi);en=n(ac,"Questo processo permette anche di ottenere risultati migliori di un addestramento da zero (a meno di non essere in possesso di moltissimi dati), motivo per cui bisognerebbe sempre partire da un modello pre-addestrato (quanto possibile compatibile con il compito da eseguire) e affinarlo."),ac.forEach(t),tr=p(e),X=r(e,"H2",{class:!0});var Yr=o(X);fe=r(Yr,"A",{id:!0,class:!0,href:!0});var rc=o(fe);Dt=r(rc,"SPAN",{});var oc=o(Dt);h(ai.$$.fragment,oc),oc.forEach(t),rc.forEach(t),tn=p(Yr),Bt=r(Yr,"SPAN",{});var lc=o(Bt);an=n(lc,"Architettura generale"),lc.forEach(t),Yr.forEach(t),ar=p(e),Ai=r(e,"P",{});var nc=o(Ai);rn=n(nc,"In questa sezione, vedremo l\u2019architettura generale del modello Transformer. Non preoccuparti se non capisci tutti i concetti: pi\xF9 avanti, troverai sezioni dettagliate per ogni componente."),nc.forEach(t),rr=p(e),h(ri.$$.fragment,e),or=p(e),F=r(e,"H2",{class:!0});var Xr=o(F);ve=r(Xr,"A",{id:!0,class:!0,href:!0});var sc=o(ve);Ot=r(sc,"SPAN",{});var dc=o(Ot);h(oi.$$.fragment,dc),dc.forEach(t),sc.forEach(t),on=p(Xr),Ct=r(Xr,"SPAN",{});var cc=o(Ct);ln=n(cc,"Introduzione"),cc.forEach(t),Xr.forEach(t),lr=p(e),Li=r(e,"P",{});var uc=o(Li);nn=n(uc,"Il modello si compone principalmente di due blocchi:"),uc.forEach(t),nr=p(e),he=r(e,"UL",{});var Fr=o(he);Mi=r(Fr,"LI",{});var Ts=o(Mi);Ut=r(Ts,"STRONG",{});var pc=o(Ut);sn=n(pc,"Encoder (sinistra)"),pc.forEach(t),dn=n(Ts,": L\u2019encoder riceve un input e ne costruisce una rappresentazione, le features. Ci\xF2 significa che il modello \xE8 ottimizzato per la comprensione dell\u2019input."),Ts.forEach(t),cn=p(Fr),Ni=r(Fr,"LI",{});var ks=o(Ni);Qt=r(ks,"STRONG",{});var mc=o(Qt);un=n(mc,"Decoder (destra)"),mc.forEach(t),pn=n(ks,": Il decoder utilizza la rappresentazione dell\u2019encoder (le features) assieme ad ulteriori input per generare la sequenza target. Ci\xF2 significa che il modello \xE8 ottimizzato per la generazione di output."),ks.forEach(t),Fr.forEach(t),sr=p(e),J=r(e,"DIV",{class:!0});var Jr=o(J);li=r(Jr,"IMG",{class:!0,src:!0,alt:!0}),mn=p(Jr),ni=r(Jr,"IMG",{class:!0,src:!0,alt:!0}),Jr.forEach(t),dr=p(e),Gi=r(e,"P",{});var fc=o(Gi);fn=n(fc,"Ognuna di queste parti pu\xF2 essere utilizzata indipendentemente, in base al compito:"),fc.forEach(t),cr=p(e),L=r(e,"UL",{});var Wi=o(L);Si=r(Wi,"LI",{});var qs=o(Si);Ht=r(qs,"STRONG",{});var vc=o(Ht);vn=n(vc,"Modelli Encoder-only"),vc.forEach(t),hn=n(qs,": Ottimi per compiti che richiedono una comprensione dell\u2019input, come la classificazione frasale e il riconoscimento delle entit\xE0 nominate."),qs.forEach(t),gn=p(Wi),Ri=r(Wi,"LI",{});var $s=o(Ri);jt=r($s,"STRONG",{});var hc=o(jt);_n=n(hc,"Modelli Decoder-only"),hc.forEach(t),En=n($s,": Ottimi per compiti generativi come la generazione testuale."),$s.forEach(t),zn=p(Wi),ge=r(Wi,"LI",{});var _a=o(ge);Vt=r(_a,"STRONG",{});var gc=o(Vt);bn=n(gc,"Modelli Encoder-decoder"),gc.forEach(t),Tn=n(_a," o "),Yt=r(_a,"STRONG",{});var _c=o(Yt);kn=n(_c,"modelli sequence-to-sequence"),_c.forEach(t),qn=n(_a,": Ottimi per compiti generativi che richiedono un input, come la traduzione o il riassunto."),_a.forEach(t),Wi.forEach(t),ur=p(e),xi=r(e,"P",{});var Ec=o(xi);$n=n(Ec,"Analizzeremo ciascuna di queste architetture indipendentemente pi\xF9 tardi nel corso."),Ec.forEach(t),pr=p(e),W=r(e,"H2",{class:!0});var Wr=o(W);_e=r(Wr,"A",{id:!0,class:!0,href:!0});var zc=o(_e);Xt=r(zc,"SPAN",{});var bc=o(Xt);h(si.$$.fragment,bc),bc.forEach(t),zc.forEach(t),In=p(Wr),Ft=r(Wr,"SPAN",{});var Tc=o(Ft);Pn=n(Tc,"Attention layers"),Tc.forEach(t),Wr.forEach(t),mr=p(e),M=r(e,"P",{});var Zi=o(M);wn=n(Zi,"Una caratteristica chiave dei modelli Transformer \xE8 che sono basati su strati speciali detti "),Jt=r(Zi,"EM",{});var kc=o(Jt);yn=n(kc,"attention layers"),kc.forEach(t),An=n(Zi,". Non a caso, il titolo del paper che introdusse l\u2019architettura Transformer era "),di=r(Zi,"A",{href:!0,rel:!0});var qc=o(di);Ln=n(qc,"\u201CAttention Is All You Need\u201D"),qc.forEach(t),Mn=n(Zi,"! Esploreremo gli attention layer nel dettaglio pi\xF9 avanti in questo corso; per ora, tutto ci\xF2 che hai bisogno di sapere \xE8 che un layer dir\xE0 al modello di prestare particolare attenzione a certe parole nella frase input (ignorando praticamente le altre) quando si occupa della rappresentazione delle singole parole."),Zi.forEach(t),fr=p(e),Di=r(e,"P",{});var $c=o(Di);Nn=n($c,"Come esempio concreto, pensa ad un compito di traduzione testuale dall\u2019inglese al francese. Dato l\u2019input \u201CYou like this course\u201D, un modello di traduzione dovr\xE0 fare riferimento alla parola adiacente \u201CYou\u201D per fornire la traduzione corretta della parola \u201Clike\u201D, perch\xE9 in francese la coniugazione del verbo \u201Clike\u201D cambia in base al soggetto. Diversamente, il resto della frase non \xE8 utile alla sua traduzione di quella precisa parola. In maniera simile, durante la traduzione di \u201Cthis\u201D il modello dovr\xE0 prestare attenzione alla parola \u201Ccourse\u201D, in quanto \u201Cthis\u201D ha traduzioni diverse se associato con nomi femminili o maschili. Di nuovo, il resto delle parole della frase non contribuiscono alla corretta traduzione di \u201Cthis\u201D. Con frasi pi\xF9 complesse (e regole grammaticali pi\xF9 complesse), il modello potrebbe aver bisogno di prestare particolare attenzione a parole ben pi\xF9 lontane nella frase per tradurre correttamente ogni parola."),$c.forEach(t),vr=p(e),Bi=r(e,"P",{});var Ic=o(Bi);Gn=n(Ic,"Lo stesso concetto si applica a qualsiasi compito che ha a che fare con il linguaggio naturale: una parola ha un senso a s\xE9 stante, ma tale senso \xE8 profondamente influenzato dal contesto, il quale \xE8 costituito da una qualsiasi parola (o parole) che precede o segue la parola sotto osservazione."),Ic.forEach(t),hr=p(e),Oi=r(e,"P",{});var Pc=o(Oi);Sn=n(Pc,"Ora che sai cosa sono gli attention layer, guardiamo un po\u2019 pi\xF9 nel dettaglio all\u2019architettura Transformer."),Pc.forEach(t),gr=p(e),Z=r(e,"H2",{class:!0});var Zr=o(Z);Ee=r(Zr,"A",{id:!0,class:!0,href:!0});var wc=o(Ee);Wt=r(wc,"SPAN",{});var yc=o(Wt);h(ci.$$.fragment,yc),yc.forEach(t),wc.forEach(t),Rn=p(Zr),Zt=r(Zr,"SPAN",{});var Ac=o(Zt);xn=n(Ac,"L'architettura originale"),Ac.forEach(t),Zr.forEach(t),_r=p(e),Ci=r(e,"P",{});var Lc=o(Ci);Dn=n(Lc,"All\u2019origine, l\u2019architettura Transformer fu creata per la traduzione. In fase di addestramento, l\u2019encoder riceve degli input (frasi) in una certa lingua, mentre il decoder riceve le stesse frasi nella lingua target d\u2019elezione. Nell\u2019encoder, gli attention layer sono in grado di utilizzare qualsiasi parola in una data frase (dato che, come abbiamo appena visto, la traduzione di una determinata parola pu\xF2 dipendere da ci\xF2 che la precede o segue nella frase). Diversamente, decoder procede in maniera sequenziale ed \xE8 capace di prestare attenzione solo alle parole della frase che ha gi\xE0 tradotto (ossia, solo le parole che precedono la parola che sta generando). Ad esempio, una volta predette le prime tre parole della frase target, le passiamo al decoder  che utilizza tutti gli input dell\u2019encoder per provare a predirre la quarta parola."),Lc.forEach(t),Er=p(e),Ui=r(e,"P",{});var Mc=o(Ui);Bn=n(Mc,"Per accelerare il processo di addestramento (quando il modello ha accesso alle frasi target), l\u2019intero target viene fornito al decoder, che per\xF2 non \xE8 in grado di accedere alle parole future (se avesse accesso alla parola in seconda posizione mentre cerca di predirre la parola in seconda posizione, il problema cesserebbe di essere complesso). Ad esempio, mentre prova a predirre la quarta parola, l\u2019attention layer avr\xE0 accesso solo alle posizioni tra la prima e la terza."),Mc.forEach(t),zr=p(e),Qi=r(e,"P",{});var Nc=o(Qi);On=n(Nc,"L\u2019architettura Transformer originale aveva la struttura qui sotto, con l\u2019encoder a sinistra e il decoder a destra:"),Nc.forEach(t),br=p(e),K=r(e,"DIV",{class:!0});var Kr=o(K);ui=r(Kr,"IMG",{class:!0,src:!0,alt:!0}),Cn=p(Kr),pi=r(Kr,"IMG",{class:!0,src:!0,alt:!0}),Kr.forEach(t),Tr=p(e),ze=r(e,"P",{});var eo=o(ze);Un=n(eo,"Nota che il primo attention layer in un "),Kt=r(eo,"EM",{});var Gc=o(Kt);Qn=n(Gc,"decoder block"),Gc.forEach(t),Hn=n(eo," presta attenzione a tutti gli input (passati) al decoder, mentre il secondo attention layer utilizza l\u2019output del encoder. Gli \xE8 perci\xF2 possibile avere accesso a tutta la frase input per meglio prevedere la parola corrente. Questa caratteristica \xE8 molto utile in quanto lingue diverse possono avere regole grammaticali diverse piazzano le parole in ordini diversi, oppure perch\xE9 il contesto che compare pi\xF9 tardi nella frase potrebbe essere utile nella determinazione della migliore traduzione di una data parola."),eo.forEach(t),kr=p(e),be=r(e,"P",{});var io=o(be);jn=n(io,"L\u2019"),ea=r(io,"EM",{});var Sc=o(ea);Vn=n(Sc,"attention mask"),Sc.forEach(t),Yn=n(io," pu\xF2 essere utilizzato anche nell\u2019encoder/decoder per evitare che il modello presti attenzione a certe parole speciali, come ad esempio parole riempitive utilizzate per rendere tutti gli input della stessa lunghezza."),io.forEach(t),qr=p(e),ee=r(e,"H2",{class:!0});var to=o(ee);Te=r(to,"A",{id:!0,class:!0,href:!0});var Rc=o(Te);ia=r(Rc,"SPAN",{});var xc=o(ia);h(mi.$$.fragment,xc),xc.forEach(t),Rc.forEach(t),Xn=p(to),ta=r(to,"SPAN",{});var Dc=o(ta);Fn=n(Dc,"Architetture vs. checkpoint"),Dc.forEach(t),to.forEach(t),$r=p(e),b=r(e,"P",{});var ke=o(b);Jn=n(ke,"Durante questo viaggio nel mondo dei modelli Transformer, incontrerai menzioni di "),aa=r(ke,"EM",{});var Bc=o(aa);Wn=n(Bc,"architetture"),Bc.forEach(t),Zn=n(ke," e "),ra=r(ke,"EM",{});var Oc=o(ra);Kn=n(Oc,"checkpoint"),Oc.forEach(t),es=n(ke,", nonch\xE9 di "),oa=r(ke,"EM",{});var Cc=o(oa);is=n(Cc,"modelli"),Cc.forEach(t),ts=n(ke,". Questi termini hanno significati leggermente diversi:"),ke.forEach(t),Ir=p(e),N=r(e,"UL",{});var Ki=o(N);Hi=r(Ki,"LI",{});var Is=o(Hi);la=r(Is,"STRONG",{});var Uc=o(la);as=n(Uc,"Architettura"),Uc.forEach(t),rs=n(Is,": Lo scheletro del modello, ossia la definizione di ogni livello e operazione che compare nel modello."),Is.forEach(t),os=p(Ki),ji=r(Ki,"LI",{});var Ps=o(ji);na=r(Ps,"STRONG",{});var Qc=o(na);ls=n(Qc,"Checkpoint"),Qc.forEach(t),ns=n(Ps,": I pesi che verranno caricati in una determinata architettura."),Ps.forEach(t),ss=p(Ki),G=r(Ki,"LI",{});var hi=o(G);sa=r(hi,"STRONG",{});var Hc=o(sa);ds=n(Hc,"Modello"),Hc.forEach(t),cs=n(hi,": Un termine generico meno preciso di \u201Carchitettura\u201D o \u201Ccheckpoint\u201D, in quanto pu\xF2 significare entrambi. In questo corso faremo la distinzione tra "),da=r(hi,"EM",{});var jc=o(da);us=n(jc,"architettura"),jc.forEach(t),ps=n(hi," e "),ca=r(hi,"EM",{});var Vc=o(ca);ms=n(Vc,"checkpoint"),Vc.forEach(t),fs=n(hi," quando sar\xE0 necessario ridurre le ambiguit\xE0."),hi.forEach(t),Ki.forEach(t),Pr=p(e),T=r(e,"P",{});var qe=o(T);vs=n(qe,"Ad esempio, BERT \xE8 un\u2019architettura, mentre "),ua=r(qe,"CODE",{});var Yc=o(ua);hs=n(Yc,"bert-base-cased"),Yc.forEach(t),gs=n(qe,", un set di pesi ("),pa=r(qe,"EM",{});var Xc=o(pa);_s=n(Xc,"weights"),Xc.forEach(t),Es=n(qe,") addestrati dal team di Google per la prima versione di BERT, \xE8 un checkpoint. Ciononostante, \xE8 possibile dire \u201Cil modello BERT\u201D e \u201Cil modello "),ma=r(qe,"CODE",{});var Fc=o(ma);zs=n(Fc,"bert-base-cased"),Fc.forEach(t),bs=n(qe,".\u201D"),qe.forEach(t),this.h()},h(){c(R,"name","hf:doc:metadata"),c(R,"content",JSON.stringify(au)),c(ie,"id","come-funzionano-i-transformer"),c(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ie,"href","#come-funzionano-i-transformer"),c(x,"class","relative group"),c(te,"id","un-po-di-storia-dei-transformer"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#un-po-di-storia-dei-transformer"),c(D,"class","relative group"),c(Pe,"class","block dark:hidden"),m(Pe.src,As="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(Pe,"src",As),c(Pe,"alt","A brief chronology of Transformers models."),c(we,"class","hidden dark:block"),m(we.src,Ls="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(we,"src",Ls),c(we,"alt","A brief chronology of Transformers models."),c(B,"class","flex justify-center"),c(ye,"href","https://arxiv.org/abs/1706.03762"),c(ye,"rel","nofollow"),c(Ae,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Ae,"rel","nofollow"),c(Le,"href","https://arxiv.org/abs/1810.04805"),c(Le,"rel","nofollow"),c(Me,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(Me,"rel","nofollow"),c(Ne,"href","https://arxiv.org/abs/1910.01108"),c(Ne,"rel","nofollow"),c(Ge,"href","https://arxiv.org/abs/1910.13461"),c(Ge,"rel","nofollow"),c(Se,"href","https://arxiv.org/abs/1910.10683"),c(Se,"rel","nofollow"),c(Re,"href","https://arxiv.org/abs/2005.14165"),c(Re,"rel","nofollow"),c(se,"id","i-transformer-sono-modelli-linguistici"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#i-transformer-sono-modelli-linguistici"),c(O,"class","relative group"),c(Ce,"class","block dark:hidden"),m(Ce.src,Ms="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(Ce,"src",Ms),c(Ce,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(Ue,"class","hidden dark:block"),m(Ue.src,Ns="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(Ue,"src",Ns),c(Ue,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(C,"class","flex justify-center"),c(Qe,"class","block dark:hidden"),m(Qe.src,Gs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(Qe,"src",Gs),c(Qe,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(He,"class","hidden dark:block"),m(He.src,Ss="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(He,"src",Ss),c(He,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(U,"class","flex justify-center"),c(ue,"id","i-transformers-sono-modelli-enormi"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#i-transformers-sono-modelli-enormi"),c(Q,"class","relative group"),m(Ye.src,Rs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(Ye,"src",Rs),c(Ye,"alt","Number of parameters of recent Transformers models"),c(Ye,"width","90%"),c(Ve,"class","flex justify-center"),c(Xe,"class","block dark:hidden"),m(Xe.src,xs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(Xe,"src",xs),c(Xe,"alt","The carbon footprint of a large language model."),c(Fe,"class","hidden dark:block"),m(Fe.src,Ds="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(Fe,"src",Ds),c(Fe,"alt","The carbon footprint of a large language model."),c(H,"class","flex justify-center"),c(pe,"id","transfer-learning"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#transfer-learning"),c(j,"class","relative group"),c(Ke,"class","block dark:hidden"),m(Ke.src,Bs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(Ke,"src",Bs),c(Ke,"alt","The pretraining of a language model is costly in both time and money."),c(ei,"class","hidden dark:block"),m(ei.src,Os="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(ei,"src",Os),c(ei,"alt","The pretraining of a language model is costly in both time and money."),c(V,"class","flex justify-center"),c(ii,"class","block dark:hidden"),m(ii.src,Cs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(ii,"src",Cs),c(ii,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(ti,"class","hidden dark:block"),m(ti.src,Us="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(ti,"src",Us),c(ti,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(Y,"class","flex justify-center"),c(fe,"id","architettura-generale"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#architettura-generale"),c(X,"class","relative group"),c(ve,"id","introduzione"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#introduzione"),c(F,"class","relative group"),c(li,"class","block dark:hidden"),m(li.src,Qs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(li,"src",Qs),c(li,"alt","Architecture of a Transformers models"),c(ni,"class","hidden dark:block"),m(ni.src,Hs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(ni,"src",Hs),c(ni,"alt","Architecture of a Transformers models"),c(J,"class","flex justify-center"),c(_e,"id","attention-layers"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#attention-layers"),c(W,"class","relative group"),c(di,"href","https://arxiv.org/abs/1706.03762"),c(di,"rel","nofollow"),c(Ee,"id","larchitettura-originale"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#larchitettura-originale"),c(Z,"class","relative group"),c(ui,"class","block dark:hidden"),m(ui.src,js="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(ui,"src",js),c(ui,"alt","Architecture of a Transformers models"),c(pi,"class","hidden dark:block"),m(pi.src,Vs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(pi,"src",Vs),c(pi,"alt","Architecture of a Transformers models"),c(K,"class","flex justify-center"),c(Te,"id","architetture-vs-checkpoint"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#architetture-vs-checkpoint"),c(ee,"class","relative group")},m(e,s){i(document.head,R),d(e,Ea,s),d(e,x,s),i(x,ie),i(ie,et),g($e,et,null),i(x,ao),i(x,it),i(it,ro),d(e,za,s),d(e,gi,s),i(gi,oo),d(e,ba,s),d(e,D,s),i(D,te),i(te,tt),g(Ie,tt,null),i(D,lo),i(D,at),i(at,no),d(e,Ta,s),d(e,_i,s),i(_i,so),d(e,ka,s),d(e,B,s),i(B,Pe),i(B,co),i(B,we),d(e,qa,s),d(e,ae,s),i(ae,uo),i(ae,ye),i(ye,po),i(ae,mo),d(e,$a,s),d(e,f,s),i(f,rt),i(rt,re),i(re,ot),i(ot,fo),i(re,vo),i(re,Ae),i(Ae,ho),i(re,go),i(f,_o),i(f,lt),i(lt,oe),i(oe,nt),i(nt,Eo),i(oe,zo),i(oe,Le),i(Le,bo),i(oe,To),i(f,ko),i(f,st),i(st,le),i(le,dt),i(dt,qo),i(le,$o),i(le,Me),i(Me,Io),i(le,Po),i(f,wo),i(f,ct),i(ct,ne),i(ne,ut),i(ut,yo),i(ne,Ao),i(ne,Ne),i(Ne,Lo),i(ne,Mo),i(f,No),i(f,pt),i(pt,q),i(q,mt),i(mt,Go),i(q,So),i(q,Ge),i(Ge,Ro),i(q,xo),i(q,Se),i(Se,Do),i(q,Bo),i(f,Oo),i(f,ft),i(ft,$),i($,vt),i(vt,Co),i($,Uo),i($,Re),i(Re,Qo),i($,Ho),i($,ht),i(ht,jo),i($,Vo),d(e,Ia,s),d(e,Ei,s),i(Ei,Yo),d(e,Pa,s),d(e,I,s),i(I,xe),i(xe,Xo),i(xe,gt),i(gt,Fo),i(xe,Jo),i(I,Wo),i(I,De),i(De,Zo),i(De,_t),i(_t,Ko),i(De,el),i(I,il),i(I,Be),i(Be,tl),i(Be,Et),i(Et,al),i(Be,rl),d(e,wa,s),d(e,zi,s),i(zi,ol),d(e,ya,s),d(e,O,s),i(O,se),i(se,zt),g(Oe,zt,null),i(O,ll),i(O,bt),i(bt,nl),d(e,Aa,s),d(e,P,s),i(P,sl),i(P,Tt),i(Tt,dl),i(P,cl),i(P,kt),i(kt,ul),i(P,pl),d(e,La,s),d(e,de,s),i(de,ml),i(de,qt),i(qt,fl),i(de,vl),d(e,Ma,s),d(e,w,s),i(w,hl),i(w,$t),i($t,gl),i(w,_l),i(w,It),i(It,El),i(w,zl),d(e,Na,s),d(e,C,s),i(C,Ce),i(C,bl),i(C,Ue),d(e,Ga,s),d(e,ce,s),i(ce,Tl),i(ce,Pt),i(Pt,kl),i(ce,ql),d(e,Sa,s),d(e,U,s),i(U,Qe),i(U,$l),i(U,He),d(e,Ra,s),d(e,Q,s),i(Q,ue),i(ue,wt),g(je,wt,null),i(Q,Il),i(Q,yt),i(yt,Pl),d(e,xa,s),d(e,bi,s),i(bi,wl),d(e,Da,s),d(e,Ve,s),i(Ve,Ye),d(e,Ba,s),d(e,Ti,s),i(Ti,yl),d(e,Oa,s),d(e,H,s),i(H,Xe),i(H,Al),i(H,Fe),d(e,Ca,s),g(Je,e,s),d(e,Ua,s),d(e,ki,s),i(ki,Ll),d(e,Qa,s),d(e,qi,s),i(qi,Ml),d(e,Ha,s),d(e,$i,s),i($i,Nl),d(e,ja,s),d(e,j,s),i(j,pe),i(pe,At),g(We,At,null),i(j,Gl),i(j,Lt),i(Lt,Sl),d(e,Va,s),g(Ze,e,s),d(e,Ya,s),d(e,Ii,s),i(Ii,Rl),d(e,Xa,s),d(e,V,s),i(V,Ke),i(V,xl),i(V,ei),d(e,Fa,s),d(e,Pi,s),i(Pi,Dl),d(e,Ja,s),d(e,y,s),i(y,Bl),i(y,Mt),i(Mt,Ol),i(y,Cl),i(y,Nt),i(Nt,Ul),i(y,Ql),d(e,Wa,s),d(e,A,s),i(A,Gt),i(Gt,Hl),i(A,jl),i(A,St),i(St,Vl),i(A,Yl),i(A,Rt),i(Rt,Xl),d(e,Za,s),d(e,me,s),i(me,Fl),i(me,xt),i(xt,Jl),i(me,Wl),d(e,Ka,s),d(e,Y,s),i(Y,ii),i(Y,Zl),i(Y,ti),d(e,er,s),d(e,wi,s),i(wi,Kl),d(e,ir,s),d(e,yi,s),i(yi,en),d(e,tr,s),d(e,X,s),i(X,fe),i(fe,Dt),g(ai,Dt,null),i(X,tn),i(X,Bt),i(Bt,an),d(e,ar,s),d(e,Ai,s),i(Ai,rn),d(e,rr,s),g(ri,e,s),d(e,or,s),d(e,F,s),i(F,ve),i(ve,Ot),g(oi,Ot,null),i(F,on),i(F,Ct),i(Ct,ln),d(e,lr,s),d(e,Li,s),i(Li,nn),d(e,nr,s),d(e,he,s),i(he,Mi),i(Mi,Ut),i(Ut,sn),i(Mi,dn),i(he,cn),i(he,Ni),i(Ni,Qt),i(Qt,un),i(Ni,pn),d(e,sr,s),d(e,J,s),i(J,li),i(J,mn),i(J,ni),d(e,dr,s),d(e,Gi,s),i(Gi,fn),d(e,cr,s),d(e,L,s),i(L,Si),i(Si,Ht),i(Ht,vn),i(Si,hn),i(L,gn),i(L,Ri),i(Ri,jt),i(jt,_n),i(Ri,En),i(L,zn),i(L,ge),i(ge,Vt),i(Vt,bn),i(ge,Tn),i(ge,Yt),i(Yt,kn),i(ge,qn),d(e,ur,s),d(e,xi,s),i(xi,$n),d(e,pr,s),d(e,W,s),i(W,_e),i(_e,Xt),g(si,Xt,null),i(W,In),i(W,Ft),i(Ft,Pn),d(e,mr,s),d(e,M,s),i(M,wn),i(M,Jt),i(Jt,yn),i(M,An),i(M,di),i(di,Ln),i(M,Mn),d(e,fr,s),d(e,Di,s),i(Di,Nn),d(e,vr,s),d(e,Bi,s),i(Bi,Gn),d(e,hr,s),d(e,Oi,s),i(Oi,Sn),d(e,gr,s),d(e,Z,s),i(Z,Ee),i(Ee,Wt),g(ci,Wt,null),i(Z,Rn),i(Z,Zt),i(Zt,xn),d(e,_r,s),d(e,Ci,s),i(Ci,Dn),d(e,Er,s),d(e,Ui,s),i(Ui,Bn),d(e,zr,s),d(e,Qi,s),i(Qi,On),d(e,br,s),d(e,K,s),i(K,ui),i(K,Cn),i(K,pi),d(e,Tr,s),d(e,ze,s),i(ze,Un),i(ze,Kt),i(Kt,Qn),i(ze,Hn),d(e,kr,s),d(e,be,s),i(be,jn),i(be,ea),i(ea,Vn),i(be,Yn),d(e,qr,s),d(e,ee,s),i(ee,Te),i(Te,ia),g(mi,ia,null),i(ee,Xn),i(ee,ta),i(ta,Fn),d(e,$r,s),d(e,b,s),i(b,Jn),i(b,aa),i(aa,Wn),i(b,Zn),i(b,ra),i(ra,Kn),i(b,es),i(b,oa),i(oa,is),i(b,ts),d(e,Ir,s),d(e,N,s),i(N,Hi),i(Hi,la),i(la,as),i(Hi,rs),i(N,os),i(N,ji),i(ji,na),i(na,ls),i(ji,ns),i(N,ss),i(N,G),i(G,sa),i(sa,ds),i(G,cs),i(G,da),i(da,us),i(G,ps),i(G,ca),i(ca,ms),i(G,fs),d(e,Pr,s),d(e,T,s),i(T,vs),i(T,ua),i(ua,hs),i(T,gs),i(T,pa),i(pa,_s),i(T,Es),i(T,ma),i(ma,zs),i(T,bs),wr=!0},p:eu,i(e){wr||(_($e.$$.fragment,e),_(Ie.$$.fragment,e),_(Oe.$$.fragment,e),_(je.$$.fragment,e),_(Je.$$.fragment,e),_(We.$$.fragment,e),_(Ze.$$.fragment,e),_(ai.$$.fragment,e),_(ri.$$.fragment,e),_(oi.$$.fragment,e),_(si.$$.fragment,e),_(ci.$$.fragment,e),_(mi.$$.fragment,e),wr=!0)},o(e){E($e.$$.fragment,e),E(Ie.$$.fragment,e),E(Oe.$$.fragment,e),E(je.$$.fragment,e),E(Je.$$.fragment,e),E(We.$$.fragment,e),E(Ze.$$.fragment,e),E(ai.$$.fragment,e),E(ri.$$.fragment,e),E(oi.$$.fragment,e),E(si.$$.fragment,e),E(ci.$$.fragment,e),E(mi.$$.fragment,e),wr=!1},d(e){t(R),e&&t(Ea),e&&t(x),z($e),e&&t(za),e&&t(gi),e&&t(ba),e&&t(D),z(Ie),e&&t(Ta),e&&t(_i),e&&t(ka),e&&t(B),e&&t(qa),e&&t(ae),e&&t($a),e&&t(f),e&&t(Ia),e&&t(Ei),e&&t(Pa),e&&t(I),e&&t(wa),e&&t(zi),e&&t(ya),e&&t(O),z(Oe),e&&t(Aa),e&&t(P),e&&t(La),e&&t(de),e&&t(Ma),e&&t(w),e&&t(Na),e&&t(C),e&&t(Ga),e&&t(ce),e&&t(Sa),e&&t(U),e&&t(Ra),e&&t(Q),z(je),e&&t(xa),e&&t(bi),e&&t(Da),e&&t(Ve),e&&t(Ba),e&&t(Ti),e&&t(Oa),e&&t(H),e&&t(Ca),z(Je,e),e&&t(Ua),e&&t(ki),e&&t(Qa),e&&t(qi),e&&t(Ha),e&&t($i),e&&t(ja),e&&t(j),z(We),e&&t(Va),z(Ze,e),e&&t(Ya),e&&t(Ii),e&&t(Xa),e&&t(V),e&&t(Fa),e&&t(Pi),e&&t(Ja),e&&t(y),e&&t(Wa),e&&t(A),e&&t(Za),e&&t(me),e&&t(Ka),e&&t(Y),e&&t(er),e&&t(wi),e&&t(ir),e&&t(yi),e&&t(tr),e&&t(X),z(ai),e&&t(ar),e&&t(Ai),e&&t(rr),z(ri,e),e&&t(or),e&&t(F),z(oi),e&&t(lr),e&&t(Li),e&&t(nr),e&&t(he),e&&t(sr),e&&t(J),e&&t(dr),e&&t(Gi),e&&t(cr),e&&t(L),e&&t(ur),e&&t(xi),e&&t(pr),e&&t(W),z(si),e&&t(mr),e&&t(M),e&&t(fr),e&&t(Di),e&&t(vr),e&&t(Bi),e&&t(hr),e&&t(Oi),e&&t(gr),e&&t(Z),z(ci),e&&t(_r),e&&t(Ci),e&&t(Er),e&&t(Ui),e&&t(zr),e&&t(Qi),e&&t(br),e&&t(K),e&&t(Tr),e&&t(ze),e&&t(kr),e&&t(be),e&&t(qr),e&&t(ee),z(mi),e&&t($r),e&&t(b),e&&t(Ir),e&&t(N),e&&t(Pr),e&&t(T)}}}const au={local:"come-funzionano-i-transformer",sections:[{local:"un-po-di-storia-dei-transformer",title:"Un po' di storia dei Transformer"},{local:"i-transformer-sono-modelli-linguistici",title:"I Transformer sono modelli linguistici"},{local:"i-transformers-sono-modelli-enormi",title:"I Transformers sono modelli enormi"},{local:"transfer-learning",title:"Transfer Learning"},{local:"architettura-generale",title:"Architettura generale"},{local:"introduzione",title:"Introduzione"},{local:"attention-layers",title:"Attention layers"},{local:"larchitettura-originale",title:"L'architettura originale"},{local:"architetture-vs-checkpoint",title:"Architetture vs. checkpoint"}],title:"Come funzionano i Transformer?"};function ru(ys){return iu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class su extends Jc{constructor(R){super();Wc(this,R,ru,tu,Zc,{})}}export{su as default,au as metadata};
