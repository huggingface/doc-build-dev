import{S as Bm,i as Dm,s as Cm,e as r,k as u,w as v,t as i,M as Vm,c as s,d as o,m as p,a as t,x as h,h as d,b as m,N as c,G as a,g as n,y as g,L as Um,q,o as E,B as _,v as Hm}from"../../chunks/vendor-hf-doc-builder.js";import{Y as El}from"../../chunks/Youtube-hf-doc-builder.js";import{I as y}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Jm(_l){let O,gr,G,ee,Za,$e,at,Ka,ot,qr,ha,rt,Er,M,ae,eo,Pe,st,ao,tt,_r,ga,it,br,R,ke,bl,dt,Ae,Tl,Tr,oe,lt,xe,nt,mt,$r,f,oo,re,ro,ut,pt,we,ct,ft,vt,so,se,to,ht,gt,ze,qt,Et,_t,io,te,lo,bt,Tt,je,$t,Pt,kt,no,ie,mo,At,xt,Ie,wt,zt,jt,uo,$,po,It,Nt,Ne,yt,Ot,ye,Gt,Mt,Rt,co,P,fo,St,Lt,Oe,Bt,Dt,vo,Ct,Vt,Pr,qa,Ut,kr,k,Ge,Ht,ho,Jt,Ft,Xt,Me,Yt,go,Qt,Wt,Zt,Re,Kt,qo,ei,ai,Ar,Ea,oi,xr,S,de,Eo,Se,ri,_o,si,wr,le,ti,bo,ii,di,zr,ne,li,To,ni,mi,jr,A,ui,$o,pi,ci,Po,fi,vi,Ir,L,Le,$l,hi,Be,Pl,Nr,me,gi,ko,qi,Ei,yr,B,De,kl,_i,Ce,Al,Or,D,ue,Ao,Ve,bi,xo,Ti,Gr,_a,$i,Mr,Ue,He,xl,Rr,ba,Pi,Sr,C,Je,wl,ki,Fe,zl,Lr,Xe,Br,Ta,Ai,Dr,$a,xi,Cr,Pa,wi,Vr,V,pe,wo,Ye,zi,zo,ji,Ur,Qe,Hr,We,jo,Ii,Ni,Jr,U,Ze,jl,yi,Ke,Il,Fr,ka,Oi,Xr,H,Io,Gi,Mi,No,Ri,Si,Yr,x,yo,Li,Bi,Oo,Di,Ci,Go,Vi,Qr,ce,Ui,Mo,Hi,Ji,Wr,J,ea,Nl,Fi,aa,yl,Zr,Aa,Xi,Kr,xa,Yi,es,F,fe,Ro,oa,Qi,So,Wi,as,wa,Zi,os,ra,rs,X,ve,Lo,sa,Ki,Bo,ed,ss,za,ad,ts,he,ja,Do,od,rd,sd,Ia,Co,td,id,is,Y,ta,Ol,dd,ia,Gl,ds,Na,ld,ls,w,ya,Vo,nd,md,ud,Oa,Uo,pd,cd,fd,ge,Ho,vd,hd,Jo,gd,qd,ns,Ga,Ed,ms,Q,qe,Fo,da,_d,Xo,bd,us,z,Td,Yo,$d,Pd,la,kd,Ad,ps,Ma,xd,cs,Ra,wd,fs,Sa,zd,vs,W,Ee,Qo,na,jd,Wo,Id,hs,La,Nd,gs,Ba,yd,qs,Da,Od,Es,Z,ma,Ml,Gd,ua,Rl,_s,Ca,Md,bs,_e,Rd,Zo,Sd,Ld,Ts,K,be,Ko,pa,Bd,er,Dd,$s,b,Cd,ar,Vd,Ud,or,Hd,Jd,rr,Fd,Xd,Ps,j,Va,sr,Yd,Qd,Wd,Ua,tr,Zd,Kd,el,I,ir,al,ol,dr,rl,sl,lr,tl,il,ks,N,dl,nr,ll,nl,mr,ml,ul,As;return $e=new y({}),Pe=new y({}),Se=new y({}),Ve=new y({}),Xe=new El({props:{id:"ftWlj4FBHTg"}}),Ye=new y({}),Qe=new El({props:{id:"BqqfQnyjmgg"}}),oa=new y({}),ra=new El({props:{id:"H39Z_720T5s"}}),sa=new y({}),da=new y({}),na=new y({}),pa=new y({}),{c(){O=r("meta"),gr=u(),G=r("h1"),ee=r("a"),Za=r("span"),v($e.$$.fragment),at=u(),Ka=r("span"),ot=i("Como os Transformers trabalham?"),qr=u(),ha=r("p"),rt=i("Nessa se\xE7\xE3o, n\xF3s olharemos para o alto n\xEDvel de arquitetura dos modelos Transformers."),Er=u(),M=r("h2"),ae=r("a"),eo=r("span"),v(Pe.$$.fragment),st=u(),ao=r("span"),tt=i("Um pouco da hist\xF3ria dos Transformers"),_r=u(),ga=r("p"),it=i("Aqui alguns pontos de refer\xEAncia na (pequena) hist\xF3ria dos modelos Transformers:"),br=u(),R=r("div"),ke=r("img"),dt=u(),Ae=r("img"),Tr=u(),oe=r("p"),lt=i("A "),xe=r("a"),nt=i("arquitetura Transformer"),mt=i(" foi introduzida em Junho de 2017. O foco de pesquisa original foi para tarefas de tradu\xE7\xE3o. Isso foi seguido pela introdu\xE7\xE3o de muitos modelos influentes, incluindo:"),$r=u(),f=r("ul"),oo=r("li"),re=r("p"),ro=r("strong"),ut=i("Junho de 2018"),pt=i(": "),we=r("a"),ct=i("GPT"),ft=i(", o primeiro modelo Transformer pr\xE9-treinado, usado para ajuste-fino em v\xE1rias tarefas de NLP e obtendo resultados estado-da-arte"),vt=u(),so=r("li"),se=r("p"),to=r("strong"),ht=i("Outubro de 2018"),gt=i(": "),ze=r("a"),qt=i("BERT"),Et=i(", outro grande modelo pr\xE9-treinado, esse outro foi designado para produzir melhores resumos de senten\xE7as(mais sobre isso no pr\xF3ximo cap\xEDtulo!)"),_t=u(),io=r("li"),te=r("p"),lo=r("strong"),bt=i("Fevereiro de 2019"),Tt=i(": "),je=r("a"),$t=i("GPT-2"),Pt=i(", uma melhor (e maior) vers\xE3o da GPT que n\xE3o foi imediatamente publicizado o seu lan\xE7amento devido a preocupa\xE7\xF5es \xE9ticas [N.T.: n\xE3o apenas por isso]"),kt=u(),no=r("li"),ie=r("p"),mo=r("strong"),At=i("Outubro de 2019"),xt=i(": "),Ie=r("a"),wt=i("DistilBERT"),zt=i(", uma vers\xE3o destilada do BERT que \xE9 60% mais r\xE1pidam 40% mais leve em mem\xF3ria, e ainda ret\xE9m 97% da performance do BERT"),jt=u(),uo=r("li"),$=r("p"),po=r("strong"),It=i("Outubro de 2019"),Nt=i(": "),Ne=r("a"),yt=i("BART"),Ot=i(" e "),ye=r("a"),Gt=i("T5"),Mt=i(", dois grandes modelos pr\xE9-treinados usando a mesma arquitetura do modelo original Transformer (os primeiros a fazerem at\xE9 ent\xE3o)"),Rt=u(),co=r("li"),P=r("p"),fo=r("strong"),St=i("Maio de 2020"),Lt=i(", "),Oe=r("a"),Bt=i("GPT-3"),Dt=i(", uma vers\xE3o ainda maior da GPT-2 que \xE9 capaz de performar bem em uma variedade de tarefas sem a necessidade de ajuste-fino (chamado de aprendizagem"),vo=r("em"),Ct=i("zero-shot"),Vt=i(")"),Pr=u(),qa=r("p"),Ut=i("Esta lista est\xE1 longe de ser abrangente e destina-se apenas a destacar alguns dos diferentes tipos de modelos de Transformers. Em linhas gerais, eles podem ser agrupados em tr\xEAs categorias:"),kr=u(),k=r("ul"),Ge=r("li"),Ht=i("GPT-like (tamb\xE9m chamados de modelos Transformers "),ho=r("em"),Jt=i("auto-regressivos"),Ft=i(")"),Xt=u(),Me=r("li"),Yt=i("BERT-like (tamb\xE9m chamados de modelos Transformers "),go=r("em"),Qt=i("auto-codificadores"),Wt=i(")"),Zt=u(),Re=r("li"),Kt=i("BART/T5-like (tamb\xE9m chamados de modelos Transformers "),qo=r("em"),ei=i("sequence-to-sequence"),ai=i(")"),Ar=u(),Ea=r("p"),oi=i("Vamos mergulhar nessas fam\xEDlias com mais profundidade mais adiante"),xr=u(),S=r("h2"),de=r("a"),Eo=r("span"),v(Se.$$.fragment),ri=u(),_o=r("span"),si=i("Transformers s\xE3o modelos de linguagem"),wr=u(),le=r("p"),ti=i("Todos os modelos de Transformer mencionados acima (GPT, BERT, BART, T5, etc.) foram treinados como "),bo=r("em"),ii=i("modelos de linguagem"),di=i(". Isso significa que eles foram treinados em grandes quantidades de texto bruto de forma auto-supervisionada. O aprendizado autossupervisionado \xE9 um tipo de treinamento no qual o objetivo \xE9 calculado automaticamente a partir das entradas do modelo. Isso significa que os humanos n\xE3o s\xE3o necess\xE1rios para rotular os dados!"),zr=u(),ne=r("p"),li=i("Este tipo de modelo desenvolve uma compreens\xE3o estat\xEDstica da linguagem em que foi treinado, mas n\xE3o \xE9 muito \xFAtil para tarefas pr\xE1ticas espec\xEDficas. Por causa disso, o modelo geral pr\xE9-treinado passa por um processo chamado "),To=r("em"),ni=i("aprendizagem de transfer\xEAncia"),mi=i(". Durante esse processo, o modelo \xE9 ajustado de maneira supervisionada - ou seja, usando r\xF3tulos anotados por humanos - em uma determinada tarefa."),jr=u(),A=r("p"),ui=i("Um exemplo de tarefa \xE9 prever a pr\xF3xima palavra em uma frase depois de ler as "),$o=r("em"),pi=i("n"),ci=i(" palavras anteriores. Isso \xE9 chamado de "),Po=r("em"),fi=i("modelagem de linguagem causal"),vi=i(" porque a sa\xEDda depende das entradas passadas e presentes, mas n\xE3o das futuras."),Ir=u(),L=r("div"),Le=r("img"),hi=u(),Be=r("img"),Nr=u(),me=r("p"),gi=i("Outro exemplo \xE9 a "),ko=r("em"),qi=i("modelagem de linguagem mascarada"),Ei=i(", na qual o modelo prev\xEA uma palavra mascarada na frase."),yr=u(),B=r("div"),De=r("img"),_i=u(),Ce=r("img"),Or=u(),D=r("h2"),ue=r("a"),Ao=r("span"),v(Ve.$$.fragment),bi=u(),xo=r("span"),Ti=i("Transformers s\xE3o modelos grandes"),Gr=u(),_a=r("p"),$i=i("Al\xE9m de alguns outliers (como o DistilBERT), a estrat\xE9gia geral para obter melhor desempenho \xE9 aumentar os tamanhos dos modelos, bem como a quantidade de dados em que s\xE3o pr\xE9-treinados."),Mr=u(),Ue=r("div"),He=r("img"),Rr=u(),ba=r("p"),Pi=i("Infelizmente, treinar um modelo, especialmente um grande, requer uma grande quantidade de dados. Isso se torna muito caro em termos de tempo e recursos de computa\xE7\xE3o. At\xE9 se traduz em impacto ambiental, como pode ser visto no gr\xE1fico a seguir."),Sr=u(),C=r("div"),Je=r("img"),ki=u(),Fe=r("img"),Lr=u(),v(Xe.$$.fragment),Br=u(),Ta=r("p"),Ai=i("E isso mostra um projeto para um modelo (muito grande) liderado por uma equipe que tenta conscientemente reduzir o impacto ambiental do pr\xE9-treinamento. Os gastos de executar muitos testes para obter os melhores hiperpar\xE2metros seria ainda maior."),Dr=u(),$a=r("p"),xi=i("Imagine se cada vez que uma equipe de pesquisa, uma organiza\xE7\xE3o estudantil ou uma empresa quisesse treinar um modelo, o fizesse do zero. Isso levaria a custos globais enormes e desnecess\xE1rios!"),Cr=u(),Pa=r("p"),wi=i("\xC9 por isso que compartilhar modelos de linguagem \xE9 fundamental: compartilhar os pesos treinados e construir em cima dos pesos j\xE1 treinados reduz o custo geral de computa\xE7\xE3o e os gastos de carbono da comunidade."),Vr=u(),V=r("h2"),pe=r("a"),wo=r("span"),v(Ye.$$.fragment),zi=u(),zo=r("span"),ji=i("Transfer\xEAncia de Aprendizagem"),Ur=u(),v(Qe.$$.fragment),Hr=u(),We=r("p"),jo=r("em"),Ii=i("Pr\xE9-treinamento"),Ni=i(" \xE9 o ato de treinar um modelo do zero: os pesos s\xE3o inicializados aleatoriamente e o treinamento come\xE7a sem nenhum conhecimento pr\xE9vio."),Jr=u(),U=r("div"),Ze=r("img"),yi=u(),Ke=r("img"),Fr=u(),ka=r("p"),Oi=i("Esse pr\xE9-treinamento geralmente \xE9 feito em grandes quantidades de dados. Portanto, requer um corpus de dados muito grande e o treinamento pode levar v\xE1rias semanas."),Xr=u(),H=r("p"),Io=r("em"),Gi=i("Ajuste fino"),Mi=i(", por outro lado, \xE9 o treinamento feito "),No=r("strong"),Ri=i("ap\xF3s"),Si=i(" um modelo ter sido pr\xE9-treinado. Para realizar o ajuste fino, primeiro voc\xEA adquire um modelo de linguagem pr\xE9-treinado e, em seguida, realiza treinamento adicional com um conjunto de dados espec\xEDfico para sua tarefa. Espere - por que n\xE3o simplesmente treinar diretamente para a tarefa final? Existem algumas raz\xF5es:"),Yr=u(),x=r("ul"),yo=r("li"),Li=i("O modelo pr\xE9-treinado j\xE1 foi treinado em um conjunto de dados que possui algumas semelhan\xE7as com o conjunto de dados de ajuste fino. O processo de ajuste fino \xE9, portanto, capaz de aproveitar o conhecimento adquirido pelo modelo inicial durante o pr\xE9-treinamento (por exemplo, com problemas de NLP, o modelo pr\xE9-treinado ter\xE1 algum tipo de compreens\xE3o estat\xEDstica da linguagem que voc\xEA est\xE1 usando para sua tarefa)."),Bi=u(),Oo=r("li"),Di=i("Como o modelo pr\xE9-treinado j\xE1 foi treinado com muitos dados, o ajuste fino requer muito menos dados para obter resultados decentes."),Ci=u(),Go=r("li"),Vi=i("Pela mesma raz\xE3o, a quantidade de tempo e recursos necess\xE1rios para obter bons resultados s\xE3o muito menores."),Qr=u(),ce=r("p"),Ui=i("Por exemplo, pode-se alavancar um modelo pr\xE9-treinado treinado no idioma ingl\xEAs e depois ajust\xE1-lo em um corpus arXiv, resultando em um modelo baseado em ci\xEAncia/pesquisa. O ajuste fino exigir\xE1 apenas uma quantidade limitada de dados: o conhecimento que o modelo pr\xE9-treinado adquiriu \xE9 \u201Ctransferido\u201D, da\xED o termo "),Mo=r("em"),Hi=i("aprendizagem de transfer\xEAncia"),Ji=i("."),Wr=u(),J=r("div"),ea=r("img"),Fi=u(),aa=r("img"),Zr=u(),Aa=r("p"),Xi=i("O ajuste fino de um modelo, portanto, tem menores custos de tempo, dados, financeiros e ambientais. Tamb\xE9m \xE9 mais r\xE1pido e f\xE1cil iterar em diferentes esquemas de ajuste fino, pois o treinamento \xE9 menos restritivo do que um pr\xE9-treinamento completo."),Kr=u(),xa=r("p"),Yi=i("Esse processo tamb\xE9m alcan\xE7ar\xE1 melhores resultados do que treinar do zero (a menos que voc\xEA tenha muitos dados), e \xE9 por isso que voc\xEA deve sempre tentar alavancar um modelo pr\xE9-treinado - um o mais pr\xF3ximo poss\xEDvel da tarefa que voc\xEA tem em m\xE3os - e  ent\xE3o fazer seu ajuste fino."),es=u(),F=r("h2"),fe=r("a"),Ro=r("span"),v(oa.$$.fragment),Qi=u(),So=r("span"),Wi=i("Arquitetura geral"),as=u(),wa=r("p"),Zi=i("Nesta se\xE7\xE3o, veremos a arquitetura geral do modelo Transformer. N\xE3o se preocupe se voc\xEA n\xE3o entender alguns dos conceitos; h\xE1 se\xE7\xF5es detalhadas posteriormente cobrindo cada um dos componentes."),os=u(),v(ra.$$.fragment),rs=u(),X=r("h2"),ve=r("a"),Lo=r("span"),v(sa.$$.fragment),Ki=u(),Bo=r("span"),ed=i("Introdu\xE7\xE3o"),ss=u(),za=r("p"),ad=i("O modelo \xE9 principalmente composto por dois blocos:"),ts=u(),he=r("ul"),ja=r("li"),Do=r("strong"),od=i("Codificador (esquerda)"),rd=i(": O codificador recebe uma entrada e constr\xF3i uma representa\xE7\xE3o dela (seus recursos). Isso significa que o modelo \xE9 otimizado para adquirir entendimento da entrada."),sd=u(),Ia=r("li"),Co=r("strong"),td=i("Decodificador (\xE0 direita)"),id=i(": O decodificador usa a representa\xE7\xE3o do codificador (recursos) junto com outras entradas para gerar uma sequ\xEAncia de destino. Isso significa que o modelo \xE9 otimizado para gerar sa\xEDdas."),is=u(),Y=r("div"),ta=r("img"),dd=u(),ia=r("img"),ds=u(),Na=r("p"),ld=i("Cada uma dessas partes pode ser usada de forma independente, dependendo da tarefa:"),ls=u(),w=r("ul"),ya=r("li"),Vo=r("strong"),nd=i("Modelos somente de codificador"),md=i(": bom para tarefas que exigem compreens\xE3o da entrada, como classifica\xE7\xE3o de senten\xE7a e reconhecimento de entidade nomeada."),ud=u(),Oa=r("li"),Uo=r("strong"),pd=i("Modelos somente decodificadores"),cd=i(": bom para tarefas generativas, como gera\xE7\xE3o de texto."),fd=u(),ge=r("li"),Ho=r("strong"),vd=i("Modelos de codificador-decodificador"),hd=i(" ou "),Jo=r("strong"),gd=i("modelos de sequ\xEAncia a sequ\xEAncia"),qd=i(": bom para tarefas generativas que exigem uma entrada, como tradu\xE7\xE3o ou resumo. (corrigit sequence to sequence)"),ns=u(),Ga=r("p"),Ed=i("Vamos mergulhar nessas arquiteturas de forma independente em se\xE7\xF5es posteriores."),ms=u(),Q=r("h2"),qe=r("a"),Fo=r("span"),v(da.$$.fragment),_d=u(),Xo=r("span"),bd=i("Camadas de Aten\xE7\xE3o"),us=u(),z=r("p"),Td=i("Uma caracter\xEDstica chave dos modelos Transformer \xE9 que eles s\xE3o constru\xEDdos com camadas especiais chamadas "),Yo=r("em"),$d=i("camadas de aten\xE7\xE3o"),Pd=i(". Na verdade, o t\xEDtulo do artigo que apresenta a arquitetura do Transformer era "),la=r("a"),kd=i("\u201CAten\xE7\xE3o \xE9 tudo que voc\xEA precisa\u201D"),Ad=i("! Exploraremos os detalhes das camadas de aten\xE7\xE3o posteriormente no curso; por enquanto, tudo o que voc\xEA precisa saber \xE9 que essa camada dir\xE1 ao modelo para prestar aten\xE7\xE3o espec\xEDfica a certas palavras na frase que voc\xEA passou (e mais ou menos ignorar as outras) ao lidar com a representa\xE7\xE3o de cada palavra."),ps=u(),Ma=r("p"),xd=i("Para contextualizar, considere a tarefa de traduzir o texto do portugu\xEAs para o franc\xEAs. Dada a entrada \u201CVoc\xEA gosta deste curso\u201D, um modelo de tradu\xE7\xE3o precisar\xE1 atender tamb\xE9m \xE0 palavra adjacente \u201CVoc\xEA\u201D para obter a tradu\xE7\xE3o adequada para a palavra \u201Cgosta\u201D, pois em franc\xEAs o verbo \u201Cgostar\u201D \xE9 conjugado de forma diferente dependendo o sujeito. O resto da frase, no entanto, n\xE3o \xE9 \xFAtil para a tradu\xE7\xE3o dessa palavra. Na mesma linha, ao traduzir \u201Cdeste\u201D o modelo tamb\xE9m precisar\xE1 prestar aten\xE7\xE3o \xE0 palavra \u201Ccurso\u201D, pois \u201Cdeste\u201D traduz-se de forma diferente dependendo se o substantivo associado \xE9 masculino ou feminino. Novamente, as outras palavras na frase n\xE3o importar\xE3o para a tradu\xE7\xE3o de \u201Cdeste\u201D. Com frases mais complexas (e regras gramaticais mais complexas), o modelo precisaria prestar aten\xE7\xE3o especial \xE0s palavras que podem aparecer mais distantes na frase para traduzir adequadamente cada palavra."),cs=u(),Ra=r("p"),wd=i("O mesmo conceito se aplica a qualquer tarefa associada \xE0 linguagem natural: uma palavra por si s\xF3 tem um significado, mas esse significado \xE9 profundamente afetado pelo contexto, que pode ser qualquer outra palavra (ou palavras) antes ou depois da palavra que est\xE1 sendo estudada."),fs=u(),Sa=r("p"),zd=i("Agora que voc\xEA tem uma ideia do que s\xE3o as camadas de aten\xE7\xE3o, vamos dar uma olhada mais de perto na arquitetura do Transformer."),vs=u(),W=r("h2"),Ee=r("a"),Qo=r("span"),v(na.$$.fragment),jd=u(),Wo=r("span"),Id=i("A arquitetura original"),hs=u(),La=r("p"),Nd=i("A arquitetura Transformer foi originalmente projetada para tradu\xE7\xE3o. Durante o treinamento, o codificador recebe entradas (frases) em um determinado idioma, enquanto o decodificador recebe as mesmas frases no idioma de destino desejado. No codificador, as camadas de aten\xE7\xE3o podem usar todas as palavras em uma frase (j\xE1 que, como acabamos de ver, a tradu\xE7\xE3o de uma determinada palavra pode ser dependente do que est\xE1 depois e antes dela na frase). O decodificador, no entanto, funciona sequencialmente e s\xF3 pode prestar aten\xE7\xE3o nas palavras da frase que ele j\xE1 traduziu (portanto, apenas as palavras anteriores \xE0 palavra que est\xE1 sendo gerada no momento). Por exemplo, quando previmos as tr\xEAs primeiras palavras do alvo traduzido, as entregamos ao decodificador que ent\xE3o usa todas as entradas do codificador para tentar prever a quarta palavra."),gs=u(),Ba=r("p"),yd=i("Para acelerar as coisas durante o treinamento (quando o modelo tem acesso \xE0s frases alvo), o decodificador \xE9 alimentado com todo o alvo, mas n\xE3o \xE9 permitido usar palavras futuras (se teve acesso \xE0 palavra na posi\xE7\xE3o 2 ao tentar prever a palavra na posi\xE7\xE3o 2, o problema n\xE3o seria muito dif\xEDcil!). Por exemplo, ao tentar prever a quarta palavra, a camada de aten\xE7\xE3o s\xF3 ter\xE1 acesso \xE0s palavras nas posi\xE7\xF5es 1 a 3."),qs=u(),Da=r("p"),Od=i("A arquitetura original do Transformer ficou assim, com o codificador \xE0 esquerda e o decodificador \xE0 direita:"),Es=u(),Z=r("div"),ma=r("img"),Gd=u(),ua=r("img"),_s=u(),Ca=r("p"),Md=i("Observe que a primeira camada de aten\xE7\xE3o em um bloco decodificador presta aten\xE7\xE3o a todas as entradas (passadas) do decodificador, mas a segunda camada de aten\xE7\xE3o usa a sa\xEDda do codificador. Ele pode, assim, acessar toda a frase de entrada para melhor prever a palavra atual. Isso \xE9 muito \xFAtil, pois diferentes idiomas podem ter regras gramaticais que colocam as palavras em ordens diferentes, ou algum contexto fornecido posteriormente na frase pode ser \xFAtil para determinar a melhor tradu\xE7\xE3o de uma determinada palavra."),bs=u(),_e=r("p"),Rd=i("A "),Zo=r("em"),Sd=i("m\xE1scara de aten\xE7\xE3o"),Ld=i(" tamb\xE9m pode ser usada no codificador/decodificador para evitar que o modelo preste aten\xE7\xE3o a algumas palavras especiais - por exemplo, a palavra de preenchimento especial usada para fazer com que todas as entradas tenham o mesmo comprimento ao agrupar frases."),Ts=u(),K=r("h2"),be=r("a"),Ko=r("span"),v(pa.$$.fragment),Bd=u(),er=r("span"),Dd=i("Arquiteturas vs. checkpoints"),$s=u(),b=r("p"),Cd=i("\xC0 medida que nos aprofundarmos nos modelos do Transformer neste curso, voc\xEA ver\xE1 men\xE7\xF5es a "),ar=r("em"),Vd=i("arquiteturas"),Ud=i(" e "),or=r("em"),Hd=i("checkpoints"),Jd=i(", bem como "),rr=r("em"),Fd=i("modelos"),Xd=i(". Todos esses termos t\xEAm significados ligeiramente diferentes:"),Ps=u(),j=r("ul"),Va=r("li"),sr=r("strong"),Yd=i("Arquitetura"),Qd=i(": Este \xE9 o esqueleto do modelo \u2014 a defini\xE7\xE3o de cada camada e cada opera\xE7\xE3o que acontece dentro do modelo."),Wd=u(),Ua=r("li"),tr=r("strong"),Zd=i("Checkpoints"),Kd=i(": Esses s\xE3o os pesos que ser\xE3o carregados em uma determinada arquitetura."),el=u(),I=r("li"),ir=r("strong"),al=i("Modelos"),ol=i(": Este \xE9 um termo abrangente que n\xE3o \xE9 t\xE3o preciso quanto \u201Carquitetura\u201D ou \u201Ccheckpoint\u201D: pode significar ambos. Este curso especificar\xE1 "),dr=r("em"),rl=i("arquitetura"),sl=i(" ou "),lr=r("em"),tl=i("checkpoint"),il=i(" quando for necess\xE1rio reduzir a ambiguidade."),ks=u(),N=r("p"),dl=i("Por exemplo, BERT \xE9 uma arquitetura enquanto "),nr=r("code"),ll=i("bert-base-cased"),nl=i(", um conjunto de pesos treinados pela equipe do Google para a primeira vers\xE3o do BERT, \xE9 um checkpoint. No entanto, pode-se dizer \u201Co modelo BERT\u201D e \u201Co modelo "),mr=r("code"),ml=i("bert-base-cased"),ul=i("\u201C."),this.h()},l(e){const l=Vm('[data-svelte="svelte-1phssyn"]',document.head);O=s(l,"META",{name:!0,content:!0}),l.forEach(o),gr=p(e),G=s(e,"H1",{class:!0});var xs=t(G);ee=s(xs,"A",{id:!0,class:!0,href:!0});var Sl=t(ee);Za=s(Sl,"SPAN",{});var Ll=t(Za);h($e.$$.fragment,Ll),Ll.forEach(o),Sl.forEach(o),at=p(xs),Ka=s(xs,"SPAN",{});var Bl=t(Ka);ot=d(Bl,"Como os Transformers trabalham?"),Bl.forEach(o),xs.forEach(o),qr=p(e),ha=s(e,"P",{});var Dl=t(ha);rt=d(Dl,"Nessa se\xE7\xE3o, n\xF3s olharemos para o alto n\xEDvel de arquitetura dos modelos Transformers."),Dl.forEach(o),Er=p(e),M=s(e,"H2",{class:!0});var ws=t(M);ae=s(ws,"A",{id:!0,class:!0,href:!0});var Cl=t(ae);eo=s(Cl,"SPAN",{});var Vl=t(eo);h(Pe.$$.fragment,Vl),Vl.forEach(o),Cl.forEach(o),st=p(ws),ao=s(ws,"SPAN",{});var Ul=t(ao);tt=d(Ul,"Um pouco da hist\xF3ria dos Transformers"),Ul.forEach(o),ws.forEach(o),_r=p(e),ga=s(e,"P",{});var Hl=t(ga);it=d(Hl,"Aqui alguns pontos de refer\xEAncia na (pequena) hist\xF3ria dos modelos Transformers:"),Hl.forEach(o),br=p(e),R=s(e,"DIV",{class:!0});var zs=t(R);ke=s(zs,"IMG",{class:!0,src:!0,alt:!0}),dt=p(zs),Ae=s(zs,"IMG",{class:!0,src:!0,alt:!0}),zs.forEach(o),Tr=p(e),oe=s(e,"P",{});var js=t(oe);lt=d(js,"A "),xe=s(js,"A",{href:!0,rel:!0});var Jl=t(xe);nt=d(Jl,"arquitetura Transformer"),Jl.forEach(o),mt=d(js," foi introduzida em Junho de 2017. O foco de pesquisa original foi para tarefas de tradu\xE7\xE3o. Isso foi seguido pela introdu\xE7\xE3o de muitos modelos influentes, incluindo:"),js.forEach(o),$r=p(e),f=s(e,"UL",{});var T=t(f);oo=s(T,"LI",{});var Fl=t(oo);re=s(Fl,"P",{});var ur=t(re);ro=s(ur,"STRONG",{});var Xl=t(ro);ut=d(Xl,"Junho de 2018"),Xl.forEach(o),pt=d(ur,": "),we=s(ur,"A",{href:!0,rel:!0});var Yl=t(we);ct=d(Yl,"GPT"),Yl.forEach(o),ft=d(ur,", o primeiro modelo Transformer pr\xE9-treinado, usado para ajuste-fino em v\xE1rias tarefas de NLP e obtendo resultados estado-da-arte"),ur.forEach(o),Fl.forEach(o),vt=p(T),so=s(T,"LI",{});var Ql=t(so);se=s(Ql,"P",{});var pr=t(se);to=s(pr,"STRONG",{});var Wl=t(to);ht=d(Wl,"Outubro de 2018"),Wl.forEach(o),gt=d(pr,": "),ze=s(pr,"A",{href:!0,rel:!0});var Zl=t(ze);qt=d(Zl,"BERT"),Zl.forEach(o),Et=d(pr,", outro grande modelo pr\xE9-treinado, esse outro foi designado para produzir melhores resumos de senten\xE7as(mais sobre isso no pr\xF3ximo cap\xEDtulo!)"),pr.forEach(o),Ql.forEach(o),_t=p(T),io=s(T,"LI",{});var Kl=t(io);te=s(Kl,"P",{});var cr=t(te);lo=s(cr,"STRONG",{});var en=t(lo);bt=d(en,"Fevereiro de 2019"),en.forEach(o),Tt=d(cr,": "),je=s(cr,"A",{href:!0,rel:!0});var an=t(je);$t=d(an,"GPT-2"),an.forEach(o),Pt=d(cr,", uma melhor (e maior) vers\xE3o da GPT que n\xE3o foi imediatamente publicizado o seu lan\xE7amento devido a preocupa\xE7\xF5es \xE9ticas [N.T.: n\xE3o apenas por isso]"),cr.forEach(o),Kl.forEach(o),kt=p(T),no=s(T,"LI",{});var on=t(no);ie=s(on,"P",{});var fr=t(ie);mo=s(fr,"STRONG",{});var rn=t(mo);At=d(rn,"Outubro de 2019"),rn.forEach(o),xt=d(fr,": "),Ie=s(fr,"A",{href:!0,rel:!0});var sn=t(Ie);wt=d(sn,"DistilBERT"),sn.forEach(o),zt=d(fr,", uma vers\xE3o destilada do BERT que \xE9 60% mais r\xE1pidam 40% mais leve em mem\xF3ria, e ainda ret\xE9m 97% da performance do BERT"),fr.forEach(o),on.forEach(o),jt=p(T),uo=s(T,"LI",{});var tn=t(uo);$=s(tn,"P",{});var ca=t($);po=s(ca,"STRONG",{});var dn=t(po);It=d(dn,"Outubro de 2019"),dn.forEach(o),Nt=d(ca,": "),Ne=s(ca,"A",{href:!0,rel:!0});var ln=t(Ne);yt=d(ln,"BART"),ln.forEach(o),Ot=d(ca," e "),ye=s(ca,"A",{href:!0,rel:!0});var nn=t(ye);Gt=d(nn,"T5"),nn.forEach(o),Mt=d(ca,", dois grandes modelos pr\xE9-treinados usando a mesma arquitetura do modelo original Transformer (os primeiros a fazerem at\xE9 ent\xE3o)"),ca.forEach(o),tn.forEach(o),Rt=p(T),co=s(T,"LI",{});var mn=t(co);P=s(mn,"P",{});var fa=t(P);fo=s(fa,"STRONG",{});var un=t(fo);St=d(un,"Maio de 2020"),un.forEach(o),Lt=d(fa,", "),Oe=s(fa,"A",{href:!0,rel:!0});var pn=t(Oe);Bt=d(pn,"GPT-3"),pn.forEach(o),Dt=d(fa,", uma vers\xE3o ainda maior da GPT-2 que \xE9 capaz de performar bem em uma variedade de tarefas sem a necessidade de ajuste-fino (chamado de aprendizagem"),vo=s(fa,"EM",{});var cn=t(vo);Ct=d(cn,"zero-shot"),cn.forEach(o),Vt=d(fa,")"),fa.forEach(o),mn.forEach(o),T.forEach(o),Pr=p(e),qa=s(e,"P",{});var fn=t(qa);Ut=d(fn,"Esta lista est\xE1 longe de ser abrangente e destina-se apenas a destacar alguns dos diferentes tipos de modelos de Transformers. Em linhas gerais, eles podem ser agrupados em tr\xEAs categorias:"),fn.forEach(o),kr=p(e),k=s(e,"UL",{});var Ha=t(k);Ge=s(Ha,"LI",{});var Is=t(Ge);Ht=d(Is,"GPT-like (tamb\xE9m chamados de modelos Transformers "),ho=s(Is,"EM",{});var vn=t(ho);Jt=d(vn,"auto-regressivos"),vn.forEach(o),Ft=d(Is,")"),Is.forEach(o),Xt=p(Ha),Me=s(Ha,"LI",{});var Ns=t(Me);Yt=d(Ns,"BERT-like (tamb\xE9m chamados de modelos Transformers "),go=s(Ns,"EM",{});var hn=t(go);Qt=d(hn,"auto-codificadores"),hn.forEach(o),Wt=d(Ns,")"),Ns.forEach(o),Zt=p(Ha),Re=s(Ha,"LI",{});var ys=t(Re);Kt=d(ys,"BART/T5-like (tamb\xE9m chamados de modelos Transformers "),qo=s(ys,"EM",{});var gn=t(qo);ei=d(gn,"sequence-to-sequence"),gn.forEach(o),ai=d(ys,")"),ys.forEach(o),Ha.forEach(o),Ar=p(e),Ea=s(e,"P",{});var qn=t(Ea);oi=d(qn,"Vamos mergulhar nessas fam\xEDlias com mais profundidade mais adiante"),qn.forEach(o),xr=p(e),S=s(e,"H2",{class:!0});var Os=t(S);de=s(Os,"A",{id:!0,class:!0,href:!0});var En=t(de);Eo=s(En,"SPAN",{});var _n=t(Eo);h(Se.$$.fragment,_n),_n.forEach(o),En.forEach(o),ri=p(Os),_o=s(Os,"SPAN",{});var bn=t(_o);si=d(bn,"Transformers s\xE3o modelos de linguagem"),bn.forEach(o),Os.forEach(o),wr=p(e),le=s(e,"P",{});var Gs=t(le);ti=d(Gs,"Todos os modelos de Transformer mencionados acima (GPT, BERT, BART, T5, etc.) foram treinados como "),bo=s(Gs,"EM",{});var Tn=t(bo);ii=d(Tn,"modelos de linguagem"),Tn.forEach(o),di=d(Gs,". Isso significa que eles foram treinados em grandes quantidades de texto bruto de forma auto-supervisionada. O aprendizado autossupervisionado \xE9 um tipo de treinamento no qual o objetivo \xE9 calculado automaticamente a partir das entradas do modelo. Isso significa que os humanos n\xE3o s\xE3o necess\xE1rios para rotular os dados!"),Gs.forEach(o),zr=p(e),ne=s(e,"P",{});var Ms=t(ne);li=d(Ms,"Este tipo de modelo desenvolve uma compreens\xE3o estat\xEDstica da linguagem em que foi treinado, mas n\xE3o \xE9 muito \xFAtil para tarefas pr\xE1ticas espec\xEDficas. Por causa disso, o modelo geral pr\xE9-treinado passa por um processo chamado "),To=s(Ms,"EM",{});var $n=t(To);ni=d($n,"aprendizagem de transfer\xEAncia"),$n.forEach(o),mi=d(Ms,". Durante esse processo, o modelo \xE9 ajustado de maneira supervisionada - ou seja, usando r\xF3tulos anotados por humanos - em uma determinada tarefa."),Ms.forEach(o),jr=p(e),A=s(e,"P",{});var Ja=t(A);ui=d(Ja,"Um exemplo de tarefa \xE9 prever a pr\xF3xima palavra em uma frase depois de ler as "),$o=s(Ja,"EM",{});var Pn=t($o);pi=d(Pn,"n"),Pn.forEach(o),ci=d(Ja," palavras anteriores. Isso \xE9 chamado de "),Po=s(Ja,"EM",{});var kn=t(Po);fi=d(kn,"modelagem de linguagem causal"),kn.forEach(o),vi=d(Ja," porque a sa\xEDda depende das entradas passadas e presentes, mas n\xE3o das futuras."),Ja.forEach(o),Ir=p(e),L=s(e,"DIV",{class:!0});var Rs=t(L);Le=s(Rs,"IMG",{class:!0,src:!0,alt:!0}),hi=p(Rs),Be=s(Rs,"IMG",{class:!0,src:!0,alt:!0}),Rs.forEach(o),Nr=p(e),me=s(e,"P",{});var Ss=t(me);gi=d(Ss,"Outro exemplo \xE9 a "),ko=s(Ss,"EM",{});var An=t(ko);qi=d(An,"modelagem de linguagem mascarada"),An.forEach(o),Ei=d(Ss,", na qual o modelo prev\xEA uma palavra mascarada na frase."),Ss.forEach(o),yr=p(e),B=s(e,"DIV",{class:!0});var Ls=t(B);De=s(Ls,"IMG",{class:!0,src:!0,alt:!0}),_i=p(Ls),Ce=s(Ls,"IMG",{class:!0,src:!0,alt:!0}),Ls.forEach(o),Or=p(e),D=s(e,"H2",{class:!0});var Bs=t(D);ue=s(Bs,"A",{id:!0,class:!0,href:!0});var xn=t(ue);Ao=s(xn,"SPAN",{});var wn=t(Ao);h(Ve.$$.fragment,wn),wn.forEach(o),xn.forEach(o),bi=p(Bs),xo=s(Bs,"SPAN",{});var zn=t(xo);Ti=d(zn,"Transformers s\xE3o modelos grandes"),zn.forEach(o),Bs.forEach(o),Gr=p(e),_a=s(e,"P",{});var jn=t(_a);$i=d(jn,"Al\xE9m de alguns outliers (como o DistilBERT), a estrat\xE9gia geral para obter melhor desempenho \xE9 aumentar os tamanhos dos modelos, bem como a quantidade de dados em que s\xE3o pr\xE9-treinados."),jn.forEach(o),Mr=p(e),Ue=s(e,"DIV",{class:!0});var In=t(Ue);He=s(In,"IMG",{src:!0,alt:!0,width:!0}),In.forEach(o),Rr=p(e),ba=s(e,"P",{});var Nn=t(ba);Pi=d(Nn,"Infelizmente, treinar um modelo, especialmente um grande, requer uma grande quantidade de dados. Isso se torna muito caro em termos de tempo e recursos de computa\xE7\xE3o. At\xE9 se traduz em impacto ambiental, como pode ser visto no gr\xE1fico a seguir."),Nn.forEach(o),Sr=p(e),C=s(e,"DIV",{class:!0});var Ds=t(C);Je=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),ki=p(Ds),Fe=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),Ds.forEach(o),Lr=p(e),h(Xe.$$.fragment,e),Br=p(e),Ta=s(e,"P",{});var yn=t(Ta);Ai=d(yn,"E isso mostra um projeto para um modelo (muito grande) liderado por uma equipe que tenta conscientemente reduzir o impacto ambiental do pr\xE9-treinamento. Os gastos de executar muitos testes para obter os melhores hiperpar\xE2metros seria ainda maior."),yn.forEach(o),Dr=p(e),$a=s(e,"P",{});var On=t($a);xi=d(On,"Imagine se cada vez que uma equipe de pesquisa, uma organiza\xE7\xE3o estudantil ou uma empresa quisesse treinar um modelo, o fizesse do zero. Isso levaria a custos globais enormes e desnecess\xE1rios!"),On.forEach(o),Cr=p(e),Pa=s(e,"P",{});var Gn=t(Pa);wi=d(Gn,"\xC9 por isso que compartilhar modelos de linguagem \xE9 fundamental: compartilhar os pesos treinados e construir em cima dos pesos j\xE1 treinados reduz o custo geral de computa\xE7\xE3o e os gastos de carbono da comunidade."),Gn.forEach(o),Vr=p(e),V=s(e,"H2",{class:!0});var Cs=t(V);pe=s(Cs,"A",{id:!0,class:!0,href:!0});var Mn=t(pe);wo=s(Mn,"SPAN",{});var Rn=t(wo);h(Ye.$$.fragment,Rn),Rn.forEach(o),Mn.forEach(o),zi=p(Cs),zo=s(Cs,"SPAN",{});var Sn=t(zo);ji=d(Sn,"Transfer\xEAncia de Aprendizagem"),Sn.forEach(o),Cs.forEach(o),Ur=p(e),h(Qe.$$.fragment,e),Hr=p(e),We=s(e,"P",{});var pl=t(We);jo=s(pl,"EM",{});var Ln=t(jo);Ii=d(Ln,"Pr\xE9-treinamento"),Ln.forEach(o),Ni=d(pl," \xE9 o ato de treinar um modelo do zero: os pesos s\xE3o inicializados aleatoriamente e o treinamento come\xE7a sem nenhum conhecimento pr\xE9vio."),pl.forEach(o),Jr=p(e),U=s(e,"DIV",{class:!0});var Vs=t(U);Ze=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),yi=p(Vs),Ke=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),Vs.forEach(o),Fr=p(e),ka=s(e,"P",{});var Bn=t(ka);Oi=d(Bn,"Esse pr\xE9-treinamento geralmente \xE9 feito em grandes quantidades de dados. Portanto, requer um corpus de dados muito grande e o treinamento pode levar v\xE1rias semanas."),Bn.forEach(o),Xr=p(e),H=s(e,"P",{});var vr=t(H);Io=s(vr,"EM",{});var Dn=t(Io);Gi=d(Dn,"Ajuste fino"),Dn.forEach(o),Mi=d(vr,", por outro lado, \xE9 o treinamento feito "),No=s(vr,"STRONG",{});var Cn=t(No);Ri=d(Cn,"ap\xF3s"),Cn.forEach(o),Si=d(vr," um modelo ter sido pr\xE9-treinado. Para realizar o ajuste fino, primeiro voc\xEA adquire um modelo de linguagem pr\xE9-treinado e, em seguida, realiza treinamento adicional com um conjunto de dados espec\xEDfico para sua tarefa. Espere - por que n\xE3o simplesmente treinar diretamente para a tarefa final? Existem algumas raz\xF5es:"),vr.forEach(o),Yr=p(e),x=s(e,"UL",{});var Fa=t(x);yo=s(Fa,"LI",{});var Vn=t(yo);Li=d(Vn,"O modelo pr\xE9-treinado j\xE1 foi treinado em um conjunto de dados que possui algumas semelhan\xE7as com o conjunto de dados de ajuste fino. O processo de ajuste fino \xE9, portanto, capaz de aproveitar o conhecimento adquirido pelo modelo inicial durante o pr\xE9-treinamento (por exemplo, com problemas de NLP, o modelo pr\xE9-treinado ter\xE1 algum tipo de compreens\xE3o estat\xEDstica da linguagem que voc\xEA est\xE1 usando para sua tarefa)."),Vn.forEach(o),Bi=p(Fa),Oo=s(Fa,"LI",{});var Un=t(Oo);Di=d(Un,"Como o modelo pr\xE9-treinado j\xE1 foi treinado com muitos dados, o ajuste fino requer muito menos dados para obter resultados decentes."),Un.forEach(o),Ci=p(Fa),Go=s(Fa,"LI",{});var Hn=t(Go);Vi=d(Hn,"Pela mesma raz\xE3o, a quantidade de tempo e recursos necess\xE1rios para obter bons resultados s\xE3o muito menores."),Hn.forEach(o),Fa.forEach(o),Qr=p(e),ce=s(e,"P",{});var Us=t(ce);Ui=d(Us,"Por exemplo, pode-se alavancar um modelo pr\xE9-treinado treinado no idioma ingl\xEAs e depois ajust\xE1-lo em um corpus arXiv, resultando em um modelo baseado em ci\xEAncia/pesquisa. O ajuste fino exigir\xE1 apenas uma quantidade limitada de dados: o conhecimento que o modelo pr\xE9-treinado adquiriu \xE9 \u201Ctransferido\u201D, da\xED o termo "),Mo=s(Us,"EM",{});var Jn=t(Mo);Hi=d(Jn,"aprendizagem de transfer\xEAncia"),Jn.forEach(o),Ji=d(Us,"."),Us.forEach(o),Wr=p(e),J=s(e,"DIV",{class:!0});var Hs=t(J);ea=s(Hs,"IMG",{class:!0,src:!0,alt:!0}),Fi=p(Hs),aa=s(Hs,"IMG",{class:!0,src:!0,alt:!0}),Hs.forEach(o),Zr=p(e),Aa=s(e,"P",{});var Fn=t(Aa);Xi=d(Fn,"O ajuste fino de um modelo, portanto, tem menores custos de tempo, dados, financeiros e ambientais. Tamb\xE9m \xE9 mais r\xE1pido e f\xE1cil iterar em diferentes esquemas de ajuste fino, pois o treinamento \xE9 menos restritivo do que um pr\xE9-treinamento completo."),Fn.forEach(o),Kr=p(e),xa=s(e,"P",{});var Xn=t(xa);Yi=d(Xn,"Esse processo tamb\xE9m alcan\xE7ar\xE1 melhores resultados do que treinar do zero (a menos que voc\xEA tenha muitos dados), e \xE9 por isso que voc\xEA deve sempre tentar alavancar um modelo pr\xE9-treinado - um o mais pr\xF3ximo poss\xEDvel da tarefa que voc\xEA tem em m\xE3os - e  ent\xE3o fazer seu ajuste fino."),Xn.forEach(o),es=p(e),F=s(e,"H2",{class:!0});var Js=t(F);fe=s(Js,"A",{id:!0,class:!0,href:!0});var Yn=t(fe);Ro=s(Yn,"SPAN",{});var Qn=t(Ro);h(oa.$$.fragment,Qn),Qn.forEach(o),Yn.forEach(o),Qi=p(Js),So=s(Js,"SPAN",{});var Wn=t(So);Wi=d(Wn,"Arquitetura geral"),Wn.forEach(o),Js.forEach(o),as=p(e),wa=s(e,"P",{});var Zn=t(wa);Zi=d(Zn,"Nesta se\xE7\xE3o, veremos a arquitetura geral do modelo Transformer. N\xE3o se preocupe se voc\xEA n\xE3o entender alguns dos conceitos; h\xE1 se\xE7\xF5es detalhadas posteriormente cobrindo cada um dos componentes."),Zn.forEach(o),os=p(e),h(ra.$$.fragment,e),rs=p(e),X=s(e,"H2",{class:!0});var Fs=t(X);ve=s(Fs,"A",{id:!0,class:!0,href:!0});var Kn=t(ve);Lo=s(Kn,"SPAN",{});var em=t(Lo);h(sa.$$.fragment,em),em.forEach(o),Kn.forEach(o),Ki=p(Fs),Bo=s(Fs,"SPAN",{});var am=t(Bo);ed=d(am,"Introdu\xE7\xE3o"),am.forEach(o),Fs.forEach(o),ss=p(e),za=s(e,"P",{});var om=t(za);ad=d(om,"O modelo \xE9 principalmente composto por dois blocos:"),om.forEach(o),ts=p(e),he=s(e,"UL",{});var Xs=t(he);ja=s(Xs,"LI",{});var cl=t(ja);Do=s(cl,"STRONG",{});var rm=t(Do);od=d(rm,"Codificador (esquerda)"),rm.forEach(o),rd=d(cl,": O codificador recebe uma entrada e constr\xF3i uma representa\xE7\xE3o dela (seus recursos). Isso significa que o modelo \xE9 otimizado para adquirir entendimento da entrada."),cl.forEach(o),sd=p(Xs),Ia=s(Xs,"LI",{});var fl=t(Ia);Co=s(fl,"STRONG",{});var sm=t(Co);td=d(sm,"Decodificador (\xE0 direita)"),sm.forEach(o),id=d(fl,": O decodificador usa a representa\xE7\xE3o do codificador (recursos) junto com outras entradas para gerar uma sequ\xEAncia de destino. Isso significa que o modelo \xE9 otimizado para gerar sa\xEDdas."),fl.forEach(o),Xs.forEach(o),is=p(e),Y=s(e,"DIV",{class:!0});var Ys=t(Y);ta=s(Ys,"IMG",{class:!0,src:!0,alt:!0}),dd=p(Ys),ia=s(Ys,"IMG",{class:!0,src:!0,alt:!0}),Ys.forEach(o),ds=p(e),Na=s(e,"P",{});var tm=t(Na);ld=d(tm,"Cada uma dessas partes pode ser usada de forma independente, dependendo da tarefa:"),tm.forEach(o),ls=p(e),w=s(e,"UL",{});var Xa=t(w);ya=s(Xa,"LI",{});var vl=t(ya);Vo=s(vl,"STRONG",{});var im=t(Vo);nd=d(im,"Modelos somente de codificador"),im.forEach(o),md=d(vl,": bom para tarefas que exigem compreens\xE3o da entrada, como classifica\xE7\xE3o de senten\xE7a e reconhecimento de entidade nomeada."),vl.forEach(o),ud=p(Xa),Oa=s(Xa,"LI",{});var hl=t(Oa);Uo=s(hl,"STRONG",{});var dm=t(Uo);pd=d(dm,"Modelos somente decodificadores"),dm.forEach(o),cd=d(hl,": bom para tarefas generativas, como gera\xE7\xE3o de texto."),hl.forEach(o),fd=p(Xa),ge=s(Xa,"LI",{});var hr=t(ge);Ho=s(hr,"STRONG",{});var lm=t(Ho);vd=d(lm,"Modelos de codificador-decodificador"),lm.forEach(o),hd=d(hr," ou "),Jo=s(hr,"STRONG",{});var nm=t(Jo);gd=d(nm,"modelos de sequ\xEAncia a sequ\xEAncia"),nm.forEach(o),qd=d(hr,": bom para tarefas generativas que exigem uma entrada, como tradu\xE7\xE3o ou resumo. (corrigit sequence to sequence)"),hr.forEach(o),Xa.forEach(o),ns=p(e),Ga=s(e,"P",{});var mm=t(Ga);Ed=d(mm,"Vamos mergulhar nessas arquiteturas de forma independente em se\xE7\xF5es posteriores."),mm.forEach(o),ms=p(e),Q=s(e,"H2",{class:!0});var Qs=t(Q);qe=s(Qs,"A",{id:!0,class:!0,href:!0});var um=t(qe);Fo=s(um,"SPAN",{});var pm=t(Fo);h(da.$$.fragment,pm),pm.forEach(o),um.forEach(o),_d=p(Qs),Xo=s(Qs,"SPAN",{});var cm=t(Xo);bd=d(cm,"Camadas de Aten\xE7\xE3o"),cm.forEach(o),Qs.forEach(o),us=p(e),z=s(e,"P",{});var Ya=t(z);Td=d(Ya,"Uma caracter\xEDstica chave dos modelos Transformer \xE9 que eles s\xE3o constru\xEDdos com camadas especiais chamadas "),Yo=s(Ya,"EM",{});var fm=t(Yo);$d=d(fm,"camadas de aten\xE7\xE3o"),fm.forEach(o),Pd=d(Ya,". Na verdade, o t\xEDtulo do artigo que apresenta a arquitetura do Transformer era "),la=s(Ya,"A",{href:!0,rel:!0});var vm=t(la);kd=d(vm,"\u201CAten\xE7\xE3o \xE9 tudo que voc\xEA precisa\u201D"),vm.forEach(o),Ad=d(Ya,"! Exploraremos os detalhes das camadas de aten\xE7\xE3o posteriormente no curso; por enquanto, tudo o que voc\xEA precisa saber \xE9 que essa camada dir\xE1 ao modelo para prestar aten\xE7\xE3o espec\xEDfica a certas palavras na frase que voc\xEA passou (e mais ou menos ignorar as outras) ao lidar com a representa\xE7\xE3o de cada palavra."),Ya.forEach(o),ps=p(e),Ma=s(e,"P",{});var hm=t(Ma);xd=d(hm,"Para contextualizar, considere a tarefa de traduzir o texto do portugu\xEAs para o franc\xEAs. Dada a entrada \u201CVoc\xEA gosta deste curso\u201D, um modelo de tradu\xE7\xE3o precisar\xE1 atender tamb\xE9m \xE0 palavra adjacente \u201CVoc\xEA\u201D para obter a tradu\xE7\xE3o adequada para a palavra \u201Cgosta\u201D, pois em franc\xEAs o verbo \u201Cgostar\u201D \xE9 conjugado de forma diferente dependendo o sujeito. O resto da frase, no entanto, n\xE3o \xE9 \xFAtil para a tradu\xE7\xE3o dessa palavra. Na mesma linha, ao traduzir \u201Cdeste\u201D o modelo tamb\xE9m precisar\xE1 prestar aten\xE7\xE3o \xE0 palavra \u201Ccurso\u201D, pois \u201Cdeste\u201D traduz-se de forma diferente dependendo se o substantivo associado \xE9 masculino ou feminino. Novamente, as outras palavras na frase n\xE3o importar\xE3o para a tradu\xE7\xE3o de \u201Cdeste\u201D. Com frases mais complexas (e regras gramaticais mais complexas), o modelo precisaria prestar aten\xE7\xE3o especial \xE0s palavras que podem aparecer mais distantes na frase para traduzir adequadamente cada palavra."),hm.forEach(o),cs=p(e),Ra=s(e,"P",{});var gm=t(Ra);wd=d(gm,"O mesmo conceito se aplica a qualquer tarefa associada \xE0 linguagem natural: uma palavra por si s\xF3 tem um significado, mas esse significado \xE9 profundamente afetado pelo contexto, que pode ser qualquer outra palavra (ou palavras) antes ou depois da palavra que est\xE1 sendo estudada."),gm.forEach(o),fs=p(e),Sa=s(e,"P",{});var qm=t(Sa);zd=d(qm,"Agora que voc\xEA tem uma ideia do que s\xE3o as camadas de aten\xE7\xE3o, vamos dar uma olhada mais de perto na arquitetura do Transformer."),qm.forEach(o),vs=p(e),W=s(e,"H2",{class:!0});var Ws=t(W);Ee=s(Ws,"A",{id:!0,class:!0,href:!0});var Em=t(Ee);Qo=s(Em,"SPAN",{});var _m=t(Qo);h(na.$$.fragment,_m),_m.forEach(o),Em.forEach(o),jd=p(Ws),Wo=s(Ws,"SPAN",{});var bm=t(Wo);Id=d(bm,"A arquitetura original"),bm.forEach(o),Ws.forEach(o),hs=p(e),La=s(e,"P",{});var Tm=t(La);Nd=d(Tm,"A arquitetura Transformer foi originalmente projetada para tradu\xE7\xE3o. Durante o treinamento, o codificador recebe entradas (frases) em um determinado idioma, enquanto o decodificador recebe as mesmas frases no idioma de destino desejado. No codificador, as camadas de aten\xE7\xE3o podem usar todas as palavras em uma frase (j\xE1 que, como acabamos de ver, a tradu\xE7\xE3o de uma determinada palavra pode ser dependente do que est\xE1 depois e antes dela na frase). O decodificador, no entanto, funciona sequencialmente e s\xF3 pode prestar aten\xE7\xE3o nas palavras da frase que ele j\xE1 traduziu (portanto, apenas as palavras anteriores \xE0 palavra que est\xE1 sendo gerada no momento). Por exemplo, quando previmos as tr\xEAs primeiras palavras do alvo traduzido, as entregamos ao decodificador que ent\xE3o usa todas as entradas do codificador para tentar prever a quarta palavra."),Tm.forEach(o),gs=p(e),Ba=s(e,"P",{});var $m=t(Ba);yd=d($m,"Para acelerar as coisas durante o treinamento (quando o modelo tem acesso \xE0s frases alvo), o decodificador \xE9 alimentado com todo o alvo, mas n\xE3o \xE9 permitido usar palavras futuras (se teve acesso \xE0 palavra na posi\xE7\xE3o 2 ao tentar prever a palavra na posi\xE7\xE3o 2, o problema n\xE3o seria muito dif\xEDcil!). Por exemplo, ao tentar prever a quarta palavra, a camada de aten\xE7\xE3o s\xF3 ter\xE1 acesso \xE0s palavras nas posi\xE7\xF5es 1 a 3."),$m.forEach(o),qs=p(e),Da=s(e,"P",{});var Pm=t(Da);Od=d(Pm,"A arquitetura original do Transformer ficou assim, com o codificador \xE0 esquerda e o decodificador \xE0 direita:"),Pm.forEach(o),Es=p(e),Z=s(e,"DIV",{class:!0});var Zs=t(Z);ma=s(Zs,"IMG",{class:!0,src:!0,alt:!0}),Gd=p(Zs),ua=s(Zs,"IMG",{class:!0,src:!0,alt:!0}),Zs.forEach(o),_s=p(e),Ca=s(e,"P",{});var km=t(Ca);Md=d(km,"Observe que a primeira camada de aten\xE7\xE3o em um bloco decodificador presta aten\xE7\xE3o a todas as entradas (passadas) do decodificador, mas a segunda camada de aten\xE7\xE3o usa a sa\xEDda do codificador. Ele pode, assim, acessar toda a frase de entrada para melhor prever a palavra atual. Isso \xE9 muito \xFAtil, pois diferentes idiomas podem ter regras gramaticais que colocam as palavras em ordens diferentes, ou algum contexto fornecido posteriormente na frase pode ser \xFAtil para determinar a melhor tradu\xE7\xE3o de uma determinada palavra."),km.forEach(o),bs=p(e),_e=s(e,"P",{});var Ks=t(_e);Rd=d(Ks,"A "),Zo=s(Ks,"EM",{});var Am=t(Zo);Sd=d(Am,"m\xE1scara de aten\xE7\xE3o"),Am.forEach(o),Ld=d(Ks," tamb\xE9m pode ser usada no codificador/decodificador para evitar que o modelo preste aten\xE7\xE3o a algumas palavras especiais - por exemplo, a palavra de preenchimento especial usada para fazer com que todas as entradas tenham o mesmo comprimento ao agrupar frases."),Ks.forEach(o),Ts=p(e),K=s(e,"H2",{class:!0});var et=t(K);be=s(et,"A",{id:!0,class:!0,href:!0});var xm=t(be);Ko=s(xm,"SPAN",{});var wm=t(Ko);h(pa.$$.fragment,wm),wm.forEach(o),xm.forEach(o),Bd=p(et),er=s(et,"SPAN",{});var zm=t(er);Dd=d(zm,"Arquiteturas vs. checkpoints"),zm.forEach(o),et.forEach(o),$s=p(e),b=s(e,"P",{});var Te=t(b);Cd=d(Te,"\xC0 medida que nos aprofundarmos nos modelos do Transformer neste curso, voc\xEA ver\xE1 men\xE7\xF5es a "),ar=s(Te,"EM",{});var jm=t(ar);Vd=d(jm,"arquiteturas"),jm.forEach(o),Ud=d(Te," e "),or=s(Te,"EM",{});var Im=t(or);Hd=d(Im,"checkpoints"),Im.forEach(o),Jd=d(Te,", bem como "),rr=s(Te,"EM",{});var Nm=t(rr);Fd=d(Nm,"modelos"),Nm.forEach(o),Xd=d(Te,". Todos esses termos t\xEAm significados ligeiramente diferentes:"),Te.forEach(o),Ps=p(e),j=s(e,"UL",{});var Qa=t(j);Va=s(Qa,"LI",{});var gl=t(Va);sr=s(gl,"STRONG",{});var ym=t(sr);Yd=d(ym,"Arquitetura"),ym.forEach(o),Qd=d(gl,": Este \xE9 o esqueleto do modelo \u2014 a defini\xE7\xE3o de cada camada e cada opera\xE7\xE3o que acontece dentro do modelo."),gl.forEach(o),Wd=p(Qa),Ua=s(Qa,"LI",{});var ql=t(Ua);tr=s(ql,"STRONG",{});var Om=t(tr);Zd=d(Om,"Checkpoints"),Om.forEach(o),Kd=d(ql,": Esses s\xE3o os pesos que ser\xE3o carregados em uma determinada arquitetura."),ql.forEach(o),el=p(Qa),I=s(Qa,"LI",{});var va=t(I);ir=s(va,"STRONG",{});var Gm=t(ir);al=d(Gm,"Modelos"),Gm.forEach(o),ol=d(va,": Este \xE9 um termo abrangente que n\xE3o \xE9 t\xE3o preciso quanto \u201Carquitetura\u201D ou \u201Ccheckpoint\u201D: pode significar ambos. Este curso especificar\xE1 "),dr=s(va,"EM",{});var Mm=t(dr);rl=d(Mm,"arquitetura"),Mm.forEach(o),sl=d(va," ou "),lr=s(va,"EM",{});var Rm=t(lr);tl=d(Rm,"checkpoint"),Rm.forEach(o),il=d(va," quando for necess\xE1rio reduzir a ambiguidade."),va.forEach(o),Qa.forEach(o),ks=p(e),N=s(e,"P",{});var Wa=t(N);dl=d(Wa,"Por exemplo, BERT \xE9 uma arquitetura enquanto "),nr=s(Wa,"CODE",{});var Sm=t(nr);ll=d(Sm,"bert-base-cased"),Sm.forEach(o),nl=d(Wa,", um conjunto de pesos treinados pela equipe do Google para a primeira vers\xE3o do BERT, \xE9 um checkpoint. No entanto, pode-se dizer \u201Co modelo BERT\u201D e \u201Co modelo "),mr=s(Wa,"CODE",{});var Lm=t(mr);ml=d(Lm,"bert-base-cased"),Lm.forEach(o),ul=d(Wa,"\u201C."),Wa.forEach(o),this.h()},h(){m(O,"name","hf:doc:metadata"),m(O,"content",JSON.stringify(Fm)),m(ee,"id","como-os-transformers-trabalham"),m(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ee,"href","#como-os-transformers-trabalham"),m(G,"class","relative group"),m(ae,"id","um-pouco-da-histria-dos-transformers"),m(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ae,"href","#um-pouco-da-histria-dos-transformers"),m(M,"class","relative group"),m(ke,"class","block dark:hidden"),c(ke.src,bl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||m(ke,"src",bl),m(ke,"alt","A brief chronology of Transformers models."),m(Ae,"class","hidden dark:block"),c(Ae.src,Tl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||m(Ae,"src",Tl),m(Ae,"alt","A brief chronology of Transformers models."),m(R,"class","flex justify-center"),m(xe,"href","https://arxiv.org/abs/1706.03762"),m(xe,"rel","nofollow"),m(we,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),m(we,"rel","nofollow"),m(ze,"href","https://arxiv.org/abs/1810.04805"),m(ze,"rel","nofollow"),m(je,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),m(je,"rel","nofollow"),m(Ie,"href","https://arxiv.org/abs/1910.01108"),m(Ie,"rel","nofollow"),m(Ne,"href","https://arxiv.org/abs/1910.13461"),m(Ne,"rel","nofollow"),m(ye,"href","https://arxiv.org/abs/1910.10683"),m(ye,"rel","nofollow"),m(Oe,"href","https://arxiv.org/abs/2005.14165"),m(Oe,"rel","nofollow"),m(de,"id","transformers-so-modelos-de-linguagem"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#transformers-so-modelos-de-linguagem"),m(S,"class","relative group"),m(Le,"class","block dark:hidden"),c(Le.src,$l="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||m(Le,"src",$l),m(Le,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),m(Be,"class","hidden dark:block"),c(Be.src,Pl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||m(Be,"src",Pl),m(Be,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),m(L,"class","flex justify-center"),m(De,"class","block dark:hidden"),c(De.src,kl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||m(De,"src",kl),m(De,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),m(Ce,"class","hidden dark:block"),c(Ce.src,Al="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||m(Ce,"src",Al),m(Ce,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),m(B,"class","flex justify-center"),m(ue,"id","transformers-so-modelos-grandes"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#transformers-so-modelos-grandes"),m(D,"class","relative group"),c(He.src,xl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||m(He,"src",xl),m(He,"alt","Number of parameters of recent Transformers models"),m(He,"width","90%"),m(Ue,"class","flex justify-center"),m(Je,"class","block dark:hidden"),c(Je.src,wl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||m(Je,"src",wl),m(Je,"alt","The carbon footprint of a large language model."),m(Fe,"class","hidden dark:block"),c(Fe.src,zl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||m(Fe,"src",zl),m(Fe,"alt","The carbon footprint of a large language model."),m(C,"class","flex justify-center"),m(pe,"id","transferncia-de-aprendizagem"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#transferncia-de-aprendizagem"),m(V,"class","relative group"),m(Ze,"class","block dark:hidden"),c(Ze.src,jl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||m(Ze,"src",jl),m(Ze,"alt","The pretraining of a language model is costly in both time and money."),m(Ke,"class","hidden dark:block"),c(Ke.src,Il="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||m(Ke,"src",Il),m(Ke,"alt","The pretraining of a language model is costly in both time and money."),m(U,"class","flex justify-center"),m(ea,"class","block dark:hidden"),c(ea.src,Nl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||m(ea,"src",Nl),m(ea,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),m(aa,"class","hidden dark:block"),c(aa.src,yl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||m(aa,"src",yl),m(aa,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),m(J,"class","flex justify-center"),m(fe,"id","arquitetura-geral"),m(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fe,"href","#arquitetura-geral"),m(F,"class","relative group"),m(ve,"id","introduo"),m(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ve,"href","#introduo"),m(X,"class","relative group"),m(ta,"class","block dark:hidden"),c(ta.src,Ol="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||m(ta,"src",Ol),m(ta,"alt","Architecture of a Transformers models"),m(ia,"class","hidden dark:block"),c(ia.src,Gl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||m(ia,"src",Gl),m(ia,"alt","Architecture of a Transformers models"),m(Y,"class","flex justify-center"),m(qe,"id","camadas-de-ateno"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#camadas-de-ateno"),m(Q,"class","relative group"),m(la,"href","https://arxiv.org/abs/1706.03762"),m(la,"rel","nofollow"),m(Ee,"id","a-arquitetura-original"),m(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ee,"href","#a-arquitetura-original"),m(W,"class","relative group"),m(ma,"class","block dark:hidden"),c(ma.src,Ml="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||m(ma,"src",Ml),m(ma,"alt","Architecture of a Transformers models"),m(ua,"class","hidden dark:block"),c(ua.src,Rl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||m(ua,"src",Rl),m(ua,"alt","Architecture of a Transformers models"),m(Z,"class","flex justify-center"),m(be,"id","arquiteturas-vs-checkpoints"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#arquiteturas-vs-checkpoints"),m(K,"class","relative group")},m(e,l){a(document.head,O),n(e,gr,l),n(e,G,l),a(G,ee),a(ee,Za),g($e,Za,null),a(G,at),a(G,Ka),a(Ka,ot),n(e,qr,l),n(e,ha,l),a(ha,rt),n(e,Er,l),n(e,M,l),a(M,ae),a(ae,eo),g(Pe,eo,null),a(M,st),a(M,ao),a(ao,tt),n(e,_r,l),n(e,ga,l),a(ga,it),n(e,br,l),n(e,R,l),a(R,ke),a(R,dt),a(R,Ae),n(e,Tr,l),n(e,oe,l),a(oe,lt),a(oe,xe),a(xe,nt),a(oe,mt),n(e,$r,l),n(e,f,l),a(f,oo),a(oo,re),a(re,ro),a(ro,ut),a(re,pt),a(re,we),a(we,ct),a(re,ft),a(f,vt),a(f,so),a(so,se),a(se,to),a(to,ht),a(se,gt),a(se,ze),a(ze,qt),a(se,Et),a(f,_t),a(f,io),a(io,te),a(te,lo),a(lo,bt),a(te,Tt),a(te,je),a(je,$t),a(te,Pt),a(f,kt),a(f,no),a(no,ie),a(ie,mo),a(mo,At),a(ie,xt),a(ie,Ie),a(Ie,wt),a(ie,zt),a(f,jt),a(f,uo),a(uo,$),a($,po),a(po,It),a($,Nt),a($,Ne),a(Ne,yt),a($,Ot),a($,ye),a(ye,Gt),a($,Mt),a(f,Rt),a(f,co),a(co,P),a(P,fo),a(fo,St),a(P,Lt),a(P,Oe),a(Oe,Bt),a(P,Dt),a(P,vo),a(vo,Ct),a(P,Vt),n(e,Pr,l),n(e,qa,l),a(qa,Ut),n(e,kr,l),n(e,k,l),a(k,Ge),a(Ge,Ht),a(Ge,ho),a(ho,Jt),a(Ge,Ft),a(k,Xt),a(k,Me),a(Me,Yt),a(Me,go),a(go,Qt),a(Me,Wt),a(k,Zt),a(k,Re),a(Re,Kt),a(Re,qo),a(qo,ei),a(Re,ai),n(e,Ar,l),n(e,Ea,l),a(Ea,oi),n(e,xr,l),n(e,S,l),a(S,de),a(de,Eo),g(Se,Eo,null),a(S,ri),a(S,_o),a(_o,si),n(e,wr,l),n(e,le,l),a(le,ti),a(le,bo),a(bo,ii),a(le,di),n(e,zr,l),n(e,ne,l),a(ne,li),a(ne,To),a(To,ni),a(ne,mi),n(e,jr,l),n(e,A,l),a(A,ui),a(A,$o),a($o,pi),a(A,ci),a(A,Po),a(Po,fi),a(A,vi),n(e,Ir,l),n(e,L,l),a(L,Le),a(L,hi),a(L,Be),n(e,Nr,l),n(e,me,l),a(me,gi),a(me,ko),a(ko,qi),a(me,Ei),n(e,yr,l),n(e,B,l),a(B,De),a(B,_i),a(B,Ce),n(e,Or,l),n(e,D,l),a(D,ue),a(ue,Ao),g(Ve,Ao,null),a(D,bi),a(D,xo),a(xo,Ti),n(e,Gr,l),n(e,_a,l),a(_a,$i),n(e,Mr,l),n(e,Ue,l),a(Ue,He),n(e,Rr,l),n(e,ba,l),a(ba,Pi),n(e,Sr,l),n(e,C,l),a(C,Je),a(C,ki),a(C,Fe),n(e,Lr,l),g(Xe,e,l),n(e,Br,l),n(e,Ta,l),a(Ta,Ai),n(e,Dr,l),n(e,$a,l),a($a,xi),n(e,Cr,l),n(e,Pa,l),a(Pa,wi),n(e,Vr,l),n(e,V,l),a(V,pe),a(pe,wo),g(Ye,wo,null),a(V,zi),a(V,zo),a(zo,ji),n(e,Ur,l),g(Qe,e,l),n(e,Hr,l),n(e,We,l),a(We,jo),a(jo,Ii),a(We,Ni),n(e,Jr,l),n(e,U,l),a(U,Ze),a(U,yi),a(U,Ke),n(e,Fr,l),n(e,ka,l),a(ka,Oi),n(e,Xr,l),n(e,H,l),a(H,Io),a(Io,Gi),a(H,Mi),a(H,No),a(No,Ri),a(H,Si),n(e,Yr,l),n(e,x,l),a(x,yo),a(yo,Li),a(x,Bi),a(x,Oo),a(Oo,Di),a(x,Ci),a(x,Go),a(Go,Vi),n(e,Qr,l),n(e,ce,l),a(ce,Ui),a(ce,Mo),a(Mo,Hi),a(ce,Ji),n(e,Wr,l),n(e,J,l),a(J,ea),a(J,Fi),a(J,aa),n(e,Zr,l),n(e,Aa,l),a(Aa,Xi),n(e,Kr,l),n(e,xa,l),a(xa,Yi),n(e,es,l),n(e,F,l),a(F,fe),a(fe,Ro),g(oa,Ro,null),a(F,Qi),a(F,So),a(So,Wi),n(e,as,l),n(e,wa,l),a(wa,Zi),n(e,os,l),g(ra,e,l),n(e,rs,l),n(e,X,l),a(X,ve),a(ve,Lo),g(sa,Lo,null),a(X,Ki),a(X,Bo),a(Bo,ed),n(e,ss,l),n(e,za,l),a(za,ad),n(e,ts,l),n(e,he,l),a(he,ja),a(ja,Do),a(Do,od),a(ja,rd),a(he,sd),a(he,Ia),a(Ia,Co),a(Co,td),a(Ia,id),n(e,is,l),n(e,Y,l),a(Y,ta),a(Y,dd),a(Y,ia),n(e,ds,l),n(e,Na,l),a(Na,ld),n(e,ls,l),n(e,w,l),a(w,ya),a(ya,Vo),a(Vo,nd),a(ya,md),a(w,ud),a(w,Oa),a(Oa,Uo),a(Uo,pd),a(Oa,cd),a(w,fd),a(w,ge),a(ge,Ho),a(Ho,vd),a(ge,hd),a(ge,Jo),a(Jo,gd),a(ge,qd),n(e,ns,l),n(e,Ga,l),a(Ga,Ed),n(e,ms,l),n(e,Q,l),a(Q,qe),a(qe,Fo),g(da,Fo,null),a(Q,_d),a(Q,Xo),a(Xo,bd),n(e,us,l),n(e,z,l),a(z,Td),a(z,Yo),a(Yo,$d),a(z,Pd),a(z,la),a(la,kd),a(z,Ad),n(e,ps,l),n(e,Ma,l),a(Ma,xd),n(e,cs,l),n(e,Ra,l),a(Ra,wd),n(e,fs,l),n(e,Sa,l),a(Sa,zd),n(e,vs,l),n(e,W,l),a(W,Ee),a(Ee,Qo),g(na,Qo,null),a(W,jd),a(W,Wo),a(Wo,Id),n(e,hs,l),n(e,La,l),a(La,Nd),n(e,gs,l),n(e,Ba,l),a(Ba,yd),n(e,qs,l),n(e,Da,l),a(Da,Od),n(e,Es,l),n(e,Z,l),a(Z,ma),a(Z,Gd),a(Z,ua),n(e,_s,l),n(e,Ca,l),a(Ca,Md),n(e,bs,l),n(e,_e,l),a(_e,Rd),a(_e,Zo),a(Zo,Sd),a(_e,Ld),n(e,Ts,l),n(e,K,l),a(K,be),a(be,Ko),g(pa,Ko,null),a(K,Bd),a(K,er),a(er,Dd),n(e,$s,l),n(e,b,l),a(b,Cd),a(b,ar),a(ar,Vd),a(b,Ud),a(b,or),a(or,Hd),a(b,Jd),a(b,rr),a(rr,Fd),a(b,Xd),n(e,Ps,l),n(e,j,l),a(j,Va),a(Va,sr),a(sr,Yd),a(Va,Qd),a(j,Wd),a(j,Ua),a(Ua,tr),a(tr,Zd),a(Ua,Kd),a(j,el),a(j,I),a(I,ir),a(ir,al),a(I,ol),a(I,dr),a(dr,rl),a(I,sl),a(I,lr),a(lr,tl),a(I,il),n(e,ks,l),n(e,N,l),a(N,dl),a(N,nr),a(nr,ll),a(N,nl),a(N,mr),a(mr,ml),a(N,ul),As=!0},p:Um,i(e){As||(q($e.$$.fragment,e),q(Pe.$$.fragment,e),q(Se.$$.fragment,e),q(Ve.$$.fragment,e),q(Xe.$$.fragment,e),q(Ye.$$.fragment,e),q(Qe.$$.fragment,e),q(oa.$$.fragment,e),q(ra.$$.fragment,e),q(sa.$$.fragment,e),q(da.$$.fragment,e),q(na.$$.fragment,e),q(pa.$$.fragment,e),As=!0)},o(e){E($e.$$.fragment,e),E(Pe.$$.fragment,e),E(Se.$$.fragment,e),E(Ve.$$.fragment,e),E(Xe.$$.fragment,e),E(Ye.$$.fragment,e),E(Qe.$$.fragment,e),E(oa.$$.fragment,e),E(ra.$$.fragment,e),E(sa.$$.fragment,e),E(da.$$.fragment,e),E(na.$$.fragment,e),E(pa.$$.fragment,e),As=!1},d(e){o(O),e&&o(gr),e&&o(G),_($e),e&&o(qr),e&&o(ha),e&&o(Er),e&&o(M),_(Pe),e&&o(_r),e&&o(ga),e&&o(br),e&&o(R),e&&o(Tr),e&&o(oe),e&&o($r),e&&o(f),e&&o(Pr),e&&o(qa),e&&o(kr),e&&o(k),e&&o(Ar),e&&o(Ea),e&&o(xr),e&&o(S),_(Se),e&&o(wr),e&&o(le),e&&o(zr),e&&o(ne),e&&o(jr),e&&o(A),e&&o(Ir),e&&o(L),e&&o(Nr),e&&o(me),e&&o(yr),e&&o(B),e&&o(Or),e&&o(D),_(Ve),e&&o(Gr),e&&o(_a),e&&o(Mr),e&&o(Ue),e&&o(Rr),e&&o(ba),e&&o(Sr),e&&o(C),e&&o(Lr),_(Xe,e),e&&o(Br),e&&o(Ta),e&&o(Dr),e&&o($a),e&&o(Cr),e&&o(Pa),e&&o(Vr),e&&o(V),_(Ye),e&&o(Ur),_(Qe,e),e&&o(Hr),e&&o(We),e&&o(Jr),e&&o(U),e&&o(Fr),e&&o(ka),e&&o(Xr),e&&o(H),e&&o(Yr),e&&o(x),e&&o(Qr),e&&o(ce),e&&o(Wr),e&&o(J),e&&o(Zr),e&&o(Aa),e&&o(Kr),e&&o(xa),e&&o(es),e&&o(F),_(oa),e&&o(as),e&&o(wa),e&&o(os),_(ra,e),e&&o(rs),e&&o(X),_(sa),e&&o(ss),e&&o(za),e&&o(ts),e&&o(he),e&&o(is),e&&o(Y),e&&o(ds),e&&o(Na),e&&o(ls),e&&o(w),e&&o(ns),e&&o(Ga),e&&o(ms),e&&o(Q),_(da),e&&o(us),e&&o(z),e&&o(ps),e&&o(Ma),e&&o(cs),e&&o(Ra),e&&o(fs),e&&o(Sa),e&&o(vs),e&&o(W),_(na),e&&o(hs),e&&o(La),e&&o(gs),e&&o(Ba),e&&o(qs),e&&o(Da),e&&o(Es),e&&o(Z),e&&o(_s),e&&o(Ca),e&&o(bs),e&&o(_e),e&&o(Ts),e&&o(K),_(pa),e&&o($s),e&&o(b),e&&o(Ps),e&&o(j),e&&o(ks),e&&o(N)}}}const Fm={local:"como-os-transformers-trabalham",sections:[{local:"um-pouco-da-histria-dos-transformers",title:"Um pouco da hist\xF3ria dos Transformers"},{local:"transformers-so-modelos-de-linguagem",title:"Transformers s\xE3o modelos de linguagem"},{local:"transformers-so-modelos-grandes",title:"Transformers s\xE3o modelos grandes"},{local:"transferncia-de-aprendizagem",title:"Transfer\xEAncia de Aprendizagem"},{local:"arquitetura-geral",title:"Arquitetura geral"},{local:"introduo",title:"Introdu\xE7\xE3o"},{local:"camadas-de-ateno",title:"Camadas de Aten\xE7\xE3o"},{local:"a-arquitetura-original",title:"A arquitetura original"},{local:"arquiteturas-vs-checkpoints",title:"Arquiteturas vs. checkpoints"}],title:"Como os Transformers trabalham?"};function Xm(_l){return Hm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zm extends Bm{constructor(O){super();Dm(this,O,Xm,Jm,Cm,{})}}export{Zm as default,Fm as metadata};
