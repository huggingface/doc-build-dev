import{S as Ae,i as ye,s as Me,e as s,k as u,w as Pe,t as d,M as Te,c as t,d as a,m as p,a as r,x as $e,h as i,b as l,G as o,g as m,y as Le,L as Oe,q as Ie,o as Se,B as ke,v as Be}from"../../chunks/vendor-hf-doc-builder.js";import{I as Re}from"../../chunks/IconCopyLink-hf-doc-builder.js";function je(de){let f,B,h,v,T,b,D,$,F,R,_,K,L,Q,V,j,q,W,g,X,Y,N,M,Z,C,P,ee,U,c,O,w,ae,oe,I,x,se,te,S,A,re,le,k,y,ne,G;return b=new Re({}),{c(){f=s("meta"),B=u(),h=s("h1"),v=s("a"),T=s("span"),Pe(b.$$.fragment),D=u(),$=s("span"),F=d("Modelos sequ\xEAncia a sequ\xEAncia"),R=u(),_=s("p"),K=d("Modelos encoder-decoder (tamb\xE9m chamados de modelos "),L=s("em"),Q=d("sequence-to-sequence"),V=d(") usam ambas as partes da arquitetura Transformer. Em cada est\xE1gio, as camadas de aten\xE7\xE3o do codificador podem acessar todas as palavras da frase inicial, enquanto as camadas de aten\xE7\xE3o do decodificador podem acessar apenas as palavras posicionadas antes de uma determinada palavra na entrada."),j=u(),q=s("p"),W=d("O pr\xE9-treinamento desses modelos pode ser feito usando os objetivos dos modelos de codificador ou decodificador, mas geralmente envolve algo um pouco mais complexo. Por exemplo,\xA0"),g=s("a"),X=d("T5"),Y=d("\xA0\xE9 pr\xE9-treinado substituindo trechos aleat\xF3rios de texto (que podem conter v\xE1rias palavras) por uma \xFAnica palavra especial de m\xE1scara, e o objetivo \xE9 prever o texto que esta palavra de m\xE1scara substitui."),N=u(),M=s("p"),Z=d("Os modelos de sequ\xEAncia a sequ\xEAncia s\xE3o mais adequados para tarefas que envolvem a gera\xE7\xE3o de novas frases dependendo de uma determinada entrada, como resumo, tradu\xE7\xE3o ou resposta a perguntas generativas."),C=u(),P=s("p"),ee=d("Os representantes desta fam\xEDlia de modelos incluem:"),U=u(),c=s("ul"),O=s("li"),w=s("a"),ae=d("BART"),oe=u(),I=s("li"),x=s("a"),se=d("mBART"),te=u(),S=s("li"),A=s("a"),re=d("Marian"),le=u(),k=s("li"),y=s("a"),ne=d("T5"),this.h()},l(e){const n=Te('[data-svelte="svelte-1phssyn"]',document.head);f=t(n,"META",{name:!0,content:!0}),n.forEach(a),B=p(e),h=t(e,"H1",{class:!0});var H=r(h);v=t(H,"A",{id:!0,class:!0,href:!0});var ie=r(v);T=t(ie,"SPAN",{});var ce=r(T);$e(b.$$.fragment,ce),ce.forEach(a),ie.forEach(a),D=p(H),$=t(H,"SPAN",{});var me=r($);F=i(me,"Modelos sequ\xEAncia a sequ\xEAncia"),me.forEach(a),H.forEach(a),R=p(e),_=t(e,"P",{});var J=r(_);K=i(J,"Modelos encoder-decoder (tamb\xE9m chamados de modelos "),L=t(J,"EM",{});var ue=r(L);Q=i(ue,"sequence-to-sequence"),ue.forEach(a),V=i(J,") usam ambas as partes da arquitetura Transformer. Em cada est\xE1gio, as camadas de aten\xE7\xE3o do codificador podem acessar todas as palavras da frase inicial, enquanto as camadas de aten\xE7\xE3o do decodificador podem acessar apenas as palavras posicionadas antes de uma determinada palavra na entrada."),J.forEach(a),j=p(e),q=t(e,"P",{});var z=r(q);W=i(z,"O pr\xE9-treinamento desses modelos pode ser feito usando os objetivos dos modelos de codificador ou decodificador, mas geralmente envolve algo um pouco mais complexo. Por exemplo,\xA0"),g=t(z,"A",{href:!0,rel:!0});var pe=r(g);X=i(pe,"T5"),pe.forEach(a),Y=i(z,"\xA0\xE9 pr\xE9-treinado substituindo trechos aleat\xF3rios de texto (que podem conter v\xE1rias palavras) por uma \xFAnica palavra especial de m\xE1scara, e o objetivo \xE9 prever o texto que esta palavra de m\xE1scara substitui."),z.forEach(a),N=p(e),M=t(e,"P",{});var fe=r(M);Z=i(fe,"Os modelos de sequ\xEAncia a sequ\xEAncia s\xE3o mais adequados para tarefas que envolvem a gera\xE7\xE3o de novas frases dependendo de uma determinada entrada, como resumo, tradu\xE7\xE3o ou resposta a perguntas generativas."),fe.forEach(a),C=p(e),P=t(e,"P",{});var he=r(P);ee=i(he,"Os representantes desta fam\xEDlia de modelos incluem:"),he.forEach(a),U=p(e),c=t(e,"UL",{});var E=r(c);O=t(E,"LI",{});var ve=r(O);w=t(ve,"A",{href:!0,rel:!0});var _e=r(w);ae=i(_e,"BART"),_e.forEach(a),ve.forEach(a),oe=p(E),I=t(E,"LI",{});var qe=r(I);x=t(qe,"A",{href:!0,rel:!0});var Ee=r(x);se=i(Ee,"mBART"),Ee.forEach(a),qe.forEach(a),te=p(E),S=t(E,"LI",{});var be=r(S);A=t(be,"A",{href:!0,rel:!0});var ge=r(A);re=i(ge,"Marian"),ge.forEach(a),be.forEach(a),le=p(E),k=t(E,"LI",{});var we=r(k);y=t(we,"A",{href:!0,rel:!0});var xe=r(y);ne=i(xe,"T5"),xe.forEach(a),we.forEach(a),E.forEach(a),this.h()},h(){l(f,"name","hf:doc:metadata"),l(f,"content",JSON.stringify(Ne)),l(v,"id","modelos-sequncia-a-sequncia"),l(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(v,"href","#modelos-sequncia-a-sequncia"),l(h,"class","relative group"),l(g,"href","https://huggingface.co/t5-base"),l(g,"rel","nofollow"),l(w,"href","https://huggingface.co/transformers/model_doc/bart.html"),l(w,"rel","nofollow"),l(x,"href","https://huggingface.co/transformers/model_doc/mbart.html"),l(x,"rel","nofollow"),l(A,"href","https://huggingface.co/transformers/model_doc/marian.html"),l(A,"rel","nofollow"),l(y,"href","https://huggingface.co/transformers/model_doc/t5.html"),l(y,"rel","nofollow")},m(e,n){o(document.head,f),m(e,B,n),m(e,h,n),o(h,v),o(v,T),Le(b,T,null),o(h,D),o(h,$),o($,F),m(e,R,n),m(e,_,n),o(_,K),o(_,L),o(L,Q),o(_,V),m(e,j,n),m(e,q,n),o(q,W),o(q,g),o(g,X),o(q,Y),m(e,N,n),m(e,M,n),o(M,Z),m(e,C,n),m(e,P,n),o(P,ee),m(e,U,n),m(e,c,n),o(c,O),o(O,w),o(w,ae),o(c,oe),o(c,I),o(I,x),o(x,se),o(c,te),o(c,S),o(S,A),o(A,re),o(c,le),o(c,k),o(k,y),o(y,ne),G=!0},p:Oe,i(e){G||(Ie(b.$$.fragment,e),G=!0)},o(e){Se(b.$$.fragment,e),G=!1},d(e){a(f),e&&a(B),e&&a(h),ke(b),e&&a(R),e&&a(_),e&&a(j),e&&a(q),e&&a(N),e&&a(M),e&&a(C),e&&a(P),e&&a(U),e&&a(c)}}}const Ne={local:"modelos-sequncia-a-sequncia",title:"Modelos sequ\xEAncia a sequ\xEAncia"};function Ce(de){return Be(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class He extends Ae{constructor(f){super();ye(this,f,Ce,je,Me,{})}}export{He as default,Ne as metadata};
