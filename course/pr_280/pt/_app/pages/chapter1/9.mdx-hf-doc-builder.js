import{S as Je,i as Ye,s as Fe,e as o,k as l,w as Ke,t as d,M as We,c as r,d as t,m as i,a as s,x as Ze,h as n,b as w,G as e,g as b,y as et,L as tt,q as at,o as ot,B as rt,v as st}from"../../chunks/vendor-hf-doc-builder.js";import{I as dt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function nt(we){let m,V,f,E,x,A,ee,$,te,X,_,ae,C,oe,re,J,D,se,Y,R,g,u,S,de,ne,G,le,ie,H,ce,me,p,h,M,fe,ue,N,pe,he,k,ve,Te,v,z,Ee,_e,I,Re,be,O,Ae,De,T,U,Be,Pe,j,ye,Le,Q,qe,F;return A=new dt({}),{c(){m=o("meta"),V=l(),f=o("h1"),E=o("a"),x=o("span"),Ke(A.$$.fragment),ee=l(),$=o("span"),te=d("Resumo"),X=l(),_=o("p"),ae=d("Nesse cap\xEDtulo, voc\xEA viu como abordar diferentes tarefas de NLP usando a fun\xE7\xE3o de alto n\xEDvel "),C=o("code"),oe=d("pipeline()"),re=d(" da biblioteca \u{1F917} Transformers. Voc\xEA tamb\xE9m viu como pesquisar e usar modelos no Hub, bem como usar a API de infer\xEAncia para testar os modelos diretamente em seu navegador."),J=l(),D=o("p"),se=d("Discutimos como os modelos Transformers funcionam em alto n\xEDvel e falamos sobre a import\xE2ncia do aprendizado de transfer\xEAncia (transfer learning) e do ajuste fino. Um aspecto chave \xE9 que voc\xEA pode usar a arquitetura completa ou apenas o codificador ou decodificador, dependendo do tipo de tarefa que voc\xEA pretende resolver. A tabela a seguir resume isso:"),Y=l(),R=o("table"),g=o("thead"),u=o("tr"),S=o("th"),de=d("Modelo"),ne=l(),G=o("th"),le=d("Exemplos"),ie=l(),H=o("th"),ce=d("Tarefas"),me=l(),p=o("tbody"),h=o("tr"),M=o("td"),fe=d("Encoder"),ue=l(),N=o("td"),pe=d("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),he=l(),k=o("td"),ve=d("Classifica\xE7\xE3o de senten\xE7as, reconhecimento de entidades nomeadas, Q&A"),Te=l(),v=o("tr"),z=o("td"),Ee=d("Decoder"),_e=l(),I=o("td"),Re=d("CTRL, GPT, GPT-2, Transformer XL"),be=l(),O=o("td"),Ae=d("Gera\xE7\xE3o de texto"),De=l(),T=o("tr"),U=o("td"),Be=d("Encoder-decoder"),Pe=l(),j=o("td"),ye=d("BART, T5, Marian, mBART"),Le=l(),Q=o("td"),qe=d("Sumariza\xE7\xE3o, tradu\xE7\xE3o, perguntas e respostas gerativas"),this.h()},l(a){const c=We('[data-svelte="svelte-1phssyn"]',document.head);m=r(c,"META",{name:!0,content:!0}),c.forEach(t),V=i(a),f=r(a,"H1",{class:!0});var K=s(f);E=r(K,"A",{id:!0,class:!0,href:!0});var xe=s(E);x=r(xe,"SPAN",{});var $e=s(x);Ze(A.$$.fragment,$e),$e.forEach(t),xe.forEach(t),ee=i(K),$=r(K,"SPAN",{});var Ce=s($);te=n(Ce,"Resumo"),Ce.forEach(t),K.forEach(t),X=i(a),_=r(a,"P",{});var W=s(_);ae=n(W,"Nesse cap\xEDtulo, voc\xEA viu como abordar diferentes tarefas de NLP usando a fun\xE7\xE3o de alto n\xEDvel "),C=r(W,"CODE",{});var ge=s(C);oe=n(ge,"pipeline()"),ge.forEach(t),re=n(W," da biblioteca \u{1F917} Transformers. Voc\xEA tamb\xE9m viu como pesquisar e usar modelos no Hub, bem como usar a API de infer\xEAncia para testar os modelos diretamente em seu navegador."),W.forEach(t),J=i(a),D=r(a,"P",{});var Se=s(D);se=n(Se,"Discutimos como os modelos Transformers funcionam em alto n\xEDvel e falamos sobre a import\xE2ncia do aprendizado de transfer\xEAncia (transfer learning) e do ajuste fino. Um aspecto chave \xE9 que voc\xEA pode usar a arquitetura completa ou apenas o codificador ou decodificador, dependendo do tipo de tarefa que voc\xEA pretende resolver. A tabela a seguir resume isso:"),Se.forEach(t),Y=i(a),R=r(a,"TABLE",{});var Z=s(R);g=r(Z,"THEAD",{});var Ge=s(g);u=r(Ge,"TR",{});var B=s(u);S=r(B,"TH",{});var He=s(S);de=n(He,"Modelo"),He.forEach(t),ne=i(B),G=r(B,"TH",{});var Me=s(G);le=n(Me,"Exemplos"),Me.forEach(t),ie=i(B),H=r(B,"TH",{});var Ne=s(H);ce=n(Ne,"Tarefas"),Ne.forEach(t),B.forEach(t),Ge.forEach(t),me=i(Z),p=r(Z,"TBODY",{});var P=s(p);h=r(P,"TR",{});var y=s(h);M=r(y,"TD",{});var ke=s(M);fe=n(ke,"Encoder"),ke.forEach(t),ue=i(y),N=r(y,"TD",{});var ze=s(N);pe=n(ze,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ze.forEach(t),he=i(y),k=r(y,"TD",{});var Ie=s(k);ve=n(Ie,"Classifica\xE7\xE3o de senten\xE7as, reconhecimento de entidades nomeadas, Q&A"),Ie.forEach(t),y.forEach(t),Te=i(P),v=r(P,"TR",{});var L=s(v);z=r(L,"TD",{});var Oe=s(z);Ee=n(Oe,"Decoder"),Oe.forEach(t),_e=i(L),I=r(L,"TD",{});var Ue=s(I);Re=n(Ue,"CTRL, GPT, GPT-2, Transformer XL"),Ue.forEach(t),be=i(L),O=r(L,"TD",{});var je=s(O);Ae=n(je,"Gera\xE7\xE3o de texto"),je.forEach(t),L.forEach(t),De=i(P),T=r(P,"TR",{});var q=s(T);U=r(q,"TD",{});var Qe=s(U);Be=n(Qe,"Encoder-decoder"),Qe.forEach(t),Pe=i(q),j=r(q,"TD",{});var Ve=s(j);ye=n(Ve,"BART, T5, Marian, mBART"),Ve.forEach(t),Le=i(q),Q=r(q,"TD",{});var Xe=s(Q);qe=n(Xe,"Sumariza\xE7\xE3o, tradu\xE7\xE3o, perguntas e respostas gerativas"),Xe.forEach(t),q.forEach(t),P.forEach(t),Z.forEach(t),this.h()},h(){w(m,"name","hf:doc:metadata"),w(m,"content",JSON.stringify(lt)),w(E,"id","resumo"),w(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(E,"href","#resumo"),w(f,"class","relative group")},m(a,c){e(document.head,m),b(a,V,c),b(a,f,c),e(f,E),e(E,x),et(A,x,null),e(f,ee),e(f,$),e($,te),b(a,X,c),b(a,_,c),e(_,ae),e(_,C),e(C,oe),e(_,re),b(a,J,c),b(a,D,c),e(D,se),b(a,Y,c),b(a,R,c),e(R,g),e(g,u),e(u,S),e(S,de),e(u,ne),e(u,G),e(G,le),e(u,ie),e(u,H),e(H,ce),e(R,me),e(R,p),e(p,h),e(h,M),e(M,fe),e(h,ue),e(h,N),e(N,pe),e(h,he),e(h,k),e(k,ve),e(p,Te),e(p,v),e(v,z),e(z,Ee),e(v,_e),e(v,I),e(I,Re),e(v,be),e(v,O),e(O,Ae),e(p,De),e(p,T),e(T,U),e(U,Be),e(T,Pe),e(T,j),e(j,ye),e(T,Le),e(T,Q),e(Q,qe),F=!0},p:tt,i(a){F||(at(A.$$.fragment,a),F=!0)},o(a){ot(A.$$.fragment,a),F=!1},d(a){t(m),a&&t(V),a&&t(f),rt(A),a&&t(X),a&&t(_),a&&t(J),a&&t(D),a&&t(Y),a&&t(R)}}}const lt={local:"resumo",title:"Resumo"};function it(we){return st(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ft extends Je{constructor(m){super();Ye(this,m,it,nt,Fe,{})}}export{ft as default,lt as metadata};
