import{S as Vv,i as Wv,s as Jv,e as l,t as a,c as o,a as r,h as n,d as t,g as p,G as s,k as m,w,m as _,x as E,y as x,q as g,o as b,B as j,b as y,M as Yv,N as Xr,p as It,v as Qv,n as Ut}from"../../chunks/vendor-hf-doc-builder.js";import{T as Na}from"../../chunks/Tip-hf-doc-builder.js";import{Y as ap}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Oa}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as S}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Gv}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Xv}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Zv(H){let i,h;return i=new Gv({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"}]}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function eg(H){let i,h;return i=new Gv({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"}]}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function sg(H){let i,h,d,$,A;return{c(){i=l("p"),h=a("\u{1F64B} Si les termes \xAB mod\xE9lisation du langage masqu\xE9 \xBB et \xAB mod\xE8le pr\xE9-entra\xEEn\xE9 \xBB ne vous sont pas familiers, consultez le "),d=l("a"),$=a("chapitre 1"),A=a(", o\xF9 nous expliquons tous ces concepts fondamentaux, vid\xE9os \xE0 l\u2019appui !"),this.h()},l(q){i=o(q,"P",{});var k=r(i);h=n(k,"\u{1F64B} Si les termes \xAB mod\xE9lisation du langage masqu\xE9 \xBB et \xAB mod\xE8le pr\xE9-entra\xEEn\xE9 \xBB ne vous sont pas familiers, consultez le "),d=o(k,"A",{href:!0});var z=r(d);$=n(z,"chapitre 1"),z.forEach(t),A=n(k,", o\xF9 nous expliquons tous ces concepts fondamentaux, vid\xE9os \xE0 l\u2019appui !"),k.forEach(t),this.h()},h(){y(d,"href","/course/fr/chapiter1")},m(q,k){p(q,i,k),s(i,h),s(i,d),s(d,$),s(i,A)},d(q){q&&t(i)}}}function tg(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K;return k=new S({props:{code:`from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),P=new S({props:{code:`model(model.dummy_inputs)  # Construire le mod\xE8le
model.summary()`,highlighted:`model(model.dummy_inputs)  <span class="hljs-comment"># Construire le mod\xE8le</span>
model.summary()`}}),U=new S({props:{code:`Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________`,highlighted:`Model: <span class="hljs-string">&quot;tf_distil_bert_for_masked_lm&quot;</span>
_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#   </span>
=================================================================
distilbert (TFDistilBertMain multiple                  <span class="hljs-number">66362880</span>  
_________________________________________________________________
vocab_transform (Dense)      multiple                  <span class="hljs-number">590592</span>    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  <span class="hljs-number">1536</span>      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  <span class="hljs-number">23866170</span>  
=================================================================
Total params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Trainable params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){i=l("p"),h=a("Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=l("code"),$=a("AutoModelForMaskedLM"),A=a(" :"),q=m(),w(k.$$.fragment),z=m(),f=l("p"),M=a("Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=l("code"),C=a("summary()"),L=a(" :"),I=m(),w(P.$$.fragment),D=m(),w(U.$$.fragment)},l(v){i=o(v,"P",{});var N=r(i);h=n(N,"Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=o(N,"CODE",{});var W=r(d);$=n(W,"AutoModelForMaskedLM"),W.forEach(t),A=n(N," :"),N.forEach(t),q=_(v),E(k.$$.fragment,v),z=_(v),f=o(v,"P",{});var G=r(f);M=n(G,"Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=o(G,"CODE",{});var J=r(O);C=n(J,"summary()"),J.forEach(t),L=n(G," :"),G.forEach(t),I=_(v),E(P.$$.fragment,v),D=_(v),E(U.$$.fragment,v)},m(v,N){p(v,i,N),s(i,h),s(i,d),s(d,$),s(i,A),p(v,q,N),x(k,v,N),p(v,z,N),p(v,f,N),s(f,M),s(f,O),s(O,C),s(f,L),p(v,I,N),x(P,v,N),p(v,D,N),x(U,v,N),K=!0},i(v){K||(g(k.$$.fragment,v),g(P.$$.fragment,v),g(U.$$.fragment,v),K=!0)},o(v){b(k.$$.fragment,v),b(P.$$.fragment,v),b(U.$$.fragment,v),K=!1},d(v){v&&t(i),v&&t(q),j(k,v),v&&t(z),v&&t(f),v&&t(I),j(P,v),v&&t(D),j(U,v)}}}function ag(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K;return k=new S({props:{code:`from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),P=new S({props:{code:`distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT nombre de param\xE8tres : {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT nombre de param\xE8tres : 110M'")`,highlighted:`distilbert_num_parameters = model.num_parameters() / <span class="hljs-number">1_000_000</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; DistilBERT nombre de param\xE8tres : <span class="hljs-subst">{<span class="hljs-built_in">round</span>(distilbert_num_parameters)}</span>M&#x27;&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; BERT nombre de param\xE8tres : 110M&#x27;&quot;</span>)`}}),U=new S({props:{code:`'>>> DistilBERT  nombre de param\xE8tres : 67M'
'>>> BERT nombre de param\xE8tres : 110M'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; DistilBERT  nombre de param\xE8tres : 67M&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; BERT nombre de param\xE8tres : 110M&#x27;</span>`}}),{c(){i=l("p"),h=a("Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=l("code"),$=a("AutoModelForMaskedLM"),A=a(" :"),q=m(),w(k.$$.fragment),z=m(),f=l("p"),M=a("Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=l("code"),C=a("num_parameters()"),L=a(" :"),I=m(),w(P.$$.fragment),D=m(),w(U.$$.fragment)},l(v){i=o(v,"P",{});var N=r(i);h=n(N,"Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=o(N,"CODE",{});var W=r(d);$=n(W,"AutoModelForMaskedLM"),W.forEach(t),A=n(N," :"),N.forEach(t),q=_(v),E(k.$$.fragment,v),z=_(v),f=o(v,"P",{});var G=r(f);M=n(G,"Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=o(G,"CODE",{});var J=r(O);C=n(J,"num_parameters()"),J.forEach(t),L=n(G," :"),G.forEach(t),I=_(v),E(P.$$.fragment,v),D=_(v),E(U.$$.fragment,v)},m(v,N){p(v,i,N),s(i,h),s(i,d),s(d,$),s(i,A),p(v,q,N),x(k,v,N),p(v,z,N),p(v,f,N),s(f,M),s(f,O),s(O,C),s(f,L),p(v,I,N),x(P,v,N),p(v,D,N),x(U,v,N),K=!0},i(v){K||(g(k.$$.fragment,v),g(P.$$.fragment,v),g(U.$$.fragment,v),K=!0)},o(v){b(k.$$.fragment,v),b(P.$$.fragment,v),b(U.$$.fragment,v),K=!1},d(v){v&&t(i),v&&t(q),j(k,v),v&&t(z),v&&t(f),v&&t(I),j(P,v),v&&t(D),j(U,v)}}}function ng(H){let i,h;return i=new S({props:{code:`import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Trouve l'emplacement de [MASK] et extrait ses logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# On choisit les candidats [MASK] avec les logits les plus \xE9lev\xE9s
# Nous annulons le tableau avant argsort pour obtenir le plus grand, et non le plus petit, logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># Trouve l&#x27;emplacement de [MASK] et extrait ses logits</span>
mask_token_index = np.argwhere(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># On choisit les candidats [MASK] avec les logits les plus \xE9lev\xE9s</span>
<span class="hljs-comment"># Nous annulons le tableau avant argsort pour obtenir le plus grand, et non le plus petit, logits</span>
top_5_tokens = np.argsort(-mask_token_logits)[:<span class="hljs-number">5</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function lg(H){let i,h;return i=new S({props:{code:`import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Trouve l'emplacement de [MASK] et extrait ses logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Choisissez les candidats [MASK] avec les logits les plus \xE9lev\xE9s
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")`,highlighted:`<span class="hljs-keyword">import</span> torch

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># Trouve l&#x27;emplacement de [MASK] et extrait ses logits</span>
mask_token_index = torch.where(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># Choisissez les candidats [MASK] avec les logits les plus \xE9lev\xE9s</span>
top_5_tokens = torch.topk(mask_token_logits, <span class="hljs-number">5</span>, dim=<span class="hljs-number">1</span>).indices[<span class="hljs-number">0</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&#x27;&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function og(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),A=a(" Cr\xE9ez un \xE9chantillon al\xE9atoire de la r\xE9partition "),q=l("code"),k=a("unsupervised"),z=a(" et v\xE9rifiez que les \xE9tiquettes ne sont ni "),f=l("code"),M=a("0"),O=a(" ni "),C=l("code"),L=a("1"),I=a(". Pendant que vous y \xEAtes, vous pouvez aussi v\xE9rifier que les \xE9tiquettes dans les \xE9chantillons "),P=l("code"),D=a("train"),U=a(" et "),K=l("code"),v=a("test"),N=a(" sont bien "),W=l("code"),G=a("0"),J=a(" ou "),ee=l("code"),re=a("1"),te=a(". C\u2019est un contr\xF4le utile que tout praticien en NLP devrait effectuer au d\xE9but d\u2019un nouveau projet !")},l(V){i=o(V,"P",{});var R=r(i);h=n(R,"\u270F\uFE0F "),d=o(R,"STRONG",{});var ie=r(d);$=n(ie,"Essayez !"),ie.forEach(t),A=n(R," Cr\xE9ez un \xE9chantillon al\xE9atoire de la r\xE9partition "),q=o(R,"CODE",{});var Q=r(q);k=n(Q,"unsupervised"),Q.forEach(t),z=n(R," et v\xE9rifiez que les \xE9tiquettes ne sont ni "),f=o(R,"CODE",{});var se=r(f);M=n(se,"0"),se.forEach(t),O=n(R," ni "),C=o(R,"CODE",{});var ue=r(C);L=n(ue,"1"),ue.forEach(t),I=n(R,". Pendant que vous y \xEAtes, vous pouvez aussi v\xE9rifier que les \xE9tiquettes dans les \xE9chantillons "),P=o(R,"CODE",{});var Z=r(P);D=n(Z,"train"),Z.forEach(t),U=n(R," et "),K=o(R,"CODE",{});var ge=r(K);v=n(ge,"test"),ge.forEach(t),N=n(R," sont bien "),W=o(R,"CODE",{});var ne=r(W);G=n(ne,"0"),ne.forEach(t),J=n(R," ou "),ee=o(R,"CODE",{});var de=r(ee);re=n(de,"1"),de.forEach(t),te=n(R,". C\u2019est un contr\xF4le utile que tout praticien en NLP devrait effectuer au d\xE9but d\u2019un nouveau projet !"),R.forEach(t)},m(V,R){p(V,i,R),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O),s(i,C),s(C,L),s(i,I),s(i,P),s(P,D),s(i,U),s(i,K),s(K,v),s(i,N),s(i,W),s(W,G),s(i,J),s(i,ee),s(ee,re),s(i,te)},d(V){V&&t(i)}}}function rg(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),A=a(" Certains "),q=l("em"),k=a("transformers"),z=a(", comme "),f=l("a"),M=a("BigBird"),O=a(" et "),C=l("a"),L=a("Longformer"),I=a(", ont une longueur de contexte beaucoup plus longue que BERT et les autres premiers "),P=l("em"),D=a("transformers"),U=a(". Instanciez le "),K=l("em"),v=a("tokenizer"),N=a(" pour l\u2019un de ces "),W=l("em"),G=a("checkpoints"),J=a(" et v\xE9rifiez que le "),ee=l("code"),re=a("model_max_length"),te=a(" correspond \xE0 ce qui est indiqu\xE9 sur sa carte."),this.h()},l(V){i=o(V,"P",{});var R=r(i);h=n(R,"\u270F\uFE0F "),d=o(R,"STRONG",{});var ie=r(d);$=n(ie,"Essayez !"),ie.forEach(t),A=n(R," Certains "),q=o(R,"EM",{});var Q=r(q);k=n(Q,"transformers"),Q.forEach(t),z=n(R,", comme "),f=o(R,"A",{href:!0,rel:!0});var se=r(f);M=n(se,"BigBird"),se.forEach(t),O=n(R," et "),C=o(R,"A",{href:!0});var ue=r(C);L=n(ue,"Longformer"),ue.forEach(t),I=n(R,", ont une longueur de contexte beaucoup plus longue que BERT et les autres premiers "),P=o(R,"EM",{});var Z=r(P);D=n(Z,"transformers"),Z.forEach(t),U=n(R,". Instanciez le "),K=o(R,"EM",{});var ge=r(K);v=n(ge,"tokenizer"),ge.forEach(t),N=n(R," pour l\u2019un de ces "),W=o(R,"EM",{});var ne=r(W);G=n(ne,"checkpoints"),ne.forEach(t),J=n(R," et v\xE9rifiez que le "),ee=o(R,"CODE",{});var de=r(ee);re=n(de,"model_max_length"),de.forEach(t),te=n(R," correspond \xE0 ce qui est indiqu\xE9 sur sa carte."),R.forEach(t),this.h()},h(){y(f,"href","https://huggingface.co/google/bigbird-roberta-base"),y(f,"rel","nofollow"),y(C,"href","hf.co/allenai/longformer-base-4096")},m(V,R){p(V,i,R),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O),s(i,C),s(C,L),s(i,I),s(i,P),s(P,D),s(i,U),s(i,K),s(K,v),s(i,N),s(i,W),s(W,G),s(i,J),s(i,ee),s(ee,re),s(i,te)},d(V){V&&t(i)}}}function ig(H){let i,h;return{c(){i=l("p"),h=a("Notez que l\u2019utilisation d\u2019une petite taille peut \xEAtre pr\xE9judiciable dans les sc\xE9narios du monde r\xE9el. Vous devez donc utiliser une taille qui correspond au cas d\u2019utilisation auquel vous appliquerez votre mod\xE8le.")},l(d){i=o(d,"P",{});var $=r(i);h=n($,"Notez que l\u2019utilisation d\u2019une petite taille peut \xEAtre pr\xE9judiciable dans les sc\xE9narios du monde r\xE9el. Vous devez donc utiliser une taille qui correspond au cas d\u2019utilisation auquel vous appliquerez votre mod\xE8le."),$.forEach(t)},m(d,$){p(d,i,$),s(i,h)},d(d){d&&t(i)}}}function ug(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez"),A=a(" Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),q=l("code"),k=a("tokenizer.decode()"),z=a(" par "),f=l("code"),M=a("tokenizer.convert_ids_to_tokens()"),O=a(" pour voir que parfois un seul "),C=l("em"),L=a("token"),I=a(" d\u2019un mot donn\xE9 est masqu\xE9 et pas les autres.")},l(P){i=o(P,"P",{});var D=r(i);h=n(D,"\u270F\uFE0F "),d=o(D,"STRONG",{});var U=r(d);$=n(U,"Essayez"),U.forEach(t),A=n(D," Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),q=o(D,"CODE",{});var K=r(q);k=n(K,"tokenizer.decode()"),K.forEach(t),z=n(D," par "),f=o(D,"CODE",{});var v=r(f);M=n(v,"tokenizer.convert_ids_to_tokens()"),v.forEach(t),O=n(D," pour voir que parfois un seul "),C=o(D,"EM",{});var N=r(C);L=n(N,"token"),N.forEach(t),I=n(D," d\u2019un mot donn\xE9 est masqu\xE9 et pas les autres."),D.forEach(t)},m(P,D){p(P,i,D),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O),s(i,C),s(C,L),s(i,I)},d(P){P&&t(i)}}}function Hv(H){let i,h,d,$,A,q,k,z,f,M,O;return{c(){i=l("p"),h=a("Un effet secondaire du masquage al\xE9atoire est que nos m\xE9triques d\u2019\xE9valuation ne seront pas d\xE9terministes lorsque nous utilisons la fonction "),d=l("code"),$=a("Trainer"),A=a(" puisque nous utilisons le m\xEAme collateur de donn\xE9es pour les \xE9chantillons d\u2019entra\xEEnement et de test. Nous verrons plus tard, lorsque nous examinerons le "),q=l("em"),k=a("finetuning"),z=a(" avec \u{1F917} "),f=l("em"),M=a("Accelerate"),O=a(", comment nous pouvons utiliser la flexibilit\xE9 d\u2019une boucle d\u2019\xE9valuation personnalis\xE9e pour geler le caract\xE8re al\xE9atoire.")},l(C){i=o(C,"P",{});var L=r(i);h=n(L,"Un effet secondaire du masquage al\xE9atoire est que nos m\xE9triques d\u2019\xE9valuation ne seront pas d\xE9terministes lorsque nous utilisons la fonction "),d=o(L,"CODE",{});var I=r(d);$=n(I,"Trainer"),I.forEach(t),A=n(L," puisque nous utilisons le m\xEAme collateur de donn\xE9es pour les \xE9chantillons d\u2019entra\xEEnement et de test. Nous verrons plus tard, lorsque nous examinerons le "),q=o(L,"EM",{});var P=r(q);k=n(P,"finetuning"),P.forEach(t),z=n(L," avec \u{1F917} "),f=o(L,"EM",{});var D=r(f);M=n(D,"Accelerate"),D.forEach(t),O=n(L,", comment nous pouvons utiliser la flexibilit\xE9 d\u2019une boucle d\u2019\xE9valuation personnalis\xE9e pour geler le caract\xE8re al\xE9atoire."),L.forEach(t)},m(C,L){p(C,i,L),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O)},d(C){C&&t(i)}}}function pg(H){let i,h;return i=new S({props:{code:`import collections
import numpy as np

from transformers.data import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr\xE9ation d'une correspondance entre les mots et les indices des tokens correspondants
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa\xE7on al\xE9atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return tf_default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers.data <span class="hljs-keyword">import</span> tf_default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># Cr\xE9ation d&#x27;une correspondance entre les mots et les indices des tokens correspondants</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Masquer des mots de fa\xE7on al\xE9atoire</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> tf_default_data_collator(features)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function dg(H){let i,h;return i=new S({props:{code:`import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr\xE9ation d'une correspondance entre les mots et les indices des tokens correspondants
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa\xE7on al\xE9atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># Cr\xE9ation d&#x27;une correspondance entre les mots et les indices des tokens correspondants</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Masquer des mots de fa\xE7on al\xE9atoire</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> default_data_collator(features)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function cg(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez"),A=a(" Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),q=l("code"),k=a("tokenizer.decode()"),z=a(" par "),f=l("code"),M=a("tokenizer.convert_ids_to_tokens()"),O=a(" pour voir que les "),C=l("em"),L=a("tokens"),I=a(" d\u2019un mot donn\xE9 sont toujours masqu\xE9s ensemble.")},l(P){i=o(P,"P",{});var D=r(i);h=n(D,"\u270F\uFE0F "),d=o(D,"STRONG",{});var U=r(d);$=n(U,"Essayez"),U.forEach(t),A=n(D," Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),q=o(D,"CODE",{});var K=r(q);k=n(K,"tokenizer.decode()"),K.forEach(t),z=n(D," par "),f=o(D,"CODE",{});var v=r(f);M=n(v,"tokenizer.convert_ids_to_tokens()"),v.forEach(t),O=n(D," pour voir que les "),C=o(D,"EM",{});var N=r(C);L=n(N,"tokens"),N.forEach(t),I=n(D," d\u2019un mot donn\xE9 sont toujours masqu\xE9s ensemble."),D.forEach(t)},m(P,D){p(P,i,D),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O),s(i,C),s(C,L),s(i,I)},d(P){P&&t(i)}}}function mg(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te,V,R,ie,Q,se,ue,Z,ge,ne,de,rs,be,ke,is,Ve,qe,us,We,je,Ne,Je,X,pe,ye,ce,ps,ze,$e,Xe,le,Ze,we,Ee,B,ae,Re,me,es,Ye,Me,Oe,Be;return k=new S({props:{code:`from transformers import TrainingArguments

batch_size = 64
# Montrer la perte d'entra\xEEnement \xE0 chaque \xE9poque
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># Montrer la perte d&#x27;entra\xEEnement \xE0 chaque \xE9poque</span>
logging_steps = <span class="hljs-built_in">len</span>(downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>,
    overwrite_output_dir=<span class="hljs-literal">True</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
)`}}),we=new S({props:{code:`from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>],
    data_collator=data_collator,
)`}}),{c(){i=l("p"),h=a("Une fois que nous sommes connect\xE9s, nous pouvons sp\xE9cifier les arguments pour le "),d=l("code"),$=a("Trainer"),A=a(" :"),q=m(),w(k.$$.fragment),z=m(),f=l("p"),M=a("Ici, nous avons modifi\xE9 quelques options par d\xE9faut, y compris "),O=l("code"),C=a("logging_steps"),L=a(" pour s\u2019assurer que nous suivons la perte d\u2019entra\xEEnement \xE0 chaque \xE9poque. Nous avons \xE9galement utilis\xE9 "),I=l("code"),P=a("fp16=True"),D=a(" pour activer l\u2019entra\xEEnement en pr\xE9cision mixte, ce qui nous donne un autre gain de vitesse. Par d\xE9faut, "),U=l("code"),K=a("Trainer"),v=a(" va supprimer toutes les colonnes qui ne font pas partie de la m\xE9thode "),N=l("code"),W=a("forward()"),G=a(" du mod\xE8le. Cela signifie que si vous utilisez le collateur de masquage de mots entiers, vous devrez \xE9galement d\xE9finir "),J=l("code"),ee=a("remove_unused_columns=False"),re=a(" pour vous assurer que nous ne perdons pas la colonne "),te=l("code"),V=a("word_ids"),R=a(" pendant l\u2019entra\xEEnement."),ie=m(),Q=l("p"),se=a("Notez que vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),ue=l("code"),Z=a("hub_model_id"),ge=a(" (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ne=l("a"),de=l("code"),rs=a("huggingface-course"),be=a(", nous avons ajout\xE9 "),ke=l("code"),is=a('hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),Ve=m(),qe=l("code"),us=a("TrainingArguments"),We=a(". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas ce sera "),je=l("code"),Ne=a('"lewtun/distilbert-finetuned-imdb"'),Je=a("."),X=m(),pe=l("p"),ye=a("Nous avons maintenant tous les ingr\xE9dients pour instancier le "),ce=l("code"),ps=a("Trainer"),ze=a(". Ici, nous utilisons juste le collateur standard "),$e=l("code"),Xe=a("data_collator"),le=a(", mais vous pouvez essayer le collateur de masquage de mots entiers et comparer les r\xE9sultats comme exercice :"),Ze=m(),w(we.$$.fragment),Ee=m(),B=l("p"),ae=a("Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),Re=l("code"),me=a("trainer.train()"),es=a(". Mais avant, regardons bri\xE8vement la "),Ye=l("em"),Me=a("perplexit\xE9"),Oe=a(" qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),this.h()},l(T){i=o(T,"P",{});var Y=r(i);h=n(Y,"Une fois que nous sommes connect\xE9s, nous pouvons sp\xE9cifier les arguments pour le "),d=o(Y,"CODE",{});var Bs=r(d);$=n(Bs,"Trainer"),Bs.forEach(t),A=n(Y," :"),Y.forEach(t),q=_(T),E(k.$$.fragment,T),z=_(T),f=o(T,"P",{});var oe=r(f);M=n(oe,"Ici, nous avons modifi\xE9 quelques options par d\xE9faut, y compris "),O=o(oe,"CODE",{});var Ce=r(O);C=n(Ce,"logging_steps"),Ce.forEach(t),L=n(oe," pour s\u2019assurer que nous suivons la perte d\u2019entra\xEEnement \xE0 chaque \xE9poque. Nous avons \xE9galement utilis\xE9 "),I=o(oe,"CODE",{});var Ps=r(I);P=n(Ps,"fp16=True"),Ps.forEach(t),D=n(oe," pour activer l\u2019entra\xEEnement en pr\xE9cision mixte, ce qui nous donne un autre gain de vitesse. Par d\xE9faut, "),U=o(oe,"CODE",{});var Fe=r(U);K=n(Fe,"Trainer"),Fe.forEach(t),v=n(oe," va supprimer toutes les colonnes qui ne font pas partie de la m\xE9thode "),N=o(oe,"CODE",{});var Qe=r(N);W=n(Qe,"forward()"),Qe.forEach(t),G=n(oe," du mod\xE8le. Cela signifie que si vous utilisez le collateur de masquage de mots entiers, vous devrez \xE9galement d\xE9finir "),J=o(oe,"CODE",{});var He=r(J);ee=n(He,"remove_unused_columns=False"),He.forEach(t),re=n(oe," pour vous assurer que nous ne perdons pas la colonne "),te=o(oe,"CODE",{});var De=r(te);V=n(De,"word_ids"),De.forEach(t),R=n(oe," pendant l\u2019entra\xEEnement."),oe.forEach(t),ie=_(T),Q=o(T,"P",{});var fe=r(Q);se=n(fe,"Notez que vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),ue=o(fe,"CODE",{});var Fs=r(ue);Z=n(Fs,"hub_model_id"),Fs.forEach(t),ge=n(fe," (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ne=o(fe,"A",{href:!0,rel:!0});var nt=r(ne);de=o(nt,"CODE",{});var Pe=r(de);rs=n(Pe,"huggingface-course"),Pe.forEach(t),nt.forEach(t),be=n(fe,", nous avons ajout\xE9 "),ke=o(fe,"CODE",{});var yt=r(ke);is=n(yt,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),yt.forEach(t),Ve=_(fe),qe=o(fe,"CODE",{});var ds=r(qe);us=n(ds,"TrainingArguments"),ds.forEach(t),We=n(fe,". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas ce sera "),je=o(fe,"CODE",{});var xe=r(je);Ne=n(xe,'"lewtun/distilbert-finetuned-imdb"'),xe.forEach(t),Je=n(fe,"."),fe.forEach(t),X=_(T),pe=o(T,"P",{});var Ie=r(pe);ye=n(Ie,"Nous avons maintenant tous les ingr\xE9dients pour instancier le "),ce=o(Ie,"CODE",{});var lt=r(ce);ps=n(lt,"Trainer"),lt.forEach(t),ze=n(Ie,". Ici, nous utilisons juste le collateur standard "),$e=o(Ie,"CODE",{});var cs=r($e);Xe=n(cs,"data_collator"),cs.forEach(t),le=n(Ie,", mais vous pouvez essayer le collateur de masquage de mots entiers et comparer les r\xE9sultats comme exercice :"),Ie.forEach(t),Ze=_(T),E(we.$$.fragment,T),Ee=_(T),B=o(T,"P",{});var gs=r(B);ae=n(gs,"Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),Re=o(gs,"CODE",{});var ot=r(Re);me=n(ot,"trainer.train()"),ot.forEach(t),es=n(gs,". Mais avant, regardons bri\xE8vement la "),Ye=o(gs,"EM",{});var ms=r(Ye);Me=n(ms,"perplexit\xE9"),ms.forEach(t),Oe=n(gs," qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),gs.forEach(t),this.h()},h(){y(ne,"href","https://huggingface.co/huggingface-course"),y(ne,"rel","nofollow")},m(T,Y){p(T,i,Y),s(i,h),s(i,d),s(d,$),s(i,A),p(T,q,Y),x(k,T,Y),p(T,z,Y),p(T,f,Y),s(f,M),s(f,O),s(O,C),s(f,L),s(f,I),s(I,P),s(f,D),s(f,U),s(U,K),s(f,v),s(f,N),s(N,W),s(f,G),s(f,J),s(J,ee),s(f,re),s(f,te),s(te,V),s(f,R),p(T,ie,Y),p(T,Q,Y),s(Q,se),s(Q,ue),s(ue,Z),s(Q,ge),s(Q,ne),s(ne,de),s(de,rs),s(Q,be),s(Q,ke),s(ke,is),s(Q,Ve),s(Q,qe),s(qe,us),s(Q,We),s(Q,je),s(je,Ne),s(Q,Je),p(T,X,Y),p(T,pe,Y),s(pe,ye),s(pe,ce),s(ce,ps),s(pe,ze),s(pe,$e),s($e,Xe),s(pe,le),p(T,Ze,Y),x(we,T,Y),p(T,Ee,Y),p(T,B,Y),s(B,ae),s(B,Re),s(Re,me),s(B,es),s(B,Ye),s(Ye,Me),s(B,Oe),Be=!0},i(T){Be||(g(k.$$.fragment,T),g(we.$$.fragment,T),Be=!0)},o(T){b(k.$$.fragment,T),b(we.$$.fragment,T),Be=!1},d(T){T&&t(i),T&&t(q),j(k,T),T&&t(z),T&&t(f),T&&t(ie),T&&t(Q),T&&t(X),T&&t(pe),T&&t(Ze),j(we,T),T&&t(Ee),T&&t(B)}}}function _g(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te,V,R,ie,Q,se,ue,Z,ge,ne,de,rs,be,ke,is,Ve,qe,us,We,je,Ne,Je,X,pe,ye,ce,ps,ze,$e,Xe,le,Ze,we,Ee;return k=new S({props:{code:`tf_train_dataset = downsampled_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = downsampled_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)

tf_eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),pe=new S({props:{code:`from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra\xEEner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Entra\xEEner en mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>, tokenizer=tokenizer
)`}}),{c(){i=l("p"),h=a("Une fois que nous sommes connect\xE9s, nous pouvons cr\xE9er nos jeux de donn\xE9es "),d=l("code"),$=a("tf.data"),A=a(". Nous n\u2019utiliserons ici que le collecteur de donn\xE9es standard, mais vous pouvez \xE9galement essayer le collecteur de masquage de mots entiers et comparer les r\xE9sultats \xE0 titre d\u2019exercice :"),q=m(),w(k.$$.fragment),z=m(),f=l("p"),M=a("Ensuite, nous configurons nos hyperparam\xE8tres d\u2019entra\xEEnement et compilons notre mod\xE8le. Nous utilisons la fonction "),O=l("code"),C=a("create_optimizer()"),L=a(" de la biblioth\xE8que \u{1F917} "),I=l("em"),P=a("Transformers"),D=a(", qui nous donne un optimiseur "),U=l("code"),K=a("AdamW"),v=a(" avec une d\xE9croissance lin\xE9aire du taux d\u2019apprentissage. Nous utilisons \xE9galement la perte int\xE9gr\xE9e au mod\xE8le, qui est la perte par d\xE9faut lorsqu\u2019aucune perte n\u2019est sp\xE9cifi\xE9e comme argument de "),N=l("code"),W=a("compile()"),G=a(", et nous d\xE9finissons la pr\xE9cision d\u2019entra\xEEnement \xE0 "),J=l("code"),ee=a('"mixed_float16"'),re=a(". Notez que si vous utilisez un GPU Colab ou un autre GPU qui n\u2019a pas le support acc\xE9l\xE9r\xE9 en float16, vous devriez probablement commenter cette ligne."),te=m(),V=l("p"),R=a("De plus, nous mettons en place un "),ie=l("code"),Q=a("PushToHubCallback"),se=a(" qui sauvegardera le mod\xE8le sur le "),ue=l("em"),Z=a("Hub"),ge=a(" apr\xE8s chaque \xE9poque. Vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),ne=l("code"),de=a("hub_model_id"),rs=a(" (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, pour pousser le mod\xE8le vers l\u2019organisation "),be=l("a"),ke=l("code"),is=a("huggingface-course"),Ve=a(", nous avons ajout\xE9 "),qe=l("code"),us=a('hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),We=a(". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas, ce sera "),je=l("code"),Ne=a('"lewtun/distilbert-finetuned-imdb"'),Je=a("."),X=m(),w(pe.$$.fragment),ye=m(),ce=l("p"),ps=a("Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),ze=l("code"),$e=a("model.fit()"),Xe=a(". Mais avant, regardons bri\xE8vement la "),le=l("em"),Ze=a("perplexit\xE9"),we=a(" qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),this.h()},l(B){i=o(B,"P",{});var ae=r(i);h=n(ae,"Une fois que nous sommes connect\xE9s, nous pouvons cr\xE9er nos jeux de donn\xE9es "),d=o(ae,"CODE",{});var Re=r(d);$=n(Re,"tf.data"),Re.forEach(t),A=n(ae,". Nous n\u2019utiliserons ici que le collecteur de donn\xE9es standard, mais vous pouvez \xE9galement essayer le collecteur de masquage de mots entiers et comparer les r\xE9sultats \xE0 titre d\u2019exercice :"),ae.forEach(t),q=_(B),E(k.$$.fragment,B),z=_(B),f=o(B,"P",{});var me=r(f);M=n(me,"Ensuite, nous configurons nos hyperparam\xE8tres d\u2019entra\xEEnement et compilons notre mod\xE8le. Nous utilisons la fonction "),O=o(me,"CODE",{});var es=r(O);C=n(es,"create_optimizer()"),es.forEach(t),L=n(me," de la biblioth\xE8que \u{1F917} "),I=o(me,"EM",{});var Ye=r(I);P=n(Ye,"Transformers"),Ye.forEach(t),D=n(me,", qui nous donne un optimiseur "),U=o(me,"CODE",{});var Me=r(U);K=n(Me,"AdamW"),Me.forEach(t),v=n(me," avec une d\xE9croissance lin\xE9aire du taux d\u2019apprentissage. Nous utilisons \xE9galement la perte int\xE9gr\xE9e au mod\xE8le, qui est la perte par d\xE9faut lorsqu\u2019aucune perte n\u2019est sp\xE9cifi\xE9e comme argument de "),N=o(me,"CODE",{});var Oe=r(N);W=n(Oe,"compile()"),Oe.forEach(t),G=n(me,", et nous d\xE9finissons la pr\xE9cision d\u2019entra\xEEnement \xE0 "),J=o(me,"CODE",{});var Be=r(J);ee=n(Be,'"mixed_float16"'),Be.forEach(t),re=n(me,". Notez que si vous utilisez un GPU Colab ou un autre GPU qui n\u2019a pas le support acc\xE9l\xE9r\xE9 en float16, vous devriez probablement commenter cette ligne."),me.forEach(t),te=_(B),V=o(B,"P",{});var T=r(V);R=n(T,"De plus, nous mettons en place un "),ie=o(T,"CODE",{});var Y=r(ie);Q=n(Y,"PushToHubCallback"),Y.forEach(t),se=n(T," qui sauvegardera le mod\xE8le sur le "),ue=o(T,"EM",{});var Bs=r(ue);Z=n(Bs,"Hub"),Bs.forEach(t),ge=n(T," apr\xE8s chaque \xE9poque. Vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),ne=o(T,"CODE",{});var oe=r(ne);de=n(oe,"hub_model_id"),oe.forEach(t),rs=n(T," (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, pour pousser le mod\xE8le vers l\u2019organisation "),be=o(T,"A",{href:!0,rel:!0});var Ce=r(be);ke=o(Ce,"CODE",{});var Ps=r(ke);is=n(Ps,"huggingface-course"),Ps.forEach(t),Ce.forEach(t),Ve=n(T,", nous avons ajout\xE9 "),qe=o(T,"CODE",{});var Fe=r(qe);us=n(Fe,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),Fe.forEach(t),We=n(T,". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas, ce sera "),je=o(T,"CODE",{});var Qe=r(je);Ne=n(Qe,'"lewtun/distilbert-finetuned-imdb"'),Qe.forEach(t),Je=n(T,"."),T.forEach(t),X=_(B),E(pe.$$.fragment,B),ye=_(B),ce=o(B,"P",{});var He=r(ce);ps=n(He,"Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),ze=o(He,"CODE",{});var De=r(ze);$e=n(De,"model.fit()"),De.forEach(t),Xe=n(He,". Mais avant, regardons bri\xE8vement la "),le=o(He,"EM",{});var fe=r(le);Ze=n(fe,"perplexit\xE9"),fe.forEach(t),we=n(He," qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),He.forEach(t),this.h()},h(){y(be,"href","https://huggingface.co/huggingface-course"),y(be,"rel","nofollow")},m(B,ae){p(B,i,ae),s(i,h),s(i,d),s(d,$),s(i,A),p(B,q,ae),x(k,B,ae),p(B,z,ae),p(B,f,ae),s(f,M),s(f,O),s(O,C),s(f,L),s(f,I),s(I,P),s(f,D),s(f,U),s(U,K),s(f,v),s(f,N),s(N,W),s(f,G),s(f,J),s(J,ee),s(f,re),p(B,te,ae),p(B,V,ae),s(V,R),s(V,ie),s(ie,Q),s(V,se),s(V,ue),s(ue,Z),s(V,ge),s(V,ne),s(ne,de),s(V,rs),s(V,be),s(be,ke),s(ke,is),s(V,Ve),s(V,qe),s(qe,us),s(V,We),s(V,je),s(je,Ne),s(V,Je),p(B,X,ae),x(pe,B,ae),p(B,ye,ae),p(B,ce,ae),s(ce,ps),s(ce,ze),s(ze,$e),s(ce,Xe),s(ce,le),s(le,Ze),s(ce,we),Ee=!0},i(B){Ee||(g(k.$$.fragment,B),g(pe.$$.fragment,B),Ee=!0)},o(B){b(k.$$.fragment,B),b(pe.$$.fragment,B),Ee=!1},d(B){B&&t(i),B&&t(q),j(k,B),B&&t(z),B&&t(f),B&&t(te),B&&t(V),B&&t(X),j(pe,B),B&&t(ye),B&&t(ce)}}}function fg(H){let i,h,d,$,A,q,k,z;return k=new S({props:{code:`import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexit\xE9 : {math.exp(eval_loss):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexit\xE9 : <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){i=l("p"),h=a("En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \xAB surpris \xBB ou \xAB perplexe \xBB vis-\xE0-vis des exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9. Celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=l("code"),$=a("model.evaluate()"),A=a(" pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),q=m(),w(k.$$.fragment)},l(f){i=o(f,"P",{});var M=r(i);h=n(M,"En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \xAB surpris \xBB ou \xAB perplexe \xBB vis-\xE0-vis des exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9. Celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=o(M,"CODE",{});var O=r(d);$=n(O,"model.evaluate()"),O.forEach(t),A=n(M," pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),M.forEach(t),q=_(f),E(k.$$.fragment,f)},m(f,M){p(f,i,M),s(i,h),s(i,d),s(d,$),s(i,A),p(f,q,M),x(k,f,M),z=!0},i(f){z||(g(k.$$.fragment,f),z=!0)},o(f){b(k.$$.fragment,f),z=!1},d(f){f&&t(i),f&&t(q),j(k,f)}}}function hg(H){let i,h,d,$,A,q,k,z;return k=new S({props:{code:`import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexity: <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){i=l("p"),h=a("En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \xAB surpris \xBB ou \xAB perplexe \xBB vis-\xE0-vis des exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9. Celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=l("code"),$=a("Trainer.evaluate()"),A=a(" pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),q=m(),w(k.$$.fragment)},l(f){i=o(f,"P",{});var M=r(i);h=n(M,"En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \xAB surpris \xBB ou \xAB perplexe \xBB vis-\xE0-vis des exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9. Celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=o(M,"CODE",{});var O=r(d);$=n(O,"Trainer.evaluate()"),O.forEach(t),A=n(M," pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),M.forEach(t),q=_(f),E(k.$$.fragment,f)},m(f,M){p(f,i,M),s(i,h),s(i,d),s(d,$),s(i,A),p(f,q,M),x(k,f,M),z=!0},i(f){z||(g(k.$$.fragment,f),z=!0)},o(f){b(k.$$.fragment,f),z=!1},d(f){f&&t(i),f&&t(q),j(k,f)}}}function vg(H){let i,h;return i=new S({props:{code:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])",highlighted:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function gg(H){let i,h;return i=new S({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function bg(H){let i,h;return i=new S({props:{code:`eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexit\xE9 : {math.exp(eval_loss):.2f}")`,highlighted:`eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexit\xE9 : <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function kg(H){let i,h;return i=new S({props:{code:`eval_results = trainer.evaluate()
print(f">>> Perplexit\xE9 : {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexit\xE9 : <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){E(i.$$.fragment,d)},m(d,$){x(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){b(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Iv(H){let i,h,d,$,A,q,k,z,f,M,O;return M=new S({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){i=l("p"),h=a("Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser la carte de mod\xE8le avec les informations d\u2019entra\xEEnement vers le "),d=l("em"),$=a("Hub"),A=a(" (les "),q=l("em"),k=a("checkpoints"),z=a(" sont sauvegard\xE9s pendant l\u2019entra\xEEnement lui-m\xEAme) :"),f=m(),w(M.$$.fragment)},l(C){i=o(C,"P",{});var L=r(i);h=n(L,"Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser la carte de mod\xE8le avec les informations d\u2019entra\xEEnement vers le "),d=o(L,"EM",{});var I=r(d);$=n(I,"Hub"),I.forEach(t),A=n(L," (les "),q=o(L,"EM",{});var P=r(q);k=n(P,"checkpoints"),P.forEach(t),z=n(L," sont sauvegard\xE9s pendant l\u2019entra\xEEnement lui-m\xEAme) :"),L.forEach(t),f=_(C),E(M.$$.fragment,C)},m(C,L){p(C,i,L),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),p(C,f,L),x(M,C,L),O=!0},i(C){O||(g(M.$$.fragment,C),O=!0)},o(C){b(M.$$.fragment,C),O=!1},d(C){C&&t(i),C&&t(f),j(M,C)}}}function qg(H){let i,h,d,$,A;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("A votre tour !"),A=a(" Ex\xE9cutez l\u2019entra\xEEnement ci-dessus apr\xE8s avoir remplac\xE9 le collecteur de donn\xE9es par le collecteur de mots entiers masqu\xE9s. Obtenez-vous de meilleurs r\xE9sultats ?")},l(q){i=o(q,"P",{});var k=r(i);h=n(k,"\u270F\uFE0F "),d=o(k,"STRONG",{});var z=r(d);$=n(z,"A votre tour !"),z.forEach(t),A=n(k," Ex\xE9cutez l\u2019entra\xEEnement ci-dessus apr\xE8s avoir remplac\xE9 le collecteur de donn\xE9es par le collecteur de mots entiers masqu\xE9s. Obtenez-vous de meilleurs r\xE9sultats ?"),k.forEach(t)},m(q,k){p(q,i,k),s(i,h),s(i,d),s(d,$),s(i,A)},d(q){q&&t(i)}}}function Uv(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te,V,R,ie,Q,se,ue,Z,ge,ne,de,rs,be,ke,is,Ve,qe,us,We,je,Ne,Je,X,pe,ye,ce,ps,ze,$e,Xe,le,Ze,we,Ee,B,ae,Re,me,es,Ye,Me,Oe,Be,T,Y,Bs,oe,Ce,Ps,Fe,Qe,He,De,fe,Fs,nt,Pe,yt,ds,xe,Ie,lt,cs,gs,ot,ms,Hs,As,rt,Ss,Ts,Ns,Is,bs,Gt,Vt,ta,ks,Wt,Jt,La,aa,_s,zt,it,na,fs,ut,Os,Qn,la,ve,Ka,pt,oa,dt,Us,Gs,Ra,ra,Ls,ia,ct,Mt,Ct,Ba,Vs;return M=new Oa({}),Ne=new S({props:{code:`def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Cr\xE9er une nouvelle colonne "masqu\xE9e" pour chaque colonne du jeu de donn\xE9es
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">insert_random_mask</span>(<span class="hljs-params">batch</span>):
    features = [<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(batch, t)) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(*batch.values())]
    masked_inputs = data_collator(features)
    <span class="hljs-comment"># Cr\xE9er une nouvelle colonne &quot;masqu\xE9e&quot; pour chaque colonne du jeu de donn\xE9es</span>
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;masked_&quot;</span> + k: v.numpy() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> masked_inputs.items()}`}}),$e=new S({props:{code:`downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)`,highlighted:`downsampled_dataset = downsampled_dataset.remove_columns([<span class="hljs-string">&quot;word_ids&quot;</span>])
eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].<span class="hljs-built_in">map</span>(
    insert_random_mask,
    batched=<span class="hljs-literal">True</span>,
    remove_columns=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        <span class="hljs-string">&quot;masked_input_ids&quot;</span>: <span class="hljs-string">&quot;input_ids&quot;</span>,
        <span class="hljs-string">&quot;masked_attention_mask&quot;</span>: <span class="hljs-string">&quot;attention_mask&quot;</span>,
        <span class="hljs-string">&quot;masked_labels&quot;</span>: <span class="hljs-string">&quot;labels&quot;</span>,
    }
)`}}),Be=new S({props:{code:`from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

batch_size = <span class="hljs-number">64</span>
train_dataloader = DataLoader(
    downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`}}),Qe=new S({props:{code:"model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)",highlighted:'model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoModelForMaskedLM</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">model_checkpoint</span>)</span>'}}),ds=new S({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`}}),Hs=new S({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),Ns=new S({props:{code:`from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">3</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),_s=new S({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-imdb-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),it=new S({props:{code:"'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/distilbert-base-uncased-finetuned-imdb-accelerate&#x27;</span>'}}),dt=new S({props:{code:`from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`}}),Ls=new S({props:{code:`from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra\xEEnement
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # Sauvegarder et t\xE9l\xE9charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> math

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Entra\xEEnement</span>
    model.train()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: <span class="hljs-built_in">len</span>(eval_dataset)]
    <span class="hljs-keyword">try</span>:
        perplexity = math.exp(torch.mean(losses))
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)

    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Epoch <span class="hljs-subst">{epoch}</span>: Perplexity: <span class="hljs-subst">{perplexity}</span>&quot;</span>)

    <span class="hljs-comment"># Sauvegarder et t\xE9l\xE9charger</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),ct=new S({props:{code:`Epoch 0: Perplexity: 11.397545307900472
Epoch 1: Perplexity: 10.904909330983092
Epoch 2: Perplexity: 10.729503505340409`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">0</span>: Perplexity: <span class="hljs-number">11.397545307900472</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">1</span>: Perplexity: <span class="hljs-number">10.904909330983092</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">2</span>: Perplexity: <span class="hljs-number">10.729503505340409</span>`}}),{c(){i=l("p"),h=a("Dans notre cas d\u2019utilisation, nous n\u2019avons pas eu besoin de faire quelque chose de sp\xE9cial avec la boucle d\u2019entra\xEEnement, mais dans certains cas, vous pourriez avoir besoin de mettre en \u0153uvre une logique personnalis\xE9e. Pour ces applications, vous pouvez utiliser \u{1F917} "),d=l("em"),$=a("Accelerate"),A=a(". Jetons un coup d\u2019\u0153il !"),q=m(),k=l("h2"),z=l("a"),f=l("span"),w(M.$$.fragment),O=m(),C=l("span"),L=l("i"),I=a("Finetuning"),P=a(" de DistilBERT avec \u{1F917} "),D=l("i"),U=a("Accelerate"),K=m(),v=l("p"),N=a("Comme nous l\u2019avons vu, avec "),W=l("code"),G=a("Trainer"),J=a(" le "),ee=l("em"),re=a("finetuning"),te=a(" d\u2019un mod\xE8le de langage masqu\xE9 est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte du "),V=l("a"),R=a("chapitre 3"),ie=a(". En fait, la seule subtilit\xE9 est l\u2019utilisation d\u2019un collateur de donn\xE9es sp\xE9cial, et nous l\u2019avons d\xE9j\xE0 couvert plus t\xF4t dans cette section !"),Q=m(),se=l("p"),ue=a("Cependant, nous avons vu que "),Z=l("code"),ge=a("DataCollatorForLanguageModeling"),ne=a(" applique aussi un masquage al\xE9atoire \xE0 chaque \xE9valuation. Nous verrons donc quelques fluctuations dans nos scores de perplexit\xE9 \xE0 chaque entrainement. Une fa\xE7on d\u2019\xE9liminer cette source d\u2019al\xE9at est d\u2019appliquer le masquage "),de=l("em"),rs=a("une fois"),be=a(" sur l\u2019ensemble de test, puis d\u2019utiliser le collateur de donn\xE9es par d\xE9faut dans \u{1F917} "),ke=l("em"),is=a("Transformers"),Ve=a(" pour collecter les batchs pendant l\u2019\xE9valuation. Pour voir comment cela fonctionne, impl\xE9mentons une fonction simple qui applique le masquage sur un batch, similaire \xE0 notre premi\xE8re rencontre avec "),qe=l("code"),us=a("DataCollatorForLanguageModeling"),We=a(" :"),je=m(),w(Ne.$$.fragment),Je=m(),X=l("p"),pe=a("Ensuite, nous allons appliquer cette fonction \xE0 notre jeu de test et laisser tomber les colonnes non masqu\xE9es afin de les remplacer par les colonnes masqu\xE9es. Vous pouvez utiliser le masquage de mots entiers en rempla\xE7ant le "),ye=l("code"),ce=a("data_collator"),ps=a(" ci-dessus par celui qui est appropri\xE9. Dans ce cas vous devez supprimer la premi\xE8re ligne ici :"),ze=m(),w($e.$$.fragment),Xe=m(),le=l("p"),Ze=a("Nous pouvons ensuite configurer les "),we=l("em"),Ee=a("dataloaders"),B=a(" comme d\u2019habitude, mais nous utiliserons le "),ae=l("code"),Re=a("default_data_collator"),me=a(" de \u{1F917} "),es=l("em"),Ye=a("Transformers"),Me=a(" pour le jeu d\u2019\xE9valuation :"),Oe=m(),w(Be.$$.fragment),T=m(),Y=l("p"),Bs=a("Nous suivons les \xE9tapes standard avec \u{1F917} "),oe=l("em"),Ce=a("Accelerate"),Ps=a(". La premi\xE8re est de charger une version fra\xEEche du mod\xE8le pr\xE9-entra\xEEn\xE9 :"),Fe=m(),w(Qe.$$.fragment),He=m(),De=l("p"),fe=a("Ensuite, nous devons sp\xE9cifier l\u2019optimiseur. Nous utiliserons le standard "),Fs=l("code"),nt=a("AdamW"),Pe=a(" :"),yt=m(),w(ds.$$.fragment),xe=m(),Ie=l("p"),lt=a("Avec ces objets, nous pouvons maintenant tout pr\xE9parer pour l\u2019entra\xEEnement avec l\u2019objet "),cs=l("code"),gs=a("Accelerator"),ot=a(" :"),ms=m(),w(Hs.$$.fragment),As=m(),rt=l("p"),Ss=a("Maintenant que notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es sont configur\xE9s, nous pouvons sp\xE9cifier le planificateur du taux d\u2019apprentissage comme suit :"),Ts=m(),w(Ns.$$.fragment),Is=m(),bs=l("p"),Gt=a("Il ne reste qu\u2019une derni\xE8re chose \xE0 faire avant de s\u2019entra\xEEner : cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),Vt=l("em"),ta=a("Hub"),ks=a(" d\u2019Hugging Face ! Nous pouvons utiliser la biblioth\xE8que \u{1F917} "),Wt=l("em"),Jt=a("Hub"),La=a(" pour g\xE9n\xE9rer d\u2019abord le nom complet de notre d\xE9p\xF4t :"),aa=m(),w(_s.$$.fragment),zt=m(),w(it.$$.fragment),na=m(),fs=l("p"),ut=a("puis cr\xE9er et cloner le d\xE9p\xF4t en utilisant la classe "),Os=l("code"),Qn=a("Repository"),la=a(" du \u{1F917} "),ve=l("em"),Ka=a("Hub"),pt=a(" :"),oa=m(),w(dt.$$.fragment),Us=m(),Gs=l("p"),Ra=a("Une fois cela fait, il ne reste plus qu\u2019\xE0 r\xE9diger la boucle compl\xE8te d\u2019entra\xEEnement et d\u2019\xE9valuation :"),ra=m(),w(Ls.$$.fragment),ia=m(),w(ct.$$.fragment),Mt=m(),Ct=l("p"),Ba=a("Cool, nous avons \xE9t\xE9 en mesure d\u2019\xE9valuer la perplexit\xE9 \xE0 chaque \xE9poque et de garantir la reproductibilit\xE9 des entra\xEEnements multiples !"),this.h()},l(c){i=o(c,"P",{});var F=r(i);h=n(F,"Dans notre cas d\u2019utilisation, nous n\u2019avons pas eu besoin de faire quelque chose de sp\xE9cial avec la boucle d\u2019entra\xEEnement, mais dans certains cas, vous pourriez avoir besoin de mettre en \u0153uvre une logique personnalis\xE9e. Pour ces applications, vous pouvez utiliser \u{1F917} "),d=o(F,"EM",{});var Xn=r(d);$=n(Xn,"Accelerate"),Xn.forEach(t),A=n(F,". Jetons un coup d\u2019\u0153il !"),F.forEach(t),q=_(c),k=o(c,"H2",{class:!0});var Yt=r(k);z=o(Yt,"A",{id:!0,class:!0,href:!0});var qs=r(z);f=o(qs,"SPAN",{});var $s=r(f);E(M.$$.fragment,$s),$s.forEach(t),qs.forEach(t),O=_(Yt),C=o(Yt,"SPAN",{});var mt=r(C);L=o(mt,"I",{});var _t=r(L);I=n(_t,"Finetuning"),_t.forEach(t),P=n(mt," de DistilBERT avec \u{1F917} "),D=o(mt,"I",{});var Zn=r(D);U=n(Zn,"Accelerate"),Zn.forEach(t),mt.forEach(t),Yt.forEach(t),K=_(c),v=o(c,"P",{});var ws=r(v);N=n(ws,"Comme nous l\u2019avons vu, avec "),W=o(ws,"CODE",{});var el=r(W);G=n(el,"Trainer"),el.forEach(t),J=n(ws," le "),ee=o(ws,"EM",{});var sl=r(ee);re=n(sl,"finetuning"),sl.forEach(t),te=n(ws," d\u2019un mod\xE8le de langage masqu\xE9 est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte du "),V=o(ws,"A",{href:!0});var Fa=r(V);R=n(Fa,"chapitre 3"),Fa.forEach(t),ie=n(ws,". En fait, la seule subtilit\xE9 est l\u2019utilisation d\u2019un collateur de donn\xE9es sp\xE9cial, et nous l\u2019avons d\xE9j\xE0 couvert plus t\xF4t dans cette section !"),ws.forEach(t),Q=_(c),se=o(c,"P",{});var Ue=r(se);ue=n(Ue,"Cependant, nous avons vu que "),Z=o(Ue,"CODE",{});var Ha=r(Z);ge=n(Ha,"DataCollatorForLanguageModeling"),Ha.forEach(t),ne=n(Ue," applique aussi un masquage al\xE9atoire \xE0 chaque \xE9valuation. Nous verrons donc quelques fluctuations dans nos scores de perplexit\xE9 \xE0 chaque entrainement. Une fa\xE7on d\u2019\xE9liminer cette source d\u2019al\xE9at est d\u2019appliquer le masquage "),de=o(Ue,"EM",{});var he=r(de);rs=n(he,"une fois"),he.forEach(t),be=n(Ue," sur l\u2019ensemble de test, puis d\u2019utiliser le collateur de donn\xE9es par d\xE9faut dans \u{1F917} "),ke=o(Ue,"EM",{});var tl=r(ke);is=n(tl,"Transformers"),tl.forEach(t),Ve=n(Ue," pour collecter les batchs pendant l\u2019\xE9valuation. Pour voir comment cela fonctionne, impl\xE9mentons une fonction simple qui applique le masquage sur un batch, similaire \xE0 notre premi\xE8re rencontre avec "),qe=o(Ue,"CODE",{});var ua=r(qe);us=n(ua,"DataCollatorForLanguageModeling"),ua.forEach(t),We=n(Ue," :"),Ue.forEach(t),je=_(c),E(Ne.$$.fragment,c),Je=_(c),X=o(c,"P",{});var pa=r(X);pe=n(pa,"Ensuite, nous allons appliquer cette fonction \xE0 notre jeu de test et laisser tomber les colonnes non masqu\xE9es afin de les remplacer par les colonnes masqu\xE9es. Vous pouvez utiliser le masquage de mots entiers en rempla\xE7ant le "),ye=o(pa,"CODE",{});var al=r(ye);ce=n(al,"data_collator"),al.forEach(t),ps=n(pa," ci-dessus par celui qui est appropri\xE9. Dans ce cas vous devez supprimer la premi\xE8re ligne ici :"),pa.forEach(t),ze=_(c),E($e.$$.fragment,c),Xe=_(c),le=o(c,"P",{});var Es=r(le);Ze=n(Es,"Nous pouvons ensuite configurer les "),we=o(Es,"EM",{});var nl=r(we);Ee=n(nl,"dataloaders"),nl.forEach(t),B=n(Es," comme d\u2019habitude, mais nous utiliserons le "),ae=o(Es,"CODE",{});var ll=r(ae);Re=n(ll,"default_data_collator"),ll.forEach(t),me=n(Es," de \u{1F917} "),es=o(Es,"EM",{});var Dt=r(es);Ye=n(Dt,"Transformers"),Dt.forEach(t),Me=n(Es," pour le jeu d\u2019\xE9valuation :"),Es.forEach(t),Oe=_(c),E(Be.$$.fragment,c),T=_(c),Y=o(c,"P",{});var Pt=r(Y);Bs=n(Pt,"Nous suivons les \xE9tapes standard avec \u{1F917} "),oe=o(Pt,"EM",{});var ol=r(oe);Ce=n(ol,"Accelerate"),ol.forEach(t),Ps=n(Pt,". La premi\xE8re est de charger une version fra\xEEche du mod\xE8le pr\xE9-entra\xEEn\xE9 :"),Pt.forEach(t),Fe=_(c),E(Qe.$$.fragment,c),He=_(c),De=o(c,"P",{});var da=r(De);fe=n(da,"Ensuite, nous devons sp\xE9cifier l\u2019optimiseur. Nous utiliserons le standard "),Fs=o(da,"CODE",{});var At=r(Fs);nt=n(At,"AdamW"),At.forEach(t),Pe=n(da," :"),da.forEach(t),yt=_(c),E(ds.$$.fragment,c),xe=_(c),Ie=o(c,"P",{});var St=r(Ie);lt=n(St,"Avec ces objets, nous pouvons maintenant tout pr\xE9parer pour l\u2019entra\xEEnement avec l\u2019objet "),cs=o(St,"CODE",{});var rl=r(cs);gs=n(rl,"Accelerator"),rl.forEach(t),ot=n(St," :"),St.forEach(t),ms=_(c),E(Hs.$$.fragment,c),As=_(c),rt=o(c,"P",{});var il=r(rt);Ss=n(il,"Maintenant que notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es sont configur\xE9s, nous pouvons sp\xE9cifier le planificateur du taux d\u2019apprentissage comme suit :"),il.forEach(t),Ts=_(c),E(Ns.$$.fragment,c),Is=_(c),bs=o(c,"P",{});var Ws=r(bs);Gt=n(Ws,"Il ne reste qu\u2019une derni\xE8re chose \xE0 faire avant de s\u2019entra\xEEner : cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),Vt=o(Ws,"EM",{});var ul=r(Vt);ta=n(ul,"Hub"),ul.forEach(t),ks=n(Ws," d\u2019Hugging Face ! Nous pouvons utiliser la biblioth\xE8que \u{1F917} "),Wt=o(Ws,"EM",{});var pl=r(Wt);Jt=n(pl,"Hub"),pl.forEach(t),La=n(Ws," pour g\xE9n\xE9rer d\u2019abord le nom complet de notre d\xE9p\xF4t :"),Ws.forEach(t),aa=_(c),E(_s.$$.fragment,c),zt=_(c),E(it.$$.fragment,c),na=_(c),fs=o(c,"P",{});var Js=r(fs);ut=n(Js,"puis cr\xE9er et cloner le d\xE9p\xF4t en utilisant la classe "),Os=o(Js,"CODE",{});var dl=r(Os);Qn=n(dl,"Repository"),dl.forEach(t),la=n(Js," du \u{1F917} "),ve=o(Js,"EM",{});var cl=r(ve);Ka=n(cl,"Hub"),cl.forEach(t),pt=n(Js," :"),Js.forEach(t),oa=_(c),E(dt.$$.fragment,c),Us=_(c),Gs=o(c,"P",{});var ca=r(Gs);Ra=n(ca,"Une fois cela fait, il ne reste plus qu\u2019\xE0 r\xE9diger la boucle compl\xE8te d\u2019entra\xEEnement et d\u2019\xE9valuation :"),ca.forEach(t),ra=_(c),E(Ls.$$.fragment,c),ia=_(c),E(ct.$$.fragment,c),Mt=_(c),Ct=o(c,"P",{});var ml=r(Ct);Ba=n(ml,"Cool, nous avons \xE9t\xE9 en mesure d\u2019\xE9valuer la perplexit\xE9 \xE0 chaque \xE9poque et de garantir la reproductibilit\xE9 des entra\xEEnements multiples !"),ml.forEach(t),this.h()},h(){y(z,"id","ifinetuningi-de-distilbert-avec-iacceleratei"),y(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(z,"href","#ifinetuningi-de-distilbert-avec-iacceleratei"),y(k,"class","relative group"),y(V,"href","/course/fr/chapter3")},m(c,F){p(c,i,F),s(i,h),s(i,d),s(d,$),s(i,A),p(c,q,F),p(c,k,F),s(k,z),s(z,f),x(M,f,null),s(k,O),s(k,C),s(C,L),s(L,I),s(C,P),s(C,D),s(D,U),p(c,K,F),p(c,v,F),s(v,N),s(v,W),s(W,G),s(v,J),s(v,ee),s(ee,re),s(v,te),s(v,V),s(V,R),s(v,ie),p(c,Q,F),p(c,se,F),s(se,ue),s(se,Z),s(Z,ge),s(se,ne),s(se,de),s(de,rs),s(se,be),s(se,ke),s(ke,is),s(se,Ve),s(se,qe),s(qe,us),s(se,We),p(c,je,F),x(Ne,c,F),p(c,Je,F),p(c,X,F),s(X,pe),s(X,ye),s(ye,ce),s(X,ps),p(c,ze,F),x($e,c,F),p(c,Xe,F),p(c,le,F),s(le,Ze),s(le,we),s(we,Ee),s(le,B),s(le,ae),s(ae,Re),s(le,me),s(le,es),s(es,Ye),s(le,Me),p(c,Oe,F),x(Be,c,F),p(c,T,F),p(c,Y,F),s(Y,Bs),s(Y,oe),s(oe,Ce),s(Y,Ps),p(c,Fe,F),x(Qe,c,F),p(c,He,F),p(c,De,F),s(De,fe),s(De,Fs),s(Fs,nt),s(De,Pe),p(c,yt,F),x(ds,c,F),p(c,xe,F),p(c,Ie,F),s(Ie,lt),s(Ie,cs),s(cs,gs),s(Ie,ot),p(c,ms,F),x(Hs,c,F),p(c,As,F),p(c,rt,F),s(rt,Ss),p(c,Ts,F),x(Ns,c,F),p(c,Is,F),p(c,bs,F),s(bs,Gt),s(bs,Vt),s(Vt,ta),s(bs,ks),s(bs,Wt),s(Wt,Jt),s(bs,La),p(c,aa,F),x(_s,c,F),p(c,zt,F),x(it,c,F),p(c,na,F),p(c,fs,F),s(fs,ut),s(fs,Os),s(Os,Qn),s(fs,la),s(fs,ve),s(ve,Ka),s(fs,pt),p(c,oa,F),x(dt,c,F),p(c,Us,F),p(c,Gs,F),s(Gs,Ra),p(c,ra,F),x(Ls,c,F),p(c,ia,F),x(ct,c,F),p(c,Mt,F),p(c,Ct,F),s(Ct,Ba),Vs=!0},i(c){Vs||(g(M.$$.fragment,c),g(Ne.$$.fragment,c),g($e.$$.fragment,c),g(Be.$$.fragment,c),g(Qe.$$.fragment,c),g(ds.$$.fragment,c),g(Hs.$$.fragment,c),g(Ns.$$.fragment,c),g(_s.$$.fragment,c),g(it.$$.fragment,c),g(dt.$$.fragment,c),g(Ls.$$.fragment,c),g(ct.$$.fragment,c),Vs=!0)},o(c){b(M.$$.fragment,c),b(Ne.$$.fragment,c),b($e.$$.fragment,c),b(Be.$$.fragment,c),b(Qe.$$.fragment,c),b(ds.$$.fragment,c),b(Hs.$$.fragment,c),b(Ns.$$.fragment,c),b(_s.$$.fragment,c),b(it.$$.fragment,c),b(dt.$$.fragment,c),b(Ls.$$.fragment,c),b(ct.$$.fragment,c),Vs=!1},d(c){c&&t(i),c&&t(q),c&&t(k),j(M),c&&t(K),c&&t(v),c&&t(Q),c&&t(se),c&&t(je),j(Ne,c),c&&t(Je),c&&t(X),c&&t(ze),j($e,c),c&&t(Xe),c&&t(le),c&&t(Oe),j(Be,c),c&&t(T),c&&t(Y),c&&t(Fe),j(Qe,c),c&&t(He),c&&t(De),c&&t(yt),j(ds,c),c&&t(xe),c&&t(Ie),c&&t(ms),j(Hs,c),c&&t(As),c&&t(rt),c&&t(Ts),j(Ns,c),c&&t(Is),c&&t(bs),c&&t(aa),j(_s,c),c&&t(zt),j(it,c),c&&t(na),c&&t(fs),c&&t(oa),j(dt,c),c&&t(Us),c&&t(Gs),c&&t(ra),j(Ls,c),c&&t(ia),j(ct,c),c&&t(Mt),c&&t(Ct)}}}function $g(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),A=a(" Pour quantifier les avantages de l\u2019adaptation au domaine, "),q=l("i"),k=a("finetunez"),z=a(" un classifieur sur le jeu de donn\xE9es IMDb pour \xE0 la fois, le "),f=l("i"),M=a("checkpoint"),O=a(" de DistilBERT pr\xE9-entra\xEEn\xE9 et e "),C=l("i"),L=a("checkpoint"),I=a(" de DistilBERT "),P=l("i"),D=a("finetun\xE9"),U=a(". Si vous avez besoin d\u2019un rafra\xEEchissement sur la classification de texte, consultez le "),K=l("a"),v=a("chapitre 3"),N=a("."),this.h()},l(W){i=o(W,"P",{});var G=r(i);h=n(G,"\u270F\uFE0F "),d=o(G,"STRONG",{});var J=r(d);$=n(J,"Essayez !"),J.forEach(t),A=n(G," Pour quantifier les avantages de l\u2019adaptation au domaine, "),q=o(G,"I",{});var ee=r(q);k=n(ee,"finetunez"),ee.forEach(t),z=n(G," un classifieur sur le jeu de donn\xE9es IMDb pour \xE0 la fois, le "),f=o(G,"I",{});var re=r(f);M=n(re,"checkpoint"),re.forEach(t),O=n(G," de DistilBERT pr\xE9-entra\xEEn\xE9 et e "),C=o(G,"I",{});var te=r(C);L=n(te,"checkpoint"),te.forEach(t),I=n(G," de DistilBERT "),P=o(G,"I",{});var V=r(P);D=n(V,"finetun\xE9"),V.forEach(t),U=n(G,". Si vous avez besoin d\u2019un rafra\xEEchissement sur la classification de texte, consultez le "),K=o(G,"A",{href:!0});var R=r(K);v=n(R,"chapitre 3"),R.forEach(t),N=n(G,"."),G.forEach(t),this.h()},h(){y(K,"href","/course/fr/chapter3")},m(W,G){p(W,i,G),s(i,h),s(i,d),s(d,$),s(i,A),s(i,q),s(q,k),s(i,z),s(i,f),s(f,M),s(i,O),s(i,C),s(C,L),s(i,I),s(i,P),s(P,D),s(i,U),s(i,K),s(K,v),s(i,N)},d(W){W&&t(i)}}}function wg(H){let i,h,d,$,A,q,k,z,f,M,O,C,L,I,P,D,U,K,v,N,W,G,J,ee,re,te,V,R,ie,Q,se,ue,Z,ge,ne,de,rs,be,ke,is,Ve,qe,us,We,je,Ne,Je,X,pe,ye,ce,ps,ze,$e,Xe,le,Ze,we,Ee,B,ae,Re,me,es,Ye,Me,Oe,Be,T,Y,Bs,oe,Ce,Ps,Fe,Qe,He,De,fe,Fs,nt,Pe,yt,ds,xe,Ie,lt,cs,gs,ot,ms,Hs,As,rt,Ss,Ts,Ns,Is,bs,Gt,Vt,ta,ks,Wt,Jt,La,aa,_s,zt,it,na,fs,ut,Os,Qn,la,ve,Ka,pt,oa,dt,Us,Gs,Ra,ra,Ls,ia,ct,Mt,Ct,Ba,Vs,c,F,Xn,Yt,qs,$s,mt,_t,Zn,ws,el,sl,Fa,Ue,Ha,he,tl,ua,pa,al,Es,nl,ll,Dt,Pt,ol,da,At,St,rl,il,Ws,ul,pl,Js,dl,cl,ca,ml,np,Zr,Ia,ei,ma,lp,eo,op,rp,si,ft,ht,_l,Ua,ti,fl,ip,ai,Qt,_a,so,Ga,up,to,pp,ni,ss,dp,Va,ao,cp,mp,no,_p,fp,lo,hp,vp,oo,gp,bp,ro,kp,qp,li,Wa,oi,Ja,ri,ts,$p,io,wp,Ep,uo,xp,jp,po,yp,zp,co,Mp,Cp,mo,Dp,Pp,ii,Ya,ui,Qa,pi,Tt,Ap,_o,Sp,Tp,fo,Np,Op,di,fa,ci,ha,Lp,hl,Kp,Rp,mi,Xt,va,ho,Xa,Bp,vo,Fp,_i,Za,fi,ga,Hp,go,Ip,Up,hi,Ae,Gp,bo,Vp,Wp,ko,Jp,Yp,qo,Qp,Xp,$o,Zp,ed,vl,sd,td,wo,ad,nd,Eo,ld,od,vi,en,gi,sn,bi,Ys,rd,xo,id,ud,jo,pd,dd,yo,cd,md,ki,Qs,_d,zo,fd,hd,Mo,vd,gd,Co,bd,kd,qi,tn,$i,an,wi,Xs,qd,Do,$d,wd,Po,Ed,xd,Ao,jd,yd,Ei,ba,xi,gl,zd,ji,nn,yi,ka,zi,qa,Md,So,Cd,Dd,Mi,ln,Ci,on,Di,bl,Pd,Pi,rn,Ai,un,Si,xs,Ad,To,Sd,Td,No,Nd,Od,Oo,Ld,Kd,Lo,Rd,Bd,Ti,pn,Ni,dn,Oi,$a,Fd,Ko,Hd,Id,Li,wa,cn,Ud,Ro,Gd,Vd,Wd,mn,Jd,Bo,Yd,Qd,Ki,kl,Xd,Ri,_n,Bi,as,Zd,Fo,ec,sc,Ho,tc,ac,Io,nc,lc,Uo,oc,rc,Go,ic,uc,Fi,Nt,pc,Vo,dc,cc,Wo,mc,_c,Hi,fn,Ii,hn,Ui,Se,fc,Jo,hc,vc,Yo,gc,bc,Qo,kc,qc,Xo,$c,wc,Zo,Ec,xc,er,jc,yc,sr,zc,Mc,Gi,vn,Vi,gn,Wi,ql,Cc,Ji,bn,Yi,kn,Qi,js,Dc,tr,Pc,Ac,ar,Sc,Tc,nr,Nc,Oc,lr,Lc,Kc,Xi,Zt,Ea,or,qn,Rc,$n,rr,Bc,Fc,ir,Hc,Zi,_e,Ic,ur,Uc,Gc,pr,Vc,Wc,$l,Jc,Yc,dr,Qc,Xc,cr,Zc,em,mr,sm,tm,_r,am,nm,fr,lm,om,hr,rm,im,eu,wn,su,Zs,um,vr,pm,dm,gr,cm,mm,br,_m,fm,tu,En,au,xn,nu,ys,hm,kr,vm,gm,qr,bm,km,$r,qm,$m,wr,wm,Em,lu,xa,ou,wl,zs,xm,Er,jm,ym,xr,zm,Mm,jr,Cm,Dm,yr,Pm,Am,ru,vt,gt,El,xl,Sm,iu,jn,uu,yn,pu,ja,du,Ms,Tm,zr,Nm,Om,Mr,Lm,Km,Cr,Rm,Bm,jl,Fm,Hm,cu,zn,mu,Mn,_u,Cs,Im,Dr,Um,Gm,Pr,Vm,Wm,Ar,Jm,Ym,Sr,Qm,Xm,fu,Cn,hu,ya,Zm,Tr,e_,s_,vu,Dn,gu,yl,t_,bu,bt,kt,zl,ea,za,Nr,Pn,a_,Or,n_,ku,An,qu,Ml,l_,$u,qt,$t,Cl,Sn,wu,Dl,o_,Eu,wt,Et,Pl,Al,r_,xu,xt,jt,Sl,Tn,ju,Tl,i_,yu,Nl,Ma,zu,Ol,sa,Ca,Lr,Nn,u_,Ll,p_,Kr,d_,Mu,Ge,c_,Rr,m_,__,Br,f_,h_,Fr,v_,g_,Hr,b_,k_,Ir,q_,$_,Ur,w_,E_,Cu,On,Du,Kl,x_,Pu,Ln,Au,Kn,Su,Rl,j_,Tu,Rn,Nu,Ot,y_,Bl,z_,M_,Gr,C_,D_,Ou,Da,Lu;d=new Xv({props:{fw:H[0]}}),z=new Oa({});const A_=[eg,Zv],Bn=[];function S_(e,u){return e[0]==="pt"?0:1}P=S_(H),D=Bn[P]=A_[P](H),ms=new ap({props:{id:"mqElG5QJWUg"}}),As=new Na({props:{$$slots:{default:[sg]},$$scope:{ctx:H}}}),Is=new Oa({});const T_=[ag,tg],Fn=[];function N_(e,u){return e[0]==="pt"?0:1}qs=N_(H),$s=Fn[qs]=T_[qs](H),Ue=new S({props:{code:'text = "This is a great [MASK]."',highlighted:'text = <span class="hljs-string">&quot;This is a great [MASK].&quot;</span>'}}),Ia=new S({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}});const O_=[lg,ng],Hn=[];function L_(e,u){return e[0]==="pt"?0:1}ft=L_(H),ht=Hn[ft]=O_[ft](H),Ua=new S({props:{code:`'>>> This is a great deal.' # C'est une bonne affaire
'>>> This is a great success.' # C'est un grand succ\xE8s
'>>> This is a great adventure.' # C'est une grande aventure
'>>> This is a great idea.' # C'est une bonne id\xE9e
'>>> This is a great feat.' # C'est un grand exploit`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great deal.&#x27;</span> <span class="hljs-comment"># C&#x27;est une bonne affaire</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great success.&#x27;</span> <span class="hljs-comment"># C&#x27;est un grand succ\xE8s</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great adventure.&#x27;</span> <span class="hljs-comment"># C&#x27;est une grande aventure</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great idea.&#x27;</span> <span class="hljs-comment"># C&#x27;est une bonne id\xE9e</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great feat.&#x27;</span> <span class="hljs-comment"># C&#x27;est un grand exploit</span>`}}),Ga=new Oa({}),Wa=new S({props:{code:`from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

imdb_dataset = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)
imdb_dataset`}}),Ja=new S({props:{code:`DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),Ya=new S({props:{code:`sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")`,highlighted:`sample = imdb_dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>))

<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> sample:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;text&#x27;</span>]}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Label: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>&#x27;&quot;</span>)`}}),Qa=new S({props:{code:`
'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'`,highlighted:`
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, &quot;kodokoo&quot; in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\&#x27;t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.&lt;br /&gt;&lt;br /&gt;Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam&quot; Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.&lt;br /&gt;&lt;br /&gt;The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. &lt;br /&gt;&lt;br /&gt;My 4 and 6 year old children love this movie and have been asking again and again to watch it. &lt;br /&gt;&lt;br /&gt;I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.&lt;br /&gt;&lt;br /&gt;I do not have older children so I do not know what they would think of it. &lt;br /&gt;&lt;br /&gt;The songs are very cute. My daughter keeps singing them over and over.&lt;br /&gt;&lt;br /&gt;Hope this helps.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 1&#x27;</span>`}}),fa=new Na({props:{$$slots:{default:[og]},$$scope:{ctx:H}}}),Xa=new Oa({}),Za=new ap({props:{id:"8PmhEIXhBvI"}}),en=new S({props:{code:`def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# Utilisation de batched=True pour activer le multithreading rapide !
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
    result = tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>])
    <span class="hljs-keyword">if</span> tokenizer.is_fast:
        result[<span class="hljs-string">&quot;word_ids&quot;</span>] = [result.word_ids(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(result[<span class="hljs-string">&quot;input_ids&quot;</span>]))]
    <span class="hljs-keyword">return</span> result


<span class="hljs-comment"># Utilisation de batched=True pour activer le multithreading rapide !</span>
tokenized_datasets = imdb_dataset.<span class="hljs-built_in">map</span>(
    tokenize_function, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>]
)
tokenized_datasets`}}),sn=new S({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),tn=new S({props:{code:"tokenizer.model_max_length",highlighted:"tokenizer.model_max_length"}}),an=new S({props:{code:"512",highlighted:'<span class="hljs-number">512</span>'}}),ba=new Na({props:{$$slots:{default:[rg]},$$scope:{ctx:H}}}),nn=new S({props:{code:"chunk_size = 128",highlighted:'chunk_size = <span class="hljs-number">128</span>'}}),ka=new Na({props:{warning:!0,$$slots:{default:[ig]},$$scope:{ctx:H}}}),ln=new S({props:{code:`# Le d\xE9coupage produit une liste de listes pour chaque caract\xE9ristique
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")`,highlighted:`<span class="hljs-comment"># Le d\xE9coupage produit une liste de listes pour chaque caract\xE9ristique</span>
tokenized_samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">3</span>]

<span class="hljs-keyword">for</span> idx, sample <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokenized_samples[<span class="hljs-string">&quot;input_ids&quot;</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review <span class="hljs-subst">{idx}</span> length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(sample)}</span>&#x27;&quot;</span>)`}}),on=new S({props:{code:`'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 0 length: 200&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 1 length: 559&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 2 length: 192&#x27;</span>`}}),rn=new S({props:{code:`concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Longueur des critiques concat\xE9n\xE9es : {total_length}'")`,highlighted:`concatenated_examples = {
    k: <span class="hljs-built_in">sum</span>(tokenized_samples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> tokenized_samples.keys()
}
total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Longueur des critiques concat\xE9n\xE9es : <span class="hljs-subst">{total_length}</span>&#x27;&quot;</span>)`}}),un=new S({props:{code:"'>>> Longueur des critiques concat\xE9n\xE9es : 951'",highlighted:'<span class="hljs-string">&#x27;&gt;&gt;&gt; Longueur des critiques concat\xE9n\xE9es : 951&#x27;</span>'}}),pn=new S({props:{code:`chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")`,highlighted:`chunks = {
    k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
    <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
}

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Chunk length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(chunk)}</span>&#x27;&quot;</span>)`}}),dn=new S({props:{code:`'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 55&#x27;</span>`}}),_n=new S({props:{code:`def group_texts(examples):
    # Concat\xE9nation de tous les textes
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Calcule la longueur des textes concat\xE9n\xE9s
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # Nous laissons tomber le dernier morceau s'il est plus petit que chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Fractionnement par chunk de max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Cr\xE9er une nouvelle colonne d'\xE9tiquettes
    result["labels"] = result["input_ids"].copy()
    return result`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-comment"># Concat\xE9nation de tous les textes</span>
    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
    <span class="hljs-comment"># Calcule la longueur des textes concat\xE9n\xE9s</span>
    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
    <span class="hljs-comment"># Nous laissons tomber le dernier morceau s&#x27;il est plus petit que chunk_size</span>
    total_length = (total_length // chunk_size) * chunk_size
    <span class="hljs-comment"># Fractionnement par chunk de max_len</span>
    result = {
        k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
    }
    <span class="hljs-comment"># Cr\xE9er une nouvelle colonne d&#x27;\xE9tiquettes</span>
    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
    <span class="hljs-keyword">return</span> result`}}),fn=new S({props:{code:`lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets`,highlighted:`lm_datasets = tokenized_datasets.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>)
lm_datasets`}}),hn=new S({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">61289</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">59905</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">122963</span>
    })
})`}}),vn=new S({props:{code:'tokenizer.decode(lm_datasets["train"][1]["input_ids"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),gn=new S({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),bn=new S({props:{code:'tokenizer.decode(lm_datasets["train"][1]["labels"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;labels&quot;</span>])'}}),kn=new S({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),qn=new Oa({}),wn=new S({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`}}),En=new S({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
<span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> samples:
    _ = sample.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> data_collator(samples)[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),xn=new S({props:{code:`'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as &quot; teachers &quot;. [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\&#x27;[MASK] satire is much closer to reality than is &quot; teachers &quot;. the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\&#x27;[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless&#x27;</span>`}}),xa=new Na({props:{$$slots:{default:[ug]},$$scope:{ctx:H}}});let hs=H[0]==="pt"&&Hv();const K_=[dg,pg],In=[];function R_(e,u){return e[0]==="pt"?0:1}vt=R_(H),gt=In[vt]=K_[vt](H),jn=new S({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
batch = whole_word_masking_data_collator(samples)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> batch[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),yn=new S({props:{code:`'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as &quot; teachers &quot;. my 35 years in the teaching profession lead me to believe that bromwell high\\&#x27;s satire is much closer to reality than is &quot; teachers &quot;. the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&#x27;</span>`}}),ja=new Na({props:{$$slots:{default:[cg]},$$scope:{ctx:H}}}),zn=new S({props:{code:`train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset`,highlighted:`train_size = <span class="hljs-number">10_000</span>
test_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.1</span> * train_size)

downsampled_dataset = lm_datasets[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(
    train_size=train_size, test_size=test_size, seed=<span class="hljs-number">42</span>
)
downsampled_dataset`}}),Mn=new S({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">10000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1000</span>
    })
})`}}),Cn=new S({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),Dn=new S({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});const B_=[_g,mg],Un=[];function F_(e,u){return e[0]==="tf"?0:1}bt=F_(H),kt=Un[bt]=B_[bt](H),Pn=new Oa({}),An=new ap({props:{id:"NURcDHhYe98"}});const H_=[hg,fg],Gn=[];function I_(e,u){return e[0]==="pt"?0:1}qt=I_(H),$t=Gn[qt]=H_[qt](H),Sn=new S({props:{code:"Perplexit\xE9 : 21.75",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexit\xE9 : <span class="hljs-number">21.75</span>'}});const U_=[gg,vg],Vn=[];function G_(e,u){return e[0]==="pt"?0:1}wt=G_(H),Et=Vn[wt]=U_[wt](H);const V_=[kg,bg],Wn=[];function W_(e,u){return e[0]==="pt"?0:1}xt=W_(H),jt=Wn[xt]=V_[xt](H),Tn=new S({props:{code:"Perplexit\xE9 : 11.32",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexit\xE9 : <span class="hljs-number">11.32</span>'}});let Le=H[0]==="pt"&&Iv();Ma=new Na({props:{$$slots:{default:[qg]},$$scope:{ctx:H}}});let Ke=H[0]==="pt"&&Uv();return Nn=new Oa({}),On=new S({props:{code:`from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

mask_filler = pipeline(
    <span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/distilbert-base-uncased-finetuned-imdb&quot;</span>
)`}}),Ln=new S({props:{code:`preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")`,highlighted:`preds = mask_filler(text)

<span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{pred[<span class="hljs-string">&#x27;sequence&#x27;</span>]}</span>&quot;</span>)`}}),Kn=new S({props:{code:`'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movie.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great film.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great story.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movies.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great character.&#x27;</span>`}}),Rn=new ap({props:{id:"0Oxphw4Q9fo"}}),Da=new Na({props:{$$slots:{default:[$g]},$$scope:{ctx:H}}}),{c(){i=l("meta"),h=m(),w(d.$$.fragment),$=m(),A=l("h1"),q=l("a"),k=l("span"),w(z.$$.fragment),f=m(),M=l("span"),O=l("i"),C=a("Finetuner"),L=a(" un mod\xE8le de langage masqu\xE9"),I=m(),D.c(),U=m(),K=l("p"),v=a("Pour de nombreuses applications de NLP impliquant des "),N=l("em"),W=a("transformers"),G=a(", vous pouvez simplement prendre un mod\xE8le pr\xE9-entra\xEEn\xE9 du "),J=l("em"),ee=a("Hub"),re=a(" et le "),te=l("em"),V=a("finetuner"),R=a(" directement sur vos donn\xE9es pour la t\xE2che \xE0 accomplir. Pour autant que le corpus utilis\xE9 pour le pr\xE9-entra\xEEnement ne soit pas trop diff\xE9rent du corpus utilis\xE9 pour le "),ie=l("em"),Q=a("finetuning"),se=a(". L\u2019apprentissage par transfert produira g\xE9n\xE9ralement de bons r\xE9sultats."),ue=m(),Z=l("p"),ge=a("Cependant, il existe quelques cas o\xF9 vous voudrez d\u2019abord "),ne=l("em"),de=a("finetuner"),rs=a(" les mod\xE8les de langue sur vos donn\xE9es, avant d\u2019entra\xEEner une t\xEAte sp\xE9cifique \xE0 la t\xE2che. Par exemple, si votre jeu de donn\xE9es contient des contrats l\xE9gaux ou des articles scientifiques, un "),be=l("em"),ke=a("transformer"),is=a(" classique comme BERT traitera g\xE9n\xE9ralement les mots sp\xE9cifiques au domaine dans votre corpus comme des "),Ve=l("em"),qe=a("tokens"),us=a(" rares et les performances r\xE9sultantes peuvent \xEAtre moins que satisfaisantes. En "),We=l("em"),je=a("finetunant"),Ne=a(" le mod\xE8le de langage sur les donn\xE9es du domaine, vous pouvez am\xE9liorer les performances de nombreuses t\xE2ches en aval, ce qui signifie que vous ne devez g\xE9n\xE9ralement effectuer cette \xE9tape qu\u2019une seule fois !"),Je=m(),X=l("p"),pe=a("Ce processus de "),ye=l("em"),ce=a("finetuning"),ps=a(" d\u2019un mod\xE8le de langage pr\xE9-entra\xEEn\xE9 sur des donn\xE9es "),ze=l("em"),$e=a("dans le domaine"),Xe=a(" est g\xE9n\xE9ralement appel\xE9 "),le=l("em"),Ze=a("adaptation au domaine"),we=a(". Il a \xE9t\xE9 popularis\xE9 en 2018 par "),Ee=l("a"),B=a("ULMFiT"),ae=a(" qui a \xE9t\xE9 l\u2019une des premi\xE8res architectures neuronales (bas\xE9es sur des LSTMs) \xE0 faire en sorte que l\u2019apprentissage par transfert fonctionne r\xE9ellement pour le NLP. Un exemple d\u2019adaptation de domaine avec ULMFiT est pr\xE9sent\xE9 dans l\u2019image ci-dessous. Dans cette section, nous ferons quelque chose de similaire mais avec un "),Re=l("em"),me=a("transformer"),es=a(" au lieu d\u2019une LSTM !"),Ye=m(),Me=l("div"),Oe=l("img"),T=m(),Y=l("img"),oe=m(),Ce=l("p"),Ps=a("\xC0 la fin de cette section, vous aurez un "),Fe=l("a"),Qe=a("mod\xE8le de langage masqu\xE9"),He=a(" sur le "),De=l("em"),fe=a("Hub"),Fs=a(" qui peut autocompl\xE9ter des phrases comme indiqu\xE9 ci-dessous :"),nt=m(),Pe=l("iframe"),ds=m(),xe=l("iframe"),lt=m(),cs=l("p"),gs=a("Allons-y !"),ot=m(),w(ms.$$.fragment),Hs=m(),w(As.$$.fragment),rt=m(),Ss=l("h2"),Ts=l("a"),Ns=l("span"),w(Is.$$.fragment),bs=m(),Gt=l("span"),Vt=a("Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"),ta=m(),ks=l("p"),Wt=a("Pour commencer, nous allons choisir un mod\xE8le pr\xE9-entra\xEEn\xE9 appropri\xE9 pour la mod\xE9lisation du langage masqu\xE9. Comme le montre la capture d\u2019\xE9cran suivante, vous pouvez trouver une liste de candidats en appliquant le filtre \xAB "),Jt=l("em"),La=a("Fill-Mask"),aa=a(" \xBB sur le "),_s=l("a"),zt=l("em"),it=a("Hub"),na=a(" :"),fs=m(),ut=l("div"),Os=l("img"),la=m(),ve=l("p"),Ka=a("Bien que les mod\xE8les de la famille BERT et RoBERTa soient les plus t\xE9l\xE9charg\xE9s, nous utiliserons un mod\xE8le appel\xE9 "),pt=l("a"),oa=a("DistilBERT"),dt=a(" qui peut \xEAtre entra\xEEn\xE9 beaucoup plus rapidement avec peu ou pas de perte de performance en aval. Ce mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide d\u2019une technique sp\xE9ciale appel\xE9e "),Us=l("a"),Gs=l("em"),Ra=a("distillation de connaissances"),ra=a(", o\xF9 un grand mod\xE8le "),Ls=l("em"),ia=a("enseignant"),ct=a(" comme BERT est utilis\xE9 pour guider l\u2019entra\xEEnement d\u2019un mod\xE8le "),Mt=l("em"),Ct=a("\xE9tudiant"),Ba=a(" qui a beaucoup moins de param\xE8tres. Une explication des d\xE9tails de la distillation de connaissances nous m\xE8nerait trop loin dans cette section mais si vous \xEAtes int\xE9ress\xE9, vous pouvez lire tout cela dans le livre "),Vs=l("a"),c=l("em"),F=a("Natural Language Processing with Transformers"),Xn=a("."),Yt=m(),$s.c(),mt=m(),_t=l("p"),Zn=a("Avec environ 67 millions de param\xE8tres, DistilBERT est environ deux fois plus petit que le mod\xE8le de base de BERT, ce qui se traduit approximativement par une acc\xE9l\xE9ration de l\u2019entra\xEEnement d\u2019un facteur deux. Voyons maintenant quels types de "),ws=l("em"),el=a("tokens"),sl=a(" ce mod\xE8le pr\xE9dit comme \xE9tant les compl\xE9ments les plus probables d\u2019un petit \xE9chantillon de texte :"),Fa=m(),w(Ue.$$.fragment),Ha=m(),he=l("p"),tl=a("En tant qu\u2019\xEAtres humains, nous pouvons imaginer de nombreuses possibilit\xE9s pour le "),ua=l("em"),pa=a("token"),al=m(),Es=l("code"),nl=a("[MASK]"),ll=a(", telles que \xAB jour \xBB, \xAB promenade \xBB ou \xAB peinture \xBB. Pour les mod\xE8les pr\xE9-entra\xEEn\xE9s, les pr\xE9dictions d\xE9pendent du corpus sur lequel le mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 puisqu\u2019il apprend \xE0 d\xE9tecter les mod\xE8les statistiques pr\xE9sents dans les donn\xE9es. Comme BERT, DistilBERT a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur les jeux de donn\xE9es "),Dt=l("a"),Pt=l("em"),ol=a("English Wikipedia"),da=a(" et "),At=l("a"),St=l("em"),rl=a("BookCorpus"),il=a(", nous nous attendons donc \xE0 ce que les pr\xE9dictions pour "),Ws=l("code"),ul=a("[MASK]"),pl=a(" refl\xE8tent ces domaines. Pour pr\xE9dire le masque, nous avons besoin du "),Js=l("em"),dl=a("tokenizer"),cl=a(" de DistilBERT pour produire les entr\xE9es du mod\xE8le, alors t\xE9l\xE9chargeons-le \xE9galement depuis le "),ca=l("em"),ml=a("Hub"),np=a(" :"),Zr=m(),w(Ia.$$.fragment),ei=m(),ma=l("p"),lp=a("Avec un "),eo=l("em"),op=a("tokenizer"),rp=a(" et un mod\xE8le, nous pouvons maintenant passer notre exemple de texte au mod\xE8le, extraire les logits, et afficher les 5 meilleurs candidats :"),si=m(),ht.c(),_l=m(),w(Ua.$$.fragment),ti=m(),fl=l("p"),ip=a("Nous pouvons voir dans les sorties que les pr\xE9dictions du mod\xE8le se r\xE9f\xE8rent \xE0 des termes de tous les jours, ce qui n\u2019est peut-\xEAtre pas surprenant \xE9tant donn\xE9 le fondement de Wikip\xE9dia. Voyons comment nous pouvons changer ce domaine pour quelque chose d\u2019un peu plus sp\xE9cialis\xE9 : des critiques de films !"),ai=m(),Qt=l("h2"),_a=l("a"),so=l("span"),w(Ga.$$.fragment),up=m(),to=l("span"),pp=a("Le jeu de donn\xE9es"),ni=m(),ss=l("p"),dp=a("Pour illustrer l\u2019adaptation au domaine, nous utiliserons le c\xE9l\xE8bre "),Va=l("a"),ao=l("em"),cp=a("Large Movie Review Dataset"),mp=a(" (ou IMDb en abr\xE9g\xE9), qui est un corpus de critiques de films souvent utilis\xE9 pour \xE9valuer les mod\xE8les d\u2019analyse de sentiments. En "),no=l("em"),_p=a("finetunant"),fp=a(" DistilBERT sur ce corpus, nous esp\xE9rons que le mod\xE8le de langage adaptera son vocabulaire des donn\xE9es factuelles de Wikip\xE9dia sur lesquelles il a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 aux \xE9l\xE9ments plus subjectifs des critiques de films. Nous pouvons obtenir les donn\xE9es du "),lo=l("em"),hp=a("Hub"),vp=a(" avec la fonction "),oo=l("code"),gp=a("load_dataset()"),bp=a(" de \u{1F917} "),ro=l("em"),kp=a("Datasets"),qp=a(" :"),li=m(),w(Wa.$$.fragment),oi=m(),w(Ja.$$.fragment),ri=m(),ts=l("p"),$p=a("Nous pouvons voir que les parties "),io=l("code"),wp=a("train"),Ep=a(" et "),uo=l("code"),xp=a("test"),jp=a(" sont chacune compos\xE9es de 25 000 critiques, alors qu\u2019il y a une partie non \xE9tiquet\xE9e appel\xE9e "),po=l("code"),yp=a("unsupervised"),zp=a(" qui contient 50 000 critiques. Jetons un coup d\u2019\u0153il \xE0 quelques \xE9chantillons pour avoir une id\xE9e du type de texte auquel nous avons affaire. Comme nous l\u2019avons fait dans les chapitres pr\xE9c\xE9dents du cours, nous allons encha\xEEner les fonctions "),co=l("code"),Mp=a("Dataset.shuffle()"),Cp=a(" et "),mo=l("code"),Dp=a("Dataset.select()"),Pp=a(" pour cr\xE9er un \xE9chantillon al\xE9atoire :"),ii=m(),w(Ya.$$.fragment),ui=m(),w(Qa.$$.fragment),pi=m(),Tt=l("p"),Ap=a("Oui, ce sont bien des critiques de films, et si vous \xEAtes assez \xE2g\xE9s, vous pouvez m\xEAme comprendre le commentaire dans la derni\xE8re critique sur le fait de poss\xE9der une version VHS \u{1F61C} ! Bien que nous n\u2019ayons pas besoin des \xE9tiquettes pour la mod\xE9lisation du langage, nous pouvons d\xE9j\xE0 voir qu\u2019un "),_o=l("code"),Sp=a("0"),Tp=a(" d\xE9note une critique n\xE9gative, tandis qu\u2019un "),fo=l("code"),Np=a("1"),Op=a(" correspond \xE0 une critique positive."),di=m(),w(fa.$$.fragment),ci=m(),ha=l("p"),Lp=a("Maintenant que nous avons jet\xE9 un coup d\u2019\u0153il rapide aux donn\xE9es, plongeons dans leur pr\xE9paration pour la mod\xE9lisation du langage masqu\xE9. Comme nous allons le voir, il y a quelques \xE9tapes suppl\xE9mentaires \xE0 suivre par rapport aux t\xE2ches de classification de s\xE9quences que nous avons vues au "),hl=l("a"),Kp=a("chapitre 3"),Rp=a(". Allons-y !"),mi=m(),Xt=l("h2"),va=l("a"),ho=l("span"),w(Xa.$$.fragment),Bp=m(),vo=l("span"),Fp=a("Pr\xE9traitement des donn\xE9es"),_i=m(),w(Za.$$.fragment),fi=m(),ga=l("p"),Hp=a("Pour la mod\xE9lisation autor\xE9gressive et la mod\xE9lisation du langage masqu\xE9, une \xE9tape commune de pr\xE9traitement consiste \xE0 concat\xE9ner tous les exemples, puis \xE0 diviser le corpus entier en morceaux de taille \xE9gale. C\u2019est tr\xE8s diff\xE9rent de notre approche habituelle, o\xF9 nous nous contentons de "),go=l("em"),Ip=a("tokenizer"),Up=a(" les exemples individuels. Pourquoi tout concat\xE9ner ? La raison est que les exemples individuels peuvent \xEAtre tronqu\xE9s s\u2019ils sont trop longs, ce qui entra\xEEnerait la perte d\u2019informations qui pourraient \xEAtre utiles pour la t\xE2che de mod\xE9lisation du langage !"),hi=m(),Ae=l("p"),Gp=a("Donc pour commencer, nous allons d\u2019abord tokeniser notre corpus comme d\u2019habitude, mais "),bo=l("em"),Vp=a("sans"),Wp=a(" mettre l\u2019option "),ko=l("code"),Jp=a("truncation=True"),Yp=a(" dans notre "),qo=l("em"),Qp=a("tokenizer"),Xp=a(". Nous allons aussi r\xE9cup\xE9rer les identifiants des mots s\u2019ils sont disponibles (ce qui sera le cas si nous utilisons un "),$o=l("em"),Zp=a("tokenizer"),ed=a(" rapide, comme d\xE9crit dans le "),vl=l("a"),sd=a("chapitre 6"),td=a("), car nous en aurons besoin plus tard pour faire le masquage de mots entiers. Nous allons envelopper cela dans une simple fonction, et pendant que nous y sommes, nous allons supprimer les colonnes "),wo=l("code"),ad=a("text"),nd=a(" et "),Eo=l("code"),ld=a("label"),od=a(" puisque nous n\u2019en avons plus besoin :"),vi=m(),w(en.$$.fragment),gi=m(),w(sn.$$.fragment),bi=m(),Ys=l("p"),rd=a("Comme DistilBERT est un mod\xE8le de type BERT, nous pouvons voir que les textes encod\xE9s sont constitu\xE9s des "),xo=l("code"),id=a("input_ids"),ud=a(" et des "),jo=l("code"),pd=a("attention_mask"),dd=a(" que nous avons vus dans d\u2019autres chapitres, ainsi que des "),yo=l("code"),cd=a("word_ids"),md=a(" que nous avons ajout\xE9s."),ki=m(),Qs=l("p"),_d=a("Maintenant que nos critiques de films ont \xE9t\xE9 tokenis\xE9es, l\u2019\xE9tape suivante consiste \xE0 les regrouper et \xE0 diviser le r\xE9sultat en chunks. Mais quelle taille doivent avoir ces "),zo=l("em"),fd=a("chunks"),hd=a(" ? Cela sera finalement d\xE9termin\xE9 par la quantit\xE9 de m\xE9moire GPU dont vous disposez, mais un bon point de d\xE9part est de voir quelle est la taille maximale du contexte du mod\xE8le. Cela peut \xEAtre d\xE9duit en inspectant l\u2019attribut "),Mo=l("code"),vd=a("model_max_length"),gd=a(" du "),Co=l("em"),bd=a("tokenizer"),kd=a(" :"),qi=m(),w(tn.$$.fragment),$i=m(),w(an.$$.fragment),wi=m(),Xs=l("p"),qd=a("Cette valeur est d\xE9riv\xE9e du fichier "),Do=l("em"),$d=a("tokenizer_config.json"),wd=a(" associ\xE9 \xE0 un "),Po=l("em"),Ed=a("checkpoint"),xd=a(". Dans ce cas, nous pouvons voir que la taille du contexte est de 512 "),Ao=l("em"),jd=a("tokens"),yd=a(", tout comme avec BERT."),Ei=m(),w(ba.$$.fragment),xi=m(),gl=l("p"),zd=a("Ainsi, pour r\xE9aliser nos exp\xE9riences sur des GPUs comme ceux disponibles sur Google Colab, nous choisirons quelque chose d\u2019un peu plus petit qui peut tenir en m\xE9moire :"),ji=m(),w(nn.$$.fragment),yi=m(),w(ka.$$.fragment),zi=m(),qa=l("p"),Md=a("Maintenant vient la partie amusante. Pour montrer comment la concat\xE9nation fonctionne, prenons quelques commentaires de notre ensemble d\u2019entra\xEEnement et affichons le nombre de "),So=l("em"),Cd=a("tokens"),Dd=a(" par commentaire :"),Mi=m(),w(ln.$$.fragment),Ci=m(),w(on.$$.fragment),Di=m(),bl=l("p"),Pd=a("Nous pouvons ensuite concat\xE9ner tous ces exemples avec une simple compr\xE9hension du dictionnaire, comme suit :"),Pi=m(),w(rn.$$.fragment),Ai=m(),w(un.$$.fragment),Si=m(),xs=l("p"),Ad=a("Super, la longueur totale est correcte. Donc maintenant, nous allons diviser les exemples concat\xE9n\xE9s en morceaux de la taille donn\xE9e par "),To=l("code"),Sd=a("block_size"),Td=a(". Pour ce faire, nous it\xE9rons sur les caract\xE9ristiques de "),No=l("code"),Nd=a("concatenated_examples"),Od=a(" et utilisons une compr\xE9hension de liste pour cr\xE9er des "),Oo=l("em"),Ld=a("chunks"),Kd=a(" de chaque caract\xE9ristique. Le r\xE9sultat est un dictionnaire de "),Lo=l("em"),Rd=a("chunks"),Bd=a(" pour chaque caract\xE9ristique :"),Ti=m(),w(pn.$$.fragment),Ni=m(),w(dn.$$.fragment),Oi=m(),$a=l("p"),Fd=a("Comme vous pouvez le voir dans cet exemple, le dernier "),Ko=l("em"),Hd=a("chunk"),Id=a(" sera g\xE9n\xE9ralement plus petit que la taille maximale des morceaux. Il y a deux strat\xE9gies principales pour g\xE9rer cela :"),Li=m(),wa=l("ul"),cn=l("li"),Ud=a("Abandonner le dernier morceau s\u2019il est plus petit que "),Ro=l("code"),Gd=a("chunk_size"),Vd=a("."),Wd=m(),mn=l("li"),Jd=a("Rembourrer le dernier morceau jusqu\u2019\xE0 ce que sa longueur soit \xE9gale \xE0 "),Bo=l("code"),Yd=a("chunk_size"),Qd=a("."),Ki=m(),kl=l("p"),Xd=a("Nous adopterons la premi\xE8re approche ici, donc nous allons envelopper toute la logique ci-dessus dans une seule fonction que nous pouvons appliquer \xE0 nos jeux de donn\xE9es tokenis\xE9s :"),Ri=m(),w(_n.$$.fragment),Bi=m(),as=l("p"),Zd=a("Notez que dans la derni\xE8re \xE9tape de "),Fo=l("code"),ec=a("group_texts()"),sc=a(" nous cr\xE9ons une nouvelle colonne "),Ho=l("code"),tc=a("labels"),ac=a(" qui est une copie de la colonne "),Io=l("code"),nc=a("input_ids"),lc=a(". Comme nous le verrons bient\xF4t, c\u2019est parce que dans la mod\xE9lisation du langage masqu\xE9, l\u2019objectif est de pr\xE9dire des "),Uo=l("em"),oc=a("tokens"),rc=a(" masqu\xE9s al\xE9atoirement dans le batch d\u2019entr\xE9e, et en cr\xE9ant une colonne "),Go=l("code"),ic=a("labels"),uc=a(", nous fournissons la v\xE9rit\xE9 de base pour notre mod\xE8le de langage \xE0 apprendre."),Fi=m(),Nt=l("p"),pc=a("Appliquons maintenant "),Vo=l("code"),dc=a("group_texts()"),cc=a(" \xE0 nos jeux de donn\xE9es tokenis\xE9s en utilisant notre fid\xE8le fonction "),Wo=l("code"),mc=a("Dataset.map()"),_c=a(" :"),Hi=m(),w(fn.$$.fragment),Ii=m(),w(hn.$$.fragment),Ui=m(),Se=l("p"),fc=a("Vous pouvez voir que le regroupement puis le d\xE9coupage des textes a produit beaucoup plus d\u2019exemples que nos 25 000 exemples initiaux pour les divisions "),Jo=l("code"),hc=a("train"),vc=a(" et "),Yo=l("code"),gc=a("test"),bc=a(". C\u2019est parce que nous avons maintenant des exemples impliquant des "),Qo=l("em"),kc=a("tokens"),qc=a(" contigus qui s\u2019\xE9tendent sur plusieurs exemples du corpus original. Vous pouvez le voir explicitement en cherchant les "),Xo=l("em"),$c=a("tokens"),wc=a(" sp\xE9ciaux "),Zo=l("code"),Ec=a("[SEP]"),xc=a(" et "),er=l("code"),jc=a("[CLS]"),yc=a(" dans l\u2019un des "),sr=l("em"),zc=a("chunks"),Mc=a(" :"),Gi=m(),w(vn.$$.fragment),Vi=m(),w(gn.$$.fragment),Wi=m(),ql=l("p"),Cc=a("Dans cet exemple, vous pouvez voir deux critiques de films qui se chevauchent, l\u2019une sur un film de lyc\xE9e et l\u2019autre sur les sans-abri. Voyons \xE9galement \xE0 quoi ressemblent les \xE9tiquettes pour la mod\xE9lisation du langage masqu\xE9 :"),Ji=m(),w(bn.$$.fragment),Yi=m(),w(kn.$$.fragment),Qi=m(),js=l("p"),Dc=a("Comme pr\xE9vu par notre fonction "),tr=l("code"),Pc=a("group_texts()"),Ac=a(" ci-dessus, cela semble identique aux "),ar=l("code"),Sc=a("input_ids"),Tc=a(" d\xE9cod\xE9s. Mais alors comment notre mod\xE8le peut-il apprendre quoi que ce soit ? Il nous manque une \xE9tape cl\xE9 : ins\xE9rer des "),nr=l("em"),Nc=a("tokens"),Oc=a(" \xE0 des positions al\xE9atoires dans les entr\xE9es ! Voyons comment nous pouvons le faire \xE0 la vol\xE9e pendant le "),lr=l("em"),Lc=a("finetuning"),Kc=a(" en utilisant un collateur de donn\xE9es sp\xE9cial."),Xi=m(),Zt=l("h2"),Ea=l("a"),or=l("span"),w(qn.$$.fragment),Rc=m(),$n=l("span"),rr=l("i"),Bc=a("Finetuning"),Fc=a(" de DistilBERT avec l'API "),ir=l("code"),Hc=a("Trainer"),Zi=m(),_e=l("p"),Ic=a("Le "),ur=l("em"),Uc=a("finetuning"),Gc=a(" d\u2019un mod\xE8le de langage masqu\xE9 est presque identique au "),pr=l("em"),Vc=a("finetuning"),Wc=a(" d\u2019un mod\xE8le de classification de s\xE9quences, comme nous l\u2019avons fait dans le "),$l=l("a"),Jc=a("chapitre 3"),Yc=a(". La seule diff\xE9rence est que nous avons besoin d\u2019un collecteur de donn\xE9es sp\xE9cial qui peut masquer de mani\xE8re al\xE9atoire certains des "),dr=l("em"),Qc=a("tokens"),Xc=a(" dans chaque batch de textes. Heureusement, \u{1F917} "),cr=l("em"),Zc=a("Transformers"),em=a(" est livr\xE9 pr\xE9par\xE9 avec un "),mr=l("code"),sm=a("DataCollatorForLanguageModeling"),tm=a(" d\xE9di\xE9 \xE0 cette t\xE2che. Nous devons juste lui passer le "),_r=l("em"),am=a("tokenizer"),nm=a(" et un argument "),fr=l("code"),lm=a("mlm_probability"),om=a(" qui sp\xE9cifie quelle fraction des "),hr=l("em"),rm=a("tokens"),im=a(" \xE0 masquer. Nous choisirons 15%, qui est la quantit\xE9 utilis\xE9e pour BERT et un choix commun dans la litt\xE9rature :"),eu=m(),w(wn.$$.fragment),su=m(),Zs=l("p"),um=a("Pour voir comment le masquage al\xE9atoire fonctionne, nous allons donner quelques exemples au collateur de donn\xE9es. Puisqu\u2019il s\u2019attend \xE0 une liste de "),vr=l("code"),pm=a("dict"),dm=a(" o\xF9 chaque "),gr=l("code"),cm=a("dict"),mm=a(" repr\xE9sente un seul morceau de texte contigu, nous it\xE9rons d\u2019abord sur le jeu de donn\xE9es avant de donner le batch au collateur. Nous supprimons la cl\xE9 "),br=l("code"),_m=a('"word_ids"'),fm=a(" pour ce collateur de donn\xE9es car il ne l\u2019attend pas :"),tu=m(),w(En.$$.fragment),au=m(),w(xn.$$.fragment),nu=m(),ys=l("p"),hm=a("Super, \xE7a a march\xE9 ! Nous pouvons voir que le "),kr=l("em"),vm=a("token"),gm=m(),qr=l("code"),bm=a("[MASK]"),km=a(" a \xE9t\xE9 ins\xE9r\xE9 de fa\xE7on al\xE9atoire \xE0 diff\xE9rents endroits dans notre texte. Ce seront les "),$r=l("em"),qm=a("tokens"),$m=a(" que notre mod\xE8le devra pr\xE9dire pendant l\u2019entra\xEEnement. Et la beaut\xE9 du collecteur de donn\xE9es est qu\u2019il va rendre al\xE9atoire l\u2019insertion du "),wr=l("code"),wm=a("[MASK]"),Em=a(" \xE0 chaque batch !"),lu=m(),w(xa.$$.fragment),ou=m(),hs&&hs.c(),wl=m(),zs=l("p"),xm=a("Lors de l\u2019entra\xEEnement des mod\xE8les pour la mod\xE9lisation du langage masqu\xE9, une technique qui peut \xEAtre utilis\xE9e est de masquer des mots entiers ensemble et pas seulement des "),Er=l("em"),jm=a("tokens"),ym=a(" individuels. Cette approche est appel\xE9e "),xr=l("em"),zm=a("masquage de mots entiers"),Mm=a(". Si nous voulons utiliser le masquage de mots entiers, nous devons construire nous-m\xEAmes un collateur de donn\xE9es. Un collateur de donn\xE9es est simplement une fonction qui prend une liste d\u2019\xE9chantillons et les convertit en un batch. Faisons-le ! Nous utiliserons les identifiants des mots calcul\xE9s plus t\xF4t pour faire une correspondance entre les indices des mots et les "),jr=l("em"),Cm=a("tokens"),Dm=a(", puis nous d\xE9ciderons al\xE9atoirement quels mots masquer et appliquerons ce masque sur les entr\xE9es. Notez que les \xE9tiquettes sont toutes "),yr=l("code"),Pm=a("-100"),Am=a(" sauf celles qui correspondent aux mots masqu\xE9s."),ru=m(),gt.c(),El=m(),xl=l("p"),Sm=a("Ensuite, nous pouvons l\u2019essayer sur les m\xEAmes \xE9chantillons que pr\xE9c\xE9demment :"),iu=m(),w(jn.$$.fragment),uu=m(),w(yn.$$.fragment),pu=m(),w(ja.$$.fragment),du=m(),Ms=l("p"),Tm=a("Maintenant que nous avons deux collateurs de donn\xE9es, les \xE9tapes restantes du "),zr=l("em"),Nm=a("finetuning"),Om=a(" sont standards. L\u2019entra\xEEnement peut prendre un certain temps sur Google Colab si vous n\u2019avez pas la chance de tomber sur un mythique GPU P100 \u{1F62D}. Ainsi nous allons d\u2019abord r\xE9duire la taille du jeu d\u2019entra\xEEnement \xE0 quelques milliers d\u2019exemples. Ne vous inqui\xE9tez pas, nous obtiendrons quand m\xEAme un mod\xE8le de langage assez d\xE9cent ! Un moyen rapide de r\xE9duire la taille d\u2019un jeu de donn\xE9es dans \u{1F917} "),Mr=l("em"),Lm=a("Datasets"),Km=a(" est la fonction "),Cr=l("code"),Rm=a("Dataset.train_test_split()"),Bm=a(" que nous avons vue au "),jl=l("a"),Fm=a("chapitre 5"),Hm=a(" :"),cu=m(),w(zn.$$.fragment),mu=m(),w(Mn.$$.fragment),_u=m(),Cs=l("p"),Im=a("Cela a automatiquement cr\xE9\xE9 de nouvelles divisions "),Dr=l("code"),Um=a("train"),Gm=a(" et "),Pr=l("code"),Vm=a("test"),Wm=a(" avec la taille du jeu d\u2019entra\xEEnement fix\xE9e \xE0 10.000 exemples et la validation fix\xE9e \xE0 10% de cela. N\u2019h\xE9sitez pas \xE0 augmenter la taille si vous avez un GPU puissant ! La prochaine chose que nous devons faire est de nous connecter au "),Ar=l("em"),Jm=a("Hub"),Ym=a(". Si vous ex\xE9cutez ce code dans un "),Sr=l("em"),Qm=a("notebook"),Xm=a(", vous pouvez le faire avec la fonction suivante :"),fu=m(),w(Cn.$$.fragment),hu=m(),ya=l("p"),Zm=a("qui affichera un "),Tr=l("em"),e_=a("widget"),s_=a(" o\xF9 vous pourrez saisir vos informations d\u2019identification. Alternativement, vous pouvez ex\xE9cuter :"),vu=m(),w(Dn.$$.fragment),gu=m(),yl=l("p"),t_=a("dans votre terminal pr\xE9f\xE9r\xE9 et connectez-vous l\xE0."),bu=m(),kt.c(),zl=m(),ea=l("h3"),za=l("a"),Nr=l("span"),w(Pn.$$.fragment),a_=m(),Or=l("span"),n_=a("Perplexit\xE9 pour les mod\xE8les de langage"),ku=m(),w(An.$$.fragment),qu=m(),Ml=l("p"),l_=a("Contrairement \xE0 d\u2019autres t\xE2ches, comme la classification de textes ou la r\xE9ponse \xE0 des questions, sur lesquelles nous disposons d\u2019un corpus \xE9tiquet\xE9 pour entra\xEEner, la mod\xE9lisation du langage ne s\u2019appuie sur aucune \xE9tiquette explicite. Alors comment d\xE9terminer ce qui fait un bon mod\xE8le de langage ? Comme pour la fonction de correction automatique de votre t\xE9l\xE9phone, un bon mod\xE8le de langage est celui qui attribue des probabilit\xE9s \xE9lev\xE9es aux phrases grammaticalement correctes et des probabilit\xE9s faibles aux phrases absurdes. Pour vous donner une meilleure id\xE9e de ce \xE0 quoi cela ressemble, vous pouvez trouver en ligne des s\xE9ries enti\xE8res de \xAB rat\xE9s d\u2019autocorrection \xBB o\xF9 le mod\xE8le d\u2019un t\xE9l\xE9phone produit des compl\xE9ments plut\xF4t amusants (et souvent inappropri\xE9s) !"),$u=m(),$t.c(),Cl=m(),w(Sn.$$.fragment),wu=m(),Dl=l("p"),o_=a("Un score de perplexit\xE9 faible signifie un meilleur mod\xE8le de langue. Nous pouvons voir ici que notre mod\xE8le de d\xE9part a une valeur assez \xE9lev\xE9e. Voyons si nous pouvons la r\xE9duire en l\u2019affinant ! Pour ce faire, nous commen\xE7ons par ex\xE9cuter la boucle d\u2019entra\xEEnement :"),Eu=m(),Et.c(),Pl=m(),Al=l("p"),r_=a("et ensuite calculer la perplexit\xE9 r\xE9sultante sur l\u2019ensemble de test comme pr\xE9c\xE9demment :"),xu=m(),jt.c(),Sl=m(),w(Tn.$$.fragment),ju=m(),Tl=l("p"),i_=a("Joli. C\u2019est une r\xE9duction consid\xE9rable de la perplexit\xE9, ce qui nous indique que le mod\xE8le a appris quelque chose sur le domaine des critiques de films !"),yu=m(),Le&&Le.c(),Nl=m(),w(Ma.$$.fragment),zu=m(),Ke&&Ke.c(),Ol=m(),sa=l("h3"),Ca=l("a"),Lr=l("span"),w(Nn.$$.fragment),u_=m(),Ll=l("span"),p_=a("Utilisation de notre mod\xE8le "),Kr=l("i"),d_=a("finetun\xE9"),Mu=m(),Ge=l("p"),c_=a("Vous pouvez interagir avec votre mod\xE8le "),Rr=l("em"),m_=a("finetun\xE9"),__=a(" soit en utilisant son "),Br=l("em"),f_=a("widget"),h_=a(" sur le "),Fr=l("em"),v_=a("Hub"),g_=a(", soit localement avec le "),Hr=l("code"),b_=a("pipeline"),k_=a(" de \u{1F917} "),Ir=l("em"),q_=a("Transformers"),$_=a(". Utilisons ce dernier pour t\xE9l\xE9charger notre mod\xE8le en utilisant le pipeline "),Ur=l("code"),w_=a("fill-mask"),E_=a(" :"),Cu=m(),w(On.$$.fragment),Du=m(),Kl=l("p"),x_=a("Nous pouvons ensuite donner au pipeline notre exemple de texte \xAB this is a great [MASK] \xBB et voir quelles sont les 5 premi\xE8res pr\xE9dictions :"),Pu=m(),w(Ln.$$.fragment),Au=m(),w(Kn.$$.fragment),Su=m(),Rl=l("p"),j_=a("Notre mod\xE8le a clairement adapt\xE9 ses pond\xE9rations pour pr\xE9dire les mots qui sont plus fortement associ\xE9s aux films !"),Tu=m(),w(Rn.$$.fragment),Nu=m(),Ot=l("p"),y_=a("Ceci conclut notre premi\xE8re exp\xE9rience d\u2019entra\xEEnement d\u2019un mod\xE8le de langage. Dans la "),Bl=l("a"),z_=a("section 6"),M_=a(", vous apprendrez comment entra\xEEner \xE0 partir de z\xE9ro un mod\xE8le autor\xE9gressif comme GPT-2. Allez-y si vous voulez voir comment vous pouvez pr\xE9-entra\xEEner votre propre "),Gr=l("em"),C_=a("transformer"),D_=a(" !"),Ou=m(),w(Da.$$.fragment),this.h()},l(e){const u=Yv('[data-svelte="svelte-1phssyn"]',document.head);i=o(u,"META",{name:!0,content:!0}),u.forEach(t),h=_(e),E(d.$$.fragment,e),$=_(e),A=o(e,"H1",{class:!0});var Jn=r(A);q=o(Jn,"A",{id:!0,class:!0,href:!0});var Fl=r(q);k=o(Fl,"SPAN",{});var Vr=r(k);E(z.$$.fragment,Vr),Vr.forEach(t),Fl.forEach(t),f=_(Jn),M=o(Jn,"SPAN",{});var Yn=r(M);O=o(Yn,"I",{});var Hl=r(O);C=n(Hl,"Finetuner"),Hl.forEach(t),L=n(Yn," un mod\xE8le de langage masqu\xE9"),Yn.forEach(t),Jn.forEach(t),I=_(e),D.l(e),U=_(e),K=o(e,"P",{});var Ks=r(K);v=n(Ks,"Pour de nombreuses applications de NLP impliquant des "),N=o(Ks,"EM",{});var Wr=r(N);W=n(Wr,"transformers"),Wr.forEach(t),G=n(Ks,", vous pouvez simplement prendre un mod\xE8le pr\xE9-entra\xEEn\xE9 du "),J=o(Ks,"EM",{});var Jr=r(J);ee=n(Jr,"Hub"),Jr.forEach(t),re=n(Ks," et le "),te=o(Ks,"EM",{});var Yr=r(te);V=n(Yr,"finetuner"),Yr.forEach(t),R=n(Ks," directement sur vos donn\xE9es pour la t\xE2che \xE0 accomplir. Pour autant que le corpus utilis\xE9 pour le pr\xE9-entra\xEEnement ne soit pas trop diff\xE9rent du corpus utilis\xE9 pour le "),ie=o(Ks,"EM",{});var Il=r(ie);Q=n(Il,"finetuning"),Il.forEach(t),se=n(Ks,". L\u2019apprentissage par transfert produira g\xE9n\xE9ralement de bons r\xE9sultats."),Ks.forEach(t),ue=_(e),Z=o(e,"P",{});var Rs=r(Z);ge=n(Rs,"Cependant, il existe quelques cas o\xF9 vous voudrez d\u2019abord "),ne=o(Rs,"EM",{});var Ul=r(ne);de=n(Ul,"finetuner"),Ul.forEach(t),rs=n(Rs," les mod\xE8les de langue sur vos donn\xE9es, avant d\u2019entra\xEEner une t\xEAte sp\xE9cifique \xE0 la t\xE2che. Par exemple, si votre jeu de donn\xE9es contient des contrats l\xE9gaux ou des articles scientifiques, un "),be=o(Rs,"EM",{});var Gl=r(be);ke=n(Gl,"transformer"),Gl.forEach(t),is=n(Rs," classique comme BERT traitera g\xE9n\xE9ralement les mots sp\xE9cifiques au domaine dans votre corpus comme des "),Ve=o(Rs,"EM",{});var Vl=r(Ve);qe=n(Vl,"tokens"),Vl.forEach(t),us=n(Rs," rares et les performances r\xE9sultantes peuvent \xEAtre moins que satisfaisantes. En "),We=o(Rs,"EM",{});var Wl=r(We);je=n(Wl,"finetunant"),Wl.forEach(t),Ne=n(Rs," le mod\xE8le de langage sur les donn\xE9es du domaine, vous pouvez am\xE9liorer les performances de nombreuses t\xE2ches en aval, ce qui signifie que vous ne devez g\xE9n\xE9ralement effectuer cette \xE9tape qu\u2019une seule fois !"),Rs.forEach(t),Je=_(e),X=o(e,"P",{});var vs=r(X);pe=n(vs,"Ce processus de "),ye=o(vs,"EM",{});var Qr=r(ye);ce=n(Qr,"finetuning"),Qr.forEach(t),ps=n(vs," d\u2019un mod\xE8le de langage pr\xE9-entra\xEEn\xE9 sur des donn\xE9es "),ze=o(vs,"EM",{});var J_=r(ze);$e=n(J_,"dans le domaine"),J_.forEach(t),Xe=n(vs," est g\xE9n\xE9ralement appel\xE9 "),le=o(vs,"EM",{});var Y_=r(le);Ze=n(Y_,"adaptation au domaine"),Y_.forEach(t),we=n(vs,". Il a \xE9t\xE9 popularis\xE9 en 2018 par "),Ee=o(vs,"A",{href:!0,rel:!0});var Q_=r(Ee);B=n(Q_,"ULMFiT"),Q_.forEach(t),ae=n(vs," qui a \xE9t\xE9 l\u2019une des premi\xE8res architectures neuronales (bas\xE9es sur des LSTMs) \xE0 faire en sorte que l\u2019apprentissage par transfert fonctionne r\xE9ellement pour le NLP. Un exemple d\u2019adaptation de domaine avec ULMFiT est pr\xE9sent\xE9 dans l\u2019image ci-dessous. Dans cette section, nous ferons quelque chose de similaire mais avec un "),Re=o(vs,"EM",{});var X_=r(Re);me=n(X_,"transformer"),X_.forEach(t),es=n(vs," au lieu d\u2019une LSTM !"),vs.forEach(t),Ye=_(e),Me=o(e,"DIV",{class:!0});var Ku=r(Me);Oe=o(Ku,"IMG",{class:!0,src:!0,alt:!0}),T=_(Ku),Y=o(Ku,"IMG",{class:!0,src:!0,alt:!0}),Ku.forEach(t),oe=_(e),Ce=o(e,"P",{});var Jl=r(Ce);Ps=n(Jl,"\xC0 la fin de cette section, vous aurez un "),Fe=o(Jl,"A",{href:!0,rel:!0});var Z_=r(Fe);Qe=n(Z_,"mod\xE8le de langage masqu\xE9"),Z_.forEach(t),He=n(Jl," sur le "),De=o(Jl,"EM",{});var ef=r(De);fe=n(ef,"Hub"),ef.forEach(t),Fs=n(Jl," qui peut autocompl\xE9ter des phrases comme indiqu\xE9 ci-dessous :"),Jl.forEach(t),nt=_(e),Pe=o(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),r(Pe).forEach(t),ds=_(e),xe=o(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),r(xe).forEach(t),lt=_(e),cs=o(e,"P",{});var sf=r(cs);gs=n(sf,"Allons-y !"),sf.forEach(t),ot=_(e),E(ms.$$.fragment,e),Hs=_(e),E(As.$$.fragment,e),rt=_(e),Ss=o(e,"H2",{class:!0});var Ru=r(Ss);Ts=o(Ru,"A",{id:!0,class:!0,href:!0});var tf=r(Ts);Ns=o(tf,"SPAN",{});var af=r(Ns);E(Is.$$.fragment,af),af.forEach(t),tf.forEach(t),bs=_(Ru),Gt=o(Ru,"SPAN",{});var nf=r(Gt);Vt=n(nf,"Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"),nf.forEach(t),Ru.forEach(t),ta=_(e),ks=o(e,"P",{});var Yl=r(ks);Wt=n(Yl,"Pour commencer, nous allons choisir un mod\xE8le pr\xE9-entra\xEEn\xE9 appropri\xE9 pour la mod\xE9lisation du langage masqu\xE9. Comme le montre la capture d\u2019\xE9cran suivante, vous pouvez trouver une liste de candidats en appliquant le filtre \xAB "),Jt=o(Yl,"EM",{});var lf=r(Jt);La=n(lf,"Fill-Mask"),lf.forEach(t),aa=n(Yl," \xBB sur le "),_s=o(Yl,"A",{href:!0,rel:!0});var of=r(_s);zt=o(of,"EM",{});var rf=r(zt);it=n(rf,"Hub"),rf.forEach(t),of.forEach(t),na=n(Yl," :"),Yl.forEach(t),fs=_(e),ut=o(e,"DIV",{class:!0});var uf=r(ut);Os=o(uf,"IMG",{src:!0,alt:!0,width:!0}),uf.forEach(t),la=_(e),ve=o(e,"P",{});var et=r(ve);Ka=n(et,"Bien que les mod\xE8les de la famille BERT et RoBERTa soient les plus t\xE9l\xE9charg\xE9s, nous utiliserons un mod\xE8le appel\xE9 "),pt=o(et,"A",{href:!0,rel:!0});var pf=r(pt);oa=n(pf,"DistilBERT"),pf.forEach(t),dt=n(et," qui peut \xEAtre entra\xEEn\xE9 beaucoup plus rapidement avec peu ou pas de perte de performance en aval. Ce mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide d\u2019une technique sp\xE9ciale appel\xE9e "),Us=o(et,"A",{href:!0,rel:!0});var df=r(Us);Gs=o(df,"EM",{});var cf=r(Gs);Ra=n(cf,"distillation de connaissances"),cf.forEach(t),df.forEach(t),ra=n(et,", o\xF9 un grand mod\xE8le "),Ls=o(et,"EM",{});var mf=r(Ls);ia=n(mf,"enseignant"),mf.forEach(t),ct=n(et," comme BERT est utilis\xE9 pour guider l\u2019entra\xEEnement d\u2019un mod\xE8le "),Mt=o(et,"EM",{});var _f=r(Mt);Ct=n(_f,"\xE9tudiant"),_f.forEach(t),Ba=n(et," qui a beaucoup moins de param\xE8tres. Une explication des d\xE9tails de la distillation de connaissances nous m\xE8nerait trop loin dans cette section mais si vous \xEAtes int\xE9ress\xE9, vous pouvez lire tout cela dans le livre "),Vs=o(et,"A",{href:!0,rel:!0});var ff=r(Vs);c=o(ff,"EM",{});var hf=r(c);F=n(hf,"Natural Language Processing with Transformers"),hf.forEach(t),ff.forEach(t),Xn=n(et,"."),et.forEach(t),Yt=_(e),$s.l(e),mt=_(e),_t=o(e,"P",{});var Bu=r(_t);Zn=n(Bu,"Avec environ 67 millions de param\xE8tres, DistilBERT est environ deux fois plus petit que le mod\xE8le de base de BERT, ce qui se traduit approximativement par une acc\xE9l\xE9ration de l\u2019entra\xEEnement d\u2019un facteur deux. Voyons maintenant quels types de "),ws=o(Bu,"EM",{});var vf=r(ws);el=n(vf,"tokens"),vf.forEach(t),sl=n(Bu," ce mod\xE8le pr\xE9dit comme \xE9tant les compl\xE9ments les plus probables d\u2019un petit \xE9chantillon de texte :"),Bu.forEach(t),Fa=_(e),E(Ue.$$.fragment,e),Ha=_(e),he=o(e,"P",{});var ns=r(he);tl=n(ns,"En tant qu\u2019\xEAtres humains, nous pouvons imaginer de nombreuses possibilit\xE9s pour le "),ua=o(ns,"EM",{});var gf=r(ua);pa=n(gf,"token"),gf.forEach(t),al=_(ns),Es=o(ns,"CODE",{});var bf=r(Es);nl=n(bf,"[MASK]"),bf.forEach(t),ll=n(ns,", telles que \xAB jour \xBB, \xAB promenade \xBB ou \xAB peinture \xBB. Pour les mod\xE8les pr\xE9-entra\xEEn\xE9s, les pr\xE9dictions d\xE9pendent du corpus sur lequel le mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 puisqu\u2019il apprend \xE0 d\xE9tecter les mod\xE8les statistiques pr\xE9sents dans les donn\xE9es. Comme BERT, DistilBERT a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur les jeux de donn\xE9es "),Dt=o(ns,"A",{href:!0,rel:!0});var kf=r(Dt);Pt=o(kf,"EM",{});var qf=r(Pt);ol=n(qf,"English Wikipedia"),qf.forEach(t),kf.forEach(t),da=n(ns," et "),At=o(ns,"A",{href:!0,rel:!0});var $f=r(At);St=o($f,"EM",{});var wf=r(St);rl=n(wf,"BookCorpus"),wf.forEach(t),$f.forEach(t),il=n(ns,", nous nous attendons donc \xE0 ce que les pr\xE9dictions pour "),Ws=o(ns,"CODE",{});var Ef=r(Ws);ul=n(Ef,"[MASK]"),Ef.forEach(t),pl=n(ns," refl\xE8tent ces domaines. Pour pr\xE9dire le masque, nous avons besoin du "),Js=o(ns,"EM",{});var xf=r(Js);dl=n(xf,"tokenizer"),xf.forEach(t),cl=n(ns," de DistilBERT pour produire les entr\xE9es du mod\xE8le, alors t\xE9l\xE9chargeons-le \xE9galement depuis le "),ca=o(ns,"EM",{});var jf=r(ca);ml=n(jf,"Hub"),jf.forEach(t),np=n(ns," :"),ns.forEach(t),Zr=_(e),E(Ia.$$.fragment,e),ei=_(e),ma=o(e,"P",{});var Fu=r(ma);lp=n(Fu,"Avec un "),eo=o(Fu,"EM",{});var yf=r(eo);op=n(yf,"tokenizer"),yf.forEach(t),rp=n(Fu," et un mod\xE8le, nous pouvons maintenant passer notre exemple de texte au mod\xE8le, extraire les logits, et afficher les 5 meilleurs candidats :"),Fu.forEach(t),si=_(e),ht.l(e),_l=_(e),E(Ua.$$.fragment,e),ti=_(e),fl=o(e,"P",{});var zf=r(fl);ip=n(zf,"Nous pouvons voir dans les sorties que les pr\xE9dictions du mod\xE8le se r\xE9f\xE8rent \xE0 des termes de tous les jours, ce qui n\u2019est peut-\xEAtre pas surprenant \xE9tant donn\xE9 le fondement de Wikip\xE9dia. Voyons comment nous pouvons changer ce domaine pour quelque chose d\u2019un peu plus sp\xE9cialis\xE9 : des critiques de films !"),zf.forEach(t),ai=_(e),Qt=o(e,"H2",{class:!0});var Hu=r(Qt);_a=o(Hu,"A",{id:!0,class:!0,href:!0});var Mf=r(_a);so=o(Mf,"SPAN",{});var Cf=r(so);E(Ga.$$.fragment,Cf),Cf.forEach(t),Mf.forEach(t),up=_(Hu),to=o(Hu,"SPAN",{});var Df=r(to);pp=n(Df,"Le jeu de donn\xE9es"),Df.forEach(t),Hu.forEach(t),ni=_(e),ss=o(e,"P",{});var st=r(ss);dp=n(st,"Pour illustrer l\u2019adaptation au domaine, nous utiliserons le c\xE9l\xE8bre "),Va=o(st,"A",{href:!0,rel:!0});var Pf=r(Va);ao=o(Pf,"EM",{});var Af=r(ao);cp=n(Af,"Large Movie Review Dataset"),Af.forEach(t),Pf.forEach(t),mp=n(st," (ou IMDb en abr\xE9g\xE9), qui est un corpus de critiques de films souvent utilis\xE9 pour \xE9valuer les mod\xE8les d\u2019analyse de sentiments. En "),no=o(st,"EM",{});var Sf=r(no);_p=n(Sf,"finetunant"),Sf.forEach(t),fp=n(st," DistilBERT sur ce corpus, nous esp\xE9rons que le mod\xE8le de langage adaptera son vocabulaire des donn\xE9es factuelles de Wikip\xE9dia sur lesquelles il a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 aux \xE9l\xE9ments plus subjectifs des critiques de films. Nous pouvons obtenir les donn\xE9es du "),lo=o(st,"EM",{});var Tf=r(lo);hp=n(Tf,"Hub"),Tf.forEach(t),vp=n(st," avec la fonction "),oo=o(st,"CODE",{});var Nf=r(oo);gp=n(Nf,"load_dataset()"),Nf.forEach(t),bp=n(st," de \u{1F917} "),ro=o(st,"EM",{});var Of=r(ro);kp=n(Of,"Datasets"),Of.forEach(t),qp=n(st," :"),st.forEach(t),li=_(e),E(Wa.$$.fragment,e),oi=_(e),E(Ja.$$.fragment,e),ri=_(e),ts=o(e,"P",{});var tt=r(ts);$p=n(tt,"Nous pouvons voir que les parties "),io=o(tt,"CODE",{});var Lf=r(io);wp=n(Lf,"train"),Lf.forEach(t),Ep=n(tt," et "),uo=o(tt,"CODE",{});var Kf=r(uo);xp=n(Kf,"test"),Kf.forEach(t),jp=n(tt," sont chacune compos\xE9es de 25 000 critiques, alors qu\u2019il y a une partie non \xE9tiquet\xE9e appel\xE9e "),po=o(tt,"CODE",{});var Rf=r(po);yp=n(Rf,"unsupervised"),Rf.forEach(t),zp=n(tt," qui contient 50 000 critiques. Jetons un coup d\u2019\u0153il \xE0 quelques \xE9chantillons pour avoir une id\xE9e du type de texte auquel nous avons affaire. Comme nous l\u2019avons fait dans les chapitres pr\xE9c\xE9dents du cours, nous allons encha\xEEner les fonctions "),co=o(tt,"CODE",{});var Bf=r(co);Mp=n(Bf,"Dataset.shuffle()"),Bf.forEach(t),Cp=n(tt," et "),mo=o(tt,"CODE",{});var Ff=r(mo);Dp=n(Ff,"Dataset.select()"),Ff.forEach(t),Pp=n(tt," pour cr\xE9er un \xE9chantillon al\xE9atoire :"),tt.forEach(t),ii=_(e),E(Ya.$$.fragment,e),ui=_(e),E(Qa.$$.fragment,e),pi=_(e),Tt=o(e,"P",{});var Ql=r(Tt);Ap=n(Ql,"Oui, ce sont bien des critiques de films, et si vous \xEAtes assez \xE2g\xE9s, vous pouvez m\xEAme comprendre le commentaire dans la derni\xE8re critique sur le fait de poss\xE9der une version VHS \u{1F61C} ! Bien que nous n\u2019ayons pas besoin des \xE9tiquettes pour la mod\xE9lisation du langage, nous pouvons d\xE9j\xE0 voir qu\u2019un "),_o=o(Ql,"CODE",{});var Hf=r(_o);Sp=n(Hf,"0"),Hf.forEach(t),Tp=n(Ql," d\xE9note une critique n\xE9gative, tandis qu\u2019un "),fo=o(Ql,"CODE",{});var If=r(fo);Np=n(If,"1"),If.forEach(t),Op=n(Ql," correspond \xE0 une critique positive."),Ql.forEach(t),di=_(e),E(fa.$$.fragment,e),ci=_(e),ha=o(e,"P",{});var Iu=r(ha);Lp=n(Iu,"Maintenant que nous avons jet\xE9 un coup d\u2019\u0153il rapide aux donn\xE9es, plongeons dans leur pr\xE9paration pour la mod\xE9lisation du langage masqu\xE9. Comme nous allons le voir, il y a quelques \xE9tapes suppl\xE9mentaires \xE0 suivre par rapport aux t\xE2ches de classification de s\xE9quences que nous avons vues au "),hl=o(Iu,"A",{href:!0});var Uf=r(hl);Kp=n(Uf,"chapitre 3"),Uf.forEach(t),Rp=n(Iu,". Allons-y !"),Iu.forEach(t),mi=_(e),Xt=o(e,"H2",{class:!0});var Uu=r(Xt);va=o(Uu,"A",{id:!0,class:!0,href:!0});var Gf=r(va);ho=o(Gf,"SPAN",{});var Vf=r(ho);E(Xa.$$.fragment,Vf),Vf.forEach(t),Gf.forEach(t),Bp=_(Uu),vo=o(Uu,"SPAN",{});var Wf=r(vo);Fp=n(Wf,"Pr\xE9traitement des donn\xE9es"),Wf.forEach(t),Uu.forEach(t),_i=_(e),E(Za.$$.fragment,e),fi=_(e),ga=o(e,"P",{});var Gu=r(ga);Hp=n(Gu,"Pour la mod\xE9lisation autor\xE9gressive et la mod\xE9lisation du langage masqu\xE9, une \xE9tape commune de pr\xE9traitement consiste \xE0 concat\xE9ner tous les exemples, puis \xE0 diviser le corpus entier en morceaux de taille \xE9gale. C\u2019est tr\xE8s diff\xE9rent de notre approche habituelle, o\xF9 nous nous contentons de "),go=o(Gu,"EM",{});var Jf=r(go);Ip=n(Jf,"tokenizer"),Jf.forEach(t),Up=n(Gu," les exemples individuels. Pourquoi tout concat\xE9ner ? La raison est que les exemples individuels peuvent \xEAtre tronqu\xE9s s\u2019ils sont trop longs, ce qui entra\xEEnerait la perte d\u2019informations qui pourraient \xEAtre utiles pour la t\xE2che de mod\xE9lisation du langage !"),Gu.forEach(t),hi=_(e),Ae=o(e,"P",{});var ls=r(Ae);Gp=n(ls,"Donc pour commencer, nous allons d\u2019abord tokeniser notre corpus comme d\u2019habitude, mais "),bo=o(ls,"EM",{});var Yf=r(bo);Vp=n(Yf,"sans"),Yf.forEach(t),Wp=n(ls," mettre l\u2019option "),ko=o(ls,"CODE",{});var Qf=r(ko);Jp=n(Qf,"truncation=True"),Qf.forEach(t),Yp=n(ls," dans notre "),qo=o(ls,"EM",{});var Xf=r(qo);Qp=n(Xf,"tokenizer"),Xf.forEach(t),Xp=n(ls,". Nous allons aussi r\xE9cup\xE9rer les identifiants des mots s\u2019ils sont disponibles (ce qui sera le cas si nous utilisons un "),$o=o(ls,"EM",{});var Zf=r($o);Zp=n(Zf,"tokenizer"),Zf.forEach(t),ed=n(ls," rapide, comme d\xE9crit dans le "),vl=o(ls,"A",{href:!0});var eh=r(vl);sd=n(eh,"chapitre 6"),eh.forEach(t),td=n(ls,"), car nous en aurons besoin plus tard pour faire le masquage de mots entiers. Nous allons envelopper cela dans une simple fonction, et pendant que nous y sommes, nous allons supprimer les colonnes "),wo=o(ls,"CODE",{});var sh=r(wo);ad=n(sh,"text"),sh.forEach(t),nd=n(ls," et "),Eo=o(ls,"CODE",{});var th=r(Eo);ld=n(th,"label"),th.forEach(t),od=n(ls," puisque nous n\u2019en avons plus besoin :"),ls.forEach(t),vi=_(e),E(en.$$.fragment,e),gi=_(e),E(sn.$$.fragment,e),bi=_(e),Ys=o(e,"P",{});var Pa=r(Ys);rd=n(Pa,"Comme DistilBERT est un mod\xE8le de type BERT, nous pouvons voir que les textes encod\xE9s sont constitu\xE9s des "),xo=o(Pa,"CODE",{});var ah=r(xo);id=n(ah,"input_ids"),ah.forEach(t),ud=n(Pa," et des "),jo=o(Pa,"CODE",{});var nh=r(jo);pd=n(nh,"attention_mask"),nh.forEach(t),dd=n(Pa," que nous avons vus dans d\u2019autres chapitres, ainsi que des "),yo=o(Pa,"CODE",{});var lh=r(yo);cd=n(lh,"word_ids"),lh.forEach(t),md=n(Pa," que nous avons ajout\xE9s."),Pa.forEach(t),ki=_(e),Qs=o(e,"P",{});var Aa=r(Qs);_d=n(Aa,"Maintenant que nos critiques de films ont \xE9t\xE9 tokenis\xE9es, l\u2019\xE9tape suivante consiste \xE0 les regrouper et \xE0 diviser le r\xE9sultat en chunks. Mais quelle taille doivent avoir ces "),zo=o(Aa,"EM",{});var oh=r(zo);fd=n(oh,"chunks"),oh.forEach(t),hd=n(Aa," ? Cela sera finalement d\xE9termin\xE9 par la quantit\xE9 de m\xE9moire GPU dont vous disposez, mais un bon point de d\xE9part est de voir quelle est la taille maximale du contexte du mod\xE8le. Cela peut \xEAtre d\xE9duit en inspectant l\u2019attribut "),Mo=o(Aa,"CODE",{});var rh=r(Mo);vd=n(rh,"model_max_length"),rh.forEach(t),gd=n(Aa," du "),Co=o(Aa,"EM",{});var ih=r(Co);bd=n(ih,"tokenizer"),ih.forEach(t),kd=n(Aa," :"),Aa.forEach(t),qi=_(e),E(tn.$$.fragment,e),$i=_(e),E(an.$$.fragment,e),wi=_(e),Xs=o(e,"P",{});var Sa=r(Xs);qd=n(Sa,"Cette valeur est d\xE9riv\xE9e du fichier "),Do=o(Sa,"EM",{});var uh=r(Do);$d=n(uh,"tokenizer_config.json"),uh.forEach(t),wd=n(Sa," associ\xE9 \xE0 un "),Po=o(Sa,"EM",{});var ph=r(Po);Ed=n(ph,"checkpoint"),ph.forEach(t),xd=n(Sa,". Dans ce cas, nous pouvons voir que la taille du contexte est de 512 "),Ao=o(Sa,"EM",{});var dh=r(Ao);jd=n(dh,"tokens"),dh.forEach(t),yd=n(Sa,", tout comme avec BERT."),Sa.forEach(t),Ei=_(e),E(ba.$$.fragment,e),xi=_(e),gl=o(e,"P",{});var ch=r(gl);zd=n(ch,"Ainsi, pour r\xE9aliser nos exp\xE9riences sur des GPUs comme ceux disponibles sur Google Colab, nous choisirons quelque chose d\u2019un peu plus petit qui peut tenir en m\xE9moire :"),ch.forEach(t),ji=_(e),E(nn.$$.fragment,e),yi=_(e),E(ka.$$.fragment,e),zi=_(e),qa=o(e,"P",{});var Vu=r(qa);Md=n(Vu,"Maintenant vient la partie amusante. Pour montrer comment la concat\xE9nation fonctionne, prenons quelques commentaires de notre ensemble d\u2019entra\xEEnement et affichons le nombre de "),So=o(Vu,"EM",{});var mh=r(So);Cd=n(mh,"tokens"),mh.forEach(t),Dd=n(Vu," par commentaire :"),Vu.forEach(t),Mi=_(e),E(ln.$$.fragment,e),Ci=_(e),E(on.$$.fragment,e),Di=_(e),bl=o(e,"P",{});var _h=r(bl);Pd=n(_h,"Nous pouvons ensuite concat\xE9ner tous ces exemples avec une simple compr\xE9hension du dictionnaire, comme suit :"),_h.forEach(t),Pi=_(e),E(rn.$$.fragment,e),Ai=_(e),E(un.$$.fragment,e),Si=_(e),xs=o(e,"P",{});var Lt=r(xs);Ad=n(Lt,"Super, la longueur totale est correcte. Donc maintenant, nous allons diviser les exemples concat\xE9n\xE9s en morceaux de la taille donn\xE9e par "),To=o(Lt,"CODE",{});var fh=r(To);Sd=n(fh,"block_size"),fh.forEach(t),Td=n(Lt,". Pour ce faire, nous it\xE9rons sur les caract\xE9ristiques de "),No=o(Lt,"CODE",{});var hh=r(No);Nd=n(hh,"concatenated_examples"),hh.forEach(t),Od=n(Lt," et utilisons une compr\xE9hension de liste pour cr\xE9er des "),Oo=o(Lt,"EM",{});var vh=r(Oo);Ld=n(vh,"chunks"),vh.forEach(t),Kd=n(Lt," de chaque caract\xE9ristique. Le r\xE9sultat est un dictionnaire de "),Lo=o(Lt,"EM",{});var gh=r(Lo);Rd=n(gh,"chunks"),gh.forEach(t),Bd=n(Lt," pour chaque caract\xE9ristique :"),Lt.forEach(t),Ti=_(e),E(pn.$$.fragment,e),Ni=_(e),E(dn.$$.fragment,e),Oi=_(e),$a=o(e,"P",{});var Wu=r($a);Fd=n(Wu,"Comme vous pouvez le voir dans cet exemple, le dernier "),Ko=o(Wu,"EM",{});var bh=r(Ko);Hd=n(bh,"chunk"),bh.forEach(t),Id=n(Wu," sera g\xE9n\xE9ralement plus petit que la taille maximale des morceaux. Il y a deux strat\xE9gies principales pour g\xE9rer cela :"),Wu.forEach(t),Li=_(e),wa=o(e,"UL",{});var Ju=r(wa);cn=o(Ju,"LI",{});var Yu=r(cn);Ud=n(Yu,"Abandonner le dernier morceau s\u2019il est plus petit que "),Ro=o(Yu,"CODE",{});var kh=r(Ro);Gd=n(kh,"chunk_size"),kh.forEach(t),Vd=n(Yu,"."),Yu.forEach(t),Wd=_(Ju),mn=o(Ju,"LI",{});var Qu=r(mn);Jd=n(Qu,"Rembourrer le dernier morceau jusqu\u2019\xE0 ce que sa longueur soit \xE9gale \xE0 "),Bo=o(Qu,"CODE",{});var qh=r(Bo);Yd=n(qh,"chunk_size"),qh.forEach(t),Qd=n(Qu,"."),Qu.forEach(t),Ju.forEach(t),Ki=_(e),kl=o(e,"P",{});var $h=r(kl);Xd=n($h,"Nous adopterons la premi\xE8re approche ici, donc nous allons envelopper toute la logique ci-dessus dans une seule fonction que nous pouvons appliquer \xE0 nos jeux de donn\xE9es tokenis\xE9s :"),$h.forEach(t),Ri=_(e),E(_n.$$.fragment,e),Bi=_(e),as=o(e,"P",{});var at=r(as);Zd=n(at,"Notez que dans la derni\xE8re \xE9tape de "),Fo=o(at,"CODE",{});var wh=r(Fo);ec=n(wh,"group_texts()"),wh.forEach(t),sc=n(at," nous cr\xE9ons une nouvelle colonne "),Ho=o(at,"CODE",{});var Eh=r(Ho);tc=n(Eh,"labels"),Eh.forEach(t),ac=n(at," qui est une copie de la colonne "),Io=o(at,"CODE",{});var xh=r(Io);nc=n(xh,"input_ids"),xh.forEach(t),lc=n(at,". Comme nous le verrons bient\xF4t, c\u2019est parce que dans la mod\xE9lisation du langage masqu\xE9, l\u2019objectif est de pr\xE9dire des "),Uo=o(at,"EM",{});var jh=r(Uo);oc=n(jh,"tokens"),jh.forEach(t),rc=n(at," masqu\xE9s al\xE9atoirement dans le batch d\u2019entr\xE9e, et en cr\xE9ant une colonne "),Go=o(at,"CODE",{});var yh=r(Go);ic=n(yh,"labels"),yh.forEach(t),uc=n(at,", nous fournissons la v\xE9rit\xE9 de base pour notre mod\xE8le de langage \xE0 apprendre."),at.forEach(t),Fi=_(e),Nt=o(e,"P",{});var Xl=r(Nt);pc=n(Xl,"Appliquons maintenant "),Vo=o(Xl,"CODE",{});var zh=r(Vo);dc=n(zh,"group_texts()"),zh.forEach(t),cc=n(Xl," \xE0 nos jeux de donn\xE9es tokenis\xE9s en utilisant notre fid\xE8le fonction "),Wo=o(Xl,"CODE",{});var Mh=r(Wo);mc=n(Mh,"Dataset.map()"),Mh.forEach(t),_c=n(Xl," :"),Xl.forEach(t),Hi=_(e),E(fn.$$.fragment,e),Ii=_(e),E(hn.$$.fragment,e),Ui=_(e),Se=o(e,"P",{});var os=r(Se);fc=n(os,"Vous pouvez voir que le regroupement puis le d\xE9coupage des textes a produit beaucoup plus d\u2019exemples que nos 25 000 exemples initiaux pour les divisions "),Jo=o(os,"CODE",{});var Ch=r(Jo);hc=n(Ch,"train"),Ch.forEach(t),vc=n(os," et "),Yo=o(os,"CODE",{});var Dh=r(Yo);gc=n(Dh,"test"),Dh.forEach(t),bc=n(os,". C\u2019est parce que nous avons maintenant des exemples impliquant des "),Qo=o(os,"EM",{});var Ph=r(Qo);kc=n(Ph,"tokens"),Ph.forEach(t),qc=n(os," contigus qui s\u2019\xE9tendent sur plusieurs exemples du corpus original. Vous pouvez le voir explicitement en cherchant les "),Xo=o(os,"EM",{});var Ah=r(Xo);$c=n(Ah,"tokens"),Ah.forEach(t),wc=n(os," sp\xE9ciaux "),Zo=o(os,"CODE",{});var Sh=r(Zo);Ec=n(Sh,"[SEP]"),Sh.forEach(t),xc=n(os," et "),er=o(os,"CODE",{});var Th=r(er);jc=n(Th,"[CLS]"),Th.forEach(t),yc=n(os," dans l\u2019un des "),sr=o(os,"EM",{});var Nh=r(sr);zc=n(Nh,"chunks"),Nh.forEach(t),Mc=n(os," :"),os.forEach(t),Gi=_(e),E(vn.$$.fragment,e),Vi=_(e),E(gn.$$.fragment,e),Wi=_(e),ql=o(e,"P",{});var Oh=r(ql);Cc=n(Oh,"Dans cet exemple, vous pouvez voir deux critiques de films qui se chevauchent, l\u2019une sur un film de lyc\xE9e et l\u2019autre sur les sans-abri. Voyons \xE9galement \xE0 quoi ressemblent les \xE9tiquettes pour la mod\xE9lisation du langage masqu\xE9 :"),Oh.forEach(t),Ji=_(e),E(bn.$$.fragment,e),Yi=_(e),E(kn.$$.fragment,e),Qi=_(e),js=o(e,"P",{});var Kt=r(js);Dc=n(Kt,"Comme pr\xE9vu par notre fonction "),tr=o(Kt,"CODE",{});var Lh=r(tr);Pc=n(Lh,"group_texts()"),Lh.forEach(t),Ac=n(Kt," ci-dessus, cela semble identique aux "),ar=o(Kt,"CODE",{});var Kh=r(ar);Sc=n(Kh,"input_ids"),Kh.forEach(t),Tc=n(Kt," d\xE9cod\xE9s. Mais alors comment notre mod\xE8le peut-il apprendre quoi que ce soit ? Il nous manque une \xE9tape cl\xE9 : ins\xE9rer des "),nr=o(Kt,"EM",{});var Rh=r(nr);Nc=n(Rh,"tokens"),Rh.forEach(t),Oc=n(Kt," \xE0 des positions al\xE9atoires dans les entr\xE9es ! Voyons comment nous pouvons le faire \xE0 la vol\xE9e pendant le "),lr=o(Kt,"EM",{});var Bh=r(lr);Lc=n(Bh,"finetuning"),Bh.forEach(t),Kc=n(Kt," en utilisant un collateur de donn\xE9es sp\xE9cial."),Kt.forEach(t),Xi=_(e),Zt=o(e,"H2",{class:!0});var Xu=r(Zt);Ea=o(Xu,"A",{id:!0,class:!0,href:!0});var Fh=r(Ea);or=o(Fh,"SPAN",{});var Hh=r(or);E(qn.$$.fragment,Hh),Hh.forEach(t),Fh.forEach(t),Rc=_(Xu),$n=o(Xu,"SPAN",{});var Zu=r($n);rr=o(Zu,"I",{});var Ih=r(rr);Bc=n(Ih,"Finetuning"),Ih.forEach(t),Fc=n(Zu," de DistilBERT avec l'API "),ir=o(Zu,"CODE",{});var Uh=r(ir);Hc=n(Uh,"Trainer"),Uh.forEach(t),Zu.forEach(t),Xu.forEach(t),Zi=_(e),_e=o(e,"P",{});var Te=r(_e);Ic=n(Te,"Le "),ur=o(Te,"EM",{});var Gh=r(ur);Uc=n(Gh,"finetuning"),Gh.forEach(t),Gc=n(Te," d\u2019un mod\xE8le de langage masqu\xE9 est presque identique au "),pr=o(Te,"EM",{});var Vh=r(pr);Vc=n(Vh,"finetuning"),Vh.forEach(t),Wc=n(Te," d\u2019un mod\xE8le de classification de s\xE9quences, comme nous l\u2019avons fait dans le "),$l=o(Te,"A",{href:!0});var Wh=r($l);Jc=n(Wh,"chapitre 3"),Wh.forEach(t),Yc=n(Te,". La seule diff\xE9rence est que nous avons besoin d\u2019un collecteur de donn\xE9es sp\xE9cial qui peut masquer de mani\xE8re al\xE9atoire certains des "),dr=o(Te,"EM",{});var Jh=r(dr);Qc=n(Jh,"tokens"),Jh.forEach(t),Xc=n(Te," dans chaque batch de textes. Heureusement, \u{1F917} "),cr=o(Te,"EM",{});var Yh=r(cr);Zc=n(Yh,"Transformers"),Yh.forEach(t),em=n(Te," est livr\xE9 pr\xE9par\xE9 avec un "),mr=o(Te,"CODE",{});var Qh=r(mr);sm=n(Qh,"DataCollatorForLanguageModeling"),Qh.forEach(t),tm=n(Te," d\xE9di\xE9 \xE0 cette t\xE2che. Nous devons juste lui passer le "),_r=o(Te,"EM",{});var Xh=r(_r);am=n(Xh,"tokenizer"),Xh.forEach(t),nm=n(Te," et un argument "),fr=o(Te,"CODE",{});var Zh=r(fr);lm=n(Zh,"mlm_probability"),Zh.forEach(t),om=n(Te," qui sp\xE9cifie quelle fraction des "),hr=o(Te,"EM",{});var ev=r(hr);rm=n(ev,"tokens"),ev.forEach(t),im=n(Te," \xE0 masquer. Nous choisirons 15%, qui est la quantit\xE9 utilis\xE9e pour BERT et un choix commun dans la litt\xE9rature :"),Te.forEach(t),eu=_(e),E(wn.$$.fragment,e),su=_(e),Zs=o(e,"P",{});var Ta=r(Zs);um=n(Ta,"Pour voir comment le masquage al\xE9atoire fonctionne, nous allons donner quelques exemples au collateur de donn\xE9es. Puisqu\u2019il s\u2019attend \xE0 une liste de "),vr=o(Ta,"CODE",{});var sv=r(vr);pm=n(sv,"dict"),sv.forEach(t),dm=n(Ta," o\xF9 chaque "),gr=o(Ta,"CODE",{});var tv=r(gr);cm=n(tv,"dict"),tv.forEach(t),mm=n(Ta," repr\xE9sente un seul morceau de texte contigu, nous it\xE9rons d\u2019abord sur le jeu de donn\xE9es avant de donner le batch au collateur. Nous supprimons la cl\xE9 "),br=o(Ta,"CODE",{});var av=r(br);_m=n(av,'"word_ids"'),av.forEach(t),fm=n(Ta," pour ce collateur de donn\xE9es car il ne l\u2019attend pas :"),Ta.forEach(t),tu=_(e),E(En.$$.fragment,e),au=_(e),E(xn.$$.fragment,e),nu=_(e),ys=o(e,"P",{});var Rt=r(ys);hm=n(Rt,"Super, \xE7a a march\xE9 ! Nous pouvons voir que le "),kr=o(Rt,"EM",{});var nv=r(kr);vm=n(nv,"token"),nv.forEach(t),gm=_(Rt),qr=o(Rt,"CODE",{});var lv=r(qr);bm=n(lv,"[MASK]"),lv.forEach(t),km=n(Rt," a \xE9t\xE9 ins\xE9r\xE9 de fa\xE7on al\xE9atoire \xE0 diff\xE9rents endroits dans notre texte. Ce seront les "),$r=o(Rt,"EM",{});var ov=r($r);qm=n(ov,"tokens"),ov.forEach(t),$m=n(Rt," que notre mod\xE8le devra pr\xE9dire pendant l\u2019entra\xEEnement. Et la beaut\xE9 du collecteur de donn\xE9es est qu\u2019il va rendre al\xE9atoire l\u2019insertion du "),wr=o(Rt,"CODE",{});var rv=r(wr);wm=n(rv,"[MASK]"),rv.forEach(t),Em=n(Rt," \xE0 chaque batch !"),Rt.forEach(t),lu=_(e),E(xa.$$.fragment,e),ou=_(e),hs&&hs.l(e),wl=_(e),zs=o(e,"P",{});var Bt=r(zs);xm=n(Bt,"Lors de l\u2019entra\xEEnement des mod\xE8les pour la mod\xE9lisation du langage masqu\xE9, une technique qui peut \xEAtre utilis\xE9e est de masquer des mots entiers ensemble et pas seulement des "),Er=o(Bt,"EM",{});var iv=r(Er);jm=n(iv,"tokens"),iv.forEach(t),ym=n(Bt," individuels. Cette approche est appel\xE9e "),xr=o(Bt,"EM",{});var uv=r(xr);zm=n(uv,"masquage de mots entiers"),uv.forEach(t),Mm=n(Bt,". Si nous voulons utiliser le masquage de mots entiers, nous devons construire nous-m\xEAmes un collateur de donn\xE9es. Un collateur de donn\xE9es est simplement une fonction qui prend une liste d\u2019\xE9chantillons et les convertit en un batch. Faisons-le ! Nous utiliserons les identifiants des mots calcul\xE9s plus t\xF4t pour faire une correspondance entre les indices des mots et les "),jr=o(Bt,"EM",{});var pv=r(jr);Cm=n(pv,"tokens"),pv.forEach(t),Dm=n(Bt,", puis nous d\xE9ciderons al\xE9atoirement quels mots masquer et appliquerons ce masque sur les entr\xE9es. Notez que les \xE9tiquettes sont toutes "),yr=o(Bt,"CODE",{});var dv=r(yr);Pm=n(dv,"-100"),dv.forEach(t),Am=n(Bt," sauf celles qui correspondent aux mots masqu\xE9s."),Bt.forEach(t),ru=_(e),gt.l(e),El=_(e),xl=o(e,"P",{});var cv=r(xl);Sm=n(cv,"Ensuite, nous pouvons l\u2019essayer sur les m\xEAmes \xE9chantillons que pr\xE9c\xE9demment :"),cv.forEach(t),iu=_(e),E(jn.$$.fragment,e),uu=_(e),E(yn.$$.fragment,e),pu=_(e),E(ja.$$.fragment,e),du=_(e),Ms=o(e,"P",{});var Ft=r(Ms);Tm=n(Ft,"Maintenant que nous avons deux collateurs de donn\xE9es, les \xE9tapes restantes du "),zr=o(Ft,"EM",{});var mv=r(zr);Nm=n(mv,"finetuning"),mv.forEach(t),Om=n(Ft," sont standards. L\u2019entra\xEEnement peut prendre un certain temps sur Google Colab si vous n\u2019avez pas la chance de tomber sur un mythique GPU P100 \u{1F62D}. Ainsi nous allons d\u2019abord r\xE9duire la taille du jeu d\u2019entra\xEEnement \xE0 quelques milliers d\u2019exemples. Ne vous inqui\xE9tez pas, nous obtiendrons quand m\xEAme un mod\xE8le de langage assez d\xE9cent ! Un moyen rapide de r\xE9duire la taille d\u2019un jeu de donn\xE9es dans \u{1F917} "),Mr=o(Ft,"EM",{});var _v=r(Mr);Lm=n(_v,"Datasets"),_v.forEach(t),Km=n(Ft," est la fonction "),Cr=o(Ft,"CODE",{});var fv=r(Cr);Rm=n(fv,"Dataset.train_test_split()"),fv.forEach(t),Bm=n(Ft," que nous avons vue au "),jl=o(Ft,"A",{href:!0});var hv=r(jl);Fm=n(hv,"chapitre 5"),hv.forEach(t),Hm=n(Ft," :"),Ft.forEach(t),cu=_(e),E(zn.$$.fragment,e),mu=_(e),E(Mn.$$.fragment,e),_u=_(e),Cs=o(e,"P",{});var Ht=r(Cs);Im=n(Ht,"Cela a automatiquement cr\xE9\xE9 de nouvelles divisions "),Dr=o(Ht,"CODE",{});var vv=r(Dr);Um=n(vv,"train"),vv.forEach(t),Gm=n(Ht," et "),Pr=o(Ht,"CODE",{});var gv=r(Pr);Vm=n(gv,"test"),gv.forEach(t),Wm=n(Ht," avec la taille du jeu d\u2019entra\xEEnement fix\xE9e \xE0 10.000 exemples et la validation fix\xE9e \xE0 10% de cela. N\u2019h\xE9sitez pas \xE0 augmenter la taille si vous avez un GPU puissant ! La prochaine chose que nous devons faire est de nous connecter au "),Ar=o(Ht,"EM",{});var bv=r(Ar);Jm=n(bv,"Hub"),bv.forEach(t),Ym=n(Ht,". Si vous ex\xE9cutez ce code dans un "),Sr=o(Ht,"EM",{});var kv=r(Sr);Qm=n(kv,"notebook"),kv.forEach(t),Xm=n(Ht,", vous pouvez le faire avec la fonction suivante :"),Ht.forEach(t),fu=_(e),E(Cn.$$.fragment,e),hu=_(e),ya=o(e,"P",{});var ep=r(ya);Zm=n(ep,"qui affichera un "),Tr=o(ep,"EM",{});var qv=r(Tr);e_=n(qv,"widget"),qv.forEach(t),s_=n(ep," o\xF9 vous pourrez saisir vos informations d\u2019identification. Alternativement, vous pouvez ex\xE9cuter :"),ep.forEach(t),vu=_(e),E(Dn.$$.fragment,e),gu=_(e),yl=o(e,"P",{});var $v=r(yl);t_=n($v,"dans votre terminal pr\xE9f\xE9r\xE9 et connectez-vous l\xE0."),$v.forEach(t),bu=_(e),kt.l(e),zl=_(e),ea=o(e,"H3",{class:!0});var sp=r(ea);za=o(sp,"A",{id:!0,class:!0,href:!0});var wv=r(za);Nr=o(wv,"SPAN",{});var Ev=r(Nr);E(Pn.$$.fragment,Ev),Ev.forEach(t),wv.forEach(t),a_=_(sp),Or=o(sp,"SPAN",{});var xv=r(Or);n_=n(xv,"Perplexit\xE9 pour les mod\xE8les de langage"),xv.forEach(t),sp.forEach(t),ku=_(e),E(An.$$.fragment,e),qu=_(e),Ml=o(e,"P",{});var jv=r(Ml);l_=n(jv,"Contrairement \xE0 d\u2019autres t\xE2ches, comme la classification de textes ou la r\xE9ponse \xE0 des questions, sur lesquelles nous disposons d\u2019un corpus \xE9tiquet\xE9 pour entra\xEEner, la mod\xE9lisation du langage ne s\u2019appuie sur aucune \xE9tiquette explicite. Alors comment d\xE9terminer ce qui fait un bon mod\xE8le de langage ? Comme pour la fonction de correction automatique de votre t\xE9l\xE9phone, un bon mod\xE8le de langage est celui qui attribue des probabilit\xE9s \xE9lev\xE9es aux phrases grammaticalement correctes et des probabilit\xE9s faibles aux phrases absurdes. Pour vous donner une meilleure id\xE9e de ce \xE0 quoi cela ressemble, vous pouvez trouver en ligne des s\xE9ries enti\xE8res de \xAB rat\xE9s d\u2019autocorrection \xBB o\xF9 le mod\xE8le d\u2019un t\xE9l\xE9phone produit des compl\xE9ments plut\xF4t amusants (et souvent inappropri\xE9s) !"),jv.forEach(t),$u=_(e),$t.l(e),Cl=_(e),E(Sn.$$.fragment,e),wu=_(e),Dl=o(e,"P",{});var yv=r(Dl);o_=n(yv,"Un score de perplexit\xE9 faible signifie un meilleur mod\xE8le de langue. Nous pouvons voir ici que notre mod\xE8le de d\xE9part a une valeur assez \xE9lev\xE9e. Voyons si nous pouvons la r\xE9duire en l\u2019affinant ! Pour ce faire, nous commen\xE7ons par ex\xE9cuter la boucle d\u2019entra\xEEnement :"),yv.forEach(t),Eu=_(e),Et.l(e),Pl=_(e),Al=o(e,"P",{});var zv=r(Al);r_=n(zv,"et ensuite calculer la perplexit\xE9 r\xE9sultante sur l\u2019ensemble de test comme pr\xE9c\xE9demment :"),zv.forEach(t),xu=_(e),jt.l(e),Sl=_(e),E(Tn.$$.fragment,e),ju=_(e),Tl=o(e,"P",{});var Mv=r(Tl);i_=n(Mv,"Joli. C\u2019est une r\xE9duction consid\xE9rable de la perplexit\xE9, ce qui nous indique que le mod\xE8le a appris quelque chose sur le domaine des critiques de films !"),Mv.forEach(t),yu=_(e),Le&&Le.l(e),Nl=_(e),E(Ma.$$.fragment,e),zu=_(e),Ke&&Ke.l(e),Ol=_(e),sa=o(e,"H3",{class:!0});var tp=r(sa);Ca=o(tp,"A",{id:!0,class:!0,href:!0});var Cv=r(Ca);Lr=o(Cv,"SPAN",{});var Dv=r(Lr);E(Nn.$$.fragment,Dv),Dv.forEach(t),Cv.forEach(t),u_=_(tp),Ll=o(tp,"SPAN",{});var P_=r(Ll);p_=n(P_,"Utilisation de notre mod\xE8le "),Kr=o(P_,"I",{});var Pv=r(Kr);d_=n(Pv,"finetun\xE9"),Pv.forEach(t),P_.forEach(t),tp.forEach(t),Mu=_(e),Ge=o(e,"P",{});var Ds=r(Ge);c_=n(Ds,"Vous pouvez interagir avec votre mod\xE8le "),Rr=o(Ds,"EM",{});var Av=r(Rr);m_=n(Av,"finetun\xE9"),Av.forEach(t),__=n(Ds," soit en utilisant son "),Br=o(Ds,"EM",{});var Sv=r(Br);f_=n(Sv,"widget"),Sv.forEach(t),h_=n(Ds," sur le "),Fr=o(Ds,"EM",{});var Tv=r(Fr);v_=n(Tv,"Hub"),Tv.forEach(t),g_=n(Ds,", soit localement avec le "),Hr=o(Ds,"CODE",{});var Nv=r(Hr);b_=n(Nv,"pipeline"),Nv.forEach(t),k_=n(Ds," de \u{1F917} "),Ir=o(Ds,"EM",{});var Ov=r(Ir);q_=n(Ov,"Transformers"),Ov.forEach(t),$_=n(Ds,". Utilisons ce dernier pour t\xE9l\xE9charger notre mod\xE8le en utilisant le pipeline "),Ur=o(Ds,"CODE",{});var Lv=r(Ur);w_=n(Lv,"fill-mask"),Lv.forEach(t),E_=n(Ds," :"),Ds.forEach(t),Cu=_(e),E(On.$$.fragment,e),Du=_(e),Kl=o(e,"P",{});var Kv=r(Kl);x_=n(Kv,"Nous pouvons ensuite donner au pipeline notre exemple de texte \xAB this is a great [MASK] \xBB et voir quelles sont les 5 premi\xE8res pr\xE9dictions :"),Kv.forEach(t),Pu=_(e),E(Ln.$$.fragment,e),Au=_(e),E(Kn.$$.fragment,e),Su=_(e),Rl=o(e,"P",{});var Rv=r(Rl);j_=n(Rv,"Notre mod\xE8le a clairement adapt\xE9 ses pond\xE9rations pour pr\xE9dire les mots qui sont plus fortement associ\xE9s aux films !"),Rv.forEach(t),Tu=_(e),E(Rn.$$.fragment,e),Nu=_(e),Ot=o(e,"P",{});var Zl=r(Ot);y_=n(Zl,"Ceci conclut notre premi\xE8re exp\xE9rience d\u2019entra\xEEnement d\u2019un mod\xE8le de langage. Dans la "),Bl=o(Zl,"A",{href:!0});var Bv=r(Bl);z_=n(Bv,"section 6"),Bv.forEach(t),M_=n(Zl,", vous apprendrez comment entra\xEEner \xE0 partir de z\xE9ro un mod\xE8le autor\xE9gressif comme GPT-2. Allez-y si vous voulez voir comment vous pouvez pr\xE9-entra\xEEner votre propre "),Gr=o(Zl,"EM",{});var Fv=r(Gr);C_=n(Fv,"transformer"),Fv.forEach(t),D_=n(Zl," !"),Zl.forEach(t),Ou=_(e),E(Da.$$.fragment,e),this.h()},h(){y(i,"name","hf:doc:metadata"),y(i,"content",JSON.stringify(Eg)),y(q,"id","ifinetuneri-un-modle-de-langage-masqu"),y(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(q,"href","#ifinetuneri-un-modle-de-langage-masqu"),y(A,"class","relative group"),y(Ee,"href","https://arxiv.org/abs/1801.06146"),y(Ee,"rel","nofollow"),y(Oe,"class","block dark:hidden"),Xr(Oe.src,Be="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg")||y(Oe,"src",Be),y(Oe,"alt","ULMFiT."),y(Y,"class","hidden dark:block"),Xr(Y.src,Bs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg")||y(Y,"src",Bs),y(Y,"alt","ULMFiT."),y(Me,"class","flex justify-center"),y(Fe,"href","https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D."),y(Fe,"rel","nofollow"),Xr(Pe.src,yt="https://hf.space/gradioiframe/course-demos/distilbert-base-uncased-finetuned-imdb/+")||y(Pe,"src",yt),y(Pe,"frameborder","0"),y(Pe,"height","300"),y(Pe,"title","Gradio app"),y(Pe,"class","block dark:hidden container p-0 flex-grow space-iframe"),y(Pe,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),y(Pe,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Xr(xe.src,Ie="https://hf.space/gradioiframe/course-demos/distilbert-base-uncased-finetuned-imdb-darkmode/+")||y(xe,"src",Ie),y(xe,"frameborder","0"),y(xe,"height","300"),y(xe,"title","Gradio app"),y(xe,"class","hidden dark:block container p-0 flex-grow space-iframe"),y(xe,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),y(xe,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),y(Ts,"id","choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu"),y(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ts,"href","#choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu"),y(Ss,"class","relative group"),y(_s,"href","https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads"),y(_s,"rel","nofollow"),Xr(Os.src,Qn="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png")||y(Os,"src",Qn),y(Os,"alt","Hub models."),y(Os,"width","80%"),y(ut,"class","flex justify-center"),y(pt,"href","https://huggingface.co/distilbert-base-uncased"),y(pt,"rel","nofollow"),y(Us,"href","https://en.wikipedia.org/wiki/Knowledge_distillation"),y(Us,"rel","nofollow"),y(Vs,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/ch05.html"),y(Vs,"rel","nofollow"),y(Dt,"href","https://huggingface.co/datasets/wikipedia"),y(Dt,"rel","nofollow"),y(At,"href","https://huggingface.co/datasets/bookcorpus"),y(At,"rel","nofollow"),y(_a,"id","le-jeu-de-donnes"),y(_a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(_a,"href","#le-jeu-de-donnes"),y(Qt,"class","relative group"),y(Va,"href","https://huggingface.co/datasets/imdb"),y(Va,"rel","nofollow"),y(hl,"href","/course/fr/chapter3"),y(va,"id","prtraitement-des-donnes"),y(va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(va,"href","#prtraitement-des-donnes"),y(Xt,"class","relative group"),y(vl,"href","/course/fr/chapter6/3"),y(Ea,"id","ifinetuningi-de-distilbert-avec-lapi-trainer"),y(Ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ea,"href","#ifinetuningi-de-distilbert-avec-lapi-trainer"),y(Zt,"class","relative group"),y($l,"href","/course/fr/chapter3"),y(jl,"href","/course/fr/chapter5"),y(za,"id","perplexit-pour-les-modles-de-langage"),y(za,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(za,"href","#perplexit-pour-les-modles-de-langage"),y(ea,"class","relative group"),y(Ca,"id","utilisation-de-notre-modle-ifinetuni"),y(Ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ca,"href","#utilisation-de-notre-modle-ifinetuni"),y(sa,"class","relative group"),y(Bl,"href","/course/fr/chapter7/section6")},m(e,u){s(document.head,i),p(e,h,u),x(d,e,u),p(e,$,u),p(e,A,u),s(A,q),s(q,k),x(z,k,null),s(A,f),s(A,M),s(M,O),s(O,C),s(M,L),p(e,I,u),Bn[P].m(e,u),p(e,U,u),p(e,K,u),s(K,v),s(K,N),s(N,W),s(K,G),s(K,J),s(J,ee),s(K,re),s(K,te),s(te,V),s(K,R),s(K,ie),s(ie,Q),s(K,se),p(e,ue,u),p(e,Z,u),s(Z,ge),s(Z,ne),s(ne,de),s(Z,rs),s(Z,be),s(be,ke),s(Z,is),s(Z,Ve),s(Ve,qe),s(Z,us),s(Z,We),s(We,je),s(Z,Ne),p(e,Je,u),p(e,X,u),s(X,pe),s(X,ye),s(ye,ce),s(X,ps),s(X,ze),s(ze,$e),s(X,Xe),s(X,le),s(le,Ze),s(X,we),s(X,Ee),s(Ee,B),s(X,ae),s(X,Re),s(Re,me),s(X,es),p(e,Ye,u),p(e,Me,u),s(Me,Oe),s(Me,T),s(Me,Y),p(e,oe,u),p(e,Ce,u),s(Ce,Ps),s(Ce,Fe),s(Fe,Qe),s(Ce,He),s(Ce,De),s(De,fe),s(Ce,Fs),p(e,nt,u),p(e,Pe,u),p(e,ds,u),p(e,xe,u),p(e,lt,u),p(e,cs,u),s(cs,gs),p(e,ot,u),x(ms,e,u),p(e,Hs,u),x(As,e,u),p(e,rt,u),p(e,Ss,u),s(Ss,Ts),s(Ts,Ns),x(Is,Ns,null),s(Ss,bs),s(Ss,Gt),s(Gt,Vt),p(e,ta,u),p(e,ks,u),s(ks,Wt),s(ks,Jt),s(Jt,La),s(ks,aa),s(ks,_s),s(_s,zt),s(zt,it),s(ks,na),p(e,fs,u),p(e,ut,u),s(ut,Os),p(e,la,u),p(e,ve,u),s(ve,Ka),s(ve,pt),s(pt,oa),s(ve,dt),s(ve,Us),s(Us,Gs),s(Gs,Ra),s(ve,ra),s(ve,Ls),s(Ls,ia),s(ve,ct),s(ve,Mt),s(Mt,Ct),s(ve,Ba),s(ve,Vs),s(Vs,c),s(c,F),s(ve,Xn),p(e,Yt,u),Fn[qs].m(e,u),p(e,mt,u),p(e,_t,u),s(_t,Zn),s(_t,ws),s(ws,el),s(_t,sl),p(e,Fa,u),x(Ue,e,u),p(e,Ha,u),p(e,he,u),s(he,tl),s(he,ua),s(ua,pa),s(he,al),s(he,Es),s(Es,nl),s(he,ll),s(he,Dt),s(Dt,Pt),s(Pt,ol),s(he,da),s(he,At),s(At,St),s(St,rl),s(he,il),s(he,Ws),s(Ws,ul),s(he,pl),s(he,Js),s(Js,dl),s(he,cl),s(he,ca),s(ca,ml),s(he,np),p(e,Zr,u),x(Ia,e,u),p(e,ei,u),p(e,ma,u),s(ma,lp),s(ma,eo),s(eo,op),s(ma,rp),p(e,si,u),Hn[ft].m(e,u),p(e,_l,u),x(Ua,e,u),p(e,ti,u),p(e,fl,u),s(fl,ip),p(e,ai,u),p(e,Qt,u),s(Qt,_a),s(_a,so),x(Ga,so,null),s(Qt,up),s(Qt,to),s(to,pp),p(e,ni,u),p(e,ss,u),s(ss,dp),s(ss,Va),s(Va,ao),s(ao,cp),s(ss,mp),s(ss,no),s(no,_p),s(ss,fp),s(ss,lo),s(lo,hp),s(ss,vp),s(ss,oo),s(oo,gp),s(ss,bp),s(ss,ro),s(ro,kp),s(ss,qp),p(e,li,u),x(Wa,e,u),p(e,oi,u),x(Ja,e,u),p(e,ri,u),p(e,ts,u),s(ts,$p),s(ts,io),s(io,wp),s(ts,Ep),s(ts,uo),s(uo,xp),s(ts,jp),s(ts,po),s(po,yp),s(ts,zp),s(ts,co),s(co,Mp),s(ts,Cp),s(ts,mo),s(mo,Dp),s(ts,Pp),p(e,ii,u),x(Ya,e,u),p(e,ui,u),x(Qa,e,u),p(e,pi,u),p(e,Tt,u),s(Tt,Ap),s(Tt,_o),s(_o,Sp),s(Tt,Tp),s(Tt,fo),s(fo,Np),s(Tt,Op),p(e,di,u),x(fa,e,u),p(e,ci,u),p(e,ha,u),s(ha,Lp),s(ha,hl),s(hl,Kp),s(ha,Rp),p(e,mi,u),p(e,Xt,u),s(Xt,va),s(va,ho),x(Xa,ho,null),s(Xt,Bp),s(Xt,vo),s(vo,Fp),p(e,_i,u),x(Za,e,u),p(e,fi,u),p(e,ga,u),s(ga,Hp),s(ga,go),s(go,Ip),s(ga,Up),p(e,hi,u),p(e,Ae,u),s(Ae,Gp),s(Ae,bo),s(bo,Vp),s(Ae,Wp),s(Ae,ko),s(ko,Jp),s(Ae,Yp),s(Ae,qo),s(qo,Qp),s(Ae,Xp),s(Ae,$o),s($o,Zp),s(Ae,ed),s(Ae,vl),s(vl,sd),s(Ae,td),s(Ae,wo),s(wo,ad),s(Ae,nd),s(Ae,Eo),s(Eo,ld),s(Ae,od),p(e,vi,u),x(en,e,u),p(e,gi,u),x(sn,e,u),p(e,bi,u),p(e,Ys,u),s(Ys,rd),s(Ys,xo),s(xo,id),s(Ys,ud),s(Ys,jo),s(jo,pd),s(Ys,dd),s(Ys,yo),s(yo,cd),s(Ys,md),p(e,ki,u),p(e,Qs,u),s(Qs,_d),s(Qs,zo),s(zo,fd),s(Qs,hd),s(Qs,Mo),s(Mo,vd),s(Qs,gd),s(Qs,Co),s(Co,bd),s(Qs,kd),p(e,qi,u),x(tn,e,u),p(e,$i,u),x(an,e,u),p(e,wi,u),p(e,Xs,u),s(Xs,qd),s(Xs,Do),s(Do,$d),s(Xs,wd),s(Xs,Po),s(Po,Ed),s(Xs,xd),s(Xs,Ao),s(Ao,jd),s(Xs,yd),p(e,Ei,u),x(ba,e,u),p(e,xi,u),p(e,gl,u),s(gl,zd),p(e,ji,u),x(nn,e,u),p(e,yi,u),x(ka,e,u),p(e,zi,u),p(e,qa,u),s(qa,Md),s(qa,So),s(So,Cd),s(qa,Dd),p(e,Mi,u),x(ln,e,u),p(e,Ci,u),x(on,e,u),p(e,Di,u),p(e,bl,u),s(bl,Pd),p(e,Pi,u),x(rn,e,u),p(e,Ai,u),x(un,e,u),p(e,Si,u),p(e,xs,u),s(xs,Ad),s(xs,To),s(To,Sd),s(xs,Td),s(xs,No),s(No,Nd),s(xs,Od),s(xs,Oo),s(Oo,Ld),s(xs,Kd),s(xs,Lo),s(Lo,Rd),s(xs,Bd),p(e,Ti,u),x(pn,e,u),p(e,Ni,u),x(dn,e,u),p(e,Oi,u),p(e,$a,u),s($a,Fd),s($a,Ko),s(Ko,Hd),s($a,Id),p(e,Li,u),p(e,wa,u),s(wa,cn),s(cn,Ud),s(cn,Ro),s(Ro,Gd),s(cn,Vd),s(wa,Wd),s(wa,mn),s(mn,Jd),s(mn,Bo),s(Bo,Yd),s(mn,Qd),p(e,Ki,u),p(e,kl,u),s(kl,Xd),p(e,Ri,u),x(_n,e,u),p(e,Bi,u),p(e,as,u),s(as,Zd),s(as,Fo),s(Fo,ec),s(as,sc),s(as,Ho),s(Ho,tc),s(as,ac),s(as,Io),s(Io,nc),s(as,lc),s(as,Uo),s(Uo,oc),s(as,rc),s(as,Go),s(Go,ic),s(as,uc),p(e,Fi,u),p(e,Nt,u),s(Nt,pc),s(Nt,Vo),s(Vo,dc),s(Nt,cc),s(Nt,Wo),s(Wo,mc),s(Nt,_c),p(e,Hi,u),x(fn,e,u),p(e,Ii,u),x(hn,e,u),p(e,Ui,u),p(e,Se,u),s(Se,fc),s(Se,Jo),s(Jo,hc),s(Se,vc),s(Se,Yo),s(Yo,gc),s(Se,bc),s(Se,Qo),s(Qo,kc),s(Se,qc),s(Se,Xo),s(Xo,$c),s(Se,wc),s(Se,Zo),s(Zo,Ec),s(Se,xc),s(Se,er),s(er,jc),s(Se,yc),s(Se,sr),s(sr,zc),s(Se,Mc),p(e,Gi,u),x(vn,e,u),p(e,Vi,u),x(gn,e,u),p(e,Wi,u),p(e,ql,u),s(ql,Cc),p(e,Ji,u),x(bn,e,u),p(e,Yi,u),x(kn,e,u),p(e,Qi,u),p(e,js,u),s(js,Dc),s(js,tr),s(tr,Pc),s(js,Ac),s(js,ar),s(ar,Sc),s(js,Tc),s(js,nr),s(nr,Nc),s(js,Oc),s(js,lr),s(lr,Lc),s(js,Kc),p(e,Xi,u),p(e,Zt,u),s(Zt,Ea),s(Ea,or),x(qn,or,null),s(Zt,Rc),s(Zt,$n),s($n,rr),s(rr,Bc),s($n,Fc),s($n,ir),s(ir,Hc),p(e,Zi,u),p(e,_e,u),s(_e,Ic),s(_e,ur),s(ur,Uc),s(_e,Gc),s(_e,pr),s(pr,Vc),s(_e,Wc),s(_e,$l),s($l,Jc),s(_e,Yc),s(_e,dr),s(dr,Qc),s(_e,Xc),s(_e,cr),s(cr,Zc),s(_e,em),s(_e,mr),s(mr,sm),s(_e,tm),s(_e,_r),s(_r,am),s(_e,nm),s(_e,fr),s(fr,lm),s(_e,om),s(_e,hr),s(hr,rm),s(_e,im),p(e,eu,u),x(wn,e,u),p(e,su,u),p(e,Zs,u),s(Zs,um),s(Zs,vr),s(vr,pm),s(Zs,dm),s(Zs,gr),s(gr,cm),s(Zs,mm),s(Zs,br),s(br,_m),s(Zs,fm),p(e,tu,u),x(En,e,u),p(e,au,u),x(xn,e,u),p(e,nu,u),p(e,ys,u),s(ys,hm),s(ys,kr),s(kr,vm),s(ys,gm),s(ys,qr),s(qr,bm),s(ys,km),s(ys,$r),s($r,qm),s(ys,$m),s(ys,wr),s(wr,wm),s(ys,Em),p(e,lu,u),x(xa,e,u),p(e,ou,u),hs&&hs.m(e,u),p(e,wl,u),p(e,zs,u),s(zs,xm),s(zs,Er),s(Er,jm),s(zs,ym),s(zs,xr),s(xr,zm),s(zs,Mm),s(zs,jr),s(jr,Cm),s(zs,Dm),s(zs,yr),s(yr,Pm),s(zs,Am),p(e,ru,u),In[vt].m(e,u),p(e,El,u),p(e,xl,u),s(xl,Sm),p(e,iu,u),x(jn,e,u),p(e,uu,u),x(yn,e,u),p(e,pu,u),x(ja,e,u),p(e,du,u),p(e,Ms,u),s(Ms,Tm),s(Ms,zr),s(zr,Nm),s(Ms,Om),s(Ms,Mr),s(Mr,Lm),s(Ms,Km),s(Ms,Cr),s(Cr,Rm),s(Ms,Bm),s(Ms,jl),s(jl,Fm),s(Ms,Hm),p(e,cu,u),x(zn,e,u),p(e,mu,u),x(Mn,e,u),p(e,_u,u),p(e,Cs,u),s(Cs,Im),s(Cs,Dr),s(Dr,Um),s(Cs,Gm),s(Cs,Pr),s(Pr,Vm),s(Cs,Wm),s(Cs,Ar),s(Ar,Jm),s(Cs,Ym),s(Cs,Sr),s(Sr,Qm),s(Cs,Xm),p(e,fu,u),x(Cn,e,u),p(e,hu,u),p(e,ya,u),s(ya,Zm),s(ya,Tr),s(Tr,e_),s(ya,s_),p(e,vu,u),x(Dn,e,u),p(e,gu,u),p(e,yl,u),s(yl,t_),p(e,bu,u),Un[bt].m(e,u),p(e,zl,u),p(e,ea,u),s(ea,za),s(za,Nr),x(Pn,Nr,null),s(ea,a_),s(ea,Or),s(Or,n_),p(e,ku,u),x(An,e,u),p(e,qu,u),p(e,Ml,u),s(Ml,l_),p(e,$u,u),Gn[qt].m(e,u),p(e,Cl,u),x(Sn,e,u),p(e,wu,u),p(e,Dl,u),s(Dl,o_),p(e,Eu,u),Vn[wt].m(e,u),p(e,Pl,u),p(e,Al,u),s(Al,r_),p(e,xu,u),Wn[xt].m(e,u),p(e,Sl,u),x(Tn,e,u),p(e,ju,u),p(e,Tl,u),s(Tl,i_),p(e,yu,u),Le&&Le.m(e,u),p(e,Nl,u),x(Ma,e,u),p(e,zu,u),Ke&&Ke.m(e,u),p(e,Ol,u),p(e,sa,u),s(sa,Ca),s(Ca,Lr),x(Nn,Lr,null),s(sa,u_),s(sa,Ll),s(Ll,p_),s(Ll,Kr),s(Kr,d_),p(e,Mu,u),p(e,Ge,u),s(Ge,c_),s(Ge,Rr),s(Rr,m_),s(Ge,__),s(Ge,Br),s(Br,f_),s(Ge,h_),s(Ge,Fr),s(Fr,v_),s(Ge,g_),s(Ge,Hr),s(Hr,b_),s(Ge,k_),s(Ge,Ir),s(Ir,q_),s(Ge,$_),s(Ge,Ur),s(Ur,w_),s(Ge,E_),p(e,Cu,u),x(On,e,u),p(e,Du,u),p(e,Kl,u),s(Kl,x_),p(e,Pu,u),x(Ln,e,u),p(e,Au,u),x(Kn,e,u),p(e,Su,u),p(e,Rl,u),s(Rl,j_),p(e,Tu,u),x(Rn,e,u),p(e,Nu,u),p(e,Ot,u),s(Ot,y_),s(Ot,Bl),s(Bl,z_),s(Ot,M_),s(Ot,Gr),s(Gr,C_),s(Ot,D_),p(e,Ou,u),x(Da,e,u),Lu=!0},p(e,[u]){const Jn={};u&1&&(Jn.fw=e[0]),d.$set(Jn);let Fl=P;P=S_(e),P!==Fl&&(Ut(),b(Bn[Fl],1,1,()=>{Bn[Fl]=null}),It(),D=Bn[P],D||(D=Bn[P]=A_[P](e),D.c()),g(D,1),D.m(U.parentNode,U));const Vr={};u&2&&(Vr.$$scope={dirty:u,ctx:e}),As.$set(Vr);let Yn=qs;qs=N_(e),qs!==Yn&&(Ut(),b(Fn[Yn],1,1,()=>{Fn[Yn]=null}),It(),$s=Fn[qs],$s||($s=Fn[qs]=T_[qs](e),$s.c()),g($s,1),$s.m(mt.parentNode,mt));let Hl=ft;ft=L_(e),ft!==Hl&&(Ut(),b(Hn[Hl],1,1,()=>{Hn[Hl]=null}),It(),ht=Hn[ft],ht||(ht=Hn[ft]=O_[ft](e),ht.c()),g(ht,1),ht.m(_l.parentNode,_l));const Ks={};u&2&&(Ks.$$scope={dirty:u,ctx:e}),fa.$set(Ks);const Wr={};u&2&&(Wr.$$scope={dirty:u,ctx:e}),ba.$set(Wr);const Jr={};u&2&&(Jr.$$scope={dirty:u,ctx:e}),ka.$set(Jr);const Yr={};u&2&&(Yr.$$scope={dirty:u,ctx:e}),xa.$set(Yr),e[0]==="pt"?hs||(hs=Hv(),hs.c(),hs.m(wl.parentNode,wl)):hs&&(hs.d(1),hs=null);let Il=vt;vt=R_(e),vt!==Il&&(Ut(),b(In[Il],1,1,()=>{In[Il]=null}),It(),gt=In[vt],gt||(gt=In[vt]=K_[vt](e),gt.c()),g(gt,1),gt.m(El.parentNode,El));const Rs={};u&2&&(Rs.$$scope={dirty:u,ctx:e}),ja.$set(Rs);let Ul=bt;bt=F_(e),bt!==Ul&&(Ut(),b(Un[Ul],1,1,()=>{Un[Ul]=null}),It(),kt=Un[bt],kt||(kt=Un[bt]=B_[bt](e),kt.c()),g(kt,1),kt.m(zl.parentNode,zl));let Gl=qt;qt=I_(e),qt!==Gl&&(Ut(),b(Gn[Gl],1,1,()=>{Gn[Gl]=null}),It(),$t=Gn[qt],$t||($t=Gn[qt]=H_[qt](e),$t.c()),g($t,1),$t.m(Cl.parentNode,Cl));let Vl=wt;wt=G_(e),wt!==Vl&&(Ut(),b(Vn[Vl],1,1,()=>{Vn[Vl]=null}),It(),Et=Vn[wt],Et||(Et=Vn[wt]=U_[wt](e),Et.c()),g(Et,1),Et.m(Pl.parentNode,Pl));let Wl=xt;xt=W_(e),xt!==Wl&&(Ut(),b(Wn[Wl],1,1,()=>{Wn[Wl]=null}),It(),jt=Wn[xt],jt||(jt=Wn[xt]=V_[xt](e),jt.c()),g(jt,1),jt.m(Sl.parentNode,Sl)),e[0]==="pt"?Le?u&1&&g(Le,1):(Le=Iv(),Le.c(),g(Le,1),Le.m(Nl.parentNode,Nl)):Le&&(Ut(),b(Le,1,1,()=>{Le=null}),It());const vs={};u&2&&(vs.$$scope={dirty:u,ctx:e}),Ma.$set(vs),e[0]==="pt"?Ke?u&1&&g(Ke,1):(Ke=Uv(),Ke.c(),g(Ke,1),Ke.m(Ol.parentNode,Ol)):Ke&&(Ut(),b(Ke,1,1,()=>{Ke=null}),It());const Qr={};u&2&&(Qr.$$scope={dirty:u,ctx:e}),Da.$set(Qr)},i(e){Lu||(g(d.$$.fragment,e),g(z.$$.fragment,e),g(D),g(ms.$$.fragment,e),g(As.$$.fragment,e),g(Is.$$.fragment,e),g($s),g(Ue.$$.fragment,e),g(Ia.$$.fragment,e),g(ht),g(Ua.$$.fragment,e),g(Ga.$$.fragment,e),g(Wa.$$.fragment,e),g(Ja.$$.fragment,e),g(Ya.$$.fragment,e),g(Qa.$$.fragment,e),g(fa.$$.fragment,e),g(Xa.$$.fragment,e),g(Za.$$.fragment,e),g(en.$$.fragment,e),g(sn.$$.fragment,e),g(tn.$$.fragment,e),g(an.$$.fragment,e),g(ba.$$.fragment,e),g(nn.$$.fragment,e),g(ka.$$.fragment,e),g(ln.$$.fragment,e),g(on.$$.fragment,e),g(rn.$$.fragment,e),g(un.$$.fragment,e),g(pn.$$.fragment,e),g(dn.$$.fragment,e),g(_n.$$.fragment,e),g(fn.$$.fragment,e),g(hn.$$.fragment,e),g(vn.$$.fragment,e),g(gn.$$.fragment,e),g(bn.$$.fragment,e),g(kn.$$.fragment,e),g(qn.$$.fragment,e),g(wn.$$.fragment,e),g(En.$$.fragment,e),g(xn.$$.fragment,e),g(xa.$$.fragment,e),g(gt),g(jn.$$.fragment,e),g(yn.$$.fragment,e),g(ja.$$.fragment,e),g(zn.$$.fragment,e),g(Mn.$$.fragment,e),g(Cn.$$.fragment,e),g(Dn.$$.fragment,e),g(kt),g(Pn.$$.fragment,e),g(An.$$.fragment,e),g($t),g(Sn.$$.fragment,e),g(Et),g(jt),g(Tn.$$.fragment,e),g(Le),g(Ma.$$.fragment,e),g(Ke),g(Nn.$$.fragment,e),g(On.$$.fragment,e),g(Ln.$$.fragment,e),g(Kn.$$.fragment,e),g(Rn.$$.fragment,e),g(Da.$$.fragment,e),Lu=!0)},o(e){b(d.$$.fragment,e),b(z.$$.fragment,e),b(D),b(ms.$$.fragment,e),b(As.$$.fragment,e),b(Is.$$.fragment,e),b($s),b(Ue.$$.fragment,e),b(Ia.$$.fragment,e),b(ht),b(Ua.$$.fragment,e),b(Ga.$$.fragment,e),b(Wa.$$.fragment,e),b(Ja.$$.fragment,e),b(Ya.$$.fragment,e),b(Qa.$$.fragment,e),b(fa.$$.fragment,e),b(Xa.$$.fragment,e),b(Za.$$.fragment,e),b(en.$$.fragment,e),b(sn.$$.fragment,e),b(tn.$$.fragment,e),b(an.$$.fragment,e),b(ba.$$.fragment,e),b(nn.$$.fragment,e),b(ka.$$.fragment,e),b(ln.$$.fragment,e),b(on.$$.fragment,e),b(rn.$$.fragment,e),b(un.$$.fragment,e),b(pn.$$.fragment,e),b(dn.$$.fragment,e),b(_n.$$.fragment,e),b(fn.$$.fragment,e),b(hn.$$.fragment,e),b(vn.$$.fragment,e),b(gn.$$.fragment,e),b(bn.$$.fragment,e),b(kn.$$.fragment,e),b(qn.$$.fragment,e),b(wn.$$.fragment,e),b(En.$$.fragment,e),b(xn.$$.fragment,e),b(xa.$$.fragment,e),b(gt),b(jn.$$.fragment,e),b(yn.$$.fragment,e),b(ja.$$.fragment,e),b(zn.$$.fragment,e),b(Mn.$$.fragment,e),b(Cn.$$.fragment,e),b(Dn.$$.fragment,e),b(kt),b(Pn.$$.fragment,e),b(An.$$.fragment,e),b($t),b(Sn.$$.fragment,e),b(Et),b(jt),b(Tn.$$.fragment,e),b(Le),b(Ma.$$.fragment,e),b(Ke),b(Nn.$$.fragment,e),b(On.$$.fragment,e),b(Ln.$$.fragment,e),b(Kn.$$.fragment,e),b(Rn.$$.fragment,e),b(Da.$$.fragment,e),Lu=!1},d(e){t(i),e&&t(h),j(d,e),e&&t($),e&&t(A),j(z),e&&t(I),Bn[P].d(e),e&&t(U),e&&t(K),e&&t(ue),e&&t(Z),e&&t(Je),e&&t(X),e&&t(Ye),e&&t(Me),e&&t(oe),e&&t(Ce),e&&t(nt),e&&t(Pe),e&&t(ds),e&&t(xe),e&&t(lt),e&&t(cs),e&&t(ot),j(ms,e),e&&t(Hs),j(As,e),e&&t(rt),e&&t(Ss),j(Is),e&&t(ta),e&&t(ks),e&&t(fs),e&&t(ut),e&&t(la),e&&t(ve),e&&t(Yt),Fn[qs].d(e),e&&t(mt),e&&t(_t),e&&t(Fa),j(Ue,e),e&&t(Ha),e&&t(he),e&&t(Zr),j(Ia,e),e&&t(ei),e&&t(ma),e&&t(si),Hn[ft].d(e),e&&t(_l),j(Ua,e),e&&t(ti),e&&t(fl),e&&t(ai),e&&t(Qt),j(Ga),e&&t(ni),e&&t(ss),e&&t(li),j(Wa,e),e&&t(oi),j(Ja,e),e&&t(ri),e&&t(ts),e&&t(ii),j(Ya,e),e&&t(ui),j(Qa,e),e&&t(pi),e&&t(Tt),e&&t(di),j(fa,e),e&&t(ci),e&&t(ha),e&&t(mi),e&&t(Xt),j(Xa),e&&t(_i),j(Za,e),e&&t(fi),e&&t(ga),e&&t(hi),e&&t(Ae),e&&t(vi),j(en,e),e&&t(gi),j(sn,e),e&&t(bi),e&&t(Ys),e&&t(ki),e&&t(Qs),e&&t(qi),j(tn,e),e&&t($i),j(an,e),e&&t(wi),e&&t(Xs),e&&t(Ei),j(ba,e),e&&t(xi),e&&t(gl),e&&t(ji),j(nn,e),e&&t(yi),j(ka,e),e&&t(zi),e&&t(qa),e&&t(Mi),j(ln,e),e&&t(Ci),j(on,e),e&&t(Di),e&&t(bl),e&&t(Pi),j(rn,e),e&&t(Ai),j(un,e),e&&t(Si),e&&t(xs),e&&t(Ti),j(pn,e),e&&t(Ni),j(dn,e),e&&t(Oi),e&&t($a),e&&t(Li),e&&t(wa),e&&t(Ki),e&&t(kl),e&&t(Ri),j(_n,e),e&&t(Bi),e&&t(as),e&&t(Fi),e&&t(Nt),e&&t(Hi),j(fn,e),e&&t(Ii),j(hn,e),e&&t(Ui),e&&t(Se),e&&t(Gi),j(vn,e),e&&t(Vi),j(gn,e),e&&t(Wi),e&&t(ql),e&&t(Ji),j(bn,e),e&&t(Yi),j(kn,e),e&&t(Qi),e&&t(js),e&&t(Xi),e&&t(Zt),j(qn),e&&t(Zi),e&&t(_e),e&&t(eu),j(wn,e),e&&t(su),e&&t(Zs),e&&t(tu),j(En,e),e&&t(au),j(xn,e),e&&t(nu),e&&t(ys),e&&t(lu),j(xa,e),e&&t(ou),hs&&hs.d(e),e&&t(wl),e&&t(zs),e&&t(ru),In[vt].d(e),e&&t(El),e&&t(xl),e&&t(iu),j(jn,e),e&&t(uu),j(yn,e),e&&t(pu),j(ja,e),e&&t(du),e&&t(Ms),e&&t(cu),j(zn,e),e&&t(mu),j(Mn,e),e&&t(_u),e&&t(Cs),e&&t(fu),j(Cn,e),e&&t(hu),e&&t(ya),e&&t(vu),j(Dn,e),e&&t(gu),e&&t(yl),e&&t(bu),Un[bt].d(e),e&&t(zl),e&&t(ea),j(Pn),e&&t(ku),j(An,e),e&&t(qu),e&&t(Ml),e&&t($u),Gn[qt].d(e),e&&t(Cl),j(Sn,e),e&&t(wu),e&&t(Dl),e&&t(Eu),Vn[wt].d(e),e&&t(Pl),e&&t(Al),e&&t(xu),Wn[xt].d(e),e&&t(Sl),j(Tn,e),e&&t(ju),e&&t(Tl),e&&t(yu),Le&&Le.d(e),e&&t(Nl),j(Ma,e),e&&t(zu),Ke&&Ke.d(e),e&&t(Ol),e&&t(sa),j(Nn),e&&t(Mu),e&&t(Ge),e&&t(Cu),j(On,e),e&&t(Du),e&&t(Kl),e&&t(Pu),j(Ln,e),e&&t(Au),j(Kn,e),e&&t(Su),e&&t(Rl),e&&t(Tu),j(Rn,e),e&&t(Nu),e&&t(Ot),e&&t(Ou),j(Da,e)}}}const Eg={local:"ifinetuneri-un-modle-de-langage-masqu",sections:[{local:"choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu",title:"Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"},{local:"le-jeu-de-donnes",title:"Le jeu de donn\xE9es"},{local:"prtraitement-des-donnes",title:"Pr\xE9traitement des donn\xE9es"},{local:"ifinetuningi-de-distilbert-avec-lapi-trainer",sections:[{local:"perplexit-pour-les-modles-de-langage",title:"Perplexit\xE9 pour les mod\xE8les de langage"}],title:"<i>Finetuning</i> de DistilBERT avec l'API `Trainer`"},{local:"ifinetuningi-de-distilbert-avec-iacceleratei",sections:[{local:"utilisation-de-notre-modle-ifinetuni",title:"Utilisation de notre mod\xE8le <i>finetun\xE9</i>"}],title:"<i>Finetuning</i> de DistilBERT avec \u{1F917} <i>Accelerate</i>"}],title:"<i>Finetuner</i> un mod\xE8le de langage masqu\xE9"};function xg(H,i,h){let d="pt";return Qv(()=>{const $=new URLSearchParams(window.location.search);h(0,d=$.get("fw")||"pt")}),[d]}class Ag extends Vv{constructor(i){super();Wv(this,i,xg,wg,Jv,{})}}export{Ag as default,Eg as metadata};
