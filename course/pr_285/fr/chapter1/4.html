<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;comment-fonctionnent-les-itransformersi&quot;,&quot;sections&quot;:[{&quot;local&quot;:&quot;court-historique-des-itransformersi&quot;,&quot;title&quot;:&quot;Court historique des &lt;i&gt;transformers&lt;/i&gt;&quot;},{&quot;local&quot;:&quot;les-itransformersi-sont-des-modles-de-langage&quot;,&quot;title&quot;:&quot;Les &lt;i&gt;transformers&lt;/i&gt; sont des modèles de langage&quot;},{&quot;local&quot;:&quot;les-itransformersi-sont-normes&quot;,&quot;title&quot;:&quot;Les &lt;i&gt;transformers&lt;/i&gt; sont énormes&quot;},{&quot;local&quot;:&quot;lapprentissage-par-transfert&quot;,&quot;title&quot;:&quot;L&#39;apprentissage par transfert&quot;},{&quot;local&quot;:&quot;architecture-gnrale&quot;,&quot;title&quot;:&quot;Architecture générale&quot;},{&quot;local&quot;:&quot;introduction&quot;,&quot;title&quot;:&quot;Introduction&quot;},{&quot;local&quot;:&quot;les-couches-dattention&quot;,&quot;title&quot;:&quot;Les couches d&#39;attention&quot;},{&quot;local&quot;:&quot;larchitecture-originale&quot;,&quot;title&quot;:&quot;L&#39;architecture originale&quot;},{&quot;local&quot;:&quot;architectures-contre-icheckpointsi&quot;,&quot;title&quot;:&quot;Architectures contre &lt;i&gt;checkpoints&lt;/i&gt;&quot;}],&quot;title&quot;:&quot;Comment fonctionnent les &lt;i&gt;transformers&lt;/i&gt; ?&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/pages/chapter1/4.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/chunks/Youtube-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_285/fr/_app/chunks/IconCopyLink-hf-doc-builder.js"> 





<h1 class="relative group"><a id="comment-fonctionnent-les-itransformersi" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#comment-fonctionnent-les-itransformersi"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Comment fonctionnent les <i>transformers</i> ?
	</span></h1>

<p>Dans cette partie, nous allons jeter un coup d’œil à l’architecture des <em>transformers</em>.</p>
<h2 class="relative group"><a id="court-historique-des-itransformersi" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#court-historique-des-itransformersi"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Court historique des <i>transformers</i></span></h2>

<p>Voici quelques dates clefs dans la courte histoire des <em>transformers</em> :</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models."></div>
<p><a href="https://arxiv.org/abs/1706.03762" rel="nofollow">L’architecture <em>Transformer</em></a> a été présentée en juin 2017. Initialement, la recherche portait sur la tâche de traduction. Elle a été suivie par l’introduction de plusieurs modèles influents, notamment :</p>
<ul><li><p><strong>Juin 2018</strong> : <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">GPT</a>, le premier <em>transformer</em> pré-entraîné et <em>finetuné</em> sur différentes tâches de NLP et ayant obtenu des résultats à l’état de l’art,</p></li>
<li><p><strong>Octobre 2018</strong> : <a href="https://arxiv.org/abs/1810.04805" rel="nofollow">BERT</a>, autre grand modèle pré-entraîné ayant été construit pour produire de meilleurs résumés de texte (plus de détails dans le chapitre suivant !),</p></li>
<li><p><strong>Février 2019</strong> : <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2</a>, une version améliorée (et plus grande) de GPT qui n’a pas été directement rendu publique pour cause de raisons éthiques,</p></li>
<li><p><strong>Octobre 2019</strong> : <a href="https://arxiv.org/abs/1910.01108" rel="nofollow">DistilBERT</a>, une version distillée de BERT étant 60% plus rapide, 40% plus légère en mémoire et conservant tout de même 97% des performances initiales de BERT,</p></li>
<li><p><strong>Octobre 2019</strong> : <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">BART</a> et <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">T5</a>, deux modèles pré-entraînés utilisant la même architecture que le <em>transformer</em> original (les premiers à faire cela),</p></li>
<li><p><strong>Mai 2020</strong> : <a href="https://arxiv.org/abs/2005.14165" rel="nofollow">GPT-3</a>, une version encore plus grande que GPT-2 ayant des performances très bonnes sur une variété de tâches ne nécessitant pas de <em>finetuning</em> (appelé <em>zero-shot learning</em>).</p></li></ul>
<p>Cette liste est loin d’être exhaustive et met en lumière certains <em>transformers</em>. Plus largement, ces modèles peuvent être regroupés en trois catégories :</p>
<ul><li>ceux de type GPT (aussi appelés <em>transformers</em> <em>autorégressifs</em>)  </li>
<li>ceux de type BERT (aussi appelés <em>transformers</em> <em>auto-encodeurs</em>)</li>
<li>ceux de type BART/T5 (aussi appelés <em>transformers</em> <em>séquence-à-séquence</em>)</li></ul>
<p>Nous verrons plus en profondeur ces familles de modèles plus tard.</p>
<h2 class="relative group"><a id="les-itransformersi-sont-des-modles-de-langage" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#les-itransformersi-sont-des-modles-de-langage"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Les <i>transformers</i> sont des modèles de langage
	</span></h2>

<p>Tous les <em>transformers</em> mentionnés ci-dessus (GPT, BERT, BART, T5, etc.) ont été entraînés comme des <em>modèles de langage</em>. Cela signifie qu’ils ont été entraînés sur une large quantité de textes bruts de manière autosupervisée. L’apprentissage autosupervisé est un type d’entraînement dans lequel l’objectif est automatiquement calculé à partir des entrées du modèle. Cela signifie que les humains ne sont pas nécessaires pour étiqueter les données !</p>
<p>Ce type de modèle développe une compréhension statistique de la langue sur laquelle il a été entraîné, mais il n’est pas très utile pour des tâches pratiques spécifiques. Pour cette raison, le modèle pré-entraîné passe ensuite par un processus appelé apprentissage par transfert. Au cours de ce processus, le modèle est <em>finetuné</em> de manière supervisée (c’est-à-dire en utilisant des étiquettes annotées par des humains) pour une tâche donnée.</p>
<p>Un exemple de tâche consiste à prédire le mot suivant dans une phrase après avoir lu les <em>n</em> mots précédents. Cette tâche est appelée <em>modélisation causale du langage</em> car la sortie dépend des entrées passées et présentes, mais pas des entrées futures.</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted."></div>
<p>Un autre exemple est la <em>modélisation du langage masqué</em>, dans laquelle le modèle prédit un mot masqué dans la phrase.</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted."></div>
<h2 class="relative group"><a id="les-itransformersi-sont-normes" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#les-itransformersi-sont-normes"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Les <i>transformers</i> sont énormes
	</span></h2>

<p>En dehors de quelques exceptions (comme DistilBERT), la stratégie générale pour obtenir de meilleure performance consiste à augmenter la taille des modèles ainsi que la quantité de données utilisées pour l’entraînement de ces derniers.</p>
<div class="flex justify-center"><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%"></div>
<p>Malheureusement, entraîner un modèle et particulièrement un très grand modèle, nécessite une importante quantité de données. Cela devient très coûteux en termes de temps et de ressources de calcul. Cela se traduit même par un impact environnemental comme le montre le graphique suivant.</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model."></div>
<iframe class="w-full xl:w-4/6 h-80" src="https://www.youtube-nocookie.com/embed/ftWlj4FBHTg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>L’image montre l’empreinte carbone pour un projet d’entraînement d’un (très grand) modèle mené par une équipe qui pourtant essaie consciemment de réduire l’impact environnemental du pré-entraînement. L’empreinte de l’exécution de nombreux essais pour obtenir les meilleurs hyperparamètres serait encore plus élevée.</p>
<p>Imaginez qu’à chaque fois qu’une équipe de recherche, une association d’étudiants ou une entreprise souhaite entraîner un modèle, elle le fasse en partant de zéro. Cela entraînerait des coûts globaux énormes et inutiles !</p>
<p>C’est pourquoi le partage des modèles du langage est primordial : partager les poids d’entraînement et construire à partir de ces poids permet de réduire les coûts de calcul globaux ainsi que l’empreinte carbone de toute la communauté.</p>
<h2 class="relative group"><a id="lapprentissage-par-transfert" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#lapprentissage-par-transfert"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>L&#39;apprentissage par transfert
	</span></h2>

<iframe class="w-full xl:w-4/6 h-80" src="https://www.youtube-nocookie.com/embed/BqqfQnyjmgg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Le pré-entraînement consiste à entraîner un modèle à partir de zéro : les poids sont initialisés de manière aléatoire et l’entraînement commence sans aucune connaissance préalable.</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money."></div>
<p>Ce pré-entraînement est généralement effectué sur de très grandes quantités de données. Il nécessite donc un très grand corpus de données et l’entraînement peut prendre jusqu’à plusieurs semaines.</p>
<p>Le <em>finetuning</em>, quant à lui, est l’entrainement effectué après qu’un modèle ait été pré-entraîné. Pour effectuer un <em>finetuning</em>, vous devez d’abord acquérir un modèle de langue pré-entraîné, puis effectuer un entraînement supplémentaire avec un jeu de données spécifiques. Mais pourquoi ne pas entraîner directement pour la tâche finale ? Il y a plusieurs raisons à cela :</p>
<ul><li>Le modèle pré-entraîné a déjà été entraîné sur un jeu de données qui présente certaines similitudes avec le jeu de données de <em>finetuning</em>. Le processus de <em>finetuning</em> est donc en mesure de tirer parti des connaissances acquises par le modèle initial lors du pré-entraînement (par exemple, pour les problèmes de langage naturel, le modèle pré-entraîné aura une certaine compréhension statistique de la langue que vous utilisez pour votre tâche)</li>
<li>Comme le modèle pré-entraîné a déjà été entraîné sur de nombreuses données, le <em>finetuning</em> nécessite beaucoup moins de données pour obtenir des résultats décents.</li>
<li>Pour la même raison, le temps et les ressources nécessaires pour obtenir de bons résultats sont beaucoup moins importants.</li></ul>
<p>Par exemple, il est possible d’exploiter un modèle pré-entraîné entraîné sur la langue anglaise, puis de le <em>finetuner</em> sur un corpus arXiv, pour obtenir un modèle basé sur la science et la recherche. Le <em>finetuning</em> ne nécessitera qu’une quantité limitée de données : les connaissances acquises par le modèle pré-entraîné sont « transférées », d’où le terme d’apprentissage par transfert.</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money."></div>
<p>Le <em>finetuning</em> d’un modèle a donc un coût moindre en termes de temps, de données, de finances et d’environnement. Il est aussi plus rapide et plus facile d’itérer sur différents schémas de <em>finetuning</em> car l’entraînement est moins contraignant qu’un pré-entraînement complet.</p>
<p>Ce processus permet également d’obtenir de meilleurs résultats que l’entraînement à partir de zéro (à moins que vous ne disposiez d’un grand nombre de données). C’est pourquoi vous devez toujours essayer de tirer parti d’un modèle pré-entraîné, c’est-à-dire un modèle aussi proche que possible de la tâche que vous avez à accomplir, et de le <em>finetuner</em>.</p>
<h2 class="relative group"><a id="architecture-gnrale" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#architecture-gnrale"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Architecture générale
	</span></h2>

<p>Dans cette section, nous allons voir l’architecture générale des <em>transformers</em>. Pas d’inquiétudes si vous ne comprenez pas tous les concepts, des sections détaillées qui couvrent chaque composant seront abordées plus tard.</p>
<iframe class="w-full xl:w-4/6 h-80" src="https://www.youtube-nocookie.com/embed/H39Z_720T5s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 class="relative group"><a id="introduction" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#introduction"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Introduction
	</span></h2>

<p>Le modèle est principalement composé de deux blocs :</p>
<ul><li><strong>Encodeur (à gauche)</strong> : l’encodeur reçoit une entrée et construit une représentation de celle-ci (ses caractéristiques). Cela signifie que le modèle est optimisé pour acquérir une compréhension venant de ces entrées.</li>
<li><strong>Décodeur (à droite)</strong> : le décodeur utilise la représentation de l’encodeur (les caractéristiques) en plus des autres entrées pour générer une séquence cible. Cela signifie que le modèle est optimisé pour générer des sorties.</li></ul>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models"></div>
<p>Chacun de ces blocs peuvent être utilisés indépendamment en fonction de la tâche que l’on souhaite traiter :</p>
<ul><li><strong>Modèles uniquement encodeurs</strong> : adaptés pour des tâches qui nécessitent une compréhension de l’entrée, comme la classification de phrases et la reconnaissance d’entités nommées.</li>
<li><strong>Modèles uniquement décodeurs</strong> : adaptés pour les tâches génératives telles que la génération de texte.</li>
<li><strong>Modèles encodeurs-décodeurs</strong> (ou <strong>modèles de séquence-à-séquence</strong>) : adaptés aux tâches génératives qui nécessitent une entrée, telles que la traduction ou le résumé de texte.</li></ul>
<p>Nous verrons plus en détails chacune de ces architectures plus tard.</p>
<h2 class="relative group"><a id="les-couches-dattention" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#les-couches-dattention"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Les couches d&#39;attention
	</span></h2>

<p>Une caractéristique clé des <em>transformers</em> est qu’ils sont construits avec des couches spéciales appelées couches d’attention. En fait, le titre du papier introduisant l’architecture <em>transformer</em> se nomme <a href="https://arxiv.org/abs/1706.03762" rel="nofollow"><em>Attention Is All You Need</em></a> ! Nous explorerons les détails des couches d’attention plus tard dans le cours. Pour l’instant, tout ce que vous devez savoir est que cette couche indique au modèle de prêter une attention spécifique à certains mots de la phrase que vous lui avez passée (et d’ignorer plus ou moins les autres) lors du traitement de la représentation de chaque mot.</p>
<p>Pour mettre cela en contexte, considérons la tâche de traduire un texte de l’anglais au français. Étant donné l’entrée « <em>You like this course</em> », un modèle de traduction devra également s’intéresser au mot adjacent « <em>You</em> » pour obtenir la traduction correcte du mot « <em>like</em> », car en français le verbe « <em>like</em> » se conjugue différemment selon le sujet. Le reste de la phrase n’est en revanche pas utile pour la traduction de ce mot. Dans le même ordre d’idées, pour traduire « <em>this</em> », le modèle devra également faire attention au mot « <em>course</em> » car « <em>this</em> » se traduit différemment selon que le nom associé est masculin ou féminin. Là encore, les autres mots de la phrase n’auront aucune importance pour la traduction de « <em>this</em> ». Avec des phrases plus complexes (et des règles de grammaire plus complexes), le modèle devra prêter une attention particulière aux mots qui pourraient apparaître plus loin dans la phrase pour traduire correctement chaque mot.</p>
<p>Le même concept s’applique à toute tâche associée au langage naturel : un mot en lui-même a un sens, mais ce sens est profondément affecté par le contexte, qui peut être n’importe quel autre mot (ou mots) avant ou après le mot étudié.</p>
<p>Maintenant que vous avez une idée plus précise des couches d’attentions, nous allons regarder de plus près l’architecture des <em>transformers</em>.</p>
<h2 class="relative group"><a id="larchitecture-originale" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#larchitecture-originale"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>L&#39;architecture originale
	</span></h2>

<p>L’architecture du <em>transformer</em> a initialement été construite pour la tâche de traduction. Pendant l’entraînement, l’encodeur reçoit des entrées (des phrases) dans une certaine langue, tandis que le décodeur reçoit la même phrase traduite dans la langue cible. Pour l’encodeur, les couches d’attention peuvent utiliser tous les mots d’une phrase (puisque comme nous venons de le voir, la traduction d’un mot donné peut dépendre de ce qui le suit ou le précède dans la phrase). Le décodeur, quant à lui, fonctionne de façon séquentielle et ne peut porter son attention qu’aux mots déjà traduits dans la phrase (donc uniquement les mots générés avant le mot en cours). Par exemple, lorsqu’on a prédit les trois premiers mots de la phrase cible, on les donne au décodeur qui utilise alors toutes les entrées de l’encodeur pour essayer de prédire le quatrième mot.</p>
<p>Pour accélérer les choses pendant l’apprentissage (lorsque le modèle a accès aux phrases cibles), le décodeur est alimenté avec la cible entière, mais il n’est pas autorisé à utiliser les mots futurs (s’il avait accès au mot en position 2 lorsqu’il essayait de prédire le mot en position 2, le problème ne serait pas très difficile !). Par exemple, en essayant de prédire le quatrième mot, la couche d’attention n’aura accès qu’aux mots des positions 1 à 3.</p>
<p>L’architecture originale du <em>transformer</em> ressemble à ceci, avec l’encodeur à gauche et le décodeur à droite :</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models"></div>
<p>Notez que la première couche d’attention dans un bloc décodeur prête attention à toutes les entrées (passées) du décodeur, mais que la deuxième couche d’attention utilise la sortie de l’encodeur. Elle peut donc accéder à l’ensemble de la phrase d’entrée pour prédire au mieux le mot actuel. C’est très utile, car différentes langues peuvent avoir des règles grammaticales qui placent les mots dans un ordre différent, ou un contexte fourni plus tard dans la phrase peut être utile pour déterminer la meilleure traduction d’un mot donné.</p>
<p>Le <em>masque d’attention</em> peut également être utilisé dans l’encodeur/décodeur pour empêcher le modèle de prêter attention à certains mots spéciaux. Par exemple, le mot de remplissage spécial (le <em>padding</em>) utilisé pour que toutes les entrées aient la même longueur lors du regroupement de phrases.</p>
<h2 class="relative group"><a id="architectures-contre-icheckpointsi" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#architectures-contre-icheckpointsi"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Architectures contre <i>checkpoints</i></span></h2>

 
En approfondissant l&#39;étude des <i>transformers</i> dans ce cours, vous verrez des mentions d&#39;<i>architectures</i> et de <i>checkpoints</i> ainsi que de <i>modèles</i>. Ces termes ont tous des significations légèrement différentes :
<ul><li><strong>Architecture</strong> : c’est le squelette du modèle, la définition de chaque couche et chaque opération qui se produit au sein du modèle.</li>
<li><strong>Checkpoints</strong> : ce sont les poids qui seront chargés dans une architecture donnée.</li>
<li><strong>Modèle</strong> : c’est un mot valise n’étant pas aussi précis que les mots « architecture » ou « <em>checkpoint</em> ». Il peut désigner l’un comme l’autre. Dans ce cours, il sera spécifié <em>architecture</em> ou <em>checkpoint</em> lorsqu’il sera essentiel de réduire toute ambiguïté.</li></ul>
<p>Par exemple, BERT est une architecture alors que <code>bert-base-cased</code> (un ensemble de poids entraîné par l’équipe de Google lors de la première sortie de BERT) est un <em>checkpoint</em>. Cependant, il est possible de dire « le modèle BERT » et « le modèle <code>bert-base-cased</code> ».</p>


		<script type="module" data-hydrate="ou0jkz">
		import { start } from "/docs/course/pr_285/fr/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="ou0jkz"]').parentNode,
			paths: {"base":"/docs/course/pr_285/fr","assets":"/docs/course/pr_285/fr"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/course/pr_285/fr/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/course/pr_285/fr/_app/pages/chapter1/4.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
