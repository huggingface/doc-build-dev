import{S as Fe,i as Ke,s as Qe,e as o,k as s,w as Ve,t as l,M as We,c as r,d as t,m as d,a as i,x as Ze,h as n,b as w,G as e,g as z,y as et,L as tt,q as at,o as ot,B as rt,v as it}from"../../chunks/vendor-hf-doc-builder.js";import{I as lt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function nt(we){let m,Y,p,T,C,b,ee,$,te,j,_,ae,I,oe,re,F,A,ie,K,R,q,f,x,le,ne,G,se,de,H,ce,me,h,u,M,pe,fe,g,he,ue,S,ve,Ee,v,k,Te,_e,N,Re,ze,O,be,Ae,E,U,De,Le,X,Be,Pe,J,ye,Q;return b=new lt({}),{c(){m=o("meta"),Y=s(),p=o("h1"),T=o("a"),C=o("span"),Ve(b.$$.fragment),ee=s(),$=o("span"),te=l("Riassunto"),j=s(),_=o("p"),ae=l("In questo capitolo, hai scoperto come approcciare diversi compiti di NLP utilizzando la funzione di alto livello "),I=o("code"),oe=l("pipeline()"),re=l(" degli \u{1F917} Transformer. Abbiamo anche visto come cercare e utilizzare i modelli dell\u2019Hub, nonch\xE9 come usare l\u2019Inference API per testare i modelli direttamente nel tuo browser."),F=s(),A=o("p"),ie=l("Abbiamo discusso di come i modelli Transformer lavorino a livello alto, e parlato dell\u2019importanza del transfer learning e dell\u2019affinamento. Un aspetto chiave \xE8 che \xE8 possibile utilizzare l\u2019architettuta completa oppure solo l\u2019encoder o il decoder, dipendentemente dal compito a cui desideri lavorare. La tabella seguente riordina questi concetti:"),K=s(),R=o("table"),q=o("thead"),f=o("tr"),x=o("th"),le=l("Modello"),ne=s(),G=o("th"),se=l("Esempi"),de=s(),H=o("th"),ce=l("Compiti"),me=s(),h=o("tbody"),u=o("tr"),M=o("td"),pe=l("Encoder"),fe=s(),g=o("td"),he=l("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ue=s(),S=o("td"),ve=l("Classificazione frasale, riconoscimento delle entit\xE0 nominate, estrazione di risposte a domande"),Ee=s(),v=o("tr"),k=o("td"),Te=l("Decoder"),_e=s(),N=o("td"),Re=l("CTRL, GPT, GPT-2, Transformer XL"),ze=s(),O=o("td"),be=l("Generazione di testi"),Ae=s(),E=o("tr"),U=o("td"),De=l("Encoder-decoder"),Le=s(),X=o("td"),Be=l("BART, T5, Marian, mBART"),Pe=s(),J=o("td"),ye=l("Riassunti, traduzione, generazione di risposte a domande"),this.h()},l(a){const c=We('[data-svelte="svelte-1phssyn"]',document.head);m=r(c,"META",{name:!0,content:!0}),c.forEach(t),Y=d(a),p=r(a,"H1",{class:!0});var V=i(p);T=r(V,"A",{id:!0,class:!0,href:!0});var Ce=i(T);C=r(Ce,"SPAN",{});var $e=i(C);Ze(b.$$.fragment,$e),$e.forEach(t),Ce.forEach(t),ee=d(V),$=r(V,"SPAN",{});var Ie=i($);te=n(Ie,"Riassunto"),Ie.forEach(t),V.forEach(t),j=d(a),_=r(a,"P",{});var W=i(_);ae=n(W,"In questo capitolo, hai scoperto come approcciare diversi compiti di NLP utilizzando la funzione di alto livello "),I=r(W,"CODE",{});var qe=i(I);oe=n(qe,"pipeline()"),qe.forEach(t),re=n(W," degli \u{1F917} Transformer. Abbiamo anche visto come cercare e utilizzare i modelli dell\u2019Hub, nonch\xE9 come usare l\u2019Inference API per testare i modelli direttamente nel tuo browser."),W.forEach(t),F=d(a),A=r(a,"P",{});var xe=i(A);ie=n(xe,"Abbiamo discusso di come i modelli Transformer lavorino a livello alto, e parlato dell\u2019importanza del transfer learning e dell\u2019affinamento. Un aspetto chiave \xE8 che \xE8 possibile utilizzare l\u2019architettuta completa oppure solo l\u2019encoder o il decoder, dipendentemente dal compito a cui desideri lavorare. La tabella seguente riordina questi concetti:"),xe.forEach(t),K=d(a),R=r(a,"TABLE",{});var Z=i(R);q=r(Z,"THEAD",{});var Ge=i(q);f=r(Ge,"TR",{});var D=i(f);x=r(D,"TH",{});var He=i(x);le=n(He,"Modello"),He.forEach(t),ne=d(D),G=r(D,"TH",{});var Me=i(G);se=n(Me,"Esempi"),Me.forEach(t),de=d(D),H=r(D,"TH",{});var ge=i(H);ce=n(ge,"Compiti"),ge.forEach(t),D.forEach(t),Ge.forEach(t),me=d(Z),h=r(Z,"TBODY",{});var L=i(h);u=r(L,"TR",{});var B=i(u);M=r(B,"TD",{});var Se=i(M);pe=n(Se,"Encoder"),Se.forEach(t),fe=d(B),g=r(B,"TD",{});var ke=i(g);he=n(ke,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ke.forEach(t),ue=d(B),S=r(B,"TD",{});var Ne=i(S);ve=n(Ne,"Classificazione frasale, riconoscimento delle entit\xE0 nominate, estrazione di risposte a domande"),Ne.forEach(t),B.forEach(t),Ee=d(L),v=r(L,"TR",{});var P=i(v);k=r(P,"TD",{});var Oe=i(k);Te=n(Oe,"Decoder"),Oe.forEach(t),_e=d(P),N=r(P,"TD",{});var Ue=i(N);Re=n(Ue,"CTRL, GPT, GPT-2, Transformer XL"),Ue.forEach(t),ze=d(P),O=r(P,"TD",{});var Xe=i(O);be=n(Xe,"Generazione di testi"),Xe.forEach(t),P.forEach(t),Ae=d(L),E=r(L,"TR",{});var y=i(E);U=r(y,"TD",{});var Je=i(U);De=n(Je,"Encoder-decoder"),Je.forEach(t),Le=d(y),X=r(y,"TD",{});var Ye=i(X);Be=n(Ye,"BART, T5, Marian, mBART"),Ye.forEach(t),Pe=d(y),J=r(y,"TD",{});var je=i(J);ye=n(je,"Riassunti, traduzione, generazione di risposte a domande"),je.forEach(t),y.forEach(t),L.forEach(t),Z.forEach(t),this.h()},h(){w(m,"name","hf:doc:metadata"),w(m,"content",JSON.stringify(st)),w(T,"id","riassunto"),w(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(T,"href","#riassunto"),w(p,"class","relative group")},m(a,c){e(document.head,m),z(a,Y,c),z(a,p,c),e(p,T),e(T,C),et(b,C,null),e(p,ee),e(p,$),e($,te),z(a,j,c),z(a,_,c),e(_,ae),e(_,I),e(I,oe),e(_,re),z(a,F,c),z(a,A,c),e(A,ie),z(a,K,c),z(a,R,c),e(R,q),e(q,f),e(f,x),e(x,le),e(f,ne),e(f,G),e(G,se),e(f,de),e(f,H),e(H,ce),e(R,me),e(R,h),e(h,u),e(u,M),e(M,pe),e(u,fe),e(u,g),e(g,he),e(u,ue),e(u,S),e(S,ve),e(h,Ee),e(h,v),e(v,k),e(k,Te),e(v,_e),e(v,N),e(N,Re),e(v,ze),e(v,O),e(O,be),e(h,Ae),e(h,E),e(E,U),e(U,De),e(E,Le),e(E,X),e(X,Be),e(E,Pe),e(E,J),e(J,ye),Q=!0},p:tt,i(a){Q||(at(b.$$.fragment,a),Q=!0)},o(a){ot(b.$$.fragment,a),Q=!1},d(a){t(m),a&&t(Y),a&&t(p),rt(b),a&&t(j),a&&t(_),a&&t(F),a&&t(A),a&&t(K),a&&t(R)}}}const st={local:"riassunto",title:"Riassunto"};function dt(we){return it(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pt extends Fe{constructor(m){super();Ke(this,m,dt,nt,Qe,{})}}export{pt as default,st as metadata};
