import{S as Tr,i as zr,s as Pr,e as r,k as h,w as g,t as n,M as Cr,c as i,d as e,m as u,a as o,x as m,h as l,b as P,G as a,g as p,y as d,q as j,o as w,B as x,v as Or}from"../../chunks/vendor-hf-doc-builder.js";import{T as we}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Dr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Mt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as $}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Nr}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Ar(D){let c,y;return{c(){c=r("p"),y=n("\u{1F4A1} This section covers BPE in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.")},l(f){c=i(f,"P",{});var b=o(c);y=l(b,"\u{1F4A1} This section covers BPE in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm."),b.forEach(e)},m(f,b){p(f,c,b),a(c,y)},d(f){f&&e(c)}}}function Br(D){let c,y,f,b,E;return{c(){c=r("p"),y=n("The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don\u2019t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called "),f=r("em"),b=n("byte-level BPE"),E=n(".")},l(k){c=i(k,"P",{});var v=o(c);y=l(v,"The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don\u2019t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called "),f=i(v,"EM",{});var z=o(f);b=l(z,"byte-level BPE"),z.forEach(e),E=l(v,"."),v.forEach(e)},m(k,v){p(k,c,v),a(c,y),a(c,f),a(f,b),a(c,E)},d(k){k&&e(c)}}}function Ir(D){let c,y,f,b,E;return{c(){c=r("p"),y=n("\u270F\uFE0F "),f=r("strong"),b=n("Now your turn!"),E=n(" What do you think the next merge rule will be?")},l(k){c=i(k,"P",{});var v=o(c);y=l(v,"\u270F\uFE0F "),f=i(v,"STRONG",{});var z=o(f);b=l(z,"Now your turn!"),z.forEach(e),E=l(v," What do you think the next merge rule will be?"),v.forEach(e)},m(k,v){p(k,c,v),a(c,y),a(c,f),a(f,b),a(c,E)},d(k){k&&e(c)}}}function Sr(D){let c,y,f,b,E,k,v,z;return{c(){c=r("p"),y=n("\u270F\uFE0F "),f=r("strong"),b=n("Now your turn!"),E=n(" How do you think  the word "),k=r("code"),v=n('"unhug"'),z=n(" will be tokenized?")},l(R){c=i(R,"P",{});var N=o(c);y=l(N,"\u270F\uFE0F "),f=i(N,"STRONG",{});var S=o(f);b=l(S,"Now your turn!"),S.forEach(e),E=l(N," How do you think  the word "),k=i(N,"CODE",{});var is=o(k);v=l(is,'"unhug"'),is.forEach(e),z=l(N," will be tokenized?"),N.forEach(e)},m(R,N){p(R,c,N),a(c,y),a(c,f),a(f,b),a(c,E),a(c,k),a(k,v),a(c,z)},d(R){R&&e(c)}}}function Gr(D){let c,y,f,b,E;return{c(){c=r("p"),y=n("\u{1F4A1} Using "),f=r("code"),b=n("train_new_from_iterator()"),E=n(" on the same corpus won\u2019t result in the exact same vocabulary. This is because when there is a choice of the most frequent pair, we selected the first one encountered, while the \u{1F917} Tokenizers library selects the first one based on its inner IDs.")},l(k){c=i(k,"P",{});var v=o(c);y=l(v,"\u{1F4A1} Using "),f=i(v,"CODE",{});var z=o(f);b=l(z,"train_new_from_iterator()"),z.forEach(e),E=l(v," on the same corpus won\u2019t result in the exact same vocabulary. This is because when there is a choice of the most frequent pair, we selected the first one encountered, while the \u{1F917} Tokenizers library selects the first one based on its inner IDs."),v.forEach(e)},m(k,v){p(k,c,v),a(c,y),a(c,f),a(f,b),a(c,E)},d(k){k&&e(c)}}}function Lr(D){let c,y;return{c(){c=r("p"),y=n("\u26A0\uFE0F Our implementation will throw an error if there is an unknown character since we didn\u2019t do anything to handle them. GPT-2 doesn\u2019t actually have an unknown token (it\u2019s impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we\u2019ve left the details out.")},l(f){c=i(f,"P",{});var b=o(c);y=l(b,"\u26A0\uFE0F Our implementation will throw an error if there is an unknown character since we didn\u2019t do anything to handle them. GPT-2 doesn\u2019t actually have an unknown token (it\u2019s impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we\u2019ve left the details out."),b.forEach(e)},m(f,b){p(f,c,b),a(c,y)},d(f){f&&e(c)}}}function Hr(D){let c,y,f,b,E,k,v,z,R,N,S,is,Ms,Kt,ja,os,wa,V,xa,W,M,xe,hs,Yt,be,Jt,ba,Ks,Zt,$a,us,ka,K,Qt,$e,Xt,sn,qa,Y,ya,J,en,ke,an,tn,va,Ys,nn,_a,Js,ln,Ea,cs,Ta,C,pn,qe,rn,on,ye,hn,un,ve,cn,fn,_e,gn,mn,Ee,dn,jn,za,fs,Pa,T,wn,Te,xn,bn,ze,$n,kn,Pe,qn,yn,Ce,vn,_n,Oe,En,Tn,De,zn,Pn,Ne,Cn,On,Ca,G,Dn,Ae,Nn,An,Be,Bn,In,Oa,gs,Da,A,Sn,Ie,Gn,Ln,Se,Hn,Rn,Ge,Wn,Fn,Na,ms,Aa,L,Un,Le,Vn,Mn,He,Kn,Yn,Ba,ds,Ia,Zs,Jn,Sa,Z,Ga,F,Q,Re,js,Zn,We,Qn,La,Qs,Xn,Ha,B,Fe,sl,el,Ue,al,tl,Ve,nl,ll,Me,pl,Ra,Xs,rl,Wa,ws,Fa,q,il,Ke,ol,hl,Ye,ul,cl,Je,fl,gl,Ze,ml,dl,Qe,jl,wl,Xe,xl,bl,sa,$l,kl,ea,ql,yl,aa,vl,_l,ta,El,Tl,na,zl,Pl,la,Cl,Ol,Ua,X,Va,U,ss,pa,xs,Dl,ra,Nl,Ma,se,Al,Ka,ee,Bl,Ya,bs,Ja,es,Il,ia,Sl,Gl,Za,$s,Qa,ae,Ll,Xa,ks,st,qs,et,te,Hl,at,ys,tt,vs,nt,as,Rl,oa,Wl,Fl,lt,_s,pt,ne,Ul,rt,Es,it,le,Vl,ot,Ts,ht,pe,Ml,ut,zs,ct,Ps,ft,re,Kl,gt,Cs,mt,Os,dt,H,Yl,ha,Jl,Zl,ua,Ql,Xl,jt,Ds,wt,ts,sp,ca,ep,ap,xt,Ns,bt,ie,tp,$t,As,kt,Bs,qt,oe,np,yt,Is,vt,he,lp,_t,Ss,Et,Gs,Tt,ue,pp,zt,Ls,Pt,Hs,Ct,ns,Ot,ce,rp,Dt,Rs,Nt,fe,ip,At,Ws,Bt,Fs,It,ls,St,ge,op,Gt;return k=new Mt({}),S=new Nr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"}]}}),os=new Dr({props:{id:"HEikzVL-lZU"}}),V=new we({props:{$$slots:{default:[Ar]},$$scope:{ctx:D}}}),hs=new Mt({}),us=new $({props:{code:'"hug", "pug", "pun", "bun", "hugs"',highlighted:'<span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-string">&quot;pug&quot;</span>, <span class="hljs-string">&quot;pun&quot;</span>, <span class="hljs-string">&quot;bun&quot;</span>, <span class="hljs-string">&quot;hugs&quot;</span>'}}),Y=new we({props:{$$slots:{default:[Br]},$$scope:{ctx:D}}}),cs=new $({props:{code:'("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),fs=new $({props:{code:'("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)',highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),gs=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),ms=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),ds=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;hug&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;hug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),Z=new we({props:{$$slots:{default:[Ir]},$$scope:{ctx:D}}}),js=new Mt({}),ws=new $({props:{code:`("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"`,highlighted:`<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;ug&quot;</span>
<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;un&quot;</span>
<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;hug&quot;</span>`}}),X=new we({props:{$$slots:{default:[Sr]},$$scope:{ctx:D}}}),xs=new Mt({}),bs=new $({props:{code:`corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]`,highlighted:`corpus = [
    <span class="hljs-string">&quot;This is the Hugging Face Course.&quot;</span>,
    <span class="hljs-string">&quot;This chapter is about tokenization.&quot;</span>,
    <span class="hljs-string">&quot;This section shows several tokenizer algorithms.&quot;</span>,
    <span class="hljs-string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,
]`}}),$s=new $({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)`}}),ks=new $({props:{code:`

`,highlighted:`<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict

word_freqs = defaultdict(<span class="hljs-built_in">int</span>)

<span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> words_with_offsets]
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_words:
        word_freqs[word] += <span class="hljs-number">1</span>

<span class="hljs-built_in">print</span>(word_freqs)`}}),qs=new $({props:{code:`defaultdict(int, {'This': 3, '\u0120is': 2, '\u0120the': 1, '\u0120Hugging': 1, '\u0120Face': 1, '\u0120Course': 1, '.': 4, '\u0120chapter': 1,
    '\u0120about': 1, '\u0120tokenization': 1, '\u0120section': 1, '\u0120shows': 1, '\u0120several': 1, '\u0120tokenizer': 1, '\u0120algorithms': 1,
    'Hopefully': 1, ',': 1, '\u0120you': 1, '\u0120will': 1, '\u0120be': 1, '\u0120able': 1, '\u0120to': 1, '\u0120understand': 1, '\u0120how': 1,
    '\u0120they': 1, '\u0120are': 1, '\u0120trained': 1, '\u0120and': 1, '\u0120generate': 1, '\u0120tokens': 1})`,highlighted:`defaultdict(<span class="hljs-built_in">int</span>, {<span class="hljs-string">&#x27;This&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Hugging&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Face&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Course&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;.&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;\u0120chapter&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;\u0120about&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokenization&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120section&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120shows&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120several&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokenizer&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120algorithms&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;Hopefully&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;,&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120you&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120will&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120be&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120able&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120understand&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120how&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;\u0120they&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120are&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120trained&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120generate&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokens&#x27;</span>: <span class="hljs-number">1</span>})`}}),ys=new $({props:{code:`
`,highlighted:`alphabet = []

<span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys():
    <span class="hljs-keyword">for</span> letter <span class="hljs-keyword">in</span> word:
        <span class="hljs-keyword">if</span> letter <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> alphabet:
            alphabet.append(letter)
alphabet.sort()

<span class="hljs-built_in">print</span>(alphabet)`}}),vs=new $({props:{code:`[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', '\u0120']`,highlighted:`[ <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;F&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>,
  <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;z&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>]`}}),_s=new $({props:{code:'vocab = ["<|endoftext|>"] + alphabet.copy()',highlighted:'vocab = [<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>] + alphabet.copy()'}}),Es=new $({props:{code:"splits = {word: [c for c in word] for word in word_freqs.keys()}",highlighted:'splits = {word: [c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys()}'}}),Ts=new $({props:{code:`def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_pair_freqs</span>(<span class="hljs-params">splits</span>):
    pair_freqs = defaultdict(<span class="hljs-built_in">int</span>)
    <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> word_freqs.items():
        split = splits[word]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">continue</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>):
            pair = (split[i], split[i + <span class="hljs-number">1</span>])
            pair_freqs[pair] += freq
    <span class="hljs-keyword">return</span> pair_freqs`}}),zs=new $({props:{code:"",highlighted:`pair_freqs = compute_pair_freqs(splits)

<span class="hljs-keyword">for</span> i, key <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pair_freqs.keys()):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span>: <span class="hljs-subst">{pair_freqs[key]}</span>&quot;</span>)
    <span class="hljs-keyword">if</span> i &gt;= <span class="hljs-number">5</span>:
        <span class="hljs-keyword">break</span>`}}),Ps=new $({props:{code:`('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('\u0120', 'i'): 2
('\u0120', 't'): 7
('t', 'h'): 3`,highlighted:`(<span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-number">3</span>
(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-number">3</span>
(<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>): <span class="hljs-number">5</span>
(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-number">2</span>
(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>): <span class="hljs-number">7</span>
(<span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-number">3</span>`}}),Cs=new $({props:{code:`
`,highlighted:`best_pair = <span class="hljs-string">&quot;&quot;</span>
max_freq = <span class="hljs-literal">None</span>

<span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freqs.items():
    <span class="hljs-keyword">if</span> max_freq <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> max_freq &lt; freq:
        best_pair = pair
        max_freq = freq

<span class="hljs-built_in">print</span>(best_pair, max_freq)`}}),Os=new $({props:{code:"('\u0120', 't') 7",highlighted:'(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>) <span class="hljs-number">7</span>'}}),Ds=new $({props:{code:`merges = {("\u0120", "t"): "\u0120t"}
vocab.append("\u0120t")`,highlighted:`merges = {(<span class="hljs-string">&quot;\u0120&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>): <span class="hljs-string">&quot;\u0120t&quot;</span>}
vocab.append(<span class="hljs-string">&quot;\u0120t&quot;</span>)`}}),Ns=new $({props:{code:"",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_pair</span>(<span class="hljs-params">a, b, splits</span>):
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs:
        split = splits[word]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">continue</span>

        i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>:
            <span class="hljs-keyword">if</span> split[i] == a <span class="hljs-keyword">and</span> split[i + <span class="hljs-number">1</span>] == b:
                split = split[:i] + [a + b] + split[i + <span class="hljs-number">2</span> :]
            <span class="hljs-keyword">else</span>:
                i += <span class="hljs-number">1</span>
        splits[word] = split
    <span class="hljs-keyword">return</span> splits`}}),As=new $({props:{code:`splits = merge_pair("\u0120", "t", splits)
print(splits["\u0120trained"])`,highlighted:`splits = merge_pair(<span class="hljs-string">&quot;\u0120&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, splits)
<span class="hljs-built_in">print</span>(splits[<span class="hljs-string">&quot;\u0120trained&quot;</span>])`}}),Bs=new $({props:{code:"['\u0120t', 'r', 'a', 'i', 'n', 'e', 'd']",highlighted:'[<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]'}}),Is=new $({props:{code:"",highlighted:`vocab_size = <span class="hljs-number">50</span>

<span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(vocab) &lt; vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = <span class="hljs-string">&quot;&quot;</span>
    max_freq = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freqs.items():
        <span class="hljs-keyword">if</span> max_freq <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> max_freq &lt; freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>]
    vocab.append(best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>])`}}),Ss=new $({props:{code:"print(merges)",highlighted:'<span class="hljs-built_in">print</span>(merges)'}}),Gs=new $({props:{code:`{('\u0120', 't'): '\u0120t', ('i', 's'): 'is', ('e', 'r'): 'er', ('\u0120', 'a'): '\u0120a', ('\u0120t', 'o'): '\u0120to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('\u0120to', 'k'): '\u0120tok',
 ('\u0120tok', 'en'): '\u0120token', ('n', 'd'): 'nd', ('\u0120', 'is'): '\u0120is', ('\u0120t', 'h'): '\u0120th', ('\u0120th', 'e'): '\u0120the',
 ('i', 'n'): 'in', ('\u0120a', 'b'): '\u0120ab', ('\u0120token', 'i'): '\u0120tokeni'}`,highlighted:`{(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>): <span class="hljs-string">&#x27;\u0120t&#x27;</span>, (<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>): <span class="hljs-string">&#x27;is&#x27;</span>, (<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>): <span class="hljs-string">&#x27;er&#x27;</span>, (<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>): <span class="hljs-string">&#x27;\u0120a&#x27;</span>, (<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>): <span class="hljs-string">&#x27;\u0120to&#x27;</span>, (<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>): <span class="hljs-string">&#x27;en&#x27;</span>,
 (<span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-string">&#x27;Th&#x27;</span>, (<span class="hljs-string">&#x27;Th&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>): <span class="hljs-string">&#x27;This&#x27;</span>, (<span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>): <span class="hljs-string">&#x27;ou&#x27;</span>, (<span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>): <span class="hljs-string">&#x27;se&#x27;</span>, (<span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>): <span class="hljs-string">&#x27;\u0120tok&#x27;</span>,
 (<span class="hljs-string">&#x27;\u0120tok&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>): <span class="hljs-string">&#x27;\u0120token&#x27;</span>, (<span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>): <span class="hljs-string">&#x27;nd&#x27;</span>, (<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>): <span class="hljs-string">&#x27;\u0120is&#x27;</span>, (<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-string">&#x27;\u0120th&#x27;</span>, (<span class="hljs-string">&#x27;\u0120th&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>): <span class="hljs-string">&#x27;\u0120the&#x27;</span>,
 (<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>): <span class="hljs-string">&#x27;in&#x27;</span>, (<span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>): <span class="hljs-string">&#x27;\u0120ab&#x27;</span>, (<span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-string">&#x27;\u0120tokeni&#x27;</span>}`}}),Ls=new $({props:{code:"print(vocab)",highlighted:'<span class="hljs-built_in">print</span>(vocab)'}}),Hs=new $({props:{code:`['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', '\u0120', '\u0120t', 'is', 'er', '\u0120a', '\u0120to', 'en', 'Th', 'This', 'ou', 'se',
 '\u0120tok', '\u0120token', 'nd', '\u0120is', '\u0120th', '\u0120the', 'in', '\u0120ab', '\u0120tokeni']`,highlighted:`[<span class="hljs-string">&#x27;&lt;|endoftext|&gt;&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;F&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>,
 <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;z&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, <span class="hljs-string">&#x27;Th&#x27;</span>, <span class="hljs-string">&#x27;This&#x27;</span>, <span class="hljs-string">&#x27;ou&#x27;</span>, <span class="hljs-string">&#x27;se&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120tok&#x27;</span>, <span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;nd&#x27;</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>, <span class="hljs-string">&#x27;\u0120th&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;\u0120ab&#x27;</span>, <span class="hljs-string">&#x27;\u0120tokeni&#x27;</span>]`}}),ns=new we({props:{$$slots:{default:[Gr]},$$scope:{ctx:D}}}),Rs=new $({props:{code:"",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">text</span>):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> pre_tokenize_result]
    splits = [[l <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> pre_tokenized_text]
    <span class="hljs-keyword">for</span> pair, merge <span class="hljs-keyword">in</span> merges.items():
        <span class="hljs-keyword">for</span> idx, split <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(splits):
            i = <span class="hljs-number">0</span>
            <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>:
                <span class="hljs-keyword">if</span> split[i] == pair[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> split[i + <span class="hljs-number">1</span>] == pair[<span class="hljs-number">1</span>]:
                    split = split[:i] + [merge] + split[i + <span class="hljs-number">2</span> :]
                <span class="hljs-keyword">else</span>:
                    i += <span class="hljs-number">1</span>
            splits[idx] = split

    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(splits, [])`}}),Ws=new $({props:{code:'tokenize("This is not a token.")',highlighted:'tokenize(<span class="hljs-string">&quot;This is not a token.&quot;</span>)'}}),Fs=new $({props:{code:"['This', '\u0120is', '\u0120', 'n', 'o', 't', '\u0120a', '\u0120token', '.']",highlighted:'[<span class="hljs-string">&#x27;This&#x27;</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),ls=new we({props:{warning:!0,$$slots:{default:[Lr]},$$scope:{ctx:D}}}),{c(){c=r("meta"),y=h(),f=r("h1"),b=r("a"),E=r("span"),g(k.$$.fragment),v=h(),z=r("span"),R=n("Byte-Pair Encoding tokenization"),N=h(),g(S.$$.fragment),is=h(),Ms=r("p"),Kt=n("Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It\u2019s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."),ja=h(),g(os.$$.fragment),wa=h(),g(V.$$.fragment),xa=h(),W=r("h2"),M=r("a"),xe=r("span"),g(hs.$$.fragment),Yt=h(),be=r("span"),Jt=n("Training algorithm"),ba=h(),Ks=r("p"),Zt=n("BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let\u2019s say our corpus uses these five words:"),$a=h(),g(us.$$.fragment),ka=h(),K=r("p"),Qt=n("The base vocabulary will then be "),$e=r("code"),Xt=n('["b", "g", "h", "n", "p", "s", "u"]'),sn=n(". For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token. That\u2019s one reason why lots of NLP models are very bad at analyzing content with emojis, for instance."),qa=h(),g(Y.$$.fragment),ya=h(),J=r("p"),en=n("After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning "),ke=r("em"),an=n("merges"),tn=n(", which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords."),va=h(),Ys=r("p"),nn=n("At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by \u201Cpair,\u201D here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step."),_a=h(),Js=r("p"),ln=n("Going back to our previous example, let\u2019s assume the words had the following frequencies:"),Ea=h(),g(cs.$$.fragment),Ta=h(),C=r("p"),pn=n("meaning "),qe=r("code"),rn=n('"hug"'),on=n(" was present 10 times in the corpus, "),ye=r("code"),hn=n('"pug"'),un=n(" 5 times, "),ve=r("code"),cn=n('"pun"'),fn=n(" 12 times, "),_e=r("code"),gn=n('"bun"'),mn=n(" 4 times, and "),Ee=r("code"),dn=n('"hugs"'),jn=n(" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:"),za=h(),g(fs.$$.fragment),Pa=h(),T=r("p"),wn=n("Then we look at pairs. The pair "),Te=r("code"),xn=n('("h", "u")'),bn=n(" is present in the words "),ze=r("code"),$n=n('"hug"'),kn=n(" and "),Pe=r("code"),qn=n('"hugs"'),yn=n(", so 15 times total in the corpus. It\u2019s not the most frequent pair, though: that honor belongs to "),Ce=r("code"),vn=n('("u", "g")'),_n=n(", which is present in "),Oe=r("code"),En=n('"hug"'),Tn=n(", "),De=r("code"),zn=n('"pug"'),Pn=n(", and "),Ne=r("code"),Cn=n('"hugs"'),On=n(", for a grand total of 20 times in the vocabulary."),Ca=h(),G=r("p"),Dn=n("Thus, the first merge rule learned by the tokenizer is "),Ae=r("code"),Nn=n('("u", "g") -> "ug"'),An=n(", which means that "),Be=r("code"),Bn=n('"ug"'),In=n(" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:"),Oa=h(),g(gs.$$.fragment),Da=h(),A=r("p"),Sn=n("Now we have some pairs that result in a token longer than two characters: the pair "),Ie=r("code"),Gn=n('("h", "ug")'),Ln=n(", for instance (present 15 times in the corpus). The most frequent pair at this stage is "),Se=r("code"),Hn=n('("u", "n")'),Rn=n(", however, present 16 times in the corpus, so the second merge rule learned is "),Ge=r("code"),Wn=n('("u", "n") -> "un"'),Fn=n(". Adding that to the vocabulary and merging all existing occurrences leads us to:"),Na=h(),g(ms.$$.fragment),Aa=h(),L=r("p"),Un=n("Now the most frequent pair is "),Le=r("code"),Vn=n('("h", "ug")'),Mn=n(", so we learn the merge rule "),He=r("code"),Kn=n('("h", "ug") -> "hug"'),Yn=n(", which gives us our first three-letter token. After the merge, the corpus looks like this:"),Ba=h(),g(ds.$$.fragment),Ia=h(),Zs=r("p"),Jn=n("And we continue like this until we reach the desired vocabulary size."),Sa=h(),g(Z.$$.fragment),Ga=h(),F=r("h2"),Q=r("a"),Re=r("span"),g(js.$$.fragment),Zn=h(),We=r("span"),Qn=n("Tokenization algorithm"),La=h(),Qs=r("p"),Xn=n("Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:"),Ha=h(),B=r("ol"),Fe=r("li"),sl=n("Normalization"),el=h(),Ue=r("li"),al=n("Pre-tokenization"),tl=h(),Ve=r("li"),nl=n("Splitting the words into individual characters"),ll=h(),Me=r("li"),pl=n("Applying the merge rules learned in order on those splits"),Ra=h(),Xs=r("p"),rl=n("Let\u2019s take the example we used during training, with the three merge rules learned:"),Wa=h(),g(ws.$$.fragment),Fa=h(),q=r("p"),il=n("The word "),Ke=r("code"),ol=n('"bug"'),hl=n(" will be tokenized as "),Ye=r("code"),ul=n('["b", "ug"]'),cl=n(". "),Je=r("code"),fl=n('"mug"'),gl=n(", however, will be tokenized as "),Ze=r("code"),ml=n('["[UNK]", "ug"]'),dl=n(" since the letter "),Qe=r("code"),jl=n('"m"'),wl=n(" was not in the base vocabulary. Likewise, the word "),Xe=r("code"),xl=n('"thug"'),bl=n(" will be tokenized as "),sa=r("code"),$l=n('["[UNK]", "hug"]'),kl=n(": the letter "),ea=r("code"),ql=n('"t"'),yl=n(" is not in the base vocabulary, and applying the merge rules results first in "),aa=r("code"),vl=n('"u"'),_l=n(" and "),ta=r("code"),El=n('"g"'),Tl=n(" being merged and then "),na=r("code"),zl=n('"hu"'),Pl=n(" and "),la=r("code"),Cl=n('"g"'),Ol=n(" being merged."),Ua=h(),g(X.$$.fragment),Va=h(),U=r("h2"),ss=r("a"),pa=r("span"),g(xs.$$.fragment),Dl=h(),ra=r("span"),Nl=n("Implementing BPE"),Ma=h(),se=r("p"),Al=n("Now let\u2019s take a look at an implementation of the BPE algorithm. This won\u2019t be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better."),Ka=h(),ee=r("p"),Bl=n("First we need a corpus, so let\u2019s create a simple one with a few sentences:"),Ya=h(),g(bs.$$.fragment),Ja=h(),es=r("p"),Il=n("Next, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the "),ia=r("code"),Sl=n("gpt2"),Gl=n(" tokenizer for the pre-tokenization:"),Za=h(),g($s.$$.fragment),Qa=h(),ae=r("p"),Ll=n("Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"),Xa=h(),g(ks.$$.fragment),st=h(),g(qs.$$.fragment),et=h(),te=r("p"),Hl=n("The next step is to compute the base vocabulary, formed by all the characters used in the corpus:"),at=h(),g(ys.$$.fragment),tt=h(),g(vs.$$.fragment),nt=h(),as=r("p"),Rl=n("We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is "),oa=r("code"),Wl=n('"<|endoftext|>"'),Fl=n(":"),lt=h(),g(_s.$$.fragment),pt=h(),ne=r("p"),Ul=n("We now need to split each word into individual characters, to be able to start training:"),rt=h(),g(Es.$$.fragment),it=h(),le=r("p"),Vl=n("Now that we are ready for training, let\u2019s write a function that computes the frequency of each pair. We\u2019ll need to use this at each step of the training:"),ot=h(),g(Ts.$$.fragment),ht=h(),pe=r("p"),Ml=n("Let\u2019s have a look at a part of this dictionary after the initial splits:"),ut=h(),g(zs.$$.fragment),ct=h(),g(Ps.$$.fragment),ft=h(),re=r("p"),Kl=n("Now, finding the most frequent pair only takes a quick loop:"),gt=h(),g(Cs.$$.fragment),mt=h(),g(Os.$$.fragment),dt=h(),H=r("p"),Yl=n("So the first merge to learn is "),ha=r("code"),Jl=n("('\u0120', 't') -> '\u0120t'"),Zl=n(", and we add "),ua=r("code"),Ql=n("'\u0120t'"),Xl=n(" to the vocabulary:"),jt=h(),g(Ds.$$.fragment),wt=h(),ts=r("p"),sp=n("To continue, we need to apply that merge in our "),ca=r("code"),ep=n("splits"),ap=n(" dictionary. Let\u2019s write another function for this:"),xt=h(),g(Ns.$$.fragment),bt=h(),ie=r("p"),tp=n("And we can have a look at the result of the first merge:"),$t=h(),g(As.$$.fragment),kt=h(),g(Bs.$$.fragment),qt=h(),oe=r("p"),np=n("Now we have everything we need to loop until we have learned all the merges we want. Let\u2019s aim for a vocab size of 50:"),yt=h(),g(Is.$$.fragment),vt=h(),he=r("p"),lp=n("As a result, we\u2019ve learned 19 merge rules (the initial vocabulary had a size of 31 \u2014 30 characters in the alphabet, plus the special token):"),_t=h(),g(Ss.$$.fragment),Et=h(),g(Gs.$$.fragment),Tt=h(),ue=r("p"),pp=n("And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges:"),zt=h(),g(Ls.$$.fragment),Pt=h(),g(Hs.$$.fragment),Ct=h(),g(ns.$$.fragment),Ot=h(),ce=r("p"),rp=n("To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:"),Dt=h(),g(Rs.$$.fragment),Nt=h(),fe=r("p"),ip=n("We can try this on any text composed of characters in the alphabet:"),At=h(),g(Ws.$$.fragment),Bt=h(),g(Fs.$$.fragment),It=h(),g(ls.$$.fragment),St=h(),ge=r("p"),op=n("That\u2019s it for the BPE algorithm! Next, we\u2019ll have a look at WordPiece."),this.h()},l(s){const t=Cr('[data-svelte="svelte-1phssyn"]',document.head);c=i(t,"META",{name:!0,content:!0}),t.forEach(e),y=u(s),f=i(s,"H1",{class:!0});var Us=o(f);b=i(Us,"A",{id:!0,class:!0,href:!0});var fa=o(b);E=i(fa,"SPAN",{});var ga=o(E);m(k.$$.fragment,ga),ga.forEach(e),fa.forEach(e),v=u(Us),z=i(Us,"SPAN",{});var ma=o(z);R=l(ma,"Byte-Pair Encoding tokenization"),ma.forEach(e),Us.forEach(e),N=u(s),m(S.$$.fragment,s),is=u(s),Ms=i(s,"P",{});var da=o(Ms);Kt=l(da,"Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It\u2019s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."),da.forEach(e),ja=u(s),m(os.$$.fragment,s),wa=u(s),m(V.$$.fragment,s),xa=u(s),W=i(s,"H2",{class:!0});var Vs=o(W);M=i(Vs,"A",{id:!0,class:!0,href:!0});var hp=o(M);xe=i(hp,"SPAN",{});var up=o(xe);m(hs.$$.fragment,up),up.forEach(e),hp.forEach(e),Yt=u(Vs),be=i(Vs,"SPAN",{});var cp=o(be);Jt=l(cp,"Training algorithm"),cp.forEach(e),Vs.forEach(e),ba=u(s),Ks=i(s,"P",{});var fp=o(Ks);Zt=l(fp,"BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let\u2019s say our corpus uses these five words:"),fp.forEach(e),$a=u(s),m(us.$$.fragment,s),ka=u(s),K=i(s,"P",{});var Lt=o(K);Qt=l(Lt,"The base vocabulary will then be "),$e=i(Lt,"CODE",{});var gp=o($e);Xt=l(gp,'["b", "g", "h", "n", "p", "s", "u"]'),gp.forEach(e),sn=l(Lt,". For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token. That\u2019s one reason why lots of NLP models are very bad at analyzing content with emojis, for instance."),Lt.forEach(e),qa=u(s),m(Y.$$.fragment,s),ya=u(s),J=i(s,"P",{});var Ht=o(J);en=l(Ht,"After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning "),ke=i(Ht,"EM",{});var mp=o(ke);an=l(mp,"merges"),mp.forEach(e),tn=l(Ht,", which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords."),Ht.forEach(e),va=u(s),Ys=i(s,"P",{});var dp=o(Ys);nn=l(dp,"At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by \u201Cpair,\u201D here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step."),dp.forEach(e),_a=u(s),Js=i(s,"P",{});var jp=o(Js);ln=l(jp,"Going back to our previous example, let\u2019s assume the words had the following frequencies:"),jp.forEach(e),Ea=u(s),m(cs.$$.fragment,s),Ta=u(s),C=i(s,"P",{});var I=o(C);pn=l(I,"meaning "),qe=i(I,"CODE",{});var wp=o(qe);rn=l(wp,'"hug"'),wp.forEach(e),on=l(I," was present 10 times in the corpus, "),ye=i(I,"CODE",{});var xp=o(ye);hn=l(xp,'"pug"'),xp.forEach(e),un=l(I," 5 times, "),ve=i(I,"CODE",{});var bp=o(ve);cn=l(bp,'"pun"'),bp.forEach(e),fn=l(I," 12 times, "),_e=i(I,"CODE",{});var $p=o(_e);gn=l($p,'"bun"'),$p.forEach(e),mn=l(I," 4 times, and "),Ee=i(I,"CODE",{});var kp=o(Ee);dn=l(kp,'"hugs"'),kp.forEach(e),jn=l(I," 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:"),I.forEach(e),za=u(s),m(fs.$$.fragment,s),Pa=u(s),T=i(s,"P",{});var O=o(T);wn=l(O,"Then we look at pairs. The pair "),Te=i(O,"CODE",{});var qp=o(Te);xn=l(qp,'("h", "u")'),qp.forEach(e),bn=l(O," is present in the words "),ze=i(O,"CODE",{});var yp=o(ze);$n=l(yp,'"hug"'),yp.forEach(e),kn=l(O," and "),Pe=i(O,"CODE",{});var vp=o(Pe);qn=l(vp,'"hugs"'),vp.forEach(e),yn=l(O,", so 15 times total in the corpus. It\u2019s not the most frequent pair, though: that honor belongs to "),Ce=i(O,"CODE",{});var _p=o(Ce);vn=l(_p,'("u", "g")'),_p.forEach(e),_n=l(O,", which is present in "),Oe=i(O,"CODE",{});var Ep=o(Oe);En=l(Ep,'"hug"'),Ep.forEach(e),Tn=l(O,", "),De=i(O,"CODE",{});var Tp=o(De);zn=l(Tp,'"pug"'),Tp.forEach(e),Pn=l(O,", and "),Ne=i(O,"CODE",{});var zp=o(Ne);Cn=l(zp,'"hugs"'),zp.forEach(e),On=l(O,", for a grand total of 20 times in the vocabulary."),O.forEach(e),Ca=u(s),G=i(s,"P",{});var me=o(G);Dn=l(me,"Thus, the first merge rule learned by the tokenizer is "),Ae=i(me,"CODE",{});var Pp=o(Ae);Nn=l(Pp,'("u", "g") -> "ug"'),Pp.forEach(e),An=l(me,", which means that "),Be=i(me,"CODE",{});var Cp=o(Be);Bn=l(Cp,'"ug"'),Cp.forEach(e),In=l(me," will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:"),me.forEach(e),Oa=u(s),m(gs.$$.fragment,s),Da=u(s),A=i(s,"P",{});var ps=o(A);Sn=l(ps,"Now we have some pairs that result in a token longer than two characters: the pair "),Ie=i(ps,"CODE",{});var Op=o(Ie);Gn=l(Op,'("h", "ug")'),Op.forEach(e),Ln=l(ps,", for instance (present 15 times in the corpus). The most frequent pair at this stage is "),Se=i(ps,"CODE",{});var Dp=o(Se);Hn=l(Dp,'("u", "n")'),Dp.forEach(e),Rn=l(ps,", however, present 16 times in the corpus, so the second merge rule learned is "),Ge=i(ps,"CODE",{});var Np=o(Ge);Wn=l(Np,'("u", "n") -> "un"'),Np.forEach(e),Fn=l(ps,". Adding that to the vocabulary and merging all existing occurrences leads us to:"),ps.forEach(e),Na=u(s),m(ms.$$.fragment,s),Aa=u(s),L=i(s,"P",{});var de=o(L);Un=l(de,"Now the most frequent pair is "),Le=i(de,"CODE",{});var Ap=o(Le);Vn=l(Ap,'("h", "ug")'),Ap.forEach(e),Mn=l(de,", so we learn the merge rule "),He=i(de,"CODE",{});var Bp=o(He);Kn=l(Bp,'("h", "ug") -> "hug"'),Bp.forEach(e),Yn=l(de,", which gives us our first three-letter token. After the merge, the corpus looks like this:"),de.forEach(e),Ba=u(s),m(ds.$$.fragment,s),Ia=u(s),Zs=i(s,"P",{});var Ip=o(Zs);Jn=l(Ip,"And we continue like this until we reach the desired vocabulary size."),Ip.forEach(e),Sa=u(s),m(Z.$$.fragment,s),Ga=u(s),F=i(s,"H2",{class:!0});var Rt=o(F);Q=i(Rt,"A",{id:!0,class:!0,href:!0});var Sp=o(Q);Re=i(Sp,"SPAN",{});var Gp=o(Re);m(js.$$.fragment,Gp),Gp.forEach(e),Sp.forEach(e),Zn=u(Rt),We=i(Rt,"SPAN",{});var Lp=o(We);Qn=l(Lp,"Tokenization algorithm"),Lp.forEach(e),Rt.forEach(e),La=u(s),Qs=i(s,"P",{});var Hp=o(Qs);Xn=l(Hp,"Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:"),Hp.forEach(e),Ha=u(s),B=i(s,"OL",{});var rs=o(B);Fe=i(rs,"LI",{});var Rp=o(Fe);sl=l(Rp,"Normalization"),Rp.forEach(e),el=u(rs),Ue=i(rs,"LI",{});var Wp=o(Ue);al=l(Wp,"Pre-tokenization"),Wp.forEach(e),tl=u(rs),Ve=i(rs,"LI",{});var Fp=o(Ve);nl=l(Fp,"Splitting the words into individual characters"),Fp.forEach(e),ll=u(rs),Me=i(rs,"LI",{});var Up=o(Me);pl=l(Up,"Applying the merge rules learned in order on those splits"),Up.forEach(e),rs.forEach(e),Ra=u(s),Xs=i(s,"P",{});var Vp=o(Xs);rl=l(Vp,"Let\u2019s take the example we used during training, with the three merge rules learned:"),Vp.forEach(e),Wa=u(s),m(ws.$$.fragment,s),Fa=u(s),q=i(s,"P",{});var _=o(q);il=l(_,"The word "),Ke=i(_,"CODE",{});var Mp=o(Ke);ol=l(Mp,'"bug"'),Mp.forEach(e),hl=l(_," will be tokenized as "),Ye=i(_,"CODE",{});var Kp=o(Ye);ul=l(Kp,'["b", "ug"]'),Kp.forEach(e),cl=l(_,". "),Je=i(_,"CODE",{});var Yp=o(Je);fl=l(Yp,'"mug"'),Yp.forEach(e),gl=l(_,", however, will be tokenized as "),Ze=i(_,"CODE",{});var Jp=o(Ze);ml=l(Jp,'["[UNK]", "ug"]'),Jp.forEach(e),dl=l(_," since the letter "),Qe=i(_,"CODE",{});var Zp=o(Qe);jl=l(Zp,'"m"'),Zp.forEach(e),wl=l(_," was not in the base vocabulary. Likewise, the word "),Xe=i(_,"CODE",{});var Qp=o(Xe);xl=l(Qp,'"thug"'),Qp.forEach(e),bl=l(_," will be tokenized as "),sa=i(_,"CODE",{});var Xp=o(sa);$l=l(Xp,'["[UNK]", "hug"]'),Xp.forEach(e),kl=l(_,": the letter "),ea=i(_,"CODE",{});var sr=o(ea);ql=l(sr,'"t"'),sr.forEach(e),yl=l(_," is not in the base vocabulary, and applying the merge rules results first in "),aa=i(_,"CODE",{});var er=o(aa);vl=l(er,'"u"'),er.forEach(e),_l=l(_," and "),ta=i(_,"CODE",{});var ar=o(ta);El=l(ar,'"g"'),ar.forEach(e),Tl=l(_," being merged and then "),na=i(_,"CODE",{});var tr=o(na);zl=l(tr,'"hu"'),tr.forEach(e),Pl=l(_," and "),la=i(_,"CODE",{});var nr=o(la);Cl=l(nr,'"g"'),nr.forEach(e),Ol=l(_," being merged."),_.forEach(e),Ua=u(s),m(X.$$.fragment,s),Va=u(s),U=i(s,"H2",{class:!0});var Wt=o(U);ss=i(Wt,"A",{id:!0,class:!0,href:!0});var lr=o(ss);pa=i(lr,"SPAN",{});var pr=o(pa);m(xs.$$.fragment,pr),pr.forEach(e),lr.forEach(e),Dl=u(Wt),ra=i(Wt,"SPAN",{});var rr=o(ra);Nl=l(rr,"Implementing BPE"),rr.forEach(e),Wt.forEach(e),Ma=u(s),se=i(s,"P",{});var ir=o(se);Al=l(ir,"Now let\u2019s take a look at an implementation of the BPE algorithm. This won\u2019t be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better."),ir.forEach(e),Ka=u(s),ee=i(s,"P",{});var or=o(ee);Bl=l(or,"First we need a corpus, so let\u2019s create a simple one with a few sentences:"),or.forEach(e),Ya=u(s),m(bs.$$.fragment,s),Ja=u(s),es=i(s,"P",{});var Ft=o(es);Il=l(Ft,"Next, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the "),ia=i(Ft,"CODE",{});var hr=o(ia);Sl=l(hr,"gpt2"),hr.forEach(e),Gl=l(Ft," tokenizer for the pre-tokenization:"),Ft.forEach(e),Za=u(s),m($s.$$.fragment,s),Qa=u(s),ae=i(s,"P",{});var ur=o(ae);Ll=l(ur,"Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"),ur.forEach(e),Xa=u(s),m(ks.$$.fragment,s),st=u(s),m(qs.$$.fragment,s),et=u(s),te=i(s,"P",{});var cr=o(te);Hl=l(cr,"The next step is to compute the base vocabulary, formed by all the characters used in the corpus:"),cr.forEach(e),at=u(s),m(ys.$$.fragment,s),tt=u(s),m(vs.$$.fragment,s),nt=u(s),as=i(s,"P",{});var Ut=o(as);Rl=l(Ut,"We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is "),oa=i(Ut,"CODE",{});var fr=o(oa);Wl=l(fr,'"<|endoftext|>"'),fr.forEach(e),Fl=l(Ut,":"),Ut.forEach(e),lt=u(s),m(_s.$$.fragment,s),pt=u(s),ne=i(s,"P",{});var gr=o(ne);Ul=l(gr,"We now need to split each word into individual characters, to be able to start training:"),gr.forEach(e),rt=u(s),m(Es.$$.fragment,s),it=u(s),le=i(s,"P",{});var mr=o(le);Vl=l(mr,"Now that we are ready for training, let\u2019s write a function that computes the frequency of each pair. We\u2019ll need to use this at each step of the training:"),mr.forEach(e),ot=u(s),m(Ts.$$.fragment,s),ht=u(s),pe=i(s,"P",{});var dr=o(pe);Ml=l(dr,"Let\u2019s have a look at a part of this dictionary after the initial splits:"),dr.forEach(e),ut=u(s),m(zs.$$.fragment,s),ct=u(s),m(Ps.$$.fragment,s),ft=u(s),re=i(s,"P",{});var jr=o(re);Kl=l(jr,"Now, finding the most frequent pair only takes a quick loop:"),jr.forEach(e),gt=u(s),m(Cs.$$.fragment,s),mt=u(s),m(Os.$$.fragment,s),dt=u(s),H=i(s,"P",{});var je=o(H);Yl=l(je,"So the first merge to learn is "),ha=i(je,"CODE",{});var wr=o(ha);Jl=l(wr,"('\u0120', 't') -> '\u0120t'"),wr.forEach(e),Zl=l(je,", and we add "),ua=i(je,"CODE",{});var xr=o(ua);Ql=l(xr,"'\u0120t'"),xr.forEach(e),Xl=l(je," to the vocabulary:"),je.forEach(e),jt=u(s),m(Ds.$$.fragment,s),wt=u(s),ts=i(s,"P",{});var Vt=o(ts);sp=l(Vt,"To continue, we need to apply that merge in our "),ca=i(Vt,"CODE",{});var br=o(ca);ep=l(br,"splits"),br.forEach(e),ap=l(Vt," dictionary. Let\u2019s write another function for this:"),Vt.forEach(e),xt=u(s),m(Ns.$$.fragment,s),bt=u(s),ie=i(s,"P",{});var $r=o(ie);tp=l($r,"And we can have a look at the result of the first merge:"),$r.forEach(e),$t=u(s),m(As.$$.fragment,s),kt=u(s),m(Bs.$$.fragment,s),qt=u(s),oe=i(s,"P",{});var kr=o(oe);np=l(kr,"Now we have everything we need to loop until we have learned all the merges we want. Let\u2019s aim for a vocab size of 50:"),kr.forEach(e),yt=u(s),m(Is.$$.fragment,s),vt=u(s),he=i(s,"P",{});var qr=o(he);lp=l(qr,"As a result, we\u2019ve learned 19 merge rules (the initial vocabulary had a size of 31 \u2014 30 characters in the alphabet, plus the special token):"),qr.forEach(e),_t=u(s),m(Ss.$$.fragment,s),Et=u(s),m(Gs.$$.fragment,s),Tt=u(s),ue=i(s,"P",{});var yr=o(ue);pp=l(yr,"And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges:"),yr.forEach(e),zt=u(s),m(Ls.$$.fragment,s),Pt=u(s),m(Hs.$$.fragment,s),Ct=u(s),m(ns.$$.fragment,s),Ot=u(s),ce=i(s,"P",{});var vr=o(ce);rp=l(vr,"To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:"),vr.forEach(e),Dt=u(s),m(Rs.$$.fragment,s),Nt=u(s),fe=i(s,"P",{});var _r=o(fe);ip=l(_r,"We can try this on any text composed of characters in the alphabet:"),_r.forEach(e),At=u(s),m(Ws.$$.fragment,s),Bt=u(s),m(Fs.$$.fragment,s),It=u(s),m(ls.$$.fragment,s),St=u(s),ge=i(s,"P",{});var Er=o(ge);op=l(Er,"That\u2019s it for the BPE algorithm! Next, we\u2019ll have a look at WordPiece."),Er.forEach(e),this.h()},h(){P(c,"name","hf:doc:metadata"),P(c,"content",JSON.stringify(Rr)),P(b,"id","bytepair-encoding-tokenization"),P(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(b,"href","#bytepair-encoding-tokenization"),P(f,"class","relative group"),P(M,"id","training-algorithm"),P(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(M,"href","#training-algorithm"),P(W,"class","relative group"),P(Q,"id","tokenization-algorithm"),P(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Q,"href","#tokenization-algorithm"),P(F,"class","relative group"),P(ss,"id","implementing-bpe"),P(ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(ss,"href","#implementing-bpe"),P(U,"class","relative group")},m(s,t){a(document.head,c),p(s,y,t),p(s,f,t),a(f,b),a(b,E),d(k,E,null),a(f,v),a(f,z),a(z,R),p(s,N,t),d(S,s,t),p(s,is,t),p(s,Ms,t),a(Ms,Kt),p(s,ja,t),d(os,s,t),p(s,wa,t),d(V,s,t),p(s,xa,t),p(s,W,t),a(W,M),a(M,xe),d(hs,xe,null),a(W,Yt),a(W,be),a(be,Jt),p(s,ba,t),p(s,Ks,t),a(Ks,Zt),p(s,$a,t),d(us,s,t),p(s,ka,t),p(s,K,t),a(K,Qt),a(K,$e),a($e,Xt),a(K,sn),p(s,qa,t),d(Y,s,t),p(s,ya,t),p(s,J,t),a(J,en),a(J,ke),a(ke,an),a(J,tn),p(s,va,t),p(s,Ys,t),a(Ys,nn),p(s,_a,t),p(s,Js,t),a(Js,ln),p(s,Ea,t),d(cs,s,t),p(s,Ta,t),p(s,C,t),a(C,pn),a(C,qe),a(qe,rn),a(C,on),a(C,ye),a(ye,hn),a(C,un),a(C,ve),a(ve,cn),a(C,fn),a(C,_e),a(_e,gn),a(C,mn),a(C,Ee),a(Ee,dn),a(C,jn),p(s,za,t),d(fs,s,t),p(s,Pa,t),p(s,T,t),a(T,wn),a(T,Te),a(Te,xn),a(T,bn),a(T,ze),a(ze,$n),a(T,kn),a(T,Pe),a(Pe,qn),a(T,yn),a(T,Ce),a(Ce,vn),a(T,_n),a(T,Oe),a(Oe,En),a(T,Tn),a(T,De),a(De,zn),a(T,Pn),a(T,Ne),a(Ne,Cn),a(T,On),p(s,Ca,t),p(s,G,t),a(G,Dn),a(G,Ae),a(Ae,Nn),a(G,An),a(G,Be),a(Be,Bn),a(G,In),p(s,Oa,t),d(gs,s,t),p(s,Da,t),p(s,A,t),a(A,Sn),a(A,Ie),a(Ie,Gn),a(A,Ln),a(A,Se),a(Se,Hn),a(A,Rn),a(A,Ge),a(Ge,Wn),a(A,Fn),p(s,Na,t),d(ms,s,t),p(s,Aa,t),p(s,L,t),a(L,Un),a(L,Le),a(Le,Vn),a(L,Mn),a(L,He),a(He,Kn),a(L,Yn),p(s,Ba,t),d(ds,s,t),p(s,Ia,t),p(s,Zs,t),a(Zs,Jn),p(s,Sa,t),d(Z,s,t),p(s,Ga,t),p(s,F,t),a(F,Q),a(Q,Re),d(js,Re,null),a(F,Zn),a(F,We),a(We,Qn),p(s,La,t),p(s,Qs,t),a(Qs,Xn),p(s,Ha,t),p(s,B,t),a(B,Fe),a(Fe,sl),a(B,el),a(B,Ue),a(Ue,al),a(B,tl),a(B,Ve),a(Ve,nl),a(B,ll),a(B,Me),a(Me,pl),p(s,Ra,t),p(s,Xs,t),a(Xs,rl),p(s,Wa,t),d(ws,s,t),p(s,Fa,t),p(s,q,t),a(q,il),a(q,Ke),a(Ke,ol),a(q,hl),a(q,Ye),a(Ye,ul),a(q,cl),a(q,Je),a(Je,fl),a(q,gl),a(q,Ze),a(Ze,ml),a(q,dl),a(q,Qe),a(Qe,jl),a(q,wl),a(q,Xe),a(Xe,xl),a(q,bl),a(q,sa),a(sa,$l),a(q,kl),a(q,ea),a(ea,ql),a(q,yl),a(q,aa),a(aa,vl),a(q,_l),a(q,ta),a(ta,El),a(q,Tl),a(q,na),a(na,zl),a(q,Pl),a(q,la),a(la,Cl),a(q,Ol),p(s,Ua,t),d(X,s,t),p(s,Va,t),p(s,U,t),a(U,ss),a(ss,pa),d(xs,pa,null),a(U,Dl),a(U,ra),a(ra,Nl),p(s,Ma,t),p(s,se,t),a(se,Al),p(s,Ka,t),p(s,ee,t),a(ee,Bl),p(s,Ya,t),d(bs,s,t),p(s,Ja,t),p(s,es,t),a(es,Il),a(es,ia),a(ia,Sl),a(es,Gl),p(s,Za,t),d($s,s,t),p(s,Qa,t),p(s,ae,t),a(ae,Ll),p(s,Xa,t),d(ks,s,t),p(s,st,t),d(qs,s,t),p(s,et,t),p(s,te,t),a(te,Hl),p(s,at,t),d(ys,s,t),p(s,tt,t),d(vs,s,t),p(s,nt,t),p(s,as,t),a(as,Rl),a(as,oa),a(oa,Wl),a(as,Fl),p(s,lt,t),d(_s,s,t),p(s,pt,t),p(s,ne,t),a(ne,Ul),p(s,rt,t),d(Es,s,t),p(s,it,t),p(s,le,t),a(le,Vl),p(s,ot,t),d(Ts,s,t),p(s,ht,t),p(s,pe,t),a(pe,Ml),p(s,ut,t),d(zs,s,t),p(s,ct,t),d(Ps,s,t),p(s,ft,t),p(s,re,t),a(re,Kl),p(s,gt,t),d(Cs,s,t),p(s,mt,t),d(Os,s,t),p(s,dt,t),p(s,H,t),a(H,Yl),a(H,ha),a(ha,Jl),a(H,Zl),a(H,ua),a(ua,Ql),a(H,Xl),p(s,jt,t),d(Ds,s,t),p(s,wt,t),p(s,ts,t),a(ts,sp),a(ts,ca),a(ca,ep),a(ts,ap),p(s,xt,t),d(Ns,s,t),p(s,bt,t),p(s,ie,t),a(ie,tp),p(s,$t,t),d(As,s,t),p(s,kt,t),d(Bs,s,t),p(s,qt,t),p(s,oe,t),a(oe,np),p(s,yt,t),d(Is,s,t),p(s,vt,t),p(s,he,t),a(he,lp),p(s,_t,t),d(Ss,s,t),p(s,Et,t),d(Gs,s,t),p(s,Tt,t),p(s,ue,t),a(ue,pp),p(s,zt,t),d(Ls,s,t),p(s,Pt,t),d(Hs,s,t),p(s,Ct,t),d(ns,s,t),p(s,Ot,t),p(s,ce,t),a(ce,rp),p(s,Dt,t),d(Rs,s,t),p(s,Nt,t),p(s,fe,t),a(fe,ip),p(s,At,t),d(Ws,s,t),p(s,Bt,t),d(Fs,s,t),p(s,It,t),d(ls,s,t),p(s,St,t),p(s,ge,t),a(ge,op),Gt=!0},p(s,[t]){const Us={};t&2&&(Us.$$scope={dirty:t,ctx:s}),V.$set(Us);const fa={};t&2&&(fa.$$scope={dirty:t,ctx:s}),Y.$set(fa);const ga={};t&2&&(ga.$$scope={dirty:t,ctx:s}),Z.$set(ga);const ma={};t&2&&(ma.$$scope={dirty:t,ctx:s}),X.$set(ma);const da={};t&2&&(da.$$scope={dirty:t,ctx:s}),ns.$set(da);const Vs={};t&2&&(Vs.$$scope={dirty:t,ctx:s}),ls.$set(Vs)},i(s){Gt||(j(k.$$.fragment,s),j(S.$$.fragment,s),j(os.$$.fragment,s),j(V.$$.fragment,s),j(hs.$$.fragment,s),j(us.$$.fragment,s),j(Y.$$.fragment,s),j(cs.$$.fragment,s),j(fs.$$.fragment,s),j(gs.$$.fragment,s),j(ms.$$.fragment,s),j(ds.$$.fragment,s),j(Z.$$.fragment,s),j(js.$$.fragment,s),j(ws.$$.fragment,s),j(X.$$.fragment,s),j(xs.$$.fragment,s),j(bs.$$.fragment,s),j($s.$$.fragment,s),j(ks.$$.fragment,s),j(qs.$$.fragment,s),j(ys.$$.fragment,s),j(vs.$$.fragment,s),j(_s.$$.fragment,s),j(Es.$$.fragment,s),j(Ts.$$.fragment,s),j(zs.$$.fragment,s),j(Ps.$$.fragment,s),j(Cs.$$.fragment,s),j(Os.$$.fragment,s),j(Ds.$$.fragment,s),j(Ns.$$.fragment,s),j(As.$$.fragment,s),j(Bs.$$.fragment,s),j(Is.$$.fragment,s),j(Ss.$$.fragment,s),j(Gs.$$.fragment,s),j(Ls.$$.fragment,s),j(Hs.$$.fragment,s),j(ns.$$.fragment,s),j(Rs.$$.fragment,s),j(Ws.$$.fragment,s),j(Fs.$$.fragment,s),j(ls.$$.fragment,s),Gt=!0)},o(s){w(k.$$.fragment,s),w(S.$$.fragment,s),w(os.$$.fragment,s),w(V.$$.fragment,s),w(hs.$$.fragment,s),w(us.$$.fragment,s),w(Y.$$.fragment,s),w(cs.$$.fragment,s),w(fs.$$.fragment,s),w(gs.$$.fragment,s),w(ms.$$.fragment,s),w(ds.$$.fragment,s),w(Z.$$.fragment,s),w(js.$$.fragment,s),w(ws.$$.fragment,s),w(X.$$.fragment,s),w(xs.$$.fragment,s),w(bs.$$.fragment,s),w($s.$$.fragment,s),w(ks.$$.fragment,s),w(qs.$$.fragment,s),w(ys.$$.fragment,s),w(vs.$$.fragment,s),w(_s.$$.fragment,s),w(Es.$$.fragment,s),w(Ts.$$.fragment,s),w(zs.$$.fragment,s),w(Ps.$$.fragment,s),w(Cs.$$.fragment,s),w(Os.$$.fragment,s),w(Ds.$$.fragment,s),w(Ns.$$.fragment,s),w(As.$$.fragment,s),w(Bs.$$.fragment,s),w(Is.$$.fragment,s),w(Ss.$$.fragment,s),w(Gs.$$.fragment,s),w(Ls.$$.fragment,s),w(Hs.$$.fragment,s),w(ns.$$.fragment,s),w(Rs.$$.fragment,s),w(Ws.$$.fragment,s),w(Fs.$$.fragment,s),w(ls.$$.fragment,s),Gt=!1},d(s){e(c),s&&e(y),s&&e(f),x(k),s&&e(N),x(S,s),s&&e(is),s&&e(Ms),s&&e(ja),x(os,s),s&&e(wa),x(V,s),s&&e(xa),s&&e(W),x(hs),s&&e(ba),s&&e(Ks),s&&e($a),x(us,s),s&&e(ka),s&&e(K),s&&e(qa),x(Y,s),s&&e(ya),s&&e(J),s&&e(va),s&&e(Ys),s&&e(_a),s&&e(Js),s&&e(Ea),x(cs,s),s&&e(Ta),s&&e(C),s&&e(za),x(fs,s),s&&e(Pa),s&&e(T),s&&e(Ca),s&&e(G),s&&e(Oa),x(gs,s),s&&e(Da),s&&e(A),s&&e(Na),x(ms,s),s&&e(Aa),s&&e(L),s&&e(Ba),x(ds,s),s&&e(Ia),s&&e(Zs),s&&e(Sa),x(Z,s),s&&e(Ga),s&&e(F),x(js),s&&e(La),s&&e(Qs),s&&e(Ha),s&&e(B),s&&e(Ra),s&&e(Xs),s&&e(Wa),x(ws,s),s&&e(Fa),s&&e(q),s&&e(Ua),x(X,s),s&&e(Va),s&&e(U),x(xs),s&&e(Ma),s&&e(se),s&&e(Ka),s&&e(ee),s&&e(Ya),x(bs,s),s&&e(Ja),s&&e(es),s&&e(Za),x($s,s),s&&e(Qa),s&&e(ae),s&&e(Xa),x(ks,s),s&&e(st),x(qs,s),s&&e(et),s&&e(te),s&&e(at),x(ys,s),s&&e(tt),x(vs,s),s&&e(nt),s&&e(as),s&&e(lt),x(_s,s),s&&e(pt),s&&e(ne),s&&e(rt),x(Es,s),s&&e(it),s&&e(le),s&&e(ot),x(Ts,s),s&&e(ht),s&&e(pe),s&&e(ut),x(zs,s),s&&e(ct),x(Ps,s),s&&e(ft),s&&e(re),s&&e(gt),x(Cs,s),s&&e(mt),x(Os,s),s&&e(dt),s&&e(H),s&&e(jt),x(Ds,s),s&&e(wt),s&&e(ts),s&&e(xt),x(Ns,s),s&&e(bt),s&&e(ie),s&&e($t),x(As,s),s&&e(kt),x(Bs,s),s&&e(qt),s&&e(oe),s&&e(yt),x(Is,s),s&&e(vt),s&&e(he),s&&e(_t),x(Ss,s),s&&e(Et),x(Gs,s),s&&e(Tt),s&&e(ue),s&&e(zt),x(Ls,s),s&&e(Pt),x(Hs,s),s&&e(Ct),x(ns,s),s&&e(Ot),s&&e(ce),s&&e(Dt),x(Rs,s),s&&e(Nt),s&&e(fe),s&&e(At),x(Ws,s),s&&e(Bt),x(Fs,s),s&&e(It),x(ls,s),s&&e(St),s&&e(ge)}}}const Rr={local:"bytepair-encoding-tokenization",sections:[{local:"training-algorithm",title:"Training algorithm"},{local:"tokenization-algorithm",title:"Tokenization algorithm"},{local:"implementing-bpe",title:"Implementing BPE"}],title:"Byte-Pair Encoding tokenization"};function Wr(D){return Or(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Jr extends Tr{constructor(c){super();zr(this,c,Wr,Hr,Pr,{})}}export{Jr as default,Rr as metadata};
