import{S as Yt,i as Jt,s as Kt,e as m,k as h,w as g,t as d,M as Wt,c as u,d as t,m as b,x as q,a as c,h as f,b as E,G as o,g as i,y as j,o as $,p as de,q as v,B as w,v as Xt,n as fe}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ut}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Gt}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Bs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Qt}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Zt}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function eo(k){let n,l;return n=new Qt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"}]}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function so(k){let n,l;return n=new Qt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"}]}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function ao(k){let n,l;return n=new Gt({props:{id:"ROxrFOEbsQE"}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function no(k){let n,l;return n=new Gt({props:{id:"M6adb1j2jPI"}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function to(k){let n,l,s,p;return n=new y({props:{code:`

`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),s=new y({props:{code:"InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]",highlighted:'InvalidArgumentError: Input to reshape <span class="hljs-keyword">is</span> a tensor <span class="hljs-keyword">with</span> <span class="hljs-number">14</span> values, but the requested shape has <span class="hljs-number">196</span> [Op:Reshape]'}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function oo(k){let n,l,s,p;return n=new y({props:{code:`

`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),s=new y({props:{code:"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",highlighted:'IndexError: Dimension out of <span class="hljs-built_in">range</span> (expected to be <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> of [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], but got <span class="hljs-number">1</span>)'}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function ro(k){let n,l,s,p;return n=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),s=new y({props:{code:`<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>,
        <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]], dtype=int32)&gt;`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function lo(k){let n,l,s,p;return n=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),s=new y({props:{code:`tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])`,highlighted:`tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,
          <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]])`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function io(k){let n,l;return n=new y({props:{code:`



`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function po(k){let n,l;return n=new y({props:{code:`



`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function mo(k){let n,l;return n=new y({props:{code:`Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)`,highlighted:`Input IDs: tf.Tensor(
[[ <span class="hljs-number">1045</span>  <span class="hljs-number">1005</span>  <span class="hljs-number">2310</span>  <span class="hljs-number">2042</span>  <span class="hljs-number">3403</span>  <span class="hljs-number">2005</span>  <span class="hljs-number">1037</span> <span class="hljs-number">17662</span> <span class="hljs-number">12172</span>  <span class="hljs-number">2607</span>  <span class="hljs-number">2026</span>  <span class="hljs-number">2878</span>
   <span class="hljs-number">2166</span>  <span class="hljs-number">1012</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), dtype=int32)
Logits: tf.Tensor([[-<span class="hljs-number">2.7276208</span>  <span class="hljs-number">2.8789377</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function uo(k){let n,l;return n=new y({props:{code:`Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]`,highlighted:`Input IDs: [[ <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>]]
Logits: [[-<span class="hljs-number">2.7276</span>,  <span class="hljs-number">2.8789</span>]]`}}),{c(){g(n.$$.fragment)},l(s){q(n.$$.fragment,s)},m(s,p){j(n,s,p),l=!0},i(s){l||(v(n.$$.fragment,s),l=!0)},o(s){$(n.$$.fragment,s),l=!1},d(s){w(n,s)}}}function co(k){let n,l,s,p,a,_,z,A;return{c(){n=m("p"),l=d("\u270F\uFE0F "),s=m("strong"),p=d("Experimente!"),a=d(" Converta esta lista de "),_=m("code"),z=d("batched_ids"),A=d(" em um tensor e passe-a atrav\xE9s de seu modelo. Verifique se voc\xEA obt\xE9m os mesmos logits que antes (mas duas vezes)!")},l(W){n=u(W,"P",{});var I=c(n);l=f(I,"\u270F\uFE0F "),s=u(I,"STRONG",{});var Fe=c(s);p=f(Fe,"Experimente!"),Fe.forEach(t),a=f(I," Converta esta lista de "),_=u(I,"CODE",{});var he=c(_);z=f(he,"batched_ids"),he.forEach(t),A=f(I," em um tensor e passe-a atrav\xE9s de seu modelo. Verifique se voc\xEA obt\xE9m os mesmos logits que antes (mas duas vezes)!"),I.forEach(t)},m(W,I){i(W,n,I),o(n,l),o(n,s),o(s,p),o(n,a),o(n,_),o(_,z),o(n,A)},d(W){W&&t(n)}}}function fo(k){let n,l,s,p;return n=new y({props:{code:`
`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(tf.constant(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(batched_ids)).logits)`}}),s=new y({props:{code:`tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor([[ <span class="hljs-number">1.5693678</span> -<span class="hljs-number">1.3894581</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor([[ <span class="hljs-number">0.5803005</span>  -<span class="hljs-number">0.41252428</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor(
[[ <span class="hljs-number">1.5693681</span> -<span class="hljs-number">1.3894582</span>]
 [ <span class="hljs-number">1.3373486</span> -<span class="hljs-number">1.2163193</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function ho(k){let n,l,s,p;return n=new y({props:{code:`
`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)`}}),s=new y({props:{code:`tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">1.3373</span>, -<span class="hljs-number">1.2163</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function bo(k){let n,l,s,p;return n=new y({props:{code:`
`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),s=new y({props:{code:`tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[ <span class="hljs-number">1.5693681</span>  -<span class="hljs-number">1.3894582</span> ]
 [ <span class="hljs-number">0.5803021</span>  -<span class="hljs-number">0.41252586</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function _o(k){let n,l,s,p;return n=new y({props:{code:`
`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),s=new y({props:{code:`tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(n.$$.fragment),l=h(),g(s.$$.fragment)},l(a){q(n.$$.fragment,a),l=b(a),q(s.$$.fragment,a)},m(a,_){j(n,a,_),i(a,l,_),j(s,a,_),p=!0},i(a){p||(v(n.$$.fragment,a),v(s.$$.fragment,a),p=!0)},o(a){$(n.$$.fragment,a),$(s.$$.fragment,a),p=!1},d(a){w(n,a),a&&t(l),w(s,a)}}}function $o(k){let n,l,s,p,a;return{c(){n=m("p"),l=d("\u270F\uFE0F "),s=m("strong"),p=d("Experimente!"),a=d(" Aplique a tokeniza\xE7\xE3o manualmente nas duas frases usadas na se\xE7\xE3o 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Passe-as atrav\xE9s do modelo e verifique se voc\xEA obt\xE9m os mesmos logits que na se\xE7\xE3o 2. Agora, agrupe-os usando o token de padding e depois crie a m\xE1scara de aten\xE7\xE3o adequada. Verifique que voc\xEA obtenha os mesmos resultados ao passar pelo modelo!")},l(_){n=u(_,"P",{});var z=c(n);l=f(z,"\u270F\uFE0F "),s=u(z,"STRONG",{});var A=c(s);p=f(A,"Experimente!"),A.forEach(t),a=f(z," Aplique a tokeniza\xE7\xE3o manualmente nas duas frases usadas na se\xE7\xE3o 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Passe-as atrav\xE9s do modelo e verifique se voc\xEA obt\xE9m os mesmos logits que na se\xE7\xE3o 2. Agora, agrupe-os usando o token de padding e depois crie a m\xE1scara de aten\xE7\xE3o adequada. Verifique que voc\xEA obtenha os mesmos resultados ao passar pelo modelo!"),z.forEach(t)},m(_,z){i(_,n,z),o(n,l),o(n,s),o(s,p),o(n,a)},d(_){_&&t(n)}}}function vo(k){let n,l,s,p,a,_,z,A,W,I,Fe,he,T,S,Le,C,N,He,Be,Ta,Vs,P,_s,Sa,Ca,be,Na,$s,Da,Oa,Ma,vs,Fa,La,ks,Ha,Rs,Ve,Ba,Us,X,ae,gs,_e,Va,qs,Ra,Gs,Re,Ua,Qs,D,O,Ue,Ge,Ga,Ys,ne,Qa,js,Ya,Ja,Js,M,F,Qe,Ye,Ka,Ks,L,H,Je,Ke,Wa,Ws,B,V,We,$e,ws,Xa,Za,Xs,ve,Zs,Xe,en,ea,te,sa,oe,sn,Es,an,nn,aa,Z,re,ys,ke,tn,zs,on,na,Ze,rn,ta,ge,oa,Y,ln,As,pn,mn,Is,un,cn,ra,qe,la,le,dn,Ps,fn,hn,ia,R,U,es,ss,bn,pa,J,_n,xs,$n,vn,Ts,kn,gn,ma,ee,ie,Ss,je,qn,Cs,jn,ua,we,Ns,wn,En,ca,as,yn,da,G,Q,ns,ts,zn,fa,os,An,ha,pe,ba,se,me,Ds,Ee,In,Os,Pn,_a,rs,xn,$a,ue,Ms,Tn,Sn,Fs,Cn,va,K,Nn,ye,Dn,On,ze,Mn,Fn,ka,ce,Ln,Ls,Hn,Bn,ga,Ae,qa;s=new Zt({props:{fw:k[0]}}),A=new Bs({});const Un=[so,eo],Ie=[];function Gn(e,r){return e[0]==="pt"?0:1}T=Gn(k),S=Ie[T]=Un[T](k);const Qn=[no,ao],Pe=[];function Yn(e,r){return e[0]==="pt"?0:1}C=Yn(k),N=Pe[C]=Qn[C](k),_e=new Bs({});const Jn=[oo,to],xe=[];function Kn(e,r){return e[0]==="pt"?0:1}D=Kn(k),O=xe[D]=Jn[D](k);const Wn=[lo,ro],Te=[];function Xn(e,r){return e[0]==="pt"?0:1}M=Xn(k),F=Te[M]=Wn[M](k);const Zn=[po,io],Se=[];function et(e,r){return e[0]==="pt"?0:1}L=et(k),H=Se[L]=Zn[L](k);const st=[uo,mo],Ce=[];function at(e,r){return e[0]==="pt"?0:1}B=at(k),V=Ce[B]=st[B](k),ve=new y({props:{code:"batched_ids = [ids, ids]",highlighted:'<span class="hljs-attr">batched_ids</span> = [ids, ids]'}}),te=new Ut({props:{$$slots:{default:[co]},$$scope:{ctx:k}}}),ke=new Bs({}),ge=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200]
]`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]
]`}}),qe=new y({props:{code:"",highlighted:`padding_id = <span class="hljs-number">100</span>

batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, padding_id],
]`}});const nt=[ho,fo],Ne=[];function tt(e,r){return e[0]==="pt"?0:1}R=tt(k),U=Ne[R]=nt[R](k),je=new Bs({});const ot=[_o,bo],De=[];function rt(e,r){return e[0]==="pt"?0:1}return G=rt(k),Q=De[G]=ot[G](k),pe=new Ut({props:{$$slots:{default:[$o]},$$scope:{ctx:k}}}),Ee=new Bs({}),Ae=new y({props:{code:"sequence = sequence[:max_sequence_length]",highlighted:"sequence = sequence[:max_sequence_length]"}}),{c(){n=m("meta"),l=h(),g(s.$$.fragment),p=h(),a=m("h1"),_=m("a"),z=m("span"),g(A.$$.fragment),W=h(),I=m("span"),Fe=d("Tratando sequ\xEAncias m\xFAltiplas"),he=h(),S.c(),Le=h(),N.c(),He=h(),Be=m("p"),Ta=d("Na se\xE7\xE3o anterior, exploramos os casos mais simples de uso: fazer infer\xEAncia sobre uma \xFAnica sequ\xEAncia de pequeno comprimento. No entanto, surgem algumas quest\xF5es:"),Vs=h(),P=m("ul"),_s=m("li"),Sa=d("Como n\xF3s tratamos diversas sequ\xEAncias?"),Ca=h(),be=m("li"),Na=d("Como n\xF3s tratamos diversas sequ\xEAncias "),$s=m("em"),Da=d("de diferentes tamanhos"),Oa=d("?"),Ma=h(),vs=m("li"),Fa=d("Os \xEDndices de vocabul\xE1rio s\xE3o as \xFAnicas entradas que permitem que um modelo funcione bem?"),La=h(),ks=m("li"),Ha=d("Existe uma sequ\xEAncia muito longa?"),Rs=h(),Ve=m("p"),Ba=d("Vamos ver que tipos de problemas estas quest\xF5es colocam, e como podemos resolv\xEA-los usando a API do \u{1F917} Transformers."),Us=h(),X=m("h2"),ae=m("a"),gs=m("span"),g(_e.$$.fragment),Va=h(),qs=m("span"),Ra=d("Modelos esperam um batch de entradas"),Gs=h(),Re=m("p"),Ua=d("No exerc\xEDcio anterior, voc\xEA viu como as sequ\xEAncias s\xE3o traduzidas em listas de n\xFAmeros. Vamos converter esta lista de n\xFAmeros em um tensor e envi\xE1-la para o modelo:"),Qs=h(),O.c(),Ue=h(),Ge=m("p"),Ga=d("Oh n\xE3o! Por que isso falhou? \u201CSeguimos os passos do pipeline na se\xE7\xE3o 2."),Ys=h(),ne=m("p"),Qa=d("O problema \xE9 que enviamos uma \xFAnica sequ\xEAncia para o modelo, enquanto que os \u{1F917} transformers esperam v\xE1rias senten\xE7as por padr\xE3o. Aqui tentamos fazer tudo o que o tokenizer fez nos bastidores quando o aplicamos a uma "),js=m("code"),Ya=d("sequ\xEAncia"),Ja=d(", mas se voc\xEA olhar com aten\xE7\xE3o, ver\xE1 que ele n\xE3o apenas converteu a lista de IDs de entrada em um tensor, mas acrescentou uma dimens\xE3o em cima dele:"),Js=h(),F.c(),Qe=h(),Ye=m("p"),Ka=d("Vamos tentar novamente e acrescentar uma nova dimens\xE3o:"),Ks=h(),H.c(),Je=h(),Ke=m("p"),Wa=d("Printamos os IDs de entrada assim como os logits resultantes - aqui est\xE1 a sa\xEDda:"),Ws=h(),V.c(),We=h(),$e=m("p"),ws=m("em"),Xa=d("Batching"),Za=d(" \xE9 o ato de enviar m\xFAltiplas senten\xE7as atrav\xE9s do modelo, todas de uma s\xF3 vez. Se voc\xEA tiver apenas uma frase, voc\xEA pode apenas construir um lote com uma \xFAnica sequ\xEAncia:"),Xs=h(),g(ve.$$.fragment),Zs=h(),Xe=m("p"),en=d("Este \xE9 um lote de duas sequ\xEAncias id\xEAnticas!"),ea=h(),g(te.$$.fragment),sa=h(),oe=m("p"),sn=d("O Batching permite que o modelo funcione quando voc\xEA o alimenta com v\xE1rias frases. Usar v\xE1rias sequ\xEAncias \xE9 t\xE3o simples quanto construir um lote com uma \xFAnica sequ\xEAncia. H\xE1 uma segunda quest\xE3o, no entanto. Quando voc\xEA est\xE1 tentando agrupar duas (ou mais) senten\xE7as, elas podem ser de comprimentos diferentes. Se voc\xEA j\xE1 trabalhou com tensores antes, voc\xEA sabe que eles precisam ser de forma retangular, ent\xE3o voc\xEA n\xE3o ser\xE1 capaz de converter a lista de IDs de entrada em um tensor diretamente. Para contornar este problema, normalmente realizamos uma "),Es=m("em"),an=d("padroniza\xE7\xE3o"),nn=d(" (padding) nas entradas."),aa=h(),Z=m("h2"),re=m("a"),ys=m("span"),g(ke.$$.fragment),tn=h(),zs=m("span"),on=d("Realizando padding nas entradas"),na=h(),Ze=m("p"),rn=d("A seguinte lista de listas n\xE3o pode ser convertida em um tensor:"),ta=h(),g(ge.$$.fragment),oa=h(),Y=m("p"),ln=d("Para contornar isso, usaremos "),As=m("em"),pn=d("padding"),mn=d(" para fazer com que nossos tensores tenham uma forma retangular. O padding garante que todas as nossas frases tenham o mesmo comprimento, acrescentando uma palavra especial chamada "),Is=m("em"),un=d("padding token"),cn=d(" \xE0s frases com menos valores. Por exemplo, se voc\xEA tiver 10 frases com 10 palavras e 1 frase com 20 palavras, o padding garantir\xE1 que todas as frases tenham 20 palavras. Em nosso exemplo, o tensor resultante se parece com isto:"),ra=h(),g(qe.$$.fragment),la=h(),le=m("p"),dn=d("O padding do ID token pode ser encontrada em "),Ps=m("code"),fn=d("tokenizer.pad_token_id"),hn=d(". Vamos utiliz\xE1-lo e enviar nossas duas frases atrav\xE9s do modelo individualmente e agrupadas em batches:"),ia=h(),U.c(),es=h(),ss=m("p"),bn=d("H\xE1 algo errado com os logits em nossas predi\xE7\xF5es em batches: a segunda fileira deveria ser a mesma que os logits para a segunda frase, mas temos valores completamente diferentes!"),pa=h(),J=m("p"),_n=d("Isto porque a caracter\xEDstica chave dos Transformer s\xE3o as camadas de aten\xE7\xE3o que "),xs=m("em"),$n=d("contextualizam"),vn=d(" cada token. Estes levar\xE3o em conta os tokens de padding, uma vez que atendem a todos os tokens de uma sequ\xEAncia. Para obter o mesmo resultado ao passar frases individuais de diferentes comprimentos pelo modelo ou ao passar um batch com as mesmas frases e os paddings aplicados, precisamos dizer a essas camadas de aten\xE7\xE3o para ignorar os tokens de padding. Isto \xE9 feito com o uso de uma m\xE1scara de aten\xE7\xE3o ("),Ts=m("em"),kn=d("attention mask"),gn=d(")."),ma=h(),ee=m("h2"),ie=m("a"),Ss=m("span"),g(je.$$.fragment),qn=h(),Cs=m("span"),jn=d("Attention masks"),ua=h(),we=m("p"),Ns=m("em"),wn=d("Attention masks"),En=d(" s\xE3o tensores com a mesma forma exata do tensor de IDs de entrada, preenchidos com 0s e 1s: 1s indicam que os tokens correspondentes devem ser atendidas, e 0s indicam que os tokens correspondentes n\xE3o devem ser atendidas (ou seja, devem ser ignoradas pelas camadas de aten\xE7\xE3o do modelo)."),ca=h(),as=m("p"),yn=d("Vamos completar o exemplo anterior com uma m\xE1scara de aten\xE7\xE3o:"),da=h(),Q.c(),ns=h(),ts=m("p"),zn=d("Agora obtemos os mesmos logits para a segunda frase do batch."),fa=h(),os=m("p"),An=d("Observe como o \xFAltimo valor da segunda sequ\xEAncia \xE9 um ID de padding, que \xE9 um valor 0 na m\xE1scara de aten\xE7\xE3o."),ha=h(),g(pe.$$.fragment),ba=h(),se=m("h2"),me=m("a"),Ds=m("span"),g(Ee.$$.fragment),In=h(),Os=m("span"),Pn=d("Sequ\xEAncias mais longas"),_a=h(),rs=m("p"),xn=d("Com os Transformer, h\xE1 um limite para os comprimentos das sequ\xEAncias, podemos passar os modelos. A maioria dos modelos manipula sequ\xEAncias de at\xE9 512 ou 1024 tokens, e se chocar\xE1 quando solicitados a processar sequ\xEAncias mais longas. H\xE1 duas solu\xE7\xF5es para este problema:"),$a=h(),ue=m("ul"),Ms=m("li"),Tn=d("Use um modelo com suporte a um comprimento mais longo de sequ\xEAncia."),Sn=h(),Fs=m("li"),Cn=d("Trunque suas sequ\xEAncias."),va=h(),K=m("p"),Nn=d("Os modelos t\xEAm diferentes comprimentos de sequ\xEAncia suportados, e alguns s\xE3o especializados no tratamento de sequ\xEAncias muito longas. O "),ye=m("a"),Dn=d("Longformer"),On=d(" \xE9 um exemplo, e outro exemplo \xE9 o "),ze=m("a"),Mn=d("LED"),Fn=d(". Se voc\xEA estiver trabalhando em uma tarefa que requer sequ\xEAncias muito longas, recomendamos que voc\xEA d\xEA uma olhada nesses modelos."),ka=h(),ce=m("p"),Ln=d("Caso contr\xE1rio, recomendamos que voc\xEA trunque suas sequ\xEAncias, especificando o par\xE2metro "),Ls=m("code"),Hn=d("max_sequence_length"),Bn=d(":"),ga=h(),g(Ae.$$.fragment),this.h()},l(e){const r=Wt('[data-svelte="svelte-1phssyn"]',document.head);n=u(r,"META",{name:!0,content:!0}),r.forEach(t),l=b(e),q(s.$$.fragment,e),p=b(e),a=u(e,"H1",{class:!0});var Oe=c(a);_=u(Oe,"A",{id:!0,class:!0,href:!0});var ls=c(_);z=u(ls,"SPAN",{});var is=c(z);q(A.$$.fragment,is),is.forEach(t),ls.forEach(t),W=b(Oe),I=u(Oe,"SPAN",{});var ps=c(I);Fe=f(ps,"Tratando sequ\xEAncias m\xFAltiplas"),ps.forEach(t),Oe.forEach(t),he=b(e),S.l(e),Le=b(e),N.l(e),He=b(e),Be=u(e,"P",{});var ms=c(Be);Ta=f(ms,"Na se\xE7\xE3o anterior, exploramos os casos mais simples de uso: fazer infer\xEAncia sobre uma \xFAnica sequ\xEAncia de pequeno comprimento. No entanto, surgem algumas quest\xF5es:"),ms.forEach(t),Vs=b(e),P=u(e,"UL",{});var x=c(P);_s=u(x,"LI",{});var us=c(_s);Sa=f(us,"Como n\xF3s tratamos diversas sequ\xEAncias?"),us.forEach(t),Ca=b(x),be=u(x,"LI",{});var Me=c(be);Na=f(Me,"Como n\xF3s tratamos diversas sequ\xEAncias "),$s=u(Me,"EM",{});var cs=c($s);Da=f(cs,"de diferentes tamanhos"),cs.forEach(t),Oa=f(Me,"?"),Me.forEach(t),Ma=b(x),vs=u(x,"LI",{});var ds=c(vs);Fa=f(ds,"Os \xEDndices de vocabul\xE1rio s\xE3o as \xFAnicas entradas que permitem que um modelo funcione bem?"),ds.forEach(t),La=b(x),ks=u(x,"LI",{});var Hs=c(ks);Ha=f(Hs,"Existe uma sequ\xEAncia muito longa?"),Hs.forEach(t),x.forEach(t),Rs=b(e),Ve=u(e,"P",{});var lt=c(Ve);Ba=f(lt,"Vamos ver que tipos de problemas estas quest\xF5es colocam, e como podemos resolv\xEA-los usando a API do \u{1F917} Transformers."),lt.forEach(t),Us=b(e),X=u(e,"H2",{class:!0});var ja=c(X);ae=u(ja,"A",{id:!0,class:!0,href:!0});var it=c(ae);gs=u(it,"SPAN",{});var pt=c(gs);q(_e.$$.fragment,pt),pt.forEach(t),it.forEach(t),Va=b(ja),qs=u(ja,"SPAN",{});var mt=c(qs);Ra=f(mt,"Modelos esperam um batch de entradas"),mt.forEach(t),ja.forEach(t),Gs=b(e),Re=u(e,"P",{});var ut=c(Re);Ua=f(ut,"No exerc\xEDcio anterior, voc\xEA viu como as sequ\xEAncias s\xE3o traduzidas em listas de n\xFAmeros. Vamos converter esta lista de n\xFAmeros em um tensor e envi\xE1-la para o modelo:"),ut.forEach(t),Qs=b(e),O.l(e),Ue=b(e),Ge=u(e,"P",{});var ct=c(Ge);Ga=f(ct,"Oh n\xE3o! Por que isso falhou? \u201CSeguimos os passos do pipeline na se\xE7\xE3o 2."),ct.forEach(t),Ys=b(e),ne=u(e,"P",{});var wa=c(ne);Qa=f(wa,"O problema \xE9 que enviamos uma \xFAnica sequ\xEAncia para o modelo, enquanto que os \u{1F917} transformers esperam v\xE1rias senten\xE7as por padr\xE3o. Aqui tentamos fazer tudo o que o tokenizer fez nos bastidores quando o aplicamos a uma "),js=u(wa,"CODE",{});var dt=c(js);Ya=f(dt,"sequ\xEAncia"),dt.forEach(t),Ja=f(wa,", mas se voc\xEA olhar com aten\xE7\xE3o, ver\xE1 que ele n\xE3o apenas converteu a lista de IDs de entrada em um tensor, mas acrescentou uma dimens\xE3o em cima dele:"),wa.forEach(t),Js=b(e),F.l(e),Qe=b(e),Ye=u(e,"P",{});var ft=c(Ye);Ka=f(ft,"Vamos tentar novamente e acrescentar uma nova dimens\xE3o:"),ft.forEach(t),Ks=b(e),H.l(e),Je=b(e),Ke=u(e,"P",{});var ht=c(Ke);Wa=f(ht,"Printamos os IDs de entrada assim como os logits resultantes - aqui est\xE1 a sa\xEDda:"),ht.forEach(t),Ws=b(e),V.l(e),We=b(e),$e=u(e,"P",{});var Vn=c($e);ws=u(Vn,"EM",{});var bt=c(ws);Xa=f(bt,"Batching"),bt.forEach(t),Za=f(Vn," \xE9 o ato de enviar m\xFAltiplas senten\xE7as atrav\xE9s do modelo, todas de uma s\xF3 vez. Se voc\xEA tiver apenas uma frase, voc\xEA pode apenas construir um lote com uma \xFAnica sequ\xEAncia:"),Vn.forEach(t),Xs=b(e),q(ve.$$.fragment,e),Zs=b(e),Xe=u(e,"P",{});var _t=c(Xe);en=f(_t,"Este \xE9 um lote de duas sequ\xEAncias id\xEAnticas!"),_t.forEach(t),ea=b(e),q(te.$$.fragment,e),sa=b(e),oe=u(e,"P",{});var Ea=c(oe);sn=f(Ea,"O Batching permite que o modelo funcione quando voc\xEA o alimenta com v\xE1rias frases. Usar v\xE1rias sequ\xEAncias \xE9 t\xE3o simples quanto construir um lote com uma \xFAnica sequ\xEAncia. H\xE1 uma segunda quest\xE3o, no entanto. Quando voc\xEA est\xE1 tentando agrupar duas (ou mais) senten\xE7as, elas podem ser de comprimentos diferentes. Se voc\xEA j\xE1 trabalhou com tensores antes, voc\xEA sabe que eles precisam ser de forma retangular, ent\xE3o voc\xEA n\xE3o ser\xE1 capaz de converter a lista de IDs de entrada em um tensor diretamente. Para contornar este problema, normalmente realizamos uma "),Es=u(Ea,"EM",{});var $t=c(Es);an=f($t,"padroniza\xE7\xE3o"),$t.forEach(t),nn=f(Ea," (padding) nas entradas."),Ea.forEach(t),aa=b(e),Z=u(e,"H2",{class:!0});var ya=c(Z);re=u(ya,"A",{id:!0,class:!0,href:!0});var vt=c(re);ys=u(vt,"SPAN",{});var kt=c(ys);q(ke.$$.fragment,kt),kt.forEach(t),vt.forEach(t),tn=b(ya),zs=u(ya,"SPAN",{});var gt=c(zs);on=f(gt,"Realizando padding nas entradas"),gt.forEach(t),ya.forEach(t),na=b(e),Ze=u(e,"P",{});var qt=c(Ze);rn=f(qt,"A seguinte lista de listas n\xE3o pode ser convertida em um tensor:"),qt.forEach(t),ta=b(e),q(ge.$$.fragment,e),oa=b(e),Y=u(e,"P",{});var fs=c(Y);ln=f(fs,"Para contornar isso, usaremos "),As=u(fs,"EM",{});var jt=c(As);pn=f(jt,"padding"),jt.forEach(t),mn=f(fs," para fazer com que nossos tensores tenham uma forma retangular. O padding garante que todas as nossas frases tenham o mesmo comprimento, acrescentando uma palavra especial chamada "),Is=u(fs,"EM",{});var wt=c(Is);un=f(wt,"padding token"),wt.forEach(t),cn=f(fs," \xE0s frases com menos valores. Por exemplo, se voc\xEA tiver 10 frases com 10 palavras e 1 frase com 20 palavras, o padding garantir\xE1 que todas as frases tenham 20 palavras. Em nosso exemplo, o tensor resultante se parece com isto:"),fs.forEach(t),ra=b(e),q(qe.$$.fragment,e),la=b(e),le=u(e,"P",{});var za=c(le);dn=f(za,"O padding do ID token pode ser encontrada em "),Ps=u(za,"CODE",{});var Et=c(Ps);fn=f(Et,"tokenizer.pad_token_id"),Et.forEach(t),hn=f(za,". Vamos utiliz\xE1-lo e enviar nossas duas frases atrav\xE9s do modelo individualmente e agrupadas em batches:"),za.forEach(t),ia=b(e),U.l(e),es=b(e),ss=u(e,"P",{});var yt=c(ss);bn=f(yt,"H\xE1 algo errado com os logits em nossas predi\xE7\xF5es em batches: a segunda fileira deveria ser a mesma que os logits para a segunda frase, mas temos valores completamente diferentes!"),yt.forEach(t),pa=b(e),J=u(e,"P",{});var hs=c(J);_n=f(hs,"Isto porque a caracter\xEDstica chave dos Transformer s\xE3o as camadas de aten\xE7\xE3o que "),xs=u(hs,"EM",{});var zt=c(xs);$n=f(zt,"contextualizam"),zt.forEach(t),vn=f(hs," cada token. Estes levar\xE3o em conta os tokens de padding, uma vez que atendem a todos os tokens de uma sequ\xEAncia. Para obter o mesmo resultado ao passar frases individuais de diferentes comprimentos pelo modelo ou ao passar um batch com as mesmas frases e os paddings aplicados, precisamos dizer a essas camadas de aten\xE7\xE3o para ignorar os tokens de padding. Isto \xE9 feito com o uso de uma m\xE1scara de aten\xE7\xE3o ("),Ts=u(hs,"EM",{});var At=c(Ts);kn=f(At,"attention mask"),At.forEach(t),gn=f(hs,")."),hs.forEach(t),ma=b(e),ee=u(e,"H2",{class:!0});var Aa=c(ee);ie=u(Aa,"A",{id:!0,class:!0,href:!0});var It=c(ie);Ss=u(It,"SPAN",{});var Pt=c(Ss);q(je.$$.fragment,Pt),Pt.forEach(t),It.forEach(t),qn=b(Aa),Cs=u(Aa,"SPAN",{});var xt=c(Cs);jn=f(xt,"Attention masks"),xt.forEach(t),Aa.forEach(t),ua=b(e),we=u(e,"P",{});var Rn=c(we);Ns=u(Rn,"EM",{});var Tt=c(Ns);wn=f(Tt,"Attention masks"),Tt.forEach(t),En=f(Rn," s\xE3o tensores com a mesma forma exata do tensor de IDs de entrada, preenchidos com 0s e 1s: 1s indicam que os tokens correspondentes devem ser atendidas, e 0s indicam que os tokens correspondentes n\xE3o devem ser atendidas (ou seja, devem ser ignoradas pelas camadas de aten\xE7\xE3o do modelo)."),Rn.forEach(t),ca=b(e),as=u(e,"P",{});var St=c(as);yn=f(St,"Vamos completar o exemplo anterior com uma m\xE1scara de aten\xE7\xE3o:"),St.forEach(t),da=b(e),Q.l(e),ns=b(e),ts=u(e,"P",{});var Ct=c(ts);zn=f(Ct,"Agora obtemos os mesmos logits para a segunda frase do batch."),Ct.forEach(t),fa=b(e),os=u(e,"P",{});var Nt=c(os);An=f(Nt,"Observe como o \xFAltimo valor da segunda sequ\xEAncia \xE9 um ID de padding, que \xE9 um valor 0 na m\xE1scara de aten\xE7\xE3o."),Nt.forEach(t),ha=b(e),q(pe.$$.fragment,e),ba=b(e),se=u(e,"H2",{class:!0});var Ia=c(se);me=u(Ia,"A",{id:!0,class:!0,href:!0});var Dt=c(me);Ds=u(Dt,"SPAN",{});var Ot=c(Ds);q(Ee.$$.fragment,Ot),Ot.forEach(t),Dt.forEach(t),In=b(Ia),Os=u(Ia,"SPAN",{});var Mt=c(Os);Pn=f(Mt,"Sequ\xEAncias mais longas"),Mt.forEach(t),Ia.forEach(t),_a=b(e),rs=u(e,"P",{});var Ft=c(rs);xn=f(Ft,"Com os Transformer, h\xE1 um limite para os comprimentos das sequ\xEAncias, podemos passar os modelos. A maioria dos modelos manipula sequ\xEAncias de at\xE9 512 ou 1024 tokens, e se chocar\xE1 quando solicitados a processar sequ\xEAncias mais longas. H\xE1 duas solu\xE7\xF5es para este problema:"),Ft.forEach(t),$a=b(e),ue=u(e,"UL",{});var Pa=c(ue);Ms=u(Pa,"LI",{});var Lt=c(Ms);Tn=f(Lt,"Use um modelo com suporte a um comprimento mais longo de sequ\xEAncia."),Lt.forEach(t),Sn=b(Pa),Fs=u(Pa,"LI",{});var Ht=c(Fs);Cn=f(Ht,"Trunque suas sequ\xEAncias."),Ht.forEach(t),Pa.forEach(t),va=b(e),K=u(e,"P",{});var bs=c(K);Nn=f(bs,"Os modelos t\xEAm diferentes comprimentos de sequ\xEAncia suportados, e alguns s\xE3o especializados no tratamento de sequ\xEAncias muito longas. O "),ye=u(bs,"A",{href:!0,rel:!0});var Bt=c(ye);Dn=f(Bt,"Longformer"),Bt.forEach(t),On=f(bs," \xE9 um exemplo, e outro exemplo \xE9 o "),ze=u(bs,"A",{href:!0,rel:!0});var Vt=c(ze);Mn=f(Vt,"LED"),Vt.forEach(t),Fn=f(bs,". Se voc\xEA estiver trabalhando em uma tarefa que requer sequ\xEAncias muito longas, recomendamos que voc\xEA d\xEA uma olhada nesses modelos."),bs.forEach(t),ka=b(e),ce=u(e,"P",{});var xa=c(ce);Ln=f(xa,"Caso contr\xE1rio, recomendamos que voc\xEA trunque suas sequ\xEAncias, especificando o par\xE2metro "),Ls=u(xa,"CODE",{});var Rt=c(Ls);Hn=f(Rt,"max_sequence_length"),Rt.forEach(t),Bn=f(xa,":"),xa.forEach(t),ga=b(e),q(Ae.$$.fragment,e),this.h()},h(){E(n,"name","hf:doc:metadata"),E(n,"content",JSON.stringify(ko)),E(_,"id","tratando-sequncias-mltiplas"),E(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(_,"href","#tratando-sequncias-mltiplas"),E(a,"class","relative group"),E(ae,"id","modelos-esperam-um-batch-de-entradas"),E(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ae,"href","#modelos-esperam-um-batch-de-entradas"),E(X,"class","relative group"),E(re,"id","realizando-padding-nas-entradas"),E(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(re,"href","#realizando-padding-nas-entradas"),E(Z,"class","relative group"),E(ie,"id","attention-masks"),E(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ie,"href","#attention-masks"),E(ee,"class","relative group"),E(me,"id","sequncias-mais-longas"),E(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(me,"href","#sequncias-mais-longas"),E(se,"class","relative group"),E(ye,"href","https://huggingface.co/transformers/model_doc/longformer.html"),E(ye,"rel","nofollow"),E(ze,"href","https://huggingface.co/transformers/model_doc/led.html"),E(ze,"rel","nofollow")},m(e,r){o(document.head,n),i(e,l,r),j(s,e,r),i(e,p,r),i(e,a,r),o(a,_),o(_,z),j(A,z,null),o(a,W),o(a,I),o(I,Fe),i(e,he,r),Ie[T].m(e,r),i(e,Le,r),Pe[C].m(e,r),i(e,He,r),i(e,Be,r),o(Be,Ta),i(e,Vs,r),i(e,P,r),o(P,_s),o(_s,Sa),o(P,Ca),o(P,be),o(be,Na),o(be,$s),o($s,Da),o(be,Oa),o(P,Ma),o(P,vs),o(vs,Fa),o(P,La),o(P,ks),o(ks,Ha),i(e,Rs,r),i(e,Ve,r),o(Ve,Ba),i(e,Us,r),i(e,X,r),o(X,ae),o(ae,gs),j(_e,gs,null),o(X,Va),o(X,qs),o(qs,Ra),i(e,Gs,r),i(e,Re,r),o(Re,Ua),i(e,Qs,r),xe[D].m(e,r),i(e,Ue,r),i(e,Ge,r),o(Ge,Ga),i(e,Ys,r),i(e,ne,r),o(ne,Qa),o(ne,js),o(js,Ya),o(ne,Ja),i(e,Js,r),Te[M].m(e,r),i(e,Qe,r),i(e,Ye,r),o(Ye,Ka),i(e,Ks,r),Se[L].m(e,r),i(e,Je,r),i(e,Ke,r),o(Ke,Wa),i(e,Ws,r),Ce[B].m(e,r),i(e,We,r),i(e,$e,r),o($e,ws),o(ws,Xa),o($e,Za),i(e,Xs,r),j(ve,e,r),i(e,Zs,r),i(e,Xe,r),o(Xe,en),i(e,ea,r),j(te,e,r),i(e,sa,r),i(e,oe,r),o(oe,sn),o(oe,Es),o(Es,an),o(oe,nn),i(e,aa,r),i(e,Z,r),o(Z,re),o(re,ys),j(ke,ys,null),o(Z,tn),o(Z,zs),o(zs,on),i(e,na,r),i(e,Ze,r),o(Ze,rn),i(e,ta,r),j(ge,e,r),i(e,oa,r),i(e,Y,r),o(Y,ln),o(Y,As),o(As,pn),o(Y,mn),o(Y,Is),o(Is,un),o(Y,cn),i(e,ra,r),j(qe,e,r),i(e,la,r),i(e,le,r),o(le,dn),o(le,Ps),o(Ps,fn),o(le,hn),i(e,ia,r),Ne[R].m(e,r),i(e,es,r),i(e,ss,r),o(ss,bn),i(e,pa,r),i(e,J,r),o(J,_n),o(J,xs),o(xs,$n),o(J,vn),o(J,Ts),o(Ts,kn),o(J,gn),i(e,ma,r),i(e,ee,r),o(ee,ie),o(ie,Ss),j(je,Ss,null),o(ee,qn),o(ee,Cs),o(Cs,jn),i(e,ua,r),i(e,we,r),o(we,Ns),o(Ns,wn),o(we,En),i(e,ca,r),i(e,as,r),o(as,yn),i(e,da,r),De[G].m(e,r),i(e,ns,r),i(e,ts,r),o(ts,zn),i(e,fa,r),i(e,os,r),o(os,An),i(e,ha,r),j(pe,e,r),i(e,ba,r),i(e,se,r),o(se,me),o(me,Ds),j(Ee,Ds,null),o(se,In),o(se,Os),o(Os,Pn),i(e,_a,r),i(e,rs,r),o(rs,xn),i(e,$a,r),i(e,ue,r),o(ue,Ms),o(Ms,Tn),o(ue,Sn),o(ue,Fs),o(Fs,Cn),i(e,va,r),i(e,K,r),o(K,Nn),o(K,ye),o(ye,Dn),o(K,On),o(K,ze),o(ze,Mn),o(K,Fn),i(e,ka,r),i(e,ce,r),o(ce,Ln),o(ce,Ls),o(Ls,Hn),o(ce,Bn),i(e,ga,r),j(Ae,e,r),qa=!0},p(e,[r]){const Oe={};r&1&&(Oe.fw=e[0]),s.$set(Oe);let ls=T;T=Gn(e),T!==ls&&(fe(),$(Ie[ls],1,1,()=>{Ie[ls]=null}),de(),S=Ie[T],S||(S=Ie[T]=Un[T](e),S.c()),v(S,1),S.m(Le.parentNode,Le));let is=C;C=Yn(e),C!==is&&(fe(),$(Pe[is],1,1,()=>{Pe[is]=null}),de(),N=Pe[C],N||(N=Pe[C]=Qn[C](e),N.c()),v(N,1),N.m(He.parentNode,He));let ps=D;D=Kn(e),D!==ps&&(fe(),$(xe[ps],1,1,()=>{xe[ps]=null}),de(),O=xe[D],O||(O=xe[D]=Jn[D](e),O.c()),v(O,1),O.m(Ue.parentNode,Ue));let ms=M;M=Xn(e),M!==ms&&(fe(),$(Te[ms],1,1,()=>{Te[ms]=null}),de(),F=Te[M],F||(F=Te[M]=Wn[M](e),F.c()),v(F,1),F.m(Qe.parentNode,Qe));let x=L;L=et(e),L!==x&&(fe(),$(Se[x],1,1,()=>{Se[x]=null}),de(),H=Se[L],H||(H=Se[L]=Zn[L](e),H.c()),v(H,1),H.m(Je.parentNode,Je));let us=B;B=at(e),B!==us&&(fe(),$(Ce[us],1,1,()=>{Ce[us]=null}),de(),V=Ce[B],V||(V=Ce[B]=st[B](e),V.c()),v(V,1),V.m(We.parentNode,We));const Me={};r&2&&(Me.$$scope={dirty:r,ctx:e}),te.$set(Me);let cs=R;R=tt(e),R!==cs&&(fe(),$(Ne[cs],1,1,()=>{Ne[cs]=null}),de(),U=Ne[R],U||(U=Ne[R]=nt[R](e),U.c()),v(U,1),U.m(es.parentNode,es));let ds=G;G=rt(e),G!==ds&&(fe(),$(De[ds],1,1,()=>{De[ds]=null}),de(),Q=De[G],Q||(Q=De[G]=ot[G](e),Q.c()),v(Q,1),Q.m(ns.parentNode,ns));const Hs={};r&2&&(Hs.$$scope={dirty:r,ctx:e}),pe.$set(Hs)},i(e){qa||(v(s.$$.fragment,e),v(A.$$.fragment,e),v(S),v(N),v(_e.$$.fragment,e),v(O),v(F),v(H),v(V),v(ve.$$.fragment,e),v(te.$$.fragment,e),v(ke.$$.fragment,e),v(ge.$$.fragment,e),v(qe.$$.fragment,e),v(U),v(je.$$.fragment,e),v(Q),v(pe.$$.fragment,e),v(Ee.$$.fragment,e),v(Ae.$$.fragment,e),qa=!0)},o(e){$(s.$$.fragment,e),$(A.$$.fragment,e),$(S),$(N),$(_e.$$.fragment,e),$(O),$(F),$(H),$(V),$(ve.$$.fragment,e),$(te.$$.fragment,e),$(ke.$$.fragment,e),$(ge.$$.fragment,e),$(qe.$$.fragment,e),$(U),$(je.$$.fragment,e),$(Q),$(pe.$$.fragment,e),$(Ee.$$.fragment,e),$(Ae.$$.fragment,e),qa=!1},d(e){t(n),e&&t(l),w(s,e),e&&t(p),e&&t(a),w(A),e&&t(he),Ie[T].d(e),e&&t(Le),Pe[C].d(e),e&&t(He),e&&t(Be),e&&t(Vs),e&&t(P),e&&t(Rs),e&&t(Ve),e&&t(Us),e&&t(X),w(_e),e&&t(Gs),e&&t(Re),e&&t(Qs),xe[D].d(e),e&&t(Ue),e&&t(Ge),e&&t(Ys),e&&t(ne),e&&t(Js),Te[M].d(e),e&&t(Qe),e&&t(Ye),e&&t(Ks),Se[L].d(e),e&&t(Je),e&&t(Ke),e&&t(Ws),Ce[B].d(e),e&&t(We),e&&t($e),e&&t(Xs),w(ve,e),e&&t(Zs),e&&t(Xe),e&&t(ea),w(te,e),e&&t(sa),e&&t(oe),e&&t(aa),e&&t(Z),w(ke),e&&t(na),e&&t(Ze),e&&t(ta),w(ge,e),e&&t(oa),e&&t(Y),e&&t(ra),w(qe,e),e&&t(la),e&&t(le),e&&t(ia),Ne[R].d(e),e&&t(es),e&&t(ss),e&&t(pa),e&&t(J),e&&t(ma),e&&t(ee),w(je),e&&t(ua),e&&t(we),e&&t(ca),e&&t(as),e&&t(da),De[G].d(e),e&&t(ns),e&&t(ts),e&&t(fa),e&&t(os),e&&t(ha),w(pe,e),e&&t(ba),e&&t(se),w(Ee),e&&t(_a),e&&t(rs),e&&t($a),e&&t(ue),e&&t(va),e&&t(K),e&&t(ka),e&&t(ce),e&&t(ga),w(Ae,e)}}}const ko={local:"tratando-sequncias-mltiplas",sections:[{local:"modelos-esperam-um-batch-de-entradas",title:"Modelos esperam um batch de entradas"},{local:"realizando-padding-nas-entradas",title:"Realizando padding nas entradas"},{local:"attention-masks",title:"Attention masks"},{local:"sequncias-mais-longas",title:"Sequ\xEAncias mais longas"}],title:"Tratando sequ\xEAncias m\xFAltiplas"};function go(k,n,l){let s="pt";return Xt(()=>{const p=new URLSearchParams(window.location.search);l(0,s=p.get("fw")||"pt")}),[s]}class Io extends Yt{constructor(n){super();Jt(this,n,go,vo,Kt,{})}}export{Io as default,ko as metadata};
