import{S as Yv,i as Kv,s as Wv,e as l,k as c,w as g,t,M as Zv,c as r,d as a,m as d,x as j,a as o,h as n,b as _,N as Jv,G as e,g as p,y as b,o as h,p as di,q as x,B as E,v as e1,n as mi}from"../../chunks/vendor-hf-doc-builder.js";import{T as Uh}from"../../chunks/Tip-hf-doc-builder.js";import{Y as fi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as sn}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Uv}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as s1}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function t1(L){let u,v;return u=new Uv({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"}]}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function n1(L){let u,v;return u=new Uv({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"}]}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function a1(L){let u,v,m,w,I;return{c(){u=l("p"),v=t("\u26A0\uFE0F Lors de la tokenisation d\u2019une seule phrase, vous ne verrez pas toujours une diff\xE9rence de vitesse entre les versions lente et rapide d\u2019un m\xEAme "),m=l("em"),w=t("tokenizer"),I=t(". En fait, la version rapide peut m\xEAme \xEAtre plus lente ! Ce n\u2019est que lorsque vous tokenisez beaucoup de textes en parall\xE8le et en m\xEAme temps que vous pourrez clairement voir la diff\xE9rence.")},l(C){u=r(C,"P",{});var R=o(u);v=n(R,"\u26A0\uFE0F Lors de la tokenisation d\u2019une seule phrase, vous ne verrez pas toujours une diff\xE9rence de vitesse entre les versions lente et rapide d\u2019un m\xEAme "),m=r(R,"EM",{});var N=o(m);w=n(N,"tokenizer"),N.forEach(a),I=n(R,". En fait, la version rapide peut m\xEAme \xEAtre plus lente ! Ce n\u2019est que lorsque vous tokenisez beaucoup de textes en parall\xE8le et en m\xEAme temps que vous pourrez clairement voir la diff\xE9rence."),R.forEach(a)},m(C,R){p(C,u,R),e(u,v),e(u,m),e(m,w),e(u,I)},d(C){C&&a(u)}}}function l1(L){let u,v,m,w,I,C,R,N,B,S,ae,X,Q,ue,z,J,A;return{c(){u=l("p"),v=t("\u270F\uFE0F "),m=l("strong"),w=t("Essayez !"),I=t(" Cr\xE9ez un "),C=l("em"),R=t("tokenizer"),N=t(" \xE0 partir des "),B=l("i"),S=t("checkpoints"),ae=c(),X=l("code"),Q=t("bert-base-cased"),ue=t(" et "),z=l("code"),J=t("roberta-base"),A=t(" et tokenisez \xAB 81s \xBB avec. Qu\u2019observez-vous ? Quels sont les identifiants des mots ?")},l(W){u=r(W,"P",{});var $=o(u);v=n($,"\u270F\uFE0F "),m=r($,"STRONG",{});var le=o(m);w=n(le,"Essayez !"),le.forEach(a),I=n($," Cr\xE9ez un "),C=r($,"EM",{});var ce=o(C);R=n(ce,"tokenizer"),ce.forEach(a),N=n($," \xE0 partir des "),B=r($,"I",{});var je=o(B);S=n(je,"checkpoints"),je.forEach(a),ae=d($),X=r($,"CODE",{});var de=o(X);Q=n(de,"bert-base-cased"),de.forEach(a),ue=n($," et "),z=r($,"CODE",{});var H=o(z);J=n(H,"roberta-base"),H.forEach(a),A=n($," et tokenisez \xAB 81s \xBB avec. Qu\u2019observez-vous ? Quels sont les identifiants des mots ?"),$.forEach(a)},m(W,$){p(W,u,$),e(u,v),e(u,m),e(m,w),e(u,I),e(u,C),e(C,R),e(u,N),e(u,B),e(B,S),e(u,ae),e(u,X),e(X,Q),e(u,ue),e(u,z),e(z,J),e(u,A)},d(W){W&&a(u)}}}function r1(L){let u,v,m,w,I,C,R,N;return{c(){u=l("p"),v=t("\u270F\uFE0F "),m=l("strong"),w=t("Essayez !"),I=t(" R\xE9digez votre propre texte et voyez si vous pouvez comprendre quels "),C=l("em"),R=t("tokens"),N=t(" sont associ\xE9s \xE0 l\u2019identifiant du mot et comment extraire les \xE9tendues de caract\xE8res pour un seul mot. Pour obtenir des points bonus, essayez d\u2019utiliser deux phrases en entr\xE9e et voyez si les identifiants ont un sens pour vous.")},l(B){u=r(B,"P",{});var S=o(u);v=n(S,"\u270F\uFE0F "),m=r(S,"STRONG",{});var ae=o(m);w=n(ae,"Essayez !"),ae.forEach(a),I=n(S," R\xE9digez votre propre texte et voyez si vous pouvez comprendre quels "),C=r(S,"EM",{});var X=o(C);R=n(X,"tokens"),X.forEach(a),N=n(S," sont associ\xE9s \xE0 l\u2019identifiant du mot et comment extraire les \xE9tendues de caract\xE8res pour un seul mot. Pour obtenir des points bonus, essayez d\u2019utiliser deux phrases en entr\xE9e et voyez si les identifiants ont un sens pour vous."),S.forEach(a)},m(B,S){p(B,u,S),e(u,v),e(u,m),e(m,w),e(u,I),e(u,C),e(C,R),e(u,N)},d(B){B&&a(u)}}}function o1(L){let u,v;return u=new fi({props:{id:"PrX4CjrVnNc"}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function i1(L){let u,v;return u=new fi({props:{id:"0E7ltQB7fM8"}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function p1(L){let u,v,m,w,I,C,R,N,B,S,ae,X,Q,ue,z,J,A,W,$,le,ce,je,de,H,we,te,ge;return Q=new y({props:{code:`
`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),H=new y({props:{code:`print(inputs["input_ids"].shape)
print(outputs.logits.shape)`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),te=new y({props:{code:`(1, 19)
(1, 19, 9)`,highlighted:`(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>)
(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>)`}}),{c(){u=l("p"),v=t("D\u2019abord, nous devons tokeniser notre entr\xE9e et la faire passer dans le mod\xE8le. Cela se fait exactement comme dans le "),m=l("a"),w=t("chapitre 2"),I=t(". Nous instancions le "),C=l("em"),R=t("tokenizer"),N=t(" et le mod\xE8le en utilisant les classes "),B=l("code"),S=t("TFAutoXxx"),ae=t(" et les utilisons ensuite dans notre exemple :"),X=c(),g(Q.$$.fragment),ue=c(),z=l("p"),J=t("Puisque nous utilisons "),A=l("code"),W=t("TFAutoModelForTokenClassification"),$=t(", nous obtenons un ensemble de logits pour chaque "),le=l("em"),ce=t("token"),je=t(" dans la s\xE9quence d\u2019entr\xE9e :"),de=c(),g(H.$$.fragment),we=c(),g(te.$$.fragment),this.h()},l(f){u=r(f,"P",{});var k=o(u);v=n(k,"D\u2019abord, nous devons tokeniser notre entr\xE9e et la faire passer dans le mod\xE8le. Cela se fait exactement comme dans le "),m=r(k,"A",{href:!0});var Me=o(m);w=n(Me,"chapitre 2"),Me.forEach(a),I=n(k,". Nous instancions le "),C=r(k,"EM",{});var Ke=o(C);R=n(Ke,"tokenizer"),Ke.forEach(a),N=n(k," et le mod\xE8le en utilisant les classes "),B=r(k,"CODE",{});var We=o(B);S=n(We,"TFAutoXxx"),We.forEach(a),ae=n(k," et les utilisons ensuite dans notre exemple :"),k.forEach(a),X=d(f),j(Q.$$.fragment,f),ue=d(f),z=r(f,"P",{});var ne=o(z);J=n(ne,"Puisque nous utilisons "),A=r(ne,"CODE",{});var Ze=o(A);W=n(Ze,"TFAutoModelForTokenClassification"),Ze.forEach(a),$=n(ne,", nous obtenons un ensemble de logits pour chaque "),le=r(ne,"EM",{});var es=o(le);ce=n(es,"token"),es.forEach(a),je=n(ne," dans la s\xE9quence d\u2019entr\xE9e :"),ne.forEach(a),de=d(f),j(H.$$.fragment,f),we=d(f),j(te.$$.fragment,f),this.h()},h(){_(m,"href","/course/fr/chapter2")},m(f,k){p(f,u,k),e(u,v),e(u,m),e(m,w),e(u,I),e(u,C),e(C,R),e(u,N),e(u,B),e(B,S),e(u,ae),p(f,X,k),b(Q,f,k),p(f,ue,k),p(f,z,k),e(z,J),e(z,A),e(A,W),e(z,$),e(z,le),e(le,ce),e(z,je),p(f,de,k),b(H,f,k),p(f,we,k),b(te,f,k),ge=!0},i(f){ge||(x(Q.$$.fragment,f),x(H.$$.fragment,f),x(te.$$.fragment,f),ge=!0)},o(f){h(Q.$$.fragment,f),h(H.$$.fragment,f),h(te.$$.fragment,f),ge=!1},d(f){f&&a(u),f&&a(X),E(Q,f),f&&a(ue),f&&a(z),f&&a(de),E(H,f),f&&a(we),E(te,f)}}}function u1(L){let u,v,m,w,I,C,R,N,B,S,ae,X,Q,ue,z,J,A,W,$,le,ce,je,de,H,we,te,ge;return Q=new y({props:{code:`
`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),H=new y({props:{code:`print(inputs["input_ids"].shape)
print(outputs.logits.shape)`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),te=new y({props:{code:`torch.Size([1, 19])
torch.Size([1, 19, 9])`,highlighted:`torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>])
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>])`}}),{c(){u=l("p"),v=t("D\u2019abord, nous devons tokeniser notre entr\xE9e et la faire passer dans le mod\xE8le. Cela se fait exactement comme dans le "),m=l("a"),w=t("chapitre 2"),I=t(". Nous instancions le "),C=l("em"),R=t("tokenizer"),N=t(" et le mod\xE8le en utilisant les classes "),B=l("code"),S=t("TFAutoXxx"),ae=t(" et les utilisons ensuite dans notre exemple :"),X=c(),g(Q.$$.fragment),ue=c(),z=l("p"),J=t("Puisque nous utilisons "),A=l("code"),W=t("AutoModelForTokenClassification"),$=t(", nous obtenons un ensemble de logits pour chaque "),le=l("em"),ce=t("token"),je=t(" dans la s\xE9quence d\u2019entr\xE9e :"),de=c(),g(H.$$.fragment),we=c(),g(te.$$.fragment),this.h()},l(f){u=r(f,"P",{});var k=o(u);v=n(k,"D\u2019abord, nous devons tokeniser notre entr\xE9e et la faire passer dans le mod\xE8le. Cela se fait exactement comme dans le "),m=r(k,"A",{href:!0});var Me=o(m);w=n(Me,"chapitre 2"),Me.forEach(a),I=n(k,". Nous instancions le "),C=r(k,"EM",{});var Ke=o(C);R=n(Ke,"tokenizer"),Ke.forEach(a),N=n(k," et le mod\xE8le en utilisant les classes "),B=r(k,"CODE",{});var We=o(B);S=n(We,"TFAutoXxx"),We.forEach(a),ae=n(k," et les utilisons ensuite dans notre exemple :"),k.forEach(a),X=d(f),j(Q.$$.fragment,f),ue=d(f),z=r(f,"P",{});var ne=o(z);J=n(ne,"Puisque nous utilisons "),A=r(ne,"CODE",{});var Ze=o(A);W=n(Ze,"AutoModelForTokenClassification"),Ze.forEach(a),$=n(ne,", nous obtenons un ensemble de logits pour chaque "),le=r(ne,"EM",{});var es=o(le);ce=n(es,"token"),es.forEach(a),je=n(ne," dans la s\xE9quence d\u2019entr\xE9e :"),ne.forEach(a),de=d(f),j(H.$$.fragment,f),we=d(f),j(te.$$.fragment,f),this.h()},h(){_(m,"href","/course/fr/chapter2")},m(f,k){p(f,u,k),e(u,v),e(u,m),e(m,w),e(u,I),e(u,C),e(C,R),e(u,N),e(u,B),e(B,S),e(u,ae),p(f,X,k),b(Q,f,k),p(f,ue,k),p(f,z,k),e(z,J),e(z,A),e(A,W),e(z,$),e(z,le),e(le,ce),e(z,je),p(f,de,k),b(H,f,k),p(f,we,k),b(te,f,k),ge=!0},i(f){ge||(x(Q.$$.fragment,f),x(H.$$.fragment,f),x(te.$$.fragment,f),ge=!0)},o(f){h(Q.$$.fragment,f),h(H.$$.fragment,f),h(te.$$.fragment,f),ge=!1},d(f){f&&a(u),f&&a(X),E(Q,f),f&&a(ue),f&&a(z),f&&a(de),E(H,f),f&&a(we),E(te,f)}}}function c1(L){let u,v;return u=new y({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

probabilities = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
predictions = predictions.numpy().tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function d1(L){let u,v;return u=new y({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
predictions = outputs.logits.argmax(dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){g(u.$$.fragment)},l(m){j(u.$$.fragment,m)},m(m,w){b(u,m,w),v=!0},i(m){v||(x(u.$$.fragment,m),v=!0)},o(m){h(u.$$.fragment,m),v=!1},d(m){E(u,m)}}}function m1(L){let u,v,m,w,I,C,R,N,B,S,ae,X,Q,ue,z,J,A,W,$,le,ce,je,de,H,we,te,ge,f,k,Me,Ke,We,ne,Ze,es,tn,hi,xi,nn,vi,gi,qt,ji,bi,wr,Cs,Cr,U,Ei,an,_i,$i,ln,ki,qi,rn,yi,wi,on,Ci,Oi,yt,Pi,zi,pn,Di,Mi,un,Ii,Ri,cn,Si,Ti,Or,us,dn,ss,Os,mn,Bi,Ai,Ni,Ps,fn,Fi,Li,Xi,hn,Hi,zs,ts,wt,xn,Gi,Vi,Ct,Qi,Ji,Ot,Ui,Yi,ns,Pt,vn,Ki,Wi,zt,Zi,ep,Dt,sp,Pr,cs,zr,as,ds,gn,Ds,tp,Mt,np,jn,ap,Dr,Ms,Mr,Ce,lp,bn,rp,op,En,ip,pp,_n,up,cp,Ir,me,dp,$n,mp,fp,kn,hp,xp,qn,vp,gp,yn,jp,bp,wn,Ep,_p,Rr,It,$p,Sr,Is,Tr,Ne,kp,Cn,qp,yp,On,wp,Cp,Br,Rs,Ar,re,Op,Pn,Pp,zp,zn,Dp,Mp,Dn,Ip,Rp,Mn,Sp,Tp,In,Bp,Ap,Rn,Np,Fp,Nr,Ss,Fr,Ts,Lr,ms,Lp,Sn,Xp,Hp,Xr,Bs,Hr,As,Gr,Oe,Gp,Tn,Vp,Qp,Bn,Jp,Up,An,Yp,Kp,Vr,Ns,Qr,Fs,Jr,be,Wp,Nn,Zp,eu,Fn,su,tu,Ln,nu,au,Xn,lu,ru,Ur,Ls,Yr,Xs,Kr,O,ou,Hn,iu,pu,Gn,uu,cu,Vn,du,mu,Qn,fu,hu,Jn,xu,vu,Un,gu,ju,Yn,bu,Eu,Kn,_u,$u,Wn,ku,qu,Zn,yu,wu,ea,Cu,Ou,sa,Pu,zu,ta,Du,Mu,na,Iu,Ru,aa,Su,Tu,Wr,Fe,Bu,la,Au,Nu,ra,Fu,Lu,Zr,fs,eo,Ee,Xu,oa,Hu,Gu,ia,Vu,Qu,pa,Ju,Uu,ua,Yu,Ku,so,Z,Wu,ca,Zu,ec,da,sc,tc,ma,nc,ac,fa,lc,rc,ha,oc,ic,xa,pc,uc,va,cc,dc,to,Hs,no,Gs,ao,_e,mc,ga,fc,hc,ja,xc,vc,ba,gc,jc,Ea,bc,Ec,lo,hs,ro,ls,xs,_a,Vs,_c,Rt,$c,$a,kc,oo,fe,qc,St,yc,wc,ka,Cc,Oc,qa,Pc,zc,Tt,Dc,Mc,ya,Ic,Rc,io,Ie,Re,Bt,rs,vs,wa,Qs,Sc,Ca,Tc,po,Le,Bc,Oa,Ac,Nc,Js,Pa,Fc,Lc,uo,Us,co,Ys,mo,$e,Xc,za,Hc,Gc,Da,Vc,Qc,Ma,Jc,Uc,Ia,Yc,Kc,fo,Ks,ho,Ws,xo,ee,Wc,Ra,Zc,ed,Sa,sd,td,Ta,nd,ad,Ba,ld,rd,Aa,od,id,Na,pd,ud,Fa,cd,dd,vo,Xe,He,La,md,fd,Xa,hd,xd,Ha,vd,gd,jd,gs,Ga,bd,Ed,Va,_d,$d,kd,js,Qa,qd,yd,Ja,wd,Cd,go,bs,Od,Ua,Pd,zd,jo,os,Es,Ya,Zs,Dd,Ka,Md,bo,Se,Te,At,_s,Id,Wa,Rd,Sd,Eo,Be,Ae,Nt,et,_o,$s,Td,Za,Bd,Ad,$o,st,ko,tt,qo,q,Nd,el,Fd,Ld,sl,Xd,Hd,tl,Gd,Vd,nl,Qd,Jd,al,Ud,Yd,ll,Kd,Wd,rl,Zd,em,ol,sm,tm,il,nm,am,pl,lm,rm,ul,om,im,cl,pm,um,dl,cm,dm,ml,mm,fm,fl,hm,xm,hl,vm,gm,xl,jm,bm,yo,T,Em,vl,_m,$m,gl,km,qm,jl,ym,wm,bl,Cm,Om,El,Pm,zm,_l,Dm,Mm,$l,Im,Rm,kl,Sm,Tm,ql,Bm,Am,yl,Nm,Fm,wl,Lm,Xm,wo,is,nt,Yh,Hm,at,Kh,Co,Ge,Gm,Cl,Vm,Qm,Ol,Jm,Um,Oo,lt,Po,rt,zo,oe,Ym,Pl,Km,Wm,zl,Zm,ef,Dl,sf,tf,Ml,nf,af,Il,lf,rf,Rl,of,pf,Do,ot,Mo,it,Io,Y,uf,Sl,cf,df,Tl,mf,ff,Bl,hf,xf,Al,vf,gf,Nl,jf,bf,Fl,Ef,_f,Ll,$f,kf,Xl,qf,yf,Ro,pt,So,ks,wf,Hl,Cf,Of,To,ut,Bo,Ft,Pf,Ao,ct,No,dt,Fo,Lt,zf,Lo,ps,qs,Gl,mt,Df,Vl,Mf,Xo,M,If,Ql,Rf,Sf,Jl,Tf,Bf,Ul,Af,Nf,Yl,Ff,Lf,Kl,Xf,Hf,Wl,Gf,Vf,Zl,Qf,Jf,er,Uf,Yf,sr,Kf,Wf,tr,Zf,eh,nr,sh,th,ar,nh,ah,Ho,G,lh,lr,rh,oh,rr,ih,ph,or,uh,ch,ir,dh,mh,pr,fh,hh,ur,xh,vh,cr,gh,jh,dr,bh,Eh,mr,_h,$h,Go,ft,Vo,ht,Qo,he,kh,fr,qh,yh,hr,wh,Ch,xr,Oh,Ph,vr,zh,Dh,gr,Mh,Ih,Jo,xt,Uo,Xt,Rh,Yo,vt,Ko,ke,Sh,jr,Th,Bh,br,Ah,Nh,Er,Fh,Lh,_r,Xh,Hh,Wo;m=new s1({props:{fw:L[0]}}),N=new sn({});const Wh=[n1,t1],gt=[];function Zh(s,i){return s[0]==="pt"?0:1}J=Zh(L),A=gt[J]=Wh[J](L),Cs=new fi({props:{id:"g8quOxoqhHQ"}}),cs=new Uh({props:{warning:!0,$$slots:{default:[a1]},$$scope:{ctx:L}}}),Ds=new sn({}),Ms=new fi({props:{id:"3umI3tm27Vw"}}),Is=new y({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
<span class="hljs-comment"># Je m&#x27;appelle Sylvain et je travaille chez Hugging Face \xE0 Brooklyn.</span>
encoding = tokenizer(example)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(encoding))`}}),Rs=new y({props:{code:"<class 'transformers.tokenization_utils_base.BatchEncoding'>",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;</span>&gt;'}}),Ss=new y({props:{code:"tokenizer.is_fast",highlighted:"tokenizer.is_fast"}}),Ts=new y({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),Bs=new y({props:{code:"encoding.is_fast",highlighted:"encoding.is_fast"}}),As=new y({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),Ns=new y({props:{code:"encoding.tokens()",highlighted:"encoding.tokens()"}}),Fs=new y({props:{code:`['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;My&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;and&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;work&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>,
 <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),Ls=new y({props:{code:"encoding.word_ids()",highlighted:"encoding.word_ids()"}}),Xs=new y({props:{code:"[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]",highlighted:'[<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-literal">None</span>]'}}),fs=new Uh({props:{$$slots:{default:[l1]},$$scope:{ctx:L}}}),Hs=new y({props:{code:`start, end = encoding.word_to_chars(3)
example[start:end]`,highlighted:`start, end = encoding.word_to_chars(<span class="hljs-number">3</span>)
example[start:end]`}}),Gs=new y({props:{code:"Sylvain",highlighted:"Sylvain"}}),hs=new Uh({props:{$$slots:{default:[r1]},$$scope:{ctx:L}}}),Vs=new sn({});const e2=[i1,o1],jt=[];function s2(s,i){return s[0]==="pt"?0:1}Ie=s2(L),Re=jt[Ie]=e2[Ie](L),Qs=new sn({}),Us=new y({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),Ys=new y({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Ks=new y({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="hljs-string">&quot;simple&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),Ws=new y({props:{code:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Zs=new sn({});const t2=[u1,p1],bt=[];function n2(s,i){return s[0]==="pt"?0:1}Se=n2(L),Te=bt[Se]=t2[Se](L);const a2=[d1,c1],Et=[];function l2(s,i){return s[0]==="pt"?0:1}return Be=l2(L),Ae=Et[Be]=a2[Be](L),et=new y({props:{code:"[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),st=new y({props:{code:"model.config.id2label",highlighted:"model.config.id2label"}}),tt=new y({props:{code:`{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}`,highlighted:`{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;O&#x27;</span>,
 <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;B-MISC&#x27;</span>,
 <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;I-MISC&#x27;</span>,
 <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;B-PER&#x27;</span>,
 <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>,
 <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;B-ORG&#x27;</span>,
 <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>,
 <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;B-LOC&#x27;</span>,
 <span class="hljs-number">8</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>}`}}),lt=new y({props:{code:`
`,highlighted:`results = []
tokens = inputs.tokens()

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        results.append(
            {<span class="hljs-string">&quot;entity&quot;</span>: label, <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred], <span class="hljs-string">&quot;word&quot;</span>: tokens[idx]}
        )

<span class="hljs-built_in">print</span>(results)`}}),rt=new y({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>}]`}}),ot=new y({props:{code:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]`,highlighted:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]`}}),it=new y({props:{code:`[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]`,highlighted:`[(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">7</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">11</span>, <span class="hljs-number">12</span>), (<span class="hljs-number">12</span>, <span class="hljs-number">14</span>), (<span class="hljs-number">14</span>, <span class="hljs-number">16</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">18</span>), (<span class="hljs-number">19</span>, <span class="hljs-number">22</span>), (<span class="hljs-number">23</span>, <span class="hljs-number">24</span>), (<span class="hljs-number">25</span>, <span class="hljs-number">29</span>), (<span class="hljs-number">30</span>, <span class="hljs-number">32</span>),
 (<span class="hljs-number">33</span>, <span class="hljs-number">35</span>), (<span class="hljs-number">35</span>, <span class="hljs-number">40</span>), (<span class="hljs-number">41</span>, <span class="hljs-number">45</span>), (<span class="hljs-number">46</span>, <span class="hljs-number">48</span>), (<span class="hljs-number">49</span>, <span class="hljs-number">57</span>), (<span class="hljs-number">57</span>, <span class="hljs-number">58</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)]`}}),pt=new y({props:{code:"example[12:14]",highlighted:'example[<span class="hljs-number">12</span>:<span class="hljs-number">14</span>]'}}),ut=new y({props:{code:"yl",highlighted:"yl"}}),ct=new y({props:{code:`
`,highlighted:`results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        start, end = offsets[idx]
        results.append(
            {
                <span class="hljs-string">&quot;entity&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred],
                <span class="hljs-string">&quot;word&quot;</span>: tokens[idx],
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )

<span class="hljs-built_in">print</span>(results)`}}),dt=new y({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),mt=new sn({}),ft=new y({props:{code:"example[33:45]",highlighted:'example[<span class="hljs-number">33</span>:<span class="hljs-number">45</span>]'}}),ht=new y({props:{code:"Hugging Face",highlighted:"Hugging Face"}}),xt=new y({props:{code:`



`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

idx = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> idx &lt; <span class="hljs-built_in">len</span>(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        <span class="hljs-comment"># Enlever le B- ou le I-</span>
        label = label[<span class="hljs-number">2</span>:]
        start, _ = offsets[idx]

        <span class="hljs-comment"># R\xE9cup\xE9rer tous les tokens \xE9tiquet\xE9s avec I-label</span>
        all_scores = []
        <span class="hljs-keyword">while</span> (
            idx &lt; <span class="hljs-built_in">len</span>(predictions)
            <span class="hljs-keyword">and</span> model.config.id2label[predictions[idx]] == <span class="hljs-string">f&quot;I-<span class="hljs-subst">{label}</span>&quot;</span>
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += <span class="hljs-number">1</span>

        <span class="hljs-comment"># Le score est la moyenne de tous les scores des tokens dans cette entit\xE9 group\xE9e</span>
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                <span class="hljs-string">&quot;entity_group&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: score,
                <span class="hljs-string">&quot;word&quot;</span>: word,
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )
    idx += <span class="hljs-number">1</span>

<span class="hljs-built_in">print</span>(results)`}}),vt=new y({props:{code:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),{c(){u=l("meta"),v=c(),g(m.$$.fragment),w=c(),I=l("h1"),C=l("a"),R=l("span"),g(N.$$.fragment),B=c(),S=l("span"),ae=t("Pouvoirs sp\xE9ciaux des "),X=l("i"),Q=t("tokenizers"),ue=t(" rapides"),z=c(),A.c(),W=c(),$=l("p"),le=t("Dans cette section, nous allons examiner de plus pr\xE8s les capacit\xE9s des "),ce=l("em"),je=t("tokenizers"),de=t(" dans \u{1F917} "),H=l("em"),we=t("Transformers"),te=t(`.
Jusqu\u2019\xE0 pr\xE9sent, nous ne les avons utilis\xE9s que pour tokeniser les entr\xE9es ou d\xE9coder les identifiants pour revenir \xE0 du texte. Mais les `),ge=l("em"),f=t("tokenizers"),k=t(", et surtout ceux soutenus par la biblioth\xE8que \u{1F917} "),Me=l("em"),Ke=t("Tokenizers"),We=t(", peuvent faire beaucoup plus. Pour illustrer ces fonctionnalit\xE9s suppl\xE9mentaires, nous allons explorer comment reproduire les r\xE9sultats des pipelines "),ne=l("code"),Ze=t("token-classification"),es=t(" (que nous avons appel\xE9 "),tn=l("code"),hi=t("ner"),xi=t(") et "),nn=l("code"),vi=t("question-answering"),gi=t(" que nous avons rencontr\xE9s pour la premi\xE8re fois dans le "),qt=l("a"),ji=t("chapitre 1"),bi=t("."),wr=c(),g(Cs.$$.fragment),Cr=c(),U=l("p"),Ei=t("Dans la discussion qui suit, nous ferons souvent la distinction entre les "),an=l("em"),_i=t("tokenizers"),$i=t(" \xAB lents \xBB et les \xAB rapides \xBB. Les "),ln=l("em"),ki=t("tokenizers"),qi=t(" lents sont ceux \xE9crits en Python \xE0 l\u2019int\xE9rieur de la biblioth\xE8que \u{1F917} "),rn=l("em"),yi=t("Transformers"),wi=t(", tandis que les rapides sont ceux fournis par \u{1F917} "),on=l("em"),Ci=t("Tokenizers"),Oi=t(" et sont cod\xE9s en Rust. Si vous vous souvenez du tableau du "),yt=l("a"),Pi=t("chapitre 5"),zi=t(" qui indiquait combien de temps il fallait \xE0 un "),pn=l("em"),Di=t("tokenizer"),Mi=t(" rapide et \xE0 un "),un=l("em"),Ii=t("tokenizer"),Ri=t(" lent pour tokeniser le jeu de donn\xE9es "),cn=l("em"),Si=t("Drug Review"),Ti=t(", vous devriez avoir une id\xE9e de la raison pour laquelle nous les appelons rapides et lents :"),Or=c(),us=l("table"),dn=l("thead"),ss=l("tr"),Os=l("th"),mn=l("em"),Bi=t("Tokenizer"),Ai=t(" rapide"),Ni=c(),Ps=l("th"),fn=l("em"),Fi=t("Tokenizer"),Li=t(" lent"),Xi=c(),hn=l("th"),Hi=c(),zs=l("tbody"),ts=l("tr"),wt=l("td"),xn=l("code"),Gi=t("batched=True"),Vi=c(),Ct=l("td"),Qi=t("10.8s"),Ji=c(),Ot=l("td"),Ui=t("4min41s"),Yi=c(),ns=l("tr"),Pt=l("td"),vn=l("code"),Ki=t("batched=False"),Wi=c(),zt=l("td"),Zi=t("59.2s"),ep=c(),Dt=l("td"),sp=t("5min3s"),Pr=c(),g(cs.$$.fragment),zr=c(),as=l("h2"),ds=l("a"),gn=l("span"),g(Ds.$$.fragment),tp=c(),Mt=l("span"),np=t("L'objet "),jn=l("i"),ap=t("BatchEncoding"),Dr=c(),g(Ms.$$.fragment),Mr=c(),Ce=l("p"),lp=t("La sortie d\u2019un "),bn=l("em"),rp=t("tokenizer"),op=t(" n\u2019est pas un simple dictionnaire Python. Ce que nous obtenons est en fait un objet sp\xE9cial "),En=l("code"),ip=t("BatchEncoding"),pp=t(". C\u2019est une sous-classe d\u2019un dictionnaire (c\u2019est pourquoi nous avons pu indexer ce r\xE9sultat sans probl\xE8me auparavant), mais avec des m\xE9thodes suppl\xE9mentaires qui sont principalement utilis\xE9es par les "),_n=l("em"),up=t("tokenizers"),cp=t(" rapides."),Ir=c(),me=l("p"),dp=t("En plus de leurs capacit\xE9s de parall\xE9lisation, la fonctionnalit\xE9 cl\xE9 des "),$n=l("em"),mp=t("tokenizers"),fp=t(" rapides est qu\u2019ils gardent toujours la trace de l\u2019\xE9tendue originale des textes d\u2019o\xF9 proviennent les "),kn=l("em"),hp=t("tokens"),xp=t(" finaux, une fonctionnalit\xE9 que nous appelons "),qn=l("em"),vp=t("mapping offset"),gp=t(". Cela permet de d\xE9bloquer des fonctionnalit\xE9s telles que le mappage de chaque mot aux "),yn=l("em"),jp=t("tokens"),bp=t(" qu\u2019il a g\xE9n\xE9r\xE9s ou le mappage de chaque caract\xE8re du texte original au "),wn=l("em"),Ep=t("token"),_p=t(" qu\u2019il contient, et vice versa."),Rr=c(),It=l("p"),$p=t("Prenons un exemple :"),Sr=c(),g(Is.$$.fragment),Tr=c(),Ne=l("p"),kp=t("Comme mentionn\xE9 pr\xE9c\xE9demment, nous obtenons un objet "),Cn=l("code"),qp=t("BatchEncoding"),yp=t(" dans la sortie du "),On=l("em"),wp=t("tokenizer"),Cp=t(" :"),Br=c(),g(Rs.$$.fragment),Ar=c(),re=l("p"),Op=t("Puisque la classe "),Pn=l("code"),Pp=t("AutoTokenizer"),zp=t(" choisit un "),zn=l("em"),Dp=t("tokenizer"),Mp=t(" rapide par d\xE9faut, nous pouvons utiliser les m\xE9thodes suppl\xE9mentaires que cet objet "),Dn=l("code"),Ip=t("BatchEncoding"),Rp=t(" fournit. Nous avons deux fa\xE7ons de v\xE9rifier si notre "),Mn=l("em"),Sp=t("tokenizer"),Tp=t(" est rapide ou lent. Nous pouvons soit v\xE9rifier l\u2019attribut "),In=l("code"),Bp=t("is_fast"),Ap=t(" du "),Rn=l("em"),Np=t("tokenizer"),Fp=t(" comme suit :"),Nr=c(),g(Ss.$$.fragment),Fr=c(),g(Ts.$$.fragment),Lr=c(),ms=l("p"),Lp=t("soit v\xE9rifier le m\xEAme attribut mais avec notre "),Sn=l("code"),Xp=t("encoding"),Hp=t(" :"),Xr=c(),g(Bs.$$.fragment),Hr=c(),g(As.$$.fragment),Gr=c(),Oe=l("p"),Gp=t("Voyons ce qu\u2019un "),Tn=l("em"),Vp=t("tokenizer"),Qp=t(" rapide nous permet de faire. Tout d\u2019abord, nous pouvons acc\xE9der aux "),Bn=l("em"),Jp=t("tokens"),Up=t(" sans avoir \xE0 reconvertir les identifiants en "),An=l("em"),Yp=t("tokens"),Kp=t(" :"),Vr=c(),g(Ns.$$.fragment),Qr=c(),g(Fs.$$.fragment),Jr=c(),be=l("p"),Wp=t("Dans ce cas, le "),Nn=l("em"),Zp=t("token"),eu=t(" \xE0 l\u2019index 5 est "),Fn=l("code"),su=t("##yl"),tu=t(" et fait partie du mot \xAB Sylvain \xBB dans la phrase originale. Nous pouvons \xE9galement utiliser la m\xE9thode "),Ln=l("code"),nu=t("word_ids()"),au=t(" pour obtenir l\u2019index du mot dont provient chaque "),Xn=l("em"),lu=t("token"),ru=t(" :"),Ur=c(),g(Ls.$$.fragment),Yr=c(),g(Xs.$$.fragment),Kr=c(),O=l("p"),ou=t("On peut voir que les "),Hn=l("em"),iu=t("tokens"),pu=t(" sp\xE9ciaux du "),Gn=l("em"),uu=t("tokenizer"),cu=t(", "),Vn=l("code"),du=t("[CLS]"),mu=t(" et "),Qn=l("code"),fu=t("[SEP]"),hu=t(", sont mis en correspondance avec "),Jn=l("code"),xu=t("None"),vu=t(" et que chaque "),Un=l("em"),gu=t("token"),ju=t(" est mis en correspondance avec le mot dont il provient. Ceci est particuli\xE8rement utile pour d\xE9terminer si un "),Yn=l("em"),bu=t("token"),Eu=t(" est au d\xE9but d\u2019un mot ou si deux "),Kn=l("em"),_u=t("tokens"),$u=t(" sont dans le m\xEAme mot. Nous pourrions nous appuyer sur le pr\xE9fixe "),Wn=l("code"),ku=t("##"),qu=t(" pour cela, mais il ne fonctionne que pour les "),Zn=l("em"),yu=t("tokenizers"),wu=t(" de type BERT. Cette m\xE9thode fonctionne pour n\u2019importe quel type de "),ea=l("em"),Cu=t("tokenizer"),Ou=t(", du moment qu\u2019il est rapide. Dans le chapitre suivant, nous verrons comment utiliser cette capacit\xE9 pour appliquer correctement les \xE9tiquettes que nous avons pour chaque mot aux "),sa=l("em"),Pu=t("tokens"),zu=t(" dans des t\xE2ches comme la reconnaissance d\u2019entit\xE9s nomm\xE9es et le POS ("),ta=l("em"),Du=t("Part-of-speech"),Mu=t("). Nous pouvons \xE9galement l\u2019utiliser pour masquer tous les "),na=l("em"),Iu=t("tokens"),Ru=t(" provenant du m\xEAme mot dans la mod\xE9lisation du langage masqu\xE9 (une technique appel\xE9e "),aa=l("em"),Su=t("whole word masking"),Tu=t(")."),Wr=c(),Fe=l("p"),Bu=t("La notion de ce qu\u2019est un mot est compliqu\xE9e. Par exemple, est-ce que \xAB I\u2019ll \xBB (contraction de \xAB I will \xBB) compte pour un ou deux mots ? Cela d\xE9pend en fait du "),la=l("em"),Au=t("tokenizer"),Nu=t(" et de l\u2019op\xE9ration de pr\xE9tok\xE9nisation qu\u2019il applique. Certains "),ra=l("em"),Fu=t("tokenizer"),Lu=t(" se contentent de s\xE9parer les espaces et consid\xE8rent donc qu\u2019il s\u2019agit d\u2019un seul mot. D\u2019autres utilisent la ponctuation en plus des espaces et consid\xE8rent donc qu\u2019il s\u2019agit de deux mots."),Zr=c(),g(fs.$$.fragment),eo=c(),Ee=l("p"),Xu=t("De m\xEAme, il existe une m\xE9thode "),oa=l("code"),Hu=t("sentence_ids()"),Gu=t(" que nous pouvons utiliser pour associer un "),ia=l("em"),Vu=t("token"),Qu=t(" \xE0 la phrase dont il provient (bien que dans ce cas, le "),pa=l("code"),Ju=t("token_type_ids"),Uu=t(" retourn\xE9 par le "),ua=l("em"),Yu=t("tokenizer"),Ku=t(" peut nous donner la m\xEAme information)."),so=c(),Z=l("p"),Wu=t("Enfin, nous pouvons faire correspondre n\u2019importe quel mot ou "),ca=l("em"),Zu=t("token"),ec=t(" aux caract\xE8res du texte d\u2019origine (et vice versa) gr\xE2ce aux m\xE9thodes "),da=l("code"),sc=t("word_to_chars()"),tc=t(" ou "),ma=l("code"),nc=t("token_to_chars()"),ac=t(" et "),fa=l("code"),lc=t("char_to_word()"),rc=t(" ou "),ha=l("code"),oc=t("char_to_token()"),ic=t(". Par exemple, la m\xE9thode "),xa=l("code"),pc=t("word_ids()"),uc=t(" nous a dit que "),va=l("code"),cc=t("##yl"),dc=t(" fait partie du mot \xE0 l\u2019indice 3, mais de quel mot s\u2019agit-il dans la phrase ? Nous pouvons le d\xE9couvrir comme ceci :"),to=c(),g(Hs.$$.fragment),no=c(),g(Gs.$$.fragment),ao=c(),_e=l("p"),mc=t("Comme nous l\u2019avons mentionn\xE9 pr\xE9c\xE9demment, tout ceci est rendu possible par le fait que le "),ga=l("em"),fc=t("tokenizer"),hc=t(" rapide garde la trace de la partie du texte d\u2019o\xF9 provient chaque "),ja=l("em"),xc=t("token"),vc=t(" dans une liste d\u2019"),ba=l("em"),gc=t("offsets"),jc=t(". Pour illustrer leur utilisation, nous allons maintenant vous montrer comment reproduire manuellement les r\xE9sultats du pipeline "),Ea=l("code"),bc=t("token-classification"),Ec=t("."),lo=c(),g(hs.$$.fragment),ro=c(),ls=l("h2"),xs=l("a"),_a=l("span"),g(Vs.$$.fragment),_c=c(),Rt=l("span"),$c=t("A l'int\xE9rieur du pipeline "),$a=l("code"),kc=t("token-classification"),oo=c(),fe=l("p"),qc=t("Dans le "),St=l("a"),yc=t("chapitre 1"),wc=t(", nous avons eu un premier aper\xE7u de la NER (o\xF9 la t\xE2che est d\u2019identifier les parties du texte qui correspondent \xE0 des entit\xE9s telles que des personnes, des lieux ou des organisations) avec la fonction "),ka=l("code"),Cc=t("pipeline()"),Oc=t(" de \u{1F917} "),qa=l("em"),Pc=t("Transformers"),zc=t(". Puis, dans le "),Tt=l("a"),Dc=t("chapitre 2"),Mc=t(", nous avons vu comment un pipeline regroupe les trois \xE9tapes n\xE9cessaires pour obtenir les pr\xE9dictions \xE0 partir d\u2019un texte brut : la tokenisation, le passage des entr\xE9es dans le mod\xE8le et le post-traitement. Les deux premi\xE8res \xE9tapes du pipeline de "),ya=l("code"),Ic=t("token-classification"),Rc=t(" sont les m\xEAmes que dans tout autre pipeline mais le post-traitement est un peu plus complexe. Voyons comment !"),io=c(),Re.c(),Bt=c(),rs=l("h3"),vs=l("a"),wa=l("span"),g(Qs.$$.fragment),Sc=c(),Ca=l("span"),Tc=t("Obtenir les r\xE9sultats de base avec le pipeline"),po=c(),Le=l("p"),Bc=t("Tout d\u2019abord, prenons un pipeline de classification de "),Oa=l("em"),Ac=t("tokens"),Nc=t(" afin d\u2019obtenir des r\xE9sultats \xE0 comparer manuellement. Le mod\xE8le utilis\xE9 par d\xE9faut est "),Js=l("a"),Pa=l("code"),Fc=t("dbmdz/bert-large-cased-finetuned-conll03-english"),Lc=t(". Il effectue une NER sur les phrases :"),uo=c(),g(Us.$$.fragment),co=c(),g(Ys.$$.fragment),mo=c(),$e=l("p"),Xc=t("Le mod\xE8le a correctement identifi\xE9 chaque "),za=l("em"),Hc=t("token"),Gc=t(" g\xE9n\xE9r\xE9 par \xAB Sylvain \xBB comme une personne, chaque "),Da=l("em"),Vc=t("token"),Qc=t(" g\xE9n\xE9r\xE9 par \xAB Hugging Face \xBB comme une organisation, et le "),Ma=l("em"),Jc=t("token"),Uc=t(" \xAB Brooklyn \xBB comme un lieu. Nous pouvons \xE9galement demander au pipeline de regrouper les "),Ia=l("em"),Yc=t("tokens"),Kc=t(" qui correspondent \xE0 la m\xEAme entit\xE9 :"),fo=c(),g(Ks.$$.fragment),ho=c(),g(Ws.$$.fragment),xo=c(),ee=l("p"),Wc=t("La propri\xE9t\xE9 "),Ra=l("code"),Zc=t("aggregation_strategy"),ed=t(" choisie va changer les scores calcul\xE9s pour chaque entit\xE9 group\xE9e. Avec "),Sa=l("code"),sd=t('"simple"'),td=t(" le score est juste la moyenne des scores de chaque "),Ta=l("em"),nd=t("token"),ad=t(" dans l\u2019entit\xE9 donn\xE9e. Par exemple, le score de \xAB Sylvain \xBB est la moyenne des scores que nous avons vu dans l\u2019exemple pr\xE9c\xE9dent pour les tokens "),Ba=l("code"),ld=t("S"),rd=t(", "),Aa=l("code"),od=t("##yl"),id=t(", "),Na=l("code"),pd=t("##va"),ud=t(", et "),Fa=l("code"),cd=t("##in"),dd=t(". D\u2019autres strat\xE9gies sont disponibles :"),vo=c(),Xe=l("ul"),He=l("li"),La=l("code"),md=t('"first"'),fd=t(", o\xF9 le score de chaque entit\xE9 est le score du premier "),Xa=l("em"),hd=t("token"),xd=t(" de cette entit\xE9 (donc pour \xAB Sylvain \xBB ce serait 0.993828, le score du token "),Ha=l("code"),vd=t("S"),gd=t(")"),jd=c(),gs=l("li"),Ga=l("code"),bd=t('"max"'),Ed=t(", o\xF9 le score de chaque entit\xE9 est le score maximal des "),Va=l("em"),_d=t("tokens"),$d=t(" de cette entit\xE9 (ainsi, pour \xAB Hugging Face \xBB, le score de \xAB Face \xBB serait de 0,98879766)."),kd=c(),js=l("li"),Qa=l("code"),qd=t('"average"'),yd=t(", o\xF9 le score de chaque entit\xE9 est la moyenne des scores des mots qui composent cette entit\xE9 (ainsi, pour \xAB Sylvain \xBB, il n\u2019y aurait pas de diff\xE9rence avec la strat\xE9gie "),Ja=l("code"),wd=t('"simple"'),Cd=t(", mais \u201CHugging Face\u201D aurait un score de 0,9819, la moyenne des scores de \xAB Hugging \xBB, 0,975, et \xAB Face \xBB, 0,98879)."),go=c(),bs=l("p"),Od=t("Voyons maintenant comment obtenir ces r\xE9sultats sans utiliser la fonction "),Ua=l("code"),Pd=t("pipeline()"),zd=t(" !"),jo=c(),os=l("h3"),Es=l("a"),Ya=l("span"),g(Zs.$$.fragment),Dd=c(),Ka=l("span"),Md=t("Des entr\xE9es aux pr\xE9dictions"),bo=c(),Te.c(),At=c(),_s=l("p"),Id=t("Nous avons un batch avec 1 s\xE9quence de 19 "),Wa=l("em"),Rd=t("tokens"),Sd=t(" et le mod\xE8le a 9 \xE9tiquettes diff\xE9rentes. Ainsi, la sortie du mod\xE8le a une forme de 1 x 19 x 9. Comme pour le pipeline de classification de texte, nous utilisons une fonction softmax pour convertir ces logits en probabilit\xE9s et nous prenons l\u2019argmax pour obtenir des pr\xE9dictions (notez que nous pouvons prendre l\u2019argmax sur les logits car la fonction softmax ne change pas l\u2019ordre) :"),Eo=c(),Ae.c(),Nt=c(),g(et.$$.fragment),_o=c(),$s=l("p"),Td=t("L\u2019attribut "),Za=l("code"),Bd=t("model.config.id2label"),Ad=t(" contient la correspondance entre les index et les \xE9tiquettes que nous pouvons utiliser pour donner un sens aux pr\xE9dictions :"),$o=c(),g(st.$$.fragment),ko=c(),g(tt.$$.fragment),qo=c(),q=l("p"),Nd=t("Comme nous l\u2019avons vu pr\xE9c\xE9demment, il y a 9 \xE9tiquettes : "),el=l("code"),Fd=t("O"),Ld=t(" est le label pour les "),sl=l("em"),Xd=t("tokens"),Hd=t(" qui ne sont dans aucune entit\xE9 nomm\xE9e (il signifie "),tl=l("em"),Gd=t("outside"),Vd=t(" (en dehors)) et nous avons ensuite deux labels pour chaque type d\u2019entit\xE9 (divers, personne, organisation et lieu). L\u2019\xE9tiquette "),nl=l("code"),Qd=t("B-XXX"),Jd=t(" indique que le "),al=l("em"),Ud=t("token"),Yd=t(" est au d\xE9but d\u2019une entit\xE9 "),ll=l("code"),Kd=t("XXX"),Wd=t(" et l\u2019\xE9tiquette "),rl=l("code"),Zd=t("I-XXX"),em=t(" indique que le "),ol=l("em"),sm=t("token"),tm=t(" est \xE0 l\u2019int\xE9rieur de l\u2019entit\xE9 "),il=l("code"),nm=t("XXX"),am=t(". Par exemple, dans l\u2019exemple actuel, nous nous attendons \xE0 ce que notre mod\xE8le classe le "),pl=l("em"),lm=t("token"),rm=c(),ul=l("code"),om=t("S"),im=t(" comme "),cl=l("code"),pm=t("B-PER"),um=t(" (d\xE9but d\u2019une entit\xE9 personne) et les "),dl=l("em"),cm=t("tokens"),dm=c(),ml=l("code"),mm=t("##yl"),fm=t(", "),fl=l("code"),hm=t("##va"),xm=t(" et "),hl=l("code"),vm=t("##in"),gm=t(" comme "),xl=l("code"),jm=t("I-PER"),bm=t(" (\xE0 l\u2019int\xE9rieur d\u2019une entit\xE9 personne)."),yo=c(),T=l("p"),Em=t("Vous pourriez penser que le mod\xE8le s\u2019est tromp\xE9 ici car il a attribu\xE9 l\u2019\xE9tiquette "),vl=l("code"),_m=t("I-PER"),$m=t(" \xE0 ces quatre "),gl=l("em"),km=t("tokens"),qm=t(" mais ce n\u2019est pas tout \xE0 fait vrai. Il existe en fait deux formats pour ces \xE9tiquettes "),jl=l("code"),ym=t("B-"),wm=t(" et "),bl=l("code"),Cm=t("I-"),Om=t(" : "),El=l("em"),Pm=t("IOB1"),zm=t(" et "),_l=l("em"),Dm=t("IOB2"),Mm=t(". Le format IOB2 (en rose ci-dessous) est celui que nous avons introduit alors que dans le format IOB1 (en bleu), les \xE9tiquettes commen\xE7ant par "),$l=l("code"),Im=t("B-"),Rm=t(" ne sont jamais utilis\xE9es que pour s\xE9parer deux entit\xE9s adjacentes du m\xEAme type. Le mod\xE8le que nous utilisons a \xE9t\xE9 "),kl=l("em"),Sm=t("finetun\xE9"),Tm=t(" sur un jeu de donn\xE9es utilisant ce format, c\u2019est pourquoi il attribue le label "),ql=l("code"),Bm=t("I-PER"),Am=t(" au "),yl=l("em"),Nm=t("token"),Fm=c(),wl=l("code"),Lm=t("S"),Xm=t("."),wo=c(),is=l("div"),nt=l("img"),Hm=c(),at=l("img"),Co=c(),Ge=l("p"),Gm=t("Nous sommes \xE0 pr\xE9sent pr\xEAts \xE0 reproduire (presque enti\xE8rement) les r\xE9sultats du premier pipeline. Nous pouvons simplement r\xE9cup\xE9rer le score et le label de chaque "),Cl=l("em"),Vm=t("token"),Qm=t(" qui n\u2019a pas \xE9t\xE9 class\xE9 comme "),Ol=l("code"),Jm=t("O"),Um=t(" :"),Oo=c(),g(lt.$$.fragment),Po=c(),g(rt.$$.fragment),zo=c(),oe=l("p"),Ym=t("C\u2019est tr\xE8s similaire \xE0 ce que nous avions avant, \xE0 une exception pr\xE8s : le pipeline nous a aussi donn\xE9 des informations sur le "),Pl=l("code"),Km=t("d\xE9but"),Wm=t(" et la "),zl=l("code"),Zm=t("fin"),ef=t(" de chaque entit\xE9 dans la phrase originale. C\u2019est l\xE0 que notre "),Dl=l("em"),sf=t("offset mapping"),tf=t(" va entrer en jeu. Pour obtenir les "),Ml=l("em"),nf=t("offsets"),af=t(", il suffit de d\xE9finir "),Il=l("code"),lf=t("return_offsets_mapping=True"),rf=t(" lorsque nous appliquons le "),Rl=l("em"),of=t("tokenizer"),pf=t(" \xE0 nos entr\xE9es :"),Do=c(),g(ot.$$.fragment),Mo=c(),g(it.$$.fragment),Io=c(),Y=l("p"),uf=t("Chaque "),Sl=l("em"),cf=t("tuple"),df=t(" est l\u2019\xE9tendue de texte correspondant \xE0 chaque "),Tl=l("em"),mf=t("token"),ff=t(" o\xF9 "),Bl=l("code"),hf=t("(0, 0)"),xf=t(" est r\xE9serv\xE9 aux "),Al=l("em"),vf=t("tokens"),gf=t(" sp\xE9ciaux. Nous avons vu pr\xE9c\xE9demment que le "),Nl=l("em"),jf=t("token"),bf=t(" \xE0 l\u2019index 5 est "),Fl=l("code"),Ef=t("##yl"),_f=t(", qui a "),Ll=l("code"),$f=t("(12, 14)"),kf=t(" comme "),Xl=l("em"),qf=t("offsets"),yf=t(" ici. Si on prend la tranche correspondante dans notre exemple :"),Ro=c(),g(pt.$$.fragment),So=c(),ks=l("p"),wf=t("nous obtenons le bon espace de texte sans le "),Hl=l("code"),Cf=t("##"),Of=t(" :"),To=c(),g(ut.$$.fragment),Bo=c(),Ft=l("p"),Pf=t("En utilisant cela, nous pouvons maintenant compl\xE9ter les r\xE9sultats pr\xE9c\xE9dents :"),Ao=c(),g(ct.$$.fragment),No=c(),g(dt.$$.fragment),Fo=c(),Lt=l("p"),zf=t("C\u2019est la m\xEAme chose que ce que nous avons obtenu avec le premier pipeline !"),Lo=c(),ps=l("h3"),qs=l("a"),Gl=l("span"),g(mt.$$.fragment),Df=c(),Vl=l("span"),Mf=t("Regroupement des entit\xE9s"),Xo=c(),M=l("p"),If=t("L\u2019utilisation des "),Ql=l("em"),Rf=t("offsets"),Sf=t(" pour d\xE9terminer les cl\xE9s de d\xE9but et de fin pour chaque entit\xE9 est pratique mais cette information n\u2019est pas strictement n\xE9cessaire. Cependant, lorsque nous voulons regrouper les entit\xE9s, les "),Jl=l("em"),Tf=t("offsets"),Bf=t(" nous \xE9pargnent un batch de code compliqu\xE9. Par exemple, si nous voulions regrouper les "),Ul=l("em"),Af=t("tokens"),Nf=c(),Yl=l("code"),Ff=t("Hu"),Lf=t(", "),Kl=l("code"),Xf=t("##gging"),Hf=t(", et "),Wl=l("code"),Gf=t("Face"),Vf=t(", nous pourrions \xE9tablir des r\xE8gles sp\xE9ciales disant que les deux premiers devraient \xEAtre attach\xE9s tout en enlevant le "),Zl=l("code"),Qf=t("##"),Jf=t(", et le "),er=l("code"),Uf=t("Face"),Yf=t(" devrait \xEAtre ajout\xE9 avec un espace puisqu\u2019il ne commence pas par "),sr=l("code"),Kf=t("##"),Wf=t(" mais cela ne fonctionnerait que pour ce type particulier de "),tr=l("em"),Zf=t("tokenizer"),eh=t(". Il faudrait \xE9crire un autre ensemble de r\xE8gles pour un "),nr=l("em"),sh=t("tokenizer"),th=t(" de type SentencePiece ou "),ar=l("em"),nh=t("Byte-Pair-Encoding"),ah=t(" (voir plus loin dans ce chapitre)."),Ho=c(),G=l("p"),lh=t("Avec les "),lr=l("em"),rh=t("offsets"),oh=t(", tout ce code personnalis\xE9 dispara\xEEt : il suffit de prendre l\u2019intervalle du texte original qui commence par le premier "),rr=l("em"),ih=t("token"),ph=t(" et se termine par le dernier "),or=l("em"),uh=t("token"),ch=t(". Ainsi, dans le cas des "),ir=l("em"),dh=t("tokens"),mh=c(),pr=l("code"),fh=t("Hu"),hh=t(", "),ur=l("code"),xh=t("##gging"),vh=t(", et "),cr=l("code"),gh=t("Face"),jh=t(", nous devrions commencer au caract\xE8re 33 (le d\xE9but de "),dr=l("code"),bh=t("Hu"),Eh=t(") et finir avant le caract\xE8re 45 (la fin de "),mr=l("code"),_h=t("Face"),$h=t(") :"),Go=c(),g(ft.$$.fragment),Vo=c(),g(ht.$$.fragment),Qo=c(),he=l("p"),kh=t("Pour \xE9crire le code qui post-traite les pr\xE9dictions tout en regroupant les entit\xE9s, nous regrouperons les entit\xE9s qui sont cons\xE9cutives et \xE9tiquet\xE9es avec "),fr=l("code"),qh=t("I-XXX"),yh=t(", \xE0 l\u2019exception de la premi\xE8re, qui peut \xEAtre \xE9tiquet\xE9e comme "),hr=l("code"),wh=t("B-XXX"),Ch=t(" ou "),xr=l("code"),Oh=t("I-XXX"),Ph=t(" (ainsi, nous arr\xEAtons de regrouper une entit\xE9 lorsque nous obtenons un "),vr=l("code"),zh=t("O"),Dh=t(", un nouveau type d\u2019entit\xE9, ou un "),gr=l("code"),Mh=t("B-XXX"),Ih=t(" qui nous indique qu\u2019une entit\xE9 du m\xEAme type commence) :"),Jo=c(),g(xt.$$.fragment),Uo=c(),Xt=l("p"),Rh=t("Et nous obtenons les m\xEAmes r\xE9sultats qu\u2019avec notre deuxi\xE8me pipeline !"),Yo=c(),g(vt.$$.fragment),Ko=c(),ke=l("p"),Sh=t("Un autre exemple de t\xE2che o\xF9 ces "),jr=l("em"),Th=t("offsets"),Bh=t(" sont extr\xEAmement utiles est la r\xE9ponse aux questions. Plonger dans ce pipeline, ce que nous ferons dans la section suivante, nous permettra de jeter un coup d\u2019\u0153il \xE0 une derni\xE8re caract\xE9ristique des "),br=l("em"),Ah=t("tokenizers"),Nh=t(" de la biblioth\xE8que \u{1F917} "),Er=l("em"),Fh=t("Transformers"),Lh=t(" : la gestion des "),_r=l("em"),Xh=t("tokens"),Hh=t(" qui d\xE9bordent lorsque nous tronquons une entr\xE9e \xE0 une longueur donn\xE9e."),this.h()},l(s){const i=Zv('[data-svelte="svelte-1phssyn"]',document.head);u=r(i,"META",{name:!0,content:!0}),i.forEach(a),v=d(s),j(m.$$.fragment,s),w=d(s),I=r(s,"H1",{class:!0});var _t=o(I);C=r(_t,"A",{id:!0,class:!0,href:!0});var Ht=o(C);R=r(Ht,"SPAN",{});var $r=o(R);j(N.$$.fragment,$r),$r.forEach(a),Ht.forEach(a),B=d(_t),S=r(_t,"SPAN",{});var $t=o(S);ae=n($t,"Pouvoirs sp\xE9ciaux des "),X=r($t,"I",{});var kr=o(X);Q=n(kr,"tokenizers"),kr.forEach(a),ue=n($t," rapides"),$t.forEach(a),_t.forEach(a),z=d(s),A.l(s),W=d(s),$=r(s,"P",{});var K=o($);le=n(K,"Dans cette section, nous allons examiner de plus pr\xE8s les capacit\xE9s des "),ce=r(K,"EM",{});var Gt=o(ce);je=n(Gt,"tokenizers"),Gt.forEach(a),de=n(K," dans \u{1F917} "),H=r(K,"EM",{});var Vt=o(H);we=n(Vt,"Transformers"),Vt.forEach(a),te=n(K,`.
Jusqu\u2019\xE0 pr\xE9sent, nous ne les avons utilis\xE9s que pour tokeniser les entr\xE9es ou d\xE9coder les identifiants pour revenir \xE0 du texte. Mais les `),ge=r(K,"EM",{});var r2=o(ge);f=n(r2,"tokenizers"),r2.forEach(a),k=n(K,", et surtout ceux soutenus par la biblioth\xE8que \u{1F917} "),Me=r(K,"EM",{});var o2=o(Me);Ke=n(o2,"Tokenizers"),o2.forEach(a),We=n(K,", peuvent faire beaucoup plus. Pour illustrer ces fonctionnalit\xE9s suppl\xE9mentaires, nous allons explorer comment reproduire les r\xE9sultats des pipelines "),ne=r(K,"CODE",{});var i2=o(ne);Ze=n(i2,"token-classification"),i2.forEach(a),es=n(K," (que nous avons appel\xE9 "),tn=r(K,"CODE",{});var p2=o(tn);hi=n(p2,"ner"),p2.forEach(a),xi=n(K,") et "),nn=r(K,"CODE",{});var u2=o(nn);vi=n(u2,"question-answering"),u2.forEach(a),gi=n(K," que nous avons rencontr\xE9s pour la premi\xE8re fois dans le "),qt=r(K,"A",{href:!0});var c2=o(qt);ji=n(c2,"chapitre 1"),c2.forEach(a),bi=n(K,"."),K.forEach(a),wr=d(s),j(Cs.$$.fragment,s),Cr=d(s),U=r(s,"P",{});var ie=o(U);Ei=n(ie,"Dans la discussion qui suit, nous ferons souvent la distinction entre les "),an=r(ie,"EM",{});var d2=o(an);_i=n(d2,"tokenizers"),d2.forEach(a),$i=n(ie," \xAB lents \xBB et les \xAB rapides \xBB. Les "),ln=r(ie,"EM",{});var m2=o(ln);ki=n(m2,"tokenizers"),m2.forEach(a),qi=n(ie," lents sont ceux \xE9crits en Python \xE0 l\u2019int\xE9rieur de la biblioth\xE8que \u{1F917} "),rn=r(ie,"EM",{});var f2=o(rn);yi=n(f2,"Transformers"),f2.forEach(a),wi=n(ie,", tandis que les rapides sont ceux fournis par \u{1F917} "),on=r(ie,"EM",{});var h2=o(on);Ci=n(h2,"Tokenizers"),h2.forEach(a),Oi=n(ie," et sont cod\xE9s en Rust. Si vous vous souvenez du tableau du "),yt=r(ie,"A",{href:!0});var x2=o(yt);Pi=n(x2,"chapitre 5"),x2.forEach(a),zi=n(ie," qui indiquait combien de temps il fallait \xE0 un "),pn=r(ie,"EM",{});var v2=o(pn);Di=n(v2,"tokenizer"),v2.forEach(a),Mi=n(ie," rapide et \xE0 un "),un=r(ie,"EM",{});var g2=o(un);Ii=n(g2,"tokenizer"),g2.forEach(a),Ri=n(ie," lent pour tokeniser le jeu de donn\xE9es "),cn=r(ie,"EM",{});var j2=o(cn);Si=n(j2,"Drug Review"),j2.forEach(a),Ti=n(ie,", vous devriez avoir une id\xE9e de la raison pour laquelle nous les appelons rapides et lents :"),ie.forEach(a),Or=d(s),us=r(s,"TABLE",{});var Zo=o(us);dn=r(Zo,"THEAD",{});var b2=o(dn);ss=r(b2,"TR",{});var Qt=o(ss);Os=r(Qt,"TH",{align:!0});var Gh=o(Os);mn=r(Gh,"EM",{});var E2=o(mn);Bi=n(E2,"Tokenizer"),E2.forEach(a),Ai=n(Gh," rapide"),Gh.forEach(a),Ni=d(Qt),Ps=r(Qt,"TH",{align:!0});var Vh=o(Ps);fn=r(Vh,"EM",{});var _2=o(fn);Fi=n(_2,"Tokenizer"),_2.forEach(a),Li=n(Vh," lent"),Vh.forEach(a),Xi=d(Qt),hn=r(Qt,"TH",{align:!0}),o(hn).forEach(a),Qt.forEach(a),b2.forEach(a),Hi=d(Zo),zs=r(Zo,"TBODY",{});var ei=o(zs);ts=r(ei,"TR",{});var Jt=o(ts);wt=r(Jt,"TD",{align:!0});var $2=o(wt);xn=r($2,"CODE",{});var k2=o(xn);Gi=n(k2,"batched=True"),k2.forEach(a),$2.forEach(a),Vi=d(Jt),Ct=r(Jt,"TD",{align:!0});var q2=o(Ct);Qi=n(q2,"10.8s"),q2.forEach(a),Ji=d(Jt),Ot=r(Jt,"TD",{align:!0});var y2=o(Ot);Ui=n(y2,"4min41s"),y2.forEach(a),Jt.forEach(a),Yi=d(ei),ns=r(ei,"TR",{});var Ut=o(ns);Pt=r(Ut,"TD",{align:!0});var w2=o(Pt);vn=r(w2,"CODE",{});var C2=o(vn);Ki=n(C2,"batched=False"),C2.forEach(a),w2.forEach(a),Wi=d(Ut),zt=r(Ut,"TD",{align:!0});var O2=o(zt);Zi=n(O2,"59.2s"),O2.forEach(a),ep=d(Ut),Dt=r(Ut,"TD",{align:!0});var P2=o(Dt);sp=n(P2,"5min3s"),P2.forEach(a),Ut.forEach(a),ei.forEach(a),Zo.forEach(a),Pr=d(s),j(cs.$$.fragment,s),zr=d(s),as=r(s,"H2",{class:!0});var si=o(as);ds=r(si,"A",{id:!0,class:!0,href:!0});var z2=o(ds);gn=r(z2,"SPAN",{});var D2=o(gn);j(Ds.$$.fragment,D2),D2.forEach(a),z2.forEach(a),tp=d(si),Mt=r(si,"SPAN",{});var Qh=o(Mt);np=n(Qh,"L'objet "),jn=r(Qh,"I",{});var M2=o(jn);ap=n(M2,"BatchEncoding"),M2.forEach(a),Qh.forEach(a),si.forEach(a),Dr=d(s),j(Ms.$$.fragment,s),Mr=d(s),Ce=r(s,"P",{});var ys=o(Ce);lp=n(ys,"La sortie d\u2019un "),bn=r(ys,"EM",{});var I2=o(bn);rp=n(I2,"tokenizer"),I2.forEach(a),op=n(ys," n\u2019est pas un simple dictionnaire Python. Ce que nous obtenons est en fait un objet sp\xE9cial "),En=r(ys,"CODE",{});var R2=o(En);ip=n(R2,"BatchEncoding"),R2.forEach(a),pp=n(ys,". C\u2019est une sous-classe d\u2019un dictionnaire (c\u2019est pourquoi nous avons pu indexer ce r\xE9sultat sans probl\xE8me auparavant), mais avec des m\xE9thodes suppl\xE9mentaires qui sont principalement utilis\xE9es par les "),_n=r(ys,"EM",{});var S2=o(_n);up=n(S2,"tokenizers"),S2.forEach(a),cp=n(ys," rapides."),ys.forEach(a),Ir=d(s),me=r(s,"P",{});var Pe=o(me);dp=n(Pe,"En plus de leurs capacit\xE9s de parall\xE9lisation, la fonctionnalit\xE9 cl\xE9 des "),$n=r(Pe,"EM",{});var T2=o($n);mp=n(T2,"tokenizers"),T2.forEach(a),fp=n(Pe," rapides est qu\u2019ils gardent toujours la trace de l\u2019\xE9tendue originale des textes d\u2019o\xF9 proviennent les "),kn=r(Pe,"EM",{});var B2=o(kn);hp=n(B2,"tokens"),B2.forEach(a),xp=n(Pe," finaux, une fonctionnalit\xE9 que nous appelons "),qn=r(Pe,"EM",{});var A2=o(qn);vp=n(A2,"mapping offset"),A2.forEach(a),gp=n(Pe,". Cela permet de d\xE9bloquer des fonctionnalit\xE9s telles que le mappage de chaque mot aux "),yn=r(Pe,"EM",{});var N2=o(yn);jp=n(N2,"tokens"),N2.forEach(a),bp=n(Pe," qu\u2019il a g\xE9n\xE9r\xE9s ou le mappage de chaque caract\xE8re du texte original au "),wn=r(Pe,"EM",{});var F2=o(wn);Ep=n(F2,"token"),F2.forEach(a),_p=n(Pe," qu\u2019il contient, et vice versa."),Pe.forEach(a),Rr=d(s),It=r(s,"P",{});var L2=o(It);$p=n(L2,"Prenons un exemple :"),L2.forEach(a),Sr=d(s),j(Is.$$.fragment,s),Tr=d(s),Ne=r(s,"P",{});var Yt=o(Ne);kp=n(Yt,"Comme mentionn\xE9 pr\xE9c\xE9demment, nous obtenons un objet "),Cn=r(Yt,"CODE",{});var X2=o(Cn);qp=n(X2,"BatchEncoding"),X2.forEach(a),yp=n(Yt," dans la sortie du "),On=r(Yt,"EM",{});var H2=o(On);wp=n(H2,"tokenizer"),H2.forEach(a),Cp=n(Yt," :"),Yt.forEach(a),Br=d(s),j(Rs.$$.fragment,s),Ar=d(s),re=r(s,"P",{});var qe=o(re);Op=n(qe,"Puisque la classe "),Pn=r(qe,"CODE",{});var G2=o(Pn);Pp=n(G2,"AutoTokenizer"),G2.forEach(a),zp=n(qe," choisit un "),zn=r(qe,"EM",{});var V2=o(zn);Dp=n(V2,"tokenizer"),V2.forEach(a),Mp=n(qe," rapide par d\xE9faut, nous pouvons utiliser les m\xE9thodes suppl\xE9mentaires que cet objet "),Dn=r(qe,"CODE",{});var Q2=o(Dn);Ip=n(Q2,"BatchEncoding"),Q2.forEach(a),Rp=n(qe," fournit. Nous avons deux fa\xE7ons de v\xE9rifier si notre "),Mn=r(qe,"EM",{});var J2=o(Mn);Sp=n(J2,"tokenizer"),J2.forEach(a),Tp=n(qe," est rapide ou lent. Nous pouvons soit v\xE9rifier l\u2019attribut "),In=r(qe,"CODE",{});var U2=o(In);Bp=n(U2,"is_fast"),U2.forEach(a),Ap=n(qe," du "),Rn=r(qe,"EM",{});var Y2=o(Rn);Np=n(Y2,"tokenizer"),Y2.forEach(a),Fp=n(qe," comme suit :"),qe.forEach(a),Nr=d(s),j(Ss.$$.fragment,s),Fr=d(s),j(Ts.$$.fragment,s),Lr=d(s),ms=r(s,"P",{});var ti=o(ms);Lp=n(ti,"soit v\xE9rifier le m\xEAme attribut mais avec notre "),Sn=r(ti,"CODE",{});var K2=o(Sn);Xp=n(K2,"encoding"),K2.forEach(a),Hp=n(ti," :"),ti.forEach(a),Xr=d(s),j(Bs.$$.fragment,s),Hr=d(s),j(As.$$.fragment,s),Gr=d(s),Oe=r(s,"P",{});var ws=o(Oe);Gp=n(ws,"Voyons ce qu\u2019un "),Tn=r(ws,"EM",{});var W2=o(Tn);Vp=n(W2,"tokenizer"),W2.forEach(a),Qp=n(ws," rapide nous permet de faire. Tout d\u2019abord, nous pouvons acc\xE9der aux "),Bn=r(ws,"EM",{});var Z2=o(Bn);Jp=n(Z2,"tokens"),Z2.forEach(a),Up=n(ws," sans avoir \xE0 reconvertir les identifiants en "),An=r(ws,"EM",{});var e7=o(An);Yp=n(e7,"tokens"),e7.forEach(a),Kp=n(ws," :"),ws.forEach(a),Vr=d(s),j(Ns.$$.fragment,s),Qr=d(s),j(Fs.$$.fragment,s),Jr=d(s),be=r(s,"P",{});var Ve=o(be);Wp=n(Ve,"Dans ce cas, le "),Nn=r(Ve,"EM",{});var s7=o(Nn);Zp=n(s7,"token"),s7.forEach(a),eu=n(Ve," \xE0 l\u2019index 5 est "),Fn=r(Ve,"CODE",{});var t7=o(Fn);su=n(t7,"##yl"),t7.forEach(a),tu=n(Ve," et fait partie du mot \xAB Sylvain \xBB dans la phrase originale. Nous pouvons \xE9galement utiliser la m\xE9thode "),Ln=r(Ve,"CODE",{});var n7=o(Ln);nu=n(n7,"word_ids()"),n7.forEach(a),au=n(Ve," pour obtenir l\u2019index du mot dont provient chaque "),Xn=r(Ve,"EM",{});var a7=o(Xn);lu=n(a7,"token"),a7.forEach(a),ru=n(Ve," :"),Ve.forEach(a),Ur=d(s),j(Ls.$$.fragment,s),Yr=d(s),j(Xs.$$.fragment,s),Kr=d(s),O=r(s,"P",{});var D=o(O);ou=n(D,"On peut voir que les "),Hn=r(D,"EM",{});var l7=o(Hn);iu=n(l7,"tokens"),l7.forEach(a),pu=n(D," sp\xE9ciaux du "),Gn=r(D,"EM",{});var r7=o(Gn);uu=n(r7,"tokenizer"),r7.forEach(a),cu=n(D,", "),Vn=r(D,"CODE",{});var o7=o(Vn);du=n(o7,"[CLS]"),o7.forEach(a),mu=n(D," et "),Qn=r(D,"CODE",{});var i7=o(Qn);fu=n(i7,"[SEP]"),i7.forEach(a),hu=n(D,", sont mis en correspondance avec "),Jn=r(D,"CODE",{});var p7=o(Jn);xu=n(p7,"None"),p7.forEach(a),vu=n(D," et que chaque "),Un=r(D,"EM",{});var u7=o(Un);gu=n(u7,"token"),u7.forEach(a),ju=n(D," est mis en correspondance avec le mot dont il provient. Ceci est particuli\xE8rement utile pour d\xE9terminer si un "),Yn=r(D,"EM",{});var c7=o(Yn);bu=n(c7,"token"),c7.forEach(a),Eu=n(D," est au d\xE9but d\u2019un mot ou si deux "),Kn=r(D,"EM",{});var d7=o(Kn);_u=n(d7,"tokens"),d7.forEach(a),$u=n(D," sont dans le m\xEAme mot. Nous pourrions nous appuyer sur le pr\xE9fixe "),Wn=r(D,"CODE",{});var m7=o(Wn);ku=n(m7,"##"),m7.forEach(a),qu=n(D," pour cela, mais il ne fonctionne que pour les "),Zn=r(D,"EM",{});var f7=o(Zn);yu=n(f7,"tokenizers"),f7.forEach(a),wu=n(D," de type BERT. Cette m\xE9thode fonctionne pour n\u2019importe quel type de "),ea=r(D,"EM",{});var h7=o(ea);Cu=n(h7,"tokenizer"),h7.forEach(a),Ou=n(D,", du moment qu\u2019il est rapide. Dans le chapitre suivant, nous verrons comment utiliser cette capacit\xE9 pour appliquer correctement les \xE9tiquettes que nous avons pour chaque mot aux "),sa=r(D,"EM",{});var x7=o(sa);Pu=n(x7,"tokens"),x7.forEach(a),zu=n(D," dans des t\xE2ches comme la reconnaissance d\u2019entit\xE9s nomm\xE9es et le POS ("),ta=r(D,"EM",{});var v7=o(ta);Du=n(v7,"Part-of-speech"),v7.forEach(a),Mu=n(D,"). Nous pouvons \xE9galement l\u2019utiliser pour masquer tous les "),na=r(D,"EM",{});var g7=o(na);Iu=n(g7,"tokens"),g7.forEach(a),Ru=n(D," provenant du m\xEAme mot dans la mod\xE9lisation du langage masqu\xE9 (une technique appel\xE9e "),aa=r(D,"EM",{});var j7=o(aa);Su=n(j7,"whole word masking"),j7.forEach(a),Tu=n(D,")."),D.forEach(a),Wr=d(s),Fe=r(s,"P",{});var Kt=o(Fe);Bu=n(Kt,"La notion de ce qu\u2019est un mot est compliqu\xE9e. Par exemple, est-ce que \xAB I\u2019ll \xBB (contraction de \xAB I will \xBB) compte pour un ou deux mots ? Cela d\xE9pend en fait du "),la=r(Kt,"EM",{});var b7=o(la);Au=n(b7,"tokenizer"),b7.forEach(a),Nu=n(Kt," et de l\u2019op\xE9ration de pr\xE9tok\xE9nisation qu\u2019il applique. Certains "),ra=r(Kt,"EM",{});var E7=o(ra);Fu=n(E7,"tokenizer"),E7.forEach(a),Lu=n(Kt," se contentent de s\xE9parer les espaces et consid\xE8rent donc qu\u2019il s\u2019agit d\u2019un seul mot. D\u2019autres utilisent la ponctuation en plus des espaces et consid\xE8rent donc qu\u2019il s\u2019agit de deux mots."),Kt.forEach(a),Zr=d(s),j(fs.$$.fragment,s),eo=d(s),Ee=r(s,"P",{});var Qe=o(Ee);Xu=n(Qe,"De m\xEAme, il existe une m\xE9thode "),oa=r(Qe,"CODE",{});var _7=o(oa);Hu=n(_7,"sentence_ids()"),_7.forEach(a),Gu=n(Qe," que nous pouvons utiliser pour associer un "),ia=r(Qe,"EM",{});var $7=o(ia);Vu=n($7,"token"),$7.forEach(a),Qu=n(Qe," \xE0 la phrase dont il provient (bien que dans ce cas, le "),pa=r(Qe,"CODE",{});var k7=o(pa);Ju=n(k7,"token_type_ids"),k7.forEach(a),Uu=n(Qe," retourn\xE9 par le "),ua=r(Qe,"EM",{});var q7=o(ua);Yu=n(q7,"tokenizer"),q7.forEach(a),Ku=n(Qe," peut nous donner la m\xEAme information)."),Qe.forEach(a),so=d(s),Z=r(s,"P",{});var xe=o(Z);Wu=n(xe,"Enfin, nous pouvons faire correspondre n\u2019importe quel mot ou "),ca=r(xe,"EM",{});var y7=o(ca);Zu=n(y7,"token"),y7.forEach(a),ec=n(xe," aux caract\xE8res du texte d\u2019origine (et vice versa) gr\xE2ce aux m\xE9thodes "),da=r(xe,"CODE",{});var w7=o(da);sc=n(w7,"word_to_chars()"),w7.forEach(a),tc=n(xe," ou "),ma=r(xe,"CODE",{});var C7=o(ma);nc=n(C7,"token_to_chars()"),C7.forEach(a),ac=n(xe," et "),fa=r(xe,"CODE",{});var O7=o(fa);lc=n(O7,"char_to_word()"),O7.forEach(a),rc=n(xe," ou "),ha=r(xe,"CODE",{});var P7=o(ha);oc=n(P7,"char_to_token()"),P7.forEach(a),ic=n(xe,". Par exemple, la m\xE9thode "),xa=r(xe,"CODE",{});var z7=o(xa);pc=n(z7,"word_ids()"),z7.forEach(a),uc=n(xe," nous a dit que "),va=r(xe,"CODE",{});var D7=o(va);cc=n(D7,"##yl"),D7.forEach(a),dc=n(xe," fait partie du mot \xE0 l\u2019indice 3, mais de quel mot s\u2019agit-il dans la phrase ? Nous pouvons le d\xE9couvrir comme ceci :"),xe.forEach(a),to=d(s),j(Hs.$$.fragment,s),no=d(s),j(Gs.$$.fragment,s),ao=d(s),_e=r(s,"P",{});var Je=o(_e);mc=n(Je,"Comme nous l\u2019avons mentionn\xE9 pr\xE9c\xE9demment, tout ceci est rendu possible par le fait que le "),ga=r(Je,"EM",{});var M7=o(ga);fc=n(M7,"tokenizer"),M7.forEach(a),hc=n(Je," rapide garde la trace de la partie du texte d\u2019o\xF9 provient chaque "),ja=r(Je,"EM",{});var I7=o(ja);xc=n(I7,"token"),I7.forEach(a),vc=n(Je," dans une liste d\u2019"),ba=r(Je,"EM",{});var R7=o(ba);gc=n(R7,"offsets"),R7.forEach(a),jc=n(Je,". Pour illustrer leur utilisation, nous allons maintenant vous montrer comment reproduire manuellement les r\xE9sultats du pipeline "),Ea=r(Je,"CODE",{});var S7=o(Ea);bc=n(S7,"token-classification"),S7.forEach(a),Ec=n(Je,"."),Je.forEach(a),lo=d(s),j(hs.$$.fragment,s),ro=d(s),ls=r(s,"H2",{class:!0});var ni=o(ls);xs=r(ni,"A",{id:!0,class:!0,href:!0});var T7=o(xs);_a=r(T7,"SPAN",{});var B7=o(_a);j(Vs.$$.fragment,B7),B7.forEach(a),T7.forEach(a),_c=d(ni),Rt=r(ni,"SPAN",{});var Jh=o(Rt);$c=n(Jh,"A l'int\xE9rieur du pipeline "),$a=r(Jh,"CODE",{});var A7=o($a);kc=n(A7,"token-classification"),A7.forEach(a),Jh.forEach(a),ni.forEach(a),oo=d(s),fe=r(s,"P",{});var ze=o(fe);qc=n(ze,"Dans le "),St=r(ze,"A",{href:!0});var N7=o(St);yc=n(N7,"chapitre 1"),N7.forEach(a),wc=n(ze,", nous avons eu un premier aper\xE7u de la NER (o\xF9 la t\xE2che est d\u2019identifier les parties du texte qui correspondent \xE0 des entit\xE9s telles que des personnes, des lieux ou des organisations) avec la fonction "),ka=r(ze,"CODE",{});var F7=o(ka);Cc=n(F7,"pipeline()"),F7.forEach(a),Oc=n(ze," de \u{1F917} "),qa=r(ze,"EM",{});var L7=o(qa);Pc=n(L7,"Transformers"),L7.forEach(a),zc=n(ze,". Puis, dans le "),Tt=r(ze,"A",{href:!0});var X7=o(Tt);Dc=n(X7,"chapitre 2"),X7.forEach(a),Mc=n(ze,", nous avons vu comment un pipeline regroupe les trois \xE9tapes n\xE9cessaires pour obtenir les pr\xE9dictions \xE0 partir d\u2019un texte brut : la tokenisation, le passage des entr\xE9es dans le mod\xE8le et le post-traitement. Les deux premi\xE8res \xE9tapes du pipeline de "),ya=r(ze,"CODE",{});var H7=o(ya);Ic=n(H7,"token-classification"),H7.forEach(a),Rc=n(ze," sont les m\xEAmes que dans tout autre pipeline mais le post-traitement est un peu plus complexe. Voyons comment !"),ze.forEach(a),io=d(s),Re.l(s),Bt=d(s),rs=r(s,"H3",{class:!0});var ai=o(rs);vs=r(ai,"A",{id:!0,class:!0,href:!0});var G7=o(vs);wa=r(G7,"SPAN",{});var V7=o(wa);j(Qs.$$.fragment,V7),V7.forEach(a),G7.forEach(a),Sc=d(ai),Ca=r(ai,"SPAN",{});var Q7=o(Ca);Tc=n(Q7,"Obtenir les r\xE9sultats de base avec le pipeline"),Q7.forEach(a),ai.forEach(a),po=d(s),Le=r(s,"P",{});var Wt=o(Le);Bc=n(Wt,"Tout d\u2019abord, prenons un pipeline de classification de "),Oa=r(Wt,"EM",{});var J7=o(Oa);Ac=n(J7,"tokens"),J7.forEach(a),Nc=n(Wt," afin d\u2019obtenir des r\xE9sultats \xE0 comparer manuellement. Le mod\xE8le utilis\xE9 par d\xE9faut est "),Js=r(Wt,"A",{href:!0,rel:!0});var U7=o(Js);Pa=r(U7,"CODE",{});var Y7=o(Pa);Fc=n(Y7,"dbmdz/bert-large-cased-finetuned-conll03-english"),Y7.forEach(a),U7.forEach(a),Lc=n(Wt,". Il effectue une NER sur les phrases :"),Wt.forEach(a),uo=d(s),j(Us.$$.fragment,s),co=d(s),j(Ys.$$.fragment,s),mo=d(s),$e=r(s,"P",{});var Ue=o($e);Xc=n(Ue,"Le mod\xE8le a correctement identifi\xE9 chaque "),za=r(Ue,"EM",{});var K7=o(za);Hc=n(K7,"token"),K7.forEach(a),Gc=n(Ue," g\xE9n\xE9r\xE9 par \xAB Sylvain \xBB comme une personne, chaque "),Da=r(Ue,"EM",{});var W7=o(Da);Vc=n(W7,"token"),W7.forEach(a),Qc=n(Ue," g\xE9n\xE9r\xE9 par \xAB Hugging Face \xBB comme une organisation, et le "),Ma=r(Ue,"EM",{});var Z7=o(Ma);Jc=n(Z7,"token"),Z7.forEach(a),Uc=n(Ue," \xAB Brooklyn \xBB comme un lieu. Nous pouvons \xE9galement demander au pipeline de regrouper les "),Ia=r(Ue,"EM",{});var ex=o(Ia);Yc=n(ex,"tokens"),ex.forEach(a),Kc=n(Ue," qui correspondent \xE0 la m\xEAme entit\xE9 :"),Ue.forEach(a),fo=d(s),j(Ks.$$.fragment,s),ho=d(s),j(Ws.$$.fragment,s),xo=d(s),ee=r(s,"P",{});var ve=o(ee);Wc=n(ve,"La propri\xE9t\xE9 "),Ra=r(ve,"CODE",{});var sx=o(Ra);Zc=n(sx,"aggregation_strategy"),sx.forEach(a),ed=n(ve," choisie va changer les scores calcul\xE9s pour chaque entit\xE9 group\xE9e. Avec "),Sa=r(ve,"CODE",{});var tx=o(Sa);sd=n(tx,'"simple"'),tx.forEach(a),td=n(ve," le score est juste la moyenne des scores de chaque "),Ta=r(ve,"EM",{});var nx=o(Ta);nd=n(nx,"token"),nx.forEach(a),ad=n(ve," dans l\u2019entit\xE9 donn\xE9e. Par exemple, le score de \xAB Sylvain \xBB est la moyenne des scores que nous avons vu dans l\u2019exemple pr\xE9c\xE9dent pour les tokens "),Ba=r(ve,"CODE",{});var ax=o(Ba);ld=n(ax,"S"),ax.forEach(a),rd=n(ve,", "),Aa=r(ve,"CODE",{});var lx=o(Aa);od=n(lx,"##yl"),lx.forEach(a),id=n(ve,", "),Na=r(ve,"CODE",{});var rx=o(Na);pd=n(rx,"##va"),rx.forEach(a),ud=n(ve,", et "),Fa=r(ve,"CODE",{});var ox=o(Fa);cd=n(ox,"##in"),ox.forEach(a),dd=n(ve,". D\u2019autres strat\xE9gies sont disponibles :"),ve.forEach(a),vo=d(s),Xe=r(s,"UL",{});var Zt=o(Xe);He=r(Zt,"LI",{});var kt=o(He);La=r(kt,"CODE",{});var ix=o(La);md=n(ix,'"first"'),ix.forEach(a),fd=n(kt,", o\xF9 le score de chaque entit\xE9 est le score du premier "),Xa=r(kt,"EM",{});var px=o(Xa);hd=n(px,"token"),px.forEach(a),xd=n(kt," de cette entit\xE9 (donc pour \xAB Sylvain \xBB ce serait 0.993828, le score du token "),Ha=r(kt,"CODE",{});var ux=o(Ha);vd=n(ux,"S"),ux.forEach(a),gd=n(kt,")"),kt.forEach(a),jd=d(Zt),gs=r(Zt,"LI",{});var qr=o(gs);Ga=r(qr,"CODE",{});var cx=o(Ga);bd=n(cx,'"max"'),cx.forEach(a),Ed=n(qr,", o\xF9 le score de chaque entit\xE9 est le score maximal des "),Va=r(qr,"EM",{});var dx=o(Va);_d=n(dx,"tokens"),dx.forEach(a),$d=n(qr," de cette entit\xE9 (ainsi, pour \xAB Hugging Face \xBB, le score de \xAB Face \xBB serait de 0,98879766)."),qr.forEach(a),kd=d(Zt),js=r(Zt,"LI",{});var yr=o(js);Qa=r(yr,"CODE",{});var mx=o(Qa);qd=n(mx,'"average"'),mx.forEach(a),yd=n(yr,", o\xF9 le score de chaque entit\xE9 est la moyenne des scores des mots qui composent cette entit\xE9 (ainsi, pour \xAB Sylvain \xBB, il n\u2019y aurait pas de diff\xE9rence avec la strat\xE9gie "),Ja=r(yr,"CODE",{});var fx=o(Ja);wd=n(fx,'"simple"'),fx.forEach(a),Cd=n(yr,", mais \u201CHugging Face\u201D aurait un score de 0,9819, la moyenne des scores de \xAB Hugging \xBB, 0,975, et \xAB Face \xBB, 0,98879)."),yr.forEach(a),Zt.forEach(a),go=d(s),bs=r(s,"P",{});var li=o(bs);Od=n(li,"Voyons maintenant comment obtenir ces r\xE9sultats sans utiliser la fonction "),Ua=r(li,"CODE",{});var hx=o(Ua);Pd=n(hx,"pipeline()"),hx.forEach(a),zd=n(li," !"),li.forEach(a),jo=d(s),os=r(s,"H3",{class:!0});var ri=o(os);Es=r(ri,"A",{id:!0,class:!0,href:!0});var xx=o(Es);Ya=r(xx,"SPAN",{});var vx=o(Ya);j(Zs.$$.fragment,vx),vx.forEach(a),xx.forEach(a),Dd=d(ri),Ka=r(ri,"SPAN",{});var gx=o(Ka);Md=n(gx,"Des entr\xE9es aux pr\xE9dictions"),gx.forEach(a),ri.forEach(a),bo=d(s),Te.l(s),At=d(s),_s=r(s,"P",{});var oi=o(_s);Id=n(oi,"Nous avons un batch avec 1 s\xE9quence de 19 "),Wa=r(oi,"EM",{});var jx=o(Wa);Rd=n(jx,"tokens"),jx.forEach(a),Sd=n(oi," et le mod\xE8le a 9 \xE9tiquettes diff\xE9rentes. Ainsi, la sortie du mod\xE8le a une forme de 1 x 19 x 9. Comme pour le pipeline de classification de texte, nous utilisons une fonction softmax pour convertir ces logits en probabilit\xE9s et nous prenons l\u2019argmax pour obtenir des pr\xE9dictions (notez que nous pouvons prendre l\u2019argmax sur les logits car la fonction softmax ne change pas l\u2019ordre) :"),oi.forEach(a),Eo=d(s),Ae.l(s),Nt=d(s),j(et.$$.fragment,s),_o=d(s),$s=r(s,"P",{});var ii=o($s);Td=n(ii,"L\u2019attribut "),Za=r(ii,"CODE",{});var bx=o(Za);Bd=n(bx,"model.config.id2label"),bx.forEach(a),Ad=n(ii," contient la correspondance entre les index et les \xE9tiquettes que nous pouvons utiliser pour donner un sens aux pr\xE9dictions :"),ii.forEach(a),$o=d(s),j(st.$$.fragment,s),ko=d(s),j(tt.$$.fragment,s),qo=d(s),q=r(s,"P",{});var P=o(q);Nd=n(P,"Comme nous l\u2019avons vu pr\xE9c\xE9demment, il y a 9 \xE9tiquettes : "),el=r(P,"CODE",{});var Ex=o(el);Fd=n(Ex,"O"),Ex.forEach(a),Ld=n(P," est le label pour les "),sl=r(P,"EM",{});var _x=o(sl);Xd=n(_x,"tokens"),_x.forEach(a),Hd=n(P," qui ne sont dans aucune entit\xE9 nomm\xE9e (il signifie "),tl=r(P,"EM",{});var $x=o(tl);Gd=n($x,"outside"),$x.forEach(a),Vd=n(P," (en dehors)) et nous avons ensuite deux labels pour chaque type d\u2019entit\xE9 (divers, personne, organisation et lieu). L\u2019\xE9tiquette "),nl=r(P,"CODE",{});var kx=o(nl);Qd=n(kx,"B-XXX"),kx.forEach(a),Jd=n(P," indique que le "),al=r(P,"EM",{});var qx=o(al);Ud=n(qx,"token"),qx.forEach(a),Yd=n(P," est au d\xE9but d\u2019une entit\xE9 "),ll=r(P,"CODE",{});var yx=o(ll);Kd=n(yx,"XXX"),yx.forEach(a),Wd=n(P," et l\u2019\xE9tiquette "),rl=r(P,"CODE",{});var wx=o(rl);Zd=n(wx,"I-XXX"),wx.forEach(a),em=n(P," indique que le "),ol=r(P,"EM",{});var Cx=o(ol);sm=n(Cx,"token"),Cx.forEach(a),tm=n(P," est \xE0 l\u2019int\xE9rieur de l\u2019entit\xE9 "),il=r(P,"CODE",{});var Ox=o(il);nm=n(Ox,"XXX"),Ox.forEach(a),am=n(P,". Par exemple, dans l\u2019exemple actuel, nous nous attendons \xE0 ce que notre mod\xE8le classe le "),pl=r(P,"EM",{});var Px=o(pl);lm=n(Px,"token"),Px.forEach(a),rm=d(P),ul=r(P,"CODE",{});var zx=o(ul);om=n(zx,"S"),zx.forEach(a),im=n(P," comme "),cl=r(P,"CODE",{});var Dx=o(cl);pm=n(Dx,"B-PER"),Dx.forEach(a),um=n(P," (d\xE9but d\u2019une entit\xE9 personne) et les "),dl=r(P,"EM",{});var Mx=o(dl);cm=n(Mx,"tokens"),Mx.forEach(a),dm=d(P),ml=r(P,"CODE",{});var Ix=o(ml);mm=n(Ix,"##yl"),Ix.forEach(a),fm=n(P,", "),fl=r(P,"CODE",{});var Rx=o(fl);hm=n(Rx,"##va"),Rx.forEach(a),xm=n(P," et "),hl=r(P,"CODE",{});var Sx=o(hl);vm=n(Sx,"##in"),Sx.forEach(a),gm=n(P," comme "),xl=r(P,"CODE",{});var Tx=o(xl);jm=n(Tx,"I-PER"),Tx.forEach(a),bm=n(P," (\xE0 l\u2019int\xE9rieur d\u2019une entit\xE9 personne)."),P.forEach(a),yo=d(s),T=r(s,"P",{});var V=o(T);Em=n(V,"Vous pourriez penser que le mod\xE8le s\u2019est tromp\xE9 ici car il a attribu\xE9 l\u2019\xE9tiquette "),vl=r(V,"CODE",{});var Bx=o(vl);_m=n(Bx,"I-PER"),Bx.forEach(a),$m=n(V," \xE0 ces quatre "),gl=r(V,"EM",{});var Ax=o(gl);km=n(Ax,"tokens"),Ax.forEach(a),qm=n(V," mais ce n\u2019est pas tout \xE0 fait vrai. Il existe en fait deux formats pour ces \xE9tiquettes "),jl=r(V,"CODE",{});var Nx=o(jl);ym=n(Nx,"B-"),Nx.forEach(a),wm=n(V," et "),bl=r(V,"CODE",{});var Fx=o(bl);Cm=n(Fx,"I-"),Fx.forEach(a),Om=n(V," : "),El=r(V,"EM",{});var Lx=o(El);Pm=n(Lx,"IOB1"),Lx.forEach(a),zm=n(V," et "),_l=r(V,"EM",{});var Xx=o(_l);Dm=n(Xx,"IOB2"),Xx.forEach(a),Mm=n(V,". Le format IOB2 (en rose ci-dessous) est celui que nous avons introduit alors que dans le format IOB1 (en bleu), les \xE9tiquettes commen\xE7ant par "),$l=r(V,"CODE",{});var Hx=o($l);Im=n(Hx,"B-"),Hx.forEach(a),Rm=n(V," ne sont jamais utilis\xE9es que pour s\xE9parer deux entit\xE9s adjacentes du m\xEAme type. Le mod\xE8le que nous utilisons a \xE9t\xE9 "),kl=r(V,"EM",{});var Gx=o(kl);Sm=n(Gx,"finetun\xE9"),Gx.forEach(a),Tm=n(V," sur un jeu de donn\xE9es utilisant ce format, c\u2019est pourquoi il attribue le label "),ql=r(V,"CODE",{});var Vx=o(ql);Bm=n(Vx,"I-PER"),Vx.forEach(a),Am=n(V," au "),yl=r(V,"EM",{});var Qx=o(yl);Nm=n(Qx,"token"),Qx.forEach(a),Fm=d(V),wl=r(V,"CODE",{});var Jx=o(wl);Lm=n(Jx,"S"),Jx.forEach(a),Xm=n(V,"."),V.forEach(a),wo=d(s),is=r(s,"DIV",{class:!0});var pi=o(is);nt=r(pi,"IMG",{class:!0,src:!0,alt:!0}),Hm=d(pi),at=r(pi,"IMG",{class:!0,src:!0,alt:!0}),pi.forEach(a),Co=d(s),Ge=r(s,"P",{});var en=o(Ge);Gm=n(en,"Nous sommes \xE0 pr\xE9sent pr\xEAts \xE0 reproduire (presque enti\xE8rement) les r\xE9sultats du premier pipeline. Nous pouvons simplement r\xE9cup\xE9rer le score et le label de chaque "),Cl=r(en,"EM",{});var Ux=o(Cl);Vm=n(Ux,"token"),Ux.forEach(a),Qm=n(en," qui n\u2019a pas \xE9t\xE9 class\xE9 comme "),Ol=r(en,"CODE",{});var Yx=o(Ol);Jm=n(Yx,"O"),Yx.forEach(a),Um=n(en," :"),en.forEach(a),Oo=d(s),j(lt.$$.fragment,s),Po=d(s),j(rt.$$.fragment,s),zo=d(s),oe=r(s,"P",{});var ye=o(oe);Ym=n(ye,"C\u2019est tr\xE8s similaire \xE0 ce que nous avions avant, \xE0 une exception pr\xE8s : le pipeline nous a aussi donn\xE9 des informations sur le "),Pl=r(ye,"CODE",{});var Kx=o(Pl);Km=n(Kx,"d\xE9but"),Kx.forEach(a),Wm=n(ye," et la "),zl=r(ye,"CODE",{});var Wx=o(zl);Zm=n(Wx,"fin"),Wx.forEach(a),ef=n(ye," de chaque entit\xE9 dans la phrase originale. C\u2019est l\xE0 que notre "),Dl=r(ye,"EM",{});var Zx=o(Dl);sf=n(Zx,"offset mapping"),Zx.forEach(a),tf=n(ye," va entrer en jeu. Pour obtenir les "),Ml=r(ye,"EM",{});var ev=o(Ml);nf=n(ev,"offsets"),ev.forEach(a),af=n(ye,", il suffit de d\xE9finir "),Il=r(ye,"CODE",{});var sv=o(Il);lf=n(sv,"return_offsets_mapping=True"),sv.forEach(a),rf=n(ye," lorsque nous appliquons le "),Rl=r(ye,"EM",{});var tv=o(Rl);of=n(tv,"tokenizer"),tv.forEach(a),pf=n(ye," \xE0 nos entr\xE9es :"),ye.forEach(a),Do=d(s),j(ot.$$.fragment,s),Mo=d(s),j(it.$$.fragment,s),Io=d(s),Y=r(s,"P",{});var pe=o(Y);uf=n(pe,"Chaque "),Sl=r(pe,"EM",{});var nv=o(Sl);cf=n(nv,"tuple"),nv.forEach(a),df=n(pe," est l\u2019\xE9tendue de texte correspondant \xE0 chaque "),Tl=r(pe,"EM",{});var av=o(Tl);mf=n(av,"token"),av.forEach(a),ff=n(pe," o\xF9 "),Bl=r(pe,"CODE",{});var lv=o(Bl);hf=n(lv,"(0, 0)"),lv.forEach(a),xf=n(pe," est r\xE9serv\xE9 aux "),Al=r(pe,"EM",{});var rv=o(Al);vf=n(rv,"tokens"),rv.forEach(a),gf=n(pe," sp\xE9ciaux. Nous avons vu pr\xE9c\xE9demment que le "),Nl=r(pe,"EM",{});var ov=o(Nl);jf=n(ov,"token"),ov.forEach(a),bf=n(pe," \xE0 l\u2019index 5 est "),Fl=r(pe,"CODE",{});var iv=o(Fl);Ef=n(iv,"##yl"),iv.forEach(a),_f=n(pe,", qui a "),Ll=r(pe,"CODE",{});var pv=o(Ll);$f=n(pv,"(12, 14)"),pv.forEach(a),kf=n(pe," comme "),Xl=r(pe,"EM",{});var uv=o(Xl);qf=n(uv,"offsets"),uv.forEach(a),yf=n(pe," ici. Si on prend la tranche correspondante dans notre exemple :"),pe.forEach(a),Ro=d(s),j(pt.$$.fragment,s),So=d(s),ks=r(s,"P",{});var ui=o(ks);wf=n(ui,"nous obtenons le bon espace de texte sans le "),Hl=r(ui,"CODE",{});var cv=o(Hl);Cf=n(cv,"##"),cv.forEach(a),Of=n(ui," :"),ui.forEach(a),To=d(s),j(ut.$$.fragment,s),Bo=d(s),Ft=r(s,"P",{});var dv=o(Ft);Pf=n(dv,"En utilisant cela, nous pouvons maintenant compl\xE9ter les r\xE9sultats pr\xE9c\xE9dents :"),dv.forEach(a),Ao=d(s),j(ct.$$.fragment,s),No=d(s),j(dt.$$.fragment,s),Fo=d(s),Lt=r(s,"P",{});var mv=o(Lt);zf=n(mv,"C\u2019est la m\xEAme chose que ce que nous avons obtenu avec le premier pipeline !"),mv.forEach(a),Lo=d(s),ps=r(s,"H3",{class:!0});var ci=o(ps);qs=r(ci,"A",{id:!0,class:!0,href:!0});var fv=o(qs);Gl=r(fv,"SPAN",{});var hv=o(Gl);j(mt.$$.fragment,hv),hv.forEach(a),fv.forEach(a),Df=d(ci),Vl=r(ci,"SPAN",{});var xv=o(Vl);Mf=n(xv,"Regroupement des entit\xE9s"),xv.forEach(a),ci.forEach(a),Xo=d(s),M=r(s,"P",{});var F=o(M);If=n(F,"L\u2019utilisation des "),Ql=r(F,"EM",{});var vv=o(Ql);Rf=n(vv,"offsets"),vv.forEach(a),Sf=n(F," pour d\xE9terminer les cl\xE9s de d\xE9but et de fin pour chaque entit\xE9 est pratique mais cette information n\u2019est pas strictement n\xE9cessaire. Cependant, lorsque nous voulons regrouper les entit\xE9s, les "),Jl=r(F,"EM",{});var gv=o(Jl);Tf=n(gv,"offsets"),gv.forEach(a),Bf=n(F," nous \xE9pargnent un batch de code compliqu\xE9. Par exemple, si nous voulions regrouper les "),Ul=r(F,"EM",{});var jv=o(Ul);Af=n(jv,"tokens"),jv.forEach(a),Nf=d(F),Yl=r(F,"CODE",{});var bv=o(Yl);Ff=n(bv,"Hu"),bv.forEach(a),Lf=n(F,", "),Kl=r(F,"CODE",{});var Ev=o(Kl);Xf=n(Ev,"##gging"),Ev.forEach(a),Hf=n(F,", et "),Wl=r(F,"CODE",{});var _v=o(Wl);Gf=n(_v,"Face"),_v.forEach(a),Vf=n(F,", nous pourrions \xE9tablir des r\xE8gles sp\xE9ciales disant que les deux premiers devraient \xEAtre attach\xE9s tout en enlevant le "),Zl=r(F,"CODE",{});var $v=o(Zl);Qf=n($v,"##"),$v.forEach(a),Jf=n(F,", et le "),er=r(F,"CODE",{});var kv=o(er);Uf=n(kv,"Face"),kv.forEach(a),Yf=n(F," devrait \xEAtre ajout\xE9 avec un espace puisqu\u2019il ne commence pas par "),sr=r(F,"CODE",{});var qv=o(sr);Kf=n(qv,"##"),qv.forEach(a),Wf=n(F," mais cela ne fonctionnerait que pour ce type particulier de "),tr=r(F,"EM",{});var yv=o(tr);Zf=n(yv,"tokenizer"),yv.forEach(a),eh=n(F,". Il faudrait \xE9crire un autre ensemble de r\xE8gles pour un "),nr=r(F,"EM",{});var wv=o(nr);sh=n(wv,"tokenizer"),wv.forEach(a),th=n(F," de type SentencePiece ou "),ar=r(F,"EM",{});var Cv=o(ar);nh=n(Cv,"Byte-Pair-Encoding"),Cv.forEach(a),ah=n(F," (voir plus loin dans ce chapitre)."),F.forEach(a),Ho=d(s),G=r(s,"P",{});var se=o(G);lh=n(se,"Avec les "),lr=r(se,"EM",{});var Ov=o(lr);rh=n(Ov,"offsets"),Ov.forEach(a),oh=n(se,", tout ce code personnalis\xE9 dispara\xEEt : il suffit de prendre l\u2019intervalle du texte original qui commence par le premier "),rr=r(se,"EM",{});var Pv=o(rr);ih=n(Pv,"token"),Pv.forEach(a),ph=n(se," et se termine par le dernier "),or=r(se,"EM",{});var zv=o(or);uh=n(zv,"token"),zv.forEach(a),ch=n(se,". Ainsi, dans le cas des "),ir=r(se,"EM",{});var Dv=o(ir);dh=n(Dv,"tokens"),Dv.forEach(a),mh=d(se),pr=r(se,"CODE",{});var Mv=o(pr);fh=n(Mv,"Hu"),Mv.forEach(a),hh=n(se,", "),ur=r(se,"CODE",{});var Iv=o(ur);xh=n(Iv,"##gging"),Iv.forEach(a),vh=n(se,", et "),cr=r(se,"CODE",{});var Rv=o(cr);gh=n(Rv,"Face"),Rv.forEach(a),jh=n(se,", nous devrions commencer au caract\xE8re 33 (le d\xE9but de "),dr=r(se,"CODE",{});var Sv=o(dr);bh=n(Sv,"Hu"),Sv.forEach(a),Eh=n(se,") et finir avant le caract\xE8re 45 (la fin de "),mr=r(se,"CODE",{});var Tv=o(mr);_h=n(Tv,"Face"),Tv.forEach(a),$h=n(se,") :"),se.forEach(a),Go=d(s),j(ft.$$.fragment,s),Vo=d(s),j(ht.$$.fragment,s),Qo=d(s),he=r(s,"P",{});var De=o(he);kh=n(De,"Pour \xE9crire le code qui post-traite les pr\xE9dictions tout en regroupant les entit\xE9s, nous regrouperons les entit\xE9s qui sont cons\xE9cutives et \xE9tiquet\xE9es avec "),fr=r(De,"CODE",{});var Bv=o(fr);qh=n(Bv,"I-XXX"),Bv.forEach(a),yh=n(De,", \xE0 l\u2019exception de la premi\xE8re, qui peut \xEAtre \xE9tiquet\xE9e comme "),hr=r(De,"CODE",{});var Av=o(hr);wh=n(Av,"B-XXX"),Av.forEach(a),Ch=n(De," ou "),xr=r(De,"CODE",{});var Nv=o(xr);Oh=n(Nv,"I-XXX"),Nv.forEach(a),Ph=n(De," (ainsi, nous arr\xEAtons de regrouper une entit\xE9 lorsque nous obtenons un "),vr=r(De,"CODE",{});var Fv=o(vr);zh=n(Fv,"O"),Fv.forEach(a),Dh=n(De,", un nouveau type d\u2019entit\xE9, ou un "),gr=r(De,"CODE",{});var Lv=o(gr);Mh=n(Lv,"B-XXX"),Lv.forEach(a),Ih=n(De," qui nous indique qu\u2019une entit\xE9 du m\xEAme type commence) :"),De.forEach(a),Jo=d(s),j(xt.$$.fragment,s),Uo=d(s),Xt=r(s,"P",{});var Xv=o(Xt);Rh=n(Xv,"Et nous obtenons les m\xEAmes r\xE9sultats qu\u2019avec notre deuxi\xE8me pipeline !"),Xv.forEach(a),Yo=d(s),j(vt.$$.fragment,s),Ko=d(s),ke=r(s,"P",{});var Ye=o(ke);Sh=n(Ye,"Un autre exemple de t\xE2che o\xF9 ces "),jr=r(Ye,"EM",{});var Hv=o(jr);Th=n(Hv,"offsets"),Hv.forEach(a),Bh=n(Ye," sont extr\xEAmement utiles est la r\xE9ponse aux questions. Plonger dans ce pipeline, ce que nous ferons dans la section suivante, nous permettra de jeter un coup d\u2019\u0153il \xE0 une derni\xE8re caract\xE9ristique des "),br=r(Ye,"EM",{});var Gv=o(br);Ah=n(Gv,"tokenizers"),Gv.forEach(a),Nh=n(Ye," de la biblioth\xE8que \u{1F917} "),Er=r(Ye,"EM",{});var Vv=o(Er);Fh=n(Vv,"Transformers"),Vv.forEach(a),Lh=n(Ye," : la gestion des "),_r=r(Ye,"EM",{});var Qv=o(_r);Xh=n(Qv,"tokens"),Qv.forEach(a),Hh=n(Ye," qui d\xE9bordent lorsque nous tronquons une entr\xE9e \xE0 une longueur donn\xE9e."),Ye.forEach(a),this.h()},h(){_(u,"name","hf:doc:metadata"),_(u,"content",JSON.stringify(f1)),_(C,"id","pouvoirs-spciaux-des-itokenizersi-rapides"),_(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(C,"href","#pouvoirs-spciaux-des-itokenizersi-rapides"),_(I,"class","relative group"),_(qt,"href","/course/fr/chapter1"),_(yt,"href","/course/fr/chapter5/3"),_(Os,"align","center"),_(Ps,"align","center"),_(hn,"align","center"),_(wt,"align","center"),_(Ct,"align","center"),_(Ot,"align","center"),_(Pt,"align","center"),_(zt,"align","center"),_(Dt,"align","center"),_(ds,"id","lobjet-ibatchencodingi"),_(ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ds,"href","#lobjet-ibatchencodingi"),_(as,"class","relative group"),_(xs,"id","a-lintrieur-du-pipeline-tokenclassification"),_(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(xs,"href","#a-lintrieur-du-pipeline-tokenclassification"),_(ls,"class","relative group"),_(St,"href","/course/fr/chapter1"),_(Tt,"href","/course/fr/chapter2"),_(vs,"id","obtenir-les-rsultats-de-base-avec-le-pipeline"),_(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(vs,"href","#obtenir-les-rsultats-de-base-avec-le-pipeline"),_(rs,"class","relative group"),_(Js,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),_(Js,"rel","nofollow"),_(Es,"id","des-entres-aux-prdictions"),_(Es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Es,"href","#des-entres-aux-prdictions"),_(os,"class","relative group"),_(nt,"class","block dark:hidden"),Jv(nt.src,Yh="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg")||_(nt,"src",Yh),_(nt,"alt","IOB1 vs IOB2 format"),_(at,"class","hidden dark:block"),Jv(at.src,Kh="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg")||_(at,"src",Kh),_(at,"alt","IOB1 vs IOB2 format"),_(is,"class","flex justify-center"),_(qs,"id","regroupement-des-entits"),_(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(qs,"href","#regroupement-des-entits"),_(ps,"class","relative group")},m(s,i){e(document.head,u),p(s,v,i),b(m,s,i),p(s,w,i),p(s,I,i),e(I,C),e(C,R),b(N,R,null),e(I,B),e(I,S),e(S,ae),e(S,X),e(X,Q),e(S,ue),p(s,z,i),gt[J].m(s,i),p(s,W,i),p(s,$,i),e($,le),e($,ce),e(ce,je),e($,de),e($,H),e(H,we),e($,te),e($,ge),e(ge,f),e($,k),e($,Me),e(Me,Ke),e($,We),e($,ne),e(ne,Ze),e($,es),e($,tn),e(tn,hi),e($,xi),e($,nn),e(nn,vi),e($,gi),e($,qt),e(qt,ji),e($,bi),p(s,wr,i),b(Cs,s,i),p(s,Cr,i),p(s,U,i),e(U,Ei),e(U,an),e(an,_i),e(U,$i),e(U,ln),e(ln,ki),e(U,qi),e(U,rn),e(rn,yi),e(U,wi),e(U,on),e(on,Ci),e(U,Oi),e(U,yt),e(yt,Pi),e(U,zi),e(U,pn),e(pn,Di),e(U,Mi),e(U,un),e(un,Ii),e(U,Ri),e(U,cn),e(cn,Si),e(U,Ti),p(s,Or,i),p(s,us,i),e(us,dn),e(dn,ss),e(ss,Os),e(Os,mn),e(mn,Bi),e(Os,Ai),e(ss,Ni),e(ss,Ps),e(Ps,fn),e(fn,Fi),e(Ps,Li),e(ss,Xi),e(ss,hn),e(us,Hi),e(us,zs),e(zs,ts),e(ts,wt),e(wt,xn),e(xn,Gi),e(ts,Vi),e(ts,Ct),e(Ct,Qi),e(ts,Ji),e(ts,Ot),e(Ot,Ui),e(zs,Yi),e(zs,ns),e(ns,Pt),e(Pt,vn),e(vn,Ki),e(ns,Wi),e(ns,zt),e(zt,Zi),e(ns,ep),e(ns,Dt),e(Dt,sp),p(s,Pr,i),b(cs,s,i),p(s,zr,i),p(s,as,i),e(as,ds),e(ds,gn),b(Ds,gn,null),e(as,tp),e(as,Mt),e(Mt,np),e(Mt,jn),e(jn,ap),p(s,Dr,i),b(Ms,s,i),p(s,Mr,i),p(s,Ce,i),e(Ce,lp),e(Ce,bn),e(bn,rp),e(Ce,op),e(Ce,En),e(En,ip),e(Ce,pp),e(Ce,_n),e(_n,up),e(Ce,cp),p(s,Ir,i),p(s,me,i),e(me,dp),e(me,$n),e($n,mp),e(me,fp),e(me,kn),e(kn,hp),e(me,xp),e(me,qn),e(qn,vp),e(me,gp),e(me,yn),e(yn,jp),e(me,bp),e(me,wn),e(wn,Ep),e(me,_p),p(s,Rr,i),p(s,It,i),e(It,$p),p(s,Sr,i),b(Is,s,i),p(s,Tr,i),p(s,Ne,i),e(Ne,kp),e(Ne,Cn),e(Cn,qp),e(Ne,yp),e(Ne,On),e(On,wp),e(Ne,Cp),p(s,Br,i),b(Rs,s,i),p(s,Ar,i),p(s,re,i),e(re,Op),e(re,Pn),e(Pn,Pp),e(re,zp),e(re,zn),e(zn,Dp),e(re,Mp),e(re,Dn),e(Dn,Ip),e(re,Rp),e(re,Mn),e(Mn,Sp),e(re,Tp),e(re,In),e(In,Bp),e(re,Ap),e(re,Rn),e(Rn,Np),e(re,Fp),p(s,Nr,i),b(Ss,s,i),p(s,Fr,i),b(Ts,s,i),p(s,Lr,i),p(s,ms,i),e(ms,Lp),e(ms,Sn),e(Sn,Xp),e(ms,Hp),p(s,Xr,i),b(Bs,s,i),p(s,Hr,i),b(As,s,i),p(s,Gr,i),p(s,Oe,i),e(Oe,Gp),e(Oe,Tn),e(Tn,Vp),e(Oe,Qp),e(Oe,Bn),e(Bn,Jp),e(Oe,Up),e(Oe,An),e(An,Yp),e(Oe,Kp),p(s,Vr,i),b(Ns,s,i),p(s,Qr,i),b(Fs,s,i),p(s,Jr,i),p(s,be,i),e(be,Wp),e(be,Nn),e(Nn,Zp),e(be,eu),e(be,Fn),e(Fn,su),e(be,tu),e(be,Ln),e(Ln,nu),e(be,au),e(be,Xn),e(Xn,lu),e(be,ru),p(s,Ur,i),b(Ls,s,i),p(s,Yr,i),b(Xs,s,i),p(s,Kr,i),p(s,O,i),e(O,ou),e(O,Hn),e(Hn,iu),e(O,pu),e(O,Gn),e(Gn,uu),e(O,cu),e(O,Vn),e(Vn,du),e(O,mu),e(O,Qn),e(Qn,fu),e(O,hu),e(O,Jn),e(Jn,xu),e(O,vu),e(O,Un),e(Un,gu),e(O,ju),e(O,Yn),e(Yn,bu),e(O,Eu),e(O,Kn),e(Kn,_u),e(O,$u),e(O,Wn),e(Wn,ku),e(O,qu),e(O,Zn),e(Zn,yu),e(O,wu),e(O,ea),e(ea,Cu),e(O,Ou),e(O,sa),e(sa,Pu),e(O,zu),e(O,ta),e(ta,Du),e(O,Mu),e(O,na),e(na,Iu),e(O,Ru),e(O,aa),e(aa,Su),e(O,Tu),p(s,Wr,i),p(s,Fe,i),e(Fe,Bu),e(Fe,la),e(la,Au),e(Fe,Nu),e(Fe,ra),e(ra,Fu),e(Fe,Lu),p(s,Zr,i),b(fs,s,i),p(s,eo,i),p(s,Ee,i),e(Ee,Xu),e(Ee,oa),e(oa,Hu),e(Ee,Gu),e(Ee,ia),e(ia,Vu),e(Ee,Qu),e(Ee,pa),e(pa,Ju),e(Ee,Uu),e(Ee,ua),e(ua,Yu),e(Ee,Ku),p(s,so,i),p(s,Z,i),e(Z,Wu),e(Z,ca),e(ca,Zu),e(Z,ec),e(Z,da),e(da,sc),e(Z,tc),e(Z,ma),e(ma,nc),e(Z,ac),e(Z,fa),e(fa,lc),e(Z,rc),e(Z,ha),e(ha,oc),e(Z,ic),e(Z,xa),e(xa,pc),e(Z,uc),e(Z,va),e(va,cc),e(Z,dc),p(s,to,i),b(Hs,s,i),p(s,no,i),b(Gs,s,i),p(s,ao,i),p(s,_e,i),e(_e,mc),e(_e,ga),e(ga,fc),e(_e,hc),e(_e,ja),e(ja,xc),e(_e,vc),e(_e,ba),e(ba,gc),e(_e,jc),e(_e,Ea),e(Ea,bc),e(_e,Ec),p(s,lo,i),b(hs,s,i),p(s,ro,i),p(s,ls,i),e(ls,xs),e(xs,_a),b(Vs,_a,null),e(ls,_c),e(ls,Rt),e(Rt,$c),e(Rt,$a),e($a,kc),p(s,oo,i),p(s,fe,i),e(fe,qc),e(fe,St),e(St,yc),e(fe,wc),e(fe,ka),e(ka,Cc),e(fe,Oc),e(fe,qa),e(qa,Pc),e(fe,zc),e(fe,Tt),e(Tt,Dc),e(fe,Mc),e(fe,ya),e(ya,Ic),e(fe,Rc),p(s,io,i),jt[Ie].m(s,i),p(s,Bt,i),p(s,rs,i),e(rs,vs),e(vs,wa),b(Qs,wa,null),e(rs,Sc),e(rs,Ca),e(Ca,Tc),p(s,po,i),p(s,Le,i),e(Le,Bc),e(Le,Oa),e(Oa,Ac),e(Le,Nc),e(Le,Js),e(Js,Pa),e(Pa,Fc),e(Le,Lc),p(s,uo,i),b(Us,s,i),p(s,co,i),b(Ys,s,i),p(s,mo,i),p(s,$e,i),e($e,Xc),e($e,za),e(za,Hc),e($e,Gc),e($e,Da),e(Da,Vc),e($e,Qc),e($e,Ma),e(Ma,Jc),e($e,Uc),e($e,Ia),e(Ia,Yc),e($e,Kc),p(s,fo,i),b(Ks,s,i),p(s,ho,i),b(Ws,s,i),p(s,xo,i),p(s,ee,i),e(ee,Wc),e(ee,Ra),e(Ra,Zc),e(ee,ed),e(ee,Sa),e(Sa,sd),e(ee,td),e(ee,Ta),e(Ta,nd),e(ee,ad),e(ee,Ba),e(Ba,ld),e(ee,rd),e(ee,Aa),e(Aa,od),e(ee,id),e(ee,Na),e(Na,pd),e(ee,ud),e(ee,Fa),e(Fa,cd),e(ee,dd),p(s,vo,i),p(s,Xe,i),e(Xe,He),e(He,La),e(La,md),e(He,fd),e(He,Xa),e(Xa,hd),e(He,xd),e(He,Ha),e(Ha,vd),e(He,gd),e(Xe,jd),e(Xe,gs),e(gs,Ga),e(Ga,bd),e(gs,Ed),e(gs,Va),e(Va,_d),e(gs,$d),e(Xe,kd),e(Xe,js),e(js,Qa),e(Qa,qd),e(js,yd),e(js,Ja),e(Ja,wd),e(js,Cd),p(s,go,i),p(s,bs,i),e(bs,Od),e(bs,Ua),e(Ua,Pd),e(bs,zd),p(s,jo,i),p(s,os,i),e(os,Es),e(Es,Ya),b(Zs,Ya,null),e(os,Dd),e(os,Ka),e(Ka,Md),p(s,bo,i),bt[Se].m(s,i),p(s,At,i),p(s,_s,i),e(_s,Id),e(_s,Wa),e(Wa,Rd),e(_s,Sd),p(s,Eo,i),Et[Be].m(s,i),p(s,Nt,i),b(et,s,i),p(s,_o,i),p(s,$s,i),e($s,Td),e($s,Za),e(Za,Bd),e($s,Ad),p(s,$o,i),b(st,s,i),p(s,ko,i),b(tt,s,i),p(s,qo,i),p(s,q,i),e(q,Nd),e(q,el),e(el,Fd),e(q,Ld),e(q,sl),e(sl,Xd),e(q,Hd),e(q,tl),e(tl,Gd),e(q,Vd),e(q,nl),e(nl,Qd),e(q,Jd),e(q,al),e(al,Ud),e(q,Yd),e(q,ll),e(ll,Kd),e(q,Wd),e(q,rl),e(rl,Zd),e(q,em),e(q,ol),e(ol,sm),e(q,tm),e(q,il),e(il,nm),e(q,am),e(q,pl),e(pl,lm),e(q,rm),e(q,ul),e(ul,om),e(q,im),e(q,cl),e(cl,pm),e(q,um),e(q,dl),e(dl,cm),e(q,dm),e(q,ml),e(ml,mm),e(q,fm),e(q,fl),e(fl,hm),e(q,xm),e(q,hl),e(hl,vm),e(q,gm),e(q,xl),e(xl,jm),e(q,bm),p(s,yo,i),p(s,T,i),e(T,Em),e(T,vl),e(vl,_m),e(T,$m),e(T,gl),e(gl,km),e(T,qm),e(T,jl),e(jl,ym),e(T,wm),e(T,bl),e(bl,Cm),e(T,Om),e(T,El),e(El,Pm),e(T,zm),e(T,_l),e(_l,Dm),e(T,Mm),e(T,$l),e($l,Im),e(T,Rm),e(T,kl),e(kl,Sm),e(T,Tm),e(T,ql),e(ql,Bm),e(T,Am),e(T,yl),e(yl,Nm),e(T,Fm),e(T,wl),e(wl,Lm),e(T,Xm),p(s,wo,i),p(s,is,i),e(is,nt),e(is,Hm),e(is,at),p(s,Co,i),p(s,Ge,i),e(Ge,Gm),e(Ge,Cl),e(Cl,Vm),e(Ge,Qm),e(Ge,Ol),e(Ol,Jm),e(Ge,Um),p(s,Oo,i),b(lt,s,i),p(s,Po,i),b(rt,s,i),p(s,zo,i),p(s,oe,i),e(oe,Ym),e(oe,Pl),e(Pl,Km),e(oe,Wm),e(oe,zl),e(zl,Zm),e(oe,ef),e(oe,Dl),e(Dl,sf),e(oe,tf),e(oe,Ml),e(Ml,nf),e(oe,af),e(oe,Il),e(Il,lf),e(oe,rf),e(oe,Rl),e(Rl,of),e(oe,pf),p(s,Do,i),b(ot,s,i),p(s,Mo,i),b(it,s,i),p(s,Io,i),p(s,Y,i),e(Y,uf),e(Y,Sl),e(Sl,cf),e(Y,df),e(Y,Tl),e(Tl,mf),e(Y,ff),e(Y,Bl),e(Bl,hf),e(Y,xf),e(Y,Al),e(Al,vf),e(Y,gf),e(Y,Nl),e(Nl,jf),e(Y,bf),e(Y,Fl),e(Fl,Ef),e(Y,_f),e(Y,Ll),e(Ll,$f),e(Y,kf),e(Y,Xl),e(Xl,qf),e(Y,yf),p(s,Ro,i),b(pt,s,i),p(s,So,i),p(s,ks,i),e(ks,wf),e(ks,Hl),e(Hl,Cf),e(ks,Of),p(s,To,i),b(ut,s,i),p(s,Bo,i),p(s,Ft,i),e(Ft,Pf),p(s,Ao,i),b(ct,s,i),p(s,No,i),b(dt,s,i),p(s,Fo,i),p(s,Lt,i),e(Lt,zf),p(s,Lo,i),p(s,ps,i),e(ps,qs),e(qs,Gl),b(mt,Gl,null),e(ps,Df),e(ps,Vl),e(Vl,Mf),p(s,Xo,i),p(s,M,i),e(M,If),e(M,Ql),e(Ql,Rf),e(M,Sf),e(M,Jl),e(Jl,Tf),e(M,Bf),e(M,Ul),e(Ul,Af),e(M,Nf),e(M,Yl),e(Yl,Ff),e(M,Lf),e(M,Kl),e(Kl,Xf),e(M,Hf),e(M,Wl),e(Wl,Gf),e(M,Vf),e(M,Zl),e(Zl,Qf),e(M,Jf),e(M,er),e(er,Uf),e(M,Yf),e(M,sr),e(sr,Kf),e(M,Wf),e(M,tr),e(tr,Zf),e(M,eh),e(M,nr),e(nr,sh),e(M,th),e(M,ar),e(ar,nh),e(M,ah),p(s,Ho,i),p(s,G,i),e(G,lh),e(G,lr),e(lr,rh),e(G,oh),e(G,rr),e(rr,ih),e(G,ph),e(G,or),e(or,uh),e(G,ch),e(G,ir),e(ir,dh),e(G,mh),e(G,pr),e(pr,fh),e(G,hh),e(G,ur),e(ur,xh),e(G,vh),e(G,cr),e(cr,gh),e(G,jh),e(G,dr),e(dr,bh),e(G,Eh),e(G,mr),e(mr,_h),e(G,$h),p(s,Go,i),b(ft,s,i),p(s,Vo,i),b(ht,s,i),p(s,Qo,i),p(s,he,i),e(he,kh),e(he,fr),e(fr,qh),e(he,yh),e(he,hr),e(hr,wh),e(he,Ch),e(he,xr),e(xr,Oh),e(he,Ph),e(he,vr),e(vr,zh),e(he,Dh),e(he,gr),e(gr,Mh),e(he,Ih),p(s,Jo,i),b(xt,s,i),p(s,Uo,i),p(s,Xt,i),e(Xt,Rh),p(s,Yo,i),b(vt,s,i),p(s,Ko,i),p(s,ke,i),e(ke,Sh),e(ke,jr),e(jr,Th),e(ke,Bh),e(ke,br),e(br,Ah),e(ke,Nh),e(ke,Er),e(Er,Fh),e(ke,Lh),e(ke,_r),e(_r,Xh),e(ke,Hh),Wo=!0},p(s,[i]){const _t={};i&1&&(_t.fw=s[0]),m.$set(_t);let Ht=J;J=Zh(s),J!==Ht&&(mi(),h(gt[Ht],1,1,()=>{gt[Ht]=null}),di(),A=gt[J],A||(A=gt[J]=Wh[J](s),A.c()),x(A,1),A.m(W.parentNode,W));const $r={};i&2&&($r.$$scope={dirty:i,ctx:s}),cs.$set($r);const $t={};i&2&&($t.$$scope={dirty:i,ctx:s}),fs.$set($t);const kr={};i&2&&(kr.$$scope={dirty:i,ctx:s}),hs.$set(kr);let K=Ie;Ie=s2(s),Ie!==K&&(mi(),h(jt[K],1,1,()=>{jt[K]=null}),di(),Re=jt[Ie],Re||(Re=jt[Ie]=e2[Ie](s),Re.c()),x(Re,1),Re.m(Bt.parentNode,Bt));let Gt=Se;Se=n2(s),Se!==Gt&&(mi(),h(bt[Gt],1,1,()=>{bt[Gt]=null}),di(),Te=bt[Se],Te||(Te=bt[Se]=t2[Se](s),Te.c()),x(Te,1),Te.m(At.parentNode,At));let Vt=Be;Be=l2(s),Be!==Vt&&(mi(),h(Et[Vt],1,1,()=>{Et[Vt]=null}),di(),Ae=Et[Be],Ae||(Ae=Et[Be]=a2[Be](s),Ae.c()),x(Ae,1),Ae.m(Nt.parentNode,Nt))},i(s){Wo||(x(m.$$.fragment,s),x(N.$$.fragment,s),x(A),x(Cs.$$.fragment,s),x(cs.$$.fragment,s),x(Ds.$$.fragment,s),x(Ms.$$.fragment,s),x(Is.$$.fragment,s),x(Rs.$$.fragment,s),x(Ss.$$.fragment,s),x(Ts.$$.fragment,s),x(Bs.$$.fragment,s),x(As.$$.fragment,s),x(Ns.$$.fragment,s),x(Fs.$$.fragment,s),x(Ls.$$.fragment,s),x(Xs.$$.fragment,s),x(fs.$$.fragment,s),x(Hs.$$.fragment,s),x(Gs.$$.fragment,s),x(hs.$$.fragment,s),x(Vs.$$.fragment,s),x(Re),x(Qs.$$.fragment,s),x(Us.$$.fragment,s),x(Ys.$$.fragment,s),x(Ks.$$.fragment,s),x(Ws.$$.fragment,s),x(Zs.$$.fragment,s),x(Te),x(Ae),x(et.$$.fragment,s),x(st.$$.fragment,s),x(tt.$$.fragment,s),x(lt.$$.fragment,s),x(rt.$$.fragment,s),x(ot.$$.fragment,s),x(it.$$.fragment,s),x(pt.$$.fragment,s),x(ut.$$.fragment,s),x(ct.$$.fragment,s),x(dt.$$.fragment,s),x(mt.$$.fragment,s),x(ft.$$.fragment,s),x(ht.$$.fragment,s),x(xt.$$.fragment,s),x(vt.$$.fragment,s),Wo=!0)},o(s){h(m.$$.fragment,s),h(N.$$.fragment,s),h(A),h(Cs.$$.fragment,s),h(cs.$$.fragment,s),h(Ds.$$.fragment,s),h(Ms.$$.fragment,s),h(Is.$$.fragment,s),h(Rs.$$.fragment,s),h(Ss.$$.fragment,s),h(Ts.$$.fragment,s),h(Bs.$$.fragment,s),h(As.$$.fragment,s),h(Ns.$$.fragment,s),h(Fs.$$.fragment,s),h(Ls.$$.fragment,s),h(Xs.$$.fragment,s),h(fs.$$.fragment,s),h(Hs.$$.fragment,s),h(Gs.$$.fragment,s),h(hs.$$.fragment,s),h(Vs.$$.fragment,s),h(Re),h(Qs.$$.fragment,s),h(Us.$$.fragment,s),h(Ys.$$.fragment,s),h(Ks.$$.fragment,s),h(Ws.$$.fragment,s),h(Zs.$$.fragment,s),h(Te),h(Ae),h(et.$$.fragment,s),h(st.$$.fragment,s),h(tt.$$.fragment,s),h(lt.$$.fragment,s),h(rt.$$.fragment,s),h(ot.$$.fragment,s),h(it.$$.fragment,s),h(pt.$$.fragment,s),h(ut.$$.fragment,s),h(ct.$$.fragment,s),h(dt.$$.fragment,s),h(mt.$$.fragment,s),h(ft.$$.fragment,s),h(ht.$$.fragment,s),h(xt.$$.fragment,s),h(vt.$$.fragment,s),Wo=!1},d(s){a(u),s&&a(v),E(m,s),s&&a(w),s&&a(I),E(N),s&&a(z),gt[J].d(s),s&&a(W),s&&a($),s&&a(wr),E(Cs,s),s&&a(Cr),s&&a(U),s&&a(Or),s&&a(us),s&&a(Pr),E(cs,s),s&&a(zr),s&&a(as),E(Ds),s&&a(Dr),E(Ms,s),s&&a(Mr),s&&a(Ce),s&&a(Ir),s&&a(me),s&&a(Rr),s&&a(It),s&&a(Sr),E(Is,s),s&&a(Tr),s&&a(Ne),s&&a(Br),E(Rs,s),s&&a(Ar),s&&a(re),s&&a(Nr),E(Ss,s),s&&a(Fr),E(Ts,s),s&&a(Lr),s&&a(ms),s&&a(Xr),E(Bs,s),s&&a(Hr),E(As,s),s&&a(Gr),s&&a(Oe),s&&a(Vr),E(Ns,s),s&&a(Qr),E(Fs,s),s&&a(Jr),s&&a(be),s&&a(Ur),E(Ls,s),s&&a(Yr),E(Xs,s),s&&a(Kr),s&&a(O),s&&a(Wr),s&&a(Fe),s&&a(Zr),E(fs,s),s&&a(eo),s&&a(Ee),s&&a(so),s&&a(Z),s&&a(to),E(Hs,s),s&&a(no),E(Gs,s),s&&a(ao),s&&a(_e),s&&a(lo),E(hs,s),s&&a(ro),s&&a(ls),E(Vs),s&&a(oo),s&&a(fe),s&&a(io),jt[Ie].d(s),s&&a(Bt),s&&a(rs),E(Qs),s&&a(po),s&&a(Le),s&&a(uo),E(Us,s),s&&a(co),E(Ys,s),s&&a(mo),s&&a($e),s&&a(fo),E(Ks,s),s&&a(ho),E(Ws,s),s&&a(xo),s&&a(ee),s&&a(vo),s&&a(Xe),s&&a(go),s&&a(bs),s&&a(jo),s&&a(os),E(Zs),s&&a(bo),bt[Se].d(s),s&&a(At),s&&a(_s),s&&a(Eo),Et[Be].d(s),s&&a(Nt),E(et,s),s&&a(_o),s&&a($s),s&&a($o),E(st,s),s&&a(ko),E(tt,s),s&&a(qo),s&&a(q),s&&a(yo),s&&a(T),s&&a(wo),s&&a(is),s&&a(Co),s&&a(Ge),s&&a(Oo),E(lt,s),s&&a(Po),E(rt,s),s&&a(zo),s&&a(oe),s&&a(Do),E(ot,s),s&&a(Mo),E(it,s),s&&a(Io),s&&a(Y),s&&a(Ro),E(pt,s),s&&a(So),s&&a(ks),s&&a(To),E(ut,s),s&&a(Bo),s&&a(Ft),s&&a(Ao),E(ct,s),s&&a(No),E(dt,s),s&&a(Fo),s&&a(Lt),s&&a(Lo),s&&a(ps),E(mt),s&&a(Xo),s&&a(M),s&&a(Ho),s&&a(G),s&&a(Go),E(ft,s),s&&a(Vo),E(ht,s),s&&a(Qo),s&&a(he),s&&a(Jo),E(xt,s),s&&a(Uo),s&&a(Xt),s&&a(Yo),E(vt,s),s&&a(Ko),s&&a(ke)}}}const f1={local:"pouvoirs-spciaux-des-itokenizersi-rapides",sections:[{local:"lobjet-ibatchencodingi",title:"L'objet <i>BatchEncoding</i>"},{local:"a-lintrieur-du-pipeline-tokenclassification",sections:[{local:"obtenir-les-rsultats-de-base-avec-le-pipeline",title:"Obtenir les r\xE9sultats de base avec le pipeline"},{local:"des-entres-aux-prdictions",title:"Des entr\xE9es aux pr\xE9dictions"},{local:"regroupement-des-entits",title:"Regroupement des entit\xE9s"}],title:"A l'int\xE9rieur du pipeline `token-classification`"}],title:"Pouvoirs sp\xE9ciaux des <i>tokenizers</i> rapides"};function h1(L,u,v){let m="pt";return e1(()=>{const w=new URLSearchParams(window.location.search);v(0,m=w.get("fw")||"pt")}),[m]}class $1 extends Yv{constructor(u){super();Kv(this,u,h1,m1,Wv,{})}}export{$1 as default,f1 as metadata};
