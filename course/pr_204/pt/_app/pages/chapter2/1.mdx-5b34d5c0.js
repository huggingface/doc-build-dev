import{S as ro,i as so,s as to,e as t,k as h,w as Xe,t as r,M as io,c as i,d as a,m as _,a as m,x as Ye,h as s,b as T,G as o,g as n,y as Ze,q as eo,o as oo,B as ao,v as mo}from"../../chunks/vendor-36eb7539.js";import{T as lo}from"../../chunks/Tip-ab092014.js";import{I as no}from"../../chunks/IconCopyLink-ec51f27f.js";function co(J){let c,v,p,f;return{c(){c=r("\u26A0\uFE0F Para beneficiar-se de todos os recursos dispon\xEDveis com o Model Hub e \u{1F917} Transformers, recomendamos  "),v=t("a"),p=r("criar uma conta"),f=r("."),this.h()},l(d){c=s(d,"\u26A0\uFE0F Para beneficiar-se de todos os recursos dispon\xEDveis com o Model Hub e \u{1F917} Transformers, recomendamos  "),v=i(d,"A",{href:!0});var u=m(v);p=s(u,"criar uma conta"),u.forEach(a),f=s(d,"."),this.h()},h(){T(v,"href","https://huggingface.co/join")},m(d,u){n(d,c,u),n(d,v,u),o(v,p),n(d,f,u)},d(d){d&&a(c),d&&a(v),d&&a(f)}}}function uo(J){let c,v,p,f,d,u,ae,N,re,K,E,se,k,te,ie,j,me,le,V,w,ne,W,b,C,D,de,ce,ue,q,F,pe,fe,G,ve,he,R,_e,Ee,be,y,B,qe,$e,X,z,Pe,Y,$,Te,H,Ae,ge,I,xe,ke,Z,P,we,Q,Ce,ye,U,ze,Ie,ee,A,oe;return u=new no({}),A=new lo({props:{$$slots:{default:[co]},$$scope:{ctx:J}}}),{c(){c=t("meta"),v=h(),p=t("h1"),f=t("a"),d=t("span"),Xe(u.$$.fragment),ae=h(),N=t("span"),re=r("Introdu\xE7\xE3o"),K=h(),E=t("p"),se=r("Como voc\xEA viu no "),k=t("a"),te=r("Capitulo 1"),ie=r(", normalmente modelos Transformers s\xE3o muito grandes. Com milh\xF5es a dezenas de "),j=t("em"),me=r("bilh\xF5es"),le=r(" de par\xE2metros, o treinamento e o deploy destes modelos \xE9 uma tarefa complicado. Al\xE9m disso, com novos modelos sendo lan\xE7ados quase diariamente e cada um tendo sua pr\xF3pria implementa\xE7\xE3o, experiment\xE1-los a todos n\xE3o \xE9 tarefa f\xE1cil."),V=h(),w=t("p"),ne=r("A biblioteca \u{1F917} Transformers foi criado para resolver este problema. Seu objetivo \xE9 fornecer uma API \xFAnica atrav\xE9s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter\xEDsticas da biblioteca s\xE3o:"),W=h(),b=t("ul"),C=t("li"),D=t("strong"),de=r("F\xE1cil de usar"),ce=r(": Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de \xFAltima gera\xE7\xE3o para infer\xEAncia pode ser feito em apenas duas linhas de c\xF3digo"),ue=h(),q=t("li"),F=t("strong"),pe=r("Flexibilidade"),fe=r(": Em sua ess\xEAncia, todos os modelos s\xE3o uma simples classe PyTorch "),G=t("code"),ve=r("nn.Module"),he=r(" ou TensorFlow "),R=t("code"),_e=r("tf.keras.Model"),Ee=r(" e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML)."),be=h(),y=t("li"),B=t("strong"),qe=r("Simplicidade"),$e=r(": Quase nenhuma abstra\xE7\xE3o \xE9 feita em toda a biblioteca. O \u201CTudo em um arquivo\u201D \xE9 um conceito principal: o \u201Cpasse para frente\u201D de um modelo \xE9 inteiramente definido em um \xFAnico arquivo, de modo que o c\xF3digo em si seja compreens\xEDvel e hacke\xE1vel."),X=h(),z=t("p"),Pe=r("Esta \xFAltima caracter\xEDstica torna \u{1F917} Transformers bem diferente de outras bibliotecas ML que modelos e/ou configura\xE7\xF5es s\xE3o compartilhados entre arquivos; em vez disso, cada modelo tem suas pr\xF3prias camadas. Al\xE9m de tornar os modelos mais acess\xEDveis e compreens\xEDveis, isto permite que voc\xEA experimente facilmente um modelo sem afetar outros."),Y=h(),$=t("p"),Te=r("Este cap\xEDtulo come\xE7ar\xE1 com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a fun\xE7\xE3o "),H=t("code"),Ae=r("pipeline()"),ge=r(" introduzida no "),I=t("a"),xe=r("Capitulo 1"),ke=r(". A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configura\xE7\xE3o, al\xE9m de mostrar como carregar um modelo e como ele processa as entradas num\xE9ricas para as previs\xF5es de sa\xEDda."),Z=h(),P=t("p"),we=r("Depois veremos a API do tokenizer, que \xE9 o outro componente principal da fun\xE7\xE3o "),Q=t("code"),Ce=r("pipeline()"),ye=r(". Os Tokenizers cuidam da primeira e da \xFAltima etapa do processamento, cuidando da convers\xE3o de texto para entradas num\xE9ricas para a rede neural, e da convers\xE3o de volta ao texto quando for necess\xE1rio. Por fim, mostraremos a voc\xEA como lidar com o envio de v\xE1rias frases atrav\xE9s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a fun\xE7\xE3o de alto n\xEDvel "),U=t("code"),ze=r("tokenizer()"),Ie=r("."),ee=h(),Xe(A.$$.fragment),this.h()},l(e){const l=io('[data-svelte="svelte-1phssyn"]',document.head);c=i(l,"META",{name:!0,content:!0}),l.forEach(a),v=_(e),p=i(e,"H1",{class:!0});var g=m(p);f=i(g,"A",{id:!0,class:!0,href:!0});var Se=m(f);d=i(Se,"SPAN",{});var Le=m(d);Ye(u.$$.fragment,Le),Le.forEach(a),Se.forEach(a),ae=_(g),N=i(g,"SPAN",{});var Ne=m(N);re=s(Ne,"Introdu\xE7\xE3o"),Ne.forEach(a),g.forEach(a),K=_(e),E=i(e,"P",{});var M=m(E);se=s(M,"Como voc\xEA viu no "),k=i(M,"A",{href:!0});var je=m(k);te=s(je,"Capitulo 1"),je.forEach(a),ie=s(M,", normalmente modelos Transformers s\xE3o muito grandes. Com milh\xF5es a dezenas de "),j=i(M,"EM",{});var De=m(j);me=s(De,"bilh\xF5es"),De.forEach(a),le=s(M," de par\xE2metros, o treinamento e o deploy destes modelos \xE9 uma tarefa complicado. Al\xE9m disso, com novos modelos sendo lan\xE7ados quase diariamente e cada um tendo sua pr\xF3pria implementa\xE7\xE3o, experiment\xE1-los a todos n\xE3o \xE9 tarefa f\xE1cil."),M.forEach(a),V=_(e),w=i(e,"P",{});var Fe=m(w);ne=s(Fe,"A biblioteca \u{1F917} Transformers foi criado para resolver este problema. Seu objetivo \xE9 fornecer uma API \xFAnica atrav\xE9s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter\xEDsticas da biblioteca s\xE3o:"),Fe.forEach(a),W=_(e),b=i(e,"UL",{});var O=m(b);C=i(O,"LI",{});var Me=m(C);D=i(Me,"STRONG",{});var Ge=m(D);de=s(Ge,"F\xE1cil de usar"),Ge.forEach(a),ce=s(Me,": Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de \xFAltima gera\xE7\xE3o para infer\xEAncia pode ser feito em apenas duas linhas de c\xF3digo"),Me.forEach(a),ue=_(O),q=i(O,"LI",{});var x=m(q);F=i(x,"STRONG",{});var Re=m(F);pe=s(Re,"Flexibilidade"),Re.forEach(a),fe=s(x,": Em sua ess\xEAncia, todos os modelos s\xE3o uma simples classe PyTorch "),G=i(x,"CODE",{});var Be=m(G);ve=s(Be,"nn.Module"),Be.forEach(a),he=s(x," ou TensorFlow "),R=i(x,"CODE",{});var He=m(R);_e=s(He,"tf.keras.Model"),He.forEach(a),Ee=s(x," e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML)."),x.forEach(a),be=_(O),y=i(O,"LI",{});var Oe=m(y);B=i(Oe,"STRONG",{});var Qe=m(B);qe=s(Qe,"Simplicidade"),Qe.forEach(a),$e=s(Oe,": Quase nenhuma abstra\xE7\xE3o \xE9 feita em toda a biblioteca. O \u201CTudo em um arquivo\u201D \xE9 um conceito principal: o \u201Cpasse para frente\u201D de um modelo \xE9 inteiramente definido em um \xFAnico arquivo, de modo que o c\xF3digo em si seja compreens\xEDvel e hacke\xE1vel."),Oe.forEach(a),O.forEach(a),X=_(e),z=i(e,"P",{});var Ue=m(z);Pe=s(Ue,"Esta \xFAltima caracter\xEDstica torna \u{1F917} Transformers bem diferente de outras bibliotecas ML que modelos e/ou configura\xE7\xF5es s\xE3o compartilhados entre arquivos; em vez disso, cada modelo tem suas pr\xF3prias camadas. Al\xE9m de tornar os modelos mais acess\xEDveis e compreens\xEDveis, isto permite que voc\xEA experimente facilmente um modelo sem afetar outros."),Ue.forEach(a),Y=_(e),$=i(e,"P",{});var S=m($);Te=s(S,"Este cap\xEDtulo come\xE7ar\xE1 com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a fun\xE7\xE3o "),H=i(S,"CODE",{});var Je=m(H);Ae=s(Je,"pipeline()"),Je.forEach(a),ge=s(S," introduzida no "),I=i(S,"A",{href:!0});var Ke=m(I);xe=s(Ke,"Capitulo 1"),Ke.forEach(a),ke=s(S,". A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configura\xE7\xE3o, al\xE9m de mostrar como carregar um modelo e como ele processa as entradas num\xE9ricas para as previs\xF5es de sa\xEDda."),S.forEach(a),Z=_(e),P=i(e,"P",{});var L=m(P);we=s(L,"Depois veremos a API do tokenizer, que \xE9 o outro componente principal da fun\xE7\xE3o "),Q=i(L,"CODE",{});var Ve=m(Q);Ce=s(Ve,"pipeline()"),Ve.forEach(a),ye=s(L,". Os Tokenizers cuidam da primeira e da \xFAltima etapa do processamento, cuidando da convers\xE3o de texto para entradas num\xE9ricas para a rede neural, e da convers\xE3o de volta ao texto quando for necess\xE1rio. Por fim, mostraremos a voc\xEA como lidar com o envio de v\xE1rias frases atrav\xE9s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a fun\xE7\xE3o de alto n\xEDvel "),U=i(L,"CODE",{});var We=m(U);ze=s(We,"tokenizer()"),We.forEach(a),Ie=s(L,"."),L.forEach(a),ee=_(e),Ye(A.$$.fragment,e),this.h()},h(){T(c,"name","hf:doc:metadata"),T(c,"content",JSON.stringify(po)),T(f,"id","introduo"),T(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(f,"href","#introduo"),T(p,"class","relative group"),T(k,"href","/course/pt/chapter1"),T(I,"href","/course/pt/chapter1")},m(e,l){o(document.head,c),n(e,v,l),n(e,p,l),o(p,f),o(f,d),Ze(u,d,null),o(p,ae),o(p,N),o(N,re),n(e,K,l),n(e,E,l),o(E,se),o(E,k),o(k,te),o(E,ie),o(E,j),o(j,me),o(E,le),n(e,V,l),n(e,w,l),o(w,ne),n(e,W,l),n(e,b,l),o(b,C),o(C,D),o(D,de),o(C,ce),o(b,ue),o(b,q),o(q,F),o(F,pe),o(q,fe),o(q,G),o(G,ve),o(q,he),o(q,R),o(R,_e),o(q,Ee),o(b,be),o(b,y),o(y,B),o(B,qe),o(y,$e),n(e,X,l),n(e,z,l),o(z,Pe),n(e,Y,l),n(e,$,l),o($,Te),o($,H),o(H,Ae),o($,ge),o($,I),o(I,xe),o($,ke),n(e,Z,l),n(e,P,l),o(P,we),o(P,Q),o(Q,Ce),o(P,ye),o(P,U),o(U,ze),o(P,Ie),n(e,ee,l),Ze(A,e,l),oe=!0},p(e,[l]){const g={};l&2&&(g.$$scope={dirty:l,ctx:e}),A.$set(g)},i(e){oe||(eo(u.$$.fragment,e),eo(A.$$.fragment,e),oe=!0)},o(e){oo(u.$$.fragment,e),oo(A.$$.fragment,e),oe=!1},d(e){a(c),e&&a(v),e&&a(p),ao(u),e&&a(K),e&&a(E),e&&a(V),e&&a(w),e&&a(W),e&&a(b),e&&a(X),e&&a(z),e&&a(Y),e&&a($),e&&a(Z),e&&a(P),e&&a(ee),ao(A,e)}}}const po={local:"introduo",title:"Introdu\xE7\xE3o"};function fo(J){return mo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Eo extends ro{constructor(c){super();so(this,c,fo,uo,to,{})}}export{Eo as default,po as metadata};
