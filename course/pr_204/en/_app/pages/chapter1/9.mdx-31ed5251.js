import{S as je,i as Fe,s as Ke,e as o,k as i,w as Qe,t as s,M as Ve,c as r,d as t,m as d,a as n,x as Ze,h as l,b as P,G as e,g as y,y as et,L as tt,q as at,o as ot,B as rt,v as nt}from"../../chunks/vendor-37701547.js";import{I as st}from"../../chunks/IconCopyLink-80214518.js";function lt(Pe){let c,J,f,v,L,R,ee,x,te,U,_,ae,S,oe,re,j,g,ne,F,w,$,m,I,se,le,q,ie,de,C,he,ce,u,p,H,fe,me,M,ue,pe,G,Te,Ee,T,N,ve,_e,z,we,ye,O,Re,ge,E,Y,ke,Ae,W,be,De,X,Be,K;return R=new st({}),{c(){c=o("meta"),J=i(),f=o("h1"),v=o("a"),L=o("span"),Qe(R.$$.fragment),ee=i(),x=o("span"),te=s("Summary"),U=i(),_=o("p"),ae=s("In this chapter, you saw how to approach different NLP tasks using the high-level "),S=o("code"),oe=s("pipeline()"),re=s(" function from \u{1F917} Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser."),j=i(),g=o("p"),ne=s("We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:"),F=i(),w=o("table"),$=o("thead"),m=o("tr"),I=o("th"),se=s("Model"),le=i(),q=o("th"),ie=s("Examples"),de=i(),C=o("th"),he=s("Tasks"),ce=i(),u=o("tbody"),p=o("tr"),H=o("td"),fe=s("Encoder"),me=i(),M=o("td"),ue=s("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),pe=i(),G=o("td"),Te=s("Sentence classification, named entity recognition, extractive question answering"),Ee=i(),T=o("tr"),N=o("td"),ve=s("Decoder"),_e=i(),z=o("td"),we=s("CTRL, GPT, GPT-2, Transformer XL"),ye=i(),O=o("td"),Re=s("Text generation"),ge=i(),E=o("tr"),Y=o("td"),ke=s("Encoder-decoder"),Ae=i(),W=o("td"),be=s("BART, T5, Marian, mBART"),De=i(),X=o("td"),Be=s("Summarization, translation, generative question answering"),this.h()},l(a){const h=Ve('[data-svelte="svelte-1phssyn"]',document.head);c=r(h,"META",{name:!0,content:!0}),h.forEach(t),J=d(a),f=r(a,"H1",{class:!0});var Q=n(f);v=r(Q,"A",{id:!0,class:!0,href:!0});var Le=n(v);L=r(Le,"SPAN",{});var xe=n(L);Ze(R.$$.fragment,xe),xe.forEach(t),Le.forEach(t),ee=d(Q),x=r(Q,"SPAN",{});var Se=n(x);te=l(Se,"Summary"),Se.forEach(t),Q.forEach(t),U=d(a),_=r(a,"P",{});var V=n(_);ae=l(V,"In this chapter, you saw how to approach different NLP tasks using the high-level "),S=r(V,"CODE",{});var $e=n(S);oe=l($e,"pipeline()"),$e.forEach(t),re=l(V," function from \u{1F917} Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser."),V.forEach(t),j=d(a),g=r(a,"P",{});var Ie=n(g);ne=l(Ie,"We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:"),Ie.forEach(t),F=d(a),w=r(a,"TABLE",{});var Z=n(w);$=r(Z,"THEAD",{});var qe=n($);m=r(qe,"TR",{});var k=n(m);I=r(k,"TH",{});var Ce=n(I);se=l(Ce,"Model"),Ce.forEach(t),le=d(k),q=r(k,"TH",{});var He=n(q);ie=l(He,"Examples"),He.forEach(t),de=d(k),C=r(k,"TH",{});var Me=n(C);he=l(Me,"Tasks"),Me.forEach(t),k.forEach(t),qe.forEach(t),ce=d(Z),u=r(Z,"TBODY",{});var A=n(u);p=r(A,"TR",{});var b=n(p);H=r(b,"TD",{});var Ge=n(H);fe=l(Ge,"Encoder"),Ge.forEach(t),me=d(b),M=r(b,"TD",{});var Ne=n(M);ue=l(Ne,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Ne.forEach(t),pe=d(b),G=r(b,"TD",{});var ze=n(G);Te=l(ze,"Sentence classification, named entity recognition, extractive question answering"),ze.forEach(t),b.forEach(t),Ee=d(A),T=r(A,"TR",{});var D=n(T);N=r(D,"TD",{});var Oe=n(N);ve=l(Oe,"Decoder"),Oe.forEach(t),_e=d(D),z=r(D,"TD",{});var Ye=n(z);we=l(Ye,"CTRL, GPT, GPT-2, Transformer XL"),Ye.forEach(t),ye=d(D),O=r(D,"TD",{});var We=n(O);Re=l(We,"Text generation"),We.forEach(t),D.forEach(t),ge=d(A),E=r(A,"TR",{});var B=n(E);Y=r(B,"TD",{});var Xe=n(Y);ke=l(Xe,"Encoder-decoder"),Xe.forEach(t),Ae=d(B),W=r(B,"TD",{});var Je=n(W);be=l(Je,"BART, T5, Marian, mBART"),Je.forEach(t),De=d(B),X=r(B,"TD",{});var Ue=n(X);Be=l(Ue,"Summarization, translation, generative question answering"),Ue.forEach(t),B.forEach(t),A.forEach(t),Z.forEach(t),this.h()},h(){P(c,"name","hf:doc:metadata"),P(c,"content",JSON.stringify(it)),P(v,"id","summary"),P(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(v,"href","#summary"),P(f,"class","relative group")},m(a,h){e(document.head,c),y(a,J,h),y(a,f,h),e(f,v),e(v,L),et(R,L,null),e(f,ee),e(f,x),e(x,te),y(a,U,h),y(a,_,h),e(_,ae),e(_,S),e(S,oe),e(_,re),y(a,j,h),y(a,g,h),e(g,ne),y(a,F,h),y(a,w,h),e(w,$),e($,m),e(m,I),e(I,se),e(m,le),e(m,q),e(q,ie),e(m,de),e(m,C),e(C,he),e(w,ce),e(w,u),e(u,p),e(p,H),e(H,fe),e(p,me),e(p,M),e(M,ue),e(p,pe),e(p,G),e(G,Te),e(u,Ee),e(u,T),e(T,N),e(N,ve),e(T,_e),e(T,z),e(z,we),e(T,ye),e(T,O),e(O,Re),e(u,ge),e(u,E),e(E,Y),e(Y,ke),e(E,Ae),e(E,W),e(W,be),e(E,De),e(E,X),e(X,Be),K=!0},p:tt,i(a){K||(at(R.$$.fragment,a),K=!0)},o(a){ot(R.$$.fragment,a),K=!1},d(a){t(c),a&&t(J),a&&t(f),rt(R),a&&t(U),a&&t(_),a&&t(j),a&&t(g),a&&t(F),a&&t(w)}}}const it={local:"summary",title:"Summary"};function dt(Pe){return nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ft extends je{constructor(c){super();Fe(this,c,dt,lt,Ke,{})}}export{ft as default,it as metadata};
