import{S as Se,i as Me,s as Ce,e as o,k as f,w as ke,t as d,M as Ne,c as r,d as t,m,a as n,x as Pe,h,b as s,G as a,g as c,y as xe,L as Ue,q as Be,o as qe,B as Ie,v as ze}from"../../chunks/vendor-37701547.js";import{Y as De}from"../../chunks/Youtube-3501dc06.js";import{I as Je}from"../../chunks/IconCopyLink-80214518.js";function Ye(fe){let p,N,v,_,P,g,Q,x,F,U,w,z,E,K,B,V,W,D,R,X,J,L,Z,Y,k,ee,j,i,q,y,te,ae,I,$,oe,re,S,T,ne,le,M,b,se,ie,C,A,ce,G;return g=new Je({}),w=new De({props:{id:"MUqNwgPjJvQ"}}),{c(){p=o("meta"),N=f(),v=o("h1"),_=o("a"),P=o("span"),ke(g.$$.fragment),Q=f(),x=o("span"),F=d("Encoder models"),U=f(),ke(w.$$.fragment),z=f(),E=o("p"),K=d("Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),B=o("em"),V=d("auto-encoding models"),W=d("."),D=f(),R=o("p"),X=d("The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),J=f(),L=o("p"),Z=d("Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),Y=f(),k=o("p"),ee=d("Representatives of this family of models include:"),j=f(),i=o("ul"),q=o("li"),y=o("a"),te=d("ALBERT"),ae=f(),I=o("li"),$=o("a"),oe=d("BERT"),re=f(),S=o("li"),T=o("a"),ne=d("DistilBERT"),le=f(),M=o("li"),b=o("a"),se=d("ELECTRA"),ie=f(),C=o("li"),A=o("a"),ce=d("RoBERTa"),this.h()},l(e){const l=Ne('[data-svelte="svelte-1phssyn"]',document.head);p=r(l,"META",{name:!0,content:!0}),l.forEach(t),N=m(e),v=r(e,"H1",{class:!0});var H=n(v);_=r(H,"A",{id:!0,class:!0,href:!0});var de=n(_);P=r(de,"SPAN",{});var me=n(P);Pe(g.$$.fragment,me),me.forEach(t),de.forEach(t),Q=m(H),x=r(H,"SPAN",{});var he=n(x);F=h(he,"Encoder models"),he.forEach(t),H.forEach(t),U=m(e),Pe(w.$$.fragment,e),z=m(e),E=r(e,"P",{});var O=n(E);K=h(O,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),B=r(O,"EM",{});var ue=n(B);V=h(ue,"auto-encoding models"),ue.forEach(t),W=h(O,"."),O.forEach(t),D=m(e),R=r(e,"P",{});var pe=n(R);X=h(pe,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),pe.forEach(t),J=m(e),L=r(e,"P",{});var ve=n(L);Z=h(ve,"Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),ve.forEach(t),Y=m(e),k=r(e,"P",{});var _e=n(k);ee=h(_e,"Representatives of this family of models include:"),_e.forEach(t),j=m(e),i=r(e,"UL",{});var u=n(i);q=r(u,"LI",{});var Ee=n(q);y=r(Ee,"A",{href:!0,rel:!0});var ge=n(y);te=h(ge,"ALBERT"),ge.forEach(t),Ee.forEach(t),ae=m(u),I=r(u,"LI",{});var we=n(I);$=r(we,"A",{href:!0,rel:!0});var ye=n($);oe=h(ye,"BERT"),ye.forEach(t),we.forEach(t),re=m(u),S=r(u,"LI",{});var $e=n(S);T=r($e,"A",{href:!0,rel:!0});var Te=n(T);ne=h(Te,"DistilBERT"),Te.forEach(t),$e.forEach(t),le=m(u),M=r(u,"LI",{});var be=n(M);b=r(be,"A",{href:!0,rel:!0});var Ae=n(b);se=h(Ae,"ELECTRA"),Ae.forEach(t),be.forEach(t),ie=m(u),C=r(u,"LI",{});var Re=n(C);A=r(Re,"A",{href:!0,rel:!0});var Le=n(A);ce=h(Le,"RoBERTa"),Le.forEach(t),Re.forEach(t),u.forEach(t),this.h()},h(){s(p,"name","hf:doc:metadata"),s(p,"content",JSON.stringify(je)),s(_,"id","encoder-models"),s(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(_,"href","#encoder-models"),s(v,"class","relative group"),s(y,"href","https://huggingface.co/transformers/model_doc/albert.html"),s(y,"rel","nofollow"),s($,"href","https://huggingface.co/transformers/model_doc/bert.html"),s($,"rel","nofollow"),s(T,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),s(T,"rel","nofollow"),s(b,"href","https://huggingface.co/transformers/model_doc/electra.html"),s(b,"rel","nofollow"),s(A,"href","https://huggingface.co/transformers/model_doc/roberta.html"),s(A,"rel","nofollow")},m(e,l){a(document.head,p),c(e,N,l),c(e,v,l),a(v,_),a(_,P),xe(g,P,null),a(v,Q),a(v,x),a(x,F),c(e,U,l),xe(w,e,l),c(e,z,l),c(e,E,l),a(E,K),a(E,B),a(B,V),a(E,W),c(e,D,l),c(e,R,l),a(R,X),c(e,J,l),c(e,L,l),a(L,Z),c(e,Y,l),c(e,k,l),a(k,ee),c(e,j,l),c(e,i,l),a(i,q),a(q,y),a(y,te),a(i,ae),a(i,I),a(I,$),a($,oe),a(i,re),a(i,S),a(S,T),a(T,ne),a(i,le),a(i,M),a(M,b),a(b,se),a(i,ie),a(i,C),a(C,A),a(A,ce),G=!0},p:Ue,i(e){G||(Be(g.$$.fragment,e),Be(w.$$.fragment,e),G=!0)},o(e){qe(g.$$.fragment,e),qe(w.$$.fragment,e),G=!1},d(e){t(p),e&&t(N),e&&t(v),Ie(g),e&&t(U),Ie(w,e),e&&t(z),e&&t(E),e&&t(D),e&&t(R),e&&t(J),e&&t(L),e&&t(Y),e&&t(k),e&&t(j),e&&t(i)}}}const je={local:"encoder-models",title:"Encoder models"};function Ge(fe){return ze(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fe extends Se{constructor(p){super();Me(this,p,Ge,Ye,Ce,{})}}export{Fe as default,je as metadata};
