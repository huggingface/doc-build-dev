import{S as bc,i as zc,s as wc,e as r,k as p,w as m,t as n,M as Pc,c as o,d as t,m as c,a as l,x as d,h as a,b as g,G as s,g as u,y as f,q as h,o as v,B as x,v as yc}from"../../chunks/vendor-hf-doc-builder.js";import{T as Mc}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Cc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Sa}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as E}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Ac}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Dc(xn){let z,ie,L,R,ne,G,ge,Y;return{c(){z=r("p"),ie=n("\u26A0\uFE0F Entra\xEEner un "),L=r("em"),R=n("tokenizer"),ne=n(" n\u2019est pas la m\xEAme chose qu\u2019entra\xEEner un mod\xE8le ! L\u2019entra\xEEnement du mod\xE8le utilise la descente de gradient stochastique pour r\xE9duire un peu plus la perte \xE0 chaque batch. Il est par nature al\xE9atoire (ce qui signifie que vous devez d\xE9finir des graines pour obtenir les m\xEAmes r\xE9sultats lorsque vous effectuez deux fois le m\xEAme entra\xEEnement). Entra\xEEner un "),G=r("em"),ge=n("tokenizer"),Y=n(" est un processus statistique qui identifie les meilleurs sous-mots \xE0 choisir pour un corpus donn\xE9. Les r\xE8gles exactes utilis\xE9es pour les choisir d\xE9pendent de l\u2019algorithme de tok\xE9nisation. Le processus est d\xE9terministe, ce qui signifie que vous obtenez toujours les m\xEAmes r\xE9sultats lorsque vous vous entra\xEEnez avec le m\xEAme algorithme sur le m\xEAme corpus.")},l(ae){z=o(ae,"P",{});var N=l(z);ie=a(N,"\u26A0\uFE0F Entra\xEEner un "),L=o(N,"EM",{});var ts=l(L);R=a(ts,"tokenizer"),ts.forEach(t),ne=a(N," n\u2019est pas la m\xEAme chose qu\u2019entra\xEEner un mod\xE8le ! L\u2019entra\xEEnement du mod\xE8le utilise la descente de gradient stochastique pour r\xE9duire un peu plus la perte \xE0 chaque batch. Il est par nature al\xE9atoire (ce qui signifie que vous devez d\xE9finir des graines pour obtenir les m\xEAmes r\xE9sultats lorsque vous effectuez deux fois le m\xEAme entra\xEEnement). Entra\xEEner un "),G=o(N,"EM",{});var ns=l(G);ge=a(ns,"tokenizer"),ns.forEach(t),Y=a(N," est un processus statistique qui identifie les meilleurs sous-mots \xE0 choisir pour un corpus donn\xE9. Les r\xE8gles exactes utilis\xE9es pour les choisir d\xE9pendent de l\u2019algorithme de tok\xE9nisation. Le processus est d\xE9terministe, ce qui signifie que vous obtenez toujours les m\xEAmes r\xE9sultats lorsque vous vous entra\xEEnez avec le m\xEAme algorithme sur le m\xEAme corpus."),N.forEach(t)},m(ae,N){u(ae,z,N),s(z,ie),s(z,L),s(L,R),s(z,ne),s(z,G),s(G,ge),s(z,Y)},d(ae){ae&&t(z)}}}function Lc(xn){let z,ie,L,R,ne,G,ge,Y,ae,N,ts,ns,_n,qe,gn,$,Ta,zs,Ia,Ga,ws,Ra,Ua,Ps,Ha,Va,as,Fa,Ya,ys,Ba,Ka,Ms,Xa,Ja,Cs,Qa,Za,As,Wa,er,Ds,sr,tr,qn,je,jn,ue,En,re,pe,Ls,Ee,nr,Ns,ar,$n,U,rr,Os,or,lr,Ss,ir,ur,Ts,pr,cr,kn,O,mr,ce,dr,Is,fr,hr,Gs,vr,xr,$e,_r,gr,ke,qr,jr,bn,be,zn,rs,Er,wn,ze,Pn,we,yn,B,$r,Rs,kr,br,Us,zr,wr,Mn,Pe,Cn,os,Pr,An,ye,Dn,H,yr,Hs,Mr,Cr,Vs,Ar,Dr,Fs,Lr,Nr,Ln,ls,Or,Nn,Me,On,is,Sr,Sn,Ce,Tn,K,Tr,Ys,Ir,Gr,Bs,Rr,Ur,In,us,Hr,Gn,Ae,Rn,ps,Vr,Un,De,Hn,cs,Fr,Vn,Le,Fn,X,Yr,Ks,Br,Kr,Xs,Xr,Jr,Yn,Ne,Bn,ms,Qr,Kn,oe,me,Js,Oe,Zr,ds,Wr,Qs,eo,Xn,J,so,Zs,to,no,Ws,ao,ro,Jn,Se,Qn,Q,oo,et,lo,io,st,uo,po,Zn,de,co,tt,mo,fo,Wn,Te,ea,Ie,sa,C,ho,nt,vo,xo,at,_o,go,rt,qo,jo,ot,Eo,$o,lt,ko,bo,ta,Z,zo,it,wo,Po,ut,yo,Mo,na,Ge,aa,fs,Co,ra,w,Ao,pt,Do,Lo,ct,No,Oo,mt,So,To,dt,Io,Go,ft,Ro,Uo,ht,Ho,Vo,Re,Fo,Yo,oa,y,Bo,vt,Ko,Xo,xt,Jo,Qo,_t,Zo,Wo,gt,el,sl,qt,tl,nl,hs,al,rl,la,b,ol,jt,ll,il,Et,ul,pl,Ue,cl,ml,$t,dl,fl,kt,hl,vl,bt,xl,_l,zt,gl,ql,wt,jl,El,ia,He,ua,Ve,pa,q,$l,Pt,kl,bl,yt,zl,wl,Mt,Pl,yl,Ct,Ml,Cl,At,Al,Dl,Dt,Ll,Nl,Lt,Ol,Sl,Nt,Tl,Il,Ot,Gl,Rl,St,Ul,Hl,Tt,Vl,Fl,ca,Fe,ma,Ye,da,vs,Yl,fa,Be,ha,Ke,va,_,Bl,It,Kl,Xl,Gt,Jl,Ql,Rt,Zl,Wl,Ut,ei,si,Ht,ti,ni,Vt,ai,ri,Ft,oi,li,Yt,ii,ui,Bt,pi,ci,Kt,mi,di,Xt,fi,hi,Jt,vi,xi,Qt,_i,gi,xa,le,fe,Zt,Xe,qi,xs,ji,Wt,Ei,_a,W,$i,en,ki,bi,sn,zi,wi,ga,Je,qa,A,Pi,tn,yi,Mi,nn,Ci,Ai,an,Di,Li,rn,Ni,Oi,on,Si,Ti,ja,Qe,Ea,he,Ii,ln,Gi,Ri,$a,Ze,ka,ve,Ui,un,Hi,Vi,ba,We,za,S,Fi,pn,Yi,Bi,cn,Ki,Xi,mn,Ji,Qi,dn,Zi,Wi,wa,es,Pa,T,eu,fn,su,tu,_s,nu,au,hn,ru,ou,vn,lu,iu,ya;return G=new Sa({}),qe=new Ac({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"}]}}),je=new Cc({props:{id:"DJimQynXZsQ"}}),ue=new Mc({props:{warning:!0,$$slots:{default:[Dc]},$$scope:{ctx:xn}}}),Ee=new Sa({}),be=new E({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Cela peut prendre quelques minutes alors prenez un th\xE9 ou un caf\xE9 pendant que vous patientez !</span>
raw_datasets = load_dataset(<span class="hljs-string">&quot;code_search_net&quot;</span>, <span class="hljs-string">&quot;python&quot;</span>)`}}),ze=new E({props:{code:'raw_datasets["train"]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]'}}),we=new E({props:{code:`Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;repository_name&#x27;</span>, <span class="hljs-string">&#x27;func_path_in_repository&#x27;</span>, <span class="hljs-string">&#x27;func_name&#x27;</span>, <span class="hljs-string">&#x27;whole_func_string&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_string&#x27;</span>, <span class="hljs-string">&#x27;func_code_tokens&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_string&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_tokens&#x27;</span>, <span class="hljs-string">&#x27;split_name&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_url&#x27;</span>
    ],
    num_rows: <span class="hljs-number">412178</span>
})`}}),Pe=new E({props:{code:'print(raw_datasets["train"][123456]["whole_func_string"])',highlighted:'<span class="hljs-built_in">print</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">123456</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>])'}}),ye=new E({props:{code:`
`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_simple_responses</span>(<span class="hljs-params">
      self, timeout_ms=<span class="hljs-literal">None</span>, info_cb=DEFAULT_MESSAGE_CALLBACK</span>):
    <span class="hljs-string">&quot;&quot;&quot;Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet&#x27;s message.
    &quot;&quot;&quot;</span>
    <span class="hljs-keyword">return</span> self._accept_responses(<span class="hljs-string">&#x27;OKAY&#x27;</span>, info_cb, timeout_ms=timeout_ms)`}}),Me=new E({props:{code:`# Ne d\xE9commentez pas la ligne suivante \xE0 moins que votre jeu de donn\xE9es soit petit !
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]`,highlighted:`<span class="hljs-comment"># Ne d\xE9commentez pas la ligne suivante \xE0 moins que votre jeu de donn\xE9es soit petit !</span>
<span class="hljs-comment"># training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]</span>`}}),Ce=new E({props:{code:`training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)`,highlighted:`training_corpus = (
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
)`}}),Ae=new E({props:{code:`gen = (i for i in range(10))
print(list(gen))
print(list(gen))`,highlighted:`gen = (i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))`}}),De=new E({props:{code:`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]`,highlighted:`[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]
[]`}}),Le=new E({props:{code:`
`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">return</span> (
        raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
    )


training_corpus = get_training_corpus()`}}),Ne=new E({props:{code:`def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
    <span class="hljs-keyword">for</span> start_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        samples = dataset[start_idx : start_idx + <span class="hljs-number">1000</span>]
        <span class="hljs-keyword">yield</span> samples[<span class="hljs-string">&quot;whole_func_string&quot;</span>]`}}),Oe=new Sa({}),Se=new E({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)`}}),Te=new E({props:{code:"",highlighted:`example = <span class="hljs-string">&#x27;&#x27;&#x27;def add_numbers(a, b):
    &quot;&quot;&quot;Add the two numbers \`a\` and \`b\`.&quot;&quot;&quot;
    return a + b&#x27;&#x27;&#x27;</span>

tokens = old_tokenizer.tokenize(example)
tokens`}}),Ie=new E({props:{code:`['def', '\u0120add', '_', 'n', 'umbers', '(', 'a', ',', '\u0120b', '):', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two',
 '\u0120numbers', '\u0120\`', 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`', '."', '""', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;umbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>,\n <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;.&quot;&#x27;</span>, <span class="hljs-string">&#x27;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),Ge=new E({props:{code:"tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)",highlighted:'tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="hljs-number">52000</span>)'}}),He=new E({props:{code:`tokens = tokenizer.tokenize(example)
tokens`,highlighted:`tokens = tokenizer.tokenize(example)
tokens`}}),Ve=new E({props:{code:`['def', '\u0120add', '_', 'numbers', '(', 'a', ',', '\u0120b', '):', '\u010A\u0120\u0120\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two', '\u0120numbers', '\u0120\`',
 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`."""', '\u010A\u0120\u0120\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;numbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>, <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>,\n <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`.&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),Fe=new E({props:{code:`print(len(tokens))
print(len(old_tokenizer.tokenize(example)))`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(tokens))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(old_tokenizer.tokenize(example)))`}}),Ye=new E({props:{code:`27
36`,highlighted:`<span class="hljs-number">27</span>
<span class="hljs-number">36</span>`}}),Be=new E({props:{code:"",highlighted:`example = <span class="hljs-string">&quot;&quot;&quot;class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    &quot;&quot;&quot;</span>
tokenizer.tokenize(example)`}}),Ke=new E({props:{code:`['class', '\u0120Linear', 'Layer', '():', '\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'init', '__(', 'self', ',', '\u0120input', '_', 'size', ',',
 '\u0120output', '_', 'size', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'weight', '\u0120=', '\u0120torch', '.', 'randn', '(', 'input', '_',
 'size', ',', '\u0120output', '_', 'size', ')', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'bias', '\u0120=', '\u0120torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', '\u010A\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'call', '__(', 'self', ',', '\u0120x', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120',
 '\u0120return', '\u0120x', '\u0120@', '\u0120self', '.', 'weights', '\u0120+', '\u0120self', '.', 'bias', '\u010A\u0120\u0120\u0120\u0120']`,highlighted:`[<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;\u0120Linear&#x27;</span>, <span class="hljs-string">&#x27;Layer&#x27;</span>, <span class="hljs-string">&#x27;():&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;init&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weight&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;randn&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>,
 <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;zeros&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>,
 <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;call&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;\u0120@&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weights&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120&#x27;</span>]`}}),Xe=new Sa({}),Je=new E({props:{code:'tokenizer.save_pretrained("code-search-net-tokenizer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),Qe=new E({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),Ze=new E({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),We=new E({props:{code:'tokenizer.push_to_hub("code-search-net-tokenizer")',highlighted:'tokenizer.push_to_hub(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),es=new E({props:{code:`# Remplacez "huggingface-course" ci-dessous par votre espace r\xE9el pour utiliser votre propre tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")`,highlighted:`<span class="hljs-comment"># Remplacez &quot;huggingface-course&quot; ci-dessous par votre espace r\xE9el pour utiliser votre propre tokenizer</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)`}}),{c(){z=r("meta"),ie=p(),L=r("h1"),R=r("a"),ne=r("span"),m(G.$$.fragment),ge=p(),Y=r("span"),ae=n("Entra\xEEner un nouveau "),N=r("i"),ts=n("tokenizer"),ns=n(" \xE0 partir d'un ancien"),_n=p(),m(qe.$$.fragment),gn=p(),$=r("p"),Ta=n("Si un mod\xE8le de langue n\u2019est pas disponible dans la langue qui vous int\xE9resse ou si votre corpus est tr\xE8s diff\xE9rent de celui sur lequel votre mod\xE8le de langue a \xE9t\xE9 entra\xEEn\xE9, vous voudrez tr\xE8s probablement r\xE9entra\xEEner le mod\xE8le \xE0 partir de z\xE9ro en utilisant un "),zs=r("em"),Ia=n("tokenizer"),Ga=n(" adapt\xE9 \xE0 vos donn\xE9es. Pour ce faire, vous devrez entra\xEEner un nouveau "),ws=r("em"),Ra=n("tokenizer"),Ua=n(" sur votre jeu de donn\xE9es. Mais qu\u2019est-ce que cela signifie exactement ? Lorsque nous avons examin\xE9 pour la premi\xE8re fois les "),Ps=r("em"),Ha=n("tokenizers"),Va=n(" dans le "),as=r("a"),Fa=n("chapitre 2"),Ya=n(", nous avons vu que la plupart des "),ys=r("em"),Ba=n("transformers"),Ka=n(" utilisent un "),Ms=r("em"),Xa=n("algorithme de tokenisation en sous-mots"),Ja=n(". Pour identifier les sous-mots qui sont int\xE9ressants et qui apparaissent le plus fr\xE9quemment dans un corpus donn\xE9, le "),Cs=r("em"),Qa=n("tokenizer"),Za=n(" doit examiner attentivement tous les textes du corpus. C\u2019est un processus que nous appelons "),As=r("em"),Wa=n("entra\xEEnement"),er=n(". Les r\xE8gles exactes qui r\xE9gissent cet apprentissage d\xE9pendent du type de "),Ds=r("em"),sr=n("tokenizer"),tr=n(" utilis\xE9. Nous passerons en revue les trois principaux algorithmes plus loin dans ce chapitre."),qn=p(),m(je.$$.fragment),jn=p(),m(ue.$$.fragment),En=p(),re=r("h2"),pe=r("a"),Ls=r("span"),m(Ee.$$.fragment),nr=p(),Ns=r("span"),ar=n("Assemblage d'un corpus"),$n=p(),U=r("p"),rr=n("Il y a une API tr\xE8s simple dans \u{1F917} "),Os=r("em"),or=n("Transformers"),lr=n(" que vous pouvez utiliser pour entra\xEEner un nouveau "),Ss=r("em"),ir=n("tokenizer"),ur=n(" avec les m\xEAmes caract\xE9ristiques qu\u2019un d\xE9j\xE0 existant : "),Ts=r("code"),pr=n("AutoTokenizer.train_new_from_iterator()"),cr=n(". Pour illustrer cela, disons que nous voulons entra\xEEner GPT-2 \xE0 partir de z\xE9ro mais dans une langue autre que l\u2019anglais. Notre premi\xE8re t\xE2che est de rassembler des batchs de donn\xE9es dans cette langue dans un corpus d\u2019entra\xEEnement. Pour avoir des exemples que tout le monde puisse comprendre, nous n\u2019utiliserons pas ici une langue comme le russe ou le chinois mais plut\xF4t une langue anglaise sp\xE9cialis\xE9e : le langage Python."),kn=p(),O=r("p"),mr=n("La biblioth\xE8que "),ce=r("a"),dr=n("\u{1F917} "),Is=r("em"),fr=n("Datasets"),hr=n(" peut nous aider \xE0 assembler un corpus de code source Python. Nous allons utiliser la fonction habituelle "),Gs=r("code"),vr=n("load_dataset()"),xr=n(" pour t\xE9l\xE9charger et mettre en cache le jeu de donn\xE9es "),$e=r("a"),_r=n("CodeSearchNet"),gr=n(". Ce jeu de donn\xE9es a \xE9t\xE9 cr\xE9\xE9 pour le "),ke=r("a"),qr=n("CodeSearchNet challenge"),jr=n(" et contient des millions de fonctions provenant de biblioth\xE8ques open source sur GitHub dans plusieurs langages de programmation. Ici, nous allons charger la partie Python de ce jeu de donn\xE9es :"),bn=p(),m(be.$$.fragment),zn=p(),rs=r("p"),Er=n("Nous pouvons jeter un coup d\u2019\u0153il au jeu d\u2019entra\xEEnement pour voir quelles sont les colonnes auxquelles nous avons acc\xE8s :"),wn=p(),m(ze.$$.fragment),Pn=p(),m(we.$$.fragment),yn=p(),B=r("p"),$r=n("Nous pouvons voir que le jeu de donn\xE9es s\xE9pare les cha\xEEnes de documents du code et sugg\xE8re une tokenization des deux. Ici, nous utiliserons simplement la colonne "),Rs=r("code"),kr=n("whole_func_string"),br=n(" pour entra\xEEner notre "),Us=r("em"),zr=n("tokenizer"),wr=n(". Nous pouvons regarder un exemple de la fa\xE7on suivante :"),Mn=p(),m(Pe.$$.fragment),Cn=p(),os=r("p"),Pr=n("qui nous affiche ce qui suit :"),An=p(),m(ye.$$.fragment),Dn=p(),H=r("p"),yr=n("La premi\xE8re chose \xE0 faire est de transformer le jeu de donn\xE9es en un "),Hs=r("em"),Mr=n("it\xE9rateur"),Cr=n(" de listes de textes. Par exemple, une liste de listes de textes. L\u2019utilisation de listes de textes permet \xE0 notre "),Vs=r("em"),Ar=n("tokenizer"),Dr=n(" d\u2019aller plus vite (l\u2019entra\xEEnement a alors lieu sur des batchs de textes au lieu de traiter des textes un par un). Et le fait que ce soit un it\xE9rateur permet d\u2019\xE9viter d\u2019avoir tout en m\xE9moire en m\xEAme temps. Si votre corpus est \xE9norme, vous voudrez profiter du fait que \u{1F917} "),Fs=r("em"),Lr=n("Datasets"),Nr=n(" ne charge pas tout en RAM mais stocke les \xE9l\xE9ments du jeu de donn\xE9es sur le disque."),Ln=p(),ls=r("p"),Or=n("Faire ce qui suit cr\xE9erait une liste de listes de 1 000 textes chacune mais chargerait tout en m\xE9moire :"),Nn=p(),m(Me.$$.fragment),On=p(),is=r("p"),Sr=n("En utilisant un g\xE9n\xE9rateur, nous pouvons \xE9viter que Python ne charge quoi que ce soit en m\xE9moire \xE0 moins que cela soit r\xE9ellement n\xE9cessaire. Pour cr\xE9er un tel g\xE9n\xE9rateur, il suffit de remplacer les crochets par des parenth\xE8ses :"),Sn=p(),m(Ce.$$.fragment),Tn=p(),K=r("p"),Tr=n("Cette ligne de code ne r\xE9cup\xE8re aucun \xE9l\xE9ment du jeu de donn\xE9es. Elle cr\xE9e simplement un objet que vous pouvez utiliser dans une boucle "),Ys=r("code"),Ir=n("for"),Gr=n(" Python. Les textes ne seront charg\xE9s que lorsque vous en aurez besoin (c\u2019est-\xE0-dire lorsque vous serez \xE0 l\u2019\xE9tape de la boucle "),Bs=r("code"),Rr=n("for"),Ur=n(" qui les requiert) et seulement 1 000 textes \xE0 la fois. De cette fa\xE7on, vous n\u2019\xE9puiserez pas toute votre m\xE9moire, m\xEAme si vous traitez un \xE9norme jeu de donn\xE9es."),In=p(),us=r("p"),Hr=n("Le probl\xE8me avec un objet g\xE9n\xE9rateur est qu\u2019il ne peut \xEAtre utilis\xE9 qu\u2019une seule fois. Ainsi, au lieu que cet objet nous donne deux fois la liste des 10 premiers chiffres :"),Gn=p(),m(Ae.$$.fragment),Rn=p(),ps=r("p"),Vr=n("on les re\xE7oit une fois et ensuite une liste vide :"),Un=p(),m(De.$$.fragment),Hn=p(),cs=r("p"),Fr=n("C\u2019est pourquoi nous d\xE9finissons une fonction qui renvoie un g\xE9n\xE9rateur \xE0 la place :"),Vn=p(),m(Le.$$.fragment),Fn=p(),X=r("p"),Yr=n("Vous pouvez \xE9galement d\xE9finir votre g\xE9n\xE9rateur \xE0 l\u2019int\xE9rieur d\u2019une boucle "),Ks=r("code"),Br=n("for"),Kr=n(" en utilisant l\u2019instruction "),Xs=r("code"),Xr=n("yield"),Jr=n(" :"),Yn=p(),m(Ne.$$.fragment),Bn=p(),ms=r("p"),Qr=n("qui produit exactement le m\xEAme g\xE9n\xE9rateur que pr\xE9c\xE9demment mais  permet d\u2019utiliser une logique plus complexe que celle que vous pouvez utiliser dans une compr\xE9hension de liste."),Kn=p(),oe=r("h2"),me=r("a"),Js=r("span"),m(Oe.$$.fragment),Zr=p(),ds=r("span"),Wr=n("Entra\xEEnement d'un nouveau "),Qs=r("i"),eo=n("tokenizer"),Xn=p(),J=r("p"),so=n("Maintenant que nous avons notre corpus sous la forme d\u2019un it\xE9rateur de batchs de textes, nous sommes pr\xEAts \xE0 entra\xEEner un nouveau "),Zs=r("em"),to=n("tokenizer"),no=n(". Pour ce faire, nous devons d\u2019abord charger le "),Ws=r("em"),ao=n("tokenizer"),ro=n(" que nous voulons coupler avec notre mod\xE8le (ici, le GPT-2) :"),Jn=p(),m(Se.$$.fragment),Qn=p(),Q=r("p"),oo=n("M\xEAme si nous allons entra\xEEner un nouveau "),et=r("em"),lo=n("tokenizer"),io=n(", c\u2019est une bonne id\xE9e de faire \xE7a pour \xE9viter de partir enti\xE8rement de z\xE9ro. De cette fa\xE7on, nous n\u2019aurons pas \xE0 sp\xE9cifier l\u2019algorithme de tok\xE9nisation ou les jetons sp\xE9ciaux que nous voulons utiliser. Notre nouveau "),st=r("em"),uo=n("tokenizer"),po=n(" sera exactement le m\xEAme que celui du GPT-2. La seule chose qui changera sera le vocabulaire qui sera d\xE9termin\xE9 lors de l\u2019entra\xEEnement sur notre corpus."),Zn=p(),de=r("p"),co=n("Voyons d\u2019abord comment ce "),tt=r("em"),mo=n("tokenizer"),fo=n(" traiterait un exemple de fonction :"),Wn=p(),m(Te.$$.fragment),ea=p(),m(Ie.$$.fragment),sa=p(),C=r("p"),ho=n("Ce "),nt=r("em"),vo=n("tokenizer"),xo=n(" poss\xE8de quelques symboles sp\xE9ciaux, comme "),at=r("code"),_o=n("\u0120"),go=n(" et "),rt=r("code"),qo=n("\u010A"),jo=n(", qui d\xE9signent respectivement les espaces et les retours \xE0 la ligne. Comme on peut le voir, ce n\u2019est pas tr\xE8s efficace. Le "),ot=r("em"),Eo=n("tokenizer"),$o=n(" renvoie des jetons individuels pour chaque espace alors qu\u2019il pourrait regrouper ceux des indentations (puisqu\u2019avoir des ensembles de quatre ou huit espaces est tr\xE8s courant dans du code). Il divise \xE9galement le nom de la fonction de fa\xE7on un peu bizarre car pas habitu\xE9 \xE0 voir des mots avec le caract\xE8re "),lt=r("code"),ko=n("_"),bo=n("."),ta=p(),Z=r("p"),zo=n("Entra\xEEnons un nouveau "),it=r("em"),wo=n("tokenizer"),Po=n(" et voyons s\u2019il r\xE9sout ces probl\xE8mes. Pour cela, nous allons utiliser la m\xE9thode "),ut=r("code"),yo=n("train_new_from_iterator()"),Mo=n(" :"),na=p(),m(Ge.$$.fragment),aa=p(),fs=r("p"),Co=n("Cette commande peut prendre un peu de temps si votre corpus est tr\xE8s grand. Pour ce jeu de donn\xE9es de 1,6 Go de textes, elle est tr\xE8s rapide (1 minute 16 secondes sur un CPU AMD Ryzen 9 3900X avec 12 c\u0153urs)."),ra=p(),w=r("p"),Ao=n("Notez que "),pt=r("code"),Do=n("AutoTokenizer.train_new_from_iterator()"),Lo=n(" ne fonctionne que si le "),ct=r("em"),No=n("tokenizer"),Oo=n(" que vous utilisez est un "),mt=r("em"),So=n("tokenizer"),To=n(" \xAB rapide \xBB. Comme vous le verrez dans la section suivante, la biblioth\xE8que \u{1F917} "),dt=r("em"),Io=n("Transformers"),Go=n(" contient deux types de "),ft=r("em"),Ro=n("tokenizers"),Uo=n(" : certains sont \xE9crits en pur Python et d\u2019autres (les rapides) sont soutenus par la biblioth\xE8que \u{1F917} "),ht=r("em"),Ho=n("Tokenizers"),Vo=n(" qui est \xE9crite dans le langage "),Re=r("a"),Fo=n("Rust"),Yo=n(". Python est le langage le plus souvent utilis\xE9 pour les applications de science des donn\xE9es et d\u2019apprentissage profond, mais lorsque quelque chose doit \xEAtre parall\xE9lis\xE9 pour \xEAtre rapide, il faut que cela soit \xE9crit dans un autre langage. Par exemple, les multiplications matricielles qui sont au c\u0153ur du calcul du mod\xE8le sont \xE9crites en CUDA, une biblioth\xE8que en C optimis\xE9e pour les GPUs."),oa=p(),y=r("p"),Bo=n("Entra\xEEner un tout nouveau "),vt=r("em"),Ko=n("tokenizer"),Xo=n(" en Python pur est atrocement lent, c\u2019est pourquoi nous avons d\xE9velopp\xE9 la biblioth\xE8que \u{1F917} "),xt=r("em"),Jo=n("Tokenizers"),Qo=n(". Notez que, tout comme vous n\u2019avez pas eu \xE0 apprendre le langage CUDA pour pouvoir ex\xE9cuter votre mod\xE8le sur un batch d\u2019entr\xE9es sur un GPU, vous n\u2019aurez pas besoin d\u2019apprendre Rust pour utiliser un "),_t=r("em"),Zo=n("tokenizer"),Wo=n(" rapide. La biblioth\xE8que \u{1F917} "),gt=r("em"),el=n("Tokenizers"),sl=n(" fournit des liaisons Python pour de nombreuses m\xE9thodes qui appellent en interne un morceau de code en Rust. Par exemple, pour parall\xE9liser l\u2019entra\xEEnement de votre nouveau "),qt=r("em"),tl=n("tokenizer"),nl=n(" ou, comme nous l\u2019avons vu dans le "),hs=r("a"),al=n("chapitre 3"),rl=n(", la tokenisation d\u2019un lot d\u2019entr\xE9es."),la=p(),b=r("p"),ol=n("La plupart des "),jt=r("em"),ll=n("transformers"),il=n(" ont un "),Et=r("em"),ul=n("tokenizer"),pl=n(" rapide de disponible. Il y a quelques exceptions que vous pouvez v\xE9rifier "),Ue=r("a"),cl=n("ici"),ml=n(". S\u2019il est disponible, l\u2019API "),$t=r("code"),dl=n("AutoTokenizer"),fl=n(" s\xE9lectionne toujours pour vous le "),kt=r("em"),hl=n("tokenizer"),vl=n(" rapide. Dans la prochaine section, nous allons jeter un coup d\u2019oeil \xE0 certaines des autres caract\xE9ristiques sp\xE9ciales des "),bt=r("em"),xl=n("tokenizers"),_l=n(" rapides, qui seront tr\xE8s utiles pour des t\xE2ches comme la classification de "),zt=r("em"),gl=n("tokens"),ql=n(" et la r\xE9ponse aux questions. Mais avant cela, essayons notre tout nouveau "),wt=r("em"),jl=n("tokenizer"),El=n(" sur l\u2019exemple pr\xE9c\xE9dent :"),ia=p(),m(He.$$.fragment),ua=p(),m(Ve.$$.fragment),pa=p(),q=r("p"),$l=n("Ici, nous voyons \xE0 nouveau les symboles sp\xE9ciaux "),Pt=r("code"),kl=n("\u0120"),bl=n(" et "),yt=r("code"),zl=n("\u010A"),wl=n(" qui indiquent les espaces et les retours \xE0 la ligne. Nous pouvons \xE9galement voir que notre "),Mt=r("em"),Pl=n("tokenizer"),yl=n(" a appris certains "),Ct=r("em"),Ml=n("tokens"),Cl=n(" qui sont tr\xE8s sp\xE9cifiques \xE0 un corpus de fonctions Python. Par exemple, il y a un token "),At=r("code"),Al=n("\u010A\u0120\u0120\u0120"),Dl=n(" qui repr\xE9sente une indentation et un "),Dt=r("em"),Ll=n("token"),Nl=p(),Lt=r("code"),Ol=n('\u0120"""'),Sl=n(" qui repr\xE9sente les trois guillemets qui commencent une "),Nt=r("em"),Tl=n("docstring"),Il=n(". Le "),Ot=r("em"),Gl=n("tokenizer"),Rl=n(" divise \xE9galement correctement le nom de la fonction sur "),St=r("code"),Ul=n("_"),Hl=n(". Il s\u2019agit d\u2019une repr\xE9sentation assez compacte. En comparaison, l\u2019utilisation du "),Tt=r("em"),Vl=n("tokenizer"),Fl=n(" en anglais \xAB simple \xBB sur le m\xEAme exemple nous donnera une phrase plus longue :"),ca=p(),m(Fe.$$.fragment),ma=p(),m(Ye.$$.fragment),da=p(),vs=r("p"),Yl=n("Prenons un autre exemple :"),fa=p(),m(Be.$$.fragment),ha=p(),m(Ke.$$.fragment),va=p(),_=r("p"),Bl=n("En plus du "),It=r("em"),Kl=n("token"),Xl=n(" correspondant \xE0 une indentation, on peut \xE9galement voir ici un "),Gt=r("em"),Jl=n("token"),Ql=n(" pour une double indentation : "),Rt=r("code"),Zl=n("\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),Wl=n(". Les mots sp\xE9ciaux de Python comme "),Ut=r("code"),ei=n("class"),si=n(", "),Ht=r("code"),ti=n("init"),ni=n(", "),Vt=r("code"),ai=n("call"),ri=n(", "),Ft=r("code"),oi=n("self"),li=n(", et "),Yt=r("code"),ii=n("return"),ui=n(" sont tous tokeniz\xE9s comme un seul "),Bt=r("em"),pi=n("token"),ci=n(". Nous pouvons voir qu\u2019en plus de s\xE9parer sur "),Kt=r("code"),mi=n("_"),di=n(" et "),Xt=r("code"),fi=n("."),hi=n(" le tokenizer s\xE9pare correctement m\xEAme les noms en minuscules. Par exemple "),Jt=r("code"),vi=n("LinearLayer"),xi=n(" est tokenis\xE9 comme "),Qt=r("code"),_i=n('["\u0120Linear", "Layer"]'),gi=n("."),xa=p(),le=r("h2"),fe=r("a"),Zt=r("span"),m(Xe.$$.fragment),qi=p(),xs=r("span"),ji=n("Sauvegarde du "),Wt=r("i"),Ei=n("tokenizer"),_a=p(),W=r("p"),$i=n("Pour \xEAtre s\xFBr de pouvoir l\u2019utiliser plus tard, nous devons sauvegarder notre nouveau "),en=r("em"),ki=n("tokenizer"),bi=n(". Comme pour les mod\xE8les, ceci est fait avec la m\xE9thode "),sn=r("code"),zi=n("save_pretrained()"),wi=n(" :"),ga=p(),m(Je.$$.fragment),qa=p(),A=r("p"),Pi=n("Cela cr\xE9era un nouveau dossier nomm\xE9 "),tn=r("em"),yi=n("code-search-net-tokenizer"),Mi=n(" contenant tous les fichiers dont le "),nn=r("em"),Ci=n("tokenizer"),Ai=n(" a besoin pour \xEAtre recharg\xE9. Si vous souhaitez partager ce "),an=r("em"),Di=n("tokenizer"),Li=n(" avec vos coll\xE8gues et amis, vous pouvez le t\xE9l\xE9charger sur le "),rn=r("em"),Ni=n("Hub"),Oi=n(" en vous connectant \xE0 votre compte. Si vous travaillez dans un "),on=r("em"),Si=n("notebook"),Ti=n(", il existe une fonction pratique pour vous aider \xE0 le faire :"),ja=p(),m(Qe.$$.fragment),Ea=p(),he=r("p"),Ii=n("Cela affichera un "),ln=r("em"),Gi=n("widget"),Ri=n(" o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face. Si vous ne travaillez pas sur un ordinateur portable, tapez simplement la ligne suivante dans votre terminal :"),$a=p(),m(Ze.$$.fragment),ka=p(),ve=r("p"),Ui=n("Une fois connect\xE9, vous pouvez pousser votre "),un=r("em"),Hi=n("tokenizer"),Vi=n(" en ex\xE9cutant la commande suivante :"),ba=p(),m(We.$$.fragment),za=p(),S=r("p"),Fi=n("Cela cr\xE9era un nouveau d\xE9p\xF4t dans votre espace avec le nom "),pn=r("code"),Yi=n("code-search-net-tokenizer"),Bi=n(" contenant le fichier "),cn=r("em"),Ki=n("tokenizer"),Xi=n(". Vous pouvez ensuite charger le "),mn=r("em"),Ji=n("tokenizer"),Qi=n(" de n\u2019importe o\xF9 avec la m\xE9thode "),dn=r("code"),Zi=n("from_pretrained()"),Wi=n(" :"),wa=p(),m(es.$$.fragment),Pa=p(),T=r("p"),eu=n("Vous \xEAtes maintenant pr\xEAt \xE0 entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro et \xE0 le "),fn=r("em"),su=n("finetuner"),tu=n(" sur votre t\xE2che ! Nous verrons cela dans le "),_s=r("a"),nu=n("chapitre 7"),au=n(", mais d\u2019abord, dans le reste de ce chapitre, nous allons examiner de plus pr\xE8s les "),hn=r("em"),ru=n("tokenizers"),ou=n(" rapides et explorer en d\xE9tail ce qui se passe lorsque nous appelons la m\xE9thode "),vn=r("code"),lu=n("train_new_from_iterator()"),iu=n("."),this.h()},l(e){const i=Pc('[data-svelte="svelte-1phssyn"]',document.head);z=o(i,"META",{name:!0,content:!0}),i.forEach(t),ie=c(e),L=o(e,"H1",{class:!0});var ss=l(L);R=o(ss,"A",{id:!0,class:!0,href:!0});var mu=l(R);ne=o(mu,"SPAN",{});var du=l(ne);d(G.$$.fragment,du),du.forEach(t),mu.forEach(t),ge=c(ss),Y=o(ss,"SPAN",{});var Ma=l(Y);ae=a(Ma,"Entra\xEEner un nouveau "),N=o(Ma,"I",{});var fu=l(N);ts=a(fu,"tokenizer"),fu.forEach(t),ns=a(Ma," \xE0 partir d'un ancien"),Ma.forEach(t),ss.forEach(t),_n=c(e),d(qe.$$.fragment,e),gn=c(e),$=o(e,"P",{});var P=l($);Ta=a(P,"Si un mod\xE8le de langue n\u2019est pas disponible dans la langue qui vous int\xE9resse ou si votre corpus est tr\xE8s diff\xE9rent de celui sur lequel votre mod\xE8le de langue a \xE9t\xE9 entra\xEEn\xE9, vous voudrez tr\xE8s probablement r\xE9entra\xEEner le mod\xE8le \xE0 partir de z\xE9ro en utilisant un "),zs=o(P,"EM",{});var hu=l(zs);Ia=a(hu,"tokenizer"),hu.forEach(t),Ga=a(P," adapt\xE9 \xE0 vos donn\xE9es. Pour ce faire, vous devrez entra\xEEner un nouveau "),ws=o(P,"EM",{});var vu=l(ws);Ra=a(vu,"tokenizer"),vu.forEach(t),Ua=a(P," sur votre jeu de donn\xE9es. Mais qu\u2019est-ce que cela signifie exactement ? Lorsque nous avons examin\xE9 pour la premi\xE8re fois les "),Ps=o(P,"EM",{});var xu=l(Ps);Ha=a(xu,"tokenizers"),xu.forEach(t),Va=a(P," dans le "),as=o(P,"A",{href:!0});var _u=l(as);Fa=a(_u,"chapitre 2"),_u.forEach(t),Ya=a(P,", nous avons vu que la plupart des "),ys=o(P,"EM",{});var gu=l(ys);Ba=a(gu,"transformers"),gu.forEach(t),Ka=a(P," utilisent un "),Ms=o(P,"EM",{});var qu=l(Ms);Xa=a(qu,"algorithme de tokenisation en sous-mots"),qu.forEach(t),Ja=a(P,". Pour identifier les sous-mots qui sont int\xE9ressants et qui apparaissent le plus fr\xE9quemment dans un corpus donn\xE9, le "),Cs=o(P,"EM",{});var ju=l(Cs);Qa=a(ju,"tokenizer"),ju.forEach(t),Za=a(P," doit examiner attentivement tous les textes du corpus. C\u2019est un processus que nous appelons "),As=o(P,"EM",{});var Eu=l(As);Wa=a(Eu,"entra\xEEnement"),Eu.forEach(t),er=a(P,". Les r\xE8gles exactes qui r\xE9gissent cet apprentissage d\xE9pendent du type de "),Ds=o(P,"EM",{});var $u=l(Ds);sr=a($u,"tokenizer"),$u.forEach(t),tr=a(P," utilis\xE9. Nous passerons en revue les trois principaux algorithmes plus loin dans ce chapitre."),P.forEach(t),qn=c(e),d(je.$$.fragment,e),jn=c(e),d(ue.$$.fragment,e),En=c(e),re=o(e,"H2",{class:!0});var Ca=l(re);pe=o(Ca,"A",{id:!0,class:!0,href:!0});var ku=l(pe);Ls=o(ku,"SPAN",{});var bu=l(Ls);d(Ee.$$.fragment,bu),bu.forEach(t),ku.forEach(t),nr=c(Ca),Ns=o(Ca,"SPAN",{});var zu=l(Ns);ar=a(zu,"Assemblage d'un corpus"),zu.forEach(t),Ca.forEach(t),$n=c(e),U=o(e,"P",{});var xe=l(U);rr=a(xe,"Il y a une API tr\xE8s simple dans \u{1F917} "),Os=o(xe,"EM",{});var wu=l(Os);or=a(wu,"Transformers"),wu.forEach(t),lr=a(xe," que vous pouvez utiliser pour entra\xEEner un nouveau "),Ss=o(xe,"EM",{});var Pu=l(Ss);ir=a(Pu,"tokenizer"),Pu.forEach(t),ur=a(xe," avec les m\xEAmes caract\xE9ristiques qu\u2019un d\xE9j\xE0 existant : "),Ts=o(xe,"CODE",{});var yu=l(Ts);pr=a(yu,"AutoTokenizer.train_new_from_iterator()"),yu.forEach(t),cr=a(xe,". Pour illustrer cela, disons que nous voulons entra\xEEner GPT-2 \xE0 partir de z\xE9ro mais dans une langue autre que l\u2019anglais. Notre premi\xE8re t\xE2che est de rassembler des batchs de donn\xE9es dans cette langue dans un corpus d\u2019entra\xEEnement. Pour avoir des exemples que tout le monde puisse comprendre, nous n\u2019utiliserons pas ici une langue comme le russe ou le chinois mais plut\xF4t une langue anglaise sp\xE9cialis\xE9e : le langage Python."),xe.forEach(t),kn=c(e),O=o(e,"P",{});var ee=l(O);mr=a(ee,"La biblioth\xE8que "),ce=o(ee,"A",{href:!0,rel:!0});var uu=l(ce);dr=a(uu,"\u{1F917} "),Is=o(uu,"EM",{});var Mu=l(Is);fr=a(Mu,"Datasets"),Mu.forEach(t),uu.forEach(t),hr=a(ee," peut nous aider \xE0 assembler un corpus de code source Python. Nous allons utiliser la fonction habituelle "),Gs=o(ee,"CODE",{});var Cu=l(Gs);vr=a(Cu,"load_dataset()"),Cu.forEach(t),xr=a(ee," pour t\xE9l\xE9charger et mettre en cache le jeu de donn\xE9es "),$e=o(ee,"A",{href:!0,rel:!0});var Au=l($e);_r=a(Au,"CodeSearchNet"),Au.forEach(t),gr=a(ee,". Ce jeu de donn\xE9es a \xE9t\xE9 cr\xE9\xE9 pour le "),ke=o(ee,"A",{href:!0,rel:!0});var Du=l(ke);qr=a(Du,"CodeSearchNet challenge"),Du.forEach(t),jr=a(ee," et contient des millions de fonctions provenant de biblioth\xE8ques open source sur GitHub dans plusieurs langages de programmation. Ici, nous allons charger la partie Python de ce jeu de donn\xE9es :"),ee.forEach(t),bn=c(e),d(be.$$.fragment,e),zn=c(e),rs=o(e,"P",{});var Lu=l(rs);Er=a(Lu,"Nous pouvons jeter un coup d\u2019\u0153il au jeu d\u2019entra\xEEnement pour voir quelles sont les colonnes auxquelles nous avons acc\xE8s :"),Lu.forEach(t),wn=c(e),d(ze.$$.fragment,e),Pn=c(e),d(we.$$.fragment,e),yn=c(e),B=o(e,"P",{});var gs=l(B);$r=a(gs,"Nous pouvons voir que le jeu de donn\xE9es s\xE9pare les cha\xEEnes de documents du code et sugg\xE8re une tokenization des deux. Ici, nous utiliserons simplement la colonne "),Rs=o(gs,"CODE",{});var Nu=l(Rs);kr=a(Nu,"whole_func_string"),Nu.forEach(t),br=a(gs," pour entra\xEEner notre "),Us=o(gs,"EM",{});var Ou=l(Us);zr=a(Ou,"tokenizer"),Ou.forEach(t),wr=a(gs,". Nous pouvons regarder un exemple de la fa\xE7on suivante :"),gs.forEach(t),Mn=c(e),d(Pe.$$.fragment,e),Cn=c(e),os=o(e,"P",{});var Su=l(os);Pr=a(Su,"qui nous affiche ce qui suit :"),Su.forEach(t),An=c(e),d(ye.$$.fragment,e),Dn=c(e),H=o(e,"P",{});var _e=l(H);yr=a(_e,"La premi\xE8re chose \xE0 faire est de transformer le jeu de donn\xE9es en un "),Hs=o(_e,"EM",{});var Tu=l(Hs);Mr=a(Tu,"it\xE9rateur"),Tu.forEach(t),Cr=a(_e," de listes de textes. Par exemple, une liste de listes de textes. L\u2019utilisation de listes de textes permet \xE0 notre "),Vs=o(_e,"EM",{});var Iu=l(Vs);Ar=a(Iu,"tokenizer"),Iu.forEach(t),Dr=a(_e," d\u2019aller plus vite (l\u2019entra\xEEnement a alors lieu sur des batchs de textes au lieu de traiter des textes un par un). Et le fait que ce soit un it\xE9rateur permet d\u2019\xE9viter d\u2019avoir tout en m\xE9moire en m\xEAme temps. Si votre corpus est \xE9norme, vous voudrez profiter du fait que \u{1F917} "),Fs=o(_e,"EM",{});var Gu=l(Fs);Lr=a(Gu,"Datasets"),Gu.forEach(t),Nr=a(_e," ne charge pas tout en RAM mais stocke les \xE9l\xE9ments du jeu de donn\xE9es sur le disque."),_e.forEach(t),Ln=c(e),ls=o(e,"P",{});var Ru=l(ls);Or=a(Ru,"Faire ce qui suit cr\xE9erait une liste de listes de 1 000 textes chacune mais chargerait tout en m\xE9moire :"),Ru.forEach(t),Nn=c(e),d(Me.$$.fragment,e),On=c(e),is=o(e,"P",{});var Uu=l(is);Sr=a(Uu,"En utilisant un g\xE9n\xE9rateur, nous pouvons \xE9viter que Python ne charge quoi que ce soit en m\xE9moire \xE0 moins que cela soit r\xE9ellement n\xE9cessaire. Pour cr\xE9er un tel g\xE9n\xE9rateur, il suffit de remplacer les crochets par des parenth\xE8ses :"),Uu.forEach(t),Sn=c(e),d(Ce.$$.fragment,e),Tn=c(e),K=o(e,"P",{});var qs=l(K);Tr=a(qs,"Cette ligne de code ne r\xE9cup\xE8re aucun \xE9l\xE9ment du jeu de donn\xE9es. Elle cr\xE9e simplement un objet que vous pouvez utiliser dans une boucle "),Ys=o(qs,"CODE",{});var Hu=l(Ys);Ir=a(Hu,"for"),Hu.forEach(t),Gr=a(qs," Python. Les textes ne seront charg\xE9s que lorsque vous en aurez besoin (c\u2019est-\xE0-dire lorsque vous serez \xE0 l\u2019\xE9tape de la boucle "),Bs=o(qs,"CODE",{});var Vu=l(Bs);Rr=a(Vu,"for"),Vu.forEach(t),Ur=a(qs," qui les requiert) et seulement 1 000 textes \xE0 la fois. De cette fa\xE7on, vous n\u2019\xE9puiserez pas toute votre m\xE9moire, m\xEAme si vous traitez un \xE9norme jeu de donn\xE9es."),qs.forEach(t),In=c(e),us=o(e,"P",{});var Fu=l(us);Hr=a(Fu,"Le probl\xE8me avec un objet g\xE9n\xE9rateur est qu\u2019il ne peut \xEAtre utilis\xE9 qu\u2019une seule fois. Ainsi, au lieu que cet objet nous donne deux fois la liste des 10 premiers chiffres :"),Fu.forEach(t),Gn=c(e),d(Ae.$$.fragment,e),Rn=c(e),ps=o(e,"P",{});var Yu=l(ps);Vr=a(Yu,"on les re\xE7oit une fois et ensuite une liste vide :"),Yu.forEach(t),Un=c(e),d(De.$$.fragment,e),Hn=c(e),cs=o(e,"P",{});var Bu=l(cs);Fr=a(Bu,"C\u2019est pourquoi nous d\xE9finissons une fonction qui renvoie un g\xE9n\xE9rateur \xE0 la place :"),Bu.forEach(t),Vn=c(e),d(Le.$$.fragment,e),Fn=c(e),X=o(e,"P",{});var js=l(X);Yr=a(js,"Vous pouvez \xE9galement d\xE9finir votre g\xE9n\xE9rateur \xE0 l\u2019int\xE9rieur d\u2019une boucle "),Ks=o(js,"CODE",{});var Ku=l(Ks);Br=a(Ku,"for"),Ku.forEach(t),Kr=a(js," en utilisant l\u2019instruction "),Xs=o(js,"CODE",{});var Xu=l(Xs);Xr=a(Xu,"yield"),Xu.forEach(t),Jr=a(js," :"),js.forEach(t),Yn=c(e),d(Ne.$$.fragment,e),Bn=c(e),ms=o(e,"P",{});var Ju=l(ms);Qr=a(Ju,"qui produit exactement le m\xEAme g\xE9n\xE9rateur que pr\xE9c\xE9demment mais  permet d\u2019utiliser une logique plus complexe que celle que vous pouvez utiliser dans une compr\xE9hension de liste."),Ju.forEach(t),Kn=c(e),oe=o(e,"H2",{class:!0});var Aa=l(oe);me=o(Aa,"A",{id:!0,class:!0,href:!0});var Qu=l(me);Js=o(Qu,"SPAN",{});var Zu=l(Js);d(Oe.$$.fragment,Zu),Zu.forEach(t),Qu.forEach(t),Zr=c(Aa),ds=o(Aa,"SPAN",{});var pu=l(ds);Wr=a(pu,"Entra\xEEnement d'un nouveau "),Qs=o(pu,"I",{});var Wu=l(Qs);eo=a(Wu,"tokenizer"),Wu.forEach(t),pu.forEach(t),Aa.forEach(t),Xn=c(e),J=o(e,"P",{});var Es=l(J);so=a(Es,"Maintenant que nous avons notre corpus sous la forme d\u2019un it\xE9rateur de batchs de textes, nous sommes pr\xEAts \xE0 entra\xEEner un nouveau "),Zs=o(Es,"EM",{});var ep=l(Zs);to=a(ep,"tokenizer"),ep.forEach(t),no=a(Es,". Pour ce faire, nous devons d\u2019abord charger le "),Ws=o(Es,"EM",{});var sp=l(Ws);ao=a(sp,"tokenizer"),sp.forEach(t),ro=a(Es," que nous voulons coupler avec notre mod\xE8le (ici, le GPT-2) :"),Es.forEach(t),Jn=c(e),d(Se.$$.fragment,e),Qn=c(e),Q=o(e,"P",{});var $s=l(Q);oo=a($s,"M\xEAme si nous allons entra\xEEner un nouveau "),et=o($s,"EM",{});var tp=l(et);lo=a(tp,"tokenizer"),tp.forEach(t),io=a($s,", c\u2019est une bonne id\xE9e de faire \xE7a pour \xE9viter de partir enti\xE8rement de z\xE9ro. De cette fa\xE7on, nous n\u2019aurons pas \xE0 sp\xE9cifier l\u2019algorithme de tok\xE9nisation ou les jetons sp\xE9ciaux que nous voulons utiliser. Notre nouveau "),st=o($s,"EM",{});var np=l(st);uo=a(np,"tokenizer"),np.forEach(t),po=a($s," sera exactement le m\xEAme que celui du GPT-2. La seule chose qui changera sera le vocabulaire qui sera d\xE9termin\xE9 lors de l\u2019entra\xEEnement sur notre corpus."),$s.forEach(t),Zn=c(e),de=o(e,"P",{});var Da=l(de);co=a(Da,"Voyons d\u2019abord comment ce "),tt=o(Da,"EM",{});var ap=l(tt);mo=a(ap,"tokenizer"),ap.forEach(t),fo=a(Da," traiterait un exemple de fonction :"),Da.forEach(t),Wn=c(e),d(Te.$$.fragment,e),ea=c(e),d(Ie.$$.fragment,e),sa=c(e),C=o(e,"P",{});var V=l(C);ho=a(V,"Ce "),nt=o(V,"EM",{});var rp=l(nt);vo=a(rp,"tokenizer"),rp.forEach(t),xo=a(V," poss\xE8de quelques symboles sp\xE9ciaux, comme "),at=o(V,"CODE",{});var op=l(at);_o=a(op,"\u0120"),op.forEach(t),go=a(V," et "),rt=o(V,"CODE",{});var lp=l(rt);qo=a(lp,"\u010A"),lp.forEach(t),jo=a(V,", qui d\xE9signent respectivement les espaces et les retours \xE0 la ligne. Comme on peut le voir, ce n\u2019est pas tr\xE8s efficace. Le "),ot=o(V,"EM",{});var ip=l(ot);Eo=a(ip,"tokenizer"),ip.forEach(t),$o=a(V," renvoie des jetons individuels pour chaque espace alors qu\u2019il pourrait regrouper ceux des indentations (puisqu\u2019avoir des ensembles de quatre ou huit espaces est tr\xE8s courant dans du code). Il divise \xE9galement le nom de la fonction de fa\xE7on un peu bizarre car pas habitu\xE9 \xE0 voir des mots avec le caract\xE8re "),lt=o(V,"CODE",{});var up=l(lt);ko=a(up,"_"),up.forEach(t),bo=a(V,"."),V.forEach(t),ta=c(e),Z=o(e,"P",{});var ks=l(Z);zo=a(ks,"Entra\xEEnons un nouveau "),it=o(ks,"EM",{});var pp=l(it);wo=a(pp,"tokenizer"),pp.forEach(t),Po=a(ks," et voyons s\u2019il r\xE9sout ces probl\xE8mes. Pour cela, nous allons utiliser la m\xE9thode "),ut=o(ks,"CODE",{});var cp=l(ut);yo=a(cp,"train_new_from_iterator()"),cp.forEach(t),Mo=a(ks," :"),ks.forEach(t),na=c(e),d(Ge.$$.fragment,e),aa=c(e),fs=o(e,"P",{});var mp=l(fs);Co=a(mp,"Cette commande peut prendre un peu de temps si votre corpus est tr\xE8s grand. Pour ce jeu de donn\xE9es de 1,6 Go de textes, elle est tr\xE8s rapide (1 minute 16 secondes sur un CPU AMD Ryzen 9 3900X avec 12 c\u0153urs)."),mp.forEach(t),ra=c(e),w=o(e,"P",{});var D=l(w);Ao=a(D,"Notez que "),pt=o(D,"CODE",{});var dp=l(pt);Do=a(dp,"AutoTokenizer.train_new_from_iterator()"),dp.forEach(t),Lo=a(D," ne fonctionne que si le "),ct=o(D,"EM",{});var fp=l(ct);No=a(fp,"tokenizer"),fp.forEach(t),Oo=a(D," que vous utilisez est un "),mt=o(D,"EM",{});var hp=l(mt);So=a(hp,"tokenizer"),hp.forEach(t),To=a(D," \xAB rapide \xBB. Comme vous le verrez dans la section suivante, la biblioth\xE8que \u{1F917} "),dt=o(D,"EM",{});var vp=l(dt);Io=a(vp,"Transformers"),vp.forEach(t),Go=a(D," contient deux types de "),ft=o(D,"EM",{});var xp=l(ft);Ro=a(xp,"tokenizers"),xp.forEach(t),Uo=a(D," : certains sont \xE9crits en pur Python et d\u2019autres (les rapides) sont soutenus par la biblioth\xE8que \u{1F917} "),ht=o(D,"EM",{});var _p=l(ht);Ho=a(_p,"Tokenizers"),_p.forEach(t),Vo=a(D," qui est \xE9crite dans le langage "),Re=o(D,"A",{href:!0,rel:!0});var gp=l(Re);Fo=a(gp,"Rust"),gp.forEach(t),Yo=a(D,". Python est le langage le plus souvent utilis\xE9 pour les applications de science des donn\xE9es et d\u2019apprentissage profond, mais lorsque quelque chose doit \xEAtre parall\xE9lis\xE9 pour \xEAtre rapide, il faut que cela soit \xE9crit dans un autre langage. Par exemple, les multiplications matricielles qui sont au c\u0153ur du calcul du mod\xE8le sont \xE9crites en CUDA, une biblioth\xE8que en C optimis\xE9e pour les GPUs."),D.forEach(t),oa=c(e),y=o(e,"P",{});var I=l(y);Bo=a(I,"Entra\xEEner un tout nouveau "),vt=o(I,"EM",{});var qp=l(vt);Ko=a(qp,"tokenizer"),qp.forEach(t),Xo=a(I," en Python pur est atrocement lent, c\u2019est pourquoi nous avons d\xE9velopp\xE9 la biblioth\xE8que \u{1F917} "),xt=o(I,"EM",{});var jp=l(xt);Jo=a(jp,"Tokenizers"),jp.forEach(t),Qo=a(I,". Notez que, tout comme vous n\u2019avez pas eu \xE0 apprendre le langage CUDA pour pouvoir ex\xE9cuter votre mod\xE8le sur un batch d\u2019entr\xE9es sur un GPU, vous n\u2019aurez pas besoin d\u2019apprendre Rust pour utiliser un "),_t=o(I,"EM",{});var Ep=l(_t);Zo=a(Ep,"tokenizer"),Ep.forEach(t),Wo=a(I," rapide. La biblioth\xE8que \u{1F917} "),gt=o(I,"EM",{});var $p=l(gt);el=a($p,"Tokenizers"),$p.forEach(t),sl=a(I," fournit des liaisons Python pour de nombreuses m\xE9thodes qui appellent en interne un morceau de code en Rust. Par exemple, pour parall\xE9liser l\u2019entra\xEEnement de votre nouveau "),qt=o(I,"EM",{});var kp=l(qt);tl=a(kp,"tokenizer"),kp.forEach(t),nl=a(I," ou, comme nous l\u2019avons vu dans le "),hs=o(I,"A",{href:!0});var bp=l(hs);al=a(bp,"chapitre 3"),bp.forEach(t),rl=a(I,", la tokenisation d\u2019un lot d\u2019entr\xE9es."),I.forEach(t),la=c(e),b=o(e,"P",{});var M=l(b);ol=a(M,"La plupart des "),jt=o(M,"EM",{});var zp=l(jt);ll=a(zp,"transformers"),zp.forEach(t),il=a(M," ont un "),Et=o(M,"EM",{});var wp=l(Et);ul=a(wp,"tokenizer"),wp.forEach(t),pl=a(M," rapide de disponible. Il y a quelques exceptions que vous pouvez v\xE9rifier "),Ue=o(M,"A",{href:!0,rel:!0});var Pp=l(Ue);cl=a(Pp,"ici"),Pp.forEach(t),ml=a(M,". S\u2019il est disponible, l\u2019API "),$t=o(M,"CODE",{});var yp=l($t);dl=a(yp,"AutoTokenizer"),yp.forEach(t),fl=a(M," s\xE9lectionne toujours pour vous le "),kt=o(M,"EM",{});var Mp=l(kt);hl=a(Mp,"tokenizer"),Mp.forEach(t),vl=a(M," rapide. Dans la prochaine section, nous allons jeter un coup d\u2019oeil \xE0 certaines des autres caract\xE9ristiques sp\xE9ciales des "),bt=o(M,"EM",{});var Cp=l(bt);xl=a(Cp,"tokenizers"),Cp.forEach(t),_l=a(M," rapides, qui seront tr\xE8s utiles pour des t\xE2ches comme la classification de "),zt=o(M,"EM",{});var Ap=l(zt);gl=a(Ap,"tokens"),Ap.forEach(t),ql=a(M," et la r\xE9ponse aux questions. Mais avant cela, essayons notre tout nouveau "),wt=o(M,"EM",{});var Dp=l(wt);jl=a(Dp,"tokenizer"),Dp.forEach(t),El=a(M," sur l\u2019exemple pr\xE9c\xE9dent :"),M.forEach(t),ia=c(e),d(He.$$.fragment,e),ua=c(e),d(Ve.$$.fragment,e),pa=c(e),q=o(e,"P",{});var k=l(q);$l=a(k,"Ici, nous voyons \xE0 nouveau les symboles sp\xE9ciaux "),Pt=o(k,"CODE",{});var Lp=l(Pt);kl=a(Lp,"\u0120"),Lp.forEach(t),bl=a(k," et "),yt=o(k,"CODE",{});var Np=l(yt);zl=a(Np,"\u010A"),Np.forEach(t),wl=a(k," qui indiquent les espaces et les retours \xE0 la ligne. Nous pouvons \xE9galement voir que notre "),Mt=o(k,"EM",{});var Op=l(Mt);Pl=a(Op,"tokenizer"),Op.forEach(t),yl=a(k," a appris certains "),Ct=o(k,"EM",{});var Sp=l(Ct);Ml=a(Sp,"tokens"),Sp.forEach(t),Cl=a(k," qui sont tr\xE8s sp\xE9cifiques \xE0 un corpus de fonctions Python. Par exemple, il y a un token "),At=o(k,"CODE",{});var Tp=l(At);Al=a(Tp,"\u010A\u0120\u0120\u0120"),Tp.forEach(t),Dl=a(k," qui repr\xE9sente une indentation et un "),Dt=o(k,"EM",{});var Ip=l(Dt);Ll=a(Ip,"token"),Ip.forEach(t),Nl=c(k),Lt=o(k,"CODE",{});var Gp=l(Lt);Ol=a(Gp,'\u0120"""'),Gp.forEach(t),Sl=a(k," qui repr\xE9sente les trois guillemets qui commencent une "),Nt=o(k,"EM",{});var Rp=l(Nt);Tl=a(Rp,"docstring"),Rp.forEach(t),Il=a(k,". Le "),Ot=o(k,"EM",{});var Up=l(Ot);Gl=a(Up,"tokenizer"),Up.forEach(t),Rl=a(k," divise \xE9galement correctement le nom de la fonction sur "),St=o(k,"CODE",{});var Hp=l(St);Ul=a(Hp,"_"),Hp.forEach(t),Hl=a(k,". Il s\u2019agit d\u2019une repr\xE9sentation assez compacte. En comparaison, l\u2019utilisation du "),Tt=o(k,"EM",{});var Vp=l(Tt);Vl=a(Vp,"tokenizer"),Vp.forEach(t),Fl=a(k," en anglais \xAB simple \xBB sur le m\xEAme exemple nous donnera une phrase plus longue :"),k.forEach(t),ca=c(e),d(Fe.$$.fragment,e),ma=c(e),d(Ye.$$.fragment,e),da=c(e),vs=o(e,"P",{});var Fp=l(vs);Yl=a(Fp,"Prenons un autre exemple :"),Fp.forEach(t),fa=c(e),d(Be.$$.fragment,e),ha=c(e),d(Ke.$$.fragment,e),va=c(e),_=o(e,"P",{});var j=l(_);Bl=a(j,"En plus du "),It=o(j,"EM",{});var Yp=l(It);Kl=a(Yp,"token"),Yp.forEach(t),Xl=a(j," correspondant \xE0 une indentation, on peut \xE9galement voir ici un "),Gt=o(j,"EM",{});var Bp=l(Gt);Jl=a(Bp,"token"),Bp.forEach(t),Ql=a(j," pour une double indentation : "),Rt=o(j,"CODE",{});var Kp=l(Rt);Zl=a(Kp,"\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),Kp.forEach(t),Wl=a(j,". Les mots sp\xE9ciaux de Python comme "),Ut=o(j,"CODE",{});var Xp=l(Ut);ei=a(Xp,"class"),Xp.forEach(t),si=a(j,", "),Ht=o(j,"CODE",{});var Jp=l(Ht);ti=a(Jp,"init"),Jp.forEach(t),ni=a(j,", "),Vt=o(j,"CODE",{});var Qp=l(Vt);ai=a(Qp,"call"),Qp.forEach(t),ri=a(j,", "),Ft=o(j,"CODE",{});var Zp=l(Ft);oi=a(Zp,"self"),Zp.forEach(t),li=a(j,", et "),Yt=o(j,"CODE",{});var Wp=l(Yt);ii=a(Wp,"return"),Wp.forEach(t),ui=a(j," sont tous tokeniz\xE9s comme un seul "),Bt=o(j,"EM",{});var ec=l(Bt);pi=a(ec,"token"),ec.forEach(t),ci=a(j,". Nous pouvons voir qu\u2019en plus de s\xE9parer sur "),Kt=o(j,"CODE",{});var sc=l(Kt);mi=a(sc,"_"),sc.forEach(t),di=a(j," et "),Xt=o(j,"CODE",{});var tc=l(Xt);fi=a(tc,"."),tc.forEach(t),hi=a(j," le tokenizer s\xE9pare correctement m\xEAme les noms en minuscules. Par exemple "),Jt=o(j,"CODE",{});var nc=l(Jt);vi=a(nc,"LinearLayer"),nc.forEach(t),xi=a(j," est tokenis\xE9 comme "),Qt=o(j,"CODE",{});var ac=l(Qt);_i=a(ac,'["\u0120Linear", "Layer"]'),ac.forEach(t),gi=a(j,"."),j.forEach(t),xa=c(e),le=o(e,"H2",{class:!0});var La=l(le);fe=o(La,"A",{id:!0,class:!0,href:!0});var rc=l(fe);Zt=o(rc,"SPAN",{});var oc=l(Zt);d(Xe.$$.fragment,oc),oc.forEach(t),rc.forEach(t),qi=c(La),xs=o(La,"SPAN",{});var cu=l(xs);ji=a(cu,"Sauvegarde du "),Wt=o(cu,"I",{});var lc=l(Wt);Ei=a(lc,"tokenizer"),lc.forEach(t),cu.forEach(t),La.forEach(t),_a=c(e),W=o(e,"P",{});var bs=l(W);$i=a(bs,"Pour \xEAtre s\xFBr de pouvoir l\u2019utiliser plus tard, nous devons sauvegarder notre nouveau "),en=o(bs,"EM",{});var ic=l(en);ki=a(ic,"tokenizer"),ic.forEach(t),bi=a(bs,". Comme pour les mod\xE8les, ceci est fait avec la m\xE9thode "),sn=o(bs,"CODE",{});var uc=l(sn);zi=a(uc,"save_pretrained()"),uc.forEach(t),wi=a(bs," :"),bs.forEach(t),ga=c(e),d(Je.$$.fragment,e),qa=c(e),A=o(e,"P",{});var F=l(A);Pi=a(F,"Cela cr\xE9era un nouveau dossier nomm\xE9 "),tn=o(F,"EM",{});var pc=l(tn);yi=a(pc,"code-search-net-tokenizer"),pc.forEach(t),Mi=a(F," contenant tous les fichiers dont le "),nn=o(F,"EM",{});var cc=l(nn);Ci=a(cc,"tokenizer"),cc.forEach(t),Ai=a(F," a besoin pour \xEAtre recharg\xE9. Si vous souhaitez partager ce "),an=o(F,"EM",{});var mc=l(an);Di=a(mc,"tokenizer"),mc.forEach(t),Li=a(F," avec vos coll\xE8gues et amis, vous pouvez le t\xE9l\xE9charger sur le "),rn=o(F,"EM",{});var dc=l(rn);Ni=a(dc,"Hub"),dc.forEach(t),Oi=a(F," en vous connectant \xE0 votre compte. Si vous travaillez dans un "),on=o(F,"EM",{});var fc=l(on);Si=a(fc,"notebook"),fc.forEach(t),Ti=a(F,", il existe une fonction pratique pour vous aider \xE0 le faire :"),F.forEach(t),ja=c(e),d(Qe.$$.fragment,e),Ea=c(e),he=o(e,"P",{});var Na=l(he);Ii=a(Na,"Cela affichera un "),ln=o(Na,"EM",{});var hc=l(ln);Gi=a(hc,"widget"),hc.forEach(t),Ri=a(Na," o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face. Si vous ne travaillez pas sur un ordinateur portable, tapez simplement la ligne suivante dans votre terminal :"),Na.forEach(t),$a=c(e),d(Ze.$$.fragment,e),ka=c(e),ve=o(e,"P",{});var Oa=l(ve);Ui=a(Oa,"Une fois connect\xE9, vous pouvez pousser votre "),un=o(Oa,"EM",{});var vc=l(un);Hi=a(vc,"tokenizer"),vc.forEach(t),Vi=a(Oa," en ex\xE9cutant la commande suivante :"),Oa.forEach(t),ba=c(e),d(We.$$.fragment,e),za=c(e),S=o(e,"P",{});var se=l(S);Fi=a(se,"Cela cr\xE9era un nouveau d\xE9p\xF4t dans votre espace avec le nom "),pn=o(se,"CODE",{});var xc=l(pn);Yi=a(xc,"code-search-net-tokenizer"),xc.forEach(t),Bi=a(se," contenant le fichier "),cn=o(se,"EM",{});var _c=l(cn);Ki=a(_c,"tokenizer"),_c.forEach(t),Xi=a(se,". Vous pouvez ensuite charger le "),mn=o(se,"EM",{});var gc=l(mn);Ji=a(gc,"tokenizer"),gc.forEach(t),Qi=a(se," de n\u2019importe o\xF9 avec la m\xE9thode "),dn=o(se,"CODE",{});var qc=l(dn);Zi=a(qc,"from_pretrained()"),qc.forEach(t),Wi=a(se," :"),se.forEach(t),wa=c(e),d(es.$$.fragment,e),Pa=c(e),T=o(e,"P",{});var te=l(T);eu=a(te,"Vous \xEAtes maintenant pr\xEAt \xE0 entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro et \xE0 le "),fn=o(te,"EM",{});var jc=l(fn);su=a(jc,"finetuner"),jc.forEach(t),tu=a(te," sur votre t\xE2che ! Nous verrons cela dans le "),_s=o(te,"A",{href:!0});var Ec=l(_s);nu=a(Ec,"chapitre 7"),Ec.forEach(t),au=a(te,", mais d\u2019abord, dans le reste de ce chapitre, nous allons examiner de plus pr\xE8s les "),hn=o(te,"EM",{});var $c=l(hn);ru=a($c,"tokenizers"),$c.forEach(t),ou=a(te," rapides et explorer en d\xE9tail ce qui se passe lorsque nous appelons la m\xE9thode "),vn=o(te,"CODE",{});var kc=l(vn);lu=a(kc,"train_new_from_iterator()"),kc.forEach(t),iu=a(te,"."),te.forEach(t),this.h()},h(){g(z,"name","hf:doc:metadata"),g(z,"content",JSON.stringify(Nc)),g(R,"id","entraner-un-nouveau-itokenizeri-partir-dun-ancien"),g(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(R,"href","#entraner-un-nouveau-itokenizeri-partir-dun-ancien"),g(L,"class","relative group"),g(as,"href","/course/fr/chapter2"),g(pe,"id","assemblage-dun-corpus"),g(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(pe,"href","#assemblage-dun-corpus"),g(re,"class","relative group"),g(ce,"href","https://github.com/huggingface/datasets"),g(ce,"rel","nofollow"),g($e,"href","https://huggingface.co/datasets/code_search_net"),g($e,"rel","nofollow"),g(ke,"href","https://wandb.ai/github/CodeSearchNet/benchmark"),g(ke,"rel","nofollow"),g(me,"id","entranement-dun-nouveau-itokenizeri"),g(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(me,"href","#entranement-dun-nouveau-itokenizeri"),g(oe,"class","relative group"),g(Re,"href","https://www.rust-lang.org"),g(Re,"rel","nofollow"),g(hs,"href","/course/fr/chapter3"),g(Ue,"href","https://huggingface.co/transformers/#supported-frameworks"),g(Ue,"rel","nofollow"),g(fe,"id","sauvegarde-du-itokenizeri"),g(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(fe,"href","#sauvegarde-du-itokenizeri"),g(le,"class","relative group"),g(_s,"href","/course/fr/chapter7")},m(e,i){s(document.head,z),u(e,ie,i),u(e,L,i),s(L,R),s(R,ne),f(G,ne,null),s(L,ge),s(L,Y),s(Y,ae),s(Y,N),s(N,ts),s(Y,ns),u(e,_n,i),f(qe,e,i),u(e,gn,i),u(e,$,i),s($,Ta),s($,zs),s(zs,Ia),s($,Ga),s($,ws),s(ws,Ra),s($,Ua),s($,Ps),s(Ps,Ha),s($,Va),s($,as),s(as,Fa),s($,Ya),s($,ys),s(ys,Ba),s($,Ka),s($,Ms),s(Ms,Xa),s($,Ja),s($,Cs),s(Cs,Qa),s($,Za),s($,As),s(As,Wa),s($,er),s($,Ds),s(Ds,sr),s($,tr),u(e,qn,i),f(je,e,i),u(e,jn,i),f(ue,e,i),u(e,En,i),u(e,re,i),s(re,pe),s(pe,Ls),f(Ee,Ls,null),s(re,nr),s(re,Ns),s(Ns,ar),u(e,$n,i),u(e,U,i),s(U,rr),s(U,Os),s(Os,or),s(U,lr),s(U,Ss),s(Ss,ir),s(U,ur),s(U,Ts),s(Ts,pr),s(U,cr),u(e,kn,i),u(e,O,i),s(O,mr),s(O,ce),s(ce,dr),s(ce,Is),s(Is,fr),s(O,hr),s(O,Gs),s(Gs,vr),s(O,xr),s(O,$e),s($e,_r),s(O,gr),s(O,ke),s(ke,qr),s(O,jr),u(e,bn,i),f(be,e,i),u(e,zn,i),u(e,rs,i),s(rs,Er),u(e,wn,i),f(ze,e,i),u(e,Pn,i),f(we,e,i),u(e,yn,i),u(e,B,i),s(B,$r),s(B,Rs),s(Rs,kr),s(B,br),s(B,Us),s(Us,zr),s(B,wr),u(e,Mn,i),f(Pe,e,i),u(e,Cn,i),u(e,os,i),s(os,Pr),u(e,An,i),f(ye,e,i),u(e,Dn,i),u(e,H,i),s(H,yr),s(H,Hs),s(Hs,Mr),s(H,Cr),s(H,Vs),s(Vs,Ar),s(H,Dr),s(H,Fs),s(Fs,Lr),s(H,Nr),u(e,Ln,i),u(e,ls,i),s(ls,Or),u(e,Nn,i),f(Me,e,i),u(e,On,i),u(e,is,i),s(is,Sr),u(e,Sn,i),f(Ce,e,i),u(e,Tn,i),u(e,K,i),s(K,Tr),s(K,Ys),s(Ys,Ir),s(K,Gr),s(K,Bs),s(Bs,Rr),s(K,Ur),u(e,In,i),u(e,us,i),s(us,Hr),u(e,Gn,i),f(Ae,e,i),u(e,Rn,i),u(e,ps,i),s(ps,Vr),u(e,Un,i),f(De,e,i),u(e,Hn,i),u(e,cs,i),s(cs,Fr),u(e,Vn,i),f(Le,e,i),u(e,Fn,i),u(e,X,i),s(X,Yr),s(X,Ks),s(Ks,Br),s(X,Kr),s(X,Xs),s(Xs,Xr),s(X,Jr),u(e,Yn,i),f(Ne,e,i),u(e,Bn,i),u(e,ms,i),s(ms,Qr),u(e,Kn,i),u(e,oe,i),s(oe,me),s(me,Js),f(Oe,Js,null),s(oe,Zr),s(oe,ds),s(ds,Wr),s(ds,Qs),s(Qs,eo),u(e,Xn,i),u(e,J,i),s(J,so),s(J,Zs),s(Zs,to),s(J,no),s(J,Ws),s(Ws,ao),s(J,ro),u(e,Jn,i),f(Se,e,i),u(e,Qn,i),u(e,Q,i),s(Q,oo),s(Q,et),s(et,lo),s(Q,io),s(Q,st),s(st,uo),s(Q,po),u(e,Zn,i),u(e,de,i),s(de,co),s(de,tt),s(tt,mo),s(de,fo),u(e,Wn,i),f(Te,e,i),u(e,ea,i),f(Ie,e,i),u(e,sa,i),u(e,C,i),s(C,ho),s(C,nt),s(nt,vo),s(C,xo),s(C,at),s(at,_o),s(C,go),s(C,rt),s(rt,qo),s(C,jo),s(C,ot),s(ot,Eo),s(C,$o),s(C,lt),s(lt,ko),s(C,bo),u(e,ta,i),u(e,Z,i),s(Z,zo),s(Z,it),s(it,wo),s(Z,Po),s(Z,ut),s(ut,yo),s(Z,Mo),u(e,na,i),f(Ge,e,i),u(e,aa,i),u(e,fs,i),s(fs,Co),u(e,ra,i),u(e,w,i),s(w,Ao),s(w,pt),s(pt,Do),s(w,Lo),s(w,ct),s(ct,No),s(w,Oo),s(w,mt),s(mt,So),s(w,To),s(w,dt),s(dt,Io),s(w,Go),s(w,ft),s(ft,Ro),s(w,Uo),s(w,ht),s(ht,Ho),s(w,Vo),s(w,Re),s(Re,Fo),s(w,Yo),u(e,oa,i),u(e,y,i),s(y,Bo),s(y,vt),s(vt,Ko),s(y,Xo),s(y,xt),s(xt,Jo),s(y,Qo),s(y,_t),s(_t,Zo),s(y,Wo),s(y,gt),s(gt,el),s(y,sl),s(y,qt),s(qt,tl),s(y,nl),s(y,hs),s(hs,al),s(y,rl),u(e,la,i),u(e,b,i),s(b,ol),s(b,jt),s(jt,ll),s(b,il),s(b,Et),s(Et,ul),s(b,pl),s(b,Ue),s(Ue,cl),s(b,ml),s(b,$t),s($t,dl),s(b,fl),s(b,kt),s(kt,hl),s(b,vl),s(b,bt),s(bt,xl),s(b,_l),s(b,zt),s(zt,gl),s(b,ql),s(b,wt),s(wt,jl),s(b,El),u(e,ia,i),f(He,e,i),u(e,ua,i),f(Ve,e,i),u(e,pa,i),u(e,q,i),s(q,$l),s(q,Pt),s(Pt,kl),s(q,bl),s(q,yt),s(yt,zl),s(q,wl),s(q,Mt),s(Mt,Pl),s(q,yl),s(q,Ct),s(Ct,Ml),s(q,Cl),s(q,At),s(At,Al),s(q,Dl),s(q,Dt),s(Dt,Ll),s(q,Nl),s(q,Lt),s(Lt,Ol),s(q,Sl),s(q,Nt),s(Nt,Tl),s(q,Il),s(q,Ot),s(Ot,Gl),s(q,Rl),s(q,St),s(St,Ul),s(q,Hl),s(q,Tt),s(Tt,Vl),s(q,Fl),u(e,ca,i),f(Fe,e,i),u(e,ma,i),f(Ye,e,i),u(e,da,i),u(e,vs,i),s(vs,Yl),u(e,fa,i),f(Be,e,i),u(e,ha,i),f(Ke,e,i),u(e,va,i),u(e,_,i),s(_,Bl),s(_,It),s(It,Kl),s(_,Xl),s(_,Gt),s(Gt,Jl),s(_,Ql),s(_,Rt),s(Rt,Zl),s(_,Wl),s(_,Ut),s(Ut,ei),s(_,si),s(_,Ht),s(Ht,ti),s(_,ni),s(_,Vt),s(Vt,ai),s(_,ri),s(_,Ft),s(Ft,oi),s(_,li),s(_,Yt),s(Yt,ii),s(_,ui),s(_,Bt),s(Bt,pi),s(_,ci),s(_,Kt),s(Kt,mi),s(_,di),s(_,Xt),s(Xt,fi),s(_,hi),s(_,Jt),s(Jt,vi),s(_,xi),s(_,Qt),s(Qt,_i),s(_,gi),u(e,xa,i),u(e,le,i),s(le,fe),s(fe,Zt),f(Xe,Zt,null),s(le,qi),s(le,xs),s(xs,ji),s(xs,Wt),s(Wt,Ei),u(e,_a,i),u(e,W,i),s(W,$i),s(W,en),s(en,ki),s(W,bi),s(W,sn),s(sn,zi),s(W,wi),u(e,ga,i),f(Je,e,i),u(e,qa,i),u(e,A,i),s(A,Pi),s(A,tn),s(tn,yi),s(A,Mi),s(A,nn),s(nn,Ci),s(A,Ai),s(A,an),s(an,Di),s(A,Li),s(A,rn),s(rn,Ni),s(A,Oi),s(A,on),s(on,Si),s(A,Ti),u(e,ja,i),f(Qe,e,i),u(e,Ea,i),u(e,he,i),s(he,Ii),s(he,ln),s(ln,Gi),s(he,Ri),u(e,$a,i),f(Ze,e,i),u(e,ka,i),u(e,ve,i),s(ve,Ui),s(ve,un),s(un,Hi),s(ve,Vi),u(e,ba,i),f(We,e,i),u(e,za,i),u(e,S,i),s(S,Fi),s(S,pn),s(pn,Yi),s(S,Bi),s(S,cn),s(cn,Ki),s(S,Xi),s(S,mn),s(mn,Ji),s(S,Qi),s(S,dn),s(dn,Zi),s(S,Wi),u(e,wa,i),f(es,e,i),u(e,Pa,i),u(e,T,i),s(T,eu),s(T,fn),s(fn,su),s(T,tu),s(T,_s),s(_s,nu),s(T,au),s(T,hn),s(hn,ru),s(T,ou),s(T,vn),s(vn,lu),s(T,iu),ya=!0},p(e,[i]){const ss={};i&2&&(ss.$$scope={dirty:i,ctx:e}),ue.$set(ss)},i(e){ya||(h(G.$$.fragment,e),h(qe.$$.fragment,e),h(je.$$.fragment,e),h(ue.$$.fragment,e),h(Ee.$$.fragment,e),h(be.$$.fragment,e),h(ze.$$.fragment,e),h(we.$$.fragment,e),h(Pe.$$.fragment,e),h(ye.$$.fragment,e),h(Me.$$.fragment,e),h(Ce.$$.fragment,e),h(Ae.$$.fragment,e),h(De.$$.fragment,e),h(Le.$$.fragment,e),h(Ne.$$.fragment,e),h(Oe.$$.fragment,e),h(Se.$$.fragment,e),h(Te.$$.fragment,e),h(Ie.$$.fragment,e),h(Ge.$$.fragment,e),h(He.$$.fragment,e),h(Ve.$$.fragment,e),h(Fe.$$.fragment,e),h(Ye.$$.fragment,e),h(Be.$$.fragment,e),h(Ke.$$.fragment,e),h(Xe.$$.fragment,e),h(Je.$$.fragment,e),h(Qe.$$.fragment,e),h(Ze.$$.fragment,e),h(We.$$.fragment,e),h(es.$$.fragment,e),ya=!0)},o(e){v(G.$$.fragment,e),v(qe.$$.fragment,e),v(je.$$.fragment,e),v(ue.$$.fragment,e),v(Ee.$$.fragment,e),v(be.$$.fragment,e),v(ze.$$.fragment,e),v(we.$$.fragment,e),v(Pe.$$.fragment,e),v(ye.$$.fragment,e),v(Me.$$.fragment,e),v(Ce.$$.fragment,e),v(Ae.$$.fragment,e),v(De.$$.fragment,e),v(Le.$$.fragment,e),v(Ne.$$.fragment,e),v(Oe.$$.fragment,e),v(Se.$$.fragment,e),v(Te.$$.fragment,e),v(Ie.$$.fragment,e),v(Ge.$$.fragment,e),v(He.$$.fragment,e),v(Ve.$$.fragment,e),v(Fe.$$.fragment,e),v(Ye.$$.fragment,e),v(Be.$$.fragment,e),v(Ke.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Qe.$$.fragment,e),v(Ze.$$.fragment,e),v(We.$$.fragment,e),v(es.$$.fragment,e),ya=!1},d(e){t(z),e&&t(ie),e&&t(L),x(G),e&&t(_n),x(qe,e),e&&t(gn),e&&t($),e&&t(qn),x(je,e),e&&t(jn),x(ue,e),e&&t(En),e&&t(re),x(Ee),e&&t($n),e&&t(U),e&&t(kn),e&&t(O),e&&t(bn),x(be,e),e&&t(zn),e&&t(rs),e&&t(wn),x(ze,e),e&&t(Pn),x(we,e),e&&t(yn),e&&t(B),e&&t(Mn),x(Pe,e),e&&t(Cn),e&&t(os),e&&t(An),x(ye,e),e&&t(Dn),e&&t(H),e&&t(Ln),e&&t(ls),e&&t(Nn),x(Me,e),e&&t(On),e&&t(is),e&&t(Sn),x(Ce,e),e&&t(Tn),e&&t(K),e&&t(In),e&&t(us),e&&t(Gn),x(Ae,e),e&&t(Rn),e&&t(ps),e&&t(Un),x(De,e),e&&t(Hn),e&&t(cs),e&&t(Vn),x(Le,e),e&&t(Fn),e&&t(X),e&&t(Yn),x(Ne,e),e&&t(Bn),e&&t(ms),e&&t(Kn),e&&t(oe),x(Oe),e&&t(Xn),e&&t(J),e&&t(Jn),x(Se,e),e&&t(Qn),e&&t(Q),e&&t(Zn),e&&t(de),e&&t(Wn),x(Te,e),e&&t(ea),x(Ie,e),e&&t(sa),e&&t(C),e&&t(ta),e&&t(Z),e&&t(na),x(Ge,e),e&&t(aa),e&&t(fs),e&&t(ra),e&&t(w),e&&t(oa),e&&t(y),e&&t(la),e&&t(b),e&&t(ia),x(He,e),e&&t(ua),x(Ve,e),e&&t(pa),e&&t(q),e&&t(ca),x(Fe,e),e&&t(ma),x(Ye,e),e&&t(da),e&&t(vs),e&&t(fa),x(Be,e),e&&t(ha),x(Ke,e),e&&t(va),e&&t(_),e&&t(xa),e&&t(le),x(Xe),e&&t(_a),e&&t(W),e&&t(ga),x(Je,e),e&&t(qa),e&&t(A),e&&t(ja),x(Qe,e),e&&t(Ea),e&&t(he),e&&t($a),x(Ze,e),e&&t(ka),e&&t(ve),e&&t(ba),x(We,e),e&&t(za),e&&t(S),e&&t(wa),x(es,e),e&&t(Pa),e&&t(T)}}}const Nc={local:"entraner-un-nouveau-itokenizeri-partir-dun-ancien",sections:[{local:"assemblage-dun-corpus",title:"Assemblage d'un corpus"},{local:"entranement-dun-nouveau-itokenizeri",title:"Entra\xEEnement d'un nouveau <i>tokenizer</i>"},{local:"sauvegarde-du-itokenizeri",title:"Sauvegarde du <i>tokenizer</i>"}],title:"Entra\xEEner un nouveau <i>tokenizer</i> \xE0 partir d'un ancien"};function Oc(xn){return yc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hc extends bc{constructor(z){super();zc(this,z,Oc,Lc,wc,{})}}export{Hc as default,Nc as metadata};
