import{S as bp,i as $p,s as qp,e as l,t as n,k as m,w as E,c as i,a as u,h as a,d as t,m as f,x as y,g as p,G as s,y as w,q as g,o as b,B as x,b as M,l as Ml,M as kp,N as Tl,p as Ea,v as jp,n as ya}from"../../chunks/vendor-hf-doc-builder.js";import{T as wa}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Ci}from"../../chunks/Youtube-hf-doc-builder.js";import{I as $r}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as L}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as gp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Ep}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function yp(O){let r,_;return r=new gp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"}]}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function wp(O){let r,_;return r=new gp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"}]}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function xp(O){let r,_;return{c(){r=l("p"),_=n("Le pr\xE9-entra\xEEnement du mod\xE8le de langue prendra un certain temps. Nous vous sugg\xE9rons donc d\u2019ex\xE9cuter d\u2019abord la boucle d\u2019entra\xEEnement sur un petit \xE9chantillon des donn\xE9es en d\xE9commentant les deux lignes dans le code ci-dessus. Assurez-vous alors que l\u2019entra\xEEnement se termine avec succ\xE8s et que les mod\xE8les sont stock\xE9s. Rien n\u2019est plus frustrant qu\u2019un entra\xEEnement qui \xE9choue \xE0 la derni\xE8re \xE9tape car vous avez oubli\xE9 de cr\xE9er un dossier ou parce qu\u2019il y a une faute de frappe \xE0 la fin de la boucle d\u2019entra\xEEnement !")},l(c){r=i(c,"P",{});var v=u(r);_=a(v,"Le pr\xE9-entra\xEEnement du mod\xE8le de langue prendra un certain temps. Nous vous sugg\xE9rons donc d\u2019ex\xE9cuter d\u2019abord la boucle d\u2019entra\xEEnement sur un petit \xE9chantillon des donn\xE9es en d\xE9commentant les deux lignes dans le code ci-dessus. Assurez-vous alors que l\u2019entra\xEEnement se termine avec succ\xE8s et que les mod\xE8les sont stock\xE9s. Rien n\u2019est plus frustrant qu\u2019un entra\xEEnement qui \xE9choue \xE0 la derni\xE8re \xE9tape car vous avez oubli\xE9 de cr\xE9er un dossier ou parce qu\u2019il y a une faute de frappe \xE0 la fin de la boucle d\u2019entra\xEEnement !"),v.forEach(t)},m(c,v){p(c,r,v),s(r,_)},d(c){c&&t(r)}}}function zp(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K,W,C,Q;return{c(){r=l("p"),_=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez !"),k=n(" Se d\xE9barrasser de tous les morceaux qui sont plus petits que la taille du contexte n\u2019\xE9tait pas un gros probl\xE8me ici parce que nous utilisons de petites fen\xEAtres de contexte. Si vous augmentez la taille du contexte (ou si vous avez un corpus de documents courts), la fraction des morceaux qui sont jet\xE9s augmentera. Une fa\xE7on plus efficace de pr\xE9parer les donn\xE9es est de joindre tous les \xE9chantillons dans un batch avec un "),$=l("em"),z=n("token"),D=m(),j=l("code"),P=n("eos_token_id"),N=n(" entre les deux, puis d\u2019effectuer le d\xE9coupage sur les s\xE9quences concat\xE9n\xE9es. Comme exercice, modifiez la fonction "),A=l("code"),h=n("tokenize()"),T=n(" pour utiliser cette approche. Notez que vous devrez mettre "),H=l("code"),I=n("truncation=False"),X=n(" et enlever les autres arguments du "),B=l("em"),ee=n("tokenizer"),K=n(" pour obtenir la s\xE9quence compl\xE8te des identifiants des "),W=l("em"),C=n("tokens"),Q=n(".")},l(U){r=i(U,"P",{});var F=u(r);_=a(F,"\u270F\uFE0F "),c=i(F,"STRONG",{});var G=u(c);v=a(G,"Essayez !"),G.forEach(t),k=a(F," Se d\xE9barrasser de tous les morceaux qui sont plus petits que la taille du contexte n\u2019\xE9tait pas un gros probl\xE8me ici parce que nous utilisons de petites fen\xEAtres de contexte. Si vous augmentez la taille du contexte (ou si vous avez un corpus de documents courts), la fraction des morceaux qui sont jet\xE9s augmentera. Une fa\xE7on plus efficace de pr\xE9parer les donn\xE9es est de joindre tous les \xE9chantillons dans un batch avec un "),$=i(F,"EM",{});var R=u($);z=a(R,"token"),R.forEach(t),D=f(F),j=i(F,"CODE",{});var re=u(j);P=a(re,"eos_token_id"),re.forEach(t),N=a(F," entre les deux, puis d\u2019effectuer le d\xE9coupage sur les s\xE9quences concat\xE9n\xE9es. Comme exercice, modifiez la fonction "),A=i(F,"CODE",{});var _e=u(A);h=a(_e,"tokenize()"),_e.forEach(t),T=a(F," pour utiliser cette approche. Notez que vous devrez mettre "),H=i(F,"CODE",{});var J=u(H);I=a(J,"truncation=False"),J.forEach(t),X=a(F," et enlever les autres arguments du "),B=i(F,"EM",{});var he=u(B);ee=a(he,"tokenizer"),he.forEach(t),K=a(F," pour obtenir la s\xE9quence compl\xE8te des identifiants des "),W=i(F,"EM",{});var ve=u(W);C=a(ve,"tokens"),ve.forEach(t),Q=a(F,"."),F.forEach(t)},m(U,F){p(U,r,F),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D),s(r,j),s(j,P),s(r,N),s(r,A),s(A,h),s(r,T),s(r,H),s(H,I),s(r,X),s(r,B),s(B,ee),s(r,K),s(r,W),s(W,C),s(r,Q)},d(U){U&&t(r)}}}function Pp(O){let r,_,c,v,k,$,z,D,j,P,N,A;return r=new L({props:{code:`from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),j=new L({props:{code:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Construit le mod\xE8le
model.summary()`,highlighted:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  <span class="hljs-comment"># Construit le mod\xE8le</span>
model.summary()`}}),N=new L({props:{code:`_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________`,highlighted:`_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#   </span>
=================================================================
transformer (TFGPT2MainLayer multiple                  <span class="hljs-number">124242432</span> 
=================================================================
Total params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Trainable params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){E(r.$$.fragment),_=m(),c=l("p"),v=n("Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=l("code"),$=n("from_pretrained()"),z=n(" puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),D=m(),E(j.$$.fragment),P=m(),E(N.$$.fragment)},l(h){y(r.$$.fragment,h),_=f(h),c=i(h,"P",{});var T=u(c);v=a(T,"Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=i(T,"CODE",{});var H=u(k);$=a(H,"from_pretrained()"),H.forEach(t),z=a(T," puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),T.forEach(t),D=f(h),y(j.$$.fragment,h),P=f(h),y(N.$$.fragment,h)},m(h,T){w(r,h,T),p(h,_,T),p(h,c,T),s(c,v),s(c,k),s(k,$),s(c,z),p(h,D,T),w(j,h,T),p(h,P,T),w(N,h,T),A=!0},i(h){A||(g(r.$$.fragment,h),g(j.$$.fragment,h),g(N.$$.fragment,h),A=!0)},o(h){b(r.$$.fragment,h),b(j.$$.fragment,h),b(N.$$.fragment,h),A=!1},d(h){x(r,h),h&&t(_),h&&t(c),h&&t(D),x(j,h),h&&t(P),x(N,h)}}}function Cp(O){let r,_,c,v,k,$,z,D,j,P,N,A;return r=new L({props:{code:`from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),j=new L({props:{code:`model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")`,highlighted:`model = GPT2LMHeadModel(config)
model_size = <span class="hljs-built_in">sum</span>(t.numel() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> model.parameters())
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;GPT-2 size: <span class="hljs-subst">{model_size/<span class="hljs-number">1000</span>**<span class="hljs-number">2</span>:<span class="hljs-number">.1</span>f}</span>M parameters&quot;</span>)`}}),N=new L({props:{code:"GPT-2 size: 124.2M parameters",highlighted:'GPT-<span class="hljs-number">2</span> size: <span class="hljs-number">124.2</span>M parameters'}}),{c(){E(r.$$.fragment),_=m(),c=l("p"),v=n("Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=l("code"),$=n("from_pretrained()"),z=n(" puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),D=m(),E(j.$$.fragment),P=m(),E(N.$$.fragment)},l(h){y(r.$$.fragment,h),_=f(h),c=i(h,"P",{});var T=u(c);v=a(T,"Avec cette configuration, nous pouvons charger un nouveau mod\xE8le. Notez que c\u2019est la premi\xE8re fois que nous n\u2019utilisons pas la fonction "),k=i(T,"CODE",{});var H=u(k);$=a(H,"from_pretrained()"),H.forEach(t),z=a(T," puisque nous initialisons nous-m\xEAmes un mod\xE8le :"),T.forEach(t),D=f(h),y(j.$$.fragment,h),P=f(h),y(N.$$.fragment,h)},m(h,T){w(r,h,T),p(h,_,T),p(h,c,T),s(c,v),s(c,k),s(k,$),s(c,z),p(h,D,T),w(j,h,T),p(h,P,T),w(N,h,T),A=!0},i(h){A||(g(r.$$.fragment,h),g(j.$$.fragment,h),g(N.$$.fragment,h),A=!0)},o(h){b(r.$$.fragment,h),b(j.$$.fragment,h),b(N.$$.fragment,h),A=!1},d(h){x(r,h),h&&t(_),h&&t(c),h&&t(D),x(j,h),h&&t(P),x(N,h)}}}function Dp(O){let r,_;return r=new L({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function Tp(O){let r,_;return r=new L({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>)`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function Mp(O){let r,_;return r=new L({props:{code:`input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)`,highlighted:`input_ids shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
attention_mask shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
labels shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function Np(O){let r,_;return r=new L({props:{code:`input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])`,highlighted:`input_ids shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
attention_mask shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
labels shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function hp(O){let r,_,c,v,k,$,z,D;return z=new L({props:{code:`tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)
tf_eval_dataset = tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),{c(){r=l("p"),_=n("Maintenant nous pouvons utiliser la m\xE9thode "),c=l("code"),v=n("to_tf_dataset()"),k=n(" pour convertir nos jeux de donn\xE9es en jeux de donn\xE9es TensorFlow avec le collateur de donn\xE9es que nous avons cr\xE9\xE9 ci-dessus :"),$=m(),E(z.$$.fragment)},l(j){r=i(j,"P",{});var P=u(r);_=a(P,"Maintenant nous pouvons utiliser la m\xE9thode "),c=i(P,"CODE",{});var N=u(c);v=a(N,"to_tf_dataset()"),N.forEach(t),k=a(P," pour convertir nos jeux de donn\xE9es en jeux de donn\xE9es TensorFlow avec le collateur de donn\xE9es que nous avons cr\xE9\xE9 ci-dessus :"),P.forEach(t),$=f(j),y(z.$$.fragment,j)},m(j,P){p(j,r,P),s(r,_),s(r,c),s(c,v),s(r,k),p(j,$,P),w(z,j,P),D=!0},i(j){D||(g(z.$$.fragment,j),D=!0)},o(j){b(z.$$.fragment,j),D=!1},d(j){j&&t(r),j&&t($),x(z,j)}}}function Ap(O){let r,_;return{c(){r=l("p"),_=n("\u26A0\uFE0F Le d\xE9placement des entr\xE9es et des \xE9tiquettes pour les aligner se fait \xE0 l\u2019int\xE9rieur du mod\xE8le, de sorte que le collecteur de donn\xE9es ne fait que copier les entr\xE9es pour cr\xE9er les \xE9tiquettes.")},l(c){r=i(c,"P",{});var v=u(r);_=a(v,"\u26A0\uFE0F Le d\xE9placement des entr\xE9es et des \xE9tiquettes pour les aligner se fait \xE0 l\u2019int\xE9rieur du mod\xE8le, de sorte que le collecteur de donn\xE9es ne fait que copier les entr\xE9es pour cr\xE9er les \xE9tiquettes."),v.forEach(t)},m(c,v){p(c,r,v),s(r,_)},d(c){c&&t(r)}}}function Lp(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K,W,C,Q,U,F;return P=new L({props:{code:`from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra\xEEner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Entra\xEEner en mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),U=new L({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>, tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`}}),{c(){r=l("p"),_=n("Tout ce qu\u2019il reste \xE0 faire est de configurer les hyperparam\xE8tres d\u2019entra\xEEnement et d\u2019appeler "),c=l("code"),v=n("compile()"),k=n(" et "),$=l("code"),z=n("fit()"),D=n(". Nous utiliserons un programme de taux d\u2019apprentissage avec un r\xE9chauffement pour am\xE9liorer la stabilit\xE9 de l\u2019entra\xEEnement :"),j=m(),E(P.$$.fragment),N=m(),A=l("p"),h=n("Maintenant, nous pouvons simplement appeler "),T=l("code"),H=n("model.fit()"),I=n(" et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019\xE9chantillon d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf\xE9s et un bon livre \xE0 lire ! Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),X=l("em"),B=n("tokenizer"),ee=n(" vers le "),K=l("em"),W=n("Hub"),C=n(" :"),Q=m(),E(U.$$.fragment)},l(G){r=i(G,"P",{});var R=u(r);_=a(R,"Tout ce qu\u2019il reste \xE0 faire est de configurer les hyperparam\xE8tres d\u2019entra\xEEnement et d\u2019appeler "),c=i(R,"CODE",{});var re=u(c);v=a(re,"compile()"),re.forEach(t),k=a(R," et "),$=i(R,"CODE",{});var _e=u($);z=a(_e,"fit()"),_e.forEach(t),D=a(R,". Nous utiliserons un programme de taux d\u2019apprentissage avec un r\xE9chauffement pour am\xE9liorer la stabilit\xE9 de l\u2019entra\xEEnement :"),R.forEach(t),j=f(G),y(P.$$.fragment,G),N=f(G),A=i(G,"P",{});var J=u(A);h=a(J,"Maintenant, nous pouvons simplement appeler "),T=i(J,"CODE",{});var he=u(T);H=a(he,"model.fit()"),he.forEach(t),I=a(J," et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019\xE9chantillon d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf\xE9s et un bon livre \xE0 lire ! Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),X=i(J,"EM",{});var ve=u(X);B=a(ve,"tokenizer"),ve.forEach(t),ee=a(J," vers le "),K=i(J,"EM",{});var de=u(K);W=a(de,"Hub"),de.forEach(t),C=a(J," :"),J.forEach(t),Q=f(G),y(U.$$.fragment,G)},m(G,R){p(G,r,R),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D),p(G,j,R),w(P,G,R),p(G,N,R),p(G,A,R),s(A,h),s(A,T),s(T,H),s(A,I),s(A,X),s(X,B),s(A,ee),s(A,K),s(K,W),s(A,C),p(G,Q,R),w(U,G,R),F=!0},i(G){F||(g(P.$$.fragment,G),g(U.$$.fragment,G),F=!0)},o(G){b(P.$$.fragment,G),b(U.$$.fragment,G),F=!1},d(G){G&&t(r),G&&t(j),x(P,G),G&&t(N),G&&t(A),G&&t(Q),x(U,G)}}}function Op(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K,W,C,Q,U,F,G,R,re,_e,J,he,ve,de,Y,ts;return H=new L({props:{code:`from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>,
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
    eval_steps=<span class="hljs-number">5_000</span>,
    logging_steps=<span class="hljs-number">5_000</span>,
    gradient_accumulation_steps=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">1</span>,
    weight_decay=<span class="hljs-number">0.1</span>,
    warmup_steps=<span class="hljs-number">1_000</span>,
    lr_scheduler_type=<span class="hljs-string">&quot;cosine&quot;</span>,
    learning_rate=<span class="hljs-number">5e-4</span>,
    save_steps=<span class="hljs-number">5_000</span>,
    fp16=<span class="hljs-literal">True</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;valid&quot;</span>],
)`}}),Q=new L({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),Y=new L({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){r=l("p"),_=n("Tout ce qu\u2019il reste \xE0 faire est de configurer les arguments d\u2019entra\xEEnement et de lancer la fonction "),c=l("code"),v=n("Trainer"),k=n(". Nous utiliserons un programme de taux d\u2019apprentissage de type cosinus avec un r\xE9chauffement et une taille de batch de 256 ("),$=l("code"),z=n("per_device_train_batch_size"),D=m(),j=l("em"),P=l("code"),N=n("gradient_accumulation_steps"),A=n("). L\u2019accumulation du gradient est utilis\xE9e lorsqu\u2019un seul batch ne tient pas en m\xE9moire, et construit le gradient de mani\xE8re incr\xE9mentale \xE0 travers plusieurs passages en avant/en arri\xE8re. Nous verrons cela en action lorsque nous cr\xE9erons la boucle d\u2019entra\xEEnement avec \u{1F917} "),h=n("Accelerate*."),T=m(),E(H.$$.fragment),I=m(),X=l("p"),B=n("Maintenant, nous pouvons simplement lancer le "),ee=l("code"),K=n("Trainer"),W=n(" et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019\xE9chantillon d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf\xE9s et un bon livre \xE0 lire !"),C=m(),E(Q.$$.fragment),U=m(),F=l("p"),G=n("Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),R=l("em"),re=n("tokenizer"),_e=n(" vers le "),J=l("em"),he=n("Hub"),ve=n(" :"),de=m(),E(Y.$$.fragment)},l(S){r=i(S,"P",{});var Z=u(r);_=a(Z,"Tout ce qu\u2019il reste \xE0 faire est de configurer les arguments d\u2019entra\xEEnement et de lancer la fonction "),c=i(Z,"CODE",{});var nt=u(c);v=a(nt,"Trainer"),nt.forEach(t),k=a(Z,". Nous utiliserons un programme de taux d\u2019apprentissage de type cosinus avec un r\xE9chauffement et une taille de batch de 256 ("),$=i(Z,"CODE",{});var Te=u($);z=a(Te,"per_device_train_batch_size"),Te.forEach(t),D=f(Z),j=i(Z,"EM",{});var gs=u(j);P=i(gs,"CODE",{});var at=u(P);N=a(at,"gradient_accumulation_steps"),at.forEach(t),A=a(gs,"). L\u2019accumulation du gradient est utilis\xE9e lorsqu\u2019un seul batch ne tient pas en m\xE9moire, et construit le gradient de mani\xE8re incr\xE9mentale \xE0 travers plusieurs passages en avant/en arri\xE8re. Nous verrons cela en action lorsque nous cr\xE9erons la boucle d\u2019entra\xEEnement avec \u{1F917} "),gs.forEach(t),h=a(Z,"Accelerate*."),Z.forEach(t),T=f(S),y(H.$$.fragment,S),I=f(S),X=i(S,"P",{});var ke=u(X);B=a(ke,"Maintenant, nous pouvons simplement lancer le "),ee=i(ke,"CODE",{});var ot=u(ee);K=a(ot,"Trainer"),ot.forEach(t),W=a(ke," et attendre que l\u2019entra\xEEnement se termine. Selon que vous l\u2019ex\xE9cutez sur la totalit\xE9 ou sur un sous-ensemble de l\u2019\xE9chantillon d\u2019entra\xEEnement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf\xE9s et un bon livre \xE0 lire !"),ke.forEach(t),C=f(S),y(Q.$$.fragment,S),U=f(S),F=i(S,"P",{});var Me=u(F);G=a(Me,"Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser le mod\xE8le et le "),R=i(Me,"EM",{});var Ne=u(R);re=a(Ne,"tokenizer"),Ne.forEach(t),_e=a(Me," vers le "),J=i(Me,"EM",{});var rt=u(J);he=a(rt,"Hub"),rt.forEach(t),ve=a(Me," :"),Me.forEach(t),de=f(S),y(Y.$$.fragment,S)},m(S,Z){p(S,r,Z),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D),s(r,j),s(j,P),s(P,N),s(j,A),s(r,h),p(S,T,Z),w(H,S,Z),p(S,I,Z),p(S,X,Z),s(X,B),s(X,ee),s(ee,K),s(X,W),p(S,C,Z),w(Q,S,Z),p(S,U,Z),p(S,F,Z),s(F,G),s(F,R),s(R,re),s(F,_e),s(F,J),s(J,he),s(F,ve),p(S,de,Z),w(Y,S,Z),ts=!0},i(S){ts||(g(H.$$.fragment,S),g(Q.$$.fragment,S),g(Y.$$.fragment,S),ts=!0)},o(S){b(H.$$.fragment,S),b(Q.$$.fragment,S),b(Y.$$.fragment,S),ts=!1},d(S){S&&t(r),S&&t(T),x(H,S),S&&t(I),S&&t(X),S&&t(C),x(Q,S),S&&t(U),S&&t(F),S&&t(de),x(Y,S)}}}function Sp(O){let r,_,c,v,k,$,z,D;return{c(){r=l("p"),_=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez !"),k=n(" Il ne nous a fallu qu\u2019une trentaine de lignes de code en plus des "),$=l("code"),z=n("TrainingArguments"),D=n(" pour passer des textes bruts \xE0 l\u2019entra\xEEnement du GPT-2. Essayez-le avec votre propre jeu de donn\xE9es et voyez si vous pouvez obtenir de bons r\xE9sultats !")},l(j){r=i(j,"P",{});var P=u(r);_=a(P,"\u270F\uFE0F "),c=i(P,"STRONG",{});var N=u(c);v=a(N,"Essayez !"),N.forEach(t),k=a(P," Il ne nous a fallu qu\u2019une trentaine de lignes de code en plus des "),$=i(P,"CODE",{});var A=u($);z=a(A,"TrainingArguments"),A.forEach(t),D=a(P," pour passer des textes bruts \xE0 l\u2019entra\xEEnement du GPT-2. Essayez-le avec votre propre jeu de donn\xE9es et voyez si vous pouvez obtenir de bons r\xE9sultats !"),P.forEach(t)},m(j,P){p(j,r,P),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D)},d(j){j&&t(r)}}}function Gp(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K;return{c(){r=l("p"),_=n("\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, vous pouvez essayer d\u2019utiliser "),c=l("code"),v=n("MirroredStrategy"),k=n(" pour acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement. Vous devrez cr\xE9er un objet "),$=l("code"),z=n("tf.distribute.MirroredStrategy"),D=n(" et vous assurer que les commandes "),j=l("code"),P=n("to_tf_dataset"),N=n(" ainsi que la cr\xE9ation du mod\xE8le et l\u2019appel \xE0 "),A=l("code"),h=n("fit()"),T=n(" sont tous ex\xE9cut\xE9s dans "),H=l("code"),I=n("scope()"),X=n(". Vous pouvez consulter la documentation \xE0 ce sujet "),B=l("a"),ee=n("ici"),K=n("."),this.h()},l(W){r=i(W,"P",{});var C=u(r);_=a(C,"\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, vous pouvez essayer d\u2019utiliser "),c=i(C,"CODE",{});var Q=u(c);v=a(Q,"MirroredStrategy"),Q.forEach(t),k=a(C," pour acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement. Vous devrez cr\xE9er un objet "),$=i(C,"CODE",{});var U=u($);z=a(U,"tf.distribute.MirroredStrategy"),U.forEach(t),D=a(C," et vous assurer que les commandes "),j=i(C,"CODE",{});var F=u(j);P=a(F,"to_tf_dataset"),F.forEach(t),N=a(C," ainsi que la cr\xE9ation du mod\xE8le et l\u2019appel \xE0 "),A=i(C,"CODE",{});var G=u(A);h=a(G,"fit()"),G.forEach(t),T=a(C," sont tous ex\xE9cut\xE9s dans "),H=i(C,"CODE",{});var R=u(H);I=a(R,"scope()"),R.forEach(t),X=a(C,". Vous pouvez consulter la documentation \xE0 ce sujet "),B=i(C,"A",{href:!0,rel:!0});var re=u(B);ee=a(re,"ici"),re.forEach(t),K=a(C,"."),C.forEach(t),this.h()},h(){M(B,"href","https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit"),M(B,"rel","nofollow")},m(W,C){p(W,r,C),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D),s(r,j),s(j,P),s(r,N),s(r,A),s(A,h),s(r,T),s(r,H),s(H,I),s(r,X),s(r,B),s(B,ee),s(r,K)},d(W){W&&t(r)}}}function Fp(O){let r,_,c,v,k;return{c(){r=l("p"),_=n("\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, essayez d\u2019y ex\xE9cuter le code. "),c=l("code"),v=n("Trainer"),k=n(" g\xE8re automatiquement plusieurs machines ce qui peut acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement.")},l($){r=i($,"P",{});var z=u(r);_=a(z,"\u{1F4A1} Si vous avez acc\xE8s \xE0 une machine avec plusieurs GPUs, essayez d\u2019y ex\xE9cuter le code. "),c=i(z,"CODE",{});var D=u(c);v=a(D,"Trainer"),D.forEach(t),k=a(z," g\xE8re automatiquement plusieurs machines ce qui peut acc\xE9l\xE9rer consid\xE9rablement l\u2019entra\xEEnement."),z.forEach(t)},m($,z){p($,r,z),s(r,_),s(r,c),s(c,v),s(r,k)},d($){$&&t(r)}}}function Hp(O){let r;function _(k,$){return k[0]==="pt"?Fp:Gp}let c=_(O),v=c(O);return{c(){v.c(),r=Ml()},l(k){v.l(k),r=Ml()},m(k,$){v.m(k,$),p(k,r,$)},p(k,$){c!==(c=_(k))&&(v.d(1),v=c(k),v&&(v.c(),v.m(r.parentNode,r)))},d(k){v.d(k),k&&t(r)}}}function Ip(O){let r,_;return r=new L({props:{code:`from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

course_model = TFGPT2LMHeadModel.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
course_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=course_model, tokenizer=course_tokenizer, device=<span class="hljs-number">0</span>
)`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function Rp(O){let r,_;return r=new L({props:{code:`import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>, device=device
)`}}),{c(){E(r.$$.fragment)},l(c){y(r.$$.fragment,c)},m(c,v){w(r,c,v),_=!0},i(c){_||(g(r.$$.fragment,c),_=!0)},o(c){b(r.$$.fragment,c),_=!1},d(c){x(r,c)}}}function Vp(O){let r,_,c,v,k,$,z,D;return{c(){r=l("p"),_=n("Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe des biblioth\xE8ques Python de science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant. Parfois, il est n\xE9cessaire de personnaliser davantage l\u2019entra\xEEnement du mod\xE8le afin d\u2019obtenir les performances n\xE9cessaires pour un cas d\u2019utilisation donn\xE9. Par exemple, que se passe-t-il si l\u2019on souhaite mettre \xE0 jour dynamiquement la taille du batch ou si l\u2019on dispose d\u2019une boucle d\u2019entra\xEEnement conditionnelle qui ignore les mauvais exemples \xE0 la vol\xE9e ? Une option serait de sous-classer le "),c=l("code"),v=n("Trainer"),k=n(" et d\u2019ajouter les changements n\xE9cessaires, mais parfois il est plus simple d\u2019\xE9crire la boucle d\u2019entra\xEEnement \xE0 partir de z\xE9ro. C\u2019est l\xE0 qu\u2019intervient \u{1F917} "),$=l("em"),z=n("Accelerate"),D=n(".")},l(j){r=i(j,"P",{});var P=u(r);_=a(P,"Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe des biblioth\xE8ques Python de science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant. Parfois, il est n\xE9cessaire de personnaliser davantage l\u2019entra\xEEnement du mod\xE8le afin d\u2019obtenir les performances n\xE9cessaires pour un cas d\u2019utilisation donn\xE9. Par exemple, que se passe-t-il si l\u2019on souhaite mettre \xE0 jour dynamiquement la taille du batch ou si l\u2019on dispose d\u2019une boucle d\u2019entra\xEEnement conditionnelle qui ignore les mauvais exemples \xE0 la vol\xE9e ? Une option serait de sous-classer le "),c=i(P,"CODE",{});var N=u(c);v=a(N,"Trainer"),N.forEach(t),k=a(P," et d\u2019ajouter les changements n\xE9cessaires, mais parfois il est plus simple d\u2019\xE9crire la boucle d\u2019entra\xEEnement \xE0 partir de z\xE9ro. C\u2019est l\xE0 qu\u2019intervient \u{1F917} "),$=i(P,"EM",{});var A=u($);z=a(A,"Accelerate"),A.forEach(t),D=a(P,"."),P.forEach(t)},m(j,P){p(j,r,P),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D)},d(j){j&&t(r)}}}function Up(O){let r,_;return{c(){r=l("p"),_=n("Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe des biblioth\xE8ques Python de science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant.")},l(c){r=i(c,"P",{});var v=u(r);_=a(v,"Au vu de ces quelques exemples, il semble que le mod\xE8le ait appris une partie de la syntaxe des biblioth\xE8ques Python de science des donn\xE9es. Bien s\xFBr, nous devrions \xE9valuer le mod\xE8le de mani\xE8re plus approfondie avant de le d\xE9ployer dans le monde r\xE9el, mais il s\u2019agit tout de m\xEAme d\u2019un prototype impressionnant."),v.forEach(t)},m(c,v){p(c,r,v),s(r,_)},d(c){c&&t(r)}}}function vp(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K,W,C,Q,U,F,G,R,re,_e,J,he,ve,de,Y,ts,S,Z,nt,Te,gs,at,ke,ot,Me,Ne,rt,xa,bs,za,Pa,lt,$s,xn,ae,Ca,it,Lt,Da,Ot,St,Ta,Gt,Ft,Ma,Ht,It,Na,Rt,zn,qs,Vt,oe,Aa,te,ge,Pn,Cn,Ae,Ut,Bt,La,Wt,ks,Oa,Xt,Dn,We,Le,Kt,js,Jt,Yt,Sa,Zt,le,Tn,xe,Ga,ut,Oe,Qt,en,Fa,sn,Es,tn,nn,Ha,an,ne,Ia,ys,Ra,Va,ws,Ua,Ba,pt,xs,Mn,Xe,zs,ct,dt,Wa,on,ze,rn,je,Nn,Se,Xa,ln,An,Ps,un,me,Ln,ns,Ka,On,Ge,Sn,Cs,pn,Fe,Gn,Ds,as,Fn,Ke,Hn,mt,In,os,rs,ls,is,Je,be,ft,cn,Rn,Ts,_t,Ms,Vn,He,Ja,dn,Un,Ns,mn,se,us,As,Ls,Ya,Os,Za,Bn,Ye,Wn,ps,fn,_n,Qa,hn,Xn,ie,Kn,Ze,Jn,Ss,ht,vt,eo,Gs,Fs,Yn,Ee,so,vn,gn,to,bn,Zn,Qe,cs,$n,no,ds,gt,qn,Ie,Qn,es,Hs,bt,Is,ea,Pe,Rs,$t,qt,ao,kt,Vs,sa,ms,ue;return v=new $r({}),K=new Ci({props:{id:"Hm8_PgVTFuc"}}),qs=new L({props:{code:`keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")`,highlighted:`keytoken_ids = []
<span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> [
    <span class="hljs-string">&quot;plt&quot;</span>,
    <span class="hljs-string">&quot;pd&quot;</span>,
    <span class="hljs-string">&quot;sk&quot;</span>,
    <span class="hljs-string">&quot;fit&quot;</span>,
    <span class="hljs-string">&quot;predict&quot;</span>,
    <span class="hljs-string">&quot; plt&quot;</span>,
    <span class="hljs-string">&quot; pd&quot;</span>,
    <span class="hljs-string">&quot; sk&quot;</span>,
    <span class="hljs-string">&quot; fit&quot;</span>,
    <span class="hljs-string">&quot; predict&quot;</span>,
    <span class="hljs-string">&quot;testtest&quot;</span>,
]:
    ids = tokenizer([keyword]).input_ids[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ids) == <span class="hljs-number">1</span>:
        keytoken_ids.append(ids[<span class="hljs-number">0</span>])
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keyword has not single token: <span class="hljs-subst">{keyword}</span>&quot;</span>)`}}),oe=new L({props:{code:"'Keyword has not single token: testtest'",highlighted:'<span class="hljs-string">&#x27;Keyword has not single token: testtest&#x27;</span>'}}),le=new L({props:{code:`from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # D\xE9calage pour que tokens < n pr\xE9disent n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calcul de la perte par token
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Redimensionnement et perte moyenne par \xE9chantillon
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculer et \xE9chelonner la pond\xE9ration
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculer la moyenne pond\xE9r\xE9e
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss`,highlighted:`<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> CrossEntropyLoss
<span class="hljs-keyword">import</span> torch


<span class="hljs-keyword">def</span> <span class="hljs-title function_">keytoken_weighted_loss</span>(<span class="hljs-params">inputs, logits, keytoken_ids, alpha=<span class="hljs-number">1.0</span></span>):
    <span class="hljs-comment"># D\xE9calage pour que tokens &lt; n pr\xE9disent n</span>
    shift_labels = inputs[..., <span class="hljs-number">1</span>:].contiguous()
    shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()
    <span class="hljs-comment"># Calcul de la perte par token</span>
    loss_fct = CrossEntropyLoss(reduce=<span class="hljs-literal">False</span>)
    loss = loss_fct(shift_logits.view(-<span class="hljs-number">1</span>, shift_logits.size(-<span class="hljs-number">1</span>)), shift_labels.view(-<span class="hljs-number">1</span>))
    <span class="hljs-comment"># Redimensionnement et perte moyenne par \xE9chantillon</span>
    loss_per_sample = loss.view(shift_logits.size(<span class="hljs-number">0</span>), shift_logits.size(<span class="hljs-number">1</span>)).mean(axis=<span class="hljs-number">1</span>)
    <span class="hljs-comment"># Calculer et \xE9chelonner la pond\xE9ration</span>
    weights = torch.stack([(inputs == kt).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">for</span> kt <span class="hljs-keyword">in</span> keytoken_ids]).<span class="hljs-built_in">sum</span>(
        axis=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]
    )
    weights = alpha * (<span class="hljs-number">1.0</span> + weights)
    <span class="hljs-comment"># Calculer la moyenne pond\xE9r\xE9e</span>
    weighted_loss = (loss_per_sample * weights).mean()
    <span class="hljs-keyword">return</span> weighted_loss`}}),xs=new L({props:{code:`from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader

tokenized_dataset.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
train_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>], batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)
eval_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>], batch_size=<span class="hljs-number">32</span>)`}}),ze=new L({props:{code:`weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]`,highlighted:`weight_decay = <span class="hljs-number">0.1</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_grouped_params</span>(<span class="hljs-params">model, no_decay=[<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-string">&quot;LayerNorm.weight&quot;</span>]</span>):
    params_with_wd, params_without_wd = [], []
    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay):
            params_without_wd.append(p)
        <span class="hljs-keyword">else</span>:
            params_with_wd.append(p)
    <span class="hljs-keyword">return</span> [
        {<span class="hljs-string">&quot;params&quot;</span>: params_with_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: weight_decay},
        {<span class="hljs-string">&quot;params&quot;</span>: params_without_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: <span class="hljs-number">0.0</span>},
    ]`}}),Ps=new L({props:{code:`def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], labels=batch[<span class="hljs-string">&quot;input_ids&quot;</span>])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    <span class="hljs-keyword">try</span>:
        perplexity = torch.exp(loss)
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)
    <span class="hljs-keyword">return</span> loss.item(), perplexity.item()`}}),Fe=new L({props:{code:"model = GPT2LMHeadModel(config)",highlighted:"model = GPT2LMHeadModel(config)"}}),Ke=new L({props:{code:`from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(get_grouped_params(model), lr=<span class="hljs-number">5e-4</span>)`}}),rs=new L({props:{code:`from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator(fp16=<span class="hljs-literal">True</span>)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),is=new wa({props:{$$slots:{default:[Bp]},$$scope:{ctx:O}}}),Ns=new L({props:{code:`num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)`,highlighted:`num_train_epochs = <span class="hljs-number">1</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name=<span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_training_steps=num_training_steps,
)`}}),Ze=new L({props:{code:`from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, get_full_repo_name

model_name = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),Ss=new L({props:{code:"'sgugger/codeparrot-ds-accelerate'",highlighted:'<span class="hljs-string">&#x27;sgugger/codeparrot-ds-accelerate&#x27;</span>'}}),Fs=new L({props:{code:`output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`output_dir = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),gt=new L({props:{code:"evaluate()",highlighted:"evaluate()"}}),Ie=new L({props:{code:"(10.934126853942871, 56057.14453125)",highlighted:'(<span class="hljs-number">10.934126853942871</span>, <span class="hljs-number">56057.14453125</span>)'}}),Rs=new L({props:{code:`from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=len(train_dataloader)
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm

gradient_accumulation_steps = <span class="hljs-number">8</span>
eval_steps = <span class="hljs-number">5_000</span>

model.train()
completed_steps = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> tqdm(
        <span class="hljs-built_in">enumerate</span>(train_dataloader, start=<span class="hljs-number">1</span>), total=<span class="hljs-built_in">len</span>(train_dataloader)
    ):
        logits = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>]).logits
        loss = keytoken_weighted_loss(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], logits, keytoken_ids)
        <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            accelerator.<span class="hljs-built_in">print</span>(
                {
                    <span class="hljs-string">&quot;lr&quot;</span>: get_lr(),
                    <span class="hljs-string">&quot;samples&quot;</span>: step * samples_per_step,
                    <span class="hljs-string">&quot;steps&quot;</span>: completed_steps,
                    <span class="hljs-string">&quot;loss/train&quot;</span>: loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        <span class="hljs-keyword">if</span> step % gradient_accumulation_steps == <span class="hljs-number">0</span>:
            accelerator.clip_grad_norm_(model.parameters(), <span class="hljs-number">1.0</span>)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> (step % (eval_steps * gradient_accumulation_steps)) == <span class="hljs-number">0</span>:
            eval_loss, perplexity = evaluate()
            accelerator.<span class="hljs-built_in">print</span>({<span class="hljs-string">&quot;loss/eval&quot;</span>: eval_loss, <span class="hljs-string">&quot;perplexity&quot;</span>: perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            <span class="hljs-keyword">if</span> accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=<span class="hljs-string">f&quot;Training in progress step <span class="hljs-subst">{step}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
                )`}}),Vs=new wa({props:{$$slots:{default:[Wp]},$$scope:{ctx:O}}}),ms=new wa({props:{$$slots:{default:[Xp]},$$scope:{ctx:O}}}),{c(){r=l("h2"),_=l("a"),c=l("span"),E(v.$$.fragment),k=m(),$=l("span"),z=n("Entra\xEEner avec \u{1F917} "),D=l("i"),j=n("Accelerate"),P=m(),N=l("p"),A=n("Nous avons vu comment entra\xEEner un mod\xE8le avec le "),h=l("code"),T=n("Trainer"),H=n(", qui permet une certaine personnalisation. Cependant, parfois nous voulons un contr\xF4le total sur la boucle d\u2019entra\xEEnement ou nous souhaitons faire quelques changements exotiques. Dans ce cas, \u{1F917} "),I=l("em"),X=n("Accelerate"),B=n(" est un excellent choix, et dans cette section, nous allons suivre les \xE9tapes pour l\u2019utiliser pour entra\xEEner notre mod\xE8le. Pour rendre les choses plus int\xE9ressantes, nous allons \xE9galement ajouter une touche \xE0 la boucle d\u2019entra\xEEnement."),ee=m(),E(K.$$.fragment),W=m(),C=l("p"),Q=n("Puisque nous sommes principalement int\xE9ress\xE9s par l\u2019autocompl\xE9tion pour les biblioth\xE8ques de science des donn\xE9es, il est logique de donner plus de poids aux \xE9chantillons d\u2019entra\xEEnement qui utilisent davantage ces biblioth\xE8ques. Nous pouvons facilement identifier ces exemples gr\xE2ce \xE0 l\u2019utilisation de mots-cl\xE9s tels que "),U=l("code"),F=n("plt"),G=n(", "),R=l("code"),re=n("pd"),_e=n(", "),J=l("code"),he=n("sk"),ve=n(", "),de=l("code"),Y=n("fit"),ts=n(", et "),S=l("code"),Z=n("predict"),nt=n(", qui sont les noms d\u2019importation les plus fr\xE9quents pour "),Te=l("code"),gs=n("matplotlib.pyplot"),at=n(", "),ke=l("code"),ot=n("pandas"),Me=n(", et "),Ne=l("code"),rt=n("sklearn"),xa=n(" ainsi que les fonctions "),bs=l("code"),za=n("fit"),Pa=n(" et "),lt=l("code"),$s=n("predict"),xn=n(" de cette derni\xE8re. Si chacun d\u2019entre eux est repr\xE9sent\xE9 par un seul "),ae=l("em"),Ca=n("token"),it=n(", nous pouvons facilement v\xE9rifier s\u2019ils apparaissent dans la s\xE9quence d\u2019entr\xE9e. Les "),Lt=l("em"),Da=n("tokens"),Ot=n(" peuvent avoir un pr\xE9fixe d\u2019espacement, donc nous v\xE9rifierons aussi ces versions dans le vocabulaire du "),St=l("em"),Ta=n("tokenizer"),Gt=n(". Pour v\xE9rifier que cela fonctionne, nous ajouterons un "),Ft=l("em"),Ma=n("token"),Ht=n(" de test qui devrait \xEAtre divis\xE9 en plusieurs "),It=l("em"),Na=n("tokens"),Rt=n(" :"),zn=m(),E(qs.$$.fragment),Vt=m(),E(oe.$$.fragment),Aa=m(),te=l("p"),ge=n("Super, \xE7a a l\u2019air de bien fonctionner ! Nous pouvons maintenant \xE9crire une fonction de perte personnalis\xE9e qui prend la s\xE9quence d\u2019entr\xE9e, les logits et les "),Pn=l("em"),Cn=n("tokens"),Ae=n(" cl\xE9s que nous venons de s\xE9lectionner comme entr\xE9es. Tout d\u2019abord, nous devons aligner les logits et les entr\xE9es : la s\xE9quence d\u2019entr\xE9e d\xE9cal\xE9e d\u2019une unit\xE9 vers la droite forme les \xE9tiquettes, puisque le "),Ut=l("em"),Bt=n("token"),La=n(" suivant est l\u2019\xE9tiquette du "),Wt=l("em"),ks=n("token"),Oa=n(" actuel. Nous pouvons y parvenir en commen\xE7ant les \xE9tiquettes \xE0 partir du deuxi\xE8me "),Xt=l("em"),Dn=n("token"),We=n(" de la s\xE9quence d\u2019entr\xE9e, puisque le mod\xE8le ne fait pas de pr\xE9diction pour le premier "),Le=l("em"),Kt=n("token"),js=n(" de toute fa\xE7on. Ensuite, nous coupons le dernier logit, car nous n\u2019avons pas d\u2019\xE9tiquette pour le "),Jt=l("em"),Yt=n("token"),Sa=n(" qui suit la s\xE9quence d\u2019entr\xE9e compl\xE8te. Avec cela, nous pouvons calculer la perte par \xE9chantillon et compter les occurrences de tous les mots-cl\xE9s dans chaque \xE9chantillon. Enfin, nous calculons la moyenne pond\xE9r\xE9e sur tous les \xE9chantillons en utilisant les occurrences comme poids. Comme nous ne voulons pas rejeter tous les \xE9chantillons qui ne contiennent pas de mots-cl\xE9s, nous ajoutons 1 aux poids :"),Zt=m(),E(le.$$.fragment),Tn=m(),xe=l("p"),Ga=n("Avant de commencer \xE0 entra\xEEner avec cette nouvelle fonction de perte g\xE9niale, nous devons pr\xE9parer quelques \xE9l\xE9ments :"),ut=m(),Oe=l("ul"),Qt=l("li"),en=n("Nous avons besoin de chargeurs de donn\xE9es pour charger les donn\xE9es par batch."),Fa=m(),sn=l("li"),Es=n("Nous devons d\xE9finir les param\xE8tres de d\xE9croissance des poids."),tn=m(),nn=l("li"),Ha=n("De temps en temps, nous voulons \xE9valuer, il est donc logique d\u2019envelopper le code d\u2019\xE9valuation dans une fonction."),an=m(),ne=l("p"),Ia=n("Commen\xE7ons par les chargeurs de donn\xE9es. Nous avons seulement besoin de d\xE9finir le format du jeu de donn\xE9es \xE0 "),ys=l("code"),Ra=n('"torch"'),Va=n(" et ensuite nous pouvons le passer \xE0 un PyTorch "),ws=l("code"),Ua=n("DataLoader"),Ba=n(" avec la taille de batch appropri\xE9e :"),pt=m(),E(xs.$$.fragment),Mn=m(),Xe=l("p"),zs=n("Ensuite, nous regroupons les param\xE8tres de fa\xE7on \xE0 ce que l\u2019optimiseur sache lesquels b\xE9n\xE9ficieront d\u2019une d\xE9croissance de poids suppl\xE9mentaire. Habituellement, tous les termes de biais et les poids de la "),ct=l("em"),dt=n("LayerNorm"),Wa=n(" en sont exempt\xE9s. Voici comment nous pouvons le faire :"),on=m(),E(ze.$$.fragment),rn=m(),je=l("p"),Nn=n("Puisque nous voulons \xE9valuer le mod\xE8le r\xE9guli\xE8rement sur l\u2019ensemble de validation pendant l\u2019entra\xEEnement, \xE9crivons une fonction pour cela aussi. Elle passe simplement par le "),Se=l("em"),Xa=n("dataloader"),ln=n(" d\u2019\xE9valuation et rassemble toutes les pertes \xE0 travers les processus :"),An=m(),E(Ps.$$.fragment),un=m(),me=l("p"),Ln=n("Avec la fonction "),ns=l("code"),Ka=n("evaluate()"),On=n(" nous pouvons rapporter la perte et la "),Ge=l("a"),Sn=n("perplexit\xE9"),Cs=n(" \xE0 intervalles r\xE9guliers. Ensuite, nous red\xE9finissons notre mod\xE8le pour nous assurer que nous entra\xEEnons \xE0 nouveau \xE0 partir de z\xE9ro :"),pn=m(),E(Fe.$$.fragment),Gn=m(),Ds=l("p"),as=n("Nous pouvons ensuite d\xE9finir notre optimiseur, en utilisant la fonction pr\xE9c\xE9dente pour diviser les param\xE8tres de d\xE9croissance des poids :"),Fn=m(),E(Ke.$$.fragment),Hn=m(),mt=l("p"),In=n("Pr\xE9parons maintenant le mod\xE8le, l\u2019optimiseur et les chargeurs de donn\xE9es pour pouvoir commencer l\u2019entra\xEEnement :"),os=m(),E(rs.$$.fragment),ls=m(),E(is.$$.fragment),Je=m(),be=l("p"),ft=n("Maintenant que nous avons envoy\xE9 notre "),cn=l("code"),Rn=n("train_dataloader"),Ts=n(" \xE0 "),_t=l("code"),Ms=n("accelerator.prepare()"),Vn=n(", nous pouvons utiliser sa longueur pour calculer le nombre d\u2019\xE9tapes d\u2019entra\xEEnement. Rappelez-vous que nous devons toujours faire cela apr\xE8s avoir pr\xE9par\xE9 le "),He=l("em"),Ja=n("dataloader"),dn=n(" car cette m\xE9thode modifiera sa longueur. Nous utilisons un programme lin\xE9aire classique du taux d\u2019apprentissage \xE0 0 :"),Un=m(),E(Ns.$$.fragment),mn=m(),se=l("p"),us=n("Enfin, pour pousser notre mod\xE8le vers le "),As=l("em"),Ls=n("Hub"),Ya=n(", nous aurons besoin de cr\xE9er un objet "),Os=l("code"),Za=n("Repository"),Bn=n(" dans un dossier de travail. Tout d\u2019abord, connectez-vous au "),Ye=l("em"),Wn=n("Hub"),ps=n(", si vous n\u2019\xEAtes pas d\xE9j\xE0 connect\xE9. Nous d\xE9terminerons le nom du d\xE9p\xF4t \xE0 partir de l\u2019identifiant du mod\xE8le que nous voulons donner \xE0 notre mod\xE8le (n\u2019h\xE9sitez pas \xE0 remplacer le "),fn=l("code"),_n=n("repo_name"),Qa=n(" par votre propre choix. Il doit juste contenir votre nom d\u2019utilisateur, ce que fait la fonction "),hn=l("code"),Xn=n("get_full_repo_name()"),ie=n(") :"),Kn=m(),E(Ze.$$.fragment),Jn=m(),E(Ss.$$.fragment),ht=m(),vt=l("p"),eo=n("Ensuite, nous pouvons cloner ce d\xE9p\xF4t dans un dossier local. S\u2019il existe d\xE9j\xE0, ce dossier local doit \xEAtre un clone existant du d\xE9p\xF4t avec lequel nous travaillons :"),Gs=m(),E(Fs.$$.fragment),Yn=m(),Ee=l("p"),so=n("Nous pouvons maintenant t\xE9l\xE9charger tout ce que nous sauvegardons dans "),vn=l("code"),gn=n("output_dir"),to=n(" en appelant la m\xE9thode "),bn=l("code"),Zn=n("repo.push_to_hub()"),Qe=n(". Cela nous aidera \xE0 t\xE9l\xE9charger les mod\xE8les interm\xE9diaires \xE0 la fin de chaque \xE9poque."),cs=m(),$n=l("p"),no=n("Avant de nous entra\xEEner, ex\xE9cutons un test rapide pour voir si la fonction d\u2019\xE9valuation fonctionne correctement :"),ds=m(),E(gt.$$.fragment),qn=m(),E(Ie.$$.fragment),Qn=m(),es=l("p"),Hs=n("Ce sont des valeurs tr\xE8s \xE9lev\xE9es pour la perte et la perplexit\xE9, mais ce n\u2019est pas surprenant puisque nous n\u2019avons pas encore entra\xEEn\xE9 le mod\xE8le. Avec cela, nous avons tout pr\xE9par\xE9 pour \xE9crire la partie principale du script d\u2019entra\xEEnement : la boucle d\u2019entra\xEEnement. Dans celle-ci, nous it\xE9rons sur le chargeur de donn\xE9es et transmettons les batchs au mod\xE8le. Avec les logits, nous pouvons alors \xE9valuer notre fonction de perte personnalis\xE9e. Nous mettons \xE0 l\u2019\xE9chelle la perte par le nombre d\u2019\xE9tapes d\u2019accumulation du gradient afin de ne pas cr\xE9er de plus grandes pertes en agr\xE9geant plus d\u2019\xE9tapes. Avant de proc\xE9der \xE0 l\u2019optimisation, nous d\xE9coupons \xE9galement les gradients pour une meilleure convergence. Enfin, tous les quelques pas, nous \xE9valuons le mod\xE8le sur l\u2019ensemble d\u2019\xE9valuation avec notre nouvelle fonction "),bt=l("code"),Is=n("evaluate()"),ea=n(" :"),Pe=m(),E(Rs.$$.fragment),$t=m(),qt=l("p"),ao=n("Et voil\xE0, vous disposez maintenant de votre propre boucle d\u2019entra\xEEnement personnalis\xE9e pour les mod\xE8les de langage causal tels que le GPT-2. Vous pouvez encore l\u2019adapter \xE0 vos besoins."),kt=m(),E(Vs.$$.fragment),sa=m(),E(ms.$$.fragment),this.h()},l(o){r=i(o,"H2",{class:!0});var q=u(r);_=i(q,"A",{id:!0,class:!0,href:!0});var Do=u(_);c=i(Do,"SPAN",{});var To=u(c);y(v.$$.fragment,To),To.forEach(t),Do.forEach(t),k=f(q),$=i(q,"SPAN",{});var kn=u($);z=a(kn,"Entra\xEEner avec \u{1F917} "),D=i(kn,"I",{});var Mo=u(D);j=a(Mo,"Accelerate"),Mo.forEach(t),kn.forEach(t),q.forEach(t),P=f(o),N=i(o,"P",{});var jt=u(N);A=a(jt,"Nous avons vu comment entra\xEEner un mod\xE8le avec le "),h=i(jt,"CODE",{});var jn=u(h);T=a(jn,"Trainer"),jn.forEach(t),H=a(jt,", qui permet une certaine personnalisation. Cependant, parfois nous voulons un contr\xF4le total sur la boucle d\u2019entra\xEEnement ou nous souhaitons faire quelques changements exotiques. Dans ce cas, \u{1F917} "),I=i(jt,"EM",{});var No=u(I);X=a(No,"Accelerate"),No.forEach(t),B=a(jt," est un excellent choix, et dans cette section, nous allons suivre les \xE9tapes pour l\u2019utiliser pour entra\xEEner notre mod\xE8le. Pour rendre les choses plus int\xE9ressantes, nous allons \xE9galement ajouter une touche \xE0 la boucle d\u2019entra\xEEnement."),jt.forEach(t),ee=f(o),y(K.$$.fragment,o),W=f(o),C=i(o,"P",{});var V=u(C);Q=a(V,"Puisque nous sommes principalement int\xE9ress\xE9s par l\u2019autocompl\xE9tion pour les biblioth\xE8ques de science des donn\xE9es, il est logique de donner plus de poids aux \xE9chantillons d\u2019entra\xEEnement qui utilisent davantage ces biblioth\xE8ques. Nous pouvons facilement identifier ces exemples gr\xE2ce \xE0 l\u2019utilisation de mots-cl\xE9s tels que "),U=i(V,"CODE",{});var ta=u(U);F=a(ta,"plt"),ta.forEach(t),G=a(V,", "),R=i(V,"CODE",{});var Ao=u(R);re=a(Ao,"pd"),Ao.forEach(t),_e=a(V,", "),J=i(V,"CODE",{});var Lo=u(J);he=a(Lo,"sk"),Lo.forEach(t),ve=a(V,", "),de=i(V,"CODE",{});var oo=u(de);Y=a(oo,"fit"),oo.forEach(t),ts=a(V,", et "),S=i(V,"CODE",{});var Et=u(S);Z=a(Et,"predict"),Et.forEach(t),nt=a(V,", qui sont les noms d\u2019importation les plus fr\xE9quents pour "),Te=i(V,"CODE",{});var ro=u(Te);gs=a(ro,"matplotlib.pyplot"),ro.forEach(t),at=a(V,", "),ke=i(V,"CODE",{});var yt=u(ke);ot=a(yt,"pandas"),yt.forEach(t),Me=a(V,", et "),Ne=i(V,"CODE",{});var lo=u(Ne);rt=a(lo,"sklearn"),lo.forEach(t),xa=a(V," ainsi que les fonctions "),bs=i(V,"CODE",{});var $e=u(bs);za=a($e,"fit"),$e.forEach(t),Pa=a(V," et "),lt=i(V,"CODE",{});var Oo=u(lt);$s=a(Oo,"predict"),Oo.forEach(t),xn=a(V," de cette derni\xE8re. Si chacun d\u2019entre eux est repr\xE9sent\xE9 par un seul "),ae=i(V,"EM",{});var na=u(ae);Ca=a(na,"token"),na.forEach(t),it=a(V,", nous pouvons facilement v\xE9rifier s\u2019ils apparaissent dans la s\xE9quence d\u2019entr\xE9e. Les "),Lt=i(V,"EM",{});var So=u(Lt);Da=a(So,"tokens"),So.forEach(t),Ot=a(V," peuvent avoir un pr\xE9fixe d\u2019espacement, donc nous v\xE9rifierons aussi ces versions dans le vocabulaire du "),St=i(V,"EM",{});var Go=u(St);Ta=a(Go,"tokenizer"),Go.forEach(t),Gt=a(V,". Pour v\xE9rifier que cela fonctionne, nous ajouterons un "),Ft=i(V,"EM",{});var aa=u(Ft);Ma=a(aa,"token"),aa.forEach(t),Ht=a(V," de test qui devrait \xEAtre divis\xE9 en plusieurs "),It=i(V,"EM",{});var Fo=u(It);Na=a(Fo,"tokens"),Fo.forEach(t),Rt=a(V," :"),V.forEach(t),zn=f(o),y(qs.$$.fragment,o),Vt=f(o),y(oe.$$.fragment,o),Aa=f(o),te=i(o,"P",{});var fe=u(te);ge=a(fe,"Super, \xE7a a l\u2019air de bien fonctionner ! Nous pouvons maintenant \xE9crire une fonction de perte personnalis\xE9e qui prend la s\xE9quence d\u2019entr\xE9e, les logits et les "),Pn=i(fe,"EM",{});var oa=u(Pn);Cn=a(oa,"tokens"),oa.forEach(t),Ae=a(fe," cl\xE9s que nous venons de s\xE9lectionner comme entr\xE9es. Tout d\u2019abord, nous devons aligner les logits et les entr\xE9es : la s\xE9quence d\u2019entr\xE9e d\xE9cal\xE9e d\u2019une unit\xE9 vers la droite forme les \xE9tiquettes, puisque le "),Ut=i(fe,"EM",{});var Ho=u(Ut);Bt=a(Ho,"token"),Ho.forEach(t),La=a(fe," suivant est l\u2019\xE9tiquette du "),Wt=i(fe,"EM",{});var Io=u(Wt);ks=a(Io,"token"),Io.forEach(t),Oa=a(fe," actuel. Nous pouvons y parvenir en commen\xE7ant les \xE9tiquettes \xE0 partir du deuxi\xE8me "),Xt=i(fe,"EM",{});var ra=u(Xt);Dn=a(ra,"token"),ra.forEach(t),We=a(fe," de la s\xE9quence d\u2019entr\xE9e, puisque le mod\xE8le ne fait pas de pr\xE9diction pour le premier "),Le=i(fe,"EM",{});var Ro=u(Le);Kt=a(Ro,"token"),Ro.forEach(t),js=a(fe," de toute fa\xE7on. Ensuite, nous coupons le dernier logit, car nous n\u2019avons pas d\u2019\xE9tiquette pour le "),Jt=i(fe,"EM",{});var Vo=u(Jt);Yt=a(Vo,"token"),Vo.forEach(t),Sa=a(fe," qui suit la s\xE9quence d\u2019entr\xE9e compl\xE8te. Avec cela, nous pouvons calculer la perte par \xE9chantillon et compter les occurrences de tous les mots-cl\xE9s dans chaque \xE9chantillon. Enfin, nous calculons la moyenne pond\xE9r\xE9e sur tous les \xE9chantillons en utilisant les occurrences comme poids. Comme nous ne voulons pas rejeter tous les \xE9chantillons qui ne contiennent pas de mots-cl\xE9s, nous ajoutons 1 aux poids :"),fe.forEach(t),Zt=f(o),y(le.$$.fragment,o),Tn=f(o),xe=i(o,"P",{});var io=u(xe);Ga=a(io,"Avant de commencer \xE0 entra\xEEner avec cette nouvelle fonction de perte g\xE9niale, nous devons pr\xE9parer quelques \xE9l\xE9ments :"),io.forEach(t),ut=f(o),Oe=i(o,"UL",{});var ss=u(Oe);Qt=i(ss,"LI",{});var Uo=u(Qt);en=a(Uo,"Nous avons besoin de chargeurs de donn\xE9es pour charger les donn\xE9es par batch."),Uo.forEach(t),Fa=f(ss),sn=i(ss,"LI",{});var uo=u(sn);Es=a(uo,"Nous devons d\xE9finir les param\xE8tres de d\xE9croissance des poids."),uo.forEach(t),tn=f(ss),nn=i(ss,"LI",{});var Us=u(nn);Ha=a(Us,"De temps en temps, nous voulons \xE9valuer, il est donc logique d\u2019envelopper le code d\u2019\xE9valuation dans une fonction."),Us.forEach(t),ss.forEach(t),an=f(o),ne=i(o,"P",{});var Bs=u(ne);Ia=a(Bs,"Commen\xE7ons par les chargeurs de donn\xE9es. Nous avons seulement besoin de d\xE9finir le format du jeu de donn\xE9es \xE0 "),ys=i(Bs,"CODE",{});var fs=u(ys);Ra=a(fs,'"torch"'),fs.forEach(t),Va=a(Bs," et ensuite nous pouvons le passer \xE0 un PyTorch "),ws=i(Bs,"CODE",{});var Ws=u(ws);Ua=a(Ws,"DataLoader"),Ws.forEach(t),Ba=a(Bs," avec la taille de batch appropri\xE9e :"),Bs.forEach(t),pt=f(o),y(xs.$$.fragment,o),Mn=f(o),Xe=i(o,"P",{});var wt=u(Xe);zs=a(wt,"Ensuite, nous regroupons les param\xE8tres de fa\xE7on \xE0 ce que l\u2019optimiseur sache lesquels b\xE9n\xE9ficieront d\u2019une d\xE9croissance de poids suppl\xE9mentaire. Habituellement, tous les termes de biais et les poids de la "),ct=i(wt,"EM",{});var xt=u(ct);dt=a(xt,"LayerNorm"),xt.forEach(t),Wa=a(wt," en sont exempt\xE9s. Voici comment nous pouvons le faire :"),wt.forEach(t),on=f(o),y(ze.$$.fragment,o),rn=f(o),je=i(o,"P",{});var la=u(je);Nn=a(la,"Puisque nous voulons \xE9valuer le mod\xE8le r\xE9guli\xE8rement sur l\u2019ensemble de validation pendant l\u2019entra\xEEnement, \xE9crivons une fonction pour cela aussi. Elle passe simplement par le "),Se=i(la,"EM",{});var ia=u(Se);Xa=a(ia,"dataloader"),ia.forEach(t),ln=a(la," d\u2019\xE9valuation et rassemble toutes les pertes \xE0 travers les processus :"),la.forEach(t),An=f(o),y(Ps.$$.fragment,o),un=f(o),me=i(o,"P",{});var zt=u(me);Ln=a(zt,"Avec la fonction "),ns=i(zt,"CODE",{});var po=u(ns);Ka=a(po,"evaluate()"),po.forEach(t),On=a(zt," nous pouvons rapporter la perte et la "),Ge=i(zt,"A",{href:!0});var qe=u(Ge);Sn=a(qe,"perplexit\xE9"),qe.forEach(t),Cs=a(zt," \xE0 intervalles r\xE9guliers. Ensuite, nous red\xE9finissons notre mod\xE8le pour nous assurer que nous entra\xEEnons \xE0 nouveau \xE0 partir de z\xE9ro :"),zt.forEach(t),pn=f(o),y(Fe.$$.fragment,o),Gn=f(o),Ds=i(o,"P",{});var Bo=u(Ds);as=a(Bo,"Nous pouvons ensuite d\xE9finir notre optimiseur, en utilisant la fonction pr\xE9c\xE9dente pour diviser les param\xE8tres de d\xE9croissance des poids :"),Bo.forEach(t),Fn=f(o),y(Ke.$$.fragment,o),Hn=f(o),mt=i(o,"P",{});var ua=u(mt);In=a(ua,"Pr\xE9parons maintenant le mod\xE8le, l\u2019optimiseur et les chargeurs de donn\xE9es pour pouvoir commencer l\u2019entra\xEEnement :"),ua.forEach(t),os=f(o),y(rs.$$.fragment,o),ls=f(o),y(is.$$.fragment,o),Je=f(o),be=i(o,"P",{});var _s=u(be);ft=a(_s,"Maintenant que nous avons envoy\xE9 notre "),cn=i(_s,"CODE",{});var Wo=u(cn);Rn=a(Wo,"train_dataloader"),Wo.forEach(t),Ts=a(_s," \xE0 "),_t=i(_s,"CODE",{});var pa=u(_t);Ms=a(pa,"accelerator.prepare()"),pa.forEach(t),Vn=a(_s,", nous pouvons utiliser sa longueur pour calculer le nombre d\u2019\xE9tapes d\u2019entra\xEEnement. Rappelez-vous que nous devons toujours faire cela apr\xE8s avoir pr\xE9par\xE9 le "),He=i(_s,"EM",{});var Xo=u(He);Ja=a(Xo,"dataloader"),Xo.forEach(t),dn=a(_s," car cette m\xE9thode modifiera sa longueur. Nous utilisons un programme lin\xE9aire classique du taux d\u2019apprentissage \xE0 0 :"),_s.forEach(t),Un=f(o),y(Ns.$$.fragment,o),mn=f(o),se=i(o,"P",{});var ye=u(se);us=a(ye,"Enfin, pour pousser notre mod\xE8le vers le "),As=i(ye,"EM",{});var ca=u(As);Ls=a(ca,"Hub"),ca.forEach(t),Ya=a(ye,", nous aurons besoin de cr\xE9er un objet "),Os=i(ye,"CODE",{});var Ko=u(Os);Za=a(Ko,"Repository"),Ko.forEach(t),Bn=a(ye," dans un dossier de travail. Tout d\u2019abord, connectez-vous au "),Ye=i(ye,"EM",{});var Jo=u(Ye);Wn=a(Jo,"Hub"),Jo.forEach(t),ps=a(ye,", si vous n\u2019\xEAtes pas d\xE9j\xE0 connect\xE9. Nous d\xE9terminerons le nom du d\xE9p\xF4t \xE0 partir de l\u2019identifiant du mod\xE8le que nous voulons donner \xE0 notre mod\xE8le (n\u2019h\xE9sitez pas \xE0 remplacer le "),fn=i(ye,"CODE",{});var da=u(fn);_n=a(da,"repo_name"),da.forEach(t),Qa=a(ye," par votre propre choix. Il doit juste contenir votre nom d\u2019utilisateur, ce que fait la fonction "),hn=i(ye,"CODE",{});var Yo=u(hn);Xn=a(Yo,"get_full_repo_name()"),Yo.forEach(t),ie=a(ye,") :"),ye.forEach(t),Kn=f(o),y(Ze.$$.fragment,o),Jn=f(o),y(Ss.$$.fragment,o),ht=f(o),vt=i(o,"P",{});var Zo=u(vt);eo=a(Zo,"Ensuite, nous pouvons cloner ce d\xE9p\xF4t dans un dossier local. S\u2019il existe d\xE9j\xE0, ce dossier local doit \xEAtre un clone existant du d\xE9p\xF4t avec lequel nous travaillons :"),Zo.forEach(t),Gs=f(o),y(Fs.$$.fragment,o),Yn=f(o),Ee=i(o,"P",{});var Xs=u(Ee);so=a(Xs,"Nous pouvons maintenant t\xE9l\xE9charger tout ce que nous sauvegardons dans "),vn=i(Xs,"CODE",{});var Re=u(vn);gn=a(Re,"output_dir"),Re.forEach(t),to=a(Xs," en appelant la m\xE9thode "),bn=i(Xs,"CODE",{});var Ve=u(bn);Zn=a(Ve,"repo.push_to_hub()"),Ve.forEach(t),Qe=a(Xs,". Cela nous aidera \xE0 t\xE9l\xE9charger les mod\xE8les interm\xE9diaires \xE0 la fin de chaque \xE9poque."),Xs.forEach(t),cs=f(o),$n=i(o,"P",{});var En=u($n);no=a(En,"Avant de nous entra\xEEner, ex\xE9cutons un test rapide pour voir si la fonction d\u2019\xE9valuation fonctionne correctement :"),En.forEach(t),ds=f(o),y(gt.$$.fragment,o),qn=f(o),y(Ie.$$.fragment,o),Qn=f(o),es=i(o,"P",{});var Ce=u(es);Hs=a(Ce,"Ce sont des valeurs tr\xE8s \xE9lev\xE9es pour la perte et la perplexit\xE9, mais ce n\u2019est pas surprenant puisque nous n\u2019avons pas encore entra\xEEn\xE9 le mod\xE8le. Avec cela, nous avons tout pr\xE9par\xE9 pour \xE9crire la partie principale du script d\u2019entra\xEEnement : la boucle d\u2019entra\xEEnement. Dans celle-ci, nous it\xE9rons sur le chargeur de donn\xE9es et transmettons les batchs au mod\xE8le. Avec les logits, nous pouvons alors \xE9valuer notre fonction de perte personnalis\xE9e. Nous mettons \xE0 l\u2019\xE9chelle la perte par le nombre d\u2019\xE9tapes d\u2019accumulation du gradient afin de ne pas cr\xE9er de plus grandes pertes en agr\xE9geant plus d\u2019\xE9tapes. Avant de proc\xE9der \xE0 l\u2019optimisation, nous d\xE9coupons \xE9galement les gradients pour une meilleure convergence. Enfin, tous les quelques pas, nous \xE9valuons le mod\xE8le sur l\u2019ensemble d\u2019\xE9valuation avec notre nouvelle fonction "),bt=i(Ce,"CODE",{});var Qo=u(bt);Is=a(Qo,"evaluate()"),Qo.forEach(t),ea=a(Ce," :"),Ce.forEach(t),Pe=f(o),y(Rs.$$.fragment,o),$t=f(o),qt=i(o,"P",{});var ma=u(qt);ao=a(ma,"Et voil\xE0, vous disposez maintenant de votre propre boucle d\u2019entra\xEEnement personnalis\xE9e pour les mod\xE8les de langage causal tels que le GPT-2. Vous pouvez encore l\u2019adapter \xE0 vos besoins."),ma.forEach(t),kt=f(o),y(Vs.$$.fragment,o),sa=f(o),y(ms.$$.fragment,o),this.h()},h(){M(_,"id","entraner-avec-iacceleratei"),M(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(_,"href","#entraner-avec-iacceleratei"),M(r,"class","relative group"),M(Ge,"href","/course/fr/chapter7/3")},m(o,q){p(o,r,q),s(r,_),s(_,c),w(v,c,null),s(r,k),s(r,$),s($,z),s($,D),s(D,j),p(o,P,q),p(o,N,q),s(N,A),s(N,h),s(h,T),s(N,H),s(N,I),s(I,X),s(N,B),p(o,ee,q),w(K,o,q),p(o,W,q),p(o,C,q),s(C,Q),s(C,U),s(U,F),s(C,G),s(C,R),s(R,re),s(C,_e),s(C,J),s(J,he),s(C,ve),s(C,de),s(de,Y),s(C,ts),s(C,S),s(S,Z),s(C,nt),s(C,Te),s(Te,gs),s(C,at),s(C,ke),s(ke,ot),s(C,Me),s(C,Ne),s(Ne,rt),s(C,xa),s(C,bs),s(bs,za),s(C,Pa),s(C,lt),s(lt,$s),s(C,xn),s(C,ae),s(ae,Ca),s(C,it),s(C,Lt),s(Lt,Da),s(C,Ot),s(C,St),s(St,Ta),s(C,Gt),s(C,Ft),s(Ft,Ma),s(C,Ht),s(C,It),s(It,Na),s(C,Rt),p(o,zn,q),w(qs,o,q),p(o,Vt,q),w(oe,o,q),p(o,Aa,q),p(o,te,q),s(te,ge),s(te,Pn),s(Pn,Cn),s(te,Ae),s(te,Ut),s(Ut,Bt),s(te,La),s(te,Wt),s(Wt,ks),s(te,Oa),s(te,Xt),s(Xt,Dn),s(te,We),s(te,Le),s(Le,Kt),s(te,js),s(te,Jt),s(Jt,Yt),s(te,Sa),p(o,Zt,q),w(le,o,q),p(o,Tn,q),p(o,xe,q),s(xe,Ga),p(o,ut,q),p(o,Oe,q),s(Oe,Qt),s(Qt,en),s(Oe,Fa),s(Oe,sn),s(sn,Es),s(Oe,tn),s(Oe,nn),s(nn,Ha),p(o,an,q),p(o,ne,q),s(ne,Ia),s(ne,ys),s(ys,Ra),s(ne,Va),s(ne,ws),s(ws,Ua),s(ne,Ba),p(o,pt,q),w(xs,o,q),p(o,Mn,q),p(o,Xe,q),s(Xe,zs),s(Xe,ct),s(ct,dt),s(Xe,Wa),p(o,on,q),w(ze,o,q),p(o,rn,q),p(o,je,q),s(je,Nn),s(je,Se),s(Se,Xa),s(je,ln),p(o,An,q),w(Ps,o,q),p(o,un,q),p(o,me,q),s(me,Ln),s(me,ns),s(ns,Ka),s(me,On),s(me,Ge),s(Ge,Sn),s(me,Cs),p(o,pn,q),w(Fe,o,q),p(o,Gn,q),p(o,Ds,q),s(Ds,as),p(o,Fn,q),w(Ke,o,q),p(o,Hn,q),p(o,mt,q),s(mt,In),p(o,os,q),w(rs,o,q),p(o,ls,q),w(is,o,q),p(o,Je,q),p(o,be,q),s(be,ft),s(be,cn),s(cn,Rn),s(be,Ts),s(be,_t),s(_t,Ms),s(be,Vn),s(be,He),s(He,Ja),s(be,dn),p(o,Un,q),w(Ns,o,q),p(o,mn,q),p(o,se,q),s(se,us),s(se,As),s(As,Ls),s(se,Ya),s(se,Os),s(Os,Za),s(se,Bn),s(se,Ye),s(Ye,Wn),s(se,ps),s(se,fn),s(fn,_n),s(se,Qa),s(se,hn),s(hn,Xn),s(se,ie),p(o,Kn,q),w(Ze,o,q),p(o,Jn,q),w(Ss,o,q),p(o,ht,q),p(o,vt,q),s(vt,eo),p(o,Gs,q),w(Fs,o,q),p(o,Yn,q),p(o,Ee,q),s(Ee,so),s(Ee,vn),s(vn,gn),s(Ee,to),s(Ee,bn),s(bn,Zn),s(Ee,Qe),p(o,cs,q),p(o,$n,q),s($n,no),p(o,ds,q),w(gt,o,q),p(o,qn,q),w(Ie,o,q),p(o,Qn,q),p(o,es,q),s(es,Hs),s(es,bt),s(bt,Is),s(es,ea),p(o,Pe,q),w(Rs,o,q),p(o,$t,q),p(o,qt,q),s(qt,ao),p(o,kt,q),w(Vs,o,q),p(o,sa,q),w(ms,o,q),ue=!0},i(o){ue||(g(v.$$.fragment,o),g(K.$$.fragment,o),g(qs.$$.fragment,o),g(oe.$$.fragment,o),g(le.$$.fragment,o),g(xs.$$.fragment,o),g(ze.$$.fragment,o),g(Ps.$$.fragment,o),g(Fe.$$.fragment,o),g(Ke.$$.fragment,o),g(rs.$$.fragment,o),g(is.$$.fragment,o),g(Ns.$$.fragment,o),g(Ze.$$.fragment,o),g(Ss.$$.fragment,o),g(Fs.$$.fragment,o),g(gt.$$.fragment,o),g(Ie.$$.fragment,o),g(Rs.$$.fragment,o),g(Vs.$$.fragment,o),g(ms.$$.fragment,o),ue=!0)},o(o){b(v.$$.fragment,o),b(K.$$.fragment,o),b(qs.$$.fragment,o),b(oe.$$.fragment,o),b(le.$$.fragment,o),b(xs.$$.fragment,o),b(ze.$$.fragment,o),b(Ps.$$.fragment,o),b(Fe.$$.fragment,o),b(Ke.$$.fragment,o),b(rs.$$.fragment,o),b(is.$$.fragment,o),b(Ns.$$.fragment,o),b(Ze.$$.fragment,o),b(Ss.$$.fragment,o),b(Fs.$$.fragment,o),b(gt.$$.fragment,o),b(Ie.$$.fragment,o),b(Rs.$$.fragment,o),b(Vs.$$.fragment,o),b(ms.$$.fragment,o),ue=!1},d(o){o&&t(r),x(v),o&&t(P),o&&t(N),o&&t(ee),x(K,o),o&&t(W),o&&t(C),o&&t(zn),x(qs,o),o&&t(Vt),x(oe,o),o&&t(Aa),o&&t(te),o&&t(Zt),x(le,o),o&&t(Tn),o&&t(xe),o&&t(ut),o&&t(Oe),o&&t(an),o&&t(ne),o&&t(pt),x(xs,o),o&&t(Mn),o&&t(Xe),o&&t(on),x(ze,o),o&&t(rn),o&&t(je),o&&t(An),x(Ps,o),o&&t(un),o&&t(me),o&&t(pn),x(Fe,o),o&&t(Gn),o&&t(Ds),o&&t(Fn),x(Ke,o),o&&t(Hn),o&&t(mt),o&&t(os),x(rs,o),o&&t(ls),x(is,o),o&&t(Je),o&&t(be),o&&t(Un),x(Ns,o),o&&t(mn),o&&t(se),o&&t(Kn),x(Ze,o),o&&t(Jn),x(Ss,o),o&&t(ht),o&&t(vt),o&&t(Gs),x(Fs,o),o&&t(Yn),o&&t(Ee),o&&t(cs),o&&t($n),o&&t(ds),x(gt,o),o&&t(qn),x(Ie,o),o&&t(Qn),o&&t(es),o&&t(Pe),x(Rs,o),o&&t($t),o&&t(qt),o&&t(kt),x(Vs,o),o&&t(sa),x(ms,o)}}}function Bp(O){let r,_,c,v,k;return{c(){r=l("p"),_=n("\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code commen\xE7ant \xE0 la cellule ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),c=l("a"),v=n("chapitre 3"),k=n(" pour plus de d\xE9tails."),this.h()},l($){r=i($,"P",{});var z=u(r);_=a(z,"\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code commen\xE7ant \xE0 la cellule ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),c=i(z,"A",{href:!0});var D=u(c);v=a(D,"chapitre 3"),D.forEach(t),k=a(z," pour plus de d\xE9tails."),z.forEach(t),this.h()},h(){M(c,"href","/course/fr/chapter3")},m($,z){p($,r,z),s(r,_),s(r,c),s(c,v),s(r,k)},d($){$&&t(r)}}}function Wp(O){let r,_,c,v,k;return{c(){r=l("p"),_=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez !"),k=n(" Vous pouvez cr\xE9er votre propre fonction de perte personnalis\xE9e, adapt\xE9e \xE0 votre cas d\u2019utilisation, ou ajouter une autre \xE9tape personnalis\xE9e dans la boucle d\u2019entra\xEEnement.")},l($){r=i($,"P",{});var z=u(r);_=a(z,"\u270F\uFE0F "),c=i(z,"STRONG",{});var D=u(c);v=a(D,"Essayez !"),D.forEach(t),k=a(z," Vous pouvez cr\xE9er votre propre fonction de perte personnalis\xE9e, adapt\xE9e \xE0 votre cas d\u2019utilisation, ou ajouter une autre \xE9tape personnalis\xE9e dans la boucle d\u2019entra\xEEnement."),z.forEach(t)},m($,z){p($,r,z),s(r,_),s(r,c),s(c,v),s(r,k)},d($){$&&t(r)}}}function Xp(O){let r,_,c,v,k,$,z,D,j,P,N;return{c(){r=l("p"),_=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez !"),k=n(" Lorsque vous effectuez de longues exp\xE9riences d\u2019entra\xEEnement, il est bon d\u2019enregistrer les mesures importantes \xE0 l\u2019aide d\u2019outils tels que "),$=l("em"),z=n("TensorBoard"),D=n(" ou "),j=l("em"),P=n("Weights & Biases"),N=n(". Ajoutez l\u2019un d\u2019eux \xE0 la boucle d\u2019entra\xEEnement afin de pouvoir toujours v\xE9rifier comment se d\xE9roule l\u2019entra\xEEnement.")},l(A){r=i(A,"P",{});var h=u(r);_=a(h,"\u270F\uFE0F "),c=i(h,"STRONG",{});var T=u(c);v=a(T,"Essayez !"),T.forEach(t),k=a(h," Lorsque vous effectuez de longues exp\xE9riences d\u2019entra\xEEnement, il est bon d\u2019enregistrer les mesures importantes \xE0 l\u2019aide d\u2019outils tels que "),$=i(h,"EM",{});var H=u($);z=a(H,"TensorBoard"),H.forEach(t),D=a(h," ou "),j=i(h,"EM",{});var I=u(j);P=a(I,"Weights & Biases"),I.forEach(t),N=a(h,". Ajoutez l\u2019un d\u2019eux \xE0 la boucle d\u2019entra\xEEnement afin de pouvoir toujours v\xE9rifier comment se d\xE9roule l\u2019entra\xEEnement."),h.forEach(t)},m(A,h){p(A,r,h),s(r,_),s(r,c),s(c,v),s(r,k),s(r,$),s($,z),s(r,D),s(r,j),s(j,P),s(r,N)},d(A){A&&t(r)}}}function Kp(O){let r,_,c,v,k,$,z,D,j,P,N,A,h,T,H,I,X,B,ee,K,W,C,Q,U,F,G,R,re,_e,J,he,ve,de,Y,ts,S,Z,nt,Te,gs,at,ke,ot,Me,Ne,rt,xa,bs,za,Pa,lt,$s,xn,ae,Ca,it,Lt,Da,Ot,St,Ta,Gt,Ft,Ma,Ht,It,Na,Rt,zn,qs,Vt,oe,Aa,te,ge,Pn,Cn,Ae,Ut,Bt,La,Wt,ks,Oa,Xt,Dn,We,Le,Kt,js,Jt,Yt,Sa,Zt,le,Tn,xe,Ga,ut,Oe,Qt,en,Fa,sn,Es,tn,nn,Ha,an,ne,Ia,ys,Ra,Va,ws,Ua,Ba,pt,xs,Mn,Xe,zs,ct,dt,Wa,on,ze,rn,je,Nn,Se,Xa,ln,An,Ps,un,me,Ln,ns,Ka,On,Ge,Sn,Cs,pn,Fe,Gn,Ds,as,Fn,Ke,Hn,mt,In,os,rs,ls,is,Je,be,ft,cn,Rn,Ts,_t,Ms,Vn,He,Ja,dn,Un,Ns,mn,se,us,As,Ls,Ya,Os,Za,Bn,Ye,Wn,ps,fn,_n,Qa,hn,Xn,ie,Kn,Ze,Jn,Ss,ht,vt,eo,Gs,Fs,Yn,Ee,so,vn,gn,to,bn,Zn,Qe,cs,$n,no,ds,gt,qn,Ie,Qn,es,Hs,bt,Is,ea,Pe,Rs,$t,qt,ao,kt,Vs,sa,ms,ue,o,q,Do,To,kn,Mo,jt,jn,No,V,ta,Ao,Lo,oo,Et,ro,yt,lo,$e,Oo,na,So,Go,aa,Fo,fe,oa,Ho,Io,ra,Ro,Vo,io,ss,Uo,uo,Us,Bs,fs,Ws,wt,xt,la,ia,zt,po,qe,Bo,ua,_s,Wo,pa,Xo,ye,ca,Ko,Jo,da,Yo,Zo,Xs,Re,Ve,En,Ce,Qo,ma,Nl,Al,qr,Ll,Ol,Vr,Ue,Sl,kr,Gl,Fl,jr,Hl,Il,Er,Rl,Vl,yr,Ul,Bl,Ur,Ks,Js,er,sr,Wl,Br,co,Wr,Ys,Zs,tr,nr,Xl,Xr,ar,fa,Kr,_a,Kl,wr,Jl,Yl,Jr,mo,Yr,ha,Zl,xr,Ql,ei,Zr,va,si,zr,ti,ni,Qr,fo,el,Qs,et,or,ga,sl,ba,tl,yn,$a,Pr,_o,ai,Cr,oi,nl,qa,ri,Dr,li,ii,al,st,tt,rr,lr,ui,ol,ho,rl,vo,ll,Pt,pi,Tr,ci,di,Mr,mi,fi,il,go,ul,bo,pl,Be,_i,Nr,hi,vi,Ar,gi,bi,Lr,$i,qi,Or,ki,ji,cl,$o,dl,qo,ml,Ct,Ei,Sr,yi,wi,Gr,xi,zi,fl,ko,_l,jo,hl,ir,ur,vl;c=new Ep({props:{fw:O[0]}}),D=new $r({});const Di=[wp,yp],Eo=[];function Ti(e,d){return e[0]==="pt"?0:1}h=Ti(O),T=Eo[h]=Di[h](O),$s=new Ci({props:{id:"Vpjb1lu0MDk"}}),js=new $r({}),zs=new L({props:{code:`def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">any_keyword_in_string</span>(<span class="hljs-params">string, keywords</span>):
    <span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> keywords:
        <span class="hljs-keyword">if</span> keyword <span class="hljs-keyword">in</span> string:
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>`}}),ze=new L({props:{code:`filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`,highlighted:`filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]
example_1 = <span class="hljs-string">&quot;import numpy as np&quot;</span>
example_2 = <span class="hljs-string">&quot;import pandas as pd&quot;</span>

<span class="hljs-built_in">print</span>(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`}}),je=new L({props:{code:"False True",highlighted:'<span class="hljs-literal">False</span> <span class="hljs-literal">True</span>'}}),me=new L({props:{code:`def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_streaming_dataset</span>(<span class="hljs-params">dataset, filters</span>):
    filtered_dict = defaultdict(<span class="hljs-built_in">list</span>)
    total = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">iter</span>(dataset)):
        total += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> any_keyword_in_string(sample[<span class="hljs-string">&quot;content&quot;</span>], filters):
            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sample.items():
                filtered_dict[k].append(v)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">len</span>(filtered_dict[<span class="hljs-string">&#x27;content&#x27;</span>])/total:<span class="hljs-number">.2</span>%}</span> of data after filtering.&quot;</span>)
    <span class="hljs-keyword">return</span> Dataset.from_dict(filtered_dict)`}}),Ge=new L({props:{code:`# Cette cellule prendra beaucoup de temps \xE0 s'ex\xE9cuter, donc vous devriez la sauter et aller \xE0 la suivante !
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)`,highlighted:`<span class="hljs-comment"># Cette cellule prendra beaucoup de temps \xE0 s&#x27;ex\xE9cuter, donc vous devriez la sauter et aller \xE0 la suivante !</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

split = <span class="hljs-string">&quot;train&quot;</span>  <span class="hljs-comment"># &quot;valid&quot;</span>
filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]

data = load_dataset(<span class="hljs-string">f&quot;transformersbook/codeparrot-<span class="hljs-subst">{split}</span>&quot;</span>, split=split, streaming=<span class="hljs-literal">True</span>)
filtered_data = filter_streaming_dataset(data, filters)`}}),Cs=new L({props:{code:"3.26% of data after filtering.",highlighted:'<span class="hljs-number">3.26</span>% of data after filtering.'}}),os=new L({props:{code:`from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="train")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DatasetDict

ds_train = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-train&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
ds_valid = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-valid&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)

raw_datasets = DatasetDict(
    {
        <span class="hljs-string">&quot;train&quot;</span>: ds_train,  <span class="hljs-comment"># .shuffle().select(range(50000)),</span>
        <span class="hljs-string">&quot;valid&quot;</span>: ds_valid,  <span class="hljs-comment"># .shuffle().select(range(500))</span>
    }
)

raw_datasets`}}),ls=new L({props:{code:`DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">606720</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">3322</span>
    })
})`}}),Je=new wa({props:{$$slots:{default:[xp]},$$scope:{ctx:O}}}),Ts=new L({props:{code:`for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")`,highlighted:`<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key.upper()}</span>: <span class="hljs-subst">{raw_datasets[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>][key][:<span class="hljs-number">200</span>]}</span>&quot;</span>)`}}),Ms=new L({props:{code:`'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:\`sklearn.utils\` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''`,highlighted:`<span class="hljs-string">&#x27;REPO_NAME: kmike/scikit-learn&#x27;</span>
<span class="hljs-string">&#x27;PATH: sklearn/utils/__init__.py&#x27;</span>
<span class="hljs-string">&#x27;COPIES: 3&#x27;</span>
<span class="hljs-string">&#x27;SIZE: 10094&#x27;</span>
<span class="hljs-string">&#x27;&#x27;&#x27;CONTENT: &quot;&quot;&quot;
The :mod:\`sklearn.utils\` module includes various utilites.
&quot;&quot;&quot;

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause&#x27;&#x27;&#x27;</span>`}}),Ls=new $r({}),Ye=new Ci({props:{id:"ma1TrR7gE7I"}}),Hs=new L({props:{code:`from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

context_length = <span class="hljs-number">128</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)

outputs = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">2</span>][<span class="hljs-string">&quot;content&quot;</span>],
    truncation=<span class="hljs-literal">True</span>,
    max_length=context_length,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_length=<span class="hljs-literal">True</span>,
)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input IDs length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(outputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input chunk lengths: <span class="hljs-subst">{(outputs[<span class="hljs-string">&#x27;length&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Chunk mapping: <span class="hljs-subst">{outputs[<span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>]}</span>&quot;</span>)`}}),Is=new L({props:{code:`Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`Input IDs length: <span class="hljs-number">34</span>
Input chunk lengths: [<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">117</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">41</span>]
Chunk mapping: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Et=new L({props:{code:`def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">element</span>):
    outputs = tokenizer(
        element[<span class="hljs-string">&quot;content&quot;</span>],
        truncation=<span class="hljs-literal">True</span>,
        max_length=context_length,
        return_overflowing_tokens=<span class="hljs-literal">True</span>,
        return_length=<span class="hljs-literal">True</span>,
    )
    input_batch = []
    <span class="hljs-keyword">for</span> length, input_ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(outputs[<span class="hljs-string">&quot;length&quot;</span>], outputs[<span class="hljs-string">&quot;input_ids&quot;</span>]):
        <span class="hljs-keyword">if</span> length == context_length:
            input_batch.append(input_ids)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;input_ids&quot;</span>: input_batch}


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(
    tokenize, batched=<span class="hljs-literal">True</span>, remove_columns=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names
)
tokenized_datasets`}}),yt=new L({props:{code:`DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">16702061</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">93164</span>
    })
})`}}),Us=new wa({props:{$$slots:{default:[zp]},$$scope:{ctx:O}}}),xt=new $r({});const Mi=[Cp,Pp],yo=[];function Ni(e,d){return e[0]==="pt"?0:1}Re=Ni(O),Ve=yo[Re]=Mi[Re](O);const Ai=[Tp,Dp],wo=[];function Li(e,d){return e[0]==="pt"?0:1}Ks=Li(O),Js=wo[Ks]=Ai[Ks](O),co=new L({props:{code:`out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")`,highlighted:`out = data_collator([tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> out:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span> shape: <span class="hljs-subst">{out[key].shape}</span>&quot;</span>)`}});const Oi=[Np,Mp],xo=[];function Si(e,d){return e[0]==="pt"?0:1}Ys=Si(O),Zs=xo[Ys]=Oi[Ys](O);let pe=O[0]==="tf"&&hp();fa=new wa({props:{warning:!0,$$slots:{default:[Ap]},$$scope:{ctx:O}}}),mo=new L({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),fo=new L({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}});const Gi=[Op,Lp],zo=[];function Fi(e,d){return e[0]==="pt"?0:1}Qs=Fi(O),et=zo[Qs]=Gi[Qs](O),ga=new wa({props:{$$slots:{default:[Sp]},$$scope:{ctx:O}}}),ba=new wa({props:{$$slots:{default:[Hp]},$$scope:{ctx:O}}}),_o=new $r({});const Hi=[Rp,Ip],Po=[];function Ii(e,d){return e[0]==="pt"?0:1}st=Ii(O),tt=Po[st]=Hi[st](O),ho=new L({props:{code:`txt = """\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),vo=new L({props:{code:`# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un nuage de points avec x, y
plt.scatter(x, y)`,highlighted:`<span class="hljs-comment"># cr\xE9er des donn\xE9es</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># cr\xE9er un nuage de points avec x, y</span>
plt.scatter(x, y)`}}),go=new L({props:{code:`txt = """\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),bo=new L({props:{code:`# cr\xE9er des donn\xE9es
x = np.random.randn(100)
y = np.random.randn(100)

# cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for`,highlighted:`<span class="hljs-comment"># cr\xE9er des donn\xE9es</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># cr\xE9er un tableau de donn\xE9es \xE0 partir de x et y</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;x&#x27;</span>: x, <span class="hljs-string">&#x27;y&#x27;</span>: y})
df.insert(<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;x&#x27;</span>, x)
<span class="hljs-keyword">for</span>`}}),$o=new L({props:{code:`txt = """\\
# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({&#x27;profession&#x27;: x, &#x27;income&#x27;:y, &#x27;name&#x27;: z})

# calculer le revenu moyen par profession
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),qo=new L({props:{code:`# tableau de donn\xE9es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession
profession = df.groupby(['profession']).mean()`,highlighted:`<span class="hljs-comment"># tableau de donn\xE9es avec profession, revenu et nom</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;profession&#x27;</span>: x, <span class="hljs-string">&#x27;income&#x27;</span>:y, <span class="hljs-string">&#x27;name&#x27;</span>: z})

<span class="hljs-comment"># calculer le revenu moyen par profession</span>
profession = df.groupby([<span class="hljs-string">&#x27;profession&#x27;</span>]).mean()`}}),ko=new L({props:{code:`txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),jo=new L({props:{code:`# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf`,highlighted:`<span class="hljs-comment"># import random forest regressor from scikit-learn</span>
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor

<span class="hljs-comment"># entra\xEEnement du mod\xE8le de for\xEAt al\xE9atoire avec 300 estimateurs sur X, y :</span>
rf = RandomForestRegressor(n_estimators=<span class="hljs-number">300</span>, random_state=random_state, max_depth=<span class="hljs-number">3</span>)
rf.fit(X, y)
rf`}});function Ri(e,d){return e[0]==="tf"?Up:Vp}let gl=Ri(O),wn=gl(O),ce=O[0]==="pt"&&vp(O);return{c(){r=l("meta"),_=m(),E(c.$$.fragment),v=m(),k=l("h1"),$=l("a"),z=l("span"),E(D.$$.fragment),j=m(),P=l("span"),N=n("Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"),A=m(),T.c(),H=m(),I=l("p"),X=n("Jusqu\u2019\xE0 pr\xE9sent, nous avons surtout r\xE9utilis\xE9 des mod\xE8les pr\xE9-entra\xEEn\xE9s et les avons "),B=l("em"),ee=n("finetun\xE9s"),K=n(" sur de nouveaux cas d\u2019usage. Comme nous l\u2019avons vu dans le "),W=l("a"),C=n("chapitre 1"),Q=n(", ceci est commun\xE9ment appel\xE9 "),U=l("em"),F=n("apprentissage par transfert"),G=n(", et il s\u2019agit d\u2019une strat\xE9gie tr\xE8s efficace pour appliquer les "),R=l("em"),re=n("transformers"),_e=n(" \xE0 la plupart des applications du monde r\xE9el o\xF9 les donn\xE9es \xE9tiquet\xE9es sont rares. Dans ce chapitre, nous allons adopter une approche diff\xE9rente consistant \xE0 entra\xEEner un mod\xE8le compl\xE8tement nouveau \xE0 partir de z\xE9ro. C\u2019est une bonne d\xE9marche \xE0 adopter si vous avez beaucoup de donn\xE9es et qu\u2019elles sont tr\xE8s diff\xE9rentes des donn\xE9es de pr\xE9-entra\xEEnement utilis\xE9es par les mod\xE8les disponibles. Cependant, le pr\xE9-entra\xEEnement d\u2019un mod\xE8le de langue n\xE9cessite beaucoup plus de ressources informatiques que le simple "),J=l("em"),he=n("finetuning"),ve=n(" d\u2019un mod\xE8le existant. Parmi les exemples o\xF9 il peut \xEAtre utile d\u2019entra\xEEner un nouveau mod\xE8le, citons les jeux de donn\xE9es constitu\xE9s de notes de musique, de s\xE9quences mol\xE9culaires telles que l\u2019ADN, ou de langages de programmation. Ces derniers ont r\xE9cemment gagn\xE9 en popularit\xE9 gr\xE2ce \xE0 des outils tels que TabNine et Copilot de GitHub (aliment\xE9s par le mod\xE8le Codex d\u2019OpenAI) qui peuvent g\xE9n\xE9rer de longues s\xE9quences de code. Cette t\xE2che de g\xE9n\xE9ration de texte est mieux abord\xE9e avec des mod\xE8les de langage autor\xE9gressifs ou causaux tels que le GPT-2."),de=m(),Y=l("p"),ts=n("Dans cette section, nous allons construire une version r\xE9duite d\u2019un mod\xE8le de g\xE9n\xE9ration de code Python. Nous nous concentrerons sur la compl\xE9tion d\u2019une ligne de code au lieu de fonctions ou de classes compl\xE8tes. Lorsque vous travaillez sur des projets de science des donn\xE9es en Python, vous \xEAtes souvent en contact avec les biblioth\xE8ques "),S=l("code"),Z=n("matplotlib"),nt=n(", "),Te=l("code"),gs=n("seaborn"),at=n(", "),ke=l("code"),ot=n("pandas"),Me=n(" et "),Ne=l("code"),rt=n("scikit-learn"),xa=n(". Lors de l\u2019utilisation de ces "),bs=l("em"),za=n("frameworks"),Pa=n(", il est fr\xE9quent d\u2019avoir besoin de rechercher des commandes sp\xE9cifiques. Il serait donc bien d\u2019utiliser un mod\xE8le pour compl\xE9ter ces appels pour nous."),lt=m(),E($s.$$.fragment),xn=m(),ae=l("p"),Ca=n("Dans le "),it=l("a"),Lt=n("chapitre 6"),Da=n(", nous avons cr\xE9\xE9 un "),Ot=l("em"),St=n("tokenizer"),Ta=n(" efficace pour traiter du code Python. Nous avons besoin d\u2019un jeu de donn\xE9es \xE0 grande \xE9chelle pour pr\xE9-entra\xEEner un mod\xE8le. Ici, nous allons appliquer notre "),Gt=l("em"),Ft=n("tokenizer"),Ma=n(" \xE0 un corpus de code Python provenant des d\xE9p\xF4ts GitHub. Nous utiliserons ensuite l\u2019API "),Ht=l("code"),It=n("Trainer"),Na=n(" et \u{1F917} "),Rt=l("em"),zn=n("Accelerate"),qs=n(" pour entra\xEEner le mod\xE8le. C\u2019est parti !"),Vt=m(),oe=l("iframe"),te=m(),ge=l("iframe"),Cn=m(),Ae=l("p"),Ut=n("Il s\u2019agit d\u2019une pr\xE9sentation du mod\xE8le qui a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide du code pr\xE9sent\xE9 dans cette section et qui a ensuit\xE9 \xE9t\xE9 t\xE9l\xE9charg\xE9 sur le "),Bt=l("em"),La=n("Hub"),Wt=n(". Vous pouvez le trouver "),ks=l("a"),Oa=n("ici"),Xt=n(". Notez qu\u2019\xE9tant donn\xE9 qu\u2019il y a un certains al\xE9at dans la g\xE9n\xE9ration du texte, vous obtiendrez probablement un r\xE9sultat l\xE9g\xE8rement diff\xE9rent."),Dn=m(),We=l("h2"),Le=l("a"),Kt=l("span"),E(js.$$.fragment),Jt=m(),Yt=l("span"),Sa=n("Collecte des donn\xE9es"),Zt=m(),le=l("p"),Tn=n("On peut trouver du code Python en abondance dans les d\xE9p\xF4ts de code tels que GitHub, que nous pouvons utiliser pour cr\xE9er un jeu de donn\xE9es en r\xE9cup\xE9rant chaque d\xE9p\xF4t Python. C\u2019est l\u2019approche adopt\xE9e dans le "),xe=l("a"),Ga=n("livre "),ut=l("em"),Oe=n("Natural Language Processing with Transformers"),Qt=n(" pour pr\xE9-entra\xEEner un grand GPT-2. En utilisant un d\xE9p\xF4t GitHub d\u2019environ 180 Go contenant approximativement 20 millions de fichiers Python, les auteurs du livre ont construit un jeu de donn\xE9es appel\xE9 "),en=l("code"),Fa=n("codeparrot"),sn=n(" qu\u2019ils ont ensuite partag\xE9 sur le "),Es=l("a"),tn=l("em"),nn=n("Hub"),Ha=n("."),an=m(),ne=l("p"),Ia=n("Cependant, entra\xEEner sur l\u2019ensemble du corpus prend beaucoup de temps et demande beaucoup de ressources de calculs. Dans notre cas, nous n\u2019avons besoin que du sous-ensemble du jeu de donn\xE9es qui est relatif aux codes portant sur la science des donn\xE9es. Commen\xE7ons donc par filtrer le jeu de donn\xE9es "),ys=l("code"),Ra=n("codeparrot"),Va=n(" en ne gardant que les fichiers incluant l\u2019une des biblioth\xE8ques de science des donn\xE9es \xE9num\xE9r\xE9es pr\xE9c\xE9demment. En raison de la taille du jeu de donn\xE9es, nous voulons \xE9viter de le t\xE9l\xE9charger. Nous utiliserons donc la fonctionnalit\xE9 de "),ws=l("em"),Ua=n("streaming"),Ba=n(" de \u{1F917} "),pt=l("em"),xs=n("Datasets"),Mn=n(" afin de le filtrer \xE0 la vol\xE9e. Pour nous aider \xE0 filtrer les \xE9chantillons de code utilisant les biblioth\xE8ques que nous avons mentionn\xE9es pr\xE9c\xE9demment, nous utilisons la fonction suivante :"),Xe=m(),E(zs.$$.fragment),ct=m(),dt=l("p"),Wa=n("Testons-le sur deux exemples :"),on=m(),E(ze.$$.fragment),rn=m(),E(je.$$.fragment),Nn=m(),Se=l("p"),Xa=n("Nous pouvons l\u2019utiliser pour cr\xE9er une fonction qui va "),ln=l("em"),An=n("streamer"),Ps=n(" le jeu de donner et filtrer les \xE9l\xE9ments que nous voulons :"),un=m(),E(me.$$.fragment),Ln=m(),ns=l("p"),Ka=n("Ensuite, nous pouvons simplement appliquer cette fonction :"),On=m(),E(Ge.$$.fragment),Sn=m(),E(Cs.$$.fragment),pn=m(),Fe=l("p"),Gn=n("Cela nous laisse avec environ 3 % du jeu de donn\xE9es original, ce qui est tout de m\xEAme assez important puisqu\u2019il fait 6 Go et se compose de 600 000 scripts Python !"),Ds=m(),as=l("p"),Fn=n("Le filtrage peut prendre de 2 \xE0 3 heures, selon votre machine et votre bande passante. Si vous ne voulez pas passer par ce long processus, nous fournissons sur le "),Ke=l("em"),Hn=n("Hub"),mt=n(" le jeu de donn\xE9es filtr\xE9 pour que vous puissiez le t\xE9l\xE9charger :"),In=m(),E(os.$$.fragment),rs=m(),E(ls.$$.fragment),is=m(),E(Je.$$.fragment),be=m(),ft=l("p"),cn=n("Examinons un exemple tir\xE9 du jeu de donn\xE9es. Nous ne montrerons que les 200 premiers caract\xE8res de chaque champ :"),Rn=m(),E(Ts.$$.fragment),_t=m(),E(Ms.$$.fragment),Vn=m(),He=l("p"),Ja=n("Nous pouvons voir que le champ "),dn=l("code"),Un=n("content"),Ns=n(" contient le code sur lequel nous voulons que notre mod\xE8le s\u2019entra\xEEne. Maintenant que nous avons un jeu de donn\xE9es, nous devons pr\xE9parer les textes afin qu\u2019ils soient dans un format appropri\xE9 pour le pr\xE9-entra\xEEnement."),mn=m(),se=l("h2"),us=l("a"),As=l("span"),E(Ls.$$.fragment),Ya=m(),Os=l("span"),Za=n("Pr\xE9paration du jeu de donn\xE9es"),Bn=m(),E(Ye.$$.fragment),Wn=m(),ps=l("p"),fn=n("La premi\xE8re \xE9tape est de tokeniser les donn\xE9es afin de pouvoir les utiliser pour l\u2019entra\xEEnement. Puisque notre objectif est d\u2019autocompl\xE9ter de courts appels de fonctions, nous pouvons garder la taille du contexte relativement petite. L\u2019avantage est que nous pouvons entra\xEEner le mod\xE8le beaucoup plus rapidement et qu\u2019il n\xE9cessite beaucoup moins de m\xE9moire. Si c\u2019est important pour votre application d\u2019avoir davantage de contexte (par exemple, si vous voulez que le mod\xE8le \xE9crive des tests unitaires bas\xE9s sur un fichier avec la d\xE9finition de la fonction), assurez-vous d\u2019augmenter ce nombre. Gardez n\xE9anmoins \xE0 l\u2019esprit que cela s\u2019accompagne d\u2019une plus grande empreinte m\xE9moire du GPU. Pour l\u2019instant, fixons la taille du contexte \xE0 128 "),_n=l("em"),Qa=n("tokens"),hn=n(", par opposition aux 1 024 ou 2 048 utilis\xE9s respectivement dans le GPT-2 et le GPT-3."),Xn=m(),ie=l("p"),Kn=n("La plupart des documents contiennent beaucoup plus de 128 "),Ze=l("em"),Jn=n("tokens"),Ss=n(", donc le fait de tronquer les entr\xE9es \xE0 la longueur maximale \xE9liminerait une grande partie de notre jeu de donn\xE9es. A la place, nous allons utiliser l\u2019option "),ht=l("code"),vt=n("return_overflowing_tokens"),eo=n(" pour tokeniser l\u2019entr\xE9e enti\xE8re et la diviser en plusieurs morceaux, comme nous l\u2019avons fait dans le "),Gs=l("a"),Fs=n("chapitre 6"),Yn=n(". Nous utiliserons \xE9galement l\u2019option "),Ee=l("code"),so=n("return_length"),vn=n(" pour retourner automatiquement la longueur de chaque morceau cr\xE9\xE9. Souvent, le dernier morceau est plus petit que la taille du contexte et nous nous en d\xE9barrasserons pour \xE9viter les probl\xE8mes de "),gn=l("em"),to=n("padding"),bn=n(". Nous n\u2019en avons pas vraiment besoin puisque de toute fa\xE7on nous avons beaucoup de donn\xE9es."),Zn=m(),Qe=l("div"),cs=l("img"),no=m(),ds=l("img"),qn=m(),Ie=l("p"),Qn=n("Voyons comment cela fonctionne en examinant les deux premiers exemples :"),es=m(),E(Hs.$$.fragment),bt=m(),E(Is.$$.fragment),ea=m(),Pe=l("p"),Rs=n("Nous pouvons voir que nous obtenons 34 morceaux \xE0 partir de ces deux exemples. En regardant leurs longueurs, nous pouvons voir qu\u2019ils se terminent avec moins de 128 "),$t=l("em"),qt=n("tokens"),ao=n(" (117 et 41, respectivement). Ils ne repr\xE9sentent qu\u2019une petite fraction du total des morceaux que nous avons (2/34), donc nous pouvons les jeter sans risque. Avec le champ "),kt=l("code"),Vs=n("overflow_to_sample_mapping"),sa=n(", nous pouvons aussi reconstruire quels morceaux appartenaient \xE0 quels \xE9chantillons d\u2019entr\xE9e."),ms=m(),ue=l("p"),o=n("Avec cette op\xE9ration, nous utilisons une fonctionnalit\xE9 pratique de la fonction "),q=l("code"),Do=n("Dataset.map()"),To=n(" de \u{1F917} "),kn=l("em"),Mo=n("Datasets"),jt=n(". En effet, celle-ci ne n\xE9cessite pas une correspondance un \xE0 un comme nous l\u2019avons vu dans la "),jn=l("a"),No=n("section 3"),V=n(". Nous pouvons cr\xE9er des batchs avec plus ou moins d\u2019\xE9l\xE9ments que le batch d\u2019entr\xE9e. C\u2019est utile lorsque l\u2019on effectue des op\xE9rations telles que l\u2019augmentation ou le filtrage des donn\xE9es qui modifient le nombre d\u2019\xE9l\xE9ments. Dans notre cas, lors de la tokenisation de chaque \xE9l\xE9ment en morceaux de longeur de la taille de contexte sp\xE9cifi\xE9e, nous cr\xE9ons de nombreux \xE9chantillons de chaque document. Nous devons juste nous assurer de supprimer les colonnes existantes, car elles ont une taille conflictuelle. Si nous voulions les garder, nous pourrions les r\xE9p\xE9ter de mani\xE8re appropri\xE9e et les retourner dans l\u2019appel "),ta=l("code"),Ao=n("Dataset.map()"),Lo=n(" :"),oo=m(),E(Et.$$.fragment),ro=m(),E(yt.$$.fragment),lo=m(),$e=l("p"),Oo=n("Nous avons maintenant 16,7 millions d\u2019exemples avec 128 "),na=l("em"),So=n("tokens"),Go=n(" chacun, ce qui correspond \xE0 environ 2,1 milliards de "),aa=l("em"),Fo=n("tokens"),fe=n(" au total. A titre de comparaison, les mod\xE8les GPT-3 et Codex d\u2019OpenAI sont entra\xEEn\xE9s sur 300 et 100 milliards de "),oa=l("em"),Ho=n("tokens"),Io=n(", respectivement. Les mod\xE8les Codex \xE9tant initialis\xE9s \xE0 partir des "),ra=l("em"),Ro=n("checkpoints"),Vo=n(" GPT-3. Notre objectif dans cette section n\u2019est pas de rivaliser avec ces mod\xE8les, qui peuvent g\xE9n\xE9rer des textes longs et coh\xE9rents, mais de cr\xE9er une version r\xE9duite fournissant une fonction d\u2019autocompl\xE9tion rapide."),io=m(),ss=l("p"),Uo=n("Maintenant que le jeu de donn\xE9es est pr\xEAt, configurons le mod\xE8le !"),uo=m(),E(Us.$$.fragment),Bs=m(),fs=l("h2"),Ws=l("a"),wt=l("span"),E(xt.$$.fragment),la=m(),ia=l("span"),zt=n("Initialisation d'un nouveau mod\xE8le"),po=m(),qe=l("p"),Bo=n("Notre premi\xE8re \xE9tape consiste \xE0 initialiser un GPT-2. Pour notre mod\xE8le, nous utiliserons la m\xEAme configuration que pour le petit mod\xE8le GPT-2. Ainsi nous chargeons la configuration pr\xE9-entra\xEEn\xE9e, nous nous assurons que la taille du "),ua=l("em"),_s=n("tokenizer"),Wo=n(" correspond \xE0 la taille du vocabulaire du mod\xE8le et nous passons les identifiants des "),pa=l("em"),Xo=n("tokens"),ye=m(),ca=l("code"),Ko=n("bos"),Jo=n(" et "),da=l("code"),Yo=n("eos"),Zo=n(" (d\xE9but et fin de s\xE9quence) :"),Xs=m(),Ve.c(),En=m(),Ce=l("p"),Qo=n("Notre mod\xE8le comporte 124 millions de param\xE8tres que nous devrons r\xE9gler. Avant de commencer l\u2019entra\xEEnement, nous devons configurer un collateur de donn\xE9es qui se chargera de cr\xE9er les batchs. Nous pouvons utiliser le collateur "),ma=l("code"),Nl=n("DataCollatorForLanguageModeling"),Al=n(", qui est con\xE7u sp\xE9cifiquement pour la mod\xE9lisation du langage (comme son nom le sugg\xE8re subtilement). En plus de l\u2019empilage et du rembourrage des batchs, il s\u2019occupe aussi de la cr\xE9ation des \xE9tiquettes du mod\xE8le de langage. Dans la mod\xE9lisation causale du langage, les entr\xE9es servent aussi d\u2019\xE9tiquettes (juste d\xE9cal\xE9es d\u2019un \xE9l\xE9ment) et que le collateur de donn\xE9es cr\xE9e \xE0 la vol\xE9e pendant l\u2019entra\xEEnement pour ne pas avoir \xE0 dupliquer les "),qr=l("code"),Ll=n("input_ids"),Ol=n("."),Vr=m(),Ue=l("p"),Sl=n("Notez que "),kr=l("code"),Gl=n("DataCollatorForLanguageModeling"),Fl=n(" supporte \xE0 la fois la mod\xE9lisation du langage masqu\xE9 (MLM pour "),jr=l("em"),Hl=n("masked language modeling"),Il=n(") et la mod\xE9lisation du langage causal (CLM pour "),Er=l("em"),Rl=n("causal language modeling"),Vl=n("). Par d\xE9faut, il pr\xE9pare les donn\xE9es pour la MLM mais nous pouvons passer \xE0 la CLM en d\xE9finissant l\u2019argument "),yr=l("code"),Ul=n("mlm=False"),Bl=n(" :"),Ur=m(),Js.c(),er=m(),sr=l("p"),Wl=n("Prenons un exemple :"),Br=m(),E(co.$$.fragment),Wr=m(),Zs.c(),tr=m(),nr=l("p"),Xl=n("Nous pouvons voir que les exemples ont \xE9t\xE9 empil\xE9s et que tous les tenseurs ont la m\xEAme forme."),Xr=m(),pe&&pe.c(),ar=m(),E(fa.$$.fragment),Kr=m(),_a=l("p"),Kl=n("Nous avons maintenant tout ce qu\u2019il faut pour entra\xEEner notre mod\xE8le. Ce n\u2019\xE9tait pas si compliqu\xE9 ! Avant de commencer l\u2019entra\xEEnement, nous devons nous connecter \xE0 Hugging Face. Si vous travaillez dans un "),wr=l("em"),Jl=n("notebook"),Yl=n(", vous pouvez le faire avec la fonction utilitaire suivante :"),Jr=m(),E(mo.$$.fragment),Yr=m(),ha=l("p"),Zl=n("Cela affichera un "),xr=l("em"),Ql=n("widget"),ei=n(" o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face."),Zr=m(),va=l("p"),si=n("Si vous ne travaillez pas dans un "),zr=l("em"),ti=n("notebook"),ni=n(", tapez simplement la ligne suivante dans votre terminal :"),Qr=m(),E(fo.$$.fragment),el=m(),et.c(),or=m(),E(ga.$$.fragment),sl=m(),E(ba.$$.fragment),tl=m(),yn=l("h2"),$a=l("a"),Pr=l("span"),E(_o.$$.fragment),ai=m(),Cr=l("span"),oi=n("G\xE9n\xE9ration de code avec le pipeline"),nl=m(),qa=l("p"),ri=n("C\u2019est maintenant le moment de v\xE9rit\xE9 : voyons comment le mod\xE8le entra\xEEn\xE9 fonctionne r\xE9ellement ! Nous pouvons voir dans les logs que la perte a diminu\xE9 r\xE9guli\xE8rement, mais pour mettre le mod\xE8le \xE0 l\u2019\xE9preuve, regardons comment il fonctionne sur certains messages. Pour ce faire, nous allons envelopper le mod\xE8le dans un "),Dr=l("code"),li=n("pipeline"),ii=n(" de g\xE9n\xE9ration de texte et, s\u2019il y en a un de disponible, utiliser un GPU pour avoir des g\xE9n\xE9rations rapidement :"),al=m(),tt.c(),rr=m(),lr=l("p"),ui=n("Let\u2019s start with the simple task of creating a scatter plot:"),ol=m(),E(ho.$$.fragment),rl=m(),E(vo.$$.fragment),ll=m(),Pt=l("p"),pi=n("Le r\xE9sultat semble correct. Est-ce que cela fonctionne aussi pour une op\xE9ration "),Tr=l("code"),ci=n("pandas"),di=n(" ? Voyons si nous pouvons cr\xE9er un "),Mr=l("code"),mi=n("DataFrame"),fi=n(" \xE0 partir de deux tableaux :"),il=m(),E(go.$$.fragment),ul=m(),E(bo.$$.fragment),pl=m(),Be=l("p"),_i=n("Bien, c\u2019est la bonne r\xE9ponse. Bien qu\u2019il ins\xE8re ensuite la colonne "),Nr=l("code"),hi=n("x"),vi=n(" \xE0 nouveau. Comme le nombre de "),Ar=l("em"),gi=n("tokens"),bi=n(" g\xE9n\xE9r\xE9s est limit\xE9, la boucle "),Lr=l("code"),$i=n("for"),qi=n(" suivante est coup\xE9e. Voyons si nous pouvons faire quelque chose d\u2019un peu plus complexe et faire en sorte que le mod\xE8le nous aide \xE0 utiliser l\u2019op\xE9ration "),Or=l("code"),ki=n("groupby"),ji=n(" :"),cl=m(),E($o.$$.fragment),dl=m(),E(qo.$$.fragment),ml=m(),Ct=l("p"),Ei=n("Pas mal, c\u2019est la bonne fa\xE7on de faire. Enfin, voyons si nous pouvons aussi l\u2019utiliser pour "),Sr=l("code"),yi=n("scikit-learn"),wi=n(" et utiliser un mod\xE8le "),Gr=l("em"),xi=n("Random Forest"),zi=n(" :"),fl=m(),E(ko.$$.fragment),_l=m(),E(jo.$$.fragment),hl=m(),wn.c(),ir=m(),ce&&ce.c(),ur=Ml(),this.h()},l(e){const d=kp('[data-svelte="svelte-1phssyn"]',document.head);r=i(d,"META",{name:!0,content:!0}),d.forEach(t),_=f(e),y(c.$$.fragment,e),v=f(e),k=i(e,"H1",{class:!0});var Co=u(k);$=i(Co,"A",{id:!0,class:!0,href:!0});var pr=u($);z=i(pr,"SPAN",{});var Fr=u(z);y(D.$$.fragment,Fr),Fr.forEach(t),pr.forEach(t),j=f(Co),P=i(Co,"SPAN",{});var Hr=u(P);N=a(Hr,"Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"),Hr.forEach(t),Co.forEach(t),A=f(e),T.l(e),H=f(e),I=i(e,"P",{});var we=u(I);X=a(we,"Jusqu\u2019\xE0 pr\xE9sent, nous avons surtout r\xE9utilis\xE9 des mod\xE8les pr\xE9-entra\xEEn\xE9s et les avons "),B=i(we,"EM",{});var cr=u(B);ee=a(cr,"finetun\xE9s"),cr.forEach(t),K=a(we," sur de nouveaux cas d\u2019usage. Comme nous l\u2019avons vu dans le "),W=i(we,"A",{href:!0});var dr=u(W);C=a(dr,"chapitre 1"),dr.forEach(t),Q=a(we,", ceci est commun\xE9ment appel\xE9 "),U=i(we,"EM",{});var Ir=u(U);F=a(Ir,"apprentissage par transfert"),Ir.forEach(t),G=a(we,", et il s\u2019agit d\u2019une strat\xE9gie tr\xE8s efficace pour appliquer les "),R=i(we,"EM",{});var mr=u(R);re=a(mr,"transformers"),mr.forEach(t),_e=a(we," \xE0 la plupart des applications du monde r\xE9el o\xF9 les donn\xE9es \xE9tiquet\xE9es sont rares. Dans ce chapitre, nous allons adopter une approche diff\xE9rente consistant \xE0 entra\xEEner un mod\xE8le compl\xE8tement nouveau \xE0 partir de z\xE9ro. C\u2019est une bonne d\xE9marche \xE0 adopter si vous avez beaucoup de donn\xE9es et qu\u2019elles sont tr\xE8s diff\xE9rentes des donn\xE9es de pr\xE9-entra\xEEnement utilis\xE9es par les mod\xE8les disponibles. Cependant, le pr\xE9-entra\xEEnement d\u2019un mod\xE8le de langue n\xE9cessite beaucoup plus de ressources informatiques que le simple "),J=i(we,"EM",{});var Rr=u(J);he=a(Rr,"finetuning"),Rr.forEach(t),ve=a(we," d\u2019un mod\xE8le existant. Parmi les exemples o\xF9 il peut \xEAtre utile d\u2019entra\xEEner un nouveau mod\xE8le, citons les jeux de donn\xE9es constitu\xE9s de notes de musique, de s\xE9quences mol\xE9culaires telles que l\u2019ADN, ou de langages de programmation. Ces derniers ont r\xE9cemment gagn\xE9 en popularit\xE9 gr\xE2ce \xE0 des outils tels que TabNine et Copilot de GitHub (aliment\xE9s par le mod\xE8le Codex d\u2019OpenAI) qui peuvent g\xE9n\xE9rer de longues s\xE9quences de code. Cette t\xE2che de g\xE9n\xE9ration de texte est mieux abord\xE9e avec des mod\xE8les de langage autor\xE9gressifs ou causaux tels que le GPT-2."),we.forEach(t),de=f(e),Y=i(e,"P",{});var De=u(Y);ts=a(De,"Dans cette section, nous allons construire une version r\xE9duite d\u2019un mod\xE8le de g\xE9n\xE9ration de code Python. Nous nous concentrerons sur la compl\xE9tion d\u2019une ligne de code au lieu de fonctions ou de classes compl\xE8tes. Lorsque vous travaillez sur des projets de science des donn\xE9es en Python, vous \xEAtes souvent en contact avec les biblioth\xE8ques "),S=i(De,"CODE",{});var fr=u(S);Z=a(fr,"matplotlib"),fr.forEach(t),nt=a(De,", "),Te=i(De,"CODE",{});var Vi=u(Te);gs=a(Vi,"seaborn"),Vi.forEach(t),at=a(De,", "),ke=i(De,"CODE",{});var Ui=u(ke);ot=a(Ui,"pandas"),Ui.forEach(t),Me=a(De," et "),Ne=i(De,"CODE",{});var Bi=u(Ne);rt=a(Bi,"scikit-learn"),Bi.forEach(t),xa=a(De,". Lors de l\u2019utilisation de ces "),bs=i(De,"EM",{});var Wi=u(bs);za=a(Wi,"frameworks"),Wi.forEach(t),Pa=a(De,", il est fr\xE9quent d\u2019avoir besoin de rechercher des commandes sp\xE9cifiques. Il serait donc bien d\u2019utiliser un mod\xE8le pour compl\xE9ter ces appels pour nous."),De.forEach(t),lt=f(e),y($s.$$.fragment,e),xn=f(e),ae=i(e,"P",{});var hs=u(ae);Ca=a(hs,"Dans le "),it=i(hs,"A",{href:!0});var Xi=u(it);Lt=a(Xi,"chapitre 6"),Xi.forEach(t),Da=a(hs,", nous avons cr\xE9\xE9 un "),Ot=i(hs,"EM",{});var Ki=u(Ot);St=a(Ki,"tokenizer"),Ki.forEach(t),Ta=a(hs," efficace pour traiter du code Python. Nous avons besoin d\u2019un jeu de donn\xE9es \xE0 grande \xE9chelle pour pr\xE9-entra\xEEner un mod\xE8le. Ici, nous allons appliquer notre "),Gt=i(hs,"EM",{});var Ji=u(Gt);Ft=a(Ji,"tokenizer"),Ji.forEach(t),Ma=a(hs," \xE0 un corpus de code Python provenant des d\xE9p\xF4ts GitHub. Nous utiliserons ensuite l\u2019API "),Ht=i(hs,"CODE",{});var Yi=u(Ht);It=a(Yi,"Trainer"),Yi.forEach(t),Na=a(hs," et \u{1F917} "),Rt=i(hs,"EM",{});var Zi=u(Rt);zn=a(Zi,"Accelerate"),Zi.forEach(t),qs=a(hs," pour entra\xEEner le mod\xE8le. C\u2019est parti !"),hs.forEach(t),Vt=f(e),oe=i(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),u(oe).forEach(t),te=f(e),ge=i(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),u(ge).forEach(t),Cn=f(e),Ae=i(e,"P",{});var _r=u(Ae);Ut=a(_r,"Il s\u2019agit d\u2019une pr\xE9sentation du mod\xE8le qui a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide du code pr\xE9sent\xE9 dans cette section et qui a ensuit\xE9 \xE9t\xE9 t\xE9l\xE9charg\xE9 sur le "),Bt=i(_r,"EM",{});var Qi=u(Bt);La=a(Qi,"Hub"),Qi.forEach(t),Wt=a(_r,". Vous pouvez le trouver "),ks=i(_r,"A",{href:!0,rel:!0});var eu=u(ks);Oa=a(eu,"ici"),eu.forEach(t),Xt=a(_r,". Notez qu\u2019\xE9tant donn\xE9 qu\u2019il y a un certains al\xE9at dans la g\xE9n\xE9ration du texte, vous obtiendrez probablement un r\xE9sultat l\xE9g\xE8rement diff\xE9rent."),_r.forEach(t),Dn=f(e),We=i(e,"H2",{class:!0});var bl=u(We);Le=i(bl,"A",{id:!0,class:!0,href:!0});var su=u(Le);Kt=i(su,"SPAN",{});var tu=u(Kt);y(js.$$.fragment,tu),tu.forEach(t),su.forEach(t),Jt=f(bl),Yt=i(bl,"SPAN",{});var nu=u(Yt);Sa=a(nu,"Collecte des donn\xE9es"),nu.forEach(t),bl.forEach(t),Zt=f(e),le=i(e,"P",{});var ka=u(le);Tn=a(ka,"On peut trouver du code Python en abondance dans les d\xE9p\xF4ts de code tels que GitHub, que nous pouvons utiliser pour cr\xE9er un jeu de donn\xE9es en r\xE9cup\xE9rant chaque d\xE9p\xF4t Python. C\u2019est l\u2019approche adopt\xE9e dans le "),xe=i(ka,"A",{href:!0,rel:!0});var Pi=u(xe);Ga=a(Pi,"livre "),ut=i(Pi,"EM",{});var au=u(ut);Oe=a(au,"Natural Language Processing with Transformers"),au.forEach(t),Pi.forEach(t),Qt=a(ka," pour pr\xE9-entra\xEEner un grand GPT-2. En utilisant un d\xE9p\xF4t GitHub d\u2019environ 180 Go contenant approximativement 20 millions de fichiers Python, les auteurs du livre ont construit un jeu de donn\xE9es appel\xE9 "),en=i(ka,"CODE",{});var ou=u(en);Fa=a(ou,"codeparrot"),ou.forEach(t),sn=a(ka," qu\u2019ils ont ensuite partag\xE9 sur le "),Es=i(ka,"A",{href:!0,rel:!0});var ru=u(Es);tn=i(ru,"EM",{});var lu=u(tn);nn=a(lu,"Hub"),lu.forEach(t),ru.forEach(t),Ha=a(ka,"."),ka.forEach(t),an=f(e),ne=i(e,"P",{});var ja=u(ne);Ia=a(ja,"Cependant, entra\xEEner sur l\u2019ensemble du corpus prend beaucoup de temps et demande beaucoup de ressources de calculs. Dans notre cas, nous n\u2019avons besoin que du sous-ensemble du jeu de donn\xE9es qui est relatif aux codes portant sur la science des donn\xE9es. Commen\xE7ons donc par filtrer le jeu de donn\xE9es "),ys=i(ja,"CODE",{});var iu=u(ys);Ra=a(iu,"codeparrot"),iu.forEach(t),Va=a(ja," en ne gardant que les fichiers incluant l\u2019une des biblioth\xE8ques de science des donn\xE9es \xE9num\xE9r\xE9es pr\xE9c\xE9demment. En raison de la taille du jeu de donn\xE9es, nous voulons \xE9viter de le t\xE9l\xE9charger. Nous utiliserons donc la fonctionnalit\xE9 de "),ws=i(ja,"EM",{});var uu=u(ws);Ua=a(uu,"streaming"),uu.forEach(t),Ba=a(ja," de \u{1F917} "),pt=i(ja,"EM",{});var pu=u(pt);xs=a(pu,"Datasets"),pu.forEach(t),Mn=a(ja," afin de le filtrer \xE0 la vol\xE9e. Pour nous aider \xE0 filtrer les \xE9chantillons de code utilisant les biblioth\xE8ques que nous avons mentionn\xE9es pr\xE9c\xE9demment, nous utilisons la fonction suivante :"),ja.forEach(t),Xe=f(e),y(zs.$$.fragment,e),ct=f(e),dt=i(e,"P",{});var cu=u(dt);Wa=a(cu,"Testons-le sur deux exemples :"),cu.forEach(t),on=f(e),y(ze.$$.fragment,e),rn=f(e),y(je.$$.fragment,e),Nn=f(e),Se=i(e,"P",{});var $l=u(Se);Xa=a($l,"Nous pouvons l\u2019utiliser pour cr\xE9er une fonction qui va "),ln=i($l,"EM",{});var du=u(ln);An=a(du,"streamer"),du.forEach(t),Ps=a($l," le jeu de donner et filtrer les \xE9l\xE9ments que nous voulons :"),$l.forEach(t),un=f(e),y(me.$$.fragment,e),Ln=f(e),ns=i(e,"P",{});var mu=u(ns);Ka=a(mu,"Ensuite, nous pouvons simplement appliquer cette fonction :"),mu.forEach(t),On=f(e),y(Ge.$$.fragment,e),Sn=f(e),y(Cs.$$.fragment,e),pn=f(e),Fe=i(e,"P",{});var fu=u(Fe);Gn=a(fu,"Cela nous laisse avec environ 3 % du jeu de donn\xE9es original, ce qui est tout de m\xEAme assez important puisqu\u2019il fait 6 Go et se compose de 600 000 scripts Python !"),fu.forEach(t),Ds=f(e),as=i(e,"P",{});var ql=u(as);Fn=a(ql,"Le filtrage peut prendre de 2 \xE0 3 heures, selon votre machine et votre bande passante. Si vous ne voulez pas passer par ce long processus, nous fournissons sur le "),Ke=i(ql,"EM",{});var _u=u(Ke);Hn=a(_u,"Hub"),_u.forEach(t),mt=a(ql," le jeu de donn\xE9es filtr\xE9 pour que vous puissiez le t\xE9l\xE9charger :"),ql.forEach(t),In=f(e),y(os.$$.fragment,e),rs=f(e),y(ls.$$.fragment,e),is=f(e),y(Je.$$.fragment,e),be=f(e),ft=i(e,"P",{});var hu=u(ft);cn=a(hu,"Examinons un exemple tir\xE9 du jeu de donn\xE9es. Nous ne montrerons que les 200 premiers caract\xE8res de chaque champ :"),hu.forEach(t),Rn=f(e),y(Ts.$$.fragment,e),_t=f(e),y(Ms.$$.fragment,e),Vn=f(e),He=i(e,"P",{});var kl=u(He);Ja=a(kl,"Nous pouvons voir que le champ "),dn=i(kl,"CODE",{});var vu=u(dn);Un=a(vu,"content"),vu.forEach(t),Ns=a(kl," contient le code sur lequel nous voulons que notre mod\xE8le s\u2019entra\xEEne. Maintenant que nous avons un jeu de donn\xE9es, nous devons pr\xE9parer les textes afin qu\u2019ils soient dans un format appropri\xE9 pour le pr\xE9-entra\xEEnement."),kl.forEach(t),mn=f(e),se=i(e,"H2",{class:!0});var jl=u(se);us=i(jl,"A",{id:!0,class:!0,href:!0});var gu=u(us);As=i(gu,"SPAN",{});var bu=u(As);y(Ls.$$.fragment,bu),bu.forEach(t),gu.forEach(t),Ya=f(jl),Os=i(jl,"SPAN",{});var $u=u(Os);Za=a($u,"Pr\xE9paration du jeu de donn\xE9es"),$u.forEach(t),jl.forEach(t),Bn=f(e),y(Ye.$$.fragment,e),Wn=f(e),ps=i(e,"P",{});var El=u(ps);fn=a(El,"La premi\xE8re \xE9tape est de tokeniser les donn\xE9es afin de pouvoir les utiliser pour l\u2019entra\xEEnement. Puisque notre objectif est d\u2019autocompl\xE9ter de courts appels de fonctions, nous pouvons garder la taille du contexte relativement petite. L\u2019avantage est que nous pouvons entra\xEEner le mod\xE8le beaucoup plus rapidement et qu\u2019il n\xE9cessite beaucoup moins de m\xE9moire. Si c\u2019est important pour votre application d\u2019avoir davantage de contexte (par exemple, si vous voulez que le mod\xE8le \xE9crive des tests unitaires bas\xE9s sur un fichier avec la d\xE9finition de la fonction), assurez-vous d\u2019augmenter ce nombre. Gardez n\xE9anmoins \xE0 l\u2019esprit que cela s\u2019accompagne d\u2019une plus grande empreinte m\xE9moire du GPU. Pour l\u2019instant, fixons la taille du contexte \xE0 128 "),_n=i(El,"EM",{});var qu=u(_n);Qa=a(qu,"tokens"),qu.forEach(t),hn=a(El,", par opposition aux 1 024 ou 2 048 utilis\xE9s respectivement dans le GPT-2 et le GPT-3."),El.forEach(t),Xn=f(e),ie=i(e,"P",{});var vs=u(ie);Kn=a(vs,"La plupart des documents contiennent beaucoup plus de 128 "),Ze=i(vs,"EM",{});var ku=u(Ze);Jn=a(ku,"tokens"),ku.forEach(t),Ss=a(vs,", donc le fait de tronquer les entr\xE9es \xE0 la longueur maximale \xE9liminerait une grande partie de notre jeu de donn\xE9es. A la place, nous allons utiliser l\u2019option "),ht=i(vs,"CODE",{});var ju=u(ht);vt=a(ju,"return_overflowing_tokens"),ju.forEach(t),eo=a(vs," pour tokeniser l\u2019entr\xE9e enti\xE8re et la diviser en plusieurs morceaux, comme nous l\u2019avons fait dans le "),Gs=i(vs,"A",{href:!0});var Eu=u(Gs);Fs=a(Eu,"chapitre 6"),Eu.forEach(t),Yn=a(vs,". Nous utiliserons \xE9galement l\u2019option "),Ee=i(vs,"CODE",{});var yu=u(Ee);so=a(yu,"return_length"),yu.forEach(t),vn=a(vs," pour retourner automatiquement la longueur de chaque morceau cr\xE9\xE9. Souvent, le dernier morceau est plus petit que la taille du contexte et nous nous en d\xE9barrasserons pour \xE9viter les probl\xE8mes de "),gn=i(vs,"EM",{});var wu=u(gn);to=a(wu,"padding"),wu.forEach(t),bn=a(vs,". Nous n\u2019en avons pas vraiment besoin puisque de toute fa\xE7on nous avons beaucoup de donn\xE9es."),vs.forEach(t),Zn=f(e),Qe=i(e,"DIV",{class:!0});var yl=u(Qe);cs=i(yl,"IMG",{class:!0,src:!0,alt:!0}),no=f(yl),ds=i(yl,"IMG",{class:!0,src:!0,alt:!0}),yl.forEach(t),qn=f(e),Ie=i(e,"P",{});var xu=u(Ie);Qn=a(xu,"Voyons comment cela fonctionne en examinant les deux premiers exemples :"),xu.forEach(t),es=f(e),y(Hs.$$.fragment,e),bt=f(e),y(Is.$$.fragment,e),ea=f(e),Pe=i(e,"P",{});var hr=u(Pe);Rs=a(hr,"Nous pouvons voir que nous obtenons 34 morceaux \xE0 partir de ces deux exemples. En regardant leurs longueurs, nous pouvons voir qu\u2019ils se terminent avec moins de 128 "),$t=i(hr,"EM",{});var zu=u($t);qt=a(zu,"tokens"),zu.forEach(t),ao=a(hr," (117 et 41, respectivement). Ils ne repr\xE9sentent qu\u2019une petite fraction du total des morceaux que nous avons (2/34), donc nous pouvons les jeter sans risque. Avec le champ "),kt=i(hr,"CODE",{});var Pu=u(kt);Vs=a(Pu,"overflow_to_sample_mapping"),Pu.forEach(t),sa=a(hr,", nous pouvons aussi reconstruire quels morceaux appartenaient \xE0 quels \xE9chantillons d\u2019entr\xE9e."),hr.forEach(t),ms=f(e),ue=i(e,"P",{});var Dt=u(ue);o=a(Dt,"Avec cette op\xE9ration, nous utilisons une fonctionnalit\xE9 pratique de la fonction "),q=i(Dt,"CODE",{});var Cu=u(q);Do=a(Cu,"Dataset.map()"),Cu.forEach(t),To=a(Dt," de \u{1F917} "),kn=i(Dt,"EM",{});var Du=u(kn);Mo=a(Du,"Datasets"),Du.forEach(t),jt=a(Dt,". En effet, celle-ci ne n\xE9cessite pas une correspondance un \xE0 un comme nous l\u2019avons vu dans la "),jn=i(Dt,"A",{href:!0});var Tu=u(jn);No=a(Tu,"section 3"),Tu.forEach(t),V=a(Dt,". Nous pouvons cr\xE9er des batchs avec plus ou moins d\u2019\xE9l\xE9ments que le batch d\u2019entr\xE9e. C\u2019est utile lorsque l\u2019on effectue des op\xE9rations telles que l\u2019augmentation ou le filtrage des donn\xE9es qui modifient le nombre d\u2019\xE9l\xE9ments. Dans notre cas, lors de la tokenisation de chaque \xE9l\xE9ment en morceaux de longeur de la taille de contexte sp\xE9cifi\xE9e, nous cr\xE9ons de nombreux \xE9chantillons de chaque document. Nous devons juste nous assurer de supprimer les colonnes existantes, car elles ont une taille conflictuelle. Si nous voulions les garder, nous pourrions les r\xE9p\xE9ter de mani\xE8re appropri\xE9e et les retourner dans l\u2019appel "),ta=i(Dt,"CODE",{});var Mu=u(ta);Ao=a(Mu,"Dataset.map()"),Mu.forEach(t),Lo=a(Dt," :"),Dt.forEach(t),oo=f(e),y(Et.$$.fragment,e),ro=f(e),y(yt.$$.fragment,e),lo=f(e),$e=i(e,"P",{});var Tt=u($e);Oo=a(Tt,"Nous avons maintenant 16,7 millions d\u2019exemples avec 128 "),na=i(Tt,"EM",{});var Nu=u(na);So=a(Nu,"tokens"),Nu.forEach(t),Go=a(Tt," chacun, ce qui correspond \xE0 environ 2,1 milliards de "),aa=i(Tt,"EM",{});var Au=u(aa);Fo=a(Au,"tokens"),Au.forEach(t),fe=a(Tt," au total. A titre de comparaison, les mod\xE8les GPT-3 et Codex d\u2019OpenAI sont entra\xEEn\xE9s sur 300 et 100 milliards de "),oa=i(Tt,"EM",{});var Lu=u(oa);Ho=a(Lu,"tokens"),Lu.forEach(t),Io=a(Tt,", respectivement. Les mod\xE8les Codex \xE9tant initialis\xE9s \xE0 partir des "),ra=i(Tt,"EM",{});var Ou=u(ra);Ro=a(Ou,"checkpoints"),Ou.forEach(t),Vo=a(Tt," GPT-3. Notre objectif dans cette section n\u2019est pas de rivaliser avec ces mod\xE8les, qui peuvent g\xE9n\xE9rer des textes longs et coh\xE9rents, mais de cr\xE9er une version r\xE9duite fournissant une fonction d\u2019autocompl\xE9tion rapide."),Tt.forEach(t),io=f(e),ss=i(e,"P",{});var Su=u(ss);Uo=a(Su,"Maintenant que le jeu de donn\xE9es est pr\xEAt, configurons le mod\xE8le !"),Su.forEach(t),uo=f(e),y(Us.$$.fragment,e),Bs=f(e),fs=i(e,"H2",{class:!0});var wl=u(fs);Ws=i(wl,"A",{id:!0,class:!0,href:!0});var Gu=u(Ws);wt=i(Gu,"SPAN",{});var Fu=u(wt);y(xt.$$.fragment,Fu),Fu.forEach(t),Gu.forEach(t),la=f(wl),ia=i(wl,"SPAN",{});var Hu=u(ia);zt=a(Hu,"Initialisation d'un nouveau mod\xE8le"),Hu.forEach(t),wl.forEach(t),po=f(e),qe=i(e,"P",{});var Mt=u(qe);Bo=a(Mt,"Notre premi\xE8re \xE9tape consiste \xE0 initialiser un GPT-2. Pour notre mod\xE8le, nous utiliserons la m\xEAme configuration que pour le petit mod\xE8le GPT-2. Ainsi nous chargeons la configuration pr\xE9-entra\xEEn\xE9e, nous nous assurons que la taille du "),ua=i(Mt,"EM",{});var Iu=u(ua);_s=a(Iu,"tokenizer"),Iu.forEach(t),Wo=a(Mt," correspond \xE0 la taille du vocabulaire du mod\xE8le et nous passons les identifiants des "),pa=i(Mt,"EM",{});var Ru=u(pa);Xo=a(Ru,"tokens"),Ru.forEach(t),ye=f(Mt),ca=i(Mt,"CODE",{});var Vu=u(ca);Ko=a(Vu,"bos"),Vu.forEach(t),Jo=a(Mt," et "),da=i(Mt,"CODE",{});var Uu=u(da);Yo=a(Uu,"eos"),Uu.forEach(t),Zo=a(Mt," (d\xE9but et fin de s\xE9quence) :"),Mt.forEach(t),Xs=f(e),Ve.l(e),En=f(e),Ce=i(e,"P",{});var vr=u(Ce);Qo=a(vr,"Notre mod\xE8le comporte 124 millions de param\xE8tres que nous devrons r\xE9gler. Avant de commencer l\u2019entra\xEEnement, nous devons configurer un collateur de donn\xE9es qui se chargera de cr\xE9er les batchs. Nous pouvons utiliser le collateur "),ma=i(vr,"CODE",{});var Bu=u(ma);Nl=a(Bu,"DataCollatorForLanguageModeling"),Bu.forEach(t),Al=a(vr,", qui est con\xE7u sp\xE9cifiquement pour la mod\xE9lisation du langage (comme son nom le sugg\xE8re subtilement). En plus de l\u2019empilage et du rembourrage des batchs, il s\u2019occupe aussi de la cr\xE9ation des \xE9tiquettes du mod\xE8le de langage. Dans la mod\xE9lisation causale du langage, les entr\xE9es servent aussi d\u2019\xE9tiquettes (juste d\xE9cal\xE9es d\u2019un \xE9l\xE9ment) et que le collateur de donn\xE9es cr\xE9e \xE0 la vol\xE9e pendant l\u2019entra\xEEnement pour ne pas avoir \xE0 dupliquer les "),qr=i(vr,"CODE",{});var Wu=u(qr);Ll=a(Wu,"input_ids"),Wu.forEach(t),Ol=a(vr,"."),vr.forEach(t),Vr=f(e),Ue=i(e,"P",{});var Nt=u(Ue);Sl=a(Nt,"Notez que "),kr=i(Nt,"CODE",{});var Xu=u(kr);Gl=a(Xu,"DataCollatorForLanguageModeling"),Xu.forEach(t),Fl=a(Nt," supporte \xE0 la fois la mod\xE9lisation du langage masqu\xE9 (MLM pour "),jr=i(Nt,"EM",{});var Ku=u(jr);Hl=a(Ku,"masked language modeling"),Ku.forEach(t),Il=a(Nt,") et la mod\xE9lisation du langage causal (CLM pour "),Er=i(Nt,"EM",{});var Ju=u(Er);Rl=a(Ju,"causal language modeling"),Ju.forEach(t),Vl=a(Nt,"). Par d\xE9faut, il pr\xE9pare les donn\xE9es pour la MLM mais nous pouvons passer \xE0 la CLM en d\xE9finissant l\u2019argument "),yr=i(Nt,"CODE",{});var Yu=u(yr);Ul=a(Yu,"mlm=False"),Yu.forEach(t),Bl=a(Nt," :"),Nt.forEach(t),Ur=f(e),Js.l(e),er=f(e),sr=i(e,"P",{});var Zu=u(sr);Wl=a(Zu,"Prenons un exemple :"),Zu.forEach(t),Br=f(e),y(co.$$.fragment,e),Wr=f(e),Zs.l(e),tr=f(e),nr=i(e,"P",{});var Qu=u(nr);Xl=a(Qu,"Nous pouvons voir que les exemples ont \xE9t\xE9 empil\xE9s et que tous les tenseurs ont la m\xEAme forme."),Qu.forEach(t),Xr=f(e),pe&&pe.l(e),ar=f(e),y(fa.$$.fragment,e),Kr=f(e),_a=i(e,"P",{});var xl=u(_a);Kl=a(xl,"Nous avons maintenant tout ce qu\u2019il faut pour entra\xEEner notre mod\xE8le. Ce n\u2019\xE9tait pas si compliqu\xE9 ! Avant de commencer l\u2019entra\xEEnement, nous devons nous connecter \xE0 Hugging Face. Si vous travaillez dans un "),wr=i(xl,"EM",{});var ep=u(wr);Jl=a(ep,"notebook"),ep.forEach(t),Yl=a(xl,", vous pouvez le faire avec la fonction utilitaire suivante :"),xl.forEach(t),Jr=f(e),y(mo.$$.fragment,e),Yr=f(e),ha=i(e,"P",{});var zl=u(ha);Zl=a(zl,"Cela affichera un "),xr=i(zl,"EM",{});var sp=u(xr);Ql=a(sp,"widget"),sp.forEach(t),ei=a(zl," o\xF9 vous pourrez entrer vos identifiants de connexion \xE0 Hugging Face."),zl.forEach(t),Zr=f(e),va=i(e,"P",{});var Pl=u(va);si=a(Pl,"Si vous ne travaillez pas dans un "),zr=i(Pl,"EM",{});var tp=u(zr);ti=a(tp,"notebook"),tp.forEach(t),ni=a(Pl,", tapez simplement la ligne suivante dans votre terminal :"),Pl.forEach(t),Qr=f(e),y(fo.$$.fragment,e),el=f(e),et.l(e),or=f(e),y(ga.$$.fragment,e),sl=f(e),y(ba.$$.fragment,e),tl=f(e),yn=i(e,"H2",{class:!0});var Cl=u(yn);$a=i(Cl,"A",{id:!0,class:!0,href:!0});var np=u($a);Pr=i(np,"SPAN",{});var ap=u(Pr);y(_o.$$.fragment,ap),ap.forEach(t),np.forEach(t),ai=f(Cl),Cr=i(Cl,"SPAN",{});var op=u(Cr);oi=a(op,"G\xE9n\xE9ration de code avec le pipeline"),op.forEach(t),Cl.forEach(t),nl=f(e),qa=i(e,"P",{});var Dl=u(qa);ri=a(Dl,"C\u2019est maintenant le moment de v\xE9rit\xE9 : voyons comment le mod\xE8le entra\xEEn\xE9 fonctionne r\xE9ellement ! Nous pouvons voir dans les logs que la perte a diminu\xE9 r\xE9guli\xE8rement, mais pour mettre le mod\xE8le \xE0 l\u2019\xE9preuve, regardons comment il fonctionne sur certains messages. Pour ce faire, nous allons envelopper le mod\xE8le dans un "),Dr=i(Dl,"CODE",{});var rp=u(Dr);li=a(rp,"pipeline"),rp.forEach(t),ii=a(Dl," de g\xE9n\xE9ration de texte et, s\u2019il y en a un de disponible, utiliser un GPU pour avoir des g\xE9n\xE9rations rapidement :"),Dl.forEach(t),al=f(e),tt.l(e),rr=f(e),lr=i(e,"P",{});var lp=u(lr);ui=a(lp,"Let\u2019s start with the simple task of creating a scatter plot:"),lp.forEach(t),ol=f(e),y(ho.$$.fragment,e),rl=f(e),y(vo.$$.fragment,e),ll=f(e),Pt=i(e,"P",{});var gr=u(Pt);pi=a(gr,"Le r\xE9sultat semble correct. Est-ce que cela fonctionne aussi pour une op\xE9ration "),Tr=i(gr,"CODE",{});var ip=u(Tr);ci=a(ip,"pandas"),ip.forEach(t),di=a(gr," ? Voyons si nous pouvons cr\xE9er un "),Mr=i(gr,"CODE",{});var up=u(Mr);mi=a(up,"DataFrame"),up.forEach(t),fi=a(gr," \xE0 partir de deux tableaux :"),gr.forEach(t),il=f(e),y(go.$$.fragment,e),ul=f(e),y(bo.$$.fragment,e),pl=f(e),Be=i(e,"P",{});var At=u(Be);_i=a(At,"Bien, c\u2019est la bonne r\xE9ponse. Bien qu\u2019il ins\xE8re ensuite la colonne "),Nr=i(At,"CODE",{});var pp=u(Nr);hi=a(pp,"x"),pp.forEach(t),vi=a(At," \xE0 nouveau. Comme le nombre de "),Ar=i(At,"EM",{});var cp=u(Ar);gi=a(cp,"tokens"),cp.forEach(t),bi=a(At," g\xE9n\xE9r\xE9s est limit\xE9, la boucle "),Lr=i(At,"CODE",{});var dp=u(Lr);$i=a(dp,"for"),dp.forEach(t),qi=a(At," suivante est coup\xE9e. Voyons si nous pouvons faire quelque chose d\u2019un peu plus complexe et faire en sorte que le mod\xE8le nous aide \xE0 utiliser l\u2019op\xE9ration "),Or=i(At,"CODE",{});var mp=u(Or);ki=a(mp,"groupby"),mp.forEach(t),ji=a(At," :"),At.forEach(t),cl=f(e),y($o.$$.fragment,e),dl=f(e),y(qo.$$.fragment,e),ml=f(e),Ct=i(e,"P",{});var br=u(Ct);Ei=a(br,"Pas mal, c\u2019est la bonne fa\xE7on de faire. Enfin, voyons si nous pouvons aussi l\u2019utiliser pour "),Sr=i(br,"CODE",{});var fp=u(Sr);yi=a(fp,"scikit-learn"),fp.forEach(t),wi=a(br," et utiliser un mod\xE8le "),Gr=i(br,"EM",{});var _p=u(Gr);xi=a(_p,"Random Forest"),_p.forEach(t),zi=a(br," :"),br.forEach(t),fl=f(e),y(ko.$$.fragment,e),_l=f(e),y(jo.$$.fragment,e),hl=f(e),wn.l(e),ir=f(e),ce&&ce.l(e),ur=Ml(),this.h()},h(){M(r,"name","hf:doc:metadata"),M(r,"content",JSON.stringify(Jp)),M($,"id","entraner-un-modle-de-langage-causal-partir-de-zro"),M($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M($,"href","#entraner-un-modle-de-langage-causal-partir-de-zro"),M(k,"class","relative group"),M(W,"href","/course/fr/chapter1"),M(it,"href","/course/fr/chapter6"),Tl(oe.src,Aa="https://hf.space/gradioiframe/course-demos/codeparrot-ds/+")||M(oe,"src",Aa),M(oe,"frameborder","0"),M(oe,"height","300"),M(oe,"title","Gradio app"),M(oe,"class","block dark:hidden container p-0 flex-grow space-iframe"),M(oe,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),M(oe,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Tl(ge.src,Pn="https://hf.space/gradioiframe/course-demos/codeparrot-ds-darkmode/+")||M(ge,"src",Pn),M(ge,"frameborder","0"),M(ge,"height","300"),M(ge,"title","Gradio app"),M(ge,"class","hidden dark:block container p-0 flex-grow space-iframe"),M(ge,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),M(ge,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),M(ks,"href","https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28"),M(ks,"rel","nofollow"),M(Le,"id","collecte-des-donnes"),M(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(Le,"href","#collecte-des-donnes"),M(We,"class","relative group"),M(xe,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/"),M(xe,"rel","nofollow"),M(Es,"href","https://huggingface.co/datasets/transformersbook/codeparrot"),M(Es,"rel","nofollow"),M(us,"id","prparation-du-jeu-de-donnes"),M(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(us,"href","#prparation-du-jeu-de-donnes"),M(se,"class","relative group"),M(Gs,"href","/course/fr/chapter6/4"),M(cs,"class","block dark:hidden"),Tl(cs.src,$n="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg")||M(cs,"src",$n),M(cs,"alt","Chunking a large texts in several pieces."),M(ds,"class","hidden dark:block"),Tl(ds.src,gt="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg")||M(ds,"src",gt),M(ds,"alt","Chunking a large texts in several pieces."),M(Qe,"class","flex justify-center"),M(jn,"href","/course/fr/chapter7/3"),M(Ws,"id","initialisation-dun-nouveau-modle"),M(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(Ws,"href","#initialisation-dun-nouveau-modle"),M(fs,"class","relative group"),M($a,"id","gnration-de-code-avec-le-pipeline"),M($a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M($a,"href","#gnration-de-code-avec-le-pipeline"),M(yn,"class","relative group")},m(e,d){s(document.head,r),p(e,_,d),w(c,e,d),p(e,v,d),p(e,k,d),s(k,$),s($,z),w(D,z,null),s(k,j),s(k,P),s(P,N),p(e,A,d),Eo[h].m(e,d),p(e,H,d),p(e,I,d),s(I,X),s(I,B),s(B,ee),s(I,K),s(I,W),s(W,C),s(I,Q),s(I,U),s(U,F),s(I,G),s(I,R),s(R,re),s(I,_e),s(I,J),s(J,he),s(I,ve),p(e,de,d),p(e,Y,d),s(Y,ts),s(Y,S),s(S,Z),s(Y,nt),s(Y,Te),s(Te,gs),s(Y,at),s(Y,ke),s(ke,ot),s(Y,Me),s(Y,Ne),s(Ne,rt),s(Y,xa),s(Y,bs),s(bs,za),s(Y,Pa),p(e,lt,d),w($s,e,d),p(e,xn,d),p(e,ae,d),s(ae,Ca),s(ae,it),s(it,Lt),s(ae,Da),s(ae,Ot),s(Ot,St),s(ae,Ta),s(ae,Gt),s(Gt,Ft),s(ae,Ma),s(ae,Ht),s(Ht,It),s(ae,Na),s(ae,Rt),s(Rt,zn),s(ae,qs),p(e,Vt,d),p(e,oe,d),p(e,te,d),p(e,ge,d),p(e,Cn,d),p(e,Ae,d),s(Ae,Ut),s(Ae,Bt),s(Bt,La),s(Ae,Wt),s(Ae,ks),s(ks,Oa),s(Ae,Xt),p(e,Dn,d),p(e,We,d),s(We,Le),s(Le,Kt),w(js,Kt,null),s(We,Jt),s(We,Yt),s(Yt,Sa),p(e,Zt,d),p(e,le,d),s(le,Tn),s(le,xe),s(xe,Ga),s(xe,ut),s(ut,Oe),s(le,Qt),s(le,en),s(en,Fa),s(le,sn),s(le,Es),s(Es,tn),s(tn,nn),s(le,Ha),p(e,an,d),p(e,ne,d),s(ne,Ia),s(ne,ys),s(ys,Ra),s(ne,Va),s(ne,ws),s(ws,Ua),s(ne,Ba),s(ne,pt),s(pt,xs),s(ne,Mn),p(e,Xe,d),w(zs,e,d),p(e,ct,d),p(e,dt,d),s(dt,Wa),p(e,on,d),w(ze,e,d),p(e,rn,d),w(je,e,d),p(e,Nn,d),p(e,Se,d),s(Se,Xa),s(Se,ln),s(ln,An),s(Se,Ps),p(e,un,d),w(me,e,d),p(e,Ln,d),p(e,ns,d),s(ns,Ka),p(e,On,d),w(Ge,e,d),p(e,Sn,d),w(Cs,e,d),p(e,pn,d),p(e,Fe,d),s(Fe,Gn),p(e,Ds,d),p(e,as,d),s(as,Fn),s(as,Ke),s(Ke,Hn),s(as,mt),p(e,In,d),w(os,e,d),p(e,rs,d),w(ls,e,d),p(e,is,d),w(Je,e,d),p(e,be,d),p(e,ft,d),s(ft,cn),p(e,Rn,d),w(Ts,e,d),p(e,_t,d),w(Ms,e,d),p(e,Vn,d),p(e,He,d),s(He,Ja),s(He,dn),s(dn,Un),s(He,Ns),p(e,mn,d),p(e,se,d),s(se,us),s(us,As),w(Ls,As,null),s(se,Ya),s(se,Os),s(Os,Za),p(e,Bn,d),w(Ye,e,d),p(e,Wn,d),p(e,ps,d),s(ps,fn),s(ps,_n),s(_n,Qa),s(ps,hn),p(e,Xn,d),p(e,ie,d),s(ie,Kn),s(ie,Ze),s(Ze,Jn),s(ie,Ss),s(ie,ht),s(ht,vt),s(ie,eo),s(ie,Gs),s(Gs,Fs),s(ie,Yn),s(ie,Ee),s(Ee,so),s(ie,vn),s(ie,gn),s(gn,to),s(ie,bn),p(e,Zn,d),p(e,Qe,d),s(Qe,cs),s(Qe,no),s(Qe,ds),p(e,qn,d),p(e,Ie,d),s(Ie,Qn),p(e,es,d),w(Hs,e,d),p(e,bt,d),w(Is,e,d),p(e,ea,d),p(e,Pe,d),s(Pe,Rs),s(Pe,$t),s($t,qt),s(Pe,ao),s(Pe,kt),s(kt,Vs),s(Pe,sa),p(e,ms,d),p(e,ue,d),s(ue,o),s(ue,q),s(q,Do),s(ue,To),s(ue,kn),s(kn,Mo),s(ue,jt),s(ue,jn),s(jn,No),s(ue,V),s(ue,ta),s(ta,Ao),s(ue,Lo),p(e,oo,d),w(Et,e,d),p(e,ro,d),w(yt,e,d),p(e,lo,d),p(e,$e,d),s($e,Oo),s($e,na),s(na,So),s($e,Go),s($e,aa),s(aa,Fo),s($e,fe),s($e,oa),s(oa,Ho),s($e,Io),s($e,ra),s(ra,Ro),s($e,Vo),p(e,io,d),p(e,ss,d),s(ss,Uo),p(e,uo,d),w(Us,e,d),p(e,Bs,d),p(e,fs,d),s(fs,Ws),s(Ws,wt),w(xt,wt,null),s(fs,la),s(fs,ia),s(ia,zt),p(e,po,d),p(e,qe,d),s(qe,Bo),s(qe,ua),s(ua,_s),s(qe,Wo),s(qe,pa),s(pa,Xo),s(qe,ye),s(qe,ca),s(ca,Ko),s(qe,Jo),s(qe,da),s(da,Yo),s(qe,Zo),p(e,Xs,d),yo[Re].m(e,d),p(e,En,d),p(e,Ce,d),s(Ce,Qo),s(Ce,ma),s(ma,Nl),s(Ce,Al),s(Ce,qr),s(qr,Ll),s(Ce,Ol),p(e,Vr,d),p(e,Ue,d),s(Ue,Sl),s(Ue,kr),s(kr,Gl),s(Ue,Fl),s(Ue,jr),s(jr,Hl),s(Ue,Il),s(Ue,Er),s(Er,Rl),s(Ue,Vl),s(Ue,yr),s(yr,Ul),s(Ue,Bl),p(e,Ur,d),wo[Ks].m(e,d),p(e,er,d),p(e,sr,d),s(sr,Wl),p(e,Br,d),w(co,e,d),p(e,Wr,d),xo[Ys].m(e,d),p(e,tr,d),p(e,nr,d),s(nr,Xl),p(e,Xr,d),pe&&pe.m(e,d),p(e,ar,d),w(fa,e,d),p(e,Kr,d),p(e,_a,d),s(_a,Kl),s(_a,wr),s(wr,Jl),s(_a,Yl),p(e,Jr,d),w(mo,e,d),p(e,Yr,d),p(e,ha,d),s(ha,Zl),s(ha,xr),s(xr,Ql),s(ha,ei),p(e,Zr,d),p(e,va,d),s(va,si),s(va,zr),s(zr,ti),s(va,ni),p(e,Qr,d),w(fo,e,d),p(e,el,d),zo[Qs].m(e,d),p(e,or,d),w(ga,e,d),p(e,sl,d),w(ba,e,d),p(e,tl,d),p(e,yn,d),s(yn,$a),s($a,Pr),w(_o,Pr,null),s(yn,ai),s(yn,Cr),s(Cr,oi),p(e,nl,d),p(e,qa,d),s(qa,ri),s(qa,Dr),s(Dr,li),s(qa,ii),p(e,al,d),Po[st].m(e,d),p(e,rr,d),p(e,lr,d),s(lr,ui),p(e,ol,d),w(ho,e,d),p(e,rl,d),w(vo,e,d),p(e,ll,d),p(e,Pt,d),s(Pt,pi),s(Pt,Tr),s(Tr,ci),s(Pt,di),s(Pt,Mr),s(Mr,mi),s(Pt,fi),p(e,il,d),w(go,e,d),p(e,ul,d),w(bo,e,d),p(e,pl,d),p(e,Be,d),s(Be,_i),s(Be,Nr),s(Nr,hi),s(Be,vi),s(Be,Ar),s(Ar,gi),s(Be,bi),s(Be,Lr),s(Lr,$i),s(Be,qi),s(Be,Or),s(Or,ki),s(Be,ji),p(e,cl,d),w($o,e,d),p(e,dl,d),w(qo,e,d),p(e,ml,d),p(e,Ct,d),s(Ct,Ei),s(Ct,Sr),s(Sr,yi),s(Ct,wi),s(Ct,Gr),s(Gr,xi),s(Ct,zi),p(e,fl,d),w(ko,e,d),p(e,_l,d),w(jo,e,d),p(e,hl,d),wn.m(e,d),p(e,ir,d),ce&&ce.m(e,d),p(e,ur,d),vl=!0},p(e,[d]){const Co={};d&1&&(Co.fw=e[0]),c.$set(Co);let pr=h;h=Ti(e),h!==pr&&(ya(),b(Eo[pr],1,1,()=>{Eo[pr]=null}),Ea(),T=Eo[h],T||(T=Eo[h]=Di[h](e),T.c()),g(T,1),T.m(H.parentNode,H));const Fr={};d&2&&(Fr.$$scope={dirty:d,ctx:e}),Je.$set(Fr);const Hr={};d&2&&(Hr.$$scope={dirty:d,ctx:e}),Us.$set(Hr);let we=Re;Re=Ni(e),Re!==we&&(ya(),b(yo[we],1,1,()=>{yo[we]=null}),Ea(),Ve=yo[Re],Ve||(Ve=yo[Re]=Mi[Re](e),Ve.c()),g(Ve,1),Ve.m(En.parentNode,En));let cr=Ks;Ks=Li(e),Ks!==cr&&(ya(),b(wo[cr],1,1,()=>{wo[cr]=null}),Ea(),Js=wo[Ks],Js||(Js=wo[Ks]=Ai[Ks](e),Js.c()),g(Js,1),Js.m(er.parentNode,er));let dr=Ys;Ys=Si(e),Ys!==dr&&(ya(),b(xo[dr],1,1,()=>{xo[dr]=null}),Ea(),Zs=xo[Ys],Zs||(Zs=xo[Ys]=Oi[Ys](e),Zs.c()),g(Zs,1),Zs.m(tr.parentNode,tr)),e[0]==="tf"?pe?d&1&&g(pe,1):(pe=hp(),pe.c(),g(pe,1),pe.m(ar.parentNode,ar)):pe&&(ya(),b(pe,1,1,()=>{pe=null}),Ea());const Ir={};d&2&&(Ir.$$scope={dirty:d,ctx:e}),fa.$set(Ir);let mr=Qs;Qs=Fi(e),Qs!==mr&&(ya(),b(zo[mr],1,1,()=>{zo[mr]=null}),Ea(),et=zo[Qs],et||(et=zo[Qs]=Gi[Qs](e),et.c()),g(et,1),et.m(or.parentNode,or));const Rr={};d&2&&(Rr.$$scope={dirty:d,ctx:e}),ga.$set(Rr);const De={};d&3&&(De.$$scope={dirty:d,ctx:e}),ba.$set(De);let fr=st;st=Ii(e),st!==fr&&(ya(),b(Po[fr],1,1,()=>{Po[fr]=null}),Ea(),tt=Po[st],tt||(tt=Po[st]=Hi[st](e),tt.c()),g(tt,1),tt.m(rr.parentNode,rr)),gl!==(gl=Ri(e))&&(wn.d(1),wn=gl(e),wn&&(wn.c(),wn.m(ir.parentNode,ir))),e[0]==="pt"?ce?d&1&&g(ce,1):(ce=vp(e),ce.c(),g(ce,1),ce.m(ur.parentNode,ur)):ce&&(ya(),b(ce,1,1,()=>{ce=null}),Ea())},i(e){vl||(g(c.$$.fragment,e),g(D.$$.fragment,e),g(T),g($s.$$.fragment,e),g(js.$$.fragment,e),g(zs.$$.fragment,e),g(ze.$$.fragment,e),g(je.$$.fragment,e),g(me.$$.fragment,e),g(Ge.$$.fragment,e),g(Cs.$$.fragment,e),g(os.$$.fragment,e),g(ls.$$.fragment,e),g(Je.$$.fragment,e),g(Ts.$$.fragment,e),g(Ms.$$.fragment,e),g(Ls.$$.fragment,e),g(Ye.$$.fragment,e),g(Hs.$$.fragment,e),g(Is.$$.fragment,e),g(Et.$$.fragment,e),g(yt.$$.fragment,e),g(Us.$$.fragment,e),g(xt.$$.fragment,e),g(Ve),g(Js),g(co.$$.fragment,e),g(Zs),g(pe),g(fa.$$.fragment,e),g(mo.$$.fragment,e),g(fo.$$.fragment,e),g(et),g(ga.$$.fragment,e),g(ba.$$.fragment,e),g(_o.$$.fragment,e),g(tt),g(ho.$$.fragment,e),g(vo.$$.fragment,e),g(go.$$.fragment,e),g(bo.$$.fragment,e),g($o.$$.fragment,e),g(qo.$$.fragment,e),g(ko.$$.fragment,e),g(jo.$$.fragment,e),g(ce),vl=!0)},o(e){b(c.$$.fragment,e),b(D.$$.fragment,e),b(T),b($s.$$.fragment,e),b(js.$$.fragment,e),b(zs.$$.fragment,e),b(ze.$$.fragment,e),b(je.$$.fragment,e),b(me.$$.fragment,e),b(Ge.$$.fragment,e),b(Cs.$$.fragment,e),b(os.$$.fragment,e),b(ls.$$.fragment,e),b(Je.$$.fragment,e),b(Ts.$$.fragment,e),b(Ms.$$.fragment,e),b(Ls.$$.fragment,e),b(Ye.$$.fragment,e),b(Hs.$$.fragment,e),b(Is.$$.fragment,e),b(Et.$$.fragment,e),b(yt.$$.fragment,e),b(Us.$$.fragment,e),b(xt.$$.fragment,e),b(Ve),b(Js),b(co.$$.fragment,e),b(Zs),b(pe),b(fa.$$.fragment,e),b(mo.$$.fragment,e),b(fo.$$.fragment,e),b(et),b(ga.$$.fragment,e),b(ba.$$.fragment,e),b(_o.$$.fragment,e),b(tt),b(ho.$$.fragment,e),b(vo.$$.fragment,e),b(go.$$.fragment,e),b(bo.$$.fragment,e),b($o.$$.fragment,e),b(qo.$$.fragment,e),b(ko.$$.fragment,e),b(jo.$$.fragment,e),b(ce),vl=!1},d(e){t(r),e&&t(_),x(c,e),e&&t(v),e&&t(k),x(D),e&&t(A),Eo[h].d(e),e&&t(H),e&&t(I),e&&t(de),e&&t(Y),e&&t(lt),x($s,e),e&&t(xn),e&&t(ae),e&&t(Vt),e&&t(oe),e&&t(te),e&&t(ge),e&&t(Cn),e&&t(Ae),e&&t(Dn),e&&t(We),x(js),e&&t(Zt),e&&t(le),e&&t(an),e&&t(ne),e&&t(Xe),x(zs,e),e&&t(ct),e&&t(dt),e&&t(on),x(ze,e),e&&t(rn),x(je,e),e&&t(Nn),e&&t(Se),e&&t(un),x(me,e),e&&t(Ln),e&&t(ns),e&&t(On),x(Ge,e),e&&t(Sn),x(Cs,e),e&&t(pn),e&&t(Fe),e&&t(Ds),e&&t(as),e&&t(In),x(os,e),e&&t(rs),x(ls,e),e&&t(is),x(Je,e),e&&t(be),e&&t(ft),e&&t(Rn),x(Ts,e),e&&t(_t),x(Ms,e),e&&t(Vn),e&&t(He),e&&t(mn),e&&t(se),x(Ls),e&&t(Bn),x(Ye,e),e&&t(Wn),e&&t(ps),e&&t(Xn),e&&t(ie),e&&t(Zn),e&&t(Qe),e&&t(qn),e&&t(Ie),e&&t(es),x(Hs,e),e&&t(bt),x(Is,e),e&&t(ea),e&&t(Pe),e&&t(ms),e&&t(ue),e&&t(oo),x(Et,e),e&&t(ro),x(yt,e),e&&t(lo),e&&t($e),e&&t(io),e&&t(ss),e&&t(uo),x(Us,e),e&&t(Bs),e&&t(fs),x(xt),e&&t(po),e&&t(qe),e&&t(Xs),yo[Re].d(e),e&&t(En),e&&t(Ce),e&&t(Vr),e&&t(Ue),e&&t(Ur),wo[Ks].d(e),e&&t(er),e&&t(sr),e&&t(Br),x(co,e),e&&t(Wr),xo[Ys].d(e),e&&t(tr),e&&t(nr),e&&t(Xr),pe&&pe.d(e),e&&t(ar),x(fa,e),e&&t(Kr),e&&t(_a),e&&t(Jr),x(mo,e),e&&t(Yr),e&&t(ha),e&&t(Zr),e&&t(va),e&&t(Qr),x(fo,e),e&&t(el),zo[Qs].d(e),e&&t(or),x(ga,e),e&&t(sl),x(ba,e),e&&t(tl),e&&t(yn),x(_o),e&&t(nl),e&&t(qa),e&&t(al),Po[st].d(e),e&&t(rr),e&&t(lr),e&&t(ol),x(ho,e),e&&t(rl),x(vo,e),e&&t(ll),e&&t(Pt),e&&t(il),x(go,e),e&&t(ul),x(bo,e),e&&t(pl),e&&t(Be),e&&t(cl),x($o,e),e&&t(dl),x(qo,e),e&&t(ml),e&&t(Ct),e&&t(fl),x(ko,e),e&&t(_l),x(jo,e),e&&t(hl),wn.d(e),e&&t(ir),ce&&ce.d(e),e&&t(ur)}}}const Jp={local:"entraner-un-modle-de-langage-causal-partir-de-zro",sections:[{local:"collecte-des-donnes",title:"Collecte des donn\xE9es"},{local:"prparation-du-jeu-de-donnes",title:"Pr\xE9paration du jeu de donn\xE9es"},{local:"initialisation-dun-nouveau-modle",title:"Initialisation d'un nouveau mod\xE8le"},{local:"gnration-de-code-avec-le-pipeline",title:"G\xE9n\xE9ration de code avec le pipeline"},{local:"entraner-avec-iacceleratei",title:"Entra\xEEner avec \u{1F917} <i>Accelerate</i>"}],title:"Entra\xEEner un mod\xE8le de langage causal \xE0 partir de z\xE9ro"};function Yp(O,r,_){let c="pt";return jp(()=>{const v=new URLSearchParams(window.location.search);_(0,c=v.get("fw")||"pt")}),[c]}class oc extends bp{constructor(r){super();$p(this,r,Yp,Kp,qp,{})}}export{oc as default,Jp as metadata};
