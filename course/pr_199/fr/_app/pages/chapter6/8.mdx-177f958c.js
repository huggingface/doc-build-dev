import{S as Uq,i as Rq,s as Wq,e as r,k as u,w as m,t as n,M as Gq,c as l,d as t,m as c,a,x as d,h as o,b as E,N as Fq,G as s,g as p,y as f,q as k,o as v,B as _,v as Iq}from"../../chunks/vendor-37701547.js";import{T as Vq}from"../../chunks/Tip-3026cd5f.js";import{Y as Xq}from"../../chunks/Youtube-3501dc06.js";import{I as ii}from"../../chunks/IconCopyLink-80214518.js";import{C as h}from"../../chunks/CodeBlock-d4353f55.js";import{D as Kq}from"../../chunks/DocNotebookDropdown-8dd4d5f2.js";function Hq(pi){let z,ge,oe,pe,qe,je,As,re,Fs,Xe,Ke,Us,ss,le,ts,Pe,He,ns,R,xe,Rs,Ws,be,Gs,Is;return{c(){z=r("p"),ge=r("strong"),oe=n("Pour aller plus loin"),pe=n(" Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=r("code"),je=n('u"\\u0085"'),As=n(` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),re=r("code"),Fs=n("normalizers.Sequence"),Xe=n(", nous n\u2019avons pas inclus les Regex que le "),Ke=r("code"),Us=n("BertNormalizer"),ss=n(" requiert quand l\u2019argument "),le=r("code"),ts=n("clean_text"),Pe=n(" est mis \xE0 "),He=r("code"),ns=n("True"),R=n(" ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),xe=r("code"),Rs=n("BertNormalizer"),Ws=n(" en ajoutant deux "),be=r("code"),Gs=n("normalizers.Replace"),Is=n(" \xE0 la s\xE9quence de normalisation.")},l(ae){z=l(ae,"P",{});var x=a(z);ge=l(x,"STRONG",{});var os=a(ge);oe=o(os,"Pour aller plus loin"),os.forEach(t),pe=o(x," Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=l(x,"CODE",{});var Cn=a(qe);je=o(Cn,'u"\\u0085"'),Cn.forEach(t),As=o(x,` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),re=l(x,"CODE",{});var Dn=a(re);Fs=o(Dn,"normalizers.Sequence"),Dn.forEach(t),Xe=o(x,", nous n\u2019avons pas inclus les Regex que le "),Ke=l(x,"CODE",{});var Ln=a(Ke);Us=o(Ln,"BertNormalizer"),Ln.forEach(t),ss=o(x," requiert quand l\u2019argument "),le=l(x,"CODE",{});var J=a(le);ts=o(J,"clean_text"),J.forEach(t),Pe=o(x," est mis \xE0 "),He=l(x,"CODE",{});var On=a(He);ns=o(On,"True"),On.forEach(t),R=o(x," ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),xe=l(x,"CODE",{});var rs=a(xe);Rs=o(rs,"BertNormalizer"),rs.forEach(t),Ws=o(x," en ajoutant deux "),be=l(x,"CODE",{});var Mn=a(be);Gs=o(Mn,"normalizers.Replace"),Mn.forEach(t),Is=o(x," \xE0 la s\xE9quence de normalisation."),x.forEach(t)},m(ae,x){p(ae,z,x),s(z,ge),s(ge,oe),s(z,pe),s(z,qe),s(qe,je),s(z,As),s(z,re),s(re,Fs),s(z,Xe),s(z,Ke),s(Ke,Us),s(z,ss),s(z,le),s(le,ts),s(z,Pe),s(z,He),s(He,ns),s(z,R),s(z,xe),s(xe,Rs),s(z,Ws),s(z,be),s(be,Gs),s(z,Is)},d(ae){ae&&t(z)}}}function Jq(pi){let z,ge,oe,pe,qe,je,As,re,Fs,Xe,Ke,Us,ss,le,ts,Pe,He,ns,R,xe,Rs,Ws,be,Gs,Is,ae,x,os,Cn,Dn,Ln,J,On,rs,Mn,xc,mo,bc,gc,fo,Pc,wc,ui,yn,Tc,ci,Je,Vs,oz,Cc,Xs,rz,mi,W,Dc,ko,Lc,Oc,vo,Mc,yc,_o,Sc,Nc,Sn,Bc,Ac,ho,Fc,Uc,di,Ks,fi,ls,Rc,Eo,Wc,Gc,ki,G,we,zo,Ic,Vc,$o,Xc,Kc,Hs,Hc,Jc,Yc,Te,qo,Zc,Qc,jo,em,sm,Js,tm,nm,om,I,xo,rm,lm,bo,am,im,go,pm,um,Po,cm,mm,wo,dm,fm,Ys,km,vm,_m,Ce,To,hm,Em,Co,zm,$m,Zs,qm,jm,xm,De,Do,bm,gm,Lo,Pm,wm,Qs,Tm,Cm,Dm,Le,Oo,Lm,Om,Mo,Mm,ym,et,Sm,Nm,vi,as,Bm,st,Am,Fm,_i,Ye,is,yo,tt,Um,So,Rm,hi,ue,Wm,No,Gm,Im,Nn,Vm,Xm,nt,Km,Hm,Ei,ot,zi,Oe,Jm,Bo,Ym,Zm,Ao,Qm,ed,$i,ps,sd,Fo,td,nd,qi,rt,ji,ce,od,Uo,rd,ld,Ro,ad,id,Wo,pd,ud,xi,Ze,us,Go,lt,cd,at,md,Io,dd,fd,bi,b,kd,Vo,vd,_d,Xo,hd,Ed,Ko,zd,$d,Ho,qd,jd,Jo,xd,bd,Yo,gd,Pd,Zo,wd,Td,Qo,Cd,Dd,gi,Me,Ld,er,Od,Md,sr,yd,Sd,Pi,it,wi,me,Nd,tr,Bd,Ad,nr,Fd,Ud,or,Rd,Wd,Ti,g,Gd,rr,Id,Vd,lr,Xd,Kd,ar,Hd,Jd,ir,Yd,Zd,pr,Qd,ef,ur,sf,tf,cr,nf,of,mr,rf,lf,Ci,pt,Di,V,af,dr,pf,uf,fr,cf,mf,kr,df,ff,vr,kf,vf,_r,_f,hf,Li,ut,Oi,ye,Ef,hr,zf,$f,Er,qf,jf,Mi,Se,xf,zr,bf,gf,$r,Pf,wf,yi,ct,Si,mt,Ni,cs,Bi,ms,Tf,qr,Cf,Df,Ai,dt,Fi,Bn,Lf,Ui,ft,Ri,ds,Of,jr,Mf,yf,Wi,kt,Gi,vt,Ii,fs,Sf,xr,Nf,Bf,Vi,_t,Xi,ht,Ki,ks,Af,br,Ff,Uf,Hi,Et,Ji,zt,Yi,de,Rf,gr,Wf,Gf,Pr,If,Vf,wr,Xf,Kf,Zi,$t,Qi,O,Hf,Tr,Jf,Yf,Cr,Zf,Qf,Dr,ek,sk,Lr,tk,nk,Or,ok,rk,Mr,lk,ak,ep,An,ik,sp,qt,tp,Ne,pk,yr,uk,ck,Sr,mk,dk,np,jt,op,Be,fk,Nr,kk,vk,Br,_k,hk,rp,xt,lp,bt,ap,j,Ek,Ar,zk,$k,Fr,qk,jk,Ur,xk,bk,Rr,gk,Pk,Wr,wk,Tk,Gr,Ck,Dk,Ir,Lk,Ok,Vr,Mk,yk,Xr,Sk,Nk,ip,P,Bk,Kr,Ak,Fk,Hr,Uk,Rk,Jr,Wk,Gk,Yr,Ik,Vk,Zr,Xk,Kk,Qr,Hk,Jk,el,Yk,Zk,sl,Qk,ev,pp,gt,up,Pt,cp,M,sv,tl,tv,nv,nl,ov,rv,ol,lv,av,rl,iv,pv,ll,uv,cv,al,mv,dv,mp,Fn,fv,dp,wt,fp,Ae,kv,il,vv,_v,pl,hv,Ev,kp,Un,zv,vp,Tt,_p,Ct,hp,Rn,$v,Ep,Dt,zp,Lt,$p,vs,qv,ul,jv,xv,qp,Ot,jp,_s,bv,cl,gv,Pv,xp,Mt,bp,yt,gp,hs,wv,ml,Tv,Cv,Pp,St,wp,Fe,Dv,dl,Lv,Ov,fl,Mv,yv,Tp,Nt,Cp,y,Sv,kl,Nv,Bv,vl,Av,Fv,_l,Uv,Rv,hl,Wv,Gv,El,Iv,Vv,zl,Xv,Kv,Dp,$,Hv,$l,Jv,Yv,ql,Zv,Qv,jl,e_,s_,xl,t_,n_,bl,o_,r_,gl,l_,a_,Pl,i_,p_,wl,u_,c_,Tl,m_,d_,Cl,f_,k_,Dl,v_,Ll,__,h_,Lp,Bt,Op,fe,E_,Ol,z_,$_,Ml,q_,j_,yl,x_,b_,Mp,At,yp,S,g_,Sl,P_,w_,Nl,T_,C_,Bl,D_,L_,Al,O_,M_,Fl,y_,S_,Ul,N_,B_,Sp,Ue,A_,Rl,F_,U_,Wl,R_,W_,Np,Qe,Es,Gl,Ft,G_,Ut,I_,Il,V_,X_,Bp,ke,K_,Vl,H_,J_,Xl,Y_,Z_,Kl,Q_,e2,Ap,Rt,Fp,ve,s2,Hl,t2,n2,Jl,o2,r2,Yl,l2,a2,Up,Wn,i2,Rp,Wt,Wp,zs,p2,Zl,u2,c2,Gp,Gt,Ip,It,Vp,Re,m2,Ql,d2,f2,ea,k2,v2,Xp,Vt,Kp,N,_2,sa,h2,E2,ta,z2,$2,na,q2,j2,oa,x2,b2,ra,g2,P2,la,w2,T2,Hp,$s,C2,aa,D2,L2,Jp,Xt,Yp,Gn,O2,Zp,Kt,Qp,Ht,eu,qs,M2,ia,y2,S2,su,Jt,tu,T,N2,pa,B2,A2,ua,F2,U2,ca,R2,W2,ma,G2,I2,da,V2,X2,fa,K2,H2,ka,J2,Y2,nu,Yt,ou,Zt,ru,In,Z2,lu,Qt,au,Vn,Q2,iu,en,pu,sn,uu,_e,eh,va,sh,th,_a,nh,oh,ha,rh,lh,cu,tn,mu,Xn,ah,du,nn,fu,We,ih,Ea,ph,uh,za,ch,mh,ku,es,js,$a,on,dh,rn,fh,qa,kh,vh,vu,Y,_h,ja,hh,Eh,xa,zh,$h,ba,qh,jh,ga,xh,bh,_u,ln,hu,Kn,gh,Eu,xs,Ph,Pa,wh,Th,zu,an,$u,he,Ch,wa,Dh,Lh,Ta,Oh,Mh,Ca,yh,Sh,qu,Ge,Nh,Da,Bh,Ah,La,Fh,Uh,ju,pn,xu,Hn,Rh,bu,un,gu,cn,Pu,bs,Wh,Oa,Gh,Ih,wu,mn,Tu,C,Vh,Ma,Xh,Kh,ya,Hh,Jh,Sa,Yh,Zh,Na,Qh,eE,Ba,sE,tE,Aa,nE,oE,Fa,rE,lE,Cu,gs,aE,Ua,iE,pE,Du,dn,Lu,Jn,uE,Ou,fn,Mu,kn,yu,w,cE,Ra,mE,dE,Wa,fE,kE,Ga,vE,_E,Ia,hE,EE,Va,zE,$E,Xa,qE,jE,Ka,xE,bE,Ha,gE,PE,Su,vn,Nu,_n,Bu,Yn,wE,Au,hn,Fu,Zn,TE,Uu,En,Ru,zn,Wu,Ps,CE,Ja,DE,LE,Gu,$n,Iu,D,OE,Ya,ME,yE,Za,SE,NE,Qa,BE,AE,ei,FE,UE,si,RE,WE,ti,GE,IE,ni,VE,XE,Vu,qn,Xu,Qn,KE,Ku,jn,Hu,Z,HE,oi,JE,YE,ri,ZE,QE,li,ez,sz,ai,tz,nz,Ju;return je=new ii({}),le=new Kq({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"}]}}),Ks=new Xq({props:{id:"MR8tZm5ViWU"}}),tt=new ii({}),ot=new h({props:{code:`from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, name=<span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        <span class="hljs-keyword">yield</span> dataset[i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;text&quot;</span>]`}}),rt=new h({props:{code:`with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\\n")`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset)):
        f.write(dataset[i][<span class="hljs-string">&quot;text&quot;</span>] + <span class="hljs-string">&quot;\\n&quot;</span>)`}}),lt=new ii({}),it=new h({props:{code:`from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),pt=new h({props:{code:"tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)",highlighted:'tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="hljs-literal">True</span>)'}}),ut=new h({props:{code:`tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`,highlighted:`tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`}}),ct=new h({props:{code:'print(tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),mt=new h({props:{code:"hello how are u?",highlighted:"hello how are u?"}}),cs=new Vq({props:{$$slots:{default:[Hq]},$$scope:{ctx:pi}}}),dt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"}}),ft=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"}}),kt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)'}}),vt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),_t=new h({props:{code:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),ht=new h({props:{code:`[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]`,highlighted:'[(<span class="hljs-string">&quot;Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">28</span>))]'}}),Et=new h({props:{code:`pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),zt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),$t=new h({props:{code:`special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
trainer = trainers.WordPieceTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens)`}}),qt=new h({props:{code:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)",highlighted:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"}}),jt=new h({props:{code:`tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>)
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),xt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),bt=new h({props:{code:`['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']`,highlighted:'[<span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),gt=new h({props:{code:`cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Pt=new h({props:{code:"(2, 3)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)'}}),wt=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,
    pair=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="hljs-string">&quot;[SEP]&quot;</span>, sep_token_id)],
)`}}),Tt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ct=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']`,highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),Dt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),Lt=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;pair&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;sentences&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Ot=new h({props:{code:'tokenizer.decoder = decoders.WordPiece(prefix="##")',highlighted:'tokenizer.decoder = decoders.WordPiece(prefix=<span class="hljs-string">&quot;##&quot;</span>)'}}),Mt=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),yt=new h({props:{code:`"let's test this tokenizer... on a pair of sentences." # Testons ce tokenizer... sur une paire de phrases.`,highlighted:'<span class="hljs-string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer... sur une paire de phrases.</span>'}}),St=new h({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Nt=new h({props:{code:'new_tokenizer = Tokenizer.from_file("tokenizer.json")',highlighted:'new_tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Bt=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # Vous pouvez charger \xE0 partir du fichier du tokenizer, alternativement
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    <span class="hljs-comment"># tokenizer_file=&quot;tokenizer.json&quot;, # Vous pouvez charger \xE0 partir du fichier du tokenizer, alternativement</span>
    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,
    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,
    cls_token=<span class="hljs-string">&quot;[CLS]&quot;</span>,
    sep_token=<span class="hljs-string">&quot;[SEP]&quot;</span>,
    mask_token=<span class="hljs-string">&quot;[MASK]&quot;</span>,
)`}}),At=new h({props:{code:`from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`}}),Ft=new ii({}),Rt=new h({props:{code:"tokenizer = Tokenizer(models.BPE())",highlighted:"tokenizer = Tokenizer(models.BPE())"}}),Wt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)",highlighted:'tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>)'}}),Gt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)'}}),It=new h({props:{code:`[('Let', (0, 3)), ("'s", (3, 5)), ('\u0120test', (5, 10)), ('\u0120pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;s&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u0120test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120pre&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
 (<span class="hljs-string">&#x27;tokenization&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;!&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Vt=new h({props:{code:`trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`trainer = trainers.BpeTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),Xt=new h({props:{code:`tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.BPE()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Kt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ht=new h({props:{code:`['L', 'et', "'", 's', '\u0120test', '\u0120this', '\u0120to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-string">&#x27;et&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u0120test&#x27;</span>, <span class="hljs-string">&#x27;\u0120this&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Jt=new h({props:{code:"tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)",highlighted:'tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="hljs-literal">False</span>)'}}),Yt=new h({props:{code:`sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]`,highlighted:`sentence = <span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[<span class="hljs-number">4</span>]
sentence[start:end]`}}),Zt=new h({props:{code:"' test'",highlighted:'<span class="hljs-string">&#x27; test&#x27;</span>'}}),Qt=new h({props:{code:"tokenizer.decoder = decoders.ByteLevel()",highlighted:"tokenizer.decoder = decoders.ByteLevel()"}}),en=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),sn=new h({props:{code:`"Let's test this tokenizer." # Testons ce tokenizer`,highlighted:'<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer</span>'}}),tn=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
)`}}),nn=new h({props:{code:`from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`}}),on=new ii({}),ln=new h({props:{code:"tokenizer = Tokenizer(models.Unigram())",highlighted:"tokenizer = Tokenizer(models.Unigram())"}}),an=new h({props:{code:`from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("\`\`", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Regex

tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [
        normalizers.Replace(<span class="hljs-string">&quot;\`\`&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.Replace(<span class="hljs-string">&quot;&#x27;&#x27;&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(<span class="hljs-string">&quot; {2,}&quot;</span>), <span class="hljs-string">&quot; &quot;</span>),
    ]
)`}}),pn=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"}}),un=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)'}}),cn=new h({props:{code:`[("\u2581Let's", (0, 5)), ('\u2581test', (5, 10)), ('\u2581the', (10, 14)), ('\u2581pre-tokenizer!', (14, 29))]`,highlighted:'[(<span class="hljs-string">&quot;\u2581Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u2581test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581the&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581pre-tokenizer!&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">29</span>))]'}}),mn=new h({props:{code:`special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>]
trainer = trainers.UnigramTrainer(
    vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens, unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),dn=new h({props:{code:`tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.Unigram()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),fn=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),kn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),vn=new h({props:{code:`cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),_n=new h({props:{code:"0 1",highlighted:'<span class="hljs-number">0</span> <span class="hljs-number">1</span>'}}),hn=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,
    pair=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],
)`}}),En=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences!&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),zn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.', '.', '.', '<sep>', '\u2581', 'on', '\u2581', 'a', '\u2581pair', 
  '\u2581of', '\u2581sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]`,highlighted:`[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;\u2581pair&#x27;</span>, 
  <span class="hljs-string">&#x27;\u2581of&#x27;</span>, <span class="hljs-string">&#x27;\u2581sentence&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;cls&gt;&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]`}}),$n=new h({props:{code:"tokenizer.decoder = decoders.Metaspace()",highlighted:"tokenizer.decoder = decoders.Metaspace()"}}),qn=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>,
    unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>,
    pad_token=<span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>,
    cls_token=<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>,
    sep_token=<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>,
    mask_token=<span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>,
    padding_side=<span class="hljs-string">&quot;left&quot;</span>,
)`}}),jn=new h({props:{code:`from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`}}),{c(){z=r("meta"),ge=u(),oe=r("h1"),pe=r("a"),qe=r("span"),m(je.$$.fragment),As=u(),re=r("span"),Fs=n("Construction d'un "),Xe=r("i"),Ke=n("tokenizer"),Us=n(", bloc par bloc"),ss=u(),m(le.$$.fragment),ts=u(),Pe=r("p"),He=n("Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),ns=u(),R=r("ul"),xe=r("li"),Rs=n("normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),Ws=u(),be=r("li"),Gs=n("pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),Is=u(),ae=r("li"),x=n("passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),os=r("em"),Cn=n("tokens"),Dn=n("),"),Ln=u(),J=r("li"),On=n("post-traitement (ajout des "),rs=r("em"),Mn=n("tokens"),xc=n(" sp\xE9ciaux du "),mo=r("em"),bc=n("tokenizer"),gc=n(", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),fo=r("em"),Pc=n("token"),wc=n(")."),ui=u(),yn=r("p"),Tc=n("Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),ci=u(),Je=r("div"),Vs=r("img"),Cc=u(),Xs=r("img"),mi=u(),W=r("p"),Dc=n("La biblioth\xE8que \u{1F917} "),ko=r("em"),Lc=n("Tokenizers"),Oc=n(" a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),vo=r("em"),Mc=n("tokenizer"),yc=n(" \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),_o=r("em"),Sc=n("tokenizer"),Nc=n(" \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=r("a"),Bc=n("section 2"),Ac=n(". Vous serez alors en mesure de construire n\u2019importe quel type de "),ho=r("em"),Fc=n("tokenizer"),Uc=n(" auquel vous pouvez penser !"),di=u(),m(Ks.$$.fragment),fi=u(),ls=r("p"),Rc=n("Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),Eo=r("code"),Wc=n("Tokenizer"),Gc=n(" avec les blocs de construction regroup\xE9s en sous-modules :"),ki=u(),G=r("ul"),we=r("li"),zo=r("code"),Ic=n("normalizers"),Vc=n(" contient tous les types de "),$o=r("code"),Xc=n("Normalizer"),Kc=n(" que vous pouvez utiliser (liste compl\xE8te "),Hs=r("a"),Hc=n("ici"),Jc=n("),"),Yc=u(),Te=r("li"),qo=r("code"),Zc=n("pre_tokenizers"),Qc=n(" contient tous les types de "),jo=r("code"),em=n("PreTokenizer"),sm=n(" que vous pouvez utiliser (liste compl\xE8te "),Js=r("a"),tm=n("ici"),nm=n("),"),om=u(),I=r("li"),xo=r("code"),rm=n("models"),lm=n(" contient les diff\xE9rents types de "),bo=r("code"),am=n("Model"),im=n(" que vous pouvez utiliser, comme "),go=r("code"),pm=n("BPE"),um=n(", "),Po=r("code"),cm=n("WordPiece"),mm=n(", et "),wo=r("code"),dm=n("Unigram"),fm=n(" (liste compl\xE8te "),Ys=r("a"),km=n("ici"),vm=n("),"),_m=u(),Ce=r("li"),To=r("code"),hm=n("trainers"),Em=n(" contient tous les diff\xE9rents types de "),Co=r("code"),zm=n("Trainer"),$m=n(" que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=r("a"),qm=n("ici"),jm=n("),"),xm=u(),De=r("li"),Do=r("code"),bm=n("post_processors"),gm=n(" contient les diff\xE9rents types de "),Lo=r("code"),Pm=n("PostProcessor"),wm=n(" que vous pouvez utiliser (liste compl\xE8te "),Qs=r("a"),Tm=n("ici"),Cm=n("),"),Dm=u(),Le=r("li"),Oo=r("code"),Lm=n("decoders"),Om=n(" contient les diff\xE9rents types de "),Mo=r("code"),Mm=n("Decoder"),ym=n(" que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=r("a"),Sm=n("ici"),Nm=n(")."),vi=u(),as=r("p"),Bm=n("Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=r("a"),Am=n("ici"),Fm=n("."),_i=u(),Ye=r("h2"),is=r("a"),yo=r("span"),m(tt.$$.fragment),Um=u(),So=r("span"),Rm=n("Acquisition d'un corpus"),hi=u(),ue=r("p"),Wm=n("Pour entra\xEEner notre nouveau "),No=r("em"),Gm=n("tokenizer"),Im=n(", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=r("a"),Vm=n("d\xE9but du chapitre"),Xm=n(", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=r("a"),Km=n("WikiText-2"),Hm=n(" :"),Ei=u(),m(ot.$$.fragment),zi=u(),Oe=r("p"),Jm=n("La fonction "),Bo=r("code"),Ym=n("get_training_corpus()"),Zm=n(" est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Ao=r("em"),Qm=n("tokenizer"),ed=n("."),$i=u(),ps=r("p"),sd=n("\u{1F917} "),Fo=r("em"),td=n("Tokenizers"),nd=n(" peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),qi=u(),m(rt.$$.fragment),ji=u(),ce=r("p"),od=n("Ensuite, nous vous montrerons comment construire vos propres "),Uo=r("em"),rd=n("tokenizers"),ld=n(" pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Ro=r("em"),ad=n("WordPiece"),id=n(", BPE et "),Wo=r("em"),pd=n("Unigram"),ud=n(". Commen\xE7ons par BERT !"),xi=u(),Ze=r("h2"),us=r("a"),Go=r("span"),m(lt.$$.fragment),cd=u(),at=r("span"),md=n("Construire un "),Io=r("i"),dd=n("tokenizer WordPiece"),fd=n(" \xE0 partir de z\xE9ro"),bi=u(),b=r("p"),kd=n("Pour construire un "),Vo=r("em"),vd=n("tokenizer"),_d=n(" avec la biblioth\xE8que \u{1F917} "),Xo=r("em"),hd=n("Tokenizers"),Ed=n(", nous commen\xE7ons par instancier un objet "),Ko=r("code"),zd=n("Tokenizer"),$d=n(" avec un "),Ho=r("code"),qd=n("model"),jd=n(". Puis nous d\xE9finissons ses attributs "),Jo=r("code"),xd=n("normalizer"),bd=n(", "),Yo=r("code"),gd=n("pre_tokenizer"),Pd=n(", "),Zo=r("code"),wd=n("post_processor"),Td=n(" et "),Qo=r("code"),Cd=n("decoder"),Dd=n(" aux valeurs que nous voulons."),gi=u(),Me=r("p"),Ld=n("Pour cet exemple, nous allons cr\xE9er un "),er=r("code"),Od=n("Tokenizer"),Md=n(" avec un mod\xE8le "),sr=r("em"),yd=n("WordPiece"),Sd=n(" :"),Pi=u(),m(it.$$.fragment),wi=u(),me=r("p"),Nd=n("Nous devons sp\xE9cifier le "),tr=r("code"),Bd=n("unk_token"),Ad=n(" pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),nr=r("code"),Fd=n("vocab"),Ud=n(" de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),or=r("code"),Rd=n("max_input_chars_per_word"),Wd=n(", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ti=u(),g=r("p"),Gd=n("La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),rr=r("code"),Id=n("BertNormalizer"),Vd=n(" a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),lr=r("code"),Xd=n("lowercase"),Kd=n(" pour mettre le texte en minuscule, "),ar=r("code"),Hd=n("strip_accents"),Jd=n(" qui enl\xE8ve les accents, "),ir=r("code"),Yd=n("clean_text"),Zd=n(" pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),pr=r("code"),Qd=n("handle_chinese_chars"),ef=n(" qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),ur=r("em"),sf=n("tokenizer"),tf=u(),cr=r("code"),nf=n("bert-base-uncased"),of=n(", nous pouvons simplement d\xE9finir ce "),mr=r("em"),rf=n("normalizer"),lf=n(" :"),Ci=u(),m(pt.$$.fragment),Di=u(),V=r("p"),af=n("Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),dr=r("em"),pf=n("tokenizer"),uf=n(", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),fr=r("em"),cf=n("Tokenizers"),mf=n(". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),kr=r("code"),df=n("Lowercase"),ff=n(" et un normaliseur "),vr=r("code"),kf=n("StripAccents"),vf=n(". Il est possible de composer plusieurs normaliseurs en utilisant une "),_r=r("code"),_f=n("Sequence"),hf=n(" :"),Li=u(),m(ut.$$.fragment),Oi=u(),ye=r("p"),Ef=n("Nous utilisons \xE9galement un normaliseur Unicode "),hr=r("code"),zf=n("NFD"),$f=n(", car sinon "),Er=r("code"),qf=n("StripAccents"),jf=n(" ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),Mi=u(),Se=r("p"),xf=n("Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),zr=r("code"),bf=n("normalize_str()"),gf=n(" du "),$r=r("code"),Pf=n("normalizer"),wf=n(" pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),yi=u(),m(ct.$$.fragment),Si=u(),m(mt.$$.fragment),Ni=u(),m(cs.$$.fragment),Bi=u(),ms=r("p"),Tf=n("L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),qr=r("code"),Cf=n("BertPreTokenizer"),Df=n(" pr\xE9construit que nous pouvons utiliser :"),Ai=u(),m(dt.$$.fragment),Fi=u(),Bn=r("p"),Lf=n("Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),Ui=u(),m(ft.$$.fragment),Ri=u(),ds=r("p"),Of=n("Notez que le "),jr=r("code"),Mf=n("Whitespace"),yf=n(" divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),Wi=u(),m(kt.$$.fragment),Gi=u(),m(vt.$$.fragment),Ii=u(),fs=r("p"),Sf=n("Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),xr=r("code"),Nf=n("WhitespaceSplit"),Bf=n(" \xE0 la place :"),Vi=u(),m(_t.$$.fragment),Xi=u(),m(ht.$$.fragment),Ki=u(),ks=r("p"),Af=n("Comme pour les normaliseurs, vous pouvez utiliser une "),br=r("code"),Ff=n("Sequence"),Uf=n(" pour composer plusieurs pr\xE9tokenizers :"),Hi=u(),m(Et.$$.fragment),Ji=u(),m(zt.$$.fragment),Yi=u(),de=r("p"),Rf=n("L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),gr=r("code"),Wf=n("WordPieceTrainer"),Gf=n(". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Pr=r("em"),If=n("Tokenizers"),Vf=n(" est que vous devez lui passer tous les "),wr=r("em"),Xf=n("tokens"),Kf=n(" sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Zi=u(),m($t.$$.fragment),Qi=u(),O=r("p"),Hf=n("En plus de sp\xE9cifier la "),Tr=r("code"),Jf=n("vocab_size"),Yf=n(" et les "),Cr=r("code"),Zf=n("special_tokens"),Qf=n(", nous pouvons d\xE9finir la "),Dr=r("code"),ek=n("min_frequency"),sk=n(" (le nombre de fois qu\u2019un "),Lr=r("em"),tk=n("token"),nk=n(" doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Or=r("code"),ok=n("continuing_subword_prefix"),rk=n(" (si nous voulons utiliser quelque chose de diff\xE9rent de "),Mr=r("code"),lk=n("##"),ak=n(")."),ep=u(),An=r("p"),ik=n("Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),sp=u(),m(qt.$$.fragment),tp=u(),Ne=r("p"),pk=n("Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),yr=r("em"),uk=n("tokenizer"),ck=n(" qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Sr=r("code"),mk=n("WordPiece"),dk=n(" vide au pr\xE9alable) :"),np=u(),m(jt.$$.fragment),op=u(),Be=r("p"),fk=n("Dans les deux cas, nous pouvons ensuite tester le "),Nr=r("em"),kk=n("tokenizer"),vk=n(" sur un texte en appelant la m\xE9thode "),Br=r("code"),_k=n("encode()"),hk=n(" :"),rp=u(),m(xt.$$.fragment),lp=u(),m(bt.$$.fragment),ap=u(),j=r("p"),Ek=n("L\u2019encodage obtenu est un "),Ar=r("code"),zk=n("Encoding"),$k=n(" contenant toutes les sorties n\xE9cessaires du "),Fr=r("em"),qk=n("tokenizer"),jk=n(" dans ses diff\xE9rents attributs : "),Ur=r("code"),xk=n("ids"),bk=n(", "),Rr=r("code"),gk=n("type_ids"),Pk=n(", "),Wr=r("code"),wk=n("tokens"),Tk=n(", "),Gr=r("code"),Ck=n("offsets"),Dk=n(", "),Ir=r("code"),Lk=n("attention_mask"),Ok=n(", "),Vr=r("code"),Mk=n("special_tokens_mask"),yk=n(" et "),Xr=r("code"),Sk=n("overflowing"),Nk=n("."),ip=u(),P=r("p"),Bk=n("La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Kr=r("em"),Ak=n("token"),Fk=u(),Hr=r("code"),Uk=n("[CLS]"),Rk=n(" au d\xE9but et le "),Jr=r("em"),Wk=n("token"),Gk=u(),Yr=r("code"),Ik=n("[SEP]"),Vk=n(" \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Zr=r("code"),Xk=n("TemplateProcessor"),Kk=n(" pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Qr=r("em"),Hk=n("tokens"),Jk=u(),el=r("code"),Yk=n("[CLS]"),Zk=n(" et "),sl=r("code"),Qk=n("[SEP]"),ev=n(" dans le vocabulaire :"),pp=u(),m(gt.$$.fragment),up=u(),m(Pt.$$.fragment),cp=u(),M=r("p"),sv=n("Pour \xE9crire le gabarit pour "),tl=r("code"),tv=n("TemplateProcessor"),nv=n(", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),nl=r("em"),ov=n("tokens"),rv=n(" sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),ol=r("code"),lv=n("$A"),av=n(", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),rl=r("code"),iv=n("$B"),pv=n(". Pour chacun de ces \xE9l\xE9ments ("),ll=r("em"),uv=n("tokens"),cv=n(" sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),al=r("em"),mv=n("token"),dv=n(" correspondant apr\xE8s un deux-points."),mp=u(),Fn=r("p"),fv=n("Le gabarit classique de BERT est donc d\xE9fini comme suit :"),dp=u(),m(wt.$$.fragment),fp=u(),Ae=r("p"),kv=n("Notez que nous devons transmettre les identifiants des "),il=r("em"),vv=n("tokens"),_v=n(" sp\xE9ciaux afin que le "),pl=r("em"),hv=n("tokenizer"),Ev=n(" puisse les convertir correctement."),kp=u(),Un=r("p"),zv=n("Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),vp=u(),m(Tt.$$.fragment),_p=u(),m(Ct.$$.fragment),hp=u(),Rn=r("p"),$v=n("Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),Ep=u(),m(Dt.$$.fragment),zp=u(),m(Lt.$$.fragment),$p=u(),vs=r("p"),qv=n("Nous avons presque fini de construire ce "),ul=r("em"),jv=n("tokenizer"),xv=n(" \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),qp=u(),m(Ot.$$.fragment),jp=u(),_s=r("p"),bv=n("Testons-le sur notre pr\xE9c\xE9dent "),cl=r("code"),gv=n("encoding"),Pv=n(" :"),xp=u(),m(Mt.$$.fragment),bp=u(),m(yt.$$.fragment),gp=u(),hs=r("p"),wv=n("G\xE9nial ! Nous pouvons enregistrer notre "),ml=r("em"),Tv=n("tokenizer"),Cv=n(" dans un seul fichier JSON comme ceci :"),Pp=u(),m(St.$$.fragment),wp=u(),Fe=r("p"),Dv=n("Nous pouvons alors recharger ce fichier dans un objet "),dl=r("code"),Lv=n("Tokenizer"),Ov=n(" avec la m\xE9thode "),fl=r("code"),Mv=n("from_file()"),yv=n(" :"),Tp=u(),m(Nt.$$.fragment),Cp=u(),y=r("p"),Sv=n("Pour utiliser ce "),kl=r("em"),Nv=n("tokenizer"),Bv=n(" dans \u{1F917} "),vl=r("em"),Av=n("Transformers"),Fv=n(", nous devons l\u2019envelopper dans un "),_l=r("code"),Uv=n("PreTrainedTokenizerFast"),Rv=n(". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),hl=r("em"),Wv=n("tokenizer"),Gv=n(" correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),El=r("code"),Iv=n("BertTokenizerFast"),Vv=n("). Si vous appliquez cette logique pour construire un tout nouveau "),zl=r("em"),Xv=n("tokenizer"),Kv=n(", vous devrez utiliser la premi\xE8re option."),Dp=u(),$=r("p"),Hv=n("Pour envelopper le "),$l=r("em"),Jv=n("tokenizer"),Yv=n(" dans un "),ql=r("code"),Zv=n("PreTrainedTokenizerFast"),Qv=n(", nous pouvons soit passer le "),jl=r("em"),e_=n("tokenizer"),s_=n(" que nous avons construit comme un "),xl=r("code"),t_=n("tokenizer_object"),n_=n(", soit passer le fichier de "),bl=r("em"),o_=n("tokenizer"),r_=n(" que nous avons sauvegard\xE9 comme "),gl=r("code"),l_=n("tokenizer_file"),a_=n(". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),Pl=r("em"),i_=n("tokens"),p_=n(" sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),wl=r("code"),u_=n("tokenizer"),c_=n(" quel "),Tl=r("em"),m_=n("token"),d_=n(" est le "),Cl=r("em"),f_=n("token"),k_=n(" de masque, quel est le "),Dl=r("em"),v_=n("token"),Ll=r("code"),__=n("[CLS]"),h_=n(", etc :"),Lp=u(),m(Bt.$$.fragment),Op=u(),fe=r("p"),E_=n("Si vous utilisez une classe de "),Ol=r("em"),z_=n("tokenizer"),$_=n(" sp\xE9cifique (comme "),Ml=r("code"),q_=n("BertTokenizerFast"),j_=n("), vous aurez seulement besoin de sp\xE9cifier les "),yl=r("em"),x_=n("tokens"),b_=n(" sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Mp=u(),m(At.$$.fragment),yp=u(),S=r("p"),g_=n("Vous pouvez ensuite utiliser ce "),Sl=r("em"),P_=n("tokenizer"),w_=n(" comme n\u2019importe quel autre "),Nl=r("em"),T_=n("tokenizer"),C_=n(" de \u{1F917} "),Bl=r("em"),D_=n("Transformers"),L_=n(". Vous pouvez le sauvegarder avec la m\xE9thode "),Al=r("code"),O_=n("save_pretrained()"),M_=n(" ou le t\xE9l\xE9charger sur le "),Fl=r("em"),y_=n("Hub"),S_=n(" avec la m\xE9thode "),Ul=r("code"),N_=n("push_to_hub()"),B_=n("."),Sp=u(),Ue=r("p"),A_=n("Maintenant que nous avons vu comment construire un "),Rl=r("em"),F_=n("tokenizer WordPiece"),U_=n(", faisons de m\xEAme pour un "),Wl=r("em"),R_=n("tokenizer"),W_=n(" BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),Np=u(),Qe=r("h2"),Es=r("a"),Gl=r("span"),m(Ft.$$.fragment),G_=u(),Ut=r("span"),I_=n("Construire un "),Il=r("i"),V_=n("tokenizer"),X_=n(" BPE \xE0 partir de z\xE9ro"),Bp=u(),ke=r("p"),K_=n("Construisons maintenant un "),Vl=r("em"),H_=n("tokenizer"),J_=n(" BPE. Comme pour le "),Xl=r("em"),Y_=n("tokenizer"),Z_=n(" BERT, nous commen\xE7ons par initialiser un "),Kl=r("code"),Q_=n("Tokenizer"),e2=n(" avec un mod\xE8le BPE :"),Ap=u(),m(Rt.$$.fragment),Fp=u(),ve=r("p"),s2=n("Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Hl=r("code"),t2=n("vocab"),n2=n(" et le "),Jl=r("code"),o2=n("merges"),r2=n(" dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Yl=r("code"),l2=n("unk_token"),a2=n(" parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Up=u(),Wn=r("p"),i2=n("GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),Rp=u(),m(Wt.$$.fragment),Wp=u(),zs=r("p"),p2=n("L\u2019option que nous avons ajout\xE9e \xE0 "),Zl=r("code"),u2=n("ByteLevel"),c2=n(" ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),Gp=u(),m(Gt.$$.fragment),Ip=u(),m(It.$$.fragment),Vp=u(),Re=r("p"),m2=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Ql=r("em"),d2=n("token"),f2=n(" sp\xE9cial est le "),ea=r("em"),k2=n("token"),v2=n(" de fin de texte :"),Xp=u(),m(Vt.$$.fragment),Kp=u(),N=r("p"),_2=n("Comme avec le "),sa=r("code"),h2=n("WordPieceTrainer"),E2=n(", ainsi que le "),ta=r("code"),z2=n("vocab_size"),$2=n(" et le "),na=r("code"),q2=n("special_tokens"),j2=n(", nous pouvons sp\xE9cifier la "),oa=r("code"),x2=n("min_frequency"),b2=n(" si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ra=r("code"),g2=n("</w>"),P2=n("), nous pouvons le d\xE9finir avec "),la=r("code"),w2=n("end_of_word_suffix"),T2=n("."),Hp=u(),$s=r("p"),C2=n("Ce "),aa=r("em"),D2=n("tokenizer"),L2=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Jp=u(),m(Xt.$$.fragment),Yp=u(),Gn=r("p"),O2=n("Regardons la tokenisation d\u2019un exemple de texte :"),Zp=u(),m(Kt.$$.fragment),Qp=u(),m(Ht.$$.fragment),eu=u(),qs=r("p"),M2=n("Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),ia=r("em"),y2=n("tokenizer"),S2=n(" du GPT-2 comme suit :"),su=u(),m(Jt.$$.fragment),tu=u(),T=r("p"),N2=n("L\u2019option "),pa=r("code"),B2=n("trim_offsets = False"),A2=n(" indique au post-processeur que nous devons laisser les "),ua=r("em"),F2=n("offsets"),U2=n(" des "),ca=r("em"),R2=n("tokens"),W2=n(" qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ma=r("em"),G2=n("offsets"),I2=n(" pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),da=r("em"),V2=n("token"),X2=n("). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),fa=r("code"),K2=n("'\u0120test'"),H2=n(" est le "),ka=r("em"),J2=n("token"),Y2=n(" \xE0 l\u2019index 4 :"),nu=u(),m(Yt.$$.fragment),ou=u(),m(Zt.$$.fragment),ru=u(),In=r("p"),Z2=n("Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),lu=u(),m(Qt.$$.fragment),au=u(),Vn=r("p"),Q2=n("et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),iu=u(),m(en.$$.fragment),pu=u(),m(sn.$$.fragment),uu=u(),_e=r("p"),eh=n("Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),va=r("code"),sh=n("PreTrainedTokenizerFast"),th=n(" ou un "),_a=r("code"),nh=n("GPT2TokenizerFast"),oh=n(" si nous voulons l\u2019utiliser dans \u{1F917} "),ha=r("em"),rh=n("Transformers"),lh=n(" :"),cu=u(),m(tn.$$.fragment),mu=u(),Xn=r("p"),ah=n("ou :"),du=u(),m(nn.$$.fragment),fu=u(),We=r("p"),ih=n("Comme dernier exemple, nous allons vous montrer comment construire un "),Ea=r("em"),ph=n("tokenizer"),uh=u(),za=r("em"),ch=n("Unigram"),mh=n(" \xE0 partir de z\xE9ro."),ku=u(),es=r("h2"),js=r("a"),$a=r("span"),m(on.$$.fragment),dh=u(),rn=r("span"),fh=n("Construire un "),qa=r("i"),kh=n("tokenizer Unigram"),vh=n(" \xE0 partir de z\xE9ro"),vu=u(),Y=r("p"),_h=n("Construisons maintenant un "),ja=r("em"),hh=n("tokenizer"),Eh=n(" XLNet. Comme pour les "),xa=r("em"),zh=n("tokenizers"),$h=n(" pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ba=r("code"),qh=n("Tokenizer"),jh=n(" avec un mod\xE8le "),ga=r("em"),xh=n("Unigram"),bh=n(" :"),_u=u(),m(ln.$$.fragment),hu=u(),Kn=r("p"),gh=n("Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),Eu=u(),xs=r("p"),Ph=n("Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),Pa=r("em"),wh=n("SentencePiece"),Th=n(") :"),zu=u(),m(an.$$.fragment),$u=u(),he=r("p"),Ch=n("Il remplace "),wa=r("code"),Dh=n("\u201C"),Lh=n(" et "),Ta=r("code"),Oh=n("\u201D"),Mh=n(" par "),Ca=r("code"),yh=n("\u201D"),Sh=n(" et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),qu=u(),Ge=r("p"),Nh=n("Le pr\xE9tokenizer \xE0 utiliser pour tout "),Da=r("em"),Bh=n("tokenizer SentencePiece"),Ah=n(" est "),La=r("code"),Fh=n("Metaspace"),Uh=n(" :"),ju=u(),m(pn.$$.fragment),xu=u(),Hn=r("p"),Rh=n("Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),bu=u(),m(un.$$.fragment),gu=u(),m(cn.$$.fragment),Pu=u(),bs=r("p"),Wh=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Oa=r("em"),Gh=n("tokens"),Ih=n(" sp\xE9ciaux :"),wu=u(),m(mn.$$.fragment),Tu=u(),C=r("p"),Vh=n("Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Ma=r("code"),Xh=n("UnigramTrainer"),Kh=n(" est le "),ya=r("code"),Hh=n("unk_token"),Jh=n(". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Sa=r("em"),Yh=n("Unigram"),Zh=n(", comme le "),Na=r("code"),Qh=n("shrinking_factor"),eE=n(" pour chaque \xE9tape o\xF9 nous enlevons des "),Ba=r("em"),sE=n("tokens"),tE=n(" (par d\xE9faut 0.75) ou le "),Aa=r("code"),nE=n("max_piece_length"),oE=n(" pour sp\xE9cifier la longueur maximale d\u2019un "),Fa=r("em"),rE=n("token"),lE=n(" donn\xE9 (par d\xE9faut 16)."),Cu=u(),gs=r("p"),aE=n("Ce "),Ua=r("em"),iE=n("tokenizer"),pE=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Du=u(),m(dn.$$.fragment),Lu=u(),Jn=r("p"),uE=n("Regardons la tokenisation de notre exemple :"),Ou=u(),m(fn.$$.fragment),Mu=u(),m(kn.$$.fragment),yu=u(),w=r("p"),cE=n("Une particularit\xE9 de XLNet est qu\u2019il place le "),Ra=r("em"),mE=n("token"),dE=u(),Wa=r("code"),fE=n("<cls>"),kE=n(" \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Ga=r("em"),vE=n("tokens"),_E=n("). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ia=r("em"),hE=n("tokens"),EE=n(" sp\xE9ciaux et les types d\u2019identifiant de "),Va=r("em"),zE=n("token"),$E=n(" avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Xa=r("em"),qE=n("tokens"),jE=u(),Ka=r("code"),xE=n("<cls>"),bE=n(" et "),Ha=r("code"),gE=n("<sep>"),PE=n(" :"),Su=u(),m(vn.$$.fragment),Nu=u(),m(_n.$$.fragment),Bu=u(),Yn=r("p"),wE=n("Le mod\xE8le ressemble \xE0 ceci :"),Au=u(),m(hn.$$.fragment),Fu=u(),Zn=r("p"),TE=n("Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),Uu=u(),m(En.$$.fragment),Ru=u(),m(zn.$$.fragment),Wu=u(),Ps=r("p"),CE=n("Enfin, nous ajoutons un d\xE9codeur "),Ja=r("code"),DE=n("Metaspace"),LE=n(" :"),Gu=u(),m($n.$$.fragment),Iu=u(),D=r("p"),OE=n("et on en a fini avec ce "),Ya=r("em"),ME=n("tokenizer"),yE=n(" ! On peut le sauvegarder et l\u2019envelopper dans un "),Za=r("code"),SE=n("PreTrainedTokenizerFast"),NE=n(" ou "),Qa=r("code"),BE=n("XLNetTokenizerFast"),AE=n(" si on veut l\u2019utiliser dans \u{1F917} "),ei=r("em"),FE=n("Transformers"),UE=n(". Une chose \xE0 noter lors de l\u2019utilisation de "),si=r("code"),RE=n("PreTrainedTokenizerFast"),WE=n(" est qu\u2019en plus des "),ti=r("em"),GE=n("tokens"),IE=n(" sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ni=r("em"),VE=n("Transformers"),XE=n(" de rembourrer \xE0 gauche :"),Vu=u(),m(qn.$$.fragment),Xu=u(),Qn=r("p"),KE=n("Ou alternativement :"),Ku=u(),m(jn.$$.fragment),Hu=u(),Z=r("p"),HE=n("Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),oi=r("em"),JE=n("tokenizers"),YE=n(" existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),ri=r("em"),ZE=n("tokenizer"),QE=n(" que vous voulez avec la biblioth\xE8que \u{1F917} "),li=r("em"),ez=n("Tokenizers"),sz=n(" et pouvoir l\u2019utiliser dans \u{1F917} "),ai=r("em"),tz=n("Transformers"),nz=n("."),this.h()},l(e){const i=Gq('[data-svelte="svelte-1phssyn"]',document.head);z=l(i,"META",{name:!0,content:!0}),i.forEach(t),ge=c(e),oe=l(e,"H1",{class:!0});var xn=a(oe);pe=l(xn,"A",{id:!0,class:!0,href:!0});var lz=a(pe);qe=l(lz,"SPAN",{});var az=a(qe);d(je.$$.fragment,az),az.forEach(t),lz.forEach(t),As=c(xn),re=l(xn,"SPAN",{});var Yu=a(re);Fs=o(Yu,"Construction d'un "),Xe=l(Yu,"I",{});var iz=a(Xe);Ke=o(iz,"tokenizer"),iz.forEach(t),Us=o(Yu,", bloc par bloc"),Yu.forEach(t),xn.forEach(t),ss=c(e),d(le.$$.fragment,e),ts=c(e),Pe=l(e,"P",{});var pz=a(Pe);He=o(pz,"Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),pz.forEach(t),ns=c(e),R=l(e,"UL",{});var ws=a(R);xe=l(ws,"LI",{});var uz=a(xe);Rs=o(uz,"normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),uz.forEach(t),Ws=c(ws),be=l(ws,"LI",{});var cz=a(be);Gs=o(cz,"pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),cz.forEach(t),Is=c(ws),ae=l(ws,"LI",{});var Zu=a(ae);x=o(Zu,"passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),os=l(Zu,"EM",{});var mz=a(os);Cn=o(mz,"tokens"),mz.forEach(t),Dn=o(Zu,"),"),Zu.forEach(t),Ln=c(ws),J=l(ws,"LI",{});var Ts=a(J);On=o(Ts,"post-traitement (ajout des "),rs=l(Ts,"EM",{});var dz=a(rs);Mn=o(dz,"tokens"),dz.forEach(t),xc=o(Ts," sp\xE9ciaux du "),mo=l(Ts,"EM",{});var fz=a(mo);bc=o(fz,"tokenizer"),fz.forEach(t),gc=o(Ts,", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),fo=l(Ts,"EM",{});var kz=a(fo);Pc=o(kz,"token"),kz.forEach(t),wc=o(Ts,")."),Ts.forEach(t),ws.forEach(t),ui=c(e),yn=l(e,"P",{});var vz=a(yn);Tc=o(vz,"Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),vz.forEach(t),ci=c(e),Je=l(e,"DIV",{class:!0});var Qu=a(Je);Vs=l(Qu,"IMG",{class:!0,src:!0,alt:!0}),Cc=c(Qu),Xs=l(Qu,"IMG",{class:!0,src:!0,alt:!0}),Qu.forEach(t),mi=c(e),W=l(e,"P",{});var Ee=a(W);Dc=o(Ee,"La biblioth\xE8que \u{1F917} "),ko=l(Ee,"EM",{});var _z=a(ko);Lc=o(_z,"Tokenizers"),_z.forEach(t),Oc=o(Ee," a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),vo=l(Ee,"EM",{});var hz=a(vo);Mc=o(hz,"tokenizer"),hz.forEach(t),yc=o(Ee," \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),_o=l(Ee,"EM",{});var Ez=a(_o);Sc=o(Ez,"tokenizer"),Ez.forEach(t),Nc=o(Ee," \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=l(Ee,"A",{href:!0});var zz=a(Sn);Bc=o(zz,"section 2"),zz.forEach(t),Ac=o(Ee,". Vous serez alors en mesure de construire n\u2019importe quel type de "),ho=l(Ee,"EM",{});var $z=a(ho);Fc=o($z,"tokenizer"),$z.forEach(t),Uc=o(Ee," auquel vous pouvez penser !"),Ee.forEach(t),di=c(e),d(Ks.$$.fragment,e),fi=c(e),ls=l(e,"P",{});var ec=a(ls);Rc=o(ec,"Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),Eo=l(ec,"CODE",{});var qz=a(Eo);Wc=o(qz,"Tokenizer"),qz.forEach(t),Gc=o(ec," avec les blocs de construction regroup\xE9s en sous-modules :"),ec.forEach(t),ki=c(e),G=l(e,"UL",{});var ze=a(G);we=l(ze,"LI",{});var bn=a(we);zo=l(bn,"CODE",{});var jz=a(zo);Ic=o(jz,"normalizers"),jz.forEach(t),Vc=o(bn," contient tous les types de "),$o=l(bn,"CODE",{});var xz=a($o);Xc=o(xz,"Normalizer"),xz.forEach(t),Kc=o(bn," que vous pouvez utiliser (liste compl\xE8te "),Hs=l(bn,"A",{href:!0,rel:!0});var bz=a(Hs);Hc=o(bz,"ici"),bz.forEach(t),Jc=o(bn,"),"),bn.forEach(t),Yc=c(ze),Te=l(ze,"LI",{});var gn=a(Te);qo=l(gn,"CODE",{});var gz=a(qo);Zc=o(gz,"pre_tokenizers"),gz.forEach(t),Qc=o(gn," contient tous les types de "),jo=l(gn,"CODE",{});var Pz=a(jo);em=o(Pz,"PreTokenizer"),Pz.forEach(t),sm=o(gn," que vous pouvez utiliser (liste compl\xE8te "),Js=l(gn,"A",{href:!0,rel:!0});var wz=a(Js);tm=o(wz,"ici"),wz.forEach(t),nm=o(gn,"),"),gn.forEach(t),om=c(ze),I=l(ze,"LI",{});var ie=a(I);xo=l(ie,"CODE",{});var Tz=a(xo);rm=o(Tz,"models"),Tz.forEach(t),lm=o(ie," contient les diff\xE9rents types de "),bo=l(ie,"CODE",{});var Cz=a(bo);am=o(Cz,"Model"),Cz.forEach(t),im=o(ie," que vous pouvez utiliser, comme "),go=l(ie,"CODE",{});var Dz=a(go);pm=o(Dz,"BPE"),Dz.forEach(t),um=o(ie,", "),Po=l(ie,"CODE",{});var Lz=a(Po);cm=o(Lz,"WordPiece"),Lz.forEach(t),mm=o(ie,", et "),wo=l(ie,"CODE",{});var Oz=a(wo);dm=o(Oz,"Unigram"),Oz.forEach(t),fm=o(ie," (liste compl\xE8te "),Ys=l(ie,"A",{href:!0,rel:!0});var Mz=a(Ys);km=o(Mz,"ici"),Mz.forEach(t),vm=o(ie,"),"),ie.forEach(t),_m=c(ze),Ce=l(ze,"LI",{});var Pn=a(Ce);To=l(Pn,"CODE",{});var yz=a(To);hm=o(yz,"trainers"),yz.forEach(t),Em=o(Pn," contient tous les diff\xE9rents types de "),Co=l(Pn,"CODE",{});var Sz=a(Co);zm=o(Sz,"Trainer"),Sz.forEach(t),$m=o(Pn," que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=l(Pn,"A",{href:!0,rel:!0});var Nz=a(Zs);qm=o(Nz,"ici"),Nz.forEach(t),jm=o(Pn,"),"),Pn.forEach(t),xm=c(ze),De=l(ze,"LI",{});var wn=a(De);Do=l(wn,"CODE",{});var Bz=a(Do);bm=o(Bz,"post_processors"),Bz.forEach(t),gm=o(wn," contient les diff\xE9rents types de "),Lo=l(wn,"CODE",{});var Az=a(Lo);Pm=o(Az,"PostProcessor"),Az.forEach(t),wm=o(wn," que vous pouvez utiliser (liste compl\xE8te "),Qs=l(wn,"A",{href:!0,rel:!0});var Fz=a(Qs);Tm=o(Fz,"ici"),Fz.forEach(t),Cm=o(wn,"),"),wn.forEach(t),Dm=c(ze),Le=l(ze,"LI",{});var Tn=a(Le);Oo=l(Tn,"CODE",{});var Uz=a(Oo);Lm=o(Uz,"decoders"),Uz.forEach(t),Om=o(Tn," contient les diff\xE9rents types de "),Mo=l(Tn,"CODE",{});var Rz=a(Mo);Mm=o(Rz,"Decoder"),Rz.forEach(t),ym=o(Tn," que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=l(Tn,"A",{href:!0,rel:!0});var Wz=a(et);Sm=o(Wz,"ici"),Wz.forEach(t),Nm=o(Tn,")."),Tn.forEach(t),ze.forEach(t),vi=c(e),as=l(e,"P",{});var sc=a(as);Bm=o(sc,"Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=l(sc,"A",{href:!0,rel:!0});var Gz=a(st);Am=o(Gz,"ici"),Gz.forEach(t),Fm=o(sc,"."),sc.forEach(t),_i=c(e),Ye=l(e,"H2",{class:!0});var tc=a(Ye);is=l(tc,"A",{id:!0,class:!0,href:!0});var Iz=a(is);yo=l(Iz,"SPAN",{});var Vz=a(yo);d(tt.$$.fragment,Vz),Vz.forEach(t),Iz.forEach(t),Um=c(tc),So=l(tc,"SPAN",{});var Xz=a(So);Rm=o(Xz,"Acquisition d'un corpus"),Xz.forEach(t),tc.forEach(t),hi=c(e),ue=l(e,"P",{});var Cs=a(ue);Wm=o(Cs,"Pour entra\xEEner notre nouveau "),No=l(Cs,"EM",{});var Kz=a(No);Gm=o(Kz,"tokenizer"),Kz.forEach(t),Im=o(Cs,", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=l(Cs,"A",{href:!0});var Hz=a(Nn);Vm=o(Hz,"d\xE9but du chapitre"),Hz.forEach(t),Xm=o(Cs,", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=l(Cs,"A",{href:!0,rel:!0});var Jz=a(nt);Km=o(Jz,"WikiText-2"),Jz.forEach(t),Hm=o(Cs," :"),Cs.forEach(t),Ei=c(e),d(ot.$$.fragment,e),zi=c(e),Oe=l(e,"P",{});var eo=a(Oe);Jm=o(eo,"La fonction "),Bo=l(eo,"CODE",{});var Yz=a(Bo);Ym=o(Yz,"get_training_corpus()"),Yz.forEach(t),Zm=o(eo," est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Ao=l(eo,"EM",{});var Zz=a(Ao);Qm=o(Zz,"tokenizer"),Zz.forEach(t),ed=o(eo,"."),eo.forEach(t),$i=c(e),ps=l(e,"P",{});var nc=a(ps);sd=o(nc,"\u{1F917} "),Fo=l(nc,"EM",{});var Qz=a(Fo);td=o(Qz,"Tokenizers"),Qz.forEach(t),nd=o(nc," peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),nc.forEach(t),qi=c(e),d(rt.$$.fragment,e),ji=c(e),ce=l(e,"P",{});var Ds=a(ce);od=o(Ds,"Ensuite, nous vous montrerons comment construire vos propres "),Uo=l(Ds,"EM",{});var e$=a(Uo);rd=o(e$,"tokenizers"),e$.forEach(t),ld=o(Ds," pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Ro=l(Ds,"EM",{});var s$=a(Ro);ad=o(s$,"WordPiece"),s$.forEach(t),id=o(Ds,", BPE et "),Wo=l(Ds,"EM",{});var t$=a(Wo);pd=o(t$,"Unigram"),t$.forEach(t),ud=o(Ds,". Commen\xE7ons par BERT !"),Ds.forEach(t),xi=c(e),Ze=l(e,"H2",{class:!0});var oc=a(Ze);us=l(oc,"A",{id:!0,class:!0,href:!0});var n$=a(us);Go=l(n$,"SPAN",{});var o$=a(Go);d(lt.$$.fragment,o$),o$.forEach(t),n$.forEach(t),cd=c(oc),at=l(oc,"SPAN",{});var rc=a(at);md=o(rc,"Construire un "),Io=l(rc,"I",{});var r$=a(Io);dd=o(r$,"tokenizer WordPiece"),r$.forEach(t),fd=o(rc," \xE0 partir de z\xE9ro"),rc.forEach(t),oc.forEach(t),bi=c(e),b=l(e,"P",{});var B=a(b);kd=o(B,"Pour construire un "),Vo=l(B,"EM",{});var l$=a(Vo);vd=o(l$,"tokenizer"),l$.forEach(t),_d=o(B," avec la biblioth\xE8que \u{1F917} "),Xo=l(B,"EM",{});var a$=a(Xo);hd=o(a$,"Tokenizers"),a$.forEach(t),Ed=o(B,", nous commen\xE7ons par instancier un objet "),Ko=l(B,"CODE",{});var i$=a(Ko);zd=o(i$,"Tokenizer"),i$.forEach(t),$d=o(B," avec un "),Ho=l(B,"CODE",{});var p$=a(Ho);qd=o(p$,"model"),p$.forEach(t),jd=o(B,". Puis nous d\xE9finissons ses attributs "),Jo=l(B,"CODE",{});var u$=a(Jo);xd=o(u$,"normalizer"),u$.forEach(t),bd=o(B,", "),Yo=l(B,"CODE",{});var c$=a(Yo);gd=o(c$,"pre_tokenizer"),c$.forEach(t),Pd=o(B,", "),Zo=l(B,"CODE",{});var m$=a(Zo);wd=o(m$,"post_processor"),m$.forEach(t),Td=o(B," et "),Qo=l(B,"CODE",{});var d$=a(Qo);Cd=o(d$,"decoder"),d$.forEach(t),Dd=o(B," aux valeurs que nous voulons."),B.forEach(t),gi=c(e),Me=l(e,"P",{});var so=a(Me);Ld=o(so,"Pour cet exemple, nous allons cr\xE9er un "),er=l(so,"CODE",{});var f$=a(er);Od=o(f$,"Tokenizer"),f$.forEach(t),Md=o(so," avec un mod\xE8le "),sr=l(so,"EM",{});var k$=a(sr);yd=o(k$,"WordPiece"),k$.forEach(t),Sd=o(so," :"),so.forEach(t),Pi=c(e),d(it.$$.fragment,e),wi=c(e),me=l(e,"P",{});var Ls=a(me);Nd=o(Ls,"Nous devons sp\xE9cifier le "),tr=l(Ls,"CODE",{});var v$=a(tr);Bd=o(v$,"unk_token"),v$.forEach(t),Ad=o(Ls," pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),nr=l(Ls,"CODE",{});var _$=a(nr);Fd=o(_$,"vocab"),_$.forEach(t),Ud=o(Ls," de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),or=l(Ls,"CODE",{});var h$=a(or);Rd=o(h$,"max_input_chars_per_word"),h$.forEach(t),Wd=o(Ls,", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ls.forEach(t),Ti=c(e),g=l(e,"P",{});var A=a(g);Gd=o(A,"La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),rr=l(A,"CODE",{});var E$=a(rr);Id=o(E$,"BertNormalizer"),E$.forEach(t),Vd=o(A," a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),lr=l(A,"CODE",{});var z$=a(lr);Xd=o(z$,"lowercase"),z$.forEach(t),Kd=o(A," pour mettre le texte en minuscule, "),ar=l(A,"CODE",{});var $$=a(ar);Hd=o($$,"strip_accents"),$$.forEach(t),Jd=o(A," qui enl\xE8ve les accents, "),ir=l(A,"CODE",{});var q$=a(ir);Yd=o(q$,"clean_text"),q$.forEach(t),Zd=o(A," pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),pr=l(A,"CODE",{});var j$=a(pr);Qd=o(j$,"handle_chinese_chars"),j$.forEach(t),ef=o(A," qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),ur=l(A,"EM",{});var x$=a(ur);sf=o(x$,"tokenizer"),x$.forEach(t),tf=c(A),cr=l(A,"CODE",{});var b$=a(cr);nf=o(b$,"bert-base-uncased"),b$.forEach(t),of=o(A,", nous pouvons simplement d\xE9finir ce "),mr=l(A,"EM",{});var g$=a(mr);rf=o(g$,"normalizer"),g$.forEach(t),lf=o(A," :"),A.forEach(t),Ci=c(e),d(pt.$$.fragment,e),Di=c(e),V=l(e,"P",{});var $e=a(V);af=o($e,"Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),dr=l($e,"EM",{});var P$=a(dr);pf=o(P$,"tokenizer"),P$.forEach(t),uf=o($e,", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),fr=l($e,"EM",{});var w$=a(fr);cf=o(w$,"Tokenizers"),w$.forEach(t),mf=o($e,". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),kr=l($e,"CODE",{});var T$=a(kr);df=o(T$,"Lowercase"),T$.forEach(t),ff=o($e," et un normaliseur "),vr=l($e,"CODE",{});var C$=a(vr);kf=o(C$,"StripAccents"),C$.forEach(t),vf=o($e,". Il est possible de composer plusieurs normaliseurs en utilisant une "),_r=l($e,"CODE",{});var D$=a(_r);_f=o(D$,"Sequence"),D$.forEach(t),hf=o($e," :"),$e.forEach(t),Li=c(e),d(ut.$$.fragment,e),Oi=c(e),ye=l(e,"P",{});var to=a(ye);Ef=o(to,"Nous utilisons \xE9galement un normaliseur Unicode "),hr=l(to,"CODE",{});var L$=a(hr);zf=o(L$,"NFD"),L$.forEach(t),$f=o(to,", car sinon "),Er=l(to,"CODE",{});var O$=a(Er);qf=o(O$,"StripAccents"),O$.forEach(t),jf=o(to," ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),to.forEach(t),Mi=c(e),Se=l(e,"P",{});var no=a(Se);xf=o(no,"Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),zr=l(no,"CODE",{});var M$=a(zr);bf=o(M$,"normalize_str()"),M$.forEach(t),gf=o(no," du "),$r=l(no,"CODE",{});var y$=a($r);Pf=o(y$,"normalizer"),y$.forEach(t),wf=o(no," pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),no.forEach(t),yi=c(e),d(ct.$$.fragment,e),Si=c(e),d(mt.$$.fragment,e),Ni=c(e),d(cs.$$.fragment,e),Bi=c(e),ms=l(e,"P",{});var lc=a(ms);Tf=o(lc,"L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),qr=l(lc,"CODE",{});var S$=a(qr);Cf=o(S$,"BertPreTokenizer"),S$.forEach(t),Df=o(lc," pr\xE9construit que nous pouvons utiliser :"),lc.forEach(t),Ai=c(e),d(dt.$$.fragment,e),Fi=c(e),Bn=l(e,"P",{});var N$=a(Bn);Lf=o(N$,"Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),N$.forEach(t),Ui=c(e),d(ft.$$.fragment,e),Ri=c(e),ds=l(e,"P",{});var ac=a(ds);Of=o(ac,"Notez que le "),jr=l(ac,"CODE",{});var B$=a(jr);Mf=o(B$,"Whitespace"),B$.forEach(t),yf=o(ac," divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),ac.forEach(t),Wi=c(e),d(kt.$$.fragment,e),Gi=c(e),d(vt.$$.fragment,e),Ii=c(e),fs=l(e,"P",{});var ic=a(fs);Sf=o(ic,"Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),xr=l(ic,"CODE",{});var A$=a(xr);Nf=o(A$,"WhitespaceSplit"),A$.forEach(t),Bf=o(ic," \xE0 la place :"),ic.forEach(t),Vi=c(e),d(_t.$$.fragment,e),Xi=c(e),d(ht.$$.fragment,e),Ki=c(e),ks=l(e,"P",{});var pc=a(ks);Af=o(pc,"Comme pour les normaliseurs, vous pouvez utiliser une "),br=l(pc,"CODE",{});var F$=a(br);Ff=o(F$,"Sequence"),F$.forEach(t),Uf=o(pc," pour composer plusieurs pr\xE9tokenizers :"),pc.forEach(t),Hi=c(e),d(Et.$$.fragment,e),Ji=c(e),d(zt.$$.fragment,e),Yi=c(e),de=l(e,"P",{});var Os=a(de);Rf=o(Os,"L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),gr=l(Os,"CODE",{});var U$=a(gr);Wf=o(U$,"WordPieceTrainer"),U$.forEach(t),Gf=o(Os,". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Pr=l(Os,"EM",{});var R$=a(Pr);If=o(R$,"Tokenizers"),R$.forEach(t),Vf=o(Os," est que vous devez lui passer tous les "),wr=l(Os,"EM",{});var W$=a(wr);Xf=o(W$,"tokens"),W$.forEach(t),Kf=o(Os," sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Os.forEach(t),Zi=c(e),d($t.$$.fragment,e),Qi=c(e),O=l(e,"P",{});var Q=a(O);Hf=o(Q,"En plus de sp\xE9cifier la "),Tr=l(Q,"CODE",{});var G$=a(Tr);Jf=o(G$,"vocab_size"),G$.forEach(t),Yf=o(Q," et les "),Cr=l(Q,"CODE",{});var I$=a(Cr);Zf=o(I$,"special_tokens"),I$.forEach(t),Qf=o(Q,", nous pouvons d\xE9finir la "),Dr=l(Q,"CODE",{});var V$=a(Dr);ek=o(V$,"min_frequency"),V$.forEach(t),sk=o(Q," (le nombre de fois qu\u2019un "),Lr=l(Q,"EM",{});var X$=a(Lr);tk=o(X$,"token"),X$.forEach(t),nk=o(Q," doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Or=l(Q,"CODE",{});var K$=a(Or);ok=o(K$,"continuing_subword_prefix"),K$.forEach(t),rk=o(Q," (si nous voulons utiliser quelque chose de diff\xE9rent de "),Mr=l(Q,"CODE",{});var H$=a(Mr);lk=o(H$,"##"),H$.forEach(t),ak=o(Q,")."),Q.forEach(t),ep=c(e),An=l(e,"P",{});var J$=a(An);ik=o(J$,"Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),J$.forEach(t),sp=c(e),d(qt.$$.fragment,e),tp=c(e),Ne=l(e,"P",{});var oo=a(Ne);pk=o(oo,"Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),yr=l(oo,"EM",{});var Y$=a(yr);uk=o(Y$,"tokenizer"),Y$.forEach(t),ck=o(oo," qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Sr=l(oo,"CODE",{});var Z$=a(Sr);mk=o(Z$,"WordPiece"),Z$.forEach(t),dk=o(oo," vide au pr\xE9alable) :"),oo.forEach(t),np=c(e),d(jt.$$.fragment,e),op=c(e),Be=l(e,"P",{});var ro=a(Be);fk=o(ro,"Dans les deux cas, nous pouvons ensuite tester le "),Nr=l(ro,"EM",{});var Q$=a(Nr);kk=o(Q$,"tokenizer"),Q$.forEach(t),vk=o(ro," sur un texte en appelant la m\xE9thode "),Br=l(ro,"CODE",{});var e1=a(Br);_k=o(e1,"encode()"),e1.forEach(t),hk=o(ro," :"),ro.forEach(t),rp=c(e),d(xt.$$.fragment,e),lp=c(e),d(bt.$$.fragment,e),ap=c(e),j=l(e,"P",{});var L=a(j);Ek=o(L,"L\u2019encodage obtenu est un "),Ar=l(L,"CODE",{});var s1=a(Ar);zk=o(s1,"Encoding"),s1.forEach(t),$k=o(L," contenant toutes les sorties n\xE9cessaires du "),Fr=l(L,"EM",{});var t1=a(Fr);qk=o(t1,"tokenizer"),t1.forEach(t),jk=o(L," dans ses diff\xE9rents attributs : "),Ur=l(L,"CODE",{});var n1=a(Ur);xk=o(n1,"ids"),n1.forEach(t),bk=o(L,", "),Rr=l(L,"CODE",{});var o1=a(Rr);gk=o(o1,"type_ids"),o1.forEach(t),Pk=o(L,", "),Wr=l(L,"CODE",{});var r1=a(Wr);wk=o(r1,"tokens"),r1.forEach(t),Tk=o(L,", "),Gr=l(L,"CODE",{});var l1=a(Gr);Ck=o(l1,"offsets"),l1.forEach(t),Dk=o(L,", "),Ir=l(L,"CODE",{});var a1=a(Ir);Lk=o(a1,"attention_mask"),a1.forEach(t),Ok=o(L,", "),Vr=l(L,"CODE",{});var i1=a(Vr);Mk=o(i1,"special_tokens_mask"),i1.forEach(t),yk=o(L," et "),Xr=l(L,"CODE",{});var p1=a(Xr);Sk=o(p1,"overflowing"),p1.forEach(t),Nk=o(L,"."),L.forEach(t),ip=c(e),P=l(e,"P",{});var F=a(P);Bk=o(F,"La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Kr=l(F,"EM",{});var u1=a(Kr);Ak=o(u1,"token"),u1.forEach(t),Fk=c(F),Hr=l(F,"CODE",{});var c1=a(Hr);Uk=o(c1,"[CLS]"),c1.forEach(t),Rk=o(F," au d\xE9but et le "),Jr=l(F,"EM",{});var m1=a(Jr);Wk=o(m1,"token"),m1.forEach(t),Gk=c(F),Yr=l(F,"CODE",{});var d1=a(Yr);Ik=o(d1,"[SEP]"),d1.forEach(t),Vk=o(F," \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Zr=l(F,"CODE",{});var f1=a(Zr);Xk=o(f1,"TemplateProcessor"),f1.forEach(t),Kk=o(F," pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Qr=l(F,"EM",{});var k1=a(Qr);Hk=o(k1,"tokens"),k1.forEach(t),Jk=c(F),el=l(F,"CODE",{});var v1=a(el);Yk=o(v1,"[CLS]"),v1.forEach(t),Zk=o(F," et "),sl=l(F,"CODE",{});var _1=a(sl);Qk=o(_1,"[SEP]"),_1.forEach(t),ev=o(F," dans le vocabulaire :"),F.forEach(t),pp=c(e),d(gt.$$.fragment,e),up=c(e),d(Pt.$$.fragment,e),cp=c(e),M=l(e,"P",{});var ee=a(M);sv=o(ee,"Pour \xE9crire le gabarit pour "),tl=l(ee,"CODE",{});var h1=a(tl);tv=o(h1,"TemplateProcessor"),h1.forEach(t),nv=o(ee,", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),nl=l(ee,"EM",{});var E1=a(nl);ov=o(E1,"tokens"),E1.forEach(t),rv=o(ee," sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),ol=l(ee,"CODE",{});var z1=a(ol);lv=o(z1,"$A"),z1.forEach(t),av=o(ee,", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),rl=l(ee,"CODE",{});var $1=a(rl);iv=o($1,"$B"),$1.forEach(t),pv=o(ee,". Pour chacun de ces \xE9l\xE9ments ("),ll=l(ee,"EM",{});var q1=a(ll);uv=o(q1,"tokens"),q1.forEach(t),cv=o(ee," sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),al=l(ee,"EM",{});var j1=a(al);mv=o(j1,"token"),j1.forEach(t),dv=o(ee," correspondant apr\xE8s un deux-points."),ee.forEach(t),mp=c(e),Fn=l(e,"P",{});var x1=a(Fn);fv=o(x1,"Le gabarit classique de BERT est donc d\xE9fini comme suit :"),x1.forEach(t),dp=c(e),d(wt.$$.fragment,e),fp=c(e),Ae=l(e,"P",{});var lo=a(Ae);kv=o(lo,"Notez que nous devons transmettre les identifiants des "),il=l(lo,"EM",{});var b1=a(il);vv=o(b1,"tokens"),b1.forEach(t),_v=o(lo," sp\xE9ciaux afin que le "),pl=l(lo,"EM",{});var g1=a(pl);hv=o(g1,"tokenizer"),g1.forEach(t),Ev=o(lo," puisse les convertir correctement."),lo.forEach(t),kp=c(e),Un=l(e,"P",{});var P1=a(Un);zv=o(P1,"Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),P1.forEach(t),vp=c(e),d(Tt.$$.fragment,e),_p=c(e),d(Ct.$$.fragment,e),hp=c(e),Rn=l(e,"P",{});var w1=a(Rn);$v=o(w1,"Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),w1.forEach(t),Ep=c(e),d(Dt.$$.fragment,e),zp=c(e),d(Lt.$$.fragment,e),$p=c(e),vs=l(e,"P",{});var uc=a(vs);qv=o(uc,"Nous avons presque fini de construire ce "),ul=l(uc,"EM",{});var T1=a(ul);jv=o(T1,"tokenizer"),T1.forEach(t),xv=o(uc," \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),uc.forEach(t),qp=c(e),d(Ot.$$.fragment,e),jp=c(e),_s=l(e,"P",{});var cc=a(_s);bv=o(cc,"Testons-le sur notre pr\xE9c\xE9dent "),cl=l(cc,"CODE",{});var C1=a(cl);gv=o(C1,"encoding"),C1.forEach(t),Pv=o(cc," :"),cc.forEach(t),xp=c(e),d(Mt.$$.fragment,e),bp=c(e),d(yt.$$.fragment,e),gp=c(e),hs=l(e,"P",{});var mc=a(hs);wv=o(mc,"G\xE9nial ! Nous pouvons enregistrer notre "),ml=l(mc,"EM",{});var D1=a(ml);Tv=o(D1,"tokenizer"),D1.forEach(t),Cv=o(mc," dans un seul fichier JSON comme ceci :"),mc.forEach(t),Pp=c(e),d(St.$$.fragment,e),wp=c(e),Fe=l(e,"P",{});var ao=a(Fe);Dv=o(ao,"Nous pouvons alors recharger ce fichier dans un objet "),dl=l(ao,"CODE",{});var L1=a(dl);Lv=o(L1,"Tokenizer"),L1.forEach(t),Ov=o(ao," avec la m\xE9thode "),fl=l(ao,"CODE",{});var O1=a(fl);Mv=o(O1,"from_file()"),O1.forEach(t),yv=o(ao," :"),ao.forEach(t),Tp=c(e),d(Nt.$$.fragment,e),Cp=c(e),y=l(e,"P",{});var se=a(y);Sv=o(se,"Pour utiliser ce "),kl=l(se,"EM",{});var M1=a(kl);Nv=o(M1,"tokenizer"),M1.forEach(t),Bv=o(se," dans \u{1F917} "),vl=l(se,"EM",{});var y1=a(vl);Av=o(y1,"Transformers"),y1.forEach(t),Fv=o(se,", nous devons l\u2019envelopper dans un "),_l=l(se,"CODE",{});var S1=a(_l);Uv=o(S1,"PreTrainedTokenizerFast"),S1.forEach(t),Rv=o(se,". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),hl=l(se,"EM",{});var N1=a(hl);Wv=o(N1,"tokenizer"),N1.forEach(t),Gv=o(se," correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),El=l(se,"CODE",{});var B1=a(El);Iv=o(B1,"BertTokenizerFast"),B1.forEach(t),Vv=o(se,"). Si vous appliquez cette logique pour construire un tout nouveau "),zl=l(se,"EM",{});var A1=a(zl);Xv=o(A1,"tokenizer"),A1.forEach(t),Kv=o(se,", vous devrez utiliser la premi\xE8re option."),se.forEach(t),Dp=c(e),$=l(e,"P",{});var q=a($);Hv=o(q,"Pour envelopper le "),$l=l(q,"EM",{});var F1=a($l);Jv=o(F1,"tokenizer"),F1.forEach(t),Yv=o(q," dans un "),ql=l(q,"CODE",{});var U1=a(ql);Zv=o(U1,"PreTrainedTokenizerFast"),U1.forEach(t),Qv=o(q,", nous pouvons soit passer le "),jl=l(q,"EM",{});var R1=a(jl);e_=o(R1,"tokenizer"),R1.forEach(t),s_=o(q," que nous avons construit comme un "),xl=l(q,"CODE",{});var W1=a(xl);t_=o(W1,"tokenizer_object"),W1.forEach(t),n_=o(q,", soit passer le fichier de "),bl=l(q,"EM",{});var G1=a(bl);o_=o(G1,"tokenizer"),G1.forEach(t),r_=o(q," que nous avons sauvegard\xE9 comme "),gl=l(q,"CODE",{});var I1=a(gl);l_=o(I1,"tokenizer_file"),I1.forEach(t),a_=o(q,". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),Pl=l(q,"EM",{});var V1=a(Pl);i_=o(V1,"tokens"),V1.forEach(t),p_=o(q," sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),wl=l(q,"CODE",{});var X1=a(wl);u_=o(X1,"tokenizer"),X1.forEach(t),c_=o(q," quel "),Tl=l(q,"EM",{});var K1=a(Tl);m_=o(K1,"token"),K1.forEach(t),d_=o(q," est le "),Cl=l(q,"EM",{});var H1=a(Cl);f_=o(H1,"token"),H1.forEach(t),k_=o(q," de masque, quel est le "),Dl=l(q,"EM",{});var J1=a(Dl);v_=o(J1,"token"),J1.forEach(t),Ll=l(q,"CODE",{});var Y1=a(Ll);__=o(Y1,"[CLS]"),Y1.forEach(t),h_=o(q,", etc :"),q.forEach(t),Lp=c(e),d(Bt.$$.fragment,e),Op=c(e),fe=l(e,"P",{});var Ms=a(fe);E_=o(Ms,"Si vous utilisez une classe de "),Ol=l(Ms,"EM",{});var Z1=a(Ol);z_=o(Z1,"tokenizer"),Z1.forEach(t),$_=o(Ms," sp\xE9cifique (comme "),Ml=l(Ms,"CODE",{});var Q1=a(Ml);q_=o(Q1,"BertTokenizerFast"),Q1.forEach(t),j_=o(Ms,"), vous aurez seulement besoin de sp\xE9cifier les "),yl=l(Ms,"EM",{});var e7=a(yl);x_=o(e7,"tokens"),e7.forEach(t),b_=o(Ms," sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Ms.forEach(t),Mp=c(e),d(At.$$.fragment,e),yp=c(e),S=l(e,"P",{});var te=a(S);g_=o(te,"Vous pouvez ensuite utiliser ce "),Sl=l(te,"EM",{});var s7=a(Sl);P_=o(s7,"tokenizer"),s7.forEach(t),w_=o(te," comme n\u2019importe quel autre "),Nl=l(te,"EM",{});var t7=a(Nl);T_=o(t7,"tokenizer"),t7.forEach(t),C_=o(te," de \u{1F917} "),Bl=l(te,"EM",{});var n7=a(Bl);D_=o(n7,"Transformers"),n7.forEach(t),L_=o(te,". Vous pouvez le sauvegarder avec la m\xE9thode "),Al=l(te,"CODE",{});var o7=a(Al);O_=o(o7,"save_pretrained()"),o7.forEach(t),M_=o(te," ou le t\xE9l\xE9charger sur le "),Fl=l(te,"EM",{});var r7=a(Fl);y_=o(r7,"Hub"),r7.forEach(t),S_=o(te," avec la m\xE9thode "),Ul=l(te,"CODE",{});var l7=a(Ul);N_=o(l7,"push_to_hub()"),l7.forEach(t),B_=o(te,"."),te.forEach(t),Sp=c(e),Ue=l(e,"P",{});var io=a(Ue);A_=o(io,"Maintenant que nous avons vu comment construire un "),Rl=l(io,"EM",{});var a7=a(Rl);F_=o(a7,"tokenizer WordPiece"),a7.forEach(t),U_=o(io,", faisons de m\xEAme pour un "),Wl=l(io,"EM",{});var i7=a(Wl);R_=o(i7,"tokenizer"),i7.forEach(t),W_=o(io," BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),io.forEach(t),Np=c(e),Qe=l(e,"H2",{class:!0});var dc=a(Qe);Es=l(dc,"A",{id:!0,class:!0,href:!0});var p7=a(Es);Gl=l(p7,"SPAN",{});var u7=a(Gl);d(Ft.$$.fragment,u7),u7.forEach(t),p7.forEach(t),G_=c(dc),Ut=l(dc,"SPAN",{});var fc=a(Ut);I_=o(fc,"Construire un "),Il=l(fc,"I",{});var c7=a(Il);V_=o(c7,"tokenizer"),c7.forEach(t),X_=o(fc," BPE \xE0 partir de z\xE9ro"),fc.forEach(t),dc.forEach(t),Bp=c(e),ke=l(e,"P",{});var ys=a(ke);K_=o(ys,"Construisons maintenant un "),Vl=l(ys,"EM",{});var m7=a(Vl);H_=o(m7,"tokenizer"),m7.forEach(t),J_=o(ys," BPE. Comme pour le "),Xl=l(ys,"EM",{});var d7=a(Xl);Y_=o(d7,"tokenizer"),d7.forEach(t),Z_=o(ys," BERT, nous commen\xE7ons par initialiser un "),Kl=l(ys,"CODE",{});var f7=a(Kl);Q_=o(f7,"Tokenizer"),f7.forEach(t),e2=o(ys," avec un mod\xE8le BPE :"),ys.forEach(t),Ap=c(e),d(Rt.$$.fragment,e),Fp=c(e),ve=l(e,"P",{});var Ss=a(ve);s2=o(Ss,"Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Hl=l(Ss,"CODE",{});var k7=a(Hl);t2=o(k7,"vocab"),k7.forEach(t),n2=o(Ss," et le "),Jl=l(Ss,"CODE",{});var v7=a(Jl);o2=o(v7,"merges"),v7.forEach(t),r2=o(Ss," dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Yl=l(Ss,"CODE",{});var _7=a(Yl);l2=o(_7,"unk_token"),_7.forEach(t),a2=o(Ss," parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Ss.forEach(t),Up=c(e),Wn=l(e,"P",{});var h7=a(Wn);i2=o(h7,"GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),h7.forEach(t),Rp=c(e),d(Wt.$$.fragment,e),Wp=c(e),zs=l(e,"P",{});var kc=a(zs);p2=o(kc,"L\u2019option que nous avons ajout\xE9e \xE0 "),Zl=l(kc,"CODE",{});var E7=a(Zl);u2=o(E7,"ByteLevel"),E7.forEach(t),c2=o(kc," ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),kc.forEach(t),Gp=c(e),d(Gt.$$.fragment,e),Ip=c(e),d(It.$$.fragment,e),Vp=c(e),Re=l(e,"P",{});var po=a(Re);m2=o(po,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Ql=l(po,"EM",{});var z7=a(Ql);d2=o(z7,"token"),z7.forEach(t),f2=o(po," sp\xE9cial est le "),ea=l(po,"EM",{});var $7=a(ea);k2=o($7,"token"),$7.forEach(t),v2=o(po," de fin de texte :"),po.forEach(t),Xp=c(e),d(Vt.$$.fragment,e),Kp=c(e),N=l(e,"P",{});var ne=a(N);_2=o(ne,"Comme avec le "),sa=l(ne,"CODE",{});var q7=a(sa);h2=o(q7,"WordPieceTrainer"),q7.forEach(t),E2=o(ne,", ainsi que le "),ta=l(ne,"CODE",{});var j7=a(ta);z2=o(j7,"vocab_size"),j7.forEach(t),$2=o(ne," et le "),na=l(ne,"CODE",{});var x7=a(na);q2=o(x7,"special_tokens"),x7.forEach(t),j2=o(ne,", nous pouvons sp\xE9cifier la "),oa=l(ne,"CODE",{});var b7=a(oa);x2=o(b7,"min_frequency"),b7.forEach(t),b2=o(ne," si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ra=l(ne,"CODE",{});var g7=a(ra);g2=o(g7,"</w>"),g7.forEach(t),P2=o(ne,"), nous pouvons le d\xE9finir avec "),la=l(ne,"CODE",{});var P7=a(la);w2=o(P7,"end_of_word_suffix"),P7.forEach(t),T2=o(ne,"."),ne.forEach(t),Hp=c(e),$s=l(e,"P",{});var vc=a($s);C2=o(vc,"Ce "),aa=l(vc,"EM",{});var w7=a(aa);D2=o(w7,"tokenizer"),w7.forEach(t),L2=o(vc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),vc.forEach(t),Jp=c(e),d(Xt.$$.fragment,e),Yp=c(e),Gn=l(e,"P",{});var T7=a(Gn);O2=o(T7,"Regardons la tokenisation d\u2019un exemple de texte :"),T7.forEach(t),Zp=c(e),d(Kt.$$.fragment,e),Qp=c(e),d(Ht.$$.fragment,e),eu=c(e),qs=l(e,"P",{});var _c=a(qs);M2=o(_c,"Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),ia=l(_c,"EM",{});var C7=a(ia);y2=o(C7,"tokenizer"),C7.forEach(t),S2=o(_c," du GPT-2 comme suit :"),_c.forEach(t),su=c(e),d(Jt.$$.fragment,e),tu=c(e),T=l(e,"P",{});var X=a(T);N2=o(X,"L\u2019option "),pa=l(X,"CODE",{});var D7=a(pa);B2=o(D7,"trim_offsets = False"),D7.forEach(t),A2=o(X," indique au post-processeur que nous devons laisser les "),ua=l(X,"EM",{});var L7=a(ua);F2=o(L7,"offsets"),L7.forEach(t),U2=o(X," des "),ca=l(X,"EM",{});var O7=a(ca);R2=o(O7,"tokens"),O7.forEach(t),W2=o(X," qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ma=l(X,"EM",{});var M7=a(ma);G2=o(M7,"offsets"),M7.forEach(t),I2=o(X," pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),da=l(X,"EM",{});var y7=a(da);V2=o(y7,"token"),y7.forEach(t),X2=o(X,"). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),fa=l(X,"CODE",{});var S7=a(fa);K2=o(S7,"'\u0120test'"),S7.forEach(t),H2=o(X," est le "),ka=l(X,"EM",{});var N7=a(ka);J2=o(N7,"token"),N7.forEach(t),Y2=o(X," \xE0 l\u2019index 4 :"),X.forEach(t),nu=c(e),d(Yt.$$.fragment,e),ou=c(e),d(Zt.$$.fragment,e),ru=c(e),In=l(e,"P",{});var B7=a(In);Z2=o(B7,"Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),B7.forEach(t),lu=c(e),d(Qt.$$.fragment,e),au=c(e),Vn=l(e,"P",{});var A7=a(Vn);Q2=o(A7,"et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),A7.forEach(t),iu=c(e),d(en.$$.fragment,e),pu=c(e),d(sn.$$.fragment,e),uu=c(e),_e=l(e,"P",{});var Ns=a(_e);eh=o(Ns,"Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),va=l(Ns,"CODE",{});var F7=a(va);sh=o(F7,"PreTrainedTokenizerFast"),F7.forEach(t),th=o(Ns," ou un "),_a=l(Ns,"CODE",{});var U7=a(_a);nh=o(U7,"GPT2TokenizerFast"),U7.forEach(t),oh=o(Ns," si nous voulons l\u2019utiliser dans \u{1F917} "),ha=l(Ns,"EM",{});var R7=a(ha);rh=o(R7,"Transformers"),R7.forEach(t),lh=o(Ns," :"),Ns.forEach(t),cu=c(e),d(tn.$$.fragment,e),mu=c(e),Xn=l(e,"P",{});var W7=a(Xn);ah=o(W7,"ou :"),W7.forEach(t),du=c(e),d(nn.$$.fragment,e),fu=c(e),We=l(e,"P",{});var uo=a(We);ih=o(uo,"Comme dernier exemple, nous allons vous montrer comment construire un "),Ea=l(uo,"EM",{});var G7=a(Ea);ph=o(G7,"tokenizer"),G7.forEach(t),uh=c(uo),za=l(uo,"EM",{});var I7=a(za);ch=o(I7,"Unigram"),I7.forEach(t),mh=o(uo," \xE0 partir de z\xE9ro."),uo.forEach(t),ku=c(e),es=l(e,"H2",{class:!0});var hc=a(es);js=l(hc,"A",{id:!0,class:!0,href:!0});var V7=a(js);$a=l(V7,"SPAN",{});var X7=a($a);d(on.$$.fragment,X7),X7.forEach(t),V7.forEach(t),dh=c(hc),rn=l(hc,"SPAN",{});var Ec=a(rn);fh=o(Ec,"Construire un "),qa=l(Ec,"I",{});var K7=a(qa);kh=o(K7,"tokenizer Unigram"),K7.forEach(t),vh=o(Ec," \xE0 partir de z\xE9ro"),Ec.forEach(t),hc.forEach(t),vu=c(e),Y=l(e,"P",{});var Ie=a(Y);_h=o(Ie,"Construisons maintenant un "),ja=l(Ie,"EM",{});var H7=a(ja);hh=o(H7,"tokenizer"),H7.forEach(t),Eh=o(Ie," XLNet. Comme pour les "),xa=l(Ie,"EM",{});var J7=a(xa);zh=o(J7,"tokenizers"),J7.forEach(t),$h=o(Ie," pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ba=l(Ie,"CODE",{});var Y7=a(ba);qh=o(Y7,"Tokenizer"),Y7.forEach(t),jh=o(Ie," avec un mod\xE8le "),ga=l(Ie,"EM",{});var Z7=a(ga);xh=o(Z7,"Unigram"),Z7.forEach(t),bh=o(Ie," :"),Ie.forEach(t),_u=c(e),d(ln.$$.fragment,e),hu=c(e),Kn=l(e,"P",{});var Q7=a(Kn);gh=o(Q7,"Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),Q7.forEach(t),Eu=c(e),xs=l(e,"P",{});var zc=a(xs);Ph=o(zc,"Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),Pa=l(zc,"EM",{});var eq=a(Pa);wh=o(eq,"SentencePiece"),eq.forEach(t),Th=o(zc,") :"),zc.forEach(t),zu=c(e),d(an.$$.fragment,e),$u=c(e),he=l(e,"P",{});var Bs=a(he);Ch=o(Bs,"Il remplace "),wa=l(Bs,"CODE",{});var sq=a(wa);Dh=o(sq,"\u201C"),sq.forEach(t),Lh=o(Bs," et "),Ta=l(Bs,"CODE",{});var tq=a(Ta);Oh=o(tq,"\u201D"),tq.forEach(t),Mh=o(Bs," par "),Ca=l(Bs,"CODE",{});var nq=a(Ca);yh=o(nq,"\u201D"),nq.forEach(t),Sh=o(Bs," et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),Bs.forEach(t),qu=c(e),Ge=l(e,"P",{});var co=a(Ge);Nh=o(co,"Le pr\xE9tokenizer \xE0 utiliser pour tout "),Da=l(co,"EM",{});var oq=a(Da);Bh=o(oq,"tokenizer SentencePiece"),oq.forEach(t),Ah=o(co," est "),La=l(co,"CODE",{});var rq=a(La);Fh=o(rq,"Metaspace"),rq.forEach(t),Uh=o(co," :"),co.forEach(t),ju=c(e),d(pn.$$.fragment,e),xu=c(e),Hn=l(e,"P",{});var lq=a(Hn);Rh=o(lq,"Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),lq.forEach(t),bu=c(e),d(un.$$.fragment,e),gu=c(e),d(cn.$$.fragment,e),Pu=c(e),bs=l(e,"P",{});var $c=a(bs);Wh=o($c,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Oa=l($c,"EM",{});var aq=a(Oa);Gh=o(aq,"tokens"),aq.forEach(t),Ih=o($c," sp\xE9ciaux :"),$c.forEach(t),wu=c(e),d(mn.$$.fragment,e),Tu=c(e),C=l(e,"P",{});var K=a(C);Vh=o(K,"Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Ma=l(K,"CODE",{});var iq=a(Ma);Xh=o(iq,"UnigramTrainer"),iq.forEach(t),Kh=o(K," est le "),ya=l(K,"CODE",{});var pq=a(ya);Hh=o(pq,"unk_token"),pq.forEach(t),Jh=o(K,". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Sa=l(K,"EM",{});var uq=a(Sa);Yh=o(uq,"Unigram"),uq.forEach(t),Zh=o(K,", comme le "),Na=l(K,"CODE",{});var cq=a(Na);Qh=o(cq,"shrinking_factor"),cq.forEach(t),eE=o(K," pour chaque \xE9tape o\xF9 nous enlevons des "),Ba=l(K,"EM",{});var mq=a(Ba);sE=o(mq,"tokens"),mq.forEach(t),tE=o(K," (par d\xE9faut 0.75) ou le "),Aa=l(K,"CODE",{});var dq=a(Aa);nE=o(dq,"max_piece_length"),dq.forEach(t),oE=o(K," pour sp\xE9cifier la longueur maximale d\u2019un "),Fa=l(K,"EM",{});var fq=a(Fa);rE=o(fq,"token"),fq.forEach(t),lE=o(K," donn\xE9 (par d\xE9faut 16)."),K.forEach(t),Cu=c(e),gs=l(e,"P",{});var qc=a(gs);aE=o(qc,"Ce "),Ua=l(qc,"EM",{});var kq=a(Ua);iE=o(kq,"tokenizer"),kq.forEach(t),pE=o(qc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),qc.forEach(t),Du=c(e),d(dn.$$.fragment,e),Lu=c(e),Jn=l(e,"P",{});var vq=a(Jn);uE=o(vq,"Regardons la tokenisation de notre exemple :"),vq.forEach(t),Ou=c(e),d(fn.$$.fragment,e),Mu=c(e),d(kn.$$.fragment,e),yu=c(e),w=l(e,"P",{});var U=a(w);cE=o(U,"Une particularit\xE9 de XLNet est qu\u2019il place le "),Ra=l(U,"EM",{});var _q=a(Ra);mE=o(_q,"token"),_q.forEach(t),dE=c(U),Wa=l(U,"CODE",{});var hq=a(Wa);fE=o(hq,"<cls>"),hq.forEach(t),kE=o(U," \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Ga=l(U,"EM",{});var Eq=a(Ga);vE=o(Eq,"tokens"),Eq.forEach(t),_E=o(U,"). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ia=l(U,"EM",{});var zq=a(Ia);hE=o(zq,"tokens"),zq.forEach(t),EE=o(U," sp\xE9ciaux et les types d\u2019identifiant de "),Va=l(U,"EM",{});var $q=a(Va);zE=o($q,"token"),$q.forEach(t),$E=o(U," avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Xa=l(U,"EM",{});var qq=a(Xa);qE=o(qq,"tokens"),qq.forEach(t),jE=c(U),Ka=l(U,"CODE",{});var jq=a(Ka);xE=o(jq,"<cls>"),jq.forEach(t),bE=o(U," et "),Ha=l(U,"CODE",{});var xq=a(Ha);gE=o(xq,"<sep>"),xq.forEach(t),PE=o(U," :"),U.forEach(t),Su=c(e),d(vn.$$.fragment,e),Nu=c(e),d(_n.$$.fragment,e),Bu=c(e),Yn=l(e,"P",{});var bq=a(Yn);wE=o(bq,"Le mod\xE8le ressemble \xE0 ceci :"),bq.forEach(t),Au=c(e),d(hn.$$.fragment,e),Fu=c(e),Zn=l(e,"P",{});var gq=a(Zn);TE=o(gq,"Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),gq.forEach(t),Uu=c(e),d(En.$$.fragment,e),Ru=c(e),d(zn.$$.fragment,e),Wu=c(e),Ps=l(e,"P",{});var jc=a(Ps);CE=o(jc,"Enfin, nous ajoutons un d\xE9codeur "),Ja=l(jc,"CODE",{});var Pq=a(Ja);DE=o(Pq,"Metaspace"),Pq.forEach(t),LE=o(jc," :"),jc.forEach(t),Gu=c(e),d($n.$$.fragment,e),Iu=c(e),D=l(e,"P",{});var H=a(D);OE=o(H,"et on en a fini avec ce "),Ya=l(H,"EM",{});var wq=a(Ya);ME=o(wq,"tokenizer"),wq.forEach(t),yE=o(H," ! On peut le sauvegarder et l\u2019envelopper dans un "),Za=l(H,"CODE",{});var Tq=a(Za);SE=o(Tq,"PreTrainedTokenizerFast"),Tq.forEach(t),NE=o(H," ou "),Qa=l(H,"CODE",{});var Cq=a(Qa);BE=o(Cq,"XLNetTokenizerFast"),Cq.forEach(t),AE=o(H," si on veut l\u2019utiliser dans \u{1F917} "),ei=l(H,"EM",{});var Dq=a(ei);FE=o(Dq,"Transformers"),Dq.forEach(t),UE=o(H,". Une chose \xE0 noter lors de l\u2019utilisation de "),si=l(H,"CODE",{});var Lq=a(si);RE=o(Lq,"PreTrainedTokenizerFast"),Lq.forEach(t),WE=o(H," est qu\u2019en plus des "),ti=l(H,"EM",{});var Oq=a(ti);GE=o(Oq,"tokens"),Oq.forEach(t),IE=o(H," sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ni=l(H,"EM",{});var Mq=a(ni);VE=o(Mq,"Transformers"),Mq.forEach(t),XE=o(H," de rembourrer \xE0 gauche :"),H.forEach(t),Vu=c(e),d(qn.$$.fragment,e),Xu=c(e),Qn=l(e,"P",{});var yq=a(Qn);KE=o(yq,"Ou alternativement :"),yq.forEach(t),Ku=c(e),d(jn.$$.fragment,e),Hu=c(e),Z=l(e,"P",{});var Ve=a(Z);HE=o(Ve,"Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),oi=l(Ve,"EM",{});var Sq=a(oi);JE=o(Sq,"tokenizers"),Sq.forEach(t),YE=o(Ve," existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),ri=l(Ve,"EM",{});var Nq=a(ri);ZE=o(Nq,"tokenizer"),Nq.forEach(t),QE=o(Ve," que vous voulez avec la biblioth\xE8que \u{1F917} "),li=l(Ve,"EM",{});var Bq=a(li);ez=o(Bq,"Tokenizers"),Bq.forEach(t),sz=o(Ve," et pouvoir l\u2019utiliser dans \u{1F917} "),ai=l(Ve,"EM",{});var Aq=a(ai);tz=o(Aq,"Transformers"),Aq.forEach(t),nz=o(Ve,"."),Ve.forEach(t),this.h()},h(){E(z,"name","hf:doc:metadata"),E(z,"content",JSON.stringify(Yq)),E(pe,"id","construction-dun-itokenizeri-bloc-par-bloc"),E(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(pe,"href","#construction-dun-itokenizeri-bloc-par-bloc"),E(oe,"class","relative group"),E(Vs,"class","block dark:hidden"),Fq(Vs.src,oz="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||E(Vs,"src",oz),E(Vs,"alt","The tokenization pipeline."),E(Xs,"class","hidden dark:block"),Fq(Xs.src,rz="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||E(Xs,"src",rz),E(Xs,"alt","The tokenization pipeline."),E(Je,"class","flex justify-center"),E(Sn,"href","/course/fr/chapter6/2"),E(Hs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"),E(Hs,"rel","nofollow"),E(Js,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers"),E(Js,"rel","nofollow"),E(Ys,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models"),E(Ys,"rel","nofollow"),E(Zs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers"),E(Zs,"rel","nofollow"),E(Qs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors"),E(Qs,"rel","nofollow"),E(et,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders"),E(et,"rel","nofollow"),E(st,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html"),E(st,"rel","nofollow"),E(is,"id","acquisition-dun-corpus"),E(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(is,"href","#acquisition-dun-corpus"),E(Ye,"class","relative group"),E(Nn,"href","/course/fr/chapter6/2"),E(nt,"href","https://huggingface.co/datasets/wikitext"),E(nt,"rel","nofollow"),E(us,"id","construire-un-itokenizer-wordpiecei-partir-de-zro"),E(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(us,"href","#construire-un-itokenizer-wordpiecei-partir-de-zro"),E(Ze,"class","relative group"),E(Es,"id","construire-un-itokenizeri-bpe-partir-de-zro"),E(Es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Es,"href","#construire-un-itokenizeri-bpe-partir-de-zro"),E(Qe,"class","relative group"),E(js,"id","construire-un-itokenizer-unigrami-partir-de-zro"),E(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(js,"href","#construire-un-itokenizer-unigrami-partir-de-zro"),E(es,"class","relative group")},m(e,i){s(document.head,z),p(e,ge,i),p(e,oe,i),s(oe,pe),s(pe,qe),f(je,qe,null),s(oe,As),s(oe,re),s(re,Fs),s(re,Xe),s(Xe,Ke),s(re,Us),p(e,ss,i),f(le,e,i),p(e,ts,i),p(e,Pe,i),s(Pe,He),p(e,ns,i),p(e,R,i),s(R,xe),s(xe,Rs),s(R,Ws),s(R,be),s(be,Gs),s(R,Is),s(R,ae),s(ae,x),s(ae,os),s(os,Cn),s(ae,Dn),s(R,Ln),s(R,J),s(J,On),s(J,rs),s(rs,Mn),s(J,xc),s(J,mo),s(mo,bc),s(J,gc),s(J,fo),s(fo,Pc),s(J,wc),p(e,ui,i),p(e,yn,i),s(yn,Tc),p(e,ci,i),p(e,Je,i),s(Je,Vs),s(Je,Cc),s(Je,Xs),p(e,mi,i),p(e,W,i),s(W,Dc),s(W,ko),s(ko,Lc),s(W,Oc),s(W,vo),s(vo,Mc),s(W,yc),s(W,_o),s(_o,Sc),s(W,Nc),s(W,Sn),s(Sn,Bc),s(W,Ac),s(W,ho),s(ho,Fc),s(W,Uc),p(e,di,i),f(Ks,e,i),p(e,fi,i),p(e,ls,i),s(ls,Rc),s(ls,Eo),s(Eo,Wc),s(ls,Gc),p(e,ki,i),p(e,G,i),s(G,we),s(we,zo),s(zo,Ic),s(we,Vc),s(we,$o),s($o,Xc),s(we,Kc),s(we,Hs),s(Hs,Hc),s(we,Jc),s(G,Yc),s(G,Te),s(Te,qo),s(qo,Zc),s(Te,Qc),s(Te,jo),s(jo,em),s(Te,sm),s(Te,Js),s(Js,tm),s(Te,nm),s(G,om),s(G,I),s(I,xo),s(xo,rm),s(I,lm),s(I,bo),s(bo,am),s(I,im),s(I,go),s(go,pm),s(I,um),s(I,Po),s(Po,cm),s(I,mm),s(I,wo),s(wo,dm),s(I,fm),s(I,Ys),s(Ys,km),s(I,vm),s(G,_m),s(G,Ce),s(Ce,To),s(To,hm),s(Ce,Em),s(Ce,Co),s(Co,zm),s(Ce,$m),s(Ce,Zs),s(Zs,qm),s(Ce,jm),s(G,xm),s(G,De),s(De,Do),s(Do,bm),s(De,gm),s(De,Lo),s(Lo,Pm),s(De,wm),s(De,Qs),s(Qs,Tm),s(De,Cm),s(G,Dm),s(G,Le),s(Le,Oo),s(Oo,Lm),s(Le,Om),s(Le,Mo),s(Mo,Mm),s(Le,ym),s(Le,et),s(et,Sm),s(Le,Nm),p(e,vi,i),p(e,as,i),s(as,Bm),s(as,st),s(st,Am),s(as,Fm),p(e,_i,i),p(e,Ye,i),s(Ye,is),s(is,yo),f(tt,yo,null),s(Ye,Um),s(Ye,So),s(So,Rm),p(e,hi,i),p(e,ue,i),s(ue,Wm),s(ue,No),s(No,Gm),s(ue,Im),s(ue,Nn),s(Nn,Vm),s(ue,Xm),s(ue,nt),s(nt,Km),s(ue,Hm),p(e,Ei,i),f(ot,e,i),p(e,zi,i),p(e,Oe,i),s(Oe,Jm),s(Oe,Bo),s(Bo,Ym),s(Oe,Zm),s(Oe,Ao),s(Ao,Qm),s(Oe,ed),p(e,$i,i),p(e,ps,i),s(ps,sd),s(ps,Fo),s(Fo,td),s(ps,nd),p(e,qi,i),f(rt,e,i),p(e,ji,i),p(e,ce,i),s(ce,od),s(ce,Uo),s(Uo,rd),s(ce,ld),s(ce,Ro),s(Ro,ad),s(ce,id),s(ce,Wo),s(Wo,pd),s(ce,ud),p(e,xi,i),p(e,Ze,i),s(Ze,us),s(us,Go),f(lt,Go,null),s(Ze,cd),s(Ze,at),s(at,md),s(at,Io),s(Io,dd),s(at,fd),p(e,bi,i),p(e,b,i),s(b,kd),s(b,Vo),s(Vo,vd),s(b,_d),s(b,Xo),s(Xo,hd),s(b,Ed),s(b,Ko),s(Ko,zd),s(b,$d),s(b,Ho),s(Ho,qd),s(b,jd),s(b,Jo),s(Jo,xd),s(b,bd),s(b,Yo),s(Yo,gd),s(b,Pd),s(b,Zo),s(Zo,wd),s(b,Td),s(b,Qo),s(Qo,Cd),s(b,Dd),p(e,gi,i),p(e,Me,i),s(Me,Ld),s(Me,er),s(er,Od),s(Me,Md),s(Me,sr),s(sr,yd),s(Me,Sd),p(e,Pi,i),f(it,e,i),p(e,wi,i),p(e,me,i),s(me,Nd),s(me,tr),s(tr,Bd),s(me,Ad),s(me,nr),s(nr,Fd),s(me,Ud),s(me,or),s(or,Rd),s(me,Wd),p(e,Ti,i),p(e,g,i),s(g,Gd),s(g,rr),s(rr,Id),s(g,Vd),s(g,lr),s(lr,Xd),s(g,Kd),s(g,ar),s(ar,Hd),s(g,Jd),s(g,ir),s(ir,Yd),s(g,Zd),s(g,pr),s(pr,Qd),s(g,ef),s(g,ur),s(ur,sf),s(g,tf),s(g,cr),s(cr,nf),s(g,of),s(g,mr),s(mr,rf),s(g,lf),p(e,Ci,i),f(pt,e,i),p(e,Di,i),p(e,V,i),s(V,af),s(V,dr),s(dr,pf),s(V,uf),s(V,fr),s(fr,cf),s(V,mf),s(V,kr),s(kr,df),s(V,ff),s(V,vr),s(vr,kf),s(V,vf),s(V,_r),s(_r,_f),s(V,hf),p(e,Li,i),f(ut,e,i),p(e,Oi,i),p(e,ye,i),s(ye,Ef),s(ye,hr),s(hr,zf),s(ye,$f),s(ye,Er),s(Er,qf),s(ye,jf),p(e,Mi,i),p(e,Se,i),s(Se,xf),s(Se,zr),s(zr,bf),s(Se,gf),s(Se,$r),s($r,Pf),s(Se,wf),p(e,yi,i),f(ct,e,i),p(e,Si,i),f(mt,e,i),p(e,Ni,i),f(cs,e,i),p(e,Bi,i),p(e,ms,i),s(ms,Tf),s(ms,qr),s(qr,Cf),s(ms,Df),p(e,Ai,i),f(dt,e,i),p(e,Fi,i),p(e,Bn,i),s(Bn,Lf),p(e,Ui,i),f(ft,e,i),p(e,Ri,i),p(e,ds,i),s(ds,Of),s(ds,jr),s(jr,Mf),s(ds,yf),p(e,Wi,i),f(kt,e,i),p(e,Gi,i),f(vt,e,i),p(e,Ii,i),p(e,fs,i),s(fs,Sf),s(fs,xr),s(xr,Nf),s(fs,Bf),p(e,Vi,i),f(_t,e,i),p(e,Xi,i),f(ht,e,i),p(e,Ki,i),p(e,ks,i),s(ks,Af),s(ks,br),s(br,Ff),s(ks,Uf),p(e,Hi,i),f(Et,e,i),p(e,Ji,i),f(zt,e,i),p(e,Yi,i),p(e,de,i),s(de,Rf),s(de,gr),s(gr,Wf),s(de,Gf),s(de,Pr),s(Pr,If),s(de,Vf),s(de,wr),s(wr,Xf),s(de,Kf),p(e,Zi,i),f($t,e,i),p(e,Qi,i),p(e,O,i),s(O,Hf),s(O,Tr),s(Tr,Jf),s(O,Yf),s(O,Cr),s(Cr,Zf),s(O,Qf),s(O,Dr),s(Dr,ek),s(O,sk),s(O,Lr),s(Lr,tk),s(O,nk),s(O,Or),s(Or,ok),s(O,rk),s(O,Mr),s(Mr,lk),s(O,ak),p(e,ep,i),p(e,An,i),s(An,ik),p(e,sp,i),f(qt,e,i),p(e,tp,i),p(e,Ne,i),s(Ne,pk),s(Ne,yr),s(yr,uk),s(Ne,ck),s(Ne,Sr),s(Sr,mk),s(Ne,dk),p(e,np,i),f(jt,e,i),p(e,op,i),p(e,Be,i),s(Be,fk),s(Be,Nr),s(Nr,kk),s(Be,vk),s(Be,Br),s(Br,_k),s(Be,hk),p(e,rp,i),f(xt,e,i),p(e,lp,i),f(bt,e,i),p(e,ap,i),p(e,j,i),s(j,Ek),s(j,Ar),s(Ar,zk),s(j,$k),s(j,Fr),s(Fr,qk),s(j,jk),s(j,Ur),s(Ur,xk),s(j,bk),s(j,Rr),s(Rr,gk),s(j,Pk),s(j,Wr),s(Wr,wk),s(j,Tk),s(j,Gr),s(Gr,Ck),s(j,Dk),s(j,Ir),s(Ir,Lk),s(j,Ok),s(j,Vr),s(Vr,Mk),s(j,yk),s(j,Xr),s(Xr,Sk),s(j,Nk),p(e,ip,i),p(e,P,i),s(P,Bk),s(P,Kr),s(Kr,Ak),s(P,Fk),s(P,Hr),s(Hr,Uk),s(P,Rk),s(P,Jr),s(Jr,Wk),s(P,Gk),s(P,Yr),s(Yr,Ik),s(P,Vk),s(P,Zr),s(Zr,Xk),s(P,Kk),s(P,Qr),s(Qr,Hk),s(P,Jk),s(P,el),s(el,Yk),s(P,Zk),s(P,sl),s(sl,Qk),s(P,ev),p(e,pp,i),f(gt,e,i),p(e,up,i),f(Pt,e,i),p(e,cp,i),p(e,M,i),s(M,sv),s(M,tl),s(tl,tv),s(M,nv),s(M,nl),s(nl,ov),s(M,rv),s(M,ol),s(ol,lv),s(M,av),s(M,rl),s(rl,iv),s(M,pv),s(M,ll),s(ll,uv),s(M,cv),s(M,al),s(al,mv),s(M,dv),p(e,mp,i),p(e,Fn,i),s(Fn,fv),p(e,dp,i),f(wt,e,i),p(e,fp,i),p(e,Ae,i),s(Ae,kv),s(Ae,il),s(il,vv),s(Ae,_v),s(Ae,pl),s(pl,hv),s(Ae,Ev),p(e,kp,i),p(e,Un,i),s(Un,zv),p(e,vp,i),f(Tt,e,i),p(e,_p,i),f(Ct,e,i),p(e,hp,i),p(e,Rn,i),s(Rn,$v),p(e,Ep,i),f(Dt,e,i),p(e,zp,i),f(Lt,e,i),p(e,$p,i),p(e,vs,i),s(vs,qv),s(vs,ul),s(ul,jv),s(vs,xv),p(e,qp,i),f(Ot,e,i),p(e,jp,i),p(e,_s,i),s(_s,bv),s(_s,cl),s(cl,gv),s(_s,Pv),p(e,xp,i),f(Mt,e,i),p(e,bp,i),f(yt,e,i),p(e,gp,i),p(e,hs,i),s(hs,wv),s(hs,ml),s(ml,Tv),s(hs,Cv),p(e,Pp,i),f(St,e,i),p(e,wp,i),p(e,Fe,i),s(Fe,Dv),s(Fe,dl),s(dl,Lv),s(Fe,Ov),s(Fe,fl),s(fl,Mv),s(Fe,yv),p(e,Tp,i),f(Nt,e,i),p(e,Cp,i),p(e,y,i),s(y,Sv),s(y,kl),s(kl,Nv),s(y,Bv),s(y,vl),s(vl,Av),s(y,Fv),s(y,_l),s(_l,Uv),s(y,Rv),s(y,hl),s(hl,Wv),s(y,Gv),s(y,El),s(El,Iv),s(y,Vv),s(y,zl),s(zl,Xv),s(y,Kv),p(e,Dp,i),p(e,$,i),s($,Hv),s($,$l),s($l,Jv),s($,Yv),s($,ql),s(ql,Zv),s($,Qv),s($,jl),s(jl,e_),s($,s_),s($,xl),s(xl,t_),s($,n_),s($,bl),s(bl,o_),s($,r_),s($,gl),s(gl,l_),s($,a_),s($,Pl),s(Pl,i_),s($,p_),s($,wl),s(wl,u_),s($,c_),s($,Tl),s(Tl,m_),s($,d_),s($,Cl),s(Cl,f_),s($,k_),s($,Dl),s(Dl,v_),s($,Ll),s(Ll,__),s($,h_),p(e,Lp,i),f(Bt,e,i),p(e,Op,i),p(e,fe,i),s(fe,E_),s(fe,Ol),s(Ol,z_),s(fe,$_),s(fe,Ml),s(Ml,q_),s(fe,j_),s(fe,yl),s(yl,x_),s(fe,b_),p(e,Mp,i),f(At,e,i),p(e,yp,i),p(e,S,i),s(S,g_),s(S,Sl),s(Sl,P_),s(S,w_),s(S,Nl),s(Nl,T_),s(S,C_),s(S,Bl),s(Bl,D_),s(S,L_),s(S,Al),s(Al,O_),s(S,M_),s(S,Fl),s(Fl,y_),s(S,S_),s(S,Ul),s(Ul,N_),s(S,B_),p(e,Sp,i),p(e,Ue,i),s(Ue,A_),s(Ue,Rl),s(Rl,F_),s(Ue,U_),s(Ue,Wl),s(Wl,R_),s(Ue,W_),p(e,Np,i),p(e,Qe,i),s(Qe,Es),s(Es,Gl),f(Ft,Gl,null),s(Qe,G_),s(Qe,Ut),s(Ut,I_),s(Ut,Il),s(Il,V_),s(Ut,X_),p(e,Bp,i),p(e,ke,i),s(ke,K_),s(ke,Vl),s(Vl,H_),s(ke,J_),s(ke,Xl),s(Xl,Y_),s(ke,Z_),s(ke,Kl),s(Kl,Q_),s(ke,e2),p(e,Ap,i),f(Rt,e,i),p(e,Fp,i),p(e,ve,i),s(ve,s2),s(ve,Hl),s(Hl,t2),s(ve,n2),s(ve,Jl),s(Jl,o2),s(ve,r2),s(ve,Yl),s(Yl,l2),s(ve,a2),p(e,Up,i),p(e,Wn,i),s(Wn,i2),p(e,Rp,i),f(Wt,e,i),p(e,Wp,i),p(e,zs,i),s(zs,p2),s(zs,Zl),s(Zl,u2),s(zs,c2),p(e,Gp,i),f(Gt,e,i),p(e,Ip,i),f(It,e,i),p(e,Vp,i),p(e,Re,i),s(Re,m2),s(Re,Ql),s(Ql,d2),s(Re,f2),s(Re,ea),s(ea,k2),s(Re,v2),p(e,Xp,i),f(Vt,e,i),p(e,Kp,i),p(e,N,i),s(N,_2),s(N,sa),s(sa,h2),s(N,E2),s(N,ta),s(ta,z2),s(N,$2),s(N,na),s(na,q2),s(N,j2),s(N,oa),s(oa,x2),s(N,b2),s(N,ra),s(ra,g2),s(N,P2),s(N,la),s(la,w2),s(N,T2),p(e,Hp,i),p(e,$s,i),s($s,C2),s($s,aa),s(aa,D2),s($s,L2),p(e,Jp,i),f(Xt,e,i),p(e,Yp,i),p(e,Gn,i),s(Gn,O2),p(e,Zp,i),f(Kt,e,i),p(e,Qp,i),f(Ht,e,i),p(e,eu,i),p(e,qs,i),s(qs,M2),s(qs,ia),s(ia,y2),s(qs,S2),p(e,su,i),f(Jt,e,i),p(e,tu,i),p(e,T,i),s(T,N2),s(T,pa),s(pa,B2),s(T,A2),s(T,ua),s(ua,F2),s(T,U2),s(T,ca),s(ca,R2),s(T,W2),s(T,ma),s(ma,G2),s(T,I2),s(T,da),s(da,V2),s(T,X2),s(T,fa),s(fa,K2),s(T,H2),s(T,ka),s(ka,J2),s(T,Y2),p(e,nu,i),f(Yt,e,i),p(e,ou,i),f(Zt,e,i),p(e,ru,i),p(e,In,i),s(In,Z2),p(e,lu,i),f(Qt,e,i),p(e,au,i),p(e,Vn,i),s(Vn,Q2),p(e,iu,i),f(en,e,i),p(e,pu,i),f(sn,e,i),p(e,uu,i),p(e,_e,i),s(_e,eh),s(_e,va),s(va,sh),s(_e,th),s(_e,_a),s(_a,nh),s(_e,oh),s(_e,ha),s(ha,rh),s(_e,lh),p(e,cu,i),f(tn,e,i),p(e,mu,i),p(e,Xn,i),s(Xn,ah),p(e,du,i),f(nn,e,i),p(e,fu,i),p(e,We,i),s(We,ih),s(We,Ea),s(Ea,ph),s(We,uh),s(We,za),s(za,ch),s(We,mh),p(e,ku,i),p(e,es,i),s(es,js),s(js,$a),f(on,$a,null),s(es,dh),s(es,rn),s(rn,fh),s(rn,qa),s(qa,kh),s(rn,vh),p(e,vu,i),p(e,Y,i),s(Y,_h),s(Y,ja),s(ja,hh),s(Y,Eh),s(Y,xa),s(xa,zh),s(Y,$h),s(Y,ba),s(ba,qh),s(Y,jh),s(Y,ga),s(ga,xh),s(Y,bh),p(e,_u,i),f(ln,e,i),p(e,hu,i),p(e,Kn,i),s(Kn,gh),p(e,Eu,i),p(e,xs,i),s(xs,Ph),s(xs,Pa),s(Pa,wh),s(xs,Th),p(e,zu,i),f(an,e,i),p(e,$u,i),p(e,he,i),s(he,Ch),s(he,wa),s(wa,Dh),s(he,Lh),s(he,Ta),s(Ta,Oh),s(he,Mh),s(he,Ca),s(Ca,yh),s(he,Sh),p(e,qu,i),p(e,Ge,i),s(Ge,Nh),s(Ge,Da),s(Da,Bh),s(Ge,Ah),s(Ge,La),s(La,Fh),s(Ge,Uh),p(e,ju,i),f(pn,e,i),p(e,xu,i),p(e,Hn,i),s(Hn,Rh),p(e,bu,i),f(un,e,i),p(e,gu,i),f(cn,e,i),p(e,Pu,i),p(e,bs,i),s(bs,Wh),s(bs,Oa),s(Oa,Gh),s(bs,Ih),p(e,wu,i),f(mn,e,i),p(e,Tu,i),p(e,C,i),s(C,Vh),s(C,Ma),s(Ma,Xh),s(C,Kh),s(C,ya),s(ya,Hh),s(C,Jh),s(C,Sa),s(Sa,Yh),s(C,Zh),s(C,Na),s(Na,Qh),s(C,eE),s(C,Ba),s(Ba,sE),s(C,tE),s(C,Aa),s(Aa,nE),s(C,oE),s(C,Fa),s(Fa,rE),s(C,lE),p(e,Cu,i),p(e,gs,i),s(gs,aE),s(gs,Ua),s(Ua,iE),s(gs,pE),p(e,Du,i),f(dn,e,i),p(e,Lu,i),p(e,Jn,i),s(Jn,uE),p(e,Ou,i),f(fn,e,i),p(e,Mu,i),f(kn,e,i),p(e,yu,i),p(e,w,i),s(w,cE),s(w,Ra),s(Ra,mE),s(w,dE),s(w,Wa),s(Wa,fE),s(w,kE),s(w,Ga),s(Ga,vE),s(w,_E),s(w,Ia),s(Ia,hE),s(w,EE),s(w,Va),s(Va,zE),s(w,$E),s(w,Xa),s(Xa,qE),s(w,jE),s(w,Ka),s(Ka,xE),s(w,bE),s(w,Ha),s(Ha,gE),s(w,PE),p(e,Su,i),f(vn,e,i),p(e,Nu,i),f(_n,e,i),p(e,Bu,i),p(e,Yn,i),s(Yn,wE),p(e,Au,i),f(hn,e,i),p(e,Fu,i),p(e,Zn,i),s(Zn,TE),p(e,Uu,i),f(En,e,i),p(e,Ru,i),f(zn,e,i),p(e,Wu,i),p(e,Ps,i),s(Ps,CE),s(Ps,Ja),s(Ja,DE),s(Ps,LE),p(e,Gu,i),f($n,e,i),p(e,Iu,i),p(e,D,i),s(D,OE),s(D,Ya),s(Ya,ME),s(D,yE),s(D,Za),s(Za,SE),s(D,NE),s(D,Qa),s(Qa,BE),s(D,AE),s(D,ei),s(ei,FE),s(D,UE),s(D,si),s(si,RE),s(D,WE),s(D,ti),s(ti,GE),s(D,IE),s(D,ni),s(ni,VE),s(D,XE),p(e,Vu,i),f(qn,e,i),p(e,Xu,i),p(e,Qn,i),s(Qn,KE),p(e,Ku,i),f(jn,e,i),p(e,Hu,i),p(e,Z,i),s(Z,HE),s(Z,oi),s(oi,JE),s(Z,YE),s(Z,ri),s(ri,ZE),s(Z,QE),s(Z,li),s(li,ez),s(Z,sz),s(Z,ai),s(ai,tz),s(Z,nz),Ju=!0},p(e,[i]){const xn={};i&2&&(xn.$$scope={dirty:i,ctx:e}),cs.$set(xn)},i(e){Ju||(k(je.$$.fragment,e),k(le.$$.fragment,e),k(Ks.$$.fragment,e),k(tt.$$.fragment,e),k(ot.$$.fragment,e),k(rt.$$.fragment,e),k(lt.$$.fragment,e),k(it.$$.fragment,e),k(pt.$$.fragment,e),k(ut.$$.fragment,e),k(ct.$$.fragment,e),k(mt.$$.fragment,e),k(cs.$$.fragment,e),k(dt.$$.fragment,e),k(ft.$$.fragment,e),k(kt.$$.fragment,e),k(vt.$$.fragment,e),k(_t.$$.fragment,e),k(ht.$$.fragment,e),k(Et.$$.fragment,e),k(zt.$$.fragment,e),k($t.$$.fragment,e),k(qt.$$.fragment,e),k(jt.$$.fragment,e),k(xt.$$.fragment,e),k(bt.$$.fragment,e),k(gt.$$.fragment,e),k(Pt.$$.fragment,e),k(wt.$$.fragment,e),k(Tt.$$.fragment,e),k(Ct.$$.fragment,e),k(Dt.$$.fragment,e),k(Lt.$$.fragment,e),k(Ot.$$.fragment,e),k(Mt.$$.fragment,e),k(yt.$$.fragment,e),k(St.$$.fragment,e),k(Nt.$$.fragment,e),k(Bt.$$.fragment,e),k(At.$$.fragment,e),k(Ft.$$.fragment,e),k(Rt.$$.fragment,e),k(Wt.$$.fragment,e),k(Gt.$$.fragment,e),k(It.$$.fragment,e),k(Vt.$$.fragment,e),k(Xt.$$.fragment,e),k(Kt.$$.fragment,e),k(Ht.$$.fragment,e),k(Jt.$$.fragment,e),k(Yt.$$.fragment,e),k(Zt.$$.fragment,e),k(Qt.$$.fragment,e),k(en.$$.fragment,e),k(sn.$$.fragment,e),k(tn.$$.fragment,e),k(nn.$$.fragment,e),k(on.$$.fragment,e),k(ln.$$.fragment,e),k(an.$$.fragment,e),k(pn.$$.fragment,e),k(un.$$.fragment,e),k(cn.$$.fragment,e),k(mn.$$.fragment,e),k(dn.$$.fragment,e),k(fn.$$.fragment,e),k(kn.$$.fragment,e),k(vn.$$.fragment,e),k(_n.$$.fragment,e),k(hn.$$.fragment,e),k(En.$$.fragment,e),k(zn.$$.fragment,e),k($n.$$.fragment,e),k(qn.$$.fragment,e),k(jn.$$.fragment,e),Ju=!0)},o(e){v(je.$$.fragment,e),v(le.$$.fragment,e),v(Ks.$$.fragment,e),v(tt.$$.fragment,e),v(ot.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ut.$$.fragment,e),v(ct.$$.fragment,e),v(mt.$$.fragment,e),v(cs.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(kt.$$.fragment,e),v(vt.$$.fragment,e),v(_t.$$.fragment,e),v(ht.$$.fragment,e),v(Et.$$.fragment,e),v(zt.$$.fragment,e),v($t.$$.fragment,e),v(qt.$$.fragment,e),v(jt.$$.fragment,e),v(xt.$$.fragment,e),v(bt.$$.fragment,e),v(gt.$$.fragment,e),v(Pt.$$.fragment,e),v(wt.$$.fragment,e),v(Tt.$$.fragment,e),v(Ct.$$.fragment,e),v(Dt.$$.fragment,e),v(Lt.$$.fragment,e),v(Ot.$$.fragment,e),v(Mt.$$.fragment,e),v(yt.$$.fragment,e),v(St.$$.fragment,e),v(Nt.$$.fragment,e),v(Bt.$$.fragment,e),v(At.$$.fragment,e),v(Ft.$$.fragment,e),v(Rt.$$.fragment,e),v(Wt.$$.fragment,e),v(Gt.$$.fragment,e),v(It.$$.fragment,e),v(Vt.$$.fragment,e),v(Xt.$$.fragment,e),v(Kt.$$.fragment,e),v(Ht.$$.fragment,e),v(Jt.$$.fragment,e),v(Yt.$$.fragment,e),v(Zt.$$.fragment,e),v(Qt.$$.fragment,e),v(en.$$.fragment,e),v(sn.$$.fragment,e),v(tn.$$.fragment,e),v(nn.$$.fragment,e),v(on.$$.fragment,e),v(ln.$$.fragment,e),v(an.$$.fragment,e),v(pn.$$.fragment,e),v(un.$$.fragment,e),v(cn.$$.fragment,e),v(mn.$$.fragment,e),v(dn.$$.fragment,e),v(fn.$$.fragment,e),v(kn.$$.fragment,e),v(vn.$$.fragment,e),v(_n.$$.fragment,e),v(hn.$$.fragment,e),v(En.$$.fragment,e),v(zn.$$.fragment,e),v($n.$$.fragment,e),v(qn.$$.fragment,e),v(jn.$$.fragment,e),Ju=!1},d(e){t(z),e&&t(ge),e&&t(oe),_(je),e&&t(ss),_(le,e),e&&t(ts),e&&t(Pe),e&&t(ns),e&&t(R),e&&t(ui),e&&t(yn),e&&t(ci),e&&t(Je),e&&t(mi),e&&t(W),e&&t(di),_(Ks,e),e&&t(fi),e&&t(ls),e&&t(ki),e&&t(G),e&&t(vi),e&&t(as),e&&t(_i),e&&t(Ye),_(tt),e&&t(hi),e&&t(ue),e&&t(Ei),_(ot,e),e&&t(zi),e&&t(Oe),e&&t($i),e&&t(ps),e&&t(qi),_(rt,e),e&&t(ji),e&&t(ce),e&&t(xi),e&&t(Ze),_(lt),e&&t(bi),e&&t(b),e&&t(gi),e&&t(Me),e&&t(Pi),_(it,e),e&&t(wi),e&&t(me),e&&t(Ti),e&&t(g),e&&t(Ci),_(pt,e),e&&t(Di),e&&t(V),e&&t(Li),_(ut,e),e&&t(Oi),e&&t(ye),e&&t(Mi),e&&t(Se),e&&t(yi),_(ct,e),e&&t(Si),_(mt,e),e&&t(Ni),_(cs,e),e&&t(Bi),e&&t(ms),e&&t(Ai),_(dt,e),e&&t(Fi),e&&t(Bn),e&&t(Ui),_(ft,e),e&&t(Ri),e&&t(ds),e&&t(Wi),_(kt,e),e&&t(Gi),_(vt,e),e&&t(Ii),e&&t(fs),e&&t(Vi),_(_t,e),e&&t(Xi),_(ht,e),e&&t(Ki),e&&t(ks),e&&t(Hi),_(Et,e),e&&t(Ji),_(zt,e),e&&t(Yi),e&&t(de),e&&t(Zi),_($t,e),e&&t(Qi),e&&t(O),e&&t(ep),e&&t(An),e&&t(sp),_(qt,e),e&&t(tp),e&&t(Ne),e&&t(np),_(jt,e),e&&t(op),e&&t(Be),e&&t(rp),_(xt,e),e&&t(lp),_(bt,e),e&&t(ap),e&&t(j),e&&t(ip),e&&t(P),e&&t(pp),_(gt,e),e&&t(up),_(Pt,e),e&&t(cp),e&&t(M),e&&t(mp),e&&t(Fn),e&&t(dp),_(wt,e),e&&t(fp),e&&t(Ae),e&&t(kp),e&&t(Un),e&&t(vp),_(Tt,e),e&&t(_p),_(Ct,e),e&&t(hp),e&&t(Rn),e&&t(Ep),_(Dt,e),e&&t(zp),_(Lt,e),e&&t($p),e&&t(vs),e&&t(qp),_(Ot,e),e&&t(jp),e&&t(_s),e&&t(xp),_(Mt,e),e&&t(bp),_(yt,e),e&&t(gp),e&&t(hs),e&&t(Pp),_(St,e),e&&t(wp),e&&t(Fe),e&&t(Tp),_(Nt,e),e&&t(Cp),e&&t(y),e&&t(Dp),e&&t($),e&&t(Lp),_(Bt,e),e&&t(Op),e&&t(fe),e&&t(Mp),_(At,e),e&&t(yp),e&&t(S),e&&t(Sp),e&&t(Ue),e&&t(Np),e&&t(Qe),_(Ft),e&&t(Bp),e&&t(ke),e&&t(Ap),_(Rt,e),e&&t(Fp),e&&t(ve),e&&t(Up),e&&t(Wn),e&&t(Rp),_(Wt,e),e&&t(Wp),e&&t(zs),e&&t(Gp),_(Gt,e),e&&t(Ip),_(It,e),e&&t(Vp),e&&t(Re),e&&t(Xp),_(Vt,e),e&&t(Kp),e&&t(N),e&&t(Hp),e&&t($s),e&&t(Jp),_(Xt,e),e&&t(Yp),e&&t(Gn),e&&t(Zp),_(Kt,e),e&&t(Qp),_(Ht,e),e&&t(eu),e&&t(qs),e&&t(su),_(Jt,e),e&&t(tu),e&&t(T),e&&t(nu),_(Yt,e),e&&t(ou),_(Zt,e),e&&t(ru),e&&t(In),e&&t(lu),_(Qt,e),e&&t(au),e&&t(Vn),e&&t(iu),_(en,e),e&&t(pu),_(sn,e),e&&t(uu),e&&t(_e),e&&t(cu),_(tn,e),e&&t(mu),e&&t(Xn),e&&t(du),_(nn,e),e&&t(fu),e&&t(We),e&&t(ku),e&&t(es),_(on),e&&t(vu),e&&t(Y),e&&t(_u),_(ln,e),e&&t(hu),e&&t(Kn),e&&t(Eu),e&&t(xs),e&&t(zu),_(an,e),e&&t($u),e&&t(he),e&&t(qu),e&&t(Ge),e&&t(ju),_(pn,e),e&&t(xu),e&&t(Hn),e&&t(bu),_(un,e),e&&t(gu),_(cn,e),e&&t(Pu),e&&t(bs),e&&t(wu),_(mn,e),e&&t(Tu),e&&t(C),e&&t(Cu),e&&t(gs),e&&t(Du),_(dn,e),e&&t(Lu),e&&t(Jn),e&&t(Ou),_(fn,e),e&&t(Mu),_(kn,e),e&&t(yu),e&&t(w),e&&t(Su),_(vn,e),e&&t(Nu),_(_n,e),e&&t(Bu),e&&t(Yn),e&&t(Au),_(hn,e),e&&t(Fu),e&&t(Zn),e&&t(Uu),_(En,e),e&&t(Ru),_(zn,e),e&&t(Wu),e&&t(Ps),e&&t(Gu),_($n,e),e&&t(Iu),e&&t(D),e&&t(Vu),_(qn,e),e&&t(Xu),e&&t(Qn),e&&t(Ku),_(jn,e),e&&t(Hu),e&&t(Z)}}}const Yq={local:"construction-dun-itokenizeri-bloc-par-bloc",sections:[{local:"acquisition-dun-corpus",title:"Acquisition d'un corpus"},{local:"construire-un-itokenizer-wordpiecei-partir-de-zro",title:"Construire un <i>tokenizer WordPiece</i> \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizeri-bpe-partir-de-zro",title:"Construire un <i>tokenizer</i> BPE \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizer-unigrami-partir-de-zro",title:"Construire un <i>tokenizer Unigram</i> \xE0 partir de z\xE9ro"}],title:"Construction d'un <i>tokenizer</i>, bloc par bloc"};function Zq(pi){return Iq(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rj extends Uq{constructor(z){super();Rq(this,z,Zq,Jq,Wq,{})}}export{rj as default,Yq as metadata};
