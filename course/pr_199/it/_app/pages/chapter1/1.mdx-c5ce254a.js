import{S as zo,i as Eo,s as bo,e as t,k as p,w as je,t as r,M as wo,c as l,d as i,m as d,a as n,x as Ke,h as o,b as s,N as _o,G as a,g as u,y as Xe,L as Lo,q as Ze,o as ea,B as aa,v as Po}from"../../chunks/vendor-50915ede.js";import{Y as yo}from"../../chunks/Youtube-d73e0ab1.js";import{I as Ha}from"../../chunks/IconCopyLink-f2a31f02.js";function Ao(_r){let b,ia,w,H,we,q,ka,Le,Ma,ra,L,k,Pe,D,Fa,ye,qa,oa,C,ta,f,Da,Ae,Ca,Ra,R,Ga,Oa,G,xa,Ba,O,Qa,Ua,x,Ja,Va,B,Wa,Ya,Q,ja,Ka,la,P,M,Ne,U,Xa,Te,Za,na,de,ei,sa,y,J,zr,ai,V,Er,ca,v,W,ii,Y,ri,oi,ti,$e,li,ni,A,si,Ie,ci,ui,Se,pi,di,ua,fe,fi,pa,_,He,mi,hi,h,gi,ke,vi,_i,j,zi,Ei,K,bi,wi,X,Li,Pi,N,yi,Z,Ai,Ni,ee,Ti,$i,da,z,Ii,ae,Si,Hi,Me,ki,Mi,fa,T,F,Fe,ie,Fi,qe,qi,ma,me,Di,ha,re,De,Ci,Ri,ga,oe,Ce,Gi,Oi,va,g,Re,xi,Bi,te,Qi,Ui,Ge,Ji,Vi,_a,le,Oe,Wi,Yi,za,ne,xe,ji,Ki,Ea,$,Be,Xi,Zi,se,er,ar,ba,I,Qe,ir,rr,ce,or,tr,wa,he,lr,La,E,ue,nr,Ue,sr,cr,ur,Je,pr,dr,Ve,fr,Pa;return q=new Ha({}),D=new Ha({}),C=new yo({props:{id:"00GKzGyWFEs"}}),U=new Ha({}),ie=new Ha({}),{c(){b=t("meta"),ia=p(),w=t("h1"),H=t("a"),we=t("span"),je(q.$$.fragment),ka=p(),Le=t("span"),Ma=r("Introduzione"),ra=p(),L=t("h2"),k=t("a"),Pe=t("span"),je(D.$$.fragment),Fa=p(),ye=t("span"),qa=r("Benvenuto/a al corso di \u{1F917}!"),oa=p(),je(C.$$.fragment),ta=p(),f=t("p"),Da=r("Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),Ae=t("em"),Ca=r("elaborazione del linguaggio naturale"),Ra=r(") utilizzando le librerie dell\u2019ecosistema di "),R=t("a"),Ga=r("Hugging Face"),Oa=r(": "),G=t("a"),xa=r("\u{1F917} Transformers"),Ba=r(", "),O=t("a"),Qa=r("\u{1F917} Datasets"),Ua=r(", "),x=t("a"),Ja=r("\u{1F917} Tokenizers"),Va=r(", e "),B=t("a"),Wa=r("\u{1F917} Accelerate"),Ya=r(". Ti insegneremo anche ad usare il nostro "),Q=t("a"),ja=r("Hugging Face Hub"),Ka=r(", che \xE8 completamente gratuito e senza pubblicit\xE0."),la=p(),P=t("h2"),M=t("a"),Ne=t("span"),je(U.$$.fragment),Xa=p(),Te=t("span"),Za=r("Contenuti"),na=p(),de=t("p"),ei=r("Eccoti un breve riassunto dei contenuti del corso:"),sa=p(),y=t("div"),J=t("img"),ai=p(),V=t("img"),ca=p(),v=t("ul"),W=t("li"),ii=r("I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),Y=t("a"),ri=r("Hugging Face Hub"),oi=r(", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),ti=p(),$e=t("li"),li=r("I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),ni=p(),A=t("li"),si=r("I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),Ie=t("em"),ci=r("dimostrazioni"),ui=r(") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),Se=t("em"),pi=r("apprendimento automatico"),di=r("), o quasi!"),ua=p(),fe=t("p"),fi=r("Questo corso:"),pa=p(),_=t("ul"),He=t("li"),mi=r("Richiede una buona conoscenza di Python"),hi=p(),h=t("li"),gi=r("Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),ke=t("em"),vi=r("apprendimento profondo"),_i=r("), come ad esempio il "),j=t("a"),zi=r("Practical Deep Learning for Coders"),Ei=r(" di "),K=t("a"),bi=r("fast.ai"),wi=r(", oppure uno dei programmi sviluppati da "),X=t("a"),Li=r("DeepLearning.AI"),Pi=p(),N=t("li"),yi=r("Non richiede conoscenze pregresse di "),Z=t("a"),Ai=r("PyTorch"),Ni=r(" o "),ee=t("a"),Ti=r("TensorFlow"),$i=r(", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),da=p(),z=t("p"),Ii=r("Quando avrai completato questo corso, ti raccomandiamo di passare al "),ae=t("a"),Si=r("Natural Language Processing Specialization"),Hi=r(" di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),Me=t("em"),ki=r("Memoria a breve termine a lungo termine"),Mi=r(")!"),fa=p(),T=t("h2"),F=t("a"),Fe=t("span"),je(ie.$$.fragment),Fi=p(),qe=t("span"),qi=r("Chi siamo?"),ma=p(),me=t("p"),Di=r("A proposito degli autori:"),ha=p(),re=t("p"),De=t("strong"),Ci=r("Matthew Carrigan"),Ri=r(" \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),ga=p(),oe=t("p"),Ce=t("strong"),Gi=r("Lysandre Debut"),Oi=r(" \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),va=p(),g=t("p"),Re=t("strong"),xi=r("Sylvain Gugger"),Bi=r(" \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),te=t("a"),Qi=r("Deep Learning for Coders with fastai and PyTorch"),Ui=r(" con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Ge=t("em"),Ji=r("apprendimento profondo"),Vi=r(") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),_a=p(),le=t("p"),Oe=t("strong"),Wi=r("Merve Noyan"),Yi=r(" \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),za=p(),ne=t("p"),xe=t("strong"),ji=r("Lucile Saulnier"),Ki=r(" \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),Ea=p(),$=t("p"),Be=t("strong"),Xi=r("Lewis Tunstall"),Zi=r(" \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),se=t("a"),er=r("O\u2019Reilly book on Transformers"),ar=r("."),ba=p(),I=t("p"),Qe=t("strong"),ir=r("Leandro von Werra"),rr=r(" \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),ce=t("a"),or=r("O\u2019Reilly book on Transformers"),tr=r(". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),wa=p(),he=t("p"),lr=r("Sei pronto/a a iniziare? In questo capitolo, imparerai:"),La=p(),E=t("ul"),ue=t("li"),nr=r("Ad utilizzare la funzione "),Ue=t("code"),sr=r("pipeline()"),cr=r(" per eseguire compiti di NLP come la generazione e classificazione di testi"),ur=p(),Je=t("li"),pr=r("L\u2019architettura dei Transformer"),dr=p(),Ve=t("li"),fr=r("Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),this.h()},l(e){const c=wo('[data-svelte="svelte-1phssyn"]',document.head);b=l(c,"META",{name:!0,content:!0}),c.forEach(i),ia=d(e),w=l(e,"H1",{class:!0});var ya=n(w);H=l(ya,"A",{id:!0,class:!0,href:!0});var br=n(H);we=l(br,"SPAN",{});var wr=n(we);Ke(q.$$.fragment,wr),wr.forEach(i),br.forEach(i),ka=d(ya),Le=l(ya,"SPAN",{});var Lr=n(Le);Ma=o(Lr,"Introduzione"),Lr.forEach(i),ya.forEach(i),ra=d(e),L=l(e,"H2",{class:!0});var Aa=n(L);k=l(Aa,"A",{id:!0,class:!0,href:!0});var Pr=n(k);Pe=l(Pr,"SPAN",{});var yr=n(Pe);Ke(D.$$.fragment,yr),yr.forEach(i),Pr.forEach(i),Fa=d(Aa),ye=l(Aa,"SPAN",{});var Ar=n(ye);qa=o(Ar,"Benvenuto/a al corso di \u{1F917}!"),Ar.forEach(i),Aa.forEach(i),oa=d(e),Ke(C.$$.fragment,e),ta=d(e),f=l(e,"P",{});var m=n(f);Da=o(m,"Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),Ae=l(m,"EM",{});var Nr=n(Ae);Ca=o(Nr,"elaborazione del linguaggio naturale"),Nr.forEach(i),Ra=o(m,") utilizzando le librerie dell\u2019ecosistema di "),R=l(m,"A",{href:!0,rel:!0});var Tr=n(R);Ga=o(Tr,"Hugging Face"),Tr.forEach(i),Oa=o(m,": "),G=l(m,"A",{href:!0,rel:!0});var $r=n(G);xa=o($r,"\u{1F917} Transformers"),$r.forEach(i),Ba=o(m,", "),O=l(m,"A",{href:!0,rel:!0});var Ir=n(O);Qa=o(Ir,"\u{1F917} Datasets"),Ir.forEach(i),Ua=o(m,", "),x=l(m,"A",{href:!0,rel:!0});var Sr=n(x);Ja=o(Sr,"\u{1F917} Tokenizers"),Sr.forEach(i),Va=o(m,", e "),B=l(m,"A",{href:!0,rel:!0});var Hr=n(B);Wa=o(Hr,"\u{1F917} Accelerate"),Hr.forEach(i),Ya=o(m,". Ti insegneremo anche ad usare il nostro "),Q=l(m,"A",{href:!0,rel:!0});var kr=n(Q);ja=o(kr,"Hugging Face Hub"),kr.forEach(i),Ka=o(m,", che \xE8 completamente gratuito e senza pubblicit\xE0."),m.forEach(i),la=d(e),P=l(e,"H2",{class:!0});var Na=n(P);M=l(Na,"A",{id:!0,class:!0,href:!0});var Mr=n(M);Ne=l(Mr,"SPAN",{});var Fr=n(Ne);Ke(U.$$.fragment,Fr),Fr.forEach(i),Mr.forEach(i),Xa=d(Na),Te=l(Na,"SPAN",{});var qr=n(Te);Za=o(qr,"Contenuti"),qr.forEach(i),Na.forEach(i),na=d(e),de=l(e,"P",{});var Dr=n(de);ei=o(Dr,"Eccoti un breve riassunto dei contenuti del corso:"),Dr.forEach(i),sa=d(e),y=l(e,"DIV",{class:!0});var Ta=n(y);J=l(Ta,"IMG",{class:!0,src:!0,alt:!0}),ai=d(Ta),V=l(Ta,"IMG",{class:!0,src:!0,alt:!0}),Ta.forEach(i),ca=d(e),v=l(e,"UL",{});var ge=n(v);W=l(ge,"LI",{});var $a=n(W);ii=o($a,"I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),Y=l($a,"A",{href:!0,rel:!0});var Cr=n(Y);ri=o(Cr,"Hugging Face Hub"),Cr.forEach(i),oi=o($a,", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),$a.forEach(i),ti=d(ge),$e=l(ge,"LI",{});var Rr=n($e);li=o(Rr,"I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),Rr.forEach(i),ni=d(ge),A=l(ge,"LI",{});var ve=n(A);si=o(ve,"I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),Ie=l(ve,"EM",{});var Gr=n(Ie);ci=o(Gr,"dimostrazioni"),Gr.forEach(i),ui=o(ve,") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),Se=l(ve,"EM",{});var Or=n(Se);pi=o(Or,"apprendimento automatico"),Or.forEach(i),di=o(ve,"), o quasi!"),ve.forEach(i),ge.forEach(i),ua=d(e),fe=l(e,"P",{});var xr=n(fe);fi=o(xr,"Questo corso:"),xr.forEach(i),pa=d(e),_=l(e,"UL",{});var _e=n(_);He=l(_e,"LI",{});var Br=n(He);mi=o(Br,"Richiede una buona conoscenza di Python"),Br.forEach(i),hi=d(_e),h=l(_e,"LI",{});var S=n(h);gi=o(S,"Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),ke=l(S,"EM",{});var Qr=n(ke);vi=o(Qr,"apprendimento profondo"),Qr.forEach(i),_i=o(S,"), come ad esempio il "),j=l(S,"A",{href:!0,rel:!0});var Ur=n(j);zi=o(Ur,"Practical Deep Learning for Coders"),Ur.forEach(i),Ei=o(S," di "),K=l(S,"A",{href:!0,rel:!0});var Jr=n(K);bi=o(Jr,"fast.ai"),Jr.forEach(i),wi=o(S,", oppure uno dei programmi sviluppati da "),X=l(S,"A",{href:!0,rel:!0});var Vr=n(X);Li=o(Vr,"DeepLearning.AI"),Vr.forEach(i),S.forEach(i),Pi=d(_e),N=l(_e,"LI",{});var ze=n(N);yi=o(ze,"Non richiede conoscenze pregresse di "),Z=l(ze,"A",{href:!0,rel:!0});var Wr=n(Z);Ai=o(Wr,"PyTorch"),Wr.forEach(i),Ni=o(ze," o "),ee=l(ze,"A",{href:!0,rel:!0});var Yr=n(ee);Ti=o(Yr,"TensorFlow"),Yr.forEach(i),$i=o(ze,", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),ze.forEach(i),_e.forEach(i),da=d(e),z=l(e,"P",{});var Ee=n(z);Ii=o(Ee,"Quando avrai completato questo corso, ti raccomandiamo di passare al "),ae=l(Ee,"A",{href:!0,rel:!0});var jr=n(ae);Si=o(jr,"Natural Language Processing Specialization"),jr.forEach(i),Hi=o(Ee," di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),Me=l(Ee,"EM",{});var Kr=n(Me);ki=o(Kr,"Memoria a breve termine a lungo termine"),Kr.forEach(i),Mi=o(Ee,")!"),Ee.forEach(i),fa=d(e),T=l(e,"H2",{class:!0});var Ia=n(T);F=l(Ia,"A",{id:!0,class:!0,href:!0});var Xr=n(F);Fe=l(Xr,"SPAN",{});var Zr=n(Fe);Ke(ie.$$.fragment,Zr),Zr.forEach(i),Xr.forEach(i),Fi=d(Ia),qe=l(Ia,"SPAN",{});var eo=n(qe);qi=o(eo,"Chi siamo?"),eo.forEach(i),Ia.forEach(i),ma=d(e),me=l(e,"P",{});var ao=n(me);Di=o(ao,"A proposito degli autori:"),ao.forEach(i),ha=d(e),re=l(e,"P",{});var mr=n(re);De=l(mr,"STRONG",{});var io=n(De);Ci=o(io,"Matthew Carrigan"),io.forEach(i),Ri=o(mr," \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),mr.forEach(i),ga=d(e),oe=l(e,"P",{});var hr=n(oe);Ce=l(hr,"STRONG",{});var ro=n(Ce);Gi=o(ro,"Lysandre Debut"),ro.forEach(i),Oi=o(hr," \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),hr.forEach(i),va=d(e),g=l(e,"P",{});var pe=n(g);Re=l(pe,"STRONG",{});var oo=n(Re);xi=o(oo,"Sylvain Gugger"),oo.forEach(i),Bi=o(pe," \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),te=l(pe,"A",{href:!0,rel:!0});var to=n(te);Qi=o(to,"Deep Learning for Coders with fastai and PyTorch"),to.forEach(i),Ui=o(pe," con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Ge=l(pe,"EM",{});var lo=n(Ge);Ji=o(lo,"apprendimento profondo"),lo.forEach(i),Vi=o(pe,") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),pe.forEach(i),_a=d(e),le=l(e,"P",{});var gr=n(le);Oe=l(gr,"STRONG",{});var no=n(Oe);Wi=o(no,"Merve Noyan"),no.forEach(i),Yi=o(gr," \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),gr.forEach(i),za=d(e),ne=l(e,"P",{});var vr=n(ne);xe=l(vr,"STRONG",{});var so=n(xe);ji=o(so,"Lucile Saulnier"),so.forEach(i),Ki=o(vr," \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),vr.forEach(i),Ea=d(e),$=l(e,"P",{});var We=n($);Be=l(We,"STRONG",{});var co=n(Be);Xi=o(co,"Lewis Tunstall"),co.forEach(i),Zi=o(We," \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),se=l(We,"A",{href:!0,rel:!0});var uo=n(se);er=o(uo,"O\u2019Reilly book on Transformers"),uo.forEach(i),ar=o(We,"."),We.forEach(i),ba=d(e),I=l(e,"P",{});var Ye=n(I);Qe=l(Ye,"STRONG",{});var po=n(Qe);ir=o(po,"Leandro von Werra"),po.forEach(i),rr=o(Ye," \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),ce=l(Ye,"A",{href:!0,rel:!0});var fo=n(ce);or=o(fo,"O\u2019Reilly book on Transformers"),fo.forEach(i),tr=o(Ye,". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),Ye.forEach(i),wa=d(e),he=l(e,"P",{});var mo=n(he);lr=o(mo,"Sei pronto/a a iniziare? In questo capitolo, imparerai:"),mo.forEach(i),La=d(e),E=l(e,"UL",{});var be=n(E);ue=l(be,"LI",{});var Sa=n(ue);nr=o(Sa,"Ad utilizzare la funzione "),Ue=l(Sa,"CODE",{});var ho=n(Ue);sr=o(ho,"pipeline()"),ho.forEach(i),cr=o(Sa," per eseguire compiti di NLP come la generazione e classificazione di testi"),Sa.forEach(i),ur=d(be),Je=l(be,"LI",{});var go=n(Je);pr=o(go,"L\u2019architettura dei Transformer"),go.forEach(i),dr=d(be),Ve=l(be,"LI",{});var vo=n(Ve);fr=o(vo,"Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),vo.forEach(i),be.forEach(i),this.h()},h(){s(b,"name","hf:doc:metadata"),s(b,"content",JSON.stringify(No)),s(H,"id","introduzione"),s(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(H,"href","#introduzione"),s(w,"class","relative group"),s(k,"id","benvenutoa-al-corso-di"),s(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(k,"href","#benvenutoa-al-corso-di"),s(L,"class","relative group"),s(R,"href","https://huggingface.co/"),s(R,"rel","nofollow"),s(G,"href","https://github.com/huggingface/transformers"),s(G,"rel","nofollow"),s(O,"href","https://github.com/huggingface/datasets"),s(O,"rel","nofollow"),s(x,"href","https://github.com/huggingface/tokenizers"),s(x,"rel","nofollow"),s(B,"href","https://github.com/huggingface/accelerate"),s(B,"rel","nofollow"),s(Q,"href","https://huggingface.co/models"),s(Q,"rel","nofollow"),s(M,"id","contenuti"),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#contenuti"),s(P,"class","relative group"),s(J,"class","block dark:hidden"),_o(J.src,zr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||s(J,"src",zr),s(J,"alt","Brief overview of the chapters of the course."),s(V,"class","hidden dark:block"),_o(V.src,Er="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||s(V,"src",Er),s(V,"alt","Brief overview of the chapters of the course."),s(y,"class","flex justify-center"),s(Y,"href","https://huggingface.co/models"),s(Y,"rel","nofollow"),s(j,"href","https://course.fast.ai/"),s(j,"rel","nofollow"),s(K,"href","https://www.fast.ai/"),s(K,"rel","nofollow"),s(X,"href","https://www.deeplearning.ai/"),s(X,"rel","nofollow"),s(Z,"href","https://pytorch.org/"),s(Z,"rel","nofollow"),s(ee,"href","https://www.tensorflow.org/"),s(ee,"rel","nofollow"),s(ae,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),s(ae,"rel","nofollow"),s(F,"id","chi-siamo"),s(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(F,"href","#chi-siamo"),s(T,"class","relative group"),s(te,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),s(te,"rel","nofollow"),s(se,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),s(se,"rel","nofollow"),s(ce,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),s(ce,"rel","nofollow")},m(e,c){a(document.head,b),u(e,ia,c),u(e,w,c),a(w,H),a(H,we),Xe(q,we,null),a(w,ka),a(w,Le),a(Le,Ma),u(e,ra,c),u(e,L,c),a(L,k),a(k,Pe),Xe(D,Pe,null),a(L,Fa),a(L,ye),a(ye,qa),u(e,oa,c),Xe(C,e,c),u(e,ta,c),u(e,f,c),a(f,Da),a(f,Ae),a(Ae,Ca),a(f,Ra),a(f,R),a(R,Ga),a(f,Oa),a(f,G),a(G,xa),a(f,Ba),a(f,O),a(O,Qa),a(f,Ua),a(f,x),a(x,Ja),a(f,Va),a(f,B),a(B,Wa),a(f,Ya),a(f,Q),a(Q,ja),a(f,Ka),u(e,la,c),u(e,P,c),a(P,M),a(M,Ne),Xe(U,Ne,null),a(P,Xa),a(P,Te),a(Te,Za),u(e,na,c),u(e,de,c),a(de,ei),u(e,sa,c),u(e,y,c),a(y,J),a(y,ai),a(y,V),u(e,ca,c),u(e,v,c),a(v,W),a(W,ii),a(W,Y),a(Y,ri),a(W,oi),a(v,ti),a(v,$e),a($e,li),a(v,ni),a(v,A),a(A,si),a(A,Ie),a(Ie,ci),a(A,ui),a(A,Se),a(Se,pi),a(A,di),u(e,ua,c),u(e,fe,c),a(fe,fi),u(e,pa,c),u(e,_,c),a(_,He),a(He,mi),a(_,hi),a(_,h),a(h,gi),a(h,ke),a(ke,vi),a(h,_i),a(h,j),a(j,zi),a(h,Ei),a(h,K),a(K,bi),a(h,wi),a(h,X),a(X,Li),a(_,Pi),a(_,N),a(N,yi),a(N,Z),a(Z,Ai),a(N,Ni),a(N,ee),a(ee,Ti),a(N,$i),u(e,da,c),u(e,z,c),a(z,Ii),a(z,ae),a(ae,Si),a(z,Hi),a(z,Me),a(Me,ki),a(z,Mi),u(e,fa,c),u(e,T,c),a(T,F),a(F,Fe),Xe(ie,Fe,null),a(T,Fi),a(T,qe),a(qe,qi),u(e,ma,c),u(e,me,c),a(me,Di),u(e,ha,c),u(e,re,c),a(re,De),a(De,Ci),a(re,Ri),u(e,ga,c),u(e,oe,c),a(oe,Ce),a(Ce,Gi),a(oe,Oi),u(e,va,c),u(e,g,c),a(g,Re),a(Re,xi),a(g,Bi),a(g,te),a(te,Qi),a(g,Ui),a(g,Ge),a(Ge,Ji),a(g,Vi),u(e,_a,c),u(e,le,c),a(le,Oe),a(Oe,Wi),a(le,Yi),u(e,za,c),u(e,ne,c),a(ne,xe),a(xe,ji),a(ne,Ki),u(e,Ea,c),u(e,$,c),a($,Be),a(Be,Xi),a($,Zi),a($,se),a(se,er),a($,ar),u(e,ba,c),u(e,I,c),a(I,Qe),a(Qe,ir),a(I,rr),a(I,ce),a(ce,or),a(I,tr),u(e,wa,c),u(e,he,c),a(he,lr),u(e,La,c),u(e,E,c),a(E,ue),a(ue,nr),a(ue,Ue),a(Ue,sr),a(ue,cr),a(E,ur),a(E,Je),a(Je,pr),a(E,dr),a(E,Ve),a(Ve,fr),Pa=!0},p:Lo,i(e){Pa||(Ze(q.$$.fragment,e),Ze(D.$$.fragment,e),Ze(C.$$.fragment,e),Ze(U.$$.fragment,e),Ze(ie.$$.fragment,e),Pa=!0)},o(e){ea(q.$$.fragment,e),ea(D.$$.fragment,e),ea(C.$$.fragment,e),ea(U.$$.fragment,e),ea(ie.$$.fragment,e),Pa=!1},d(e){i(b),e&&i(ia),e&&i(w),aa(q),e&&i(ra),e&&i(L),aa(D),e&&i(oa),aa(C,e),e&&i(ta),e&&i(f),e&&i(la),e&&i(P),aa(U),e&&i(na),e&&i(de),e&&i(sa),e&&i(y),e&&i(ca),e&&i(v),e&&i(ua),e&&i(fe),e&&i(pa),e&&i(_),e&&i(da),e&&i(z),e&&i(fa),e&&i(T),aa(ie),e&&i(ma),e&&i(me),e&&i(ha),e&&i(re),e&&i(ga),e&&i(oe),e&&i(va),e&&i(g),e&&i(_a),e&&i(le),e&&i(za),e&&i(ne),e&&i(Ea),e&&i($),e&&i(ba),e&&i(I),e&&i(wa),e&&i(he),e&&i(La),e&&i(E)}}}const No={local:"introduzione",sections:[{local:"benvenutoa-al-corso-di",title:"Benvenuto/a al corso di \u{1F917}!"},{local:"contenuti",title:"Contenuti"},{local:"chi-siamo",title:"Chi siamo?"}],title:"Introduzione"};function To(_r){return Po(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ho extends zo{constructor(b){super();Eo(this,b,To,Ao,bo,{})}}export{Ho as default,No as metadata};
