var Ot=Object.defineProperty,It=Object.defineProperties;var jt=Object.getOwnPropertyDescriptors;var vt=Object.getOwnPropertySymbols;var Mt=Object.prototype.hasOwnProperty,Ct=Object.prototype.propertyIsEnumerable;var $t=(v,a,o)=>a in v?Ot(v,a,{enumerable:!0,configurable:!0,writable:!0,value:o}):v[a]=o,Ca=(v,a)=>{for(var o in a||(a={}))Mt.call(a,o)&&$t(v,o,a[o]);if(vt)for(var o of vt(a))Ct.call(a,o)&&$t(v,o,a[o]);return v},Ha=(v,a)=>It(v,jt(a));import{D as Ht,S as Et,i as At,s as yt,e as c,k as w,Y as Nt,l as _a,c as u,a as m,m as q,Z as St,d as t,b as d,g,F as n,Q as Pt,t as I,h as j,j as Dt,_ as Qt,L as _t,G as Ut,$ as Ft,v as Tt,a0 as gt,a1 as Lt,w as S,M as Vt,x as P,y as T,o as y,p as kt,q as N,B as O,n as bt}from"../../chunks/vendor-e6c5d93e.js";import{I as D}from"../../chunks/IconCopyLink-7b8d27fe.js";import{C as Qa}from"../../chunks/CodeBlock-37867453.js";import{F as Bt}from"../../chunks/FrameworkSwitchCourse-ab838f47.js";const Da=Ht({});function xt(v,a,o){const l=v.slice();return l[11]=a[o],l[13]=o,l}function wt(v){let a,o,l=v[11].correct?"Correct!":"Incorrect.",i,h,f,A=v[11].explain+"",k;return{c(){a=c("div"),o=c("span"),i=I(l),h=w(),f=new Nt,this.h()},l($){a=u($,"DIV",{class:!0});var z=m(a);o=u(z,"SPAN",{class:!0});var x=m(o);i=j(x,l),x.forEach(t),h=q(z),f=St(z),z.forEach(t),this.h()},h(){d(o,"class","font-bold"),f.a=null,d(a,"class",k="alert alert-"+(v[11].correct?"success":"error")+" mt-1 mb-2.5 leading-normal")},m($,z){g($,a,z),n(a,o),n(o,i),n(a,h),f.m(A,a)},p($,z){z&1&&l!==(l=$[11].correct?"Correct!":"Incorrect.")&&Dt(i,l),z&1&&A!==(A=$[11].explain+"")&&f.p(A),z&1&&k!==(k="alert alert-"+($[11].correct?"success":"error")+" mt-1 mb-2.5 leading-normal")&&d(a,"class",k)},d($){$&&t(a)}}}function qt(v){let a,o,l,i,h,f=v[11].text+"",A,k=v[4]&&v[4].includes(v[13]),$,z,x,p=k&&wt(v);return{c(){a=c("label"),o=c("input"),i=w(),h=new Nt,A=w(),p&&p.c(),$=_a(),this.h()},l(r){a=u(r,"LABEL",{class:!0});var s=m(a);o=u(s,"INPUT",{autocomplete:!0,class:!0,name:!0,type:!0}),i=q(s),h=St(s),s.forEach(t),A=q(r),p&&p.l(r),$=_a(),this.h()},h(){d(o,"autocomplete","off"),d(o,"class","form-input -mt-1.5 mr-2"),d(o,"name","choice"),d(o,"type","checkbox"),o.__value=l=v[13],o.value=o.__value,v[7][0].push(o),h.a=null,d(a,"class","block")},m(r,s){g(r,a,s),n(a,o),o.checked=~v[3].indexOf(o.__value),n(a,i),h.m(f,a),g(r,A,s),p&&p.m(r,s),g(r,$,s),z||(x=Pt(o,"change",v[6]),z=!0)},p(r,s){s&8&&(o.checked=~r[3].indexOf(o.__value)),s&1&&f!==(f=r[11].text+"")&&h.p(f),s&16&&(k=r[4]&&r[4].includes(r[13])),k?p?p.p(r,s):(p=wt(r),p.c(),p.m($.parentNode,$)):p&&(p.d(1),p=null)},d(r){r&&t(a),v[7][0].splice(v[7][0].indexOf(o),1),r&&t(A),p&&p.d(r),r&&t($),z=!1,x()}}}function zt(v){let a;function o(h,f){return h[1]?Gt:h[2]?Rt:Yt}let l=o(v),i=l(v);return{c(){a=c("div"),i.c(),this.h()},l(h){a=u(h,"DIV",{class:!0});var f=m(a);i.l(f),f.forEach(t),this.h()},h(){d(a,"class","font-semibold leading-snug")},m(h,f){g(h,a,f),i.m(a,null)},p(h,f){l!==(l=o(h))&&(i.d(1),i=l(h),i&&(i.c(),i.m(a,null)))},d(h){h&&t(a),i.d()}}}function Yt(v){let a,o;return{c(){a=c("span"),o=I("You got all the answers!"),this.h()},l(l){a=u(l,"SPAN",{class:!0});var i=m(a);o=j(i,"You got all the answers!"),i.forEach(t),this.h()},h(){d(a,"class","text-green-900 dark:text-green-200")},m(l,i){g(l,a,i),n(a,o)},d(l){l&&t(a)}}}function Rt(v){let a,o;return{c(){a=c("span"),o=I("You didn't select all the correct answers, there's more!"),this.h()},l(l){a=u(l,"SPAN",{class:!0});var i=m(a);o=j(i,"You didn't select all the correct answers, there's more!"),i.forEach(t),this.h()},h(){d(a,"class","text-red-900 dark:text-red-200")},m(l,i){g(l,a,i),n(a,o)},d(l){l&&t(a)}}}function Gt(v){let a,o;return{c(){a=c("span"),o=I("Looks like at least one of your answers is wrong, try again!"),this.h()},l(l){a=u(l,"SPAN",{class:!0});var i=m(a);o=j(i,"Looks like at least one of your answers is wrong, try again!"),i.forEach(t),this.h()},h(){d(a,"class","text-red-900 dark:text-red-200")},m(l,i){g(l,a,i),n(a,o)},d(l){l&&t(a)}}}function Jt(v){let a,o,l,i,h,f,A,k,$,z,x=v[0],p=[];for(let s=0;s<x.length;s+=1)p[s]=qt(xt(v,x,s));let r=v[4].length&&zt(v);return{c(){a=c("div"),o=c("form");for(let s=0;s<p.length;s+=1)p[s].c();l=w(),i=c("div"),h=c("button"),f=I("Submit"),k=w(),r&&r.c(),this.h()},l(s){a=u(s,"DIV",{});var b=m(a);o=u(b,"FORM",{});var E=m(o);for(let H=0;H<p.length;H+=1)p[H].l(E);l=q(E),i=u(E,"DIV",{class:!0});var M=m(i);h=u(M,"BUTTON",{class:!0,type:!0});var C=m(h);f=j(C,"Submit"),C.forEach(t),k=q(M),r&&r.l(M),M.forEach(t),E.forEach(t),b.forEach(t),this.h()},h(){d(h,"class","btn px-4 mr-4"),d(h,"type","submit"),h.disabled=A=!v[3].length,d(i,"class","flex flex-row items-center mt-3")},m(s,b){g(s,a,b),n(a,o);for(let E=0;E<p.length;E+=1)p[E].m(o,null);n(o,l),n(o,i),n(i,h),n(h,f),n(i,k),r&&r.m(i,null),$||(z=Pt(o,"submit",Qt(v[8])),$=!0)},p(s,[b]){if(b&25){x=s[0];let E;for(E=0;E<x.length;E+=1){const M=xt(s,x,E);p[E]?p[E].p(M,b):(p[E]=qt(M),p[E].c(),p[E].m(o,l))}for(;E<p.length;E+=1)p[E].d(1);p.length=x.length}b&8&&A!==(A=!s[3].length)&&(h.disabled=A),s[4].length?r?r.p(s,b):(r=zt(s),r.c(),r.m(i,null)):r&&(r.d(1),r=null)},i:_t,o:_t,d(s){s&&t(a),Ut(p,s),r&&r.d(),$=!1,z()}}}function Wt(v="_"){return`${v}${Math.random().toString(36).substr(2,9)}`}function Zt(v,a,o){let l;Ft(v,Da,s=>o(9,l=s));let{choices:i}=a;const h=Wt();let f=!1,A=!1,k=[],$=[];Tt(()=>{gt(Da,l=Ha(Ca({},l),{[h]:{correct:!1}}),l)});function z(){o(1,f=!1),o(2,A=!1);for(let b=0;b<i.length;b++){const E=i[b];E.correct&&!k.includes(b)?o(2,A=!0):!E.correct&&k.includes(b)&&o(1,f=!0)}if(o(4,$=k),gt(Da,l=Ha(Ca({},l),{[h]:{correct:!A&&!f}}),l),Object.values(l).every(({correct:b})=>b)){const b=new Event("ChapterComplete");window.dispatchEvent(b)}}const x=[[]];function p(){k=Lt(x[0],this.__value,this.checked),o(3,k)}const r=()=>z();return v.$$set=s=>{"choices"in s&&o(0,i=s.choices)},[i,f,A,k,$,z,p,x,r]}class Q extends Et{constructor(a){super();At(this,a,Zt,Jt,yt,{choices:0})}}function Kt(v){let a,o,l,i,h,f,A,k,$,z,x,p,r;return i=new D({}),p=new Q({props:{choices:[{text:"Um modelo que treina automaticamente com seus dados",explain:"Incorreto. Voc\xEA est\xE1 confundindo isto com nosso produto de <a href='https://huggingface.co/autonlp'>AutoNLP</a>?"},{text:"Um objeto que devolve a arquitetura correta com base em um checkpoint",explain:"Exatamente: o <code>TFAutoModel</code> s\xF3 precisa saber o checkpoint para saber como inicializar ent\xE3o devolver a arquitetura correta.",correct:!0},{text:"Um modelo que detecta automaticamente a linguagem utilizada para suas entradas a fim de carregar os pesos corretos",explain:"Incorreto; embora alguns checkpoints e modelos sejam capazes de lidar com v\xE1rios idiomas, n\xE3o h\xE1 ferramentas embutidas para sele\xE7\xE3o autom\xE1tica de checkpoints de acordo com o idioma. Voc\xEA deve ir para o  <a href='https://huggingface.co/models'>Model Hub</a> para encontrar o melhor checkpoint para realizar sua tarefa!"}]}}),{c(){a=c("h3"),o=c("a"),l=c("span"),S(i.$$.fragment),h=w(),f=c("span"),A=I("5. O que seria um "),k=c("code"),$=I("TFAutoModel"),z=I("?"),x=w(),S(p.$$.fragment),this.h()},l(s){a=u(s,"H3",{class:!0});var b=m(a);o=u(b,"A",{id:!0,class:!0,href:!0});var E=m(o);l=u(E,"SPAN",{});var M=m(l);P(i.$$.fragment,M),M.forEach(t),E.forEach(t),h=q(b),f=u(b,"SPAN",{});var C=m(f);A=j(C,"5. O que seria um "),k=u(C,"CODE",{});var H=m(k);$=j(H,"TFAutoModel"),H.forEach(t),z=j(C,"?"),C.forEach(t),b.forEach(t),x=q(s),P(p.$$.fragment,s),this.h()},h(){d(o,"id","5.-o-que-seria-um-<code>tfautomodel</code>?"),d(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o,"href","#5.-o-que-seria-um-<code>tfautomodel</code>?"),d(a,"class","relative group")},m(s,b){g(s,a,b),n(a,o),n(o,l),T(i,l,null),n(a,h),n(a,f),n(f,A),n(f,k),n(k,$),n(f,z),g(s,x,b),T(p,s,b),r=!0},i(s){r||(N(i.$$.fragment,s),N(p.$$.fragment,s),r=!0)},o(s){y(i.$$.fragment,s),y(p.$$.fragment,s),r=!1},d(s){s&&t(a),O(i),s&&t(x),O(p,s)}}}function Xt(v){let a,o,l,i,h,f,A,k,$,z,x,p,r;return i=new D({}),p=new Q({props:{choices:[{text:"Um modelo que treina automaticamente com seus dados",explain:"Incorreto. Voc\xEA est\xE1 confundindo isto com nosso produto de <a href='https://huggingface.co/autonlp'>AutoNLP</a>?"},{text:"Um objeto que devolve a arquitetura correta com base em um checkpoint",explain:"Exatamente: o <code>AutoModel</code> s\xF3 precisa saber o checkpoint para saber como inicializar ent\xE3o devolver a arquitetura correta.",correct:!0},{text:"Um modelo que detecta automaticamente a linguagem utilizada para suas entradas a fim de carregar os pesos corretos",explain:"Incorreto; embora alguns checkpoints e modelos sejam capazes de lidar com v\xE1rios idiomas, n\xE3o h\xE1 ferramentas embutidas para sele\xE7\xE3o autom\xE1tica de checkpoints de acordo com o idioma. Voc\xEA deve ir para o  <a href='https://huggingface.co/models'>Model Hub</a> para encontrar o melhor checkpoint para realizar sua tarefa!"}]}}),{c(){a=c("h3"),o=c("a"),l=c("span"),S(i.$$.fragment),h=w(),f=c("span"),A=I("5. O que seria um "),k=c("code"),$=I("AutoModel"),z=I("?"),x=w(),S(p.$$.fragment),this.h()},l(s){a=u(s,"H3",{class:!0});var b=m(a);o=u(b,"A",{id:!0,class:!0,href:!0});var E=m(o);l=u(E,"SPAN",{});var M=m(l);P(i.$$.fragment,M),M.forEach(t),E.forEach(t),h=q(b),f=u(b,"SPAN",{});var C=m(f);A=j(C,"5. O que seria um "),k=u(C,"CODE",{});var H=m(k);$=j(H,"AutoModel"),H.forEach(t),z=j(C,"?"),C.forEach(t),b.forEach(t),x=q(s),P(p.$$.fragment,s),this.h()},h(){d(o,"id","5.-o-que-seria-um-<code>automodel</code>?"),d(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o,"href","#5.-o-que-seria-um-<code>automodel</code>?"),d(a,"class","relative group")},m(s,b){g(s,a,b),n(a,o),n(o,l),T(i,l,null),n(a,h),n(a,f),n(f,A),n(f,k),n(k,$),n(f,z),g(s,x,b),T(p,s,b),r=!0},i(s){r||(N(i.$$.fragment,s),N(p.$$.fragment,s),r=!0)},o(s){y(i.$$.fragment,s),y(p.$$.fragment,s),r=!1},d(s){s&&t(a),O(i),s&&t(x),O(p,s)}}}function eo(v){let a,o,l,i,h,f,A,k,$,z,x,p;return i=new D({}),$=new Qa({props:{code:`from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),x=new Q({props:{choices:[{text:"N\xE3o, parece correto.",explain:"Infelizmente, acoplar um modelo com um tokenizer que foi treinado com um checkpoint diferente raramente \xE9 uma boa ideia. O modelo n\xE3o foi treinado para fazer sentido a partir da sa\xEDda deste tokenizer, ent\xE3o a sa\xEDda do modelo (isto se ele realmente funcionar!) n\xE3o far\xE1 nenhum sentido."},{text:"O tokenizer e o modelo devem ser sempre a partir do mesmo checkpoint.",explain:"Correto!",correct:!0},{text:"\xC9 uma boa pr\xE1tica realizar o padding e truncar com o tokenizer, pois cada entrada \xE9 um batch.",explain:"\xC9 verdade que cada entrada do modelo precisa ser um batch. Entretanto, truncando ou realizando o padding desta sequ\xEAncia n\xE3o faria necessariamente sentido, pois existe apenas uma delas, e estas s\xE3o t\xE9cnicas para criar batches de uma lista de senten\xE7as."}]}}),{c(){a=c("h3"),o=c("a"),l=c("span"),S(i.$$.fragment),h=w(),f=c("span"),A=I("10. Tem algo errado com o c\xF3digo abaixo?"),k=w(),S($.$$.fragment),z=w(),S(x.$$.fragment),this.h()},l(r){a=u(r,"H3",{class:!0});var s=m(a);o=u(s,"A",{id:!0,class:!0,href:!0});var b=m(o);l=u(b,"SPAN",{});var E=m(l);P(i.$$.fragment,E),E.forEach(t),b.forEach(t),h=q(s),f=u(s,"SPAN",{});var M=m(f);A=j(M,"10. Tem algo errado com o c\xF3digo abaixo?"),M.forEach(t),s.forEach(t),k=q(r),P($.$$.fragment,r),z=q(r),P(x.$$.fragment,r),this.h()},h(){d(o,"id","10.-tem-algo-errado-com-o-c\xF3digo-abaixo?"),d(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o,"href","#10.-tem-algo-errado-com-o-c\xF3digo-abaixo?"),d(a,"class","relative group")},m(r,s){g(r,a,s),n(a,o),n(o,l),T(i,l,null),n(a,h),n(a,f),n(f,A),g(r,k,s),T($,r,s),g(r,z,s),T(x,r,s),p=!0},i(r){p||(N(i.$$.fragment,r),N($.$$.fragment,r),N(x.$$.fragment,r),p=!0)},o(r){y(i.$$.fragment,r),y($.$$.fragment,r),y(x.$$.fragment,r),p=!1},d(r){r&&t(a),O(i),r&&t(k),O($,r),r&&t(z),O(x,r)}}}function ao(v){let a,o,l,i,h,f,A,k,$,z,x,p;return i=new D({}),$=new Qa({props:{code:`from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),x=new Q({props:{choices:[{text:"N\xE3o, parece correto.",explain:"Infelizmente, acoplar um modelo com um tokenizer que foi treinado com um checkpoint diferente raramente \xE9 uma boa ideia. O modelo n\xE3o foi treinado para fazer sentido a partir da sa\xEDda deste tokenizer, ent\xE3o a sa\xEDda do modelo (isto se ele realmente funcionar!) n\xE3o far\xE1 nenhum sentido."},{text:"O tokenizer e o modelo devem ser sempre a partir do mesmo checkpoint.",explain:"Correto!",correct:!0},{text:"\xC9 uma boa pr\xE1tica realizar o padding e truncar com o tokenizer, pois cada entrada \xE9 um batch.",explain:"\xC9 verdade que cada entrada do modelo precisa ser um batch. Entretanto, truncando ou realizando o padding desta sequ\xEAncia n\xE3o faria necessariamente sentido, pois existe apenas uma delas, e estas s\xE3o t\xE9cnicas para criar batches de uma lista de senten\xE7as."}]}}),{c(){a=c("h3"),o=c("a"),l=c("span"),S(i.$$.fragment),h=w(),f=c("span"),A=I("10. Tem algo errado com o c\xF3digo abaixo?"),k=w(),S($.$$.fragment),z=w(),S(x.$$.fragment),this.h()},l(r){a=u(r,"H3",{class:!0});var s=m(a);o=u(s,"A",{id:!0,class:!0,href:!0});var b=m(o);l=u(b,"SPAN",{});var E=m(l);P(i.$$.fragment,E),E.forEach(t),b.forEach(t),h=q(s),f=u(s,"SPAN",{});var M=m(f);A=j(M,"10. Tem algo errado com o c\xF3digo abaixo?"),M.forEach(t),s.forEach(t),k=q(r),P($.$$.fragment,r),z=q(r),P(x.$$.fragment,r),this.h()},h(){d(o,"id","10.-tem-algo-errado-com-o-c\xF3digo-abaixo?"),d(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o,"href","#10.-tem-algo-errado-com-o-c\xF3digo-abaixo?"),d(a,"class","relative group")},m(r,s){g(r,a,s),n(a,o),n(o,l),T(i,l,null),n(a,h),n(a,f),n(f,A),g(r,k,s),T($,r,s),g(r,z,s),T(x,r,s),p=!0},i(r){p||(N(i.$$.fragment,r),N($.$$.fragment,r),N(x.$$.fragment,r),p=!0)},o(r){y(i.$$.fragment,r),y($.$$.fragment,r),y(x.$$.fragment,r),p=!1},d(r){r&&t(a),O(i),r&&t(k),O($,r),r&&t(z),O(x,r)}}}function to(v){let a,o,l,i,h,f,A,k,$,z,x,p,r,s,b,E,M,C,H,Ve,se,Be,B,K,Se,ne,ga,Pe,ka,Ye,ie,Re,Y,X,Te,le,ba,Oe,xa,Ge,ce,Je,R,ee,Ie,ue,wa,de,qa,je,za,Ea,We,me,Ze,U,F,Ee,G,ae,Me,pe,Aa,Ce,ya,Ke,fe,Xe,J,te,He,he,Na,De,Sa,ea,ve,aa,W,oe,Qe,$e,Pa,Ue,Ta,ta,_e,oa,Z,re,Fe,ge,Oa,ke,Ia,Le,ja,Ma,ra,be,sa,xe,na,L,V,Ae,ia;l=new Bt({props:{fw:v[0]}}),k=new D({}),E=new D({}),se=new Q({props:{choices:[{text:"Primeiro, o modelo trata de textos e devolve previs\xF5es em bruto. O tokenizer, ent\xE3o, tr\xE1s sentido a estas previs\xF5es e as converte de volta ao texto quando necess\xE1rio.",explain:"O modelo n\xE3o consegue entender o texto! O tokenizer deve primeiro simbolizar o texto e convert\xEA-lo em IDs para que seja compreens\xEDvel pelo modelo."},{text:"Primeiro, o tokenizer trata de textos e devolve as identifica\xE7\xF5es (IDs). O modelo lida com estas identifica\xE7\xF5es e produz uma predi\xE7\xE3o, que pode ser algum texto.",explain:"A predi\xE7\xE3o do modelo n\xE3o pode ser feita de imediato. O tokenizer tem que ser usado para converter a predi\xE7\xE3o de volta ao texto!"},{text:"O tokenizer trata de textos e devolve os IDs. O modelo lida com estes IDs e produz uma predi\xE7\xE3o. O tokenizer pode ent\xE3o ser usado mais uma vez para converter estas previs\xF5es de volta para algum texto.",explain:"Correto! O tokenizer pode ser usado tanto para a tokeniza\xE7\xE3o quanto para a destokeniza\xE7\xE3o.",correct:!0}]}}),ne=new D({}),ie=new Q({props:{choices:[{text:"2: O comprimento da sequ\xEAncia e o tamanho do batch (lote)",explain:"Falso! A sa\xEDda do tensor pelo modelo tem uma terceira dimens\xE3o: o tamanho das camadas ocultas"},{text:"2: O comprimento da sequ\xEAncia e o tamanho das camadas ocultas",explain:"Falso! Todos os Transformer lidam com batches, mesmo com uma \xFAnica sequ\xEAncia; isso seria um tamanho de batch de 1!"},{text:"3: O comprimento da sequ\xEAncia, o tamanho do batch (lote) e o tamanho das camadas ocultas",explain:"Correto!",correct:!0}]}}),le=new D({}),ce=new Q({props:{choices:[{text:"WordPiece",explain:"Exatamente, este \xE9 um exemplo de Tokeniza\xE7\xE3o por sub-palavra!",correct:!0},{text:"Character-based tokenization",explain:"Character-based tokenization n\xE3o \xE9 uma tokeniza\xE7\xE3o por sub-palavra!."},{text:"Divis\xE3o no espa\xE7o em branco e pontua\xE7\xE3o",explain:"Esse \xE9 um esquema de tokenization baseado em palavras!"},{text:"BPE",explain:"Exatamente, este \xE9 um exemplo de Tokeniza\xE7\xE3o por sub-palavra!",correct:!0},{text:"Unigram",explain:"Exatamente, este \xE9 um exemplo de Tokeniza\xE7\xE3o por sub-palavra!",correct:!0},{text:"Nenhuma das acimas",explain:"Errado!"}]}}),ue=new D({}),me=new Q({props:{choices:[{text:"Um componente dos transformers de base que redireciona os tensores para suas camadas corretas",explain:"Incorreto! N\xE3o existe tal componente."},{text:"Tamb\xE9m conhecido como mecanismo de auto-aten\xE7\xE3o (*self-attention*), ele adapta a representa\xE7\xE3o de um s\xEDmbolo de acordo com os outros tokens da sequ\xEAncia",explain:"Incorreto! A camada de auto-aten\xE7\xE3o cont\xE9m 'attention heads', mas estas n\xE3o s\xE3o as heads de adapta\xE7\xE3o."},{text:"Um componente adicional, geralmente composto de uma ou poucas camadas, para converter as previs\xF5es do Transformer em uma sa\xEDda espec\xEDfica de tarefa",explain:"\xC9 isso mesmo. As heads de adapta\xE7\xE3o, tamb\xE9m conhecidos simplesmente como heads, surgem em diferentes formas: heads de modelagem de linguagem, heads de resposta a perguntas, heads de classifica\xE7\xE3o de sequ\xEAncia.. ",correct:!0}]}});const Ua=[Xt,Kt],we=[];function Fa(e,_){return e[0]==="pt"?0:1}U=Fa(v),F=we[U]=Ua[U](v),pe=new D({}),fe=new Q({props:{choices:[{text:"Truncar",explain:"Sim, a truncagem \xE9 uma forma correta de sequ\xEAncias de sa\xEDda noturna para que elas se encaixem em forma retangular.No entanto, seria a \xFAnica?",correct:!0},{text:"Retornar tensores",explain:"Enquanto as outras t\xE9cnicas permitem retornar tensores retangulares, retornar tensores n\xE3o \xE9 \xFAtil quando realizar batches de sequ\xEAncias juntas."},{text:"Padding",explain:"Sim, padding \xE9 uma forma correta para as sequ\xEAncias de sa\xEDda para que elas se encaixem em uma forma retangular. No entanto, seria a \xFAnica?",correct:!0},{text:"Attention masking",explain:"Exatamente! As attention masks s\xE3o de primordial import\xE2ncia quando se trata de sequ\xEAncias de diferentes tamanhos. No entanto, essa n\xE3o \xE9 a \xFAnica t\xE9cnica a ser conhecida.",correct:!0}]}}),he=new D({}),ve=new Q({props:{choices:[{text:"Suaviza os logits para que sejam mais confi\xE1veis.",explain:"N\xE3o, a fun\xE7\xE3o SoftMax n\xE3o afeta a confiabilidade dos resultados."},{text:"Aplica um limite inferior e um limite superior para que eles sejam compreens\xEDveis.",explain:"Correto! Os valores resultantes est\xE3o vinculados entre 0 e 1. Mas essa n\xE3o \xE9 a \xFAnica raz\xE3o pela qual usamos uma fun\xE7\xE3o SoftMax.",correct:!0},{text:"A soma total da sa\xEDda \xE9 1, resultando em uma poss\xEDvel interpreta\xE7\xE3o probabil\xEDstica.",explain:"Correto! Mas essa n\xE3o \xE9 a \xFAnica raz\xE3o pela qual usamos uma fun\xE7\xE3o SoftMax.",correct:!0}]}}),$e=new D({}),_e=new Q({props:{choices:[{text:"<code>encode</code>, pois pode codificar texto em IDs e IDs em predi\xE7\xF5es",explain:"Errado! O m\xE9todo <code>encode</code> existe na tokeniza\xE7\xE3o, por\xE9m n\xE3o existe nos modelos."},{text:"Chamando diretamente o objeto de tokeniza\xE7\xE3o (tokenizer).",explain:"Exatamente! O m\xE9todo <code>__call__</code> do tokenizer \xE9 um m\xE9todo muito poderoso que pode lidar com praticamente qualquer coisa. \xC9 tamb\xE9m o m\xE9todo usado para recuperar as predi\xE7\xF5es de um modelo.",correct:!0},{text:"<code>padding</code>",explain:"Errado! O padding \xE9 muito \xFAtil, mas \xE9 apenas uma parte da API do tokenizer."},{text:"<code>tokenize</code>",explain:"O m\xE9todo <code>tokenize</code> \xE9 indiscutivelmente um dos m\xE9todos mais \xFAteis, mas n\xE3o \xE9 o n\xFAcleo do API do tokenizer."}]}}),ge=new D({}),be=new Qa({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
result = tokenizer.tokenize(<span class="hljs-string">&quot;Hello!&quot;</span>)`}}),xe=new Q({props:{choices:[{text:"Uma lista de strings, sendo cada uma delas um token",explain:"Exatamente! Converta isto em IDs, e envie-os para um modelo!",correct:!0},{text:"Uma lista de IDs",explain:"Incorreto; isto \xE9 o para o os m\xE9todos <code>__call__</code> ou <code>convert_tokens_to_ids</code>!"},{text:"Uma string contendo todos os tokens ",explain:"Isto seria sub\xF3timo, pois o objetivo \xE9 dividir a string em v\xE1rios tokens."}]}});const La=[ao,eo],qe=[];function Va(e,_){return e[0]==="pt"?0:1}return L=Va(v),V=qe[L]=La[L](v),{c(){a=c("meta"),o=w(),S(l.$$.fragment),i=w(),h=c("h1"),f=c("a"),A=c("span"),S(k.$$.fragment),$=w(),z=c("span"),x=I("Question\xE1rio de fim de cap\xEDtulo"),p=w(),r=c("h3"),s=c("a"),b=c("span"),S(E.$$.fragment),M=w(),C=c("span"),H=I("1. Qual \xE9 a ordem do pipeline para a modelagem de linguagem?"),Ve=w(),S(se.$$.fragment),Be=w(),B=c("h3"),K=c("a"),Se=c("span"),S(ne.$$.fragment),ga=w(),Pe=c("span"),ka=I("2. Quantas dimens\xF5es tem o tensor do Transformer de base, e quais s\xE3o elas?"),Ye=w(),S(ie.$$.fragment),Re=w(),Y=c("h3"),X=c("a"),Te=c("span"),S(le.$$.fragment),ba=w(),Oe=c("span"),xa=I("3. Qual dos seguintes \xE9 um exemplo de Tokeniza\xE7\xE3o por sub-palavras?"),Ge=w(),S(ce.$$.fragment),Je=w(),R=c("h3"),ee=c("a"),Ie=c("span"),S(ue.$$.fragment),wa=w(),de=c("span"),qa=I("4. O que \xE9 uma "),je=c("em"),za=I("model head"),Ea=I("?"),We=w(),S(me.$$.fragment),Ze=w(),F.c(),Ee=w(),G=c("h3"),ae=c("a"),Me=c("span"),S(pe.$$.fragment),Aa=w(),Ce=c("span"),ya=I("6. Quais s\xE3o as t\xE9cnicas a serem observadas quando realizar batches com sequ\xEAncias de diferentes tamanhos?"),Ke=w(),S(fe.$$.fragment),Xe=w(),J=c("h3"),te=c("a"),He=c("span"),S(he.$$.fragment),Na=w(),De=c("span"),Sa=I("7. Qual \xE9 o objetivo de aplicar uma fun\xE7\xE3o SoftMax \xE0 sa\xEDda de logits para um modelo de classifica\xE7\xE3o sequencial??"),ea=w(),S(ve.$$.fragment),aa=w(),W=c("h3"),oe=c("a"),Qe=c("span"),S($e.$$.fragment),Pa=w(),Ue=c("span"),Ta=I("8. Qual \xE9 o m\xE9todo core da API tokenizer?"),ta=w(),S(_e.$$.fragment),oa=w(),Z=c("h3"),re=c("a"),Fe=c("span"),S(ge.$$.fragment),Oa=w(),ke=c("span"),Ia=I("9. O que a vari\xE1vel "),Le=c("code"),ja=I("result"),Ma=I(" cont\xE9m nesta peda\xE7o de c\xF3digo?"),ra=w(),S(be.$$.fragment),sa=w(),S(xe.$$.fragment),na=w(),V.c(),Ae=_a(),this.h()},l(e){const _=Vt('[data-svelte="svelte-1phssyn"]',document.head);a=u(_,"META",{name:!0,content:!0}),_.forEach(t),o=q(e),P(l.$$.fragment,e),i=q(e),h=u(e,"H1",{class:!0});var ze=m(h);f=u(ze,"A",{id:!0,class:!0,href:!0});var ye=m(f);A=u(ye,"SPAN",{});var Ne=m(A);P(k.$$.fragment,Ne),Ne.forEach(t),ye.forEach(t),$=q(ze),z=u(ze,"SPAN",{});var Ba=m(z);x=j(Ba,"Question\xE1rio de fim de cap\xEDtulo"),Ba.forEach(t),ze.forEach(t),p=q(e),r=u(e,"H3",{class:!0});var la=m(r);s=u(la,"A",{id:!0,class:!0,href:!0});var Ya=m(s);b=u(Ya,"SPAN",{});var Ra=m(b);P(E.$$.fragment,Ra),Ra.forEach(t),Ya.forEach(t),M=q(la),C=u(la,"SPAN",{});var Ga=m(C);H=j(Ga,"1. Qual \xE9 a ordem do pipeline para a modelagem de linguagem?"),Ga.forEach(t),la.forEach(t),Ve=q(e),P(se.$$.fragment,e),Be=q(e),B=u(e,"H3",{class:!0});var ca=m(B);K=u(ca,"A",{id:!0,class:!0,href:!0});var Ja=m(K);Se=u(Ja,"SPAN",{});var Wa=m(Se);P(ne.$$.fragment,Wa),Wa.forEach(t),Ja.forEach(t),ga=q(ca),Pe=u(ca,"SPAN",{});var Za=m(Pe);ka=j(Za,"2. Quantas dimens\xF5es tem o tensor do Transformer de base, e quais s\xE3o elas?"),Za.forEach(t),ca.forEach(t),Ye=q(e),P(ie.$$.fragment,e),Re=q(e),Y=u(e,"H3",{class:!0});var ua=m(Y);X=u(ua,"A",{id:!0,class:!0,href:!0});var Ka=m(X);Te=u(Ka,"SPAN",{});var Xa=m(Te);P(le.$$.fragment,Xa),Xa.forEach(t),Ka.forEach(t),ba=q(ua),Oe=u(ua,"SPAN",{});var et=m(Oe);xa=j(et,"3. Qual dos seguintes \xE9 um exemplo de Tokeniza\xE7\xE3o por sub-palavras?"),et.forEach(t),ua.forEach(t),Ge=q(e),P(ce.$$.fragment,e),Je=q(e),R=u(e,"H3",{class:!0});var da=m(R);ee=u(da,"A",{id:!0,class:!0,href:!0});var at=m(ee);Ie=u(at,"SPAN",{});var tt=m(Ie);P(ue.$$.fragment,tt),tt.forEach(t),at.forEach(t),wa=q(da),de=u(da,"SPAN",{});var ma=m(de);qa=j(ma,"4. O que \xE9 uma "),je=u(ma,"EM",{});var ot=m(je);za=j(ot,"model head"),ot.forEach(t),Ea=j(ma,"?"),ma.forEach(t),da.forEach(t),We=q(e),P(me.$$.fragment,e),Ze=q(e),F.l(e),Ee=q(e),G=u(e,"H3",{class:!0});var pa=m(G);ae=u(pa,"A",{id:!0,class:!0,href:!0});var rt=m(ae);Me=u(rt,"SPAN",{});var st=m(Me);P(pe.$$.fragment,st),st.forEach(t),rt.forEach(t),Aa=q(pa),Ce=u(pa,"SPAN",{});var nt=m(Ce);ya=j(nt,"6. Quais s\xE3o as t\xE9cnicas a serem observadas quando realizar batches com sequ\xEAncias de diferentes tamanhos?"),nt.forEach(t),pa.forEach(t),Ke=q(e),P(fe.$$.fragment,e),Xe=q(e),J=u(e,"H3",{class:!0});var fa=m(J);te=u(fa,"A",{id:!0,class:!0,href:!0});var it=m(te);He=u(it,"SPAN",{});var lt=m(He);P(he.$$.fragment,lt),lt.forEach(t),it.forEach(t),Na=q(fa),De=u(fa,"SPAN",{});var ct=m(De);Sa=j(ct,"7. Qual \xE9 o objetivo de aplicar uma fun\xE7\xE3o SoftMax \xE0 sa\xEDda de logits para um modelo de classifica\xE7\xE3o sequencial??"),ct.forEach(t),fa.forEach(t),ea=q(e),P(ve.$$.fragment,e),aa=q(e),W=u(e,"H3",{class:!0});var ha=m(W);oe=u(ha,"A",{id:!0,class:!0,href:!0});var ut=m(oe);Qe=u(ut,"SPAN",{});var dt=m(Qe);P($e.$$.fragment,dt),dt.forEach(t),ut.forEach(t),Pa=q(ha),Ue=u(ha,"SPAN",{});var mt=m(Ue);Ta=j(mt,"8. Qual \xE9 o m\xE9todo core da API tokenizer?"),mt.forEach(t),ha.forEach(t),ta=q(e),P(_e.$$.fragment,e),oa=q(e),Z=u(e,"H3",{class:!0});var va=m(Z);re=u(va,"A",{id:!0,class:!0,href:!0});var pt=m(re);Fe=u(pt,"SPAN",{});var ft=m(Fe);P(ge.$$.fragment,ft),ft.forEach(t),pt.forEach(t),Oa=q(va),ke=u(va,"SPAN",{});var $a=m(ke);Ia=j($a,"9. O que a vari\xE1vel "),Le=u($a,"CODE",{});var ht=m(Le);ja=j(ht,"result"),ht.forEach(t),Ma=j($a," cont\xE9m nesta peda\xE7o de c\xF3digo?"),$a.forEach(t),va.forEach(t),ra=q(e),P(be.$$.fragment,e),sa=q(e),P(xe.$$.fragment,e),na=q(e),V.l(e),Ae=_a(),this.h()},h(){d(a,"name","hf:doc:metadata"),d(a,"content",JSON.stringify(oo)),d(f,"id","questionrio-de-fim-de-captulo"),d(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(f,"href","#questionrio-de-fim-de-captulo"),d(h,"class","relative group"),d(s,"id","1.-qual-\xE9-a-ordem-do-pipeline-para-a-modelagem-de-linguagem?"),d(s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(s,"href","#1.-qual-\xE9-a-ordem-do-pipeline-para-a-modelagem-de-linguagem?"),d(r,"class","relative group"),d(K,"id","2.-quantas-dimens\xF5es-tem-o-tensor-do-transformer-de-base,-e-quais-s\xE3o-elas?"),d(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K,"href","#2.-quantas-dimens\xF5es-tem-o-tensor-do-transformer-de-base,-e-quais-s\xE3o-elas?"),d(B,"class","relative group"),d(X,"id","3.-qual-dos-seguintes-\xE9-um-exemplo-de-tokeniza\xE7\xE3o-por-sub-palavras?"),d(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(X,"href","#3.-qual-dos-seguintes-\xE9-um-exemplo-de-tokeniza\xE7\xE3o-por-sub-palavras?"),d(Y,"class","relative group"),d(ee,"id","4.-o-que-\xE9-uma-<em>model-head</em>?"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#4.-o-que-\xE9-uma-<em>model-head</em>?"),d(R,"class","relative group"),d(ae,"id","6.-quais-s\xE3o-as-t\xE9cnicas-a-serem-observadas-quando-realizar-batches-com-sequ\xEAncias-de-diferentes-tamanhos?"),d(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ae,"href","#6.-quais-s\xE3o-as-t\xE9cnicas-a-serem-observadas-quando-realizar-batches-com-sequ\xEAncias-de-diferentes-tamanhos?"),d(G,"class","relative group"),d(te,"id","7.-qual-\xE9-o-objetivo-de-aplicar-uma-fun\xE7\xE3o-softmax-\xE0-sa\xEDda-de-logits-para-um-modelo-de-classifica\xE7\xE3o-sequencial??"),d(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(te,"href","#7.-qual-\xE9-o-objetivo-de-aplicar-uma-fun\xE7\xE3o-softmax-\xE0-sa\xEDda-de-logits-para-um-modelo-de-classifica\xE7\xE3o-sequencial??"),d(J,"class","relative group"),d(oe,"id","8.-qual-\xE9-o-m\xE9todo-core-da-api-tokenizer?"),d(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(oe,"href","#8.-qual-\xE9-o-m\xE9todo-core-da-api-tokenizer?"),d(W,"class","relative group"),d(re,"id","9.-o-que-a-vari\xE1vel-<code>result</code>-cont\xE9m-nesta-peda\xE7o-de-c\xF3digo?"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#9.-o-que-a-vari\xE1vel-<code>result</code>-cont\xE9m-nesta-peda\xE7o-de-c\xF3digo?"),d(Z,"class","relative group")},m(e,_){n(document.head,a),g(e,o,_),T(l,e,_),g(e,i,_),g(e,h,_),n(h,f),n(f,A),T(k,A,null),n(h,$),n(h,z),n(z,x),g(e,p,_),g(e,r,_),n(r,s),n(s,b),T(E,b,null),n(r,M),n(r,C),n(C,H),g(e,Ve,_),T(se,e,_),g(e,Be,_),g(e,B,_),n(B,K),n(K,Se),T(ne,Se,null),n(B,ga),n(B,Pe),n(Pe,ka),g(e,Ye,_),T(ie,e,_),g(e,Re,_),g(e,Y,_),n(Y,X),n(X,Te),T(le,Te,null),n(Y,ba),n(Y,Oe),n(Oe,xa),g(e,Ge,_),T(ce,e,_),g(e,Je,_),g(e,R,_),n(R,ee),n(ee,Ie),T(ue,Ie,null),n(R,wa),n(R,de),n(de,qa),n(de,je),n(je,za),n(de,Ea),g(e,We,_),T(me,e,_),g(e,Ze,_),we[U].m(e,_),g(e,Ee,_),g(e,G,_),n(G,ae),n(ae,Me),T(pe,Me,null),n(G,Aa),n(G,Ce),n(Ce,ya),g(e,Ke,_),T(fe,e,_),g(e,Xe,_),g(e,J,_),n(J,te),n(te,He),T(he,He,null),n(J,Na),n(J,De),n(De,Sa),g(e,ea,_),T(ve,e,_),g(e,aa,_),g(e,W,_),n(W,oe),n(oe,Qe),T($e,Qe,null),n(W,Pa),n(W,Ue),n(Ue,Ta),g(e,ta,_),T(_e,e,_),g(e,oa,_),g(e,Z,_),n(Z,re),n(re,Fe),T(ge,Fe,null),n(Z,Oa),n(Z,ke),n(ke,Ia),n(ke,Le),n(Le,ja),n(ke,Ma),g(e,ra,_),T(be,e,_),g(e,sa,_),T(xe,e,_),g(e,na,_),qe[L].m(e,_),g(e,Ae,_),ia=!0},p(e,[_]){const ze={};_&1&&(ze.fw=e[0]),l.$set(ze);let ye=U;U=Fa(e),U!==ye&&(bt(),y(we[ye],1,1,()=>{we[ye]=null}),kt(),F=we[U],F||(F=we[U]=Ua[U](e),F.c()),N(F,1),F.m(Ee.parentNode,Ee));let Ne=L;L=Va(e),L!==Ne&&(bt(),y(qe[Ne],1,1,()=>{qe[Ne]=null}),kt(),V=qe[L],V||(V=qe[L]=La[L](e),V.c()),N(V,1),V.m(Ae.parentNode,Ae))},i(e){ia||(N(l.$$.fragment,e),N(k.$$.fragment,e),N(E.$$.fragment,e),N(se.$$.fragment,e),N(ne.$$.fragment,e),N(ie.$$.fragment,e),N(le.$$.fragment,e),N(ce.$$.fragment,e),N(ue.$$.fragment,e),N(me.$$.fragment,e),N(F),N(pe.$$.fragment,e),N(fe.$$.fragment,e),N(he.$$.fragment,e),N(ve.$$.fragment,e),N($e.$$.fragment,e),N(_e.$$.fragment,e),N(ge.$$.fragment,e),N(be.$$.fragment,e),N(xe.$$.fragment,e),N(V),ia=!0)},o(e){y(l.$$.fragment,e),y(k.$$.fragment,e),y(E.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(ie.$$.fragment,e),y(le.$$.fragment,e),y(ce.$$.fragment,e),y(ue.$$.fragment,e),y(me.$$.fragment,e),y(F),y(pe.$$.fragment,e),y(fe.$$.fragment,e),y(he.$$.fragment,e),y(ve.$$.fragment,e),y($e.$$.fragment,e),y(_e.$$.fragment,e),y(ge.$$.fragment,e),y(be.$$.fragment,e),y(xe.$$.fragment,e),y(V),ia=!1},d(e){t(a),e&&t(o),O(l,e),e&&t(i),e&&t(h),O(k),e&&t(p),e&&t(r),O(E),e&&t(Ve),O(se,e),e&&t(Be),e&&t(B),O(ne),e&&t(Ye),O(ie,e),e&&t(Re),e&&t(Y),O(le),e&&t(Ge),O(ce,e),e&&t(Je),e&&t(R),O(ue),e&&t(We),O(me,e),e&&t(Ze),we[U].d(e),e&&t(Ee),e&&t(G),O(pe),e&&t(Ke),O(fe,e),e&&t(Xe),e&&t(J),O(he),e&&t(ea),O(ve,e),e&&t(aa),e&&t(W),O($e),e&&t(ta),O(_e,e),e&&t(oa),e&&t(Z),O(ge),e&&t(ra),O(be,e),e&&t(sa),O(xe,e),e&&t(na),qe[L].d(e),e&&t(Ae)}}}const oo={local:"questionrio-de-fim-de-captulo",title:"Question\xE1rio de fim de cap\xEDtulo"};function ro(v,a,o){let l="pt";return Tt(()=>{const i=new URLSearchParams(window.location.search);o(0,l=i.get("fw")||"pt")}),[l]}class uo extends Et{constructor(a){super();At(this,a,ro,to,yt,{})}}export{uo as default,oo as metadata};
