import{S as ep,i as tp,s as sp,e as r,k as u,w as h,t as n,M as np,c as o,d as s,m as c,a as l,x as v,h as a,b as m,N as Xi,F as e,g as p,y as k,q as E,o as _,B as b,v as ap}from"../../chunks/vendor-1e8b365d.js";import{T as rp}from"../../chunks/Tip-62b14c6e.js";import{Y as Zi}from"../../chunks/Youtube-c2a8cc39.js";import{I as Fs}from"../../chunks/IconCopyLink-483c28ba.js";import{C as ne}from"../../chunks/CodeBlock-e5764662.js";import{D as op}from"../../chunks/DocNotebookDropdown-37d928d3.js";function lp(Gs){let d,me,x,M,ae,y,Se,re,oe,de,B,J,f,He,O,Be,Oe;return{c(){d=r("p"),me=n("\u270F\uFE0F "),x=r("strong"),M=n("Essayez !"),ae=n(" Chargez un "),y=r("em"),Se=n("tokenizer"),re=n(" depuis le "),oe=r("em"),de=n("checkpoint"),B=u(),J=r("code"),f=n("bert-base-cased"),He=n(" et passez-lui le m\xEAme exemple. Quelles sont les principales diff\xE9rences que vous pouvez voir entre les versions cas\xE9e et non cas\xE9e du "),O=r("em"),Be=n("tokenizer"),Oe=n(" ?")},l(U){d=o(U,"P",{});var g=l(d);me=a(g,"\u270F\uFE0F "),x=o(g,"STRONG",{});var ct=l(x);M=a(ct,"Essayez !"),ct.forEach(s),ae=a(g," Chargez un "),y=o(g,"EM",{});var fe=l(y);Se=a(fe,"tokenizer"),fe.forEach(s),re=a(g," depuis le "),oe=o(g,"EM",{});var mt=l(oe);de=a(mt,"checkpoint"),mt.forEach(s),B=c(g),J=o(g,"CODE",{});var dt=l(J);f=a(dt,"bert-base-cased"),dt.forEach(s),He=a(g," et passez-lui le m\xEAme exemple. Quelles sont les principales diff\xE9rences que vous pouvez voir entre les versions cas\xE9e et non cas\xE9e du "),O=o(g,"EM",{});var he=l(O);Be=a(he,"tokenizer"),he.forEach(s),Oe=a(g," ?"),g.forEach(s)},m(U,g){p(U,d,g),e(d,me),e(d,x),e(x,M),e(d,ae),e(d,y),e(y,Se),e(d,re),e(d,oe),e(oe,de),e(d,B),e(d,J),e(J,f),e(d,He),e(d,O),e(O,Be),e(d,Oe)},d(U){U&&s(d)}}}function ip(Gs){let d,me,x,M,ae,y,Se,re,oe,de,B,J,f,He,O,Be,Oe,U,g,ct,fe,mt,dt,he,Ln,Fn,Nt,Gn,Wn,Ws,le,Ue,cl,Vn,Re,ml,Vs,j,Jn,St,Yn,Kn,Ht,Qn,Xn,Bt,Zn,ea,Ot,ta,sa,Js,ie,ve,Ut,Ie,na,Rt,aa,Ys,Le,Ks,Y,ra,Fe,oa,la,It,ia,pa,Qs,$,ua,Lt,ca,ma,Ft,da,fa,Gt,ha,va,Wt,ka,Ea,Vt,_a,ba,Xs,Ge,Zs,We,en,D,ga,Jt,$a,za,Yt,qa,xa,Kt,ja,Pa,tn,Ve,sn,Je,nn,ke,wa,Qt,Ta,ya,an,Ee,rn,pe,_e,Xt,Ye,Ma,Zt,Da,on,Ke,ln,z,Ca,es,Aa,Na,ft,Sa,Ha,ts,Ba,Oa,ss,Ua,Ra,ns,Ia,La,pn,P,Fa,as,Ga,Wa,rs,Va,Ja,os,Ya,Ka,ls,Qa,Xa,un,Qe,cn,Xe,mn,w,Za,is,er,tr,ps,sr,nr,us,ar,rr,cs,or,lr,dn,C,ir,ms,pr,ur,ds,cr,mr,fs,dr,fr,fn,Ze,hn,K,hr,hs,vr,kr,vs,Er,_r,vn,et,kn,Q,br,ks,gr,$r,Es,zr,qr,En,be,xr,_s,jr,Pr,_n,tt,bn,st,gn,q,wr,bs,Tr,yr,gs,Mr,Dr,$s,Cr,Ar,zs,Nr,Sr,qs,Hr,Br,$n,ge,Or,xs,Ur,Rr,zn,ue,$e,js,nt,Ir,Ps,Lr,qn,R,at,Fr,Gr,ws,Wr,Vr,ht,Jr,Yr,xn,T,Kr,Ts,Qr,Xr,ys,Zr,eo,Ms,to,so,Ds,no,ao,jn,ce,ze,Cs,rt,ro,As,oo,Pn,vt,lo,wn,qe,Ns,I,kt,io,po,Et,uo,co,_t,mo,fo,bt,ho,vo,L,F,gt,ko,Eo,ot,_o,Ss,bo,go,lt,$o,Hs,zo,qo,xe,xo,Bs,jo,Po,wo,G,$t,To,yo,je,Mo,Os,Do,Co,Ao,X,No,Us,So,Ho,Rs,Bo,Oo,Uo,Pe,Ro,Is,Io,Lo,Fo,W,zt,Go,Wo,qt,Vo,Jo,xt,Yo,Ko,it,Qo,Ls,Xo,Zo,V,jt,el,tl,Pt,sl,nl,wt,al,rl,Tt,ol,Tn,yt,ll,yn;return y=new Fs({}),B=new op({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"}]}}),Ie=new Fs({}),Le=new Zi({props:{id:"4IIC2jI9CaU"}}),Ge=new ne({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(tokenizer.backend_tokenizer))`}}),We=new ne({props:{code:"<class 'tokenizers.Tokenizer'>",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;tokenizers.Tokenizer&#x27;</span>&gt;'}}),Ve=new ne({props:{code:'print(tokenizer.backend_tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),Je=new ne({props:{code:"'hello how are u?'",highlighted:'<span class="hljs-string">&#x27;hello how are u?&#x27;</span>'}}),Ee=new rp({props:{$$slots:{default:[lp]},$$scope:{ctx:Gs}}}),Ye=new Fs({}),Ke=new Zi({props:{id:"grlLV8AIXug"}}),Qe=new ne({props:{code:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")',highlighted:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)'}}),Xe=new ne({props:{code:"[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]",highlighted:'[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;you&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">19</span>)), (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]'}}),Ze=new ne({props:{code:`tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),et=new ne({props:{code:`[('Hello', (0, 5)), (',', (5, 6)), ('\u0120how', (6, 10)), ('\u0120are', (10, 14)), ('\u0120', (14, 15)), ('\u0120you', (15, 19)),
 ('?', (19, 20))]`,highlighted:`[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u0120how&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120are&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u0120&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)), (<span class="hljs-string">&#x27;\u0120you&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">19</span>)),
 (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]`}}),tt=new ne({props:{code:`tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),st=new ne({props:{code:"[('\u2581Hello,', (0, 6)), ('\u2581how', (7, 10)), ('\u2581are', (11, 14)), ('\u2581you?', (16, 20))]",highlighted:'[(<span class="hljs-string">&#x27;\u2581Hello,&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u2581how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581you?&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">20</span>))]'}}),nt=new Fs({}),rt=new Fs({}),{c(){d=r("meta"),me=u(),x=r("h1"),M=r("a"),ae=r("span"),h(y.$$.fragment),Se=u(),re=r("span"),oe=n("Normalisation et pr\xE9-tokenization"),de=u(),h(B.$$.fragment),J=u(),f=r("p"),He=n("Avant de nous plonger plus profond\xE9ment dans les trois algorithmes de tok\xE9nisation en sous-mots les plus courants utilis\xE9s avec les "),O=r("em"),Be=n("transformers"),Oe=n(" ("),U=r("em"),g=n("Byte-Pair Encoding"),ct=n(" (BPE), "),fe=r("em"),mt=n("WordPiece"),dt=n(" et "),he=r("em"),Ln=n("Unigram"),Fn=n("), nous allons d\u2019abord examiner le pr\xE9traitement que chaque "),Nt=r("em"),Gn=n("tokenizer"),Wn=n(" applique au texte. Voici un aper\xE7u de haut niveau des \xE9tapes du pipeline de tokenisation :"),Ws=u(),le=r("div"),Ue=r("img"),Vn=u(),Re=r("img"),Vs=u(),j=r("p"),Jn=n("Avant de diviser un texte en sous-"),St=r("em"),Yn=n("tokens"),Kn=n(" (selon son mod\xE8le), le "),Ht=r("em"),Qn=n("tokenizer"),Xn=n(" effectue deux \xE9tapes : "),Bt=r("em"),Zn=n("normalisation"),ea=n(" et "),Ot=r("em"),ta=n("pr\xE9-tok\xE9nisation"),sa=n("."),Js=u(),ie=r("h2"),ve=r("a"),Ut=r("span"),h(Ie.$$.fragment),na=u(),Rt=r("span"),aa=n("Normalisation"),Ys=u(),h(Le.$$.fragment),Ks=u(),Y=r("p"),ra=n("L\u2019\xE9tape de normalisation implique un nettoyage g\xE9n\xE9ral, comme la suppression des espaces blancs inutiles, la mise en minuscules et/ou la suppression des accents. Si vous \xEAtes familier avec la "),Fe=r("a"),oa=n("normalisation Unicode"),la=n(" (comme NFC ou NFKC), c\u2019est aussi quelque chose que le "),It=r("em"),ia=n("tokenizer"),pa=n(" peut appliquer."),Qs=u(),$=r("p"),ua=n("Le "),Lt=r("code"),ca=n("tokenizer"),ma=n(" de \u{1F917} "),Ft=r("em"),da=n("Transformers"),fa=n(" poss\xE8de un attribut appel\xE9 "),Gt=r("code"),ha=n("backend_tokenizer"),va=n(" qui donne acc\xE8s au "),Wt=r("em"),ka=n("tokenizer"),Ea=n(" sous-jacent de la biblioth\xE8que \u{1F917} "),Vt=r("em"),_a=n("Tokenizers"),ba=n(" :"),Xs=u(),h(Ge.$$.fragment),Zs=u(),h(We.$$.fragment),en=u(),D=r("p"),ga=n("L\u2019attribut "),Jt=r("code"),$a=n("normalizer"),za=n(" de l\u2019objet "),Yt=r("code"),qa=n("tokenizer"),xa=n(" poss\xE8de une m\xE9thode "),Kt=r("code"),ja=n("normalize_str()"),Pa=n(" que nous pouvons utiliser pour voir comment la normalisation est effectu\xE9e :"),tn=u(),h(Ve.$$.fragment),sn=u(),h(Je.$$.fragment),nn=u(),ke=r("p"),wa=n("Dans cet exemple, puisque nous avons choisi le point de contr\xF4le "),Qt=r("code"),Ta=n("bert-base-uncased"),ya=n(", la normalisation a appliqu\xE9 la minuscule et supprim\xE9 les accents."),an=u(),h(Ee.$$.fragment),rn=u(),pe=r("h2"),_e=r("a"),Xt=r("span"),h(Ye.$$.fragment),Ma=u(),Zt=r("span"),Da=n("Pr\xE9-tokenization"),on=u(),h(Ke.$$.fragment),ln=u(),z=r("p"),Ca=n("Comme nous le verrons dans les sections suivantes, un "),es=r("em"),Aa=n("tokenizer"),Na=n(" ne peut pas \xEAtre entra\xEEn\xE9 uniquement sur du texte brut. Au lieu de cela, nous devons d\u2019abord diviser les textes en petites entit\xE9s, comme des mots. C\u2019est l\xE0 qu\u2019intervient l\u2019\xE9tape de pr\xE9-tok\xE9nisation. Comme nous l\u2019avons vu dans le "),ft=r("a"),Sa=n("Chapitre 2"),Ha=n(", un "),ts=r("em"),Ba=n("tokenizer"),Oa=n(" bas\xE9 sur les mots peut simplement diviser un texte brut en mots sur les espaces et la ponctuation. Ces mots constitueront les limites des sous-"),ss=r("em"),Ua=n("tokens"),Ra=n(" que le "),ns=r("em"),Ia=n("tokenizer"),La=n(" peut apprendre pendant son entra\xEEnement."),pn=u(),P=r("p"),Fa=n("Pour voir comment un "),as=r("em"),Ga=n("tokenizer"),Wa=n(" rapide effectue la pr\xE9-tok\xE9nisation, nous pouvons utiliser la m\xE9thode "),rs=r("code"),Va=n("pre_tokenize_str()"),Ja=n("de l\u2019attribut "),os=r("code"),Ya=n("pre_tokenizer"),Ka=n(" de l\u2019objet "),ls=r("code"),Qa=n("tokenizer"),Xa=n(" :"),un=u(),h(Qe.$$.fragment),cn=u(),h(Xe.$$.fragment),mn=u(),w=r("p"),Za=n("Remarquez que le "),is=r("em"),er=n("tokenizer"),tr=n(" garde d\xE9j\xE0 la trace des d\xE9calages, ce qui lui permet de nous donner la correspondance des d\xE9calages que nous avons utilis\xE9e dans la section pr\xE9c\xE9dente. Ici, le "),ps=r("em"),sr=n("tokenizer"),nr=n(" ignore les deux espaces et les remplace par un seul, mais le d\xE9calage saute entre "),us=r("code"),ar=n("are"),rr=n(" et "),cs=r("code"),or=n("you"),lr=n(" pour en tenir compte."),dn=u(),C=r("p"),ir=n("Puisque nous utilisons un "),ms=r("em"),pr=n("tokenizer"),ur=n(" de BERT, la pr\xE9-tok\xE9nisation implique la s\xE9paration des espaces et de la ponctuation. D\u2019autres "),ds=r("em"),cr=n("tokenizers"),mr=n(" peuvent avoir des r\xE8gles diff\xE9rentes pour cette \xE9tape. Par exemple, si nous utilisons le "),fs=r("em"),dr=n("tokenizer"),fr=n(" GPT-2 :"),fn=u(),h(Ze.$$.fragment),hn=u(),K=r("p"),hr=n("cela s\xE9parera aussi sur les espaces et la ponctuation, mais il gardera les espaces et les remplacera par un symbole "),hs=r("code"),vr=n("\u0120"),kr=n(", ce qui lui permettra de r\xE9cup\xE9rer les espaces originaux si nous d\xE9codons les "),vs=r("em"),Er=n("tokens"),_r=n(" :"),vn=u(),h(et.$$.fragment),kn=u(),Q=r("p"),br=n("Notez \xE9galement que, contrairement au "),ks=r("em"),gr=n("tokenizer"),$r=n(" de BERT, ce "),Es=r("em"),zr=n("tokenizer"),qr=n(" n\u2019ignore pas les doubles espaces."),En=u(),be=r("p"),xr=n("Pour un dernier exemple, regardons le "),_s=r("em"),jr=n("tokenizer"),Pr=n(" T5, qui est bas\xE9 sur l\u2019algorithme SentencePiece :"),_n=u(),h(tt.$$.fragment),bn=u(),h(st.$$.fragment),gn=u(),q=r("p"),wr=n("Comme le "),bs=r("em"),Tr=n("tokenizer"),yr=n(" du GPT-2, celui-ci garde les espaces et les remplace par un token sp\xE9cifique ("),gs=r("code"),Mr=n("_"),Dr=n("), mais le tokenizer T5 ne s\xE9pare que sur les espaces, pas sur la ponctuation. Notez \xE9galement qu\u2019il a ajout\xE9 un espace par d\xE9faut au d\xE9but de la phrase (avant "),$s=r("code"),Cr=n("Hello"),Ar=n(") et a ignor\xE9 le double espace entre "),zs=r("code"),Nr=n("are"),Sr=n(" et "),qs=r("code"),Hr=n("you"),Br=n("."),$n=u(),ge=r("p"),Or=n("Maintenant que nous avons vu un peu comment les diff\xE9rents "),xs=r("em"),Ur=n("tokenizers"),Rr=n(" traitent le texte, nous pouvons commencer \xE0 explorer les algorithmes sous-jacents eux-m\xEAmes. Nous commencerons par jeter un coup d\u2019oeil rapide sur le tr\xE8s r\xE9pandu SentencePiece ; puis, au cours des trois sections suivantes, nous examinerons le fonctionnement des trois principaux algorithmes utilis\xE9s pour la tokenisation en sous-mots."),zn=u(),ue=r("h2"),$e=r("a"),js=r("span"),h(nt.$$.fragment),Ir=u(),Ps=r("span"),Lr=n("SentencePiece"),qn=u(),R=r("p"),at=r("a"),Fr=n("SentencePiece"),Gr=n(" est un algorithme de tokenization pour le pr\xE9traitement du texte que vous pouvez utiliser avec n\u2019importe lequel des mod\xE8les que nous verrons dans les trois prochaines sections. Il consid\xE8re le texte comme une s\xE9quence de caract\xE8res Unicode, et remplace les espaces par un caract\xE8re sp\xE9cial, "),ws=r("code"),Wr=n("\u2581"),Vr=n(". Utilis\xE9 en conjonction avec l\u2019algorithme Unigram (voir "),ht=r("a"),Jr=n("section 7"),Yr=n("), il ne n\xE9cessite m\xEAme pas d\u2019\xE9tape de pr\xE9-tok\xE9nisation, ce qui est tr\xE8s utile pour les langues o\xF9 le caract\xE8re espace n\u2019est pas utilis\xE9 (comme le chinois ou le japonais)."),xn=u(),T=r("p"),Kr=n("L\u2019autre caract\xE9ristique principale de SentencePiece est le "),Ts=r("em"),Qr=n("tokenizer"),Xr=n(" r\xE9versible : comme il n\u2019y a pas de traitement sp\xE9cial des espaces, le d\xE9codage des "),ys=r("em"),Zr=n("tokens"),eo=n(" se fait simplement en les concat\xE9nant et en rempla\xE7ant les "),Ms=r("code"),to=n("_"),so=n(" par des espaces, ce qui donne le texte normalis\xE9. Comme nous l\u2019avons vu pr\xE9c\xE9demment, le "),Ds=r("em"),no=n("tokenizer"),ao=n(" de BERT supprime les espaces r\xE9p\xE9titifs, donc sa tokenisation n\u2019est pas r\xE9versible."),jn=u(),ce=r("h2"),ze=r("a"),Cs=r("span"),h(rt.$$.fragment),ro=u(),As=r("span"),oo=n("Aper\xE7u de l'algorithme"),Pn=u(),vt=r("p"),lo=n("Dans les sections suivantes, nous allons nous plonger dans les trois principaux algorithmes de tokenisation des sous-mots : BPE (utilis\xE9 par GPT-2 et autres), WordPiece (utilis\xE9 par exemple par BERT), et Unigram (utilis\xE9 par T5 et autres). Avant de commencer, voici un rapide aper\xE7u du fonctionnement de chacun d\u2019entre eux. N\u2019h\xE9sitez pas \xE0 revenir \xE0 ce tableau apr\xE8s avoir lu chacune des sections suivantes si cela n\u2019a pas encore de sens pour vous."),wn=u(),qe=r("table"),Ns=r("thead"),I=r("tr"),kt=r("th"),io=n("Mod\xE8le"),po=u(),Et=r("th"),uo=n("BPE"),co=u(),_t=r("th"),mo=n("WordPiece"),fo=u(),bt=r("th"),ho=n("Unigramme"),vo=u(),L=r("tbody"),F=r("tr"),gt=r("td"),ko=n("Entra\xEEnement"),Eo=u(),ot=r("td"),_o=n("Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Ss=r("em"),bo=n("tokens"),go=u(),lt=r("td"),$o=n("Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Hs=r("em"),zo=n("tokens"),qo=u(),xe=r("td"),xo=n("Part d\u2019un grand vocabulaire et apprend des r\xE8gles pour supprimer les "),Bs=r("em"),jo=n("tokens"),Po=n("."),wo=u(),G=r("tr"),$t=r("td"),To=n("\xC9tape d\u2019Entra\xEEnement"),yo=u(),je=r("td"),Mo=n("Fusionne les "),Os=r("em"),Do=n("tokens"),Co=n(" correspondant \xE0 la paire la plus commune"),Ao=u(),X=r("td"),No=n("Fusionne les "),Us=r("em"),So=n("tokens"),Ho=n(" correspondant \xE0 la paire ayant le meilleur score bas\xE9 sur la fr\xE9quence de la paire, en privil\xE9giant les paires o\xF9 chaque "),Rs=r("em"),Bo=n("token"),Oo=n(" individuel est moins fr\xE9quent"),Uo=u(),Pe=r("td"),Ro=n("Supprime tous les "),Is=r("em"),Io=n("tokens"),Lo=n(" du vocabulaire qui minimiseront la perte calcul\xE9e sur le corpus entier"),Fo=u(),W=r("tr"),zt=r("td"),Go=n("Apprendre"),Wo=u(),qt=r("td"),Vo=n("Fusionner des r\xE8gles et un vocabulaire"),Jo=u(),xt=r("td"),Yo=n("Juste un vocabulaire"),Ko=u(),it=r("td"),Qo=n("Un vocabulaire avec un score pour chaque "),Ls=r("em"),Xo=n("token"),Zo=u(),V=r("tr"),jt=r("td"),el=n("Encodage"),tl=u(),Pt=r("td"),sl=n("D\xE9coupe un mot en caract\xE8res et applique les fusions apprises pendant l\u2019entra\xEEnement"),nl=u(),wt=r("td"),al=n("Trouve le plus long sous-mot depuis le d\xE9but qui est dans le vocabulaire, puis fait de m\xEAme pour le reste du mot"),rl=u(),Tt=r("td"),ol=n("Trouve la division la plus probable en tokens, en utilisant les scores appris pendant l\u2019entra\xEEnement"),Tn=u(),yt=r("p"),ll=n("Maintenant, plongeons dans BPE !"),this.h()},l(t){const i=np('[data-svelte="svelte-1phssyn"]',document.head);d=o(i,"META",{name:!0,content:!0}),i.forEach(s),me=c(t),x=o(t,"H1",{class:!0});var pt=l(x);M=o(pt,"A",{id:!0,class:!0,href:!0});var dl=l(M);ae=o(dl,"SPAN",{});var fl=l(ae);v(y.$$.fragment,fl),fl.forEach(s),dl.forEach(s),Se=c(pt),re=o(pt,"SPAN",{});var hl=l(re);oe=a(hl,"Normalisation et pr\xE9-tokenization"),hl.forEach(s),pt.forEach(s),de=c(t),v(B.$$.fragment,t),J=c(t),f=o(t,"P",{});var A=l(f);He=a(A,"Avant de nous plonger plus profond\xE9ment dans les trois algorithmes de tok\xE9nisation en sous-mots les plus courants utilis\xE9s avec les "),O=o(A,"EM",{});var vl=l(O);Be=a(vl,"transformers"),vl.forEach(s),Oe=a(A," ("),U=o(A,"EM",{});var kl=l(U);g=a(kl,"Byte-Pair Encoding"),kl.forEach(s),ct=a(A," (BPE), "),fe=o(A,"EM",{});var El=l(fe);mt=a(El,"WordPiece"),El.forEach(s),dt=a(A," et "),he=o(A,"EM",{});var _l=l(he);Ln=a(_l,"Unigram"),_l.forEach(s),Fn=a(A,"), nous allons d\u2019abord examiner le pr\xE9traitement que chaque "),Nt=o(A,"EM",{});var bl=l(Nt);Gn=a(bl,"tokenizer"),bl.forEach(s),Wn=a(A," applique au texte. Voici un aper\xE7u de haut niveau des \xE9tapes du pipeline de tokenisation :"),A.forEach(s),Ws=c(t),le=o(t,"DIV",{class:!0});var Mn=l(le);Ue=o(Mn,"IMG",{class:!0,src:!0,alt:!0}),Vn=c(Mn),Re=o(Mn,"IMG",{class:!0,src:!0,alt:!0}),Mn.forEach(s),Vs=c(t),j=o(t,"P",{});var Z=l(j);Jn=a(Z,"Avant de diviser un texte en sous-"),St=o(Z,"EM",{});var gl=l(St);Yn=a(gl,"tokens"),gl.forEach(s),Kn=a(Z," (selon son mod\xE8le), le "),Ht=o(Z,"EM",{});var $l=l(Ht);Qn=a($l,"tokenizer"),$l.forEach(s),Xn=a(Z," effectue deux \xE9tapes : "),Bt=o(Z,"EM",{});var zl=l(Bt);Zn=a(zl,"normalisation"),zl.forEach(s),ea=a(Z," et "),Ot=o(Z,"EM",{});var ql=l(Ot);ta=a(ql,"pr\xE9-tok\xE9nisation"),ql.forEach(s),sa=a(Z,"."),Z.forEach(s),Js=c(t),ie=o(t,"H2",{class:!0});var Dn=l(ie);ve=o(Dn,"A",{id:!0,class:!0,href:!0});var xl=l(ve);Ut=o(xl,"SPAN",{});var jl=l(Ut);v(Ie.$$.fragment,jl),jl.forEach(s),xl.forEach(s),na=c(Dn),Rt=o(Dn,"SPAN",{});var Pl=l(Rt);aa=a(Pl,"Normalisation"),Pl.forEach(s),Dn.forEach(s),Ys=c(t),v(Le.$$.fragment,t),Ks=c(t),Y=o(t,"P",{});var Mt=l(Y);ra=a(Mt,"L\u2019\xE9tape de normalisation implique un nettoyage g\xE9n\xE9ral, comme la suppression des espaces blancs inutiles, la mise en minuscules et/ou la suppression des accents. Si vous \xEAtes familier avec la "),Fe=o(Mt,"A",{href:!0,rel:!0});var wl=l(Fe);oa=a(wl,"normalisation Unicode"),wl.forEach(s),la=a(Mt," (comme NFC ou NFKC), c\u2019est aussi quelque chose que le "),It=o(Mt,"EM",{});var Tl=l(It);ia=a(Tl,"tokenizer"),Tl.forEach(s),pa=a(Mt," peut appliquer."),Mt.forEach(s),Qs=c(t),$=o(t,"P",{});var N=l($);ua=a(N,"Le "),Lt=o(N,"CODE",{});var yl=l(Lt);ca=a(yl,"tokenizer"),yl.forEach(s),ma=a(N," de \u{1F917} "),Ft=o(N,"EM",{});var Ml=l(Ft);da=a(Ml,"Transformers"),Ml.forEach(s),fa=a(N," poss\xE8de un attribut appel\xE9 "),Gt=o(N,"CODE",{});var Dl=l(Gt);ha=a(Dl,"backend_tokenizer"),Dl.forEach(s),va=a(N," qui donne acc\xE8s au "),Wt=o(N,"EM",{});var Cl=l(Wt);ka=a(Cl,"tokenizer"),Cl.forEach(s),Ea=a(N," sous-jacent de la biblioth\xE8que \u{1F917} "),Vt=o(N,"EM",{});var Al=l(Vt);_a=a(Al,"Tokenizers"),Al.forEach(s),ba=a(N," :"),N.forEach(s),Xs=c(t),v(Ge.$$.fragment,t),Zs=c(t),v(We.$$.fragment,t),en=c(t),D=o(t,"P",{});var we=l(D);ga=a(we,"L\u2019attribut "),Jt=o(we,"CODE",{});var Nl=l(Jt);$a=a(Nl,"normalizer"),Nl.forEach(s),za=a(we," de l\u2019objet "),Yt=o(we,"CODE",{});var Sl=l(Yt);qa=a(Sl,"tokenizer"),Sl.forEach(s),xa=a(we," poss\xE8de une m\xE9thode "),Kt=o(we,"CODE",{});var Hl=l(Kt);ja=a(Hl,"normalize_str()"),Hl.forEach(s),Pa=a(we," que nous pouvons utiliser pour voir comment la normalisation est effectu\xE9e :"),we.forEach(s),tn=c(t),v(Ve.$$.fragment,t),sn=c(t),v(Je.$$.fragment,t),nn=c(t),ke=o(t,"P",{});var Cn=l(ke);wa=a(Cn,"Dans cet exemple, puisque nous avons choisi le point de contr\xF4le "),Qt=o(Cn,"CODE",{});var Bl=l(Qt);Ta=a(Bl,"bert-base-uncased"),Bl.forEach(s),ya=a(Cn,", la normalisation a appliqu\xE9 la minuscule et supprim\xE9 les accents."),Cn.forEach(s),an=c(t),v(Ee.$$.fragment,t),rn=c(t),pe=o(t,"H2",{class:!0});var An=l(pe);_e=o(An,"A",{id:!0,class:!0,href:!0});var Ol=l(_e);Xt=o(Ol,"SPAN",{});var Ul=l(Xt);v(Ye.$$.fragment,Ul),Ul.forEach(s),Ol.forEach(s),Ma=c(An),Zt=o(An,"SPAN",{});var Rl=l(Zt);Da=a(Rl,"Pr\xE9-tokenization"),Rl.forEach(s),An.forEach(s),on=c(t),v(Ke.$$.fragment,t),ln=c(t),z=o(t,"P",{});var S=l(z);Ca=a(S,"Comme nous le verrons dans les sections suivantes, un "),es=o(S,"EM",{});var Il=l(es);Aa=a(Il,"tokenizer"),Il.forEach(s),Na=a(S," ne peut pas \xEAtre entra\xEEn\xE9 uniquement sur du texte brut. Au lieu de cela, nous devons d\u2019abord diviser les textes en petites entit\xE9s, comme des mots. C\u2019est l\xE0 qu\u2019intervient l\u2019\xE9tape de pr\xE9-tok\xE9nisation. Comme nous l\u2019avons vu dans le "),ft=o(S,"A",{href:!0});var Ll=l(ft);Sa=a(Ll,"Chapitre 2"),Ll.forEach(s),Ha=a(S,", un "),ts=o(S,"EM",{});var Fl=l(ts);Ba=a(Fl,"tokenizer"),Fl.forEach(s),Oa=a(S," bas\xE9 sur les mots peut simplement diviser un texte brut en mots sur les espaces et la ponctuation. Ces mots constitueront les limites des sous-"),ss=o(S,"EM",{});var Gl=l(ss);Ua=a(Gl,"tokens"),Gl.forEach(s),Ra=a(S," que le "),ns=o(S,"EM",{});var Wl=l(ns);Ia=a(Wl,"tokenizer"),Wl.forEach(s),La=a(S," peut apprendre pendant son entra\xEEnement."),S.forEach(s),pn=c(t),P=o(t,"P",{});var ee=l(P);Fa=a(ee,"Pour voir comment un "),as=o(ee,"EM",{});var Vl=l(as);Ga=a(Vl,"tokenizer"),Vl.forEach(s),Wa=a(ee," rapide effectue la pr\xE9-tok\xE9nisation, nous pouvons utiliser la m\xE9thode "),rs=o(ee,"CODE",{});var Jl=l(rs);Va=a(Jl,"pre_tokenize_str()"),Jl.forEach(s),Ja=a(ee,"de l\u2019attribut "),os=o(ee,"CODE",{});var Yl=l(os);Ya=a(Yl,"pre_tokenizer"),Yl.forEach(s),Ka=a(ee," de l\u2019objet "),ls=o(ee,"CODE",{});var Kl=l(ls);Qa=a(Kl,"tokenizer"),Kl.forEach(s),Xa=a(ee," :"),ee.forEach(s),un=c(t),v(Qe.$$.fragment,t),cn=c(t),v(Xe.$$.fragment,t),mn=c(t),w=o(t,"P",{});var te=l(w);Za=a(te,"Remarquez que le "),is=o(te,"EM",{});var Ql=l(is);er=a(Ql,"tokenizer"),Ql.forEach(s),tr=a(te," garde d\xE9j\xE0 la trace des d\xE9calages, ce qui lui permet de nous donner la correspondance des d\xE9calages que nous avons utilis\xE9e dans la section pr\xE9c\xE9dente. Ici, le "),ps=o(te,"EM",{});var Xl=l(ps);sr=a(Xl,"tokenizer"),Xl.forEach(s),nr=a(te," ignore les deux espaces et les remplace par un seul, mais le d\xE9calage saute entre "),us=o(te,"CODE",{});var Zl=l(us);ar=a(Zl,"are"),Zl.forEach(s),rr=a(te," et "),cs=o(te,"CODE",{});var ei=l(cs);or=a(ei,"you"),ei.forEach(s),lr=a(te," pour en tenir compte."),te.forEach(s),dn=c(t),C=o(t,"P",{});var Te=l(C);ir=a(Te,"Puisque nous utilisons un "),ms=o(Te,"EM",{});var ti=l(ms);pr=a(ti,"tokenizer"),ti.forEach(s),ur=a(Te," de BERT, la pr\xE9-tok\xE9nisation implique la s\xE9paration des espaces et de la ponctuation. D\u2019autres "),ds=o(Te,"EM",{});var si=l(ds);cr=a(si,"tokenizers"),si.forEach(s),mr=a(Te," peuvent avoir des r\xE8gles diff\xE9rentes pour cette \xE9tape. Par exemple, si nous utilisons le "),fs=o(Te,"EM",{});var ni=l(fs);dr=a(ni,"tokenizer"),ni.forEach(s),fr=a(Te," GPT-2 :"),Te.forEach(s),fn=c(t),v(Ze.$$.fragment,t),hn=c(t),K=o(t,"P",{});var Dt=l(K);hr=a(Dt,"cela s\xE9parera aussi sur les espaces et la ponctuation, mais il gardera les espaces et les remplacera par un symbole "),hs=o(Dt,"CODE",{});var ai=l(hs);vr=a(ai,"\u0120"),ai.forEach(s),kr=a(Dt,", ce qui lui permettra de r\xE9cup\xE9rer les espaces originaux si nous d\xE9codons les "),vs=o(Dt,"EM",{});var ri=l(vs);Er=a(ri,"tokens"),ri.forEach(s),_r=a(Dt," :"),Dt.forEach(s),vn=c(t),v(et.$$.fragment,t),kn=c(t),Q=o(t,"P",{});var Ct=l(Q);br=a(Ct,"Notez \xE9galement que, contrairement au "),ks=o(Ct,"EM",{});var oi=l(ks);gr=a(oi,"tokenizer"),oi.forEach(s),$r=a(Ct," de BERT, ce "),Es=o(Ct,"EM",{});var li=l(Es);zr=a(li,"tokenizer"),li.forEach(s),qr=a(Ct," n\u2019ignore pas les doubles espaces."),Ct.forEach(s),En=c(t),be=o(t,"P",{});var Nn=l(be);xr=a(Nn,"Pour un dernier exemple, regardons le "),_s=o(Nn,"EM",{});var ii=l(_s);jr=a(ii,"tokenizer"),ii.forEach(s),Pr=a(Nn," T5, qui est bas\xE9 sur l\u2019algorithme SentencePiece :"),Nn.forEach(s),_n=c(t),v(tt.$$.fragment,t),bn=c(t),v(st.$$.fragment,t),gn=c(t),q=o(t,"P",{});var H=l(q);wr=a(H,"Comme le "),bs=o(H,"EM",{});var pi=l(bs);Tr=a(pi,"tokenizer"),pi.forEach(s),yr=a(H," du GPT-2, celui-ci garde les espaces et les remplace par un token sp\xE9cifique ("),gs=o(H,"CODE",{});var ui=l(gs);Mr=a(ui,"_"),ui.forEach(s),Dr=a(H,"), mais le tokenizer T5 ne s\xE9pare que sur les espaces, pas sur la ponctuation. Notez \xE9galement qu\u2019il a ajout\xE9 un espace par d\xE9faut au d\xE9but de la phrase (avant "),$s=o(H,"CODE",{});var ci=l($s);Cr=a(ci,"Hello"),ci.forEach(s),Ar=a(H,") et a ignor\xE9 le double espace entre "),zs=o(H,"CODE",{});var mi=l(zs);Nr=a(mi,"are"),mi.forEach(s),Sr=a(H," et "),qs=o(H,"CODE",{});var di=l(qs);Hr=a(di,"you"),di.forEach(s),Br=a(H,"."),H.forEach(s),$n=c(t),ge=o(t,"P",{});var Sn=l(ge);Or=a(Sn,"Maintenant que nous avons vu un peu comment les diff\xE9rents "),xs=o(Sn,"EM",{});var fi=l(xs);Ur=a(fi,"tokenizers"),fi.forEach(s),Rr=a(Sn," traitent le texte, nous pouvons commencer \xE0 explorer les algorithmes sous-jacents eux-m\xEAmes. Nous commencerons par jeter un coup d\u2019oeil rapide sur le tr\xE8s r\xE9pandu SentencePiece ; puis, au cours des trois sections suivantes, nous examinerons le fonctionnement des trois principaux algorithmes utilis\xE9s pour la tokenisation en sous-mots."),Sn.forEach(s),zn=c(t),ue=o(t,"H2",{class:!0});var Hn=l(ue);$e=o(Hn,"A",{id:!0,class:!0,href:!0});var hi=l($e);js=o(hi,"SPAN",{});var vi=l(js);v(nt.$$.fragment,vi),vi.forEach(s),hi.forEach(s),Ir=c(Hn),Ps=o(Hn,"SPAN",{});var ki=l(Ps);Lr=a(ki,"SentencePiece"),ki.forEach(s),Hn.forEach(s),qn=c(t),R=o(t,"P",{});var ut=l(R);at=o(ut,"A",{href:!0,rel:!0});var Ei=l(at);Fr=a(Ei,"SentencePiece"),Ei.forEach(s),Gr=a(ut," est un algorithme de tokenization pour le pr\xE9traitement du texte que vous pouvez utiliser avec n\u2019importe lequel des mod\xE8les que nous verrons dans les trois prochaines sections. Il consid\xE8re le texte comme une s\xE9quence de caract\xE8res Unicode, et remplace les espaces par un caract\xE8re sp\xE9cial, "),ws=o(ut,"CODE",{});var _i=l(ws);Wr=a(_i,"\u2581"),_i.forEach(s),Vr=a(ut,". Utilis\xE9 en conjonction avec l\u2019algorithme Unigram (voir "),ht=o(ut,"A",{href:!0});var bi=l(ht);Jr=a(bi,"section 7"),bi.forEach(s),Yr=a(ut,"), il ne n\xE9cessite m\xEAme pas d\u2019\xE9tape de pr\xE9-tok\xE9nisation, ce qui est tr\xE8s utile pour les langues o\xF9 le caract\xE8re espace n\u2019est pas utilis\xE9 (comme le chinois ou le japonais)."),ut.forEach(s),xn=c(t),T=o(t,"P",{});var se=l(T);Kr=a(se,"L\u2019autre caract\xE9ristique principale de SentencePiece est le "),Ts=o(se,"EM",{});var gi=l(Ts);Qr=a(gi,"tokenizer"),gi.forEach(s),Xr=a(se," r\xE9versible : comme il n\u2019y a pas de traitement sp\xE9cial des espaces, le d\xE9codage des "),ys=o(se,"EM",{});var $i=l(ys);Zr=a($i,"tokens"),$i.forEach(s),eo=a(se," se fait simplement en les concat\xE9nant et en rempla\xE7ant les "),Ms=o(se,"CODE",{});var zi=l(Ms);to=a(zi,"_"),zi.forEach(s),so=a(se," par des espaces, ce qui donne le texte normalis\xE9. Comme nous l\u2019avons vu pr\xE9c\xE9demment, le "),Ds=o(se,"EM",{});var qi=l(Ds);no=a(qi,"tokenizer"),qi.forEach(s),ao=a(se," de BERT supprime les espaces r\xE9p\xE9titifs, donc sa tokenisation n\u2019est pas r\xE9versible."),se.forEach(s),jn=c(t),ce=o(t,"H2",{class:!0});var Bn=l(ce);ze=o(Bn,"A",{id:!0,class:!0,href:!0});var xi=l(ze);Cs=o(xi,"SPAN",{});var ji=l(Cs);v(rt.$$.fragment,ji),ji.forEach(s),xi.forEach(s),ro=c(Bn),As=o(Bn,"SPAN",{});var Pi=l(As);oo=a(Pi,"Aper\xE7u de l'algorithme"),Pi.forEach(s),Bn.forEach(s),Pn=c(t),vt=o(t,"P",{});var wi=l(vt);lo=a(wi,"Dans les sections suivantes, nous allons nous plonger dans les trois principaux algorithmes de tokenisation des sous-mots : BPE (utilis\xE9 par GPT-2 et autres), WordPiece (utilis\xE9 par exemple par BERT), et Unigram (utilis\xE9 par T5 et autres). Avant de commencer, voici un rapide aper\xE7u du fonctionnement de chacun d\u2019entre eux. N\u2019h\xE9sitez pas \xE0 revenir \xE0 ce tableau apr\xE8s avoir lu chacune des sections suivantes si cela n\u2019a pas encore de sens pour vous."),wi.forEach(s),wn=c(t),qe=o(t,"TABLE",{});var On=l(qe);Ns=o(On,"THEAD",{});var Ti=l(Ns);I=o(Ti,"TR",{});var ye=l(I);kt=o(ye,"TH",{align:!0});var yi=l(kt);io=a(yi,"Mod\xE8le"),yi.forEach(s),po=c(ye),Et=o(ye,"TH",{align:!0});var Mi=l(Et);uo=a(Mi,"BPE"),Mi.forEach(s),co=c(ye),_t=o(ye,"TH",{align:!0});var Di=l(_t);mo=a(Di,"WordPiece"),Di.forEach(s),fo=c(ye),bt=o(ye,"TH",{align:!0});var Ci=l(bt);ho=a(Ci,"Unigramme"),Ci.forEach(s),ye.forEach(s),Ti.forEach(s),vo=c(On),L=o(On,"TBODY",{});var Me=l(L);F=o(Me,"TR",{});var De=l(F);gt=o(De,"TD",{align:!0});var Ai=l(gt);ko=a(Ai,"Entra\xEEnement"),Ai.forEach(s),Eo=c(De),ot=o(De,"TD",{align:!0});var il=l(ot);_o=a(il,"Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Ss=o(il,"EM",{});var Ni=l(Ss);bo=a(Ni,"tokens"),Ni.forEach(s),il.forEach(s),go=c(De),lt=o(De,"TD",{align:!0});var pl=l(lt);$o=a(pl,"Part d\u2019un petit vocabulaire et apprend des r\xE8gles pour fusionner les "),Hs=o(pl,"EM",{});var Si=l(Hs);zo=a(Si,"tokens"),Si.forEach(s),pl.forEach(s),qo=c(De),xe=o(De,"TD",{align:!0});var Un=l(xe);xo=a(Un,"Part d\u2019un grand vocabulaire et apprend des r\xE8gles pour supprimer les "),Bs=o(Un,"EM",{});var Hi=l(Bs);jo=a(Hi,"tokens"),Hi.forEach(s),Po=a(Un,"."),Un.forEach(s),De.forEach(s),wo=c(Me),G=o(Me,"TR",{});var Ce=l(G);$t=o(Ce,"TD",{align:!0});var Bi=l($t);To=a(Bi,"\xC9tape d\u2019Entra\xEEnement"),Bi.forEach(s),yo=c(Ce),je=o(Ce,"TD",{align:!0});var Rn=l(je);Mo=a(Rn,"Fusionne les "),Os=o(Rn,"EM",{});var Oi=l(Os);Do=a(Oi,"tokens"),Oi.forEach(s),Co=a(Rn," correspondant \xE0 la paire la plus commune"),Rn.forEach(s),Ao=c(Ce),X=o(Ce,"TD",{align:!0});var At=l(X);No=a(At,"Fusionne les "),Us=o(At,"EM",{});var Ui=l(Us);So=a(Ui,"tokens"),Ui.forEach(s),Ho=a(At," correspondant \xE0 la paire ayant le meilleur score bas\xE9 sur la fr\xE9quence de la paire, en privil\xE9giant les paires o\xF9 chaque "),Rs=o(At,"EM",{});var Ri=l(Rs);Bo=a(Ri,"token"),Ri.forEach(s),Oo=a(At," individuel est moins fr\xE9quent"),At.forEach(s),Uo=c(Ce),Pe=o(Ce,"TD",{align:!0});var In=l(Pe);Ro=a(In,"Supprime tous les "),Is=o(In,"EM",{});var Ii=l(Is);Io=a(Ii,"tokens"),Ii.forEach(s),Lo=a(In," du vocabulaire qui minimiseront la perte calcul\xE9e sur le corpus entier"),In.forEach(s),Ce.forEach(s),Fo=c(Me),W=o(Me,"TR",{});var Ae=l(W);zt=o(Ae,"TD",{align:!0});var Li=l(zt);Go=a(Li,"Apprendre"),Li.forEach(s),Wo=c(Ae),qt=o(Ae,"TD",{align:!0});var Fi=l(qt);Vo=a(Fi,"Fusionner des r\xE8gles et un vocabulaire"),Fi.forEach(s),Jo=c(Ae),xt=o(Ae,"TD",{align:!0});var Gi=l(xt);Yo=a(Gi,"Juste un vocabulaire"),Gi.forEach(s),Ko=c(Ae),it=o(Ae,"TD",{align:!0});var ul=l(it);Qo=a(ul,"Un vocabulaire avec un score pour chaque "),Ls=o(ul,"EM",{});var Wi=l(Ls);Xo=a(Wi,"token"),Wi.forEach(s),ul.forEach(s),Ae.forEach(s),Zo=c(Me),V=o(Me,"TR",{});var Ne=l(V);jt=o(Ne,"TD",{align:!0});var Vi=l(jt);el=a(Vi,"Encodage"),Vi.forEach(s),tl=c(Ne),Pt=o(Ne,"TD",{align:!0});var Ji=l(Pt);sl=a(Ji,"D\xE9coupe un mot en caract\xE8res et applique les fusions apprises pendant l\u2019entra\xEEnement"),Ji.forEach(s),nl=c(Ne),wt=o(Ne,"TD",{align:!0});var Yi=l(wt);al=a(Yi,"Trouve le plus long sous-mot depuis le d\xE9but qui est dans le vocabulaire, puis fait de m\xEAme pour le reste du mot"),Yi.forEach(s),rl=c(Ne),Tt=o(Ne,"TD",{align:!0});var Ki=l(Tt);ol=a(Ki,"Trouve la division la plus probable en tokens, en utilisant les scores appris pendant l\u2019entra\xEEnement"),Ki.forEach(s),Ne.forEach(s),Me.forEach(s),On.forEach(s),Tn=c(t),yt=o(t,"P",{});var Qi=l(yt);ll=a(Qi,"Maintenant, plongeons dans BPE !"),Qi.forEach(s),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(pp)),m(M,"id","normalisation-et-prtokenization"),m(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(M,"href","#normalisation-et-prtokenization"),m(x,"class","relative group"),m(Ue,"class","block dark:hidden"),Xi(Ue.src,cl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||m(Ue,"src",cl),m(Ue,"alt","The tokenization pipeline."),m(Re,"class","hidden dark:block"),Xi(Re.src,ml="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||m(Re,"src",ml),m(Re,"alt","The tokenization pipeline."),m(le,"class","flex justify-center"),m(ve,"id","normalisation"),m(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ve,"href","#normalisation"),m(ie,"class","relative group"),m(Fe,"href","http://www.unicode.org/reports/tr15/"),m(Fe,"rel","nofollow"),m(_e,"id","prtokenization"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#prtokenization"),m(pe,"class","relative group"),m(ft,"href","/course/fr/chapter2"),m($e,"id","sentencepiece"),m($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m($e,"href","#sentencepiece"),m(ue,"class","relative group"),m(at,"href","https://github.com/google/sentencepiece"),m(at,"rel","nofollow"),m(ht,"href","/course/fr/chapter7/7"),m(ze,"id","aperu-de-lalgorithme"),m(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ze,"href","#aperu-de-lalgorithme"),m(ce,"class","relative group"),m(kt,"align","center"),m(Et,"align","center"),m(_t,"align","center"),m(bt,"align","center"),m(gt,"align","center"),m(ot,"align","center"),m(lt,"align","center"),m(xe,"align","center"),m($t,"align","center"),m(je,"align","center"),m(X,"align","center"),m(Pe,"align","center"),m(zt,"align","center"),m(qt,"align","center"),m(xt,"align","center"),m(it,"align","center"),m(jt,"align","center"),m(Pt,"align","center"),m(wt,"align","center"),m(Tt,"align","center")},m(t,i){e(document.head,d),p(t,me,i),p(t,x,i),e(x,M),e(M,ae),k(y,ae,null),e(x,Se),e(x,re),e(re,oe),p(t,de,i),k(B,t,i),p(t,J,i),p(t,f,i),e(f,He),e(f,O),e(O,Be),e(f,Oe),e(f,U),e(U,g),e(f,ct),e(f,fe),e(fe,mt),e(f,dt),e(f,he),e(he,Ln),e(f,Fn),e(f,Nt),e(Nt,Gn),e(f,Wn),p(t,Ws,i),p(t,le,i),e(le,Ue),e(le,Vn),e(le,Re),p(t,Vs,i),p(t,j,i),e(j,Jn),e(j,St),e(St,Yn),e(j,Kn),e(j,Ht),e(Ht,Qn),e(j,Xn),e(j,Bt),e(Bt,Zn),e(j,ea),e(j,Ot),e(Ot,ta),e(j,sa),p(t,Js,i),p(t,ie,i),e(ie,ve),e(ve,Ut),k(Ie,Ut,null),e(ie,na),e(ie,Rt),e(Rt,aa),p(t,Ys,i),k(Le,t,i),p(t,Ks,i),p(t,Y,i),e(Y,ra),e(Y,Fe),e(Fe,oa),e(Y,la),e(Y,It),e(It,ia),e(Y,pa),p(t,Qs,i),p(t,$,i),e($,ua),e($,Lt),e(Lt,ca),e($,ma),e($,Ft),e(Ft,da),e($,fa),e($,Gt),e(Gt,ha),e($,va),e($,Wt),e(Wt,ka),e($,Ea),e($,Vt),e(Vt,_a),e($,ba),p(t,Xs,i),k(Ge,t,i),p(t,Zs,i),k(We,t,i),p(t,en,i),p(t,D,i),e(D,ga),e(D,Jt),e(Jt,$a),e(D,za),e(D,Yt),e(Yt,qa),e(D,xa),e(D,Kt),e(Kt,ja),e(D,Pa),p(t,tn,i),k(Ve,t,i),p(t,sn,i),k(Je,t,i),p(t,nn,i),p(t,ke,i),e(ke,wa),e(ke,Qt),e(Qt,Ta),e(ke,ya),p(t,an,i),k(Ee,t,i),p(t,rn,i),p(t,pe,i),e(pe,_e),e(_e,Xt),k(Ye,Xt,null),e(pe,Ma),e(pe,Zt),e(Zt,Da),p(t,on,i),k(Ke,t,i),p(t,ln,i),p(t,z,i),e(z,Ca),e(z,es),e(es,Aa),e(z,Na),e(z,ft),e(ft,Sa),e(z,Ha),e(z,ts),e(ts,Ba),e(z,Oa),e(z,ss),e(ss,Ua),e(z,Ra),e(z,ns),e(ns,Ia),e(z,La),p(t,pn,i),p(t,P,i),e(P,Fa),e(P,as),e(as,Ga),e(P,Wa),e(P,rs),e(rs,Va),e(P,Ja),e(P,os),e(os,Ya),e(P,Ka),e(P,ls),e(ls,Qa),e(P,Xa),p(t,un,i),k(Qe,t,i),p(t,cn,i),k(Xe,t,i),p(t,mn,i),p(t,w,i),e(w,Za),e(w,is),e(is,er),e(w,tr),e(w,ps),e(ps,sr),e(w,nr),e(w,us),e(us,ar),e(w,rr),e(w,cs),e(cs,or),e(w,lr),p(t,dn,i),p(t,C,i),e(C,ir),e(C,ms),e(ms,pr),e(C,ur),e(C,ds),e(ds,cr),e(C,mr),e(C,fs),e(fs,dr),e(C,fr),p(t,fn,i),k(Ze,t,i),p(t,hn,i),p(t,K,i),e(K,hr),e(K,hs),e(hs,vr),e(K,kr),e(K,vs),e(vs,Er),e(K,_r),p(t,vn,i),k(et,t,i),p(t,kn,i),p(t,Q,i),e(Q,br),e(Q,ks),e(ks,gr),e(Q,$r),e(Q,Es),e(Es,zr),e(Q,qr),p(t,En,i),p(t,be,i),e(be,xr),e(be,_s),e(_s,jr),e(be,Pr),p(t,_n,i),k(tt,t,i),p(t,bn,i),k(st,t,i),p(t,gn,i),p(t,q,i),e(q,wr),e(q,bs),e(bs,Tr),e(q,yr),e(q,gs),e(gs,Mr),e(q,Dr),e(q,$s),e($s,Cr),e(q,Ar),e(q,zs),e(zs,Nr),e(q,Sr),e(q,qs),e(qs,Hr),e(q,Br),p(t,$n,i),p(t,ge,i),e(ge,Or),e(ge,xs),e(xs,Ur),e(ge,Rr),p(t,zn,i),p(t,ue,i),e(ue,$e),e($e,js),k(nt,js,null),e(ue,Ir),e(ue,Ps),e(Ps,Lr),p(t,qn,i),p(t,R,i),e(R,at),e(at,Fr),e(R,Gr),e(R,ws),e(ws,Wr),e(R,Vr),e(R,ht),e(ht,Jr),e(R,Yr),p(t,xn,i),p(t,T,i),e(T,Kr),e(T,Ts),e(Ts,Qr),e(T,Xr),e(T,ys),e(ys,Zr),e(T,eo),e(T,Ms),e(Ms,to),e(T,so),e(T,Ds),e(Ds,no),e(T,ao),p(t,jn,i),p(t,ce,i),e(ce,ze),e(ze,Cs),k(rt,Cs,null),e(ce,ro),e(ce,As),e(As,oo),p(t,Pn,i),p(t,vt,i),e(vt,lo),p(t,wn,i),p(t,qe,i),e(qe,Ns),e(Ns,I),e(I,kt),e(kt,io),e(I,po),e(I,Et),e(Et,uo),e(I,co),e(I,_t),e(_t,mo),e(I,fo),e(I,bt),e(bt,ho),e(qe,vo),e(qe,L),e(L,F),e(F,gt),e(gt,ko),e(F,Eo),e(F,ot),e(ot,_o),e(ot,Ss),e(Ss,bo),e(F,go),e(F,lt),e(lt,$o),e(lt,Hs),e(Hs,zo),e(F,qo),e(F,xe),e(xe,xo),e(xe,Bs),e(Bs,jo),e(xe,Po),e(L,wo),e(L,G),e(G,$t),e($t,To),e(G,yo),e(G,je),e(je,Mo),e(je,Os),e(Os,Do),e(je,Co),e(G,Ao),e(G,X),e(X,No),e(X,Us),e(Us,So),e(X,Ho),e(X,Rs),e(Rs,Bo),e(X,Oo),e(G,Uo),e(G,Pe),e(Pe,Ro),e(Pe,Is),e(Is,Io),e(Pe,Lo),e(L,Fo),e(L,W),e(W,zt),e(zt,Go),e(W,Wo),e(W,qt),e(qt,Vo),e(W,Jo),e(W,xt),e(xt,Yo),e(W,Ko),e(W,it),e(it,Qo),e(it,Ls),e(Ls,Xo),e(L,Zo),e(L,V),e(V,jt),e(jt,el),e(V,tl),e(V,Pt),e(Pt,sl),e(V,nl),e(V,wt),e(wt,al),e(V,rl),e(V,Tt),e(Tt,ol),p(t,Tn,i),p(t,yt,i),e(yt,ll),yn=!0},p(t,[i]){const pt={};i&2&&(pt.$$scope={dirty:i,ctx:t}),Ee.$set(pt)},i(t){yn||(E(y.$$.fragment,t),E(B.$$.fragment,t),E(Ie.$$.fragment,t),E(Le.$$.fragment,t),E(Ge.$$.fragment,t),E(We.$$.fragment,t),E(Ve.$$.fragment,t),E(Je.$$.fragment,t),E(Ee.$$.fragment,t),E(Ye.$$.fragment,t),E(Ke.$$.fragment,t),E(Qe.$$.fragment,t),E(Xe.$$.fragment,t),E(Ze.$$.fragment,t),E(et.$$.fragment,t),E(tt.$$.fragment,t),E(st.$$.fragment,t),E(nt.$$.fragment,t),E(rt.$$.fragment,t),yn=!0)},o(t){_(y.$$.fragment,t),_(B.$$.fragment,t),_(Ie.$$.fragment,t),_(Le.$$.fragment,t),_(Ge.$$.fragment,t),_(We.$$.fragment,t),_(Ve.$$.fragment,t),_(Je.$$.fragment,t),_(Ee.$$.fragment,t),_(Ye.$$.fragment,t),_(Ke.$$.fragment,t),_(Qe.$$.fragment,t),_(Xe.$$.fragment,t),_(Ze.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(st.$$.fragment,t),_(nt.$$.fragment,t),_(rt.$$.fragment,t),yn=!1},d(t){s(d),t&&s(me),t&&s(x),b(y),t&&s(de),b(B,t),t&&s(J),t&&s(f),t&&s(Ws),t&&s(le),t&&s(Vs),t&&s(j),t&&s(Js),t&&s(ie),b(Ie),t&&s(Ys),b(Le,t),t&&s(Ks),t&&s(Y),t&&s(Qs),t&&s($),t&&s(Xs),b(Ge,t),t&&s(Zs),b(We,t),t&&s(en),t&&s(D),t&&s(tn),b(Ve,t),t&&s(sn),b(Je,t),t&&s(nn),t&&s(ke),t&&s(an),b(Ee,t),t&&s(rn),t&&s(pe),b(Ye),t&&s(on),b(Ke,t),t&&s(ln),t&&s(z),t&&s(pn),t&&s(P),t&&s(un),b(Qe,t),t&&s(cn),b(Xe,t),t&&s(mn),t&&s(w),t&&s(dn),t&&s(C),t&&s(fn),b(Ze,t),t&&s(hn),t&&s(K),t&&s(vn),b(et,t),t&&s(kn),t&&s(Q),t&&s(En),t&&s(be),t&&s(_n),b(tt,t),t&&s(bn),b(st,t),t&&s(gn),t&&s(q),t&&s($n),t&&s(ge),t&&s(zn),t&&s(ue),b(nt),t&&s(qn),t&&s(R),t&&s(xn),t&&s(T),t&&s(jn),t&&s(ce),b(rt),t&&s(Pn),t&&s(vt),t&&s(wn),t&&s(qe),t&&s(Tn),t&&s(yt)}}}const pp={local:"normalisation-et-prtokenization",sections:[{local:"normalisation",title:"Normalisation"},{local:"prtokenization",title:"Pr\xE9-tokenization"},{local:"sentencepiece",title:"SentencePiece"},{local:"aperu-de-lalgorithme",title:"Aper\xE7u de l'algorithme"}],title:"Normalisation et pr\xE9-tokenization"};function up(Gs){return ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kp extends ep{constructor(d){super();tp(this,d,up,ip,sp,{})}}export{kp as default,pp as metadata};
