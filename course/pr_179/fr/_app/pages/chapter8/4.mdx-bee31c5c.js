import{S as Zh,i as Xh,s as eb,e as l,k as p,w as c,t as a,M as sb,c as i,d as s,m as d,x as m,a as u,h as n,b as q,F as t,g as o,y as f,q as v,o as h,B as b,v as tb}from"../../chunks/vendor-1e8b365d.js";import{T as Lt}from"../../chunks/Tip-62b14c6e.js";import{Y as rb}from"../../chunks/Youtube-c2a8cc39.js";import{I as O}from"../../chunks/IconCopyLink-483c28ba.js";import{C as $}from"../../chunks/CodeBlock-e5764662.js";import{D as ab}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as nb}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function ob(P){let _,j,g,k,E;return{c(){_=l("p"),j=a("\u270F\uFE0F "),g=l("em"),k=a("Votre tour !"),E=a(" V\xE9rifiez que tout semble correct avec le deuxi\xE8me \xE9l\xE9ment du jeu de donn\xE9es d\u2019entra\xEEnement.")},l(y){_=i(y,"P",{});var z=u(_);j=n(z,"\u270F\uFE0F "),g=i(z,"EM",{});var w=u(g);k=n(w,"Votre tour !"),w.forEach(s),E=n(z," V\xE9rifiez que tout semble correct avec le deuxi\xE8me \xE9l\xE9ment du jeu de donn\xE9es d\u2019entra\xEEnement."),z.forEach(s)},m(y,z){o(y,_,z),t(_,j),t(_,g),t(g,k),t(_,E)},d(y){y&&s(_)}}}function lb(P){let _,j;return{c(){_=l("p"),j=a("In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models.")},l(g){_=i(g,"P",{});var k=u(_);j=n(k,"In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models."),k.forEach(s)},m(g,k){o(g,_,k),t(_,j)},d(g){g&&s(_)}}}function ib(P){let _,j,g,k,E,y,z,w;return{c(){_=l("p"),j=a("\u{1F4A1} Vous devriez toujours vous assurer que vous pouvez ex\xE9cuter "),g=l("code"),k=a("trainer.evaluate()"),E=a(" avant de lancer "),y=l("code"),z=a("trainer.train()"),w=a(", pour \xE9viter de gaspiller beaucoup de ressources de calcul avant de tomber sur une erreur.")},l(D){_=i(D,"P",{});var x=u(_);j=n(x,"\u{1F4A1} Vous devriez toujours vous assurer que vous pouvez ex\xE9cuter "),g=i(x,"CODE",{});var B=u(g);k=n(B,"trainer.evaluate()"),B.forEach(s),E=n(x," avant de lancer "),y=i(x,"CODE",{});var U=u(y);z=n(U,"trainer.train()"),U.forEach(s),w=n(x,", pour \xE9viter de gaspiller beaucoup de ressources de calcul avant de tomber sur une erreur."),x.forEach(s)},m(D,x){o(D,_,x),t(_,j),t(_,g),t(g,k),t(_,E),t(_,y),t(y,z),t(_,w)},d(D){D&&s(_)}}}function ub(P){let _,j,g,k,E,y,z,w,D,x,B;return{c(){_=l("p"),j=a("\u{1F4A1} Si vous utilisez une boucle d\u2019entra\xEEnement manuelle, les m\xEAmes \xE9tapes s\u2019appliquent pour d\xE9boguer votre pipeline d\u2019entra\xEEnement, mais il est plus facile de les s\xE9parer. Assurez-vous cependant de ne pas avoir oubli\xE9 le "),g=l("code"),k=a("model.eval()"),E=a(" ou le "),y=l("code"),z=a("model.train()"),w=a(" aux bons endroits, ou le "),D=l("code"),x=a("zero_grad()"),B=a(" \xE0 chaque \xE9tape !")},l(U){_=i(U,"P",{});var C=u(_);j=n(C,"\u{1F4A1} Si vous utilisez une boucle d\u2019entra\xEEnement manuelle, les m\xEAmes \xE9tapes s\u2019appliquent pour d\xE9boguer votre pipeline d\u2019entra\xEEnement, mais il est plus facile de les s\xE9parer. Assurez-vous cependant de ne pas avoir oubli\xE9 le "),g=i(C,"CODE",{});var cs=u(g);k=n(cs,"model.eval()"),cs.forEach(s),E=n(C," ou le "),y=i(C,"CODE",{});var T=u(y);z=n(T,"model.train()"),T.forEach(s),w=n(C," aux bons endroits, ou le "),D=i(C,"CODE",{});var Nt=u(D);x=n(Nt,"zero_grad()"),Nt.forEach(s),B=n(C," \xE0 chaque \xE9tape !"),C.forEach(s)},m(U,C){o(U,_,C),t(_,j),t(_,g),t(g,k),t(_,E),t(_,y),t(y,z),t(_,w),t(_,D),t(D,x),t(_,B)},d(U){U&&s(_)}}}function pb(P){let _,j;return{c(){_=l("p"),j=a("\u26A0\uFE0F Si vous effectuez un entra\xEEnement distribu\xE9, imprimez des \xE9chantillons de votre ensemble de donn\xE9es dans chaque processus et v\xE9rifiez par trois fois que vous obtenez la m\xEAme chose. Un bug courant consiste \xE0 avoir une source d\u2019al\xE9a dans la cr\xE9ation des donn\xE9es qui fait que chaque processus a une version diff\xE9rente de l\u2019ensemble de donn\xE9es.")},l(g){_=i(g,"P",{});var k=u(_);j=n(k,"\u26A0\uFE0F Si vous effectuez un entra\xEEnement distribu\xE9, imprimez des \xE9chantillons de votre ensemble de donn\xE9es dans chaque processus et v\xE9rifiez par trois fois que vous obtenez la m\xEAme chose. Un bug courant consiste \xE0 avoir une source d\u2019al\xE9a dans la cr\xE9ation des donn\xE9es qui fait que chaque processus a une version diff\xE9rente de l\u2019ensemble de donn\xE9es."),k.forEach(s)},m(g,k){o(g,_,k),t(_,j)},d(g){g&&s(_)}}}function db(P){let _,j;return{c(){_=l("p"),j=a("\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes.")},l(g){_=i(g,"P",{});var k=u(_);j=n(k,"\u{1F4A1} Si vos donn\xE9es d\u2019entra\xEEnement ne sont pas \xE9quilibr\xE9es, veillez \xE0 cr\xE9er un batch de donn\xE9es d\u2019entra\xEEnement contenant toutes les \xE9tiquettes."),k.forEach(s)},m(g,k){o(g,_,k),t(_,j)},d(g){g&&s(_)}}}function cb(P){let _,j,g,k,E;return{c(){_=l("p"),j=a("\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),g=l("code"),k=a("Trainer"),E=a(" apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet.")},l(y){_=i(y,"P",{});var z=u(_);j=n(z,"\u26A0\uFE0F Vous devrez recr\xE9er votre mod\xE8le et votre "),g=i(z,"CODE",{});var w=u(g);k=n(w,"Trainer"),w.forEach(s),E=n(z," apr\xE8s ce test, car le mod\xE8le obtenu ne sera probablement pas capable de r\xE9cup\xE9rer et d\u2019apprendre quelque chose d\u2019utile sur votre jeu de donn\xE9es complet."),z.forEach(s)},m(y,z){o(y,_,z),t(_,j),t(_,g),t(g,k),t(_,E)},d(y){y&&s(_)}}}function mb(P){let _,j,g,k,E,y,z,w,D,x,B,U,C,cs,T,Nt,Lr,lu,iu,Ut,uu,pu,Nr,du,cu,An,oe,_e,Ur,ms,mu,Or,fu,Dn,fs,Tn,Y,vu,Mr,hu,bu,Vr,_u,qu,Sn,qe,$u,Ir,gu,ku,Ln,$e,ju,vs,yu,zu,Nn,hs,Un,Ot,Eu,On,bs,Mn,le,ge,Fr,_s,wu,Gr,xu,Vn,ke,Cu,Wr,Pu,Au,In,je,Du,Rr,Tu,Su,Fn,qs,Gn,$s,Wn,J,Lu,Hr,Nu,Uu,Br,Ou,Mu,Rn,A,Vu,Yr,Iu,Fu,Jr,Gu,Wu,Kr,Ru,Hu,Qr,Bu,Yu,Zr,Ju,Ku,Hn,gs,Bn,Mt,Qu,Yn,ks,Jn,Vt,Zu,Kn,js,Qn,It,Xu,Zn,ye,ep,Xr,sp,tp,Xn,ys,eo,zs,so,Ft,rp,to,Es,ro,ws,ao,S,ap,ea,np,op,sa,lp,ip,ta,up,pp,ra,dp,cp,no,xs,oo,Cs,lo,K,mp,Ps,fp,vp,aa,hp,bp,io,ze,_p,na,qp,$p,uo,As,po,Ds,co,Ee,gp,oa,kp,jp,mo,Ts,fo,Ss,vo,Gt,yp,ho,Ls,bo,Ns,_o,Q,zp,la,Ep,wp,ia,xp,Cp,qo,Us,$o,Os,go,Z,Pp,ua,Ap,Dp,pa,Tp,Sp,ko,we,Lp,da,Np,Up,jo,xe,yo,Wt,Op,zo,Rt,Mp,Eo,ie,Ce,ca,Ms,Vp,ma,Ip,wo,L,Fp,fa,Gp,Wp,va,Rp,Hp,ha,Bp,Yp,ba,Jp,Kp,xo,Vs,Co,Ht,Qp,Po,Is,Ao,X,Zp,_a,Xp,ed,qa,sd,td,Do,Fs,To,Gs,So,M,rd,$a,ad,nd,ga,od,ld,ka,id,ud,Lo,V,pd,ja,dd,cd,ya,md,fd,za,vd,hd,No,Ws,Uo,Bt,bd,Oo,Rs,Mo,Yt,_d,Vo,Jt,qd,Io,Hs,Fo,N,$d,Ea,gd,kd,wa,jd,yd,xa,zd,Ed,Ca,wd,xd,Go,Bs,Wo,Kt,Cd,Ro,Qt,Pd,Ho,ue,Pe,Pa,Ys,Ad,Aa,Dd,Bo,Zt,Td,Yo,Js,Jo,ee,Sd,Da,Ld,Nd,Ta,Ud,Od,Ko,Xt,Md,Qo,er,Vd,Zo,Ae,Id,Sa,Fd,Gd,Xo,Ks,el,Qs,sl,De,Wd,La,Rd,Hd,tl,Zs,rl,Xs,al,sr,Bd,nl,et,ol,Te,Yd,Na,Jd,Kd,ll,st,il,tr,Qd,ul,tt,pl,Se,Zd,Ua,Xd,ec,dl,pe,Le,Oa,rt,sc,Ma,tc,cl,rr,rc,ml,Ne,ac,Va,nc,oc,fl,at,vl,ar,lc,hl,se,ic,Ia,uc,pc,Fa,dc,cc,bl,nt,_l,Ue,mc,Ga,fc,vc,ql,de,Oe,Wa,ot,hc,Ra,bc,$l,Me,_c,Ha,qc,$c,gl,nr,gc,kl,Ve,jl,ce,Ie,Ba,lt,kc,Ya,jc,yl,Fe,yc,Ja,zc,Ec,zl,it,El,ut,wl,or,wc,xl,Ge,xc,Ka,Cc,Pc,Cl,pt,Pl,dt,Al,We,Dl,lr,Ac,Tl,ct,Sl,Re,Dc,Qa,Tc,Sc,Ll,mt,Nl,I,Lc,Za,Nc,Uc,Xa,Oc,Mc,en,Vc,Ic,Ul,ft,Ol,vt,Ml,F,Fc,sn,Gc,Wc,tn,Rc,Hc,rn,Bc,Yc,Vl,ht,Il,bt,Fl,He,Jc,an,Kc,Qc,Gl,_t,Wl,qt,Rl,ir,Zc,Hl,ur,Xc,Bl,$t,Yl,pr,em,Jl,Be,Kl,me,Ye,nn,gt,sm,on,tm,Ql,dr,rm,Zl,fe,Je,ln,kt,am,un,nm,Xl,cr,om,ei,G,pn,lm,im,dn,um,pm,cn,dm,cm,mn,mm,si,Ke,ti,mr,fm,ri,fr,vm,ai,vr,hm,ni,ve,Qe,fn,jt,bm,vn,_m,oi,hr,qm,li,Ze,$m,hn,gm,km,ii,yt,ui,Xe,pi,es,jm,bn,ym,zm,di,zt,ci,Et,mi,br,Em,fi,_r,wm,vi,ss,hi,he,ts,_n,wt,xm,qn,Cm,bi,rs,Pm,$n,Am,Dm,_i,qr,Tm,qi,$r,Sm,$i,be,as,gn,xt,Lm,kn,Nm,gi,ns,Um,Ct,Om,Mm,ki,gr,Vm,ji,W,kr,Pt,Im,Fm,Gm,jr,At,Wm,Rm,Hm,yr,Dt,Bm,Ym,Jm,zr,Tt,Km,Qm,yi,te,Zm,jn,Xm,ef,yn,sf,tf,zi;return g=new nb({props:{fw:P[0]}}),w=new O({}),C=new ab({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"}]}}),ms=new O({}),fs=new rb({props:{id:"L-WSwUWde1U"}}),hs=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=raw_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),bs=new $({props:{code:"'ValueError: You have to specify either input_ids or inputs_embeds'",highlighted:'<span class="hljs-string">&#x27;ValueError: You have to specify either input_ids or inputs_embeds&#x27;</span>'}}),_s=new O({}),qs=new $({props:{code:"trainer.train_dataset[0]",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>]'}}),$s=new $({props:{code:`{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}`,highlighted:`{<span class="hljs-string">&#x27;hypothesis&#x27;</span>: <span class="hljs-string">&#x27;Product and geography are what make cream skimming work. &#x27;</span>,
 <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;premise&#x27;</span>: <span class="hljs-string">&#x27;Conceptually cream skimming has two basic dimensions - product and geography.&#x27;</span>}`}}),gs=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),ks=new $({props:{code:"'ValueError: expected sequence of length 43 at dim 1 (got 37)'",highlighted:'<span class="hljs-string">&#x27;ValueError: expected sequence of length 43 at dim 1 (got 37)&#x27;</span>'}}),js=new $({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch`}}),ys=new $({props:{code:'tokenizer.decode(trainer.train_dataset[0]["input_ids"])',highlighted:'tokenizer.decode(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),zs=new $({props:{code:"'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'",highlighted:'<span class="hljs-string">&#x27;[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]&#x27;</span>'}}),Es=new $({props:{code:"trainer.train_dataset[0].keys()",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>].keys()'}}),ws=new $({props:{code:"dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;hypothesis&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;premise&#x27;</span>])'}}),xs=new $({props:{code:"type(trainer.model)",highlighted:'<span class="hljs-built_in">type</span>(trainer.model)'}}),Cs=new $({props:{code:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification",highlighted:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"}}),As=new $({props:{code:'tokenizer.decode(trainer.train_dataset[0]["attention_mask"])',highlighted:'tokenizer.decode(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>])'}}),Ds=new $({props:{code:"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]",highlighted:'[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),Ts=new $({props:{code:`len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)`,highlighted:`<span class="hljs-built_in">len</span>(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>]) == <span class="hljs-built_in">len</span>(
    trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>]
)`}}),Ss=new $({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),Ls=new $({props:{code:'trainer.train_dataset[0]["label"]',highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;label&quot;</span>]'}}),Ns=new $({props:{code:"1",highlighted:'<span class="hljs-number">1</span>'}}),Us=new $({props:{code:'trainer.train_dataset.features["label"].names',highlighted:'trainer.train_dataset.features[<span class="hljs-string">&quot;label&quot;</span>].names'}}),Os=new $({props:{code:"['entailment', 'neutral', 'contradiction']",highlighted:'[<span class="hljs-string">&#x27;entailment&#x27;</span>, <span class="hljs-string">&#x27;neutral&#x27;</span>, <span class="hljs-string">&#x27;contradiction&#x27;</span>]'}}),xe=new Lt({props:{$$slots:{default:[ob]},$$scope:{ctx:P}}}),Ms=new O({}),Vs=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),Is=new $({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch

ValueError: expected sequence of length <span class="hljs-number">45</span> at dim <span class="hljs-number">1</span> (got <span class="hljs-number">76</span>)`}}),Fs=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`}}),Gs=new $({props:{code:"<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>",highlighted:'&lt;function transformers.data.data_collator.default_data_collator(features: <span class="hljs-type">List</span>[InputDataClass], return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]&gt;'}}),Ws=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),Rs=new $({props:{code:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",highlighted:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"}}),Hs=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),Bs=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),Ys=new O({}),Js=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),Ks=new $({props:{code:"outputs = trainer.model.cpu()(**batch)",highlighted:"outputs = trainer.model.cpu()(**batch)"}}),Qs=new $({props:{code:`~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.`,highlighted:`~/.pyenv/versions/<span class="hljs-number">3.7</span><span class="hljs-number">.9</span>/envs/base/lib/python3<span class="hljs-number">.7</span>/site-packages/torch/nn/functional.py <span class="hljs-keyword">in</span> nll_loss(<span class="hljs-built_in">input</span>, target, weight, size_average, ignore_index, reduce, reduction)
   <span class="hljs-number">2386</span>         )
   <span class="hljs-number">2387</span>     <span class="hljs-keyword">if</span> dim == <span class="hljs-number">2</span>:
-&gt; <span class="hljs-number">2388</span>         ret = torch._C._nn.nll_loss(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)
   <span class="hljs-number">2389</span>     <span class="hljs-keyword">elif</span> dim == <span class="hljs-number">4</span>:
   <span class="hljs-number">2390</span>         ret = torch._C._nn.nll_loss2d(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target <span class="hljs-number">2</span> <span class="hljs-keyword">is</span> out of bounds.`}}),Zs=new $({props:{code:"trainer.model.config.num_labels",highlighted:"trainer.model.config.num_labels"}}),Xs=new $({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),et=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),st=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

outputs = trainer.model.cpu()(**batch)`}}),tt=new $({props:{code:`import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

outputs = trainer.model.to(device)(**batch)`}}),rt=new O({}),at=new $({props:{code:`loss = outputs.loss
loss.backward()`,highlighted:`loss = outputs.loss
loss.backward()`}}),nt=new $({props:{code:`trainer.create_optimizer()
trainer.optimizer.step()`,highlighted:`trainer.create_optimizer()
trainer.optimizer.step()`}}),ot=new O({}),Ve=new Lt({props:{$$slots:{default:[lb]},$$scope:{ctx:P}}}),lt=new O({}),it=new $({props:{code:`# This will take a long time and error out, so you shouldn't run this cell
trainer.train()`,highlighted:`<span class="hljs-comment"># This will take a long time and error out, so you shouldn&#x27;t run this cell</span>
trainer.train()`}}),ut=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),pt=new $({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),dt=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),We=new Lt({props:{$$slots:{default:[ib]},$$scope:{ctx:P}}}),ct=new $({props:{code:`for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_eval_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)`}}),mt=new $({props:{code:`~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()`,highlighted:`~/git/datasets/src/datasets/metric.py <span class="hljs-keyword">in</span> add_batch(self, predictions, references)
    <span class="hljs-number">431</span>         <span class="hljs-string">&quot;&quot;&quot;
    432         batch = {&quot;predictions&quot;: predictions, &quot;references&quot;: references}
--&gt; 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()</span>`}}),ft=new $({props:{code:`predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))`,highlighted:`predictions = outputs.logits.cpu().numpy()
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].cpu().numpy()

compute_metrics((predictions, labels))`}}),vt=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),ht=new $({props:{code:"predictions.shape, labels.shape",highlighted:"predictions.shape, labels.shape"}}),bt=new $({props:{code:"((8, 3), (8,))",highlighted:'((<span class="hljs-number">8</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">8</span>,))'}}),_t=new $({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`}}),qt=new $({props:{code:"{'accuracy': 0.625}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.625</span>}'}}),$t=new $({props:{code:`import numpy as np
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),Be=new Lt({props:{$$slots:{default:[ub]},$$scope:{ctx:P}}}),gt=new O({}),kt=new O({}),Ke=new Lt({props:{warning:!0,$$slots:{default:[pb]},$$scope:{ctx:P}}}),jt=new O({}),yt=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
trainer.create_optimizer()

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`}}),Xe=new Lt({props:{$$slots:{default:[db]},$$scope:{ctx:P}}}),zt=new $({props:{code:`with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`,highlighted:`<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`}}),Et=new $({props:{code:"{'accuracy': 1.0}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">1.0</span>}'}}),ss=new Lt({props:{warning:!0,$$slots:{default:[cb]},$$scope:{ctx:P}}}),wt=new O({}),xt=new O({}),{c(){_=l("meta"),j=p(),c(g.$$.fragment),k=p(),E=l("h1"),y=l("a"),z=l("span"),c(w.$$.fragment),D=p(),x=l("span"),B=a("D\xE9bogage du pipeline d'entra\xEEnement"),U=p(),c(C.$$.fragment),cs=p(),T=l("p"),Nt=a("Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),Lr=l("em"),lu=a("finetuner"),iu=a(" un mod\xE8le sur une t\xE2che donn\xE9e, en suivant consciencieusement les conseils du "),Ut=l("a"),uu=a("Chapitre 7"),pu=a(". Mais lorsque vous lancez la commande "),Nr=l("code"),du=a("model.fit()"),cu=a(", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur, mais le mod\xE8le r\xE9sultant est merdique. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),An=p(),oe=l("h2"),_e=l("a"),Ur=l("span"),c(ms.$$.fragment),mu=p(),Or=l("span"),fu=a("D\xE9boguer le pipeline d'entra\xEEnement"),Dn=p(),c(fs.$$.fragment),Tn=p(),Y=l("p"),vu=a("Le probl\xE8me lorsque vous rencontrez une erreur dans "),Mr=l("code"),hu=a("trainer.train()"),bu=a(" est qu\u2019elle peut provenir de plusieurs sources, car le "),Vr=l("code"),_u=a("Trainer"),qu=a(" assemble g\xE9n\xE9ralement des batchs de choses. Il convertit les jeux de donn\xE9es en chargeurs de donn\xE9es, donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, il prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, il calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),Sn=p(),qe=l("p"),$u=a("La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Ir=l("code"),gu=a("trainer.train()"),ku=a(" est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),Ln=p(),$e=l("p"),ju=a("Pour le d\xE9montrer, nous utiliserons le script suivant qui tente d\u2019ajuster un mod\xE8le DistilBERT sur le "),vs=l("a"),yu=a("jeu de donn\xE9es MNLI"),zu=a(" :"),Nn=p(),c(hs.$$.fragment),Un=p(),Ot=l("p"),Eu=a("Si vous essayez de l\u2019ex\xE9cuter, vous serez confront\xE9 \xE0 une erreur plut\xF4t cryptique :"),On=p(),c(bs.$$.fragment),Mn=p(),le=l("h3"),ge=l("a"),Fr=l("span"),c(_s.$$.fragment),wu=p(),Gr=l("span"),xu=a("V\xE9rifiez vos donn\xE9es"),Vn=p(),ke=l("p"),Cu=a("Cela va sans dire, mais si vos donn\xE9es sont corrompues, le "),Wr=l("code"),Pu=a("Trainer"),Au=a(" ne sera pas capable de former des batchs, et encore moins d\u2019entra\xEEner votre mod\xE8le. Donc, tout d\u2019abord, vous devez jeter un coup d\u2019oeil \xE0 ce qui se trouve dans votre ensemble d\u2019entra\xEEnement."),In=p(),je=l("p"),Du=a("Pour \xE9viter d\u2019innombrables heures pass\xE9es \xE0 essayer de corriger quelque chose qui n\u2019est pas la source du bug, nous vous recommandons d\u2019utiliser "),Rr=l("code"),Tu=a("trainer.train_dataset"),Su=a(" pour vos v\xE9rifications et rien d\u2019autre. Faisons donc cela ici :"),Fn=p(),c(qs.$$.fragment),Gn=p(),c($s.$$.fragment),Wn=p(),J=l("p"),Lu=a("Vous remarquez quelque chose d\u2019anormal ? Ceci, en conjonction avec le message d\u2019erreur sur les "),Hr=l("code"),Nu=a("input_ids"),Uu=a(" manquants, devrait vous faire r\xE9aliser que ce sont des textes, et non des nombres que le mod\xE8le peut comprendre. Ici, l\u2019erreur originale est tr\xE8s trompeuse parce que le "),Br=l("code"),Ou=a("Trainer"),Mu=a(" enl\xE8ve automatiquement les colonnes qui ne correspondent pas \xE0 la signature du mod\xE8le (c\u2019est-\xE0-dire, les arguments attendus par le mod\xE8le). Cela signifie qu\u2019ici, tout, sauf les \xE9tiquettes, a \xE9t\xE9 \xE9limin\xE9. Il n\u2019y avait donc aucun probl\xE8me \xE0 cr\xE9er des batchs et \xE0 les envoyer ensuite au mod\xE8le, qui s\u2019est plaint \xE0 son tour de ne pas avoir re\xE7u les bons arguments."),Rn=p(),A=l("p"),Vu=a("Pourquoi les donn\xE9es n\u2019ont-elles pas \xE9t\xE9 trait\xE9es ? Nous avons utilis\xE9 la m\xE9thode "),Yr=l("code"),Iu=a("Dataset.map()"),Fu=a(" sur les ensembles de donn\xE9es pour appliquer le "),Jr=l("em"),Gu=a("tokenizer"),Wu=a(" sur chaque \xE9chantillon. Mais si vous regardez attentivement le code, vous verrez que nous avons fait une erreur en passant les ensembles d\u2019entra\xEEnement et d\u2019\xE9valuation au "),Kr=l("code"),Ru=a("Trainer"),Hu=a(". Au lieu d\u2019utiliser "),Qr=l("code"),Bu=a("tokenized_datasets"),Yu=a(" ici, nous avons utilis\xE9 "),Zr=l("code"),Ju=a("raw_datasets"),Ku=a(" \u{1F926}. Alors corrigeons \xE7a !"),Hn=p(),c(gs.$$.fragment),Bn=p(),Mt=l("p"),Qu=a("Ce nouveau code donnera maintenant une erreur diff\xE9rente (progr\xE8s !) :"),Yn=p(),c(ks.$$.fragment),Jn=p(),Vt=l("p"),Zu=a("En regardant la trace, nous pouvons voir que l\u2019erreur se produit dans l\u2019\xE9tape de collationnement des donn\xE9es :"),Kn=p(),c(js.$$.fragment),Qn=p(),It=l("p"),Xu=a("Donc, nous devrions passer \xE0 cela. Mais avant cela, finissons d\u2019inspecter nos donn\xE9es, pour \xEAtre s\xFBrs \xE0 100% qu\u2019elles sont correctes."),Zn=p(),ye=l("p"),ep=a("Une chose que vous devriez toujours faire lorsque vous d\xE9boguez une session d\u2019entra\xEEnement est de jeter un coup d\u2019oeil aux entr\xE9es d\xE9cod\xE9es de votre mod\xE8le. Nous ne pouvons pas donner un sens aux chiffres que nous lui fournissons directement, nous devons donc examiner ce que ces chiffres repr\xE9sentent. Dans le domaine de la vision par ordinateur, par exemple, cela signifie regarder les images d\xE9cod\xE9es des pixels que vous passez, dans le domaine de la parole, cela signifie \xE9couter les \xE9chantillons audio d\xE9cod\xE9s, et pour notre exemple NLP, cela signifie utiliser notre "),Xr=l("em"),sp=a("tokenizer"),tp=a(" pour d\xE9coder les entr\xE9es :"),Xn=p(),c(ys.$$.fragment),eo=p(),c(zs.$$.fragment),so=p(),Ft=l("p"),rp=a("Cela semble donc correct. Vous devriez faire cela pour toutes les cl\xE9s dans les entr\xE9es :"),to=p(),c(Es.$$.fragment),ro=p(),c(ws.$$.fragment),ao=p(),S=l("p"),ap=a("Note that the keys that don\u2019t correspond to inputs accepted by the model will be automatically discarded, so here we will only keep "),ea=l("code"),np=a("input_ids"),op=a(", "),sa=l("code"),lp=a("attention_mask"),ip=a(", and "),ta=l("code"),up=a("label"),pp=a(" (which will be renamed "),ra=l("code"),dp=a("labels"),cp=a("). To double-check the model signature, you can print the class of your model, then go check its documentation:"),no=p(),c(xs.$$.fragment),oo=p(),c(Cs.$$.fragment),lo=p(),K=l("p"),mp=a("Donc dans notre cas, nous pouvons v\xE9rifier les param\xE8tres accept\xE9s sur "),Ps=l("a"),fp=a("cette page"),vp=a(". Le "),aa=l("code"),hp=a("Trainer"),bp=a(" va \xE9galement enregistrer les colonnes qu\u2019il rejette."),io=p(),ze=l("p"),_p=a("Nous avons v\xE9rifi\xE9 que les IDs d\u2019entr\xE9e sont corrects en les d\xE9codant. Ensuite, il y a le "),na=l("code"),qp=a("attention_mask"),$p=a(" :"),uo=p(),c(As.$$.fragment),po=p(),c(Ds.$$.fragment),co=p(),Ee=l("p"),gp=a("Comme nous n\u2019avons pas appliqu\xE9 de "),oa=l("em"),kp=a("padding"),jp=a(" dans notre pr\xE9traitement, cela semble parfaitement naturel. Pour \xEAtre s\xFBr qu\u2019il n\u2019y a pas de probl\xE8me avec ce masque d\u2019attention, v\xE9rifions qu\u2019il est de la m\xEAme longueur que nos identifiants d\u2019entr\xE9e :"),mo=p(),c(Ts.$$.fragment),fo=p(),c(Ss.$$.fragment),vo=p(),Gt=l("p"),yp=a("C\u2019est bien ! Enfin, v\xE9rifions notre \xE9tiquette :"),ho=p(),c(Ls.$$.fragment),bo=p(),c(Ns.$$.fragment),_o=p(),Q=l("p"),zp=a("Comme les ID d\u2019entr\xE9e, c\u2019est un nombre qui n\u2019a pas vraiment de sens en soi. Comme nous l\u2019avons vu pr\xE9c\xE9demment, la correspondance entre les entiers et les noms d\u2019\xE9tiquettes est stock\xE9e dans l\u2019attribut "),la=l("code"),Ep=a("names"),wp=a(" de la "),ia=l("em"),xp=a("caract\xE9ristique"),Cp=a(" correspondante de l\u2019ensemble de donn\xE9es :"),qo=p(),c(Us.$$.fragment),$o=p(),c(Os.$$.fragment),go=p(),Z=l("p"),Pp=a("Donc "),ua=l("code"),Ap=a("1"),Dp=a(" signifie "),pa=l("code"),Tp=a("neutral"),Sp=a(", ce qui signifie que les deux phrases que nous avons vues ci-dessus ne sont pas en contradiction, et que la premi\xE8re n\u2019implique pas la seconde. Cela semble correct !"),ko=p(),we=l("p"),Lp=a("Nous n\u2019avons pas d\u2019ID de type de "),da=l("em"),Np=a("token"),Up=a(" ici, puisque DistilBERT ne les attend pas ; si vous en avez dans votre mod\xE8le, vous devriez \xE9galement vous assurer qu\u2019ils correspondent correctement \xE0 l\u2019endroit o\xF9 se trouvent la premi\xE8re et la deuxi\xE8me phrase dans l\u2019entr\xE9e."),jo=p(),c(xe.$$.fragment),yo=p(),Wt=l("p"),Op=a("Nous ne v\xE9rifions ici que l\u2019ensemble d\u2019entra\xEEnement, mais vous devez bien s\xFBr v\xE9rifier de la m\xEAme fa\xE7on les ensembles de validation et de test."),zo=p(),Rt=l("p"),Mp=a("Maintenant que nous savons que nos ensembles de donn\xE9es sont bons, il est temps de v\xE9rifier l\u2019\xE9tape suivante du pipeline d\u2019entra\xEEnement."),Eo=p(),ie=l("h3"),Ce=l("a"),ca=l("span"),c(Ms.$$.fragment),Vp=p(),ma=l("span"),Ip=a("Des jeux de donn\xE9es aux chargeurs de donn\xE9es"),wo=p(),L=l("p"),Fp=a("La prochaine chose qui peut mal tourner dans le pipeline d\u2019entra\xEEnement est lorsque le "),fa=l("code"),Gp=a("Trainer"),Wp=a(" essaie de former des batchs \xE0 partir de l\u2019ensemble d\u2019entra\xEEnement ou de validation. Une fois que vous \xEAtes s\xFBr que les jeux de donn\xE9es du "),va=l("code"),Rp=a("Trainer"),Hp=a(" sont corrects, vous pouvez essayer de former manuellement un batch en ex\xE9cutant ce qui suit (remplacez "),ha=l("code"),Bp=a("train"),Yp=a(" par "),ba=l("code"),Jp=a("eval"),Kp=a(" pour le dataloader de validation) :"),xo=p(),c(Vs.$$.fragment),Co=p(),Ht=l("p"),Qp=a("Ce code cr\xE9e le dataloader d\u2019entra\xEEnement, puis le parcourt en s\u2019arr\xEAtant \xE0 la premi\xE8re it\xE9ration. Si le code s\u2019ex\xE9cute sans erreur, vous avez le premier batch d\u2019entra\xEEnement que vous pouvez inspecter, et si le code se trompe, vous \xEAtes s\xFBr que le probl\xE8me se situe dans le dataloader, comme c\u2019est le cas ici :"),Po=p(),c(Is.$$.fragment),Ao=p(),X=l("p"),Zp=a("L\u2019inspection de la derni\xE8re image du traceback devrait suffire \xE0 vous donner un indice, mais creusons un peu plus. La plupart des probl\xE8mes lors de la cr\xE9ation d\u2019un batch sont dus \xE0 l\u2019assemblage des exemples en un seul batch, donc la premi\xE8re chose \xE0 v\xE9rifier en cas de doute est le "),_a=l("code"),Xp=a("collate_fn"),ed=a(" utilis\xE9 par votre "),qa=l("code"),sd=a("DataLoader"),td=a(" :"),Do=p(),c(Fs.$$.fragment),To=p(),c(Gs.$$.fragment),So=p(),M=l("p"),rd=a("C\u2019est donc le collateur "),$a=l("code"),ad=a("default_data_collator"),nd=a(", mais ce n\u2019est pas ce que nous voulons dans ce cas. Nous voulons rembourrer nos exemples \xE0 la phrase la plus longue du batch, ce qui est fait par le collateur "),ga=l("code"),od=a("DataCollatorWithPadding"),ld=a(". Et ce collateur de donn\xE9es est cens\xE9 \xEAtre utilis\xE9 par d\xE9faut par le "),ka=l("code"),id=a("Trainer"),ud=a(", alors pourquoi n\u2019est-il pas utilis\xE9 ici ?"),Lo=p(),V=l("p"),pd=a("La r\xE9ponse est que nous n\u2019avons pas pass\xE9 le "),ja=l("code"),dd=a("tokenizer"),cd=a(" au "),ya=l("code"),md=a("Trainer"),fd=a(", donc il ne pouvait pas cr\xE9er le "),za=l("code"),vd=a("DataCollatorWithPadding"),hd=a(" que nous voulons. En pratique, il ne faut jamais h\xE9siter \xE0 transmettre explicitement le collateur de donn\xE9es que l\u2019on veut utiliser, pour \xEAtre s\xFBr d\u2019\xE9viter ce genre d\u2019erreurs. Adaptons notre code pour faire exactement cela :"),No=p(),c(Ws.$$.fragment),Uo=p(),Bt=l("p"),bd=a("La bonne nouvelle ? Nous n\u2019avons plus la m\xEAme erreur qu\u2019avant, ce qui est un progr\xE8s certain. La mauvaise nouvelle ? Nous obtenons une erreur CUDA inf\xE2me \xE0 la place :"),Oo=p(),c(Rs.$$.fragment),Mo=p(),Yt=l("p"),_d=a("C\u2019est une mauvaise chose car les erreurs CUDA sont extr\xEAmement difficiles \xE0 d\xE9boguer en g\xE9n\xE9ral. Nous verrons dans une minute comment r\xE9soudre ce probl\xE8me, mais terminons d\u2019abord notre analyse de la cr\xE9ation de batchs."),Vo=p(),Jt=l("p"),qd=a("Si vous \xEAtes s\xFBr que votre collecteur de donn\xE9es est le bon, vous devriez essayer de l\u2019appliquer sur quelques \xE9chantillons de votre ensemble de donn\xE9es :"),Io=p(),c(Hs.$$.fragment),Fo=p(),N=l("p"),$d=a("Ce code \xE9chouera parce que le "),Ea=l("code"),gd=a("train_dataset"),kd=a(" contient des colonnes de type string, que le "),wa=l("code"),jd=a("Trainer"),yd=a(" supprime habituellement. Vous pouvez les supprimer manuellement, ou si vous voulez reproduire exactement ce que le "),xa=l("code"),zd=a("Trainer"),Ed=a(" fait en coulisse, vous pouvez appeler la m\xE9thode priv\xE9e "),Ca=l("code"),wd=a("Trainer._remove_unused_columns()"),xd=a(" qui fait cela :"),Go=p(),c(Bs.$$.fragment),Wo=p(),Kt=l("p"),Cd=a("Vous devriez alors \xEAtre en mesure de d\xE9boguer manuellement ce qui se passe dans le collecteur de donn\xE9es si l\u2019erreur persiste."),Ro=p(),Qt=l("p"),Pd=a("Maintenant que nous avons d\xE9bogu\xE9 le processus de cr\xE9ation de batch, il est temps d\u2019en passer un dans le mod\xE8le !"),Ho=p(),ue=l("h3"),Pe=l("a"),Pa=l("span"),c(Ys.$$.fragment),Ad=p(),Aa=l("span"),Dd=a("Passage par le mod\xE8le"),Bo=p(),Zt=l("p"),Td=a("Vous devriez \xEAtre en mesure d\u2019obtenir un batch en ex\xE9cutant la commande suivante :"),Yo=p(),c(Js.$$.fragment),Jo=p(),ee=l("p"),Sd=a("Si vous ex\xE9cutez ce code dans un "),Da=l("em"),Ld=a("notebook"),Nd=a(", vous risquez d\u2019obtenir une erreur CUDA similaire \xE0 celle que nous avons vue pr\xE9c\xE9demment, auquel cas vous devrez red\xE9marrer votre notebook et r\xE9ex\xE9cuter le dernier extrait sans la ligne "),Ta=l("code"),Ud=a("trainer.train()"),Od=a(". C\u2019est la deuxi\xE8me chose la plus ennuyeuse \xE0 propos des erreurs CUDA : elles cassent irr\xE9m\xE9diablement votre noyau. La chose la plus ennuyeuse \xE0 leur sujet est le fait qu\u2019elles sont difficiles \xE0 d\xE9boguer."),Ko=p(),Xt=l("p"),Md=a("Comment cela se fait-il ? Cela tient \xE0 la fa\xE7on dont les GPU fonctionnent. Ils sont extr\xEAmement efficaces pour ex\xE9cuter un batch d\u2019op\xE9rations en parall\xE8le, mais l\u2019inconv\xE9nient est que lorsque l\u2019une de ces instructions entra\xEEne une erreur, vous ne le savez pas imm\xE9diatement. Ce n\u2019est que lorsque le programme appelle une synchronisation des multiples processus sur le GPU qu\u2019il r\xE9alise que quelque chose s\u2019est mal pass\xE9, de sorte que l\u2019erreur est en fait soulev\xE9e \xE0 un endroit qui n\u2019a rien \xE0 voir avec ce qui l\u2019a cr\xE9\xE9e. Par exemple, si nous regardons notre traceback pr\xE9c\xE9dent, l\u2019erreur a \xE9t\xE9 soulev\xE9e pendant la passe arri\xE8re, mais nous verrons dans une minute qu\u2019elle provient en fait de quelque chose dans la passe avant."),Qo=p(),er=l("p"),Vd=a("Alors comment d\xE9boguer ces erreurs ? La r\xE9ponse est simple : nous ne le faisons pas. \xC0 moins que votre erreur CUDA ne soit une erreur out-of-memory (ce qui signifie qu\u2019il n\u2019y a pas assez de m\xE9moire dans votre GPU), vous devez toujours revenir au CPU pour la d\xE9boguer."),Zo=p(),Ae=l("p"),Id=a("Pour faire cela dans notre cas, nous devons juste remettre le mod\xE8le sur le CPU et l\u2019appeler sur notre batch. Le batch retourn\xE9 par le "),Sa=l("code"),Fd=a("DataLoader"),Gd=a(" n\u2019a pas encore \xE9t\xE9 d\xE9plac\xE9 sur le GPU :"),Xo=p(),c(Ks.$$.fragment),el=p(),c(Qs.$$.fragment),sl=p(),De=l("p"),Wd=a("Donc, l\u2019image devient plus claire. Au lieu d\u2019avoir une erreur CUDA, nous avons maintenant une "),La=l("code"),Rd=a("IndexError"),Hd=a(" dans le calcul de la perte (donc rien \xE0 voir avec le backward pass, comme nous l\u2019avons dit plus t\xF4t). Plus pr\xE9cis\xE9ment, nous pouvons voir que c\u2019est la cible 2 qui cr\xE9e l\u2019erreur, donc c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre de labels de notre mod\xE8le :"),tl=p(),c(Zs.$$.fragment),rl=p(),c(Xs.$$.fragment),al=p(),sr=l("p"),Bd=a("Avec deux \xE9tiquettes, seuls les 0 et les 1 sont autoris\xE9s comme cibles, mais d\u2019apr\xE8s le message d\u2019erreur, nous avons obtenu un 2. Obtenir un 2 est en fait normal : si nous nous souvenons des noms d\u2019\xE9tiquettes que nous avons extraits plus t\xF4t, il y en avait trois, donc nous avons des indices 0, 1 et 2 dans notre ensemble de donn\xE9es. Le probl\xE8me est que nous n\u2019avons pas indiqu\xE9 cela \xE0 notre mod\xE8le, qui aurait d\xFB \xEAtre cr\xE9\xE9 avec trois \xE9tiquettes. Alors, corrigeons cela !"),nl=p(),c(et.$$.fragment),ol=p(),Te=l("p"),Yd=a("Nous n\u2019incluons pas encore la ligne "),Na=l("code"),Jd=a("trainer.train()"),Kd=a(", pour prendre le temps de v\xE9rifier que tout se passe bien. Si nous demandons un batch et le passons \xE0 notre mod\xE8le, il fonctionne maintenant sans erreur !"),ll=p(),c(st.$$.fragment),il=p(),tr=l("p"),Qd=a("L\u2019\xE9tape suivante consiste alors \xE0 revenir au GPU et \xE0 v\xE9rifier que tout fonctionne encore :"),ul=p(),c(tt.$$.fragment),pl=p(),Se=l("p"),Zd=a("Si vous obtenez toujours une erreur, assurez-vous de red\xE9marrer votre "),Ua=l("em"),Xd=a("notebook"),ec=a(" et d\u2019ex\xE9cuter uniquement la derni\xE8re version du script."),dl=p(),pe=l("h3"),Le=l("a"),Oa=l("span"),c(rt.$$.fragment),sc=p(),Ma=l("span"),tc=a("Ex\xE9cution d'une \xE9tape d'optimisation"),cl=p(),rr=l("p"),rc=a("Maintenant que nous savons que nous pouvons construire des batchs qui passent r\xE9ellement par le mod\xE8le, nous sommes pr\xEAts pour l\u2019\xE9tape suivante du pipeline d\u2019entra\xEEnement : calculer les gradients et effectuer une \xE9tape d\u2019optimisation."),ml=p(),Ne=l("p"),ac=a("La premi\xE8re partie est juste une question d\u2019appel de la m\xE9thode "),Va=l("code"),nc=a("backward()"),oc=a(" sur la perte :"),fl=p(),c(at.$$.fragment),vl=p(),ar=l("p"),lc=a("Il est plut\xF4t rare d\u2019obtenir une erreur \xE0 ce stade, mais si vous en obtenez une, assurez-vous de retourner au CPU pour obtenir un message d\u2019erreur utile."),hl=p(),se=l("p"),ic=a("Pour effectuer l\u2019\xE9tape d\u2019optimisation, il suffit de cr\xE9er le "),Ia=l("code"),uc=a("optimizer"),pc=a(" et d\u2019appeler sa m\xE9thode "),Fa=l("code"),dc=a("step()"),cc=a(" :"),bl=p(),c(nt.$$.fragment),_l=p(),Ue=l("p"),mc=a("Encore une fois, si vous utilisez l\u2019optimiseur par d\xE9faut dans le "),Ga=l("code"),fc=a("Trainer"),vc=a(", vous ne devriez pas avoir d\u2019erreur \xE0 ce stade, mais si vous avez un optimiseur personnalis\xE9, il pourrait y avoir quelques probl\xE8mes \xE0 d\xE9boguer ici. N\u2019oubliez pas de revenir au CPU si vous obtenez une erreur CUDA bizarre \xE0 ce stade. En parlant d\u2019erreurs CUDA, nous avons mentionn\xE9 pr\xE9c\xE9demment un cas particulier. Voyons cela maintenant."),ql=p(),de=l("h3"),Oe=l("a"),Wa=l("span"),c(ot.$$.fragment),hc=p(),Ra=l("span"),bc=a("G\xE9rer les erreurs CUDA hors-m\xE9moire"),$l=p(),Me=l("p"),_c=a("Chaque fois que vous obtenez un message d\u2019erreur qui commence par "),Ha=l("code"),qc=a("RuntimeError : CUDA out of memory"),$c=a(", cela indique que vous \xEAtes \xE0 court de m\xE9moire GPU. Cela n\u2019est pas directement li\xE9 \xE0 votre code, et cela peut arriver avec un script qui fonctionne parfaitement bien. Cette erreur signifie que vous avez essay\xE9 de mettre trop de choses dans la m\xE9moire interne de votre GPU, et que cela a entra\xEEn\xE9 une erreur. Comme pour d\u2019autres erreurs CUDA, vous devrez red\xE9marrer votre noyau pour \xEAtre en mesure d\u2019ex\xE9cuter \xE0 nouveau votre entra\xEEnement."),gl=p(),nr=l("p"),gc=a("Pour r\xE9soudre ce probl\xE8me, il suffit d\u2019utiliser moins d\u2019espace GPU, ce qui est souvent plus facile \xE0 dire qu\u2019\xE0 faire. Tout d\u2019abord, assurez-vous que vous n\u2019avez pas deux mod\xE8les sur le GPU en m\xEAme temps (sauf si cela est n\xE9cessaire pour votre probl\xE8me, bien s\xFBr). Ensuite, vous devriez probablement r\xE9duire la taille de votre batch, car elle affecte directement les tailles de toutes les sorties interm\xE9diaires du mod\xE8le et leurs gradients. Si le probl\xE8me persiste, envisagez d\u2019utiliser une version plus petite de votre mod\xE8le."),kl=p(),c(Ve.$$.fragment),jl=p(),ce=l("h3"),Ie=l("a"),Ba=l("span"),c(lt.$$.fragment),kc=p(),Ya=l("span"),jc=a("\xC9valuation du mod\xE8le"),yl=p(),Fe=l("p"),yc=a("Maintenant que nous avons r\xE9solu tous les probl\xE8mes li\xE9s \xE0 notre code, tout est parfait et l\u2019entra\xEEnement devrait se d\xE9rouler sans probl\xE8me, n\u2019est-ce pas ? Pas si vite ! Si vous ex\xE9cutez la commande "),Ja=l("code"),zc=a("trainer.train()"),Ec=a(", tout aura l\u2019air bien au d\xE9but, mais apr\xE8s un moment vous obtiendrez ce qui suit :"),zl=p(),c(it.$$.fragment),El=p(),c(ut.$$.fragment),wl=p(),or=l("p"),wc=a("Vous r\xE9aliserez que cette erreur appara\xEEt pendant la phase d\u2019\xE9valuation, donc c\u2019est la derni\xE8re chose que nous aurons besoin de d\xE9boguer."),xl=p(),Ge=l("p"),xc=a("Vous pouvez ex\xE9cuter la boucle d\u2019\xE9valuation du "),Ka=l("code"),Cc=a("Trainer"),Pc=a(" ind\xE9pendamment de l\u2019entra\xEEnement comme ceci :"),Cl=p(),c(pt.$$.fragment),Pl=p(),c(dt.$$.fragment),Al=p(),c(We.$$.fragment),Dl=p(),lr=l("p"),Ac=a("Avant de tenter de d\xE9boguer un probl\xE8me dans la boucle d\u2019\xE9valuation, vous devez d\u2019abord vous assurer que vous avez examin\xE9 les donn\xE9es, que vous \xEAtes en mesure de former un batch correctement et que vous pouvez ex\xE9cuter votre mod\xE8le sur ces donn\xE9es. Nous avons effectu\xE9 toutes ces \xE9tapes, et le code suivant peut donc \xEAtre ex\xE9cut\xE9 sans erreur :"),Tl=p(),c(ct.$$.fragment),Sl=p(),Re=l("p"),Dc=a("L\u2019erreur survient plus tard, \xE0 la fin de la phase d\u2019\xE9valuation, et si nous regardons la "),Qa=l("em"),Tc=a("traceback"),Sc=a(", nous voyons ceci :"),Ll=p(),c(mt.$$.fragment),Nl=p(),I=l("p"),Lc=a("Cela nous indique que l\u2019erreur provient du module "),Za=l("code"),Nc=a("datasets/metric.py"),Uc=a(" donc c\u2019est un probl\xE8me avec notre fonction "),Xa=l("code"),Oc=a("compute_metrics()"),Mc=a(". Elle prend un "),en=l("em"),Vc=a("tuple"),Ic=a(" avec les logits et les labels sous forme de tableaux NumPy, alors essayons de lui fournir cela :"),Ul=p(),c(ft.$$.fragment),Ol=p(),c(vt.$$.fragment),Ml=p(),F=l("p"),Fc=a("Nous obtenons la m\xEAme erreur, donc le probl\xE8me vient bien de cette fonction. Si on regarde son code, on voit qu\u2019elle transmet simplement les "),sn=l("code"),Gc=a("predictions"),Wc=a(" et les "),tn=l("code"),Rc=a("labels"),Hc=a(" \xE0 "),rn=l("code"),Bc=a("metric.compute()"),Yc=a(". Y a-t-il donc un probl\xE8me avec cette m\xE9thode ? Pas vraiment. Jetons un coup d\u2019oeil rapide aux formes :"),Vl=p(),c(ht.$$.fragment),Il=p(),c(bt.$$.fragment),Fl=p(),He=l("p"),Jc=a("Nos pr\xE9dictions sont toujours des logits, et non les pr\xE9dictions r\xE9elles, c\u2019est pourquoi la m\xE9trique retourne cette erreur (quelque peu obscure). La correction est assez simple, il suffit d\u2019ajouter un argmax dans la fonction "),an=l("code"),Kc=a("compute_metrics()"),Qc=a(" :"),Gl=p(),c(_t.$$.fragment),Wl=p(),c(qt.$$.fragment),Rl=p(),ir=l("p"),Zc=a("Maintenant notre erreur est corrig\xE9e ! C\u2019\xE9tait la derni\xE8re, donc notre script va maintenant entra\xEEner un mod\xE8le correctement."),Hl=p(),ur=l("p"),Xc=a("Pour r\xE9f\xE9rence, voici le script compl\xE8tement corrig\xE9 :"),Bl=p(),c($t.$$.fragment),Yl=p(),pr=l("p"),em=a("Dans ce cas, il n\u2019y a plus de probl\xE8me, et notre script va affiner un mod\xE8le qui devrait donner des r\xE9sultats raisonnables. Mais que faire lorsque l\u2019entra\xEEnement se d\xE9roule sans erreur, et que le mod\xE8le entra\xEEn\xE9 n\u2019est pas du tout performant ? C\u2019est la partie la plus difficile de l\u2019apprentissage automatique, et nous allons vous montrer quelques techniques qui peuvent vous aider."),Jl=p(),c(Be.$$.fragment),Kl=p(),me=l("h2"),Ye=l("a"),nn=l("span"),c(gt.$$.fragment),sm=p(),on=l("span"),tm=a("D\xE9boguer les erreurs silencieuses pendant l'entra\xEEnement"),Ql=p(),dr=l("p"),rm=a("Que peut-on faire pour d\xE9boguer un entra\xEEnement qui se termine sans erreur mais qui ne donne pas de bons r\xE9sultats ? Nous allons vous donner quelques pistes ici, mais sachez que ce type de d\xE9bogage est la partie la plus difficile de l\u2019apprentissage automatique, et qu\u2019il n\u2019y a pas de r\xE9ponse magique."),Zl=p(),fe=l("h3"),Je=l("a"),ln=l("span"),c(kt.$$.fragment),am=p(),un=l("span"),nm=a("V\xE9rifiez vos donn\xE9es (encore !)"),Xl=p(),cr=l("p"),om=a("Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. Si un bogue corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre ensemble de donn\xE9es. Commencez donc toujours par rev\xE9rifier vos entr\xE9es et \xE9tiquettes d\xE9cod\xE9es, et posez-vous les questions suivantes :"),ei=p(),G=l("ul"),pn=l("li"),lm=a("les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),im=p(),dn=l("li"),um=a("\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),pm=p(),cn=l("li"),dm=a("y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),cm=p(),mn=l("li"),mm=a("quelle devrait \xEAtre la perte/m\xE9trie si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),si=p(),c(Ke.$$.fragment),ti=p(),mr=l("p"),fm=a("Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le et d\xE9codez-les \xE9galement. Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, c\u2019est peut-\xEAtre parce que votre ensemble de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification) ; des techniques comme le sur\xE9chantillonnage de classes rares peuvent aider."),ri=p(),fr=l("p"),vm=a("Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez \xE0 nouveau la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e, car il y a probablement un bug \xE0 ce niveau. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),ai=p(),vr=l("p"),hm=a("Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),ni=p(),ve=l("h3"),Qe=l("a"),fn=l("span"),c(jt.$$.fragment),bm=p(),vn=l("span"),_m=a("Surentra\xEEnement du mod\xE8le sur un seul batch"),oi=p(),hr=l("p"),qm=a("Le surentra\xEEnement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement, car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse, mais qu\u2019il se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),li=p(),Ze=l("p"),$m=a("Une fois que vous avez d\xE9fini votre "),hn=l("code"),gm=a("Trainer"),km=a(", c\u2019est tr\xE8s facile ; il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis d\u2019ex\xE9cuter une petite boucle d\u2019entra\xEEnement manuel en utilisant uniquement ce batch pour quelque chose comme 20 \xE9tapes :"),ii=p(),c(yt.$$.fragment),ui=p(),c(Xe.$$.fragment),pi=p(),es=l("p"),jm=a("Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le m\xEAme "),bn=l("code"),ym=a("batch"),zm=a(". Calculons la m\xE9trique sur les pr\xE9dictions r\xE9sultantes :"),di=p(),c(zt.$$.fragment),ci=p(),c(Et.$$.fragment),mi=p(),br=l("p"),Em=a("100% de pr\xE9cision, voil\xE0 un bel exemple de surentra\xEEnement (ce qui signifie que si vous essayez votre mod\xE8le sur n\u2019importe quelle autre phrase, il vous donnera tr\xE8s probablement une mauvaise r\xE9ponse) !"),fi=p(),_r=l("p"),wm=a("Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es, et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),vi=p(),c(ss.$$.fragment),hi=p(),he=l("h3"),ts=l("a"),_n=l("span"),c(wt.$$.fragment),xm=p(),qn=l("span"),Cm=a("Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base."),bi=p(),rs=l("p"),Pm=a("Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique, mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),$n=l("code"),Am=a("Trainer"),Dm=a(" fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats, donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),_i=p(),qr=l("p"),Tm=a("Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 l\u2019affiner un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres, mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),qi=p(),$r=l("p"),Sm=a("Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),$i=p(),be=l("h3"),as=l("a"),gn=l("span"),c(xt.$$.fragment),Lm=p(),kn=l("span"),Nm=a("Demander de l'aide"),gi=p(),ns=l("p"),Um=a("Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me, mais si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),Ct=l("a"),Om=a("forum"),Mm=a("."),ki=p(),gr=l("p"),Vm=a("Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),ji=p(),W=l("ul"),kr=l("li"),Pt=l("a"),Im=a("\u201CLa reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie\u201D"),Fm=a(" par Joel Grus"),Gm=p(),jr=l("li"),At=l("a"),Wm=a("\u201CListe de contr\xF4le pour le d\xE9bogage des r\xE9seaux neuronaux\u201D"),Rm=a(" par Cecelia Shao"),Hm=p(),yr=l("li"),Dt=l("a"),Bm=a("\u201CComment tester unitairement le code d\u2019apprentissage automatique\u201D"),Ym=a(" par Chase Roberts"),Jm=p(),zr=l("li"),Tt=l("a"),Km=a("\u201CUne recette pour Entra\xEEner les r\xE9seaux neuronaux\u201D"),Qm=a(" par Andrej Karpathy"),yi=p(),te=l("p"),Zm=a("Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019Entra\xEEnement des r\xE9seaux neuronaux ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),jn=l("em"),Xm=a("Transformers"),ef=a(" ou \u{1F917} "),yn=l("em"),sf=a("Datasets"),tf=a(" qui ne semble pas correct, vous avez peut-\xEAtre rencontr\xE9 un bogue. Vous devez absolument nous en parler, et dans la section suivante, nous allons vous expliquer exactement comment faire."),this.h()},l(e){const r=sb('[data-svelte="svelte-1phssyn"]',document.head);_=i(r,"META",{name:!0,content:!0}),r.forEach(s),j=d(e),m(g.$$.fragment,e),k=d(e),E=i(e,"H1",{class:!0});var St=u(E);y=i(St,"A",{id:!0,class:!0,href:!0});var zn=u(y);z=i(zn,"SPAN",{});var En=u(z);m(w.$$.fragment,En),En.forEach(s),zn.forEach(s),D=d(St),x=i(St,"SPAN",{});var wn=u(x);B=n(wn,"D\xE9bogage du pipeline d'entra\xEEnement"),wn.forEach(s),St.forEach(s),U=d(e),m(C.$$.fragment,e),cs=d(e),T=i(e,"P",{});var H=u(T);Nt=n(H,"Vous avez \xE9crit un magnifique script pour entra\xEEner ou "),Lr=i(H,"EM",{});var xn=u(Lr);lu=n(xn,"finetuner"),xn.forEach(s),iu=n(H," un mod\xE8le sur une t\xE2che donn\xE9e, en suivant consciencieusement les conseils du "),Ut=i(H,"A",{href:!0});var Cn=u(Ut);uu=n(Cn,"Chapitre 7"),Cn.forEach(s),pu=n(H,". Mais lorsque vous lancez la commande "),Nr=i(H,"CODE",{});var Pn=u(Nr);du=n(Pn,"model.fit()"),Pn.forEach(s),cu=n(H,", quelque chose d\u2019horrible se produit : vous obtenez une erreur \u{1F631} ! Ou pire, tout semble aller bien et l\u2019entra\xEEnement se d\xE9roule sans erreur, mais le mod\xE8le r\xE9sultant est merdique. Dans cette section, nous allons vous montrer ce que vous pouvez faire pour d\xE9boguer ce genre de probl\xE8mes."),H.forEach(s),An=d(e),oe=i(e,"H2",{class:!0});var Ei=u(oe);_e=i(Ei,"A",{id:!0,class:!0,href:!0});var lf=u(_e);Ur=i(lf,"SPAN",{});var uf=u(Ur);m(ms.$$.fragment,uf),uf.forEach(s),lf.forEach(s),mu=d(Ei),Or=i(Ei,"SPAN",{});var pf=u(Or);fu=n(pf,"D\xE9boguer le pipeline d'entra\xEEnement"),pf.forEach(s),Ei.forEach(s),Dn=d(e),m(fs.$$.fragment,e),Tn=d(e),Y=i(e,"P",{});var Er=u(Y);vu=n(Er,"Le probl\xE8me lorsque vous rencontrez une erreur dans "),Mr=i(Er,"CODE",{});var df=u(Mr);hu=n(df,"trainer.train()"),df.forEach(s),bu=n(Er," est qu\u2019elle peut provenir de plusieurs sources, car le "),Vr=i(Er,"CODE",{});var cf=u(Vr);_u=n(cf,"Trainer"),cf.forEach(s),qu=n(Er," assemble g\xE9n\xE9ralement des batchs de choses. Il convertit les jeux de donn\xE9es en chargeurs de donn\xE9es, donc le probl\xE8me pourrait \xEAtre quelque chose d\u2019erron\xE9 dans votre jeu de donn\xE9es, ou un probl\xE8me en essayant de regrouper les \xE9l\xE9ments des jeux de donn\xE9es ensemble. Ensuite, il prend un batch de donn\xE9es et le transmet au mod\xE8le, le probl\xE8me peut donc se situer dans le code du mod\xE8le. Apr\xE8s cela, il calcule les gradients et effectue l\u2019\xE9tape d\u2019optimisation, le probl\xE8me peut donc \xE9galement se situer dans votre optimiseur. Et m\xEAme si tout se passe bien pendant l\u2019entra\xEEnement, quelque chose peut encore mal tourner pendant l\u2019\xE9valuation si votre m\xE9trique pose probl\xE8me."),Er.forEach(s),Sn=d(e),qe=i(e,"P",{});var wi=u(qe);$u=n(wi,"La meilleure fa\xE7on de d\xE9boguer une erreur qui survient dans "),Ir=i(wi,"CODE",{});var mf=u(Ir);gu=n(mf,"trainer.train()"),mf.forEach(s),ku=n(wi," est de passer manuellement en revue tout le pipeline pour voir o\xF9 les choses se sont mal pass\xE9es. L\u2019erreur est alors souvent tr\xE8s facile \xE0 r\xE9soudre."),wi.forEach(s),Ln=d(e),$e=i(e,"P",{});var xi=u($e);ju=n(xi,"Pour le d\xE9montrer, nous utiliserons le script suivant qui tente d\u2019ajuster un mod\xE8le DistilBERT sur le "),vs=i(xi,"A",{href:!0,rel:!0});var ff=u(vs);yu=n(ff,"jeu de donn\xE9es MNLI"),ff.forEach(s),zu=n(xi," :"),xi.forEach(s),Nn=d(e),m(hs.$$.fragment,e),Un=d(e),Ot=i(e,"P",{});var vf=u(Ot);Eu=n(vf,"Si vous essayez de l\u2019ex\xE9cuter, vous serez confront\xE9 \xE0 une erreur plut\xF4t cryptique :"),vf.forEach(s),On=d(e),m(bs.$$.fragment,e),Mn=d(e),le=i(e,"H3",{class:!0});var Ci=u(le);ge=i(Ci,"A",{id:!0,class:!0,href:!0});var hf=u(ge);Fr=i(hf,"SPAN",{});var bf=u(Fr);m(_s.$$.fragment,bf),bf.forEach(s),hf.forEach(s),wu=d(Ci),Gr=i(Ci,"SPAN",{});var _f=u(Gr);xu=n(_f,"V\xE9rifiez vos donn\xE9es"),_f.forEach(s),Ci.forEach(s),Vn=d(e),ke=i(e,"P",{});var Pi=u(ke);Cu=n(Pi,"Cela va sans dire, mais si vos donn\xE9es sont corrompues, le "),Wr=i(Pi,"CODE",{});var qf=u(Wr);Pu=n(qf,"Trainer"),qf.forEach(s),Au=n(Pi," ne sera pas capable de former des batchs, et encore moins d\u2019entra\xEEner votre mod\xE8le. Donc, tout d\u2019abord, vous devez jeter un coup d\u2019oeil \xE0 ce qui se trouve dans votre ensemble d\u2019entra\xEEnement."),Pi.forEach(s),In=d(e),je=i(e,"P",{});var Ai=u(je);Du=n(Ai,"Pour \xE9viter d\u2019innombrables heures pass\xE9es \xE0 essayer de corriger quelque chose qui n\u2019est pas la source du bug, nous vous recommandons d\u2019utiliser "),Rr=i(Ai,"CODE",{});var $f=u(Rr);Tu=n($f,"trainer.train_dataset"),$f.forEach(s),Su=n(Ai," pour vos v\xE9rifications et rien d\u2019autre. Faisons donc cela ici :"),Ai.forEach(s),Fn=d(e),m(qs.$$.fragment,e),Gn=d(e),m($s.$$.fragment,e),Wn=d(e),J=i(e,"P",{});var wr=u(J);Lu=n(wr,"Vous remarquez quelque chose d\u2019anormal ? Ceci, en conjonction avec le message d\u2019erreur sur les "),Hr=i(wr,"CODE",{});var gf=u(Hr);Nu=n(gf,"input_ids"),gf.forEach(s),Uu=n(wr," manquants, devrait vous faire r\xE9aliser que ce sont des textes, et non des nombres que le mod\xE8le peut comprendre. Ici, l\u2019erreur originale est tr\xE8s trompeuse parce que le "),Br=i(wr,"CODE",{});var kf=u(Br);Ou=n(kf,"Trainer"),kf.forEach(s),Mu=n(wr," enl\xE8ve automatiquement les colonnes qui ne correspondent pas \xE0 la signature du mod\xE8le (c\u2019est-\xE0-dire, les arguments attendus par le mod\xE8le). Cela signifie qu\u2019ici, tout, sauf les \xE9tiquettes, a \xE9t\xE9 \xE9limin\xE9. Il n\u2019y avait donc aucun probl\xE8me \xE0 cr\xE9er des batchs et \xE0 les envoyer ensuite au mod\xE8le, qui s\u2019est plaint \xE0 son tour de ne pas avoir re\xE7u les bons arguments."),wr.forEach(s),Rn=d(e),A=i(e,"P",{});var R=u(A);Vu=n(R,"Pourquoi les donn\xE9es n\u2019ont-elles pas \xE9t\xE9 trait\xE9es ? Nous avons utilis\xE9 la m\xE9thode "),Yr=i(R,"CODE",{});var jf=u(Yr);Iu=n(jf,"Dataset.map()"),jf.forEach(s),Fu=n(R," sur les ensembles de donn\xE9es pour appliquer le "),Jr=i(R,"EM",{});var yf=u(Jr);Gu=n(yf,"tokenizer"),yf.forEach(s),Wu=n(R," sur chaque \xE9chantillon. Mais si vous regardez attentivement le code, vous verrez que nous avons fait une erreur en passant les ensembles d\u2019entra\xEEnement et d\u2019\xE9valuation au "),Kr=i(R,"CODE",{});var zf=u(Kr);Ru=n(zf,"Trainer"),zf.forEach(s),Hu=n(R,". Au lieu d\u2019utiliser "),Qr=i(R,"CODE",{});var Ef=u(Qr);Bu=n(Ef,"tokenized_datasets"),Ef.forEach(s),Yu=n(R," ici, nous avons utilis\xE9 "),Zr=i(R,"CODE",{});var wf=u(Zr);Ju=n(wf,"raw_datasets"),wf.forEach(s),Ku=n(R," \u{1F926}. Alors corrigeons \xE7a !"),R.forEach(s),Hn=d(e),m(gs.$$.fragment,e),Bn=d(e),Mt=i(e,"P",{});var xf=u(Mt);Qu=n(xf,"Ce nouveau code donnera maintenant une erreur diff\xE9rente (progr\xE8s !) :"),xf.forEach(s),Yn=d(e),m(ks.$$.fragment,e),Jn=d(e),Vt=i(e,"P",{});var Cf=u(Vt);Zu=n(Cf,"En regardant la trace, nous pouvons voir que l\u2019erreur se produit dans l\u2019\xE9tape de collationnement des donn\xE9es :"),Cf.forEach(s),Kn=d(e),m(js.$$.fragment,e),Qn=d(e),It=i(e,"P",{});var Pf=u(It);Xu=n(Pf,"Donc, nous devrions passer \xE0 cela. Mais avant cela, finissons d\u2019inspecter nos donn\xE9es, pour \xEAtre s\xFBrs \xE0 100% qu\u2019elles sont correctes."),Pf.forEach(s),Zn=d(e),ye=i(e,"P",{});var Di=u(ye);ep=n(Di,"Une chose que vous devriez toujours faire lorsque vous d\xE9boguez une session d\u2019entra\xEEnement est de jeter un coup d\u2019oeil aux entr\xE9es d\xE9cod\xE9es de votre mod\xE8le. Nous ne pouvons pas donner un sens aux chiffres que nous lui fournissons directement, nous devons donc examiner ce que ces chiffres repr\xE9sentent. Dans le domaine de la vision par ordinateur, par exemple, cela signifie regarder les images d\xE9cod\xE9es des pixels que vous passez, dans le domaine de la parole, cela signifie \xE9couter les \xE9chantillons audio d\xE9cod\xE9s, et pour notre exemple NLP, cela signifie utiliser notre "),Xr=i(Di,"EM",{});var Af=u(Xr);sp=n(Af,"tokenizer"),Af.forEach(s),tp=n(Di," pour d\xE9coder les entr\xE9es :"),Di.forEach(s),Xn=d(e),m(ys.$$.fragment,e),eo=d(e),m(zs.$$.fragment,e),so=d(e),Ft=i(e,"P",{});var Df=u(Ft);rp=n(Df,"Cela semble donc correct. Vous devriez faire cela pour toutes les cl\xE9s dans les entr\xE9es :"),Df.forEach(s),to=d(e),m(Es.$$.fragment,e),ro=d(e),m(ws.$$.fragment,e),ao=d(e),S=i(e,"P",{});var re=u(S);ap=n(re,"Note that the keys that don\u2019t correspond to inputs accepted by the model will be automatically discarded, so here we will only keep "),ea=i(re,"CODE",{});var Tf=u(ea);np=n(Tf,"input_ids"),Tf.forEach(s),op=n(re,", "),sa=i(re,"CODE",{});var Sf=u(sa);lp=n(Sf,"attention_mask"),Sf.forEach(s),ip=n(re,", and "),ta=i(re,"CODE",{});var Lf=u(ta);up=n(Lf,"label"),Lf.forEach(s),pp=n(re," (which will be renamed "),ra=i(re,"CODE",{});var Nf=u(ra);dp=n(Nf,"labels"),Nf.forEach(s),cp=n(re,"). To double-check the model signature, you can print the class of your model, then go check its documentation:"),re.forEach(s),no=d(e),m(xs.$$.fragment,e),oo=d(e),m(Cs.$$.fragment,e),lo=d(e),K=i(e,"P",{});var xr=u(K);mp=n(xr,"Donc dans notre cas, nous pouvons v\xE9rifier les param\xE8tres accept\xE9s sur "),Ps=i(xr,"A",{href:!0,rel:!0});var Uf=u(Ps);fp=n(Uf,"cette page"),Uf.forEach(s),vp=n(xr,". Le "),aa=i(xr,"CODE",{});var Of=u(aa);hp=n(Of,"Trainer"),Of.forEach(s),bp=n(xr," va \xE9galement enregistrer les colonnes qu\u2019il rejette."),xr.forEach(s),io=d(e),ze=i(e,"P",{});var Ti=u(ze);_p=n(Ti,"Nous avons v\xE9rifi\xE9 que les IDs d\u2019entr\xE9e sont corrects en les d\xE9codant. Ensuite, il y a le "),na=i(Ti,"CODE",{});var Mf=u(na);qp=n(Mf,"attention_mask"),Mf.forEach(s),$p=n(Ti," :"),Ti.forEach(s),uo=d(e),m(As.$$.fragment,e),po=d(e),m(Ds.$$.fragment,e),co=d(e),Ee=i(e,"P",{});var Si=u(Ee);gp=n(Si,"Comme nous n\u2019avons pas appliqu\xE9 de "),oa=i(Si,"EM",{});var Vf=u(oa);kp=n(Vf,"padding"),Vf.forEach(s),jp=n(Si," dans notre pr\xE9traitement, cela semble parfaitement naturel. Pour \xEAtre s\xFBr qu\u2019il n\u2019y a pas de probl\xE8me avec ce masque d\u2019attention, v\xE9rifions qu\u2019il est de la m\xEAme longueur que nos identifiants d\u2019entr\xE9e :"),Si.forEach(s),mo=d(e),m(Ts.$$.fragment,e),fo=d(e),m(Ss.$$.fragment,e),vo=d(e),Gt=i(e,"P",{});var If=u(Gt);yp=n(If,"C\u2019est bien ! Enfin, v\xE9rifions notre \xE9tiquette :"),If.forEach(s),ho=d(e),m(Ls.$$.fragment,e),bo=d(e),m(Ns.$$.fragment,e),_o=d(e),Q=i(e,"P",{});var Cr=u(Q);zp=n(Cr,"Comme les ID d\u2019entr\xE9e, c\u2019est un nombre qui n\u2019a pas vraiment de sens en soi. Comme nous l\u2019avons vu pr\xE9c\xE9demment, la correspondance entre les entiers et les noms d\u2019\xE9tiquettes est stock\xE9e dans l\u2019attribut "),la=i(Cr,"CODE",{});var Ff=u(la);Ep=n(Ff,"names"),Ff.forEach(s),wp=n(Cr," de la "),ia=i(Cr,"EM",{});var Gf=u(ia);xp=n(Gf,"caract\xE9ristique"),Gf.forEach(s),Cp=n(Cr," correspondante de l\u2019ensemble de donn\xE9es :"),Cr.forEach(s),qo=d(e),m(Us.$$.fragment,e),$o=d(e),m(Os.$$.fragment,e),go=d(e),Z=i(e,"P",{});var Pr=u(Z);Pp=n(Pr,"Donc "),ua=i(Pr,"CODE",{});var Wf=u(ua);Ap=n(Wf,"1"),Wf.forEach(s),Dp=n(Pr," signifie "),pa=i(Pr,"CODE",{});var Rf=u(pa);Tp=n(Rf,"neutral"),Rf.forEach(s),Sp=n(Pr,", ce qui signifie que les deux phrases que nous avons vues ci-dessus ne sont pas en contradiction, et que la premi\xE8re n\u2019implique pas la seconde. Cela semble correct !"),Pr.forEach(s),ko=d(e),we=i(e,"P",{});var Li=u(we);Lp=n(Li,"Nous n\u2019avons pas d\u2019ID de type de "),da=i(Li,"EM",{});var Hf=u(da);Np=n(Hf,"token"),Hf.forEach(s),Up=n(Li," ici, puisque DistilBERT ne les attend pas ; si vous en avez dans votre mod\xE8le, vous devriez \xE9galement vous assurer qu\u2019ils correspondent correctement \xE0 l\u2019endroit o\xF9 se trouvent la premi\xE8re et la deuxi\xE8me phrase dans l\u2019entr\xE9e."),Li.forEach(s),jo=d(e),m(xe.$$.fragment,e),yo=d(e),Wt=i(e,"P",{});var Bf=u(Wt);Op=n(Bf,"Nous ne v\xE9rifions ici que l\u2019ensemble d\u2019entra\xEEnement, mais vous devez bien s\xFBr v\xE9rifier de la m\xEAme fa\xE7on les ensembles de validation et de test."),Bf.forEach(s),zo=d(e),Rt=i(e,"P",{});var Yf=u(Rt);Mp=n(Yf,"Maintenant que nous savons que nos ensembles de donn\xE9es sont bons, il est temps de v\xE9rifier l\u2019\xE9tape suivante du pipeline d\u2019entra\xEEnement."),Yf.forEach(s),Eo=d(e),ie=i(e,"H3",{class:!0});var Ni=u(ie);Ce=i(Ni,"A",{id:!0,class:!0,href:!0});var Jf=u(Ce);ca=i(Jf,"SPAN",{});var Kf=u(ca);m(Ms.$$.fragment,Kf),Kf.forEach(s),Jf.forEach(s),Vp=d(Ni),ma=i(Ni,"SPAN",{});var Qf=u(ma);Ip=n(Qf,"Des jeux de donn\xE9es aux chargeurs de donn\xE9es"),Qf.forEach(s),Ni.forEach(s),wo=d(e),L=i(e,"P",{});var ae=u(L);Fp=n(ae,"La prochaine chose qui peut mal tourner dans le pipeline d\u2019entra\xEEnement est lorsque le "),fa=i(ae,"CODE",{});var Zf=u(fa);Gp=n(Zf,"Trainer"),Zf.forEach(s),Wp=n(ae," essaie de former des batchs \xE0 partir de l\u2019ensemble d\u2019entra\xEEnement ou de validation. Une fois que vous \xEAtes s\xFBr que les jeux de donn\xE9es du "),va=i(ae,"CODE",{});var Xf=u(va);Rp=n(Xf,"Trainer"),Xf.forEach(s),Hp=n(ae," sont corrects, vous pouvez essayer de former manuellement un batch en ex\xE9cutant ce qui suit (remplacez "),ha=i(ae,"CODE",{});var ev=u(ha);Bp=n(ev,"train"),ev.forEach(s),Yp=n(ae," par "),ba=i(ae,"CODE",{});var sv=u(ba);Jp=n(sv,"eval"),sv.forEach(s),Kp=n(ae," pour le dataloader de validation) :"),ae.forEach(s),xo=d(e),m(Vs.$$.fragment,e),Co=d(e),Ht=i(e,"P",{});var tv=u(Ht);Qp=n(tv,"Ce code cr\xE9e le dataloader d\u2019entra\xEEnement, puis le parcourt en s\u2019arr\xEAtant \xE0 la premi\xE8re it\xE9ration. Si le code s\u2019ex\xE9cute sans erreur, vous avez le premier batch d\u2019entra\xEEnement que vous pouvez inspecter, et si le code se trompe, vous \xEAtes s\xFBr que le probl\xE8me se situe dans le dataloader, comme c\u2019est le cas ici :"),tv.forEach(s),Po=d(e),m(Is.$$.fragment,e),Ao=d(e),X=i(e,"P",{});var Ar=u(X);Zp=n(Ar,"L\u2019inspection de la derni\xE8re image du traceback devrait suffire \xE0 vous donner un indice, mais creusons un peu plus. La plupart des probl\xE8mes lors de la cr\xE9ation d\u2019un batch sont dus \xE0 l\u2019assemblage des exemples en un seul batch, donc la premi\xE8re chose \xE0 v\xE9rifier en cas de doute est le "),_a=i(Ar,"CODE",{});var rv=u(_a);Xp=n(rv,"collate_fn"),rv.forEach(s),ed=n(Ar," utilis\xE9 par votre "),qa=i(Ar,"CODE",{});var av=u(qa);sd=n(av,"DataLoader"),av.forEach(s),td=n(Ar," :"),Ar.forEach(s),Do=d(e),m(Fs.$$.fragment,e),To=d(e),m(Gs.$$.fragment,e),So=d(e),M=i(e,"P",{});var os=u(M);rd=n(os,"C\u2019est donc le collateur "),$a=i(os,"CODE",{});var nv=u($a);ad=n(nv,"default_data_collator"),nv.forEach(s),nd=n(os,", mais ce n\u2019est pas ce que nous voulons dans ce cas. Nous voulons rembourrer nos exemples \xE0 la phrase la plus longue du batch, ce qui est fait par le collateur "),ga=i(os,"CODE",{});var ov=u(ga);od=n(ov,"DataCollatorWithPadding"),ov.forEach(s),ld=n(os,". Et ce collateur de donn\xE9es est cens\xE9 \xEAtre utilis\xE9 par d\xE9faut par le "),ka=i(os,"CODE",{});var lv=u(ka);id=n(lv,"Trainer"),lv.forEach(s),ud=n(os,", alors pourquoi n\u2019est-il pas utilis\xE9 ici ?"),os.forEach(s),Lo=d(e),V=i(e,"P",{});var ls=u(V);pd=n(ls,"La r\xE9ponse est que nous n\u2019avons pas pass\xE9 le "),ja=i(ls,"CODE",{});var iv=u(ja);dd=n(iv,"tokenizer"),iv.forEach(s),cd=n(ls," au "),ya=i(ls,"CODE",{});var uv=u(ya);md=n(uv,"Trainer"),uv.forEach(s),fd=n(ls,", donc il ne pouvait pas cr\xE9er le "),za=i(ls,"CODE",{});var pv=u(za);vd=n(pv,"DataCollatorWithPadding"),pv.forEach(s),hd=n(ls," que nous voulons. En pratique, il ne faut jamais h\xE9siter \xE0 transmettre explicitement le collateur de donn\xE9es que l\u2019on veut utiliser, pour \xEAtre s\xFBr d\u2019\xE9viter ce genre d\u2019erreurs. Adaptons notre code pour faire exactement cela :"),ls.forEach(s),No=d(e),m(Ws.$$.fragment,e),Uo=d(e),Bt=i(e,"P",{});var dv=u(Bt);bd=n(dv,"La bonne nouvelle ? Nous n\u2019avons plus la m\xEAme erreur qu\u2019avant, ce qui est un progr\xE8s certain. La mauvaise nouvelle ? Nous obtenons une erreur CUDA inf\xE2me \xE0 la place :"),dv.forEach(s),Oo=d(e),m(Rs.$$.fragment,e),Mo=d(e),Yt=i(e,"P",{});var cv=u(Yt);_d=n(cv,"C\u2019est une mauvaise chose car les erreurs CUDA sont extr\xEAmement difficiles \xE0 d\xE9boguer en g\xE9n\xE9ral. Nous verrons dans une minute comment r\xE9soudre ce probl\xE8me, mais terminons d\u2019abord notre analyse de la cr\xE9ation de batchs."),cv.forEach(s),Vo=d(e),Jt=i(e,"P",{});var mv=u(Jt);qd=n(mv,"Si vous \xEAtes s\xFBr que votre collecteur de donn\xE9es est le bon, vous devriez essayer de l\u2019appliquer sur quelques \xE9chantillons de votre ensemble de donn\xE9es :"),mv.forEach(s),Io=d(e),m(Hs.$$.fragment,e),Fo=d(e),N=i(e,"P",{});var ne=u(N);$d=n(ne,"Ce code \xE9chouera parce que le "),Ea=i(ne,"CODE",{});var fv=u(Ea);gd=n(fv,"train_dataset"),fv.forEach(s),kd=n(ne," contient des colonnes de type string, que le "),wa=i(ne,"CODE",{});var vv=u(wa);jd=n(vv,"Trainer"),vv.forEach(s),yd=n(ne," supprime habituellement. Vous pouvez les supprimer manuellement, ou si vous voulez reproduire exactement ce que le "),xa=i(ne,"CODE",{});var hv=u(xa);zd=n(hv,"Trainer"),hv.forEach(s),Ed=n(ne," fait en coulisse, vous pouvez appeler la m\xE9thode priv\xE9e "),Ca=i(ne,"CODE",{});var bv=u(Ca);wd=n(bv,"Trainer._remove_unused_columns()"),bv.forEach(s),xd=n(ne," qui fait cela :"),ne.forEach(s),Go=d(e),m(Bs.$$.fragment,e),Wo=d(e),Kt=i(e,"P",{});var _v=u(Kt);Cd=n(_v,"Vous devriez alors \xEAtre en mesure de d\xE9boguer manuellement ce qui se passe dans le collecteur de donn\xE9es si l\u2019erreur persiste."),_v.forEach(s),Ro=d(e),Qt=i(e,"P",{});var qv=u(Qt);Pd=n(qv,"Maintenant que nous avons d\xE9bogu\xE9 le processus de cr\xE9ation de batch, il est temps d\u2019en passer un dans le mod\xE8le !"),qv.forEach(s),Ho=d(e),ue=i(e,"H3",{class:!0});var Ui=u(ue);Pe=i(Ui,"A",{id:!0,class:!0,href:!0});var $v=u(Pe);Pa=i($v,"SPAN",{});var gv=u(Pa);m(Ys.$$.fragment,gv),gv.forEach(s),$v.forEach(s),Ad=d(Ui),Aa=i(Ui,"SPAN",{});var kv=u(Aa);Dd=n(kv,"Passage par le mod\xE8le"),kv.forEach(s),Ui.forEach(s),Bo=d(e),Zt=i(e,"P",{});var jv=u(Zt);Td=n(jv,"Vous devriez \xEAtre en mesure d\u2019obtenir un batch en ex\xE9cutant la commande suivante :"),jv.forEach(s),Yo=d(e),m(Js.$$.fragment,e),Jo=d(e),ee=i(e,"P",{});var Dr=u(ee);Sd=n(Dr,"Si vous ex\xE9cutez ce code dans un "),Da=i(Dr,"EM",{});var yv=u(Da);Ld=n(yv,"notebook"),yv.forEach(s),Nd=n(Dr,", vous risquez d\u2019obtenir une erreur CUDA similaire \xE0 celle que nous avons vue pr\xE9c\xE9demment, auquel cas vous devrez red\xE9marrer votre notebook et r\xE9ex\xE9cuter le dernier extrait sans la ligne "),Ta=i(Dr,"CODE",{});var zv=u(Ta);Ud=n(zv,"trainer.train()"),zv.forEach(s),Od=n(Dr,". C\u2019est la deuxi\xE8me chose la plus ennuyeuse \xE0 propos des erreurs CUDA : elles cassent irr\xE9m\xE9diablement votre noyau. La chose la plus ennuyeuse \xE0 leur sujet est le fait qu\u2019elles sont difficiles \xE0 d\xE9boguer."),Dr.forEach(s),Ko=d(e),Xt=i(e,"P",{});var Ev=u(Xt);Md=n(Ev,"Comment cela se fait-il ? Cela tient \xE0 la fa\xE7on dont les GPU fonctionnent. Ils sont extr\xEAmement efficaces pour ex\xE9cuter un batch d\u2019op\xE9rations en parall\xE8le, mais l\u2019inconv\xE9nient est que lorsque l\u2019une de ces instructions entra\xEEne une erreur, vous ne le savez pas imm\xE9diatement. Ce n\u2019est que lorsque le programme appelle une synchronisation des multiples processus sur le GPU qu\u2019il r\xE9alise que quelque chose s\u2019est mal pass\xE9, de sorte que l\u2019erreur est en fait soulev\xE9e \xE0 un endroit qui n\u2019a rien \xE0 voir avec ce qui l\u2019a cr\xE9\xE9e. Par exemple, si nous regardons notre traceback pr\xE9c\xE9dent, l\u2019erreur a \xE9t\xE9 soulev\xE9e pendant la passe arri\xE8re, mais nous verrons dans une minute qu\u2019elle provient en fait de quelque chose dans la passe avant."),Ev.forEach(s),Qo=d(e),er=i(e,"P",{});var wv=u(er);Vd=n(wv,"Alors comment d\xE9boguer ces erreurs ? La r\xE9ponse est simple : nous ne le faisons pas. \xC0 moins que votre erreur CUDA ne soit une erreur out-of-memory (ce qui signifie qu\u2019il n\u2019y a pas assez de m\xE9moire dans votre GPU), vous devez toujours revenir au CPU pour la d\xE9boguer."),wv.forEach(s),Zo=d(e),Ae=i(e,"P",{});var Oi=u(Ae);Id=n(Oi,"Pour faire cela dans notre cas, nous devons juste remettre le mod\xE8le sur le CPU et l\u2019appeler sur notre batch. Le batch retourn\xE9 par le "),Sa=i(Oi,"CODE",{});var xv=u(Sa);Fd=n(xv,"DataLoader"),xv.forEach(s),Gd=n(Oi," n\u2019a pas encore \xE9t\xE9 d\xE9plac\xE9 sur le GPU :"),Oi.forEach(s),Xo=d(e),m(Ks.$$.fragment,e),el=d(e),m(Qs.$$.fragment,e),sl=d(e),De=i(e,"P",{});var Mi=u(De);Wd=n(Mi,"Donc, l\u2019image devient plus claire. Au lieu d\u2019avoir une erreur CUDA, nous avons maintenant une "),La=i(Mi,"CODE",{});var Cv=u(La);Rd=n(Cv,"IndexError"),Cv.forEach(s),Hd=n(Mi," dans le calcul de la perte (donc rien \xE0 voir avec le backward pass, comme nous l\u2019avons dit plus t\xF4t). Plus pr\xE9cis\xE9ment, nous pouvons voir que c\u2019est la cible 2 qui cr\xE9e l\u2019erreur, donc c\u2019est un tr\xE8s bon moment pour v\xE9rifier le nombre de labels de notre mod\xE8le :"),Mi.forEach(s),tl=d(e),m(Zs.$$.fragment,e),rl=d(e),m(Xs.$$.fragment,e),al=d(e),sr=i(e,"P",{});var Pv=u(sr);Bd=n(Pv,"Avec deux \xE9tiquettes, seuls les 0 et les 1 sont autoris\xE9s comme cibles, mais d\u2019apr\xE8s le message d\u2019erreur, nous avons obtenu un 2. Obtenir un 2 est en fait normal : si nous nous souvenons des noms d\u2019\xE9tiquettes que nous avons extraits plus t\xF4t, il y en avait trois, donc nous avons des indices 0, 1 et 2 dans notre ensemble de donn\xE9es. Le probl\xE8me est que nous n\u2019avons pas indiqu\xE9 cela \xE0 notre mod\xE8le, qui aurait d\xFB \xEAtre cr\xE9\xE9 avec trois \xE9tiquettes. Alors, corrigeons cela !"),Pv.forEach(s),nl=d(e),m(et.$$.fragment,e),ol=d(e),Te=i(e,"P",{});var Vi=u(Te);Yd=n(Vi,"Nous n\u2019incluons pas encore la ligne "),Na=i(Vi,"CODE",{});var Av=u(Na);Jd=n(Av,"trainer.train()"),Av.forEach(s),Kd=n(Vi,", pour prendre le temps de v\xE9rifier que tout se passe bien. Si nous demandons un batch et le passons \xE0 notre mod\xE8le, il fonctionne maintenant sans erreur !"),Vi.forEach(s),ll=d(e),m(st.$$.fragment,e),il=d(e),tr=i(e,"P",{});var Dv=u(tr);Qd=n(Dv,"L\u2019\xE9tape suivante consiste alors \xE0 revenir au GPU et \xE0 v\xE9rifier que tout fonctionne encore :"),Dv.forEach(s),ul=d(e),m(tt.$$.fragment,e),pl=d(e),Se=i(e,"P",{});var Ii=u(Se);Zd=n(Ii,"Si vous obtenez toujours une erreur, assurez-vous de red\xE9marrer votre "),Ua=i(Ii,"EM",{});var Tv=u(Ua);Xd=n(Tv,"notebook"),Tv.forEach(s),ec=n(Ii," et d\u2019ex\xE9cuter uniquement la derni\xE8re version du script."),Ii.forEach(s),dl=d(e),pe=i(e,"H3",{class:!0});var Fi=u(pe);Le=i(Fi,"A",{id:!0,class:!0,href:!0});var Sv=u(Le);Oa=i(Sv,"SPAN",{});var Lv=u(Oa);m(rt.$$.fragment,Lv),Lv.forEach(s),Sv.forEach(s),sc=d(Fi),Ma=i(Fi,"SPAN",{});var Nv=u(Ma);tc=n(Nv,"Ex\xE9cution d'une \xE9tape d'optimisation"),Nv.forEach(s),Fi.forEach(s),cl=d(e),rr=i(e,"P",{});var Uv=u(rr);rc=n(Uv,"Maintenant que nous savons que nous pouvons construire des batchs qui passent r\xE9ellement par le mod\xE8le, nous sommes pr\xEAts pour l\u2019\xE9tape suivante du pipeline d\u2019entra\xEEnement : calculer les gradients et effectuer une \xE9tape d\u2019optimisation."),Uv.forEach(s),ml=d(e),Ne=i(e,"P",{});var Gi=u(Ne);ac=n(Gi,"La premi\xE8re partie est juste une question d\u2019appel de la m\xE9thode "),Va=i(Gi,"CODE",{});var Ov=u(Va);nc=n(Ov,"backward()"),Ov.forEach(s),oc=n(Gi," sur la perte :"),Gi.forEach(s),fl=d(e),m(at.$$.fragment,e),vl=d(e),ar=i(e,"P",{});var Mv=u(ar);lc=n(Mv,"Il est plut\xF4t rare d\u2019obtenir une erreur \xE0 ce stade, mais si vous en obtenez une, assurez-vous de retourner au CPU pour obtenir un message d\u2019erreur utile."),Mv.forEach(s),hl=d(e),se=i(e,"P",{});var Tr=u(se);ic=n(Tr,"Pour effectuer l\u2019\xE9tape d\u2019optimisation, il suffit de cr\xE9er le "),Ia=i(Tr,"CODE",{});var Vv=u(Ia);uc=n(Vv,"optimizer"),Vv.forEach(s),pc=n(Tr," et d\u2019appeler sa m\xE9thode "),Fa=i(Tr,"CODE",{});var Iv=u(Fa);dc=n(Iv,"step()"),Iv.forEach(s),cc=n(Tr," :"),Tr.forEach(s),bl=d(e),m(nt.$$.fragment,e),_l=d(e),Ue=i(e,"P",{});var Wi=u(Ue);mc=n(Wi,"Encore une fois, si vous utilisez l\u2019optimiseur par d\xE9faut dans le "),Ga=i(Wi,"CODE",{});var Fv=u(Ga);fc=n(Fv,"Trainer"),Fv.forEach(s),vc=n(Wi,", vous ne devriez pas avoir d\u2019erreur \xE0 ce stade, mais si vous avez un optimiseur personnalis\xE9, il pourrait y avoir quelques probl\xE8mes \xE0 d\xE9boguer ici. N\u2019oubliez pas de revenir au CPU si vous obtenez une erreur CUDA bizarre \xE0 ce stade. En parlant d\u2019erreurs CUDA, nous avons mentionn\xE9 pr\xE9c\xE9demment un cas particulier. Voyons cela maintenant."),Wi.forEach(s),ql=d(e),de=i(e,"H3",{class:!0});var Ri=u(de);Oe=i(Ri,"A",{id:!0,class:!0,href:!0});var Gv=u(Oe);Wa=i(Gv,"SPAN",{});var Wv=u(Wa);m(ot.$$.fragment,Wv),Wv.forEach(s),Gv.forEach(s),hc=d(Ri),Ra=i(Ri,"SPAN",{});var Rv=u(Ra);bc=n(Rv,"G\xE9rer les erreurs CUDA hors-m\xE9moire"),Rv.forEach(s),Ri.forEach(s),$l=d(e),Me=i(e,"P",{});var Hi=u(Me);_c=n(Hi,"Chaque fois que vous obtenez un message d\u2019erreur qui commence par "),Ha=i(Hi,"CODE",{});var Hv=u(Ha);qc=n(Hv,"RuntimeError : CUDA out of memory"),Hv.forEach(s),$c=n(Hi,", cela indique que vous \xEAtes \xE0 court de m\xE9moire GPU. Cela n\u2019est pas directement li\xE9 \xE0 votre code, et cela peut arriver avec un script qui fonctionne parfaitement bien. Cette erreur signifie que vous avez essay\xE9 de mettre trop de choses dans la m\xE9moire interne de votre GPU, et que cela a entra\xEEn\xE9 une erreur. Comme pour d\u2019autres erreurs CUDA, vous devrez red\xE9marrer votre noyau pour \xEAtre en mesure d\u2019ex\xE9cuter \xE0 nouveau votre entra\xEEnement."),Hi.forEach(s),gl=d(e),nr=i(e,"P",{});var Bv=u(nr);gc=n(Bv,"Pour r\xE9soudre ce probl\xE8me, il suffit d\u2019utiliser moins d\u2019espace GPU, ce qui est souvent plus facile \xE0 dire qu\u2019\xE0 faire. Tout d\u2019abord, assurez-vous que vous n\u2019avez pas deux mod\xE8les sur le GPU en m\xEAme temps (sauf si cela est n\xE9cessaire pour votre probl\xE8me, bien s\xFBr). Ensuite, vous devriez probablement r\xE9duire la taille de votre batch, car elle affecte directement les tailles de toutes les sorties interm\xE9diaires du mod\xE8le et leurs gradients. Si le probl\xE8me persiste, envisagez d\u2019utiliser une version plus petite de votre mod\xE8le."),Bv.forEach(s),kl=d(e),m(Ve.$$.fragment,e),jl=d(e),ce=i(e,"H3",{class:!0});var Bi=u(ce);Ie=i(Bi,"A",{id:!0,class:!0,href:!0});var Yv=u(Ie);Ba=i(Yv,"SPAN",{});var Jv=u(Ba);m(lt.$$.fragment,Jv),Jv.forEach(s),Yv.forEach(s),kc=d(Bi),Ya=i(Bi,"SPAN",{});var Kv=u(Ya);jc=n(Kv,"\xC9valuation du mod\xE8le"),Kv.forEach(s),Bi.forEach(s),yl=d(e),Fe=i(e,"P",{});var Yi=u(Fe);yc=n(Yi,"Maintenant que nous avons r\xE9solu tous les probl\xE8mes li\xE9s \xE0 notre code, tout est parfait et l\u2019entra\xEEnement devrait se d\xE9rouler sans probl\xE8me, n\u2019est-ce pas ? Pas si vite ! Si vous ex\xE9cutez la commande "),Ja=i(Yi,"CODE",{});var Qv=u(Ja);zc=n(Qv,"trainer.train()"),Qv.forEach(s),Ec=n(Yi,", tout aura l\u2019air bien au d\xE9but, mais apr\xE8s un moment vous obtiendrez ce qui suit :"),Yi.forEach(s),zl=d(e),m(it.$$.fragment,e),El=d(e),m(ut.$$.fragment,e),wl=d(e),or=i(e,"P",{});var Zv=u(or);wc=n(Zv,"Vous r\xE9aliserez que cette erreur appara\xEEt pendant la phase d\u2019\xE9valuation, donc c\u2019est la derni\xE8re chose que nous aurons besoin de d\xE9boguer."),Zv.forEach(s),xl=d(e),Ge=i(e,"P",{});var Ji=u(Ge);xc=n(Ji,"Vous pouvez ex\xE9cuter la boucle d\u2019\xE9valuation du "),Ka=i(Ji,"CODE",{});var Xv=u(Ka);Cc=n(Xv,"Trainer"),Xv.forEach(s),Pc=n(Ji," ind\xE9pendamment de l\u2019entra\xEEnement comme ceci :"),Ji.forEach(s),Cl=d(e),m(pt.$$.fragment,e),Pl=d(e),m(dt.$$.fragment,e),Al=d(e),m(We.$$.fragment,e),Dl=d(e),lr=i(e,"P",{});var eh=u(lr);Ac=n(eh,"Avant de tenter de d\xE9boguer un probl\xE8me dans la boucle d\u2019\xE9valuation, vous devez d\u2019abord vous assurer que vous avez examin\xE9 les donn\xE9es, que vous \xEAtes en mesure de former un batch correctement et que vous pouvez ex\xE9cuter votre mod\xE8le sur ces donn\xE9es. Nous avons effectu\xE9 toutes ces \xE9tapes, et le code suivant peut donc \xEAtre ex\xE9cut\xE9 sans erreur :"),eh.forEach(s),Tl=d(e),m(ct.$$.fragment,e),Sl=d(e),Re=i(e,"P",{});var Ki=u(Re);Dc=n(Ki,"L\u2019erreur survient plus tard, \xE0 la fin de la phase d\u2019\xE9valuation, et si nous regardons la "),Qa=i(Ki,"EM",{});var sh=u(Qa);Tc=n(sh,"traceback"),sh.forEach(s),Sc=n(Ki,", nous voyons ceci :"),Ki.forEach(s),Ll=d(e),m(mt.$$.fragment,e),Nl=d(e),I=i(e,"P",{});var is=u(I);Lc=n(is,"Cela nous indique que l\u2019erreur provient du module "),Za=i(is,"CODE",{});var th=u(Za);Nc=n(th,"datasets/metric.py"),th.forEach(s),Uc=n(is," donc c\u2019est un probl\xE8me avec notre fonction "),Xa=i(is,"CODE",{});var rh=u(Xa);Oc=n(rh,"compute_metrics()"),rh.forEach(s),Mc=n(is,". Elle prend un "),en=i(is,"EM",{});var ah=u(en);Vc=n(ah,"tuple"),ah.forEach(s),Ic=n(is," avec les logits et les labels sous forme de tableaux NumPy, alors essayons de lui fournir cela :"),is.forEach(s),Ul=d(e),m(ft.$$.fragment,e),Ol=d(e),m(vt.$$.fragment,e),Ml=d(e),F=i(e,"P",{});var us=u(F);Fc=n(us,"Nous obtenons la m\xEAme erreur, donc le probl\xE8me vient bien de cette fonction. Si on regarde son code, on voit qu\u2019elle transmet simplement les "),sn=i(us,"CODE",{});var nh=u(sn);Gc=n(nh,"predictions"),nh.forEach(s),Wc=n(us," et les "),tn=i(us,"CODE",{});var oh=u(tn);Rc=n(oh,"labels"),oh.forEach(s),Hc=n(us," \xE0 "),rn=i(us,"CODE",{});var lh=u(rn);Bc=n(lh,"metric.compute()"),lh.forEach(s),Yc=n(us,". Y a-t-il donc un probl\xE8me avec cette m\xE9thode ? Pas vraiment. Jetons un coup d\u2019oeil rapide aux formes :"),us.forEach(s),Vl=d(e),m(ht.$$.fragment,e),Il=d(e),m(bt.$$.fragment,e),Fl=d(e),He=i(e,"P",{});var Qi=u(He);Jc=n(Qi,"Nos pr\xE9dictions sont toujours des logits, et non les pr\xE9dictions r\xE9elles, c\u2019est pourquoi la m\xE9trique retourne cette erreur (quelque peu obscure). La correction est assez simple, il suffit d\u2019ajouter un argmax dans la fonction "),an=i(Qi,"CODE",{});var ih=u(an);Kc=n(ih,"compute_metrics()"),ih.forEach(s),Qc=n(Qi," :"),Qi.forEach(s),Gl=d(e),m(_t.$$.fragment,e),Wl=d(e),m(qt.$$.fragment,e),Rl=d(e),ir=i(e,"P",{});var uh=u(ir);Zc=n(uh,"Maintenant notre erreur est corrig\xE9e ! C\u2019\xE9tait la derni\xE8re, donc notre script va maintenant entra\xEEner un mod\xE8le correctement."),uh.forEach(s),Hl=d(e),ur=i(e,"P",{});var ph=u(ur);Xc=n(ph,"Pour r\xE9f\xE9rence, voici le script compl\xE8tement corrig\xE9 :"),ph.forEach(s),Bl=d(e),m($t.$$.fragment,e),Yl=d(e),pr=i(e,"P",{});var dh=u(pr);em=n(dh,"Dans ce cas, il n\u2019y a plus de probl\xE8me, et notre script va affiner un mod\xE8le qui devrait donner des r\xE9sultats raisonnables. Mais que faire lorsque l\u2019entra\xEEnement se d\xE9roule sans erreur, et que le mod\xE8le entra\xEEn\xE9 n\u2019est pas du tout performant ? C\u2019est la partie la plus difficile de l\u2019apprentissage automatique, et nous allons vous montrer quelques techniques qui peuvent vous aider."),dh.forEach(s),Jl=d(e),m(Be.$$.fragment,e),Kl=d(e),me=i(e,"H2",{class:!0});var Zi=u(me);Ye=i(Zi,"A",{id:!0,class:!0,href:!0});var ch=u(Ye);nn=i(ch,"SPAN",{});var mh=u(nn);m(gt.$$.fragment,mh),mh.forEach(s),ch.forEach(s),sm=d(Zi),on=i(Zi,"SPAN",{});var fh=u(on);tm=n(fh,"D\xE9boguer les erreurs silencieuses pendant l'entra\xEEnement"),fh.forEach(s),Zi.forEach(s),Ql=d(e),dr=i(e,"P",{});var vh=u(dr);rm=n(vh,"Que peut-on faire pour d\xE9boguer un entra\xEEnement qui se termine sans erreur mais qui ne donne pas de bons r\xE9sultats ? Nous allons vous donner quelques pistes ici, mais sachez que ce type de d\xE9bogage est la partie la plus difficile de l\u2019apprentissage automatique, et qu\u2019il n\u2019y a pas de r\xE9ponse magique."),vh.forEach(s),Zl=d(e),fe=i(e,"H3",{class:!0});var Xi=u(fe);Je=i(Xi,"A",{id:!0,class:!0,href:!0});var hh=u(Je);ln=i(hh,"SPAN",{});var bh=u(ln);m(kt.$$.fragment,bh),bh.forEach(s),hh.forEach(s),am=d(Xi),un=i(Xi,"SPAN",{});var _h=u(un);nm=n(_h,"V\xE9rifiez vos donn\xE9es (encore !)"),_h.forEach(s),Xi.forEach(s),Xl=d(e),cr=i(e,"P",{});var qh=u(cr);om=n(qh,"Votre mod\xE8le n\u2019apprendra quelque chose que s\u2019il est r\xE9ellement possible d\u2019apprendre quelque chose de vos donn\xE9es. Si un bogue corrompt les donn\xE9es ou si les \xE9tiquettes sont attribu\xE9es de mani\xE8re al\xE9atoire, il est tr\xE8s probable que vous n\u2019obtiendrez aucun entra\xEEnement de mod\xE8le sur votre ensemble de donn\xE9es. Commencez donc toujours par rev\xE9rifier vos entr\xE9es et \xE9tiquettes d\xE9cod\xE9es, et posez-vous les questions suivantes :"),qh.forEach(s),ei=d(e),G=i(e,"UL",{});var ps=u(G);pn=i(ps,"LI",{});var $h=u(pn);lm=n($h,"les donn\xE9es d\xE9cod\xE9es sont-elles compr\xE9hensibles ?"),$h.forEach(s),im=d(ps),dn=i(ps,"LI",{});var gh=u(dn);um=n(gh,"\xEAtes-vous d\u2019accord avec les \xE9tiquettes ?"),gh.forEach(s),pm=d(ps),cn=i(ps,"LI",{});var kh=u(cn);dm=n(kh,"y a-t-il une \xE9tiquette qui est plus courante que les autres ?"),kh.forEach(s),cm=d(ps),mn=i(ps,"LI",{});var jh=u(mn);mm=n(jh,"quelle devrait \xEAtre la perte/m\xE9trie si le mod\xE8le pr\xE9disait une r\xE9ponse al\xE9atoire/toujours la m\xEAme r\xE9ponse ?"),jh.forEach(s),ps.forEach(s),si=d(e),m(Ke.$$.fragment,e),ti=d(e),mr=i(e,"P",{});var yh=u(mr);fm=n(yh,"Apr\xE8s avoir examin\xE9 vos donn\xE9es, examinez quelques-unes des pr\xE9dictions du mod\xE8le et d\xE9codez-les \xE9galement. Si le mod\xE8le pr\xE9dit toujours la m\xEAme chose, c\u2019est peut-\xEAtre parce que votre ensemble de donn\xE9es est biais\xE9 en faveur d\u2019une cat\xE9gorie (pour les probl\xE8mes de classification) ; des techniques comme le sur\xE9chantillonnage de classes rares peuvent aider."),yh.forEach(s),ri=d(e),fr=i(e,"P",{});var zh=u(fr);vm=n(zh,"Si la perte/la m\xE9trique que vous obtenez sur votre mod\xE8le initial est tr\xE8s diff\xE9rente de la perte/la m\xE9trique \xE0 laquelle vous vous attendez pour des pr\xE9dictions al\xE9atoires, v\xE9rifiez \xE0 nouveau la fa\xE7on dont votre perte ou votre m\xE9trique est calcul\xE9e, car il y a probablement un bug \xE0 ce niveau. Si vous utilisez plusieurs pertes que vous ajoutez \xE0 la fin, assurez-vous qu\u2019elles sont de la m\xEAme \xE9chelle."),zh.forEach(s),ai=d(e),vr=i(e,"P",{});var Eh=u(vr);hm=n(Eh,"Lorsque vous \xEAtes s\xFBr que vos donn\xE9es sont parfaites, vous pouvez voir si le mod\xE8le est capable de s\u2019entra\xEEner sur elles gr\xE2ce \xE0 un test simple."),Eh.forEach(s),ni=d(e),ve=i(e,"H3",{class:!0});var eu=u(ve);Qe=i(eu,"A",{id:!0,class:!0,href:!0});var wh=u(Qe);fn=i(wh,"SPAN",{});var xh=u(fn);m(jt.$$.fragment,xh),xh.forEach(s),wh.forEach(s),bm=d(eu),vn=i(eu,"SPAN",{});var Ch=u(vn);_m=n(Ch,"Surentra\xEEnement du mod\xE8le sur un seul batch"),Ch.forEach(s),eu.forEach(s),oi=d(e),hr=i(e,"P",{});var Ph=u(hr);qm=n(Ph,"Le surentra\xEEnement est g\xE9n\xE9ralement une chose que nous essayons d\u2019\xE9viter lors de l\u2019entra\xEEnement, car cela signifie que le mod\xE8le n\u2019apprend pas \xE0 reconna\xEEtre les caract\xE9ristiques g\xE9n\xE9rales que nous voulons qu\u2019il reconnaisse, mais qu\u2019il se contente de m\xE9moriser les \xE9chantillons d\u2019entra\xEEnement. Cependant, essayer d\u2019entra\xEEner votre mod\xE8le sur un batch encore et encore est un bon test pour v\xE9rifier si le probl\xE8me tel que vous l\u2019avez formul\xE9 peut \xEAtre r\xE9solu par le mod\xE8le que vous essayez d\u2019entra\xEEner. Cela vous aidera \xE9galement \xE0 voir si votre taux d\u2019apprentissage initial est trop \xE9lev\xE9."),Ph.forEach(s),li=d(e),Ze=i(e,"P",{});var su=u(Ze);$m=n(su,"Une fois que vous avez d\xE9fini votre "),hn=i(su,"CODE",{});var Ah=u(hn);gm=n(Ah,"Trainer"),Ah.forEach(s),km=n(su,", c\u2019est tr\xE8s facile ; il suffit de prendre un batch de donn\xE9es d\u2019entra\xEEnement, puis d\u2019ex\xE9cuter une petite boucle d\u2019entra\xEEnement manuel en utilisant uniquement ce batch pour quelque chose comme 20 \xE9tapes :"),su.forEach(s),ii=d(e),m(yt.$$.fragment,e),ui=d(e),m(Xe.$$.fragment,e),pi=d(e),es=i(e,"P",{});var tu=u(es);jm=n(tu,"Le mod\xE8le r\xE9sultant devrait avoir des r\xE9sultats proches de la perfection sur le m\xEAme "),bn=i(tu,"CODE",{});var Dh=u(bn);ym=n(Dh,"batch"),Dh.forEach(s),zm=n(tu,". Calculons la m\xE9trique sur les pr\xE9dictions r\xE9sultantes :"),tu.forEach(s),di=d(e),m(zt.$$.fragment,e),ci=d(e),m(Et.$$.fragment,e),mi=d(e),br=i(e,"P",{});var Th=u(br);Em=n(Th,"100% de pr\xE9cision, voil\xE0 un bel exemple de surentra\xEEnement (ce qui signifie que si vous essayez votre mod\xE8le sur n\u2019importe quelle autre phrase, il vous donnera tr\xE8s probablement une mauvaise r\xE9ponse) !"),Th.forEach(s),fi=d(e),_r=i(e,"P",{});var Sh=u(_r);wm=n(Sh,"Si vous ne parvenez pas \xE0 ce que votre mod\xE8le obtienne des r\xE9sultats parfaits comme celui-ci, cela signifie qu\u2019il y a quelque chose qui ne va pas dans la fa\xE7on dont vous avez formul\xE9 le probl\xE8me ou dans vos donn\xE9es, et vous devez donc y rem\xE9dier. Ce n\u2019est que lorsque vous parviendrez \xE0 passer le test de surentra\xEEnement que vous pourrez \xEAtre s\xFBr que votre mod\xE8le peut r\xE9ellement apprendre quelque chose."),Sh.forEach(s),vi=d(e),m(ss.$$.fragment,e),hi=d(e),he=i(e,"H3",{class:!0});var ru=u(he);ts=i(ru,"A",{id:!0,class:!0,href:!0});var Lh=u(ts);_n=i(Lh,"SPAN",{});var Nh=u(_n);m(wt.$$.fragment,Nh),Nh.forEach(s),Lh.forEach(s),xm=d(ru),qn=i(ru,"SPAN",{});var Uh=u(qn);Cm=n(Uh,"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base."),Uh.forEach(s),ru.forEach(s),bi=d(e),rs=i(e,"P",{});var au=u(rs);Pm=n(au,"Le r\xE9glage des hyperparam\xE8tres est toujours consid\xE9r\xE9 comme la partie la plus difficile de l\u2019apprentissage automatique, mais c\u2019est juste la derni\xE8re \xE9tape pour vous aider \xE0 gagner un peu sur la m\xE9trique. La plupart du temps, les hyperparam\xE8tres par d\xE9faut du "),$n=i(au,"CODE",{});var Oh=u($n);Am=n(Oh,"Trainer"),Oh.forEach(s),Dm=n(au," fonctionneront tr\xE8s bien pour vous donner de bons r\xE9sultats, donc ne vous lancez pas dans une recherche d\u2019hyperparam\xE8tres longue et co\xFBteuse jusqu\u2019\xE0 ce que vous ayez quelque chose qui batte la ligne de base que vous avez sur votre jeu de donn\xE9es."),au.forEach(s),_i=d(e),qr=i(e,"P",{});var Mh=u(qr);Tm=n(Mh,"Une fois que vous avez un mod\xE8le suffisamment bon, vous pouvez commencer \xE0 l\u2019affiner un peu. N\u2019essayez pas de lancer un millier d\u2019ex\xE9cutions avec diff\xE9rents hyperparam\xE8tres, mais comparez quelques ex\xE9cutions avec diff\xE9rentes valeurs pour un hyperparam\xE8tre afin de vous faire une id\xE9e de celui qui a le plus d\u2019impact."),Mh.forEach(s),qi=d(e),$r=i(e,"P",{});var Vh=u($r);Sm=n(Vh,"Si vous modifiez le mod\xE8le lui-m\xEAme, restez simple et n\u2019essayez rien que vous ne puissiez raisonnablement justifier. Veillez toujours \xE0 revenir au test de surentra\xEEnement pour v\xE9rifier que votre modification n\u2019a pas eu de cons\xE9quences inattendues."),Vh.forEach(s),$i=d(e),be=i(e,"H3",{class:!0});var nu=u(be);as=i(nu,"A",{id:!0,class:!0,href:!0});var Ih=u(as);gn=i(Ih,"SPAN",{});var Fh=u(gn);m(xt.$$.fragment,Fh),Fh.forEach(s),Ih.forEach(s),Lm=d(nu),kn=i(nu,"SPAN",{});var Gh=u(kn);Nm=n(Gh,"Demander de l'aide"),Gh.forEach(s),nu.forEach(s),gi=d(e),ns=i(e,"P",{});var ou=u(ns);Um=n(ou,"Nous esp\xE9rons que vous avez trouv\xE9 dans cette section des conseils qui vous ont aid\xE9 \xE0 r\xE9soudre votre probl\xE8me, mais si ce n\u2019est pas le cas, n\u2019oubliez pas que vous pouvez toujours demander de l\u2019aide \xE0 la communaut\xE9 sur le "),Ct=i(ou,"A",{href:!0,rel:!0});var Wh=u(Ct);Om=n(Wh,"forum"),Wh.forEach(s),Mm=n(ou,"."),ou.forEach(s),ki=d(e),gr=i(e,"P",{});var Rh=u(gr);Vm=n(Rh,"Voici quelques ressources (en anglais) suppl\xE9mentaires qui peuvent s\u2019av\xE9rer utiles :"),Rh.forEach(s),ji=d(e),W=i(e,"UL",{});var ds=u(W);kr=i(ds,"LI",{});var rf=u(kr);Pt=i(rf,"A",{href:!0,rel:!0});var Hh=u(Pt);Im=n(Hh,"\u201CLa reproductibilit\xE9 comme vecteur des meilleures pratiques d\u2019ing\xE9nierie\u201D"),Hh.forEach(s),Fm=n(rf," par Joel Grus"),rf.forEach(s),Gm=d(ds),jr=i(ds,"LI",{});var af=u(jr);At=i(af,"A",{href:!0,rel:!0});var Bh=u(At);Wm=n(Bh,"\u201CListe de contr\xF4le pour le d\xE9bogage des r\xE9seaux neuronaux\u201D"),Bh.forEach(s),Rm=n(af," par Cecelia Shao"),af.forEach(s),Hm=d(ds),yr=i(ds,"LI",{});var nf=u(yr);Dt=i(nf,"A",{href:!0,rel:!0});var Yh=u(Dt);Bm=n(Yh,"\u201CComment tester unitairement le code d\u2019apprentissage automatique\u201D"),Yh.forEach(s),Ym=n(nf," par Chase Roberts"),nf.forEach(s),Jm=d(ds),zr=i(ds,"LI",{});var of=u(zr);Tt=i(of,"A",{href:!0,rel:!0});var Jh=u(Tt);Km=n(Jh,"\u201CUne recette pour Entra\xEEner les r\xE9seaux neuronaux\u201D"),Jh.forEach(s),Qm=n(of," par Andrej Karpathy"),of.forEach(s),ds.forEach(s),yi=d(e),te=i(e,"P",{});var Sr=u(te);Zm=n(Sr,"Bien s\xFBr, tous les probl\xE8mes rencontr\xE9s lors de l\u2019Entra\xEEnement des r\xE9seaux neuronaux ne sont pas forc\xE9ment de votre faute ! Si vous rencontrez quelque chose dans la biblioth\xE8que \u{1F917} "),jn=i(Sr,"EM",{});var Kh=u(jn);Xm=n(Kh,"Transformers"),Kh.forEach(s),ef=n(Sr," ou \u{1F917} "),yn=i(Sr,"EM",{});var Qh=u(yn);sf=n(Qh,"Datasets"),Qh.forEach(s),tf=n(Sr," qui ne semble pas correct, vous avez peut-\xEAtre rencontr\xE9 un bogue. Vous devez absolument nous en parler, et dans la section suivante, nous allons vous expliquer exactement comment faire."),Sr.forEach(s),this.h()},h(){q(_,"name","hf:doc:metadata"),q(_,"content",JSON.stringify(fb)),q(y,"id","dbogage-du-pipeline-dentranement"),q(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(y,"href","#dbogage-du-pipeline-dentranement"),q(E,"class","relative group"),q(Ut,"href","/course/fr/chapter7"),q(_e,"id","dboguer-le-pipeline-dentranement"),q(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(_e,"href","#dboguer-le-pipeline-dentranement"),q(oe,"class","relative group"),q(vs,"href","https://huggingface.co/datasets/glue"),q(vs,"rel","nofollow"),q(ge,"id","vrifiez-vos-donnes"),q(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(ge,"href","#vrifiez-vos-donnes"),q(le,"class","relative group"),q(Ps,"href","https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification"),q(Ps,"rel","nofollow"),q(Ce,"id","des-jeux-de-donnes-aux-chargeurs-de-donnes"),q(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Ce,"href","#des-jeux-de-donnes-aux-chargeurs-de-donnes"),q(ie,"class","relative group"),q(Pe,"id","passage-par-le-modle"),q(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Pe,"href","#passage-par-le-modle"),q(ue,"class","relative group"),q(Le,"id","excution-dune-tape-doptimisation"),q(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Le,"href","#excution-dune-tape-doptimisation"),q(pe,"class","relative group"),q(Oe,"id","grer-les-erreurs-cuda-horsmmoire"),q(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Oe,"href","#grer-les-erreurs-cuda-horsmmoire"),q(de,"class","relative group"),q(Ie,"id","valuation-du-modle"),q(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Ie,"href","#valuation-du-modle"),q(ce,"class","relative group"),q(Ye,"id","dboguer-les-erreurs-silencieuses-pendant-lentranement"),q(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Ye,"href","#dboguer-les-erreurs-silencieuses-pendant-lentranement"),q(me,"class","relative group"),q(Je,"id","vrifiez-vos-donnes-encore"),q(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Je,"href","#vrifiez-vos-donnes-encore"),q(fe,"class","relative group"),q(Qe,"id","surentranement-du-modle-sur-un-seul-batch"),q(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Qe,"href","#surentranement-du-modle-sur-un-seul-batch"),q(ve,"class","relative group"),q(ts,"id","ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),q(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(ts,"href","#ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base"),q(he,"class","relative group"),q(as,"id","demander-de-laide"),q(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(as,"href","#demander-de-laide"),q(be,"class","relative group"),q(Ct,"href","https://discuss.huggingface.co/"),q(Ct,"rel","nofollow"),q(Pt,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),q(Pt,"rel","nofollow"),q(At,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),q(At,"rel","nofollow"),q(Dt,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),q(Dt,"rel","nofollow"),q(Tt,"href","http://karpathy.github.io/2019/04/25/recipe/"),q(Tt,"rel","nofollow")},m(e,r){t(document.head,_),o(e,j,r),f(g,e,r),o(e,k,r),o(e,E,r),t(E,y),t(y,z),f(w,z,null),t(E,D),t(E,x),t(x,B),o(e,U,r),f(C,e,r),o(e,cs,r),o(e,T,r),t(T,Nt),t(T,Lr),t(Lr,lu),t(T,iu),t(T,Ut),t(Ut,uu),t(T,pu),t(T,Nr),t(Nr,du),t(T,cu),o(e,An,r),o(e,oe,r),t(oe,_e),t(_e,Ur),f(ms,Ur,null),t(oe,mu),t(oe,Or),t(Or,fu),o(e,Dn,r),f(fs,e,r),o(e,Tn,r),o(e,Y,r),t(Y,vu),t(Y,Mr),t(Mr,hu),t(Y,bu),t(Y,Vr),t(Vr,_u),t(Y,qu),o(e,Sn,r),o(e,qe,r),t(qe,$u),t(qe,Ir),t(Ir,gu),t(qe,ku),o(e,Ln,r),o(e,$e,r),t($e,ju),t($e,vs),t(vs,yu),t($e,zu),o(e,Nn,r),f(hs,e,r),o(e,Un,r),o(e,Ot,r),t(Ot,Eu),o(e,On,r),f(bs,e,r),o(e,Mn,r),o(e,le,r),t(le,ge),t(ge,Fr),f(_s,Fr,null),t(le,wu),t(le,Gr),t(Gr,xu),o(e,Vn,r),o(e,ke,r),t(ke,Cu),t(ke,Wr),t(Wr,Pu),t(ke,Au),o(e,In,r),o(e,je,r),t(je,Du),t(je,Rr),t(Rr,Tu),t(je,Su),o(e,Fn,r),f(qs,e,r),o(e,Gn,r),f($s,e,r),o(e,Wn,r),o(e,J,r),t(J,Lu),t(J,Hr),t(Hr,Nu),t(J,Uu),t(J,Br),t(Br,Ou),t(J,Mu),o(e,Rn,r),o(e,A,r),t(A,Vu),t(A,Yr),t(Yr,Iu),t(A,Fu),t(A,Jr),t(Jr,Gu),t(A,Wu),t(A,Kr),t(Kr,Ru),t(A,Hu),t(A,Qr),t(Qr,Bu),t(A,Yu),t(A,Zr),t(Zr,Ju),t(A,Ku),o(e,Hn,r),f(gs,e,r),o(e,Bn,r),o(e,Mt,r),t(Mt,Qu),o(e,Yn,r),f(ks,e,r),o(e,Jn,r),o(e,Vt,r),t(Vt,Zu),o(e,Kn,r),f(js,e,r),o(e,Qn,r),o(e,It,r),t(It,Xu),o(e,Zn,r),o(e,ye,r),t(ye,ep),t(ye,Xr),t(Xr,sp),t(ye,tp),o(e,Xn,r),f(ys,e,r),o(e,eo,r),f(zs,e,r),o(e,so,r),o(e,Ft,r),t(Ft,rp),o(e,to,r),f(Es,e,r),o(e,ro,r),f(ws,e,r),o(e,ao,r),o(e,S,r),t(S,ap),t(S,ea),t(ea,np),t(S,op),t(S,sa),t(sa,lp),t(S,ip),t(S,ta),t(ta,up),t(S,pp),t(S,ra),t(ra,dp),t(S,cp),o(e,no,r),f(xs,e,r),o(e,oo,r),f(Cs,e,r),o(e,lo,r),o(e,K,r),t(K,mp),t(K,Ps),t(Ps,fp),t(K,vp),t(K,aa),t(aa,hp),t(K,bp),o(e,io,r),o(e,ze,r),t(ze,_p),t(ze,na),t(na,qp),t(ze,$p),o(e,uo,r),f(As,e,r),o(e,po,r),f(Ds,e,r),o(e,co,r),o(e,Ee,r),t(Ee,gp),t(Ee,oa),t(oa,kp),t(Ee,jp),o(e,mo,r),f(Ts,e,r),o(e,fo,r),f(Ss,e,r),o(e,vo,r),o(e,Gt,r),t(Gt,yp),o(e,ho,r),f(Ls,e,r),o(e,bo,r),f(Ns,e,r),o(e,_o,r),o(e,Q,r),t(Q,zp),t(Q,la),t(la,Ep),t(Q,wp),t(Q,ia),t(ia,xp),t(Q,Cp),o(e,qo,r),f(Us,e,r),o(e,$o,r),f(Os,e,r),o(e,go,r),o(e,Z,r),t(Z,Pp),t(Z,ua),t(ua,Ap),t(Z,Dp),t(Z,pa),t(pa,Tp),t(Z,Sp),o(e,ko,r),o(e,we,r),t(we,Lp),t(we,da),t(da,Np),t(we,Up),o(e,jo,r),f(xe,e,r),o(e,yo,r),o(e,Wt,r),t(Wt,Op),o(e,zo,r),o(e,Rt,r),t(Rt,Mp),o(e,Eo,r),o(e,ie,r),t(ie,Ce),t(Ce,ca),f(Ms,ca,null),t(ie,Vp),t(ie,ma),t(ma,Ip),o(e,wo,r),o(e,L,r),t(L,Fp),t(L,fa),t(fa,Gp),t(L,Wp),t(L,va),t(va,Rp),t(L,Hp),t(L,ha),t(ha,Bp),t(L,Yp),t(L,ba),t(ba,Jp),t(L,Kp),o(e,xo,r),f(Vs,e,r),o(e,Co,r),o(e,Ht,r),t(Ht,Qp),o(e,Po,r),f(Is,e,r),o(e,Ao,r),o(e,X,r),t(X,Zp),t(X,_a),t(_a,Xp),t(X,ed),t(X,qa),t(qa,sd),t(X,td),o(e,Do,r),f(Fs,e,r),o(e,To,r),f(Gs,e,r),o(e,So,r),o(e,M,r),t(M,rd),t(M,$a),t($a,ad),t(M,nd),t(M,ga),t(ga,od),t(M,ld),t(M,ka),t(ka,id),t(M,ud),o(e,Lo,r),o(e,V,r),t(V,pd),t(V,ja),t(ja,dd),t(V,cd),t(V,ya),t(ya,md),t(V,fd),t(V,za),t(za,vd),t(V,hd),o(e,No,r),f(Ws,e,r),o(e,Uo,r),o(e,Bt,r),t(Bt,bd),o(e,Oo,r),f(Rs,e,r),o(e,Mo,r),o(e,Yt,r),t(Yt,_d),o(e,Vo,r),o(e,Jt,r),t(Jt,qd),o(e,Io,r),f(Hs,e,r),o(e,Fo,r),o(e,N,r),t(N,$d),t(N,Ea),t(Ea,gd),t(N,kd),t(N,wa),t(wa,jd),t(N,yd),t(N,xa),t(xa,zd),t(N,Ed),t(N,Ca),t(Ca,wd),t(N,xd),o(e,Go,r),f(Bs,e,r),o(e,Wo,r),o(e,Kt,r),t(Kt,Cd),o(e,Ro,r),o(e,Qt,r),t(Qt,Pd),o(e,Ho,r),o(e,ue,r),t(ue,Pe),t(Pe,Pa),f(Ys,Pa,null),t(ue,Ad),t(ue,Aa),t(Aa,Dd),o(e,Bo,r),o(e,Zt,r),t(Zt,Td),o(e,Yo,r),f(Js,e,r),o(e,Jo,r),o(e,ee,r),t(ee,Sd),t(ee,Da),t(Da,Ld),t(ee,Nd),t(ee,Ta),t(Ta,Ud),t(ee,Od),o(e,Ko,r),o(e,Xt,r),t(Xt,Md),o(e,Qo,r),o(e,er,r),t(er,Vd),o(e,Zo,r),o(e,Ae,r),t(Ae,Id),t(Ae,Sa),t(Sa,Fd),t(Ae,Gd),o(e,Xo,r),f(Ks,e,r),o(e,el,r),f(Qs,e,r),o(e,sl,r),o(e,De,r),t(De,Wd),t(De,La),t(La,Rd),t(De,Hd),o(e,tl,r),f(Zs,e,r),o(e,rl,r),f(Xs,e,r),o(e,al,r),o(e,sr,r),t(sr,Bd),o(e,nl,r),f(et,e,r),o(e,ol,r),o(e,Te,r),t(Te,Yd),t(Te,Na),t(Na,Jd),t(Te,Kd),o(e,ll,r),f(st,e,r),o(e,il,r),o(e,tr,r),t(tr,Qd),o(e,ul,r),f(tt,e,r),o(e,pl,r),o(e,Se,r),t(Se,Zd),t(Se,Ua),t(Ua,Xd),t(Se,ec),o(e,dl,r),o(e,pe,r),t(pe,Le),t(Le,Oa),f(rt,Oa,null),t(pe,sc),t(pe,Ma),t(Ma,tc),o(e,cl,r),o(e,rr,r),t(rr,rc),o(e,ml,r),o(e,Ne,r),t(Ne,ac),t(Ne,Va),t(Va,nc),t(Ne,oc),o(e,fl,r),f(at,e,r),o(e,vl,r),o(e,ar,r),t(ar,lc),o(e,hl,r),o(e,se,r),t(se,ic),t(se,Ia),t(Ia,uc),t(se,pc),t(se,Fa),t(Fa,dc),t(se,cc),o(e,bl,r),f(nt,e,r),o(e,_l,r),o(e,Ue,r),t(Ue,mc),t(Ue,Ga),t(Ga,fc),t(Ue,vc),o(e,ql,r),o(e,de,r),t(de,Oe),t(Oe,Wa),f(ot,Wa,null),t(de,hc),t(de,Ra),t(Ra,bc),o(e,$l,r),o(e,Me,r),t(Me,_c),t(Me,Ha),t(Ha,qc),t(Me,$c),o(e,gl,r),o(e,nr,r),t(nr,gc),o(e,kl,r),f(Ve,e,r),o(e,jl,r),o(e,ce,r),t(ce,Ie),t(Ie,Ba),f(lt,Ba,null),t(ce,kc),t(ce,Ya),t(Ya,jc),o(e,yl,r),o(e,Fe,r),t(Fe,yc),t(Fe,Ja),t(Ja,zc),t(Fe,Ec),o(e,zl,r),f(it,e,r),o(e,El,r),f(ut,e,r),o(e,wl,r),o(e,or,r),t(or,wc),o(e,xl,r),o(e,Ge,r),t(Ge,xc),t(Ge,Ka),t(Ka,Cc),t(Ge,Pc),o(e,Cl,r),f(pt,e,r),o(e,Pl,r),f(dt,e,r),o(e,Al,r),f(We,e,r),o(e,Dl,r),o(e,lr,r),t(lr,Ac),o(e,Tl,r),f(ct,e,r),o(e,Sl,r),o(e,Re,r),t(Re,Dc),t(Re,Qa),t(Qa,Tc),t(Re,Sc),o(e,Ll,r),f(mt,e,r),o(e,Nl,r),o(e,I,r),t(I,Lc),t(I,Za),t(Za,Nc),t(I,Uc),t(I,Xa),t(Xa,Oc),t(I,Mc),t(I,en),t(en,Vc),t(I,Ic),o(e,Ul,r),f(ft,e,r),o(e,Ol,r),f(vt,e,r),o(e,Ml,r),o(e,F,r),t(F,Fc),t(F,sn),t(sn,Gc),t(F,Wc),t(F,tn),t(tn,Rc),t(F,Hc),t(F,rn),t(rn,Bc),t(F,Yc),o(e,Vl,r),f(ht,e,r),o(e,Il,r),f(bt,e,r),o(e,Fl,r),o(e,He,r),t(He,Jc),t(He,an),t(an,Kc),t(He,Qc),o(e,Gl,r),f(_t,e,r),o(e,Wl,r),f(qt,e,r),o(e,Rl,r),o(e,ir,r),t(ir,Zc),o(e,Hl,r),o(e,ur,r),t(ur,Xc),o(e,Bl,r),f($t,e,r),o(e,Yl,r),o(e,pr,r),t(pr,em),o(e,Jl,r),f(Be,e,r),o(e,Kl,r),o(e,me,r),t(me,Ye),t(Ye,nn),f(gt,nn,null),t(me,sm),t(me,on),t(on,tm),o(e,Ql,r),o(e,dr,r),t(dr,rm),o(e,Zl,r),o(e,fe,r),t(fe,Je),t(Je,ln),f(kt,ln,null),t(fe,am),t(fe,un),t(un,nm),o(e,Xl,r),o(e,cr,r),t(cr,om),o(e,ei,r),o(e,G,r),t(G,pn),t(pn,lm),t(G,im),t(G,dn),t(dn,um),t(G,pm),t(G,cn),t(cn,dm),t(G,cm),t(G,mn),t(mn,mm),o(e,si,r),f(Ke,e,r),o(e,ti,r),o(e,mr,r),t(mr,fm),o(e,ri,r),o(e,fr,r),t(fr,vm),o(e,ai,r),o(e,vr,r),t(vr,hm),o(e,ni,r),o(e,ve,r),t(ve,Qe),t(Qe,fn),f(jt,fn,null),t(ve,bm),t(ve,vn),t(vn,_m),o(e,oi,r),o(e,hr,r),t(hr,qm),o(e,li,r),o(e,Ze,r),t(Ze,$m),t(Ze,hn),t(hn,gm),t(Ze,km),o(e,ii,r),f(yt,e,r),o(e,ui,r),f(Xe,e,r),o(e,pi,r),o(e,es,r),t(es,jm),t(es,bn),t(bn,ym),t(es,zm),o(e,di,r),f(zt,e,r),o(e,ci,r),f(Et,e,r),o(e,mi,r),o(e,br,r),t(br,Em),o(e,fi,r),o(e,_r,r),t(_r,wm),o(e,vi,r),f(ss,e,r),o(e,hi,r),o(e,he,r),t(he,ts),t(ts,_n),f(wt,_n,null),t(he,xm),t(he,qn),t(qn,Cm),o(e,bi,r),o(e,rs,r),t(rs,Pm),t(rs,$n),t($n,Am),t(rs,Dm),o(e,_i,r),o(e,qr,r),t(qr,Tm),o(e,qi,r),o(e,$r,r),t($r,Sm),o(e,$i,r),o(e,be,r),t(be,as),t(as,gn),f(xt,gn,null),t(be,Lm),t(be,kn),t(kn,Nm),o(e,gi,r),o(e,ns,r),t(ns,Um),t(ns,Ct),t(Ct,Om),t(ns,Mm),o(e,ki,r),o(e,gr,r),t(gr,Vm),o(e,ji,r),o(e,W,r),t(W,kr),t(kr,Pt),t(Pt,Im),t(kr,Fm),t(W,Gm),t(W,jr),t(jr,At),t(At,Wm),t(jr,Rm),t(W,Hm),t(W,yr),t(yr,Dt),t(Dt,Bm),t(yr,Ym),t(W,Jm),t(W,zr),t(zr,Tt),t(Tt,Km),t(zr,Qm),o(e,yi,r),o(e,te,r),t(te,Zm),t(te,jn),t(jn,Xm),t(te,ef),t(te,yn),t(yn,sf),t(te,tf),zi=!0},p(e,[r]){const St={};r&1&&(St.fw=e[0]),g.$set(St);const zn={};r&2&&(zn.$$scope={dirty:r,ctx:e}),xe.$set(zn);const En={};r&2&&(En.$$scope={dirty:r,ctx:e}),Ve.$set(En);const wn={};r&2&&(wn.$$scope={dirty:r,ctx:e}),We.$set(wn);const H={};r&2&&(H.$$scope={dirty:r,ctx:e}),Be.$set(H);const xn={};r&2&&(xn.$$scope={dirty:r,ctx:e}),Ke.$set(xn);const Cn={};r&2&&(Cn.$$scope={dirty:r,ctx:e}),Xe.$set(Cn);const Pn={};r&2&&(Pn.$$scope={dirty:r,ctx:e}),ss.$set(Pn)},i(e){zi||(v(g.$$.fragment,e),v(w.$$.fragment,e),v(C.$$.fragment,e),v(ms.$$.fragment,e),v(fs.$$.fragment,e),v(hs.$$.fragment,e),v(bs.$$.fragment,e),v(_s.$$.fragment,e),v(qs.$$.fragment,e),v($s.$$.fragment,e),v(gs.$$.fragment,e),v(ks.$$.fragment,e),v(js.$$.fragment,e),v(ys.$$.fragment,e),v(zs.$$.fragment,e),v(Es.$$.fragment,e),v(ws.$$.fragment,e),v(xs.$$.fragment,e),v(Cs.$$.fragment,e),v(As.$$.fragment,e),v(Ds.$$.fragment,e),v(Ts.$$.fragment,e),v(Ss.$$.fragment,e),v(Ls.$$.fragment,e),v(Ns.$$.fragment,e),v(Us.$$.fragment,e),v(Os.$$.fragment,e),v(xe.$$.fragment,e),v(Ms.$$.fragment,e),v(Vs.$$.fragment,e),v(Is.$$.fragment,e),v(Fs.$$.fragment,e),v(Gs.$$.fragment,e),v(Ws.$$.fragment,e),v(Rs.$$.fragment,e),v(Hs.$$.fragment,e),v(Bs.$$.fragment,e),v(Ys.$$.fragment,e),v(Js.$$.fragment,e),v(Ks.$$.fragment,e),v(Qs.$$.fragment,e),v(Zs.$$.fragment,e),v(Xs.$$.fragment,e),v(et.$$.fragment,e),v(st.$$.fragment,e),v(tt.$$.fragment,e),v(rt.$$.fragment,e),v(at.$$.fragment,e),v(nt.$$.fragment,e),v(ot.$$.fragment,e),v(Ve.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(ut.$$.fragment,e),v(pt.$$.fragment,e),v(dt.$$.fragment,e),v(We.$$.fragment,e),v(ct.$$.fragment,e),v(mt.$$.fragment,e),v(ft.$$.fragment,e),v(vt.$$.fragment,e),v(ht.$$.fragment,e),v(bt.$$.fragment,e),v(_t.$$.fragment,e),v(qt.$$.fragment,e),v($t.$$.fragment,e),v(Be.$$.fragment,e),v(gt.$$.fragment,e),v(kt.$$.fragment,e),v(Ke.$$.fragment,e),v(jt.$$.fragment,e),v(yt.$$.fragment,e),v(Xe.$$.fragment,e),v(zt.$$.fragment,e),v(Et.$$.fragment,e),v(ss.$$.fragment,e),v(wt.$$.fragment,e),v(xt.$$.fragment,e),zi=!0)},o(e){h(g.$$.fragment,e),h(w.$$.fragment,e),h(C.$$.fragment,e),h(ms.$$.fragment,e),h(fs.$$.fragment,e),h(hs.$$.fragment,e),h(bs.$$.fragment,e),h(_s.$$.fragment,e),h(qs.$$.fragment,e),h($s.$$.fragment,e),h(gs.$$.fragment,e),h(ks.$$.fragment,e),h(js.$$.fragment,e),h(ys.$$.fragment,e),h(zs.$$.fragment,e),h(Es.$$.fragment,e),h(ws.$$.fragment,e),h(xs.$$.fragment,e),h(Cs.$$.fragment,e),h(As.$$.fragment,e),h(Ds.$$.fragment,e),h(Ts.$$.fragment,e),h(Ss.$$.fragment,e),h(Ls.$$.fragment,e),h(Ns.$$.fragment,e),h(Us.$$.fragment,e),h(Os.$$.fragment,e),h(xe.$$.fragment,e),h(Ms.$$.fragment,e),h(Vs.$$.fragment,e),h(Is.$$.fragment,e),h(Fs.$$.fragment,e),h(Gs.$$.fragment,e),h(Ws.$$.fragment,e),h(Rs.$$.fragment,e),h(Hs.$$.fragment,e),h(Bs.$$.fragment,e),h(Ys.$$.fragment,e),h(Js.$$.fragment,e),h(Ks.$$.fragment,e),h(Qs.$$.fragment,e),h(Zs.$$.fragment,e),h(Xs.$$.fragment,e),h(et.$$.fragment,e),h(st.$$.fragment,e),h(tt.$$.fragment,e),h(rt.$$.fragment,e),h(at.$$.fragment,e),h(nt.$$.fragment,e),h(ot.$$.fragment,e),h(Ve.$$.fragment,e),h(lt.$$.fragment,e),h(it.$$.fragment,e),h(ut.$$.fragment,e),h(pt.$$.fragment,e),h(dt.$$.fragment,e),h(We.$$.fragment,e),h(ct.$$.fragment,e),h(mt.$$.fragment,e),h(ft.$$.fragment,e),h(vt.$$.fragment,e),h(ht.$$.fragment,e),h(bt.$$.fragment,e),h(_t.$$.fragment,e),h(qt.$$.fragment,e),h($t.$$.fragment,e),h(Be.$$.fragment,e),h(gt.$$.fragment,e),h(kt.$$.fragment,e),h(Ke.$$.fragment,e),h(jt.$$.fragment,e),h(yt.$$.fragment,e),h(Xe.$$.fragment,e),h(zt.$$.fragment,e),h(Et.$$.fragment,e),h(ss.$$.fragment,e),h(wt.$$.fragment,e),h(xt.$$.fragment,e),zi=!1},d(e){s(_),e&&s(j),b(g,e),e&&s(k),e&&s(E),b(w),e&&s(U),b(C,e),e&&s(cs),e&&s(T),e&&s(An),e&&s(oe),b(ms),e&&s(Dn),b(fs,e),e&&s(Tn),e&&s(Y),e&&s(Sn),e&&s(qe),e&&s(Ln),e&&s($e),e&&s(Nn),b(hs,e),e&&s(Un),e&&s(Ot),e&&s(On),b(bs,e),e&&s(Mn),e&&s(le),b(_s),e&&s(Vn),e&&s(ke),e&&s(In),e&&s(je),e&&s(Fn),b(qs,e),e&&s(Gn),b($s,e),e&&s(Wn),e&&s(J),e&&s(Rn),e&&s(A),e&&s(Hn),b(gs,e),e&&s(Bn),e&&s(Mt),e&&s(Yn),b(ks,e),e&&s(Jn),e&&s(Vt),e&&s(Kn),b(js,e),e&&s(Qn),e&&s(It),e&&s(Zn),e&&s(ye),e&&s(Xn),b(ys,e),e&&s(eo),b(zs,e),e&&s(so),e&&s(Ft),e&&s(to),b(Es,e),e&&s(ro),b(ws,e),e&&s(ao),e&&s(S),e&&s(no),b(xs,e),e&&s(oo),b(Cs,e),e&&s(lo),e&&s(K),e&&s(io),e&&s(ze),e&&s(uo),b(As,e),e&&s(po),b(Ds,e),e&&s(co),e&&s(Ee),e&&s(mo),b(Ts,e),e&&s(fo),b(Ss,e),e&&s(vo),e&&s(Gt),e&&s(ho),b(Ls,e),e&&s(bo),b(Ns,e),e&&s(_o),e&&s(Q),e&&s(qo),b(Us,e),e&&s($o),b(Os,e),e&&s(go),e&&s(Z),e&&s(ko),e&&s(we),e&&s(jo),b(xe,e),e&&s(yo),e&&s(Wt),e&&s(zo),e&&s(Rt),e&&s(Eo),e&&s(ie),b(Ms),e&&s(wo),e&&s(L),e&&s(xo),b(Vs,e),e&&s(Co),e&&s(Ht),e&&s(Po),b(Is,e),e&&s(Ao),e&&s(X),e&&s(Do),b(Fs,e),e&&s(To),b(Gs,e),e&&s(So),e&&s(M),e&&s(Lo),e&&s(V),e&&s(No),b(Ws,e),e&&s(Uo),e&&s(Bt),e&&s(Oo),b(Rs,e),e&&s(Mo),e&&s(Yt),e&&s(Vo),e&&s(Jt),e&&s(Io),b(Hs,e),e&&s(Fo),e&&s(N),e&&s(Go),b(Bs,e),e&&s(Wo),e&&s(Kt),e&&s(Ro),e&&s(Qt),e&&s(Ho),e&&s(ue),b(Ys),e&&s(Bo),e&&s(Zt),e&&s(Yo),b(Js,e),e&&s(Jo),e&&s(ee),e&&s(Ko),e&&s(Xt),e&&s(Qo),e&&s(er),e&&s(Zo),e&&s(Ae),e&&s(Xo),b(Ks,e),e&&s(el),b(Qs,e),e&&s(sl),e&&s(De),e&&s(tl),b(Zs,e),e&&s(rl),b(Xs,e),e&&s(al),e&&s(sr),e&&s(nl),b(et,e),e&&s(ol),e&&s(Te),e&&s(ll),b(st,e),e&&s(il),e&&s(tr),e&&s(ul),b(tt,e),e&&s(pl),e&&s(Se),e&&s(dl),e&&s(pe),b(rt),e&&s(cl),e&&s(rr),e&&s(ml),e&&s(Ne),e&&s(fl),b(at,e),e&&s(vl),e&&s(ar),e&&s(hl),e&&s(se),e&&s(bl),b(nt,e),e&&s(_l),e&&s(Ue),e&&s(ql),e&&s(de),b(ot),e&&s($l),e&&s(Me),e&&s(gl),e&&s(nr),e&&s(kl),b(Ve,e),e&&s(jl),e&&s(ce),b(lt),e&&s(yl),e&&s(Fe),e&&s(zl),b(it,e),e&&s(El),b(ut,e),e&&s(wl),e&&s(or),e&&s(xl),e&&s(Ge),e&&s(Cl),b(pt,e),e&&s(Pl),b(dt,e),e&&s(Al),b(We,e),e&&s(Dl),e&&s(lr),e&&s(Tl),b(ct,e),e&&s(Sl),e&&s(Re),e&&s(Ll),b(mt,e),e&&s(Nl),e&&s(I),e&&s(Ul),b(ft,e),e&&s(Ol),b(vt,e),e&&s(Ml),e&&s(F),e&&s(Vl),b(ht,e),e&&s(Il),b(bt,e),e&&s(Fl),e&&s(He),e&&s(Gl),b(_t,e),e&&s(Wl),b(qt,e),e&&s(Rl),e&&s(ir),e&&s(Hl),e&&s(ur),e&&s(Bl),b($t,e),e&&s(Yl),e&&s(pr),e&&s(Jl),b(Be,e),e&&s(Kl),e&&s(me),b(gt),e&&s(Ql),e&&s(dr),e&&s(Zl),e&&s(fe),b(kt),e&&s(Xl),e&&s(cr),e&&s(ei),e&&s(G),e&&s(si),b(Ke,e),e&&s(ti),e&&s(mr),e&&s(ri),e&&s(fr),e&&s(ai),e&&s(vr),e&&s(ni),e&&s(ve),b(jt),e&&s(oi),e&&s(hr),e&&s(li),e&&s(Ze),e&&s(ii),b(yt,e),e&&s(ui),b(Xe,e),e&&s(pi),e&&s(es),e&&s(di),b(zt,e),e&&s(ci),b(Et,e),e&&s(mi),e&&s(br),e&&s(fi),e&&s(_r),e&&s(vi),b(ss,e),e&&s(hi),e&&s(he),b(wt),e&&s(bi),e&&s(rs),e&&s(_i),e&&s(qr),e&&s(qi),e&&s($r),e&&s($i),e&&s(be),b(xt),e&&s(gi),e&&s(ns),e&&s(ki),e&&s(gr),e&&s(ji),e&&s(W),e&&s(yi),e&&s(te)}}}const fb={local:"dbogage-du-pipeline-dentranement",sections:[{local:"dboguer-le-pipeline-dentranement",sections:[{local:"vrifiez-vos-donnes",title:"V\xE9rifiez vos donn\xE9es"},{local:"des-jeux-de-donnes-aux-chargeurs-de-donnes",title:"Des jeux de donn\xE9es aux chargeurs de donn\xE9es"},{local:"passage-par-le-modle",title:"Passage par le mod\xE8le"},{local:"excution-dune-tape-doptimisation",title:"Ex\xE9cution d'une \xE9tape d'optimisation"},{local:"grer-les-erreurs-cuda-horsmmoire",title:"G\xE9rer les erreurs CUDA hors-m\xE9moire"},{local:"valuation-du-modle",title:"\xC9valuation du mod\xE8le"}],title:"D\xE9boguer le pipeline d'entra\xEEnement"},{local:"dboguer-les-erreurs-silencieuses-pendant-lentranement",sections:[{local:"vrifiez-vos-donnes-encore",title:"V\xE9rifiez vos donn\xE9es (encore !)"},{local:"surentranement-du-modle-sur-un-seul-batch",title:"Surentra\xEEnement du mod\xE8le sur un seul batch"},{local:"ne-rglez-rien-tant-que-vous-navez-pas-une-premire-ligne-de-base",title:"Ne r\xE9glez rien tant que vous n'avez pas une premi\xE8re ligne de base."},{local:"demander-de-laide",title:"Demander de l'aide"}],title:"D\xE9boguer les erreurs silencieuses pendant l'entra\xEEnement"}],title:"D\xE9bogage du pipeline d'entra\xEEnement"};function vb(P,_,j){let g="pt";return tb(()=>{const k=new URLSearchParams(window.location.search);j(0,g=k.get("fw")||"pt")}),[g]}class jb extends Zh{constructor(_){super();Xh(this,_,vb,mb,eb,{})}}export{jb as default,fb as metadata};
