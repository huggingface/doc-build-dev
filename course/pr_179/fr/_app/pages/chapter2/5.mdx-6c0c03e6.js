import{S as wr,i as Er,s as yr,e as p,k as h,w as g,t as o,M as zr,c,d as s,m as _,x as q,a as m,h as i,b as E,F as n,g as d,y as j,o as v,p as we,q as $,B as w,v as Ar,n as Ee}from"../../chunks/vendor-1e8b365d.js";import{T as gr}from"../../chunks/Tip-62b14c6e.js";import{Y as qr}from"../../chunks/Youtube-c2a8cc39.js";import{I as fn}from"../../chunks/IconCopyLink-483c28ba.js";import{C as y}from"../../chunks/CodeBlock-e5764662.js";import{D as jr}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as xr}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Ir(k){let a,u;return a=new jr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"}]}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Mr(k){let a,u;return a=new jr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"}]}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Pr(k){let a,u;return a=new qr({props:{id:"ROxrFOEbsQE"}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Tr(k){let a,u;return a=new qr({props:{id:"M6adb1j2jPI"}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Fr(k){let a,u,t,f;return a=new y({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),t=new y({props:{code:"InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]",highlighted:'InvalidArgumentError: Input to reshape <span class="hljs-keyword">is</span> a tensor <span class="hljs-keyword">with</span> <span class="hljs-number">14</span> values, but the requested shape has <span class="hljs-number">196</span> [Op:Reshape]'}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Sr(k){let a,u,t,f;return a=new y({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),t=new y({props:{code:"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",highlighted:'IndexError: Dimension out of <span class="hljs-built_in">range</span> (expected to be <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> of [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], but got <span class="hljs-number">1</span>)'}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Cr(k){let a,u,t,f;return a=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new y({props:{code:`<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>,
        <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]], dtype=int32)&gt;`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Lr(k){let a,u,t,f;return a=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new y({props:{code:`tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])`,highlighted:`tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,
          <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]])`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Nr(k){let a,u;return a=new y({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Dr(k){let a,u;return a=new y({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Hr(k){let a,u;return a=new y({props:{code:`Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)`,highlighted:`Input IDs: tf.Tensor(
[[ <span class="hljs-number">1045</span>  <span class="hljs-number">1005</span>  <span class="hljs-number">2310</span>  <span class="hljs-number">2042</span>  <span class="hljs-number">3403</span>  <span class="hljs-number">2005</span>  <span class="hljs-number">1037</span> <span class="hljs-number">17662</span> <span class="hljs-number">12172</span>  <span class="hljs-number">2607</span>  <span class="hljs-number">2026</span>  <span class="hljs-number">2878</span>
   <span class="hljs-number">2166</span>  <span class="hljs-number">1012</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), dtype=int32)
Logits: tf.Tensor([[-<span class="hljs-number">2.7276208</span>  <span class="hljs-number">2.8789377</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Or(k){let a,u;return a=new y({props:{code:`Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]`,highlighted:`Input IDs: [[ <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>]]
Logits: [[-<span class="hljs-number">2.7276</span>,  <span class="hljs-number">2.8789</span>]]`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Br(k){let a,u,t,f,l,b,T,I;return{c(){a=p("p"),u=o("\u270F\uFE0F "),t=p("strong"),f=o("Essayez !"),l=o(" Convertissez cette liste "),b=p("code"),T=o("batched_ids"),I=o(" en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !")},l(M){a=c(M,"P",{});var z=m(a);u=i(z,"\u270F\uFE0F "),t=c(z,"STRONG",{});var ae=m(t);f=i(ae,"Essayez !"),ae.forEach(s),l=i(z," Convertissez cette liste "),b=c(z,"CODE",{});var ee=m(b);T=i(ee,"batched_ids"),ee.forEach(s),I=i(z," en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !"),z.forEach(s)},m(M,z){d(M,a,z),n(a,u),n(a,t),n(t,f),n(a,l),n(a,b),n(b,T),n(a,I)},d(M){M&&s(a)}}}function Ur(k){let a,u,t,f;return a=new y({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(tf.constant(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(batched_ids)).logits)`}}),t=new y({props:{code:`tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor([[ <span class="hljs-number">1.5693678</span> -<span class="hljs-number">1.3894581</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor([[ <span class="hljs-number">0.5803005</span>  -<span class="hljs-number">0.41252428</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor(
[[ <span class="hljs-number">1.5693681</span> -<span class="hljs-number">1.3894582</span>]
 [ <span class="hljs-number">1.3373486</span> -<span class="hljs-number">1.2163193</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Vr(k){let a,u,t,f;return a=new y({props:{code:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)`}}),t=new y({props:{code:`tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">1.3373</span>, -<span class="hljs-number">1.2163</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Jr(k){let a,u,t,f;return a=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new y({props:{code:`tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[ <span class="hljs-number">1.5693681</span>  -<span class="hljs-number">1.3894582</span> ]
 [ <span class="hljs-number">0.5803021</span>  -<span class="hljs-number">0.41252586</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Rr(k){let a,u,t,f;return a=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new y({props:{code:`tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Gr(k){let a,u,t,f,l,b,T,I;return{c(){a=p("p"),u=o("\u270F\uFE0F "),t=p("strong"),f=o("Essayez !"),l=o(" Appliquez la tokenisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life.  \xBB et \xAB I hate this so much! \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),b=p("em"),T=o("padding"),I=o(" et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !")},l(M){a=c(M,"P",{});var z=m(a);u=i(z,"\u270F\uFE0F "),t=c(z,"STRONG",{});var ae=m(t);f=i(ae,"Essayez !"),ae.forEach(s),l=i(z," Appliquez la tokenisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life.  \xBB et \xAB I hate this so much! \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),b=c(z,"EM",{});var ee=m(b);T=i(ee,"padding"),ee.forEach(s),I=i(z," et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !"),z.forEach(s)},m(M,z){d(M,a,z),n(a,u),n(a,t),n(t,f),n(a,l),n(a,b),n(b,T),n(a,I)},d(M){M&&s(a)}}}function Yr(k){let a,u,t,f,l,b,T,I,M,z,ae,ee,N,D,Qe,H,O,Ke,We,it,hn,F,ws,ut,pt,ye,ct,Es,mt,dt,ft,ys,ht,_t,zs,bt,_n,pe,vt,As,$t,kt,bn,re,ce,xs,ze,gt,Is,qt,vn,Xe,jt,$n,B,U,Ze,es,wt,kn,S,Et,Ms,yt,zt,Ps,At,xt,Ts,It,Mt,gn,V,J,ss,ns,Pt,qn,R,G,ts,ls,Tt,jn,Y,Q,as,me,Ft,Fs,St,Ct,wn,Ae,En,rs,Lt,yn,de,zn,C,Nt,Ss,Dt,Ht,Cs,Ot,Bt,Ls,Ut,Vt,An,oe,fe,Ns,xe,Jt,Ds,Rt,xn,os,Gt,In,Ie,Mn,P,Yt,Hs,Qt,Kt,Os,Wt,Xt,Bs,Zt,el,Us,sl,nl,Pn,Me,Tn,se,tl,Vs,ll,al,Js,rl,ol,Fn,K,W,is,us,il,Sn,A,ul,Rs,pl,cl,Gs,ml,dl,Ys,fl,hl,Qs,_l,bl,Ks,vl,$l,Ws,kl,gl,Xs,ql,jl,Zs,wl,El,Cn,ie,he,en,Pe,yl,sn,zl,Ln,ps,Al,Nn,_e,Te,xl,nn,Il,Ml,Pl,Fe,Tl,tn,Fl,Sl,Dn,cs,Cl,Hn,X,Z,ms,ds,Ll,On,be,Nl,ln,Dl,Hl,Bn,ve,Un,ue,$e,an,Se,Ol,rn,Bl,Vn,ne,Ul,on,Vl,Jl,un,Rl,Gl,Jn,ke,pn,Yl,Ql,cn,Kl,Rn,te,Wl,Ce,Xl,Zl,Le,ea,sa,Gn,ge,na,mn,ta,la,Yn,Ne,Qn;t=new xr({props:{fw:k[0]}}),I=new fn({});const aa=[Mr,Ir],De=[];function ra(e,r){return e[0]==="pt"?0:1}N=ra(k),D=De[N]=aa[N](k);const oa=[Tr,Pr],He=[];function ia(e,r){return e[0]==="pt"?0:1}H=ia(k),O=He[H]=oa[H](k),ze=new fn({});const ua=[Sr,Fr],Oe=[];function pa(e,r){return e[0]==="pt"?0:1}B=pa(k),U=Oe[B]=ua[B](k);const ca=[Lr,Cr],Be=[];function ma(e,r){return e[0]==="pt"?0:1}V=ma(k),J=Be[V]=ca[V](k);const da=[Dr,Nr],Ue=[];function fa(e,r){return e[0]==="pt"?0:1}R=fa(k),G=Ue[R]=da[R](k);const ha=[Or,Hr],Ve=[];function _a(e,r){return e[0]==="pt"?0:1}Y=_a(k),Q=Ve[Y]=ha[Y](k),Ae=new y({props:{code:"batched_ids = [ids, ids]",highlighted:'<span class="hljs-attr">batched_ids</span> = [ids, ids]'}}),de=new gr({props:{$$slots:{default:[Br]},$$scope:{ctx:k}}}),xe=new fn({}),Ie=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200]
]`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]
]`}}),Me=new y({props:{code:`padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]`,highlighted:`padding_id = <span class="hljs-number">100</span>

batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, padding_id],
]`}});const ba=[Vr,Ur],Je=[];function va(e,r){return e[0]==="pt"?0:1}K=va(k),W=Je[K]=ba[K](k),Pe=new fn({});const $a=[Rr,Jr],Re=[];function ka(e,r){return e[0]==="pt"?0:1}return X=ka(k),Z=Re[X]=$a[X](k),ve=new gr({props:{$$slots:{default:[Gr]},$$scope:{ctx:k}}}),Se=new fn({}),Ne=new y({props:{code:"sequence = sequence[:max_sequence_length]",highlighted:"sequence = sequence[:max_sequence_length]"}}),{c(){a=p("meta"),u=h(),g(t.$$.fragment),f=h(),l=p("h1"),b=p("a"),T=p("span"),g(I.$$.fragment),M=h(),z=p("span"),ae=o("Manipulation de plusieurs s\xE9quences"),ee=h(),D.c(),Qe=h(),O.c(),Ke=h(),We=p("p"),it=o("Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),hn=h(),F=p("ul"),ws=p("li"),ut=o("comment g\xE9rer de plusieurs s\xE9quences ?"),pt=h(),ye=p("li"),ct=o("comment g\xE9rer de plusieurs s\xE9quences "),Es=p("em"),mt=o("de longueurs diff\xE9rentes"),dt=o(" ?"),ft=h(),ys=p("li"),ht=o("les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),_t=h(),zs=p("li"),bt=o("existe-t-il une s\xE9quence trop longue ?"),_n=h(),pe=p("p"),vt=o("Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),As=p("em"),$t=o("Transformers"),kt=o("."),bn=h(),re=p("h2"),ce=p("a"),xs=p("span"),g(ze.$$.fragment),gt=h(),Is=p("span"),qt=o("Les mod\xE8les attendent un batch d'entr\xE9es"),vn=h(),Xe=p("p"),jt=o(`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),$n=h(),U.c(),Ze=h(),es=p("p"),wt=o("Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),kn=h(),S=p("p"),Et=o("Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Ms=p("em"),yt=o("Transformers"),zt=o(" attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Ps=p("em"),At=o("tokenizer"),xt=o(" fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ts=p("code"),It=o("s\xE9quence"),Mt=o(". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),gn=h(),J.c(),ss=h(),ns=p("p"),Pt=o("Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),qn=h(),G.c(),ts=h(),ls=p("p"),Tt=o("Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),jn=h(),Q.c(),as=h(),me=p("p"),Ft=o("Le \xAB "),Fs=p("em"),St=o("batching"),Ct=o(" \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),wn=h(),g(Ae.$$.fragment),En=h(),rs=p("p"),Lt=o("Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),yn=h(),g(de.$$.fragment),zn=h(),C=p("p"),Nt=o("Utiliser des "),Ss=p("em"),Dt=o("batchs"),Ht=o(" permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Cs=p("em"),Ot=o("rembourrer"),Bt=o(" (le "),Ls=p("em"),Ut=o("padding"),Vt=o(" en anglais) les entr\xE9es."),An=h(),oe=p("h2"),fe=p("a"),Ns=p("span"),g(xe.$$.fragment),Jt=h(),Ds=p("span"),Rt=o("*Padding* des entr\xE9es"),xn=h(),os=p("p"),Gt=o("La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),In=h(),g(Ie.$$.fragment),Mn=h(),P=p("p"),Yt=o("Afin de contourner ce probl\xE8me, nous utilisons le "),Hs=p("em"),Qt=o("padding"),Kt=o(" pour que nos tenseurs aient une forme rectangulaire. Le "),Os=p("em"),Wt=o("padding"),Xt=o(" permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Bs=p("em"),Zt=o("padding token"),el=o(" aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Us=p("em"),sl=o("padding"),nl=o(" fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),Pn=h(),g(Me.$$.fragment),Tn=h(),se=p("p"),tl=o("L\u2019identifiant du jeton de "),Vs=p("em"),ll=o("padding"),al=o(" peut \xEAtre trouv\xE9 dans "),Js=p("code"),rl=o("tokenizer.pad_token_id"),ol=o(". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),Fn=h(),W.c(),is=h(),us=p("p"),il=o("Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),Sn=h(),A=p("p"),ul=o("C\u2019est parce que dans un "),Rs=p("em"),pl=o("transformer"),cl=o(" les couches d\u2019attention "),Gs=p("em"),ml=o("contextualisent"),dl=o(" chaque "),Ys=p("em"),fl=o("token"),hl=o(". Celles-ci prennent en compte les "),Qs=p("em"),_l=o("tokens"),bl=o(" de "),Ks=p("em"),vl=o("padding"),$l=o(" puisqu\u2019elles analysent tous les "),Ws=p("em"),kl=o("tokens"),gl=o(" d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),Xs=p("em"),ql=o("padding"),jl=o(", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),Zs=p("em"),wl=o("padding"),El=o(". Ceci est fait en utilisant un masque d\u2019attention."),Cn=h(),ie=p("h2"),he=p("a"),en=p("span"),g(Pe.$$.fragment),yl=h(),sn=p("span"),zl=o("Masques d'attention"),Ln=h(),ps=p("p"),Al=o("Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),Nn=h(),_e=p("ul"),Te=p("li"),xl=o("1 indique que les "),nn=p("em"),Il=o("tokens"),Ml=o(" correspondants doivent \xEAtre analys\xE9s"),Pl=h(),Fe=p("li"),Tl=o("0 indique que les "),tn=p("em"),Fl=o("tokens"),Sl=o(" correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),Dn=h(),cs=p("p"),Cl=o("Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),Hn=h(),Z.c(),ms=h(),ds=p("p"),Ll=o("Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),On=h(),be=p("p"),Nl=o("Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),ln=p("em"),Dl=o("padding"),Hl=o(" valant 0 dans le masque d\u2019attention."),Bn=h(),g(ve.$$.fragment),Un=h(),ue=p("h2"),$e=p("a"),an=p("span"),g(Se.$$.fragment),Ol=h(),rn=p("span"),Bl=o("S\xE9quences plus longues"),Vn=h(),ne=p("p"),Ul=o("Les "),on=p("em"),Vl=o("transformers"),Jl=o(" acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),un=p("em"),Rl=o("tokens"),Gl=o(" et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),Jn=h(),ke=p("ul"),pn=p("li"),Yl=o("utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),Ql=h(),cn=p("li"),Kl=o("tronquer les s\xE9quences."),Rn=h(),te=p("p"),Wl=o("Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),Ce=p("a"),Xl=o("Longformer"),Zl=o(" ou le "),Le=p("a"),ea=o("LED"),sa=o(". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),Gn=h(),ge=p("p"),na=o("Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),mn=p("code"),ta=o("max_sequence_length"),la=o(" :"),Yn=h(),g(Ne.$$.fragment),this.h()},l(e){const r=zr('[data-svelte="svelte-1phssyn"]',document.head);a=c(r,"META",{name:!0,content:!0}),r.forEach(s),u=_(e),q(t.$$.fragment,e),f=_(e),l=c(e,"H1",{class:!0});var Ge=m(l);b=c(Ge,"A",{id:!0,class:!0,href:!0});var fs=m(b);T=c(fs,"SPAN",{});var hs=m(T);q(I.$$.fragment,hs),hs.forEach(s),fs.forEach(s),M=_(Ge),z=c(Ge,"SPAN",{});var _s=m(z);ae=i(_s,"Manipulation de plusieurs s\xE9quences"),_s.forEach(s),Ge.forEach(s),ee=_(e),D.l(e),Qe=_(e),O.l(e),Ke=_(e),We=c(e,"P",{});var bs=m(We);it=i(bs,"Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),bs.forEach(s),hn=_(e),F=c(e,"UL",{});var L=m(F);ws=c(L,"LI",{});var vs=m(ws);ut=i(vs,"comment g\xE9rer de plusieurs s\xE9quences ?"),vs.forEach(s),pt=_(L),ye=c(L,"LI",{});var Ye=m(ye);ct=i(Ye,"comment g\xE9rer de plusieurs s\xE9quences "),Es=c(Ye,"EM",{});var $s=m(Es);mt=i($s,"de longueurs diff\xE9rentes"),$s.forEach(s),dt=i(Ye," ?"),Ye.forEach(s),ft=_(L),ys=c(L,"LI",{});var ks=m(ys);ht=i(ks,"les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),ks.forEach(s),_t=_(L),zs=c(L,"LI",{});var dn=m(zs);bt=i(dn,"existe-t-il une s\xE9quence trop longue ?"),dn.forEach(s),L.forEach(s),_n=_(e),pe=c(e,"P",{});var Kn=m(pe);vt=i(Kn,"Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),As=c(Kn,"EM",{});var ga=m(As);$t=i(ga,"Transformers"),ga.forEach(s),kt=i(Kn,"."),Kn.forEach(s),bn=_(e),re=c(e,"H2",{class:!0});var Wn=m(re);ce=c(Wn,"A",{id:!0,class:!0,href:!0});var qa=m(ce);xs=c(qa,"SPAN",{});var ja=m(xs);q(ze.$$.fragment,ja),ja.forEach(s),qa.forEach(s),gt=_(Wn),Is=c(Wn,"SPAN",{});var wa=m(Is);qt=i(wa,"Les mod\xE8les attendent un batch d'entr\xE9es"),wa.forEach(s),Wn.forEach(s),vn=_(e),Xe=c(e,"P",{});var Ea=m(Xe);jt=i(Ea,`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),Ea.forEach(s),$n=_(e),U.l(e),Ze=_(e),es=c(e,"P",{});var ya=m(es);wt=i(ya,"Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),ya.forEach(s),kn=_(e),S=c(e,"P",{});var qe=m(S);Et=i(qe,"Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Ms=c(qe,"EM",{});var za=m(Ms);yt=i(za,"Transformers"),za.forEach(s),zt=i(qe," attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Ps=c(qe,"EM",{});var Aa=m(Ps);At=i(Aa,"tokenizer"),Aa.forEach(s),xt=i(qe," fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ts=c(qe,"CODE",{});var xa=m(Ts);It=i(xa,"s\xE9quence"),xa.forEach(s),Mt=i(qe,". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),qe.forEach(s),gn=_(e),J.l(e),ss=_(e),ns=c(e,"P",{});var Ia=m(ns);Pt=i(Ia,"Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),Ia.forEach(s),qn=_(e),G.l(e),ts=_(e),ls=c(e,"P",{});var Ma=m(ls);Tt=i(Ma,"Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),Ma.forEach(s),jn=_(e),Q.l(e),as=_(e),me=c(e,"P",{});var Xn=m(me);Ft=i(Xn,"Le \xAB "),Fs=c(Xn,"EM",{});var Pa=m(Fs);St=i(Pa,"batching"),Pa.forEach(s),Ct=i(Xn," \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),Xn.forEach(s),wn=_(e),q(Ae.$$.fragment,e),En=_(e),rs=c(e,"P",{});var Ta=m(rs);Lt=i(Ta,"Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),Ta.forEach(s),yn=_(e),q(de.$$.fragment,e),zn=_(e),C=c(e,"P",{});var je=m(C);Nt=i(je,"Utiliser des "),Ss=c(je,"EM",{});var Fa=m(Ss);Dt=i(Fa,"batchs"),Fa.forEach(s),Ht=i(je," permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Cs=c(je,"EM",{});var Sa=m(Cs);Ot=i(Sa,"rembourrer"),Sa.forEach(s),Bt=i(je," (le "),Ls=c(je,"EM",{});var Ca=m(Ls);Ut=i(Ca,"padding"),Ca.forEach(s),Vt=i(je," en anglais) les entr\xE9es."),je.forEach(s),An=_(e),oe=c(e,"H2",{class:!0});var Zn=m(oe);fe=c(Zn,"A",{id:!0,class:!0,href:!0});var La=m(fe);Ns=c(La,"SPAN",{});var Na=m(Ns);q(xe.$$.fragment,Na),Na.forEach(s),La.forEach(s),Jt=_(Zn),Ds=c(Zn,"SPAN",{});var Da=m(Ds);Rt=i(Da,"*Padding* des entr\xE9es"),Da.forEach(s),Zn.forEach(s),xn=_(e),os=c(e,"P",{});var Ha=m(os);Gt=i(Ha,"La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),Ha.forEach(s),In=_(e),q(Ie.$$.fragment,e),Mn=_(e),P=c(e,"P",{});var le=m(P);Yt=i(le,"Afin de contourner ce probl\xE8me, nous utilisons le "),Hs=c(le,"EM",{});var Oa=m(Hs);Qt=i(Oa,"padding"),Oa.forEach(s),Kt=i(le," pour que nos tenseurs aient une forme rectangulaire. Le "),Os=c(le,"EM",{});var Ba=m(Os);Wt=i(Ba,"padding"),Ba.forEach(s),Xt=i(le," permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Bs=c(le,"EM",{});var Ua=m(Bs);Zt=i(Ua,"padding token"),Ua.forEach(s),el=i(le," aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Us=c(le,"EM",{});var Va=m(Us);sl=i(Va,"padding"),Va.forEach(s),nl=i(le," fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),le.forEach(s),Pn=_(e),q(Me.$$.fragment,e),Tn=_(e),se=c(e,"P",{});var gs=m(se);tl=i(gs,"L\u2019identifiant du jeton de "),Vs=c(gs,"EM",{});var Ja=m(Vs);ll=i(Ja,"padding"),Ja.forEach(s),al=i(gs," peut \xEAtre trouv\xE9 dans "),Js=c(gs,"CODE",{});var Ra=m(Js);rl=i(Ra,"tokenizer.pad_token_id"),Ra.forEach(s),ol=i(gs,". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),gs.forEach(s),Fn=_(e),W.l(e),is=_(e),us=c(e,"P",{});var Ga=m(us);il=i(Ga,"Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),Ga.forEach(s),Sn=_(e),A=c(e,"P",{});var x=m(A);ul=i(x,"C\u2019est parce que dans un "),Rs=c(x,"EM",{});var Ya=m(Rs);pl=i(Ya,"transformer"),Ya.forEach(s),cl=i(x," les couches d\u2019attention "),Gs=c(x,"EM",{});var Qa=m(Gs);ml=i(Qa,"contextualisent"),Qa.forEach(s),dl=i(x," chaque "),Ys=c(x,"EM",{});var Ka=m(Ys);fl=i(Ka,"token"),Ka.forEach(s),hl=i(x,". Celles-ci prennent en compte les "),Qs=c(x,"EM",{});var Wa=m(Qs);_l=i(Wa,"tokens"),Wa.forEach(s),bl=i(x," de "),Ks=c(x,"EM",{});var Xa=m(Ks);vl=i(Xa,"padding"),Xa.forEach(s),$l=i(x," puisqu\u2019elles analysent tous les "),Ws=c(x,"EM",{});var Za=m(Ws);kl=i(Za,"tokens"),Za.forEach(s),gl=i(x," d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),Xs=c(x,"EM",{});var er=m(Xs);ql=i(er,"padding"),er.forEach(s),jl=i(x,", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),Zs=c(x,"EM",{});var sr=m(Zs);wl=i(sr,"padding"),sr.forEach(s),El=i(x,". Ceci est fait en utilisant un masque d\u2019attention."),x.forEach(s),Cn=_(e),ie=c(e,"H2",{class:!0});var et=m(ie);he=c(et,"A",{id:!0,class:!0,href:!0});var nr=m(he);en=c(nr,"SPAN",{});var tr=m(en);q(Pe.$$.fragment,tr),tr.forEach(s),nr.forEach(s),yl=_(et),sn=c(et,"SPAN",{});var lr=m(sn);zl=i(lr,"Masques d'attention"),lr.forEach(s),et.forEach(s),Ln=_(e),ps=c(e,"P",{});var ar=m(ps);Al=i(ar,"Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),ar.forEach(s),Nn=_(e),_e=c(e,"UL",{});var st=m(_e);Te=c(st,"LI",{});var nt=m(Te);xl=i(nt,"1 indique que les "),nn=c(nt,"EM",{});var rr=m(nn);Il=i(rr,"tokens"),rr.forEach(s),Ml=i(nt," correspondants doivent \xEAtre analys\xE9s"),nt.forEach(s),Pl=_(st),Fe=c(st,"LI",{});var tt=m(Fe);Tl=i(tt,"0 indique que les "),tn=c(tt,"EM",{});var or=m(tn);Fl=i(or,"tokens"),or.forEach(s),Sl=i(tt," correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),tt.forEach(s),st.forEach(s),Dn=_(e),cs=c(e,"P",{});var ir=m(cs);Cl=i(ir,"Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),ir.forEach(s),Hn=_(e),Z.l(e),ms=_(e),ds=c(e,"P",{});var ur=m(ds);Ll=i(ur,"Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),ur.forEach(s),On=_(e),be=c(e,"P",{});var lt=m(be);Nl=i(lt,"Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),ln=c(lt,"EM",{});var pr=m(ln);Dl=i(pr,"padding"),pr.forEach(s),Hl=i(lt," valant 0 dans le masque d\u2019attention."),lt.forEach(s),Bn=_(e),q(ve.$$.fragment,e),Un=_(e),ue=c(e,"H2",{class:!0});var at=m(ue);$e=c(at,"A",{id:!0,class:!0,href:!0});var cr=m($e);an=c(cr,"SPAN",{});var mr=m(an);q(Se.$$.fragment,mr),mr.forEach(s),cr.forEach(s),Ol=_(at),rn=c(at,"SPAN",{});var dr=m(rn);Bl=i(dr,"S\xE9quences plus longues"),dr.forEach(s),at.forEach(s),Vn=_(e),ne=c(e,"P",{});var qs=m(ne);Ul=i(qs,"Les "),on=c(qs,"EM",{});var fr=m(on);Vl=i(fr,"transformers"),fr.forEach(s),Jl=i(qs," acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),un=c(qs,"EM",{});var hr=m(un);Rl=i(hr,"tokens"),hr.forEach(s),Gl=i(qs," et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),qs.forEach(s),Jn=_(e),ke=c(e,"UL",{});var rt=m(ke);pn=c(rt,"LI",{});var _r=m(pn);Yl=i(_r,"utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),_r.forEach(s),Ql=_(rt),cn=c(rt,"LI",{});var br=m(cn);Kl=i(br,"tronquer les s\xE9quences."),br.forEach(s),rt.forEach(s),Rn=_(e),te=c(e,"P",{});var js=m(te);Wl=i(js,"Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),Ce=c(js,"A",{href:!0,rel:!0});var vr=m(Ce);Xl=i(vr,"Longformer"),vr.forEach(s),Zl=i(js," ou le "),Le=c(js,"A",{href:!0,rel:!0});var $r=m(Le);ea=i($r,"LED"),$r.forEach(s),sa=i(js,". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),js.forEach(s),Gn=_(e),ge=c(e,"P",{});var ot=m(ge);na=i(ot,"Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),mn=c(ot,"CODE",{});var kr=m(mn);ta=i(kr,"max_sequence_length"),kr.forEach(s),la=i(ot," :"),ot.forEach(s),Yn=_(e),q(Ne.$$.fragment,e),this.h()},h(){E(a,"name","hf:doc:metadata"),E(a,"content",JSON.stringify(Qr)),E(b,"id","manipulation-de-plusieurs-squences"),E(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(b,"href","#manipulation-de-plusieurs-squences"),E(l,"class","relative group"),E(ce,"id","les-modles-attendent-un-batch-dentres"),E(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ce,"href","#les-modles-attendent-un-batch-dentres"),E(re,"class","relative group"),E(fe,"id","padding-des-entres"),E(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(fe,"href","#padding-des-entres"),E(oe,"class","relative group"),E(he,"id","masques-dattention"),E(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(he,"href","#masques-dattention"),E(ie,"class","relative group"),E($e,"id","squences-plus-longues"),E($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E($e,"href","#squences-plus-longues"),E(ue,"class","relative group"),E(Ce,"href","https://huggingface.co/transformers/model_doc/longformer.html"),E(Ce,"rel","nofollow"),E(Le,"href","https://huggingface.co/transformers/model_doc/led.html"),E(Le,"rel","nofollow")},m(e,r){n(document.head,a),d(e,u,r),j(t,e,r),d(e,f,r),d(e,l,r),n(l,b),n(b,T),j(I,T,null),n(l,M),n(l,z),n(z,ae),d(e,ee,r),De[N].m(e,r),d(e,Qe,r),He[H].m(e,r),d(e,Ke,r),d(e,We,r),n(We,it),d(e,hn,r),d(e,F,r),n(F,ws),n(ws,ut),n(F,pt),n(F,ye),n(ye,ct),n(ye,Es),n(Es,mt),n(ye,dt),n(F,ft),n(F,ys),n(ys,ht),n(F,_t),n(F,zs),n(zs,bt),d(e,_n,r),d(e,pe,r),n(pe,vt),n(pe,As),n(As,$t),n(pe,kt),d(e,bn,r),d(e,re,r),n(re,ce),n(ce,xs),j(ze,xs,null),n(re,gt),n(re,Is),n(Is,qt),d(e,vn,r),d(e,Xe,r),n(Xe,jt),d(e,$n,r),Oe[B].m(e,r),d(e,Ze,r),d(e,es,r),n(es,wt),d(e,kn,r),d(e,S,r),n(S,Et),n(S,Ms),n(Ms,yt),n(S,zt),n(S,Ps),n(Ps,At),n(S,xt),n(S,Ts),n(Ts,It),n(S,Mt),d(e,gn,r),Be[V].m(e,r),d(e,ss,r),d(e,ns,r),n(ns,Pt),d(e,qn,r),Ue[R].m(e,r),d(e,ts,r),d(e,ls,r),n(ls,Tt),d(e,jn,r),Ve[Y].m(e,r),d(e,as,r),d(e,me,r),n(me,Ft),n(me,Fs),n(Fs,St),n(me,Ct),d(e,wn,r),j(Ae,e,r),d(e,En,r),d(e,rs,r),n(rs,Lt),d(e,yn,r),j(de,e,r),d(e,zn,r),d(e,C,r),n(C,Nt),n(C,Ss),n(Ss,Dt),n(C,Ht),n(C,Cs),n(Cs,Ot),n(C,Bt),n(C,Ls),n(Ls,Ut),n(C,Vt),d(e,An,r),d(e,oe,r),n(oe,fe),n(fe,Ns),j(xe,Ns,null),n(oe,Jt),n(oe,Ds),n(Ds,Rt),d(e,xn,r),d(e,os,r),n(os,Gt),d(e,In,r),j(Ie,e,r),d(e,Mn,r),d(e,P,r),n(P,Yt),n(P,Hs),n(Hs,Qt),n(P,Kt),n(P,Os),n(Os,Wt),n(P,Xt),n(P,Bs),n(Bs,Zt),n(P,el),n(P,Us),n(Us,sl),n(P,nl),d(e,Pn,r),j(Me,e,r),d(e,Tn,r),d(e,se,r),n(se,tl),n(se,Vs),n(Vs,ll),n(se,al),n(se,Js),n(Js,rl),n(se,ol),d(e,Fn,r),Je[K].m(e,r),d(e,is,r),d(e,us,r),n(us,il),d(e,Sn,r),d(e,A,r),n(A,ul),n(A,Rs),n(Rs,pl),n(A,cl),n(A,Gs),n(Gs,ml),n(A,dl),n(A,Ys),n(Ys,fl),n(A,hl),n(A,Qs),n(Qs,_l),n(A,bl),n(A,Ks),n(Ks,vl),n(A,$l),n(A,Ws),n(Ws,kl),n(A,gl),n(A,Xs),n(Xs,ql),n(A,jl),n(A,Zs),n(Zs,wl),n(A,El),d(e,Cn,r),d(e,ie,r),n(ie,he),n(he,en),j(Pe,en,null),n(ie,yl),n(ie,sn),n(sn,zl),d(e,Ln,r),d(e,ps,r),n(ps,Al),d(e,Nn,r),d(e,_e,r),n(_e,Te),n(Te,xl),n(Te,nn),n(nn,Il),n(Te,Ml),n(_e,Pl),n(_e,Fe),n(Fe,Tl),n(Fe,tn),n(tn,Fl),n(Fe,Sl),d(e,Dn,r),d(e,cs,r),n(cs,Cl),d(e,Hn,r),Re[X].m(e,r),d(e,ms,r),d(e,ds,r),n(ds,Ll),d(e,On,r),d(e,be,r),n(be,Nl),n(be,ln),n(ln,Dl),n(be,Hl),d(e,Bn,r),j(ve,e,r),d(e,Un,r),d(e,ue,r),n(ue,$e),n($e,an),j(Se,an,null),n(ue,Ol),n(ue,rn),n(rn,Bl),d(e,Vn,r),d(e,ne,r),n(ne,Ul),n(ne,on),n(on,Vl),n(ne,Jl),n(ne,un),n(un,Rl),n(ne,Gl),d(e,Jn,r),d(e,ke,r),n(ke,pn),n(pn,Yl),n(ke,Ql),n(ke,cn),n(cn,Kl),d(e,Rn,r),d(e,te,r),n(te,Wl),n(te,Ce),n(Ce,Xl),n(te,Zl),n(te,Le),n(Le,ea),n(te,sa),d(e,Gn,r),d(e,ge,r),n(ge,na),n(ge,mn),n(mn,ta),n(ge,la),d(e,Yn,r),j(Ne,e,r),Qn=!0},p(e,[r]){const Ge={};r&1&&(Ge.fw=e[0]),t.$set(Ge);let fs=N;N=ra(e),N!==fs&&(Ee(),v(De[fs],1,1,()=>{De[fs]=null}),we(),D=De[N],D||(D=De[N]=aa[N](e),D.c()),$(D,1),D.m(Qe.parentNode,Qe));let hs=H;H=ia(e),H!==hs&&(Ee(),v(He[hs],1,1,()=>{He[hs]=null}),we(),O=He[H],O||(O=He[H]=oa[H](e),O.c()),$(O,1),O.m(Ke.parentNode,Ke));let _s=B;B=pa(e),B!==_s&&(Ee(),v(Oe[_s],1,1,()=>{Oe[_s]=null}),we(),U=Oe[B],U||(U=Oe[B]=ua[B](e),U.c()),$(U,1),U.m(Ze.parentNode,Ze));let bs=V;V=ma(e),V!==bs&&(Ee(),v(Be[bs],1,1,()=>{Be[bs]=null}),we(),J=Be[V],J||(J=Be[V]=ca[V](e),J.c()),$(J,1),J.m(ss.parentNode,ss));let L=R;R=fa(e),R!==L&&(Ee(),v(Ue[L],1,1,()=>{Ue[L]=null}),we(),G=Ue[R],G||(G=Ue[R]=da[R](e),G.c()),$(G,1),G.m(ts.parentNode,ts));let vs=Y;Y=_a(e),Y!==vs&&(Ee(),v(Ve[vs],1,1,()=>{Ve[vs]=null}),we(),Q=Ve[Y],Q||(Q=Ve[Y]=ha[Y](e),Q.c()),$(Q,1),Q.m(as.parentNode,as));const Ye={};r&2&&(Ye.$$scope={dirty:r,ctx:e}),de.$set(Ye);let $s=K;K=va(e),K!==$s&&(Ee(),v(Je[$s],1,1,()=>{Je[$s]=null}),we(),W=Je[K],W||(W=Je[K]=ba[K](e),W.c()),$(W,1),W.m(is.parentNode,is));let ks=X;X=ka(e),X!==ks&&(Ee(),v(Re[ks],1,1,()=>{Re[ks]=null}),we(),Z=Re[X],Z||(Z=Re[X]=$a[X](e),Z.c()),$(Z,1),Z.m(ms.parentNode,ms));const dn={};r&2&&(dn.$$scope={dirty:r,ctx:e}),ve.$set(dn)},i(e){Qn||($(t.$$.fragment,e),$(I.$$.fragment,e),$(D),$(O),$(ze.$$.fragment,e),$(U),$(J),$(G),$(Q),$(Ae.$$.fragment,e),$(de.$$.fragment,e),$(xe.$$.fragment,e),$(Ie.$$.fragment,e),$(Me.$$.fragment,e),$(W),$(Pe.$$.fragment,e),$(Z),$(ve.$$.fragment,e),$(Se.$$.fragment,e),$(Ne.$$.fragment,e),Qn=!0)},o(e){v(t.$$.fragment,e),v(I.$$.fragment,e),v(D),v(O),v(ze.$$.fragment,e),v(U),v(J),v(G),v(Q),v(Ae.$$.fragment,e),v(de.$$.fragment,e),v(xe.$$.fragment,e),v(Ie.$$.fragment,e),v(Me.$$.fragment,e),v(W),v(Pe.$$.fragment,e),v(Z),v(ve.$$.fragment,e),v(Se.$$.fragment,e),v(Ne.$$.fragment,e),Qn=!1},d(e){s(a),e&&s(u),w(t,e),e&&s(f),e&&s(l),w(I),e&&s(ee),De[N].d(e),e&&s(Qe),He[H].d(e),e&&s(Ke),e&&s(We),e&&s(hn),e&&s(F),e&&s(_n),e&&s(pe),e&&s(bn),e&&s(re),w(ze),e&&s(vn),e&&s(Xe),e&&s($n),Oe[B].d(e),e&&s(Ze),e&&s(es),e&&s(kn),e&&s(S),e&&s(gn),Be[V].d(e),e&&s(ss),e&&s(ns),e&&s(qn),Ue[R].d(e),e&&s(ts),e&&s(ls),e&&s(jn),Ve[Y].d(e),e&&s(as),e&&s(me),e&&s(wn),w(Ae,e),e&&s(En),e&&s(rs),e&&s(yn),w(de,e),e&&s(zn),e&&s(C),e&&s(An),e&&s(oe),w(xe),e&&s(xn),e&&s(os),e&&s(In),w(Ie,e),e&&s(Mn),e&&s(P),e&&s(Pn),w(Me,e),e&&s(Tn),e&&s(se),e&&s(Fn),Je[K].d(e),e&&s(is),e&&s(us),e&&s(Sn),e&&s(A),e&&s(Cn),e&&s(ie),w(Pe),e&&s(Ln),e&&s(ps),e&&s(Nn),e&&s(_e),e&&s(Dn),e&&s(cs),e&&s(Hn),Re[X].d(e),e&&s(ms),e&&s(ds),e&&s(On),e&&s(be),e&&s(Bn),w(ve,e),e&&s(Un),e&&s(ue),w(Se),e&&s(Vn),e&&s(ne),e&&s(Jn),e&&s(ke),e&&s(Rn),e&&s(te),e&&s(Gn),e&&s(ge),e&&s(Yn),w(Ne,e)}}}const Qr={local:"manipulation-de-plusieurs-squences",sections:[{local:"les-modles-attendent-un-batch-dentres",title:"Les mod\xE8les attendent un batch d'entr\xE9es"},{local:"padding-des-entres",title:"*Padding* des entr\xE9es"},{local:"masques-dattention",title:"Masques d'attention"},{local:"squences-plus-longues",title:"S\xE9quences plus longues"}],title:"Manipulation de plusieurs s\xE9quences"};function Kr(k,a,u){let t="pt";return Ar(()=>{const f=new URLSearchParams(window.location.search);u(0,t=f.get("fw")||"pt")}),[t]}class lo extends wr{constructor(a){super();Er(this,a,Kr,Yr,yr,{})}}export{lo as default,Qr as metadata};
