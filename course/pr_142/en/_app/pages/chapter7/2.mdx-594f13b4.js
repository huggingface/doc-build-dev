import{S as md,i as _d,s as bd,e as o,w as j,k as c,t as a,c as r,a as i,x as E,d as s,m as h,h as l,b,g as p,F as t,y as x,q as w,o as g,B as O,M as wd,N as Up,p as So,v as gd,n as Fo,L as cd}from"../../chunks/vendor-1e8b365d.js";import{T as Lo}from"../../chunks/Tip-62b14c6e.js";import{Y as hd}from"../../chunks/Youtube-c2a8cc39.js";import{I as Ct}from"../../chunks/IconCopyLink-483c28ba.js";import{C as q}from"../../chunks/CodeBlock-e5764662.js";import{D as fd}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as kd}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function vd(Y){let u,k;return u=new fd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"}]}}),{c(){j(u.$$.fragment)},l(m){E(u.$$.fragment,m)},m(m,$){x(u,m,$),k=!0},i(m){k||(w(u.$$.fragment,m),k=!0)},o(m){g(u.$$.fragment,m),k=!1},d(m){O(u,m)}}}function $d(Y){let u,k;return u=new fd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"}]}}),{c(){j(u.$$.fragment)},l(m){E(u.$$.fragment,m)},m(m,$){x(u,m,$),k=!0},i(m){k||(w(u.$$.fragment,m),k=!0)},o(m){g(u.$$.fragment,m),k=!1},d(m){O(u,m)}}}function yd(Y){let u,k,m,$,C,v,y,T;return{c(){u=o("p"),k=a("\u{1F4A1} As long as your dataset consists of texts split into words with their corresponding labels, you will be able to adapt the data processing procedures described here to your own dataset. Refer back to "),m=o("a"),$=a("Chapter 5"),C=a(" if you need a refresher on how to load your own custom data in a "),v=o("code"),y=a("Dataset"),T=a("."),this.h()},l(D){u=r(D,"P",{});var A=i(u);k=l(A,"\u{1F4A1} As long as your dataset consists of texts split into words with their corresponding labels, you will be able to adapt the data processing procedures described here to your own dataset. Refer back to "),m=r(A,"A",{href:!0});var B=i(m);$=l(B,"Chapter 5"),B.forEach(s),C=l(A," if you need a refresher on how to load your own custom data in a "),v=r(A,"CODE",{});var P=i(v);y=l(P,"Dataset"),P.forEach(s),T=l(A,"."),A.forEach(s),this.h()},h(){b(m,"href","/course/chapter5")},m(D,A){p(D,u,A),t(u,k),t(u,m),t(m,$),t(u,C),t(u,v),t(v,y),t(u,T)},d(D){D&&s(u)}}}function jd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u270F\uFE0F "),m=o("strong"),$=a("Your turn!"),C=a(" Print the same two sentences with their POS or chunking labels.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u270F\uFE0F "),m=r(y,"STRONG",{});var T=i(m);$=l(T,"Your turn!"),T.forEach(s),C=l(y," Print the same two sentences with their POS or chunking labels."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function Ed(Y){let u,k,m,$,C,v,y,T;return{c(){u=o("p"),k=a("\u270F\uFE0F "),m=o("strong"),$=a("Your turn!"),C=a(" Some researchers prefer to attribute only one label per word, and assign "),v=o("code"),y=a("-100"),T=a(" to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule.")},l(D){u=r(D,"P",{});var A=i(u);k=l(A,"\u270F\uFE0F "),m=r(A,"STRONG",{});var B=i(m);$=l(B,"Your turn!"),B.forEach(s),C=l(A," Some researchers prefer to attribute only one label per word, and assign "),v=r(A,"CODE",{});var P=i(v);y=l(P,"-100"),P.forEach(s),T=l(A," to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule."),A.forEach(s)},m(D,A){p(D,u,A),t(u,k),t(u,m),t(m,$),t(u,C),t(u,v),t(v,y),t(u,T)},d(D){D&&s(u)}}}function xd(Y){let u,k,m,$,C,v,y,T,D,A,B;return $=new Ct({}),{c(){u=o("h2"),k=o("a"),m=o("span"),j($.$$.fragment),C=c(),v=o("span"),y=a("Fine-tuning the model with Keras"),T=c(),D=o("p"),A=a("The actual code using Keras will be very similar to before; the only changes are the way the data is collated into a batch and the metric computation function."),this.h()},l(P){u=r(P,"H2",{class:!0});var S=i(u);k=r(S,"A",{id:!0,class:!0,href:!0});var M=i(k);m=r(M,"SPAN",{});var N=i(m);E($.$$.fragment,N),N.forEach(s),M.forEach(s),C=h(S),v=r(S,"SPAN",{});var H=i(v);y=l(H,"Fine-tuning the model with Keras"),H.forEach(s),S.forEach(s),T=h(P),D=r(P,"P",{});var Z=i(D);A=l(Z,"The actual code using Keras will be very similar to before; the only changes are the way the data is collated into a batch and the metric computation function."),Z.forEach(s),this.h()},h(){b(k,"id","finetuning-the-model-with-keras"),b(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(k,"href","#finetuning-the-model-with-keras"),b(u,"class","relative group")},m(P,S){p(P,u,S),t(u,k),t(k,m),x($,m,null),t(u,C),t(u,v),t(v,y),p(P,T,S),p(P,D,S),t(D,A),B=!0},i(P){B||(w($.$$.fragment,P),B=!0)},o(P){g($.$$.fragment,P),B=!1},d(P){P&&s(u),O($),P&&s(T),P&&s(D)}}}function Od(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z;return $=new Ct({}),{c(){u=o("h2"),k=o("a"),m=o("span"),j($.$$.fragment),C=c(),v=o("span"),y=a("Fine-tuning the model with the "),T=o("code"),D=a("Trainer"),A=a(" API"),B=c(),P=o("p"),S=a("The actual code using the "),M=o("code"),N=a("Trainer"),H=a(" will be the same as before; the only changes are the way the data is collated into a batch and the metric computation function."),this.h()},l(F){u=r(F,"H2",{class:!0});var G=i(u);k=r(G,"A",{id:!0,class:!0,href:!0});var z=i(k);m=r(z,"SPAN",{});var U=i(m);E($.$$.fragment,U),U.forEach(s),z.forEach(s),C=h(G),v=r(G,"SPAN",{});var L=i(v);y=l(L,"Fine-tuning the model with the "),T=r(L,"CODE",{});var X=i(T);D=l(X,"Trainer"),X.forEach(s),A=l(L," API"),L.forEach(s),G.forEach(s),B=h(F),P=r(F,"P",{});var te=i(P);S=l(te,"The actual code using the "),M=r(te,"CODE",{});var se=i(M);N=l(se,"Trainer"),se.forEach(s),H=l(te," will be the same as before; the only changes are the way the data is collated into a batch and the metric computation function."),te.forEach(s),this.h()},h(){b(k,"id","finetuning-the-model-with-the-trainer-api"),b(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(k,"href","#finetuning-the-model-with-the-trainer-api"),b(u,"class","relative group")},m(F,G){p(F,u,G),t(u,k),t(k,m),x($,m,null),t(u,C),t(u,v),t(v,y),t(v,T),t(T,D),t(v,A),p(F,B,G),p(F,P,G),t(P,S),t(P,M),t(M,N),t(P,H),Z=!0},i(F){Z||(w($.$$.fragment,F),Z=!0)},o(F){g($.$$.fragment,F),Z=!1},d(F){F&&s(u),O($),F&&s(B),F&&s(P)}}}function Cd(Y){let u,k;return u=new q({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
)`}}),{c(){j(u.$$.fragment)},l(m){E(u.$$.fragment,m)},m(m,$){x(u,m,$),k=!0},i(m){k||(w(u.$$.fragment,m),k=!0)},o(m){g(u.$$.fragment,m),k=!1},d(m){O(u,m)}}}function Td(Y){let u,k;return u=new q({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`}}),{c(){j(u.$$.fragment)},l(m){E(u.$$.fragment,m)},m(m,$){x(u,m,$),k=!0},i(m){k||(w(u.$$.fragment,m),k=!0)},o(m){g(u.$$.fragment,m),k=!1},d(m){O(u,m)}}}function Dd(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M;return A=new q({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
)

tf_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
)`}}),{c(){u=o("p"),k=a("Our data collator is ready to go! Now let\u2019s use it to make a "),m=o("code"),$=a("tf.data.Dataset"),C=a(" with the "),v=o("code"),y=a("to_tf_dataset()"),T=a(" method."),D=c(),j(A.$$.fragment),B=c(),P=o("p"),S=a("Next stop: the model itself.")},l(N){u=r(N,"P",{});var H=i(u);k=l(H,"Our data collator is ready to go! Now let\u2019s use it to make a "),m=r(H,"CODE",{});var Z=i(m);$=l(Z,"tf.data.Dataset"),Z.forEach(s),C=l(H," with the "),v=r(H,"CODE",{});var F=i(v);y=l(F,"to_tf_dataset()"),F.forEach(s),T=l(H," method."),H.forEach(s),D=h(N),E(A.$$.fragment,N),B=h(N),P=r(N,"P",{});var G=i(P);S=l(G,"Next stop: the model itself."),G.forEach(s)},m(N,H){p(N,u,H),t(u,k),t(u,m),t(m,$),t(u,C),t(u,v),t(v,y),t(u,T),p(N,D,H),x(A,N,H),p(N,B,H),p(N,P,H),t(P,S),M=!0},i(N){M||(w(A.$$.fragment,N),M=!0)},o(N){g(A.$$.fragment,N),M=!1},d(N){N&&s(u),N&&s(D),O(A,N),N&&s(B),N&&s(P)}}}function Pd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("As we can see, the second set of labels has been padded to the length of the first one using "),m=o("code"),$=a("-100"),C=a("s.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"As we can see, the second set of labels has been padded to the length of the first one using "),m=r(y,"CODE",{});var T=i(m);$=l(T,"-100"),T.forEach(s),C=l(y,"s."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},i:cd,o:cd,d(v){v&&s(u)}}}function dd(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U,L,X,te,se,ne,K,ce,J,W,R,ae,he,ye,le,Ie,V,st,ie,dt,ts,at,Js,Tt,ss,_e,Ts,Oe,de,He,as,Q,Ce,ut,re,Dt,ls,za,lt,We,Qs,Ne,Pt,ft,zt,qt,ns,ue,mt,At,be,ea,fe,It,Nt,je,qa,Se,Aa,Ia,St,Te,Ds,ee,De,_t,we,Na,nt,Sa,ta,me,Ps,zs,sa,bt,os,ot,aa,Ge,la,Ue,rs,ge,na,Ye,wt,gt,ke,Xe,qs,As,Fa,is,kt,Ft,vt,oa,$t,Ze,Fe,Lt,rt,La,ps,ve,Is,Le;return $=new Ct({}),K=new q({props:{code:`id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}`,highlighted:`id2label = {<span class="hljs-built_in">str</span>(i): label <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(label_names)}
label2id = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}`}}),le=new q({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`}}),_e=new q({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),Oe=new q({props:{code:"9",highlighted:'<span class="hljs-number">9</span>'}}),He=new Lo({props:{warning:!0,$$slots:{default:[zd]},$$scope:{ctx:Y}}}),re=new Ct({}),Pt=new q({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),be=new q({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),De=new q({props:{code:`from transformers import create_optimizer
import tensorflow as tf

# Train in mixed-precision float16
# Comment this line out if you're using a GPU that will not benefit from this
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># Train in mixed-precision float16</span>
<span class="hljs-comment"># Comment this line out if you&#x27;re using a GPU that will not benefit from this</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)

<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_epochs = <span class="hljs-number">3</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),Ue=new q({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(output_dir=<span class="hljs-string">&quot;bert-finetuned-ner&quot;</span>, tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)`}}),Fe=new Lo({props:{$$slots:{default:[qd]},$$scope:{ctx:Y}}}),{c(){u=o("h3"),k=o("a"),m=o("span"),j($.$$.fragment),C=c(),v=o("span"),y=a("Defining the model"),T=c(),D=o("p"),A=a("Since we are working on a token classification problem, we will use the "),B=o("code"),P=a("TFAutoModelForTokenClassification"),S=a(" class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the "),M=o("code"),N=a("num_labels"),H=a(" argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it\u2019s better to set the correct label correspondences instead."),Z=c(),F=o("p"),G=a("They should be set by two dictionaries, "),z=o("code"),U=a("id2label"),L=a(" and "),X=o("code"),te=a("label2id"),se=a(", which contain the mapping from ID to label and vice versa:"),ne=c(),j(K.$$.fragment),ce=c(),J=o("p"),W=a("Now we can just pass them to the "),R=o("code"),ae=a("TFAutoModelForTokenClassification.from_pretrained()"),he=a(" method, and they will be set in the model\u2019s configuration, then properly saved and uploaded to the Hub:"),ye=c(),j(le.$$.fragment),Ie=c(),V=o("p"),st=a("Like when we defined our "),ie=o("code"),dt=a("TFAutoModelForSequenceClassification"),ts=a(" in "),at=o("a"),Js=a("Chapter 3"),Tt=a(", creating the model issues a warning that some weights were not used (the ones from the pretraining head) and some other weights are randomly initialized (the ones from the new token classification head), and that this model should be trained. We will do that in a minute, but first let\u2019s double-check that our model has the right number of labels:"),ss=c(),j(_e.$$.fragment),Ts=c(),j(Oe.$$.fragment),de=c(),j(He.$$.fragment),as=c(),Q=o("h3"),Ce=o("a"),ut=o("span"),j(re.$$.fragment),Dt=c(),ls=o("span"),za=a("Fine-tuning the model"),lt=c(),We=o("p"),Qs=a("We are now ready to train our model! We have just a little more housekeeping to do first, though: we should log in to Hugging Face and define our training hyperparameters. If you\u2019re working in a notebook, there\u2019s a convenience function to help you with this:"),Ne=c(),j(Pt.$$.fragment),ft=c(),zt=o("p"),qt=a("This will display a widget where you can enter your Hugging Face login credentials."),ns=c(),ue=o("p"),mt=a("If you aren\u2019t working in a notebook, just type the following line in your terminal:"),At=c(),j(be.$$.fragment),ea=c(),fe=o("p"),It=a("After logging in, we can prepare everything we need to compile our model. \u{1F917} Transformers provides a convenient "),Nt=o("code"),je=a("create_optimizer()"),qa=a(" function that will give you an "),Se=o("code"),Aa=a("AdamW"),Ia=a(" optimizer with appropriate settings for the weight decay and learning rate decay, both of which will improve your model\u2019s performance compared to the built-in "),St=o("code"),Te=a("Adam"),Ds=a(" optimizer:"),ee=c(),j(De.$$.fragment),_t=c(),we=o("p"),Na=a("Note also that we don\u2019t supply a "),nt=o("code"),Sa=a("loss"),ta=a(" argument to "),me=o("code"),Ps=a("compile()"),zs=a(". This is because the models can actually compute loss internally \u2014 if you compile without a loss and supply your labels in the input dictionary (as we do in our datasets), then the model will train using that internal loss, which will be appropriate for the task and model type you have chosen."),sa=c(),bt=o("p"),os=a("Next, we define a "),ot=o("code"),aa=a("PushToHubCallback"),Ge=a(" to upload our model to the Hub during training, and fit the model with that callback:"),la=c(),j(Ue.$$.fragment),rs=c(),ge=o("p"),na=a("You can specify the full name of the repository you want to push to with the "),Ye=o("code"),wt=a("hub_model_id"),gt=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),ke=o("a"),Xe=o("code"),qs=a("huggingface-course"),As=a(" organization"),Fa=a(", we added "),is=o("code"),kt=a('hub_model_id="huggingface-course/bert-finetuned-ner"'),Ft=a(". By default, the repository used will be in your namespace and named after the output directory you set, for example "),vt=o("code"),oa=a('"cool_huggingface_user/bert-finetuned-ner"'),$t=a("."),Ze=c(),j(Fe.$$.fragment),Lt=c(),rt=o("p"),La=a("Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),ps=c(),ve=o("p"),Is=a("At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task \u2014 congratulations! But how good is our model, really? We should evaluate some metrics to find out."),this.h()},l(f){u=r(f,"H3",{class:!0});var I=i(u);k=r(I,"A",{id:!0,class:!0,href:!0});var cs=i(k);m=r(cs,"SPAN",{});var Ns=i(m);E($.$$.fragment,Ns),Ns.forEach(s),cs.forEach(s),C=h(I),v=r(I,"SPAN",{});var Rt=i(v);y=l(Rt,"Defining the model"),Rt.forEach(s),I.forEach(s),T=h(f),D=r(f,"P",{});var hs=i(D);A=l(hs,"Since we are working on a token classification problem, we will use the "),B=r(hs,"CODE",{});var Ss=i(B);P=l(Ss,"TFAutoModelForTokenClassification"),Ss.forEach(s),S=l(hs," class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the "),M=r(hs,"CODE",{});var ds=i(M);N=l(ds,"num_labels"),ds.forEach(s),H=l(hs," argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it\u2019s better to set the correct label correspondences instead."),hs.forEach(s),Z=h(f),F=r(f,"P",{});var Bt=i(F);G=l(Bt,"They should be set by two dictionaries, "),z=r(Bt,"CODE",{});var Mt=i(z);U=l(Mt,"id2label"),Mt.forEach(s),L=l(Bt," and "),X=r(Bt,"CODE",{});var us=i(X);te=l(us,"label2id"),us.forEach(s),se=l(Bt,", which contain the mapping from ID to label and vice versa:"),Bt.forEach(s),ne=h(f),E(K.$$.fragment,f),ce=h(f),J=r(f,"P",{});var pe=i(J);W=l(pe,"Now we can just pass them to the "),R=r(pe,"CODE",{});var Ra=i(R);ae=l(Ra,"TFAutoModelForTokenClassification.from_pretrained()"),Ra.forEach(s),he=l(pe," method, and they will be set in the model\u2019s configuration, then properly saved and uploaded to the Hub:"),pe.forEach(s),ye=h(f),E(le.$$.fragment,f),Ie=h(f),V=r(f,"P",{});var $e=i(V);st=l($e,"Like when we defined our "),ie=r($e,"CODE",{});var Ht=i(ie);dt=l(Ht,"TFAutoModelForSequenceClassification"),Ht.forEach(s),ts=l($e," in "),at=r($e,"A",{href:!0});var ra=i(at);Js=l(ra,"Chapter 3"),ra.forEach(s),Tt=l($e,", creating the model issues a warning that some weights were not used (the ones from the pretraining head) and some other weights are randomly initialized (the ones from the new token classification head), and that this model should be trained. We will do that in a minute, but first let\u2019s double-check that our model has the right number of labels:"),$e.forEach(s),ss=h(f),E(_e.$$.fragment,f),Ts=h(f),E(Oe.$$.fragment,f),de=h(f),E(He.$$.fragment,f),as=h(f),Q=r(f,"H3",{class:!0});var Ke=i(Q);Ce=r(Ke,"A",{id:!0,class:!0,href:!0});var _l=i(Ce);ut=r(_l,"SPAN",{});var ia=i(ut);E(re.$$.fragment,ia),ia.forEach(s),_l.forEach(s),Dt=h(Ke),ls=r(Ke,"SPAN",{});var pa=i(ls);za=l(pa,"Fine-tuning the model"),pa.forEach(s),Ke.forEach(s),lt=h(f),We=r(f,"P",{});var Ba=i(We);Qs=l(Ba,"We are now ready to train our model! We have just a little more housekeeping to do first, though: we should log in to Hugging Face and define our training hyperparameters. If you\u2019re working in a notebook, there\u2019s a convenience function to help you with this:"),Ba.forEach(s),Ne=h(f),E(Pt.$$.fragment,f),ft=h(f),zt=r(f,"P",{});var Wt=i(zt);qt=l(Wt,"This will display a widget where you can enter your Hugging Face login credentials."),Wt.forEach(s),ns=h(f),ue=r(f,"P",{});var ca=i(ue);mt=l(ca,"If you aren\u2019t working in a notebook, just type the following line in your terminal:"),ca.forEach(s),At=h(f),E(be.$$.fragment,f),ea=h(f),fe=r(f,"P",{});var Re=i(fe);It=l(Re,"After logging in, we can prepare everything we need to compile our model. \u{1F917} Transformers provides a convenient "),Nt=r(Re,"CODE",{});var bl=i(Nt);je=l(bl,"create_optimizer()"),bl.forEach(s),qa=l(Re," function that will give you an "),Se=r(Re,"CODE",{});var ha=i(Se);Aa=l(ha,"AdamW"),ha.forEach(s),Ia=l(Re," optimizer with appropriate settings for the weight decay and learning rate decay, both of which will improve your model\u2019s performance compared to the built-in "),St=r(Re,"CODE",{});var Ma=i(St);Te=l(Ma,"Adam"),Ma.forEach(s),Ds=l(Re," optimizer:"),Re.forEach(s),ee=h(f),E(De.$$.fragment,f),_t=h(f),we=r(f,"P",{});var Pe=i(we);Na=l(Pe,"Note also that we don\u2019t supply a "),nt=r(Pe,"CODE",{});var yt=i(nt);Sa=l(yt,"loss"),yt.forEach(s),ta=l(Pe," argument to "),me=r(Pe,"CODE",{});var jt=i(me);Ps=l(jt,"compile()"),jt.forEach(s),zs=l(Pe,". This is because the models can actually compute loss internally \u2014 if you compile without a loss and supply your labels in the input dictionary (as we do in our datasets), then the model will train using that internal loss, which will be appropriate for the task and model type you have chosen."),Pe.forEach(s),sa=h(f),bt=r(f,"P",{});var it=i(bt);os=l(it,"Next, we define a "),ot=r(it,"CODE",{});var fs=i(ot);aa=l(fs,"PushToHubCallback"),fs.forEach(s),Ge=l(it," to upload our model to the Hub during training, and fit the model with that callback:"),it.forEach(s),la=h(f),E(Ue.$$.fragment,f),rs=h(f),ge=r(f,"P",{});var Ee=i(ge);na=l(Ee,"You can specify the full name of the repository you want to push to with the "),Ye=r(Ee,"CODE",{});var pt=i(Ye);wt=l(pt,"hub_model_id"),pt.forEach(s),gt=l(Ee," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),ke=r(Ee,"A",{href:!0,rel:!0});var da=i(ke);Xe=r(da,"CODE",{});var xe=i(Xe);qs=l(xe,"huggingface-course"),xe.forEach(s),As=l(da," organization"),da.forEach(s),Fa=l(Ee,", we added "),is=r(Ee,"CODE",{});var wl=i(is);kt=l(wl,'hub_model_id="huggingface-course/bert-finetuned-ner"'),wl.forEach(s),Ft=l(Ee,". By default, the repository used will be in your namespace and named after the output directory you set, for example "),vt=r(Ee,"CODE",{});var ua=i(vt);oa=l(ua,'"cool_huggingface_user/bert-finetuned-ner"'),ua.forEach(s),$t=l(Ee,"."),Ee.forEach(s),Ze=h(f),E(Fe.$$.fragment,f),Lt=h(f),rt=r(f,"P",{});var Fs=i(rt);La=l(Fs,"Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),Fs.forEach(s),ps=h(f),ve=r(f,"P",{});var gl=i(ve);Is=l(gl,"At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task \u2014 congratulations! But how good is our model, really? We should evaluate some metrics to find out."),gl.forEach(s),this.h()},h(){b(k,"id","defining-the-model"),b(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(k,"href","#defining-the-model"),b(u,"class","relative group"),b(at,"href","/course/chapter3"),b(Ce,"id","finetuning-the-model"),b(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ce,"href","#finetuning-the-model"),b(Q,"class","relative group"),b(ke,"href","https://huggingface.co/huggingface-course"),b(ke,"rel","nofollow")},m(f,I){p(f,u,I),t(u,k),t(k,m),x($,m,null),t(u,C),t(u,v),t(v,y),p(f,T,I),p(f,D,I),t(D,A),t(D,B),t(B,P),t(D,S),t(D,M),t(M,N),t(D,H),p(f,Z,I),p(f,F,I),t(F,G),t(F,z),t(z,U),t(F,L),t(F,X),t(X,te),t(F,se),p(f,ne,I),x(K,f,I),p(f,ce,I),p(f,J,I),t(J,W),t(J,R),t(R,ae),t(J,he),p(f,ye,I),x(le,f,I),p(f,Ie,I),p(f,V,I),t(V,st),t(V,ie),t(ie,dt),t(V,ts),t(V,at),t(at,Js),t(V,Tt),p(f,ss,I),x(_e,f,I),p(f,Ts,I),x(Oe,f,I),p(f,de,I),x(He,f,I),p(f,as,I),p(f,Q,I),t(Q,Ce),t(Ce,ut),x(re,ut,null),t(Q,Dt),t(Q,ls),t(ls,za),p(f,lt,I),p(f,We,I),t(We,Qs),p(f,Ne,I),x(Pt,f,I),p(f,ft,I),p(f,zt,I),t(zt,qt),p(f,ns,I),p(f,ue,I),t(ue,mt),p(f,At,I),x(be,f,I),p(f,ea,I),p(f,fe,I),t(fe,It),t(fe,Nt),t(Nt,je),t(fe,qa),t(fe,Se),t(Se,Aa),t(fe,Ia),t(fe,St),t(St,Te),t(fe,Ds),p(f,ee,I),x(De,f,I),p(f,_t,I),p(f,we,I),t(we,Na),t(we,nt),t(nt,Sa),t(we,ta),t(we,me),t(me,Ps),t(we,zs),p(f,sa,I),p(f,bt,I),t(bt,os),t(bt,ot),t(ot,aa),t(bt,Ge),p(f,la,I),x(Ue,f,I),p(f,rs,I),p(f,ge,I),t(ge,na),t(ge,Ye),t(Ye,wt),t(ge,gt),t(ge,ke),t(ke,Xe),t(Xe,qs),t(ke,As),t(ge,Fa),t(ge,is),t(is,kt),t(ge,Ft),t(ge,vt),t(vt,oa),t(ge,$t),p(f,Ze,I),x(Fe,f,I),p(f,Lt,I),p(f,rt,I),t(rt,La),p(f,ps,I),p(f,ve,I),t(ve,Is),Le=!0},i(f){Le||(w($.$$.fragment,f),w(K.$$.fragment,f),w(le.$$.fragment,f),w(_e.$$.fragment,f),w(Oe.$$.fragment,f),w(He.$$.fragment,f),w(re.$$.fragment,f),w(Pt.$$.fragment,f),w(be.$$.fragment,f),w(De.$$.fragment,f),w(Ue.$$.fragment,f),w(Fe.$$.fragment,f),Le=!0)},o(f){g($.$$.fragment,f),g(K.$$.fragment,f),g(le.$$.fragment,f),g(_e.$$.fragment,f),g(Oe.$$.fragment,f),g(He.$$.fragment,f),g(re.$$.fragment,f),g(Pt.$$.fragment,f),g(be.$$.fragment,f),g(De.$$.fragment,f),g(Ue.$$.fragment,f),g(Fe.$$.fragment,f),Le=!1},d(f){f&&s(u),O($),f&&s(T),f&&s(D),f&&s(Z),f&&s(F),f&&s(ne),O(K,f),f&&s(ce),f&&s(J),f&&s(ye),O(le,f),f&&s(Ie),f&&s(V),f&&s(ss),O(_e,f),f&&s(Ts),O(Oe,f),f&&s(de),O(He,f),f&&s(as),f&&s(Q),O(re),f&&s(lt),f&&s(We),f&&s(Ne),O(Pt,f),f&&s(ft),f&&s(zt),f&&s(ns),f&&s(ue),f&&s(At),O(be,f),f&&s(ea),f&&s(fe),f&&s(ee),O(De,f),f&&s(_t),f&&s(we),f&&s(sa),f&&s(bt),f&&s(la),O(Ue,f),f&&s(rs),f&&s(ge),f&&s(Ze),O(Fe,f),f&&s(Lt),f&&s(rt),f&&s(ps),f&&s(ve)}}}function zd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u26A0\uFE0F If you have a model with the wrong number of labels, you will get an obscure error when calling "),m=o("code"),$=a("model.fit()"),C=a(" later. This can be annoying to debug, so make sure you do this check to confirm you have the expected number of labels.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u26A0\uFE0F If you have a model with the wrong number of labels, you will get an obscure error when calling "),m=r(y,"CODE",{});var T=i(m);$=l(T,"model.fit()"),T.forEach(s),C=l(y," later. This can be annoying to debug, so make sure you do this check to confirm you have the expected number of labels."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function qd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when calling "),m=o("code"),$=a("model.fit()"),C=a(" and will need to set a new name.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when calling "),m=r(y,"CODE",{});var T=i(m);$=l(T,"model.fit()"),T.forEach(s),C=l(y," and will need to set a new name."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function Ad(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U;return B=new q({props:{code:"!pip install seqeval",highlighted:"!pip install seqeval"}}),{c(){u=o("p"),k=a("The traditional framework used to evaluate token classification prediction is "),m=o("a"),$=o("em"),C=a("seqeval"),v=a(". To use this metric, we first need to install the "),y=o("em"),T=a("seqeval"),D=a(" library:"),A=c(),j(B.$$.fragment),P=c(),S=o("p"),M=a("We can then load it via the "),N=o("code"),H=a("load_metric()"),Z=a(" function like we did in "),F=o("a"),G=a("Chapter 3"),z=a(":"),this.h()},l(L){u=r(L,"P",{});var X=i(u);k=l(X,"The traditional framework used to evaluate token classification prediction is "),m=r(X,"A",{href:!0,rel:!0});var te=i(m);$=r(te,"EM",{});var se=i($);C=l(se,"seqeval"),se.forEach(s),te.forEach(s),v=l(X,". To use this metric, we first need to install the "),y=r(X,"EM",{});var ne=i(y);T=l(ne,"seqeval"),ne.forEach(s),D=l(X," library:"),X.forEach(s),A=h(L),E(B.$$.fragment,L),P=h(L),S=r(L,"P",{});var K=i(S);M=l(K,"We can then load it via the "),N=r(K,"CODE",{});var ce=i(N);H=l(ce,"load_metric()"),ce.forEach(s),Z=l(K," function like we did in "),F=r(K,"A",{href:!0});var J=i(F);G=l(J,"Chapter 3"),J.forEach(s),z=l(K,":"),K.forEach(s),this.h()},h(){b(m,"href","https://github.com/chakki-works/seqeval"),b(m,"rel","nofollow"),b(F,"href","/course/chapter3")},m(L,X){p(L,u,X),t(u,k),t(u,m),t(m,$),t($,C),t(u,v),t(u,y),t(y,T),t(u,D),p(L,A,X),x(B,L,X),p(L,P,X),p(L,S,X),t(S,M),t(S,N),t(N,H),t(S,Z),t(S,F),t(F,G),t(S,z),U=!0},i(L){U||(w(B.$$.fragment,L),U=!0)},o(L){g(B.$$.fragment,L),U=!1},d(L){L&&s(u),L&&s(A),O(B,L),L&&s(P),L&&s(S)}}}function Id(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U,L,X,te,se,ne,K,ce,J,W;return z=new q({props:{code:"!pip install seqeval",highlighted:"!pip install seqeval"}}),{c(){u=o("p"),k=a("To have the "),m=o("code"),$=a("Trainer"),C=a(" compute a metric every epoch, we will need to define a "),v=o("code"),y=a("compute_metrics()"),T=a(" function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values."),D=c(),A=o("p"),B=a("The traditional framework used to evaluate token classification prediction is "),P=o("a"),S=o("em"),M=a("seqeval"),N=a(". To use this metric, we first need to install the "),H=o("em"),Z=a("seqeval"),F=a(" library:"),G=c(),j(z.$$.fragment),U=c(),L=o("p"),X=a("We can then load it via the "),te=o("code"),se=a("load_metric()"),ne=a(" function like we did in "),K=o("a"),ce=a("Chapter 3"),J=a(":"),this.h()},l(R){u=r(R,"P",{});var ae=i(u);k=l(ae,"To have the "),m=r(ae,"CODE",{});var he=i(m);$=l(he,"Trainer"),he.forEach(s),C=l(ae," compute a metric every epoch, we will need to define a "),v=r(ae,"CODE",{});var ye=i(v);y=l(ye,"compute_metrics()"),ye.forEach(s),T=l(ae," function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values."),ae.forEach(s),D=h(R),A=r(R,"P",{});var le=i(A);B=l(le,"The traditional framework used to evaluate token classification prediction is "),P=r(le,"A",{href:!0,rel:!0});var Ie=i(P);S=r(Ie,"EM",{});var V=i(S);M=l(V,"seqeval"),V.forEach(s),Ie.forEach(s),N=l(le,". To use this metric, we first need to install the "),H=r(le,"EM",{});var st=i(H);Z=l(st,"seqeval"),st.forEach(s),F=l(le," library:"),le.forEach(s),G=h(R),E(z.$$.fragment,R),U=h(R),L=r(R,"P",{});var ie=i(L);X=l(ie,"We can then load it via the "),te=r(ie,"CODE",{});var dt=i(te);se=l(dt,"load_metric()"),dt.forEach(s),ne=l(ie," function like we did in "),K=r(ie,"A",{href:!0});var ts=i(K);ce=l(ts,"Chapter 3"),ts.forEach(s),J=l(ie,":"),ie.forEach(s),this.h()},h(){b(P,"href","https://github.com/chakki-works/seqeval"),b(P,"rel","nofollow"),b(K,"href","/course/chapter3")},m(R,ae){p(R,u,ae),t(u,k),t(u,m),t(m,$),t(u,C),t(u,v),t(v,y),t(u,T),p(R,D,ae),p(R,A,ae),t(A,B),t(A,P),t(P,S),t(S,M),t(A,N),t(A,H),t(H,Z),t(A,F),p(R,G,ae),x(z,R,ae),p(R,U,ae),p(R,L,ae),t(L,X),t(L,te),t(te,se),t(L,ne),t(L,K),t(K,ce),t(L,J),W=!0},i(R){W||(w(z.$$.fragment,R),W=!0)},o(R){g(z.$$.fragment,R),W=!1},d(R){R&&s(u),R&&s(D),R&&s(A),R&&s(G),O(z,R),R&&s(U),R&&s(L)}}}function Nd(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G;return S=new q({props:{code:`import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

all_predictions = []
all_labels = []
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tf_eval_dataset:
    logits = model.predict(batch)[<span class="hljs-string">&quot;logits&quot;</span>]
    labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]
    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
    <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels):
        <span class="hljs-keyword">for</span> predicted_idx, label_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label):
            <span class="hljs-keyword">if</span> label_idx == -<span class="hljs-number">100</span>:
                <span class="hljs-keyword">continue</span>
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])`}}),N=new q({props:{code:`{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}`,highlighted:`{<span class="hljs-string">&#x27;LOC&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">0.91</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.92</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.91</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">1668</span>},
 <span class="hljs-string">&#x27;MISC&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">0.70</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.79</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.74</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">702</span>},
 <span class="hljs-string">&#x27;ORG&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">0.85</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.90</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">1661</span>},
 <span class="hljs-string">&#x27;PER&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">1617</span>},
 <span class="hljs-string">&#x27;overall_precision&#x27;</span>: <span class="hljs-number">0.87</span>,
 <span class="hljs-string">&#x27;overall_recall&#x27;</span>: <span class="hljs-number">0.91</span>,
 <span class="hljs-string">&#x27;overall_f1&#x27;</span>: <span class="hljs-number">0.89</span>,
 <span class="hljs-string">&#x27;overall_accuracy&#x27;</span>: <span class="hljs-number">0.97</span>}`}}),{c(){u=o("p"),k=a("This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. Now let\u2019s see what happens if we try using our actual model predictions to compute some real scores."),m=c(),$=o("p"),C=a("TensorFlow doesn\u2019t like concatenating our predictions together, because they have variable sequence lengths. This means we can\u2019t just use "),v=o("code"),y=a("model.predict()"),T=a(" \u2014 but that\u2019s not going to stop us. We\u2019ll get some predictions a batch at a time and concatenate them into one big long list as we go, dropping the "),D=o("code"),A=a("-100"),B=a(" tokens that indicate masking/padding, then compute metrics on the list at the end:"),P=c(),j(S.$$.fragment),M=c(),j(N.$$.fragment),H=c(),Z=o("p"),F=a("How did your model do, compared to ours? If you got similar numbers, your training was a success!")},l(z){u=r(z,"P",{});var U=i(u);k=l(U,"This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. Now let\u2019s see what happens if we try using our actual model predictions to compute some real scores."),U.forEach(s),m=h(z),$=r(z,"P",{});var L=i($);C=l(L,"TensorFlow doesn\u2019t like concatenating our predictions together, because they have variable sequence lengths. This means we can\u2019t just use "),v=r(L,"CODE",{});var X=i(v);y=l(X,"model.predict()"),X.forEach(s),T=l(L," \u2014 but that\u2019s not going to stop us. We\u2019ll get some predictions a batch at a time and concatenate them into one big long list as we go, dropping the "),D=r(L,"CODE",{});var te=i(D);A=l(te,"-100"),te.forEach(s),B=l(L," tokens that indicate masking/padding, then compute metrics on the list at the end:"),L.forEach(s),P=h(z),E(S.$$.fragment,z),M=h(z),E(N.$$.fragment,z),H=h(z),Z=r(z,"P",{});var se=i(Z);F=l(se,"How did your model do, compared to ours? If you got similar numbers, your training was a success!"),se.forEach(s)},m(z,U){p(z,u,U),t(u,k),p(z,m,U),p(z,$,U),t($,C),t($,v),t(v,y),t($,T),t($,D),t(D,A),t($,B),p(z,P,U),x(S,z,U),p(z,M,U),x(N,z,U),p(z,H,U),p(z,Z,U),t(Z,F),G=!0},i(z){G||(w(S.$$.fragment,z),w(N.$$.fragment,z),G=!0)},o(z){g(S.$$.fragment,z),g(N.$$.fragment,z),G=!1},d(z){z&&s(u),z&&s(m),z&&s($),z&&s(P),O(S,z),z&&s(M),O(N,z),z&&s(H),z&&s(Z)}}}function Sd(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U,L,X,te,se,ne,K,ce,J;return G=new q({props:{code:`import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Remove ignored index (special tokens) and convert to labels</span>
    true_labels = [[label_names[l] <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> label <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]
    true_predictions = [
        [label_names[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;precision&quot;</span>: all_metrics[<span class="hljs-string">&quot;overall_precision&quot;</span>],
        <span class="hljs-string">&quot;recall&quot;</span>: all_metrics[<span class="hljs-string">&quot;overall_recall&quot;</span>],
        <span class="hljs-string">&quot;f1&quot;</span>: all_metrics[<span class="hljs-string">&quot;overall_f1&quot;</span>],
        <span class="hljs-string">&quot;accuracy&quot;</span>: all_metrics[<span class="hljs-string">&quot;overall_accuracy&quot;</span>],
    }`}}),{c(){u=o("p"),k=a("This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. For our metric computation we will only keep the overall score, but feel free to tweak the "),m=o("code"),$=a("compute_metrics()"),C=a(" function to return all the metrics you would like reported."),v=c(),y=o("p"),T=a("This "),D=o("code"),A=a("compute_metrics()"),B=a(" function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don\u2019t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is "),P=o("code"),S=a("-100"),M=a(", then pass the results to the "),N=o("code"),H=a("metric.compute()"),Z=a(" method:"),F=c(),j(G.$$.fragment),z=c(),U=o("p"),L=a("Now that this is done, we are almost ready to define our "),X=o("code"),te=a("Trainer"),se=a(". We just need a "),ne=o("code"),K=a("model"),ce=a(" to fine-tune!")},l(W){u=r(W,"P",{});var R=i(u);k=l(R,"This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. For our metric computation we will only keep the overall score, but feel free to tweak the "),m=r(R,"CODE",{});var ae=i(m);$=l(ae,"compute_metrics()"),ae.forEach(s),C=l(R," function to return all the metrics you would like reported."),R.forEach(s),v=h(W),y=r(W,"P",{});var he=i(y);T=l(he,"This "),D=r(he,"CODE",{});var ye=i(D);A=l(ye,"compute_metrics()"),ye.forEach(s),B=l(he," function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don\u2019t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is "),P=r(he,"CODE",{});var le=i(P);S=l(le,"-100"),le.forEach(s),M=l(he,", then pass the results to the "),N=r(he,"CODE",{});var Ie=i(N);H=l(Ie,"metric.compute()"),Ie.forEach(s),Z=l(he," method:"),he.forEach(s),F=h(W),E(G.$$.fragment,W),z=h(W),U=r(W,"P",{});var V=i(U);L=l(V,"Now that this is done, we are almost ready to define our "),X=r(V,"CODE",{});var st=i(X);te=l(st,"Trainer"),st.forEach(s),se=l(V,". We just need a "),ne=r(V,"CODE",{});var ie=i(ne);K=l(ie,"model"),ie.forEach(s),ce=l(V," to fine-tune!"),V.forEach(s)},m(W,R){p(W,u,R),t(u,k),t(u,m),t(m,$),t(u,C),p(W,v,R),p(W,y,R),t(y,T),t(y,D),t(D,A),t(y,B),t(y,P),t(P,S),t(y,M),t(y,N),t(N,H),t(y,Z),p(W,F,R),x(G,W,R),p(W,z,R),p(W,U,R),t(U,L),t(U,X),t(X,te),t(U,se),t(U,ne),t(ne,K),t(U,ce),J=!0},i(W){J||(w(G.$$.fragment,W),J=!0)},o(W){g(G.$$.fragment,W),J=!1},d(W){W&&s(u),W&&s(v),W&&s(y),W&&s(F),O(G,W),W&&s(z),W&&s(U)}}}function ud(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U,L,X,te,se,ne,K,ce,J,W,R,ae,he,ye,le,Ie,V,st,ie,dt,ts,at,Js,Tt,ss,_e,Ts,Oe,de,He,as,Q,Ce,ut,re,Dt,ls,za,lt,We,Qs,Ne,Pt,ft,zt,qt,ns,ue,mt,At,be,ea,fe,It,Nt,je,qa,Se,Aa,Ia,St,Te,Ds,ee,De,_t,we,Na,nt,Sa,ta,me,Ps,zs,sa,bt,os,ot,aa,Ge,la,Ue,rs,ge,na,Ye,wt,gt,ke,Xe,qs,As,Fa,is,kt,Ft,vt,oa,$t,Ze,Fe,Lt,rt,La,ps,ve,Is,Le,f,I,cs,Ns,Rt,hs,Ss,ds,Bt,Mt,us,pe,Ra,$e,Ht,ra,Ke,_l,ia,pa,Ba,Wt,ca,Re,bl,ha,Ma,Pe,yt,jt,it,fs,Ee,pt,da,xe,wl,ua,Fs,gl,Ql,Fn,Ve,fa,en,Ro,Ln,Ha,Gt,ma,Bo,Rn,Ls,Bn,ms,tn,sn,Mo,Ho,Et,an,Wo,Mn,Rs,Hn,_a,ln,nn,Go,Uo,Ut,Bs,Wn,Wa,kl,_s,Yo,Ga,Xo,Zo,on,bs,rn,Gn,Ua,vl,xt,Ko,Ya,Vo,Jo,$l,yl,Qo,jl,Xa,Un,ba,wa,Yt,Za,El,xl,er,Ol,Ka,Yn,Xt,Va,Cl,ws,tr,Ja,sr,ar,Tl,Ms,ga,Dl,Ot,Xn,Zt,ka,Pl,Je,lr,Qa,nr,Zn,Hs,Kn,gs,Vn,ks,Jn,zl,pn,Qn,vs,va,$a,cn,hn,or,rr,Qe,el,ql,ze,ir,tl,pr,cr,Ws,hr,dr,Gs,ur,fr,dn,Us,mr,sl,_r,br,Al,al,eo,ya,ll,$s,to,Ys,so,Il,un,ao,nl,fn,oe,lo,Xs,no,qe,mn,_n,wr,bn,wn,gr,gn,kn,kr,vn,$n,vr,yn,jn,$r,En,oo,ja,ro,ct,yr,xn,io,ol,Ea,Zs,po,et,co;return $=new Ct({}),K=new q({props:{code:`id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}`,highlighted:`id2label = {<span class="hljs-built_in">str</span>(i): label <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(label_names)}
label2id = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}`}}),le=new q({props:{code:`from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`}}),_e=new q({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),Oe=new q({props:{code:"9",highlighted:'<span class="hljs-number">9</span>'}}),He=new Lo({props:{warning:!0,$$slots:{default:[Fd]},$$scope:{ctx:Y}}}),re=new Ct({}),qt=new q({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),It=new q({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),Te=new q({props:{code:`from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

args = TrainingArguments(
    <span class="hljs-string">&quot;bert-finetuned-ner&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)`}}),wt=new Lo({props:{$$slots:{default:[Ld]},$$scope:{ctx:Y}}}),kt=new q({props:{code:`from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()`}}),ve=new q({props:{code:'trainer.push_to_hub(commit_message="Training complete")',highlighted:'trainer.push_to_hub(commit_message=<span class="hljs-string">&quot;Training complete&quot;</span>)'}}),cs=new q({props:{code:"'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed&#x27;</span>'}}),Ke=new Ct({}),it=new Ct({}),Ha=new q({props:{code:`from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=<span class="hljs-number">8</span>
)`}}),Ls=new q({props:{code:`model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)`}}),Rs=new q({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`}}),Bs=new q({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),Wa=new Lo({props:{$$slots:{default:[Rd]},$$scope:{ctx:Y}}}),Ua=new q({props:{code:`from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">3</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),wa=new q({props:{code:`from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, get_full_repo_name

model_name = <span class="hljs-string">&quot;bert-finetuned-ner-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),Za=new q({props:{code:"'sgugger/bert-finetuned-ner-accelerate'",highlighted:'<span class="hljs-string">&#x27;sgugger/bert-finetuned-ner-accelerate&#x27;</span>'}}),Ka=new q({props:{code:`output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`output_dir = <span class="hljs-string">&quot;bert-finetuned-ner-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),Ot=new Ct({}),ks=new q({props:{code:`def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess</span>(<span class="hljs-params">predictions, labels</span>):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    <span class="hljs-comment"># Remove ignored index (special tokens) and convert to labels</span>
    true_labels = [[label_names[l] <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> label <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]
    true_predictions = [
        [label_names[p] <span class="hljs-keyword">for</span> (p, l) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prediction, label) <span class="hljs-keyword">if</span> l != -<span class="hljs-number">100</span>]
        <span class="hljs-keyword">for</span> prediction, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(predictions, labels)
    ]
    <span class="hljs-keyword">return</span> true_labels, true_predictions`}}),Ys=new q({props:{code:`from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Training</span>
    model.train()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-<span class="hljs-number">1</span>)
        labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

        <span class="hljs-comment"># Necessary to pad predictions and labels for being gathered</span>
        predictions = accelerator.pad_across_processes(predictions, dim=<span class="hljs-number">1</span>, pad_index=-<span class="hljs-number">100</span>)
        labels = accelerator.pad_across_processes(labels, dim=<span class="hljs-number">1</span>, pad_index=-<span class="hljs-number">100</span>)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    <span class="hljs-built_in">print</span>(
        <span class="hljs-string">f&quot;epoch <span class="hljs-subst">{epoch}</span>:&quot;</span>,
        {
            key: results[<span class="hljs-string">f&quot;overall_<span class="hljs-subst">{key}</span>&quot;</span>]
            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;precision&quot;</span>, <span class="hljs-string">&quot;recall&quot;</span>, <span class="hljs-string">&quot;f1&quot;</span>, <span class="hljs-string">&quot;accuracy&quot;</span>]
        },
    )

    <span class="hljs-comment"># Save and upload</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),nl=new q({props:{code:`accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)`,highlighted:`accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)`}}),{c(){u=o("h3"),k=o("a"),m=o("span"),j($.$$.fragment),C=c(),v=o("span"),y=a("Defining the model"),T=c(),D=o("p"),A=a("Since we are working on a token classification problem, we will use the "),B=o("code"),P=a("AutoModelForTokenClassification"),S=a(" class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the "),M=o("code"),N=a("num_labels"),H=a(" argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it\u2019s better to set the correct label correspondences instead."),Z=c(),F=o("p"),G=a("They should be set by two dictionaries, "),z=o("code"),U=a("id2label"),L=a(" and "),X=o("code"),te=a("label2id"),se=a(", which contain the mappings from ID to label and vice versa:"),ne=c(),j(K.$$.fragment),ce=c(),J=o("p"),W=a("Now we can just pass them to the "),R=o("code"),ae=a("AutoModelForTokenClassification.from_pretrained()"),he=a(" method, and they will be set in the model\u2019s configuration and then properly saved and uploaded to the Hub:"),ye=c(),j(le.$$.fragment),Ie=c(),V=o("p"),st=a("Like when we defined our "),ie=o("code"),dt=a("AutoModelForSequenceClassification"),ts=a(" in "),at=o("a"),Js=a("Chapter 3"),Tt=a(", creating the model issues a warning that some weights were not used (the ones from the pretraining head) and some other weights are randomly initialized (the ones from the new token classification head), and that this model should be trained. We will do that in a minute, but first let\u2019s double-check that our model has the right number of labels:"),ss=c(),j(_e.$$.fragment),Ts=c(),j(Oe.$$.fragment),de=c(),j(He.$$.fragment),as=c(),Q=o("h3"),Ce=o("a"),ut=o("span"),j(re.$$.fragment),Dt=c(),ls=o("span"),za=a("Fine-tuning the model"),lt=c(),We=o("p"),Qs=a("We are now ready to train our model! We just need to do two last things before we define our "),Ne=o("code"),Pt=a("Trainer"),ft=a(": log in to Hugging Face and define our training arguments. If you\u2019re working in a notebook, there\u2019s a convenience function to help you with this:"),zt=c(),j(qt.$$.fragment),ns=c(),ue=o("p"),mt=a("This will display a widget where you can enter your Hugging Face login credentials."),At=c(),be=o("p"),ea=a("If you aren\u2019t working in a notebook, just type the following line in your terminal:"),fe=c(),j(It.$$.fragment),Nt=c(),je=o("p"),qa=a("Once this is done, we can define our "),Se=o("code"),Aa=a("TrainingArguments"),Ia=a(":"),St=c(),j(Te.$$.fragment),Ds=c(),ee=o("p"),De=a("You\u2019ve seen most of those before: we set some hyperparameters (like the learning rate, the number of epochs to train for, and the weight decay), and we specify "),_t=o("code"),we=a("push_to_hub=True"),Na=a(" to indicate that we want to save the model and evaluate it at the end of every epoch, and that we want to upload our results to the Model Hub. Note that you can specify the name of the repository you want to push to with the "),nt=o("code"),Sa=a("hub_model_id"),ta=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),me=o("a"),Ps=o("code"),zs=a("huggingface-course"),sa=a(" organization"),bt=a(", we added "),os=o("code"),ot=a('hub_model_id="huggingface-course/bert-finetuned-ner"'),aa=a(" to "),Ge=o("code"),la=a("TrainingArguments"),Ue=a(". By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be "),rs=o("code"),ge=a('"sgugger/bert-finetuned-ner"'),na=a("."),Ye=c(),j(wt.$$.fragment),gt=c(),ke=o("p"),Xe=a("Finally, we just pass everything to the "),qs=o("code"),As=a("Trainer"),Fa=a(" and launch the training:"),is=c(),j(kt.$$.fragment),Ft=c(),vt=o("p"),oa=a("Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),$t=c(),Ze=o("p"),Fe=a("Once the training is complete, we use the "),Lt=o("code"),rt=a("push_to_hub()"),La=a(" method to make sure we upload the most recent version of the model:"),ps=c(),j(ve.$$.fragment),Is=c(),Le=o("p"),f=a("This command returns the URL of the commit it just did, if you want to inspect it:"),I=c(),j(cs.$$.fragment),Ns=c(),Rt=o("p"),hs=a("The "),Ss=o("code"),ds=a("Trainer"),Bt=a(" also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task \u2014 congratulations!"),Mt=c(),us=o("p"),pe=a("If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using \u{1F917} Accelerate."),Ra=c(),$e=o("h2"),Ht=o("a"),ra=o("span"),j(Ke.$$.fragment),_l=c(),ia=o("span"),pa=a("A custom training loop"),Ba=c(),Wt=o("p"),ca=a("Let\u2019s now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in "),Re=o("a"),bl=a("Chapter 3"),ha=a(", with a few changes for the evaluation."),Ma=c(),Pe=o("h3"),yt=o("a"),jt=o("span"),j(it.$$.fragment),fs=c(),Ee=o("span"),pt=a("Preparing everything for training"),da=c(),xe=o("p"),wl=a("First we need to build the "),ua=o("code"),Fs=a("DataLoader"),gl=a("s from our datasets. We\u2019ll reuse our "),Ql=o("code"),Fn=a("data_collator"),Ve=a(" as a "),fa=o("code"),en=a("collate_fn"),Ro=a(" and shuffle the training set, but not the validation set:"),Ln=c(),j(Ha.$$.fragment),Gt=c(),ma=o("p"),Bo=a("Next we reinstantiate our model, to make sure we\u2019re not continuing the fine-tuning from before but starting from the BERT pretrained model again:"),Rn=c(),j(Ls.$$.fragment),Bn=c(),ms=o("p"),tn=a("Then we will need an optimizer. We\u2019ll use the classic "),sn=o("code"),Mo=a("AdamW"),Ho=a(", which is like "),Et=o("code"),an=a("Adam"),Wo=a(", but with a fix in the way weight decay is applied:"),Mn=c(),j(Rs.$$.fragment),Hn=c(),_a=o("p"),ln=a("Once we have all those objects, we can send them to the "),nn=o("code"),Go=a("accelerator.prepare()"),Uo=a(" method:"),Ut=c(),j(Bs.$$.fragment),Wn=c(),j(Wa.$$.fragment),kl=c(),_s=o("p"),Yo=a("Now that we have sent our "),Ga=o("code"),Xo=a("train_dataloader"),Zo=a(" to "),on=o("code"),bs=a("accelerator.prepare()"),rn=a(", we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"),Gn=c(),j(Ua.$$.fragment),vl=c(),xt=o("p"),Ko=a("Lastly, to push our model to the Hub, we will need to create a "),Ya=o("code"),Vo=a("Repository"),Jo=a(" object in a working folder. First log in to Hugging Face, if you\u2019re not logged in already. We\u2019ll determine the repository name from the model ID we want to give our model (feel free to replace the "),$l=o("code"),yl=a("repo_name"),Qo=a(" with your own choice; it just needs to contain your username, which is what the function "),jl=o("code"),Xa=a("get_full_repo_name()"),Un=a(" does):"),ba=c(),j(wa.$$.fragment),Yt=c(),j(Za.$$.fragment),El=c(),xl=o("p"),er=a("Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:"),Ol=c(),j(Ka.$$.fragment),Yn=c(),Xt=o("p"),Va=a("We can now upload anything we save in "),Cl=o("code"),ws=a("output_dir"),tr=a(" by calling the "),Ja=o("code"),sr=a("repo.push_to_hub()"),ar=a(" method. This will help us upload the intermediate models at the end of each epoch."),Tl=c(),Ms=o("h3"),ga=o("a"),Dl=o("span"),j(Ot.$$.fragment),Xn=c(),Zt=o("span"),ka=a("Training loop"),Pl=c(),Je=o("p"),lr=a("We are now ready to write the full training loop. To simplify its evaluation part, we define this "),Qa=o("code"),nr=a("postprocess()"),Zn=a(" function that takes predictions and labels and converts them to lists of strings, like our "),Hs=o("code"),Kn=a("metric"),gs=a(" object expects:"),Vn=c(),j(ks.$$.fragment),Jn=c(),zl=o("p"),pn=a("Then we can write the training loop. After defining a progress bar to follow how training goes, the loop has three parts:"),Qn=c(),vs=o("ul"),va=o("li"),$a=a("The training in itself, which is the classic iteration over the "),cn=o("code"),hn=a("train_dataloader"),or=a(", forward pass through the model, then backward pass and optimizer step."),rr=c(),Qe=o("li"),el=a("The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use "),ql=o("code"),ze=a("accelerator.pad_across_processes()"),ir=a(" to make the predictions and labels the same shape before calling the "),tl=o("code"),pr=a("gather()"),cr=a(" method. If we don\u2019t do this, the evaluation will either error out or hang forever. Then we send the results to "),Ws=o("code"),hr=a("metric.add_batch()"),dr=a(" and call "),Gs=o("code"),ur=a("metric.compute()"),fr=a(" once the evaluation loop is over."),dn=c(),Us=o("li"),mr=a("Saving and uploading, where we first save the model and the tokenizer, then call "),sl=o("code"),_r=a("repo.push_to_hub()"),br=a(". Notice that we use the argument "),Al=o("code"),al=a("blocking=False"),eo=a(" to tell the \u{1F917} Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background."),ya=c(),ll=o("p"),$s=a("Here\u2019s the complete code for the training loop:"),to=c(),j(Ys.$$.fragment),so=c(),Il=o("p"),un=a("In case this is the first time you\u2019re seeing a model saved with \u{1F917} Accelerate, let\u2019s take a moment to inspect the three lines of code that go with it:"),ao=c(),j(nl.$$.fragment),fn=c(),oe=o("p"),lo=a("The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the "),Xs=o("code"),no=a("unwrapped_model"),qe=a(", which is the base model we defined. The "),mn=o("code"),_n=a("accelerator.prepare()"),wr=a(" method changes the model to work in distributed training, so it won\u2019t have the "),bn=o("code"),wn=a("save_pretrained()"),gr=a(" method anymore; the "),gn=o("code"),kn=a("accelerator.unwrap_model()"),kr=a(" method undoes that step. Lastly, we call "),vn=o("code"),$n=a("save_pretrained()"),vr=a(" but tell that method to use "),yn=o("code"),jn=a("accelerator.save()"),$r=a(" instead of "),En=o("code"),oo=a("torch.save()"),ja=a("."),ro=c(),ct=o("p"),yr=a("Once this is done, you should have a model that produces results pretty similar to the one trained with the "),xn=o("code"),io=a("Trainer"),ol=a(". You can check the model we trained using this code at "),Ea=o("a"),Zs=o("em"),po=a("huggingface-course/bert-finetuned-ner-accelerate"),et=a(". And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!"),this.h()},l(n){u=r(n,"H3",{class:!0});var _=i(u);k=r(_,"A",{id:!0,class:!0,href:!0});var pi=i(k);m=r(pi,"SPAN",{});var ho=i(m);E($.$$.fragment,ho),ho.forEach(s),pi.forEach(s),C=h(_),v=r(_,"SPAN",{});var ci=i(v);y=l(ci,"Defining the model"),ci.forEach(s),_.forEach(s),T=h(n),D=r(n,"P",{});var Nl=i(D);A=l(Nl,"Since we are working on a token classification problem, we will use the "),B=r(Nl,"CODE",{});var uo=i(B);P=l(uo,"AutoModelForTokenClassification"),uo.forEach(s),S=l(Nl," class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the "),M=r(Nl,"CODE",{});var hi=i(M);N=l(hi,"num_labels"),hi.forEach(s),H=l(Nl," argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it\u2019s better to set the correct label correspondences instead."),Nl.forEach(s),Z=h(n),F=r(n,"P",{});var Sl=i(F);G=l(Sl,"They should be set by two dictionaries, "),z=r(Sl,"CODE",{});var fo=i(z);U=l(fo,"id2label"),fo.forEach(s),L=l(Sl," and "),X=r(Sl,"CODE",{});var di=i(X);te=l(di,"label2id"),di.forEach(s),se=l(Sl,", which contain the mappings from ID to label and vice versa:"),Sl.forEach(s),ne=h(n),E(K.$$.fragment,n),ce=h(n),J=r(n,"P",{});var mo=i(J);W=l(mo,"Now we can just pass them to the "),R=r(mo,"CODE",{});var jr=i(R);ae=l(jr,"AutoModelForTokenClassification.from_pretrained()"),jr.forEach(s),he=l(mo," method, and they will be set in the model\u2019s configuration and then properly saved and uploaded to the Hub:"),mo.forEach(s),ye=h(n),E(le.$$.fragment,n),Ie=h(n),V=r(n,"P",{});var ys=i(V);st=l(ys,"Like when we defined our "),ie=r(ys,"CODE",{});var Er=i(ie);dt=l(Er,"AutoModelForSequenceClassification"),Er.forEach(s),ts=l(ys," in "),at=r(ys,"A",{href:!0});var On=i(at);Js=l(On,"Chapter 3"),On.forEach(s),Tt=l(ys,", creating the model issues a warning that some weights were not used (the ones from the pretraining head) and some other weights are randomly initialized (the ones from the new token classification head), and that this model should be trained. We will do that in a minute, but first let\u2019s double-check that our model has the right number of labels:"),ys.forEach(s),ss=h(n),E(_e.$$.fragment,n),Ts=h(n),E(Oe.$$.fragment,n),de=h(n),E(He.$$.fragment,n),as=h(n),Q=r(n,"H3",{class:!0});var _o=i(Q);Ce=r(_o,"A",{id:!0,class:!0,href:!0});var xr=i(Ce);ut=r(xr,"SPAN",{});var Fl=i(ut);E(re.$$.fragment,Fl),Fl.forEach(s),xr.forEach(s),Dt=h(_o),ls=r(_o,"SPAN",{});var Or=i(ls);za=l(Or,"Fine-tuning the model"),Or.forEach(s),_o.forEach(s),lt=h(n),We=r(n,"P",{});var xa=i(We);Qs=l(xa,"We are now ready to train our model! We just need to do two last things before we define our "),Ne=r(xa,"CODE",{});var Cr=i(Ne);Pt=l(Cr,"Trainer"),Cr.forEach(s),ft=l(xa,": log in to Hugging Face and define our training arguments. If you\u2019re working in a notebook, there\u2019s a convenience function to help you with this:"),xa.forEach(s),zt=h(n),E(qt.$$.fragment,n),ns=h(n),ue=r(n,"P",{});var Ks=i(ue);mt=l(Ks,"This will display a widget where you can enter your Hugging Face login credentials."),Ks.forEach(s),At=h(n),be=r(n,"P",{});var ui=i(be);ea=l(ui,"If you aren\u2019t working in a notebook, just type the following line in your terminal:"),ui.forEach(s),fe=h(n),E(It.$$.fragment,n),Nt=h(n),je=r(n,"P",{});var Ll=i(je);qa=l(Ll,"Once this is done, we can define our "),Se=r(Ll,"CODE",{});var fi=i(Se);Aa=l(fi,"TrainingArguments"),fi.forEach(s),Ia=l(Ll,":"),Ll.forEach(s),St=h(n),E(Te.$$.fragment,n),Ds=h(n),ee=r(n,"P",{});var tt=i(ee);De=l(tt,"You\u2019ve seen most of those before: we set some hyperparameters (like the learning rate, the number of epochs to train for, and the weight decay), and we specify "),_t=r(tt,"CODE",{});var bo=i(_t);we=l(bo,"push_to_hub=True"),bo.forEach(s),Na=l(tt," to indicate that we want to save the model and evaluate it at the end of every epoch, and that we want to upload our results to the Model Hub. Note that you can specify the name of the repository you want to push to with the "),nt=r(tt,"CODE",{});var mi=i(nt);Sa=l(mi,"hub_model_id"),mi.forEach(s),ta=l(tt," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),me=r(tt,"A",{href:!0,rel:!0});var Tr=i(me);Ps=r(Tr,"CODE",{});var Dr=i(Ps);zs=l(Dr,"huggingface-course"),Dr.forEach(s),sa=l(Tr," organization"),Tr.forEach(s),bt=l(tt,", we added "),os=r(tt,"CODE",{});var rl=i(os);ot=l(rl,'hub_model_id="huggingface-course/bert-finetuned-ner"'),rl.forEach(s),aa=l(tt," to "),Ge=r(tt,"CODE",{});var Pr=i(Ge);la=l(Pr,"TrainingArguments"),Pr.forEach(s),Ue=l(tt,". By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be "),rs=r(tt,"CODE",{});var ht=i(rs);ge=l(ht,'"sgugger/bert-finetuned-ner"'),ht.forEach(s),na=l(tt,"."),tt.forEach(s),Ye=h(n),E(wt.$$.fragment,n),gt=h(n),ke=r(n,"P",{});var wo=i(ke);Xe=l(wo,"Finally, we just pass everything to the "),qs=r(wo,"CODE",{});var go=i(qs);As=l(go,"Trainer"),go.forEach(s),Fa=l(wo," and launch the training:"),wo.forEach(s),is=h(n),E(kt.$$.fragment,n),Ft=h(n),vt=r(n,"P",{});var _i=i(vt);oa=l(_i,"Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."),_i.forEach(s),$t=h(n),Ze=r(n,"P",{});var ko=i(Ze);Fe=l(ko,"Once the training is complete, we use the "),Lt=r(ko,"CODE",{});var vo=i(Lt);rt=l(vo,"push_to_hub()"),vo.forEach(s),La=l(ko," method to make sure we upload the most recent version of the model:"),ko.forEach(s),ps=h(n),E(ve.$$.fragment,n),Is=h(n),Le=r(n,"P",{});var bi=i(Le);f=l(bi,"This command returns the URL of the commit it just did, if you want to inspect it:"),bi.forEach(s),I=h(n),E(cs.$$.fragment,n),Ns=h(n),Rt=r(n,"P",{});var $o=i(Rt);hs=l($o,"The "),Ss=r($o,"CODE",{});var yo=i(Ss);ds=l(yo,"Trainer"),yo.forEach(s),Bt=l($o," also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task \u2014 congratulations!"),$o.forEach(s),Mt=h(n),us=r(n,"P",{});var wi=i(us);pe=l(wi,"If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using \u{1F917} Accelerate."),wi.forEach(s),Ra=h(n),$e=r(n,"H2",{class:!0});var jo=i($e);Ht=r(jo,"A",{id:!0,class:!0,href:!0});var Eo=i(Ht);ra=r(Eo,"SPAN",{});var gi=i(ra);E(Ke.$$.fragment,gi),gi.forEach(s),Eo.forEach(s),_l=h(jo),ia=r(jo,"SPAN",{});var ki=i(ia);pa=l(ki,"A custom training loop"),ki.forEach(s),jo.forEach(s),Ba=h(n),Wt=r(n,"P",{});var Cn=i(Wt);ca=l(Cn,"Let\u2019s now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in "),Re=r(Cn,"A",{href:!0});var Rl=i(Re);bl=l(Rl,"Chapter 3"),Rl.forEach(s),ha=l(Cn,", with a few changes for the evaluation."),Cn.forEach(s),Ma=h(n),Pe=r(n,"H3",{class:!0});var Tn=i(Pe);yt=r(Tn,"A",{id:!0,class:!0,href:!0});var Dn=i(yt);jt=r(Dn,"SPAN",{});var vi=i(jt);E(it.$$.fragment,vi),vi.forEach(s),Dn.forEach(s),fs=h(Tn),Ee=r(Tn,"SPAN",{});var zr=i(Ee);pt=l(zr,"Preparing everything for training"),zr.forEach(s),Tn.forEach(s),da=h(n),xe=r(n,"P",{});var Kt=i(xe);wl=l(Kt,"First we need to build the "),ua=r(Kt,"CODE",{});var $i=i(ua);Fs=l($i,"DataLoader"),$i.forEach(s),gl=l(Kt,"s from our datasets. We\u2019ll reuse our "),Ql=r(Kt,"CODE",{});var qr=i(Ql);Fn=l(qr,"data_collator"),qr.forEach(s),Ve=l(Kt," as a "),fa=r(Kt,"CODE",{});var Bl=i(fa);en=l(Bl,"collate_fn"),Bl.forEach(s),Ro=l(Kt," and shuffle the training set, but not the validation set:"),Kt.forEach(s),Ln=h(n),E(Ha.$$.fragment,n),Gt=h(n),ma=r(n,"P",{});var Ar=i(ma);Bo=l(Ar,"Next we reinstantiate our model, to make sure we\u2019re not continuing the fine-tuning from before but starting from the BERT pretrained model again:"),Ar.forEach(s),Rn=h(n),E(Ls.$$.fragment,n),Bn=h(n),ms=r(n,"P",{});var Vt=i(ms);tn=l(Vt,"Then we will need an optimizer. We\u2019ll use the classic "),sn=r(Vt,"CODE",{});var yi=i(sn);Mo=l(yi,"AdamW"),yi.forEach(s),Ho=l(Vt,", which is like "),Et=r(Vt,"CODE",{});var Pn=i(Et);an=l(Pn,"Adam"),Pn.forEach(s),Wo=l(Vt,", but with a fix in the way weight decay is applied:"),Vt.forEach(s),Mn=h(n),E(Rs.$$.fragment,n),Hn=h(n),_a=r(n,"P",{});var xo=i(_a);ln=l(xo,"Once we have all those objects, we can send them to the "),nn=r(xo,"CODE",{});var ji=i(nn);Go=l(ji,"accelerator.prepare()"),ji.forEach(s),Uo=l(xo," method:"),xo.forEach(s),Ut=h(n),E(Bs.$$.fragment,n),Wn=h(n),E(Wa.$$.fragment,n),kl=h(n),_s=r(n,"P",{});var il=i(_s);Yo=l(il,"Now that we have sent our "),Ga=r(il,"CODE",{});var js=i(Ga);Xo=l(js,"train_dataloader"),js.forEach(s),Zo=l(il," to "),on=r(il,"CODE",{});var Es=i(on);bs=l(Es,"accelerator.prepare()"),Es.forEach(s),rn=l(il,", we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"),il.forEach(s),Gn=h(n),E(Ua.$$.fragment,n),vl=h(n),xt=r(n,"P",{});var Jt=i(xt);Ko=l(Jt,"Lastly, to push our model to the Hub, we will need to create a "),Ya=r(Jt,"CODE",{});var Oa=i(Ya);Vo=l(Oa,"Repository"),Oa.forEach(s),Jo=l(Jt," object in a working folder. First log in to Hugging Face, if you\u2019re not logged in already. We\u2019ll determine the repository name from the model ID we want to give our model (feel free to replace the "),$l=r(Jt,"CODE",{});var pl=i($l);yl=l(pl,"repo_name"),pl.forEach(s),Qo=l(Jt," with your own choice; it just needs to contain your username, which is what the function "),jl=r(Jt,"CODE",{});var Oo=i(jl);Xa=l(Oo,"get_full_repo_name()"),Oo.forEach(s),Un=l(Jt," does):"),Jt.forEach(s),ba=h(n),E(wa.$$.fragment,n),Yt=h(n),E(Za.$$.fragment,n),El=h(n),xl=r(n,"P",{});var Ml=i(xl);er=l(Ml,"Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:"),Ml.forEach(s),Ol=h(n),E(Ka.$$.fragment,n),Yn=h(n),Xt=r(n,"P",{});var Hl=i(Xt);Va=l(Hl,"We can now upload anything we save in "),Cl=r(Hl,"CODE",{});var Co=i(Cl);ws=l(Co,"output_dir"),Co.forEach(s),tr=l(Hl," by calling the "),Ja=r(Hl,"CODE",{});var Ei=i(Ja);sr=l(Ei,"repo.push_to_hub()"),Ei.forEach(s),ar=l(Hl," method. This will help us upload the intermediate models at the end of each epoch."),Hl.forEach(s),Tl=h(n),Ms=r(n,"H3",{class:!0});var zn=i(Ms);ga=r(zn,"A",{id:!0,class:!0,href:!0});var Qt=i(ga);Dl=r(Qt,"SPAN",{});var xi=i(Dl);E(Ot.$$.fragment,xi),xi.forEach(s),Qt.forEach(s),Xn=h(zn),Zt=r(zn,"SPAN",{});var To=i(Zt);ka=l(To,"Training loop"),To.forEach(s),zn.forEach(s),Pl=h(n),Je=r(n,"P",{});var Wl=i(Je);lr=l(Wl,"We are now ready to write the full training loop. To simplify its evaluation part, we define this "),Qa=r(Wl,"CODE",{});var Oi=i(Qa);nr=l(Oi,"postprocess()"),Oi.forEach(s),Zn=l(Wl," function that takes predictions and labels and converts them to lists of strings, like our "),Hs=r(Wl,"CODE",{});var qn=i(Hs);Kn=l(qn,"metric"),qn.forEach(s),gs=l(Wl," object expects:"),Wl.forEach(s),Vn=h(n),E(ks.$$.fragment,n),Jn=h(n),zl=r(n,"P",{});var Ci=i(zl);pn=l(Ci,"Then we can write the training loop. After defining a progress bar to follow how training goes, the loop has three parts:"),Ci.forEach(s),Qn=h(n),vs=r(n,"UL",{});var Gl=i(vs);va=r(Gl,"LI",{});var Ul=i(va);$a=l(Ul,"The training in itself, which is the classic iteration over the "),cn=r(Ul,"CODE",{});var Ti=i(cn);hn=l(Ti,"train_dataloader"),Ti.forEach(s),or=l(Ul,", forward pass through the model, then backward pass and optimizer step."),Ul.forEach(s),rr=h(Gl),Qe=r(Gl,"LI",{});var xs=i(Qe);el=l(xs,"The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use "),ql=r(xs,"CODE",{});var Ir=i(ql);ze=l(Ir,"accelerator.pad_across_processes()"),Ir.forEach(s),ir=l(xs," to make the predictions and labels the same shape before calling the "),tl=r(xs,"CODE",{});var es=i(tl);pr=l(es,"gather()"),es.forEach(s),cr=l(xs," method. If we don\u2019t do this, the evaluation will either error out or hang forever. Then we send the results to "),Ws=r(xs,"CODE",{});var Di=i(Ws);hr=l(Di,"metric.add_batch()"),Di.forEach(s),dr=l(xs," and call "),Gs=r(xs,"CODE",{});var Yl=i(Gs);ur=l(Yl,"metric.compute()"),Yl.forEach(s),fr=l(xs," once the evaluation loop is over."),xs.forEach(s),dn=h(Gl),Us=r(Gl,"LI",{});var Ca=i(Us);mr=l(Ca,"Saving and uploading, where we first save the model and the tokenizer, then call "),sl=r(Ca,"CODE",{});var Pi=i(sl);_r=l(Pi,"repo.push_to_hub()"),Pi.forEach(s),br=l(Ca,". Notice that we use the argument "),Al=r(Ca,"CODE",{});var zi=i(Al);al=l(zi,"blocking=False"),zi.forEach(s),eo=l(Ca," to tell the \u{1F917} Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background."),Ca.forEach(s),Gl.forEach(s),ya=h(n),ll=r(n,"P",{});var Do=i(ll);$s=l(Do,"Here\u2019s the complete code for the training loop:"),Do.forEach(s),to=h(n),E(Ys.$$.fragment,n),so=h(n),Il=r(n,"P",{});var qi=i(Il);un=l(qi,"In case this is the first time you\u2019re seeing a model saved with \u{1F917} Accelerate, let\u2019s take a moment to inspect the three lines of code that go with it:"),qi.forEach(s),ao=h(n),E(nl.$$.fragment,n),fn=h(n),oe=r(n,"P",{});var Ae=i(oe);lo=l(Ae,"The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the "),Xs=r(Ae,"CODE",{});var Po=i(Xs);no=l(Po,"unwrapped_model"),Po.forEach(s),qe=l(Ae,", which is the base model we defined. The "),mn=r(Ae,"CODE",{});var Ai=i(mn);_n=l(Ai,"accelerator.prepare()"),Ai.forEach(s),wr=l(Ae," method changes the model to work in distributed training, so it won\u2019t have the "),bn=r(Ae,"CODE",{});var Ii=i(bn);wn=l(Ii,"save_pretrained()"),Ii.forEach(s),gr=l(Ae," method anymore; the "),gn=r(Ae,"CODE",{});var Nr=i(gn);kn=l(Nr,"accelerator.unwrap_model()"),Nr.forEach(s),kr=l(Ae," method undoes that step. Lastly, we call "),vn=r(Ae,"CODE",{});var Os=i(vn);$n=l(Os,"save_pretrained()"),Os.forEach(s),vr=l(Ae," but tell that method to use "),yn=r(Ae,"CODE",{});var Cs=i(yn);jn=l(Cs,"accelerator.save()"),Cs.forEach(s),$r=l(Ae," instead of "),En=r(Ae,"CODE",{});var An=i(En);oo=l(An,"torch.save()"),An.forEach(s),ja=l(Ae,"."),Ae.forEach(s),ro=h(n),ct=r(n,"P",{});var Vs=i(ct);yr=l(Vs,"Once this is done, you should have a model that produces results pretty similar to the one trained with the "),xn=r(Vs,"CODE",{});var Ni=i(xn);io=l(Ni,"Trainer"),Ni.forEach(s),ol=l(Vs,". You can check the model we trained using this code at "),Ea=r(Vs,"A",{href:!0,rel:!0});var Sr=i(Ea);Zs=r(Sr,"EM",{});var Xl=i(Zs);po=l(Xl,"huggingface-course/bert-finetuned-ner-accelerate"),Xl.forEach(s),Sr.forEach(s),et=l(Vs,". And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!"),Vs.forEach(s),this.h()},h(){b(k,"id","defining-the-model"),b(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(k,"href","#defining-the-model"),b(u,"class","relative group"),b(at,"href","/course/chapter3"),b(Ce,"id","finetuning-the-model"),b(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ce,"href","#finetuning-the-model"),b(Q,"class","relative group"),b(me,"href","https://huggingface.co/huggingface-course"),b(me,"rel","nofollow"),b(Ht,"id","a-custom-training-loop"),b(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ht,"href","#a-custom-training-loop"),b($e,"class","relative group"),b(Re,"href","/course/chapter3/4"),b(yt,"id","preparing-everything-for-training"),b(yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(yt,"href","#preparing-everything-for-training"),b(Pe,"class","relative group"),b(ga,"id","training-loop"),b(ga,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(ga,"href","#training-loop"),b(Ms,"class","relative group"),b(Ea,"href","https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate"),b(Ea,"rel","nofollow")},m(n,_){p(n,u,_),t(u,k),t(k,m),x($,m,null),t(u,C),t(u,v),t(v,y),p(n,T,_),p(n,D,_),t(D,A),t(D,B),t(B,P),t(D,S),t(D,M),t(M,N),t(D,H),p(n,Z,_),p(n,F,_),t(F,G),t(F,z),t(z,U),t(F,L),t(F,X),t(X,te),t(F,se),p(n,ne,_),x(K,n,_),p(n,ce,_),p(n,J,_),t(J,W),t(J,R),t(R,ae),t(J,he),p(n,ye,_),x(le,n,_),p(n,Ie,_),p(n,V,_),t(V,st),t(V,ie),t(ie,dt),t(V,ts),t(V,at),t(at,Js),t(V,Tt),p(n,ss,_),x(_e,n,_),p(n,Ts,_),x(Oe,n,_),p(n,de,_),x(He,n,_),p(n,as,_),p(n,Q,_),t(Q,Ce),t(Ce,ut),x(re,ut,null),t(Q,Dt),t(Q,ls),t(ls,za),p(n,lt,_),p(n,We,_),t(We,Qs),t(We,Ne),t(Ne,Pt),t(We,ft),p(n,zt,_),x(qt,n,_),p(n,ns,_),p(n,ue,_),t(ue,mt),p(n,At,_),p(n,be,_),t(be,ea),p(n,fe,_),x(It,n,_),p(n,Nt,_),p(n,je,_),t(je,qa),t(je,Se),t(Se,Aa),t(je,Ia),p(n,St,_),x(Te,n,_),p(n,Ds,_),p(n,ee,_),t(ee,De),t(ee,_t),t(_t,we),t(ee,Na),t(ee,nt),t(nt,Sa),t(ee,ta),t(ee,me),t(me,Ps),t(Ps,zs),t(me,sa),t(ee,bt),t(ee,os),t(os,ot),t(ee,aa),t(ee,Ge),t(Ge,la),t(ee,Ue),t(ee,rs),t(rs,ge),t(ee,na),p(n,Ye,_),x(wt,n,_),p(n,gt,_),p(n,ke,_),t(ke,Xe),t(ke,qs),t(qs,As),t(ke,Fa),p(n,is,_),x(kt,n,_),p(n,Ft,_),p(n,vt,_),t(vt,oa),p(n,$t,_),p(n,Ze,_),t(Ze,Fe),t(Ze,Lt),t(Lt,rt),t(Ze,La),p(n,ps,_),x(ve,n,_),p(n,Is,_),p(n,Le,_),t(Le,f),p(n,I,_),x(cs,n,_),p(n,Ns,_),p(n,Rt,_),t(Rt,hs),t(Rt,Ss),t(Ss,ds),t(Rt,Bt),p(n,Mt,_),p(n,us,_),t(us,pe),p(n,Ra,_),p(n,$e,_),t($e,Ht),t(Ht,ra),x(Ke,ra,null),t($e,_l),t($e,ia),t(ia,pa),p(n,Ba,_),p(n,Wt,_),t(Wt,ca),t(Wt,Re),t(Re,bl),t(Wt,ha),p(n,Ma,_),p(n,Pe,_),t(Pe,yt),t(yt,jt),x(it,jt,null),t(Pe,fs),t(Pe,Ee),t(Ee,pt),p(n,da,_),p(n,xe,_),t(xe,wl),t(xe,ua),t(ua,Fs),t(xe,gl),t(xe,Ql),t(Ql,Fn),t(xe,Ve),t(xe,fa),t(fa,en),t(xe,Ro),p(n,Ln,_),x(Ha,n,_),p(n,Gt,_),p(n,ma,_),t(ma,Bo),p(n,Rn,_),x(Ls,n,_),p(n,Bn,_),p(n,ms,_),t(ms,tn),t(ms,sn),t(sn,Mo),t(ms,Ho),t(ms,Et),t(Et,an),t(ms,Wo),p(n,Mn,_),x(Rs,n,_),p(n,Hn,_),p(n,_a,_),t(_a,ln),t(_a,nn),t(nn,Go),t(_a,Uo),p(n,Ut,_),x(Bs,n,_),p(n,Wn,_),x(Wa,n,_),p(n,kl,_),p(n,_s,_),t(_s,Yo),t(_s,Ga),t(Ga,Xo),t(_s,Zo),t(_s,on),t(on,bs),t(_s,rn),p(n,Gn,_),x(Ua,n,_),p(n,vl,_),p(n,xt,_),t(xt,Ko),t(xt,Ya),t(Ya,Vo),t(xt,Jo),t(xt,$l),t($l,yl),t(xt,Qo),t(xt,jl),t(jl,Xa),t(xt,Un),p(n,ba,_),x(wa,n,_),p(n,Yt,_),x(Za,n,_),p(n,El,_),p(n,xl,_),t(xl,er),p(n,Ol,_),x(Ka,n,_),p(n,Yn,_),p(n,Xt,_),t(Xt,Va),t(Xt,Cl),t(Cl,ws),t(Xt,tr),t(Xt,Ja),t(Ja,sr),t(Xt,ar),p(n,Tl,_),p(n,Ms,_),t(Ms,ga),t(ga,Dl),x(Ot,Dl,null),t(Ms,Xn),t(Ms,Zt),t(Zt,ka),p(n,Pl,_),p(n,Je,_),t(Je,lr),t(Je,Qa),t(Qa,nr),t(Je,Zn),t(Je,Hs),t(Hs,Kn),t(Je,gs),p(n,Vn,_),x(ks,n,_),p(n,Jn,_),p(n,zl,_),t(zl,pn),p(n,Qn,_),p(n,vs,_),t(vs,va),t(va,$a),t(va,cn),t(cn,hn),t(va,or),t(vs,rr),t(vs,Qe),t(Qe,el),t(Qe,ql),t(ql,ze),t(Qe,ir),t(Qe,tl),t(tl,pr),t(Qe,cr),t(Qe,Ws),t(Ws,hr),t(Qe,dr),t(Qe,Gs),t(Gs,ur),t(Qe,fr),t(vs,dn),t(vs,Us),t(Us,mr),t(Us,sl),t(sl,_r),t(Us,br),t(Us,Al),t(Al,al),t(Us,eo),p(n,ya,_),p(n,ll,_),t(ll,$s),p(n,to,_),x(Ys,n,_),p(n,so,_),p(n,Il,_),t(Il,un),p(n,ao,_),x(nl,n,_),p(n,fn,_),p(n,oe,_),t(oe,lo),t(oe,Xs),t(Xs,no),t(oe,qe),t(oe,mn),t(mn,_n),t(oe,wr),t(oe,bn),t(bn,wn),t(oe,gr),t(oe,gn),t(gn,kn),t(oe,kr),t(oe,vn),t(vn,$n),t(oe,vr),t(oe,yn),t(yn,jn),t(oe,$r),t(oe,En),t(En,oo),t(oe,ja),p(n,ro,_),p(n,ct,_),t(ct,yr),t(ct,xn),t(xn,io),t(ct,ol),t(ct,Ea),t(Ea,Zs),t(Zs,po),t(ct,et),co=!0},i(n){co||(w($.$$.fragment,n),w(K.$$.fragment,n),w(le.$$.fragment,n),w(_e.$$.fragment,n),w(Oe.$$.fragment,n),w(He.$$.fragment,n),w(re.$$.fragment,n),w(qt.$$.fragment,n),w(It.$$.fragment,n),w(Te.$$.fragment,n),w(wt.$$.fragment,n),w(kt.$$.fragment,n),w(ve.$$.fragment,n),w(cs.$$.fragment,n),w(Ke.$$.fragment,n),w(it.$$.fragment,n),w(Ha.$$.fragment,n),w(Ls.$$.fragment,n),w(Rs.$$.fragment,n),w(Bs.$$.fragment,n),w(Wa.$$.fragment,n),w(Ua.$$.fragment,n),w(wa.$$.fragment,n),w(Za.$$.fragment,n),w(Ka.$$.fragment,n),w(Ot.$$.fragment,n),w(ks.$$.fragment,n),w(Ys.$$.fragment,n),w(nl.$$.fragment,n),co=!0)},o(n){g($.$$.fragment,n),g(K.$$.fragment,n),g(le.$$.fragment,n),g(_e.$$.fragment,n),g(Oe.$$.fragment,n),g(He.$$.fragment,n),g(re.$$.fragment,n),g(qt.$$.fragment,n),g(It.$$.fragment,n),g(Te.$$.fragment,n),g(wt.$$.fragment,n),g(kt.$$.fragment,n),g(ve.$$.fragment,n),g(cs.$$.fragment,n),g(Ke.$$.fragment,n),g(it.$$.fragment,n),g(Ha.$$.fragment,n),g(Ls.$$.fragment,n),g(Rs.$$.fragment,n),g(Bs.$$.fragment,n),g(Wa.$$.fragment,n),g(Ua.$$.fragment,n),g(wa.$$.fragment,n),g(Za.$$.fragment,n),g(Ka.$$.fragment,n),g(Ot.$$.fragment,n),g(ks.$$.fragment,n),g(Ys.$$.fragment,n),g(nl.$$.fragment,n),co=!1},d(n){n&&s(u),O($),n&&s(T),n&&s(D),n&&s(Z),n&&s(F),n&&s(ne),O(K,n),n&&s(ce),n&&s(J),n&&s(ye),O(le,n),n&&s(Ie),n&&s(V),n&&s(ss),O(_e,n),n&&s(Ts),O(Oe,n),n&&s(de),O(He,n),n&&s(as),n&&s(Q),O(re),n&&s(lt),n&&s(We),n&&s(zt),O(qt,n),n&&s(ns),n&&s(ue),n&&s(At),n&&s(be),n&&s(fe),O(It,n),n&&s(Nt),n&&s(je),n&&s(St),O(Te,n),n&&s(Ds),n&&s(ee),n&&s(Ye),O(wt,n),n&&s(gt),n&&s(ke),n&&s(is),O(kt,n),n&&s(Ft),n&&s(vt),n&&s($t),n&&s(Ze),n&&s(ps),O(ve,n),n&&s(Is),n&&s(Le),n&&s(I),O(cs,n),n&&s(Ns),n&&s(Rt),n&&s(Mt),n&&s(us),n&&s(Ra),n&&s($e),O(Ke),n&&s(Ba),n&&s(Wt),n&&s(Ma),n&&s(Pe),O(it),n&&s(da),n&&s(xe),n&&s(Ln),O(Ha,n),n&&s(Gt),n&&s(ma),n&&s(Rn),O(Ls,n),n&&s(Bn),n&&s(ms),n&&s(Mn),O(Rs,n),n&&s(Hn),n&&s(_a),n&&s(Ut),O(Bs,n),n&&s(Wn),O(Wa,n),n&&s(kl),n&&s(_s),n&&s(Gn),O(Ua,n),n&&s(vl),n&&s(xt),n&&s(ba),O(wa,n),n&&s(Yt),O(Za,n),n&&s(El),n&&s(xl),n&&s(Ol),O(Ka,n),n&&s(Yn),n&&s(Xt),n&&s(Tl),n&&s(Ms),O(Ot),n&&s(Pl),n&&s(Je),n&&s(Vn),O(ks,n),n&&s(Jn),n&&s(zl),n&&s(Qn),n&&s(vs),n&&s(ya),n&&s(ll),n&&s(to),O(Ys,n),n&&s(so),n&&s(Il),n&&s(ao),O(nl,n),n&&s(fn),n&&s(oe),n&&s(ro),n&&s(ct)}}}function Fd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u26A0\uFE0F If you have a model with the wrong number of labels, you will get an obscure error when calling the "),m=o("code"),$=a("Trainer.train()"),C=a(" method later on (something like \u201CCUDA error: device-side assert triggered\u201D). This is the number one cause of bugs reported by users for such errors, so make sure you do this check to confirm that you have the expected number of labels.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u26A0\uFE0F If you have a model with the wrong number of labels, you will get an obscure error when calling the "),m=r(y,"CODE",{});var T=i(m);$=l(T,"Trainer.train()"),T.forEach(s),C=l(y," method later on (something like \u201CCUDA error: device-side assert triggered\u201D). This is the number one cause of bugs reported by users for such errors, so make sure you do this check to confirm that you have the expected number of labels."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function Ld(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when defining your "),m=o("code"),$=a("Trainer"),C=a(" and will need to set a new name.")},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u{1F4A1} If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn\u2019t, you\u2019ll get an error when defining your "),m=r(y,"CODE",{});var T=i(m);$=l(T,"Trainer"),T.forEach(s),C=l(y," and will need to set a new name."),y.forEach(s)},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function Rd(Y){let u,k,m,$,C;return{c(){u=o("p"),k=a("\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code starting from the cell above into a dedicated training function. See "),m=o("a"),$=a("Chapter 3"),C=a(" for more details."),this.h()},l(v){u=r(v,"P",{});var y=i(u);k=l(y,"\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code starting from the cell above into a dedicated training function. See "),m=r(y,"A",{href:!0});var T=i(m);$=l(T,"Chapter 3"),T.forEach(s),C=l(y," for more details."),y.forEach(s),this.h()},h(){b(m,"href","/course/chapter3")},m(v,y){p(v,u,y),t(u,k),t(u,m),t(m,$),t(u,C)},d(v){v&&s(u)}}}function Bd(Y){let u,k,m,$,C,v,y,T,D,A,B,P,S,M,N,H,Z,F,G,z,U,L,X,te,se,ne,K,ce,J,W,R,ae,he,ye,le,Ie,V,st,ie,dt,ts,at,Js,Tt,ss,_e,Ts,Oe,de,He,as,Q,Ce,ut,re,Dt,ls,za,lt,We,Qs,Ne,Pt,ft,zt,qt,ns,ue,mt,At,be,ea,fe,It,Nt,je,qa,Se,Aa,Ia,St,Te,Ds,ee,De,_t,we,Na,nt,Sa,ta,me,Ps,zs,sa,bt,os,ot,aa,Ge,la,Ue,rs,ge,na,Ye,wt,gt,ke,Xe,qs,As,Fa,is,kt,Ft,vt,oa,$t,Ze,Fe,Lt,rt,La,ps,ve,Is,Le,f,I,cs,Ns,Rt,hs,Ss,ds,Bt,Mt,us,pe,Ra,$e,Ht,ra,Ke,_l,ia,pa,Ba,Wt,ca,Re,bl,ha,Ma,Pe,yt,jt,it,fs,Ee,pt,da,xe,wl,ua,Fs,gl,Ql,Fn,Ve,fa,en,Ro,Ln,Ha,Gt,ma,Bo,Rn,Ls,Bn,ms,tn,sn,Mo,Ho,Et,an,Wo,Mn,Rs,Hn,_a,ln,nn,Go,Uo,Ut,Bs,Wn,Wa,kl,_s,Yo,Ga,Xo,Zo,on,bs,rn,Gn,Ua,vl,xt,Ko,Ya,Vo,Jo,$l,yl,Qo,jl,Xa,Un,ba,wa,Yt,Za,El,xl,er,Ol,Ka,Yn,Xt,Va,Cl,ws,tr,Ja,sr,ar,Tl,Ms,ga,Dl,Ot,Xn,Zt,ka,Pl,Je,lr,Qa,nr,Zn,Hs,Kn,gs,Vn,ks,Jn,zl,pn,Qn,vs,va,$a,cn,hn,or,rr,Qe,el,ql,ze,ir,tl,pr,cr,Ws,hr,dr,Gs,ur,fr,dn,Us,mr,sl,_r,br,Al,al,eo,ya,ll,$s,to,Ys,so,Il,un,ao,nl,fn,oe,lo,Xs,no,qe,mn,_n,wr,bn,wn,gr,gn,kn,kr,vn,$n,vr,yn,jn,$r,En,oo,ja,ro,ct,yr,xn,io,ol,Ea,Zs,po,et,co,n,_,pi,ho,ci,Nl,uo,hi,Sl,fo,di,mo,jr,ys,Er,On,_o,xr,Fl,Or,xa,Cr,Ks,ui,Ll,fi,tt,bo,mi,Tr,Dr,rl,Pr,ht,wo,go,_i,ko,vo,bi,$o,yo,wi,jo,Eo,gi,ki,Cn,Rl,Tn,Dn,vi,zr,Kt,$i,qr,Bl,Ar,Vt,yi,Pn,xo,ji,il,js,Es,Jt,Oa,pl,Oo,Ml,Hl,Co,Ei,zn,Qt,xi,To,Wl,Oi,qn,Ci,Gl,Ul,Ti,xs,Ir,es,Di,Yl,Ca,Pi,zi,Do,qi,Ae,Po,Ai,Ii,Nr,Os,Cs,An,Vs,Ni,Sr,Xl,cp,Fr,hp,Si,Yp,dp,Lr,up,Rr,fp,cl,hl,Fi,Li,In,zo,sp,Br,Xp,ap,Zp,mp,dl,ul,Ri,Mr,_p,Bi,Kp,bp,Hr,wp,Wr,gp,Mi,Vp,kp,Gr,vp,Hi,Jp,$p,Ur,yp,fl,ml,Wi,Gi,Nn,qo,lp,Yr,Qp,np,ec,jp,Ao,tc,op,sc,ac,Ep,Xr,xp,Zr,Op,Ui,lc,Cp;m=new kd({props:{fw:Y[0]}}),T=new Ct({});const oc=[$d,vd],Kr=[];function rc(e,d){return e[0]==="pt"?0:1}S=rc(Y),M=Kr[S]=oc[S](Y),Tt=new hd({props:{id:"wVHdVlPScxA"}}),be=new Ct({}),Te=new Lo({props:{$$slots:{default:[yd]},$$scope:{ctx:Y}}}),we=new Ct({}),ot=new q({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("conll2003")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;conll2003&quot;</span>)`}}),Ye=new q({props:{code:"raw_datasets",highlighted:"raw_datasets"}}),gt=new q({props:{code:`DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;chunk_tags&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;ner_tags&#x27;</span>, <span class="hljs-string">&#x27;pos_tags&#x27;</span>, <span class="hljs-string">&#x27;tokens&#x27;</span>],
        num_rows: <span class="hljs-number">14041</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;chunk_tags&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;ner_tags&#x27;</span>, <span class="hljs-string">&#x27;pos_tags&#x27;</span>, <span class="hljs-string">&#x27;tokens&#x27;</span>],
        num_rows: <span class="hljs-number">3250</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;chunk_tags&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;ner_tags&#x27;</span>, <span class="hljs-string">&#x27;pos_tags&#x27;</span>, <span class="hljs-string">&#x27;tokens&#x27;</span>],
        num_rows: <span class="hljs-number">3453</span>
    })
})`}}),$t=new q({props:{code:'raw_datasets["train"][0]["tokens"]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;tokens&quot;</span>]'}}),Fe=new q({props:{code:"['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']",highlighted:'[<span class="hljs-string">&#x27;EU&#x27;</span>, <span class="hljs-string">&#x27;rejects&#x27;</span>, <span class="hljs-string">&#x27;German&#x27;</span>, <span class="hljs-string">&#x27;call&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;boycott&#x27;</span>, <span class="hljs-string">&#x27;British&#x27;</span>, <span class="hljs-string">&#x27;lamb&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),ve=new q({props:{code:'raw_datasets["train"][0]["ner_tags"]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;ner_tags&quot;</span>]'}}),Le=new q({props:{code:"[3, 0, 7, 0, 0, 0, 7, 0, 0]",highlighted:'[<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),ds=new q({props:{code:`ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature`,highlighted:`ner_feature = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;ner_tags&quot;</span>]
ner_feature`}}),Mt=new q({props:{code:"Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)",highlighted:'<span class="hljs-type">Sequence</span>(feature=ClassLabel(num_classes=<span class="hljs-number">9</span>, names=[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>), length=-<span class="hljs-number">1</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)'}}),jt=new q({props:{code:`label_names = ner_feature.feature.names
label_names`,highlighted:`label_names = ner_feature.feature.names
label_names`}}),fs=new q({props:{code:"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']",highlighted:'[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>]'}}),Xa=new q({props:{code:`words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)`,highlighted:`words = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;tokens&quot;</span>]
labels = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;ner_tags&quot;</span>]
line1 = <span class="hljs-string">&quot;&quot;</span>
line2 = <span class="hljs-string">&quot;&quot;</span>
<span class="hljs-keyword">for</span> word, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, labels):
    full_label = label_names[label]
    max_length = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(word), <span class="hljs-built_in">len</span>(full_label))
    line1 += word + <span class="hljs-string">&quot; &quot;</span> * (max_length - <span class="hljs-built_in">len</span>(word) + <span class="hljs-number">1</span>)
    line2 += full_label + <span class="hljs-string">&quot; &quot;</span> * (max_length - <span class="hljs-built_in">len</span>(full_label) + <span class="hljs-number">1</span>)

<span class="hljs-built_in">print</span>(line1)
<span class="hljs-built_in">print</span>(line2)`}}),ba=new q({props:{code:`'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'`,highlighted:`<span class="hljs-string">&#x27;EU    rejects German call to boycott British lamb .&#x27;</span>
<span class="hljs-string">&#x27;B-ORG O       B-MISC O    O  O       B-MISC  O    O&#x27;</span>`}}),Va=new q({props:{code:`'Germany \\'s representative to the European Union \\'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'`,highlighted:`<span class="hljs-string">&#x27;Germany \\&#x27;s representative to the European Union \\&#x27;s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .&#x27;</span>
<span class="hljs-string">&#x27;B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O&#x27;</span>`}}),Ot=new Lo({props:{$$slots:{default:[jd]},$$scope:{ctx:Y}}}),Je=new Ct({}),Hs=new hd({props:{id:"iY2AZYdZAr0"}}),el=new q({props:{code:`from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;bert-base-cased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}}),al=new q({props:{code:"tokenizer.is_fast",highlighted:"tokenizer.is_fast"}}),ya=new q({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),oe=new q({props:{code:`inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()`,highlighted:`inputs = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;tokens&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
inputs.tokens()`}}),Xs=new q({props:{code:"['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;EU&#x27;</span>, <span class="hljs-string">&#x27;rejects&#x27;</span>, <span class="hljs-string">&#x27;German&#x27;</span>, <span class="hljs-string">&#x27;call&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;boycott&#x27;</span>, <span class="hljs-string">&#x27;British&#x27;</span>, <span class="hljs-string">&#x27;la&#x27;</span>, <span class="hljs-string">&#x27;##mb&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),ol=new q({props:{code:"inputs.word_ids()",highlighted:"inputs.word_ids()"}}),Zs=new q({props:{code:"[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]",highlighted:'[<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-literal">None</span>]'}}),ys=new q({props:{code:`def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">align_labels_with_tokens</span>(<span class="hljs-params">labels, word_ids</span>):
    new_labels = []
    current_word = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> word_ids:
        <span class="hljs-keyword">if</span> word_id != current_word:
            <span class="hljs-comment"># Start of a new word!</span>
            current_word = word_id
            label = -<span class="hljs-number">100</span> <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> labels[word_id]
            new_labels.append(label)
        <span class="hljs-keyword">elif</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-comment"># Special token</span>
            new_labels.append(-<span class="hljs-number">100</span>)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-comment"># Same word as previous token</span>
            label = labels[word_id]
            <span class="hljs-comment"># If the label is B-XXX we change it to I-XXX</span>
            <span class="hljs-keyword">if</span> label % <span class="hljs-number">2</span> == <span class="hljs-number">1</span>:
                label += <span class="hljs-number">1</span>
            new_labels.append(label)

    <span class="hljs-keyword">return</span> new_labels`}}),Fl=new q({props:{code:`labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))`,highlighted:`labels = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;ner_tags&quot;</span>]
word_ids = inputs.word_ids()
<span class="hljs-built_in">print</span>(labels)
<span class="hljs-built_in">print</span>(align_labels_with_tokens(labels, word_ids))`}}),xa=new q({props:{code:`[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]`,highlighted:`[<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
[-<span class="hljs-number">100</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">100</span>]`}}),rl=new Lo({props:{$$slots:{default:[Ed]},$$scope:{ctx:Y}}}),Rl=new q({props:{code:`def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
    tokenized_inputs = tokenizer(
        examples[<span class="hljs-string">&quot;tokens&quot;</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>
    )
    all_labels = examples[<span class="hljs-string">&quot;ner_tags&quot;</span>]
    new_labels = []
    <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = new_labels
    <span class="hljs-keyword">return</span> tokenized_inputs`}}),Bl=new q({props:{code:`tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(
    tokenize_and_align_labels,
    batched=<span class="hljs-literal">True</span>,
    remove_columns=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names,
)`}});const ic=[Od,xd],Vr=[];function pc(e,d){return e[0]==="pt"?0:1}js=pc(Y),Es=Vr[js]=ic[js](Y),Ml=new Ct({});const cc=[Td,Cd],Jr=[];function hc(e,d){return e[0]==="pt"?0:1}Os=hc(Y),Cs=Jr[Os]=cc[Os](Y),Xl=new q({props:{code:`batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]`,highlighted:`batch = data_collator([tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)])
batch[<span class="hljs-string">&quot;labels&quot;</span>]`}}),Fr=new q({props:{code:`tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])`,highlighted:`tensor([[-<span class="hljs-number">100</span>,    <span class="hljs-number">3</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">7</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">7</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>, -<span class="hljs-number">100</span>],
        [-<span class="hljs-number">100</span>,    <span class="hljs-number">1</span>,    <span class="hljs-number">2</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>, -<span class="hljs-number">100</span>]])`}}),Lr=new q({props:{code:`for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])`,highlighted:`<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):
    <span class="hljs-built_in">print</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i][<span class="hljs-string">&quot;labels&quot;</span>])`}}),Rr=new q({props:{code:`[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]`,highlighted:`[-<span class="hljs-number">100</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">100</span>]
[-<span class="hljs-number">100</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, -<span class="hljs-number">100</span>]`}});const dc=[Pd,Dd],Qr=[];function uc(e,d){return e[0]==="pt"?0:1}cl=uc(Y),hl=Qr[cl]=dc[cl](Y);let Be=Y[0]==="tf"&&dd(Y);Br=new Ct({});const fc=[Id,Ad],ei=[];function mc(e,d){return e[0]==="pt"?0:1}dl=mc(Y),ul=ei[dl]=fc[dl](Y),Mr=new q({props:{code:`from datasets import load_metric

metric = load_metric("seqeval")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;seqeval&quot;</span>)`}}),Hr=new q({props:{code:`labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels`,highlighted:`labels = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;ner_tags&quot;</span>]
labels = [label_names[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> labels]
labels`}}),Wr=new q({props:{code:"['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']",highlighted:'[<span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]'}}),Gr=new q({props:{code:`predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])`,highlighted:`predictions = labels.copy()
predictions[<span class="hljs-number">2</span>] = <span class="hljs-string">&quot;O&quot;</span>
metric.compute(predictions=[predictions], references=[labels])`}}),Ur=new q({props:{code:`{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}`,highlighted:`{<span class="hljs-string">&#x27;MISC&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.5</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.67</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">2</span>},
 <span class="hljs-string">&#x27;ORG&#x27;</span>: {<span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;number&#x27;</span>: <span class="hljs-number">1</span>},
 <span class="hljs-string">&#x27;overall_precision&#x27;</span>: <span class="hljs-number">1.0</span>,
 <span class="hljs-string">&#x27;overall_recall&#x27;</span>: <span class="hljs-number">0.67</span>,
 <span class="hljs-string">&#x27;overall_f1&#x27;</span>: <span class="hljs-number">0.8</span>,
 <span class="hljs-string">&#x27;overall_accuracy&#x27;</span>: <span class="hljs-number">0.89</span>}`}});const _c=[Sd,Nd],ti=[];function bc(e,d){return e[0]==="pt"?0:1}fl=bc(Y),ml=ti[fl]=_c[fl](Y);let Me=Y[0]==="pt"&&ud(Y);return Yr=new Ct({}),Xr=new q({props:{code:`from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-comment"># Replace this with your own checkpoint</span>
model_checkpoint = <span class="hljs-string">&quot;huggingface-course/bert-finetuned-ner&quot;</span>
token_classifier = pipeline(
    <span class="hljs-string">&quot;token-classification&quot;</span>, model=model_checkpoint, aggregation_strategy=<span class="hljs-string">&quot;simple&quot;</span>
)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),Zr=new q({props:{code:`[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9988506</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9647625</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9986118</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),{c(){u=o("meta"),k=c(),j(m.$$.fragment),$=c(),C=o("h1"),v=o("a"),y=o("span"),j(T.$$.fragment),D=c(),A=o("span"),B=a("Token classification"),P=c(),M.c(),N=c(),H=o("p"),Z=a("The first application we\u2019ll explore is token classification. This generic task encompasses any problem that can be formulated as \u201Cattributing a label to each token in a sentence,\u201D such as:"),F=c(),G=o("ul"),z=o("li"),U=o("strong"),L=a("Named entity recognition (NER)"),X=a(": Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \u201Cno entity.\u201D"),te=c(),se=o("li"),ne=o("strong"),K=a("Part-of-speech tagging (POS)"),ce=a(": Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.)."),J=c(),W=o("li"),R=o("strong"),ae=a("Chunking"),he=a(": Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually "),ye=o("code"),le=a("B-"),Ie=a(") to any tokens that are at the beginning of a chunk, another label (usually "),V=o("code"),st=a("I-"),ie=a(") to tokens that are inside a chunk, and a third label (usually "),dt=o("code"),ts=a("O"),at=a(") to tokens that don\u2019t belong to any chunk."),Js=c(),j(Tt.$$.fragment),ss=c(),_e=o("p"),Ts=a("Of course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:"),Oe=c(),de=o("iframe"),as=c(),Q=o("iframe"),ut=c(),re=o("a"),Dt=o("img"),za=c(),lt=o("img"),Qs=c(),Ne=o("p"),Pt=a("You can find the model we\u2019ll train and upload to the Hub and double-check its predictions "),ft=o("a"),zt=a("here"),qt=a("."),ns=c(),ue=o("h2"),mt=o("a"),At=o("span"),j(be.$$.fragment),ea=c(),fe=o("span"),It=a("Preparing the data"),Nt=c(),je=o("p"),qa=a("First things first, we need a dataset suitable for token classification. In this section we will use the "),Se=o("a"),Aa=a("CoNLL-2003 dataset"),Ia=a(", which contains news stories from Reuters."),St=c(),j(Te.$$.fragment),Ds=c(),ee=o("h3"),De=o("a"),_t=o("span"),j(we.$$.fragment),Na=c(),nt=o("span"),Sa=a("The CoNLL-2003 dataset"),ta=c(),me=o("p"),Ps=a("To load the CoNLL-2003 dataset, we use the "),zs=o("code"),sa=a("load_dataset()"),bt=a(" method from the \u{1F917} Datasets library:"),os=c(),j(ot.$$.fragment),aa=c(),Ge=o("p"),la=a("This will download and cache the dataset, like we saw in "),Ue=o("a"),rs=a("Chapter 3"),ge=a(" for the GLUE MRPC dataset. Inspecting this object shows us the columns present and the split between the training, validation, and test sets:"),na=c(),j(Ye.$$.fragment),wt=c(),j(gt.$$.fragment),ke=c(),Xe=o("p"),qs=a("In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called "),As=o("code"),Fa=a("tokens"),is=a(", but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization)."),kt=c(),Ft=o("p"),vt=a("Let\u2019s have a look at the first element of the training set:"),oa=c(),j($t.$$.fragment),Ze=c(),j(Fe.$$.fragment),Lt=c(),rt=o("p"),La=a("Since we want to perform named entity recognition, we will look at the NER tags:"),ps=c(),j(ve.$$.fragment),Is=c(),j(Le.$$.fragment),f=c(),I=o("p"),cs=a("Those are the labels as integers ready for training, but they\u2019re not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the "),Ns=o("code"),Rt=a("features"),hs=a(" attribute of our dataset:"),Ss=c(),j(ds.$$.fragment),Bt=c(),j(Mt.$$.fragment),us=c(),pe=o("p"),Ra=a("So this column contains elements that are sequences of "),$e=o("code"),Ht=a("ClassLabel"),ra=a("s. The type of the elements of the sequence is in the "),Ke=o("code"),_l=a("feature"),ia=a(" attribute of this "),pa=o("code"),Ba=a("ner_feature"),Wt=a(", and we can access the list of names by looking at the "),ca=o("code"),Re=a("names"),bl=a(" attribute of that "),ha=o("code"),Ma=a("feature"),Pe=a(":"),yt=c(),j(jt.$$.fragment),it=c(),j(fs.$$.fragment),Ee=c(),pt=o("p"),da=a("We already saw these labels when digging into the "),xe=o("code"),wl=a("token-classification"),ua=a(" pipeline in "),Fs=o("a"),gl=a("Chapter 6"),Ql=a(", but for a quick refresher:"),Fn=c(),Ve=o("ul"),fa=o("li"),en=o("code"),Ro=a("O"),Ln=a(" means the word doesn\u2019t correspond to any entity."),Ha=c(),Gt=o("li"),ma=o("code"),Bo=a("B-PER"),Rn=a("/"),Ls=o("code"),Bn=a("I-PER"),ms=a(" means the word corresponds to the beginning of/is inside a "),tn=o("em"),sn=a("person"),Mo=a(" entity."),Ho=c(),Et=o("li"),an=o("code"),Wo=a("B-ORG"),Mn=a("/"),Rs=o("code"),Hn=a("I-ORG"),_a=a(" means the word corresponds to the beginning of/is inside an "),ln=o("em"),nn=a("organization"),Go=a(" entity."),Uo=c(),Ut=o("li"),Bs=o("code"),Wn=a("B-LOC"),Wa=a("/"),kl=o("code"),_s=a("I-LOC"),Yo=a(" means the word corresponds to the beginning of/is inside a "),Ga=o("em"),Xo=a("location"),Zo=a(" entity."),on=c(),bs=o("li"),rn=o("code"),Gn=a("B-MISC"),Ua=a("/"),vl=o("code"),xt=a("I-MISC"),Ko=a(" means the word corresponds to the beginning of/is inside a "),Ya=o("em"),Vo=a("miscellaneous"),Jo=a(" entity."),$l=c(),yl=o("p"),Qo=a("Now decoding the labels we saw earlier gives us this:"),jl=c(),j(Xa.$$.fragment),Un=c(),j(ba.$$.fragment),wa=c(),Yt=o("p"),Za=a("And for an example mixing "),El=o("code"),xl=a("B-"),er=a(" and "),Ol=o("code"),Ka=a("I-"),Yn=a(" labels, here\u2019s what the same code gives us on the element of the training set at index 4:"),Xt=c(),j(Va.$$.fragment),Cl=c(),ws=o("p"),tr=a("As we can see, entities spanning two words, like \u201CEuropean Union\u201D and \u201CWerner Zwingmann,\u201D are attributed a "),Ja=o("code"),sr=a("B-"),ar=a(" label for the first word and an "),Tl=o("code"),Ms=a("I-"),ga=a(" label for the second."),Dl=c(),j(Ot.$$.fragment),Xn=c(),Zt=o("h3"),ka=o("a"),Pl=o("span"),j(Je.$$.fragment),lr=c(),Qa=o("span"),nr=a("Processing the data"),Zn=c(),j(Hs.$$.fragment),Kn=c(),gs=o("p"),Vn=a("As usual, our texts need to be converted to token IDs before the model can make sense of them. As we saw in "),ks=o("a"),Jn=a("Chapter 6"),zl=a(", a big difference in the case of token classification tasks is that we have pre-tokenized inputs. Fortunately, the tokenizer API can deal with that pretty easily; we just need to warn the "),pn=o("code"),Qn=a("tokenizer"),vs=a(" with a special flag."),va=c(),$a=o("p"),cn=a("To begin, let\u2019s create our "),hn=o("code"),or=a("tokenizer"),rr=a(" object. As we said before, we will be using a BERT pretrained model, so we\u2019ll start by downloading and caching the associated tokenizer:"),Qe=c(),j(el.$$.fragment),ql=c(),ze=o("p"),ir=a("You can replace the "),tl=o("code"),pr=a("model_checkpoint"),cr=a(" with any other model you prefer from the "),Ws=o("a"),hr=a("Hub"),dr=a(", or with a local folder in which you\u2019ve saved a pretrained model and a tokenizer. The only constraint is that the tokenizer needs to be backed by the \u{1F917} Tokenizers library, so there\u2019s a \u201Cfast\u201D version available. You can see all the architectures that come with a fast version in "),Gs=o("a"),ur=a("this big table"),fr=a(", and to check that the "),dn=o("code"),Us=a("tokenizer"),mr=a(" object you\u2019re using is indeed backed by \u{1F917} Tokenizers you can look at its "),sl=o("code"),_r=a("is_fast"),br=a(" attribute:"),Al=c(),j(al.$$.fragment),eo=c(),j(ya.$$.fragment),ll=c(),$s=o("p"),to=a("To tokenize a pre-tokenized input, we can use our "),Ys=o("code"),so=a("tokenizer"),Il=a(" as usual and just add "),un=o("code"),ao=a("is_split_into_words=True"),nl=a(":"),fn=c(),j(oe.$$.fragment),lo=c(),j(Xs.$$.fragment),no=c(),qe=o("p"),mn=a("As we can see, the tokenizer added the special tokens used by the model ("),_n=o("code"),wr=a("[CLS]"),bn=a(" at the beginning and "),wn=o("code"),gr=a("[SEP]"),gn=a(" at the end) and left most of the words untouched. The word "),kn=o("code"),kr=a("lamb"),vn=a(", however, was tokenized into two subwords, "),$n=o("code"),vr=a("la"),yn=a(" and "),jn=o("code"),$r=a("##mb"),En=a(". This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 12 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words."),oo=c(),ja=o("p"),ro=a("Fortunately, because we\u2019re using a fast tokenizer we have access to the \u{1F917} Tokenizers superpowers, which means we can easily map each token to its corresponding word (as seen in "),ct=o("a"),yr=a("Chapter 6"),xn=a("):"),io=c(),j(ol.$$.fragment),Ea=c(),j(Zs.$$.fragment),po=c(),et=o("p"),co=a("With a tiny bit of work, we can then expand our label list to match the tokens. The first rule we\u2019ll apply is that special tokens get a label of "),n=o("code"),_=a("-100"),pi=a(". This is because by default "),ho=o("code"),ci=a("-100"),Nl=a(" is an index that is ignored in the loss function we will use (cross entropy). Then, each token gets the same label as the token that started the word it\u2019s inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the "),uo=o("code"),hi=a("B-"),Sl=a(" with "),fo=o("code"),di=a("I-"),mo=a(" (since the token does not begin the entity):"),jr=c(),j(ys.$$.fragment),Er=c(),On=o("p"),_o=a("Let\u2019s try it out on our first sentence:"),xr=c(),j(Fl.$$.fragment),Or=c(),j(xa.$$.fragment),Cr=c(),Ks=o("p"),ui=a("As we can see, our function added the "),Ll=o("code"),fi=a("-100"),tt=a(" for the two special tokens at the beginning and the end, and a new "),bo=o("code"),mi=a("0"),Tr=a(" for our word that was split into two tokens."),Dr=c(),j(rl.$$.fragment),Pr=c(),ht=o("p"),wo=a("To preprocess our whole dataset, we need to tokenize all the inputs and apply "),go=o("code"),_i=a("align_labels_with_tokens()"),ko=a(" on all the labels. To take advantage of the speed of our fast tokenizer, it\u2019s best to tokenize lots of texts at the same time, so we\u2019ll write a function that processes a list of examples and use the "),vo=o("code"),bi=a("Dataset.map()"),$o=a(" method with the option "),yo=o("code"),wi=a("batched=True"),jo=a(". The only thing that is different from our previous example is that the "),Eo=o("code"),gi=a("word_ids()"),ki=a(" function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:"),Cn=c(),j(Rl.$$.fragment),Tn=c(),Dn=o("p"),vi=a("Note that we haven\u2019t padded our inputs yet; we\u2019ll do that later, when creating the batches with a data collator."),zr=c(),Kt=o("p"),$i=a("We can now apply all that preprocessing in one go on the other splits of our dataset:"),qr=c(),j(Bl.$$.fragment),Ar=c(),Vt=o("p"),yi=a("We\u2019ve done the hardest part! Now that the data has been preprocessed, the actual training will look a lot like what we did in "),Pn=o("a"),xo=a("Chapter 3"),ji=a("."),il=c(),Es.c(),Jt=c(),Oa=o("h3"),pl=o("a"),Oo=o("span"),j(Ml.$$.fragment),Hl=c(),Co=o("span"),Ei=a("Data collation"),zn=c(),Qt=o("p"),xi=a("We can\u2019t just use a "),To=o("code"),Wl=a("DataCollatorWithPadding"),Oi=a(" like in "),qn=o("a"),Ci=a("Chapter 3"),Gl=a(" because that only pads the inputs (input IDs, attention mask, and token type IDs). Here our labels should be padded the exact same way as the inputs so that they stay the same size, using "),Ul=o("code"),Ti=a("-100"),xs=a(" as a value so that the corresponding predictions are ignored in the loss computation."),Ir=c(),es=o("p"),Di=a("This is all done by a "),Yl=o("a"),Ca=o("code"),Pi=a("DataCollatorForTokenClassification"),zi=a(". Like the "),Do=o("code"),qi=a("DataCollatorWithPadding"),Ae=a(", it takes the "),Po=o("code"),Ai=a("tokenizer"),Ii=a(" used to preprocess the inputs:"),Nr=c(),Cs.c(),An=c(),Vs=o("p"),Ni=a("To test this on a few samples, we can just call it on a list of examples from our tokenized training set:"),Sr=c(),j(Xl.$$.fragment),cp=c(),j(Fr.$$.fragment),hp=c(),Si=o("p"),Yp=a("Let\u2019s compare this to the labels for the first and second elements in our dataset:"),dp=c(),j(Lr.$$.fragment),up=c(),j(Rr.$$.fragment),fp=c(),hl.c(),Fi=c(),Be&&Be.c(),Li=c(),In=o("h3"),zo=o("a"),sp=o("span"),j(Br.$$.fragment),Xp=c(),ap=o("span"),Zp=a("Metrics"),mp=c(),ul.c(),Ri=c(),j(Mr.$$.fragment),_p=c(),Bi=o("p"),Kp=a("This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let\u2019s see how it works. First, we\u2019ll get the labels for our first training example:"),bp=c(),j(Hr.$$.fragment),wp=c(),j(Wr.$$.fragment),gp=c(),Mi=o("p"),Vp=a("We can then create fake predictions for those by just changing the value at index 2:"),kp=c(),j(Gr.$$.fragment),vp=c(),Hi=o("p"),Jp=a("Note that the metric takes a list of predictions (not just one) and a list of labels. Here\u2019s the output:"),$p=c(),j(Ur.$$.fragment),yp=c(),ml.c(),Wi=c(),Me&&Me.c(),Gi=c(),Nn=o("h2"),qo=o("a"),lp=o("span"),j(Yr.$$.fragment),Qp=c(),np=o("span"),ec=a("Using the fine-tuned model"),jp=c(),Ao=o("p"),tc=a("We\u2019ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a "),op=o("code"),sc=a("pipeline"),ac=a(", you just have to specify the proper model identifier:"),Ep=c(),j(Xr.$$.fragment),xp=c(),j(Zr.$$.fragment),Op=c(),Ui=o("p"),lc=a("Great! Our model is working as well as the default one for this pipeline!"),this.h()},l(e){const d=wd('[data-svelte="svelte-1phssyn"]',document.head);u=r(d,"META",{name:!0,content:!0}),d.forEach(s),k=h(e),E(m.$$.fragment,e),$=h(e),C=r(e,"H1",{class:!0});var si=i(C);v=r(si,"A",{id:!0,class:!0,href:!0});var Yi=i(v);y=r(Yi,"SPAN",{});var rp=i(y);E(T.$$.fragment,rp),rp.forEach(s),Yi.forEach(s),D=h(si),A=r(si,"SPAN",{});var ip=i(A);B=l(ip,"Token classification"),ip.forEach(s),si.forEach(s),P=h(e),M.l(e),N=h(e),H=r(e,"P",{});var pp=i(H);Z=l(pp,"The first application we\u2019ll explore is token classification. This generic task encompasses any problem that can be formulated as \u201Cattributing a label to each token in a sentence,\u201D such as:"),pp.forEach(s),F=h(e),G=r(e,"UL",{});var Zl=i(G);z=r(Zl,"LI",{});var ai=i(z);U=r(ai,"STRONG",{});var Xi=i(U);L=l(Xi,"Named entity recognition (NER)"),Xi.forEach(s),X=l(ai,": Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \u201Cno entity.\u201D"),ai.forEach(s),te=h(Zl),se=r(Zl,"LI",{});var li=i(se);ne=r(li,"STRONG",{});var Zi=i(ne);K=l(Zi,"Part-of-speech tagging (POS)"),Zi.forEach(s),ce=l(li,": Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.)."),li.forEach(s),J=h(Zl),W=r(Zl,"LI",{});var Sn=i(W);R=r(Sn,"STRONG",{});var wc=i(R);ae=l(wc,"Chunking"),wc.forEach(s),he=l(Sn,": Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually "),ye=r(Sn,"CODE",{});var gc=i(ye);le=l(gc,"B-"),gc.forEach(s),Ie=l(Sn,") to any tokens that are at the beginning of a chunk, another label (usually "),V=r(Sn,"CODE",{});var kc=i(V);st=l(kc,"I-"),kc.forEach(s),ie=l(Sn,") to tokens that are inside a chunk, and a third label (usually "),dt=r(Sn,"CODE",{});var vc=i(dt);ts=l(vc,"O"),vc.forEach(s),at=l(Sn,") to tokens that don\u2019t belong to any chunk."),Sn.forEach(s),Zl.forEach(s),Js=h(e),E(Tt.$$.fragment,e),ss=h(e),_e=r(e,"P",{});var $c=i(_e);Ts=l($c,"Of course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:"),$c.forEach(s),Oe=h(e),de=r(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),i(de).forEach(s),as=h(e),Q=r(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),i(Q).forEach(s),ut=h(e),re=r(e,"A",{class:!0,href:!0});var Tp=i(re);Dt=r(Tp,"IMG",{class:!0,src:!0,alt:!0}),za=h(Tp),lt=r(Tp,"IMG",{class:!0,src:!0,alt:!0}),Tp.forEach(s),Qs=h(e),Ne=r(e,"P",{});var Dp=i(Ne);Pt=l(Dp,"You can find the model we\u2019ll train and upload to the Hub and double-check its predictions "),ft=r(Dp,"A",{href:!0,rel:!0});var yc=i(ft);zt=l(yc,"here"),yc.forEach(s),qt=l(Dp,"."),Dp.forEach(s),ns=h(e),ue=r(e,"H2",{class:!0});var Pp=i(ue);mt=r(Pp,"A",{id:!0,class:!0,href:!0});var jc=i(mt);At=r(jc,"SPAN",{});var Ec=i(At);E(be.$$.fragment,Ec),Ec.forEach(s),jc.forEach(s),ea=h(Pp),fe=r(Pp,"SPAN",{});var xc=i(fe);It=l(xc,"Preparing the data"),xc.forEach(s),Pp.forEach(s),Nt=h(e),je=r(e,"P",{});var zp=i(je);qa=l(zp,"First things first, we need a dataset suitable for token classification. In this section we will use the "),Se=r(zp,"A",{href:!0,rel:!0});var Oc=i(Se);Aa=l(Oc,"CoNLL-2003 dataset"),Oc.forEach(s),Ia=l(zp,", which contains news stories from Reuters."),zp.forEach(s),St=h(e),E(Te.$$.fragment,e),Ds=h(e),ee=r(e,"H3",{class:!0});var qp=i(ee);De=r(qp,"A",{id:!0,class:!0,href:!0});var Cc=i(De);_t=r(Cc,"SPAN",{});var Tc=i(_t);E(we.$$.fragment,Tc),Tc.forEach(s),Cc.forEach(s),Na=h(qp),nt=r(qp,"SPAN",{});var Dc=i(nt);Sa=l(Dc,"The CoNLL-2003 dataset"),Dc.forEach(s),qp.forEach(s),ta=h(e),me=r(e,"P",{});var Ap=i(me);Ps=l(Ap,"To load the CoNLL-2003 dataset, we use the "),zs=r(Ap,"CODE",{});var Pc=i(zs);sa=l(Pc,"load_dataset()"),Pc.forEach(s),bt=l(Ap," method from the \u{1F917} Datasets library:"),Ap.forEach(s),os=h(e),E(ot.$$.fragment,e),aa=h(e),Ge=r(e,"P",{});var Ip=i(Ge);la=l(Ip,"This will download and cache the dataset, like we saw in "),Ue=r(Ip,"A",{href:!0});var zc=i(Ue);rs=l(zc,"Chapter 3"),zc.forEach(s),ge=l(Ip," for the GLUE MRPC dataset. Inspecting this object shows us the columns present and the split between the training, validation, and test sets:"),Ip.forEach(s),na=h(e),E(Ye.$$.fragment,e),wt=h(e),E(gt.$$.fragment,e),ke=h(e),Xe=r(e,"P",{});var Np=i(Xe);qs=l(Np,"In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called "),As=r(Np,"CODE",{});var qc=i(As);Fa=l(qc,"tokens"),qc.forEach(s),is=l(Np,", but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization)."),Np.forEach(s),kt=h(e),Ft=r(e,"P",{});var Ac=i(Ft);vt=l(Ac,"Let\u2019s have a look at the first element of the training set:"),Ac.forEach(s),oa=h(e),E($t.$$.fragment,e),Ze=h(e),E(Fe.$$.fragment,e),Lt=h(e),rt=r(e,"P",{});var Ic=i(rt);La=l(Ic,"Since we want to perform named entity recognition, we will look at the NER tags:"),Ic.forEach(s),ps=h(e),E(ve.$$.fragment,e),Is=h(e),E(Le.$$.fragment,e),f=h(e),I=r(e,"P",{});var Sp=i(I);cs=l(Sp,"Those are the labels as integers ready for training, but they\u2019re not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the "),Ns=r(Sp,"CODE",{});var Nc=i(Ns);Rt=l(Nc,"features"),Nc.forEach(s),hs=l(Sp," attribute of our dataset:"),Sp.forEach(s),Ss=h(e),E(ds.$$.fragment,e),Bt=h(e),E(Mt.$$.fragment,e),us=h(e),pe=r(e,"P",{});var Ta=i(pe);Ra=l(Ta,"So this column contains elements that are sequences of "),$e=r(Ta,"CODE",{});var Sc=i($e);Ht=l(Sc,"ClassLabel"),Sc.forEach(s),ra=l(Ta,"s. The type of the elements of the sequence is in the "),Ke=r(Ta,"CODE",{});var Fc=i(Ke);_l=l(Fc,"feature"),Fc.forEach(s),ia=l(Ta," attribute of this "),pa=r(Ta,"CODE",{});var Lc=i(pa);Ba=l(Lc,"ner_feature"),Lc.forEach(s),Wt=l(Ta,", and we can access the list of names by looking at the "),ca=r(Ta,"CODE",{});var Rc=i(ca);Re=l(Rc,"names"),Rc.forEach(s),bl=l(Ta," attribute of that "),ha=r(Ta,"CODE",{});var Bc=i(ha);Ma=l(Bc,"feature"),Bc.forEach(s),Pe=l(Ta,":"),Ta.forEach(s),yt=h(e),E(jt.$$.fragment,e),it=h(e),E(fs.$$.fragment,e),Ee=h(e),pt=r(e,"P",{});var Ki=i(pt);da=l(Ki,"We already saw these labels when digging into the "),xe=r(Ki,"CODE",{});var Mc=i(xe);wl=l(Mc,"token-classification"),Mc.forEach(s),ua=l(Ki," pipeline in "),Fs=r(Ki,"A",{href:!0});var Hc=i(Fs);gl=l(Hc,"Chapter 6"),Hc.forEach(s),Ql=l(Ki,", but for a quick refresher:"),Ki.forEach(s),Fn=h(e),Ve=r(e,"UL",{});var Kl=i(Ve);fa=r(Kl,"LI",{});var nc=i(fa);en=r(nc,"CODE",{});var Wc=i(en);Ro=l(Wc,"O"),Wc.forEach(s),Ln=l(nc," means the word doesn\u2019t correspond to any entity."),nc.forEach(s),Ha=h(Kl),Gt=r(Kl,"LI",{});var ni=i(Gt);ma=r(ni,"CODE",{});var Gc=i(ma);Bo=l(Gc,"B-PER"),Gc.forEach(s),Rn=l(ni,"/"),Ls=r(ni,"CODE",{});var Uc=i(Ls);Bn=l(Uc,"I-PER"),Uc.forEach(s),ms=l(ni," means the word corresponds to the beginning of/is inside a "),tn=r(ni,"EM",{});var Yc=i(tn);sn=l(Yc,"person"),Yc.forEach(s),Mo=l(ni," entity."),ni.forEach(s),Ho=h(Kl),Et=r(Kl,"LI",{});var oi=i(Et);an=r(oi,"CODE",{});var Xc=i(an);Wo=l(Xc,"B-ORG"),Xc.forEach(s),Mn=l(oi,"/"),Rs=r(oi,"CODE",{});var Zc=i(Rs);Hn=l(Zc,"I-ORG"),Zc.forEach(s),_a=l(oi," means the word corresponds to the beginning of/is inside an "),ln=r(oi,"EM",{});var Kc=i(ln);nn=l(Kc,"organization"),Kc.forEach(s),Go=l(oi," entity."),oi.forEach(s),Uo=h(Kl),Ut=r(Kl,"LI",{});var ri=i(Ut);Bs=r(ri,"CODE",{});var Vc=i(Bs);Wn=l(Vc,"B-LOC"),Vc.forEach(s),Wa=l(ri,"/"),kl=r(ri,"CODE",{});var Jc=i(kl);_s=l(Jc,"I-LOC"),Jc.forEach(s),Yo=l(ri," means the word corresponds to the beginning of/is inside a "),Ga=r(ri,"EM",{});var Qc=i(Ga);Xo=l(Qc,"location"),Qc.forEach(s),Zo=l(ri," entity."),ri.forEach(s),on=h(Kl),bs=r(Kl,"LI",{});var ii=i(bs);rn=r(ii,"CODE",{});var eh=i(rn);Gn=l(eh,"B-MISC"),eh.forEach(s),Ua=l(ii,"/"),vl=r(ii,"CODE",{});var th=i(vl);xt=l(th,"I-MISC"),th.forEach(s),Ko=l(ii," means the word corresponds to the beginning of/is inside a "),Ya=r(ii,"EM",{});var sh=i(Ya);Vo=l(sh,"miscellaneous"),sh.forEach(s),Jo=l(ii," entity."),ii.forEach(s),Kl.forEach(s),$l=h(e),yl=r(e,"P",{});var ah=i(yl);Qo=l(ah,"Now decoding the labels we saw earlier gives us this:"),ah.forEach(s),jl=h(e),E(Xa.$$.fragment,e),Un=h(e),E(ba.$$.fragment,e),wa=h(e),Yt=r(e,"P",{});var Vi=i(Yt);Za=l(Vi,"And for an example mixing "),El=r(Vi,"CODE",{});var lh=i(El);xl=l(lh,"B-"),lh.forEach(s),er=l(Vi," and "),Ol=r(Vi,"CODE",{});var nh=i(Ol);Ka=l(nh,"I-"),nh.forEach(s),Yn=l(Vi," labels, here\u2019s what the same code gives us on the element of the training set at index 4:"),Vi.forEach(s),Xt=h(e),E(Va.$$.fragment,e),Cl=h(e),ws=r(e,"P",{});var Ji=i(ws);tr=l(Ji,"As we can see, entities spanning two words, like \u201CEuropean Union\u201D and \u201CWerner Zwingmann,\u201D are attributed a "),Ja=r(Ji,"CODE",{});var oh=i(Ja);sr=l(oh,"B-"),oh.forEach(s),ar=l(Ji," label for the first word and an "),Tl=r(Ji,"CODE",{});var rh=i(Tl);Ms=l(rh,"I-"),rh.forEach(s),ga=l(Ji," label for the second."),Ji.forEach(s),Dl=h(e),E(Ot.$$.fragment,e),Xn=h(e),Zt=r(e,"H3",{class:!0});var Fp=i(Zt);ka=r(Fp,"A",{id:!0,class:!0,href:!0});var ih=i(ka);Pl=r(ih,"SPAN",{});var ph=i(Pl);E(Je.$$.fragment,ph),ph.forEach(s),ih.forEach(s),lr=h(Fp),Qa=r(Fp,"SPAN",{});var ch=i(Qa);nr=l(ch,"Processing the data"),ch.forEach(s),Fp.forEach(s),Zn=h(e),E(Hs.$$.fragment,e),Kn=h(e),gs=r(e,"P",{});var Qi=i(gs);Vn=l(Qi,"As usual, our texts need to be converted to token IDs before the model can make sense of them. As we saw in "),ks=r(Qi,"A",{href:!0});var hh=i(ks);Jn=l(hh,"Chapter 6"),hh.forEach(s),zl=l(Qi,", a big difference in the case of token classification tasks is that we have pre-tokenized inputs. Fortunately, the tokenizer API can deal with that pretty easily; we just need to warn the "),pn=r(Qi,"CODE",{});var dh=i(pn);Qn=l(dh,"tokenizer"),dh.forEach(s),vs=l(Qi," with a special flag."),Qi.forEach(s),va=h(e),$a=r(e,"P",{});var Lp=i($a);cn=l(Lp,"To begin, let\u2019s create our "),hn=r(Lp,"CODE",{});var uh=i(hn);or=l(uh,"tokenizer"),uh.forEach(s),rr=l(Lp," object. As we said before, we will be using a BERT pretrained model, so we\u2019ll start by downloading and caching the associated tokenizer:"),Lp.forEach(s),Qe=h(e),E(el.$$.fragment,e),ql=h(e),ze=r(e,"P",{});var Da=i(ze);ir=l(Da,"You can replace the "),tl=r(Da,"CODE",{});var fh=i(tl);pr=l(fh,"model_checkpoint"),fh.forEach(s),cr=l(Da," with any other model you prefer from the "),Ws=r(Da,"A",{href:!0,rel:!0});var mh=i(Ws);hr=l(mh,"Hub"),mh.forEach(s),dr=l(Da,", or with a local folder in which you\u2019ve saved a pretrained model and a tokenizer. The only constraint is that the tokenizer needs to be backed by the \u{1F917} Tokenizers library, so there\u2019s a \u201Cfast\u201D version available. You can see all the architectures that come with a fast version in "),Gs=r(Da,"A",{href:!0,rel:!0});var _h=i(Gs);ur=l(_h,"this big table"),_h.forEach(s),fr=l(Da,", and to check that the "),dn=r(Da,"CODE",{});var bh=i(dn);Us=l(bh,"tokenizer"),bh.forEach(s),mr=l(Da," object you\u2019re using is indeed backed by \u{1F917} Tokenizers you can look at its "),sl=r(Da,"CODE",{});var wh=i(sl);_r=l(wh,"is_fast"),wh.forEach(s),br=l(Da," attribute:"),Da.forEach(s),Al=h(e),E(al.$$.fragment,e),eo=h(e),E(ya.$$.fragment,e),ll=h(e),$s=r(e,"P",{});var ep=i($s);to=l(ep,"To tokenize a pre-tokenized input, we can use our "),Ys=r(ep,"CODE",{});var gh=i(Ys);so=l(gh,"tokenizer"),gh.forEach(s),Il=l(ep," as usual and just add "),un=r(ep,"CODE",{});var kh=i(un);ao=l(kh,"is_split_into_words=True"),kh.forEach(s),nl=l(ep,":"),ep.forEach(s),fn=h(e),E(oe.$$.fragment,e),lo=h(e),E(Xs.$$.fragment,e),no=h(e),qe=r(e,"P",{});var Pa=i(qe);mn=l(Pa,"As we can see, the tokenizer added the special tokens used by the model ("),_n=r(Pa,"CODE",{});var vh=i(_n);wr=l(vh,"[CLS]"),vh.forEach(s),bn=l(Pa," at the beginning and "),wn=r(Pa,"CODE",{});var $h=i(wn);gr=l($h,"[SEP]"),$h.forEach(s),gn=l(Pa," at the end) and left most of the words untouched. The word "),kn=r(Pa,"CODE",{});var yh=i(kn);kr=l(yh,"lamb"),yh.forEach(s),vn=l(Pa,", however, was tokenized into two subwords, "),$n=r(Pa,"CODE",{});var jh=i($n);vr=l(jh,"la"),jh.forEach(s),yn=l(Pa," and "),jn=r(Pa,"CODE",{});var Eh=i(jn);$r=l(Eh,"##mb"),Eh.forEach(s),En=l(Pa,". This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 12 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words."),Pa.forEach(s),oo=h(e),ja=r(e,"P",{});var Rp=i(ja);ro=l(Rp,"Fortunately, because we\u2019re using a fast tokenizer we have access to the \u{1F917} Tokenizers superpowers, which means we can easily map each token to its corresponding word (as seen in "),ct=r(Rp,"A",{href:!0});var xh=i(ct);yr=l(xh,"Chapter 6"),xh.forEach(s),xn=l(Rp,"):"),Rp.forEach(s),io=h(e),E(ol.$$.fragment,e),Ea=h(e),E(Zs.$$.fragment,e),po=h(e),et=r(e,"P",{});var Vl=i(et);co=l(Vl,"With a tiny bit of work, we can then expand our label list to match the tokens. The first rule we\u2019ll apply is that special tokens get a label of "),n=r(Vl,"CODE",{});var Oh=i(n);_=l(Oh,"-100"),Oh.forEach(s),pi=l(Vl,". This is because by default "),ho=r(Vl,"CODE",{});var Ch=i(ho);ci=l(Ch,"-100"),Ch.forEach(s),Nl=l(Vl," is an index that is ignored in the loss function we will use (cross entropy). Then, each token gets the same label as the token that started the word it\u2019s inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the "),uo=r(Vl,"CODE",{});var Th=i(uo);hi=l(Th,"B-"),Th.forEach(s),Sl=l(Vl," with "),fo=r(Vl,"CODE",{});var Dh=i(fo);di=l(Dh,"I-"),Dh.forEach(s),mo=l(Vl," (since the token does not begin the entity):"),Vl.forEach(s),jr=h(e),E(ys.$$.fragment,e),Er=h(e),On=r(e,"P",{});var Ph=i(On);_o=l(Ph,"Let\u2019s try it out on our first sentence:"),Ph.forEach(s),xr=h(e),E(Fl.$$.fragment,e),Or=h(e),E(xa.$$.fragment,e),Cr=h(e),Ks=r(e,"P",{});var tp=i(Ks);ui=l(tp,"As we can see, our function added the "),Ll=r(tp,"CODE",{});var zh=i(Ll);fi=l(zh,"-100"),zh.forEach(s),tt=l(tp," for the two special tokens at the beginning and the end, and a new "),bo=r(tp,"CODE",{});var qh=i(bo);mi=l(qh,"0"),qh.forEach(s),Tr=l(tp," for our word that was split into two tokens."),tp.forEach(s),Dr=h(e),E(rl.$$.fragment,e),Pr=h(e),ht=r(e,"P",{});var Jl=i(ht);wo=l(Jl,"To preprocess our whole dataset, we need to tokenize all the inputs and apply "),go=r(Jl,"CODE",{});var Ah=i(go);_i=l(Ah,"align_labels_with_tokens()"),Ah.forEach(s),ko=l(Jl," on all the labels. To take advantage of the speed of our fast tokenizer, it\u2019s best to tokenize lots of texts at the same time, so we\u2019ll write a function that processes a list of examples and use the "),vo=r(Jl,"CODE",{});var Ih=i(vo);bi=l(Ih,"Dataset.map()"),Ih.forEach(s),$o=l(Jl," method with the option "),yo=r(Jl,"CODE",{});var Nh=i(yo);wi=l(Nh,"batched=True"),Nh.forEach(s),jo=l(Jl,". The only thing that is different from our previous example is that the "),Eo=r(Jl,"CODE",{});var Sh=i(Eo);gi=l(Sh,"word_ids()"),Sh.forEach(s),ki=l(Jl," function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:"),Jl.forEach(s),Cn=h(e),E(Rl.$$.fragment,e),Tn=h(e),Dn=r(e,"P",{});var Fh=i(Dn);vi=l(Fh,"Note that we haven\u2019t padded our inputs yet; we\u2019ll do that later, when creating the batches with a data collator."),Fh.forEach(s),zr=h(e),Kt=r(e,"P",{});var Lh=i(Kt);$i=l(Lh,"We can now apply all that preprocessing in one go on the other splits of our dataset:"),Lh.forEach(s),qr=h(e),E(Bl.$$.fragment,e),Ar=h(e),Vt=r(e,"P",{});var Bp=i(Vt);yi=l(Bp,"We\u2019ve done the hardest part! Now that the data has been preprocessed, the actual training will look a lot like what we did in "),Pn=r(Bp,"A",{href:!0});var Rh=i(Pn);xo=l(Rh,"Chapter 3"),Rh.forEach(s),ji=l(Bp,"."),Bp.forEach(s),il=h(e),Es.l(e),Jt=h(e),Oa=r(e,"H3",{class:!0});var Mp=i(Oa);pl=r(Mp,"A",{id:!0,class:!0,href:!0});var Bh=i(pl);Oo=r(Bh,"SPAN",{});var Mh=i(Oo);E(Ml.$$.fragment,Mh),Mh.forEach(s),Bh.forEach(s),Hl=h(Mp),Co=r(Mp,"SPAN",{});var Hh=i(Co);Ei=l(Hh,"Data collation"),Hh.forEach(s),Mp.forEach(s),zn=h(e),Qt=r(e,"P",{});var Io=i(Qt);xi=l(Io,"We can\u2019t just use a "),To=r(Io,"CODE",{});var Wh=i(To);Wl=l(Wh,"DataCollatorWithPadding"),Wh.forEach(s),Oi=l(Io," like in "),qn=r(Io,"A",{href:!0});var Gh=i(qn);Ci=l(Gh,"Chapter 3"),Gh.forEach(s),Gl=l(Io," because that only pads the inputs (input IDs, attention mask, and token type IDs). Here our labels should be padded the exact same way as the inputs so that they stay the same size, using "),Ul=r(Io,"CODE",{});var Uh=i(Ul);Ti=l(Uh,"-100"),Uh.forEach(s),xs=l(Io," as a value so that the corresponding predictions are ignored in the loss computation."),Io.forEach(s),Ir=h(e),es=r(e,"P",{});var No=i(es);Di=l(No,"This is all done by a "),Yl=r(No,"A",{href:!0,rel:!0});var Yh=i(Yl);Ca=r(Yh,"CODE",{});var Xh=i(Ca);Pi=l(Xh,"DataCollatorForTokenClassification"),Xh.forEach(s),Yh.forEach(s),zi=l(No,". Like the "),Do=r(No,"CODE",{});var Zh=i(Do);qi=l(Zh,"DataCollatorWithPadding"),Zh.forEach(s),Ae=l(No,", it takes the "),Po=r(No,"CODE",{});var Kh=i(Po);Ai=l(Kh,"tokenizer"),Kh.forEach(s),Ii=l(No," used to preprocess the inputs:"),No.forEach(s),Nr=h(e),Cs.l(e),An=h(e),Vs=r(e,"P",{});var Vh=i(Vs);Ni=l(Vh,"To test this on a few samples, we can just call it on a list of examples from our tokenized training set:"),Vh.forEach(s),Sr=h(e),E(Xl.$$.fragment,e),cp=h(e),E(Fr.$$.fragment,e),hp=h(e),Si=r(e,"P",{});var Jh=i(Si);Yp=l(Jh,"Let\u2019s compare this to the labels for the first and second elements in our dataset:"),Jh.forEach(s),dp=h(e),E(Lr.$$.fragment,e),up=h(e),E(Rr.$$.fragment,e),fp=h(e),hl.l(e),Fi=h(e),Be&&Be.l(e),Li=h(e),In=r(e,"H3",{class:!0});var Hp=i(In);zo=r(Hp,"A",{id:!0,class:!0,href:!0});var Qh=i(zo);sp=r(Qh,"SPAN",{});var ed=i(sp);E(Br.$$.fragment,ed),ed.forEach(s),Qh.forEach(s),Xp=h(Hp),ap=r(Hp,"SPAN",{});var td=i(ap);Zp=l(td,"Metrics"),td.forEach(s),Hp.forEach(s),mp=h(e),ul.l(e),Ri=h(e),E(Mr.$$.fragment,e),_p=h(e),Bi=r(e,"P",{});var sd=i(Bi);Kp=l(sd,"This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let\u2019s see how it works. First, we\u2019ll get the labels for our first training example:"),sd.forEach(s),bp=h(e),E(Hr.$$.fragment,e),wp=h(e),E(Wr.$$.fragment,e),gp=h(e),Mi=r(e,"P",{});var ad=i(Mi);Vp=l(ad,"We can then create fake predictions for those by just changing the value at index 2:"),ad.forEach(s),kp=h(e),E(Gr.$$.fragment,e),vp=h(e),Hi=r(e,"P",{});var ld=i(Hi);Jp=l(ld,"Note that the metric takes a list of predictions (not just one) and a list of labels. Here\u2019s the output:"),ld.forEach(s),$p=h(e),E(Ur.$$.fragment,e),yp=h(e),ml.l(e),Wi=h(e),Me&&Me.l(e),Gi=h(e),Nn=r(e,"H2",{class:!0});var Wp=i(Nn);qo=r(Wp,"A",{id:!0,class:!0,href:!0});var nd=i(qo);lp=r(nd,"SPAN",{});var od=i(lp);E(Yr.$$.fragment,od),od.forEach(s),nd.forEach(s),Qp=h(Wp),np=r(Wp,"SPAN",{});var rd=i(np);ec=l(rd,"Using the fine-tuned model"),rd.forEach(s),Wp.forEach(s),jp=h(e),Ao=r(e,"P",{});var Gp=i(Ao);tc=l(Gp,"We\u2019ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a "),op=r(Gp,"CODE",{});var id=i(op);sc=l(id,"pipeline"),id.forEach(s),ac=l(Gp,", you just have to specify the proper model identifier:"),Gp.forEach(s),Ep=h(e),E(Xr.$$.fragment,e),xp=h(e),E(Zr.$$.fragment,e),Op=h(e),Ui=r(e,"P",{});var pd=i(Ui);lc=l(pd,"Great! Our model is working as well as the default one for this pipeline!"),pd.forEach(s),this.h()},h(){b(u,"name","hf:doc:metadata"),b(u,"content",JSON.stringify(Md)),b(v,"id","token-classification"),b(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(v,"href","#token-classification"),b(C,"class","relative group"),Up(de.src,He="https://hf.space/gradioiframe/course-demos/bert-finetuned-ner/+")||b(de,"src",He),b(de,"frameborder","0"),b(de,"height","350"),b(de,"title","Gradio app"),b(de,"class","block dark:hidden container p-0 flex-grow space-iframe"),b(de,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),b(de,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Up(Q.src,Ce="https://hf.space/gradioiframe/course-demos/bert-finetuned-ner-darkmode/+")||b(Q,"src",Ce),b(Q,"frameborder","0"),b(Q,"height","350"),b(Q,"title","Gradio app"),b(Q,"class","hidden dark:block container p-0 flex-grow space-iframe"),b(Q,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),b(Q,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),b(Dt,"class","block dark:hidden lg:w-3/5"),Up(Dt.src,ls="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png")||b(Dt,"src",ls),b(Dt,"alt","One-hot encoded labels for question answering."),b(lt,"class","hidden dark:block lg:w-3/5"),Up(lt.src,We="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png")||b(lt,"src",We),b(lt,"alt","One-hot encoded labels for question answering."),b(re,"class","flex justify-center"),b(re,"href","/huggingface-course/bert-finetuned-ner"),b(ft,"href","https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn"),b(ft,"rel","nofollow"),b(mt,"id","preparing-the-data"),b(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(mt,"href","#preparing-the-data"),b(ue,"class","relative group"),b(Se,"href","https://huggingface.co/datasets/conll2003"),b(Se,"rel","nofollow"),b(De,"id","the-conll2003-dataset"),b(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(De,"href","#the-conll2003-dataset"),b(ee,"class","relative group"),b(Ue,"href","/course/chapter3"),b(Fs,"href","/course/chapter6/3"),b(ka,"id","processing-the-data"),b(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(ka,"href","#processing-the-data"),b(Zt,"class","relative group"),b(ks,"href","/course/chapter6/"),b(Ws,"href","https://huggingface.co/models"),b(Ws,"rel","nofollow"),b(Gs,"href","https://huggingface.co/transformers/#supported-frameworks"),b(Gs,"rel","nofollow"),b(ct,"href","/course/chapter6/3"),b(Pn,"href","/course/chapter3"),b(pl,"id","data-collation"),b(pl,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(pl,"href","#data-collation"),b(Oa,"class","relative group"),b(qn,"href","/course/chapter3"),b(Yl,"href","https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification"),b(Yl,"rel","nofollow"),b(zo,"id","metrics"),b(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(zo,"href","#metrics"),b(In,"class","relative group"),b(qo,"id","using-the-finetuned-model"),b(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(qo,"href","#using-the-finetuned-model"),b(Nn,"class","relative group")},m(e,d){t(document.head,u),p(e,k,d),x(m,e,d),p(e,$,d),p(e,C,d),t(C,v),t(v,y),x(T,y,null),t(C,D),t(C,A),t(A,B),p(e,P,d),Kr[S].m(e,d),p(e,N,d),p(e,H,d),t(H,Z),p(e,F,d),p(e,G,d),t(G,z),t(z,U),t(U,L),t(z,X),t(G,te),t(G,se),t(se,ne),t(ne,K),t(se,ce),t(G,J),t(G,W),t(W,R),t(R,ae),t(W,he),t(W,ye),t(ye,le),t(W,Ie),t(W,V),t(V,st),t(W,ie),t(W,dt),t(dt,ts),t(W,at),p(e,Js,d),x(Tt,e,d),p(e,ss,d),p(e,_e,d),t(_e,Ts),p(e,Oe,d),p(e,de,d),p(e,as,d),p(e,Q,d),p(e,ut,d),p(e,re,d),t(re,Dt),t(re,za),t(re,lt),p(e,Qs,d),p(e,Ne,d),t(Ne,Pt),t(Ne,ft),t(ft,zt),t(Ne,qt),p(e,ns,d),p(e,ue,d),t(ue,mt),t(mt,At),x(be,At,null),t(ue,ea),t(ue,fe),t(fe,It),p(e,Nt,d),p(e,je,d),t(je,qa),t(je,Se),t(Se,Aa),t(je,Ia),p(e,St,d),x(Te,e,d),p(e,Ds,d),p(e,ee,d),t(ee,De),t(De,_t),x(we,_t,null),t(ee,Na),t(ee,nt),t(nt,Sa),p(e,ta,d),p(e,me,d),t(me,Ps),t(me,zs),t(zs,sa),t(me,bt),p(e,os,d),x(ot,e,d),p(e,aa,d),p(e,Ge,d),t(Ge,la),t(Ge,Ue),t(Ue,rs),t(Ge,ge),p(e,na,d),x(Ye,e,d),p(e,wt,d),x(gt,e,d),p(e,ke,d),p(e,Xe,d),t(Xe,qs),t(Xe,As),t(As,Fa),t(Xe,is),p(e,kt,d),p(e,Ft,d),t(Ft,vt),p(e,oa,d),x($t,e,d),p(e,Ze,d),x(Fe,e,d),p(e,Lt,d),p(e,rt,d),t(rt,La),p(e,ps,d),x(ve,e,d),p(e,Is,d),x(Le,e,d),p(e,f,d),p(e,I,d),t(I,cs),t(I,Ns),t(Ns,Rt),t(I,hs),p(e,Ss,d),x(ds,e,d),p(e,Bt,d),x(Mt,e,d),p(e,us,d),p(e,pe,d),t(pe,Ra),t(pe,$e),t($e,Ht),t(pe,ra),t(pe,Ke),t(Ke,_l),t(pe,ia),t(pe,pa),t(pa,Ba),t(pe,Wt),t(pe,ca),t(ca,Re),t(pe,bl),t(pe,ha),t(ha,Ma),t(pe,Pe),p(e,yt,d),x(jt,e,d),p(e,it,d),x(fs,e,d),p(e,Ee,d),p(e,pt,d),t(pt,da),t(pt,xe),t(xe,wl),t(pt,ua),t(pt,Fs),t(Fs,gl),t(pt,Ql),p(e,Fn,d),p(e,Ve,d),t(Ve,fa),t(fa,en),t(en,Ro),t(fa,Ln),t(Ve,Ha),t(Ve,Gt),t(Gt,ma),t(ma,Bo),t(Gt,Rn),t(Gt,Ls),t(Ls,Bn),t(Gt,ms),t(Gt,tn),t(tn,sn),t(Gt,Mo),t(Ve,Ho),t(Ve,Et),t(Et,an),t(an,Wo),t(Et,Mn),t(Et,Rs),t(Rs,Hn),t(Et,_a),t(Et,ln),t(ln,nn),t(Et,Go),t(Ve,Uo),t(Ve,Ut),t(Ut,Bs),t(Bs,Wn),t(Ut,Wa),t(Ut,kl),t(kl,_s),t(Ut,Yo),t(Ut,Ga),t(Ga,Xo),t(Ut,Zo),t(Ve,on),t(Ve,bs),t(bs,rn),t(rn,Gn),t(bs,Ua),t(bs,vl),t(vl,xt),t(bs,Ko),t(bs,Ya),t(Ya,Vo),t(bs,Jo),p(e,$l,d),p(e,yl,d),t(yl,Qo),p(e,jl,d),x(Xa,e,d),p(e,Un,d),x(ba,e,d),p(e,wa,d),p(e,Yt,d),t(Yt,Za),t(Yt,El),t(El,xl),t(Yt,er),t(Yt,Ol),t(Ol,Ka),t(Yt,Yn),p(e,Xt,d),x(Va,e,d),p(e,Cl,d),p(e,ws,d),t(ws,tr),t(ws,Ja),t(Ja,sr),t(ws,ar),t(ws,Tl),t(Tl,Ms),t(ws,ga),p(e,Dl,d),x(Ot,e,d),p(e,Xn,d),p(e,Zt,d),t(Zt,ka),t(ka,Pl),x(Je,Pl,null),t(Zt,lr),t(Zt,Qa),t(Qa,nr),p(e,Zn,d),x(Hs,e,d),p(e,Kn,d),p(e,gs,d),t(gs,Vn),t(gs,ks),t(ks,Jn),t(gs,zl),t(gs,pn),t(pn,Qn),t(gs,vs),p(e,va,d),p(e,$a,d),t($a,cn),t($a,hn),t(hn,or),t($a,rr),p(e,Qe,d),x(el,e,d),p(e,ql,d),p(e,ze,d),t(ze,ir),t(ze,tl),t(tl,pr),t(ze,cr),t(ze,Ws),t(Ws,hr),t(ze,dr),t(ze,Gs),t(Gs,ur),t(ze,fr),t(ze,dn),t(dn,Us),t(ze,mr),t(ze,sl),t(sl,_r),t(ze,br),p(e,Al,d),x(al,e,d),p(e,eo,d),x(ya,e,d),p(e,ll,d),p(e,$s,d),t($s,to),t($s,Ys),t(Ys,so),t($s,Il),t($s,un),t(un,ao),t($s,nl),p(e,fn,d),x(oe,e,d),p(e,lo,d),x(Xs,e,d),p(e,no,d),p(e,qe,d),t(qe,mn),t(qe,_n),t(_n,wr),t(qe,bn),t(qe,wn),t(wn,gr),t(qe,gn),t(qe,kn),t(kn,kr),t(qe,vn),t(qe,$n),t($n,vr),t(qe,yn),t(qe,jn),t(jn,$r),t(qe,En),p(e,oo,d),p(e,ja,d),t(ja,ro),t(ja,ct),t(ct,yr),t(ja,xn),p(e,io,d),x(ol,e,d),p(e,Ea,d),x(Zs,e,d),p(e,po,d),p(e,et,d),t(et,co),t(et,n),t(n,_),t(et,pi),t(et,ho),t(ho,ci),t(et,Nl),t(et,uo),t(uo,hi),t(et,Sl),t(et,fo),t(fo,di),t(et,mo),p(e,jr,d),x(ys,e,d),p(e,Er,d),p(e,On,d),t(On,_o),p(e,xr,d),x(Fl,e,d),p(e,Or,d),x(xa,e,d),p(e,Cr,d),p(e,Ks,d),t(Ks,ui),t(Ks,Ll),t(Ll,fi),t(Ks,tt),t(Ks,bo),t(bo,mi),t(Ks,Tr),p(e,Dr,d),x(rl,e,d),p(e,Pr,d),p(e,ht,d),t(ht,wo),t(ht,go),t(go,_i),t(ht,ko),t(ht,vo),t(vo,bi),t(ht,$o),t(ht,yo),t(yo,wi),t(ht,jo),t(ht,Eo),t(Eo,gi),t(ht,ki),p(e,Cn,d),x(Rl,e,d),p(e,Tn,d),p(e,Dn,d),t(Dn,vi),p(e,zr,d),p(e,Kt,d),t(Kt,$i),p(e,qr,d),x(Bl,e,d),p(e,Ar,d),p(e,Vt,d),t(Vt,yi),t(Vt,Pn),t(Pn,xo),t(Vt,ji),p(e,il,d),Vr[js].m(e,d),p(e,Jt,d),p(e,Oa,d),t(Oa,pl),t(pl,Oo),x(Ml,Oo,null),t(Oa,Hl),t(Oa,Co),t(Co,Ei),p(e,zn,d),p(e,Qt,d),t(Qt,xi),t(Qt,To),t(To,Wl),t(Qt,Oi),t(Qt,qn),t(qn,Ci),t(Qt,Gl),t(Qt,Ul),t(Ul,Ti),t(Qt,xs),p(e,Ir,d),p(e,es,d),t(es,Di),t(es,Yl),t(Yl,Ca),t(Ca,Pi),t(es,zi),t(es,Do),t(Do,qi),t(es,Ae),t(es,Po),t(Po,Ai),t(es,Ii),p(e,Nr,d),Jr[Os].m(e,d),p(e,An,d),p(e,Vs,d),t(Vs,Ni),p(e,Sr,d),x(Xl,e,d),p(e,cp,d),x(Fr,e,d),p(e,hp,d),p(e,Si,d),t(Si,Yp),p(e,dp,d),x(Lr,e,d),p(e,up,d),x(Rr,e,d),p(e,fp,d),Qr[cl].m(e,d),p(e,Fi,d),Be&&Be.m(e,d),p(e,Li,d),p(e,In,d),t(In,zo),t(zo,sp),x(Br,sp,null),t(In,Xp),t(In,ap),t(ap,Zp),p(e,mp,d),ei[dl].m(e,d),p(e,Ri,d),x(Mr,e,d),p(e,_p,d),p(e,Bi,d),t(Bi,Kp),p(e,bp,d),x(Hr,e,d),p(e,wp,d),x(Wr,e,d),p(e,gp,d),p(e,Mi,d),t(Mi,Vp),p(e,kp,d),x(Gr,e,d),p(e,vp,d),p(e,Hi,d),t(Hi,Jp),p(e,$p,d),x(Ur,e,d),p(e,yp,d),ti[fl].m(e,d),p(e,Wi,d),Me&&Me.m(e,d),p(e,Gi,d),p(e,Nn,d),t(Nn,qo),t(qo,lp),x(Yr,lp,null),t(Nn,Qp),t(Nn,np),t(np,ec),p(e,jp,d),p(e,Ao,d),t(Ao,tc),t(Ao,op),t(op,sc),t(Ao,ac),p(e,Ep,d),x(Xr,e,d),p(e,xp,d),x(Zr,e,d),p(e,Op,d),p(e,Ui,d),t(Ui,lc),Cp=!0},p(e,[d]){const si={};d&1&&(si.fw=e[0]),m.$set(si);let Yi=S;S=rc(e),S!==Yi&&(Fo(),g(Kr[Yi],1,1,()=>{Kr[Yi]=null}),So(),M=Kr[S],M||(M=Kr[S]=oc[S](e),M.c()),w(M,1),M.m(N.parentNode,N));const rp={};d&2&&(rp.$$scope={dirty:d,ctx:e}),Te.$set(rp);const ip={};d&2&&(ip.$$scope={dirty:d,ctx:e}),Ot.$set(ip);const pp={};d&2&&(pp.$$scope={dirty:d,ctx:e}),rl.$set(pp);let Zl=js;js=pc(e),js!==Zl&&(Fo(),g(Vr[Zl],1,1,()=>{Vr[Zl]=null}),So(),Es=Vr[js],Es||(Es=Vr[js]=ic[js](e),Es.c()),w(Es,1),Es.m(Jt.parentNode,Jt));let ai=Os;Os=hc(e),Os!==ai&&(Fo(),g(Jr[ai],1,1,()=>{Jr[ai]=null}),So(),Cs=Jr[Os],Cs||(Cs=Jr[Os]=cc[Os](e),Cs.c()),w(Cs,1),Cs.m(An.parentNode,An));let Xi=cl;cl=uc(e),cl!==Xi&&(Fo(),g(Qr[Xi],1,1,()=>{Qr[Xi]=null}),So(),hl=Qr[cl],hl||(hl=Qr[cl]=dc[cl](e),hl.c()),w(hl,1),hl.m(Fi.parentNode,Fi)),e[0]==="tf"?Be?d&1&&w(Be,1):(Be=dd(e),Be.c(),w(Be,1),Be.m(Li.parentNode,Li)):Be&&(Fo(),g(Be,1,1,()=>{Be=null}),So());let li=dl;dl=mc(e),dl!==li&&(Fo(),g(ei[li],1,1,()=>{ei[li]=null}),So(),ul=ei[dl],ul||(ul=ei[dl]=fc[dl](e),ul.c()),w(ul,1),ul.m(Ri.parentNode,Ri));let Zi=fl;fl=bc(e),fl!==Zi&&(Fo(),g(ti[Zi],1,1,()=>{ti[Zi]=null}),So(),ml=ti[fl],ml||(ml=ti[fl]=_c[fl](e),ml.c()),w(ml,1),ml.m(Wi.parentNode,Wi)),e[0]==="pt"?Me?d&1&&w(Me,1):(Me=ud(e),Me.c(),w(Me,1),Me.m(Gi.parentNode,Gi)):Me&&(Fo(),g(Me,1,1,()=>{Me=null}),So())},i(e){Cp||(w(m.$$.fragment,e),w(T.$$.fragment,e),w(M),w(Tt.$$.fragment,e),w(be.$$.fragment,e),w(Te.$$.fragment,e),w(we.$$.fragment,e),w(ot.$$.fragment,e),w(Ye.$$.fragment,e),w(gt.$$.fragment,e),w($t.$$.fragment,e),w(Fe.$$.fragment,e),w(ve.$$.fragment,e),w(Le.$$.fragment,e),w(ds.$$.fragment,e),w(Mt.$$.fragment,e),w(jt.$$.fragment,e),w(fs.$$.fragment,e),w(Xa.$$.fragment,e),w(ba.$$.fragment,e),w(Va.$$.fragment,e),w(Ot.$$.fragment,e),w(Je.$$.fragment,e),w(Hs.$$.fragment,e),w(el.$$.fragment,e),w(al.$$.fragment,e),w(ya.$$.fragment,e),w(oe.$$.fragment,e),w(Xs.$$.fragment,e),w(ol.$$.fragment,e),w(Zs.$$.fragment,e),w(ys.$$.fragment,e),w(Fl.$$.fragment,e),w(xa.$$.fragment,e),w(rl.$$.fragment,e),w(Rl.$$.fragment,e),w(Bl.$$.fragment,e),w(Es),w(Ml.$$.fragment,e),w(Cs),w(Xl.$$.fragment,e),w(Fr.$$.fragment,e),w(Lr.$$.fragment,e),w(Rr.$$.fragment,e),w(hl),w(Be),w(Br.$$.fragment,e),w(ul),w(Mr.$$.fragment,e),w(Hr.$$.fragment,e),w(Wr.$$.fragment,e),w(Gr.$$.fragment,e),w(Ur.$$.fragment,e),w(ml),w(Me),w(Yr.$$.fragment,e),w(Xr.$$.fragment,e),w(Zr.$$.fragment,e),Cp=!0)},o(e){g(m.$$.fragment,e),g(T.$$.fragment,e),g(M),g(Tt.$$.fragment,e),g(be.$$.fragment,e),g(Te.$$.fragment,e),g(we.$$.fragment,e),g(ot.$$.fragment,e),g(Ye.$$.fragment,e),g(gt.$$.fragment,e),g($t.$$.fragment,e),g(Fe.$$.fragment,e),g(ve.$$.fragment,e),g(Le.$$.fragment,e),g(ds.$$.fragment,e),g(Mt.$$.fragment,e),g(jt.$$.fragment,e),g(fs.$$.fragment,e),g(Xa.$$.fragment,e),g(ba.$$.fragment,e),g(Va.$$.fragment,e),g(Ot.$$.fragment,e),g(Je.$$.fragment,e),g(Hs.$$.fragment,e),g(el.$$.fragment,e),g(al.$$.fragment,e),g(ya.$$.fragment,e),g(oe.$$.fragment,e),g(Xs.$$.fragment,e),g(ol.$$.fragment,e),g(Zs.$$.fragment,e),g(ys.$$.fragment,e),g(Fl.$$.fragment,e),g(xa.$$.fragment,e),g(rl.$$.fragment,e),g(Rl.$$.fragment,e),g(Bl.$$.fragment,e),g(Es),g(Ml.$$.fragment,e),g(Cs),g(Xl.$$.fragment,e),g(Fr.$$.fragment,e),g(Lr.$$.fragment,e),g(Rr.$$.fragment,e),g(hl),g(Be),g(Br.$$.fragment,e),g(ul),g(Mr.$$.fragment,e),g(Hr.$$.fragment,e),g(Wr.$$.fragment,e),g(Gr.$$.fragment,e),g(Ur.$$.fragment,e),g(ml),g(Me),g(Yr.$$.fragment,e),g(Xr.$$.fragment,e),g(Zr.$$.fragment,e),Cp=!1},d(e){s(u),e&&s(k),O(m,e),e&&s($),e&&s(C),O(T),e&&s(P),Kr[S].d(e),e&&s(N),e&&s(H),e&&s(F),e&&s(G),e&&s(Js),O(Tt,e),e&&s(ss),e&&s(_e),e&&s(Oe),e&&s(de),e&&s(as),e&&s(Q),e&&s(ut),e&&s(re),e&&s(Qs),e&&s(Ne),e&&s(ns),e&&s(ue),O(be),e&&s(Nt),e&&s(je),e&&s(St),O(Te,e),e&&s(Ds),e&&s(ee),O(we),e&&s(ta),e&&s(me),e&&s(os),O(ot,e),e&&s(aa),e&&s(Ge),e&&s(na),O(Ye,e),e&&s(wt),O(gt,e),e&&s(ke),e&&s(Xe),e&&s(kt),e&&s(Ft),e&&s(oa),O($t,e),e&&s(Ze),O(Fe,e),e&&s(Lt),e&&s(rt),e&&s(ps),O(ve,e),e&&s(Is),O(Le,e),e&&s(f),e&&s(I),e&&s(Ss),O(ds,e),e&&s(Bt),O(Mt,e),e&&s(us),e&&s(pe),e&&s(yt),O(jt,e),e&&s(it),O(fs,e),e&&s(Ee),e&&s(pt),e&&s(Fn),e&&s(Ve),e&&s($l),e&&s(yl),e&&s(jl),O(Xa,e),e&&s(Un),O(ba,e),e&&s(wa),e&&s(Yt),e&&s(Xt),O(Va,e),e&&s(Cl),e&&s(ws),e&&s(Dl),O(Ot,e),e&&s(Xn),e&&s(Zt),O(Je),e&&s(Zn),O(Hs,e),e&&s(Kn),e&&s(gs),e&&s(va),e&&s($a),e&&s(Qe),O(el,e),e&&s(ql),e&&s(ze),e&&s(Al),O(al,e),e&&s(eo),O(ya,e),e&&s(ll),e&&s($s),e&&s(fn),O(oe,e),e&&s(lo),O(Xs,e),e&&s(no),e&&s(qe),e&&s(oo),e&&s(ja),e&&s(io),O(ol,e),e&&s(Ea),O(Zs,e),e&&s(po),e&&s(et),e&&s(jr),O(ys,e),e&&s(Er),e&&s(On),e&&s(xr),O(Fl,e),e&&s(Or),O(xa,e),e&&s(Cr),e&&s(Ks),e&&s(Dr),O(rl,e),e&&s(Pr),e&&s(ht),e&&s(Cn),O(Rl,e),e&&s(Tn),e&&s(Dn),e&&s(zr),e&&s(Kt),e&&s(qr),O(Bl,e),e&&s(Ar),e&&s(Vt),e&&s(il),Vr[js].d(e),e&&s(Jt),e&&s(Oa),O(Ml),e&&s(zn),e&&s(Qt),e&&s(Ir),e&&s(es),e&&s(Nr),Jr[Os].d(e),e&&s(An),e&&s(Vs),e&&s(Sr),O(Xl,e),e&&s(cp),O(Fr,e),e&&s(hp),e&&s(Si),e&&s(dp),O(Lr,e),e&&s(up),O(Rr,e),e&&s(fp),Qr[cl].d(e),e&&s(Fi),Be&&Be.d(e),e&&s(Li),e&&s(In),O(Br),e&&s(mp),ei[dl].d(e),e&&s(Ri),O(Mr,e),e&&s(_p),e&&s(Bi),e&&s(bp),O(Hr,e),e&&s(wp),O(Wr,e),e&&s(gp),e&&s(Mi),e&&s(kp),O(Gr,e),e&&s(vp),e&&s(Hi),e&&s($p),O(Ur,e),e&&s(yp),ti[fl].d(e),e&&s(Wi),Me&&Me.d(e),e&&s(Gi),e&&s(Nn),O(Yr),e&&s(jp),e&&s(Ao),e&&s(Ep),O(Xr,e),e&&s(xp),O(Zr,e),e&&s(Op),e&&s(Ui)}}}const Md={local:"token-classification",sections:[{local:"preparing-the-data",sections:[{local:"the-conll2003-dataset",title:"The CoNLL-2003 dataset"},{local:"processing-the-data",title:"Processing the data"}],title:"Preparing the data"},{local:"finetuning-the-model-with-the-trainer-api",title:"Fine-tuning the model with the `Trainer` API"},{local:"finetuning-the-model-with-keras",sections:[{local:"data-collation",title:"Data collation"},{local:"defining-the-model",title:"Defining the model"},{local:"finetuning-the-model",title:"Fine-tuning the model"},{local:"metrics",title:"Metrics"},{local:"defining-the-model",title:"Defining the model"},{local:"finetuning-the-model",title:"Fine-tuning the model"}],title:"Fine-tuning the model with Keras"},{local:"a-custom-training-loop",sections:[{local:"preparing-everything-for-training",title:"Preparing everything for training"},{local:"training-loop",title:"Training loop"}],title:"A custom training loop"},{local:"using-the-finetuned-model",title:"Using the fine-tuned model"}],title:"Token classification"};function Hd(Y,u,k){let m="pt";return gd(()=>{const $=new URLSearchParams(window.location.search);k(0,m=$.get("fw")||"pt")}),[m]}class Vd extends md{constructor(u){super();_d(this,u,Hd,Bd,bd,{})}}export{Vd as default,Md as metadata};
