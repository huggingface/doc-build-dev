import{S as Ae,i as ke,s as Ie,e as r,k as d,w as $e,t as m,M as Se,c as l,d as t,m as c,a,x as Te,h,b as n,F as o,g as i,y as xe,L as Ce,q as Pe,o as Le,B as be,v as De}from"../../chunks/vendor-1e8b365d.js";import{Y as Re}from"../../chunks/Youtube-c2a8cc39.js";import{I as qe}from"../../chunks/IconCopyLink-483c28ba.js";function Ge(se){let u,q,p,v,A,y,J,k,O,G,E,M,_,Q,I,j,z,N,P,K,U,L,V,X,b,W,Y,f,S,g,Z,ee,C,$,te,oe,D,T,re,le,R,x,ae,B;return y=new qe({}),E=new Re({props:{id:"d_ixlCubqQw"}}),{c(){u=r("meta"),q=d(),p=r("h1"),v=r("a"),A=r("span"),$e(y.$$.fragment),J=d(),k=r("span"),O=m("Decoder models"),G=d(),$e(E.$$.fragment),M=d(),_=r("p"),Q=m("Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=r("em"),j=m("auto-regressive models"),z=m("."),N=d(),P=r("p"),K=m("The pretraining of decoder models usually revolves around predicting the next word in the sentence."),U=d(),L=r("p"),V=m("These models are best suited for tasks involving text generation."),X=d(),b=r("p"),W=m("Representatives of this family of models include:"),Y=d(),f=r("ul"),S=r("li"),g=r("a"),Z=m("CTRL"),ee=d(),C=r("li"),$=r("a"),te=m("GPT"),oe=d(),D=r("li"),T=r("a"),re=m("GPT-2"),le=d(),R=r("li"),x=r("a"),ae=m("Transformer XL"),this.h()},l(e){const s=Se('[data-svelte="svelte-1phssyn"]',document.head);u=l(s,"META",{name:!0,content:!0}),s.forEach(t),q=c(e),p=l(e,"H1",{class:!0});var F=a(p);v=l(F,"A",{id:!0,class:!0,href:!0});var ne=a(v);A=l(ne,"SPAN",{});var ie=a(A);Te(y.$$.fragment,ie),ie.forEach(t),ne.forEach(t),J=c(F),k=l(F,"SPAN",{});var fe=a(k);O=h(fe,"Decoder models"),fe.forEach(t),F.forEach(t),G=c(e),Te(E.$$.fragment,e),M=c(e),_=l(e,"P",{});var H=a(_);Q=h(H,"Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=l(H,"EM",{});var de=a(I);j=h(de,"auto-regressive models"),de.forEach(t),z=h(H,"."),H.forEach(t),N=c(e),P=l(e,"P",{});var me=a(P);K=h(me,"The pretraining of decoder models usually revolves around predicting the next word in the sentence."),me.forEach(t),U=c(e),L=l(e,"P",{});var ce=a(L);V=h(ce,"These models are best suited for tasks involving text generation."),ce.forEach(t),X=c(e),b=l(e,"P",{});var he=a(b);W=h(he,"Representatives of this family of models include:"),he.forEach(t),Y=c(e),f=l(e,"UL",{});var w=a(f);S=l(w,"LI",{});var ue=a(S);g=l(ue,"A",{href:!0,rel:!0});var pe=a(g);Z=h(pe,"CTRL"),pe.forEach(t),ue.forEach(t),ee=c(w),C=l(w,"LI",{});var ve=a(C);$=l(ve,"A",{href:!0,rel:!0});var _e=a($);te=h(_e,"GPT"),_e.forEach(t),ve.forEach(t),oe=c(w),D=l(w,"LI",{});var we=a(D);T=l(we,"A",{href:!0,rel:!0});var ye=a(T);re=h(ye,"GPT-2"),ye.forEach(t),we.forEach(t),le=c(w),R=l(w,"LI",{});var Ee=a(R);x=l(Ee,"A",{href:!0,rel:!0});var ge=a(x);ae=h(ge,"Transformer XL"),ge.forEach(t),Ee.forEach(t),w.forEach(t),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Me)),n(v,"id","decoder-models"),n(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(v,"href","#decoder-models"),n(p,"class","relative group"),n(g,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),n(g,"rel","nofollow"),n($,"href","https://huggingface.co/transformers/model_doc/gpt.html"),n($,"rel","nofollow"),n(T,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),n(T,"rel","nofollow"),n(x,"href","https://huggingface.co/transformers/model_doc/transformerxl.html"),n(x,"rel","nofollow")},m(e,s){o(document.head,u),i(e,q,s),i(e,p,s),o(p,v),o(v,A),xe(y,A,null),o(p,J),o(p,k),o(k,O),i(e,G,s),xe(E,e,s),i(e,M,s),i(e,_,s),o(_,Q),o(_,I),o(I,j),o(_,z),i(e,N,s),i(e,P,s),o(P,K),i(e,U,s),i(e,L,s),o(L,V),i(e,X,s),i(e,b,s),o(b,W),i(e,Y,s),i(e,f,s),o(f,S),o(S,g),o(g,Z),o(f,ee),o(f,C),o(C,$),o($,te),o(f,oe),o(f,D),o(D,T),o(T,re),o(f,le),o(f,R),o(R,x),o(x,ae),B=!0},p:Ce,i(e){B||(Pe(y.$$.fragment,e),Pe(E.$$.fragment,e),B=!0)},o(e){Le(y.$$.fragment,e),Le(E.$$.fragment,e),B=!1},d(e){t(u),e&&t(q),e&&t(p),be(y),e&&t(G),be(E,e),e&&t(M),e&&t(_),e&&t(N),e&&t(P),e&&t(U),e&&t(L),e&&t(X),e&&t(b),e&&t(Y),e&&t(f)}}}const Me={local:"decoder-models",title:"Decoder models"};function Ne(se){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Be extends Ae{constructor(u){super();ke(this,u,Ne,Ge,Ie,{})}}export{Be as default,Me as metadata};
