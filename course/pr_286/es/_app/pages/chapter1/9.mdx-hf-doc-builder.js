import{S as Fe,i as Ke,s as Qe,e as o,k as l,w as Ve,t as n,M as We,c as r,d as t,m as i,a as s,x as Ze,h as d,b as q,G as e,g as R,y as et,L as tt,q as at,o as ot,B as rt,v as st}from"../../chunks/vendor-hf-doc-builder.js";import{I as nt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function dt(qe){let u,J,f,E,w,y,ee,x,te,Y,_,ae,$,oe,re,F,D,se,K,b,g,m,G,ne,de,H,le,ie,M,ce,ue,p,h,j,fe,me,I,pe,he,S,ve,Te,v,k,Ee,_e,N,be,Re,O,ye,De,T,U,Ae,Le,z,Be,Pe,X,Ce,Q;return y=new nt({}),{c(){u=o("meta"),J=l(),f=o("h1"),E=o("a"),w=o("span"),Ve(y.$$.fragment),ee=l(),x=o("span"),te=n("Resumen"),Y=l(),_=o("p"),ae=n("En este cap\xEDtulo viste c\xF3mo abordar diferentes tareas de PLN usando la funci\xF3n de alto nivel "),$=o("code"),oe=n("pipeline()"),re=n(" de \u{1F917} Transformers. Tambi\xE9n viste como buscar modelos en el Hub, as\xED como usar la API de Inferencia para probar los modelos directamente en tu navegador."),F=l(),D=o("p"),se=n("Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s\xF3lo el codificador o decodificador, dependiendo de qu\xE9 tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:"),K=l(),b=o("table"),g=o("thead"),m=o("tr"),G=o("th"),ne=n("Modelo"),de=l(),H=o("th"),le=n("Ejemplos"),ie=l(),M=o("th"),ce=n("Tareas"),ue=l(),p=o("tbody"),h=o("tr"),j=o("td"),fe=n("Codificador"),me=l(),I=o("td"),pe=n("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),he=l(),S=o("td"),ve=n("Clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas"),Te=l(),v=o("tr"),k=o("td"),Ee=n("Decodificador"),_e=l(),N=o("td"),be=n("CTRL, GPT, GPT-2, Transformer XL"),Re=l(),O=o("td"),ye=n("Generaci\xF3n de texto"),De=l(),T=o("tr"),U=o("td"),Ae=n("Codificador-decodificador"),Le=l(),z=o("td"),Be=n("BART, T5, Marian, mBART"),Pe=l(),X=o("td"),Ce=n("Resumen, traducci\xF3n, respuesta generativa a preguntas"),this.h()},l(a){const c=We('[data-svelte="svelte-1phssyn"]',document.head);u=r(c,"META",{name:!0,content:!0}),c.forEach(t),J=i(a),f=r(a,"H1",{class:!0});var V=s(f);E=r(V,"A",{id:!0,class:!0,href:!0});var we=s(E);w=r(we,"SPAN",{});var xe=s(w);Ze(y.$$.fragment,xe),xe.forEach(t),we.forEach(t),ee=i(V),x=r(V,"SPAN",{});var $e=s(x);te=d($e,"Resumen"),$e.forEach(t),V.forEach(t),Y=i(a),_=r(a,"P",{});var W=s(_);ae=d(W,"En este cap\xEDtulo viste c\xF3mo abordar diferentes tareas de PLN usando la funci\xF3n de alto nivel "),$=r(W,"CODE",{});var ge=s($);oe=d(ge,"pipeline()"),ge.forEach(t),re=d(W," de \u{1F917} Transformers. Tambi\xE9n viste como buscar modelos en el Hub, as\xED como usar la API de Inferencia para probar los modelos directamente en tu navegador."),W.forEach(t),F=i(a),D=r(a,"P",{});var Ge=s(D);se=d(Ge,"Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s\xF3lo el codificador o decodificador, dependiendo de qu\xE9 tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:"),Ge.forEach(t),K=i(a),b=r(a,"TABLE",{});var Z=s(b);g=r(Z,"THEAD",{});var He=s(g);m=r(He,"TR",{});var A=s(m);G=r(A,"TH",{});var Me=s(G);ne=d(Me,"Modelo"),Me.forEach(t),de=i(A),H=r(A,"TH",{});var je=s(H);le=d(je,"Ejemplos"),je.forEach(t),ie=i(A),M=r(A,"TH",{});var Ie=s(M);ce=d(Ie,"Tareas"),Ie.forEach(t),A.forEach(t),He.forEach(t),ue=i(Z),p=r(Z,"TBODY",{});var L=s(p);h=r(L,"TR",{});var B=s(h);j=r(B,"TD",{});var Se=s(j);fe=d(Se,"Codificador"),Se.forEach(t),me=i(B),I=r(B,"TD",{});var ke=s(I);pe=d(ke,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ke.forEach(t),he=i(B),S=r(B,"TD",{});var Ne=s(S);ve=d(Ne,"Clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas"),Ne.forEach(t),B.forEach(t),Te=i(L),v=r(L,"TR",{});var P=s(v);k=r(P,"TD",{});var Oe=s(k);Ee=d(Oe,"Decodificador"),Oe.forEach(t),_e=i(P),N=r(P,"TD",{});var Ue=s(N);be=d(Ue,"CTRL, GPT, GPT-2, Transformer XL"),Ue.forEach(t),Re=i(P),O=r(P,"TD",{});var ze=s(O);ye=d(ze,"Generaci\xF3n de texto"),ze.forEach(t),P.forEach(t),De=i(L),T=r(L,"TR",{});var C=s(T);U=r(C,"TD",{});var Xe=s(U);Ae=d(Xe,"Codificador-decodificador"),Xe.forEach(t),Le=i(C),z=r(C,"TD",{});var Je=s(z);Be=d(Je,"BART, T5, Marian, mBART"),Je.forEach(t),Pe=i(C),X=r(C,"TD",{});var Ye=s(X);Ce=d(Ye,"Resumen, traducci\xF3n, respuesta generativa a preguntas"),Ye.forEach(t),C.forEach(t),L.forEach(t),Z.forEach(t),this.h()},h(){q(u,"name","hf:doc:metadata"),q(u,"content",JSON.stringify(lt)),q(E,"id","resumen"),q(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(E,"href","#resumen"),q(f,"class","relative group")},m(a,c){e(document.head,u),R(a,J,c),R(a,f,c),e(f,E),e(E,w),et(y,w,null),e(f,ee),e(f,x),e(x,te),R(a,Y,c),R(a,_,c),e(_,ae),e(_,$),e($,oe),e(_,re),R(a,F,c),R(a,D,c),e(D,se),R(a,K,c),R(a,b,c),e(b,g),e(g,m),e(m,G),e(G,ne),e(m,de),e(m,H),e(H,le),e(m,ie),e(m,M),e(M,ce),e(b,ue),e(b,p),e(p,h),e(h,j),e(j,fe),e(h,me),e(h,I),e(I,pe),e(h,he),e(h,S),e(S,ve),e(p,Te),e(p,v),e(v,k),e(k,Ee),e(v,_e),e(v,N),e(N,be),e(v,Re),e(v,O),e(O,ye),e(p,De),e(p,T),e(T,U),e(U,Ae),e(T,Le),e(T,z),e(z,Be),e(T,Pe),e(T,X),e(X,Ce),Q=!0},p:tt,i(a){Q||(at(y.$$.fragment,a),Q=!0)},o(a){ot(y.$$.fragment,a),Q=!1},d(a){t(u),a&&t(J),a&&t(f),rt(y),a&&t(Y),a&&t(_),a&&t(F),a&&t(D),a&&t(K),a&&t(b)}}}const lt={local:"resumen",title:"Resumen"};function it(qe){return st(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ft extends Fe{constructor(u){super();Ke(this,u,it,dt,Qe,{})}}export{ft as default,lt as metadata};
