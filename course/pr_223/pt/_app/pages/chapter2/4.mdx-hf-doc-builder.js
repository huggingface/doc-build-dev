import{S as lm,i as pm,s as dm,e as t,k as p,w as k,t as m,M as um,c as i,d as a,m as d,x as h,a as n,h as l,b as u,N as Ca,G as o,g as r,y as $,o as v,p as cm,q as b,B as _,v as fm,n as vm}from"../../chunks/vendor-hf-doc-builder.js";import{T as bm}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Io}from"../../chunks/Youtube-hf-doc-builder.js";import{I as B}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as A}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as mm}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as km}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function hm(T){let c,z;return c=new mm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){k(c.$$.fragment)},l(f){h(c.$$.fragment,f)},m(f,q){$(c,f,q),z=!0},i(f){z||(b(c.$$.fragment,f),z=!0)},o(f){v(c.$$.fragment,f),z=!1},d(f){_(c,f)}}}function $m(T){let c,z;return c=new mm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){k(c.$$.fragment)},l(f){h(c.$$.fragment,f)},m(f,q){$(c,f,q),z=!0},i(f){z||(b(c.$$.fragment,f),z=!0)},o(f){v(c.$$.fragment,f),z=!1},d(f){_(c,f)}}}function _m(T){let c,z,f,q,w,g,x,P;return{c(){c=t("p"),z=m("Similar ao "),f=t("code"),q=m("TFAutoModel"),w=m(", a classe "),g=t("code"),x=m("AutoTokenizer"),P=m(" ira carregar a classe tokenizer apropriada na biblioteca com base no nome do checkpoint, e pode ser utilizada diretamente com qualquer checkpoint:")},l(j){c=i(j,"P",{});var E=n(c);z=l(E,"Similar ao "),f=i(E,"CODE",{});var U=n(f);q=l(U,"TFAutoModel"),U.forEach(a),w=l(E,", a classe "),g=i(E,"CODE",{});var N=n(g);x=l(N,"AutoTokenizer"),N.forEach(a),P=l(E," ira carregar a classe tokenizer apropriada na biblioteca com base no nome do checkpoint, e pode ser utilizada diretamente com qualquer checkpoint:"),E.forEach(a)},m(j,E){r(j,c,E),o(c,z),o(c,f),o(f,q),o(c,w),o(c,g),o(g,x),o(c,P)},d(j){j&&a(c)}}}function zm(T){let c,z,f,q,w,g,x,P;return{c(){c=t("p"),z=m("Similar ao "),f=t("code"),q=m("AutoModel"),w=m(", a classe "),g=t("code"),x=m("AutoTokenizer"),P=m(" ira carregar a classe tokenizer apropriada na biblioteca com base no nome do checkpoint, e pode ser utilizada diretamente com qualquer checkpoint:")},l(j){c=i(j,"P",{});var E=n(c);z=l(E,"Similar ao "),f=i(E,"CODE",{});var U=n(f);q=l(U,"AutoModel"),U.forEach(a),w=l(E,", a classe "),g=i(E,"CODE",{});var N=n(g);x=l(N,"AutoTokenizer"),N.forEach(a),P=l(E," ira carregar a classe tokenizer apropriada na biblioteca com base no nome do checkpoint, e pode ser utilizada diretamente com qualquer checkpoint:"),E.forEach(a)},m(j,E){r(j,c,E),o(c,z),o(c,f),o(f,q),o(c,w),o(c,g),o(g,x),o(c,P)},d(j){j&&a(c)}}}function gm(T){let c,z,f,q,w;return{c(){c=t("p"),z=m("\u270F\uFE0F "),f=t("strong"),q=m("Experimente realizar isso!"),w=m(" Replicar os dois \xFAltimos passos (tokeniza\xE7\xE3o e convers\xE3o para IDs de entrada) nas frases de entrada que usamos na se\xE7\xE3o 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Verifique se voc\xEA recebe os mesmos IDs de entrada que recebemos antes!")},l(g){c=i(g,"P",{});var x=n(c);z=l(x,"\u270F\uFE0F "),f=i(x,"STRONG",{});var P=n(f);q=l(P,"Experimente realizar isso!"),P.forEach(a),w=l(x," Replicar os dois \xFAltimos passos (tokeniza\xE7\xE3o e convers\xE3o para IDs de entrada) nas frases de entrada que usamos na se\xE7\xE3o 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Verifique se voc\xEA recebe os mesmos IDs de entrada que recebemos antes!"),x.forEach(a)},m(g,x){r(g,c,x),o(c,z),o(c,f),o(f,q),o(c,w)},d(g){g&&a(c)}}}function Em(T){let c,z,f,q,w,g,x,P,j,E,U,N,I,D,ta,ge,Do,ia,gr,No,na,Er,Oo,Ee,So,ma,qr,Co,la,wr,Ho,L,oe,Ha,qe,xr,Ma,Pr,Mo,we,Bo,se,jr,Ba,yr,Ar,Uo,G,xe,xi,Tr,Pe,Pi,Lo,re,Ir,Ua,Dr,Nr,Go,je,Fo,ye,Ro,pa,Or,Jo,da,Sr,Vo,ua,Cr,Ko,Ae,Hr,La,Mr,Yo,te,Br,Ga,Ur,Lr,Wo,F,ie,Fa,Te,Gr,Ra,Fr,Qo,Ie,Xo,ca,Rr,Zo,ne,Ja,Jr,Vr,Va,Kr,es,fa,Yr,as,R,De,ji,Wr,Ne,yi,os,va,Qr,ss,ba,Xr,rs,me,Zr,Ka,et,at,ts,J,le,Ya,Oe,ot,Wa,st,is,Se,ns,ka,rt,ms,ha,tt,ls,$a,it,ps,V,Ce,Ai,nt,He,Ti,ds,_a,mt,us,za,lt,cs,K,pe,Qa,Me,pt,Xa,dt,fs,ga,ut,vs,O,Za,ct,ft,eo,vt,bt,ao,kt,bs,Ea,ht,ks,Y,de,oo,Be,$t,so,_t,hs,y,zt,ro,gt,Et,to,qt,wt,io,xt,Pt,no,jt,yt,$s,ue,At,mo,Tt,It,_s,Ue,zs,qa,Le,gs,wa,Dt,Es,Ge,qs,Fe,ws,xa,Nt,xs,Re,Ps,S,Ot,lo,St,Ct,po,Ht,Mt,js,W,ce,uo,Je,Bt,co,Ut,ys,Ve,As,fe,Lt,fo,Gt,Ft,Ts,ve,Rt,vo,Jt,Vt,Is,C,Kt,bo,Yt,Wt,ko,Qt,Xt,Ds,Pa,Zt,Ns,Q,be,ho,Ke,ei,$o,ai,Os,ke,oi,_o,si,ri,Ss,Ye,Cs,ja,ti,Hs,We,Ms,ya,ii,Bs,X,he,zo,Qe,ni,go,mi,Us,$e,li,Eo,pi,di,Ls,Xe,Gs,Ze,Fs,Aa,ui,Rs,_e,Js,Z,ze,qo,ea,ci,wo,fi,Vs,ee,xo,vi,bi,Po,ki,hi,Ks,aa,Ys,oa,Ws,H,$i,jo,_i,zi,yo,gi,Ei,Qs,Ta,qi,Xs;f=new km({props:{fw:T[0]}}),P=new B({});const Ii=[$m,hm],sa=[];function Di(e,s){return e[0]==="pt"?0:1}I=Di(T),D=sa[I]=Ii[I](T),ge=new Io({props:{id:"VFp38yj8h3A"}}),Ee=new A({props:{code:"Jim Henson was a puppeteer",highlighted:'<span class="hljs-comment">Jim Henson was a puppeteer</span>'}}),qe=new B({}),we=new Io({props:{id:"nhJxYji1aho"}}),je=new A({props:{code:`tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson was a puppeteer&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),ye=new A({props:{code:"['Jim', 'Henson', 'was', 'a', 'puppeteer']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;puppeteer&#x27;</span>]'}}),Te=new B({}),Ie=new Io({props:{id:"ssLq_EK2jLE"}}),Oe=new B({}),Se=new Io({props:{id:"zHvTiHr506c"}}),Me=new B({}),Be=new B({}),Ue=new A({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function Ni(e,s){return e[0]==="pt"?zm:_m}let Zs=Ni(T),ae=Zs(T);return Le=new A({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Ge=new A({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),Fe=new A({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Re=new A({props:{code:'tokenizer.save_pretrained("directory_on_my_computer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}}),Je=new B({}),Ve=new Io({props:{id:"Yffk5aydLzg"}}),Ke=new B({}),Ye=new A({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),We=new A({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),Qe=new B({}),Xe=new A({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),Ze=new A({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),_e=new bm({props:{$$slots:{default:[gm]},$$scope:{ctx:T}}}),ea=new B({}),aa=new A({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),oa=new A({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){c=t("meta"),z=p(),k(f.$$.fragment),q=p(),w=t("h1"),g=t("a"),x=t("span"),k(P.$$.fragment),j=p(),E=t("span"),U=m("Tokenizers"),N=p(),D.c(),ta=p(),k(ge.$$.fragment),Do=p(),ia=t("p"),gr=m("Os Tokenizers s\xE3o um dos componentes centrais do pipeline da PNL. Eles t\xEAm um prop\xF3sito: traduzir texto em dados que podem ser processados pelo modelo. Os modelos s\xF3 podem processar n\xFAmeros, portanto os tokenizers precisam converter nossas entradas de texto em dados num\xE9ricos. Nesta se\xE7\xE3o, vamos explorar exatamente o que acontece no pipeline de tokeniza\xE7\xE3o."),No=p(),na=t("p"),Er=m("Nas tarefas de PNL, os dados que s\xE3o geralmente processados s\xE3o texto bruto. Aqui est\xE1 um exemplo de tal texto:"),Oo=p(),k(Ee.$$.fragment),So=p(),ma=t("p"),qr=m("Entretanto, os modelos s\xF3 podem processar n\xFAmeros, portanto, precisamos encontrar uma maneira de converter o texto bruto em n\xFAmeros. Isto \xE9 o que os tokenizers fazem, e h\xE1 muitas maneiras de se fazer isso. O objetivo \xE9 encontrar a representa\xE7\xE3o mais significativa - ou seja, a que faz mais sentido para o modelo - e, se poss\xEDvel, a menor representa\xE7\xE3o."),Co=p(),la=t("p"),wr=m("Vamos dar uma olhada em alguns exemplos de algoritmos de tokeniza\xE7\xE3o, e tentar responder algumas das perguntas que voc\xEA possa ter sobre tokeniza\xE7\xE3o."),Ho=p(),L=t("h2"),oe=t("a"),Ha=t("span"),k(qe.$$.fragment),xr=p(),Ma=t("span"),Pr=m("Baseado em palavras (word-based)"),Mo=p(),k(we.$$.fragment),Bo=p(),se=t("p"),jr=m("O primeiro tipo de tokenizer que me vem \xE0 mente \xE9 "),Ba=t("em"),yr=m("baseado em palavras"),Ar=m(". \xC9 geralmente muito f\xE1cil de instalar e usar com apenas algumas regras, e muitas vezes produz resultados decentes. Por exemplo, na imagem abaixo, o objetivo \xE9 dividir o texto bruto em palavras e encontrar uma representa\xE7\xE3o num\xE9rica para cada uma delas:"),Uo=p(),G=t("div"),xe=t("img"),Tr=p(),Pe=t("img"),Lo=p(),re=t("p"),Ir=m("H\xE1 diferentes maneiras de dividir o texto. Por exemplo, poder\xEDamos utilizar o espa\xE7o em branco para simbolizar o texto em palavras, usando a fun\xE7\xE3o "),Ua=t("code"),Dr=m("split()"),Nr=m(" do Python:"),Go=p(),k(je.$$.fragment),Fo=p(),k(ye.$$.fragment),Ro=p(),pa=t("p"),Or=m("H\xE1 tamb\xE9m varia\xE7\xF5es de tokenizers de palavras que t\xEAm regras extras para pontua\xE7\xE3o. Com este tipo de tokenizer, podemos terminar com alguns \u201Cvocabul\xE1rios\u201D bem grandes, onde um vocabul\xE1rio \xE9 definido pelo n\xFAmero total de tokens independentes que tem no texto de exemplo."),Jo=p(),da=t("p"),Sr=m("A cada palavra \xE9 atribu\xEDda uma identifica\xE7\xE3o, come\xE7ando em 0 e indo at\xE9 o tamanho do vocabul\xE1rio. O modelo utiliza estas identifica\xE7\xF5es para identificar cada palavra."),Vo=p(),ua=t("p"),Cr=m("Se quisermos cobrir completamente um idioma com um tokenizer baseado em palavras, precisaremos ter um identificador para cada palavra no idioma, o que gerar\xE1 uma enorme quantidade de tokens. Por exemplo, existem mais de 500.000 palavras no idioma ingl\xEAs, portanto, para construir um mapa a partir de cada palavra para um ID de entrada, precisar\xEDamos manter um registro desse grande n\xFAmero de IDs. Al\xE9m disso, palavras como \u201Cdog\u201D s\xE3o representadas de forma diferente de palavras como \u201Cdogs\u201D, e o modelo inicialmente n\xE3o ter\xE1 como saber que \u201Cdog\u201D e \u201Cdogs\u201D s\xE3o semelhantes: ele identificar\xE1 as duas palavras como n\xE3o relacionadas. O mesmo se aplica a outras palavras semelhantes, como \u201Crun\u201D e \u201Crunning\u201D, que o modelo n\xE3o ver\xE1 inicialmente como sendo semelhantes."),Ko=p(),Ae=t("p"),Hr=m("Finalmente, precisamos de token personalizada para representar palavras que n\xE3o est\xE3o em nosso vocabul\xE1rio. Isto \xE9 conhecido como o s\xEDmbolo \u201Cunknown\u201D (desconhecido), frequentemente representado como \u201D[UNK]\u201D ou \u201D"),La=t("unk"),Mr=m("\u201D. Geralmente \xE9 um mau sinal se voc\xEA v\xEA que o tokenizer est\xE1 produzindo muitos desses tokens, pois n\xE3o foi capaz de recuperar uma representa\xE7\xE3o sensata de uma palavra e voc\xEA est\xE1 perdendo informa\xE7\xF5es ao longo do caminho. O objetivo ao elaborar o vocabul\xE1rio \xE9 faz\xEA-lo de tal forma que o tokenizer transforme o menor n\xFAmero poss\xEDvel de palavras no token desconhecido."),Yo=p(),te=t("p"),Br=m("Uma maneira de reduzir a quantidade de tokens desconhecidas \xE9 ir um n\xEDvel mais fundo, usando um tokenizer "),Ga=t("em"),Ur=m("baseado em caracteres"),Lr=m("."),Wo=p(),F=t("h2"),ie=t("a"),Fa=t("span"),k(Te.$$.fragment),Gr=p(),Ra=t("span"),Fr=m("Baseado em caracteres (Character-based)"),Qo=p(),k(Ie.$$.fragment),Xo=p(),ca=t("p"),Rr=m("Os tokenizers baseados em caracteres dividem o texto em caracteres, ao inv\xE9s de palavras. Isto tem dois benef\xEDcios principais:"),Zo=p(),ne=t("ul"),Ja=t("li"),Jr=m("O vocabul\xE1rio ser\xE1 muito menor;"),Vr=p(),Va=t("li"),Kr=m("H\xE1 muito menos tokes fora de vocabul\xE1rio (desconhecidas), uma vez que cada palavra pode ser constru\xEDda a partir de personagens."),es=p(),fa=t("p"),Yr=m("Mas tamb\xE9m aqui surgem algumas quest\xF5es sobre os espa\xE7os e \xE0 pontua\xE7\xE3o:"),as=p(),R=t("div"),De=t("img"),Wr=p(),Ne=t("img"),os=p(),va=t("p"),Qr=m("Esta abordagem tamb\xE9m n\xE3o \xE9 perfeita. Como a representa\xE7\xE3o agora \xE9 baseada em caracteres e n\xE3o em palavras, pode-se argumentar que, intuitivamente, ela \xE9 menos significativa: cada caractere n\xE3o significa muito por si s\xF3, ao contrario do caso das palavras. No entanto, isto novamente difere de acordo com o idioma; em chin\xEAs, por exemplo, cada caractere traz mais informa\xE7\xF5es do que um caractere em um idioma latino."),ss=p(),ba=t("p"),Xr=m("Outra coisa a considerar \xE9 que acabaremos com uma quantidade muito grande de tokens a serem processadas por nosso modelo: enquanto uma palavra seria apenas um \xFAnico token com um tokenizer baseado em palavras, ela pode facilmente se transformar em 10 ou mais tokens quando convertida em caracteres."),rs=p(),me=t("p"),Zr=m("Para obter o melhor dos dois mundos, podemos usar uma terceira t\xE9cnica que combina as duas abordagens: "),Ka=t("em"),et=m("Tokeniza\xE7\xE3o por sub-palavras"),at=m("."),ts=p(),J=t("h2"),le=t("a"),Ya=t("span"),k(Oe.$$.fragment),ot=p(),Wa=t("span"),st=m("Tokeniza\xE7\xE3o por sub-palavras (Subword tokenization)"),is=p(),k(Se.$$.fragment),ns=p(),ka=t("p"),rt=m("Algoritmos de tokeniza\xE7\xE3o de sub-palavras baseiam-se no princ\xEDpio de que palavras frequentemente usadas n\xE3o devem ser divididas em sub-palavras menores, mas palavras raras devem ser decompostas em sub-palavras significativas."),ms=p(),ha=t("p"),tt=m("Por exemplo, \u201Cirritantemente\u201D poderia ser considerado uma palavra rara e poderia ser decomposto em \u201Cirritante\u201D e \u201Cmente\u201D. \xC9 prov\xE1vel que ambas apare\xE7am mais frequentemente como sub-palavras isoladas, enquanto ao mesmo tempo o significado de \u201Cirritantemente\u201D \xE9 mantido pelo significado composto de \u201Cirritante\u201D e \u201Cmente\u201D."),ls=p(),$a=t("p"),it=m("Aqui est\xE1 um exemplo que mostra como um algoritmo de tokeniza\xE7\xE3o de uma sub-palavra indicaria a sequ\xEAncia \u201CLet\u2019s do tokenization!"),ps=p(),V=t("div"),Ce=t("img"),nt=p(),He=t("img"),ds=p(),_a=t("p"),mt=m("Estas sub-palavras acabam fornecendo muito significado sem\xE2ntico: por exemplo, no exemplo acima \u201Ctokenization\u201D foi dividido em \u201Ctoken\u201D e \u201Cization\u201D, dois tokens que t\xEAm um significado sem\xE2ntico enquanto s\xE3o eficientes em termos de espa\xE7o (apenas dois tokens s\xE3o necess\xE1rios para representar uma palavra longa). Isto nos permite ter uma cobertura relativamente boa com pequenos vocabul\xE1rios, e perto de nenhum token desconhecido."),us=p(),za=t("p"),lt=m("Esta abordagem \xE9 especialmente \xFAtil em idiomas aglutinativos como o turco, onde \xE9 poss\xEDvel formar palavras (quase) arbitrariamente longas e complexas, encadeando sub-palavras."),cs=p(),K=t("h3"),pe=t("a"),Qa=t("span"),k(Me.$$.fragment),pt=p(),Xa=t("span"),dt=m("E outros!"),fs=p(),ga=t("p"),ut=m("Sem surpresas, h\xE1 muito mais t\xE9cnicas por a\xED. Para citar algumas:"),vs=p(),O=t("ul"),Za=t("li"),ct=m("Byte-level BPE, utilizada no GPT-2"),ft=p(),eo=t("li"),vt=m("WordPiece, utilizada em BERT"),bt=p(),ao=t("li"),kt=m("SentencePiece ou Unigram, como as utilizadas em v\xE1rios modelos  multil\xEDngue"),bs=p(),Ea=t("p"),ht=m("Agora voc\xEA deve ter conhecimento suficiente de como funcionam os tokenizers para come\xE7ar a utilizar a API."),ks=p(),Y=t("h2"),de=t("a"),oo=t("span"),k(Be.$$.fragment),$t=p(),so=t("span"),_t=m("Carregando e salvando"),hs=p(),y=t("p"),zt=m("Carregando e salvando tokenizers \xE9 t\xE3o simples quanto com os modelos. Na verdade, ele se baseia nos mesmos dois m\xE9todos: "),ro=t("code"),gt=m("from_pretrained()"),Et=m(" e "),to=t("code"),qt=m("save_pretrained()"),wt=m(". Estes m\xE9todos ir\xE3o carregar ou salvar o algoritmo utilizado pelo tokenizer (um pouco como a "),io=t("em"),xt=m("arquitetura"),Pt=m(" do modelo), bem como seu vocabul\xE1rio (um pouco como os "),no=t("em"),jt=m("pesos"),yt=m(" do modelo)."),$s=p(),ue=t("p"),At=m("O carregamento do tokenizer BERT treinado com o mesmo checkpoint do BERT \xE9 feito da mesma forma que o carregamento do modelo, exceto que utilizamos a classe "),mo=t("code"),Tt=m("BertTokenizer"),It=m(":"),_s=p(),k(Ue.$$.fragment),zs=p(),ae.c(),qa=p(),k(Le.$$.fragment),gs=p(),wa=t("p"),Dt=m("Agora podemos usar o tokenizer, como mostrado na se\xE7\xE3o anterior:"),Es=p(),k(Ge.$$.fragment),qs=p(),k(Fe.$$.fragment),ws=p(),xa=t("p"),Nt=m("Salvar um tokenizer \xE9 id\xEAntico a salvar um modelo:"),xs=p(),k(Re.$$.fragment),Ps=p(),S=t("p"),Ot=m("Falaremos mais sobre "),lo=t("code"),St=m("token_type_ids' no [Cap\xEDtulo 3](/course/pt/chapter3), e explicaremos a "),Ct=m("attention_mask\u2019 um pouco mais tarde. Primeiro, vamos ver como os "),po=t("code"),Ht=m("input_ids"),Mt=m(" s\xE3o gerados. Para fazer isso, precisaremos olhar os m\xE9todos intermedi\xE1rios do tokenizer."),js=p(),W=t("h2"),ce=t("a"),uo=t("span"),k(Je.$$.fragment),Bt=p(),co=t("span"),Ut=m("Encoding"),ys=p(),k(Ve.$$.fragment),As=p(),fe=t("p"),Lt=m("Traduzir texto para n\xFAmeros \xE9 conhecido como "),fo=t("em"),Gt=m("encoding"),Ft=m(". O encoding \xE9 feito em um processo de duas etapas: a tokeniza\xE7\xE3o, seguida pela convers\xE3o para IDs de entrada."),Ts=p(),ve=t("p"),Rt=m("Como vimos, o primeiro passo \xE9 dividir o texto em palavras (ou partes de palavras, s\xEDmbolos de pontua\xE7\xE3o, etc.), normalmente chamadas de "),vo=t("em"),Jt=m("tokens"),Vt=m(". H\xE1 v\xE1rias regras que podem guiar esse processo, e \xE9 por isso que precisamos instanciar o tokenizer usando o nome do modelo, para nos certificarmos de usar as mesmas regras que foram usadas quando o modelo foi pr\xE9-treinado."),Is=p(),C=t("p"),Kt=m("O segundo passo \xE9 converter esses tokens em n\xFAmeros, para que possamos construir um tensor a partir deles e aliment\xE1-los com o modelo. Para isso, o tokenizer tem um "),bo=t("em"),Yt=m("vocabul\xE1rio"),Wt=m(" (vocabulary), que \xE9 a parte que realizamos o download quando o instanciamos com o m\xE9todo "),ko=t("code"),Qt=m("from_pretrained()"),Xt=m(". Mais uma vez, precisamos utilizar o mesmo vocabul\xE1rio utilizado quando o modelo foi pr\xE9-treinado."),Ds=p(),Pa=t("p"),Zt=m("Para entender melhor os dois passos, vamos explor\xE1-los separadamente. Note que usaremos alguns m\xE9todos que executam partes da pipeline de tokeniza\xE7\xE3o separadamente para mostrar os resultados intermedi\xE1rios dessas etapas, mas na pr\xE1tica, voc\xEA deve chamar o tokenizer diretamente em suas entradas (como mostrado na se\xE7\xE3o 2)."),Ns=p(),Q=t("h3"),be=t("a"),ho=t("span"),k(Ke.$$.fragment),ei=p(),$o=t("span"),ai=m("Tokeniza\xE7\xE3o"),Os=p(),ke=t("p"),oi=m("O processo de tokenization \xE9 feito atrav\xE9s do m\xE9todo "),_o=t("code"),si=m("tokenize()"),ri=m(" do tokenizer:"),Ss=p(),k(Ye.$$.fragment),Cs=p(),ja=t("p"),ti=m("A sa\xEDda deste m\xE9todo \xE9 uma lista de strings, ou tokens:"),Hs=p(),k(We.$$.fragment),Ms=p(),ya=t("p"),ii=m("Este tokenizer \xE9 um tokenizer de sub-palavras: ele divide as palavras at\xE9 obter tokens que podem ser representadas por seu vocabul\xE1rio. \xC9 o caso aqui do \u201Ctransformer\u201D, que \xE9 dividido em dois tokens: \u201Ctransform\u201D e \u201C##er\u201D."),Bs=p(),X=t("h3"),he=t("a"),zo=t("span"),k(Qe.$$.fragment),ni=p(),go=t("span"),mi=m("Desde os tokens at\xE9 IDs de entrada"),Us=p(),$e=t("p"),li=m("A convers\xE3o para IDs de entrada \xE9 feita pelo m\xE9todo de tokeniza\xE7\xE3o "),Eo=t("code"),pi=m("convert_tokens_to_ids()"),di=m(":"),Ls=p(),k(Xe.$$.fragment),Gs=p(),k(Ze.$$.fragment),Fs=p(),Aa=t("p"),ui=m("Estas sa\xEDdas, uma vez convertidas no tensor com a estrutura apropriada, podem ent\xE3o ser usadas como entradas para um modelo como visto anteriormente neste cap\xEDtulo."),Rs=p(),k(_e.$$.fragment),Js=p(),Z=t("h2"),ze=t("a"),qo=t("span"),k(ea.$$.fragment),ci=p(),wo=t("span"),fi=m("Decoding"),Vs=p(),ee=t("p"),xo=t("em"),vi=m("Decoding"),bi=m(" vai pela dire\xE7\xE3o ao contr\xE1rio: a partir de \xEDndices de vocabul\xE1rio, queremos obter uma string. Isto pode ser feito com o m\xE9todo "),Po=t("code"),ki=m("decode()"),hi=m(" da seguinte forma:"),Ks=p(),k(aa.$$.fragment),Ys=p(),k(oa.$$.fragment),Ws=p(),H=t("p"),$i=m("Observe que o m\xE9todo "),jo=t("code"),_i=m("decode"),zi=m(" n\xE3o apenas converte os \xEDndices em tokens, mas tamb\xE9m agrupa os tokens que fizeram parte das mesmas palavras para produzir uma frase leg\xEDvel. Este comportamento ser\xE1 extremamente \xFAtil quando utilizamos modelos que preveem um novo texto (seja texto gerado a partir de um prompt, ou para problemas de "),yo=t("em"),gi=m("sequence-to-sequence"),Ei=m(" como tradu\xE7\xE3o ou sumariza\xE7\xE3o)."),Qs=p(),Ta=t("p"),qi=m("At\xE9 agora voc\xEA j\xE1 deve entender as opera\xE7\xF5es at\xF4micas que um tokenizer pode lidar: tokeniza\xE7\xE3o, convers\xE3o para IDs, e convers\xE3o de IDs de volta para uma string. Entretanto, acabamos de come\xE7ar a ver a ponta do iceberg. Na se\xE7\xE3o seguinte, vamos nos aproximar de seus limites e dar uma olhada em como super\xE1-los."),this.h()},l(e){const s=um('[data-svelte="svelte-1phssyn"]',document.head);c=i(s,"META",{name:!0,content:!0}),s.forEach(a),z=d(e),h(f.$$.fragment,e),q=d(e),w=i(e,"H1",{class:!0});var ra=n(w);g=i(ra,"A",{id:!0,class:!0,href:!0});var Ia=n(g);x=i(Ia,"SPAN",{});var Ao=n(x);h(P.$$.fragment,Ao),Ao.forEach(a),Ia.forEach(a),j=d(ra),E=i(ra,"SPAN",{});var Oi=n(E);U=l(Oi,"Tokenizers"),Oi.forEach(a),ra.forEach(a),N=d(e),D.l(e),ta=d(e),h(ge.$$.fragment,e),Do=d(e),ia=i(e,"P",{});var Si=n(ia);gr=l(Si,"Os Tokenizers s\xE3o um dos componentes centrais do pipeline da PNL. Eles t\xEAm um prop\xF3sito: traduzir texto em dados que podem ser processados pelo modelo. Os modelos s\xF3 podem processar n\xFAmeros, portanto os tokenizers precisam converter nossas entradas de texto em dados num\xE9ricos. Nesta se\xE7\xE3o, vamos explorar exatamente o que acontece no pipeline de tokeniza\xE7\xE3o."),Si.forEach(a),No=d(e),na=i(e,"P",{});var Ci=n(na);Er=l(Ci,"Nas tarefas de PNL, os dados que s\xE3o geralmente processados s\xE3o texto bruto. Aqui est\xE1 um exemplo de tal texto:"),Ci.forEach(a),Oo=d(e),h(Ee.$$.fragment,e),So=d(e),ma=i(e,"P",{});var Hi=n(ma);qr=l(Hi,"Entretanto, os modelos s\xF3 podem processar n\xFAmeros, portanto, precisamos encontrar uma maneira de converter o texto bruto em n\xFAmeros. Isto \xE9 o que os tokenizers fazem, e h\xE1 muitas maneiras de se fazer isso. O objetivo \xE9 encontrar a representa\xE7\xE3o mais significativa - ou seja, a que faz mais sentido para o modelo - e, se poss\xEDvel, a menor representa\xE7\xE3o."),Hi.forEach(a),Co=d(e),la=i(e,"P",{});var Mi=n(la);wr=l(Mi,"Vamos dar uma olhada em alguns exemplos de algoritmos de tokeniza\xE7\xE3o, e tentar responder algumas das perguntas que voc\xEA possa ter sobre tokeniza\xE7\xE3o."),Mi.forEach(a),Ho=d(e),L=i(e,"H2",{class:!0});var er=n(L);oe=i(er,"A",{id:!0,class:!0,href:!0});var Bi=n(oe);Ha=i(Bi,"SPAN",{});var Ui=n(Ha);h(qe.$$.fragment,Ui),Ui.forEach(a),Bi.forEach(a),xr=d(er),Ma=i(er,"SPAN",{});var Li=n(Ma);Pr=l(Li,"Baseado em palavras (word-based)"),Li.forEach(a),er.forEach(a),Mo=d(e),h(we.$$.fragment,e),Bo=d(e),se=i(e,"P",{});var ar=n(se);jr=l(ar,"O primeiro tipo de tokenizer que me vem \xE0 mente \xE9 "),Ba=i(ar,"EM",{});var Gi=n(Ba);yr=l(Gi,"baseado em palavras"),Gi.forEach(a),Ar=l(ar,". \xC9 geralmente muito f\xE1cil de instalar e usar com apenas algumas regras, e muitas vezes produz resultados decentes. Por exemplo, na imagem abaixo, o objetivo \xE9 dividir o texto bruto em palavras e encontrar uma representa\xE7\xE3o num\xE9rica para cada uma delas:"),ar.forEach(a),Uo=d(e),G=i(e,"DIV",{class:!0});var or=n(G);xe=i(or,"IMG",{class:!0,src:!0,alt:!0}),Tr=d(or),Pe=i(or,"IMG",{class:!0,src:!0,alt:!0}),or.forEach(a),Lo=d(e),re=i(e,"P",{});var sr=n(re);Ir=l(sr,"H\xE1 diferentes maneiras de dividir o texto. Por exemplo, poder\xEDamos utilizar o espa\xE7o em branco para simbolizar o texto em palavras, usando a fun\xE7\xE3o "),Ua=i(sr,"CODE",{});var Fi=n(Ua);Dr=l(Fi,"split()"),Fi.forEach(a),Nr=l(sr," do Python:"),sr.forEach(a),Go=d(e),h(je.$$.fragment,e),Fo=d(e),h(ye.$$.fragment,e),Ro=d(e),pa=i(e,"P",{});var Ri=n(pa);Or=l(Ri,"H\xE1 tamb\xE9m varia\xE7\xF5es de tokenizers de palavras que t\xEAm regras extras para pontua\xE7\xE3o. Com este tipo de tokenizer, podemos terminar com alguns \u201Cvocabul\xE1rios\u201D bem grandes, onde um vocabul\xE1rio \xE9 definido pelo n\xFAmero total de tokens independentes que tem no texto de exemplo."),Ri.forEach(a),Jo=d(e),da=i(e,"P",{});var Ji=n(da);Sr=l(Ji,"A cada palavra \xE9 atribu\xEDda uma identifica\xE7\xE3o, come\xE7ando em 0 e indo at\xE9 o tamanho do vocabul\xE1rio. O modelo utiliza estas identifica\xE7\xF5es para identificar cada palavra."),Ji.forEach(a),Vo=d(e),ua=i(e,"P",{});var Vi=n(ua);Cr=l(Vi,"Se quisermos cobrir completamente um idioma com um tokenizer baseado em palavras, precisaremos ter um identificador para cada palavra no idioma, o que gerar\xE1 uma enorme quantidade de tokens. Por exemplo, existem mais de 500.000 palavras no idioma ingl\xEAs, portanto, para construir um mapa a partir de cada palavra para um ID de entrada, precisar\xEDamos manter um registro desse grande n\xFAmero de IDs. Al\xE9m disso, palavras como \u201Cdog\u201D s\xE3o representadas de forma diferente de palavras como \u201Cdogs\u201D, e o modelo inicialmente n\xE3o ter\xE1 como saber que \u201Cdog\u201D e \u201Cdogs\u201D s\xE3o semelhantes: ele identificar\xE1 as duas palavras como n\xE3o relacionadas. O mesmo se aplica a outras palavras semelhantes, como \u201Crun\u201D e \u201Crunning\u201D, que o modelo n\xE3o ver\xE1 inicialmente como sendo semelhantes."),Vi.forEach(a),Ko=d(e),Ae=i(e,"P",{});var wi=n(Ae);Hr=l(wi,"Finalmente, precisamos de token personalizada para representar palavras que n\xE3o est\xE3o em nosso vocabul\xE1rio. Isto \xE9 conhecido como o s\xEDmbolo \u201Cunknown\u201D (desconhecido), frequentemente representado como \u201D[UNK]\u201D ou \u201D"),La=i(wi,"UNK",{});var Ki=n(La);Mr=l(Ki,"\u201D. Geralmente \xE9 um mau sinal se voc\xEA v\xEA que o tokenizer est\xE1 produzindo muitos desses tokens, pois n\xE3o foi capaz de recuperar uma representa\xE7\xE3o sensata de uma palavra e voc\xEA est\xE1 perdendo informa\xE7\xF5es ao longo do caminho. O objetivo ao elaborar o vocabul\xE1rio \xE9 faz\xEA-lo de tal forma que o tokenizer transforme o menor n\xFAmero poss\xEDvel de palavras no token desconhecido."),Ki.forEach(a),wi.forEach(a),Yo=d(e),te=i(e,"P",{});var rr=n(te);Br=l(rr,"Uma maneira de reduzir a quantidade de tokens desconhecidas \xE9 ir um n\xEDvel mais fundo, usando um tokenizer "),Ga=i(rr,"EM",{});var Yi=n(Ga);Ur=l(Yi,"baseado em caracteres"),Yi.forEach(a),Lr=l(rr,"."),rr.forEach(a),Wo=d(e),F=i(e,"H2",{class:!0});var tr=n(F);ie=i(tr,"A",{id:!0,class:!0,href:!0});var Wi=n(ie);Fa=i(Wi,"SPAN",{});var Qi=n(Fa);h(Te.$$.fragment,Qi),Qi.forEach(a),Wi.forEach(a),Gr=d(tr),Ra=i(tr,"SPAN",{});var Xi=n(Ra);Fr=l(Xi,"Baseado em caracteres (Character-based)"),Xi.forEach(a),tr.forEach(a),Qo=d(e),h(Ie.$$.fragment,e),Xo=d(e),ca=i(e,"P",{});var Zi=n(ca);Rr=l(Zi,"Os tokenizers baseados em caracteres dividem o texto em caracteres, ao inv\xE9s de palavras. Isto tem dois benef\xEDcios principais:"),Zi.forEach(a),Zo=d(e),ne=i(e,"UL",{});var ir=n(ne);Ja=i(ir,"LI",{});var en=n(Ja);Jr=l(en,"O vocabul\xE1rio ser\xE1 muito menor;"),en.forEach(a),Vr=d(ir),Va=i(ir,"LI",{});var an=n(Va);Kr=l(an,"H\xE1 muito menos tokes fora de vocabul\xE1rio (desconhecidas), uma vez que cada palavra pode ser constru\xEDda a partir de personagens."),an.forEach(a),ir.forEach(a),es=d(e),fa=i(e,"P",{});var on=n(fa);Yr=l(on,"Mas tamb\xE9m aqui surgem algumas quest\xF5es sobre os espa\xE7os e \xE0 pontua\xE7\xE3o:"),on.forEach(a),as=d(e),R=i(e,"DIV",{class:!0});var nr=n(R);De=i(nr,"IMG",{class:!0,src:!0,alt:!0}),Wr=d(nr),Ne=i(nr,"IMG",{class:!0,src:!0,alt:!0}),nr.forEach(a),os=d(e),va=i(e,"P",{});var sn=n(va);Qr=l(sn,"Esta abordagem tamb\xE9m n\xE3o \xE9 perfeita. Como a representa\xE7\xE3o agora \xE9 baseada em caracteres e n\xE3o em palavras, pode-se argumentar que, intuitivamente, ela \xE9 menos significativa: cada caractere n\xE3o significa muito por si s\xF3, ao contrario do caso das palavras. No entanto, isto novamente difere de acordo com o idioma; em chin\xEAs, por exemplo, cada caractere traz mais informa\xE7\xF5es do que um caractere em um idioma latino."),sn.forEach(a),ss=d(e),ba=i(e,"P",{});var rn=n(ba);Xr=l(rn,"Outra coisa a considerar \xE9 que acabaremos com uma quantidade muito grande de tokens a serem processadas por nosso modelo: enquanto uma palavra seria apenas um \xFAnico token com um tokenizer baseado em palavras, ela pode facilmente se transformar em 10 ou mais tokens quando convertida em caracteres."),rn.forEach(a),rs=d(e),me=i(e,"P",{});var mr=n(me);Zr=l(mr,"Para obter o melhor dos dois mundos, podemos usar uma terceira t\xE9cnica que combina as duas abordagens: "),Ka=i(mr,"EM",{});var tn=n(Ka);et=l(tn,"Tokeniza\xE7\xE3o por sub-palavras"),tn.forEach(a),at=l(mr,"."),mr.forEach(a),ts=d(e),J=i(e,"H2",{class:!0});var lr=n(J);le=i(lr,"A",{id:!0,class:!0,href:!0});var nn=n(le);Ya=i(nn,"SPAN",{});var mn=n(Ya);h(Oe.$$.fragment,mn),mn.forEach(a),nn.forEach(a),ot=d(lr),Wa=i(lr,"SPAN",{});var ln=n(Wa);st=l(ln,"Tokeniza\xE7\xE3o por sub-palavras (Subword tokenization)"),ln.forEach(a),lr.forEach(a),is=d(e),h(Se.$$.fragment,e),ns=d(e),ka=i(e,"P",{});var pn=n(ka);rt=l(pn,"Algoritmos de tokeniza\xE7\xE3o de sub-palavras baseiam-se no princ\xEDpio de que palavras frequentemente usadas n\xE3o devem ser divididas em sub-palavras menores, mas palavras raras devem ser decompostas em sub-palavras significativas."),pn.forEach(a),ms=d(e),ha=i(e,"P",{});var dn=n(ha);tt=l(dn,"Por exemplo, \u201Cirritantemente\u201D poderia ser considerado uma palavra rara e poderia ser decomposto em \u201Cirritante\u201D e \u201Cmente\u201D. \xC9 prov\xE1vel que ambas apare\xE7am mais frequentemente como sub-palavras isoladas, enquanto ao mesmo tempo o significado de \u201Cirritantemente\u201D \xE9 mantido pelo significado composto de \u201Cirritante\u201D e \u201Cmente\u201D."),dn.forEach(a),ls=d(e),$a=i(e,"P",{});var un=n($a);it=l(un,"Aqui est\xE1 um exemplo que mostra como um algoritmo de tokeniza\xE7\xE3o de uma sub-palavra indicaria a sequ\xEAncia \u201CLet\u2019s do tokenization!"),un.forEach(a),ps=d(e),V=i(e,"DIV",{class:!0});var pr=n(V);Ce=i(pr,"IMG",{class:!0,src:!0,alt:!0}),nt=d(pr),He=i(pr,"IMG",{class:!0,src:!0,alt:!0}),pr.forEach(a),ds=d(e),_a=i(e,"P",{});var cn=n(_a);mt=l(cn,"Estas sub-palavras acabam fornecendo muito significado sem\xE2ntico: por exemplo, no exemplo acima \u201Ctokenization\u201D foi dividido em \u201Ctoken\u201D e \u201Cization\u201D, dois tokens que t\xEAm um significado sem\xE2ntico enquanto s\xE3o eficientes em termos de espa\xE7o (apenas dois tokens s\xE3o necess\xE1rios para representar uma palavra longa). Isto nos permite ter uma cobertura relativamente boa com pequenos vocabul\xE1rios, e perto de nenhum token desconhecido."),cn.forEach(a),us=d(e),za=i(e,"P",{});var fn=n(za);lt=l(fn,"Esta abordagem \xE9 especialmente \xFAtil em idiomas aglutinativos como o turco, onde \xE9 poss\xEDvel formar palavras (quase) arbitrariamente longas e complexas, encadeando sub-palavras."),fn.forEach(a),cs=d(e),K=i(e,"H3",{class:!0});var dr=n(K);pe=i(dr,"A",{id:!0,class:!0,href:!0});var vn=n(pe);Qa=i(vn,"SPAN",{});var bn=n(Qa);h(Me.$$.fragment,bn),bn.forEach(a),vn.forEach(a),pt=d(dr),Xa=i(dr,"SPAN",{});var kn=n(Xa);dt=l(kn,"E outros!"),kn.forEach(a),dr.forEach(a),fs=d(e),ga=i(e,"P",{});var hn=n(ga);ut=l(hn,"Sem surpresas, h\xE1 muito mais t\xE9cnicas por a\xED. Para citar algumas:"),hn.forEach(a),vs=d(e),O=i(e,"UL",{});var Da=n(O);Za=i(Da,"LI",{});var $n=n(Za);ct=l($n,"Byte-level BPE, utilizada no GPT-2"),$n.forEach(a),ft=d(Da),eo=i(Da,"LI",{});var _n=n(eo);vt=l(_n,"WordPiece, utilizada em BERT"),_n.forEach(a),bt=d(Da),ao=i(Da,"LI",{});var zn=n(ao);kt=l(zn,"SentencePiece ou Unigram, como as utilizadas em v\xE1rios modelos  multil\xEDngue"),zn.forEach(a),Da.forEach(a),bs=d(e),Ea=i(e,"P",{});var gn=n(Ea);ht=l(gn,"Agora voc\xEA deve ter conhecimento suficiente de como funcionam os tokenizers para come\xE7ar a utilizar a API."),gn.forEach(a),ks=d(e),Y=i(e,"H2",{class:!0});var ur=n(Y);de=i(ur,"A",{id:!0,class:!0,href:!0});var En=n(de);oo=i(En,"SPAN",{});var qn=n(oo);h(Be.$$.fragment,qn),qn.forEach(a),En.forEach(a),$t=d(ur),so=i(ur,"SPAN",{});var wn=n(so);_t=l(wn,"Carregando e salvando"),wn.forEach(a),ur.forEach(a),hs=d(e),y=i(e,"P",{});var M=n(y);zt=l(M,"Carregando e salvando tokenizers \xE9 t\xE3o simples quanto com os modelos. Na verdade, ele se baseia nos mesmos dois m\xE9todos: "),ro=i(M,"CODE",{});var xn=n(ro);gt=l(xn,"from_pretrained()"),xn.forEach(a),Et=l(M," e "),to=i(M,"CODE",{});var Pn=n(to);qt=l(Pn,"save_pretrained()"),Pn.forEach(a),wt=l(M,". Estes m\xE9todos ir\xE3o carregar ou salvar o algoritmo utilizado pelo tokenizer (um pouco como a "),io=i(M,"EM",{});var jn=n(io);xt=l(jn,"arquitetura"),jn.forEach(a),Pt=l(M," do modelo), bem como seu vocabul\xE1rio (um pouco como os "),no=i(M,"EM",{});var yn=n(no);jt=l(yn,"pesos"),yn.forEach(a),yt=l(M," do modelo)."),M.forEach(a),$s=d(e),ue=i(e,"P",{});var cr=n(ue);At=l(cr,"O carregamento do tokenizer BERT treinado com o mesmo checkpoint do BERT \xE9 feito da mesma forma que o carregamento do modelo, exceto que utilizamos a classe "),mo=i(cr,"CODE",{});var An=n(mo);Tt=l(An,"BertTokenizer"),An.forEach(a),It=l(cr,":"),cr.forEach(a),_s=d(e),h(Ue.$$.fragment,e),zs=d(e),ae.l(e),qa=d(e),h(Le.$$.fragment,e),gs=d(e),wa=i(e,"P",{});var Tn=n(wa);Dt=l(Tn,"Agora podemos usar o tokenizer, como mostrado na se\xE7\xE3o anterior:"),Tn.forEach(a),Es=d(e),h(Ge.$$.fragment,e),qs=d(e),h(Fe.$$.fragment,e),ws=d(e),xa=i(e,"P",{});var In=n(xa);Nt=l(In,"Salvar um tokenizer \xE9 id\xEAntico a salvar um modelo:"),In.forEach(a),xs=d(e),h(Re.$$.fragment,e),Ps=d(e),S=i(e,"P",{});var Na=n(S);Ot=l(Na,"Falaremos mais sobre "),lo=i(Na,"CODE",{});var Dn=n(lo);St=l(Dn,"token_type_ids' no [Cap\xEDtulo 3](/course/pt/chapter3), e explicaremos a "),Dn.forEach(a),Ct=l(Na,"attention_mask\u2019 um pouco mais tarde. Primeiro, vamos ver como os "),po=i(Na,"CODE",{});var Nn=n(po);Ht=l(Nn,"input_ids"),Nn.forEach(a),Mt=l(Na," s\xE3o gerados. Para fazer isso, precisaremos olhar os m\xE9todos intermedi\xE1rios do tokenizer."),Na.forEach(a),js=d(e),W=i(e,"H2",{class:!0});var fr=n(W);ce=i(fr,"A",{id:!0,class:!0,href:!0});var On=n(ce);uo=i(On,"SPAN",{});var Sn=n(uo);h(Je.$$.fragment,Sn),Sn.forEach(a),On.forEach(a),Bt=d(fr),co=i(fr,"SPAN",{});var Cn=n(co);Ut=l(Cn,"Encoding"),Cn.forEach(a),fr.forEach(a),ys=d(e),h(Ve.$$.fragment,e),As=d(e),fe=i(e,"P",{});var vr=n(fe);Lt=l(vr,"Traduzir texto para n\xFAmeros \xE9 conhecido como "),fo=i(vr,"EM",{});var Hn=n(fo);Gt=l(Hn,"encoding"),Hn.forEach(a),Ft=l(vr,". O encoding \xE9 feito em um processo de duas etapas: a tokeniza\xE7\xE3o, seguida pela convers\xE3o para IDs de entrada."),vr.forEach(a),Ts=d(e),ve=i(e,"P",{});var br=n(ve);Rt=l(br,"Como vimos, o primeiro passo \xE9 dividir o texto em palavras (ou partes de palavras, s\xEDmbolos de pontua\xE7\xE3o, etc.), normalmente chamadas de "),vo=i(br,"EM",{});var Mn=n(vo);Jt=l(Mn,"tokens"),Mn.forEach(a),Vt=l(br,". H\xE1 v\xE1rias regras que podem guiar esse processo, e \xE9 por isso que precisamos instanciar o tokenizer usando o nome do modelo, para nos certificarmos de usar as mesmas regras que foram usadas quando o modelo foi pr\xE9-treinado."),br.forEach(a),Is=d(e),C=i(e,"P",{});var Oa=n(C);Kt=l(Oa,"O segundo passo \xE9 converter esses tokens em n\xFAmeros, para que possamos construir um tensor a partir deles e aliment\xE1-los com o modelo. Para isso, o tokenizer tem um "),bo=i(Oa,"EM",{});var Bn=n(bo);Yt=l(Bn,"vocabul\xE1rio"),Bn.forEach(a),Wt=l(Oa," (vocabulary), que \xE9 a parte que realizamos o download quando o instanciamos com o m\xE9todo "),ko=i(Oa,"CODE",{});var Un=n(ko);Qt=l(Un,"from_pretrained()"),Un.forEach(a),Xt=l(Oa,". Mais uma vez, precisamos utilizar o mesmo vocabul\xE1rio utilizado quando o modelo foi pr\xE9-treinado."),Oa.forEach(a),Ds=d(e),Pa=i(e,"P",{});var Ln=n(Pa);Zt=l(Ln,"Para entender melhor os dois passos, vamos explor\xE1-los separadamente. Note que usaremos alguns m\xE9todos que executam partes da pipeline de tokeniza\xE7\xE3o separadamente para mostrar os resultados intermedi\xE1rios dessas etapas, mas na pr\xE1tica, voc\xEA deve chamar o tokenizer diretamente em suas entradas (como mostrado na se\xE7\xE3o 2)."),Ln.forEach(a),Ns=d(e),Q=i(e,"H3",{class:!0});var kr=n(Q);be=i(kr,"A",{id:!0,class:!0,href:!0});var Gn=n(be);ho=i(Gn,"SPAN",{});var Fn=n(ho);h(Ke.$$.fragment,Fn),Fn.forEach(a),Gn.forEach(a),ei=d(kr),$o=i(kr,"SPAN",{});var Rn=n($o);ai=l(Rn,"Tokeniza\xE7\xE3o"),Rn.forEach(a),kr.forEach(a),Os=d(e),ke=i(e,"P",{});var hr=n(ke);oi=l(hr,"O processo de tokenization \xE9 feito atrav\xE9s do m\xE9todo "),_o=i(hr,"CODE",{});var Jn=n(_o);si=l(Jn,"tokenize()"),Jn.forEach(a),ri=l(hr," do tokenizer:"),hr.forEach(a),Ss=d(e),h(Ye.$$.fragment,e),Cs=d(e),ja=i(e,"P",{});var Vn=n(ja);ti=l(Vn,"A sa\xEDda deste m\xE9todo \xE9 uma lista de strings, ou tokens:"),Vn.forEach(a),Hs=d(e),h(We.$$.fragment,e),Ms=d(e),ya=i(e,"P",{});var Kn=n(ya);ii=l(Kn,"Este tokenizer \xE9 um tokenizer de sub-palavras: ele divide as palavras at\xE9 obter tokens que podem ser representadas por seu vocabul\xE1rio. \xC9 o caso aqui do \u201Ctransformer\u201D, que \xE9 dividido em dois tokens: \u201Ctransform\u201D e \u201C##er\u201D."),Kn.forEach(a),Bs=d(e),X=i(e,"H3",{class:!0});var $r=n(X);he=i($r,"A",{id:!0,class:!0,href:!0});var Yn=n(he);zo=i(Yn,"SPAN",{});var Wn=n(zo);h(Qe.$$.fragment,Wn),Wn.forEach(a),Yn.forEach(a),ni=d($r),go=i($r,"SPAN",{});var Qn=n(go);mi=l(Qn,"Desde os tokens at\xE9 IDs de entrada"),Qn.forEach(a),$r.forEach(a),Us=d(e),$e=i(e,"P",{});var _r=n($e);li=l(_r,"A convers\xE3o para IDs de entrada \xE9 feita pelo m\xE9todo de tokeniza\xE7\xE3o "),Eo=i(_r,"CODE",{});var Xn=n(Eo);pi=l(Xn,"convert_tokens_to_ids()"),Xn.forEach(a),di=l(_r,":"),_r.forEach(a),Ls=d(e),h(Xe.$$.fragment,e),Gs=d(e),h(Ze.$$.fragment,e),Fs=d(e),Aa=i(e,"P",{});var Zn=n(Aa);ui=l(Zn,"Estas sa\xEDdas, uma vez convertidas no tensor com a estrutura apropriada, podem ent\xE3o ser usadas como entradas para um modelo como visto anteriormente neste cap\xEDtulo."),Zn.forEach(a),Rs=d(e),h(_e.$$.fragment,e),Js=d(e),Z=i(e,"H2",{class:!0});var zr=n(Z);ze=i(zr,"A",{id:!0,class:!0,href:!0});var em=n(ze);qo=i(em,"SPAN",{});var am=n(qo);h(ea.$$.fragment,am),am.forEach(a),em.forEach(a),ci=d(zr),wo=i(zr,"SPAN",{});var om=n(wo);fi=l(om,"Decoding"),om.forEach(a),zr.forEach(a),Vs=d(e),ee=i(e,"P",{});var To=n(ee);xo=i(To,"EM",{});var sm=n(xo);vi=l(sm,"Decoding"),sm.forEach(a),bi=l(To," vai pela dire\xE7\xE3o ao contr\xE1rio: a partir de \xEDndices de vocabul\xE1rio, queremos obter uma string. Isto pode ser feito com o m\xE9todo "),Po=i(To,"CODE",{});var rm=n(Po);ki=l(rm,"decode()"),rm.forEach(a),hi=l(To," da seguinte forma:"),To.forEach(a),Ks=d(e),h(aa.$$.fragment,e),Ys=d(e),h(oa.$$.fragment,e),Ws=d(e),H=i(e,"P",{});var Sa=n(H);$i=l(Sa,"Observe que o m\xE9todo "),jo=i(Sa,"CODE",{});var tm=n(jo);_i=l(tm,"decode"),tm.forEach(a),zi=l(Sa," n\xE3o apenas converte os \xEDndices em tokens, mas tamb\xE9m agrupa os tokens que fizeram parte das mesmas palavras para produzir uma frase leg\xEDvel. Este comportamento ser\xE1 extremamente \xFAtil quando utilizamos modelos que preveem um novo texto (seja texto gerado a partir de um prompt, ou para problemas de "),yo=i(Sa,"EM",{});var im=n(yo);gi=l(im,"sequence-to-sequence"),im.forEach(a),Ei=l(Sa," como tradu\xE7\xE3o ou sumariza\xE7\xE3o)."),Sa.forEach(a),Qs=d(e),Ta=i(e,"P",{});var nm=n(Ta);qi=l(nm,"At\xE9 agora voc\xEA j\xE1 deve entender as opera\xE7\xF5es at\xF4micas que um tokenizer pode lidar: tokeniza\xE7\xE3o, convers\xE3o para IDs, e convers\xE3o de IDs de volta para uma string. Entretanto, acabamos de come\xE7ar a ver a ponta do iceberg. Na se\xE7\xE3o seguinte, vamos nos aproximar de seus limites e dar uma olhada em como super\xE1-los."),nm.forEach(a),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(qm)),u(g,"id","tokenizers"),u(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(g,"href","#tokenizers"),u(w,"class","relative group"),u(oe,"id","baseado-em-palavras-wordbased"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#baseado-em-palavras-wordbased"),u(L,"class","relative group"),u(xe,"class","block dark:hidden"),Ca(xe.src,xi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||u(xe,"src",xi),u(xe,"alt","Um exemplo de tokeniza\xE7\xE3o baseado em palavras."),u(Pe,"class","hidden dark:block"),Ca(Pe.src,Pi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||u(Pe,"src",Pi),u(Pe,"alt","Um exemplo de tokeniza\xE7\xE3o baseado em palavras."),u(G,"class","flex justify-center"),u(ie,"id","baseado-em-caracteres-characterbased"),u(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ie,"href","#baseado-em-caracteres-characterbased"),u(F,"class","relative group"),u(De,"class","block dark:hidden"),Ca(De.src,ji="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||u(De,"src",ji),u(De,"alt","Um exemplo de tokeniza\xE7\xE3o baseado em caracteres."),u(Ne,"class","hidden dark:block"),Ca(Ne.src,yi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||u(Ne,"src",yi),u(Ne,"alt","Um exemplo de tokeniza\xE7\xE3o baseado em caracteres."),u(R,"class","flex justify-center"),u(le,"id","tokenizao-por-subpalavras-subword-tokenization"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#tokenizao-por-subpalavras-subword-tokenization"),u(J,"class","relative group"),u(Ce,"class","block dark:hidden"),Ca(Ce.src,Ai="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||u(Ce,"src",Ai),u(Ce,"alt","Exemplo de algoritmo de tokeniza\xE7\xE3o por sub-palavras."),u(He,"class","hidden dark:block"),Ca(He.src,Ti="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||u(He,"src",Ti),u(He,"alt","Exemplo de algoritmo de tokeniza\xE7\xE3o por sub-palavras."),u(V,"class","flex justify-center"),u(pe,"id","e-outros"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#e-outros"),u(K,"class","relative group"),u(de,"id","carregando-e-salvando"),u(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(de,"href","#carregando-e-salvando"),u(Y,"class","relative group"),u(ce,"id","encoding"),u(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ce,"href","#encoding"),u(W,"class","relative group"),u(be,"id","tokenizao"),u(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(be,"href","#tokenizao"),u(Q,"class","relative group"),u(he,"id","desde-os-tokens-at-ids-de-entrada"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#desde-os-tokens-at-ids-de-entrada"),u(X,"class","relative group"),u(ze,"id","decoding"),u(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ze,"href","#decoding"),u(Z,"class","relative group")},m(e,s){o(document.head,c),r(e,z,s),$(f,e,s),r(e,q,s),r(e,w,s),o(w,g),o(g,x),$(P,x,null),o(w,j),o(w,E),o(E,U),r(e,N,s),sa[I].m(e,s),r(e,ta,s),$(ge,e,s),r(e,Do,s),r(e,ia,s),o(ia,gr),r(e,No,s),r(e,na,s),o(na,Er),r(e,Oo,s),$(Ee,e,s),r(e,So,s),r(e,ma,s),o(ma,qr),r(e,Co,s),r(e,la,s),o(la,wr),r(e,Ho,s),r(e,L,s),o(L,oe),o(oe,Ha),$(qe,Ha,null),o(L,xr),o(L,Ma),o(Ma,Pr),r(e,Mo,s),$(we,e,s),r(e,Bo,s),r(e,se,s),o(se,jr),o(se,Ba),o(Ba,yr),o(se,Ar),r(e,Uo,s),r(e,G,s),o(G,xe),o(G,Tr),o(G,Pe),r(e,Lo,s),r(e,re,s),o(re,Ir),o(re,Ua),o(Ua,Dr),o(re,Nr),r(e,Go,s),$(je,e,s),r(e,Fo,s),$(ye,e,s),r(e,Ro,s),r(e,pa,s),o(pa,Or),r(e,Jo,s),r(e,da,s),o(da,Sr),r(e,Vo,s),r(e,ua,s),o(ua,Cr),r(e,Ko,s),r(e,Ae,s),o(Ae,Hr),o(Ae,La),o(La,Mr),r(e,Yo,s),r(e,te,s),o(te,Br),o(te,Ga),o(Ga,Ur),o(te,Lr),r(e,Wo,s),r(e,F,s),o(F,ie),o(ie,Fa),$(Te,Fa,null),o(F,Gr),o(F,Ra),o(Ra,Fr),r(e,Qo,s),$(Ie,e,s),r(e,Xo,s),r(e,ca,s),o(ca,Rr),r(e,Zo,s),r(e,ne,s),o(ne,Ja),o(Ja,Jr),o(ne,Vr),o(ne,Va),o(Va,Kr),r(e,es,s),r(e,fa,s),o(fa,Yr),r(e,as,s),r(e,R,s),o(R,De),o(R,Wr),o(R,Ne),r(e,os,s),r(e,va,s),o(va,Qr),r(e,ss,s),r(e,ba,s),o(ba,Xr),r(e,rs,s),r(e,me,s),o(me,Zr),o(me,Ka),o(Ka,et),o(me,at),r(e,ts,s),r(e,J,s),o(J,le),o(le,Ya),$(Oe,Ya,null),o(J,ot),o(J,Wa),o(Wa,st),r(e,is,s),$(Se,e,s),r(e,ns,s),r(e,ka,s),o(ka,rt),r(e,ms,s),r(e,ha,s),o(ha,tt),r(e,ls,s),r(e,$a,s),o($a,it),r(e,ps,s),r(e,V,s),o(V,Ce),o(V,nt),o(V,He),r(e,ds,s),r(e,_a,s),o(_a,mt),r(e,us,s),r(e,za,s),o(za,lt),r(e,cs,s),r(e,K,s),o(K,pe),o(pe,Qa),$(Me,Qa,null),o(K,pt),o(K,Xa),o(Xa,dt),r(e,fs,s),r(e,ga,s),o(ga,ut),r(e,vs,s),r(e,O,s),o(O,Za),o(Za,ct),o(O,ft),o(O,eo),o(eo,vt),o(O,bt),o(O,ao),o(ao,kt),r(e,bs,s),r(e,Ea,s),o(Ea,ht),r(e,ks,s),r(e,Y,s),o(Y,de),o(de,oo),$(Be,oo,null),o(Y,$t),o(Y,so),o(so,_t),r(e,hs,s),r(e,y,s),o(y,zt),o(y,ro),o(ro,gt),o(y,Et),o(y,to),o(to,qt),o(y,wt),o(y,io),o(io,xt),o(y,Pt),o(y,no),o(no,jt),o(y,yt),r(e,$s,s),r(e,ue,s),o(ue,At),o(ue,mo),o(mo,Tt),o(ue,It),r(e,_s,s),$(Ue,e,s),r(e,zs,s),ae.m(e,s),r(e,qa,s),$(Le,e,s),r(e,gs,s),r(e,wa,s),o(wa,Dt),r(e,Es,s),$(Ge,e,s),r(e,qs,s),$(Fe,e,s),r(e,ws,s),r(e,xa,s),o(xa,Nt),r(e,xs,s),$(Re,e,s),r(e,Ps,s),r(e,S,s),o(S,Ot),o(S,lo),o(lo,St),o(S,Ct),o(S,po),o(po,Ht),o(S,Mt),r(e,js,s),r(e,W,s),o(W,ce),o(ce,uo),$(Je,uo,null),o(W,Bt),o(W,co),o(co,Ut),r(e,ys,s),$(Ve,e,s),r(e,As,s),r(e,fe,s),o(fe,Lt),o(fe,fo),o(fo,Gt),o(fe,Ft),r(e,Ts,s),r(e,ve,s),o(ve,Rt),o(ve,vo),o(vo,Jt),o(ve,Vt),r(e,Is,s),r(e,C,s),o(C,Kt),o(C,bo),o(bo,Yt),o(C,Wt),o(C,ko),o(ko,Qt),o(C,Xt),r(e,Ds,s),r(e,Pa,s),o(Pa,Zt),r(e,Ns,s),r(e,Q,s),o(Q,be),o(be,ho),$(Ke,ho,null),o(Q,ei),o(Q,$o),o($o,ai),r(e,Os,s),r(e,ke,s),o(ke,oi),o(ke,_o),o(_o,si),o(ke,ri),r(e,Ss,s),$(Ye,e,s),r(e,Cs,s),r(e,ja,s),o(ja,ti),r(e,Hs,s),$(We,e,s),r(e,Ms,s),r(e,ya,s),o(ya,ii),r(e,Bs,s),r(e,X,s),o(X,he),o(he,zo),$(Qe,zo,null),o(X,ni),o(X,go),o(go,mi),r(e,Us,s),r(e,$e,s),o($e,li),o($e,Eo),o(Eo,pi),o($e,di),r(e,Ls,s),$(Xe,e,s),r(e,Gs,s),$(Ze,e,s),r(e,Fs,s),r(e,Aa,s),o(Aa,ui),r(e,Rs,s),$(_e,e,s),r(e,Js,s),r(e,Z,s),o(Z,ze),o(ze,qo),$(ea,qo,null),o(Z,ci),o(Z,wo),o(wo,fi),r(e,Vs,s),r(e,ee,s),o(ee,xo),o(xo,vi),o(ee,bi),o(ee,Po),o(Po,ki),o(ee,hi),r(e,Ks,s),$(aa,e,s),r(e,Ys,s),$(oa,e,s),r(e,Ws,s),r(e,H,s),o(H,$i),o(H,jo),o(jo,_i),o(H,zi),o(H,yo),o(yo,gi),o(H,Ei),r(e,Qs,s),r(e,Ta,s),o(Ta,qi),Xs=!0},p(e,[s]){const ra={};s&1&&(ra.fw=e[0]),f.$set(ra);let Ia=I;I=Di(e),I!==Ia&&(vm(),v(sa[Ia],1,1,()=>{sa[Ia]=null}),cm(),D=sa[I],D||(D=sa[I]=Ii[I](e),D.c()),b(D,1),D.m(ta.parentNode,ta)),Zs!==(Zs=Ni(e))&&(ae.d(1),ae=Zs(e),ae&&(ae.c(),ae.m(qa.parentNode,qa)));const Ao={};s&2&&(Ao.$$scope={dirty:s,ctx:e}),_e.$set(Ao)},i(e){Xs||(b(f.$$.fragment,e),b(P.$$.fragment,e),b(D),b(ge.$$.fragment,e),b(Ee.$$.fragment,e),b(qe.$$.fragment,e),b(we.$$.fragment,e),b(je.$$.fragment,e),b(ye.$$.fragment,e),b(Te.$$.fragment,e),b(Ie.$$.fragment,e),b(Oe.$$.fragment,e),b(Se.$$.fragment,e),b(Me.$$.fragment,e),b(Be.$$.fragment,e),b(Ue.$$.fragment,e),b(Le.$$.fragment,e),b(Ge.$$.fragment,e),b(Fe.$$.fragment,e),b(Re.$$.fragment,e),b(Je.$$.fragment,e),b(Ve.$$.fragment,e),b(Ke.$$.fragment,e),b(Ye.$$.fragment,e),b(We.$$.fragment,e),b(Qe.$$.fragment,e),b(Xe.$$.fragment,e),b(Ze.$$.fragment,e),b(_e.$$.fragment,e),b(ea.$$.fragment,e),b(aa.$$.fragment,e),b(oa.$$.fragment,e),Xs=!0)},o(e){v(f.$$.fragment,e),v(P.$$.fragment,e),v(D),v(ge.$$.fragment,e),v(Ee.$$.fragment,e),v(qe.$$.fragment,e),v(we.$$.fragment,e),v(je.$$.fragment,e),v(ye.$$.fragment,e),v(Te.$$.fragment,e),v(Ie.$$.fragment,e),v(Oe.$$.fragment,e),v(Se.$$.fragment,e),v(Me.$$.fragment,e),v(Be.$$.fragment,e),v(Ue.$$.fragment,e),v(Le.$$.fragment,e),v(Ge.$$.fragment,e),v(Fe.$$.fragment,e),v(Re.$$.fragment,e),v(Je.$$.fragment,e),v(Ve.$$.fragment,e),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(We.$$.fragment,e),v(Qe.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),v(_e.$$.fragment,e),v(ea.$$.fragment,e),v(aa.$$.fragment,e),v(oa.$$.fragment,e),Xs=!1},d(e){a(c),e&&a(z),_(f,e),e&&a(q),e&&a(w),_(P),e&&a(N),sa[I].d(e),e&&a(ta),_(ge,e),e&&a(Do),e&&a(ia),e&&a(No),e&&a(na),e&&a(Oo),_(Ee,e),e&&a(So),e&&a(ma),e&&a(Co),e&&a(la),e&&a(Ho),e&&a(L),_(qe),e&&a(Mo),_(we,e),e&&a(Bo),e&&a(se),e&&a(Uo),e&&a(G),e&&a(Lo),e&&a(re),e&&a(Go),_(je,e),e&&a(Fo),_(ye,e),e&&a(Ro),e&&a(pa),e&&a(Jo),e&&a(da),e&&a(Vo),e&&a(ua),e&&a(Ko),e&&a(Ae),e&&a(Yo),e&&a(te),e&&a(Wo),e&&a(F),_(Te),e&&a(Qo),_(Ie,e),e&&a(Xo),e&&a(ca),e&&a(Zo),e&&a(ne),e&&a(es),e&&a(fa),e&&a(as),e&&a(R),e&&a(os),e&&a(va),e&&a(ss),e&&a(ba),e&&a(rs),e&&a(me),e&&a(ts),e&&a(J),_(Oe),e&&a(is),_(Se,e),e&&a(ns),e&&a(ka),e&&a(ms),e&&a(ha),e&&a(ls),e&&a($a),e&&a(ps),e&&a(V),e&&a(ds),e&&a(_a),e&&a(us),e&&a(za),e&&a(cs),e&&a(K),_(Me),e&&a(fs),e&&a(ga),e&&a(vs),e&&a(O),e&&a(bs),e&&a(Ea),e&&a(ks),e&&a(Y),_(Be),e&&a(hs),e&&a(y),e&&a($s),e&&a(ue),e&&a(_s),_(Ue,e),e&&a(zs),ae.d(e),e&&a(qa),_(Le,e),e&&a(gs),e&&a(wa),e&&a(Es),_(Ge,e),e&&a(qs),_(Fe,e),e&&a(ws),e&&a(xa),e&&a(xs),_(Re,e),e&&a(Ps),e&&a(S),e&&a(js),e&&a(W),_(Je),e&&a(ys),_(Ve,e),e&&a(As),e&&a(fe),e&&a(Ts),e&&a(ve),e&&a(Is),e&&a(C),e&&a(Ds),e&&a(Pa),e&&a(Ns),e&&a(Q),_(Ke),e&&a(Os),e&&a(ke),e&&a(Ss),_(Ye,e),e&&a(Cs),e&&a(ja),e&&a(Hs),_(We,e),e&&a(Ms),e&&a(ya),e&&a(Bs),e&&a(X),_(Qe),e&&a(Us),e&&a($e),e&&a(Ls),_(Xe,e),e&&a(Gs),_(Ze,e),e&&a(Fs),e&&a(Aa),e&&a(Rs),_(_e,e),e&&a(Js),e&&a(Z),_(ea),e&&a(Vs),e&&a(ee),e&&a(Ks),_(aa,e),e&&a(Ys),_(oa,e),e&&a(Ws),e&&a(H),e&&a(Qs),e&&a(Ta)}}}const qm={local:"tokenizers",sections:[{local:"baseado-em-palavras-wordbased",title:"Baseado em palavras (word-based)"},{local:"baseado-em-caracteres-characterbased",title:"Baseado em caracteres (Character-based)"},{local:"tokenizao-por-subpalavras-subword-tokenization",sections:[{local:"e-outros",title:"E outros!"}],title:"Tokeniza\xE7\xE3o por sub-palavras (Subword tokenization)"},{local:"carregando-e-salvando",title:"Carregando e salvando"},{local:"encoding",sections:[{local:"tokenizao",title:"Tokeniza\xE7\xE3o"},{local:"desde-os-tokens-at-ids-de-entrada",title:"Desde os tokens at\xE9 IDs de entrada"}],title:"Encoding"},{local:"decoding",title:"Decoding"}],title:"Tokenizers"};function wm(T,c,z){let f="pt";return fm(()=>{const q=new URLSearchParams(window.location.search);z(0,f=q.get("fw")||"pt")}),[f]}class Dm extends lm{constructor(c){super();pm(this,c,wm,Em,dm,{})}}export{Dm as default,qm as metadata};
