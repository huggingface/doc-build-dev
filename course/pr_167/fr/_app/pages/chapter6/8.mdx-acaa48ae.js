import{S as Nq,i as Bq,s as Aq,e as r,k as u,w as m,t as n,M as Fq,c as l,d as t,m as c,a,x as d,h as o,b as E,N as Sq,F as s,g as p,y as f,q as k,o as v,B as _,v as Uq}from"../../chunks/vendor-1e8b365d.js";import{T as Rq}from"../../chunks/Tip-62b14c6e.js";import{Y as Wq}from"../../chunks/Youtube-c2a8cc39.js";import{I as ai}from"../../chunks/IconCopyLink-483c28ba.js";import{C as h}from"../../chunks/CodeBlock-e5764662.js";import{D as Gq}from"../../chunks/DocNotebookDropdown-37d928d3.js";function Iq(ii){let z,ge,oe,ie,$e,qe,As,je,Fs,ss,re,ts,Pe,Xe,ns,R,xe,Us,Rs,be,Ws,Gs,le,Is,Ke;return{c(){z=r("p"),ge=r("strong"),oe=n("Pour aller plus loin"),ie=n(" Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),$e=r("code"),qe=n('u"\\u0085"'),As=n(` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),je=r("code"),Fs=n("normalizers.Sequence"),ss=n(", nous n\u2019avons pas inclus les Regex que le "),re=r("code"),ts=n("BertNormalizer"),Pe=n(" requiert quand l\u2019argument "),Xe=r("code"),ns=n("clean_text"),R=n(" est mis \xE0 "),xe=r("code"),Us=n("True"),Rs=n(" ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),be=r("code"),Ws=n("BertNormalizer"),Gs=n(" en ajoutant deux "),le=r("code"),Is=n("normalizers.Replace"),Ke=n(" \xE0 la s\xE9quence de normalisation.")},l(He){z=l(He,"P",{});var x=a(z);ge=l(x,"STRONG",{});var Cn=a(ge);oe=o(Cn,"Pour aller plus loin"),Cn.forEach(t),ie=o(x," Si vous testez les deux versions des normaliseurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),$e=l(x,"CODE",{});var J=a($e);qe=o(J,'u"\\u0085"'),J.forEach(t),As=o(x,` vous remarquerez s\xFBrement qu\u2019ils ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),je=l(x,"CODE",{});var Dn=a(je);Fs=o(Dn,"normalizers.Sequence"),Dn.forEach(t),ss=o(x,", nous n\u2019avons pas inclus les Regex que le "),re=l(x,"CODE",{});var os=a(re);ts=o(os,"BertNormalizer"),os.forEach(t),Pe=o(x," requiert quand l\u2019argument "),Xe=l(x,"CODE",{});var Ln=a(Xe);ns=o(Ln,"clean_text"),Ln.forEach(t),R=o(x," est mis \xE0 "),xe=l(x,"CODE",{});var On=a(xe);Us=o(On,"True"),On.forEach(t),Rs=o(x," ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),be=l(x,"CODE",{});var rs=a(be);Ws=o(rs,"BertNormalizer"),rs.forEach(t),Gs=o(x," en ajoutant deux "),le=l(x,"CODE",{});var Mn=a(le);Is=o(Mn,"normalizers.Replace"),Mn.forEach(t),Ke=o(x," \xE0 la s\xE9quence de normalisation."),x.forEach(t)},m(He,x){p(He,z,x),s(z,ge),s(ge,oe),s(z,ie),s(z,$e),s($e,qe),s(z,As),s(z,je),s(je,Fs),s(z,ss),s(z,re),s(re,ts),s(z,Pe),s(z,Xe),s(Xe,ns),s(z,R),s(z,xe),s(xe,Us),s(z,Rs),s(z,be),s(be,Ws),s(z,Gs),s(z,le),s(le,Is),s(z,Ke)},d(He){He&&t(z)}}}function Vq(ii){let z,ge,oe,ie,$e,qe,As,je,Fs,ss,re,ts,Pe,Xe,ns,R,xe,Us,Rs,be,Ws,Gs,le,Is,Ke,He,x,Cn,J,Dn,os,Ln,On,rs,Mn,qc,mo,jc,xc,pi,yn,bc,ui,Je,Vs,ez,gc,Xs,sz,ci,W,Pc,fo,wc,Tc,ko,Cc,Dc,vo,Lc,Oc,Sn,Mc,yc,_o,Sc,Nc,mi,Ks,di,ls,Bc,ho,Ac,Fc,fi,G,we,Eo,Uc,Rc,zo,Wc,Gc,Hs,Ic,Vc,Xc,Te,$o,Kc,Hc,qo,Jc,Yc,Js,Zc,Qc,em,I,jo,sm,tm,xo,nm,om,bo,rm,lm,go,am,im,Po,pm,um,Ys,cm,mm,dm,Ce,wo,fm,km,To,vm,_m,Zs,hm,Em,zm,De,Co,$m,qm,Do,jm,xm,Qs,bm,gm,Pm,Le,Lo,wm,Tm,Oo,Cm,Dm,et,Lm,Om,ki,as,Mm,st,ym,Sm,vi,Ye,is,Mo,tt,Nm,yo,Bm,_i,pe,Am,So,Fm,Um,Nn,Rm,Wm,nt,Gm,Im,hi,ot,Ei,Oe,Vm,No,Xm,Km,Bo,Hm,Jm,zi,ps,Ym,Ao,Zm,Qm,$i,rt,qi,ue,ed,Fo,sd,td,Uo,nd,od,Ro,rd,ld,ji,Ze,us,Wo,lt,ad,at,id,Go,pd,ud,xi,b,cd,Io,md,dd,Vo,fd,kd,Xo,vd,_d,Ko,hd,Ed,Ho,zd,$d,Jo,qd,jd,Yo,xd,bd,Zo,gd,Pd,bi,Me,wd,Qo,Td,Cd,er,Dd,Ld,gi,it,Pi,ce,Od,sr,Md,yd,tr,Sd,Nd,nr,Bd,Ad,wi,g,Fd,or,Ud,Rd,rr,Wd,Gd,lr,Id,Vd,ar,Xd,Kd,ir,Hd,Jd,pr,Yd,Zd,ur,Qd,ef,cr,sf,tf,Ti,pt,Ci,V,nf,mr,of,rf,dr,lf,af,fr,pf,uf,kr,cf,mf,vr,df,ff,Di,ut,Li,ye,kf,_r,vf,_f,hr,hf,Ef,Oi,Se,zf,Er,$f,qf,zr,jf,xf,Mi,ct,yi,mt,Si,cs,Ni,ms,bf,$r,gf,Pf,Bi,dt,Ai,Bn,wf,Fi,ft,Ui,ds,Tf,qr,Cf,Df,Ri,kt,Wi,vt,Gi,fs,Lf,jr,Of,Mf,Ii,_t,Vi,ht,Xi,ks,yf,xr,Sf,Nf,Ki,Et,Hi,zt,Ji,me,Bf,br,Af,Ff,gr,Uf,Rf,Pr,Wf,Gf,Yi,$t,Zi,O,If,wr,Vf,Xf,Tr,Kf,Hf,Cr,Jf,Yf,Dr,Zf,Qf,Lr,ek,sk,Or,tk,nk,Qi,An,ok,ep,qt,sp,Ne,rk,Mr,lk,ak,yr,ik,pk,tp,jt,np,Be,uk,Sr,ck,mk,Nr,dk,fk,op,xt,rp,bt,lp,j,kk,Br,vk,_k,Ar,hk,Ek,Fr,zk,$k,Ur,qk,jk,Rr,xk,bk,Wr,gk,Pk,Gr,wk,Tk,Ir,Ck,Dk,Vr,Lk,Ok,ap,P,Mk,Xr,yk,Sk,Kr,Nk,Bk,Hr,Ak,Fk,Jr,Uk,Rk,Yr,Wk,Gk,Zr,Ik,Vk,Qr,Xk,Kk,el,Hk,Jk,ip,gt,pp,Pt,up,M,Yk,sl,Zk,Qk,tl,ev,sv,nl,tv,nv,ol,ov,rv,rl,lv,av,ll,iv,pv,cp,Fn,uv,mp,wt,dp,Ae,cv,al,mv,dv,il,fv,kv,fp,Un,vv,kp,Tt,vp,Ct,_p,Rn,_v,hp,Dt,Ep,Lt,zp,vs,hv,pl,Ev,zv,$p,Ot,qp,_s,$v,ul,qv,jv,jp,Mt,xp,yt,bp,hs,xv,cl,bv,gv,gp,St,Pp,Fe,Pv,ml,wv,Tv,dl,Cv,Dv,wp,Nt,Tp,y,Lv,fl,Ov,Mv,kl,yv,Sv,vl,Nv,Bv,_l,Av,Fv,hl,Uv,Rv,El,Wv,Gv,Cp,$,Iv,zl,Vv,Xv,$l,Kv,Hv,ql,Jv,Yv,jl,Zv,Qv,xl,e_,s_,bl,t_,n_,gl,o_,r_,Pl,l_,a_,wl,i_,p_,Tl,u_,c_,Cl,m_,Dl,d_,f_,Dp,Bt,Lp,de,k_,Ll,v_,__,Ol,h_,E_,Ml,z_,$_,Op,At,Mp,S,q_,yl,j_,x_,Sl,b_,g_,Nl,P_,w_,Bl,T_,C_,Al,D_,L_,Fl,O_,M_,yp,Ue,y_,Ul,S_,N_,Rl,B_,A_,Sp,Qe,Es,Wl,Ft,F_,Ut,U_,Gl,R_,W_,Np,fe,G_,Il,I_,V_,Vl,X_,K_,Xl,H_,J_,Bp,Rt,Ap,ke,Y_,Kl,Z_,Q_,Hl,e2,s2,Jl,t2,n2,Fp,Wn,o2,Up,Wt,Rp,zs,r2,Yl,l2,a2,Wp,Gt,Gp,It,Ip,Re,i2,Zl,p2,u2,Ql,c2,m2,Vp,Vt,Xp,N,d2,ea,f2,k2,sa,v2,_2,ta,h2,E2,na,z2,$2,oa,q2,j2,ra,x2,b2,Kp,$s,g2,la,P2,w2,Hp,Xt,Jp,Gn,T2,Yp,Kt,Zp,Ht,Qp,qs,C2,aa,D2,L2,eu,Jt,su,T,O2,ia,M2,y2,pa,S2,N2,ua,B2,A2,ca,F2,U2,ma,R2,W2,da,G2,I2,fa,V2,X2,tu,Yt,nu,Zt,ou,In,K2,ru,Qt,lu,Vn,H2,au,en,iu,sn,pu,ve,J2,ka,Y2,Z2,va,Q2,eh,_a,sh,th,uu,tn,cu,Xn,nh,mu,nn,du,We,oh,ha,rh,lh,Ea,ah,ih,fu,es,js,za,on,ph,rn,uh,$a,ch,mh,ku,Y,dh,qa,fh,kh,ja,vh,_h,xa,hh,Eh,ba,zh,$h,vu,ln,_u,Kn,qh,hu,xs,jh,ga,xh,bh,Eu,an,zu,_e,gh,Pa,Ph,wh,wa,Th,Ch,Ta,Dh,Lh,$u,Ge,Oh,Ca,Mh,yh,Da,Sh,Nh,qu,pn,ju,Hn,Bh,xu,un,bu,cn,gu,bs,Ah,La,Fh,Uh,Pu,mn,wu,C,Rh,Oa,Wh,Gh,Ma,Ih,Vh,ya,Xh,Kh,Sa,Hh,Jh,Na,Yh,Zh,Ba,Qh,eE,Aa,sE,tE,Tu,gs,nE,Fa,oE,rE,Cu,dn,Du,Jn,lE,Lu,fn,Ou,kn,Mu,w,aE,Ua,iE,pE,Ra,uE,cE,Wa,mE,dE,Ga,fE,kE,Ia,vE,_E,Va,hE,EE,Xa,zE,$E,Ka,qE,jE,yu,vn,Su,_n,Nu,Yn,xE,Bu,hn,Au,Zn,bE,Fu,En,Uu,zn,Ru,Ps,gE,Ha,PE,wE,Wu,$n,Gu,D,TE,Ja,CE,DE,Ya,LE,OE,Za,ME,yE,Qa,SE,NE,ei,BE,AE,si,FE,UE,ti,RE,WE,Iu,qn,Vu,Qn,GE,Xu,jn,Ku,Z,IE,ni,VE,XE,oi,KE,HE,ri,JE,YE,li,ZE,QE,Hu;return qe=new ai({}),re=new Gq({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"}]}}),Ks=new Wq({props:{id:"MR8tZm5ViWU"}}),tt=new ai({}),ot=new h({props:{code:`from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, name=<span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        <span class="hljs-keyword">yield</span> dataset[i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;text&quot;</span>]`}}),rt=new h({props:{code:`with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\\n")`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset)):
        f.write(dataset[i][<span class="hljs-string">&quot;text&quot;</span>] + <span class="hljs-string">&quot;\\n&quot;</span>)`}}),lt=new ai({}),it=new h({props:{code:`from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),pt=new h({props:{code:"tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)",highlighted:'tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="hljs-literal">True</span>)'}}),ut=new h({props:{code:`tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`,highlighted:`tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`}}),ct=new h({props:{code:'print(tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),mt=new h({props:{code:"hello how are u?",highlighted:"hello how are u?"}}),cs=new Rq({props:{$$slots:{default:[Iq]},$$scope:{ctx:ii}}}),dt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"}}),ft=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"}}),kt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)'}}),vt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),_t=new h({props:{code:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),ht=new h({props:{code:`[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]`,highlighted:'[(<span class="hljs-string">&quot;Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">28</span>))]'}}),Et=new h({props:{code:`pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),zt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),$t=new h({props:{code:`special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
trainer = trainers.WordPieceTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens)`}}),qt=new h({props:{code:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)",highlighted:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"}}),jt=new h({props:{code:`tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>)
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),xt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),bt=new h({props:{code:`['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']`,highlighted:'[<span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),gt=new h({props:{code:`cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Pt=new h({props:{code:"(2, 3)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)'}}),wt=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,
    pair=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="hljs-string">&quot;[SEP]&quot;</span>, sep_token_id)],
)`}}),Tt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ct=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']`,highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),Dt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),Lt=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;pair&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;sentences&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Ot=new h({props:{code:'tokenizer.decoder = decoders.WordPiece(prefix="##")',highlighted:'tokenizer.decoder = decoders.WordPiece(prefix=<span class="hljs-string">&quot;##&quot;</span>)'}}),Mt=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),yt=new h({props:{code:`"let's test this tokenizer... on a pair of sentences." # Testons ce tokenizer... sur une paire de phrases.`,highlighted:'<span class="hljs-string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer... sur une paire de phrases.</span>'}}),St=new h({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Nt=new h({props:{code:'new_tokenizer = Tokenizer.from_file("tokenizer.json")',highlighted:'new_tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Bt=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # Vous pouvez charger \xE0 partir du fichier du tokenizer, alternativement
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    <span class="hljs-comment"># tokenizer_file=&quot;tokenizer.json&quot;, # Vous pouvez charger \xE0 partir du fichier du tokenizer, alternativement</span>
    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,
    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,
    cls_token=<span class="hljs-string">&quot;[CLS]&quot;</span>,
    sep_token=<span class="hljs-string">&quot;[SEP]&quot;</span>,
    mask_token=<span class="hljs-string">&quot;[MASK]&quot;</span>,
)`}}),At=new h({props:{code:`from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`}}),Ft=new ai({}),Rt=new h({props:{code:"tokenizer = Tokenizer(models.BPE())",highlighted:"tokenizer = Tokenizer(models.BPE())"}}),Wt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)",highlighted:'tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>)'}}),Gt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)'}}),It=new h({props:{code:`[('Let', (0, 3)), ("'s", (3, 5)), ('\u0120test', (5, 10)), ('\u0120pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;s&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u0120test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120pre&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
 (<span class="hljs-string">&#x27;tokenization&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;!&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Vt=new h({props:{code:`trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`trainer = trainers.BpeTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),Xt=new h({props:{code:`tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.BPE()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Kt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ht=new h({props:{code:`['L', 'et', "'", 's', '\u0120test', '\u0120this', '\u0120to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-string">&#x27;et&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u0120test&#x27;</span>, <span class="hljs-string">&#x27;\u0120this&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Jt=new h({props:{code:"tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)",highlighted:'tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="hljs-literal">False</span>)'}}),Yt=new h({props:{code:`sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]`,highlighted:`sentence = <span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[<span class="hljs-number">4</span>]
sentence[start:end]`}}),Zt=new h({props:{code:"' test'",highlighted:'<span class="hljs-string">&#x27; test&#x27;</span>'}}),Qt=new h({props:{code:"tokenizer.decoder = decoders.ByteLevel()",highlighted:"tokenizer.decoder = decoders.ByteLevel()"}}),en=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),sn=new h({props:{code:`"Let's test this tokenizer." # Testons ce tokenizer`,highlighted:'<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer</span>'}}),tn=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
)`}}),nn=new h({props:{code:`from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`}}),on=new ai({}),ln=new h({props:{code:"tokenizer = Tokenizer(models.Unigram())",highlighted:"tokenizer = Tokenizer(models.Unigram())"}}),an=new h({props:{code:`from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("\`\`", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Regex

tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [
        normalizers.Replace(<span class="hljs-string">&quot;\`\`&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.Replace(<span class="hljs-string">&quot;&#x27;&#x27;&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(<span class="hljs-string">&quot; {2,}&quot;</span>), <span class="hljs-string">&quot; &quot;</span>),
    ]
)`}}),pn=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"}}),un=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)'}}),cn=new h({props:{code:`[("\u2581Let's", (0, 5)), ('\u2581test', (5, 10)), ('\u2581the', (10, 14)), ('\u2581pre-tokenizer!', (14, 29))]`,highlighted:'[(<span class="hljs-string">&quot;\u2581Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u2581test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581the&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581pre-tokenizer!&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">29</span>))]'}}),mn=new h({props:{code:`special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>]
trainer = trainers.UnigramTrainer(
    vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens, unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),dn=new h({props:{code:`tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.Unigram()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),fn=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),kn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),vn=new h({props:{code:`cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),_n=new h({props:{code:"0 1",highlighted:'<span class="hljs-number">0</span> <span class="hljs-number">1</span>'}}),hn=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,
    pair=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],
)`}}),En=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences!&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),zn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.', '.', '.', '<sep>', '\u2581', 'on', '\u2581', 'a', '\u2581pair', 
  '\u2581of', '\u2581sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]`,highlighted:`[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;\u2581pair&#x27;</span>, 
  <span class="hljs-string">&#x27;\u2581of&#x27;</span>, <span class="hljs-string">&#x27;\u2581sentence&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;cls&gt;&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]`}}),$n=new h({props:{code:"tokenizer.decoder = decoders.Metaspace()",highlighted:"tokenizer.decoder = decoders.Metaspace()"}}),qn=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>,
    unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>,
    pad_token=<span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>,
    cls_token=<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>,
    sep_token=<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>,
    mask_token=<span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>,
    padding_side=<span class="hljs-string">&quot;left&quot;</span>,
)`}}),jn=new h({props:{code:`from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`}}),{c(){z=r("meta"),ge=u(),oe=r("h1"),ie=r("a"),$e=r("span"),m(qe.$$.fragment),As=u(),je=r("span"),Fs=n("Construction d'un *tokenizer*, bloc par bloc"),ss=u(),m(re.$$.fragment),ts=u(),Pe=r("p"),Xe=n("Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),ns=u(),R=r("ul"),xe=r("li"),Us=n("normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),Rs=u(),be=r("li"),Ws=n("pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),Gs=u(),le=r("li"),Is=n("passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),Ke=r("em"),He=n("tokens"),x=n("),"),Cn=u(),J=r("li"),Dn=n("post-traitement (ajout des "),os=r("em"),Ln=n("tokens"),On=n(" sp\xE9ciaux du "),rs=r("em"),Mn=n("tokenizer"),qc=n(", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),mo=r("em"),jc=n("token"),xc=n(")."),pi=u(),yn=r("p"),bc=n("Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),ui=u(),Je=r("div"),Vs=r("img"),gc=u(),Xs=r("img"),ci=u(),W=r("p"),Pc=n("La biblioth\xE8que \u{1F917} "),fo=r("em"),wc=n("Tokenizers"),Tc=n(" a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),ko=r("em"),Cc=n("tokenizer"),Dc=n(" \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),vo=r("em"),Lc=n("tokenizer"),Oc=n(" \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=r("a"),Mc=n("section 2"),yc=n(". Vous serez alors en mesure de construire n\u2019importe quel type de "),_o=r("em"),Sc=n("tokenizer"),Nc=n(" auquel vous pouvez penser !"),mi=u(),m(Ks.$$.fragment),di=u(),ls=r("p"),Bc=n("Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),ho=r("code"),Ac=n("Tokenizer"),Fc=n(" avec les blocs de construction regroup\xE9s en sous-modules :"),fi=u(),G=r("ul"),we=r("li"),Eo=r("code"),Uc=n("normalizers"),Rc=n(" contient tous les types de "),zo=r("code"),Wc=n("Normalizer"),Gc=n(" que vous pouvez utiliser (liste compl\xE8te "),Hs=r("a"),Ic=n("ici"),Vc=n("),"),Xc=u(),Te=r("li"),$o=r("code"),Kc=n("pre_tokenizers"),Hc=n(" contient tous les types de "),qo=r("code"),Jc=n("PreTokenizer"),Yc=n(" que vous pouvez utiliser (liste compl\xE8te "),Js=r("a"),Zc=n("ici"),Qc=n("),"),em=u(),I=r("li"),jo=r("code"),sm=n("models"),tm=n(" contient les diff\xE9rents types de "),xo=r("code"),nm=n("Model"),om=n(" que vous pouvez utiliser, comme "),bo=r("code"),rm=n("BPE"),lm=n(", "),go=r("code"),am=n("WordPiece"),im=n(", et "),Po=r("code"),pm=n("Unigram"),um=n(" (liste compl\xE8te "),Ys=r("a"),cm=n("ici"),mm=n("),"),dm=u(),Ce=r("li"),wo=r("code"),fm=n("trainers"),km=n(" contient tous les diff\xE9rents types de "),To=r("code"),vm=n("Trainer"),_m=n(" que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=r("a"),hm=n("ici"),Em=n("),"),zm=u(),De=r("li"),Co=r("code"),$m=n("post_processors"),qm=n(" contient les diff\xE9rents types de "),Do=r("code"),jm=n("PostProcessor"),xm=n(" que vous pouvez utiliser (liste compl\xE8te "),Qs=r("a"),bm=n("ici"),gm=n("),"),Pm=u(),Le=r("li"),Lo=r("code"),wm=n("decoders"),Tm=n(" contient les diff\xE9rents types de "),Oo=r("code"),Cm=n("Decoder"),Dm=n(" que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=r("a"),Lm=n("ici"),Om=n(")."),ki=u(),as=r("p"),Mm=n("Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=r("a"),ym=n("ici"),Sm=n("."),vi=u(),Ye=r("h2"),is=r("a"),Mo=r("span"),m(tt.$$.fragment),Nm=u(),yo=r("span"),Bm=n("Acquisition d'un corpus"),_i=u(),pe=r("p"),Am=n("Pour entra\xEEner notre nouveau "),So=r("em"),Fm=n("tokenizer"),Um=n(", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=r("a"),Rm=n("d\xE9but du chapitre"),Wm=n(", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=r("a"),Gm=n("WikiText-2"),Im=n(" :"),hi=u(),m(ot.$$.fragment),Ei=u(),Oe=r("p"),Vm=n("La fonction "),No=r("code"),Xm=n("get_training_corpus()"),Km=n(" est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Bo=r("em"),Hm=n("tokenizer"),Jm=n("."),zi=u(),ps=r("p"),Ym=n("\u{1F917} "),Ao=r("em"),Zm=n("Tokenizers"),Qm=n(" peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),$i=u(),m(rt.$$.fragment),qi=u(),ue=r("p"),ed=n("Ensuite, nous vous montrerons comment construire vos propres "),Fo=r("em"),sd=n("tokenizers"),td=n(" pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Uo=r("em"),nd=n("WordPiece"),od=n(", BPE et "),Ro=r("em"),rd=n("Unigram"),ld=n(". Commen\xE7ons par BERT !"),ji=u(),Ze=r("h2"),us=r("a"),Wo=r("span"),m(lt.$$.fragment),ad=u(),at=r("span"),id=n("Construire un "),Go=r("i"),pd=n("tokenizer WordPiece"),ud=n(" \xE0 partir de z\xE9ro"),xi=u(),b=r("p"),cd=n("Pour construire un "),Io=r("em"),md=n("tokenizer"),dd=n(" avec la biblioth\xE8que \u{1F917} "),Vo=r("em"),fd=n("Tokenizers"),kd=n(", nous commen\xE7ons par instancier un objet "),Xo=r("code"),vd=n("Tokenizer"),_d=n(" avec un "),Ko=r("code"),hd=n("model"),Ed=n(". Puis nous d\xE9finissons ses attributs "),Ho=r("code"),zd=n("normalizer"),$d=n(", "),Jo=r("code"),qd=n("pre_tokenizer"),jd=n(", "),Yo=r("code"),xd=n("post_processor"),bd=n(" et "),Zo=r("code"),gd=n("decoder"),Pd=n(" aux valeurs que nous voulons."),bi=u(),Me=r("p"),wd=n("Pour cet exemple, nous allons cr\xE9er un "),Qo=r("code"),Td=n("Tokenizer"),Cd=n(" avec un mod\xE8le "),er=r("em"),Dd=n("WordPiece"),Ld=n(" :"),gi=u(),m(it.$$.fragment),Pi=u(),ce=r("p"),Od=n("Nous devons sp\xE9cifier le "),sr=r("code"),Md=n("unk_token"),yd=n(" pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),tr=r("code"),Sd=n("vocab"),Nd=n(" de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),nr=r("code"),Bd=n("max_input_chars_per_word"),Ad=n(", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),wi=u(),g=r("p"),Fd=n("La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),or=r("code"),Ud=n("BertNormalizer"),Rd=n(" a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),rr=r("code"),Wd=n("lowercase"),Gd=n(" pour mettre le texte en minuscule, "),lr=r("code"),Id=n("strip_accents"),Vd=n(" qui enl\xE8ve les accents, "),ar=r("code"),Xd=n("clean_text"),Kd=n(" pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),ir=r("code"),Hd=n("handle_chinese_chars"),Jd=n(" qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),pr=r("em"),Yd=n("tokenizer"),Zd=u(),ur=r("code"),Qd=n("bert-base-uncased"),ef=n(", nous pouvons simplement d\xE9finir ce "),cr=r("em"),sf=n("normalizer"),tf=n(" :"),Ti=u(),m(pt.$$.fragment),Ci=u(),V=r("p"),nf=n("Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),mr=r("em"),of=n("tokenizer"),rf=n(", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),dr=r("em"),lf=n("Tokenizers"),af=n(". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),fr=r("code"),pf=n("Lowercase"),uf=n(" et un normaliseur "),kr=r("code"),cf=n("StripAccents"),mf=n(". Il est possible de composer plusieurs normaliseurs en utilisant une "),vr=r("code"),df=n("Sequence"),ff=n(" :"),Di=u(),m(ut.$$.fragment),Li=u(),ye=r("p"),kf=n("Nous utilisons \xE9galement un normaliseur Unicode "),_r=r("code"),vf=n("NFD"),_f=n(", car sinon "),hr=r("code"),hf=n("StripAccents"),Ef=n(" ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),Oi=u(),Se=r("p"),zf=n("Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),Er=r("code"),$f=n("normalize_str()"),qf=n(" du "),zr=r("code"),jf=n("normalizer"),xf=n(" pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),Mi=u(),m(ct.$$.fragment),yi=u(),m(mt.$$.fragment),Si=u(),m(cs.$$.fragment),Ni=u(),ms=r("p"),bf=n("L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),$r=r("code"),gf=n("BertPreTokenizer"),Pf=n(" pr\xE9construit que nous pouvons utiliser :"),Bi=u(),m(dt.$$.fragment),Ai=u(),Bn=r("p"),wf=n("Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),Fi=u(),m(ft.$$.fragment),Ui=u(),ds=r("p"),Tf=n("Notez que le "),qr=r("code"),Cf=n("Whitespace"),Df=n(" divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),Ri=u(),m(kt.$$.fragment),Wi=u(),m(vt.$$.fragment),Gi=u(),fs=r("p"),Lf=n("Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),jr=r("code"),Of=n("WhitespaceSplit"),Mf=n(" \xE0 la place :"),Ii=u(),m(_t.$$.fragment),Vi=u(),m(ht.$$.fragment),Xi=u(),ks=r("p"),yf=n("Comme pour les normaliseurs, vous pouvez utiliser une "),xr=r("code"),Sf=n("Sequence"),Nf=n(" pour composer plusieurs pr\xE9tokenizers :"),Ki=u(),m(Et.$$.fragment),Hi=u(),m(zt.$$.fragment),Ji=u(),me=r("p"),Bf=n("L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),br=r("code"),Af=n("WordPieceTrainer"),Ff=n(". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),gr=r("em"),Uf=n("Tokenizers"),Rf=n(" est que vous devez lui passer tous les "),Pr=r("em"),Wf=n("tokens"),Gf=n(" sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Yi=u(),m($t.$$.fragment),Zi=u(),O=r("p"),If=n("En plus de sp\xE9cifier la "),wr=r("code"),Vf=n("vocab_size"),Xf=n(" et les "),Tr=r("code"),Kf=n("special_tokens"),Hf=n(", nous pouvons d\xE9finir la "),Cr=r("code"),Jf=n("min_frequency"),Yf=n(" (le nombre de fois qu\u2019un "),Dr=r("em"),Zf=n("token"),Qf=n(" doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Lr=r("code"),ek=n("continuing_subword_prefix"),sk=n(" (si nous voulons utiliser quelque chose de diff\xE9rent de "),Or=r("code"),tk=n("##"),nk=n(")."),Qi=u(),An=r("p"),ok=n("Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),ep=u(),m(qt.$$.fragment),sp=u(),Ne=r("p"),rk=n("Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),Mr=r("em"),lk=n("tokenizer"),ak=n(" qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),yr=r("code"),ik=n("WordPiece"),pk=n(" vide au pr\xE9alable) :"),tp=u(),m(jt.$$.fragment),np=u(),Be=r("p"),uk=n("Dans les deux cas, nous pouvons ensuite tester le "),Sr=r("em"),ck=n("tokenizer"),mk=n(" sur un texte en appelant la m\xE9thode "),Nr=r("code"),dk=n("encode()"),fk=n(" :"),op=u(),m(xt.$$.fragment),rp=u(),m(bt.$$.fragment),lp=u(),j=r("p"),kk=n("L\u2019encodage obtenu est un "),Br=r("code"),vk=n("Encoding"),_k=n(" contenant toutes les sorties n\xE9cessaires du "),Ar=r("em"),hk=n("tokenizer"),Ek=n(" dans ses diff\xE9rents attributs : "),Fr=r("code"),zk=n("ids"),$k=n(", "),Ur=r("code"),qk=n("type_ids"),jk=n(", "),Rr=r("code"),xk=n("tokens"),bk=n(", "),Wr=r("code"),gk=n("offsets"),Pk=n(", "),Gr=r("code"),wk=n("attention_mask"),Tk=n(", "),Ir=r("code"),Ck=n("special_tokens_mask"),Dk=n(" et "),Vr=r("code"),Lk=n("overflowing"),Ok=n("."),ap=u(),P=r("p"),Mk=n("La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Xr=r("em"),yk=n("token"),Sk=u(),Kr=r("code"),Nk=n("[CLS]"),Bk=n(" au d\xE9but et le "),Hr=r("em"),Ak=n("token"),Fk=u(),Jr=r("code"),Uk=n("[SEP]"),Rk=n(" \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Yr=r("code"),Wk=n("TemplateProcessor"),Gk=n(" pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Zr=r("em"),Ik=n("tokens"),Vk=u(),Qr=r("code"),Xk=n("[CLS]"),Kk=n(" et "),el=r("code"),Hk=n("[SEP]"),Jk=n(" dans le vocabulaire :"),ip=u(),m(gt.$$.fragment),pp=u(),m(Pt.$$.fragment),up=u(),M=r("p"),Yk=n("Pour \xE9crire le gabarit pour "),sl=r("code"),Zk=n("TemplateProcessor"),Qk=n(", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),tl=r("em"),ev=n("tokens"),sv=n(" sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),nl=r("code"),tv=n("$A"),nv=n(", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),ol=r("code"),ov=n("$B"),rv=n(". Pour chacun de ces \xE9l\xE9ments ("),rl=r("em"),lv=n("tokens"),av=n(" sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),ll=r("em"),iv=n("token"),pv=n(" correspondant apr\xE8s un deux-points."),cp=u(),Fn=r("p"),uv=n("Le gabarit classique de BERT est donc d\xE9fini comme suit :"),mp=u(),m(wt.$$.fragment),dp=u(),Ae=r("p"),cv=n("Notez que nous devons transmettre les identifiants des "),al=r("em"),mv=n("tokens"),dv=n(" sp\xE9ciaux afin que le "),il=r("em"),fv=n("tokenizer"),kv=n(" puisse les convertir correctement."),fp=u(),Un=r("p"),vv=n("Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),kp=u(),m(Tt.$$.fragment),vp=u(),m(Ct.$$.fragment),_p=u(),Rn=r("p"),_v=n("Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),hp=u(),m(Dt.$$.fragment),Ep=u(),m(Lt.$$.fragment),zp=u(),vs=r("p"),hv=n("Nous avons presque fini de construire ce "),pl=r("em"),Ev=n("tokenizer"),zv=n(" \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),$p=u(),m(Ot.$$.fragment),qp=u(),_s=r("p"),$v=n("Testons-le sur notre pr\xE9c\xE9dent "),ul=r("code"),qv=n("encoding"),jv=n(" :"),jp=u(),m(Mt.$$.fragment),xp=u(),m(yt.$$.fragment),bp=u(),hs=r("p"),xv=n("G\xE9nial ! Nous pouvons enregistrer notre "),cl=r("em"),bv=n("tokenizer"),gv=n(" dans un seul fichier JSON comme ceci :"),gp=u(),m(St.$$.fragment),Pp=u(),Fe=r("p"),Pv=n("Nous pouvons alors recharger ce fichier dans un objet "),ml=r("code"),wv=n("Tokenizer"),Tv=n(" avec la m\xE9thode "),dl=r("code"),Cv=n("from_file()"),Dv=n(" :"),wp=u(),m(Nt.$$.fragment),Tp=u(),y=r("p"),Lv=n("Pour utiliser ce "),fl=r("em"),Ov=n("tokenizer"),Mv=n(" dans \u{1F917} "),kl=r("em"),yv=n("Transformers"),Sv=n(", nous devons l\u2019envelopper dans un "),vl=r("code"),Nv=n("PreTrainedTokenizerFast"),Bv=n(". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),_l=r("em"),Av=n("tokenizer"),Fv=n(" correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),hl=r("code"),Uv=n("BertTokenizerFast"),Rv=n("). Si vous appliquez cette logique pour construire un tout nouveau "),El=r("em"),Wv=n("tokenizer"),Gv=n(", vous devrez utiliser la premi\xE8re option."),Cp=u(),$=r("p"),Iv=n("Pour envelopper le "),zl=r("em"),Vv=n("tokenizer"),Xv=n(" dans un "),$l=r("code"),Kv=n("PreTrainedTokenizerFast"),Hv=n(", nous pouvons soit passer le "),ql=r("em"),Jv=n("tokenizer"),Yv=n(" que nous avons construit comme un "),jl=r("code"),Zv=n("tokenizer_object"),Qv=n(", soit passer le fichier de "),xl=r("em"),e_=n("tokenizer"),s_=n(" que nous avons sauvegard\xE9 comme "),bl=r("code"),t_=n("tokenizer_file"),n_=n(". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),gl=r("em"),o_=n("tokens"),r_=n(" sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),Pl=r("code"),l_=n("tokenizer"),a_=n(" quel "),wl=r("em"),i_=n("token"),p_=n(" est le "),Tl=r("em"),u_=n("token"),c_=n(" de masque, quel est le "),Cl=r("em"),m_=n("token"),Dl=r("code"),d_=n("[CLS]"),f_=n(", etc :"),Dp=u(),m(Bt.$$.fragment),Lp=u(),de=r("p"),k_=n("Si vous utilisez une classe de "),Ll=r("em"),v_=n("tokenizer"),__=n(" sp\xE9cifique (comme "),Ol=r("code"),h_=n("BertTokenizerFast"),E_=n("), vous aurez seulement besoin de sp\xE9cifier les "),Ml=r("em"),z_=n("tokens"),$_=n(" sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Op=u(),m(At.$$.fragment),Mp=u(),S=r("p"),q_=n("Vous pouvez ensuite utiliser ce "),yl=r("em"),j_=n("tokenizer"),x_=n(" comme n\u2019importe quel autre "),Sl=r("em"),b_=n("tokenizer"),g_=n(" de \u{1F917} "),Nl=r("em"),P_=n("Transformers"),w_=n(". Vous pouvez le sauvegarder avec la m\xE9thode "),Bl=r("code"),T_=n("save_pretrained()"),C_=n(" ou le t\xE9l\xE9charger sur le "),Al=r("em"),D_=n("Hub"),L_=n(" avec la m\xE9thode "),Fl=r("code"),O_=n("push_to_hub()"),M_=n("."),yp=u(),Ue=r("p"),y_=n("Maintenant que nous avons vu comment construire un "),Ul=r("em"),S_=n("tokenizer WordPiece"),N_=n(", faisons de m\xEAme pour un "),Rl=r("em"),B_=n("tokenizer"),A_=n(" BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),Sp=u(),Qe=r("h2"),Es=r("a"),Wl=r("span"),m(Ft.$$.fragment),F_=u(),Ut=r("span"),U_=n("Construire un "),Gl=r("i"),R_=n("tokenizer"),W_=n(" BPE \xE0 partir de z\xE9ro"),Np=u(),fe=r("p"),G_=n("Construisons maintenant un "),Il=r("em"),I_=n("tokenizer"),V_=n(" BPE. Comme pour le "),Vl=r("em"),X_=n("tokenizer"),K_=n(" BERT, nous commen\xE7ons par initialiser un "),Xl=r("code"),H_=n("Tokenizer"),J_=n(" avec un mod\xE8le BPE :"),Bp=u(),m(Rt.$$.fragment),Ap=u(),ke=r("p"),Y_=n("Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Kl=r("code"),Z_=n("vocab"),Q_=n(" et le "),Hl=r("code"),e2=n("merges"),s2=n(" dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Jl=r("code"),t2=n("unk_token"),n2=n(" parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Fp=u(),Wn=r("p"),o2=n("GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),Up=u(),m(Wt.$$.fragment),Rp=u(),zs=r("p"),r2=n("L\u2019option que nous avons ajout\xE9e \xE0 "),Yl=r("code"),l2=n("ByteLevel"),a2=n(" ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),Wp=u(),m(Gt.$$.fragment),Gp=u(),m(It.$$.fragment),Ip=u(),Re=r("p"),i2=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Zl=r("em"),p2=n("token"),u2=n(" sp\xE9cial est le "),Ql=r("em"),c2=n("token"),m2=n(" de fin de texte :"),Vp=u(),m(Vt.$$.fragment),Xp=u(),N=r("p"),d2=n("Comme avec le "),ea=r("code"),f2=n("WordPieceTrainer"),k2=n(", ainsi que le "),sa=r("code"),v2=n("vocab_size"),_2=n(" et le "),ta=r("code"),h2=n("special_tokens"),E2=n(", nous pouvons sp\xE9cifier la "),na=r("code"),z2=n("min_frequency"),$2=n(" si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),oa=r("code"),q2=n("</w>"),j2=n("), nous pouvons le d\xE9finir avec "),ra=r("code"),x2=n("end_of_word_suffix"),b2=n("."),Kp=u(),$s=r("p"),g2=n("Ce "),la=r("em"),P2=n("tokenizer"),w2=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Hp=u(),m(Xt.$$.fragment),Jp=u(),Gn=r("p"),T2=n("Regardons la tokenisation d\u2019un exemple de texte :"),Yp=u(),m(Kt.$$.fragment),Zp=u(),m(Ht.$$.fragment),Qp=u(),qs=r("p"),C2=n("Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),aa=r("em"),D2=n("tokenizer"),L2=n(" du GPT-2 comme suit :"),eu=u(),m(Jt.$$.fragment),su=u(),T=r("p"),O2=n("L\u2019option "),ia=r("code"),M2=n("trim_offsets = False"),y2=n(" indique au post-processeur que nous devons laisser les "),pa=r("em"),S2=n("offsets"),N2=n(" des "),ua=r("em"),B2=n("tokens"),A2=n(" qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ca=r("em"),F2=n("offsets"),U2=n(" pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),ma=r("em"),R2=n("token"),W2=n("). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),da=r("code"),G2=n("'\u0120test'"),I2=n(" est le "),fa=r("em"),V2=n("token"),X2=n(" \xE0 l\u2019index 4 :"),tu=u(),m(Yt.$$.fragment),nu=u(),m(Zt.$$.fragment),ou=u(),In=r("p"),K2=n("Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),ru=u(),m(Qt.$$.fragment),lu=u(),Vn=r("p"),H2=n("et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),au=u(),m(en.$$.fragment),iu=u(),m(sn.$$.fragment),pu=u(),ve=r("p"),J2=n("Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),ka=r("code"),Y2=n("PreTrainedTokenizerFast"),Z2=n(" ou un "),va=r("code"),Q2=n("GPT2TokenizerFast"),eh=n(" si nous voulons l\u2019utiliser dans \u{1F917} "),_a=r("em"),sh=n("Transformers"),th=n(" :"),uu=u(),m(tn.$$.fragment),cu=u(),Xn=r("p"),nh=n("ou :"),mu=u(),m(nn.$$.fragment),du=u(),We=r("p"),oh=n("Comme dernier exemple, nous allons vous montrer comment construire un "),ha=r("em"),rh=n("tokenizer"),lh=u(),Ea=r("em"),ah=n("Unigram"),ih=n(" \xE0 partir de z\xE9ro."),fu=u(),es=r("h2"),js=r("a"),za=r("span"),m(on.$$.fragment),ph=u(),rn=r("span"),uh=n("Construire un "),$a=r("i"),ch=n("tokenizer Unigram"),mh=n(" \xE0 partir de z\xE9ro"),ku=u(),Y=r("p"),dh=n("Construisons maintenant un "),qa=r("em"),fh=n("tokenizer"),kh=n(" XLNet. Comme pour les "),ja=r("em"),vh=n("tokenizers"),_h=n(" pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),xa=r("code"),hh=n("Tokenizer"),Eh=n(" avec un mod\xE8le "),ba=r("em"),zh=n("Unigram"),$h=n(" :"),vu=u(),m(ln.$$.fragment),_u=u(),Kn=r("p"),qh=n("Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),hu=u(),xs=r("p"),jh=n("Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),ga=r("em"),xh=n("SentencePiece"),bh=n(") :"),Eu=u(),m(an.$$.fragment),zu=u(),_e=r("p"),gh=n("Il remplace "),Pa=r("code"),Ph=n("\u201C"),wh=n(" et "),wa=r("code"),Th=n("\u201D"),Ch=n(" par "),Ta=r("code"),Dh=n("\u201D"),Lh=n(" et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),$u=u(),Ge=r("p"),Oh=n("Le pr\xE9tokenizer \xE0 utiliser pour tout "),Ca=r("em"),Mh=n("tokenizer SentencePiece"),yh=n(" est "),Da=r("code"),Sh=n("Metaspace"),Nh=n(" :"),qu=u(),m(pn.$$.fragment),ju=u(),Hn=r("p"),Bh=n("Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),xu=u(),m(un.$$.fragment),bu=u(),m(cn.$$.fragment),gu=u(),bs=r("p"),Ah=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),La=r("em"),Fh=n("tokens"),Uh=n(" sp\xE9ciaux :"),Pu=u(),m(mn.$$.fragment),wu=u(),C=r("p"),Rh=n("Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Oa=r("code"),Wh=n("UnigramTrainer"),Gh=n(" est le "),Ma=r("code"),Ih=n("unk_token"),Vh=n(". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),ya=r("em"),Xh=n("Unigram"),Kh=n(", comme le "),Sa=r("code"),Hh=n("shrinking_factor"),Jh=n(" pour chaque \xE9tape o\xF9 nous enlevons des "),Na=r("em"),Yh=n("tokens"),Zh=n(" (par d\xE9faut 0.75) ou le "),Ba=r("code"),Qh=n("max_piece_length"),eE=n(" pour sp\xE9cifier la longueur maximale d\u2019un "),Aa=r("em"),sE=n("token"),tE=n(" donn\xE9 (par d\xE9faut 16)."),Tu=u(),gs=r("p"),nE=n("Ce "),Fa=r("em"),oE=n("tokenizer"),rE=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Cu=u(),m(dn.$$.fragment),Du=u(),Jn=r("p"),lE=n("Regardons la tokenisation de notre exemple :"),Lu=u(),m(fn.$$.fragment),Ou=u(),m(kn.$$.fragment),Mu=u(),w=r("p"),aE=n("Une particularit\xE9 de XLNet est qu\u2019il place le "),Ua=r("em"),iE=n("token"),pE=u(),Ra=r("code"),uE=n("<cls>"),cE=n(" \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Wa=r("em"),mE=n("tokens"),dE=n("). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ga=r("em"),fE=n("tokens"),kE=n(" sp\xE9ciaux et les types d\u2019identifiant de "),Ia=r("em"),vE=n("token"),_E=n(" avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Va=r("em"),hE=n("tokens"),EE=u(),Xa=r("code"),zE=n("<cls>"),$E=n(" et "),Ka=r("code"),qE=n("<sep>"),jE=n(" :"),yu=u(),m(vn.$$.fragment),Su=u(),m(_n.$$.fragment),Nu=u(),Yn=r("p"),xE=n("Le mod\xE8le ressemble \xE0 ceci :"),Bu=u(),m(hn.$$.fragment),Au=u(),Zn=r("p"),bE=n("Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),Fu=u(),m(En.$$.fragment),Uu=u(),m(zn.$$.fragment),Ru=u(),Ps=r("p"),gE=n("Enfin, nous ajoutons un d\xE9codeur "),Ha=r("code"),PE=n("Metaspace"),wE=n(" :"),Wu=u(),m($n.$$.fragment),Gu=u(),D=r("p"),TE=n("et on en a fini avec ce "),Ja=r("em"),CE=n("tokenizer"),DE=n(" ! On peut le sauvegarder et l\u2019envelopper dans un "),Ya=r("code"),LE=n("PreTrainedTokenizerFast"),OE=n(" ou "),Za=r("code"),ME=n("XLNetTokenizerFast"),yE=n(" si on veut l\u2019utiliser dans \u{1F917} "),Qa=r("em"),SE=n("Transformers"),NE=n(". Une chose \xE0 noter lors de l\u2019utilisation de "),ei=r("code"),BE=n("PreTrainedTokenizerFast"),AE=n(" est qu\u2019en plus des "),si=r("em"),FE=n("tokens"),UE=n(" sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ti=r("em"),RE=n("Transformers"),WE=n(" de rembourrer \xE0 gauche :"),Iu=u(),m(qn.$$.fragment),Vu=u(),Qn=r("p"),GE=n("Ou alternativement :"),Xu=u(),m(jn.$$.fragment),Ku=u(),Z=r("p"),IE=n("Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),ni=r("em"),VE=n("tokenizers"),XE=n(" existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),oi=r("em"),KE=n("tokenizer"),HE=n(" que vous voulez avec la biblioth\xE8que \u{1F917} "),ri=r("em"),JE=n("Tokenizers"),YE=n(" et pouvoir l\u2019utiliser dans \u{1F917} "),li=r("em"),ZE=n("Transformers"),QE=n("."),this.h()},l(e){const i=Fq('[data-svelte="svelte-1phssyn"]',document.head);z=l(i,"META",{name:!0,content:!0}),i.forEach(t),ge=c(e),oe=l(e,"H1",{class:!0});var xn=a(oe);ie=l(xn,"A",{id:!0,class:!0,href:!0});var tz=a(ie);$e=l(tz,"SPAN",{});var nz=a($e);d(qe.$$.fragment,nz),nz.forEach(t),tz.forEach(t),As=c(xn),je=l(xn,"SPAN",{});var oz=a(je);Fs=o(oz,"Construction d'un *tokenizer*, bloc par bloc"),oz.forEach(t),xn.forEach(t),ss=c(e),d(re.$$.fragment,e),ts=c(e),Pe=l(e,"P",{});var rz=a(Pe);Xe=o(rz,"Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),rz.forEach(t),ns=c(e),R=l(e,"UL",{});var ws=a(R);xe=l(ws,"LI",{});var lz=a(xe);Us=o(lz,"normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),"),lz.forEach(t),Rs=c(ws),be=l(ws,"LI",{});var az=a(be);Ws=o(az,"pr\xE9tok\xE9nisation (division de l\u2019entr\xE9e en mots),"),az.forEach(t),Gs=c(ws),le=l(ws,"LI",{});var Ju=a(le);Is=o(Ju,"passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),Ke=l(Ju,"EM",{});var iz=a(Ke);He=o(iz,"tokens"),iz.forEach(t),x=o(Ju,"),"),Ju.forEach(t),Cn=c(ws),J=l(ws,"LI",{});var Ts=a(J);Dn=o(Ts,"post-traitement (ajout des "),os=l(Ts,"EM",{});var pz=a(os);Ln=o(pz,"tokens"),pz.forEach(t),On=o(Ts," sp\xE9ciaux du "),rs=l(Ts,"EM",{});var uz=a(rs);Mn=o(uz,"tokenizer"),uz.forEach(t),qc=o(Ts,", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),mo=l(Ts,"EM",{});var cz=a(mo);jc=o(cz,"token"),cz.forEach(t),xc=o(Ts,")."),Ts.forEach(t),ws.forEach(t),pi=c(e),yn=l(e,"P",{});var mz=a(yn);bc=o(mz,"Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),mz.forEach(t),ui=c(e),Je=l(e,"DIV",{class:!0});var Yu=a(Je);Vs=l(Yu,"IMG",{class:!0,src:!0,alt:!0}),gc=c(Yu),Xs=l(Yu,"IMG",{class:!0,src:!0,alt:!0}),Yu.forEach(t),ci=c(e),W=l(e,"P",{});var he=a(W);Pc=o(he,"La biblioth\xE8que \u{1F917} "),fo=l(he,"EM",{});var dz=a(fo);wc=o(dz,"Tokenizers"),dz.forEach(t),Tc=o(he," a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes. Vous pouvez les m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),ko=l(he,"EM",{});var fz=a(ko);Cc=o(fz,"tokenizer"),fz.forEach(t),Dc=o(he," \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),vo=l(he,"EM",{});var kz=a(vo);Lc=o(kz,"tokenizer"),kz.forEach(t),Oc=o(he," \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),Sn=l(he,"A",{href:!0});var vz=a(Sn);Mc=o(vz,"section 2"),vz.forEach(t),yc=o(he,". Vous serez alors en mesure de construire n\u2019importe quel type de "),_o=l(he,"EM",{});var _z=a(_o);Sc=o(_z,"tokenizer"),_z.forEach(t),Nc=o(he," auquel vous pouvez penser !"),he.forEach(t),mi=c(e),d(Ks.$$.fragment,e),di=c(e),ls=l(e,"P",{});var Zu=a(ls);Bc=o(Zu,"Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),ho=l(Zu,"CODE",{});var hz=a(ho);Ac=o(hz,"Tokenizer"),hz.forEach(t),Fc=o(Zu," avec les blocs de construction regroup\xE9s en sous-modules :"),Zu.forEach(t),fi=c(e),G=l(e,"UL",{});var Ee=a(G);we=l(Ee,"LI",{});var bn=a(we);Eo=l(bn,"CODE",{});var Ez=a(Eo);Uc=o(Ez,"normalizers"),Ez.forEach(t),Rc=o(bn," contient tous les types de "),zo=l(bn,"CODE",{});var zz=a(zo);Wc=o(zz,"Normalizer"),zz.forEach(t),Gc=o(bn," que vous pouvez utiliser (liste compl\xE8te "),Hs=l(bn,"A",{href:!0,rel:!0});var $z=a(Hs);Ic=o($z,"ici"),$z.forEach(t),Vc=o(bn,"),"),bn.forEach(t),Xc=c(Ee),Te=l(Ee,"LI",{});var gn=a(Te);$o=l(gn,"CODE",{});var qz=a($o);Kc=o(qz,"pre_tokenizers"),qz.forEach(t),Hc=o(gn," contient tous les types de "),qo=l(gn,"CODE",{});var jz=a(qo);Jc=o(jz,"PreTokenizer"),jz.forEach(t),Yc=o(gn," que vous pouvez utiliser (liste compl\xE8te "),Js=l(gn,"A",{href:!0,rel:!0});var xz=a(Js);Zc=o(xz,"ici"),xz.forEach(t),Qc=o(gn,"),"),gn.forEach(t),em=c(Ee),I=l(Ee,"LI",{});var ae=a(I);jo=l(ae,"CODE",{});var bz=a(jo);sm=o(bz,"models"),bz.forEach(t),tm=o(ae," contient les diff\xE9rents types de "),xo=l(ae,"CODE",{});var gz=a(xo);nm=o(gz,"Model"),gz.forEach(t),om=o(ae," que vous pouvez utiliser, comme "),bo=l(ae,"CODE",{});var Pz=a(bo);rm=o(Pz,"BPE"),Pz.forEach(t),lm=o(ae,", "),go=l(ae,"CODE",{});var wz=a(go);am=o(wz,"WordPiece"),wz.forEach(t),im=o(ae,", et "),Po=l(ae,"CODE",{});var Tz=a(Po);pm=o(Tz,"Unigram"),Tz.forEach(t),um=o(ae," (liste compl\xE8te "),Ys=l(ae,"A",{href:!0,rel:!0});var Cz=a(Ys);cm=o(Cz,"ici"),Cz.forEach(t),mm=o(ae,"),"),ae.forEach(t),dm=c(Ee),Ce=l(Ee,"LI",{});var Pn=a(Ce);wo=l(Pn,"CODE",{});var Dz=a(wo);fm=o(Dz,"trainers"),Dz.forEach(t),km=o(Pn," contient tous les diff\xE9rents types de "),To=l(Pn,"CODE",{});var Lz=a(To);vm=o(Lz,"Trainer"),Lz.forEach(t),_m=o(Pn," que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Zs=l(Pn,"A",{href:!0,rel:!0});var Oz=a(Zs);hm=o(Oz,"ici"),Oz.forEach(t),Em=o(Pn,"),"),Pn.forEach(t),zm=c(Ee),De=l(Ee,"LI",{});var wn=a(De);Co=l(wn,"CODE",{});var Mz=a(Co);$m=o(Mz,"post_processors"),Mz.forEach(t),qm=o(wn," contient les diff\xE9rents types de "),Do=l(wn,"CODE",{});var yz=a(Do);jm=o(yz,"PostProcessor"),yz.forEach(t),xm=o(wn," que vous pouvez utiliser (liste compl\xE8te "),Qs=l(wn,"A",{href:!0,rel:!0});var Sz=a(Qs);bm=o(Sz,"ici"),Sz.forEach(t),gm=o(wn,"),"),wn.forEach(t),Pm=c(Ee),Le=l(Ee,"LI",{});var Tn=a(Le);Lo=l(Tn,"CODE",{});var Nz=a(Lo);wm=o(Nz,"decoders"),Nz.forEach(t),Tm=o(Tn," contient les diff\xE9rents types de "),Oo=l(Tn,"CODE",{});var Bz=a(Oo);Cm=o(Bz,"Decoder"),Bz.forEach(t),Dm=o(Tn," que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),et=l(Tn,"A",{href:!0,rel:!0});var Az=a(et);Lm=o(Az,"ici"),Az.forEach(t),Om=o(Tn,")."),Tn.forEach(t),Ee.forEach(t),ki=c(e),as=l(e,"P",{});var Qu=a(as);Mm=o(Qu,"Vous pouvez trouver la liste compl\xE8te des blocs de construction "),st=l(Qu,"A",{href:!0,rel:!0});var Fz=a(st);ym=o(Fz,"ici"),Fz.forEach(t),Sm=o(Qu,"."),Qu.forEach(t),vi=c(e),Ye=l(e,"H2",{class:!0});var ec=a(Ye);is=l(ec,"A",{id:!0,class:!0,href:!0});var Uz=a(is);Mo=l(Uz,"SPAN",{});var Rz=a(Mo);d(tt.$$.fragment,Rz),Rz.forEach(t),Uz.forEach(t),Nm=c(ec),yo=l(ec,"SPAN",{});var Wz=a(yo);Bm=o(Wz,"Acquisition d'un corpus"),Wz.forEach(t),ec.forEach(t),_i=c(e),pe=l(e,"P",{});var Cs=a(pe);Am=o(Cs,"Pour entra\xEEner notre nouveau "),So=l(Cs,"EM",{});var Gz=a(So);Fm=o(Gz,"tokenizer"),Gz.forEach(t),Um=o(Cs,", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir ce corpus sont similaires \xE0 celles que nous avons suivies au "),Nn=l(Cs,"A",{href:!0});var Iz=a(Nn);Rm=o(Iz,"d\xE9but du chapitre"),Iz.forEach(t),Wm=o(Cs,", mais cette fois nous utiliserons le jeu de donn\xE9es "),nt=l(Cs,"A",{href:!0,rel:!0});var Vz=a(nt);Gm=o(Vz,"WikiText-2"),Vz.forEach(t),Im=o(Cs," :"),Cs.forEach(t),hi=c(e),d(ot.$$.fragment,e),Ei=c(e),Oe=l(e,"P",{});var eo=a(Oe);Vm=o(eo,"La fonction "),No=l(eo,"CODE",{});var Xz=a(No);Xm=o(Xz,"get_training_corpus()"),Xz.forEach(t),Km=o(eo," est un g\xE9n\xE9rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Bo=l(eo,"EM",{});var Kz=a(Bo);Hm=o(Kz,"tokenizer"),Kz.forEach(t),Jm=o(eo,"."),eo.forEach(t),zi=c(e),ps=l(e,"P",{});var sc=a(ps);Ym=o(sc,"\u{1F917} "),Ao=l(sc,"EM",{});var Hz=a(Ao);Zm=o(Hz,"Tokenizers"),Hz.forEach(t),Qm=o(sc," peut aussi \xEAtre entra\xEEn\xE9 directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis\xE9 en local :"),sc.forEach(t),$i=c(e),d(rt.$$.fragment,e),qi=c(e),ue=l(e,"P",{});var Ds=a(ue);ed=o(Ds,"Ensuite, nous vous montrerons comment construire vos propres "),Fo=l(Ds,"EM",{});var Jz=a(Fo);sd=o(Jz,"tokenizers"),Jz.forEach(t),td=o(Ds," pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),Uo=l(Ds,"EM",{});var Yz=a(Uo);nd=o(Yz,"WordPiece"),Yz.forEach(t),od=o(Ds,", BPE et "),Ro=l(Ds,"EM",{});var Zz=a(Ro);rd=o(Zz,"Unigram"),Zz.forEach(t),ld=o(Ds,". Commen\xE7ons par BERT !"),Ds.forEach(t),ji=c(e),Ze=l(e,"H2",{class:!0});var tc=a(Ze);us=l(tc,"A",{id:!0,class:!0,href:!0});var Qz=a(us);Wo=l(Qz,"SPAN",{});var e$=a(Wo);d(lt.$$.fragment,e$),e$.forEach(t),Qz.forEach(t),ad=c(tc),at=l(tc,"SPAN",{});var nc=a(at);id=o(nc,"Construire un "),Go=l(nc,"I",{});var s$=a(Go);pd=o(s$,"tokenizer WordPiece"),s$.forEach(t),ud=o(nc," \xE0 partir de z\xE9ro"),nc.forEach(t),tc.forEach(t),xi=c(e),b=l(e,"P",{});var B=a(b);cd=o(B,"Pour construire un "),Io=l(B,"EM",{});var t$=a(Io);md=o(t$,"tokenizer"),t$.forEach(t),dd=o(B," avec la biblioth\xE8que \u{1F917} "),Vo=l(B,"EM",{});var n$=a(Vo);fd=o(n$,"Tokenizers"),n$.forEach(t),kd=o(B,", nous commen\xE7ons par instancier un objet "),Xo=l(B,"CODE",{});var o$=a(Xo);vd=o(o$,"Tokenizer"),o$.forEach(t),_d=o(B," avec un "),Ko=l(B,"CODE",{});var r$=a(Ko);hd=o(r$,"model"),r$.forEach(t),Ed=o(B,". Puis nous d\xE9finissons ses attributs "),Ho=l(B,"CODE",{});var l$=a(Ho);zd=o(l$,"normalizer"),l$.forEach(t),$d=o(B,", "),Jo=l(B,"CODE",{});var a$=a(Jo);qd=o(a$,"pre_tokenizer"),a$.forEach(t),jd=o(B,", "),Yo=l(B,"CODE",{});var i$=a(Yo);xd=o(i$,"post_processor"),i$.forEach(t),bd=o(B," et "),Zo=l(B,"CODE",{});var p$=a(Zo);gd=o(p$,"decoder"),p$.forEach(t),Pd=o(B," aux valeurs que nous voulons."),B.forEach(t),bi=c(e),Me=l(e,"P",{});var so=a(Me);wd=o(so,"Pour cet exemple, nous allons cr\xE9er un "),Qo=l(so,"CODE",{});var u$=a(Qo);Td=o(u$,"Tokenizer"),u$.forEach(t),Cd=o(so," avec un mod\xE8le "),er=l(so,"EM",{});var c$=a(er);Dd=o(c$,"WordPiece"),c$.forEach(t),Ld=o(so," :"),so.forEach(t),gi=c(e),d(it.$$.fragment,e),Pi=c(e),ce=l(e,"P",{});var Ls=a(ce);Od=o(Ls,"Nous devons sp\xE9cifier le "),sr=l(Ls,"CODE",{});var m$=a(sr);Md=o(m$,"unk_token"),m$.forEach(t),yd=o(Ls," pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),tr=l(Ls,"CODE",{});var d$=a(tr);Sd=o(d$,"vocab"),d$.forEach(t),Nd=o(Ls," de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),nr=l(Ls,"CODE",{});var f$=a(nr);Bd=o(f$,"max_input_chars_per_word"),f$.forEach(t),Ad=o(Ls,", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ls.forEach(t),wi=c(e),g=l(e,"P",{});var A=a(g);Fd=o(A,"La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation. Puisque BERT est largement utilis\xE9, une fonction "),or=l(A,"CODE",{});var k$=a(or);Ud=o(k$,"BertNormalizer"),k$.forEach(t),Rd=o(A," a \xE9t\xE9 cr\xE9\xE9e avec les options classiques que nous pouvons d\xE9finir pour BERT : "),rr=l(A,"CODE",{});var v$=a(rr);Wd=o(v$,"lowercase"),v$.forEach(t),Gd=o(A," pour mettre le texte en minuscule, "),lr=l(A,"CODE",{});var _$=a(lr);Id=o(_$,"strip_accents"),_$.forEach(t),Vd=o(A," qui enl\xE8ve les accents, "),ar=l(A,"CODE",{});var h$=a(ar);Xd=o(h$,"clean_text"),h$.forEach(t),Kd=o(A," pour enlever tous les caract\xE8res de contr\xF4le et fusionner des espaces r\xE9p\xE9t\xE9s par un seul, et "),ir=l(A,"CODE",{});var E$=a(ir);Hd=o(E$,"handle_chinese_chars"),E$.forEach(t),Jd=o(A," qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),pr=l(A,"EM",{});var z$=a(pr);Yd=o(z$,"tokenizer"),z$.forEach(t),Zd=c(A),ur=l(A,"CODE",{});var $$=a(ur);Qd=o($$,"bert-base-uncased"),$$.forEach(t),ef=o(A,", nous pouvons simplement d\xE9finir ce "),cr=l(A,"EM",{});var q$=a(cr);sf=o(q$,"normalizer"),q$.forEach(t),tf=o(A," :"),A.forEach(t),Ti=c(e),d(pt.$$.fragment,e),Ci=c(e),V=l(e,"P",{});var ze=a(V);nf=o(ze,"Cependant, g\xE9n\xE9ralement, lorsque vous construisez un nouveau "),mr=l(ze,"EM",{});var j$=a(mr);of=o(j$,"tokenizer"),j$.forEach(t),rf=o(ze,", vous n\u2019avez pas acc\xE8s \xE0 un normaliseur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),dr=l(ze,"EM",{});var x$=a(dr);lf=o(x$,"Tokenizers"),x$.forEach(t),af=o(ze,". Donc voyons comment cr\xE9er le normaliseur de BERT manuellement. La biblioth\xE8que fournit un normaliseur "),fr=l(ze,"CODE",{});var b$=a(fr);pf=o(b$,"Lowercase"),b$.forEach(t),uf=o(ze," et un normaliseur "),kr=l(ze,"CODE",{});var g$=a(kr);cf=o(g$,"StripAccents"),g$.forEach(t),mf=o(ze,". Il est possible de composer plusieurs normaliseurs en utilisant une "),vr=l(ze,"CODE",{});var P$=a(vr);df=o(P$,"Sequence"),P$.forEach(t),ff=o(ze," :"),ze.forEach(t),Di=c(e),d(ut.$$.fragment,e),Li=c(e),ye=l(e,"P",{});var to=a(ye);kf=o(to,"Nous utilisons \xE9galement un normaliseur Unicode "),_r=l(to,"CODE",{});var w$=a(_r);vf=o(w$,"NFD"),w$.forEach(t),_f=o(to,", car sinon "),hr=l(to,"CODE",{});var T$=a(hr);hf=o(T$,"StripAccents"),T$.forEach(t),Ef=o(to," ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),to.forEach(t),Oi=c(e),Se=l(e,"P",{});var no=a(Se);zf=o(no,"Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),Er=l(no,"CODE",{});var C$=a(Er);$f=o(C$,"normalize_str()"),C$.forEach(t),qf=o(no," du "),zr=l(no,"CODE",{});var D$=a(zr);jf=o(D$,"normalizer"),D$.forEach(t),xf=o(no," pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),no.forEach(t),Mi=c(e),d(ct.$$.fragment,e),yi=c(e),d(mt.$$.fragment,e),Si=c(e),d(cs.$$.fragment,e),Ni=c(e),ms=l(e,"P",{});var oc=a(ms);bf=o(oc,"L\u2019\xE9tape suivante est la pr\xE9tokenisation. Encore une fois, il y a un "),$r=l(oc,"CODE",{});var L$=a($r);gf=o(L$,"BertPreTokenizer"),L$.forEach(t),Pf=o(oc," pr\xE9construit que nous pouvons utiliser :"),oc.forEach(t),Bi=c(e),d(dt.$$.fragment,e),Ai=c(e),Bn=l(e,"P",{});var O$=a(Bn);wf=o(O$,"Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),O$.forEach(t),Fi=c(e),d(ft.$$.fragment,e),Ui=c(e),ds=l(e,"P",{});var rc=a(ds);Tf=o(rc,"Notez que le "),qr=l(rc,"CODE",{});var M$=a(qr);Cf=o(M$,"Whitespace"),M$.forEach(t),Df=o(rc," divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :"),rc.forEach(t),Ri=c(e),d(kt.$$.fragment,e),Wi=c(e),d(vt.$$.fragment,e),Gi=c(e),fs=l(e,"P",{});var lc=a(fs);Lf=o(lc,"Si vous voulez seulement s\xE9parer sur les espaces, vous devez utiliser "),jr=l(lc,"CODE",{});var y$=a(jr);Of=o(y$,"WhitespaceSplit"),y$.forEach(t),Mf=o(lc," \xE0 la place :"),lc.forEach(t),Ii=c(e),d(_t.$$.fragment,e),Vi=c(e),d(ht.$$.fragment,e),Xi=c(e),ks=l(e,"P",{});var ac=a(ks);yf=o(ac,"Comme pour les normaliseurs, vous pouvez utiliser une "),xr=l(ac,"CODE",{});var S$=a(xr);Sf=o(S$,"Sequence"),S$.forEach(t),Nf=o(ac," pour composer plusieurs pr\xE9tokenizers :"),ac.forEach(t),Ki=c(e),d(Et.$$.fragment,e),Hi=c(e),d(zt.$$.fragment,e),Ji=c(e),me=l(e,"P",{});var Os=a(me);Bf=o(Os,"L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),br=l(Os,"CODE",{});var N$=a(br);Af=o(N$,"WordPieceTrainer"),N$.forEach(t),Ff=o(Os,". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),gr=l(Os,"EM",{});var B$=a(gr);Uf=o(B$,"Tokenizers"),B$.forEach(t),Rf=o(Os," est que vous devez lui passer tous les "),Pr=l(Os,"EM",{});var A$=a(Pr);Wf=o(A$,"tokens"),A$.forEach(t),Gf=o(Os," sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Os.forEach(t),Yi=c(e),d($t.$$.fragment,e),Zi=c(e),O=l(e,"P",{});var Q=a(O);If=o(Q,"En plus de sp\xE9cifier la "),wr=l(Q,"CODE",{});var F$=a(wr);Vf=o(F$,"vocab_size"),F$.forEach(t),Xf=o(Q," et les "),Tr=l(Q,"CODE",{});var U$=a(Tr);Kf=o(U$,"special_tokens"),U$.forEach(t),Hf=o(Q,", nous pouvons d\xE9finir la "),Cr=l(Q,"CODE",{});var R$=a(Cr);Jf=o(R$,"min_frequency"),R$.forEach(t),Yf=o(Q," (le nombre de fois qu\u2019un "),Dr=l(Q,"EM",{});var W$=a(Dr);Zf=o(W$,"token"),W$.forEach(t),Qf=o(Q," doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Lr=l(Q,"CODE",{});var G$=a(Lr);ek=o(G$,"continuing_subword_prefix"),G$.forEach(t),sk=o(Q," (si nous voulons utiliser quelque chose de diff\xE9rent de "),Or=l(Q,"CODE",{});var I$=a(Or);tk=o(I$,"##"),I$.forEach(t),nk=o(Q,")."),Q.forEach(t),Qi=c(e),An=l(e,"P",{});var V$=a(An);ok=o(V$,"Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),V$.forEach(t),ep=c(e),d(qt.$$.fragment,e),sp=c(e),Ne=l(e,"P",{});var oo=a(Ne);rk=o(oo,"Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),Mr=l(oo,"EM",{});var X$=a(Mr);lk=o(X$,"tokenizer"),X$.forEach(t),ak=o(oo," qui ressemblerait alors \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),yr=l(oo,"CODE",{});var K$=a(yr);ik=o(K$,"WordPiece"),K$.forEach(t),pk=o(oo," vide au pr\xE9alable) :"),oo.forEach(t),tp=c(e),d(jt.$$.fragment,e),np=c(e),Be=l(e,"P",{});var ro=a(Be);uk=o(ro,"Dans les deux cas, nous pouvons ensuite tester le "),Sr=l(ro,"EM",{});var H$=a(Sr);ck=o(H$,"tokenizer"),H$.forEach(t),mk=o(ro," sur un texte en appelant la m\xE9thode "),Nr=l(ro,"CODE",{});var J$=a(Nr);dk=o(J$,"encode()"),J$.forEach(t),fk=o(ro," :"),ro.forEach(t),op=c(e),d(xt.$$.fragment,e),rp=c(e),d(bt.$$.fragment,e),lp=c(e),j=l(e,"P",{});var L=a(j);kk=o(L,"L\u2019encodage obtenu est un "),Br=l(L,"CODE",{});var Y$=a(Br);vk=o(Y$,"Encoding"),Y$.forEach(t),_k=o(L," contenant toutes les sorties n\xE9cessaires du "),Ar=l(L,"EM",{});var Z$=a(Ar);hk=o(Z$,"tokenizer"),Z$.forEach(t),Ek=o(L," dans ses diff\xE9rents attributs : "),Fr=l(L,"CODE",{});var Q$=a(Fr);zk=o(Q$,"ids"),Q$.forEach(t),$k=o(L,", "),Ur=l(L,"CODE",{});var e1=a(Ur);qk=o(e1,"type_ids"),e1.forEach(t),jk=o(L,", "),Rr=l(L,"CODE",{});var s1=a(Rr);xk=o(s1,"tokens"),s1.forEach(t),bk=o(L,", "),Wr=l(L,"CODE",{});var t1=a(Wr);gk=o(t1,"offsets"),t1.forEach(t),Pk=o(L,", "),Gr=l(L,"CODE",{});var n1=a(Gr);wk=o(n1,"attention_mask"),n1.forEach(t),Tk=o(L,", "),Ir=l(L,"CODE",{});var o1=a(Ir);Ck=o(o1,"special_tokens_mask"),o1.forEach(t),Dk=o(L," et "),Vr=l(L,"CODE",{});var r1=a(Vr);Lk=o(r1,"overflowing"),r1.forEach(t),Ok=o(L,"."),L.forEach(t),ap=c(e),P=l(e,"P",{});var F=a(P);Mk=o(F,"La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Xr=l(F,"EM",{});var l1=a(Xr);yk=o(l1,"token"),l1.forEach(t),Sk=c(F),Kr=l(F,"CODE",{});var a1=a(Kr);Nk=o(a1,"[CLS]"),a1.forEach(t),Bk=o(F," au d\xE9but et le "),Hr=l(F,"EM",{});var i1=a(Hr);Ak=o(i1,"token"),i1.forEach(t),Fk=c(F),Jr=l(F,"CODE",{});var p1=a(Jr);Uk=o(p1,"[SEP]"),p1.forEach(t),Rk=o(F," \xE0 la fin (ou apr\xE8s chaque phrase si nous avons une paire de phrases). Nous utiliserons "),Yr=l(F,"CODE",{});var u1=a(Yr);Wk=o(u1,"TemplateProcessor"),u1.forEach(t),Gk=o(F," pour cela, mais d\u2019abord nous devons conna\xEEtre les identifiants des "),Zr=l(F,"EM",{});var c1=a(Zr);Ik=o(c1,"tokens"),c1.forEach(t),Vk=c(F),Qr=l(F,"CODE",{});var m1=a(Qr);Xk=o(m1,"[CLS]"),m1.forEach(t),Kk=o(F," et "),el=l(F,"CODE",{});var d1=a(el);Hk=o(d1,"[SEP]"),d1.forEach(t),Jk=o(F," dans le vocabulaire :"),F.forEach(t),ip=c(e),d(gt.$$.fragment,e),pp=c(e),d(Pt.$$.fragment,e),up=c(e),M=l(e,"P",{});var ee=a(M);Yk=o(ee,"Pour \xE9crire le gabarit pour "),sl=l(ee,"CODE",{});var f1=a(sl);Zk=o(f1,"TemplateProcessor"),f1.forEach(t),Qk=o(ee,", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),tl=l(ee,"EM",{});var k1=a(tl);ev=o(k1,"tokens"),k1.forEach(t),sv=o(ee," sp\xE9ciaux que nous voulons utiliser. La premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),nl=l(ee,"CODE",{});var v1=a(nl);tv=o(v1,"$A"),v1.forEach(t),nv=o(ee,", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),ol=l(ee,"CODE",{});var _1=a(ol);ov=o(_1,"$B"),_1.forEach(t),rv=o(ee,". Pour chacun de ces \xE9l\xE9ments ("),rl=l(ee,"EM",{});var h1=a(rl);lv=o(h1,"tokens"),h1.forEach(t),av=o(ee," sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019identifiant du "),ll=l(ee,"EM",{});var E1=a(ll);iv=o(E1,"token"),E1.forEach(t),pv=o(ee," correspondant apr\xE8s un deux-points."),ee.forEach(t),cp=c(e),Fn=l(e,"P",{});var z1=a(Fn);uv=o(z1,"Le gabarit classique de BERT est donc d\xE9fini comme suit :"),z1.forEach(t),mp=c(e),d(wt.$$.fragment,e),dp=c(e),Ae=l(e,"P",{});var lo=a(Ae);cv=o(lo,"Notez que nous devons transmettre les identifiants des "),al=l(lo,"EM",{});var $1=a(al);mv=o($1,"tokens"),$1.forEach(t),dv=o(lo," sp\xE9ciaux afin que le "),il=l(lo,"EM",{});var q1=a(il);fv=o(q1,"tokenizer"),q1.forEach(t),kv=o(lo," puisse les convertir correctement."),lo.forEach(t),fp=c(e),Un=l(e,"P",{});var j1=a(Un);vv=o(j1,"Une fois cela ajout\xE9, revenons \xE0 notre exemple pr\xE9c\xE9dent donnera :"),j1.forEach(t),kp=c(e),d(Tt.$$.fragment,e),vp=c(e),d(Ct.$$.fragment,e),_p=c(e),Rn=l(e,"P",{});var x1=a(Rn);_v=o(x1,"Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),x1.forEach(t),hp=c(e),d(Dt.$$.fragment,e),Ep=c(e),d(Lt.$$.fragment,e),zp=c(e),vs=l(e,"P",{});var ic=a(vs);hv=o(ic,"Nous avons presque fini de construire ce "),pl=l(ic,"EM",{});var b1=a(pl);Ev=o(b1,"tokenizer"),b1.forEach(t),zv=o(ic," \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),ic.forEach(t),$p=c(e),d(Ot.$$.fragment,e),qp=c(e),_s=l(e,"P",{});var pc=a(_s);$v=o(pc,"Testons-le sur notre pr\xE9c\xE9dent "),ul=l(pc,"CODE",{});var g1=a(ul);qv=o(g1,"encoding"),g1.forEach(t),jv=o(pc," :"),pc.forEach(t),jp=c(e),d(Mt.$$.fragment,e),xp=c(e),d(yt.$$.fragment,e),bp=c(e),hs=l(e,"P",{});var uc=a(hs);xv=o(uc,"G\xE9nial ! Nous pouvons enregistrer notre "),cl=l(uc,"EM",{});var P1=a(cl);bv=o(P1,"tokenizer"),P1.forEach(t),gv=o(uc," dans un seul fichier JSON comme ceci :"),uc.forEach(t),gp=c(e),d(St.$$.fragment,e),Pp=c(e),Fe=l(e,"P",{});var ao=a(Fe);Pv=o(ao,"Nous pouvons alors recharger ce fichier dans un objet "),ml=l(ao,"CODE",{});var w1=a(ml);wv=o(w1,"Tokenizer"),w1.forEach(t),Tv=o(ao," avec la m\xE9thode "),dl=l(ao,"CODE",{});var T1=a(dl);Cv=o(T1,"from_file()"),T1.forEach(t),Dv=o(ao," :"),ao.forEach(t),wp=c(e),d(Nt.$$.fragment,e),Tp=c(e),y=l(e,"P",{});var se=a(y);Lv=o(se,"Pour utiliser ce "),fl=l(se,"EM",{});var C1=a(fl);Ov=o(C1,"tokenizer"),C1.forEach(t),Mv=o(se," dans \u{1F917} "),kl=l(se,"EM",{});var D1=a(kl);yv=o(D1,"Transformers"),D1.forEach(t),Sv=o(se,", nous devons l\u2019envelopper dans un "),vl=l(se,"CODE",{});var L1=a(vl);Nv=o(L1,"PreTrainedTokenizerFast"),L1.forEach(t),Bv=o(se,". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),_l=l(se,"EM",{});var O1=a(_l);Av=o(O1,"tokenizer"),O1.forEach(t),Fv=o(se," correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),hl=l(se,"CODE",{});var M1=a(hl);Uv=o(M1,"BertTokenizerFast"),M1.forEach(t),Rv=o(se,"). Si vous appliquez cette logique pour construire un tout nouveau "),El=l(se,"EM",{});var y1=a(El);Wv=o(y1,"tokenizer"),y1.forEach(t),Gv=o(se,", vous devrez utiliser la premi\xE8re option."),se.forEach(t),Cp=c(e),$=l(e,"P",{});var q=a($);Iv=o(q,"Pour envelopper le "),zl=l(q,"EM",{});var S1=a(zl);Vv=o(S1,"tokenizer"),S1.forEach(t),Xv=o(q," dans un "),$l=l(q,"CODE",{});var N1=a($l);Kv=o(N1,"PreTrainedTokenizerFast"),N1.forEach(t),Hv=o(q,", nous pouvons soit passer le "),ql=l(q,"EM",{});var B1=a(ql);Jv=o(B1,"tokenizer"),B1.forEach(t),Yv=o(q," que nous avons construit comme un "),jl=l(q,"CODE",{});var A1=a(jl);Zv=o(A1,"tokenizer_object"),A1.forEach(t),Qv=o(q,", soit passer le fichier de "),xl=l(q,"EM",{});var F1=a(xl);e_=o(F1,"tokenizer"),F1.forEach(t),s_=o(q," que nous avons sauvegard\xE9 comme "),bl=l(q,"CODE",{});var U1=a(bl);t_=o(U1,"tokenizer_file"),U1.forEach(t),n_=o(q,". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),gl=l(q,"EM",{});var R1=a(gl);o_=o(R1,"tokens"),R1.forEach(t),r_=o(q," sp\xE9ciaux car cette classe ne peut pas d\xE9duire de l\u2019objet "),Pl=l(q,"CODE",{});var W1=a(Pl);l_=o(W1,"tokenizer"),W1.forEach(t),a_=o(q," quel "),wl=l(q,"EM",{});var G1=a(wl);i_=o(G1,"token"),G1.forEach(t),p_=o(q," est le "),Tl=l(q,"EM",{});var I1=a(Tl);u_=o(I1,"token"),I1.forEach(t),c_=o(q," de masque, quel est le "),Cl=l(q,"EM",{});var V1=a(Cl);m_=o(V1,"token"),V1.forEach(t),Dl=l(q,"CODE",{});var X1=a(Dl);d_=o(X1,"[CLS]"),X1.forEach(t),f_=o(q,", etc :"),q.forEach(t),Dp=c(e),d(Bt.$$.fragment,e),Lp=c(e),de=l(e,"P",{});var Ms=a(de);k_=o(Ms,"Si vous utilisez une classe de "),Ll=l(Ms,"EM",{});var K1=a(Ll);v_=o(K1,"tokenizer"),K1.forEach(t),__=o(Ms," sp\xE9cifique (comme "),Ol=l(Ms,"CODE",{});var H1=a(Ol);h_=o(H1,"BertTokenizerFast"),H1.forEach(t),E_=o(Ms,"), vous aurez seulement besoin de sp\xE9cifier les "),Ml=l(Ms,"EM",{});var J1=a(Ml);z_=o(J1,"tokens"),J1.forEach(t),$_=o(Ms," sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),Ms.forEach(t),Op=c(e),d(At.$$.fragment,e),Mp=c(e),S=l(e,"P",{});var te=a(S);q_=o(te,"Vous pouvez ensuite utiliser ce "),yl=l(te,"EM",{});var Y1=a(yl);j_=o(Y1,"tokenizer"),Y1.forEach(t),x_=o(te," comme n\u2019importe quel autre "),Sl=l(te,"EM",{});var Z1=a(Sl);b_=o(Z1,"tokenizer"),Z1.forEach(t),g_=o(te," de \u{1F917} "),Nl=l(te,"EM",{});var Q1=a(Nl);P_=o(Q1,"Transformers"),Q1.forEach(t),w_=o(te,". Vous pouvez le sauvegarder avec la m\xE9thode "),Bl=l(te,"CODE",{});var e7=a(Bl);T_=o(e7,"save_pretrained()"),e7.forEach(t),C_=o(te," ou le t\xE9l\xE9charger sur le "),Al=l(te,"EM",{});var s7=a(Al);D_=o(s7,"Hub"),s7.forEach(t),L_=o(te," avec la m\xE9thode "),Fl=l(te,"CODE",{});var t7=a(Fl);O_=o(t7,"push_to_hub()"),t7.forEach(t),M_=o(te,"."),te.forEach(t),yp=c(e),Ue=l(e,"P",{});var io=a(Ue);y_=o(io,"Maintenant que nous avons vu comment construire un "),Ul=l(io,"EM",{});var n7=a(Ul);S_=o(n7,"tokenizer WordPiece"),n7.forEach(t),N_=o(io,", faisons de m\xEAme pour un "),Rl=l(io,"EM",{});var o7=a(Rl);B_=o(o7,"tokenizer"),o7.forEach(t),A_=o(io," BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes. Nous ne soulignerons que les diff\xE9rences."),io.forEach(t),Sp=c(e),Qe=l(e,"H2",{class:!0});var cc=a(Qe);Es=l(cc,"A",{id:!0,class:!0,href:!0});var r7=a(Es);Wl=l(r7,"SPAN",{});var l7=a(Wl);d(Ft.$$.fragment,l7),l7.forEach(t),r7.forEach(t),F_=c(cc),Ut=l(cc,"SPAN",{});var mc=a(Ut);U_=o(mc,"Construire un "),Gl=l(mc,"I",{});var a7=a(Gl);R_=o(a7,"tokenizer"),a7.forEach(t),W_=o(mc," BPE \xE0 partir de z\xE9ro"),mc.forEach(t),cc.forEach(t),Np=c(e),fe=l(e,"P",{});var ys=a(fe);G_=o(ys,"Construisons maintenant un "),Il=l(ys,"EM",{});var i7=a(Il);I_=o(i7,"tokenizer"),i7.forEach(t),V_=o(ys," BPE. Comme pour le "),Vl=l(ys,"EM",{});var p7=a(Vl);X_=o(p7,"tokenizer"),p7.forEach(t),K_=o(ys," BERT, nous commen\xE7ons par initialiser un "),Xl=l(ys,"CODE",{});var u7=a(Xl);H_=o(u7,"Tokenizer"),u7.forEach(t),J_=o(ys," avec un mod\xE8le BPE :"),ys.forEach(t),Bp=c(e),d(Rt.$$.fragment,e),Ap=c(e),ke=l(e,"P",{});var Ss=a(ke);Y_=o(Ss,"Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Kl=l(Ss,"CODE",{});var c7=a(Kl);Z_=o(c7,"vocab"),c7.forEach(t),Q_=o(Ss," et le "),Hl=l(Ss,"CODE",{});var m7=a(Hl);e2=o(m7,"merges"),m7.forEach(t),s2=o(Ss," dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Jl=l(Ss,"CODE",{});var d7=a(Jl);t2=o(d7,"unk_token"),d7.forEach(t),n2=o(Ss," parce que le GPT-2 utilise un BPE au niveau de l\u2019octet."),Ss.forEach(t),Fp=c(e),Wn=l(e,"P",{});var f7=a(Wn);o2=o(f7,"GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9tok\xE9nisation :"),f7.forEach(t),Up=c(e),d(Wt.$$.fragment,e),Rp=c(e),zs=l(e,"P",{});var dc=a(zs);r2=o(dc,"L\u2019option que nous avons ajout\xE9e \xE0 "),Yl=l(dc,"CODE",{});var k7=a(Yl);l2=o(k7,"ByteLevel"),k7.forEach(t),a2=o(dc," ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),dc.forEach(t),Wp=c(e),d(Gt.$$.fragment,e),Gp=c(e),d(It.$$.fragment,e),Ip=c(e),Re=l(e,"P",{});var po=a(Re);i2=o(po,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour le GPT-2, le seul "),Zl=l(po,"EM",{});var v7=a(Zl);p2=o(v7,"token"),v7.forEach(t),u2=o(po," sp\xE9cial est le "),Ql=l(po,"EM",{});var _7=a(Ql);c2=o(_7,"token"),_7.forEach(t),m2=o(po," de fin de texte :"),po.forEach(t),Vp=c(e),d(Vt.$$.fragment,e),Xp=c(e),N=l(e,"P",{});var ne=a(N);d2=o(ne,"Comme avec le "),ea=l(ne,"CODE",{});var h7=a(ea);f2=o(h7,"WordPieceTrainer"),h7.forEach(t),k2=o(ne,", ainsi que le "),sa=l(ne,"CODE",{});var E7=a(sa);v2=o(E7,"vocab_size"),E7.forEach(t),_2=o(ne," et le "),ta=l(ne,"CODE",{});var z7=a(ta);h2=o(z7,"special_tokens"),z7.forEach(t),E2=o(ne,", nous pouvons sp\xE9cifier la "),na=l(ne,"CODE",{});var $7=a(na);z2=o($7,"min_frequency"),$7.forEach(t),$2=o(ne," si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),oa=l(ne,"CODE",{});var q7=a(oa);q2=o(q7,"</w>"),q7.forEach(t),j2=o(ne,"), nous pouvons le d\xE9finir avec "),ra=l(ne,"CODE",{});var j7=a(ra);x2=o(j7,"end_of_word_suffix"),j7.forEach(t),b2=o(ne,"."),ne.forEach(t),Kp=c(e),$s=l(e,"P",{});var fc=a($s);g2=o(fc,"Ce "),la=l(fc,"EM",{});var x7=a(la);P2=o(x7,"tokenizer"),x7.forEach(t),w2=o(fc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),fc.forEach(t),Hp=c(e),d(Xt.$$.fragment,e),Jp=c(e),Gn=l(e,"P",{});var b7=a(Gn);T2=o(b7,"Regardons la tokenisation d\u2019un exemple de texte :"),b7.forEach(t),Yp=c(e),d(Kt.$$.fragment,e),Zp=c(e),d(Ht.$$.fragment,e),Qp=c(e),qs=l(e,"P",{});var kc=a(qs);C2=o(kc,"Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),aa=l(kc,"EM",{});var g7=a(aa);D2=o(g7,"tokenizer"),g7.forEach(t),L2=o(kc," du GPT-2 comme suit :"),kc.forEach(t),eu=c(e),d(Jt.$$.fragment,e),su=c(e),T=l(e,"P",{});var X=a(T);O2=o(X,"L\u2019option "),ia=l(X,"CODE",{});var P7=a(ia);M2=o(P7,"trim_offsets = False"),P7.forEach(t),y2=o(X," indique au post-processeur que nous devons laisser les "),pa=l(X,"EM",{});var w7=a(pa);S2=o(w7,"offsets"),w7.forEach(t),N2=o(X," des "),ua=l(X,"EM",{});var T7=a(ua);B2=o(T7,"tokens"),T7.forEach(t),A2=o(X," qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),ca=l(X,"EM",{});var C7=a(ca);F2=o(C7,"offsets"),C7.forEach(t),U2=o(X," pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du "),ma=l(X,"EM",{});var D7=a(ma);R2=o(D7,"token"),D7.forEach(t),W2=o(X,"). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),da=l(X,"CODE",{});var L7=a(da);G2=o(L7,"'\u0120test'"),L7.forEach(t),I2=o(X," est le "),fa=l(X,"EM",{});var O7=a(fa);V2=o(O7,"token"),O7.forEach(t),X2=o(X," \xE0 l\u2019index 4 :"),X.forEach(t),tu=c(e),d(Yt.$$.fragment,e),nu=c(e),d(Zt.$$.fragment,e),ou=c(e),In=l(e,"P",{});var M7=a(In);K2=o(M7,"Enfin, nous ajoutons un d\xE9codeur au niveau de l\u2019octet :"),M7.forEach(t),ru=c(e),d(Qt.$$.fragment,e),lu=c(e),Vn=l(e,"P",{});var y7=a(Vn);H2=o(y7,"et nous pouvons v\xE9rifier qu\u2019il fonctionne correctement :"),y7.forEach(t),au=c(e),d(en.$$.fragment,e),iu=c(e),d(sn.$$.fragment,e),pu=c(e),ve=l(e,"P",{});var Ns=a(ve);J2=o(Ns,"Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),ka=l(Ns,"CODE",{});var S7=a(ka);Y2=o(S7,"PreTrainedTokenizerFast"),S7.forEach(t),Z2=o(Ns," ou un "),va=l(Ns,"CODE",{});var N7=a(va);Q2=o(N7,"GPT2TokenizerFast"),N7.forEach(t),eh=o(Ns," si nous voulons l\u2019utiliser dans \u{1F917} "),_a=l(Ns,"EM",{});var B7=a(_a);sh=o(B7,"Transformers"),B7.forEach(t),th=o(Ns," :"),Ns.forEach(t),uu=c(e),d(tn.$$.fragment,e),cu=c(e),Xn=l(e,"P",{});var A7=a(Xn);nh=o(A7,"ou :"),A7.forEach(t),mu=c(e),d(nn.$$.fragment,e),du=c(e),We=l(e,"P",{});var uo=a(We);oh=o(uo,"Comme dernier exemple, nous allons vous montrer comment construire un "),ha=l(uo,"EM",{});var F7=a(ha);rh=o(F7,"tokenizer"),F7.forEach(t),lh=c(uo),Ea=l(uo,"EM",{});var U7=a(Ea);ah=o(U7,"Unigram"),U7.forEach(t),ih=o(uo," \xE0 partir de z\xE9ro."),uo.forEach(t),fu=c(e),es=l(e,"H2",{class:!0});var vc=a(es);js=l(vc,"A",{id:!0,class:!0,href:!0});var R7=a(js);za=l(R7,"SPAN",{});var W7=a(za);d(on.$$.fragment,W7),W7.forEach(t),R7.forEach(t),ph=c(vc),rn=l(vc,"SPAN",{});var _c=a(rn);uh=o(_c,"Construire un "),$a=l(_c,"I",{});var G7=a($a);ch=o(G7,"tokenizer Unigram"),G7.forEach(t),mh=o(_c," \xE0 partir de z\xE9ro"),_c.forEach(t),vc.forEach(t),ku=c(e),Y=l(e,"P",{});var Ie=a(Y);dh=o(Ie,"Construisons maintenant un "),qa=l(Ie,"EM",{});var I7=a(qa);fh=o(I7,"tokenizer"),I7.forEach(t),kh=o(Ie," XLNet. Comme pour les "),ja=l(Ie,"EM",{});var V7=a(ja);vh=o(V7,"tokenizers"),V7.forEach(t),_h=o(Ie," pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),xa=l(Ie,"CODE",{});var X7=a(xa);hh=o(X7,"Tokenizer"),X7.forEach(t),Eh=o(Ie," avec un mod\xE8le "),ba=l(Ie,"EM",{});var K7=a(ba);zh=o(K7,"Unigram"),K7.forEach(t),$h=o(Ie," :"),Ie.forEach(t),vu=c(e),d(ln.$$.fragment,e),_u=c(e),Kn=l(e,"P",{});var H7=a(Kn);qh=o(H7,"Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),H7.forEach(t),hu=c(e),xs=l(e,"P",{});var hc=a(xs);jh=o(hc,"Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),ga=l(hc,"EM",{});var J7=a(ga);xh=o(J7,"SentencePiece"),J7.forEach(t),bh=o(hc,") :"),hc.forEach(t),Eu=c(e),d(an.$$.fragment,e),zu=c(e),_e=l(e,"P",{});var Bs=a(_e);gh=o(Bs,"Il remplace "),Pa=l(Bs,"CODE",{});var Y7=a(Pa);Ph=o(Y7,"\u201C"),Y7.forEach(t),wh=o(Bs," et "),wa=l(Bs,"CODE",{});var Z7=a(wa);Th=o(Z7,"\u201D"),Z7.forEach(t),Ch=o(Bs," par "),Ta=l(Bs,"CODE",{});var Q7=a(Ta);Dh=o(Q7,"\u201D"),Q7.forEach(t),Lh=o(Bs," et toute s\xE9quence de deux espaces ou plus par un seul espace, de plus il supprime les accents."),Bs.forEach(t),$u=c(e),Ge=l(e,"P",{});var co=a(Ge);Oh=o(co,"Le pr\xE9tokenizer \xE0 utiliser pour tout "),Ca=l(co,"EM",{});var eq=a(Ca);Mh=o(eq,"tokenizer SentencePiece"),eq.forEach(t),yh=o(co," est "),Da=l(co,"CODE",{});var sq=a(Da);Sh=o(sq,"Metaspace"),sq.forEach(t),Nh=o(co," :"),co.forEach(t),qu=c(e),d(pn.$$.fragment,e),ju=c(e),Hn=l(e,"P",{});var tq=a(Hn);Bh=o(tq,"Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9tok\xE9nisation sur le m\xEAme exemple de texte que pr\xE9c\xE9demment :"),tq.forEach(t),xu=c(e),d(un.$$.fragment,e),bu=c(e),d(cn.$$.fragment,e),gu=c(e),bs=l(e,"P",{});var Ec=a(bs);Ah=o(Ec,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),La=l(Ec,"EM",{});var nq=a(La);Fh=o(nq,"tokens"),nq.forEach(t),Uh=o(Ec," sp\xE9ciaux :"),Ec.forEach(t),Pu=c(e),d(mn.$$.fragment,e),wu=c(e),C=l(e,"P",{});var K=a(C);Rh=o(K,"Un argument tr\xE8s important \xE0 ne pas oublier pour le "),Oa=l(K,"CODE",{});var oq=a(Oa);Wh=o(oq,"UnigramTrainer"),oq.forEach(t),Gh=o(K," est le "),Ma=l(K,"CODE",{});var rq=a(Ma);Ih=o(rq,"unk_token"),rq.forEach(t),Vh=o(K,". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),ya=l(K,"EM",{});var lq=a(ya);Xh=o(lq,"Unigram"),lq.forEach(t),Kh=o(K,", comme le "),Sa=l(K,"CODE",{});var aq=a(Sa);Hh=o(aq,"shrinking_factor"),aq.forEach(t),Jh=o(K," pour chaque \xE9tape o\xF9 nous enlevons des "),Na=l(K,"EM",{});var iq=a(Na);Yh=o(iq,"tokens"),iq.forEach(t),Zh=o(K," (par d\xE9faut 0.75) ou le "),Ba=l(K,"CODE",{});var pq=a(Ba);Qh=o(pq,"max_piece_length"),pq.forEach(t),eE=o(K," pour sp\xE9cifier la longueur maximale d\u2019un "),Aa=l(K,"EM",{});var uq=a(Aa);sE=o(uq,"token"),uq.forEach(t),tE=o(K," donn\xE9 (par d\xE9faut 16)."),K.forEach(t),Tu=c(e),gs=l(e,"P",{});var zc=a(gs);nE=o(zc,"Ce "),Fa=l(zc,"EM",{});var cq=a(Fa);oE=o(cq,"tokenizer"),cq.forEach(t),rE=o(zc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),zc.forEach(t),Cu=c(e),d(dn.$$.fragment,e),Du=c(e),Jn=l(e,"P",{});var mq=a(Jn);lE=o(mq,"Regardons la tokenisation de notre exemple :"),mq.forEach(t),Lu=c(e),d(fn.$$.fragment,e),Ou=c(e),d(kn.$$.fragment,e),Mu=c(e),w=l(e,"P",{});var U=a(w);aE=o(U,"Une particularit\xE9 de XLNet est qu\u2019il place le "),Ua=l(U,"EM",{});var dq=a(Ua);iE=o(dq,"token"),dq.forEach(t),pE=c(U),Ra=l(U,"CODE",{});var fq=a(Ra);uE=o(fq,"<cls>"),fq.forEach(t),cE=o(U," \xE0 la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres "),Wa=l(U,"EM",{});var kq=a(Wa);mE=o(kq,"tokens"),kq.forEach(t),dE=o(U,"). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ga=l(U,"EM",{});var vq=a(Ga);fE=o(vq,"tokens"),vq.forEach(t),kE=o(U," sp\xE9ciaux et les types d\u2019identifiant de "),Ia=l(U,"EM",{});var _q=a(Ia);vE=o(_q,"token"),_q.forEach(t),_E=o(U," avec un mod\xE8le, comme pour BERT. Mais d\u2019abord nous devons obtenir les identifiants des "),Va=l(U,"EM",{});var hq=a(Va);hE=o(hq,"tokens"),hq.forEach(t),EE=c(U),Xa=l(U,"CODE",{});var Eq=a(Xa);zE=o(Eq,"<cls>"),Eq.forEach(t),$E=o(U," et "),Ka=l(U,"CODE",{});var zq=a(Ka);qE=o(zq,"<sep>"),zq.forEach(t),jE=o(U," :"),U.forEach(t),yu=c(e),d(vn.$$.fragment,e),Su=c(e),d(_n.$$.fragment,e),Nu=c(e),Yn=l(e,"P",{});var $q=a(Yn);xE=o($q,"Le mod\xE8le ressemble \xE0 ceci :"),$q.forEach(t),Bu=c(e),d(hn.$$.fragment,e),Au=c(e),Zn=l(e,"P",{});var qq=a(Zn);bE=o(qq,"Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),qq.forEach(t),Fu=c(e),d(En.$$.fragment,e),Uu=c(e),d(zn.$$.fragment,e),Ru=c(e),Ps=l(e,"P",{});var $c=a(Ps);gE=o($c,"Enfin, nous ajoutons un d\xE9codeur "),Ha=l($c,"CODE",{});var jq=a(Ha);PE=o(jq,"Metaspace"),jq.forEach(t),wE=o($c," :"),$c.forEach(t),Wu=c(e),d($n.$$.fragment,e),Gu=c(e),D=l(e,"P",{});var H=a(D);TE=o(H,"et on en a fini avec ce "),Ja=l(H,"EM",{});var xq=a(Ja);CE=o(xq,"tokenizer"),xq.forEach(t),DE=o(H," ! On peut le sauvegarder et l\u2019envelopper dans un "),Ya=l(H,"CODE",{});var bq=a(Ya);LE=o(bq,"PreTrainedTokenizerFast"),bq.forEach(t),OE=o(H," ou "),Za=l(H,"CODE",{});var gq=a(Za);ME=o(gq,"XLNetTokenizerFast"),gq.forEach(t),yE=o(H," si on veut l\u2019utiliser dans \u{1F917} "),Qa=l(H,"EM",{});var Pq=a(Qa);SE=o(Pq,"Transformers"),Pq.forEach(t),NE=o(H,". Une chose \xE0 noter lors de l\u2019utilisation de "),ei=l(H,"CODE",{});var wq=a(ei);BE=o(wq,"PreTrainedTokenizerFast"),wq.forEach(t),AE=o(H," est qu\u2019en plus des "),si=l(H,"EM",{});var Tq=a(si);FE=o(Tq,"tokens"),Tq.forEach(t),UE=o(H," sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),ti=l(H,"EM",{});var Cq=a(ti);RE=o(Cq,"Transformers"),Cq.forEach(t),WE=o(H," de rembourrer \xE0 gauche :"),H.forEach(t),Iu=c(e),d(qn.$$.fragment,e),Vu=c(e),Qn=l(e,"P",{});var Dq=a(Qn);GE=o(Dq,"Ou alternativement :"),Dq.forEach(t),Xu=c(e),d(jn.$$.fragment,e),Ku=c(e),Z=l(e,"P",{});var Ve=a(Z);IE=o(Ve,"Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),ni=l(Ve,"EM",{});var Lq=a(ni);VE=o(Lq,"tokenizers"),Lq.forEach(t),XE=o(Ve," existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),oi=l(Ve,"EM",{});var Oq=a(oi);KE=o(Oq,"tokenizer"),Oq.forEach(t),HE=o(Ve," que vous voulez avec la biblioth\xE8que \u{1F917} "),ri=l(Ve,"EM",{});var Mq=a(ri);JE=o(Mq,"Tokenizers"),Mq.forEach(t),YE=o(Ve," et pouvoir l\u2019utiliser dans \u{1F917} "),li=l(Ve,"EM",{});var yq=a(li);ZE=o(yq,"Transformers"),yq.forEach(t),QE=o(Ve,"."),Ve.forEach(t),this.h()},h(){E(z,"name","hf:doc:metadata"),E(z,"content",JSON.stringify(Xq)),E(ie,"id","construction-dun-tokenizer-bloc-par-bloc"),E(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ie,"href","#construction-dun-tokenizer-bloc-par-bloc"),E(oe,"class","relative group"),E(Vs,"class","block dark:hidden"),Sq(Vs.src,ez="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||E(Vs,"src",ez),E(Vs,"alt","The tokenization pipeline."),E(Xs,"class","hidden dark:block"),Sq(Xs.src,sz="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||E(Xs,"src",sz),E(Xs,"alt","The tokenization pipeline."),E(Je,"class","flex justify-center"),E(Sn,"href","/course/fr/chapter6/2"),E(Hs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"),E(Hs,"rel","nofollow"),E(Js,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers"),E(Js,"rel","nofollow"),E(Ys,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models"),E(Ys,"rel","nofollow"),E(Zs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers"),E(Zs,"rel","nofollow"),E(Qs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors"),E(Qs,"rel","nofollow"),E(et,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders"),E(et,"rel","nofollow"),E(st,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html"),E(st,"rel","nofollow"),E(is,"id","acquisition-dun-corpus"),E(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(is,"href","#acquisition-dun-corpus"),E(Ye,"class","relative group"),E(Nn,"href","/course/fr/chapter6/2"),E(nt,"href","https://huggingface.co/datasets/wikitext"),E(nt,"rel","nofollow"),E(us,"id","construire-un-itokenizer-wordpiecei-partir-de-zro"),E(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(us,"href","#construire-un-itokenizer-wordpiecei-partir-de-zro"),E(Ze,"class","relative group"),E(Es,"id","construire-un-itokenizeri-bpe-partir-de-zro"),E(Es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Es,"href","#construire-un-itokenizeri-bpe-partir-de-zro"),E(Qe,"class","relative group"),E(js,"id","construire-un-itokenizer-unigrami-partir-de-zro"),E(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(js,"href","#construire-un-itokenizer-unigrami-partir-de-zro"),E(es,"class","relative group")},m(e,i){s(document.head,z),p(e,ge,i),p(e,oe,i),s(oe,ie),s(ie,$e),f(qe,$e,null),s(oe,As),s(oe,je),s(je,Fs),p(e,ss,i),f(re,e,i),p(e,ts,i),p(e,Pe,i),s(Pe,Xe),p(e,ns,i),p(e,R,i),s(R,xe),s(xe,Us),s(R,Rs),s(R,be),s(be,Ws),s(R,Gs),s(R,le),s(le,Is),s(le,Ke),s(Ke,He),s(le,x),s(R,Cn),s(R,J),s(J,Dn),s(J,os),s(os,Ln),s(J,On),s(J,rs),s(rs,Mn),s(J,qc),s(J,mo),s(mo,jc),s(J,xc),p(e,pi,i),p(e,yn,i),s(yn,bc),p(e,ui,i),p(e,Je,i),s(Je,Vs),s(Je,gc),s(Je,Xs),p(e,ci,i),p(e,W,i),s(W,Pc),s(W,fo),s(fo,wc),s(W,Tc),s(W,ko),s(ko,Cc),s(W,Dc),s(W,vo),s(vo,Lc),s(W,Oc),s(W,Sn),s(Sn,Mc),s(W,yc),s(W,_o),s(_o,Sc),s(W,Nc),p(e,mi,i),f(Ks,e,i),p(e,di,i),p(e,ls,i),s(ls,Bc),s(ls,ho),s(ho,Ac),s(ls,Fc),p(e,fi,i),p(e,G,i),s(G,we),s(we,Eo),s(Eo,Uc),s(we,Rc),s(we,zo),s(zo,Wc),s(we,Gc),s(we,Hs),s(Hs,Ic),s(we,Vc),s(G,Xc),s(G,Te),s(Te,$o),s($o,Kc),s(Te,Hc),s(Te,qo),s(qo,Jc),s(Te,Yc),s(Te,Js),s(Js,Zc),s(Te,Qc),s(G,em),s(G,I),s(I,jo),s(jo,sm),s(I,tm),s(I,xo),s(xo,nm),s(I,om),s(I,bo),s(bo,rm),s(I,lm),s(I,go),s(go,am),s(I,im),s(I,Po),s(Po,pm),s(I,um),s(I,Ys),s(Ys,cm),s(I,mm),s(G,dm),s(G,Ce),s(Ce,wo),s(wo,fm),s(Ce,km),s(Ce,To),s(To,vm),s(Ce,_m),s(Ce,Zs),s(Zs,hm),s(Ce,Em),s(G,zm),s(G,De),s(De,Co),s(Co,$m),s(De,qm),s(De,Do),s(Do,jm),s(De,xm),s(De,Qs),s(Qs,bm),s(De,gm),s(G,Pm),s(G,Le),s(Le,Lo),s(Lo,wm),s(Le,Tm),s(Le,Oo),s(Oo,Cm),s(Le,Dm),s(Le,et),s(et,Lm),s(Le,Om),p(e,ki,i),p(e,as,i),s(as,Mm),s(as,st),s(st,ym),s(as,Sm),p(e,vi,i),p(e,Ye,i),s(Ye,is),s(is,Mo),f(tt,Mo,null),s(Ye,Nm),s(Ye,yo),s(yo,Bm),p(e,_i,i),p(e,pe,i),s(pe,Am),s(pe,So),s(So,Fm),s(pe,Um),s(pe,Nn),s(Nn,Rm),s(pe,Wm),s(pe,nt),s(nt,Gm),s(pe,Im),p(e,hi,i),f(ot,e,i),p(e,Ei,i),p(e,Oe,i),s(Oe,Vm),s(Oe,No),s(No,Xm),s(Oe,Km),s(Oe,Bo),s(Bo,Hm),s(Oe,Jm),p(e,zi,i),p(e,ps,i),s(ps,Ym),s(ps,Ao),s(Ao,Zm),s(ps,Qm),p(e,$i,i),f(rt,e,i),p(e,qi,i),p(e,ue,i),s(ue,ed),s(ue,Fo),s(Fo,sd),s(ue,td),s(ue,Uo),s(Uo,nd),s(ue,od),s(ue,Ro),s(Ro,rd),s(ue,ld),p(e,ji,i),p(e,Ze,i),s(Ze,us),s(us,Wo),f(lt,Wo,null),s(Ze,ad),s(Ze,at),s(at,id),s(at,Go),s(Go,pd),s(at,ud),p(e,xi,i),p(e,b,i),s(b,cd),s(b,Io),s(Io,md),s(b,dd),s(b,Vo),s(Vo,fd),s(b,kd),s(b,Xo),s(Xo,vd),s(b,_d),s(b,Ko),s(Ko,hd),s(b,Ed),s(b,Ho),s(Ho,zd),s(b,$d),s(b,Jo),s(Jo,qd),s(b,jd),s(b,Yo),s(Yo,xd),s(b,bd),s(b,Zo),s(Zo,gd),s(b,Pd),p(e,bi,i),p(e,Me,i),s(Me,wd),s(Me,Qo),s(Qo,Td),s(Me,Cd),s(Me,er),s(er,Dd),s(Me,Ld),p(e,gi,i),f(it,e,i),p(e,Pi,i),p(e,ce,i),s(ce,Od),s(ce,sr),s(sr,Md),s(ce,yd),s(ce,tr),s(tr,Sd),s(ce,Nd),s(ce,nr),s(nr,Bd),s(ce,Ad),p(e,wi,i),p(e,g,i),s(g,Fd),s(g,or),s(or,Ud),s(g,Rd),s(g,rr),s(rr,Wd),s(g,Gd),s(g,lr),s(lr,Id),s(g,Vd),s(g,ar),s(ar,Xd),s(g,Kd),s(g,ir),s(ir,Hd),s(g,Jd),s(g,pr),s(pr,Yd),s(g,Zd),s(g,ur),s(ur,Qd),s(g,ef),s(g,cr),s(cr,sf),s(g,tf),p(e,Ti,i),f(pt,e,i),p(e,Ci,i),p(e,V,i),s(V,nf),s(V,mr),s(mr,of),s(V,rf),s(V,dr),s(dr,lf),s(V,af),s(V,fr),s(fr,pf),s(V,uf),s(V,kr),s(kr,cf),s(V,mf),s(V,vr),s(vr,df),s(V,ff),p(e,Di,i),f(ut,e,i),p(e,Li,i),p(e,ye,i),s(ye,kf),s(ye,_r),s(_r,vf),s(ye,_f),s(ye,hr),s(hr,hf),s(ye,Ef),p(e,Oi,i),p(e,Se,i),s(Se,zf),s(Se,Er),s(Er,$f),s(Se,qf),s(Se,zr),s(zr,jf),s(Se,xf),p(e,Mi,i),f(ct,e,i),p(e,yi,i),f(mt,e,i),p(e,Si,i),f(cs,e,i),p(e,Ni,i),p(e,ms,i),s(ms,bf),s(ms,$r),s($r,gf),s(ms,Pf),p(e,Bi,i),f(dt,e,i),p(e,Ai,i),p(e,Bn,i),s(Bn,wf),p(e,Fi,i),f(ft,e,i),p(e,Ui,i),p(e,ds,i),s(ds,Tf),s(ds,qr),s(qr,Cf),s(ds,Df),p(e,Ri,i),f(kt,e,i),p(e,Wi,i),f(vt,e,i),p(e,Gi,i),p(e,fs,i),s(fs,Lf),s(fs,jr),s(jr,Of),s(fs,Mf),p(e,Ii,i),f(_t,e,i),p(e,Vi,i),f(ht,e,i),p(e,Xi,i),p(e,ks,i),s(ks,yf),s(ks,xr),s(xr,Sf),s(ks,Nf),p(e,Ki,i),f(Et,e,i),p(e,Hi,i),f(zt,e,i),p(e,Ji,i),p(e,me,i),s(me,Bf),s(me,br),s(br,Af),s(me,Ff),s(me,gr),s(gr,Uf),s(me,Rf),s(me,Pr),s(Pr,Wf),s(me,Gf),p(e,Yi,i),f($t,e,i),p(e,Zi,i),p(e,O,i),s(O,If),s(O,wr),s(wr,Vf),s(O,Xf),s(O,Tr),s(Tr,Kf),s(O,Hf),s(O,Cr),s(Cr,Jf),s(O,Yf),s(O,Dr),s(Dr,Zf),s(O,Qf),s(O,Lr),s(Lr,ek),s(O,sk),s(O,Or),s(Or,tk),s(O,nk),p(e,Qi,i),p(e,An,i),s(An,ok),p(e,ep,i),f(qt,e,i),p(e,sp,i),p(e,Ne,i),s(Ne,rk),s(Ne,Mr),s(Mr,lk),s(Ne,ak),s(Ne,yr),s(yr,ik),s(Ne,pk),p(e,tp,i),f(jt,e,i),p(e,np,i),p(e,Be,i),s(Be,uk),s(Be,Sr),s(Sr,ck),s(Be,mk),s(Be,Nr),s(Nr,dk),s(Be,fk),p(e,op,i),f(xt,e,i),p(e,rp,i),f(bt,e,i),p(e,lp,i),p(e,j,i),s(j,kk),s(j,Br),s(Br,vk),s(j,_k),s(j,Ar),s(Ar,hk),s(j,Ek),s(j,Fr),s(Fr,zk),s(j,$k),s(j,Ur),s(Ur,qk),s(j,jk),s(j,Rr),s(Rr,xk),s(j,bk),s(j,Wr),s(Wr,gk),s(j,Pk),s(j,Gr),s(Gr,wk),s(j,Tk),s(j,Ir),s(Ir,Ck),s(j,Dk),s(j,Vr),s(Vr,Lk),s(j,Ok),p(e,ap,i),p(e,P,i),s(P,Mk),s(P,Xr),s(Xr,yk),s(P,Sk),s(P,Kr),s(Kr,Nk),s(P,Bk),s(P,Hr),s(Hr,Ak),s(P,Fk),s(P,Jr),s(Jr,Uk),s(P,Rk),s(P,Yr),s(Yr,Wk),s(P,Gk),s(P,Zr),s(Zr,Ik),s(P,Vk),s(P,Qr),s(Qr,Xk),s(P,Kk),s(P,el),s(el,Hk),s(P,Jk),p(e,ip,i),f(gt,e,i),p(e,pp,i),f(Pt,e,i),p(e,up,i),p(e,M,i),s(M,Yk),s(M,sl),s(sl,Zk),s(M,Qk),s(M,tl),s(tl,ev),s(M,sv),s(M,nl),s(nl,tv),s(M,nv),s(M,ol),s(ol,ov),s(M,rv),s(M,rl),s(rl,lv),s(M,av),s(M,ll),s(ll,iv),s(M,pv),p(e,cp,i),p(e,Fn,i),s(Fn,uv),p(e,mp,i),f(wt,e,i),p(e,dp,i),p(e,Ae,i),s(Ae,cv),s(Ae,al),s(al,mv),s(Ae,dv),s(Ae,il),s(il,fv),s(Ae,kv),p(e,fp,i),p(e,Un,i),s(Un,vv),p(e,kp,i),f(Tt,e,i),p(e,vp,i),f(Ct,e,i),p(e,_p,i),p(e,Rn,i),s(Rn,_v),p(e,hp,i),f(Dt,e,i),p(e,Ep,i),f(Lt,e,i),p(e,zp,i),p(e,vs,i),s(vs,hv),s(vs,pl),s(pl,Ev),s(vs,zv),p(e,$p,i),f(Ot,e,i),p(e,qp,i),p(e,_s,i),s(_s,$v),s(_s,ul),s(ul,qv),s(_s,jv),p(e,jp,i),f(Mt,e,i),p(e,xp,i),f(yt,e,i),p(e,bp,i),p(e,hs,i),s(hs,xv),s(hs,cl),s(cl,bv),s(hs,gv),p(e,gp,i),f(St,e,i),p(e,Pp,i),p(e,Fe,i),s(Fe,Pv),s(Fe,ml),s(ml,wv),s(Fe,Tv),s(Fe,dl),s(dl,Cv),s(Fe,Dv),p(e,wp,i),f(Nt,e,i),p(e,Tp,i),p(e,y,i),s(y,Lv),s(y,fl),s(fl,Ov),s(y,Mv),s(y,kl),s(kl,yv),s(y,Sv),s(y,vl),s(vl,Nv),s(y,Bv),s(y,_l),s(_l,Av),s(y,Fv),s(y,hl),s(hl,Uv),s(y,Rv),s(y,El),s(El,Wv),s(y,Gv),p(e,Cp,i),p(e,$,i),s($,Iv),s($,zl),s(zl,Vv),s($,Xv),s($,$l),s($l,Kv),s($,Hv),s($,ql),s(ql,Jv),s($,Yv),s($,jl),s(jl,Zv),s($,Qv),s($,xl),s(xl,e_),s($,s_),s($,bl),s(bl,t_),s($,n_),s($,gl),s(gl,o_),s($,r_),s($,Pl),s(Pl,l_),s($,a_),s($,wl),s(wl,i_),s($,p_),s($,Tl),s(Tl,u_),s($,c_),s($,Cl),s(Cl,m_),s($,Dl),s(Dl,d_),s($,f_),p(e,Dp,i),f(Bt,e,i),p(e,Lp,i),p(e,de,i),s(de,k_),s(de,Ll),s(Ll,v_),s(de,__),s(de,Ol),s(Ol,h_),s(de,E_),s(de,Ml),s(Ml,z_),s(de,$_),p(e,Op,i),f(At,e,i),p(e,Mp,i),p(e,S,i),s(S,q_),s(S,yl),s(yl,j_),s(S,x_),s(S,Sl),s(Sl,b_),s(S,g_),s(S,Nl),s(Nl,P_),s(S,w_),s(S,Bl),s(Bl,T_),s(S,C_),s(S,Al),s(Al,D_),s(S,L_),s(S,Fl),s(Fl,O_),s(S,M_),p(e,yp,i),p(e,Ue,i),s(Ue,y_),s(Ue,Ul),s(Ul,S_),s(Ue,N_),s(Ue,Rl),s(Rl,B_),s(Ue,A_),p(e,Sp,i),p(e,Qe,i),s(Qe,Es),s(Es,Wl),f(Ft,Wl,null),s(Qe,F_),s(Qe,Ut),s(Ut,U_),s(Ut,Gl),s(Gl,R_),s(Ut,W_),p(e,Np,i),p(e,fe,i),s(fe,G_),s(fe,Il),s(Il,I_),s(fe,V_),s(fe,Vl),s(Vl,X_),s(fe,K_),s(fe,Xl),s(Xl,H_),s(fe,J_),p(e,Bp,i),f(Rt,e,i),p(e,Ap,i),p(e,ke,i),s(ke,Y_),s(ke,Kl),s(Kl,Z_),s(ke,Q_),s(ke,Hl),s(Hl,e2),s(ke,s2),s(ke,Jl),s(Jl,t2),s(ke,n2),p(e,Fp,i),p(e,Wn,i),s(Wn,o2),p(e,Up,i),f(Wt,e,i),p(e,Rp,i),p(e,zs,i),s(zs,r2),s(zs,Yl),s(Yl,l2),s(zs,a2),p(e,Wp,i),f(Gt,e,i),p(e,Gp,i),f(It,e,i),p(e,Ip,i),p(e,Re,i),s(Re,i2),s(Re,Zl),s(Zl,p2),s(Re,u2),s(Re,Ql),s(Ql,c2),s(Re,m2),p(e,Vp,i),f(Vt,e,i),p(e,Xp,i),p(e,N,i),s(N,d2),s(N,ea),s(ea,f2),s(N,k2),s(N,sa),s(sa,v2),s(N,_2),s(N,ta),s(ta,h2),s(N,E2),s(N,na),s(na,z2),s(N,$2),s(N,oa),s(oa,q2),s(N,j2),s(N,ra),s(ra,x2),s(N,b2),p(e,Kp,i),p(e,$s,i),s($s,g2),s($s,la),s(la,P2),s($s,w2),p(e,Hp,i),f(Xt,e,i),p(e,Jp,i),p(e,Gn,i),s(Gn,T2),p(e,Yp,i),f(Kt,e,i),p(e,Zp,i),f(Ht,e,i),p(e,Qp,i),p(e,qs,i),s(qs,C2),s(qs,aa),s(aa,D2),s(qs,L2),p(e,eu,i),f(Jt,e,i),p(e,su,i),p(e,T,i),s(T,O2),s(T,ia),s(ia,M2),s(T,y2),s(T,pa),s(pa,S2),s(T,N2),s(T,ua),s(ua,B2),s(T,A2),s(T,ca),s(ca,F2),s(T,U2),s(T,ma),s(ma,R2),s(T,W2),s(T,da),s(da,G2),s(T,I2),s(T,fa),s(fa,V2),s(T,X2),p(e,tu,i),f(Yt,e,i),p(e,nu,i),f(Zt,e,i),p(e,ou,i),p(e,In,i),s(In,K2),p(e,ru,i),f(Qt,e,i),p(e,lu,i),p(e,Vn,i),s(Vn,H2),p(e,au,i),f(en,e,i),p(e,iu,i),f(sn,e,i),p(e,pu,i),p(e,ve,i),s(ve,J2),s(ve,ka),s(ka,Y2),s(ve,Z2),s(ve,va),s(va,Q2),s(ve,eh),s(ve,_a),s(_a,sh),s(ve,th),p(e,uu,i),f(tn,e,i),p(e,cu,i),p(e,Xn,i),s(Xn,nh),p(e,mu,i),f(nn,e,i),p(e,du,i),p(e,We,i),s(We,oh),s(We,ha),s(ha,rh),s(We,lh),s(We,Ea),s(Ea,ah),s(We,ih),p(e,fu,i),p(e,es,i),s(es,js),s(js,za),f(on,za,null),s(es,ph),s(es,rn),s(rn,uh),s(rn,$a),s($a,ch),s(rn,mh),p(e,ku,i),p(e,Y,i),s(Y,dh),s(Y,qa),s(qa,fh),s(Y,kh),s(Y,ja),s(ja,vh),s(Y,_h),s(Y,xa),s(xa,hh),s(Y,Eh),s(Y,ba),s(ba,zh),s(Y,$h),p(e,vu,i),f(ln,e,i),p(e,_u,i),p(e,Kn,i),s(Kn,qh),p(e,hu,i),p(e,xs,i),s(xs,jh),s(xs,ga),s(ga,xh),s(xs,bh),p(e,Eu,i),f(an,e,i),p(e,zu,i),p(e,_e,i),s(_e,gh),s(_e,Pa),s(Pa,Ph),s(_e,wh),s(_e,wa),s(wa,Th),s(_e,Ch),s(_e,Ta),s(Ta,Dh),s(_e,Lh),p(e,$u,i),p(e,Ge,i),s(Ge,Oh),s(Ge,Ca),s(Ca,Mh),s(Ge,yh),s(Ge,Da),s(Da,Sh),s(Ge,Nh),p(e,qu,i),f(pn,e,i),p(e,ju,i),p(e,Hn,i),s(Hn,Bh),p(e,xu,i),f(un,e,i),p(e,bu,i),f(cn,e,i),p(e,gu,i),p(e,bs,i),s(bs,Ah),s(bs,La),s(La,Fh),s(bs,Uh),p(e,Pu,i),f(mn,e,i),p(e,wu,i),p(e,C,i),s(C,Rh),s(C,Oa),s(Oa,Wh),s(C,Gh),s(C,Ma),s(Ma,Ih),s(C,Vh),s(C,ya),s(ya,Xh),s(C,Kh),s(C,Sa),s(Sa,Hh),s(C,Jh),s(C,Na),s(Na,Yh),s(C,Zh),s(C,Ba),s(Ba,Qh),s(C,eE),s(C,Aa),s(Aa,sE),s(C,tE),p(e,Tu,i),p(e,gs,i),s(gs,nE),s(gs,Fa),s(Fa,oE),s(gs,rE),p(e,Cu,i),f(dn,e,i),p(e,Du,i),p(e,Jn,i),s(Jn,lE),p(e,Lu,i),f(fn,e,i),p(e,Ou,i),f(kn,e,i),p(e,Mu,i),p(e,w,i),s(w,aE),s(w,Ua),s(Ua,iE),s(w,pE),s(w,Ra),s(Ra,uE),s(w,cE),s(w,Wa),s(Wa,mE),s(w,dE),s(w,Ga),s(Ga,fE),s(w,kE),s(w,Ia),s(Ia,vE),s(w,_E),s(w,Va),s(Va,hE),s(w,EE),s(w,Xa),s(Xa,zE),s(w,$E),s(w,Ka),s(Ka,qE),s(w,jE),p(e,yu,i),f(vn,e,i),p(e,Su,i),f(_n,e,i),p(e,Nu,i),p(e,Yn,i),s(Yn,xE),p(e,Bu,i),f(hn,e,i),p(e,Au,i),p(e,Zn,i),s(Zn,bE),p(e,Fu,i),f(En,e,i),p(e,Uu,i),f(zn,e,i),p(e,Ru,i),p(e,Ps,i),s(Ps,gE),s(Ps,Ha),s(Ha,PE),s(Ps,wE),p(e,Wu,i),f($n,e,i),p(e,Gu,i),p(e,D,i),s(D,TE),s(D,Ja),s(Ja,CE),s(D,DE),s(D,Ya),s(Ya,LE),s(D,OE),s(D,Za),s(Za,ME),s(D,yE),s(D,Qa),s(Qa,SE),s(D,NE),s(D,ei),s(ei,BE),s(D,AE),s(D,si),s(si,FE),s(D,UE),s(D,ti),s(ti,RE),s(D,WE),p(e,Iu,i),f(qn,e,i),p(e,Vu,i),p(e,Qn,i),s(Qn,GE),p(e,Xu,i),f(jn,e,i),p(e,Ku,i),p(e,Z,i),s(Z,IE),s(Z,ni),s(ni,VE),s(Z,XE),s(Z,oi),s(oi,KE),s(Z,HE),s(Z,ri),s(ri,JE),s(Z,YE),s(Z,li),s(li,ZE),s(Z,QE),Hu=!0},p(e,[i]){const xn={};i&2&&(xn.$$scope={dirty:i,ctx:e}),cs.$set(xn)},i(e){Hu||(k(qe.$$.fragment,e),k(re.$$.fragment,e),k(Ks.$$.fragment,e),k(tt.$$.fragment,e),k(ot.$$.fragment,e),k(rt.$$.fragment,e),k(lt.$$.fragment,e),k(it.$$.fragment,e),k(pt.$$.fragment,e),k(ut.$$.fragment,e),k(ct.$$.fragment,e),k(mt.$$.fragment,e),k(cs.$$.fragment,e),k(dt.$$.fragment,e),k(ft.$$.fragment,e),k(kt.$$.fragment,e),k(vt.$$.fragment,e),k(_t.$$.fragment,e),k(ht.$$.fragment,e),k(Et.$$.fragment,e),k(zt.$$.fragment,e),k($t.$$.fragment,e),k(qt.$$.fragment,e),k(jt.$$.fragment,e),k(xt.$$.fragment,e),k(bt.$$.fragment,e),k(gt.$$.fragment,e),k(Pt.$$.fragment,e),k(wt.$$.fragment,e),k(Tt.$$.fragment,e),k(Ct.$$.fragment,e),k(Dt.$$.fragment,e),k(Lt.$$.fragment,e),k(Ot.$$.fragment,e),k(Mt.$$.fragment,e),k(yt.$$.fragment,e),k(St.$$.fragment,e),k(Nt.$$.fragment,e),k(Bt.$$.fragment,e),k(At.$$.fragment,e),k(Ft.$$.fragment,e),k(Rt.$$.fragment,e),k(Wt.$$.fragment,e),k(Gt.$$.fragment,e),k(It.$$.fragment,e),k(Vt.$$.fragment,e),k(Xt.$$.fragment,e),k(Kt.$$.fragment,e),k(Ht.$$.fragment,e),k(Jt.$$.fragment,e),k(Yt.$$.fragment,e),k(Zt.$$.fragment,e),k(Qt.$$.fragment,e),k(en.$$.fragment,e),k(sn.$$.fragment,e),k(tn.$$.fragment,e),k(nn.$$.fragment,e),k(on.$$.fragment,e),k(ln.$$.fragment,e),k(an.$$.fragment,e),k(pn.$$.fragment,e),k(un.$$.fragment,e),k(cn.$$.fragment,e),k(mn.$$.fragment,e),k(dn.$$.fragment,e),k(fn.$$.fragment,e),k(kn.$$.fragment,e),k(vn.$$.fragment,e),k(_n.$$.fragment,e),k(hn.$$.fragment,e),k(En.$$.fragment,e),k(zn.$$.fragment,e),k($n.$$.fragment,e),k(qn.$$.fragment,e),k(jn.$$.fragment,e),Hu=!0)},o(e){v(qe.$$.fragment,e),v(re.$$.fragment,e),v(Ks.$$.fragment,e),v(tt.$$.fragment,e),v(ot.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ut.$$.fragment,e),v(ct.$$.fragment,e),v(mt.$$.fragment,e),v(cs.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(kt.$$.fragment,e),v(vt.$$.fragment,e),v(_t.$$.fragment,e),v(ht.$$.fragment,e),v(Et.$$.fragment,e),v(zt.$$.fragment,e),v($t.$$.fragment,e),v(qt.$$.fragment,e),v(jt.$$.fragment,e),v(xt.$$.fragment,e),v(bt.$$.fragment,e),v(gt.$$.fragment,e),v(Pt.$$.fragment,e),v(wt.$$.fragment,e),v(Tt.$$.fragment,e),v(Ct.$$.fragment,e),v(Dt.$$.fragment,e),v(Lt.$$.fragment,e),v(Ot.$$.fragment,e),v(Mt.$$.fragment,e),v(yt.$$.fragment,e),v(St.$$.fragment,e),v(Nt.$$.fragment,e),v(Bt.$$.fragment,e),v(At.$$.fragment,e),v(Ft.$$.fragment,e),v(Rt.$$.fragment,e),v(Wt.$$.fragment,e),v(Gt.$$.fragment,e),v(It.$$.fragment,e),v(Vt.$$.fragment,e),v(Xt.$$.fragment,e),v(Kt.$$.fragment,e),v(Ht.$$.fragment,e),v(Jt.$$.fragment,e),v(Yt.$$.fragment,e),v(Zt.$$.fragment,e),v(Qt.$$.fragment,e),v(en.$$.fragment,e),v(sn.$$.fragment,e),v(tn.$$.fragment,e),v(nn.$$.fragment,e),v(on.$$.fragment,e),v(ln.$$.fragment,e),v(an.$$.fragment,e),v(pn.$$.fragment,e),v(un.$$.fragment,e),v(cn.$$.fragment,e),v(mn.$$.fragment,e),v(dn.$$.fragment,e),v(fn.$$.fragment,e),v(kn.$$.fragment,e),v(vn.$$.fragment,e),v(_n.$$.fragment,e),v(hn.$$.fragment,e),v(En.$$.fragment,e),v(zn.$$.fragment,e),v($n.$$.fragment,e),v(qn.$$.fragment,e),v(jn.$$.fragment,e),Hu=!1},d(e){t(z),e&&t(ge),e&&t(oe),_(qe),e&&t(ss),_(re,e),e&&t(ts),e&&t(Pe),e&&t(ns),e&&t(R),e&&t(pi),e&&t(yn),e&&t(ui),e&&t(Je),e&&t(ci),e&&t(W),e&&t(mi),_(Ks,e),e&&t(di),e&&t(ls),e&&t(fi),e&&t(G),e&&t(ki),e&&t(as),e&&t(vi),e&&t(Ye),_(tt),e&&t(_i),e&&t(pe),e&&t(hi),_(ot,e),e&&t(Ei),e&&t(Oe),e&&t(zi),e&&t(ps),e&&t($i),_(rt,e),e&&t(qi),e&&t(ue),e&&t(ji),e&&t(Ze),_(lt),e&&t(xi),e&&t(b),e&&t(bi),e&&t(Me),e&&t(gi),_(it,e),e&&t(Pi),e&&t(ce),e&&t(wi),e&&t(g),e&&t(Ti),_(pt,e),e&&t(Ci),e&&t(V),e&&t(Di),_(ut,e),e&&t(Li),e&&t(ye),e&&t(Oi),e&&t(Se),e&&t(Mi),_(ct,e),e&&t(yi),_(mt,e),e&&t(Si),_(cs,e),e&&t(Ni),e&&t(ms),e&&t(Bi),_(dt,e),e&&t(Ai),e&&t(Bn),e&&t(Fi),_(ft,e),e&&t(Ui),e&&t(ds),e&&t(Ri),_(kt,e),e&&t(Wi),_(vt,e),e&&t(Gi),e&&t(fs),e&&t(Ii),_(_t,e),e&&t(Vi),_(ht,e),e&&t(Xi),e&&t(ks),e&&t(Ki),_(Et,e),e&&t(Hi),_(zt,e),e&&t(Ji),e&&t(me),e&&t(Yi),_($t,e),e&&t(Zi),e&&t(O),e&&t(Qi),e&&t(An),e&&t(ep),_(qt,e),e&&t(sp),e&&t(Ne),e&&t(tp),_(jt,e),e&&t(np),e&&t(Be),e&&t(op),_(xt,e),e&&t(rp),_(bt,e),e&&t(lp),e&&t(j),e&&t(ap),e&&t(P),e&&t(ip),_(gt,e),e&&t(pp),_(Pt,e),e&&t(up),e&&t(M),e&&t(cp),e&&t(Fn),e&&t(mp),_(wt,e),e&&t(dp),e&&t(Ae),e&&t(fp),e&&t(Un),e&&t(kp),_(Tt,e),e&&t(vp),_(Ct,e),e&&t(_p),e&&t(Rn),e&&t(hp),_(Dt,e),e&&t(Ep),_(Lt,e),e&&t(zp),e&&t(vs),e&&t($p),_(Ot,e),e&&t(qp),e&&t(_s),e&&t(jp),_(Mt,e),e&&t(xp),_(yt,e),e&&t(bp),e&&t(hs),e&&t(gp),_(St,e),e&&t(Pp),e&&t(Fe),e&&t(wp),_(Nt,e),e&&t(Tp),e&&t(y),e&&t(Cp),e&&t($),e&&t(Dp),_(Bt,e),e&&t(Lp),e&&t(de),e&&t(Op),_(At,e),e&&t(Mp),e&&t(S),e&&t(yp),e&&t(Ue),e&&t(Sp),e&&t(Qe),_(Ft),e&&t(Np),e&&t(fe),e&&t(Bp),_(Rt,e),e&&t(Ap),e&&t(ke),e&&t(Fp),e&&t(Wn),e&&t(Up),_(Wt,e),e&&t(Rp),e&&t(zs),e&&t(Wp),_(Gt,e),e&&t(Gp),_(It,e),e&&t(Ip),e&&t(Re),e&&t(Vp),_(Vt,e),e&&t(Xp),e&&t(N),e&&t(Kp),e&&t($s),e&&t(Hp),_(Xt,e),e&&t(Jp),e&&t(Gn),e&&t(Yp),_(Kt,e),e&&t(Zp),_(Ht,e),e&&t(Qp),e&&t(qs),e&&t(eu),_(Jt,e),e&&t(su),e&&t(T),e&&t(tu),_(Yt,e),e&&t(nu),_(Zt,e),e&&t(ou),e&&t(In),e&&t(ru),_(Qt,e),e&&t(lu),e&&t(Vn),e&&t(au),_(en,e),e&&t(iu),_(sn,e),e&&t(pu),e&&t(ve),e&&t(uu),_(tn,e),e&&t(cu),e&&t(Xn),e&&t(mu),_(nn,e),e&&t(du),e&&t(We),e&&t(fu),e&&t(es),_(on),e&&t(ku),e&&t(Y),e&&t(vu),_(ln,e),e&&t(_u),e&&t(Kn),e&&t(hu),e&&t(xs),e&&t(Eu),_(an,e),e&&t(zu),e&&t(_e),e&&t($u),e&&t(Ge),e&&t(qu),_(pn,e),e&&t(ju),e&&t(Hn),e&&t(xu),_(un,e),e&&t(bu),_(cn,e),e&&t(gu),e&&t(bs),e&&t(Pu),_(mn,e),e&&t(wu),e&&t(C),e&&t(Tu),e&&t(gs),e&&t(Cu),_(dn,e),e&&t(Du),e&&t(Jn),e&&t(Lu),_(fn,e),e&&t(Ou),_(kn,e),e&&t(Mu),e&&t(w),e&&t(yu),_(vn,e),e&&t(Su),_(_n,e),e&&t(Nu),e&&t(Yn),e&&t(Bu),_(hn,e),e&&t(Au),e&&t(Zn),e&&t(Fu),_(En,e),e&&t(Uu),_(zn,e),e&&t(Ru),e&&t(Ps),e&&t(Wu),_($n,e),e&&t(Gu),e&&t(D),e&&t(Iu),_(qn,e),e&&t(Vu),e&&t(Qn),e&&t(Xu),_(jn,e),e&&t(Ku),e&&t(Z)}}}const Xq={local:"construction-dun-tokenizer-bloc-par-bloc",sections:[{local:"acquisition-dun-corpus",title:"Acquisition d'un corpus"},{local:"construire-un-itokenizer-wordpiecei-partir-de-zro",title:"Construire un <i>tokenizer WordPiece</i> \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizeri-bpe-partir-de-zro",title:"Construire un <i>tokenizer</i> BPE \xE0 partir de z\xE9ro"},{local:"construire-un-itokenizer-unigrami-partir-de-zro",title:"Construire un <i>tokenizer Unigram</i> \xE0 partir de z\xE9ro"}],title:"Construction d'un *tokenizer*, bloc par bloc"};function Kq(ii){return Uq(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sj extends Nq{constructor(z){super();Bq(this,z,Kq,Vq,Aq,{})}}export{sj as default,Xq as metadata};
