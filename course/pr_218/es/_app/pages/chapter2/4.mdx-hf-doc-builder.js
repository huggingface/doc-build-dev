import{S as Bi,i as Gi,s as Ri,e as t,k as d,w as k,t as i,M as Ji,c as n,d as a,m as c,x as h,a as l,h as p,b as u,N as Ma,G as o,g as r,y as $,o as b,p as Fi,q as v,B as _,v as Yi,n as Vi}from"../../chunks/vendor-hf-doc-builder.js";import{T as Wi}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Uo}from"../../chunks/Youtube-hf-doc-builder.js";import{I as G}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Ui}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Ki}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Qi(D){let m,E;return m=new Ui({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){k(m.$$.fragment)},l(f){h(m.$$.fragment,f)},m(f,y){$(m,f,y),E=!0},i(f){E||(v(m.$$.fragment,f),E=!0)},o(f){b(m.$$.fragment,f),E=!1},d(f){_(m,f)}}}function Xi(D){let m,E;return m=new Ui({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){k(m.$$.fragment)},l(f){h(m.$$.fragment,f)},m(f,y){$(m,f,y),E=!0},i(f){E||(v(m.$$.fragment,f),E=!0)},o(f){b(m.$$.fragment,f),E=!1},d(f){_(m,f)}}}function Zi(D){let m,E,f,y,q,z,w,j;return{c(){m=t("p"),E=i("Al igual que "),f=t("code"),y=i("TFAutoModel"),q=i(", la clase "),z=t("code"),w=i("AutoTokenizer"),j=i(" tomar\xE1 la clase de tokenizador adecuada en la biblioteca basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:")},l(P){m=n(P,"P",{});var g=l(m);E=p(g,"Al igual que "),f=n(g,"CODE",{});var R=l(f);y=p(R,"TFAutoModel"),R.forEach(a),q=p(g,", la clase "),z=n(g,"CODE",{});var L=l(z);w=p(L,"AutoTokenizer"),L.forEach(a),j=p(g," tomar\xE1 la clase de tokenizador adecuada en la biblioteca basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:"),g.forEach(a)},m(P,g){r(P,m,g),o(m,E),o(m,f),o(f,y),o(m,q),o(m,z),o(z,w),o(m,j)},d(P){P&&a(m)}}}function ep(D){let m,E,f,y,q,z,w,j;return{c(){m=t("p"),E=i("Al igual que "),f=t("code"),y=i("AutoModel"),q=i(", la clase "),z=t("code"),w=i("AutoTokenizer"),j=i(" tomar\xE1 la clase de tokenizador adecuada en la biblioteca basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:")},l(P){m=n(P,"P",{});var g=l(m);E=p(g,"Al igual que "),f=n(g,"CODE",{});var R=l(f);y=p(R,"AutoModel"),R.forEach(a),q=p(g,", la clase "),z=n(g,"CODE",{});var L=l(z);w=p(L,"AutoTokenizer"),L.forEach(a),j=p(g," tomar\xE1 la clase de tokenizador adecuada en la biblioteca basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:"),g.forEach(a)},m(P,g){r(P,m,g),o(m,E),o(m,f),o(f,y),o(m,q),o(m,z),o(z,w),o(m,j)},d(P){P&&a(m)}}}function ap(D){let m,E,f,y,q;return{c(){m=t("p"),E=i("\u270F\uFE0F "),f=t("strong"),y=i("Try it out!"),q=i(" Replica los dos \xFAltimos pasos (tokenizaci\xF3n y conversi\xF3n a IDs de entrada) en las frases de entrada que utilizamos en la secci\xF3n 2 (\u201CLlevo toda la vida esperando un curso de HuggingFace\u201D y \u201C\xA1Odio tanto esto!\u201D). Comprueba que obtienes los mismos ID de entrada que obtuvimos antes!")},l(z){m=n(z,"P",{});var w=l(m);E=p(w,"\u270F\uFE0F "),f=n(w,"STRONG",{});var j=l(f);y=p(j,"Try it out!"),j.forEach(a),q=p(w," Replica los dos \xFAltimos pasos (tokenizaci\xF3n y conversi\xF3n a IDs de entrada) en las frases de entrada que utilizamos en la secci\xF3n 2 (\u201CLlevo toda la vida esperando un curso de HuggingFace\u201D y \u201C\xA1Odio tanto esto!\u201D). Comprueba que obtienes los mismos ID de entrada que obtuvimos antes!"),w.forEach(a)},m(z,w){r(z,m,w),o(m,E),o(m,f),o(f,y),o(m,q)},d(z){z&&a(m)}}}function op(D){let m,E,f,y,q,z,w,j,P,g,R,L,S,N,ca,Pe,Bo,ua,Sr,Go,ma,Nr,Ro,xe,Jo,fa,Lr,Fo,ba,Ir,Yo,J,re,Ua,Ae,Or,Ba,Hr,Vo,Te,Wo,I,Mr,Ga,Ur,Br,Ra,Gr,Rr,Ko,F,De,ol,Jr,Ce,sl,Qo,te,Fr,Ja,Yr,Vr,Xo,Se,Zo,Ne,es,va,Wr,as,ka,Kr,os,ha,Qr,ss,Le,Xr,Fa,Zr,rs,ne,et,Ya,at,ot,ts,Y,le,Va,Ie,st,Wa,rt,ns,Oe,ls,ie,tt,Ka,nt,lt,is,pe,Qa,it,pt,Xa,dt,ps,$a,ct,ds,V,He,rl,ut,Me,tl,cs,_a,mt,us,de,ft,Za,bt,vt,ms,ce,kt,eo,ht,$t,fs,W,ue,ao,Ue,_t,oo,Et,bs,Be,vs,Ea,zt,ks,za,gt,hs,ga,yt,$s,K,Ge,nl,qt,Re,ll,_s,ya,wt,Es,qa,jt,zs,Q,me,so,Je,Pt,ro,xt,gs,wa,At,ys,O,to,Tt,Dt,no,Ct,St,lo,Nt,qs,ja,Lt,ws,X,fe,io,Fe,It,po,Ot,js,x,Ht,co,Mt,Ut,uo,Bt,Gt,mo,Rt,Jt,fo,Ft,Yt,Ps,be,Vt,bo,Wt,Kt,xs,Ye,As,Pa,Ve,Ts,xa,Qt,Ds,We,Cs,Ke,Ss,Aa,Xt,Ns,Qe,Ls,A,Zt,vo,en,an,Ta,on,sn,ko,rn,tn,ho,nn,ln,Is,Z,ve,$o,Xe,pn,_o,dn,Os,Ze,Hs,ke,cn,Eo,un,mn,Ms,he,fn,zo,bn,vn,Us,H,kn,go,hn,$n,yo,_n,En,Bs,$e,zn,qo,gn,yn,Gs,ee,_e,wo,ea,qn,jo,wn,Rs,Ee,jn,Po,Pn,xn,Js,aa,Fs,Da,An,Ys,oa,Vs,C,Tn,xo,Dn,Cn,Ao,Sn,Nn,To,Ln,In,Ws,ae,ze,Do,sa,On,Co,Hn,Ks,ge,Mn,So,Un,Bn,Qs,ra,Xs,ta,Zs,Ca,Gn,er,ye,ar,oe,qe,No,na,Rn,Lo,Jn,or,M,Fn,Io,Yn,Vn,Oo,Wn,Kn,sr,la,rr,ia,tr,we,Qn,Ho,Xn,Zn,nr,Sa,el,lr;f=new Ki({props:{fw:D[0]}}),j=new G({});const il=[Xi,Qi],pa=[];function pl(e,s){return e[0]==="pt"?0:1}S=pl(D),N=pa[S]=il[S](D),Pe=new Uo({props:{id:"VFp38yj8h3A"}}),xe=new T({props:{code:"Jim Henson era un titiritero",highlighted:'<span class="hljs-attribute">Jim Henson era un titiritero</span>'}}),Ae=new G({}),Te=new Uo({props:{id:"nhJxYji1aho"}}),Se=new T({props:{code:`tokenized_text = "Jim Henson era un titiritero".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson era un titiritero&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),Ne=new T({props:{code:"['Jim', 'Henson', 'era', 'un', 'titiritero']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;era&#x27;</span>, <span class="hljs-string">&#x27;un&#x27;</span>, <span class="hljs-string">&#x27;titiritero&#x27;</span>]'}}),Ie=new G({}),Oe=new Uo({props:{id:"ssLq_EK2jLE"}}),Ue=new G({}),Be=new Uo({props:{id:"zHvTiHr506c"}}),Je=new G({}),Fe=new G({}),Ye=new T({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function dl(e,s){return e[0]==="pt"?ep:Zi}let ir=dl(D),se=ir(D);return Ve=new T({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),We=new T({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),Ke=new T({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Qe=new T({props:{code:'tokenizer.save_pretrained("directorio_en_mi_computador")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directorio_en_mi_computador&quot;</span>)'}}),Xe=new G({}),Ze=new Uo({props:{id:"Yffk5aydLzg"}}),ea=new G({}),aa=new T({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),oa=new T({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),sa=new G({}),ra=new T({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),ta=new T({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),ye=new Wi({props:{$$slots:{default:[ap]},$$scope:{ctx:D}}}),na=new G({}),la=new T({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),ia=new T({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){m=t("meta"),E=d(),k(f.$$.fragment),y=d(),q=t("h1"),z=t("a"),w=t("span"),k(j.$$.fragment),P=d(),g=t("span"),R=i("Tokenizadores"),L=d(),N.c(),ca=d(),k(Pe.$$.fragment),Bo=d(),ua=t("p"),Sr=i("Los tokenizadores son uno de los componentes fundamentales del pipeline en NLP. Sirven para traducir texto en datos que los modelos puedan procesar; es decir, de texto a valores num\xE9ricos. En esta secci\xF3n veremos en qu\xE9 se fundamenta todo el proceso de tokenizado."),Go=d(),ma=t("p"),Nr=i("En las tareas de NLP, los datos generalmente ingresan como texto crudo. Por ejemplo:"),Ro=d(),k(xe.$$.fragment),Jo=d(),fa=t("p"),Lr=i("Sin embargo, necesitamos una forma de convertir el texto crudo a valores num\xE9ricos para los modelos. Eso es precisamente lo que hacen los tokenizadores, y existe una variedad de formas en que puede hacerse. El objetivo final es obetener valores que sean cortos pero muy significativos para el modelo."),Fo=d(),ba=t("p"),Ir=i("Veamos algunos algoritmos de tokenizaci\xF3n, e intentemos atacar algunas preguntas que puedas tener."),Yo=d(),J=t("h2"),re=t("a"),Ua=t("span"),k(Ae.$$.fragment),Or=d(),Ba=t("span"),Hr=i("Tokenizaci\xF3n Word-based"),Vo=d(),k(Te.$$.fragment),Wo=d(),I=t("p"),Mr=i("El primer tokenizador que nos ocurre es el "),Ga=t("em"),Ur=i("word-based"),Br=i(" ("),Ra=t("em"),Gr=i("basado-en-palabras"),Rr=i("). Es generalmente sencillo, con pocas normas, y generalmente da buenos resultados. Por ejemplo, en la imagen a continuaci\xF3n separamos el texto en palabras y buscamos una representaci\xF3n num\xE9rica."),Ko=d(),F=t("div"),De=t("img"),Jr=d(),Ce=t("img"),Qo=d(),te=t("p"),Fr=i("Existen varias formas de separar el texto. Por ejemp\u013Ao, podr\xEDamos usar los espacios para tokenizar usando Python y la funci\xF3n "),Ja=t("code"),Yr=i("split()"),Vr=i("."),Xo=d(),k(Se.$$.fragment),Zo=d(),k(Ne.$$.fragment),es=d(),va=t("p"),Wr=i("Tambi\xE9n hay variaciones de tokenizadores de palabras que tienen reglas adicionales para la puntuaci\xF3n. Con este tipo de tokenizador, podemos acabar con unos \u201Cvocabularios\u201D bastante grandes, donde un vocabulario se define por el n\xFAmero total de tokens independientes que tenemos en nuestro corpus."),as=d(),ka=t("p"),Kr=i("A cada palabra se le asigna un ID, empezando por 0 y subiendo hasta el tama\xF1o del vocabulario. El modelo utiliza estos ID para identificar cada palabra."),os=d(),ha=t("p"),Qr=i("Si queremos cubrir completamente un idioma con un tokenizador basado en palabras, necesitaremos tener un identificador para cada palabra del idioma, lo que generar\xE1 una enorme cantidad de tokens. Por ejemplo, hay m\xE1s de 500.000 palabras en el idioma ingl\xE9s, por lo que para construir un mapa de cada palabra a un identificador de entrada necesitar\xEDamos hacer un seguimiento de esa cantidad de identificadores. Adem\xE1s, palabras como \u201Cperro\u201D se representan de forma diferente a palabras como \u201Cperros\u201D, y el modelo no tendr\xE1 forma de saber que \u201Cperro\u201D y \u201Cperros\u201D son similares: identificar\xE1 las dos palabras como no relacionadas. Lo mismo ocurre con otras palabras similares, como \u201Ccorrer\u201D y \u201Ccorriendo\u201D, que el modelo no ver\xE1 inicialmente como similares."),ss=d(),Le=t("p"),Xr=i("Por \xFAltimo, necesitamos un token personalizado para representar palabras que no est\xE1n en nuestro vocabulario. Esto se conoce como el token \u201Cdesconocido\u201D, a menudo representado como \u201D[UNK]\u201D o \u201D"),Fa=t("unk"),Zr=i("\u201D. Generalmente, si el tokenizador est\xE1 produciendo muchos de estos tokens es una mala se\xF1al, ya que no fue capaz de recuperar una representaci\xF3n de alguna palabra y est\xE1 perdiendo informaci\xF3n en el proceso. El objetivo al elaborar el vocabulario es hacerlo de tal manera que el tokenizador tokenice el menor n\xFAmero de palabras posibles en tokens desconocidos."),rs=d(),ne=t("p"),et=i("Una forma de reducir la cantidad de tokens desconocidos es ir un poco m\xE1s all\xE1, utilizando un tokenizador "),Ya=t("em"),at=i("word-based"),ot=i("."),ts=d(),Y=t("h2"),le=t("a"),Va=t("span"),k(Ie.$$.fragment),st=d(),Wa=t("span"),rt=i("Tokenizaci\xF3n Character-based"),ns=d(),k(Oe.$$.fragment),ls=d(),ie=t("p"),tt=i(`Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:
Un tokenizador `),Ka=t("em"),nt=i("character-based"),lt=i(" separa el texto en caracteres, y no en palabras. Conllevando dos beneficios principales:"),is=d(),pe=t("ul"),Qa=t("li"),it=i("Obtenemos un vocabulario mucho m\xE1s corto."),pt=d(),Xa=t("li"),dt=i("Habr\xE1 muchos menos tokens por fuera del vocabulatio conocido."),ps=d(),$a=t("p"),ct=i("No obstante, pueden surgir incovenientes por los espacios en blanco y signos de puntuaci\xF3n."),ds=d(),V=t("div"),He=t("img"),ut=d(),Me=t("img"),cs=d(),_a=t("p"),mt=i("As\xED, este m\xE9todo tampoco es perfecto. Dada que la representaci\xF3n se construy\xF3 con caracteres, uno podr\xEDa pensar intuitivamente que resulta menos significativo: Cada una de las palabras no significa mucho por separado, mientras que las palabras s\xED. Sin embargo, eso es dependiente del idioma. Por ejemplo en Chino, cada uno de los caracteres conlleva m\xE1s informaci\xF3n que en un idioma latino."),us=d(),de=t("p"),ft=i("Otro aspecto a considerar es que terminamos con una gran cantidad de tokens que el modelo debe procesar, mientras que en el caso del tokenizador "),Za=t("em"),bt=i("word-based"),vt=i(", un token representa una palabra, en la representaci\xF3n de caracteres f\xE1cilmente puede necesitar m\xE1s de 10 tokens."),ms=d(),ce=t("p"),kt=i("Para obtener lo mejor de ambos mundos, podemos usar una combinaci\xF3n de las t\xE9cnicas: la tokenizaci\xF3n por "),eo=t("em"),ht=i("subword tokenization"),$t=i("."),fs=d(),W=t("h2"),ue=t("a"),ao=t("span"),k(Ue.$$.fragment),_t=d(),oo=t("span"),Et=i("Tokenizaci\xF3n por Subword"),bs=d(),k(Be.$$.fragment),vs=d(),Ea=t("p"),zt=i("Los algoritmos de tokenizaci\xF3n de subpalabras se basan en el principio de que las palabras de uso frecuente no deben dividirse, mientras que las palabras raras deben descomponerse en subpalabras significativas."),ks=d(),za=t("p"),gt=i("Por ejemplo, \u201Cextra\xF1amente\u201D podr\xEDa considerarse una palabra rara y podr\xEDa descomponerse en \u201Cextra\xF1a\u201D y \u201Cmente\u201D. Es probable que ambas aparezcan con m\xE1s frecuencia como subpalabras independientes, mientras que al mismo tiempo el significado de \u201Cextra\xF1amente\u201D se mantiene por el significado compuesto de \u201Cextra\xF1a\u201D y \u201Cmente\u201D."),hs=d(),ga=t("p"),yt=i("Este es un ejemplo que muestra c\xF3mo un algoritmo de tokenizaci\xF3n de subpalabras tokenizar\xEDa la secuencia \u201CLet\u2019s do tokenization!\u201C:"),$s=d(),K=t("div"),Ge=t("img"),qt=d(),Re=t("img"),_s=d(),ya=t("p"),wt=i("Estas subpalabras terminan aportando mucho significado sem\xE1ntico: por ejemplo, en el ejemplo anterior, \u201Ctokenizaci\xF3n\u201D se dividi\xF3 en \u201Ctoken\u201D y \u201Cizaci\xF3n\u201D, dos tokens que tienen un significado sem\xE1ntico y a la vez son eficientes en cuanto al espacio (s\xF3lo se necesitan dos tokens para representar una palabra larga). Esto nos permite tener una cobertura relativamente buena con vocabularios peque\xF1os y casi sin tokens desconocidos."),Es=d(),qa=t("p"),jt=i("Este enfoque es especialmente \xFAtil en algunos idimas como el turco, donde se pueden formar palabras complejas (casi) arbitrariamente largas encadenando subpalabras."),zs=d(),Q=t("h3"),me=t("a"),so=t("span"),k(Je.$$.fragment),Pt=d(),ro=t("span"),xt=i("Y m\xE1s!"),gs=d(),wa=t("p"),At=i("Como es l\xF3gico, existen muchas m\xE1s t\xE9cnicas. Por nombrar algunas:"),ys=d(),O=t("ul"),to=t("li"),Tt=i("Byte-level BPE (a nivel de bytes), como usa GPT-2"),Dt=d(),no=t("li"),Ct=i("WordPiece, usado por BERT"),St=d(),lo=t("li"),Nt=i("SentencePiece or Unigram (pedazo de sentencia o unigrama), como se usa en los modelos multilengua"),qs=d(),ja=t("p"),Lt=i("A este punto, deber\xEDas tener conocimientos suficientes sobre el funcionamiento de los tokenizadores para empezar a utilizar la API."),ws=d(),X=t("h2"),fe=t("a"),io=t("span"),k(Fe.$$.fragment),It=d(),po=t("span"),Ot=i("Cargando y guardando"),js=d(),x=t("p"),Ht=i("Cargar y guardar tokenizadores es tan sencillo como lo es con los modelos. En realidad, se basa en los mismos dos m\xE9todos: "),co=t("code"),Mt=i("from_pretrained()"),Ut=i(" y "),uo=t("code"),Bt=i("save_pretrained()"),Gt=i(". Estos m\xE9todos cargar\xE1n o guardar\xE1n el algoritmo utilizado por el tokenizador (un poco como la "),mo=t("em"),Rt=i("arquitectura"),Jt=i(" del modelo) as\xED como su vocabulario (un poco como los "),fo=t("em"),Ft=i("pesos"),Yt=i(" del modelo)."),Ps=d(),be=t("p"),Vt=i("La carga del tokenizador BERT entrenado con el mismo punto de control que BERT se realiza de la misma manera que la carga del modelo, excepto que utilizamos la clase "),bo=t("code"),Wt=i("BertTokenizer"),Kt=i(":"),xs=d(),k(Ye.$$.fragment),As=d(),se.c(),Pa=d(),k(Ve.$$.fragment),Ts=d(),xa=t("p"),Qt=i("Ahora podemos utilizar el tokenizador como se muestra en la secci\xF3n anterior:"),Ds=d(),k(We.$$.fragment),Cs=d(),k(Ke.$$.fragment),Ss=d(),Aa=t("p"),Xt=i("Guardar un tokenizador es id\xE9ntico a guardar un modelo:"),Ns=d(),k(Qe.$$.fragment),Ls=d(),A=t("p"),Zt=i("Hablaremos m\xE1s sobre "),vo=t("code"),en=i("token_type_ids"),an=i(" en el "),Ta=t("a"),on=i("Cap\xEDtulo 3"),sn=i(", y explicaremos la clave "),ko=t("code"),rn=i("attention_mask"),tn=i(" un poco m\xE1s tarde. Primero, veamos c\xF3mo se generan los "),ho=t("code"),nn=i("input_ids"),ln=i(". Para ello, tendremos que ver los m\xE9todos intermedios del tokenizador."),Is=d(),Z=t("h2"),ve=t("a"),$o=t("span"),k(Xe.$$.fragment),pn=d(),_o=t("span"),dn=i("Encoding"),Os=d(),k(Ze.$$.fragment),Hs=d(),ke=t("p"),cn=i("La traducci\xF3n de texto a n\xFAmeros se conoce como "),Eo=t("em"),un=i("codificaci\xF3n"),mn=i(". La codificaci\xF3n se realiza en un proceso de dos pasos: la tokenizaci\xF3n, seguida de la conversi\xF3n a IDs de entrada."),Ms=d(),he=t("p"),fn=i("Como hemos visto, el primer paso es dividir el texto en palabras (o partes de palabras, s\xEDmbolos de puntuaci\xF3n, etc.), normalmente llamadas "),zo=t("em"),bn=i("tokens"),vn=i(". Hay m\xFAltiples reglas que pueden gobernar ese proceso, por lo que necesitamos instanciar el tokenizador usando el nombre del modelo, para asegurarnos de que usamos las mismas reglas que se usaron cuando se preentren\xF3 el modelo."),Us=d(),H=t("p"),kn=i("El segundo paso es convertir esos tokens en n\xFAmeros, para poder construir un tensor con ellos y alimentar el modelo. Para ello, el tokenizador tiene un "),go=t("em"),hn=i("vocabulario"),$n=i(", que es la parte que descargamos cuando lo instanciamos con el m\xE9todo "),yo=t("code"),_n=i("from_pretrained()"),En=i(". De nuevo, necesitamos usar el mismo vocabulario que se us\xF3 cuando el modelo fue preentrenado."),Bs=d(),$e=t("p"),zn=i("Para entender mejor los dos pasos, los exploraremos por separado. Ten en cuenta que utilizaremos algunos m\xE9todos que realizan partes del proceso de tokenizaci\xF3n por separado para mostrarte los resultados intermedios de esos pasos, pero en la pr\xE1ctica, deber\xEDas llamar al tokenizador directamente en tus "),qo=t("em"),gn=i("inputs"),yn=i(" (como se muestra en la secci\xF3n 2)."),Gs=d(),ee=t("h3"),_e=t("a"),wo=t("span"),k(ea.$$.fragment),qn=d(),jo=t("span"),wn=i("Tokenization"),Rs=d(),Ee=t("p"),jn=i("El proceso de tokenizaci\xF3n se realiza mediante el m\xE9todo "),Po=t("code"),Pn=i("tokenize()"),xn=i(" del tokenizador:"),Js=d(),k(aa.$$.fragment),Fs=d(),Da=t("p"),An=i("La salida de este m\xE9todo es una lista de cadenas, o tokens:"),Ys=d(),k(oa.$$.fragment),Vs=d(),C=t("p"),Tn=i("Este tokenizador es un tokenizador de subpalabras: divide las palabras hasta obtener tokens que puedan ser representados por su vocabulario. Este es el caso de "),xo=t("code"),Dn=i("transformer"),Cn=i(", que se divide en dos tokens: "),Ao=t("code"),Sn=i("transform"),Nn=i(" y "),To=t("code"),Ln=i("##er"),In=i("."),Ws=d(),ae=t("h3"),ze=t("a"),Do=t("span"),k(sa.$$.fragment),On=d(),Co=t("span"),Hn=i("De tokens a IDs de entrada"),Ks=d(),ge=t("p"),Mn=i("La conversi\xF3n a IDs de entrada se hace con el m\xE9todo del tokenizador "),So=t("code"),Un=i("convert_tokens_to_ids()"),Bn=i(":"),Qs=d(),k(ra.$$.fragment),Xs=d(),k(ta.$$.fragment),Zs=d(),Ca=t("p"),Gn=i("Estos resultados, una vez convertidos en el tensor del marco apropiado, pueden utilizarse como entradas de un modelo, como se ha visto anteriormente en este cap\xEDtulo."),er=d(),k(ye.$$.fragment),ar=d(),oe=t("h2"),qe=t("a"),No=t("span"),k(na.$$.fragment),Rn=d(),Lo=t("span"),Jn=i("Decodificaci\xF3n"),or=d(),M=t("p"),Fn=i("La "),Io=t("em"),Yn=i("decodificaci\xF3n"),Vn=i(" va al rev\xE9s: a partir de los \xEDndices del vocabulario, queremos obtener una cadena. Esto se puede hacer con el m\xE9todo "),Oo=t("code"),Wn=i("decode()"),Kn=i(" de la siguiente manera:"),sr=d(),k(la.$$.fragment),rr=d(),k(ia.$$.fragment),tr=d(),we=t("p"),Qn=i("Notemos que el m\xE9todo "),Ho=t("code"),Xn=i("decode"),Zn=i(" no s\xF3lo convierte los \xEDndices de nuevo en tokens, sino que tambi\xE9n agrupa los tokens que formaban parte de las mismas palabras para producir una frase legible. Este comportamiento ser\xE1 extremadamente \xFAtil cuando utilicemos modelos que predigan texto nuevo (ya sea texto generado a partir de una indicaci\xF3n, o para problemas de secuencia a secuencia como la traducci\xF3n o el resumen)."),nr=d(),Sa=t("p"),el=i("A estas alturas deber\xEDas entender las operaciones at\xF3micas que un tokenizador puede manejar: tokenizaci\xF3n, conversi\xF3n a IDs, y conversi\xF3n de IDs de vuelta a una cadena. Sin embargo, s\xF3lo hemos rozado la punta del iceberg. En la siguiente secci\xF3n, llevaremos nuestro enfoque a sus l\xEDmites y echaremos un vistazo a c\xF3mo superarlos."),this.h()},l(e){const s=Ji('[data-svelte="svelte-1phssyn"]',document.head);m=n(s,"META",{name:!0,content:!0}),s.forEach(a),E=c(e),h(f.$$.fragment,e),y=c(e),q=n(e,"H1",{class:!0});var da=l(q);z=n(da,"A",{id:!0,class:!0,href:!0});var Na=l(z);w=n(Na,"SPAN",{});var Mo=l(w);h(j.$$.fragment,Mo),Mo.forEach(a),Na.forEach(a),P=c(da),g=n(da,"SPAN",{});var cl=l(g);R=p(cl,"Tokenizadores"),cl.forEach(a),da.forEach(a),L=c(e),N.l(e),ca=c(e),h(Pe.$$.fragment,e),Bo=c(e),ua=n(e,"P",{});var ul=l(ua);Sr=p(ul,"Los tokenizadores son uno de los componentes fundamentales del pipeline en NLP. Sirven para traducir texto en datos que los modelos puedan procesar; es decir, de texto a valores num\xE9ricos. En esta secci\xF3n veremos en qu\xE9 se fundamenta todo el proceso de tokenizado."),ul.forEach(a),Go=c(e),ma=n(e,"P",{});var ml=l(ma);Nr=p(ml,"En las tareas de NLP, los datos generalmente ingresan como texto crudo. Por ejemplo:"),ml.forEach(a),Ro=c(e),h(xe.$$.fragment,e),Jo=c(e),fa=n(e,"P",{});var fl=l(fa);Lr=p(fl,"Sin embargo, necesitamos una forma de convertir el texto crudo a valores num\xE9ricos para los modelos. Eso es precisamente lo que hacen los tokenizadores, y existe una variedad de formas en que puede hacerse. El objetivo final es obetener valores que sean cortos pero muy significativos para el modelo."),fl.forEach(a),Fo=c(e),ba=n(e,"P",{});var bl=l(ba);Ir=p(bl,"Veamos algunos algoritmos de tokenizaci\xF3n, e intentemos atacar algunas preguntas que puedas tener."),bl.forEach(a),Yo=c(e),J=n(e,"H2",{class:!0});var pr=l(J);re=n(pr,"A",{id:!0,class:!0,href:!0});var vl=l(re);Ua=n(vl,"SPAN",{});var kl=l(Ua);h(Ae.$$.fragment,kl),kl.forEach(a),vl.forEach(a),Or=c(pr),Ba=n(pr,"SPAN",{});var hl=l(Ba);Hr=p(hl,"Tokenizaci\xF3n Word-based"),hl.forEach(a),pr.forEach(a),Vo=c(e),h(Te.$$.fragment,e),Wo=c(e),I=n(e,"P",{});var La=l(I);Mr=p(La,"El primer tokenizador que nos ocurre es el "),Ga=n(La,"EM",{});var $l=l(Ga);Ur=p($l,"word-based"),$l.forEach(a),Br=p(La," ("),Ra=n(La,"EM",{});var _l=l(Ra);Gr=p(_l,"basado-en-palabras"),_l.forEach(a),Rr=p(La,"). Es generalmente sencillo, con pocas normas, y generalmente da buenos resultados. Por ejemplo, en la imagen a continuaci\xF3n separamos el texto en palabras y buscamos una representaci\xF3n num\xE9rica."),La.forEach(a),Ko=c(e),F=n(e,"DIV",{class:!0});var dr=l(F);De=n(dr,"IMG",{class:!0,src:!0,alt:!0}),Jr=c(dr),Ce=n(dr,"IMG",{class:!0,src:!0,alt:!0}),dr.forEach(a),Qo=c(e),te=n(e,"P",{});var cr=l(te);Fr=p(cr,"Existen varias formas de separar el texto. Por ejemp\u013Ao, podr\xEDamos usar los espacios para tokenizar usando Python y la funci\xF3n "),Ja=n(cr,"CODE",{});var El=l(Ja);Yr=p(El,"split()"),El.forEach(a),Vr=p(cr,"."),cr.forEach(a),Xo=c(e),h(Se.$$.fragment,e),Zo=c(e),h(Ne.$$.fragment,e),es=c(e),va=n(e,"P",{});var zl=l(va);Wr=p(zl,"Tambi\xE9n hay variaciones de tokenizadores de palabras que tienen reglas adicionales para la puntuaci\xF3n. Con este tipo de tokenizador, podemos acabar con unos \u201Cvocabularios\u201D bastante grandes, donde un vocabulario se define por el n\xFAmero total de tokens independientes que tenemos en nuestro corpus."),zl.forEach(a),as=c(e),ka=n(e,"P",{});var gl=l(ka);Kr=p(gl,"A cada palabra se le asigna un ID, empezando por 0 y subiendo hasta el tama\xF1o del vocabulario. El modelo utiliza estos ID para identificar cada palabra."),gl.forEach(a),os=c(e),ha=n(e,"P",{});var yl=l(ha);Qr=p(yl,"Si queremos cubrir completamente un idioma con un tokenizador basado en palabras, necesitaremos tener un identificador para cada palabra del idioma, lo que generar\xE1 una enorme cantidad de tokens. Por ejemplo, hay m\xE1s de 500.000 palabras en el idioma ingl\xE9s, por lo que para construir un mapa de cada palabra a un identificador de entrada necesitar\xEDamos hacer un seguimiento de esa cantidad de identificadores. Adem\xE1s, palabras como \u201Cperro\u201D se representan de forma diferente a palabras como \u201Cperros\u201D, y el modelo no tendr\xE1 forma de saber que \u201Cperro\u201D y \u201Cperros\u201D son similares: identificar\xE1 las dos palabras como no relacionadas. Lo mismo ocurre con otras palabras similares, como \u201Ccorrer\u201D y \u201Ccorriendo\u201D, que el modelo no ver\xE1 inicialmente como similares."),yl.forEach(a),ss=c(e),Le=n(e,"P",{});var al=l(Le);Xr=p(al,"Por \xFAltimo, necesitamos un token personalizado para representar palabras que no est\xE1n en nuestro vocabulario. Esto se conoce como el token \u201Cdesconocido\u201D, a menudo representado como \u201D[UNK]\u201D o \u201D"),Fa=n(al,"UNK",{});var ql=l(Fa);Zr=p(ql,"\u201D. Generalmente, si el tokenizador est\xE1 produciendo muchos de estos tokens es una mala se\xF1al, ya que no fue capaz de recuperar una representaci\xF3n de alguna palabra y est\xE1 perdiendo informaci\xF3n en el proceso. El objetivo al elaborar el vocabulario es hacerlo de tal manera que el tokenizador tokenice el menor n\xFAmero de palabras posibles en tokens desconocidos."),ql.forEach(a),al.forEach(a),rs=c(e),ne=n(e,"P",{});var ur=l(ne);et=p(ur,"Una forma de reducir la cantidad de tokens desconocidos es ir un poco m\xE1s all\xE1, utilizando un tokenizador "),Ya=n(ur,"EM",{});var wl=l(Ya);at=p(wl,"word-based"),wl.forEach(a),ot=p(ur,"."),ur.forEach(a),ts=c(e),Y=n(e,"H2",{class:!0});var mr=l(Y);le=n(mr,"A",{id:!0,class:!0,href:!0});var jl=l(le);Va=n(jl,"SPAN",{});var Pl=l(Va);h(Ie.$$.fragment,Pl),Pl.forEach(a),jl.forEach(a),st=c(mr),Wa=n(mr,"SPAN",{});var xl=l(Wa);rt=p(xl,"Tokenizaci\xF3n Character-based"),xl.forEach(a),mr.forEach(a),ns=c(e),h(Oe.$$.fragment,e),ls=c(e),ie=n(e,"P",{});var fr=l(ie);tt=p(fr,`Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:
Un tokenizador `),Ka=n(fr,"EM",{});var Al=l(Ka);nt=p(Al,"character-based"),Al.forEach(a),lt=p(fr," separa el texto en caracteres, y no en palabras. Conllevando dos beneficios principales:"),fr.forEach(a),is=c(e),pe=n(e,"UL",{});var br=l(pe);Qa=n(br,"LI",{});var Tl=l(Qa);it=p(Tl,"Obtenemos un vocabulario mucho m\xE1s corto."),Tl.forEach(a),pt=c(br),Xa=n(br,"LI",{});var Dl=l(Xa);dt=p(Dl,"Habr\xE1 muchos menos tokens por fuera del vocabulatio conocido."),Dl.forEach(a),br.forEach(a),ps=c(e),$a=n(e,"P",{});var Cl=l($a);ct=p(Cl,"No obstante, pueden surgir incovenientes por los espacios en blanco y signos de puntuaci\xF3n."),Cl.forEach(a),ds=c(e),V=n(e,"DIV",{class:!0});var vr=l(V);He=n(vr,"IMG",{class:!0,src:!0,alt:!0}),ut=c(vr),Me=n(vr,"IMG",{class:!0,src:!0,alt:!0}),vr.forEach(a),cs=c(e),_a=n(e,"P",{});var Sl=l(_a);mt=p(Sl,"As\xED, este m\xE9todo tampoco es perfecto. Dada que la representaci\xF3n se construy\xF3 con caracteres, uno podr\xEDa pensar intuitivamente que resulta menos significativo: Cada una de las palabras no significa mucho por separado, mientras que las palabras s\xED. Sin embargo, eso es dependiente del idioma. Por ejemplo en Chino, cada uno de los caracteres conlleva m\xE1s informaci\xF3n que en un idioma latino."),Sl.forEach(a),us=c(e),de=n(e,"P",{});var kr=l(de);ft=p(kr,"Otro aspecto a considerar es que terminamos con una gran cantidad de tokens que el modelo debe procesar, mientras que en el caso del tokenizador "),Za=n(kr,"EM",{});var Nl=l(Za);bt=p(Nl,"word-based"),Nl.forEach(a),vt=p(kr,", un token representa una palabra, en la representaci\xF3n de caracteres f\xE1cilmente puede necesitar m\xE1s de 10 tokens."),kr.forEach(a),ms=c(e),ce=n(e,"P",{});var hr=l(ce);kt=p(hr,"Para obtener lo mejor de ambos mundos, podemos usar una combinaci\xF3n de las t\xE9cnicas: la tokenizaci\xF3n por "),eo=n(hr,"EM",{});var Ll=l(eo);ht=p(Ll,"subword tokenization"),Ll.forEach(a),$t=p(hr,"."),hr.forEach(a),fs=c(e),W=n(e,"H2",{class:!0});var $r=l(W);ue=n($r,"A",{id:!0,class:!0,href:!0});var Il=l(ue);ao=n(Il,"SPAN",{});var Ol=l(ao);h(Ue.$$.fragment,Ol),Ol.forEach(a),Il.forEach(a),_t=c($r),oo=n($r,"SPAN",{});var Hl=l(oo);Et=p(Hl,"Tokenizaci\xF3n por Subword"),Hl.forEach(a),$r.forEach(a),bs=c(e),h(Be.$$.fragment,e),vs=c(e),Ea=n(e,"P",{});var Ml=l(Ea);zt=p(Ml,"Los algoritmos de tokenizaci\xF3n de subpalabras se basan en el principio de que las palabras de uso frecuente no deben dividirse, mientras que las palabras raras deben descomponerse en subpalabras significativas."),Ml.forEach(a),ks=c(e),za=n(e,"P",{});var Ul=l(za);gt=p(Ul,"Por ejemplo, \u201Cextra\xF1amente\u201D podr\xEDa considerarse una palabra rara y podr\xEDa descomponerse en \u201Cextra\xF1a\u201D y \u201Cmente\u201D. Es probable que ambas aparezcan con m\xE1s frecuencia como subpalabras independientes, mientras que al mismo tiempo el significado de \u201Cextra\xF1amente\u201D se mantiene por el significado compuesto de \u201Cextra\xF1a\u201D y \u201Cmente\u201D."),Ul.forEach(a),hs=c(e),ga=n(e,"P",{});var Bl=l(ga);yt=p(Bl,"Este es un ejemplo que muestra c\xF3mo un algoritmo de tokenizaci\xF3n de subpalabras tokenizar\xEDa la secuencia \u201CLet\u2019s do tokenization!\u201C:"),Bl.forEach(a),$s=c(e),K=n(e,"DIV",{class:!0});var _r=l(K);Ge=n(_r,"IMG",{class:!0,src:!0,alt:!0}),qt=c(_r),Re=n(_r,"IMG",{class:!0,src:!0,alt:!0}),_r.forEach(a),_s=c(e),ya=n(e,"P",{});var Gl=l(ya);wt=p(Gl,"Estas subpalabras terminan aportando mucho significado sem\xE1ntico: por ejemplo, en el ejemplo anterior, \u201Ctokenizaci\xF3n\u201D se dividi\xF3 en \u201Ctoken\u201D y \u201Cizaci\xF3n\u201D, dos tokens que tienen un significado sem\xE1ntico y a la vez son eficientes en cuanto al espacio (s\xF3lo se necesitan dos tokens para representar una palabra larga). Esto nos permite tener una cobertura relativamente buena con vocabularios peque\xF1os y casi sin tokens desconocidos."),Gl.forEach(a),Es=c(e),qa=n(e,"P",{});var Rl=l(qa);jt=p(Rl,"Este enfoque es especialmente \xFAtil en algunos idimas como el turco, donde se pueden formar palabras complejas (casi) arbitrariamente largas encadenando subpalabras."),Rl.forEach(a),zs=c(e),Q=n(e,"H3",{class:!0});var Er=l(Q);me=n(Er,"A",{id:!0,class:!0,href:!0});var Jl=l(me);so=n(Jl,"SPAN",{});var Fl=l(so);h(Je.$$.fragment,Fl),Fl.forEach(a),Jl.forEach(a),Pt=c(Er),ro=n(Er,"SPAN",{});var Yl=l(ro);xt=p(Yl,"Y m\xE1s!"),Yl.forEach(a),Er.forEach(a),gs=c(e),wa=n(e,"P",{});var Vl=l(wa);At=p(Vl,"Como es l\xF3gico, existen muchas m\xE1s t\xE9cnicas. Por nombrar algunas:"),Vl.forEach(a),ys=c(e),O=n(e,"UL",{});var Ia=l(O);to=n(Ia,"LI",{});var Wl=l(to);Tt=p(Wl,"Byte-level BPE (a nivel de bytes), como usa GPT-2"),Wl.forEach(a),Dt=c(Ia),no=n(Ia,"LI",{});var Kl=l(no);Ct=p(Kl,"WordPiece, usado por BERT"),Kl.forEach(a),St=c(Ia),lo=n(Ia,"LI",{});var Ql=l(lo);Nt=p(Ql,"SentencePiece or Unigram (pedazo de sentencia o unigrama), como se usa en los modelos multilengua"),Ql.forEach(a),Ia.forEach(a),qs=c(e),ja=n(e,"P",{});var Xl=l(ja);Lt=p(Xl,"A este punto, deber\xEDas tener conocimientos suficientes sobre el funcionamiento de los tokenizadores para empezar a utilizar la API."),Xl.forEach(a),ws=c(e),X=n(e,"H2",{class:!0});var zr=l(X);fe=n(zr,"A",{id:!0,class:!0,href:!0});var Zl=l(fe);io=n(Zl,"SPAN",{});var ei=l(io);h(Fe.$$.fragment,ei),ei.forEach(a),Zl.forEach(a),It=c(zr),po=n(zr,"SPAN",{});var ai=l(po);Ot=p(ai,"Cargando y guardando"),ai.forEach(a),zr.forEach(a),js=c(e),x=n(e,"P",{});var U=l(x);Ht=p(U,"Cargar y guardar tokenizadores es tan sencillo como lo es con los modelos. En realidad, se basa en los mismos dos m\xE9todos: "),co=n(U,"CODE",{});var oi=l(co);Mt=p(oi,"from_pretrained()"),oi.forEach(a),Ut=p(U," y "),uo=n(U,"CODE",{});var si=l(uo);Bt=p(si,"save_pretrained()"),si.forEach(a),Gt=p(U,". Estos m\xE9todos cargar\xE1n o guardar\xE1n el algoritmo utilizado por el tokenizador (un poco como la "),mo=n(U,"EM",{});var ri=l(mo);Rt=p(ri,"arquitectura"),ri.forEach(a),Jt=p(U," del modelo) as\xED como su vocabulario (un poco como los "),fo=n(U,"EM",{});var ti=l(fo);Ft=p(ti,"pesos"),ti.forEach(a),Yt=p(U," del modelo)."),U.forEach(a),Ps=c(e),be=n(e,"P",{});var gr=l(be);Vt=p(gr,"La carga del tokenizador BERT entrenado con el mismo punto de control que BERT se realiza de la misma manera que la carga del modelo, excepto que utilizamos la clase "),bo=n(gr,"CODE",{});var ni=l(bo);Wt=p(ni,"BertTokenizer"),ni.forEach(a),Kt=p(gr,":"),gr.forEach(a),xs=c(e),h(Ye.$$.fragment,e),As=c(e),se.l(e),Pa=c(e),h(Ve.$$.fragment,e),Ts=c(e),xa=n(e,"P",{});var li=l(xa);Qt=p(li,"Ahora podemos utilizar el tokenizador como se muestra en la secci\xF3n anterior:"),li.forEach(a),Ds=c(e),h(We.$$.fragment,e),Cs=c(e),h(Ke.$$.fragment,e),Ss=c(e),Aa=n(e,"P",{});var ii=l(Aa);Xt=p(ii,"Guardar un tokenizador es id\xE9ntico a guardar un modelo:"),ii.forEach(a),Ns=c(e),h(Qe.$$.fragment,e),Ls=c(e),A=n(e,"P",{});var B=l(A);Zt=p(B,"Hablaremos m\xE1s sobre "),vo=n(B,"CODE",{});var pi=l(vo);en=p(pi,"token_type_ids"),pi.forEach(a),an=p(B," en el "),Ta=n(B,"A",{href:!0});var di=l(Ta);on=p(di,"Cap\xEDtulo 3"),di.forEach(a),sn=p(B,", y explicaremos la clave "),ko=n(B,"CODE",{});var ci=l(ko);rn=p(ci,"attention_mask"),ci.forEach(a),tn=p(B," un poco m\xE1s tarde. Primero, veamos c\xF3mo se generan los "),ho=n(B,"CODE",{});var ui=l(ho);nn=p(ui,"input_ids"),ui.forEach(a),ln=p(B,". Para ello, tendremos que ver los m\xE9todos intermedios del tokenizador."),B.forEach(a),Is=c(e),Z=n(e,"H2",{class:!0});var yr=l(Z);ve=n(yr,"A",{id:!0,class:!0,href:!0});var mi=l(ve);$o=n(mi,"SPAN",{});var fi=l($o);h(Xe.$$.fragment,fi),fi.forEach(a),mi.forEach(a),pn=c(yr),_o=n(yr,"SPAN",{});var bi=l(_o);dn=p(bi,"Encoding"),bi.forEach(a),yr.forEach(a),Os=c(e),h(Ze.$$.fragment,e),Hs=c(e),ke=n(e,"P",{});var qr=l(ke);cn=p(qr,"La traducci\xF3n de texto a n\xFAmeros se conoce como "),Eo=n(qr,"EM",{});var vi=l(Eo);un=p(vi,"codificaci\xF3n"),vi.forEach(a),mn=p(qr,". La codificaci\xF3n se realiza en un proceso de dos pasos: la tokenizaci\xF3n, seguida de la conversi\xF3n a IDs de entrada."),qr.forEach(a),Ms=c(e),he=n(e,"P",{});var wr=l(he);fn=p(wr,"Como hemos visto, el primer paso es dividir el texto en palabras (o partes de palabras, s\xEDmbolos de puntuaci\xF3n, etc.), normalmente llamadas "),zo=n(wr,"EM",{});var ki=l(zo);bn=p(ki,"tokens"),ki.forEach(a),vn=p(wr,". Hay m\xFAltiples reglas que pueden gobernar ese proceso, por lo que necesitamos instanciar el tokenizador usando el nombre del modelo, para asegurarnos de que usamos las mismas reglas que se usaron cuando se preentren\xF3 el modelo."),wr.forEach(a),Us=c(e),H=n(e,"P",{});var Oa=l(H);kn=p(Oa,"El segundo paso es convertir esos tokens en n\xFAmeros, para poder construir un tensor con ellos y alimentar el modelo. Para ello, el tokenizador tiene un "),go=n(Oa,"EM",{});var hi=l(go);hn=p(hi,"vocabulario"),hi.forEach(a),$n=p(Oa,", que es la parte que descargamos cuando lo instanciamos con el m\xE9todo "),yo=n(Oa,"CODE",{});var $i=l(yo);_n=p($i,"from_pretrained()"),$i.forEach(a),En=p(Oa,". De nuevo, necesitamos usar el mismo vocabulario que se us\xF3 cuando el modelo fue preentrenado."),Oa.forEach(a),Bs=c(e),$e=n(e,"P",{});var jr=l($e);zn=p(jr,"Para entender mejor los dos pasos, los exploraremos por separado. Ten en cuenta que utilizaremos algunos m\xE9todos que realizan partes del proceso de tokenizaci\xF3n por separado para mostrarte los resultados intermedios de esos pasos, pero en la pr\xE1ctica, deber\xEDas llamar al tokenizador directamente en tus "),qo=n(jr,"EM",{});var _i=l(qo);gn=p(_i,"inputs"),_i.forEach(a),yn=p(jr," (como se muestra en la secci\xF3n 2)."),jr.forEach(a),Gs=c(e),ee=n(e,"H3",{class:!0});var Pr=l(ee);_e=n(Pr,"A",{id:!0,class:!0,href:!0});var Ei=l(_e);wo=n(Ei,"SPAN",{});var zi=l(wo);h(ea.$$.fragment,zi),zi.forEach(a),Ei.forEach(a),qn=c(Pr),jo=n(Pr,"SPAN",{});var gi=l(jo);wn=p(gi,"Tokenization"),gi.forEach(a),Pr.forEach(a),Rs=c(e),Ee=n(e,"P",{});var xr=l(Ee);jn=p(xr,"El proceso de tokenizaci\xF3n se realiza mediante el m\xE9todo "),Po=n(xr,"CODE",{});var yi=l(Po);Pn=p(yi,"tokenize()"),yi.forEach(a),xn=p(xr," del tokenizador:"),xr.forEach(a),Js=c(e),h(aa.$$.fragment,e),Fs=c(e),Da=n(e,"P",{});var qi=l(Da);An=p(qi,"La salida de este m\xE9todo es una lista de cadenas, o tokens:"),qi.forEach(a),Ys=c(e),h(oa.$$.fragment,e),Vs=c(e),C=n(e,"P",{});var je=l(C);Tn=p(je,"Este tokenizador es un tokenizador de subpalabras: divide las palabras hasta obtener tokens que puedan ser representados por su vocabulario. Este es el caso de "),xo=n(je,"CODE",{});var wi=l(xo);Dn=p(wi,"transformer"),wi.forEach(a),Cn=p(je,", que se divide en dos tokens: "),Ao=n(je,"CODE",{});var ji=l(Ao);Sn=p(ji,"transform"),ji.forEach(a),Nn=p(je," y "),To=n(je,"CODE",{});var Pi=l(To);Ln=p(Pi,"##er"),Pi.forEach(a),In=p(je,"."),je.forEach(a),Ws=c(e),ae=n(e,"H3",{class:!0});var Ar=l(ae);ze=n(Ar,"A",{id:!0,class:!0,href:!0});var xi=l(ze);Do=n(xi,"SPAN",{});var Ai=l(Do);h(sa.$$.fragment,Ai),Ai.forEach(a),xi.forEach(a),On=c(Ar),Co=n(Ar,"SPAN",{});var Ti=l(Co);Hn=p(Ti,"De tokens a IDs de entrada"),Ti.forEach(a),Ar.forEach(a),Ks=c(e),ge=n(e,"P",{});var Tr=l(ge);Mn=p(Tr,"La conversi\xF3n a IDs de entrada se hace con el m\xE9todo del tokenizador "),So=n(Tr,"CODE",{});var Di=l(So);Un=p(Di,"convert_tokens_to_ids()"),Di.forEach(a),Bn=p(Tr,":"),Tr.forEach(a),Qs=c(e),h(ra.$$.fragment,e),Xs=c(e),h(ta.$$.fragment,e),Zs=c(e),Ca=n(e,"P",{});var Ci=l(Ca);Gn=p(Ci,"Estos resultados, una vez convertidos en el tensor del marco apropiado, pueden utilizarse como entradas de un modelo, como se ha visto anteriormente en este cap\xEDtulo."),Ci.forEach(a),er=c(e),h(ye.$$.fragment,e),ar=c(e),oe=n(e,"H2",{class:!0});var Dr=l(oe);qe=n(Dr,"A",{id:!0,class:!0,href:!0});var Si=l(qe);No=n(Si,"SPAN",{});var Ni=l(No);h(na.$$.fragment,Ni),Ni.forEach(a),Si.forEach(a),Rn=c(Dr),Lo=n(Dr,"SPAN",{});var Li=l(Lo);Jn=p(Li,"Decodificaci\xF3n"),Li.forEach(a),Dr.forEach(a),or=c(e),M=n(e,"P",{});var Ha=l(M);Fn=p(Ha,"La "),Io=n(Ha,"EM",{});var Ii=l(Io);Yn=p(Ii,"decodificaci\xF3n"),Ii.forEach(a),Vn=p(Ha," va al rev\xE9s: a partir de los \xEDndices del vocabulario, queremos obtener una cadena. Esto se puede hacer con el m\xE9todo "),Oo=n(Ha,"CODE",{});var Oi=l(Oo);Wn=p(Oi,"decode()"),Oi.forEach(a),Kn=p(Ha," de la siguiente manera:"),Ha.forEach(a),sr=c(e),h(la.$$.fragment,e),rr=c(e),h(ia.$$.fragment,e),tr=c(e),we=n(e,"P",{});var Cr=l(we);Qn=p(Cr,"Notemos que el m\xE9todo "),Ho=n(Cr,"CODE",{});var Hi=l(Ho);Xn=p(Hi,"decode"),Hi.forEach(a),Zn=p(Cr," no s\xF3lo convierte los \xEDndices de nuevo en tokens, sino que tambi\xE9n agrupa los tokens que formaban parte de las mismas palabras para producir una frase legible. Este comportamiento ser\xE1 extremadamente \xFAtil cuando utilicemos modelos que predigan texto nuevo (ya sea texto generado a partir de una indicaci\xF3n, o para problemas de secuencia a secuencia como la traducci\xF3n o el resumen)."),Cr.forEach(a),nr=c(e),Sa=n(e,"P",{});var Mi=l(Sa);el=p(Mi,"A estas alturas deber\xEDas entender las operaciones at\xF3micas que un tokenizador puede manejar: tokenizaci\xF3n, conversi\xF3n a IDs, y conversi\xF3n de IDs de vuelta a una cadena. Sin embargo, s\xF3lo hemos rozado la punta del iceberg. En la siguiente secci\xF3n, llevaremos nuestro enfoque a sus l\xEDmites y echaremos un vistazo a c\xF3mo superarlos."),Mi.forEach(a),this.h()},h(){u(m,"name","hf:doc:metadata"),u(m,"content",JSON.stringify(sp)),u(z,"id","tokenizadores"),u(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(z,"href","#tokenizadores"),u(q,"class","relative group"),u(re,"id","tokenizacin-wordbased"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#tokenizacin-wordbased"),u(J,"class","relative group"),u(De,"class","block dark:hidden"),Ma(De.src,ol="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||u(De,"src",ol),u(De,"alt","Un ejemplo de tokenizador _word-based_."),u(Ce,"class","hidden dark:block"),Ma(Ce.src,sl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||u(Ce,"src",sl),u(Ce,"alt","Un ejemplo de tokenizador _word-based_."),u(F,"class","flex justify-center"),u(le,"id","tokenizacin-characterbased"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#tokenizacin-characterbased"),u(Y,"class","relative group"),u(He,"class","block dark:hidden"),Ma(He.src,rl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||u(He,"src",rl),u(He,"alt","Ejemplo de tokenizador basado en palabras."),u(Me,"class","hidden dark:block"),Ma(Me.src,tl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||u(Me,"src",tl),u(Me,"alt","Ejemplo de tokenizador basado en palabras."),u(V,"class","flex justify-center"),u(ue,"id","tokenizacin-por-subword"),u(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ue,"href","#tokenizacin-por-subword"),u(W,"class","relative group"),u(Ge,"class","block dark:hidden"),Ma(Ge.src,nl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||u(Ge,"src",nl),u(Ge,"alt","Un tokenizador basado en subpalabras."),u(Re,"class","hidden dark:block"),Ma(Re.src,ll="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||u(Re,"src",ll),u(Re,"alt","Un tokenizador basado en subpalabras."),u(K,"class","flex justify-center"),u(me,"id","y-ms"),u(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(me,"href","#y-ms"),u(Q,"class","relative group"),u(fe,"id","cargando-y-guardando"),u(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(fe,"href","#cargando-y-guardando"),u(X,"class","relative group"),u(Ta,"href","/course/chapter3"),u(ve,"id","encoding"),u(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ve,"href","#encoding"),u(Z,"class","relative group"),u(_e,"id","tokenization"),u(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_e,"href","#tokenization"),u(ee,"class","relative group"),u(ze,"id","de-tokens-a-ids-de-entrada"),u(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ze,"href","#de-tokens-a-ids-de-entrada"),u(ae,"class","relative group"),u(qe,"id","decodificacin"),u(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(qe,"href","#decodificacin"),u(oe,"class","relative group")},m(e,s){o(document.head,m),r(e,E,s),$(f,e,s),r(e,y,s),r(e,q,s),o(q,z),o(z,w),$(j,w,null),o(q,P),o(q,g),o(g,R),r(e,L,s),pa[S].m(e,s),r(e,ca,s),$(Pe,e,s),r(e,Bo,s),r(e,ua,s),o(ua,Sr),r(e,Go,s),r(e,ma,s),o(ma,Nr),r(e,Ro,s),$(xe,e,s),r(e,Jo,s),r(e,fa,s),o(fa,Lr),r(e,Fo,s),r(e,ba,s),o(ba,Ir),r(e,Yo,s),r(e,J,s),o(J,re),o(re,Ua),$(Ae,Ua,null),o(J,Or),o(J,Ba),o(Ba,Hr),r(e,Vo,s),$(Te,e,s),r(e,Wo,s),r(e,I,s),o(I,Mr),o(I,Ga),o(Ga,Ur),o(I,Br),o(I,Ra),o(Ra,Gr),o(I,Rr),r(e,Ko,s),r(e,F,s),o(F,De),o(F,Jr),o(F,Ce),r(e,Qo,s),r(e,te,s),o(te,Fr),o(te,Ja),o(Ja,Yr),o(te,Vr),r(e,Xo,s),$(Se,e,s),r(e,Zo,s),$(Ne,e,s),r(e,es,s),r(e,va,s),o(va,Wr),r(e,as,s),r(e,ka,s),o(ka,Kr),r(e,os,s),r(e,ha,s),o(ha,Qr),r(e,ss,s),r(e,Le,s),o(Le,Xr),o(Le,Fa),o(Fa,Zr),r(e,rs,s),r(e,ne,s),o(ne,et),o(ne,Ya),o(Ya,at),o(ne,ot),r(e,ts,s),r(e,Y,s),o(Y,le),o(le,Va),$(Ie,Va,null),o(Y,st),o(Y,Wa),o(Wa,rt),r(e,ns,s),$(Oe,e,s),r(e,ls,s),r(e,ie,s),o(ie,tt),o(ie,Ka),o(Ka,nt),o(ie,lt),r(e,is,s),r(e,pe,s),o(pe,Qa),o(Qa,it),o(pe,pt),o(pe,Xa),o(Xa,dt),r(e,ps,s),r(e,$a,s),o($a,ct),r(e,ds,s),r(e,V,s),o(V,He),o(V,ut),o(V,Me),r(e,cs,s),r(e,_a,s),o(_a,mt),r(e,us,s),r(e,de,s),o(de,ft),o(de,Za),o(Za,bt),o(de,vt),r(e,ms,s),r(e,ce,s),o(ce,kt),o(ce,eo),o(eo,ht),o(ce,$t),r(e,fs,s),r(e,W,s),o(W,ue),o(ue,ao),$(Ue,ao,null),o(W,_t),o(W,oo),o(oo,Et),r(e,bs,s),$(Be,e,s),r(e,vs,s),r(e,Ea,s),o(Ea,zt),r(e,ks,s),r(e,za,s),o(za,gt),r(e,hs,s),r(e,ga,s),o(ga,yt),r(e,$s,s),r(e,K,s),o(K,Ge),o(K,qt),o(K,Re),r(e,_s,s),r(e,ya,s),o(ya,wt),r(e,Es,s),r(e,qa,s),o(qa,jt),r(e,zs,s),r(e,Q,s),o(Q,me),o(me,so),$(Je,so,null),o(Q,Pt),o(Q,ro),o(ro,xt),r(e,gs,s),r(e,wa,s),o(wa,At),r(e,ys,s),r(e,O,s),o(O,to),o(to,Tt),o(O,Dt),o(O,no),o(no,Ct),o(O,St),o(O,lo),o(lo,Nt),r(e,qs,s),r(e,ja,s),o(ja,Lt),r(e,ws,s),r(e,X,s),o(X,fe),o(fe,io),$(Fe,io,null),o(X,It),o(X,po),o(po,Ot),r(e,js,s),r(e,x,s),o(x,Ht),o(x,co),o(co,Mt),o(x,Ut),o(x,uo),o(uo,Bt),o(x,Gt),o(x,mo),o(mo,Rt),o(x,Jt),o(x,fo),o(fo,Ft),o(x,Yt),r(e,Ps,s),r(e,be,s),o(be,Vt),o(be,bo),o(bo,Wt),o(be,Kt),r(e,xs,s),$(Ye,e,s),r(e,As,s),se.m(e,s),r(e,Pa,s),$(Ve,e,s),r(e,Ts,s),r(e,xa,s),o(xa,Qt),r(e,Ds,s),$(We,e,s),r(e,Cs,s),$(Ke,e,s),r(e,Ss,s),r(e,Aa,s),o(Aa,Xt),r(e,Ns,s),$(Qe,e,s),r(e,Ls,s),r(e,A,s),o(A,Zt),o(A,vo),o(vo,en),o(A,an),o(A,Ta),o(Ta,on),o(A,sn),o(A,ko),o(ko,rn),o(A,tn),o(A,ho),o(ho,nn),o(A,ln),r(e,Is,s),r(e,Z,s),o(Z,ve),o(ve,$o),$(Xe,$o,null),o(Z,pn),o(Z,_o),o(_o,dn),r(e,Os,s),$(Ze,e,s),r(e,Hs,s),r(e,ke,s),o(ke,cn),o(ke,Eo),o(Eo,un),o(ke,mn),r(e,Ms,s),r(e,he,s),o(he,fn),o(he,zo),o(zo,bn),o(he,vn),r(e,Us,s),r(e,H,s),o(H,kn),o(H,go),o(go,hn),o(H,$n),o(H,yo),o(yo,_n),o(H,En),r(e,Bs,s),r(e,$e,s),o($e,zn),o($e,qo),o(qo,gn),o($e,yn),r(e,Gs,s),r(e,ee,s),o(ee,_e),o(_e,wo),$(ea,wo,null),o(ee,qn),o(ee,jo),o(jo,wn),r(e,Rs,s),r(e,Ee,s),o(Ee,jn),o(Ee,Po),o(Po,Pn),o(Ee,xn),r(e,Js,s),$(aa,e,s),r(e,Fs,s),r(e,Da,s),o(Da,An),r(e,Ys,s),$(oa,e,s),r(e,Vs,s),r(e,C,s),o(C,Tn),o(C,xo),o(xo,Dn),o(C,Cn),o(C,Ao),o(Ao,Sn),o(C,Nn),o(C,To),o(To,Ln),o(C,In),r(e,Ws,s),r(e,ae,s),o(ae,ze),o(ze,Do),$(sa,Do,null),o(ae,On),o(ae,Co),o(Co,Hn),r(e,Ks,s),r(e,ge,s),o(ge,Mn),o(ge,So),o(So,Un),o(ge,Bn),r(e,Qs,s),$(ra,e,s),r(e,Xs,s),$(ta,e,s),r(e,Zs,s),r(e,Ca,s),o(Ca,Gn),r(e,er,s),$(ye,e,s),r(e,ar,s),r(e,oe,s),o(oe,qe),o(qe,No),$(na,No,null),o(oe,Rn),o(oe,Lo),o(Lo,Jn),r(e,or,s),r(e,M,s),o(M,Fn),o(M,Io),o(Io,Yn),o(M,Vn),o(M,Oo),o(Oo,Wn),o(M,Kn),r(e,sr,s),$(la,e,s),r(e,rr,s),$(ia,e,s),r(e,tr,s),r(e,we,s),o(we,Qn),o(we,Ho),o(Ho,Xn),o(we,Zn),r(e,nr,s),r(e,Sa,s),o(Sa,el),lr=!0},p(e,[s]){const da={};s&1&&(da.fw=e[0]),f.$set(da);let Na=S;S=pl(e),S!==Na&&(Vi(),b(pa[Na],1,1,()=>{pa[Na]=null}),Fi(),N=pa[S],N||(N=pa[S]=il[S](e),N.c()),v(N,1),N.m(ca.parentNode,ca)),ir!==(ir=dl(e))&&(se.d(1),se=ir(e),se&&(se.c(),se.m(Pa.parentNode,Pa)));const Mo={};s&2&&(Mo.$$scope={dirty:s,ctx:e}),ye.$set(Mo)},i(e){lr||(v(f.$$.fragment,e),v(j.$$.fragment,e),v(N),v(Pe.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Te.$$.fragment,e),v(Se.$$.fragment,e),v(Ne.$$.fragment,e),v(Ie.$$.fragment,e),v(Oe.$$.fragment,e),v(Ue.$$.fragment,e),v(Be.$$.fragment,e),v(Je.$$.fragment,e),v(Fe.$$.fragment,e),v(Ye.$$.fragment,e),v(Ve.$$.fragment,e),v(We.$$.fragment,e),v(Ke.$$.fragment,e),v(Qe.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),v(ea.$$.fragment,e),v(aa.$$.fragment,e),v(oa.$$.fragment,e),v(sa.$$.fragment,e),v(ra.$$.fragment,e),v(ta.$$.fragment,e),v(ye.$$.fragment,e),v(na.$$.fragment,e),v(la.$$.fragment,e),v(ia.$$.fragment,e),lr=!0)},o(e){b(f.$$.fragment,e),b(j.$$.fragment,e),b(N),b(Pe.$$.fragment,e),b(xe.$$.fragment,e),b(Ae.$$.fragment,e),b(Te.$$.fragment,e),b(Se.$$.fragment,e),b(Ne.$$.fragment,e),b(Ie.$$.fragment,e),b(Oe.$$.fragment,e),b(Ue.$$.fragment,e),b(Be.$$.fragment,e),b(Je.$$.fragment,e),b(Fe.$$.fragment,e),b(Ye.$$.fragment,e),b(Ve.$$.fragment,e),b(We.$$.fragment,e),b(Ke.$$.fragment,e),b(Qe.$$.fragment,e),b(Xe.$$.fragment,e),b(Ze.$$.fragment,e),b(ea.$$.fragment,e),b(aa.$$.fragment,e),b(oa.$$.fragment,e),b(sa.$$.fragment,e),b(ra.$$.fragment,e),b(ta.$$.fragment,e),b(ye.$$.fragment,e),b(na.$$.fragment,e),b(la.$$.fragment,e),b(ia.$$.fragment,e),lr=!1},d(e){a(m),e&&a(E),_(f,e),e&&a(y),e&&a(q),_(j),e&&a(L),pa[S].d(e),e&&a(ca),_(Pe,e),e&&a(Bo),e&&a(ua),e&&a(Go),e&&a(ma),e&&a(Ro),_(xe,e),e&&a(Jo),e&&a(fa),e&&a(Fo),e&&a(ba),e&&a(Yo),e&&a(J),_(Ae),e&&a(Vo),_(Te,e),e&&a(Wo),e&&a(I),e&&a(Ko),e&&a(F),e&&a(Qo),e&&a(te),e&&a(Xo),_(Se,e),e&&a(Zo),_(Ne,e),e&&a(es),e&&a(va),e&&a(as),e&&a(ka),e&&a(os),e&&a(ha),e&&a(ss),e&&a(Le),e&&a(rs),e&&a(ne),e&&a(ts),e&&a(Y),_(Ie),e&&a(ns),_(Oe,e),e&&a(ls),e&&a(ie),e&&a(is),e&&a(pe),e&&a(ps),e&&a($a),e&&a(ds),e&&a(V),e&&a(cs),e&&a(_a),e&&a(us),e&&a(de),e&&a(ms),e&&a(ce),e&&a(fs),e&&a(W),_(Ue),e&&a(bs),_(Be,e),e&&a(vs),e&&a(Ea),e&&a(ks),e&&a(za),e&&a(hs),e&&a(ga),e&&a($s),e&&a(K),e&&a(_s),e&&a(ya),e&&a(Es),e&&a(qa),e&&a(zs),e&&a(Q),_(Je),e&&a(gs),e&&a(wa),e&&a(ys),e&&a(O),e&&a(qs),e&&a(ja),e&&a(ws),e&&a(X),_(Fe),e&&a(js),e&&a(x),e&&a(Ps),e&&a(be),e&&a(xs),_(Ye,e),e&&a(As),se.d(e),e&&a(Pa),_(Ve,e),e&&a(Ts),e&&a(xa),e&&a(Ds),_(We,e),e&&a(Cs),_(Ke,e),e&&a(Ss),e&&a(Aa),e&&a(Ns),_(Qe,e),e&&a(Ls),e&&a(A),e&&a(Is),e&&a(Z),_(Xe),e&&a(Os),_(Ze,e),e&&a(Hs),e&&a(ke),e&&a(Ms),e&&a(he),e&&a(Us),e&&a(H),e&&a(Bs),e&&a($e),e&&a(Gs),e&&a(ee),_(ea),e&&a(Rs),e&&a(Ee),e&&a(Js),_(aa,e),e&&a(Fs),e&&a(Da),e&&a(Ys),_(oa,e),e&&a(Vs),e&&a(C),e&&a(Ws),e&&a(ae),_(sa),e&&a(Ks),e&&a(ge),e&&a(Qs),_(ra,e),e&&a(Xs),_(ta,e),e&&a(Zs),e&&a(Ca),e&&a(er),_(ye,e),e&&a(ar),e&&a(oe),_(na),e&&a(or),e&&a(M),e&&a(sr),_(la,e),e&&a(rr),_(ia,e),e&&a(tr),e&&a(we),e&&a(nr),e&&a(Sa)}}}const sp={local:"tokenizadores",sections:[{local:"tokenizacin-wordbased",title:"Tokenizaci\xF3n Word-based"},{local:"tokenizacin-characterbased",title:"Tokenizaci\xF3n Character-based"},{local:"tokenizacin-por-subword",sections:[{local:"y-ms",title:"Y m\xE1s!"}],title:"Tokenizaci\xF3n por Subword"},{local:"cargando-y-guardando",title:"Cargando y guardando"},{local:"encoding",sections:[{local:"tokenization",title:"Tokenization"},{local:"de-tokens-a-ids-de-entrada",title:"De tokens a IDs de entrada"}],title:"Encoding"},{local:"decodificacin",title:"Decodificaci\xF3n"}],title:"Tokenizadores"};function rp(D,m,E){let f="pt";return Yi(()=>{const y=new URLSearchParams(window.location.search);E(0,f=y.get("fw")||"pt")}),[f]}class up extends Bi{constructor(m){super();Gi(this,m,rp,op,Ri,{})}}export{up as default,sp as metadata};
