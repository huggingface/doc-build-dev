import{S as Vd,i as Nd,s as Rd,e as o,t,k as h,w as $,c as d,a as u,h as a,d as s,m,x as D,g as c,G as n,y as j,q as _,o as w,B as E,l as Md,M as Hd,b as T,p as Sn,v as Ud,n as Tn}from"../../chunks/vendor-hf-doc-builder.js";import{T as Yl,Y as zt,D as Id}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{I as Ma}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as q}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Kd}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Gd(x){let i,p;return i=new Id({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function Zd(x){let i,p;return i=new Id({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function Jd(x){let i,p,r,g,z,f,b,S;return b=new q({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Genau wie vorher
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.
    "This course is amazing!",  # Dieser Kurs ist fantastisch!
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Dies ist neu
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># Genau wie vorher</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Dieser Kurs ist fantastisch!</span>
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># Dies ist neu</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){i=o("p"),p=t("Wir fahren mit dem Beispiel aus dem "),r=o("a"),g=t("vorigen Kapitel"),z=t(" fort. Folgenderweise w\xFCrden wir einen Sequenzklassifikator mit einem Batch in Tensorflow trainieren:"),f=h(),$(b.$$.fragment),this.h()},l(k){i=d(k,"P",{});var v=u(i);p=a(v,"Wir fahren mit dem Beispiel aus dem "),r=d(v,"A",{href:!0});var L=u(r);g=a(L,"vorigen Kapitel"),L.forEach(s),z=a(v," fort. Folgenderweise w\xFCrden wir einen Sequenzklassifikator mit einem Batch in Tensorflow trainieren:"),v.forEach(s),f=m(k),D(b.$$.fragment,k),this.h()},h(){T(r,"href","/course/chapter2")},m(k,v){c(k,i,v),n(i,p),n(i,r),n(r,g),n(i,z),c(k,f,v),j(b,k,v),S=!0},i(k){S||(_(b.$$.fragment,k),S=!0)},o(k){w(b.$$.fragment,k),S=!1},d(k){k&&s(i),k&&s(f),E(b,k)}}}function Yd(x){let i,p,r,g,z,f,b,S;return b=new q({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Genau wie vorher
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.
    "This course is amazing!",  # Dieser Kurs ist fantastisch!
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Dies ist neu
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># Genau wie vorher</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Dieser Kurs ist fantastisch!</span>
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Dies ist neu</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){i=o("p"),p=t("Wir fahren mit dem Beispiel aus dem "),r=o("a"),g=t("vorigen Kapitel"),z=t(" fort. Folgenderweise w\xFCrden wir einen Sequenzklassifikator mit einem Batch in PyTorch trainieren:"),f=h(),$(b.$$.fragment),this.h()},l(k){i=d(k,"P",{});var v=u(i);p=a(v,"Wir fahren mit dem Beispiel aus dem "),r=d(v,"A",{href:!0});var L=u(r);g=a(L,"vorigen Kapitel"),L.forEach(s),z=a(v," fort. Folgenderweise w\xFCrden wir einen Sequenzklassifikator mit einem Batch in PyTorch trainieren:"),v.forEach(s),f=m(k),D(b.$$.fragment,k),this.h()},h(){T(r,"href","/course/chapter2")},m(k,v){c(k,i,v),n(i,p),n(i,r),n(r,g),n(i,z),c(k,f,v),j(b,k,v),S=!0},i(k){S||(_(b.$$.fragment,k),S=!0)},o(k){w(b.$$.fragment,k),S=!1},d(k){k&&s(i),k&&s(f),E(b,k)}}}function Qd(x){let i,p;return i=new zt({props:{id:"W_gMJF0xomE"}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function Xd(x){let i,p;return i=new zt({props:{id:"_BZearw7f0w"}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function eu(x){let i,p,r,g,z;return{c(){i=o("p"),p=t("\u270F\uFE0F "),r=o("strong"),g=t("Probier es aus!"),z=t(" Sieh dir das Element 15 der Trainingsdaten und Element 87 des Validierungsdaten an. Was sind ihre Labels?")},l(f){i=d(f,"P",{});var b=u(i);p=a(b,"\u270F\uFE0F "),r=d(b,"STRONG",{});var S=u(r);g=a(S,"Probier es aus!"),S.forEach(s),z=a(b," Sieh dir das Element 15 der Trainingsdaten und Element 87 des Validierungsdaten an. Was sind ihre Labels?"),b.forEach(s)},m(f,b){c(f,i,b),n(i,p),n(i,r),n(r,g),n(i,z)},d(f){f&&s(i)}}}function nu(x){let i,p;return i=new zt({props:{id:"P-rZWqcB6CE"}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function su(x){let i,p;return i=new zt({props:{id:"0u3ioSwev3s"}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function tu(x){let i,p,r,g,z;return{c(){i=o("p"),p=t("\u270F\uFE0F "),r=o("strong"),g=t("Probier es aus!"),z=t(" Nimm Element 15 der Trainingsdaten und tokenisiere die beiden S\xE4tze separat und als Paar. Wo liegt der Unterschied zwischen den beiden Ergebnissen?")},l(f){i=d(f,"P",{});var b=u(i);p=a(b,"\u270F\uFE0F "),r=d(b,"STRONG",{});var S=u(r);g=a(S,"Probier es aus!"),S.forEach(s),z=a(b," Nimm Element 15 der Trainingsdaten und tokenisiere die beiden S\xE4tze separat und als Paar. Wo liegt der Unterschied zwischen den beiden Ergebnissen?"),b.forEach(s)},m(f,b){c(f,i,b),n(i,p),n(i,r),n(r,g),n(i,z)},d(f){f&&s(i)}}}function au(x){let i,p,r,g,z,f,b,S;return{c(){i=o("p"),p=t("Die Funktion, die f\xFCr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als "),r=o("em"),g=t("Collate-Funktion"),z=t(" bezeichnet. Es ist ein Argument, das du \xFCbergeben kannst, wenn du einen "),f=o("code"),b=t("DataLoader"),S=t(" baust, wobei es standardm\xE4\xDFig eine Funktion ist, die die Daten in tf.Tensor umwandelt und zusammenf\xFCgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m\xF6glich, da die Inputs nicht alle gleich gro\xDF sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und \xFCberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann \u2013 TPUs bevorzugen feste Formen, auch wenn das ein zus\xE4tzliches Padding erfordert.")},l(k){i=d(k,"P",{});var v=u(i);p=a(v,"Die Funktion, die f\xFCr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als "),r=d(v,"EM",{});var L=u(r);g=a(L,"Collate-Funktion"),L.forEach(s),z=a(v," bezeichnet. Es ist ein Argument, das du \xFCbergeben kannst, wenn du einen "),f=d(v,"CODE",{});var M=u(f);b=a(M,"DataLoader"),M.forEach(s),S=a(v," baust, wobei es standardm\xE4\xDFig eine Funktion ist, die die Daten in tf.Tensor umwandelt und zusammenf\xFCgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m\xF6glich, da die Inputs nicht alle gleich gro\xDF sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und \xFCberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann \u2013 TPUs bevorzugen feste Formen, auch wenn das ein zus\xE4tzliches Padding erfordert."),v.forEach(s)},m(k,v){c(k,i,v),n(i,p),n(i,r),n(r,g),n(i,z),n(i,f),n(f,b),n(i,S)},d(k){k&&s(i)}}}function iu(x){let i,p,r,g,z,f,b,S;return{c(){i=o("p"),p=t("Die Funktion, die f\xFCr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als "),r=o("em"),g=t("Collate-Funktion"),z=t(" bezeichnet. Es ist ein Argument, das du \xFCbergeben kannst, wenn du einen "),f=o("code"),b=t("DataLoader"),S=t(" baust, wobei es standardm\xE4\xDFig eine Funktion ist, die die Daten in PyTorch-Tensoren umwandelt und zusammenf\xFCgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m\xF6glich, da die Inputs nicht alle gleich gro\xDF sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und \xFCberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann \u2013 TPUs bevorzugen feste Formen, auch wenn das ein zus\xE4tzliches Padding erfordert.")},l(k){i=d(k,"P",{});var v=u(i);p=a(v,"Die Funktion, die f\xFCr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als "),r=d(v,"EM",{});var L=u(r);g=a(L,"Collate-Funktion"),L.forEach(s),z=a(v," bezeichnet. Es ist ein Argument, das du \xFCbergeben kannst, wenn du einen "),f=d(v,"CODE",{});var M=u(f);b=a(M,"DataLoader"),M.forEach(s),S=a(v," baust, wobei es standardm\xE4\xDFig eine Funktion ist, die die Daten in PyTorch-Tensoren umwandelt und zusammenf\xFCgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m\xF6glich, da die Inputs nicht alle gleich gro\xDF sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und \xFCberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann \u2013 TPUs bevorzugen feste Formen, auch wenn das ein zus\xE4tzliches Padding erfordert."),v.forEach(s)},m(k,v){c(k,i,v),n(i,p),n(i,r),n(r,g),n(i,z),n(i,f),n(f,b),n(i,S)},d(k){k&&s(i)}}}function ru(x){let i,p;return i=new q({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function lu(x){let i,p;return i=new q({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function ou(x){let i,p,r,g,z;return i=new q({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment),p=h(),r=o("p"),g=t("Das sieht gut aus! Jetzt, da wir vom Rohtext zu Batches \xFCbergegangen sind, mit denen unser Modell umgehen kann, sind wir bereit zum fein-tunen!")},l(f){D(i.$$.fragment,f),p=m(f),r=d(f,"P",{});var b=u(r);g=a(b,"Das sieht gut aus! Jetzt, da wir vom Rohtext zu Batches \xFCbergegangen sind, mit denen unser Modell umgehen kann, sind wir bereit zum fein-tunen!"),b.forEach(s)},m(f,b){j(i,f,b),c(f,p,b),c(f,r,b),n(r,g),z=!0},i(f){z||(_(i.$$.fragment,f),z=!0)},o(f){w(i.$$.fragment,f),z=!1},d(f){E(i,f),f&&s(p),f&&s(r)}}}function du(x){let i,p;return i=new q({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment)},l(r){D(i.$$.fragment,r)},m(r,g){j(i,r,g),p=!0},i(r){p||(_(i.$$.fragment,r),p=!0)},o(r){w(i.$$.fragment,r),p=!1},d(r){E(i,r)}}}function uu(x){let i,p,r,g,z;return{c(){i=o("p"),p=t("\u270F\uFE0F "),r=o("strong"),g=t("Probier es aus!"),z=t(" Repliziere die Vorverarbeitung auf dem GLUE SST-2-Datensatz. Es ist ein bisschen anders, da es aus einzelnen S\xE4tzen statt aus Paaren besteht, aber der Rest von dem, was wir gemacht haben, sollte gleich aussehen. Alternative w\xE4re eine schwierigere Herausforderung, eine Vorverarbeitungsfunktion zu schreiben, die bei allen GLUE-Aufgaben funktioniert.")},l(f){i=d(f,"P",{});var b=u(i);p=a(b,"\u270F\uFE0F "),r=d(b,"STRONG",{});var S=u(r);g=a(S,"Probier es aus!"),S.forEach(s),z=a(b," Repliziere die Vorverarbeitung auf dem GLUE SST-2-Datensatz. Es ist ein bisschen anders, da es aus einzelnen S\xE4tzen statt aus Paaren besteht, aber der Rest von dem, was wir gemacht haben, sollte gleich aussehen. Alternative w\xE4re eine schwierigere Herausforderung, eine Vorverarbeitungsfunktion zu schreiben, die bei allen GLUE-Aufgaben funktioniert."),b.forEach(s)},m(f,b){c(f,i,b),n(i,p),n(i,r),n(r,g),n(i,z)},d(f){f&&s(i)}}}function Wd(x){let i,p,r,g,z,f,b,S,k,v,L,M,R,H,X,O,W,Z,oe,je;return O=new q({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){i=o("p"),p=t("Jetzt, da wir unseren Datensatz und einen DataCollator haben, m\xFCssen wir sie verbinden. Wir k\xF6nnten Batches manuell laden und sortieren, aber das ist eine Menge Arbeit und wahrscheinlich auch nicht sehr sonderlich performant. Stattdessen gibt es eine einfache Methode, die dieses Problem performant l\xF6st: "),r=o("code"),g=t("to_tf_dataset()"),z=t(". Dadurch wird ein "),f=o("code"),b=t("tf.data.Dataset"),S=t(" um den Datensatz gewickelt, mit einer optionalen Kollatierungsfunktion. "),k=o("code"),v=t("tf.data.Dataset"),L=t(" ist ein natives TensorFlow-Format, das Keras f\xFCr "),M=o("code"),R=t("model.fit()"),H=t(" verwenden kann. Diese Methode kann einen \u{1F917}-Datensatz ohne Umst\xE4nde in ein f\xFCrs Training vorbereitetes Format konvertieren. Sehen wir es uns nun mit unserem Datensatz in Aktion an!"),X=h(),$(O.$$.fragment),W=h(),Z=o("p"),oe=t("Und das was\u2019s! Wir k\xF6nnen Datens\xE4tze in das n\xE4chste Kapitel mitnehmen, wo das Training nach all der harten Arbeit der Datenvorverarbeitung angenehm unkompliziert sein wird.")},l(y){i=d(y,"P",{});var C=u(i);p=a(C,"Jetzt, da wir unseren Datensatz und einen DataCollator haben, m\xFCssen wir sie verbinden. Wir k\xF6nnten Batches manuell laden und sortieren, aber das ist eine Menge Arbeit und wahrscheinlich auch nicht sehr sonderlich performant. Stattdessen gibt es eine einfache Methode, die dieses Problem performant l\xF6st: "),r=d(C,"CODE",{});var yn=u(r);g=a(yn,"to_tf_dataset()"),yn.forEach(s),z=a(C,". Dadurch wird ein "),f=d(C,"CODE",{});var de=u(f);b=a(de,"tf.data.Dataset"),de.forEach(s),S=a(C," um den Datensatz gewickelt, mit einer optionalen Kollatierungsfunktion. "),k=d(C,"CODE",{});var Pn=u(k);v=a(Pn,"tf.data.Dataset"),Pn.forEach(s),L=a(C," ist ein natives TensorFlow-Format, das Keras f\xFCr "),M=d(C,"CODE",{});var qn=u(M);R=a(qn,"model.fit()"),qn.forEach(s),H=a(C," verwenden kann. Diese Methode kann einen \u{1F917}-Datensatz ohne Umst\xE4nde in ein f\xFCrs Training vorbereitetes Format konvertieren. Sehen wir es uns nun mit unserem Datensatz in Aktion an!"),C.forEach(s),X=m(y),D(O.$$.fragment,y),W=m(y),Z=d(y,"P",{});var He=u(Z);oe=a(He,"Und das was\u2019s! Wir k\xF6nnen Datens\xE4tze in das n\xE4chste Kapitel mitnehmen, wo das Training nach all der harten Arbeit der Datenvorverarbeitung angenehm unkompliziert sein wird."),He.forEach(s)},m(y,C){c(y,i,C),n(i,p),n(i,r),n(r,g),n(i,z),n(i,f),n(f,b),n(i,S),n(i,k),n(k,v),n(i,L),n(i,M),n(M,R),n(i,H),c(y,X,C),j(O,y,C),c(y,W,C),c(y,Z,C),n(Z,oe),je=!0},i(y){je||(_(O.$$.fragment,y),je=!0)},o(y){w(O.$$.fragment,y),je=!1},d(y){y&&s(i),y&&s(X),E(O,y),y&&s(W),y&&s(Z)}}}function cu(x){let i,p,r,g,z,f,b,S,k,v,L,M,R,H,X,O,W,Z,oe,je,y,C,yn,de,Pn,qn,He,ze,Ee,ls,Ue,Wa,os,Ia,vt,ee,ne,Cn,ue,Va,Ke,Na,Ra,Ge,Ha,Ua,$t,An,Ka,Dt,Ze,jt,Je,Et,I,Ga,ds,Za,Ja,us,Ya,Qa,cs,Xa,ei,ps,ni,si,hs,ti,ai,xt,ce,ii,ms,ri,li,fs,oi,di,St,xe,ui,bs,ci,pi,Tt,Ye,yt,Qe,Pt,pe,hi,gs,mi,fi,_s,bi,gi,qt,Xe,Ct,en,At,B,_i,ws,wi,ki,ks,zi,vi,zs,$i,Di,vs,ji,Ei,$s,xi,Si,Ds,Ti,yi,js,Pi,qi,Bt,Se,Lt,ve,Te,Es,nn,Ci,xs,Ai,Ot,se,te,Bn,ye,Bi,Ln,Li,Oi,Ft,sn,Mt,On,Fi,Wt,tn,It,an,Vt,U,Mi,Fn,Wi,Ii,Ss,Vi,Ni,Ts,Ri,Hi,ys,Ui,Ki,Nt,Pe,Rt,qe,Gi,Ps,Zi,Ji,Ht,rn,Ut,Mn,Yi,Kt,ln,Gt,Ce,Qi,qs,Xi,er,Zt,on,Jt,K,nr,Cs,sr,tr,As,ar,ir,Bs,rr,lr,Ls,or,dr,Yt,Ae,ur,Os,cr,pr,Qt,he,hr,Wn,mr,fr,Fs,br,gr,Xt,In,_r,ea,Be,wr,Ms,kr,zr,na,me,vr,Vn,$r,Dr,Nn,jr,Er,sa,dn,ta,G,xr,Ws,Sr,Tr,Is,yr,Pr,Vs,qr,Cr,un,Ar,Br,aa,fe,Lr,cn,Ns,Or,Fr,Rs,Mr,Wr,ia,pn,ra,P,Ir,Hs,Vr,Nr,Us,Rr,Hr,Ks,Ur,Kr,Gs,Gr,Zr,Zs,Jr,Yr,Js,Qr,Xr,Ys,el,nl,Qs,sl,tl,hn,al,il,Xs,rl,ll,la,Le,ol,et,dl,ul,oa,be,cl,nt,pl,hl,st,ml,fl,da,mn,ua,Rn,bl,ca,fn,pa,ge,gl,tt,_l,wl,at,kl,zl,ha,V,vl,it,$l,Dl,rt,jl,El,lt,xl,Sl,ot,Tl,yl,dt,Pl,ql,ma,Oe,Cl,ut,Al,Bl,fa,$e,Fe,ct,bn,Ll,pt,Ol,ba,gn,ga,Hn,Me,Fl,ht,Ml,Wl,_a,ae,ie,Un,J,Il,mt,Vl,Nl,ft,Rl,Hl,bt,Ul,Kl,wa,_n,ka,wn,za,We,Gl,gt,Zl,Jl,va,kn,$a,re,le,Kn,Ie,Da,Gn,ja;r=new Kd({props:{fw:x[0]}}),S=new Ma({});const Ql=[Zd,Gd],zn=[];function Xl(e,l){return e[0]==="pt"?0:1}R=Xl(x),H=zn[R]=Ql[R](x);const eo=[Yd,Jd],vn=[];function no(e,l){return e[0]==="pt"?0:1}O=no(x),W=vn[O]=eo[O](x),Ue=new Ma({});const so=[Xd,Qd],$n=[];function to(e,l){return e[0]==="pt"?0:1}ee=to(x),ne=$n[ee]=so[ee](x),Ze=new q({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),Je=new q({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),Ye=new q({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),Qe=new q({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>}`}}),Xe=new q({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),en=new q({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Se=new Yl({props:{$$slots:{default:[eu]},$$scope:{ctx:x}}}),nn=new Ma({});const ao=[su,nu],Dn=[];function io(e,l){return e[0]==="pt"?0:1}se=io(x),te=Dn[se]=ao[se](x),sn=new q({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),tn=new q({props:{code:`inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>)
inputs`}}),an=new q({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Pe=new Yl({props:{$$slots:{default:[tu]},$$scope:{ctx:x}}}),rn=new q({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),ln=new q({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),on=new q({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),dn=new q({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),pn=new q({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),mn=new q({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),fn=new q({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),bn=new Ma({}),gn=new zt({props:{id:"7q5NyFT8REg"}});function ro(e,l){return e[0]==="pt"?iu:au}let Ea=ro(x),De=Ea(x);const lo=[lu,ru],jn=[];function oo(e,l){return e[0]==="pt"?0:1}ae=oo(x),ie=jn[ae]=lo[ae](x),_n=new q({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),wn=new q({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),kn=new q({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const uo=[du,ou],En=[];function co(e,l){return e[0]==="tf"?0:1}re=co(x),le=En[re]=uo[re](x),Ie=new Yl({props:{$$slots:{default:[uu]},$$scope:{ctx:x}}});let F=x[0]==="tf"&&Wd();return{c(){i=o("meta"),p=h(),$(r.$$.fragment),g=h(),z=o("h1"),f=o("a"),b=o("span"),$(S.$$.fragment),k=h(),v=o("span"),L=t("Vorbereitung der Daten"),M=h(),H.c(),X=h(),W.c(),Z=h(),oe=o("p"),je=t("Nat\xFCrlich w\xFCrde das Training von Modellen mit nur zwei S\xE4tzen keine sonderlich guten Ergebnisse liefern. Um bessere Ergebnisse zu erzielen, m\xFCssen wir einen gr\xF6\xDFeren Datensatz vorbereiten."),y=h(),C=o("p"),yn=t("In diesem Abschnitt verwenden wir den MRPC-Datensatz (Microsoft Research Paraphrase Corpus) als Beispiel. Dieser wurde in einem "),de=o("a"),Pn=t("Paper"),qn=t(" von William B. Dolan und Chris Brockett ver\xF6ffentlicht. Der Datensatz besteht aus insgesamt 5.801 Satzpaaren und enth\xE4lt ein Label, das angibt, ob es sich bei einem Paar um Paraphrasen handelt (d.h. ob beide S\xE4tze dasselbe bedeuten). Wir haben diesen Datensatz f\xFCr dieses Kapitel ausgew\xE4hlt, weil es sich um einen kleinen Datensatz handelt, sodass es einfach ist, w\xE4hrend dem Training zu experimentieren."),He=h(),ze=o("h3"),Ee=o("a"),ls=o("span"),$(Ue.$$.fragment),Wa=h(),os=o("span"),Ia=t("Laden eines Datensatzes vom Hub"),vt=h(),ne.c(),Cn=h(),ue=o("p"),Va=t("Das Hub enth\xE4lt nicht nur Modelle; Es hat auch mehrere Datens\xE4tze in vielen verschiedenen Sprachen. Du kannst die Datens\xE4tze "),Ke=o("a"),Na=t("hier"),Ra=t(" durchsuchen, und wir empfehlen, einen weiteren Datensatz zu laden und zu verarbeiten, sobald Sie diesen Abschnitt abgeschlossen haben (die Dokumentation befindet sich [hier](https: //huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Aber jetzt konzentrieren wir uns auf den MRPC-Datensatz! Dies ist einer der 10 Datens\xE4tze, aus denen sich das "),Ge=o("a"),Ha=t("GLUE-Benchmark"),Ua=t(" zusammensetzt. Dies ist ein akademisches Benchmark, das verwendet wird, um die Performance von ML-Modellen in 10 verschiedenen Textklassifizierungsaufgaben zu messen."),$t=h(),An=o("p"),Ka=t("Die Bibliothek \u{1F917} Datasets bietet einen leichten Befehl zum Herunterladen und Caching eines Datensatzes aus dem Hub. Wir k\xF6nnen den MRPC-Datensatz wie folgt herunterladen:"),Dt=h(),$(Ze.$$.fragment),jt=h(),$(Je.$$.fragment),Et=h(),I=o("p"),Ga=t("Wie du sehen kannst, erhalten wir ein "),ds=o("code"),Za=t("DatasetDict"),Ja=t("-Objekt, das die Trainingsdaten, die Validierungsdaten und die Testdaten enth\xE4lt. Jedes Objekt enth\xE4lt mehrere Spalten ("),us=o("code"),Ya=t("sentence1"),Qa=t(", "),cs=o("code"),Xa=t("sentence2"),ei=t(", "),ps=o("code"),ni=t("label"),si=t(" und "),hs=o("code"),ti=t("idx"),ai=t(") und eine unterschiedliche Anzahl an Zeilen, dies ist die Anzahl der Elemente in jedem Datensatz (also gibt es 3.668 Satzpaare in den Trainingsdaten, 408 in den Validierungsdaten und 1.725 in den Testdaten)."),xt=h(),ce=o("p"),ii=t("Dieser Befehl l\xE4dt das Dataset herunter und speichert es im Cache, standardm\xE4\xDFig in "),ms=o("em"),ri=t("~/.cache/huggingface/dataset"),li=t(". Wir Erinnern uns an Kapitel 2, dass der Cache-Ordner anpasst werden kann, indem man die Umgebungsvariable "),fs=o("code"),oi=t("HF_HOME"),di=t(" setzt."),St=h(),xe=o("p"),ui=t("Wir k\xF6nnen auf jedes Satzpaar in unserem "),bs=o("code"),ci=t("raw_datasets"),pi=t("-Objekt zugreifen, indem wir wie bei einem Dictionary einen Schl\xFCsselwert als Index verwenden:"),Tt=h(),$(Ye.$$.fragment),yt=h(),$(Qe.$$.fragment),Pt=h(),pe=o("p"),hi=t("Wir stellen fest, dass die Labels bereits Ganzzahlen sind, sodass wir dort keine Vorverarbeitung durchf\xFChren m\xFCssen. Wir k\xF6nnen die "),gs=o("code"),mi=t("features"),fi=t(" von "),_s=o("code"),bi=t("raw_train_dataset"),gi=t(" untersuchen, um zu erfahren, welche Ganzzahl welchem Label entspricht. Der folgende Befehl gibt uns den Variablentyp zur\xFCck:"),qt=h(),$(Xe.$$.fragment),Ct=h(),$(en.$$.fragment),At=h(),B=o("p"),_i=t("Hinter den Kulissen ist "),ws=o("code"),wi=t("label"),ki=t(" vom Typ "),ks=o("code"),zi=t("ClassLabel"),vi=t(", und die Zuordnung von Ganzzahlen zum Labelnamen wird im Ordner "),zs=o("em"),$i=t("names"),Di=t(" gespeichert. "),vs=o("code"),ji=t("0"),Ei=t(" entspricht "),$s=o("code"),xi=t("not_equivalent"),Si=t(", also \u201Cnicht \xE4quivalent\u201D, und "),Ds=o("code"),Ti=t("1"),yi=t(" entspricht "),js=o("code"),Pi=t("equivalent"),qi=t(", also \u201C\xE4quivalent\u201D."),Bt=h(),$(Se.$$.fragment),Lt=h(),ve=o("h3"),Te=o("a"),Es=o("span"),$(nn.$$.fragment),Ci=h(),xs=o("span"),Ai=t("Vorverarbeitung eines Datensatzes"),Ot=h(),te.c(),Bn=h(),ye=o("p"),Bi=t("Um den Datensatz vorzubereiten, m\xFCssen wir den Text in Zahlen umwandeln, die das Modell sinnvoll verarbeiten kann. Im "),Ln=o("a"),Li=t("vorherigen Kapitel"),Oi=t(" haben wir gesehen, dass dies mit einem Tokenizer gemacht wird. Wir k\xF6nnen den Tokenizer mit einem Satz oder einer Liste von S\xE4tzen f\xFCttern, sodass wir die ersten und zweiten S\xE4tze jedes Paares wie folgt direkt tokenisieren k\xF6nnen:"),Ft=h(),$(sn.$$.fragment),Mt=h(),On=o("p"),Fi=t("Wir k\xF6nnen jedoch nicht einfach zwei Sequenzen an das Modell \xFCbergeben und eine Vorhersage erhalten, ob die beiden S\xE4tze paraphrasiert sind oder nicht. Wir m\xFCssen die beiden Sequenzen als Paar behandeln und die entsprechende Vorverarbeitung anwenden. Gl\xFCcklicherweise kann der Tokenizer auch ein Sequenzpaar nehmen und es so vorbereiten, wie es unser BERT-Modell erwartet:"),Wt=h(),$(tn.$$.fragment),It=h(),$(an.$$.fragment),Vt=h(),U=o("p"),Mi=t("In "),Fn=o("a"),Wi=t("Kapitel 2"),Ii=t(" haben wir die Schl\xFCsselwerte "),Ss=o("code"),Vi=t("input_ids"),Ni=t(" und "),Ts=o("code"),Ri=t("attention_mask"),Hi=t(" behandelt, allerdings haben wir es aufgeschoben, \xFCber "),ys=o("code"),Ui=t("token_type_ids"),Ki=t(" zu sprechen. In diesem Beispiel teilt diese dem Modell mit, welcher Teil des Input der erste Satz und welcher der zweite Satz ist."),Nt=h(),$(Pe.$$.fragment),Rt=h(),qe=o("p"),Gi=t("Wenn wir die IDs in "),Ps=o("code"),Zi=t("input_ids"),Ji=t(" zur\xFCck in Worte dekodieren:"),Ht=h(),$(rn.$$.fragment),Ut=h(),Mn=o("p"),Yi=t("dann bekommen wir:"),Kt=h(),$(ln.$$.fragment),Gt=h(),Ce=o("p"),Qi=t("Wir sehen also wenn es zwei S\xE4tze gibt, dass das Modell erwartet, dass die Inputs die Form \u201D[CLS] Satz1 [SEP] Satz2 [SEP]\u201D haben. Wenn wir dies mit den "),qs=o("code"),Xi=t("token_type_ids"),er=t(" abgleichen, erhalten wir:"),Zt=h(),$(on.$$.fragment),Jt=h(),K=o("p"),nr=t("Wie du sehen kannst, haben die Teile der Eingabe, die "),Cs=o("code"),sr=t("[CLS] Satz1 [SEP]"),tr=t(" entsprechen, alle eine Token-Typ-ID von "),As=o("code"),ar=t("0"),ir=t(", w\xE4hrend die anderen Teile, die "),Bs=o("code"),rr=t("Satz2 [SEP]"),lr=t(" entsprechen, alle einer Token-Typ-ID von "),Ls=o("code"),or=t("1"),dr=t(" enthalten."),Yt=h(),Ae=o("p"),ur=t("Beachte, dass die Auswahl eines anderen Checkpoints nicht unbedingt die "),Os=o("code"),cr=t("token_type_ids"),pr=t(" in Ihren tokenisierten Inputs haben (z.B. werden sie nicht zur\xFCckgegeben, wenn ein DistilBERT-Modell verwendet wird). Sie werden nur zur\xFCckgegeben, wenn das Modell wei\xDF was damit zu tun ist, weil es die Toke-Typ-Ids w\xE4hrend des Vortrainings gesehen hat."),Qt=h(),he=o("p"),hr=t("In diesem Fall ist BERT mit Token-Typ-IDs vortrainiert worden, und zus\xE4tzlich zu dem maskierten Sprachmodellierungsziel aud "),Wn=o("a"),mr=t("Kapitel 1"),fr=t(", hat es ein zus\xE4tzliches Vorhersageziel namens "),Fs=o("em"),br=t("next sentence prediction"),gr=t(" (d.h. Vorhersage des n\xE4chsten Satzes). Das Ziel dieser Aufgabe ist es, die Beziehung zwischen Satzpaaren zu modellieren."),Xt=h(),In=o("p"),_r=t("Bei der Vorhersage des n\xE4chsten Satzes werden dem Modell Satzpaare (mit zuf\xE4llig maskierten Token) bereitgestellt und erwartet, vorherzusagen, ob auf den ersten Satz der zweite Satz folgt. Um die Aufgabe non-trivial zu machen, folgen sich die H\xE4lfte der S\xE4tze in dem Originaldokument, aus dem sie extrahiert wurden, aufeinander, und in der anderen H\xE4lfte stammen die beiden S\xE4tze aus zwei verschiedenen Dokumenten."),ea=h(),Be=o("p"),wr=t("Im Allgemeinen muss man sich keine Gedanken dar\xFCber machen, ob Ihre tokenisierten Inputs "),Ms=o("code"),kr=t("token_type_ids"),zr=t(" enthalten oder nicht: Solange du denselben Checkpoint f\xFCr den Tokenizer und das Modell verwendest, ist alles in Ordnung, da der Tokenizer wei\xDF, was er dem Modell bereitstellen soll."),na=h(),me=o("p"),vr=t("Nachdem wir nun gesehen haben, wie unser Tokenizer mit einem Satzpaar umgehen kann, k\xF6nnen wir damit unseren gesamten Datensatz tokenisieren: Wie im "),Vn=o("a"),$r=t("vorherigen Kapitel"),Dr=t(" k\xF6nnen wir dem Tokenizer eine Liste von Satzpaaren einspeisen, indem du ihm die Liste der ersten S\xE4tze und dann die Liste der zweiten S\xE4tze gibst. Dies ist auch kompatibel mit den Optionen zum Padding und Trunkieren, die wir in "),Nn=o("a"),jr=t("Kapitel 2"),Er=t(" gesehen haben. Eine M\xF6glichkeit, den Trainingsdatensatz vorzuverarbeiten, ist also:"),sa=h(),$(dn.$$.fragment),ta=h(),G=o("p"),xr=t("Das funktioniert gut, hat aber den Nachteil, dass ein Dictionary zur\xFCckgegeben wird (mit unseren Schl\xFCsselw\xF6rtern "),Ws=o("code"),Sr=t("input_ids"),Tr=t(", "),Is=o("code"),yr=t("attention_mask"),Pr=t(" und "),Vs=o("code"),qr=t("token_type_ids"),Cr=t(" und Werten aus Listen von Listen). Es funktioniert auch nur, wenn du gen\xFCgend RAM hast, um den gesamten Datensatz w\xE4hrend der Tokenisierung zu im RAM zwischen zu speichern (w\xE4hrend die Datens\xE4tze aus der Bibliothek \u{1F917} Datasets "),un=o("a"),Ar=t("Apache Arrow"),Br=t(" Dateien sind, die auf der Festplatte gespeichert sind, sodass nur die gew\xFCnschten Samples im RAM geladen sind)."),aa=h(),fe=o("p"),Lr=t("Um die Daten als Datensatz zu speichern, verwenden wir die Methode "),cn=o("a"),Ns=o("code"),Or=t("Dataset.map()"),Fr=t(". Dies gew\xE4hrt uns zus\xE4tzliche Flexibilit\xE4t, wenn wir zus\xE4tzliche Vorverarbeitung als nur die Tokenisierung ben\xF6tigen. Die "),Rs=o("code"),Mr=t("map()"),Wr=t("-Methode funktioniert, indem sie eine Funktion auf jedes Element des Datensatzes anwendet, also definieren wir eine Funktion, die unsere Inputs tokenisiert:"),ia=h(),$(pn.$$.fragment),ra=h(),P=o("p"),Ir=t("Diese Funktion nimmt ein Dictionary (wie die Elemente unseres Datensatzes) und gibt ein neues Dictionary mit den Schl\xFCsselwerten "),Hs=o("code"),Vr=t("input_ids"),Nr=t(", "),Us=o("code"),Rr=t("attention_mask"),Hr=t(" und "),Ks=o("code"),Ur=t("token_type_ids"),Kr=t(" zur\xFCck. Beachte, dass es auch funktioniert, wenn das "),Gs=o("code"),Gr=t("example"),Zr=t("-Dictionary mehrere Beispiele enth\xE4lt (jeder Schl\xFCsselwert als Liste von S\xE4tzen), da der "),Zs=o("code"),Jr=t("Tokenizer"),Yr=t(", wie zuvor gesehen, mit Listen von Satzpaaren arbeitet. Dadurch k\xF6nnen wir die Option "),Js=o("code"),Qr=t("batched=True"),Xr=t(" in unserem Aufruf von "),Ys=o("code"),el=t("map()"),nl=t(" verwenden, was die Tokenisierung erheblich beschleunigt. Der "),Qs=o("code"),sl=t("tokenizer"),tl=t(" wurde in Rust geschriebenen und ist in der Bibliothek "),hn=o("a"),al=t("\u{1F917} Tokenizers"),il=t(" verf\xFCgbar. Dieser Tokenizer kann sehr schnell arbeiten, wenn wir ihm viele Inputs auf einmal zum Verarbeiten geben. Note that we\u2019ve left the "),Xs=o("code"),rl=t("padding"),ll=t(" argument out in our tokenization function for now."),la=h(),Le=o("p"),ol=t("Beachte, dass wir das "),et=o("code"),dl=t("padding"),ul=t("-Argument vorerst in unserer Tokenisierungsfunktion ausgelassen haben. Dies liegt daran, dass das Anwenden von Padding auf alle Elemente unserer Daten auf die maximale L\xE4nge nicht effizient ist: Es ist besser, die Proben aufzuf\xFCllen, wenn wir ein Batch erstellen, da wir dann nur auf die maximale L\xE4nge in diesem Batch auff\xFCllen m\xFCssen und nicht auf die maximale L\xE4nge in den gesamten Datensatz. Dies kann viel Zeit und Rechenleistung sparen, besonders wenn die Eingaben stark variable L\xE4ngen haben!"),oa=h(),be=o("p"),cl=t("So wenden wir die Tokenisierungsfunktion auf alle unsere Datens\xE4tze gleichzeitig an. In unserem Aufruf von "),nt=o("code"),pl=t("map"),hl=t(" verwenden wir "),st=o("code"),ml=t("batched=True"),fl=t(", damit die Funktion auf mehrere Elemente des Datensatzes gleichzeitig angewendet wird und nicht auf jedes Element separat. Dies erm\xF6glicht eine schnellere Vorverarbeitung."),da=h(),$(mn.$$.fragment),ua=h(),Rn=o("p"),bl=t("Die Bibliothek \u{1F917} Datasets verarbeitet Datens\xE4tzen indem sie neue Felder hinzuzuf\xFCgen, eines f\xFCr jeden Schl\xFCssel im Dictionary, der von der Vorverarbeitungsfunktion zur\xFCckgegeben wird:"),ca=h(),$(fn.$$.fragment),pa=h(),ge=o("p"),gl=t("Du kannst sogar Multiprocessing verwenden, wenn du die Vorverarbeitungsfunktion mit "),tt=o("code"),_l=t("map()"),wl=t(" anwendest, indem du ein "),at=o("code"),kl=t("num_proc"),zl=t("-Argument \xFCbergiebst. Wir haben dies hier nicht getan, weil die \u{1F917} Tokenizers-Bibliothek bereits mehrere Threads verwendet, um unsere Samples schneller zu tokenisieren. Wenn du keinen schnellen Tokenizer verwendest, der von dieser Bibliothek unterst\xFCtzt wird, w\xFCrde dies allerdings die Vorverarbeitung beschleunigen."),ha=h(),V=o("p"),vl=t("Unsere "),it=o("code"),$l=t("tokenize_function"),Dl=t(" gibt ein Dictionary mit den Schl\xFCsselwerten "),rt=o("code"),jl=t("input_ids"),El=t(", "),lt=o("code"),xl=t("attention_mask"),Sl=t(" und "),ot=o("code"),Tl=t("token_type_ids"),yl=t(" zur\xFCck, also werden diese drei Felder zu allen Splits unseres Datensatzes hinzugef\xFCgt. Beachte, dass wir auch vorhandene Felder \xE4ndern k\xF6nnten, wenn unsere Vorverarbeitungsfunktion einen neuen Wert f\xFCr einen vorhandenen Schl\xFCsselwert in dem Datensatz zur\xFCckgegeben h\xE4tte, auf den wir "),dt=o("code"),Pl=t("map()"),ql=t(" angewendet haben."),ma=h(),Oe=o("p"),Cl=t("Zuletzt, m\xFCssen wir alle Beispiele auf die L\xE4nge des l\xE4ngsten Elements aufzuf\xFCllen, wenn wir Elemente zusammenfassen \u2013 eine Technik, die wir als "),ut=o("em"),Al=t("Dynamisches Padding"),Bl=t(" bezeichnen."),fa=h(),$e=o("h3"),Fe=o("a"),ct=o("span"),$(bn.$$.fragment),Ll=h(),pt=o("span"),Ol=t("Dynamisches Padding"),ba=h(),$(gn.$$.fragment),ga=h(),De.c(),Hn=h(),Me=o("p"),Fl=t("In der Praxis m\xFCssen wir eine Collate-Funktion definieren, die die korrekte Menge an Padding auf die Elemente des Datensatzes anwendet, die wir in einem Batch haben m\xF6chten. Gl\xFCcklicherweise stellt uns die \u{1F917} Transformers-Bibliothek \xFCber "),ht=o("code"),Ml=t("DataCollatorWithPadding"),Wl=t(" eine solche Funktion zur Verf\xFCgung. Wenn sie instanziert wird, braucht es einen Tokenizer (um zu wissen, welches Padding-token verwendet werden soll und ob das Modell erwartet, dass sich das Padding links oder rechts von den Inputs befindet) und \xFCbernimmt alles was wir brauchen:"),_a=h(),ie.c(),Un=h(),J=o("p"),Il=t("Um dieses neue Werkzeug zu testen, nehmen wir einige Elemente aus den Trainingsdaten, die wir als Batch verwenden m\xF6chten. Hier entfernen wir die Spalten "),mt=o("code"),Vl=t("idx"),Nl=t(", "),ft=o("code"),Rl=t("sentence1"),Hl=t(" und "),bt=o("code"),Ul=t("sentence2"),Kl=t(", da sie nicht ben\xF6tigt werden und Strings enthalten (wir k\xF6nnen keine Tensoren mit Strings erstellen) und sehen uns die L\xE4nge jedes Eintrags im Batch an:"),wa=h(),$(_n.$$.fragment),ka=h(),$(wn.$$.fragment),za=h(),We=o("p"),Gl=t("Wenig \xFCberraschen erhalten wir Samples unterschiedlicher L\xE4nge von 32 bis 67. Dynamisches Padding bedeutet, dass die Elemente in diesem Batch alle auf eine L\xE4nge von 67 aufgef\xFCllt werden, die maximale L\xE4nge innerhalb des Batches. Ohne dynamisches Auff\xFCllen m\xFCssten alle Eintr\xE4ge auf die maximale L\xE4nge im gesamten Datensatz oder auf die maximale L\xE4nge die das Modell akzeptiert, aufgef\xFCllt werden. Lass uns noch einmal \xFCberpr\xFCfen, ob unser "),gt=o("code"),Zl=t("data_collator"),Jl=t(" den Stapel dynamisch richtig auff\xFCllt:"),va=h(),$(kn.$$.fragment),$a=h(),le.c(),Kn=h(),$(Ie.$$.fragment),Da=h(),F&&F.c(),Gn=Md(),this.h()},l(e){const l=Hd('[data-svelte="svelte-1phssyn"]',document.head);i=d(l,"META",{name:!0,content:!0}),l.forEach(s),p=m(e),D(r.$$.fragment,e),g=m(e),z=d(e,"H1",{class:!0});var xn=u(z);f=d(xn,"A",{id:!0,class:!0,href:!0});var Zn=u(f);b=d(Zn,"SPAN",{});var Jn=u(b);D(S.$$.fragment,Jn),Jn.forEach(s),Zn.forEach(s),k=m(xn),v=d(xn,"SPAN",{});var Yn=u(v);L=a(Yn,"Vorbereitung der Daten"),Yn.forEach(s),xn.forEach(s),M=m(e),H.l(e),X=m(e),W.l(e),Z=m(e),oe=d(e,"P",{});var _t=u(oe);je=a(_t,"Nat\xFCrlich w\xFCrde das Training von Modellen mit nur zwei S\xE4tzen keine sonderlich guten Ergebnisse liefern. Um bessere Ergebnisse zu erzielen, m\xFCssen wir einen gr\xF6\xDFeren Datensatz vorbereiten."),_t.forEach(s),y=m(e),C=d(e,"P",{});var Ve=u(C);yn=a(Ve,"In diesem Abschnitt verwenden wir den MRPC-Datensatz (Microsoft Research Paraphrase Corpus) als Beispiel. Dieser wurde in einem "),de=d(Ve,"A",{href:!0,rel:!0});var wt=u(de);Pn=a(wt,"Paper"),wt.forEach(s),qn=a(Ve," von William B. Dolan und Chris Brockett ver\xF6ffentlicht. Der Datensatz besteht aus insgesamt 5.801 Satzpaaren und enth\xE4lt ein Label, das angibt, ob es sich bei einem Paar um Paraphrasen handelt (d.h. ob beide S\xE4tze dasselbe bedeuten). Wir haben diesen Datensatz f\xFCr dieses Kapitel ausgew\xE4hlt, weil es sich um einen kleinen Datensatz handelt, sodass es einfach ist, w\xE4hrend dem Training zu experimentieren."),Ve.forEach(s),He=m(e),ze=d(e,"H3",{class:!0});var Ne=u(ze);Ee=d(Ne,"A",{id:!0,class:!0,href:!0});var Qn=u(Ee);ls=d(Qn,"SPAN",{});var kt=u(ls);D(Ue.$$.fragment,kt),kt.forEach(s),Qn.forEach(s),Wa=m(Ne),os=d(Ne,"SPAN",{});var po=u(os);Ia=a(po,"Laden eines Datensatzes vom Hub"),po.forEach(s),Ne.forEach(s),vt=m(e),ne.l(e),Cn=m(e),ue=d(e,"P",{});var Xn=u(ue);Va=a(Xn,"Das Hub enth\xE4lt nicht nur Modelle; Es hat auch mehrere Datens\xE4tze in vielen verschiedenen Sprachen. Du kannst die Datens\xE4tze "),Ke=d(Xn,"A",{href:!0,rel:!0});var ho=u(Ke);Na=a(ho,"hier"),ho.forEach(s),Ra=a(Xn," durchsuchen, und wir empfehlen, einen weiteren Datensatz zu laden und zu verarbeiten, sobald Sie diesen Abschnitt abgeschlossen haben (die Dokumentation befindet sich [hier](https: //huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Aber jetzt konzentrieren wir uns auf den MRPC-Datensatz! Dies ist einer der 10 Datens\xE4tze, aus denen sich das "),Ge=d(Xn,"A",{href:!0,rel:!0});var mo=u(Ge);Ha=a(mo,"GLUE-Benchmark"),mo.forEach(s),Ua=a(Xn," zusammensetzt. Dies ist ein akademisches Benchmark, das verwendet wird, um die Performance von ML-Modellen in 10 verschiedenen Textklassifizierungsaufgaben zu messen."),Xn.forEach(s),$t=m(e),An=d(e,"P",{});var fo=u(An);Ka=a(fo,"Die Bibliothek \u{1F917} Datasets bietet einen leichten Befehl zum Herunterladen und Caching eines Datensatzes aus dem Hub. Wir k\xF6nnen den MRPC-Datensatz wie folgt herunterladen:"),fo.forEach(s),Dt=m(e),D(Ze.$$.fragment,e),jt=m(e),D(Je.$$.fragment,e),Et=m(e),I=d(e,"P",{});var Y=u(I);Ga=a(Y,"Wie du sehen kannst, erhalten wir ein "),ds=d(Y,"CODE",{});var bo=u(ds);Za=a(bo,"DatasetDict"),bo.forEach(s),Ja=a(Y,"-Objekt, das die Trainingsdaten, die Validierungsdaten und die Testdaten enth\xE4lt. Jedes Objekt enth\xE4lt mehrere Spalten ("),us=d(Y,"CODE",{});var go=u(us);Ya=a(go,"sentence1"),go.forEach(s),Qa=a(Y,", "),cs=d(Y,"CODE",{});var _o=u(cs);Xa=a(_o,"sentence2"),_o.forEach(s),ei=a(Y,", "),ps=d(Y,"CODE",{});var wo=u(ps);ni=a(wo,"label"),wo.forEach(s),si=a(Y," und "),hs=d(Y,"CODE",{});var ko=u(hs);ti=a(ko,"idx"),ko.forEach(s),ai=a(Y,") und eine unterschiedliche Anzahl an Zeilen, dies ist die Anzahl der Elemente in jedem Datensatz (also gibt es 3.668 Satzpaare in den Trainingsdaten, 408 in den Validierungsdaten und 1.725 in den Testdaten)."),Y.forEach(s),xt=m(e),ce=d(e,"P",{});var es=u(ce);ii=a(es,"Dieser Befehl l\xE4dt das Dataset herunter und speichert es im Cache, standardm\xE4\xDFig in "),ms=d(es,"EM",{});var zo=u(ms);ri=a(zo,"~/.cache/huggingface/dataset"),zo.forEach(s),li=a(es,". Wir Erinnern uns an Kapitel 2, dass der Cache-Ordner anpasst werden kann, indem man die Umgebungsvariable "),fs=d(es,"CODE",{});var vo=u(fs);oi=a(vo,"HF_HOME"),vo.forEach(s),di=a(es," setzt."),es.forEach(s),St=m(e),xe=d(e,"P",{});var xa=u(xe);ui=a(xa,"Wir k\xF6nnen auf jedes Satzpaar in unserem "),bs=d(xa,"CODE",{});var $o=u(bs);ci=a($o,"raw_datasets"),$o.forEach(s),pi=a(xa,"-Objekt zugreifen, indem wir wie bei einem Dictionary einen Schl\xFCsselwert als Index verwenden:"),xa.forEach(s),Tt=m(e),D(Ye.$$.fragment,e),yt=m(e),D(Qe.$$.fragment,e),Pt=m(e),pe=d(e,"P",{});var ns=u(pe);hi=a(ns,"Wir stellen fest, dass die Labels bereits Ganzzahlen sind, sodass wir dort keine Vorverarbeitung durchf\xFChren m\xFCssen. Wir k\xF6nnen die "),gs=d(ns,"CODE",{});var Do=u(gs);mi=a(Do,"features"),Do.forEach(s),fi=a(ns," von "),_s=d(ns,"CODE",{});var jo=u(_s);bi=a(jo,"raw_train_dataset"),jo.forEach(s),gi=a(ns," untersuchen, um zu erfahren, welche Ganzzahl welchem Label entspricht. Der folgende Befehl gibt uns den Variablentyp zur\xFCck:"),ns.forEach(s),qt=m(e),D(Xe.$$.fragment,e),Ct=m(e),D(en.$$.fragment,e),At=m(e),B=d(e,"P",{});var N=u(B);_i=a(N,"Hinter den Kulissen ist "),ws=d(N,"CODE",{});var Eo=u(ws);wi=a(Eo,"label"),Eo.forEach(s),ki=a(N," vom Typ "),ks=d(N,"CODE",{});var xo=u(ks);zi=a(xo,"ClassLabel"),xo.forEach(s),vi=a(N,", und die Zuordnung von Ganzzahlen zum Labelnamen wird im Ordner "),zs=d(N,"EM",{});var So=u(zs);$i=a(So,"names"),So.forEach(s),Di=a(N," gespeichert. "),vs=d(N,"CODE",{});var To=u(vs);ji=a(To,"0"),To.forEach(s),Ei=a(N," entspricht "),$s=d(N,"CODE",{});var yo=u($s);xi=a(yo,"not_equivalent"),yo.forEach(s),Si=a(N,", also \u201Cnicht \xE4quivalent\u201D, und "),Ds=d(N,"CODE",{});var Po=u(Ds);Ti=a(Po,"1"),Po.forEach(s),yi=a(N," entspricht "),js=d(N,"CODE",{});var qo=u(js);Pi=a(qo,"equivalent"),qo.forEach(s),qi=a(N,", also \u201C\xE4quivalent\u201D."),N.forEach(s),Bt=m(e),D(Se.$$.fragment,e),Lt=m(e),ve=d(e,"H3",{class:!0});var Sa=u(ve);Te=d(Sa,"A",{id:!0,class:!0,href:!0});var Co=u(Te);Es=d(Co,"SPAN",{});var Ao=u(Es);D(nn.$$.fragment,Ao),Ao.forEach(s),Co.forEach(s),Ci=m(Sa),xs=d(Sa,"SPAN",{});var Bo=u(xs);Ai=a(Bo,"Vorverarbeitung eines Datensatzes"),Bo.forEach(s),Sa.forEach(s),Ot=m(e),te.l(e),Bn=m(e),ye=d(e,"P",{});var Ta=u(ye);Bi=a(Ta,"Um den Datensatz vorzubereiten, m\xFCssen wir den Text in Zahlen umwandeln, die das Modell sinnvoll verarbeiten kann. Im "),Ln=d(Ta,"A",{href:!0});var Lo=u(Ln);Li=a(Lo,"vorherigen Kapitel"),Lo.forEach(s),Oi=a(Ta," haben wir gesehen, dass dies mit einem Tokenizer gemacht wird. Wir k\xF6nnen den Tokenizer mit einem Satz oder einer Liste von S\xE4tzen f\xFCttern, sodass wir die ersten und zweiten S\xE4tze jedes Paares wie folgt direkt tokenisieren k\xF6nnen:"),Ta.forEach(s),Ft=m(e),D(sn.$$.fragment,e),Mt=m(e),On=d(e,"P",{});var Oo=u(On);Fi=a(Oo,"Wir k\xF6nnen jedoch nicht einfach zwei Sequenzen an das Modell \xFCbergeben und eine Vorhersage erhalten, ob die beiden S\xE4tze paraphrasiert sind oder nicht. Wir m\xFCssen die beiden Sequenzen als Paar behandeln und die entsprechende Vorverarbeitung anwenden. Gl\xFCcklicherweise kann der Tokenizer auch ein Sequenzpaar nehmen und es so vorbereiten, wie es unser BERT-Modell erwartet:"),Oo.forEach(s),Wt=m(e),D(tn.$$.fragment,e),It=m(e),D(an.$$.fragment,e),Vt=m(e),U=d(e,"P",{});var _e=u(U);Mi=a(_e,"In "),Fn=d(_e,"A",{href:!0});var Fo=u(Fn);Wi=a(Fo,"Kapitel 2"),Fo.forEach(s),Ii=a(_e," haben wir die Schl\xFCsselwerte "),Ss=d(_e,"CODE",{});var Mo=u(Ss);Vi=a(Mo,"input_ids"),Mo.forEach(s),Ni=a(_e," und "),Ts=d(_e,"CODE",{});var Wo=u(Ts);Ri=a(Wo,"attention_mask"),Wo.forEach(s),Hi=a(_e," behandelt, allerdings haben wir es aufgeschoben, \xFCber "),ys=d(_e,"CODE",{});var Io=u(ys);Ui=a(Io,"token_type_ids"),Io.forEach(s),Ki=a(_e," zu sprechen. In diesem Beispiel teilt diese dem Modell mit, welcher Teil des Input der erste Satz und welcher der zweite Satz ist."),_e.forEach(s),Nt=m(e),D(Pe.$$.fragment,e),Rt=m(e),qe=d(e,"P",{});var ya=u(qe);Gi=a(ya,"Wenn wir die IDs in "),Ps=d(ya,"CODE",{});var Vo=u(Ps);Zi=a(Vo,"input_ids"),Vo.forEach(s),Ji=a(ya," zur\xFCck in Worte dekodieren:"),ya.forEach(s),Ht=m(e),D(rn.$$.fragment,e),Ut=m(e),Mn=d(e,"P",{});var No=u(Mn);Yi=a(No,"dann bekommen wir:"),No.forEach(s),Kt=m(e),D(ln.$$.fragment,e),Gt=m(e),Ce=d(e,"P",{});var Pa=u(Ce);Qi=a(Pa,"Wir sehen also wenn es zwei S\xE4tze gibt, dass das Modell erwartet, dass die Inputs die Form \u201D[CLS] Satz1 [SEP] Satz2 [SEP]\u201D haben. Wenn wir dies mit den "),qs=d(Pa,"CODE",{});var Ro=u(qs);Xi=a(Ro,"token_type_ids"),Ro.forEach(s),er=a(Pa," abgleichen, erhalten wir:"),Pa.forEach(s),Zt=m(e),D(on.$$.fragment,e),Jt=m(e),K=d(e,"P",{});var we=u(K);nr=a(we,"Wie du sehen kannst, haben die Teile der Eingabe, die "),Cs=d(we,"CODE",{});var Ho=u(Cs);sr=a(Ho,"[CLS] Satz1 [SEP]"),Ho.forEach(s),tr=a(we," entsprechen, alle eine Token-Typ-ID von "),As=d(we,"CODE",{});var Uo=u(As);ar=a(Uo,"0"),Uo.forEach(s),ir=a(we,", w\xE4hrend die anderen Teile, die "),Bs=d(we,"CODE",{});var Ko=u(Bs);rr=a(Ko,"Satz2 [SEP]"),Ko.forEach(s),lr=a(we," entsprechen, alle einer Token-Typ-ID von "),Ls=d(we,"CODE",{});var Go=u(Ls);or=a(Go,"1"),Go.forEach(s),dr=a(we," enthalten."),we.forEach(s),Yt=m(e),Ae=d(e,"P",{});var qa=u(Ae);ur=a(qa,"Beachte, dass die Auswahl eines anderen Checkpoints nicht unbedingt die "),Os=d(qa,"CODE",{});var Zo=u(Os);cr=a(Zo,"token_type_ids"),Zo.forEach(s),pr=a(qa," in Ihren tokenisierten Inputs haben (z.B. werden sie nicht zur\xFCckgegeben, wenn ein DistilBERT-Modell verwendet wird). Sie werden nur zur\xFCckgegeben, wenn das Modell wei\xDF was damit zu tun ist, weil es die Toke-Typ-Ids w\xE4hrend des Vortrainings gesehen hat."),qa.forEach(s),Qt=m(e),he=d(e,"P",{});var ss=u(he);hr=a(ss,"In diesem Fall ist BERT mit Token-Typ-IDs vortrainiert worden, und zus\xE4tzlich zu dem maskierten Sprachmodellierungsziel aud "),Wn=d(ss,"A",{href:!0});var Jo=u(Wn);mr=a(Jo,"Kapitel 1"),Jo.forEach(s),fr=a(ss,", hat es ein zus\xE4tzliches Vorhersageziel namens "),Fs=d(ss,"EM",{});var Yo=u(Fs);br=a(Yo,"next sentence prediction"),Yo.forEach(s),gr=a(ss," (d.h. Vorhersage des n\xE4chsten Satzes). Das Ziel dieser Aufgabe ist es, die Beziehung zwischen Satzpaaren zu modellieren."),ss.forEach(s),Xt=m(e),In=d(e,"P",{});var Qo=u(In);_r=a(Qo,"Bei der Vorhersage des n\xE4chsten Satzes werden dem Modell Satzpaare (mit zuf\xE4llig maskierten Token) bereitgestellt und erwartet, vorherzusagen, ob auf den ersten Satz der zweite Satz folgt. Um die Aufgabe non-trivial zu machen, folgen sich die H\xE4lfte der S\xE4tze in dem Originaldokument, aus dem sie extrahiert wurden, aufeinander, und in der anderen H\xE4lfte stammen die beiden S\xE4tze aus zwei verschiedenen Dokumenten."),Qo.forEach(s),ea=m(e),Be=d(e,"P",{});var Ca=u(Be);wr=a(Ca,"Im Allgemeinen muss man sich keine Gedanken dar\xFCber machen, ob Ihre tokenisierten Inputs "),Ms=d(Ca,"CODE",{});var Xo=u(Ms);kr=a(Xo,"token_type_ids"),Xo.forEach(s),zr=a(Ca," enthalten oder nicht: Solange du denselben Checkpoint f\xFCr den Tokenizer und das Modell verwendest, ist alles in Ordnung, da der Tokenizer wei\xDF, was er dem Modell bereitstellen soll."),Ca.forEach(s),na=m(e),me=d(e,"P",{});var ts=u(me);vr=a(ts,"Nachdem wir nun gesehen haben, wie unser Tokenizer mit einem Satzpaar umgehen kann, k\xF6nnen wir damit unseren gesamten Datensatz tokenisieren: Wie im "),Vn=d(ts,"A",{href:!0});var ed=u(Vn);$r=a(ed,"vorherigen Kapitel"),ed.forEach(s),Dr=a(ts," k\xF6nnen wir dem Tokenizer eine Liste von Satzpaaren einspeisen, indem du ihm die Liste der ersten S\xE4tze und dann die Liste der zweiten S\xE4tze gibst. Dies ist auch kompatibel mit den Optionen zum Padding und Trunkieren, die wir in "),Nn=d(ts,"A",{href:!0});var nd=u(Nn);jr=a(nd,"Kapitel 2"),nd.forEach(s),Er=a(ts," gesehen haben. Eine M\xF6glichkeit, den Trainingsdatensatz vorzuverarbeiten, ist also:"),ts.forEach(s),sa=m(e),D(dn.$$.fragment,e),ta=m(e),G=d(e,"P",{});var ke=u(G);xr=a(ke,"Das funktioniert gut, hat aber den Nachteil, dass ein Dictionary zur\xFCckgegeben wird (mit unseren Schl\xFCsselw\xF6rtern "),Ws=d(ke,"CODE",{});var sd=u(Ws);Sr=a(sd,"input_ids"),sd.forEach(s),Tr=a(ke,", "),Is=d(ke,"CODE",{});var td=u(Is);yr=a(td,"attention_mask"),td.forEach(s),Pr=a(ke," und "),Vs=d(ke,"CODE",{});var ad=u(Vs);qr=a(ad,"token_type_ids"),ad.forEach(s),Cr=a(ke," und Werten aus Listen von Listen). Es funktioniert auch nur, wenn du gen\xFCgend RAM hast, um den gesamten Datensatz w\xE4hrend der Tokenisierung zu im RAM zwischen zu speichern (w\xE4hrend die Datens\xE4tze aus der Bibliothek \u{1F917} Datasets "),un=d(ke,"A",{href:!0,rel:!0});var id=u(un);Ar=a(id,"Apache Arrow"),id.forEach(s),Br=a(ke," Dateien sind, die auf der Festplatte gespeichert sind, sodass nur die gew\xFCnschten Samples im RAM geladen sind)."),ke.forEach(s),aa=m(e),fe=d(e,"P",{});var as=u(fe);Lr=a(as,"Um die Daten als Datensatz zu speichern, verwenden wir die Methode "),cn=d(as,"A",{href:!0,rel:!0});var rd=u(cn);Ns=d(rd,"CODE",{});var ld=u(Ns);Or=a(ld,"Dataset.map()"),ld.forEach(s),rd.forEach(s),Fr=a(as,". Dies gew\xE4hrt uns zus\xE4tzliche Flexibilit\xE4t, wenn wir zus\xE4tzliche Vorverarbeitung als nur die Tokenisierung ben\xF6tigen. Die "),Rs=d(as,"CODE",{});var od=u(Rs);Mr=a(od,"map()"),od.forEach(s),Wr=a(as,"-Methode funktioniert, indem sie eine Funktion auf jedes Element des Datensatzes anwendet, also definieren wir eine Funktion, die unsere Inputs tokenisiert:"),as.forEach(s),ia=m(e),D(pn.$$.fragment,e),ra=m(e),P=d(e,"P",{});var A=u(P);Ir=a(A,"Diese Funktion nimmt ein Dictionary (wie die Elemente unseres Datensatzes) und gibt ein neues Dictionary mit den Schl\xFCsselwerten "),Hs=d(A,"CODE",{});var dd=u(Hs);Vr=a(dd,"input_ids"),dd.forEach(s),Nr=a(A,", "),Us=d(A,"CODE",{});var ud=u(Us);Rr=a(ud,"attention_mask"),ud.forEach(s),Hr=a(A," und "),Ks=d(A,"CODE",{});var cd=u(Ks);Ur=a(cd,"token_type_ids"),cd.forEach(s),Kr=a(A," zur\xFCck. Beachte, dass es auch funktioniert, wenn das "),Gs=d(A,"CODE",{});var pd=u(Gs);Gr=a(pd,"example"),pd.forEach(s),Zr=a(A,"-Dictionary mehrere Beispiele enth\xE4lt (jeder Schl\xFCsselwert als Liste von S\xE4tzen), da der "),Zs=d(A,"CODE",{});var hd=u(Zs);Jr=a(hd,"Tokenizer"),hd.forEach(s),Yr=a(A,", wie zuvor gesehen, mit Listen von Satzpaaren arbeitet. Dadurch k\xF6nnen wir die Option "),Js=d(A,"CODE",{});var md=u(Js);Qr=a(md,"batched=True"),md.forEach(s),Xr=a(A," in unserem Aufruf von "),Ys=d(A,"CODE",{});var fd=u(Ys);el=a(fd,"map()"),fd.forEach(s),nl=a(A," verwenden, was die Tokenisierung erheblich beschleunigt. Der "),Qs=d(A,"CODE",{});var bd=u(Qs);sl=a(bd,"tokenizer"),bd.forEach(s),tl=a(A," wurde in Rust geschriebenen und ist in der Bibliothek "),hn=d(A,"A",{href:!0,rel:!0});var gd=u(hn);al=a(gd,"\u{1F917} Tokenizers"),gd.forEach(s),il=a(A," verf\xFCgbar. Dieser Tokenizer kann sehr schnell arbeiten, wenn wir ihm viele Inputs auf einmal zum Verarbeiten geben. Note that we\u2019ve left the "),Xs=d(A,"CODE",{});var _d=u(Xs);rl=a(_d,"padding"),_d.forEach(s),ll=a(A," argument out in our tokenization function for now."),A.forEach(s),la=m(e),Le=d(e,"P",{});var Aa=u(Le);ol=a(Aa,"Beachte, dass wir das "),et=d(Aa,"CODE",{});var wd=u(et);dl=a(wd,"padding"),wd.forEach(s),ul=a(Aa,"-Argument vorerst in unserer Tokenisierungsfunktion ausgelassen haben. Dies liegt daran, dass das Anwenden von Padding auf alle Elemente unserer Daten auf die maximale L\xE4nge nicht effizient ist: Es ist besser, die Proben aufzuf\xFCllen, wenn wir ein Batch erstellen, da wir dann nur auf die maximale L\xE4nge in diesem Batch auff\xFCllen m\xFCssen und nicht auf die maximale L\xE4nge in den gesamten Datensatz. Dies kann viel Zeit und Rechenleistung sparen, besonders wenn die Eingaben stark variable L\xE4ngen haben!"),Aa.forEach(s),oa=m(e),be=d(e,"P",{});var is=u(be);cl=a(is,"So wenden wir die Tokenisierungsfunktion auf alle unsere Datens\xE4tze gleichzeitig an. In unserem Aufruf von "),nt=d(is,"CODE",{});var kd=u(nt);pl=a(kd,"map"),kd.forEach(s),hl=a(is," verwenden wir "),st=d(is,"CODE",{});var zd=u(st);ml=a(zd,"batched=True"),zd.forEach(s),fl=a(is,", damit die Funktion auf mehrere Elemente des Datensatzes gleichzeitig angewendet wird und nicht auf jedes Element separat. Dies erm\xF6glicht eine schnellere Vorverarbeitung."),is.forEach(s),da=m(e),D(mn.$$.fragment,e),ua=m(e),Rn=d(e,"P",{});var vd=u(Rn);bl=a(vd,"Die Bibliothek \u{1F917} Datasets verarbeitet Datens\xE4tzen indem sie neue Felder hinzuzuf\xFCgen, eines f\xFCr jeden Schl\xFCssel im Dictionary, der von der Vorverarbeitungsfunktion zur\xFCckgegeben wird:"),vd.forEach(s),ca=m(e),D(fn.$$.fragment,e),pa=m(e),ge=d(e,"P",{});var rs=u(ge);gl=a(rs,"Du kannst sogar Multiprocessing verwenden, wenn du die Vorverarbeitungsfunktion mit "),tt=d(rs,"CODE",{});var $d=u(tt);_l=a($d,"map()"),$d.forEach(s),wl=a(rs," anwendest, indem du ein "),at=d(rs,"CODE",{});var Dd=u(at);kl=a(Dd,"num_proc"),Dd.forEach(s),zl=a(rs,"-Argument \xFCbergiebst. Wir haben dies hier nicht getan, weil die \u{1F917} Tokenizers-Bibliothek bereits mehrere Threads verwendet, um unsere Samples schneller zu tokenisieren. Wenn du keinen schnellen Tokenizer verwendest, der von dieser Bibliothek unterst\xFCtzt wird, w\xFCrde dies allerdings die Vorverarbeitung beschleunigen."),rs.forEach(s),ha=m(e),V=d(e,"P",{});var Q=u(V);vl=a(Q,"Unsere "),it=d(Q,"CODE",{});var jd=u(it);$l=a(jd,"tokenize_function"),jd.forEach(s),Dl=a(Q," gibt ein Dictionary mit den Schl\xFCsselwerten "),rt=d(Q,"CODE",{});var Ed=u(rt);jl=a(Ed,"input_ids"),Ed.forEach(s),El=a(Q,", "),lt=d(Q,"CODE",{});var xd=u(lt);xl=a(xd,"attention_mask"),xd.forEach(s),Sl=a(Q," und "),ot=d(Q,"CODE",{});var Sd=u(ot);Tl=a(Sd,"token_type_ids"),Sd.forEach(s),yl=a(Q," zur\xFCck, also werden diese drei Felder zu allen Splits unseres Datensatzes hinzugef\xFCgt. Beachte, dass wir auch vorhandene Felder \xE4ndern k\xF6nnten, wenn unsere Vorverarbeitungsfunktion einen neuen Wert f\xFCr einen vorhandenen Schl\xFCsselwert in dem Datensatz zur\xFCckgegeben h\xE4tte, auf den wir "),dt=d(Q,"CODE",{});var Td=u(dt);Pl=a(Td,"map()"),Td.forEach(s),ql=a(Q," angewendet haben."),Q.forEach(s),ma=m(e),Oe=d(e,"P",{});var Ba=u(Oe);Cl=a(Ba,"Zuletzt, m\xFCssen wir alle Beispiele auf die L\xE4nge des l\xE4ngsten Elements aufzuf\xFCllen, wenn wir Elemente zusammenfassen \u2013 eine Technik, die wir als "),ut=d(Ba,"EM",{});var yd=u(ut);Al=a(yd,"Dynamisches Padding"),yd.forEach(s),Bl=a(Ba," bezeichnen."),Ba.forEach(s),fa=m(e),$e=d(e,"H3",{class:!0});var La=u($e);Fe=d(La,"A",{id:!0,class:!0,href:!0});var Pd=u(Fe);ct=d(Pd,"SPAN",{});var qd=u(ct);D(bn.$$.fragment,qd),qd.forEach(s),Pd.forEach(s),Ll=m(La),pt=d(La,"SPAN",{});var Cd=u(pt);Ol=a(Cd,"Dynamisches Padding"),Cd.forEach(s),La.forEach(s),ba=m(e),D(gn.$$.fragment,e),ga=m(e),De.l(e),Hn=m(e),Me=d(e,"P",{});var Oa=u(Me);Fl=a(Oa,"In der Praxis m\xFCssen wir eine Collate-Funktion definieren, die die korrekte Menge an Padding auf die Elemente des Datensatzes anwendet, die wir in einem Batch haben m\xF6chten. Gl\xFCcklicherweise stellt uns die \u{1F917} Transformers-Bibliothek \xFCber "),ht=d(Oa,"CODE",{});var Ad=u(ht);Ml=a(Ad,"DataCollatorWithPadding"),Ad.forEach(s),Wl=a(Oa," eine solche Funktion zur Verf\xFCgung. Wenn sie instanziert wird, braucht es einen Tokenizer (um zu wissen, welches Padding-token verwendet werden soll und ob das Modell erwartet, dass sich das Padding links oder rechts von den Inputs befindet) und \xFCbernimmt alles was wir brauchen:"),Oa.forEach(s),_a=m(e),ie.l(e),Un=m(e),J=d(e,"P",{});var Re=u(J);Il=a(Re,"Um dieses neue Werkzeug zu testen, nehmen wir einige Elemente aus den Trainingsdaten, die wir als Batch verwenden m\xF6chten. Hier entfernen wir die Spalten "),mt=d(Re,"CODE",{});var Bd=u(mt);Vl=a(Bd,"idx"),Bd.forEach(s),Nl=a(Re,", "),ft=d(Re,"CODE",{});var Ld=u(ft);Rl=a(Ld,"sentence1"),Ld.forEach(s),Hl=a(Re," und "),bt=d(Re,"CODE",{});var Od=u(bt);Ul=a(Od,"sentence2"),Od.forEach(s),Kl=a(Re,", da sie nicht ben\xF6tigt werden und Strings enthalten (wir k\xF6nnen keine Tensoren mit Strings erstellen) und sehen uns die L\xE4nge jedes Eintrags im Batch an:"),Re.forEach(s),wa=m(e),D(_n.$$.fragment,e),ka=m(e),D(wn.$$.fragment,e),za=m(e),We=d(e,"P",{});var Fa=u(We);Gl=a(Fa,"Wenig \xFCberraschen erhalten wir Samples unterschiedlicher L\xE4nge von 32 bis 67. Dynamisches Padding bedeutet, dass die Elemente in diesem Batch alle auf eine L\xE4nge von 67 aufgef\xFCllt werden, die maximale L\xE4nge innerhalb des Batches. Ohne dynamisches Auff\xFCllen m\xFCssten alle Eintr\xE4ge auf die maximale L\xE4nge im gesamten Datensatz oder auf die maximale L\xE4nge die das Modell akzeptiert, aufgef\xFCllt werden. Lass uns noch einmal \xFCberpr\xFCfen, ob unser "),gt=d(Fa,"CODE",{});var Fd=u(gt);Zl=a(Fd,"data_collator"),Fd.forEach(s),Jl=a(Fa," den Stapel dynamisch richtig auff\xFCllt:"),Fa.forEach(s),va=m(e),D(kn.$$.fragment,e),$a=m(e),le.l(e),Kn=m(e),D(Ie.$$.fragment,e),Da=m(e),F&&F.l(e),Gn=Md(),this.h()},h(){T(i,"name","hf:doc:metadata"),T(i,"content",JSON.stringify(pu)),T(f,"id","vorbereitung-der-daten"),T(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(f,"href","#vorbereitung-der-daten"),T(z,"class","relative group"),T(de,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),T(de,"rel","nofollow"),T(Ee,"id","laden-eines-datensatzes-vom-hub"),T(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(Ee,"href","#laden-eines-datensatzes-vom-hub"),T(ze,"class","relative group"),T(Ke,"href","https://huggingface.co/datasets"),T(Ke,"rel","nofollow"),T(Ge,"href","https://gluebenchmark.com/"),T(Ge,"rel","nofollow"),T(Te,"id","vorverarbeitung-eines-datensatzes"),T(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(Te,"href","#vorverarbeitung-eines-datensatzes"),T(ve,"class","relative group"),T(Ln,"href","/course/chapter2"),T(Fn,"href","/course/chapter2"),T(Wn,"href","/course/chapter1"),T(Vn,"href","/course/chapter2"),T(Nn,"href","/course/chapter2"),T(un,"href","https://arrow.apache.org/"),T(un,"rel","nofollow"),T(cn,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),T(cn,"rel","nofollow"),T(hn,"href","https://github.com/huggingface/tokenizers"),T(hn,"rel","nofollow"),T(Fe,"id","dynamisches-padding"),T(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(Fe,"href","#dynamisches-padding"),T($e,"class","relative group")},m(e,l){n(document.head,i),c(e,p,l),j(r,e,l),c(e,g,l),c(e,z,l),n(z,f),n(f,b),j(S,b,null),n(z,k),n(z,v),n(v,L),c(e,M,l),zn[R].m(e,l),c(e,X,l),vn[O].m(e,l),c(e,Z,l),c(e,oe,l),n(oe,je),c(e,y,l),c(e,C,l),n(C,yn),n(C,de),n(de,Pn),n(C,qn),c(e,He,l),c(e,ze,l),n(ze,Ee),n(Ee,ls),j(Ue,ls,null),n(ze,Wa),n(ze,os),n(os,Ia),c(e,vt,l),$n[ee].m(e,l),c(e,Cn,l),c(e,ue,l),n(ue,Va),n(ue,Ke),n(Ke,Na),n(ue,Ra),n(ue,Ge),n(Ge,Ha),n(ue,Ua),c(e,$t,l),c(e,An,l),n(An,Ka),c(e,Dt,l),j(Ze,e,l),c(e,jt,l),j(Je,e,l),c(e,Et,l),c(e,I,l),n(I,Ga),n(I,ds),n(ds,Za),n(I,Ja),n(I,us),n(us,Ya),n(I,Qa),n(I,cs),n(cs,Xa),n(I,ei),n(I,ps),n(ps,ni),n(I,si),n(I,hs),n(hs,ti),n(I,ai),c(e,xt,l),c(e,ce,l),n(ce,ii),n(ce,ms),n(ms,ri),n(ce,li),n(ce,fs),n(fs,oi),n(ce,di),c(e,St,l),c(e,xe,l),n(xe,ui),n(xe,bs),n(bs,ci),n(xe,pi),c(e,Tt,l),j(Ye,e,l),c(e,yt,l),j(Qe,e,l),c(e,Pt,l),c(e,pe,l),n(pe,hi),n(pe,gs),n(gs,mi),n(pe,fi),n(pe,_s),n(_s,bi),n(pe,gi),c(e,qt,l),j(Xe,e,l),c(e,Ct,l),j(en,e,l),c(e,At,l),c(e,B,l),n(B,_i),n(B,ws),n(ws,wi),n(B,ki),n(B,ks),n(ks,zi),n(B,vi),n(B,zs),n(zs,$i),n(B,Di),n(B,vs),n(vs,ji),n(B,Ei),n(B,$s),n($s,xi),n(B,Si),n(B,Ds),n(Ds,Ti),n(B,yi),n(B,js),n(js,Pi),n(B,qi),c(e,Bt,l),j(Se,e,l),c(e,Lt,l),c(e,ve,l),n(ve,Te),n(Te,Es),j(nn,Es,null),n(ve,Ci),n(ve,xs),n(xs,Ai),c(e,Ot,l),Dn[se].m(e,l),c(e,Bn,l),c(e,ye,l),n(ye,Bi),n(ye,Ln),n(Ln,Li),n(ye,Oi),c(e,Ft,l),j(sn,e,l),c(e,Mt,l),c(e,On,l),n(On,Fi),c(e,Wt,l),j(tn,e,l),c(e,It,l),j(an,e,l),c(e,Vt,l),c(e,U,l),n(U,Mi),n(U,Fn),n(Fn,Wi),n(U,Ii),n(U,Ss),n(Ss,Vi),n(U,Ni),n(U,Ts),n(Ts,Ri),n(U,Hi),n(U,ys),n(ys,Ui),n(U,Ki),c(e,Nt,l),j(Pe,e,l),c(e,Rt,l),c(e,qe,l),n(qe,Gi),n(qe,Ps),n(Ps,Zi),n(qe,Ji),c(e,Ht,l),j(rn,e,l),c(e,Ut,l),c(e,Mn,l),n(Mn,Yi),c(e,Kt,l),j(ln,e,l),c(e,Gt,l),c(e,Ce,l),n(Ce,Qi),n(Ce,qs),n(qs,Xi),n(Ce,er),c(e,Zt,l),j(on,e,l),c(e,Jt,l),c(e,K,l),n(K,nr),n(K,Cs),n(Cs,sr),n(K,tr),n(K,As),n(As,ar),n(K,ir),n(K,Bs),n(Bs,rr),n(K,lr),n(K,Ls),n(Ls,or),n(K,dr),c(e,Yt,l),c(e,Ae,l),n(Ae,ur),n(Ae,Os),n(Os,cr),n(Ae,pr),c(e,Qt,l),c(e,he,l),n(he,hr),n(he,Wn),n(Wn,mr),n(he,fr),n(he,Fs),n(Fs,br),n(he,gr),c(e,Xt,l),c(e,In,l),n(In,_r),c(e,ea,l),c(e,Be,l),n(Be,wr),n(Be,Ms),n(Ms,kr),n(Be,zr),c(e,na,l),c(e,me,l),n(me,vr),n(me,Vn),n(Vn,$r),n(me,Dr),n(me,Nn),n(Nn,jr),n(me,Er),c(e,sa,l),j(dn,e,l),c(e,ta,l),c(e,G,l),n(G,xr),n(G,Ws),n(Ws,Sr),n(G,Tr),n(G,Is),n(Is,yr),n(G,Pr),n(G,Vs),n(Vs,qr),n(G,Cr),n(G,un),n(un,Ar),n(G,Br),c(e,aa,l),c(e,fe,l),n(fe,Lr),n(fe,cn),n(cn,Ns),n(Ns,Or),n(fe,Fr),n(fe,Rs),n(Rs,Mr),n(fe,Wr),c(e,ia,l),j(pn,e,l),c(e,ra,l),c(e,P,l),n(P,Ir),n(P,Hs),n(Hs,Vr),n(P,Nr),n(P,Us),n(Us,Rr),n(P,Hr),n(P,Ks),n(Ks,Ur),n(P,Kr),n(P,Gs),n(Gs,Gr),n(P,Zr),n(P,Zs),n(Zs,Jr),n(P,Yr),n(P,Js),n(Js,Qr),n(P,Xr),n(P,Ys),n(Ys,el),n(P,nl),n(P,Qs),n(Qs,sl),n(P,tl),n(P,hn),n(hn,al),n(P,il),n(P,Xs),n(Xs,rl),n(P,ll),c(e,la,l),c(e,Le,l),n(Le,ol),n(Le,et),n(et,dl),n(Le,ul),c(e,oa,l),c(e,be,l),n(be,cl),n(be,nt),n(nt,pl),n(be,hl),n(be,st),n(st,ml),n(be,fl),c(e,da,l),j(mn,e,l),c(e,ua,l),c(e,Rn,l),n(Rn,bl),c(e,ca,l),j(fn,e,l),c(e,pa,l),c(e,ge,l),n(ge,gl),n(ge,tt),n(tt,_l),n(ge,wl),n(ge,at),n(at,kl),n(ge,zl),c(e,ha,l),c(e,V,l),n(V,vl),n(V,it),n(it,$l),n(V,Dl),n(V,rt),n(rt,jl),n(V,El),n(V,lt),n(lt,xl),n(V,Sl),n(V,ot),n(ot,Tl),n(V,yl),n(V,dt),n(dt,Pl),n(V,ql),c(e,ma,l),c(e,Oe,l),n(Oe,Cl),n(Oe,ut),n(ut,Al),n(Oe,Bl),c(e,fa,l),c(e,$e,l),n($e,Fe),n(Fe,ct),j(bn,ct,null),n($e,Ll),n($e,pt),n(pt,Ol),c(e,ba,l),j(gn,e,l),c(e,ga,l),De.m(e,l),c(e,Hn,l),c(e,Me,l),n(Me,Fl),n(Me,ht),n(ht,Ml),n(Me,Wl),c(e,_a,l),jn[ae].m(e,l),c(e,Un,l),c(e,J,l),n(J,Il),n(J,mt),n(mt,Vl),n(J,Nl),n(J,ft),n(ft,Rl),n(J,Hl),n(J,bt),n(bt,Ul),n(J,Kl),c(e,wa,l),j(_n,e,l),c(e,ka,l),j(wn,e,l),c(e,za,l),c(e,We,l),n(We,Gl),n(We,gt),n(gt,Zl),n(We,Jl),c(e,va,l),j(kn,e,l),c(e,$a,l),En[re].m(e,l),c(e,Kn,l),j(Ie,e,l),c(e,Da,l),F&&F.m(e,l),c(e,Gn,l),ja=!0},p(e,[l]){const xn={};l&1&&(xn.fw=e[0]),r.$set(xn);let Zn=R;R=Xl(e),R!==Zn&&(Tn(),w(zn[Zn],1,1,()=>{zn[Zn]=null}),Sn(),H=zn[R],H||(H=zn[R]=Ql[R](e),H.c()),_(H,1),H.m(X.parentNode,X));let Jn=O;O=no(e),O!==Jn&&(Tn(),w(vn[Jn],1,1,()=>{vn[Jn]=null}),Sn(),W=vn[O],W||(W=vn[O]=eo[O](e),W.c()),_(W,1),W.m(Z.parentNode,Z));let Yn=ee;ee=to(e),ee!==Yn&&(Tn(),w($n[Yn],1,1,()=>{$n[Yn]=null}),Sn(),ne=$n[ee],ne||(ne=$n[ee]=so[ee](e),ne.c()),_(ne,1),ne.m(Cn.parentNode,Cn));const _t={};l&2&&(_t.$$scope={dirty:l,ctx:e}),Se.$set(_t);let Ve=se;se=io(e),se!==Ve&&(Tn(),w(Dn[Ve],1,1,()=>{Dn[Ve]=null}),Sn(),te=Dn[se],te||(te=Dn[se]=ao[se](e),te.c()),_(te,1),te.m(Bn.parentNode,Bn));const wt={};l&2&&(wt.$$scope={dirty:l,ctx:e}),Pe.$set(wt),Ea!==(Ea=ro(e))&&(De.d(1),De=Ea(e),De&&(De.c(),De.m(Hn.parentNode,Hn)));let Ne=ae;ae=oo(e),ae!==Ne&&(Tn(),w(jn[Ne],1,1,()=>{jn[Ne]=null}),Sn(),ie=jn[ae],ie||(ie=jn[ae]=lo[ae](e),ie.c()),_(ie,1),ie.m(Un.parentNode,Un));let Qn=re;re=co(e),re!==Qn&&(Tn(),w(En[Qn],1,1,()=>{En[Qn]=null}),Sn(),le=En[re],le||(le=En[re]=uo[re](e),le.c()),_(le,1),le.m(Kn.parentNode,Kn));const kt={};l&2&&(kt.$$scope={dirty:l,ctx:e}),Ie.$set(kt),e[0]==="tf"?F?l&1&&_(F,1):(F=Wd(),F.c(),_(F,1),F.m(Gn.parentNode,Gn)):F&&(Tn(),w(F,1,1,()=>{F=null}),Sn())},i(e){ja||(_(r.$$.fragment,e),_(S.$$.fragment,e),_(H),_(W),_(Ue.$$.fragment,e),_(ne),_(Ze.$$.fragment,e),_(Je.$$.fragment,e),_(Ye.$$.fragment,e),_(Qe.$$.fragment,e),_(Xe.$$.fragment,e),_(en.$$.fragment,e),_(Se.$$.fragment,e),_(nn.$$.fragment,e),_(te),_(sn.$$.fragment,e),_(tn.$$.fragment,e),_(an.$$.fragment,e),_(Pe.$$.fragment,e),_(rn.$$.fragment,e),_(ln.$$.fragment,e),_(on.$$.fragment,e),_(dn.$$.fragment,e),_(pn.$$.fragment,e),_(mn.$$.fragment,e),_(fn.$$.fragment,e),_(bn.$$.fragment,e),_(gn.$$.fragment,e),_(ie),_(_n.$$.fragment,e),_(wn.$$.fragment,e),_(kn.$$.fragment,e),_(le),_(Ie.$$.fragment,e),_(F),ja=!0)},o(e){w(r.$$.fragment,e),w(S.$$.fragment,e),w(H),w(W),w(Ue.$$.fragment,e),w(ne),w(Ze.$$.fragment,e),w(Je.$$.fragment,e),w(Ye.$$.fragment,e),w(Qe.$$.fragment,e),w(Xe.$$.fragment,e),w(en.$$.fragment,e),w(Se.$$.fragment,e),w(nn.$$.fragment,e),w(te),w(sn.$$.fragment,e),w(tn.$$.fragment,e),w(an.$$.fragment,e),w(Pe.$$.fragment,e),w(rn.$$.fragment,e),w(ln.$$.fragment,e),w(on.$$.fragment,e),w(dn.$$.fragment,e),w(pn.$$.fragment,e),w(mn.$$.fragment,e),w(fn.$$.fragment,e),w(bn.$$.fragment,e),w(gn.$$.fragment,e),w(ie),w(_n.$$.fragment,e),w(wn.$$.fragment,e),w(kn.$$.fragment,e),w(le),w(Ie.$$.fragment,e),w(F),ja=!1},d(e){s(i),e&&s(p),E(r,e),e&&s(g),e&&s(z),E(S),e&&s(M),zn[R].d(e),e&&s(X),vn[O].d(e),e&&s(Z),e&&s(oe),e&&s(y),e&&s(C),e&&s(He),e&&s(ze),E(Ue),e&&s(vt),$n[ee].d(e),e&&s(Cn),e&&s(ue),e&&s($t),e&&s(An),e&&s(Dt),E(Ze,e),e&&s(jt),E(Je,e),e&&s(Et),e&&s(I),e&&s(xt),e&&s(ce),e&&s(St),e&&s(xe),e&&s(Tt),E(Ye,e),e&&s(yt),E(Qe,e),e&&s(Pt),e&&s(pe),e&&s(qt),E(Xe,e),e&&s(Ct),E(en,e),e&&s(At),e&&s(B),e&&s(Bt),E(Se,e),e&&s(Lt),e&&s(ve),E(nn),e&&s(Ot),Dn[se].d(e),e&&s(Bn),e&&s(ye),e&&s(Ft),E(sn,e),e&&s(Mt),e&&s(On),e&&s(Wt),E(tn,e),e&&s(It),E(an,e),e&&s(Vt),e&&s(U),e&&s(Nt),E(Pe,e),e&&s(Rt),e&&s(qe),e&&s(Ht),E(rn,e),e&&s(Ut),e&&s(Mn),e&&s(Kt),E(ln,e),e&&s(Gt),e&&s(Ce),e&&s(Zt),E(on,e),e&&s(Jt),e&&s(K),e&&s(Yt),e&&s(Ae),e&&s(Qt),e&&s(he),e&&s(Xt),e&&s(In),e&&s(ea),e&&s(Be),e&&s(na),e&&s(me),e&&s(sa),E(dn,e),e&&s(ta),e&&s(G),e&&s(aa),e&&s(fe),e&&s(ia),E(pn,e),e&&s(ra),e&&s(P),e&&s(la),e&&s(Le),e&&s(oa),e&&s(be),e&&s(da),E(mn,e),e&&s(ua),e&&s(Rn),e&&s(ca),E(fn,e),e&&s(pa),e&&s(ge),e&&s(ha),e&&s(V),e&&s(ma),e&&s(Oe),e&&s(fa),e&&s($e),E(bn),e&&s(ba),E(gn,e),e&&s(ga),De.d(e),e&&s(Hn),e&&s(Me),e&&s(_a),jn[ae].d(e),e&&s(Un),e&&s(J),e&&s(wa),E(_n,e),e&&s(ka),E(wn,e),e&&s(za),e&&s(We),e&&s(va),E(kn,e),e&&s($a),En[re].d(e),e&&s(Kn),E(Ie,e),e&&s(Da),F&&F.d(e),e&&s(Gn)}}}const pu={local:"vorbereitung-der-daten",sections:[{local:"laden-eines-datensatzes-vom-hub",title:"Laden eines Datensatzes vom Hub"},{local:"vorverarbeitung-eines-datensatzes",title:"Vorverarbeitung eines Datensatzes"},{local:"dynamisches-padding",title:"Dynamisches Padding"}],title:"Vorbereitung der Daten"};function hu(x,i,p){let r="pt";return Ud(()=>{const g=new URLSearchParams(window.location.search);p(0,r=g.get("fw")||"pt")}),[r]}class wu extends Vd{constructor(i){super();Nd(this,i,hu,cu,Rd,{})}}export{wu as default,pu as metadata};
