import{S as Oh,i as Kh,s as Lh,e as l,t as a,c as o,a as r,h as n,d as t,g as p,F as s,k as c,w,m as _,x,y as E,q as g,o as v,B as j,b as y,M as Rh,N as yr,p as Pt,v as Bh,n as Dt}from"../../chunks/vendor-1e8b365d.js";import{T as xa}from"../../chunks/Tip-62b14c6e.js";import{Y as zu}from"../../chunks/Youtube-c2a8cc39.js";import{I as Ea}from"../../chunks/IconCopyLink-483c28ba.js";import{C as D}from"../../chunks/CodeBlock-e5764662.js";import{D as Nh}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Fh}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Hh(B){let i,h;return i=new Nh({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"}]}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Ih(B){let i,h;return i=new Nh({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"}]}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Uh(B){let i,h,d,$,P;return{c(){i=l("p"),h=a("\u{1F64B} Si les termes \u201Cmod\xE9lisation du langage masqu\xE9\u201D et \u201Cmod\xE8le pr\xE9-entra\xEEn\xE9\u201D ne vous sont pas familiers, consultez le "),d=l("a"),$=a("Chapitre 1"),P=a(", o\xF9 nous expliquons tous ces concepts fondamentaux, vid\xE9os \xE0 l\u2019appui !"),this.h()},l(b){i=o(b,"P",{});var k=r(i);h=n(k,"\u{1F64B} Si les termes \u201Cmod\xE9lisation du langage masqu\xE9\u201D et \u201Cmod\xE8le pr\xE9-entra\xEEn\xE9\u201D ne vous sont pas familiers, consultez le "),d=o(k,"A",{href:!0});var M=r(d);$=n(M,"Chapitre 1"),M.forEach(t),P=n(k,", o\xF9 nous expliquons tous ces concepts fondamentaux, vid\xE9os \xE0 l\u2019appui !"),k.forEach(t),this.h()},h(){y(d,"href","/course/fr/chapiter1")},m(b,k){p(b,i,k),s(i,h),s(i,d),s(d,$),s(i,P)},d(b){b&&t(i)}}}function Gh(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U;return k=new D({props:{code:`from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),S=new D({props:{code:`model(model.dummy_inputs)  # Build the model
model.summary()`,highlighted:`model(model.dummy_inputs)  <span class="hljs-comment"># Build the model</span>
model.summary()`}}),H=new D({props:{code:`Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________`,highlighted:`Model: <span class="hljs-string">&quot;tf_distil_bert_for_masked_lm&quot;</span>
_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#   </span>
=================================================================
distilbert (TFDistilBertMain multiple                  <span class="hljs-number">66362880</span>  
_________________________________________________________________
vocab_transform (Dense)      multiple                  <span class="hljs-number">590592</span>    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  <span class="hljs-number">1536</span>      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  <span class="hljs-number">23866170</span>  
=================================================================
Total params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Trainable params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){i=l("p"),h=a("Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=l("code"),$=a("AutoModelForMaskedLM"),P=a(" :"),b=c(),w(k.$$.fragment),M=c(),f=l("p"),C=a("Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=l("code"),K=a("summary()"),L=a(" :"),F=c(),w(S.$$.fragment),A=c(),w(H.$$.fragment)},l(q){i=o(q,"P",{});var T=r(i);h=n(T,"Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=o(T,"CODE",{});var Y=r(d);$=n(Y,"AutoModelForMaskedLM"),Y.forEach(t),P=n(T," :"),T.forEach(t),b=_(q),x(k.$$.fragment,q),M=_(q),f=o(q,"P",{});var Z=r(f);C=n(Z,"Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=o(Z,"CODE",{});var Q=r(O);K=n(Q,"summary()"),Q.forEach(t),L=n(Z," :"),Z.forEach(t),F=_(q),x(S.$$.fragment,q),A=_(q),x(H.$$.fragment,q)},m(q,T){p(q,i,T),s(i,h),s(i,d),s(d,$),s(i,P),p(q,b,T),E(k,q,T),p(q,M,T),p(q,f,T),s(f,C),s(f,O),s(O,K),s(f,L),p(q,F,T),E(S,q,T),p(q,A,T),E(H,q,T),U=!0},i(q){U||(g(k.$$.fragment,q),g(S.$$.fragment,q),g(H.$$.fragment,q),U=!0)},o(q){v(k.$$.fragment,q),v(S.$$.fragment,q),v(H.$$.fragment,q),U=!1},d(q){q&&t(i),q&&t(b),j(k,q),q&&t(M),q&&t(f),q&&t(F),j(S,q),q&&t(A),j(H,q)}}}function Vh(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U;return k=new D({props:{code:`from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),S=new D({props:{code:`distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")`,highlighted:`distilbert_num_parameters = model.num_parameters() / <span class="hljs-number">1_000_000</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; DistilBERT number of parameters: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(distilbert_num_parameters)}</span>M&#x27;&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; BERT number of parameters: 110M&#x27;&quot;</span>)`}}),H=new D({props:{code:`'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; DistilBERT number of parameters: 67M&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; BERT number of parameters: 110M&#x27;</span>`}}),{c(){i=l("p"),h=a("Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=l("code"),$=a("AutoModelForMaskedLM"),P=a(" :"),b=c(),w(k.$$.fragment),M=c(),f=l("p"),C=a("Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=l("code"),K=a("num_parameters()"),L=a(" :"),F=c(),w(S.$$.fragment),A=c(),w(H.$$.fragment)},l(q){i=o(q,"P",{});var T=r(i);h=n(T,"Allons-y et t\xE9l\xE9chargeons DistilBERT en utilisant la classe "),d=o(T,"CODE",{});var Y=r(d);$=n(Y,"AutoModelForMaskedLM"),Y.forEach(t),P=n(T," :"),T.forEach(t),b=_(q),x(k.$$.fragment,q),M=_(q),f=o(q,"P",{});var Z=r(f);C=n(Z,"Nous pouvons voir combien de param\xE8tres ce mod\xE8le poss\xE8de en appelant la m\xE9thode "),O=o(Z,"CODE",{});var Q=r(O);K=n(Q,"num_parameters()"),Q.forEach(t),L=n(Z," :"),Z.forEach(t),F=_(q),x(S.$$.fragment,q),A=_(q),x(H.$$.fragment,q)},m(q,T){p(q,i,T),s(i,h),s(i,d),s(d,$),s(i,P),p(q,b,T),E(k,q,T),p(q,M,T),p(q,f,T),s(f,C),s(f,O),s(O,K),s(f,L),p(q,F,T),E(S,q,T),p(q,A,T),E(H,q,T),U=!0},i(q){U||(g(k.$$.fragment,q),g(S.$$.fragment,q),g(H.$$.fragment,q),U=!0)},o(q){v(k.$$.fragment,q),v(S.$$.fragment,q),v(H.$$.fragment,q),U=!1},d(q){q&&t(i),q&&t(b),j(k,q),q&&t(M),q&&t(f),q&&t(F),j(S,q),q&&t(A),j(H,q)}}}function Wh(B){let i,h;return i=new D({props:{code:`import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Trouvez l'emplacement de [MASK] et extrayez ses logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# On choisit les candidats [MASK] avec les logits les plus \xE9lev\xE9s
# Nous annulons le tableau avant argsort pour obtenir le plus grand, et non le plus petit, logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># Trouvez l&#x27;emplacement de [MASK] et extrayez ses logits</span>
mask_token_index = np.argwhere(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># On choisit les candidats [MASK] avec les logits les plus \xE9lev\xE9s</span>
<span class="hljs-comment"># Nous annulons le tableau avant argsort pour obtenir le plus grand, et non le plus petit, logits</span>
top_5_tokens = np.argsort(-mask_token_logits)[:<span class="hljs-number">5</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Jh(B){let i,h;return i=new D({props:{code:`import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Trouvez l'emplacement de [MASK] et extrayez ses logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Choisissez les candidats [MASK] avec les logits les plus \xE9lev\xE9s
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")`,highlighted:`<span class="hljs-keyword">import</span> torch

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># Trouvez l&#x27;emplacement de [MASK] et extrayez ses logits</span>
mask_token_index = torch.where(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># Choisissez les candidats [MASK] avec les logits les plus \xE9lev\xE9s</span>
top_5_tokens = torch.topk(mask_token_logits, <span class="hljs-number">5</span>, dim=<span class="hljs-number">1</span>).indices[<span class="hljs-number">0</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&#x27;&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Yh(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q,G,W,ne;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),P=a(" Cr\xE9ez un \xE9chantillon al\xE9atoire de la r\xE9partition "),b=l("code"),k=a("unsupervised"),M=a(" et v\xE9rifiez que les \xE9tiquettes ne sont ni "),f=l("code"),C=a("0"),O=a(" ni "),K=l("code"),L=a("1"),F=a(". Pendant que vous y \xEAtes, vous pouvez aussi v\xE9rifier que les \xE9tiquettes dans les fractions "),S=l("code"),A=a("train"),H=a(" et "),U=l("code"),q=a("test"),T=a(" sont bien "),Y=l("code"),Z=a("0"),Q=a(" ou "),G=l("code"),W=a("1"),ne=a(". C\u2019est un contr\xF4le utile que tout praticien en NLP devrait effectuer au d\xE9but d\u2019un nouveau projet !")},l(V){i=o(V,"P",{});var I=r(i);h=n(I,"\u270F\uFE0F "),d=o(I,"STRONG",{});var re=r(d);$=n(re,"Essayez !"),re.forEach(t),P=n(I," Cr\xE9ez un \xE9chantillon al\xE9atoire de la r\xE9partition "),b=o(I,"CODE",{});var X=r(b);k=n(X,"unsupervised"),X.forEach(t),M=n(I," et v\xE9rifiez que les \xE9tiquettes ne sont ni "),f=o(I,"CODE",{});var qe=r(f);C=n(qe,"0"),qe.forEach(t),O=n(I," ni "),K=o(I,"CODE",{});var le=r(K);L=n(le,"1"),le.forEach(t),F=n(I,". Pendant que vous y \xEAtes, vous pouvez aussi v\xE9rifier que les \xE9tiquettes dans les fractions "),S=o(I,"CODE",{});var Ee=r(S);A=n(Ee,"train"),Ee.forEach(t),H=n(I," et "),U=o(I,"CODE",{});var Be=r(U);q=n(Be,"test"),Be.forEach(t),T=n(I," sont bien "),Y=o(I,"CODE",{});var ie=r(Y);Z=n(ie,"0"),ie.forEach(t),Q=n(I," ou "),G=o(I,"CODE",{});var $e=r(G);W=n($e,"1"),$e.forEach(t),ne=n(I,". C\u2019est un contr\xF4le utile que tout praticien en NLP devrait effectuer au d\xE9but d\u2019un nouveau projet !"),I.forEach(t)},m(V,I){p(V,i,I),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M),s(i,f),s(f,C),s(i,O),s(i,K),s(K,L),s(i,F),s(i,S),s(S,A),s(i,H),s(i,U),s(U,q),s(i,T),s(i,Y),s(Y,Z),s(i,Q),s(i,G),s(G,W),s(i,ne)},d(V){V&&t(i)}}}function Qh(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),P=a(" Certains "),b=l("em"),k=a("transformers"),M=a(", comme "),f=l("a"),C=a("BigBird"),O=a(" et "),K=l("a"),L=a("Longformer"),F=a(", ont une longueur de contexte beaucoup plus longue que BERT et les autres premiers "),S=l("em"),A=a("transformers"),H=a(". Instanciez le "),U=l("em"),q=a("tokenizer"),T=a(" pour l\u2019un de ces points de contr\xF4le et v\xE9rifiez que le "),Y=l("code"),Z=a("model_max_length"),Q=a(" correspond \xE0 ce qui est indiqu\xE9 sur sa carte."),this.h()},l(G){i=o(G,"P",{});var W=r(i);h=n(W,"\u270F\uFE0F "),d=o(W,"STRONG",{});var ne=r(d);$=n(ne,"Essayez !"),ne.forEach(t),P=n(W," Certains "),b=o(W,"EM",{});var V=r(b);k=n(V,"transformers"),V.forEach(t),M=n(W,", comme "),f=o(W,"A",{href:!0,rel:!0});var I=r(f);C=n(I,"BigBird"),I.forEach(t),O=n(W," et "),K=o(W,"A",{href:!0});var re=r(K);L=n(re,"Longformer"),re.forEach(t),F=n(W,", ont une longueur de contexte beaucoup plus longue que BERT et les autres premiers "),S=o(W,"EM",{});var X=r(S);A=n(X,"transformers"),X.forEach(t),H=n(W,". Instanciez le "),U=o(W,"EM",{});var qe=r(U);q=n(qe,"tokenizer"),qe.forEach(t),T=n(W," pour l\u2019un de ces points de contr\xF4le et v\xE9rifiez que le "),Y=o(W,"CODE",{});var le=r(Y);Z=n(le,"model_max_length"),le.forEach(t),Q=n(W," correspond \xE0 ce qui est indiqu\xE9 sur sa carte."),W.forEach(t),this.h()},h(){y(f,"href","https://huggingface.co/google/bigbird-roberta-base"),y(f,"rel","nofollow"),y(K,"href","hf.co/allenai/longformer-base-4096")},m(G,W){p(G,i,W),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M),s(i,f),s(f,C),s(i,O),s(i,K),s(K,L),s(i,F),s(i,S),s(S,A),s(i,H),s(i,U),s(U,q),s(i,T),s(i,Y),s(Y,Z),s(i,Q)},d(G){G&&t(i)}}}function Xh(B){let i,h;return{c(){i=l("p"),h=a("Notez que l\u2019utilisation d\u2019une petite taille de fragment peut \xEAtre pr\xE9judiciable dans les sc\xE9narios du monde r\xE9el, vous devez donc utiliser une taille qui correspond au cas d\u2019utilisation auquel vous appliquerez votre mod\xE8le.")},l(d){i=o(d,"P",{});var $=r(i);h=n($,"Notez que l\u2019utilisation d\u2019une petite taille de fragment peut \xEAtre pr\xE9judiciable dans les sc\xE9narios du monde r\xE9el, vous devez donc utiliser une taille qui correspond au cas d\u2019utilisation auquel vous appliquerez votre mod\xE8le."),$.forEach(t)},m(d,$){p(d,i,$),s(i,h)},d(d){d&&t(i)}}}function Zh(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez"),P=a(" Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),b=l("code"),k=a("tokenizer.decode()"),M=a(" par "),f=l("code"),C=a("tokenizer.convert_ids_to_tokens()"),O=a(" pour voir que parfois un seul "),K=l("em"),L=a("token"),F=a(" d\u2019un mot donn\xE9 est masqu\xE9, et pas les autres.")},l(S){i=o(S,"P",{});var A=r(i);h=n(A,"\u270F\uFE0F "),d=o(A,"STRONG",{});var H=r(d);$=n(H,"Essayez"),H.forEach(t),P=n(A," Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),b=o(A,"CODE",{});var U=r(b);k=n(U,"tokenizer.decode()"),U.forEach(t),M=n(A," par "),f=o(A,"CODE",{});var q=r(f);C=n(q,"tokenizer.convert_ids_to_tokens()"),q.forEach(t),O=n(A," pour voir que parfois un seul "),K=o(A,"EM",{});var T=r(K);L=n(T,"token"),T.forEach(t),F=n(A," d\u2019un mot donn\xE9 est masqu\xE9, et pas les autres."),A.forEach(t)},m(S,A){p(S,i,A),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M),s(i,f),s(f,C),s(i,O),s(i,K),s(K,L),s(i,F)},d(S){S&&t(i)}}}function Ah(B){let i,h,d,$,P,b,k,M,f,C,O;return{c(){i=l("p"),h=a("Un effet secondaire du masquage al\xE9atoire est que nos m\xE9triques d\u2019\xE9valuation ne seront pas d\xE9terministes lorsque nous utilisons le "),d=l("code"),$=a("Trainer"),P=a(", puisque nous utilisons le m\xEAme collateur de donn\xE9es pour les ensembles d\u2019entra\xEEnement et de test. Nous verrons plus tard, lorsque nous examinerons le "),b=l("em"),k=a("finetuning"),M=a(" avec \u{1F917} "),f=l("em"),C=a("Accelerate"),O=a(", comment nous pouvons utiliser la flexibilit\xE9 d\u2019une boucle d\u2019\xE9valuation personnalis\xE9e pour geler le caract\xE8re al\xE9atoire.")},l(K){i=o(K,"P",{});var L=r(i);h=n(L,"Un effet secondaire du masquage al\xE9atoire est que nos m\xE9triques d\u2019\xE9valuation ne seront pas d\xE9terministes lorsque nous utilisons le "),d=o(L,"CODE",{});var F=r(d);$=n(F,"Trainer"),F.forEach(t),P=n(L,", puisque nous utilisons le m\xEAme collateur de donn\xE9es pour les ensembles d\u2019entra\xEEnement et de test. Nous verrons plus tard, lorsque nous examinerons le "),b=o(L,"EM",{});var S=r(b);k=n(S,"finetuning"),S.forEach(t),M=n(L," avec \u{1F917} "),f=o(L,"EM",{});var A=r(f);C=n(A,"Accelerate"),A.forEach(t),O=n(L,", comment nous pouvons utiliser la flexibilit\xE9 d\u2019une boucle d\u2019\xE9valuation personnalis\xE9e pour geler le caract\xE8re al\xE9atoire."),L.forEach(t)},m(K,L){p(K,i,L),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M),s(i,f),s(f,C),s(i,O)},d(K){K&&t(i)}}}function eg(B){let i,h;return i=new D({props:{code:`import collections
import numpy as np

from transformers.data import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr\xE9ation d'une carte entre les mots et les indices des tokens correspondants.
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa\xE7on al\xE9atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return tf_default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers.data <span class="hljs-keyword">import</span> tf_default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># Cr\xE9ation d&#x27;une carte entre les mots et les indices des tokens correspondants.</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Masquer des mots de fa\xE7on al\xE9atoire</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> tf_default_data_collator(features)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function sg(B){let i,h;return i=new D({props:{code:`import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr\xE9ation d'une carte entre les mots et les indices des tokens correspondants.
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa\xE7on al\xE9atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># Cr\xE9ation d&#x27;une carte entre les mots et les indices des tokens correspondants.</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Masquer des mots de fa\xE7on al\xE9atoire</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> default_data_collator(features)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function tg(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez"),P=a(" Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),b=l("code"),k=a("tokenizer.decode()"),M=a(" par "),f=l("code"),C=a("tokenizer.convert_ids_to_tokens()"),O=a(" pour voir que les "),K=l("em"),L=a("tokens"),F=a(" d\u2019un mot donn\xE9 sont toujours masqu\xE9s ensemble.")},l(S){i=o(S,"P",{});var A=r(i);h=n(A,"\u270F\uFE0F "),d=o(A,"STRONG",{});var H=r(d);$=n(H,"Essayez"),H.forEach(t),P=n(A," Ex\xE9cutez le code ci-dessus plusieurs fois pour voir le masquage al\xE9atoire se produire sous vos yeux ! Remplacez aussi la m\xE9thode "),b=o(A,"CODE",{});var U=r(b);k=n(U,"tokenizer.decode()"),U.forEach(t),M=n(A," par "),f=o(A,"CODE",{});var q=r(f);C=n(q,"tokenizer.convert_ids_to_tokens()"),q.forEach(t),O=n(A," pour voir que les "),K=o(A,"EM",{});var T=r(K);L=n(T,"tokens"),T.forEach(t),F=n(A," d\u2019un mot donn\xE9 sont toujours masqu\xE9s ensemble."),A.forEach(t)},m(S,A){p(S,i,A),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M),s(i,f),s(f,C),s(i,O),s(i,K),s(K,L),s(i,F)},d(S){S&&t(i)}}}function ag(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q,G,W,ne,V,I,re,X,qe,le,Ee,Be,ie,$e,ue,je,ce,Xe,ve,Te,Ue,Ge,ye,_e,ze,de,pe,be,Me,fe,Ne,Ve,we,Ce,he,We,N,se,ke,oe,Oe,Ze,Je;return k=new D({props:{code:`from transformers import TrainingArguments

batch_size = 64
# Montrer la perte d'entra\xEEnement \xE0 chaque \xE9poque.
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># Montrer la perte d&#x27;entra\xEEnement \xE0 chaque \xE9poque.</span>
logging_steps = <span class="hljs-built_in">len</span>(downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>,
    overwrite_output_dir=<span class="hljs-literal">True</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
)`}}),we=new D({props:{code:`from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>],
    data_collator=data_collator,
)`}}),{c(){i=l("p"),h=a("Une fois que nous sommes connect\xE9s, nous pouvons sp\xE9cifier les arguments pour le "),d=l("code"),$=a("Trainer"),P=a(" :"),b=c(),w(k.$$.fragment),M=c(),f=l("p"),C=a("Ici, nous avons modifi\xE9 quelques options par d\xE9faut, y compris "),O=l("code"),K=a("logging_steps"),L=a(" pour s\u2019assurer que nous suivons la perte d\u2019entra\xEEnement \xE0 chaque \xE9poque. Nous avons \xE9galement utilis\xE9 "),F=l("code"),S=a("fp16=True"),A=a(" pour activer l\u2019entra\xEEnement en pr\xE9cision mixte, ce qui nous donne un autre gain de vitesse. Par d\xE9faut, le "),H=l("code"),U=a("Trainer"),q=a(" va supprimer toutes les colonnes qui ne font pas partie de la m\xE9thode "),T=l("code"),Y=a("forward()"),Z=a(" du mod\xE8le. Cela signifie que si vous utilisez le collateur de masquage de mots entiers, vous devrez \xE9galement d\xE9finir "),Q=l("code"),G=a("remove_unused_columns=False"),W=a(" pour vous assurer que nous ne perdons pas la colonne "),ne=l("code"),V=a("word_ids"),I=a(" pendant l\u2019entra\xEEnement."),re=c(),X=l("p"),qe=a("Notez que vous pouvez sp\xE9cifier le nom du r\xE9f\xE9rentiel vers lequel vous voulez pousser avec l\u2019argument "),le=l("code"),Ee=a("hub_model_id"),Be=a(" (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ie=l("a"),$e=l("code"),ue=a("huggingface-course"),je=a(", nous avons ajout\xE9 "),ce=l("code"),Xe=a('hub_model_id="huggingface-course/distilbert-finetuned-imdb"``TrainingArguments'),ve=a(". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas ce sera "),Te=l("code"),Ue=a('"lewtun/distilbert-finetuned-imdb"'),Ge=a("."),ye=c(),_e=l("p"),ze=a("Nous avons maintenant tous les ingr\xE9dients pour instancier le "),de=l("code"),pe=a("Trainer"),be=a(". Ici, nous utilisons juste le collateur standard "),Me=l("code"),fe=a("data_collator"),Ne=a(", mais vous pouvez essayer le collateur de masquage de mots entiers et comparer les r\xE9sultats comme exercice :"),Ve=c(),w(we.$$.fragment),Ce=c(),he=l("p"),We=a("Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),N=l("code"),se=a("trainer.train()"),ke=a(" . Mais avant de le faire, regardons bri\xE8vement la "),oe=l("em"),Oe=a("perplexit\xE9"),Ze=a(", qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),this.h()},l(R){i=o(R,"P",{});var ee=r(i);h=n(ee,"Une fois que nous sommes connect\xE9s, nous pouvons sp\xE9cifier les arguments pour le "),d=o(ee,"CODE",{});var J=r(d);$=n(J,"Trainer"),J.forEach(t),P=n(ee," :"),ee.forEach(t),b=_(R),x(k.$$.fragment,R),M=_(R),f=o(R,"P",{});var te=r(f);C=n(te,"Ici, nous avons modifi\xE9 quelques options par d\xE9faut, y compris "),O=o(te,"CODE",{});var us=r(O);K=n(us,"logging_steps"),us.forEach(t),L=n(te," pour s\u2019assurer que nous suivons la perte d\u2019entra\xEEnement \xE0 chaque \xE9poque. Nous avons \xE9galement utilis\xE9 "),F=o(te,"CODE",{});var ae=r(F);S=n(ae,"fp16=True"),ae.forEach(t),A=n(te," pour activer l\u2019entra\xEEnement en pr\xE9cision mixte, ce qui nous donne un autre gain de vitesse. Par d\xE9faut, le "),H=o(te,"CODE",{});var Ms=r(H);U=n(Ms,"Trainer"),Ms.forEach(t),q=n(te," va supprimer toutes les colonnes qui ne font pas partie de la m\xE9thode "),T=o(te,"CODE",{});var es=r(T);Y=n(es,"forward()"),es.forEach(t),Z=n(te," du mod\xE8le. Cela signifie que si vous utilisez le collateur de masquage de mots entiers, vous devrez \xE9galement d\xE9finir "),Q=o(te,"CODE",{});var ss=r(Q);G=n(ss,"remove_unused_columns=False"),ss.forEach(t),W=n(te," pour vous assurer que nous ne perdons pas la colonne "),ne=o(te,"CODE",{});var Ye=r(ne);V=n(Ye,"word_ids"),Ye.forEach(t),I=n(te," pendant l\u2019entra\xEEnement."),te.forEach(t),re=_(R),X=o(R,"P",{});var ge=r(X);qe=n(ge,"Notez que vous pouvez sp\xE9cifier le nom du r\xE9f\xE9rentiel vers lequel vous voulez pousser avec l\u2019argument "),le=o(ge,"CODE",{});var xe=r(le);Ee=n(xe,"hub_model_id"),xe.forEach(t),Be=n(ge," (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ie=o(ge,"A",{href:!0,rel:!0});var Cs=r(ie);$e=o(Cs,"CODE",{});var Pe=r($e);ue=n(Pe,"huggingface-course"),Pe.forEach(t),Cs.forEach(t),je=n(ge,", nous avons ajout\xE9 "),ce=o(ge,"CODE",{});var Vs=r(ce);Xe=n(Vs,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"``TrainingArguments'),Vs.forEach(t),ve=n(ge,". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas ce sera "),Te=o(ge,"CODE",{});var Fe=r(Te);Ue=n(Fe,'"lewtun/distilbert-finetuned-imdb"'),Fe.forEach(t),Ge=n(ge,"."),ge.forEach(t),ye=_(R),_e=o(R,"P",{});var Ke=r(_e);ze=n(Ke,"Nous avons maintenant tous les ingr\xE9dients pour instancier le "),de=o(Ke,"CODE",{});var Ps=r(de);pe=n(Ps,"Trainer"),Ps.forEach(t),be=n(Ke,". Ici, nous utilisons juste le collateur standard "),Me=o(Ke,"CODE",{});var ps=r(Me);fe=n(ps,"data_collator"),ps.forEach(t),Ne=n(Ke,", mais vous pouvez essayer le collateur de masquage de mots entiers et comparer les r\xE9sultats comme exercice :"),Ke.forEach(t),Ve=_(R),x(we.$$.fragment,R),Ce=_(R),he=o(R,"P",{});var He=r(he);We=n(He,"Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),N=o(He,"CODE",{});var xs=r(N);se=n(xs,"trainer.train()"),xs.forEach(t),ke=n(He," . Mais avant de le faire, regardons bri\xE8vement la "),oe=o(He,"EM",{});var Es=r(oe);Oe=n(Es,"perplexit\xE9"),Es.forEach(t),Ze=n(He,", qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),He.forEach(t),this.h()},h(){y(ie,"href","https://huggingface.co/huggingface-course"),y(ie,"rel","nofollow")},m(R,ee){p(R,i,ee),s(i,h),s(i,d),s(d,$),s(i,P),p(R,b,ee),E(k,R,ee),p(R,M,ee),p(R,f,ee),s(f,C),s(f,O),s(O,K),s(f,L),s(f,F),s(F,S),s(f,A),s(f,H),s(H,U),s(f,q),s(f,T),s(T,Y),s(f,Z),s(f,Q),s(Q,G),s(f,W),s(f,ne),s(ne,V),s(f,I),p(R,re,ee),p(R,X,ee),s(X,qe),s(X,le),s(le,Ee),s(X,Be),s(X,ie),s(ie,$e),s($e,ue),s(X,je),s(X,ce),s(ce,Xe),s(X,ve),s(X,Te),s(Te,Ue),s(X,Ge),p(R,ye,ee),p(R,_e,ee),s(_e,ze),s(_e,de),s(de,pe),s(_e,be),s(_e,Me),s(Me,fe),s(_e,Ne),p(R,Ve,ee),E(we,R,ee),p(R,Ce,ee),p(R,he,ee),s(he,We),s(he,N),s(N,se),s(he,ke),s(he,oe),s(oe,Oe),s(he,Ze),Je=!0},i(R){Je||(g(k.$$.fragment,R),g(we.$$.fragment,R),Je=!0)},o(R){v(k.$$.fragment,R),v(we.$$.fragment,R),Je=!1},d(R){R&&t(i),R&&t(b),j(k,R),R&&t(M),R&&t(f),R&&t(re),R&&t(X),R&&t(ye),R&&t(_e),R&&t(Ve),j(we,R),R&&t(Ce),R&&t(he)}}}function ng(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q,G,W,ne,V,I,re,X,qe,le,Ee,Be,ie,$e,ue,je,ce,Xe,ve,Te,Ue,Ge,ye,_e,ze,de,pe,be,Me,fe,Ne,Ve,we,Ce,he,We;return k=new D({props:{code:`tf_train_dataset = downsampled_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = downsampled_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)

tf_eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),de=new D({props:{code:`from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>, tokenizer=tokenizer
)`}}),{c(){i=l("p"),h=a("Une fois que nous sommes connect\xE9s, nous pouvons cr\xE9er nos jeux de donn\xE9es "),d=l("code"),$=a("tf.data"),P=a(". Nous n\u2019utiliserons ici que le collecteur de donn\xE9es standard, mais vous pouvez \xE9galement essayer le collecteur de masquage de mots entiers et comparer les r\xE9sultats \xE0 titre d\u2019exercice :"),b=c(),w(k.$$.fragment),M=c(),f=l("p"),C=a("Ensuite, nous configurons nos hyperparam\xE8tres d\u2019entra\xEEnement et compilons notre mod\xE8le. Nous utilisons la fonction "),O=l("code"),K=a("create_optimizer()"),L=a(" de la biblioth\xE8que \u{1F917} "),F=l("em"),S=a("Transformers"),A=a(", qui nous donne un optimiseur "),H=l("code"),U=a("AdamW"),q=a(" avec une d\xE9croissance lin\xE9aire du taux d\u2019apprentissage. Nous utilisons \xE9galement la perte int\xE9gr\xE9e au mod\xE8le, qui est la perte par d\xE9faut lorsqu\u2019aucune perte n\u2019est sp\xE9cifi\xE9e comme argument de "),T=l("code"),Y=a("compile()"),Z=a(", et nous d\xE9finissons la pr\xE9cision d\u2019entra\xEEnement \xE0 "),Q=l("code"),G=a('"mixed_float16"'),W=a(". Notez que si vous utilisez un GPU Colab ou un autre GPU qui n\u2019a pas le support acc\xE9l\xE9r\xE9 de float16, vous devriez probablement commenter cette ligne."),ne=c(),V=l("p"),I=a("De plus, nous mettons en place un "),re=l("code"),X=a("PushToHubCallback"),qe=a(" qui sauvegardera le mod\xE8le sur le Hub apr\xE8s chaque \xE9poque. Vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),le=l("code"),Ee=a("hub_model_id"),Be=a(" (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, pour pousser le mod\xE8le vers l\u2019organisation ["),ie=l("code"),$e=a("huggingface-course"),ue=a("] ("),je=l("a"),ce=a("https://huggingface.co/huggingface-course"),Xe=a("), nous avons ajout\xE9 "),ve=l("code"),Te=a('hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),Ue=a(". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas, ce sera "),Ge=l("code"),ye=a('"lewtun/distilbert-finetuned-imdb"'),_e=a("."),ze=c(),w(de.$$.fragment),pe=c(),be=l("p"),Me=a("Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),fe=l("code"),Ne=a("model.fit()"),Ve=a(". Mais avant de le faire, regardons bri\xE8vement la "),we=l("em"),Ce=a("perplexit\xE9"),he=a(", qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),this.h()},l(N){i=o(N,"P",{});var se=r(i);h=n(se,"Une fois que nous sommes connect\xE9s, nous pouvons cr\xE9er nos jeux de donn\xE9es "),d=o(se,"CODE",{});var ke=r(d);$=n(ke,"tf.data"),ke.forEach(t),P=n(se,". Nous n\u2019utiliserons ici que le collecteur de donn\xE9es standard, mais vous pouvez \xE9galement essayer le collecteur de masquage de mots entiers et comparer les r\xE9sultats \xE0 titre d\u2019exercice :"),se.forEach(t),b=_(N),x(k.$$.fragment,N),M=_(N),f=o(N,"P",{});var oe=r(f);C=n(oe,"Ensuite, nous configurons nos hyperparam\xE8tres d\u2019entra\xEEnement et compilons notre mod\xE8le. Nous utilisons la fonction "),O=o(oe,"CODE",{});var Oe=r(O);K=n(Oe,"create_optimizer()"),Oe.forEach(t),L=n(oe," de la biblioth\xE8que \u{1F917} "),F=o(oe,"EM",{});var Ze=r(F);S=n(Ze,"Transformers"),Ze.forEach(t),A=n(oe,", qui nous donne un optimiseur "),H=o(oe,"CODE",{});var Je=r(H);U=n(Je,"AdamW"),Je.forEach(t),q=n(oe," avec une d\xE9croissance lin\xE9aire du taux d\u2019apprentissage. Nous utilisons \xE9galement la perte int\xE9gr\xE9e au mod\xE8le, qui est la perte par d\xE9faut lorsqu\u2019aucune perte n\u2019est sp\xE9cifi\xE9e comme argument de "),T=o(oe,"CODE",{});var R=r(T);Y=n(R,"compile()"),R.forEach(t),Z=n(oe,", et nous d\xE9finissons la pr\xE9cision d\u2019entra\xEEnement \xE0 "),Q=o(oe,"CODE",{});var ee=r(Q);G=n(ee,'"mixed_float16"'),ee.forEach(t),W=n(oe,". Notez que si vous utilisez un GPU Colab ou un autre GPU qui n\u2019a pas le support acc\xE9l\xE9r\xE9 de float16, vous devriez probablement commenter cette ligne."),oe.forEach(t),ne=_(N),V=o(N,"P",{});var J=r(V);I=n(J,"De plus, nous mettons en place un "),re=o(J,"CODE",{});var te=r(re);X=n(te,"PushToHubCallback"),te.forEach(t),qe=n(J," qui sauvegardera le mod\xE8le sur le Hub apr\xE8s chaque \xE9poque. Vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),le=o(J,"CODE",{});var us=r(le);Ee=n(us,"hub_model_id"),us.forEach(t),Be=n(J," (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, pour pousser le mod\xE8le vers l\u2019organisation ["),ie=o(J,"CODE",{});var ae=r(ie);$e=n(ae,"huggingface-course"),ae.forEach(t),ue=n(J,"] ("),je=o(J,"A",{href:!0,rel:!0});var Ms=r(je);ce=n(Ms,"https://huggingface.co/huggingface-course"),Ms.forEach(t),Xe=n(J,"), nous avons ajout\xE9 "),ve=o(J,"CODE",{});var es=r(ve);Te=n(es,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),es.forEach(t),Ue=n(J,". Par d\xE9faut, le d\xE9p\xF4t utilis\xE9 sera dans votre espace de noms et nomm\xE9 apr\xE8s le r\xE9pertoire de sortie que vous avez d\xE9fini, donc dans notre cas, ce sera "),Ge=o(J,"CODE",{});var ss=r(Ge);ye=n(ss,'"lewtun/distilbert-finetuned-imdb"'),ss.forEach(t),_e=n(J,"."),J.forEach(t),ze=_(N),x(de.$$.fragment,N),pe=_(N),be=o(N,"P",{});var Ye=r(be);Me=n(Ye,"Nous sommes maintenant pr\xEAts \xE0 ex\xE9cuter "),fe=o(Ye,"CODE",{});var ge=r(fe);Ne=n(ge,"model.fit()"),ge.forEach(t),Ve=n(Ye,". Mais avant de le faire, regardons bri\xE8vement la "),we=o(Ye,"EM",{});var xe=r(we);Ce=n(xe,"perplexit\xE9"),xe.forEach(t),he=n(Ye,", qui est une m\xE9trique commune pour \xE9valuer la performance des mod\xE8les de langage."),Ye.forEach(t),this.h()},h(){y(je,"href","https://huggingface.co/huggingface-course"),y(je,"rel","nofollow")},m(N,se){p(N,i,se),s(i,h),s(i,d),s(d,$),s(i,P),p(N,b,se),E(k,N,se),p(N,M,se),p(N,f,se),s(f,C),s(f,O),s(O,K),s(f,L),s(f,F),s(F,S),s(f,A),s(f,H),s(H,U),s(f,q),s(f,T),s(T,Y),s(f,Z),s(f,Q),s(Q,G),s(f,W),p(N,ne,se),p(N,V,se),s(V,I),s(V,re),s(re,X),s(V,qe),s(V,le),s(le,Ee),s(V,Be),s(V,ie),s(ie,$e),s(V,ue),s(V,je),s(je,ce),s(V,Xe),s(V,ve),s(ve,Te),s(V,Ue),s(V,Ge),s(Ge,ye),s(V,_e),p(N,ze,se),E(de,N,se),p(N,pe,se),p(N,be,se),s(be,Me),s(be,fe),s(fe,Ne),s(be,Ve),s(be,we),s(we,Ce),s(be,he),We=!0},i(N){We||(g(k.$$.fragment,N),g(de.$$.fragment,N),We=!0)},o(N){v(k.$$.fragment,N),v(de.$$.fragment,N),We=!1},d(N){N&&t(i),N&&t(b),j(k,N),N&&t(M),N&&t(f),N&&t(ne),N&&t(V),N&&t(ze),j(de,N),N&&t(pe),N&&t(be)}}}function lg(B){let i,h,d,$,P,b,k,M;return k=new D({props:{code:`import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexity: <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){i=l("p"),h=a("En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \u201Csurpris\u201D ou \u201Cperplexe\u201D par les exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9, mais celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la m\xE9thode "),d=l("code"),$=a("model.evaluate()"),P=a(" pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),b=c(),w(k.$$.fragment)},l(f){i=o(f,"P",{});var C=r(i);h=n(C,"En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \u201Csurpris\u201D ou \u201Cperplexe\u201D par les exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9, mais celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la m\xE9thode "),d=o(C,"CODE",{});var O=r(d);$=n(O,"model.evaluate()"),O.forEach(t),P=n(C," pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),C.forEach(t),b=_(f),x(k.$$.fragment,f)},m(f,C){p(f,i,C),s(i,h),s(i,d),s(d,$),s(i,P),p(f,b,C),E(k,f,C),M=!0},i(f){M||(g(k.$$.fragment,f),M=!0)},o(f){v(k.$$.fragment,f),M=!1},d(f){f&&t(i),f&&t(b),j(k,f)}}}function og(B){let i,h,d,$,P,b,k,M;return k=new D({props:{code:`import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexity: <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){i=l("p"),h=a("En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \u201Csurpris\u201D ou \u201Cperplexe\u201D par les exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9, mais celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=l("code"),$=a("Trainer.evaluate()"),P=a(" pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),b=c(),w(k.$$.fragment)},l(f){i=o(f,"P",{});var C=r(i);h=n(C,"En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa\xE7on de mesurer la qualit\xE9 de notre mod\xE8le de langage est de calculer les probabilit\xE9s qu\u2019il attribue au mot suivant dans toutes les phrases de l\u2019ensemble de test. Des probabilit\xE9s \xE9lev\xE9es indiquent que le mod\xE8le n\u2019est pas \u201Csurpris\u201D ou \u201Cperplexe\u201D par les exemples non vus, et sugg\xE8rent qu\u2019il a appris les mod\xE8les de base de la grammaire de la langue. Il existe plusieurs d\xE9finitions math\xE9matiques de la perplexit\xE9, mais celle que nous utiliserons la d\xE9finit comme l\u2019exponentielle de la perte d\u2019entropie crois\xE9e. Ainsi, nous pouvons calculer la perplexit\xE9 de notre mod\xE8le pr\xE9-entra\xEEn\xE9 en utilisant la fonction "),d=o(C,"CODE",{});var O=r(d);$=n(O,"Trainer.evaluate()"),O.forEach(t),P=n(C," pour calculer la perte d\u2019entropie crois\xE9e sur l\u2019ensemble de test, puis en prenant l\u2019exponentielle du r\xE9sultat :"),C.forEach(t),b=_(f),x(k.$$.fragment,f)},m(f,C){p(f,i,C),s(i,h),s(i,d),s(d,$),s(i,P),p(f,b,C),E(k,f,C),M=!0},i(f){M||(g(k.$$.fragment,f),M=!0)},o(f){v(k.$$.fragment,f),M=!1},d(f){f&&t(i),f&&t(b),j(k,f)}}}function rg(B){let i,h;return i=new D({props:{code:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])",highlighted:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function ig(B){let i,h;return i=new D({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function ug(B){let i,h;return i=new D({props:{code:`eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")`,highlighted:`eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexity: <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function pg(B){let i,h;return i=new D({props:{code:`eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexity: <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(i.$$.fragment)},l(d){x(i.$$.fragment,d)},m(d,$){E(i,d,$),h=!0},i(d){h||(g(i.$$.fragment,d),h=!0)},o(d){v(i.$$.fragment,d),h=!1},d(d){j(i,d)}}}function Sh(B){let i,h,d,$,P;return $=new D({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){i=l("p"),h=a("Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser la carte mod\xE8le avec les informations d\u2019entra\xEEnement vers le Hub (les points de contr\xF4le sont sauvegard\xE9s pendant l\u2019entra\xEEnement lui-m\xEAme) :"),d=c(),w($.$$.fragment)},l(b){i=o(b,"P",{});var k=r(i);h=n(k,"Une fois l\u2019entra\xEEnement termin\xE9, nous pouvons pousser la carte mod\xE8le avec les informations d\u2019entra\xEEnement vers le Hub (les points de contr\xF4le sont sauvegard\xE9s pendant l\u2019entra\xEEnement lui-m\xEAme) :"),k.forEach(t),d=_(b),x($.$$.fragment,b)},m(b,k){p(b,i,k),s(i,h),p(b,d,k),E($,b,k),P=!0},i(b){P||(g($.$$.fragment,b),P=!0)},o(b){v($.$$.fragment,b),P=!1},d(b){b&&t(i),b&&t(d),j($,b)}}}function dg(B){let i,h,d,$,P;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Votre tour !"),P=a(" Ex\xE9cutez l\u2019entra\xEEnement ci-dessus apr\xE8s avoir remplac\xE9 le collecteur de donn\xE9es par le collecteur de mots entiers masqu\xE9s. Obtenez-vous de meilleurs r\xE9sultats ?")},l(b){i=o(b,"P",{});var k=r(i);h=n(k,"\u270F\uFE0F "),d=o(k,"STRONG",{});var M=r(d);$=n(M,"Votre tour !"),M.forEach(t),P=n(k," Ex\xE9cutez l\u2019entra\xEEnement ci-dessus apr\xE8s avoir remplac\xE9 le collecteur de donn\xE9es par le collecteur de mots entiers masqu\xE9s. Obtenez-vous de meilleurs r\xE9sultats ?"),k.forEach(t)},m(b,k){p(b,i,k),s(i,h),s(i,d),s(d,$),s(i,P)},d(b){b&&t(i)}}}function Th(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q,G,W,ne,V,I,re,X,qe,le,Ee,Be,ie,$e,ue,je,ce,Xe,ve,Te,Ue,Ge,ye,_e,ze,de,pe,be,Me,fe,Ne,Ve,we,Ce,he,We,N,se,ke,oe,Oe,Ze,Je,R,ee,J,te,us,ae,Ms,es,ss,Ye,ge,xe,Cs,Pe,Vs,Fe,Ke,Ps,ps,He,xs,Es,It,ds,Ws,mt,_s,ja,js,ya,za,ct,Js,Ys,Ma,Ds,Qe,Qs,As,fs,Ca,ys,At,Pa,St,Xs,Tt,Ut,Zs,Nt,Ie,ms,et,ts,Gt,zs,Vt,_t,Wt,Ss;return C=new Ea({}),ce=new D({props:{code:`def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Create a new "masked" column for each column in the dataset
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">insert_random_mask</span>(<span class="hljs-params">batch</span>):
    features = [<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(batch, t)) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(*batch.values())]
    masked_inputs = data_collator(features)
    <span class="hljs-comment"># Create a new &quot;masked&quot; column for each column in the dataset</span>
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;masked_&quot;</span> + k: v.numpy() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> masked_inputs.items()}`}}),ze=new D({props:{code:`downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)`,highlighted:`downsampled_dataset = downsampled_dataset.remove_columns([<span class="hljs-string">&quot;word_ids&quot;</span>])
eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].<span class="hljs-built_in">map</span>(
    insert_random_mask,
    batched=<span class="hljs-literal">True</span>,
    remove_columns=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        <span class="hljs-string">&quot;masked_input_ids&quot;</span>: <span class="hljs-string">&quot;input_ids&quot;</span>,
        <span class="hljs-string">&quot;masked_attention_mask&quot;</span>: <span class="hljs-string">&quot;attention_mask&quot;</span>,
        <span class="hljs-string">&quot;masked_labels&quot;</span>: <span class="hljs-string">&quot;labels&quot;</span>,
    }
)`}}),ke=new D({props:{code:`from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

batch_size = <span class="hljs-number">64</span>
train_dataloader = DataLoader(
    downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`}}),te=new D({props:{code:"model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)",highlighted:'model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoModelForMaskedLM</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">model_checkpoint</span>)</span>'}}),xe=new D({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`}}),He=new D({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),Ws=new D({props:{code:`from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">3</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),Ds=new D({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-imdb-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),Qs=new D({props:{code:"'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/distilbert-base-uncased-finetuned-imdb-accelerate&#x27;</span>'}}),Zs=new D({props:{code:`from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`}}),ts=new D({props:{code:`from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra\xEEnement
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # Sauvegarder et t\xE9l\xE9charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> math

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Entra\xEEnement</span>
    model.train()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: <span class="hljs-built_in">len</span>(eval_dataset)]
    <span class="hljs-keyword">try</span>:
        perplexity = math.exp(torch.mean(losses))
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)

    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Epoch <span class="hljs-subst">{epoch}</span>: Perplexity: <span class="hljs-subst">{perplexity}</span>&quot;</span>)

    <span class="hljs-comment"># Sauvegarder et t\xE9l\xE9charger</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),zs=new D({props:{code:`Epoch 0: Perplexity: 11.397545307900472
Epoch 1: Perplexity: 10.904909330983092
Epoch 2: Perplexity: 10.729503505340409`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">0</span>: Perplexity: <span class="hljs-number">11.397545307900472</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">1</span>: Perplexity: <span class="hljs-number">10.904909330983092</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">2</span>: Perplexity: <span class="hljs-number">10.729503505340409</span>`}}),{c(){i=l("p"),h=a("Dans notre cas d\u2019utilisation, nous n\u2019avons pas eu besoin de faire quelque chose de sp\xE9cial avec la boucle d\u2019entra\xEEnement, mais dans certains cas, vous pourriez avoir besoin de mettre en \u0153uvre une logique personnalis\xE9e. Pour ces applications, vous pouvez utiliser \u{1F917} "),d=l("em"),$=a("Accelerate"),P=a(". Jetons un coup d\u2019\u0153il !"),b=c(),k=l("h2"),M=l("a"),f=l("span"),w(C.$$.fragment),O=c(),K=l("span"),L=a("*Finetuning* de DistilBERT avec \u{1F917} Accelerate"),F=c(),S=l("p"),A=a("Comme nous l\u2019avons vu avec le "),H=l("code"),U=a("Trainer"),q=a(", le r\xE9glage fin d\u2019un mod\xE8le de langage masqu\xE9 est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte du "),T=l("a"),Y=a("Chapitre 3"),Z=a(". En fait, la seule subtilit\xE9 est l\u2019utilisation d\u2019un collateur de donn\xE9es sp\xE9cial, et nous l\u2019avons d\xE9j\xE0 couvert plus t\xF4t dans cette section !"),Q=c(),G=l("p"),W=a("Cependant, nous avons vu que "),ne=l("code"),V=a("DataCollatorForLanguageModeling"),I=a(" applique aussi un masquage al\xE9atoire \xE0 chaque \xE9valuation, donc nous verrons quelques fluctuations dans nos scores de perplexit\xE9 \xE0 chaque entrainement. Une fa\xE7on d\u2019\xE9liminer cette source d\u2019al\xE9atoire est d\u2019appliquer le masquage "),re=l("em"),X=a("une fois"),qe=a(" sur l\u2019ensemble de test, puis d\u2019utiliser le collateur de donn\xE9es par d\xE9faut dans \u{1F917} "),le=l("em"),Ee=a("Transformers"),Be=a(" pour collecter les lots pendant l\u2019\xE9valuation. Pour voir comment cela fonctionne, impl\xE9mentons une fonction simple qui applique le masquage sur un lot, similaire \xE0 notre premi\xE8re rencontre avec "),ie=l("code"),$e=a("DataCollatorForLanguageModeling"),ue=a(" :"),je=c(),w(ce.$$.fragment),Xe=c(),ve=l("p"),Te=a("Ensuite, nous allons appliquer cette fonction \xE0 notre jeu de test et laisser tomber les colonnes non masqu\xE9es afin de les remplacer par les colonnes masqu\xE9es. Vous pouvez utiliser le masquage de mots entiers en rempla\xE7ant le "),Ue=l("code"),Ge=a("data_collator"),ye=a(" ci-dessus par celui qui est appropri\xE9, dans ce cas vous devez supprimer la premi\xE8re ligne ici :"),_e=c(),w(ze.$$.fragment),de=c(),pe=l("p"),be=a("Nous pouvons ensuite configurer les "),Me=l("em"),fe=a("dataloaders"),Ne=a(" comme d\u2019habitude, mais nous utiliserons le "),Ve=l("code"),we=a("default_data_collator"),Ce=a(" de \u{1F917} "),he=l("em"),We=a("Transformers"),N=a(" pour le jeu d\u2019\xE9valuation :"),se=c(),w(ke.$$.fragment),oe=c(),Oe=l("p"),Ze=a("Forme ici, nous suivons les \xE9tapes standard avec \u{1F917} "),Je=l("em"),R=a("Accelerate"),ee=a(". Le premier ordre du jour est de charger une version fra\xEEche du mod\xE8le pr\xE9-entra\xEEn\xE9 :"),J=c(),w(te.$$.fragment),us=c(),ae=l("p"),Ms=a("Ensuite, nous devons sp\xE9cifier l\u2019optimiseur ; nous utiliserons le standard "),es=l("code"),ss=a("AdamW"),Ye=a(" :"),ge=c(),w(xe.$$.fragment),Cs=c(),Pe=l("p"),Vs=a("Avec ces objets, nous pouvons maintenant tout pr\xE9parer pour l\u2019entra\xEEnement avec l\u2019objet "),Fe=l("code"),Ke=a("Accelerator"),Ps=a(" :"),ps=c(),w(He.$$.fragment),xs=c(),Es=l("p"),It=a("Maintenant que notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es sont configur\xE9s, nous pouvons sp\xE9cifier le planificateur du taux d\u2019apprentissage comme suit :"),ds=c(),w(Ws.$$.fragment),mt=c(),_s=l("p"),ja=a("Il ne reste qu\u2019une derni\xE8re chose \xE0 faire avant de s\u2019entra\xEEner : cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),js=l("em"),ya=a("Hub"),za=a(" d\u2019Hugging Face ! Nous pouvons utiliser la biblioth\xE8que \u{1F917} "),ct=l("em"),Js=a("Hub"),Ys=a(" pour g\xE9n\xE9rer d\u2019abord le nom complet de notre d\xE9p\xF4t :"),Ma=c(),w(Ds.$$.fragment),Qe=c(),w(Qs.$$.fragment),As=c(),fs=l("p"),Ca=a("puis cr\xE9er et cloner le r\xE9f\xE9rentiel en utilisant la classe "),ys=l("code"),At=a("Repository"),Pa=a(" du \u{1F917} "),St=l("em"),Xs=a("Hub"),Tt=a(" :"),Ut=c(),w(Zs.$$.fragment),Nt=c(),Ie=l("p"),ms=a("Une fois cela fait, il ne reste plus qu\u2019\xE0 r\xE9diger la boucle compl\xE8te d\u2019entra\xEEnement et d\u2019\xE9valuation :"),et=c(),w(ts.$$.fragment),Gt=c(),w(zs.$$.fragment),Vt=c(),_t=l("p"),Wt=a("Cool, nous avons \xE9t\xE9 en mesure d\u2019\xE9valuer la perplexit\xE9 \xE0 chaque \xE9poque et de garantir la reproductibilit\xE9 des entra\xEEnements multiples !"),this.h()},l(m){i=o(m,"P",{});var z=r(i);h=n(z,"Dans notre cas d\u2019utilisation, nous n\u2019avons pas eu besoin de faire quelque chose de sp\xE9cial avec la boucle d\u2019entra\xEEnement, mais dans certains cas, vous pourriez avoir besoin de mettre en \u0153uvre une logique personnalis\xE9e. Pour ces applications, vous pouvez utiliser \u{1F917} "),d=o(z,"EM",{});var Bn=r(d);$=n(Bn,"Accelerate"),Bn.forEach(t),P=n(z,". Jetons un coup d\u2019\u0153il !"),z.forEach(t),b=_(m),k=o(m,"H2",{class:!0});var ft=r(k);M=o(ft,"A",{id:!0,class:!0,href:!0});var Fn=r(M);f=o(Fn,"SPAN",{});var Hn=r(f);x(C.$$.fragment,Hn),Hn.forEach(t),Fn.forEach(t),O=_(ft),K=o(ft,"SPAN",{});var Jt=r(K);L=n(Jt,"*Finetuning* de DistilBERT avec \u{1F917} Accelerate"),Jt.forEach(t),ft.forEach(t),F=_(m),S=o(m,"P",{});var ht=r(S);A=n(ht,"Comme nous l\u2019avons vu avec le "),H=o(ht,"CODE",{});var In=r(H);U=n(In,"Trainer"),In.forEach(t),q=n(ht,", le r\xE9glage fin d\u2019un mod\xE8le de langage masqu\xE9 est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte du "),T=o(ht,"A",{href:!0});var gt=r(T);Y=n(gt,"Chapitre 3"),gt.forEach(t),Z=n(ht,". En fait, la seule subtilit\xE9 est l\u2019utilisation d\u2019un collateur de donn\xE9es sp\xE9cial, et nous l\u2019avons d\xE9j\xE0 couvert plus t\xF4t dans cette section !"),ht.forEach(t),Q=_(m),G=o(m,"P",{});var hs=r(G);W=n(hs,"Cependant, nous avons vu que "),ne=o(hs,"CODE",{});var Un=r(ne);V=n(Un,"DataCollatorForLanguageModeling"),Un.forEach(t),I=n(hs," applique aussi un masquage al\xE9atoire \xE0 chaque \xE9valuation, donc nous verrons quelques fluctuations dans nos scores de perplexit\xE9 \xE0 chaque entrainement. Une fa\xE7on d\u2019\xE9liminer cette source d\u2019al\xE9atoire est d\u2019appliquer le masquage "),re=o(hs,"EM",{});var vt=r(re);X=n(vt,"une fois"),vt.forEach(t),qe=n(hs," sur l\u2019ensemble de test, puis d\u2019utiliser le collateur de donn\xE9es par d\xE9faut dans \u{1F917} "),le=o(hs,"EM",{});var Gn=r(le);Ee=n(Gn,"Transformers"),Gn.forEach(t),Be=n(hs," pour collecter les lots pendant l\u2019\xE9valuation. Pour voir comment cela fonctionne, impl\xE9mentons une fonction simple qui applique le masquage sur un lot, similaire \xE0 notre premi\xE8re rencontre avec "),ie=o(hs,"CODE",{});var Vn=r(ie);$e=n(Vn,"DataCollatorForLanguageModeling"),Vn.forEach(t),ue=n(hs," :"),hs.forEach(t),je=_(m),x(ce.$$.fragment,m),Xe=_(m),ve=o(m,"P",{});var bt=r(ve);Te=n(bt,"Ensuite, nous allons appliquer cette fonction \xE0 notre jeu de test et laisser tomber les colonnes non masqu\xE9es afin de les remplacer par les colonnes masqu\xE9es. Vous pouvez utiliser le masquage de mots entiers en rempla\xE7ant le "),Ue=o(bt,"CODE",{});var Wn=r(Ue);Ge=n(Wn,"data_collator"),Wn.forEach(t),ye=n(bt," ci-dessus par celui qui est appropri\xE9, dans ce cas vous devez supprimer la premi\xE8re ligne ici :"),bt.forEach(t),_e=_(m),x(ze.$$.fragment,m),de=_(m),pe=o(m,"P",{});var Ts=r(pe);be=n(Ts,"Nous pouvons ensuite configurer les "),Me=o(Ts,"EM",{});var Yt=r(Me);fe=n(Yt,"dataloaders"),Yt.forEach(t),Ne=n(Ts," comme d\u2019habitude, mais nous utiliserons le "),Ve=o(Ts,"CODE",{});var Jn=r(Ve);we=n(Jn,"default_data_collator"),Jn.forEach(t),Ce=n(Ts," de \u{1F917} "),he=o(Ts,"EM",{});var Yn=r(he);We=n(Yn,"Transformers"),Yn.forEach(t),N=n(Ts," pour le jeu d\u2019\xE9valuation :"),Ts.forEach(t),se=_(m),x(ke.$$.fragment,m),oe=_(m),Oe=o(m,"P",{});var kt=r(Oe);Ze=n(kt,"Forme ici, nous suivons les \xE9tapes standard avec \u{1F917} "),Je=o(kt,"EM",{});var Qn=r(Je);R=n(Qn,"Accelerate"),Qn.forEach(t),ee=n(kt,". Le premier ordre du jour est de charger une version fra\xEEche du mod\xE8le pr\xE9-entra\xEEn\xE9 :"),kt.forEach(t),J=_(m),x(te.$$.fragment,m),us=_(m),ae=o(m,"P",{});var Qt=r(ae);Ms=n(Qt,"Ensuite, nous devons sp\xE9cifier l\u2019optimiseur ; nous utiliserons le standard "),es=o(Qt,"CODE",{});var Da=r(es);ss=n(Da,"AdamW"),Da.forEach(t),Ye=n(Qt," :"),Qt.forEach(t),ge=_(m),x(xe.$$.fragment,m),Cs=_(m),Pe=o(m,"P",{});var Ns=r(Pe);Vs=n(Ns,"Avec ces objets, nous pouvons maintenant tout pr\xE9parer pour l\u2019entra\xEEnement avec l\u2019objet "),Fe=o(Ns,"CODE",{});var Aa=r(Fe);Ke=n(Aa,"Accelerator"),Aa.forEach(t),Ps=n(Ns," :"),Ns.forEach(t),ps=_(m),x(He.$$.fragment,m),xs=_(m),Es=o(m,"P",{});var st=r(Es);It=n(st,"Maintenant que notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es sont configur\xE9s, nous pouvons sp\xE9cifier le planificateur du taux d\u2019apprentissage comme suit :"),st.forEach(t),ds=_(m),x(Ws.$$.fragment,m),mt=_(m),_s=o(m,"P",{});var qt=r(_s);ja=n(qt,"Il ne reste qu\u2019une derni\xE8re chose \xE0 faire avant de s\u2019entra\xEEner : cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),js=o(qt,"EM",{});var Xt=r(js);ya=n(Xt,"Hub"),Xt.forEach(t),za=n(qt," d\u2019Hugging Face ! Nous pouvons utiliser la biblioth\xE8que \u{1F917} "),ct=o(qt,"EM",{});var Xn=r(ct);Js=n(Xn,"Hub"),Xn.forEach(t),Ys=n(qt," pour g\xE9n\xE9rer d\u2019abord le nom complet de notre d\xE9p\xF4t :"),qt.forEach(t),Ma=_(m),x(Ds.$$.fragment,m),Qe=_(m),x(Qs.$$.fragment,m),As=_(m),fs=o(m,"P",{});var $t=r(fs);Ca=n($t,"puis cr\xE9er et cloner le r\xE9f\xE9rentiel en utilisant la classe "),ys=o($t,"CODE",{});var Sa=r(ys);At=n(Sa,"Repository"),Sa.forEach(t),Pa=n($t," du \u{1F917} "),St=o($t,"EM",{});var gs=r(St);Xs=n(gs,"Hub"),gs.forEach(t),Tt=n($t," :"),$t.forEach(t),Ut=_(m),x(Zs.$$.fragment,m),Nt=_(m),Ie=o(m,"P",{});var vs=r(Ie);ms=n(vs,"Une fois cela fait, il ne reste plus qu\u2019\xE0 r\xE9diger la boucle compl\xE8te d\u2019entra\xEEnement et d\u2019\xE9valuation :"),vs.forEach(t),et=_(m),x(ts.$$.fragment,m),Gt=_(m),x(zs.$$.fragment,m),Vt=_(m),_t=o(m,"P",{});var Ot=r(_t);Wt=n(Ot,"Cool, nous avons \xE9t\xE9 en mesure d\u2019\xE9valuer la perplexit\xE9 \xE0 chaque \xE9poque et de garantir la reproductibilit\xE9 des entra\xEEnements multiples !"),Ot.forEach(t),this.h()},h(){y(M,"id","finetuning-de-distilbert-avec-accelerate"),y(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(M,"href","#finetuning-de-distilbert-avec-accelerate"),y(k,"class","relative group"),y(T,"href","/course/fr/chapter3")},m(m,z){p(m,i,z),s(i,h),s(i,d),s(d,$),s(i,P),p(m,b,z),p(m,k,z),s(k,M),s(M,f),E(C,f,null),s(k,O),s(k,K),s(K,L),p(m,F,z),p(m,S,z),s(S,A),s(S,H),s(H,U),s(S,q),s(S,T),s(T,Y),s(S,Z),p(m,Q,z),p(m,G,z),s(G,W),s(G,ne),s(ne,V),s(G,I),s(G,re),s(re,X),s(G,qe),s(G,le),s(le,Ee),s(G,Be),s(G,ie),s(ie,$e),s(G,ue),p(m,je,z),E(ce,m,z),p(m,Xe,z),p(m,ve,z),s(ve,Te),s(ve,Ue),s(Ue,Ge),s(ve,ye),p(m,_e,z),E(ze,m,z),p(m,de,z),p(m,pe,z),s(pe,be),s(pe,Me),s(Me,fe),s(pe,Ne),s(pe,Ve),s(Ve,we),s(pe,Ce),s(pe,he),s(he,We),s(pe,N),p(m,se,z),E(ke,m,z),p(m,oe,z),p(m,Oe,z),s(Oe,Ze),s(Oe,Je),s(Je,R),s(Oe,ee),p(m,J,z),E(te,m,z),p(m,us,z),p(m,ae,z),s(ae,Ms),s(ae,es),s(es,ss),s(ae,Ye),p(m,ge,z),E(xe,m,z),p(m,Cs,z),p(m,Pe,z),s(Pe,Vs),s(Pe,Fe),s(Fe,Ke),s(Pe,Ps),p(m,ps,z),E(He,m,z),p(m,xs,z),p(m,Es,z),s(Es,It),p(m,ds,z),E(Ws,m,z),p(m,mt,z),p(m,_s,z),s(_s,ja),s(_s,js),s(js,ya),s(_s,za),s(_s,ct),s(ct,Js),s(_s,Ys),p(m,Ma,z),E(Ds,m,z),p(m,Qe,z),E(Qs,m,z),p(m,As,z),p(m,fs,z),s(fs,Ca),s(fs,ys),s(ys,At),s(fs,Pa),s(fs,St),s(St,Xs),s(fs,Tt),p(m,Ut,z),E(Zs,m,z),p(m,Nt,z),p(m,Ie,z),s(Ie,ms),p(m,et,z),E(ts,m,z),p(m,Gt,z),E(zs,m,z),p(m,Vt,z),p(m,_t,z),s(_t,Wt),Ss=!0},i(m){Ss||(g(C.$$.fragment,m),g(ce.$$.fragment,m),g(ze.$$.fragment,m),g(ke.$$.fragment,m),g(te.$$.fragment,m),g(xe.$$.fragment,m),g(He.$$.fragment,m),g(Ws.$$.fragment,m),g(Ds.$$.fragment,m),g(Qs.$$.fragment,m),g(Zs.$$.fragment,m),g(ts.$$.fragment,m),g(zs.$$.fragment,m),Ss=!0)},o(m){v(C.$$.fragment,m),v(ce.$$.fragment,m),v(ze.$$.fragment,m),v(ke.$$.fragment,m),v(te.$$.fragment,m),v(xe.$$.fragment,m),v(He.$$.fragment,m),v(Ws.$$.fragment,m),v(Ds.$$.fragment,m),v(Qs.$$.fragment,m),v(Zs.$$.fragment,m),v(ts.$$.fragment,m),v(zs.$$.fragment,m),Ss=!1},d(m){m&&t(i),m&&t(b),m&&t(k),j(C),m&&t(F),m&&t(S),m&&t(Q),m&&t(G),m&&t(je),j(ce,m),m&&t(Xe),m&&t(ve),m&&t(_e),j(ze,m),m&&t(de),m&&t(pe),m&&t(se),j(ke,m),m&&t(oe),m&&t(Oe),m&&t(J),j(te,m),m&&t(us),m&&t(ae),m&&t(ge),j(xe,m),m&&t(Cs),m&&t(Pe),m&&t(ps),j(He,m),m&&t(xs),m&&t(Es),m&&t(ds),j(Ws,m),m&&t(mt),m&&t(_s),m&&t(Ma),j(Ds,m),m&&t(Qe),j(Qs,m),m&&t(As),m&&t(fs),m&&t(Ut),j(Zs,m),m&&t(Nt),m&&t(Ie),m&&t(et),j(ts,m),m&&t(Gt),j(zs,m),m&&t(Vt),m&&t(_t)}}}function mg(B){let i,h,d,$,P,b,k,M;return{c(){i=l("p"),h=a("\u270F\uFE0F "),d=l("strong"),$=a("Essayez !"),P=a(" Pour quantifier les avantages de l\u2019adaptation au domaine, ajustez un classificateur sur les \xE9tiquettes IMDb pour les points de contr\xF4le DistilBERT pr\xE9-entra\xEEn\xE9s et ajust\xE9s. Si vous avez besoin d\u2019un rafra\xEEchissement sur la classification de texte, consultez le "),b=l("a"),k=a("Chapitre 3"),M=a("."),this.h()},l(f){i=o(f,"P",{});var C=r(i);h=n(C,"\u270F\uFE0F "),d=o(C,"STRONG",{});var O=r(d);$=n(O,"Essayez !"),O.forEach(t),P=n(C," Pour quantifier les avantages de l\u2019adaptation au domaine, ajustez un classificateur sur les \xE9tiquettes IMDb pour les points de contr\xF4le DistilBERT pr\xE9-entra\xEEn\xE9s et ajust\xE9s. Si vous avez besoin d\u2019un rafra\xEEchissement sur la classification de texte, consultez le "),b=o(C,"A",{href:!0});var K=r(b);k=n(K,"Chapitre 3"),K.forEach(t),M=n(C,"."),C.forEach(t),this.h()},h(){y(b,"href","/course/fr/chapter3")},m(f,C){p(f,i,C),s(i,h),s(i,d),s(d,$),s(i,P),s(i,b),s(b,k),s(i,M)},d(f){f&&t(i)}}}function cg(B){let i,h,d,$,P,b,k,M,f,C,O,K,L,F,S,A,H,U,q,T,Y,Z,Q,G,W,ne,V,I,re,X,qe,le,Ee,Be,ie,$e,ue,je,ce,Xe,ve,Te,Ue,Ge,ye,_e,ze,de,pe,be,Me,fe,Ne,Ve,we,Ce,he,We,N,se,ke,oe,Oe,Ze,Je,R,ee,J,te,us,ae,Ms,es,ss,Ye,ge,xe,Cs,Pe,Vs,Fe,Ke,Ps,ps,He,xs,Es,It,ds,Ws,mt,_s,ja,js,ya,za,ct,Js,Ys,Ma,Ds,Qe,Qs,As,fs,Ca,ys,At,Pa,St,Xs,Tt,Ut,Zs,Nt,Ie,ms,et,ts,Gt,zs,Vt,_t,Wt,Ss,m,z,Bn,ft,Fn,Hn,Jt,ht,In,gt,hs,Un,vt,Gn,Vn,bt,Wn,Ts,Yt,Jn,Yn,kt,Qn,Qt,Da,Ns,Aa,st,qt,Xt,Xn,$t,Sa,gs,vs,Ot,Ta,zr,Zn,Mu,Mr,Kt,Zt,Ol,Na,Cu,Kl,Pu,Cr,bs,Du,Oa,Au,Su,Ll,Tu,Nu,Rl,Ou,Ku,Bl,Lu,Ru,Pr,Ka,Dr,La,Ar,as,Bu,Fl,Fu,Hu,Hl,Iu,Uu,Il,Gu,Vu,Ul,Wu,Ju,Gl,Yu,Qu,Sr,Ra,Tr,Ba,Nr,wt,Xu,Vl,Zu,ep,Wl,sp,tp,Or,ea,Kr,sa,ap,el,np,lp,Lr,Lt,ta,Jl,Fa,op,Yl,rp,Rr,Ha,Br,aa,ip,Ql,up,pp,Fr,De,dp,Xl,mp,cp,Zl,_p,fp,eo,hp,gp,so,vp,bp,sl,kp,qp,to,$p,wp,ao,xp,Ep,Hr,Ia,Ir,Ua,Ur,Os,jp,no,yp,zp,lo,Mp,Cp,oo,Pp,Dp,Gr,Ks,Ap,ro,Sp,Tp,io,Np,Op,uo,Kp,Lp,Vr,Ga,Wr,Va,Jr,xt,Rp,po,Bp,Fp,mo,Hp,Ip,Yr,na,Qr,tl,Up,Xr,Wa,Zr,la,ei,oa,Gp,co,Vp,Wp,si,Ja,ti,Ya,ai,al,Jp,ni,Qa,li,Xa,oi,Ls,Yp,_o,Qp,Xp,fo,Zp,ed,ho,sd,td,ri,Za,ii,en,ui,ra,ad,go,nd,ld,pi,ia,sn,od,vo,rd,id,ud,tn,pd,bo,dd,md,di,nl,cd,mi,an,ci,ns,_d,ko,fd,hd,qo,gd,vd,$o,bd,kd,wo,qd,$d,xo,wd,xd,_i,Et,Ed,Eo,jd,yd,jo,zd,Md,fi,nn,hi,ln,gi,Ae,Cd,yo,Pd,Dd,zo,Ad,Sd,Mo,Td,Nd,Co,Od,Kd,Po,Ld,Rd,Do,Bd,Fd,Ao,Hd,Id,vi,on,bi,rn,ki,ll,Ud,qi,un,$i,pn,wi,Rs,Gd,So,Vd,Wd,To,Jd,Yd,No,Qd,Xd,xi,Rt,ua,Oo,dn,Zd,mn,em,Ko,sm,tm,Ei,me,am,Lo,nm,lm,Ro,om,rm,ol,im,um,Bo,pm,dm,Fo,mm,cm,Ho,_m,fm,Io,hm,gm,Uo,vm,bm,Go,km,qm,ji,cn,yi,Bs,$m,Vo,wm,xm,Wo,Em,jm,Jo,ym,zm,zi,_n,Mi,fn,Ci,ks,Mm,Yo,Cm,Pm,Qo,Dm,Am,Xo,Sm,Tm,Zo,Nm,Om,Pi,pa,Di,rl,qs,Km,er,Lm,Rm,sr,Bm,Fm,tr,Hm,Im,ar,Um,Gm,Ai,tt,at,il,ul,Vm,Si,hn,Ti,gn,Ni,da,Oi,Fs,Wm,nr,Jm,Ym,lr,Qm,Xm,pl,Zm,ec,Ki,vn,Li,bn,Ri,$s,sc,or,tc,ac,rr,nc,lc,ir,oc,rc,ur,ic,uc,Bi,kn,Fi,ma,pc,pr,dc,mc,Hi,qn,Ii,dl,cc,Ui,nt,lt,ml,Bt,ca,dr,$n,_c,mr,fc,Gi,wn,Vi,cl,hc,Wi,ot,rt,_l,xn,Ji,fl,gc,Yi,it,ut,hl,gl,vc,Qi,pt,dt,vl,En,Xi,bl,bc,Zi,kl,_a,eu,ql,Ft,fa,cr,jn,kc,_r,qc,su,ls,$c,fr,wc,xc,hr,Ec,jc,gr,yc,zc,vr,Mc,Cc,br,Pc,Dc,tu,yn,au,$l,Ac,nu,zn,lu,Mn,ou,wl,Sc,ru,Cn,iu,jt,Tc,xl,Nc,Oc,kr,Kc,Lc,uu,ha,pu;d=new Fh({props:{fw:B[0]}}),M=new Ea({});const Rc=[Ih,Hh],Pn=[];function Bc(e,u){return e[0]==="pt"?0:1}L=Bc(B),F=Pn[L]=Rc[L](B),xe=new zu({props:{id:"mqElG5QJWUg"}}),Pe=new xa({props:{$$slots:{default:[Uh]},$$scope:{ctx:B}}}),ps=new Ea({});const Fc=[Vh,Gh],Dn=[];function Hc(e,u){return e[0]==="pt"?0:1}Ie=Hc(B),ms=Dn[Ie]=Fc[Ie](B),Ss=new D({props:{code:'text = "This is a great [MASK]."',highlighted:'text = <span class="hljs-string">&quot;This is a great [MASK].&quot;</span>'}}),Ns=new D({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}});const Ic=[Jh,Wh],An=[];function Uc(e,u){return e[0]==="pt"?0:1}gs=Uc(B),vs=An[gs]=Ic[gs](B),Ta=new D({props:{code:`'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great deal.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great success.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great adventure.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great idea.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great feat.&#x27;</span>`}}),Na=new Ea({}),Ka=new D({props:{code:`from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

imdb_dataset = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)
imdb_dataset`}}),La=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),Ra=new D({props:{code:`sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")`,highlighted:`sample = imdb_dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>))

<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> sample:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;text&#x27;</span>]}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Label: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>&#x27;&quot;</span>)`}}),Ba=new D({props:{code:`
'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'`,highlighted:`
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, &quot;kodokoo&quot; in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\&#x27;t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.&lt;br /&gt;&lt;br /&gt;Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam&quot; Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.&lt;br /&gt;&lt;br /&gt;The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. &lt;br /&gt;&lt;br /&gt;My 4 and 6 year old children love this movie and have been asking again and again to watch it. &lt;br /&gt;&lt;br /&gt;I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.&lt;br /&gt;&lt;br /&gt;I do not have older children so I do not know what they would think of it. &lt;br /&gt;&lt;br /&gt;The songs are very cute. My daughter keeps singing them over and over.&lt;br /&gt;&lt;br /&gt;Hope this helps.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 1&#x27;</span>`}}),ea=new xa({props:{$$slots:{default:[Yh]},$$scope:{ctx:B}}}),Fa=new Ea({}),Ha=new zu({props:{id:"8PmhEIXhBvI"}}),Ia=new D({props:{code:`def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# Use batched=True to activate fast multithreading!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
    result = tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>])
    <span class="hljs-keyword">if</span> tokenizer.is_fast:
        result[<span class="hljs-string">&quot;word_ids&quot;</span>] = [result.word_ids(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(result[<span class="hljs-string">&quot;input_ids&quot;</span>]))]
    <span class="hljs-keyword">return</span> result


<span class="hljs-comment"># Use batched=True to activate fast multithreading!</span>
tokenized_datasets = imdb_dataset.<span class="hljs-built_in">map</span>(
    tokenize_function, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>]
)
tokenized_datasets`}}),Ua=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),Ga=new D({props:{code:"tokenizer.model_max_length",highlighted:"tokenizer.model_max_length"}}),Va=new D({props:{code:"512",highlighted:'<span class="hljs-number">512</span>'}}),na=new xa({props:{$$slots:{default:[Qh]},$$scope:{ctx:B}}}),Wa=new D({props:{code:"chunk_size = 128",highlighted:'chunk_size = <span class="hljs-number">128</span>'}}),la=new xa({props:{warning:!0,$$slots:{default:[Xh]},$$scope:{ctx:B}}}),Ja=new D({props:{code:`# Le d\xE9coupage produit une liste de listes pour chaque caract\xE9ristique
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")`,highlighted:`<span class="hljs-comment"># Le d\xE9coupage produit une liste de listes pour chaque caract\xE9ristique</span>
tokenized_samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">3</span>]

<span class="hljs-keyword">for</span> idx, sample <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokenized_samples[<span class="hljs-string">&quot;input_ids&quot;</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review <span class="hljs-subst">{idx}</span> length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(sample)}</span>&#x27;&quot;</span>)`}}),Ya=new D({props:{code:`'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 0 length: 200&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 1 length: 559&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 2 length: 192&#x27;</span>`}}),Qa=new D({props:{code:`concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")`,highlighted:`concatenated_examples = {
    k: <span class="hljs-built_in">sum</span>(tokenized_samples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> tokenized_samples.keys()
}
total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Concatenated reviews length: <span class="hljs-subst">{total_length}</span>&#x27;&quot;</span>)`}}),Xa=new D({props:{code:"'>>> Concatenated reviews length: 951'",highlighted:'<span class="hljs-string">&#x27;&gt;&gt;&gt; Concatenated reviews length: 951&#x27;</span>'}}),Za=new D({props:{code:`chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")`,highlighted:`chunks = {
    k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
    <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
}

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Chunk length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(chunk)}</span>&#x27;&quot;</span>)`}}),en=new D({props:{code:`'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 55&#x27;</span>`}}),an=new D({props:{code:`def group_texts(examples):
    # Concat\xE9nation de tous les textes
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Calculer la longueur des textes concat\xE9n\xE9s
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # Nous laissons tomber le dernier morceau s'il est plus petit que chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Split by chunks of max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Create a new labels column
    result["labels"] = result["input_ids"].copy()
    return result`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-comment"># Concat\xE9nation de tous les textes</span>
    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
    <span class="hljs-comment"># Calculer la longueur des textes concat\xE9n\xE9s</span>
    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
    <span class="hljs-comment"># Nous laissons tomber le dernier morceau s&#x27;il est plus petit que chunk_size</span>
    total_length = (total_length // chunk_size) * chunk_size
    <span class="hljs-comment"># Split by chunks of max_len</span>
    result = {
        k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
    }
    <span class="hljs-comment"># Create a new labels column</span>
    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
    <span class="hljs-keyword">return</span> result`}}),nn=new D({props:{code:`lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets`,highlighted:`lm_datasets = tokenized_datasets.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>)
lm_datasets`}}),ln=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">61289</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">59905</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">122963</span>
    })
})`}}),on=new D({props:{code:'tokenizer.decode(lm_datasets["train"][1]["input_ids"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),rn=new D({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),un=new D({props:{code:'tokenizer.decode(lm_datasets["train"][1]["labels"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;labels&quot;</span>])'}}),pn=new D({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),dn=new Ea({}),cn=new D({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`}}),_n=new D({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
<span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> samples:
    _ = sample.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> data_collator(samples)[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),fn=new D({props:{code:`'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as &quot; teachers &quot;. [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\&#x27;[MASK] satire is much closer to reality than is &quot; teachers &quot;. the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\&#x27;[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless&#x27;</span>`}}),pa=new xa({props:{$$slots:{default:[Zh]},$$scope:{ctx:B}}});let cs=B[0]==="pt"&&Ah();const Gc=[sg,eg],Sn=[];function Vc(e,u){return e[0]==="pt"?0:1}tt=Vc(B),at=Sn[tt]=Gc[tt](B),hn=new D({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
batch = whole_word_masking_data_collator(samples)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> batch[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),gn=new D({props:{code:`'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as &quot; teachers &quot;. my 35 years in the teaching profession lead me to believe that bromwell high\\&#x27;s satire is much closer to reality than is &quot; teachers &quot;. the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&#x27;</span>`}}),da=new xa({props:{$$slots:{default:[tg]},$$scope:{ctx:B}}}),vn=new D({props:{code:`train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset`,highlighted:`train_size = <span class="hljs-number">10_000</span>
test_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.1</span> * train_size)

downsampled_dataset = lm_datasets[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(
    train_size=train_size, test_size=test_size, seed=<span class="hljs-number">42</span>
)
downsampled_dataset`}}),bn=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">10000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1000</span>
    })
})`}}),kn=new D({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),qn=new D({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});const Wc=[ng,ag],Tn=[];function Jc(e,u){return e[0]==="tf"?0:1}nt=Jc(B),lt=Tn[nt]=Wc[nt](B),$n=new Ea({}),wn=new zu({props:{id:"NURcDHhYe98"}});const Yc=[og,lg],Nn=[];function Qc(e,u){return e[0]==="pt"?0:1}ot=Qc(B),rt=Nn[ot]=Yc[ot](B),xn=new D({props:{code:"Perplexity: 21.75",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexity: <span class="hljs-number">21.75</span>'}});const Xc=[ig,rg],On=[];function Zc(e,u){return e[0]==="pt"?0:1}it=Zc(B),ut=On[it]=Xc[it](B);const e_=[pg,ug],Kn=[];function s_(e,u){return e[0]==="pt"?0:1}pt=s_(B),dt=Kn[pt]=e_[pt](B),En=new D({props:{code:"Perplexity: 11.32",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexity: <span class="hljs-number">11.32</span>'}});let Le=B[0]==="pt"&&Sh();_a=new xa({props:{$$slots:{default:[dg]},$$scope:{ctx:B}}});let Re=B[0]==="pt"&&Th();return jn=new Ea({}),yn=new D({props:{code:`from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

mask_filler = pipeline(
    <span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/distilbert-base-uncased-finetuned-imdb&quot;</span>
)`}}),zn=new D({props:{code:`preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")`,highlighted:`preds = mask_filler(text)

<span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{pred[<span class="hljs-string">&#x27;sequence&#x27;</span>]}</span>&quot;</span>)`}}),Mn=new D({props:{code:`'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movie.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great film.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great story.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movies.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great character.&#x27;</span>`}}),Cn=new zu({props:{id:"0Oxphw4Q9fo"}}),ha=new xa({props:{$$slots:{default:[mg]},$$scope:{ctx:B}}}),{c(){i=l("meta"),h=c(),w(d.$$.fragment),$=c(),P=l("h1"),b=l("a"),k=l("span"),w(M.$$.fragment),f=c(),C=l("span"),O=a("*Finetuner* un mod\xE8le de langage masqu\xE9"),K=c(),F.c(),S=c(),A=l("p"),H=a("Pour de nombreuses applications de NLP impliquant des "),U=l("em"),q=a("transformers"),T=a(", vous pouvez simplement prendre un mod\xE8le pr\xE9-entra\xEEn\xE9 du "),Y=l("em"),Z=a("Hub"),Q=a(" et l\u2019ajuster directement sur vos donn\xE9es pour la t\xE2che \xE0 accomplir. Pour autant que le corpus utilis\xE9 pour le pr\xE9-entra\xEEnement ne soit pas trop diff\xE9rent du corpus utilis\xE9 pour le "),G=l("em"),W=a("finetuning"),ne=a(", l\u2019apprentissage par transfert produira g\xE9n\xE9ralement de bons r\xE9sultats."),V=c(),I=l("p"),re=a("Cependant, il existe quelques cas o\xF9 vous voudrez d\u2019abord affiner les mod\xE8les de langue sur vos donn\xE9es, avant d\u2019entra\xEEner une t\xEAte sp\xE9cifique \xE0 la t\xE2che. Par exemple, si votre ensemble de donn\xE9es contient des contrats l\xE9gaux ou des articles scientifiques, un mod\xE8le de transformation classique comme BERT traitera g\xE9n\xE9ralement les mots sp\xE9cifiques au domaine dans votre corpus comme des "),X=l("em"),qe=a("tokens"),le=a(" rares, et les performances r\xE9sultantes peuvent \xEAtre moins que satisfaisantes. En "),Ee=l("em"),Be=a("finetunant"),ie=a(" le mod\xE8le linguistique sur les donn\xE9es du domaine, vous pouvez am\xE9liorer les performances de nombreuses t\xE2ches en aval, ce qui signifie que vous ne devez g\xE9n\xE9ralement effectuer cette \xE9tape qu\u2019une seule fois !"),$e=c(),ue=l("p"),je=a("Ce processus d\u2019ajustement fin d\u2019un mod\xE8le de langage pr\xE9-entra\xEEn\xE9 sur des donn\xE9es "),ce=l("em"),Xe=a("in-domain"),ve=a(" est g\xE9n\xE9ralement appel\xE9 "),Te=l("em"),Ue=a("adaptation au domaine"),Ge=a(". Il a \xE9t\xE9 popularis\xE9 en 2018 par [ULMFiT("),ye=l("a"),_e=a("https://arxiv.org/abs/1801.06146"),ze=a("), qui a \xE9t\xE9 l\u2019une des premi\xE8res architectures neuronales (bas\xE9es sur les LSTM) \xE0 faire en sorte que l\u2019apprentissage par transfert fonctionne r\xE9ellement pour le NLP. Un exemple d\u2019adaptation de domaine avec ULMFiT est pr\xE9sent\xE9 dans l\u2019image ci-dessous ; dans cette section, nous ferons quelque chose de similaire, mais avec un "),de=l("em"),pe=a("transformer"),be=a(" au lieu d\u2019un LSTM !"),Me=c(),fe=l("div"),Ne=l("img"),we=c(),Ce=l("img"),We=c(),N=l("p"),se=a("\xC0 la fin de cette section, vous aurez un "),ke=l("a"),oe=a("mod\xE8le de langage masqu\xE9"),Oe=a(" sur le "),Ze=l("em"),Je=a("Hub"),R=a(" qui peut autocompl\xE9ter des phrases comme indiqu\xE9 ci-dessous :"),ee=c(),J=l("iframe"),us=c(),ae=l("iframe"),es=c(),ss=l("p"),Ye=a("Plongeons-y !"),ge=c(),w(xe.$$.fragment),Cs=c(),w(Pe.$$.fragment),Vs=c(),Fe=l("h2"),Ke=l("a"),Ps=l("span"),w(ps.$$.fragment),He=c(),xs=l("span"),Es=a("Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"),It=c(),ds=l("p"),Ws=a("Pour commencer, nous allons choisir un mod\xE8le pr\xE9-entra\xEEn\xE9 appropri\xE9 pour la mod\xE9lisation du langage masqu\xE9. Comme le montre la capture d\u2019\xE9cran suivante, vous pouvez trouver une liste de candidats en appliquant le filtre \u201CFill-Mask\u201D sur le ["),mt=l("em"),_s=a("Hub"),ja=a("] ("),js=l("a"),ya=a("https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads"),za=a(") :"),ct=c(),Js=l("div"),Ys=l("img"),Ds=c(),Qe=l("p"),Qs=a("Bien que les mod\xE8les de la famille BERT et RoBERTa soient les plus t\xE9l\xE9charg\xE9s, nous utiliserons un mod\xE8le appel\xE9 "),As=l("a"),fs=a("DistilBERT"),Ca=a(`
qui peut \xEAtre entra\xEEn\xE9 beaucoup plus rapidement avec peu ou pas de perte de performance en aval. Ce mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide d\u2019une technique sp\xE9ciale appel\xE9e `),ys=l("a"),At=l("em"),Pa=a("distillation de connaissances"),St=a(", o\xF9 un grand \u201Cmod\xE8le ma\xEEtre\u201D comme BERT est utilis\xE9 pour guider l\u2019entra\xEEnement d\u2019un \u201Cmod\xE8le \xE9l\xE8ve\u201D qui a beaucoup moins de param\xE8tres. Une explication des d\xE9tails de la distillation de connaissances nous m\xE8nerait trop loin dans cette section, mais si vous \xEAtes int\xE9ress\xE9, vous pouvez lire tout cela dans "),Xs=l("a"),Tt=l("em"),Ut=a("Natural Language Processing with Transformers"),Zs=a(" (famili\xE8rement connu comme le manuel Transformers)."),Nt=c(),ms.c(),et=c(),ts=l("p"),Gt=a("Avec environ 67 millions de param\xE8tres, DistilBERT est environ deux fois plus petit que le mod\xE8le de base de BERT, ce qui se traduit approximativement par une acc\xE9l\xE9ration de l\u2019entra\xEEnement d\u2019un facteur deux - tr\xE8s bien ! Voyons maintenant quels types de "),zs=l("em"),Vt=a("tokens"),_t=a(" ce mod\xE8le pr\xE9dit comme \xE9tant les compl\xE9ments les plus probables d\u2019un petit \xE9chantillon de texte :"),Wt=c(),w(Ss.$$.fragment),m=c(),z=l("p"),Bn=a("En tant qu\u2019\xEAtres humains, nous pouvons imaginer de nombreuses possibilit\xE9s pour le "),ft=l("em"),Fn=a("token"),Hn=c(),Jt=l("code"),ht=a("[MASK]"),In=a(", telles que \u201Cjour\u201D, \u201Cpromenade\u201D ou \u201Cpeinture\u201D. Pour les mod\xE8les pr\xE9-entra\xEEn\xE9s, les pr\xE9dictions d\xE9pendent du corpus sur lequel le mod\xE8le a \xE9t\xE9 entra\xEEn\xE9, puisqu\u2019il apprend \xE0 d\xE9tecter les mod\xE8les statistiques pr\xE9sents dans les donn\xE9es. Comme BERT, DistilBERT a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur les ensembles de donn\xE9es "),gt=l("a"),hs=a("English Wikipedia"),Un=a(" et "),vt=l("a"),Gn=a("BookCorpus"),Vn=a(", nous nous attendons donc \xE0 ce que les pr\xE9dictions pour "),bt=l("code"),Wn=a("[MASK]"),Ts=a(" refl\xE8tent ces domaines. Pour pr\xE9dire le masque, nous avons besoin du "),Yt=l("em"),Jn=a("tokenizer"),Yn=a(" de DistilBERT pour produire les entr\xE9es du mod\xE8le, alors t\xE9l\xE9chargeons-le \xE9galement depuis le "),kt=l("em"),Qn=a("Hub"),Qt=a(" :"),Da=c(),w(Ns.$$.fragment),Aa=c(),st=l("p"),qt=a("Avec un "),Xt=l("em"),Xn=a("tokenizer"),$t=a(" et un mod\xE8le, nous pouvons maintenant passer notre exemple de texte au mod\xE8le, extraire les logits, et imprimer les 5 meilleurs candidats :"),Sa=c(),vs.c(),Ot=c(),w(Ta.$$.fragment),zr=c(),Zn=l("p"),Mu=a("Nous pouvons voir dans les sorties que les pr\xE9dictions du mod\xE8le se r\xE9f\xE8rent \xE0 des termes de tous les jours, ce qui n\u2019est peut-\xEAtre pas surprenant \xE9tant donn\xE9 le fondement de la Wikip\xE9dia anglaise. Voyons comment nous pouvons changer ce domaine pour quelque chose d\u2019un peu plus sp\xE9cialis\xE9 : des critiques de films tr\xE8s polaris\xE9es !"),Mr=c(),Kt=l("h2"),Zt=l("a"),Ol=l("span"),w(Na.$$.fragment),Cu=c(),Kl=l("span"),Pu=a("Le jeu de donn\xE9es"),Cr=c(),bs=l("p"),Du=a("Pour illustrer l\u2019adaptation au domaine, nous utiliserons le c\xE9l\xE8bre "),Oa=l("a"),Au=a("Large Movie Review Dataset"),Su=a(" (ou IMDb en abr\xE9g\xE9), qui est un corpus de critiques de films souvent utilis\xE9 pour \xE9valuer les mod\xE8les d\u2019analyse de sentiments. En affinant DistilBERT sur ce corpus, nous esp\xE9rons que le mod\xE8le de langage adaptera son vocabulaire des donn\xE9es factuelles de Wikip\xE9dia sur lesquelles il a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 aux \xE9l\xE9ments plus subjectifs des critiques de films. Nous pouvons obtenir les donn\xE9es du "),Ll=l("em"),Tu=a("Hub"),Nu=a(" avec la fonction "),Rl=l("code"),Ou=a("load_dataset()"),Ku=a(" de \u{1F917} "),Bl=l("em"),Lu=a("Datasets"),Ru=a(" :"),Pr=c(),w(Ka.$$.fragment),Dr=c(),w(La.$$.fragment),Ar=c(),as=l("p"),Bu=a("Nous pouvons voir que les parties "),Fl=l("code"),Fu=a("train"),Hu=a(" et "),Hl=l("code"),Iu=a("test"),Uu=a(" sont chacune compos\xE9es de 25 000 critiques, alors qu\u2019il y a une partie non \xE9tiquet\xE9e appel\xE9e "),Il=l("code"),Gu=a("unsupervised"),Vu=a(" qui contient 50 000 critiques. Jetons un coup d\u2019\u0153il  \xE0 quelques \xE9chantillons pour avoir une id\xE9e du type de texte auquel nous avons affaire. Comme nous l\u2019avons fait dans les chapitres pr\xE9c\xE9dents du cours, nous allons encha\xEEner les fonctions "),Ul=l("code"),Wu=a("Dataset.shuffle()"),Ju=a(" et "),Gl=l("code"),Yu=a("Dataset.select()"),Qu=a(" pour cr\xE9er un \xE9chantillon al\xE9atoire :"),Sr=c(),w(Ra.$$.fragment),Tr=c(),w(Ba.$$.fragment),Nr=c(),wt=l("p"),Xu=a("Oui, ce sont bien des critiques de films, et si vous \xEAtes assez vieux, vous pouvez m\xEAme comprendre le commentaire dans la derni\xE8re critique sur le fait de poss\xE9der une version VHS \u{1F61C} ! Bien que nous n\u2019ayons pas besoin des \xE9tiquettes pour la mod\xE9lisation du langage, nous pouvons d\xE9j\xE0 voir qu\u2019un "),Vl=l("code"),Zu=a("0"),ep=a(" d\xE9note une critique n\xE9gative, tandis qu\u2019un "),Wl=l("code"),sp=a("1"),tp=a(" correspond \xE0 une critique positive."),Or=c(),w(ea.$$.fragment),Kr=c(),sa=l("p"),ap=a("Maintenant que nous avons jet\xE9 un coup d\u2019\u0153il rapide aux donn\xE9es, plongeons dans leur pr\xE9paration pour la mod\xE9lisation du langage masqu\xE9. Comme nous allons le voir, il y a quelques \xE9tapes suppl\xE9mentaires \xE0 suivre par rapport aux t\xE2ches de classification de s\xE9quences que nous avons vues au "),el=l("a"),np=a("Chapitre 3"),lp=a(". Allons-y !"),Lr=c(),Lt=l("h2"),ta=l("a"),Jl=l("span"),w(Fa.$$.fragment),op=c(),Yl=l("span"),rp=a("Pr\xE9traitement des donn\xE9es"),Rr=c(),w(Ha.$$.fragment),Br=c(),aa=l("p"),ip=a("Pour la mod\xE9lisation autor\xE9gressive et la mod\xE9lisation du langage masqu\xE9, une \xE9tape commune de pr\xE9traitement consiste \xE0 concat\xE9ner tous les exemples, puis \xE0 diviser le corpus entier en morceaux de taille \xE9gale. C\u2019est tr\xE8s diff\xE9rent de notre approche habituelle, o\xF9 nous nous contentons de "),Ql=l("em"),up=a("tokenizer"),pp=a(" les exemples individuels. Pourquoi tout concat\xE9ner ? La raison est que les exemples individuels peuvent \xEAtre tronqu\xE9s s\u2019ils sont trop longs, ce qui entra\xEEnerait la perte d\u2019informations qui pourraient \xEAtre utiles pour la t\xE2che de mod\xE9lisation du langage !"),Fr=c(),De=l("p"),dp=a("Donc pour commencer, nous allons d\u2019abord tokeniser notre corpus comme d\u2019habitude, mais "),Xl=l("em"),mp=a("sans"),cp=a(" mettre l\u2019option "),Zl=l("code"),_p=a("truncation=True"),fp=a(" dans notre "),eo=l("em"),hp=a("tokenizer"),gp=a(". Nous allons aussi r\xE9cup\xE9rer les IDs des mots s\u2019ils sont disponibles (ce qui sera le cas si nous utilisons un "),so=l("em"),vp=a("tokenizer"),bp=a(" rapide, comme d\xE9crit dans "),sl=l("a"),kp=a("Chapter 6"),qp=a("), car nous en aurons besoin plus tard pour faire le masquage des mots entiers. Nous allons envelopper cela dans une simple fonction, et pendant que nous y sommes, nous allons supprimer les colonnes "),to=l("code"),$p=a("text"),wp=a(" et "),ao=l("code"),xp=a("label"),Ep=a(" puisque nous n\u2019en avons plus besoin :"),Hr=c(),w(Ia.$$.fragment),Ir=c(),w(Ua.$$.fragment),Ur=c(),Os=l("p"),jp=a("Comme DistilBERT est un mod\xE8le de type BERT, nous pouvons voir que les textes encod\xE9s sont constitu\xE9s des "),no=l("code"),yp=a("input_ids"),zp=a(" et des "),lo=l("code"),Mp=a("attention_mask"),Cp=a(" que nous avons vus dans d\u2019autres chapitres, ainsi que des "),oo=l("code"),Pp=a("word_ids"),Dp=a(" que nous avons ajout\xE9s."),Gr=c(),Ks=l("p"),Ap=a("Maintenant que nos critiques de films ont \xE9t\xE9 tokenis\xE9es, l\u2019\xE9tape suivante consiste \xE0 les regrouper et \xE0 diviser le r\xE9sultat en chunks. Mais quelle taille doivent avoir ces "),ro=l("em"),Sp=a("chunks"),Tp=a(" ? Cela sera finalement d\xE9termin\xE9 par la quantit\xE9 de m\xE9moire GPU dont vous disposez, mais un bon point de d\xE9part est de voir quelle est la taille maximale du contexte du mod\xE8le. Cela peut \xEAtre d\xE9duit en inspectant l\u2019attribut "),io=l("code"),Np=a("model_max_length"),Op=a(" du "),uo=l("em"),Kp=a("tokenizer"),Lp=a(" :"),Vr=c(),w(Ga.$$.fragment),Wr=c(),w(Va.$$.fragment),Jr=c(),xt=l("p"),Rp=a("Cette valeur est d\xE9riv\xE9e du fichier "),po=l("em"),Bp=a("tokenizer_config.json"),Fp=a(" associ\xE9 \xE0 un point de contr\xF4le ; dans ce cas, nous pouvons voir que la taille du contexte est de 512 "),mo=l("em"),Hp=a("tokens"),Ip=a(", tout comme avec BERT."),Yr=c(),w(na.$$.fragment),Qr=c(),tl=l("p"),Up=a("Ainsi, pour r\xE9aliser nos exp\xE9riences sur des GPU comme ceux de Google Colab, nous choisirons quelque chose d\u2019un peu plus petit qui peut tenir en m\xE9moire :"),Xr=c(),w(Wa.$$.fragment),Zr=c(),w(la.$$.fragment),ei=c(),oa=l("p"),Gp=a("Maintenant vient la partie amusante. Pour montrer comment la concat\xE9nation fonctionne, prenons quelques commentaires de notre ensemble d\u2019entra\xEEnement et imprimons le nombre de "),co=l("em"),Vp=a("tokens"),Wp=a(" par commentaire :"),si=c(),w(Ja.$$.fragment),ti=c(),w(Ya.$$.fragment),ai=c(),al=l("p"),Jp=a("Nous pouvons ensuite concat\xE9ner tous ces exemples avec une simple compr\xE9hension du dictionnaire, comme suit :"),ni=c(),w(Qa.$$.fragment),li=c(),w(Xa.$$.fragment),oi=c(),Ls=l("p"),Yp=a("Super, la longueur totale est correcte. Donc maintenant, nous allons diviser les exemples concat\xE9n\xE9s en morceaux de la taille donn\xE9e par "),_o=l("code"),Qp=a("block_size"),Xp=a(". Pour ce faire, nous it\xE9rons sur les caract\xE9ristiques de "),fo=l("code"),Zp=a("concatenated_examples"),ed=a(" et utilisons une compr\xE9hension de liste pour cr\xE9er des tranches de chaque caract\xE9ristique. Le r\xE9sultat est un dictionnaire de "),ho=l("em"),sd=a("chunks"),td=a(" pour chaque caract\xE9ristique :"),ri=c(),w(Za.$$.fragment),ii=c(),w(en.$$.fragment),ui=c(),ra=l("p"),ad=a("Comme vous pouvez le voir dans cet exemple, le dernier "),go=l("em"),nd=a("chunk"),ld=a(" sera g\xE9n\xE9ralement plus petit que la taille maximale des morceaux. Il y a deux strat\xE9gies principales pour g\xE9rer cela :"),pi=c(),ia=l("ul"),sn=l("li"),od=a("abandonner le dernier morceau s\u2019il est plus petit que "),vo=l("code"),rd=a("chunk_size"),id=a("."),ud=c(),tn=l("li"),pd=a("remplir le dernier morceau jusqu\u2019\xE0 ce que sa longueur soit \xE9gale \xE0 "),bo=l("code"),dd=a("chunk_size"),md=a("."),di=c(),nl=l("p"),cd=a("Nous adopterons la premi\xE8re approche ici, donc nous allons envelopper toute la logique ci-dessus dans une seule fonction que nous pouvons appliquer \xE0 nos jeux de donn\xE9es tokenis\xE9s :"),mi=c(),w(an.$$.fragment),ci=c(),ns=l("p"),_d=a("Notez que dans la derni\xE8re \xE9tape de "),ko=l("code"),fd=a("group_texts()"),hd=a(" nous cr\xE9ons une nouvelle colonne "),qo=l("code"),gd=a("labels"),vd=a(" qui est une copie de la colonne "),$o=l("code"),bd=a("input_ids"),kd=a(". Comme nous le verrons bient\xF4t, c\u2019est parce que dans la mod\xE9lisation du langage masqu\xE9, l\u2019objectif est de pr\xE9dire des "),wo=l("em"),qd=a("tokens"),$d=a(" masqu\xE9s al\xE9atoirement dans le batch d\u2019entr\xE9e, et en cr\xE9ant une colonne "),xo=l("code"),wd=a("labels"),xd=a(", nous fournissons la v\xE9rit\xE9 de base pour notre mod\xE8le de langage \xE0 apprendre."),_i=c(),Et=l("p"),Ed=a("Appliquons maintenant "),Eo=l("code"),jd=a("group_texts()"),yd=a(" \xE0 nos jeux de donn\xE9es tokenis\xE9s en utilisant notre fid\xE8le fonction "),jo=l("code"),zd=a("Dataset.map()"),Md=a(" :"),fi=c(),w(nn.$$.fragment),hi=c(),w(ln.$$.fragment),gi=c(),Ae=l("p"),Cd=a("Vous pouvez voir que le regroupement puis le d\xE9coupage des textes a produit beaucoup plus d\u2019exemples que nos 25 000 exemples initiaux pour les divisions "),yo=l("code"),Pd=a("train"),Dd=a(" et "),zo=l("code"),Ad=a("test"),Sd=a(". C\u2019est parce que nous avons maintenant des exemples impliquant des \u201D"),Mo=l("em"),Td=a("tokens"),Nd=a(" contigus\u201D qui s\u2019\xE9tendent sur plusieurs exemples du corpus original. Vous pouvez le voir explicitement en cherchant les "),Co=l("em"),Od=a("tokens"),Kd=a(" sp\xE9ciaux "),Po=l("code"),Ld=a("[SEP]"),Rd=a(" et "),Do=l("code"),Bd=a("[CLS]"),Fd=a(" dans l\u2019un des "),Ao=l("em"),Hd=a("chunks"),Id=a(" :"),vi=c(),w(on.$$.fragment),bi=c(),w(rn.$$.fragment),ki=c(),ll=l("p"),Ud=a("Dans cet exemple, vous pouvez voir deux critiques de films qui se chevauchent, l\u2019une sur un film de lyc\xE9e et l\u2019autre sur les sans-abri. Voyons \xE9galement \xE0 quoi ressemblent les \xE9tiquettes pour la mod\xE9lisation du langage masqu\xE9 :"),qi=c(),w(un.$$.fragment),$i=c(),w(pn.$$.fragment),wi=c(),Rs=l("p"),Gd=a("Comme pr\xE9vu par notre fonction "),So=l("code"),Vd=a("group_texts()"),Wd=a(" ci-dessus, cela semble identique aux "),To=l("code"),Jd=a("input_ids"),Yd=a(" d\xE9cod\xE9s - mais alors comment notre mod\xE8le peut-il apprendre quoi que ce soit ? Il nous manque une \xE9tape cl\xE9 : ins\xE9rer des "),No=l("em"),Qd=a("tokens"),Xd=a(" \xE0 des positions al\xE9atoires dans les entr\xE9es ! Voyons comment nous pouvons le faire \xE0 la vol\xE9e pendant le r\xE9glage fin en utilisant un collateur de donn\xE9es sp\xE9cial."),xi=c(),Rt=l("h2"),ua=l("a"),Oo=l("span"),w(dn.$$.fragment),Zd=c(),mn=l("span"),em=a("*Finetuning* de DistilBERT avec l'API "),Ko=l("code"),sm=a("Trainer"),tm=a("."),Ei=c(),me=l("p"),am=a("Le "),Lo=l("em"),nm=a("finetuning"),lm=a(" d\u2019un mod\xE8le de langage masqu\xE9 est presque identique au "),Ro=l("em"),om=a("finetuning"),rm=a(" d\u2019un mod\xE8le de classification de s\xE9quences, comme nous l\u2019avons fait dans le "),ol=l("a"),im=a("Chapitre 3"),um=a(". La seule diff\xE9rence est que nous avons besoin d\u2019un collecteur de donn\xE9es sp\xE9cial qui peut masquer de mani\xE8re al\xE9atoire certains des "),Bo=l("em"),pm=a("tokens"),dm=a(" dans chaque lot de textes. Heureusement, \u{1F917} "),Fo=l("em"),mm=a("Transformers"),cm=a(" est livr\xE9 pr\xE9par\xE9 avec un "),Ho=l("code"),_m=a("DataCollatorForLanguageModeling"),fm=a(" d\xE9di\xE9 \xE0 cette t\xE2che. Nous devons juste lui passer le "),Io=l("em"),hm=a("tokenizer"),gm=a(" et un argument "),Uo=l("code"),vm=a("mlm_probability"),bm=a(" qui sp\xE9cifie quelle fraction des "),Go=l("em"),km=a("tokens"),qm=a(" \xE0 masquer. Nous choisirons 15%, qui est la quantit\xE9 utilis\xE9e pour BERT et un choix commun dans la litt\xE9rature :"),ji=c(),w(cn.$$.fragment),yi=c(),Bs=l("p"),$m=a("Pour voir comment le masquage al\xE9atoire fonctionne, nous allons donner quelques exemples au compilateur de donn\xE9es. Puisqu\u2019il s\u2019attend \xE0 une liste de "),Vo=l("code"),wm=a("dict"),xm=a("s, o\xF9 chaque "),Wo=l("code"),Em=a("dict"),jm=a(" repr\xE9sente un seul morceau de texte contigu, nous it\xE9rons d\u2019abord sur le jeu de donn\xE9es avant de nourrir le lot au collateur. Nous supprimons la cl\xE9 "),Jo=l("code"),ym=a('"word_ids"'),zm=a(" pour ce collateur de donn\xE9es car il ne l\u2019attend pas :"),zi=c(),w(_n.$$.fragment),Mi=c(),w(fn.$$.fragment),Ci=c(),ks=l("p"),Mm=a("Super, \xE7a a march\xE9 ! Nous pouvons voir que le "),Yo=l("em"),Cm=a("token"),Pm=c(),Qo=l("code"),Dm=a("[MASK]"),Am=a(" a \xE9t\xE9 ins\xE9r\xE9 de fa\xE7on al\xE9atoire \xE0 diff\xE9rents endroits dans notre texte. Ce seront les "),Xo=l("em"),Sm=a("tokens"),Tm=a(" que notre mod\xE8le devra pr\xE9dire pendant l\u2019entra\xEEnement. Et la beaut\xE9 du collecteur de donn\xE9es est qu\u2019il va rendre al\xE9atoire l\u2019insertion du "),Zo=l("code"),Nm=a("[MASK]"),Om=a(" \xE0 chaque lot !"),Pi=c(),w(pa.$$.fragment),Di=c(),cs&&cs.c(),rl=c(),qs=l("p"),Km=a("Lors de l\u2019entra\xEEnement des mod\xE8les pour la mod\xE9lisation du langage masqu\xE9, une technique qui peut \xEAtre utilis\xE9e est de masquer des mots entiers ensemble, et pas seulement des "),er=l("em"),Lm=a("tokens"),Rm=a(" individuels. Cette approche est appel\xE9e "),sr=l("em"),Bm=a("masquage de mots entiers"),Fm=a(". Si nous voulons utiliser le masquage de mots entiers, nous devons construire nous-m\xEAmes un collateur de donn\xE9es. Un collateur de donn\xE9es est simplement une fonction qui prend une liste d\u2019\xE9chantillons et les convertit en un lot, alors faisons-le maintenant ! Nous utiliserons les IDs des mots calcul\xE9s plus t\xF4t pour faire une correspondance entre les indices des mots et les "),tr=l("em"),Hm=a("tokens"),Im=a(" correspondants, puis nous d\xE9ciderons al\xE9atoirement quels mots masquer et appliquerons ce masque sur les entr\xE9es. Notez que les \xE9tiquettes sont toutes "),ar=l("code"),Um=a("-100"),Gm=a(" sauf celles qui correspondent aux mots masqu\xE9s."),Ai=c(),at.c(),il=c(),ul=l("p"),Vm=a("Ensuite, nous pouvons l\u2019essayer sur les m\xEAmes \xE9chantillons que pr\xE9c\xE9demment :"),Si=c(),w(hn.$$.fragment),Ti=c(),w(gn.$$.fragment),Ni=c(),w(da.$$.fragment),Oi=c(),Fs=l("p"),Wm=a("Maintenant que nous avons deux collateurs de donn\xE9es, le reste des \xE9tapes de mise au point est standard. L\u2019entra\xEEnement peut prendre un certain temps sur Google Colab si vous n\u2019avez pas la chance d\u2019avoir un mythique GPU P100 \u{1F62D}, donc nous allons d\u2019abord r\xE9duire la taille de l\u2019ensemble d\u2019entra\xEEnement \xE0 quelques milliers d\u2019exemples. Ne vous inqui\xE9tez pas, nous obtiendrons quand m\xEAme un mod\xE8le de langage assez d\xE9cent ! Un moyen rapide de r\xE9duire la taille d\u2019un jeu de donn\xE9es dans \u{1F917} "),nr=l("em"),Jm=a("Datasets"),Ym=a(" est la fonction "),lr=l("code"),Qm=a("Dataset.train_test_split()"),Xm=a(" que nous avons vue au "),pl=l("a"),Zm=a("Chapitre 5"),ec=a(" :"),Ki=c(),w(vn.$$.fragment),Li=c(),w(bn.$$.fragment),Ri=c(),$s=l("p"),sc=a("Cela a automatiquement cr\xE9\xE9 de nouvelles divisions "),or=l("code"),tc=a("train"),ac=a(" et "),rr=l("code"),nc=a("test"),lc=a(", avec la taille de l\u2019ensemble d\u2019entra\xEEnement fix\xE9e \xE0 10.000 exemples et la validation fix\xE9e \xE0 10% de cela. N\u2019h\xE9sitez pas \xE0 augmenter cela si vous avez un GPU puissant ! La prochaine chose que nous devons faire est de nous connecter au "),ir=l("em"),oc=a("Hub"),rc=a(". Si vous ex\xE9cutez ce code dans un "),ur=l("em"),ic=a("notebook"),uc=a(", vous pouvez le faire avec la fonction utilitaire suivante :"),Bi=c(),w(kn.$$.fragment),Fi=c(),ma=l("p"),pc=a("qui affichera un "),pr=l("em"),dc=a("widget"),mc=a(" o\xF9 vous pourrez saisir vos informations d\u2019identification. Alternativement, vous pouvez ex\xE9cuter :"),Hi=c(),w(qn.$$.fragment),Ii=c(),dl=l("p"),cc=a("dans votre terminal pr\xE9f\xE9r\xE9 et connectez-vous l\xE0."),Ui=c(),lt.c(),ml=c(),Bt=l("h3"),ca=l("a"),dr=l("span"),w($n.$$.fragment),_c=c(),mr=l("span"),fc=a("Perplexit\xE9 pour les mod\xE8les de langage"),Gi=c(),w(wn.$$.fragment),Vi=c(),cl=l("p"),hc=a("Contrairement \xE0 d\u2019autres t\xE2ches, comme la classification de textes ou la r\xE9ponse \xE0 des questions, sur lesquelles nous disposons d\u2019un corpus \xE9tiquet\xE9 pour nous entra\xEEner, la mod\xE9lisation du langage ne s\u2019appuie sur aucune \xE9tiquette explicite. Alors comment d\xE9terminer ce qui fait un bon mod\xE8le de langage ? Comme pour la fonction de correction automatique de votre t\xE9l\xE9phone, un bon mod\xE8le de langage est celui qui attribue des probabilit\xE9s \xE9lev\xE9es aux phrases grammaticalement correctes et des probabilit\xE9s faibles aux phrases absurdes. Pour vous donner une meilleure id\xE9e de ce \xE0 quoi cela ressemble, vous pouvez trouver en ligne des s\xE9ries enti\xE8res de \u201Crat\xE9s d\u2019autocorrection\u201D, o\xF9 le mod\xE8le du t\xE9l\xE9phone d\u2019une personne a produit des compl\xE9ments plut\xF4t amusants (et souvent inappropri\xE9s) !"),Wi=c(),rt.c(),_l=c(),w(xn.$$.fragment),Ji=c(),fl=l("p"),gc=a("Un score de perplexit\xE9 plus faible signifie un meilleur mod\xE8le de langue, et nous pouvons voir ici que notre mod\xE8le de d\xE9part a une valeur assez \xE9lev\xE9e. Voyons si nous pouvons la r\xE9duire en l\u2019affinant ! Pour ce faire, nous commen\xE7ons par ex\xE9cuter la boucle d\u2019entra\xEEnement :"),Yi=c(),ut.c(),hl=c(),gl=l("p"),vc=a("et ensuite calculer la perplexit\xE9 r\xE9sultante sur l\u2019ensemble de test comme pr\xE9c\xE9demment :"),Qi=c(),dt.c(),vl=c(),w(En.$$.fragment),Xi=c(),bl=l("p"),bc=a("Joli. C\u2019est une r\xE9duction consid\xE9rable de la perplexit\xE9, ce qui nous indique que le mod\xE8le a appris quelque chose sur le domaine des critiques de films !"),Zi=c(),Le&&Le.c(),kl=c(),w(_a.$$.fragment),eu=c(),Re&&Re.c(),ql=c(),Ft=l("h3"),fa=l("a"),cr=l("span"),w(jn.$$.fragment),kc=c(),_r=l("span"),qc=a("Utilisation de notre mod\xE8le *finetun\xE9*"),su=c(),ls=l("p"),$c=a("Vous pouvez interagir avec votre mod\xE8le affin\xE9 soit en utilisant son "),fr=l("em"),wc=a("widget"),xc=a(" sur le "),hr=l("em"),Ec=a("Hub"),jc=a(", soit localement avec le "),gr=l("code"),yc=a("pipeline"),zc=a(" de \u{1F917} "),vr=l("em"),Mc=a("Transformers"),Cc=a(". Utilisons cette derni\xE8re pour t\xE9l\xE9charger notre mod\xE8le en utilisant le pipeline "),br=l("code"),Pc=a("fill-mask"),Dc=a(" :"),tu=c(),w(yn.$$.fragment),au=c(),$l=l("p"),Ac=a("Nous pouvons ensuite alimenter le pipeline avec notre exemple de texte \u201CC\u2019est un grand [MASK]\u201D et voir quelles sont les 5 premi\xE8res pr\xE9dictions :"),nu=c(),w(zn.$$.fragment),lu=c(),w(Mn.$$.fragment),ou=c(),wl=l("p"),Sc=a("Notre mod\xE8le a clairement adapt\xE9 ses pond\xE9rations pour pr\xE9dire les mots qui sont plus fortement associ\xE9s aux films !"),ru=c(),w(Cn.$$.fragment),iu=c(),jt=l("p"),Tc=a("Ceci conclut notre premi\xE8re exp\xE9rience d\u2019entra\xEEnement d\u2019un mod\xE8le de langage. Dans la "),xl=l("a"),Nc=a("section 6"),Oc=a(", vous apprendrez comment entra\xEEner un mod\xE8le autor\xE9gressif comme GPT-2 \xE0 partir de z\xE9ro ; allez-y si vous voulez voir comment vous pouvez pr\xE9-entra\xEEner votre propre "),kr=l("em"),Kc=a("transformer"),Lc=a(" !"),uu=c(),w(ha.$$.fragment),this.h()},l(e){const u=Rh('[data-svelte="svelte-1phssyn"]',document.head);i=o(u,"META",{name:!0,content:!0}),u.forEach(t),h=_(e),x(d.$$.fragment,e),$=_(e),P=o(e,"H1",{class:!0});var Ln=r(P);b=o(Ln,"A",{id:!0,class:!0,href:!0});var El=r(b);k=o(El,"SPAN",{});var qr=r(k);x(M.$$.fragment,qr),qr.forEach(t),El.forEach(t),f=_(Ln),C=o(Ln,"SPAN",{});var jl=r(C);O=n(jl,"*Finetuner* un mod\xE8le de langage masqu\xE9"),jl.forEach(t),Ln.forEach(t),K=_(e),F.l(e),S=_(e),A=o(e,"P",{});var Hs=r(A);H=n(Hs,"Pour de nombreuses applications de NLP impliquant des "),U=o(Hs,"EM",{});var $r=r(U);q=n($r,"transformers"),$r.forEach(t),T=n(Hs,", vous pouvez simplement prendre un mod\xE8le pr\xE9-entra\xEEn\xE9 du "),Y=o(Hs,"EM",{});var wr=r(Y);Z=n(wr,"Hub"),wr.forEach(t),Q=n(Hs," et l\u2019ajuster directement sur vos donn\xE9es pour la t\xE2che \xE0 accomplir. Pour autant que le corpus utilis\xE9 pour le pr\xE9-entra\xEEnement ne soit pas trop diff\xE9rent du corpus utilis\xE9 pour le "),G=o(Hs,"EM",{});var xr=r(G);W=n(xr,"finetuning"),xr.forEach(t),ne=n(Hs,", l\u2019apprentissage par transfert produira g\xE9n\xE9ralement de bons r\xE9sultats."),Hs.forEach(t),V=_(e),I=o(e,"P",{});var Ht=r(I);re=n(Ht,"Cependant, il existe quelques cas o\xF9 vous voudrez d\u2019abord affiner les mod\xE8les de langue sur vos donn\xE9es, avant d\u2019entra\xEEner une t\xEAte sp\xE9cifique \xE0 la t\xE2che. Par exemple, si votre ensemble de donn\xE9es contient des contrats l\xE9gaux ou des articles scientifiques, un mod\xE8le de transformation classique comme BERT traitera g\xE9n\xE9ralement les mots sp\xE9cifiques au domaine dans votre corpus comme des "),X=o(Ht,"EM",{});var yl=r(X);qe=n(yl,"tokens"),yl.forEach(t),le=n(Ht," rares, et les performances r\xE9sultantes peuvent \xEAtre moins que satisfaisantes. En "),Ee=o(Ht,"EM",{});var Er=r(Ee);Be=n(Er,"finetunant"),Er.forEach(t),ie=n(Ht," le mod\xE8le linguistique sur les donn\xE9es du domaine, vous pouvez am\xE9liorer les performances de nombreuses t\xE2ches en aval, ce qui signifie que vous ne devez g\xE9n\xE9ralement effectuer cette \xE9tape qu\u2019une seule fois !"),Ht.forEach(t),$e=_(e),ue=o(e,"P",{});var ws=r(ue);je=n(ws,"Ce processus d\u2019ajustement fin d\u2019un mod\xE8le de langage pr\xE9-entra\xEEn\xE9 sur des donn\xE9es "),ce=o(ws,"EM",{});var zl=r(ce);Xe=n(zl,"in-domain"),zl.forEach(t),ve=n(ws," est g\xE9n\xE9ralement appel\xE9 "),Te=o(ws,"EM",{});var Ml=r(Te);Ue=n(Ml,"adaptation au domaine"),Ml.forEach(t),Ge=n(ws,". Il a \xE9t\xE9 popularis\xE9 en 2018 par [ULMFiT("),ye=o(ws,"A",{href:!0,rel:!0});var Cl=r(ye);_e=n(Cl,"https://arxiv.org/abs/1801.06146"),Cl.forEach(t),ze=n(ws,"), qui a \xE9t\xE9 l\u2019une des premi\xE8res architectures neuronales (bas\xE9es sur les LSTM) \xE0 faire en sorte que l\u2019apprentissage par transfert fonctionne r\xE9ellement pour le NLP. Un exemple d\u2019adaptation de domaine avec ULMFiT est pr\xE9sent\xE9 dans l\u2019image ci-dessous ; dans cette section, nous ferons quelque chose de similaire, mais avec un "),de=o(ws,"EM",{});var jr=r(de);pe=n(jr,"transformer"),jr.forEach(t),be=n(ws," au lieu d\u2019un LSTM !"),ws.forEach(t),Me=_(e),fe=o(e,"DIV",{class:!0});var Rn=r(fe);Ne=o(Rn,"IMG",{class:!0,src:!0,alt:!0}),we=_(Rn),Ce=o(Rn,"IMG",{class:!0,src:!0,alt:!0}),Rn.forEach(t),We=_(e),N=o(e,"P",{});var Pl=r(N);se=n(Pl,"\xC0 la fin de cette section, vous aurez un "),ke=o(Pl,"A",{href:!0,rel:!0});var t_=r(ke);oe=n(t_,"mod\xE8le de langage masqu\xE9"),t_.forEach(t),Oe=n(Pl," sur le "),Ze=o(Pl,"EM",{});var a_=r(Ze);Je=n(a_,"Hub"),a_.forEach(t),R=n(Pl," qui peut autocompl\xE9ter des phrases comme indiqu\xE9 ci-dessous :"),Pl.forEach(t),ee=_(e),J=o(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),r(J).forEach(t),us=_(e),ae=o(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),r(ae).forEach(t),es=_(e),ss=o(e,"P",{});var n_=r(ss);Ye=n(n_,"Plongeons-y !"),n_.forEach(t),ge=_(e),x(xe.$$.fragment,e),Cs=_(e),x(Pe.$$.fragment,e),Vs=_(e),Fe=o(e,"H2",{class:!0});var du=r(Fe);Ke=o(du,"A",{id:!0,class:!0,href:!0});var l_=r(Ke);Ps=o(l_,"SPAN",{});var o_=r(Ps);x(ps.$$.fragment,o_),o_.forEach(t),l_.forEach(t),He=_(du),xs=o(du,"SPAN",{});var r_=r(xs);Es=n(r_,"Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"),r_.forEach(t),du.forEach(t),It=_(e),ds=o(e,"P",{});var Dl=r(ds);Ws=n(Dl,"Pour commencer, nous allons choisir un mod\xE8le pr\xE9-entra\xEEn\xE9 appropri\xE9 pour la mod\xE9lisation du langage masqu\xE9. Comme le montre la capture d\u2019\xE9cran suivante, vous pouvez trouver une liste de candidats en appliquant le filtre \u201CFill-Mask\u201D sur le ["),mt=o(Dl,"EM",{});var i_=r(mt);_s=n(i_,"Hub"),i_.forEach(t),ja=n(Dl,"] ("),js=o(Dl,"A",{href:!0,rel:!0});var u_=r(js);ya=n(u_,"https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads"),u_.forEach(t),za=n(Dl,") :"),Dl.forEach(t),ct=_(e),Js=o(e,"DIV",{class:!0});var p_=r(Js);Ys=o(p_,"IMG",{src:!0,alt:!0,width:!0}),p_.forEach(t),Ds=_(e),Qe=o(e,"P",{});var ga=r(Qe);Qs=n(ga,"Bien que les mod\xE8les de la famille BERT et RoBERTa soient les plus t\xE9l\xE9charg\xE9s, nous utiliserons un mod\xE8le appel\xE9 "),As=o(ga,"A",{href:!0,rel:!0});var d_=r(As);fs=n(d_,"DistilBERT"),d_.forEach(t),Ca=n(ga,`
qui peut \xEAtre entra\xEEn\xE9 beaucoup plus rapidement avec peu ou pas de perte de performance en aval. Ce mod\xE8le a \xE9t\xE9 entra\xEEn\xE9 \xE0 l\u2019aide d\u2019une technique sp\xE9ciale appel\xE9e `),ys=o(ga,"A",{href:!0,rel:!0});var m_=r(ys);At=o(m_,"EM",{});var c_=r(At);Pa=n(c_,"distillation de connaissances"),c_.forEach(t),m_.forEach(t),St=n(ga,", o\xF9 un grand \u201Cmod\xE8le ma\xEEtre\u201D comme BERT est utilis\xE9 pour guider l\u2019entra\xEEnement d\u2019un \u201Cmod\xE8le \xE9l\xE8ve\u201D qui a beaucoup moins de param\xE8tres. Une explication des d\xE9tails de la distillation de connaissances nous m\xE8nerait trop loin dans cette section, mais si vous \xEAtes int\xE9ress\xE9, vous pouvez lire tout cela dans "),Xs=o(ga,"A",{href:!0,rel:!0});var __=r(Xs);Tt=o(__,"EM",{});var f_=r(Tt);Ut=n(f_,"Natural Language Processing with Transformers"),f_.forEach(t),__.forEach(t),Zs=n(ga," (famili\xE8rement connu comme le manuel Transformers)."),ga.forEach(t),Nt=_(e),ms.l(e),et=_(e),ts=o(e,"P",{});var mu=r(ts);Gt=n(mu,"Avec environ 67 millions de param\xE8tres, DistilBERT est environ deux fois plus petit que le mod\xE8le de base de BERT, ce qui se traduit approximativement par une acc\xE9l\xE9ration de l\u2019entra\xEEnement d\u2019un facteur deux - tr\xE8s bien ! Voyons maintenant quels types de "),zs=o(mu,"EM",{});var h_=r(zs);Vt=n(h_,"tokens"),h_.forEach(t),_t=n(mu," ce mod\xE8le pr\xE9dit comme \xE9tant les compl\xE9ments les plus probables d\u2019un petit \xE9chantillon de texte :"),mu.forEach(t),Wt=_(e),x(Ss.$$.fragment,e),m=_(e),z=o(e,"P",{});var os=r(z);Bn=n(os,"En tant qu\u2019\xEAtres humains, nous pouvons imaginer de nombreuses possibilit\xE9s pour le "),ft=o(os,"EM",{});var g_=r(ft);Fn=n(g_,"token"),g_.forEach(t),Hn=_(os),Jt=o(os,"CODE",{});var v_=r(Jt);ht=n(v_,"[MASK]"),v_.forEach(t),In=n(os,", telles que \u201Cjour\u201D, \u201Cpromenade\u201D ou \u201Cpeinture\u201D. Pour les mod\xE8les pr\xE9-entra\xEEn\xE9s, les pr\xE9dictions d\xE9pendent du corpus sur lequel le mod\xE8le a \xE9t\xE9 entra\xEEn\xE9, puisqu\u2019il apprend \xE0 d\xE9tecter les mod\xE8les statistiques pr\xE9sents dans les donn\xE9es. Comme BERT, DistilBERT a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur les ensembles de donn\xE9es "),gt=o(os,"A",{href:!0,rel:!0});var b_=r(gt);hs=n(b_,"English Wikipedia"),b_.forEach(t),Un=n(os," et "),vt=o(os,"A",{href:!0,rel:!0});var k_=r(vt);Gn=n(k_,"BookCorpus"),k_.forEach(t),Vn=n(os,", nous nous attendons donc \xE0 ce que les pr\xE9dictions pour "),bt=o(os,"CODE",{});var q_=r(bt);Wn=n(q_,"[MASK]"),q_.forEach(t),Ts=n(os," refl\xE8tent ces domaines. Pour pr\xE9dire le masque, nous avons besoin du "),Yt=o(os,"EM",{});var $_=r(Yt);Jn=n($_,"tokenizer"),$_.forEach(t),Yn=n(os," de DistilBERT pour produire les entr\xE9es du mod\xE8le, alors t\xE9l\xE9chargeons-le \xE9galement depuis le "),kt=o(os,"EM",{});var w_=r(kt);Qn=n(w_,"Hub"),w_.forEach(t),Qt=n(os," :"),os.forEach(t),Da=_(e),x(Ns.$$.fragment,e),Aa=_(e),st=o(e,"P",{});var cu=r(st);qt=n(cu,"Avec un "),Xt=o(cu,"EM",{});var x_=r(Xt);Xn=n(x_,"tokenizer"),x_.forEach(t),$t=n(cu," et un mod\xE8le, nous pouvons maintenant passer notre exemple de texte au mod\xE8le, extraire les logits, et imprimer les 5 meilleurs candidats :"),cu.forEach(t),Sa=_(e),vs.l(e),Ot=_(e),x(Ta.$$.fragment,e),zr=_(e),Zn=o(e,"P",{});var E_=r(Zn);Mu=n(E_,"Nous pouvons voir dans les sorties que les pr\xE9dictions du mod\xE8le se r\xE9f\xE8rent \xE0 des termes de tous les jours, ce qui n\u2019est peut-\xEAtre pas surprenant \xE9tant donn\xE9 le fondement de la Wikip\xE9dia anglaise. Voyons comment nous pouvons changer ce domaine pour quelque chose d\u2019un peu plus sp\xE9cialis\xE9 : des critiques de films tr\xE8s polaris\xE9es !"),E_.forEach(t),Mr=_(e),Kt=o(e,"H2",{class:!0});var _u=r(Kt);Zt=o(_u,"A",{id:!0,class:!0,href:!0});var j_=r(Zt);Ol=o(j_,"SPAN",{});var y_=r(Ol);x(Na.$$.fragment,y_),y_.forEach(t),j_.forEach(t),Cu=_(_u),Kl=o(_u,"SPAN",{});var z_=r(Kl);Pu=n(z_,"Le jeu de donn\xE9es"),z_.forEach(t),_u.forEach(t),Cr=_(e),bs=o(e,"P",{});var yt=r(bs);Du=n(yt,"Pour illustrer l\u2019adaptation au domaine, nous utiliserons le c\xE9l\xE8bre "),Oa=o(yt,"A",{href:!0,rel:!0});var M_=r(Oa);Au=n(M_,"Large Movie Review Dataset"),M_.forEach(t),Su=n(yt," (ou IMDb en abr\xE9g\xE9), qui est un corpus de critiques de films souvent utilis\xE9 pour \xE9valuer les mod\xE8les d\u2019analyse de sentiments. En affinant DistilBERT sur ce corpus, nous esp\xE9rons que le mod\xE8le de langage adaptera son vocabulaire des donn\xE9es factuelles de Wikip\xE9dia sur lesquelles il a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 aux \xE9l\xE9ments plus subjectifs des critiques de films. Nous pouvons obtenir les donn\xE9es du "),Ll=o(yt,"EM",{});var C_=r(Ll);Tu=n(C_,"Hub"),C_.forEach(t),Nu=n(yt," avec la fonction "),Rl=o(yt,"CODE",{});var P_=r(Rl);Ou=n(P_,"load_dataset()"),P_.forEach(t),Ku=n(yt," de \u{1F917} "),Bl=o(yt,"EM",{});var D_=r(Bl);Lu=n(D_,"Datasets"),D_.forEach(t),Ru=n(yt," :"),yt.forEach(t),Pr=_(e),x(Ka.$$.fragment,e),Dr=_(e),x(La.$$.fragment,e),Ar=_(e),as=o(e,"P",{});var Is=r(as);Bu=n(Is,"Nous pouvons voir que les parties "),Fl=o(Is,"CODE",{});var A_=r(Fl);Fu=n(A_,"train"),A_.forEach(t),Hu=n(Is," et "),Hl=o(Is,"CODE",{});var S_=r(Hl);Iu=n(S_,"test"),S_.forEach(t),Uu=n(Is," sont chacune compos\xE9es de 25 000 critiques, alors qu\u2019il y a une partie non \xE9tiquet\xE9e appel\xE9e "),Il=o(Is,"CODE",{});var T_=r(Il);Gu=n(T_,"unsupervised"),T_.forEach(t),Vu=n(Is," qui contient 50 000 critiques. Jetons un coup d\u2019\u0153il  \xE0 quelques \xE9chantillons pour avoir une id\xE9e du type de texte auquel nous avons affaire. Comme nous l\u2019avons fait dans les chapitres pr\xE9c\xE9dents du cours, nous allons encha\xEEner les fonctions "),Ul=o(Is,"CODE",{});var N_=r(Ul);Wu=n(N_,"Dataset.shuffle()"),N_.forEach(t),Ju=n(Is," et "),Gl=o(Is,"CODE",{});var O_=r(Gl);Yu=n(O_,"Dataset.select()"),O_.forEach(t),Qu=n(Is," pour cr\xE9er un \xE9chantillon al\xE9atoire :"),Is.forEach(t),Sr=_(e),x(Ra.$$.fragment,e),Tr=_(e),x(Ba.$$.fragment,e),Nr=_(e),wt=o(e,"P",{});var Al=r(wt);Xu=n(Al,"Oui, ce sont bien des critiques de films, et si vous \xEAtes assez vieux, vous pouvez m\xEAme comprendre le commentaire dans la derni\xE8re critique sur le fait de poss\xE9der une version VHS \u{1F61C} ! Bien que nous n\u2019ayons pas besoin des \xE9tiquettes pour la mod\xE9lisation du langage, nous pouvons d\xE9j\xE0 voir qu\u2019un "),Vl=o(Al,"CODE",{});var K_=r(Vl);Zu=n(K_,"0"),K_.forEach(t),ep=n(Al," d\xE9note une critique n\xE9gative, tandis qu\u2019un "),Wl=o(Al,"CODE",{});var L_=r(Wl);sp=n(L_,"1"),L_.forEach(t),tp=n(Al," correspond \xE0 une critique positive."),Al.forEach(t),Or=_(e),x(ea.$$.fragment,e),Kr=_(e),sa=o(e,"P",{});var fu=r(sa);ap=n(fu,"Maintenant que nous avons jet\xE9 un coup d\u2019\u0153il rapide aux donn\xE9es, plongeons dans leur pr\xE9paration pour la mod\xE9lisation du langage masqu\xE9. Comme nous allons le voir, il y a quelques \xE9tapes suppl\xE9mentaires \xE0 suivre par rapport aux t\xE2ches de classification de s\xE9quences que nous avons vues au "),el=o(fu,"A",{href:!0});var R_=r(el);np=n(R_,"Chapitre 3"),R_.forEach(t),lp=n(fu,". Allons-y !"),fu.forEach(t),Lr=_(e),Lt=o(e,"H2",{class:!0});var hu=r(Lt);ta=o(hu,"A",{id:!0,class:!0,href:!0});var B_=r(ta);Jl=o(B_,"SPAN",{});var F_=r(Jl);x(Fa.$$.fragment,F_),F_.forEach(t),B_.forEach(t),op=_(hu),Yl=o(hu,"SPAN",{});var H_=r(Yl);rp=n(H_,"Pr\xE9traitement des donn\xE9es"),H_.forEach(t),hu.forEach(t),Rr=_(e),x(Ha.$$.fragment,e),Br=_(e),aa=o(e,"P",{});var gu=r(aa);ip=n(gu,"Pour la mod\xE9lisation autor\xE9gressive et la mod\xE9lisation du langage masqu\xE9, une \xE9tape commune de pr\xE9traitement consiste \xE0 concat\xE9ner tous les exemples, puis \xE0 diviser le corpus entier en morceaux de taille \xE9gale. C\u2019est tr\xE8s diff\xE9rent de notre approche habituelle, o\xF9 nous nous contentons de "),Ql=o(gu,"EM",{});var I_=r(Ql);up=n(I_,"tokenizer"),I_.forEach(t),pp=n(gu," les exemples individuels. Pourquoi tout concat\xE9ner ? La raison est que les exemples individuels peuvent \xEAtre tronqu\xE9s s\u2019ils sont trop longs, ce qui entra\xEEnerait la perte d\u2019informations qui pourraient \xEAtre utiles pour la t\xE2che de mod\xE9lisation du langage !"),gu.forEach(t),Fr=_(e),De=o(e,"P",{});var rs=r(De);dp=n(rs,"Donc pour commencer, nous allons d\u2019abord tokeniser notre corpus comme d\u2019habitude, mais "),Xl=o(rs,"EM",{});var U_=r(Xl);mp=n(U_,"sans"),U_.forEach(t),cp=n(rs," mettre l\u2019option "),Zl=o(rs,"CODE",{});var G_=r(Zl);_p=n(G_,"truncation=True"),G_.forEach(t),fp=n(rs," dans notre "),eo=o(rs,"EM",{});var V_=r(eo);hp=n(V_,"tokenizer"),V_.forEach(t),gp=n(rs,". Nous allons aussi r\xE9cup\xE9rer les IDs des mots s\u2019ils sont disponibles (ce qui sera le cas si nous utilisons un "),so=o(rs,"EM",{});var W_=r(so);vp=n(W_,"tokenizer"),W_.forEach(t),bp=n(rs," rapide, comme d\xE9crit dans "),sl=o(rs,"A",{href:!0});var J_=r(sl);kp=n(J_,"Chapter 6"),J_.forEach(t),qp=n(rs,"), car nous en aurons besoin plus tard pour faire le masquage des mots entiers. Nous allons envelopper cela dans une simple fonction, et pendant que nous y sommes, nous allons supprimer les colonnes "),to=o(rs,"CODE",{});var Y_=r(to);$p=n(Y_,"text"),Y_.forEach(t),wp=n(rs," et "),ao=o(rs,"CODE",{});var Q_=r(ao);xp=n(Q_,"label"),Q_.forEach(t),Ep=n(rs," puisque nous n\u2019en avons plus besoin :"),rs.forEach(t),Hr=_(e),x(Ia.$$.fragment,e),Ir=_(e),x(Ua.$$.fragment,e),Ur=_(e),Os=o(e,"P",{});var va=r(Os);jp=n(va,"Comme DistilBERT est un mod\xE8le de type BERT, nous pouvons voir que les textes encod\xE9s sont constitu\xE9s des "),no=o(va,"CODE",{});var X_=r(no);yp=n(X_,"input_ids"),X_.forEach(t),zp=n(va," et des "),lo=o(va,"CODE",{});var Z_=r(lo);Mp=n(Z_,"attention_mask"),Z_.forEach(t),Cp=n(va," que nous avons vus dans d\u2019autres chapitres, ainsi que des "),oo=o(va,"CODE",{});var ef=r(oo);Pp=n(ef,"word_ids"),ef.forEach(t),Dp=n(va," que nous avons ajout\xE9s."),va.forEach(t),Gr=_(e),Ks=o(e,"P",{});var ba=r(Ks);Ap=n(ba,"Maintenant que nos critiques de films ont \xE9t\xE9 tokenis\xE9es, l\u2019\xE9tape suivante consiste \xE0 les regrouper et \xE0 diviser le r\xE9sultat en chunks. Mais quelle taille doivent avoir ces "),ro=o(ba,"EM",{});var sf=r(ro);Sp=n(sf,"chunks"),sf.forEach(t),Tp=n(ba," ? Cela sera finalement d\xE9termin\xE9 par la quantit\xE9 de m\xE9moire GPU dont vous disposez, mais un bon point de d\xE9part est de voir quelle est la taille maximale du contexte du mod\xE8le. Cela peut \xEAtre d\xE9duit en inspectant l\u2019attribut "),io=o(ba,"CODE",{});var tf=r(io);Np=n(tf,"model_max_length"),tf.forEach(t),Op=n(ba," du "),uo=o(ba,"EM",{});var af=r(uo);Kp=n(af,"tokenizer"),af.forEach(t),Lp=n(ba," :"),ba.forEach(t),Vr=_(e),x(Ga.$$.fragment,e),Wr=_(e),x(Va.$$.fragment,e),Jr=_(e),xt=o(e,"P",{});var Sl=r(xt);Rp=n(Sl,"Cette valeur est d\xE9riv\xE9e du fichier "),po=o(Sl,"EM",{});var nf=r(po);Bp=n(nf,"tokenizer_config.json"),nf.forEach(t),Fp=n(Sl," associ\xE9 \xE0 un point de contr\xF4le ; dans ce cas, nous pouvons voir que la taille du contexte est de 512 "),mo=o(Sl,"EM",{});var lf=r(mo);Hp=n(lf,"tokens"),lf.forEach(t),Ip=n(Sl,", tout comme avec BERT."),Sl.forEach(t),Yr=_(e),x(na.$$.fragment,e),Qr=_(e),tl=o(e,"P",{});var of=r(tl);Up=n(of,"Ainsi, pour r\xE9aliser nos exp\xE9riences sur des GPU comme ceux de Google Colab, nous choisirons quelque chose d\u2019un peu plus petit qui peut tenir en m\xE9moire :"),of.forEach(t),Xr=_(e),x(Wa.$$.fragment,e),Zr=_(e),x(la.$$.fragment,e),ei=_(e),oa=o(e,"P",{});var vu=r(oa);Gp=n(vu,"Maintenant vient la partie amusante. Pour montrer comment la concat\xE9nation fonctionne, prenons quelques commentaires de notre ensemble d\u2019entra\xEEnement et imprimons le nombre de "),co=o(vu,"EM",{});var rf=r(co);Vp=n(rf,"tokens"),rf.forEach(t),Wp=n(vu," par commentaire :"),vu.forEach(t),si=_(e),x(Ja.$$.fragment,e),ti=_(e),x(Ya.$$.fragment,e),ai=_(e),al=o(e,"P",{});var uf=r(al);Jp=n(uf,"Nous pouvons ensuite concat\xE9ner tous ces exemples avec une simple compr\xE9hension du dictionnaire, comme suit :"),uf.forEach(t),ni=_(e),x(Qa.$$.fragment,e),li=_(e),x(Xa.$$.fragment,e),oi=_(e),Ls=o(e,"P",{});var ka=r(Ls);Yp=n(ka,"Super, la longueur totale est correcte. Donc maintenant, nous allons diviser les exemples concat\xE9n\xE9s en morceaux de la taille donn\xE9e par "),_o=o(ka,"CODE",{});var pf=r(_o);Qp=n(pf,"block_size"),pf.forEach(t),Xp=n(ka,". Pour ce faire, nous it\xE9rons sur les caract\xE9ristiques de "),fo=o(ka,"CODE",{});var df=r(fo);Zp=n(df,"concatenated_examples"),df.forEach(t),ed=n(ka," et utilisons une compr\xE9hension de liste pour cr\xE9er des tranches de chaque caract\xE9ristique. Le r\xE9sultat est un dictionnaire de "),ho=o(ka,"EM",{});var mf=r(ho);sd=n(mf,"chunks"),mf.forEach(t),td=n(ka," pour chaque caract\xE9ristique :"),ka.forEach(t),ri=_(e),x(Za.$$.fragment,e),ii=_(e),x(en.$$.fragment,e),ui=_(e),ra=o(e,"P",{});var bu=r(ra);ad=n(bu,"Comme vous pouvez le voir dans cet exemple, le dernier "),go=o(bu,"EM",{});var cf=r(go);nd=n(cf,"chunk"),cf.forEach(t),ld=n(bu," sera g\xE9n\xE9ralement plus petit que la taille maximale des morceaux. Il y a deux strat\xE9gies principales pour g\xE9rer cela :"),bu.forEach(t),pi=_(e),ia=o(e,"UL",{});var ku=r(ia);sn=o(ku,"LI",{});var qu=r(sn);od=n(qu,"abandonner le dernier morceau s\u2019il est plus petit que "),vo=o(qu,"CODE",{});var _f=r(vo);rd=n(_f,"chunk_size"),_f.forEach(t),id=n(qu,"."),qu.forEach(t),ud=_(ku),tn=o(ku,"LI",{});var $u=r(tn);pd=n($u,"remplir le dernier morceau jusqu\u2019\xE0 ce que sa longueur soit \xE9gale \xE0 "),bo=o($u,"CODE",{});var ff=r(bo);dd=n(ff,"chunk_size"),ff.forEach(t),md=n($u,"."),$u.forEach(t),ku.forEach(t),di=_(e),nl=o(e,"P",{});var hf=r(nl);cd=n(hf,"Nous adopterons la premi\xE8re approche ici, donc nous allons envelopper toute la logique ci-dessus dans une seule fonction que nous pouvons appliquer \xE0 nos jeux de donn\xE9es tokenis\xE9s :"),hf.forEach(t),mi=_(e),x(an.$$.fragment,e),ci=_(e),ns=o(e,"P",{});var Us=r(ns);_d=n(Us,"Notez que dans la derni\xE8re \xE9tape de "),ko=o(Us,"CODE",{});var gf=r(ko);fd=n(gf,"group_texts()"),gf.forEach(t),hd=n(Us," nous cr\xE9ons une nouvelle colonne "),qo=o(Us,"CODE",{});var vf=r(qo);gd=n(vf,"labels"),vf.forEach(t),vd=n(Us," qui est une copie de la colonne "),$o=o(Us,"CODE",{});var bf=r($o);bd=n(bf,"input_ids"),bf.forEach(t),kd=n(Us,". Comme nous le verrons bient\xF4t, c\u2019est parce que dans la mod\xE9lisation du langage masqu\xE9, l\u2019objectif est de pr\xE9dire des "),wo=o(Us,"EM",{});var kf=r(wo);qd=n(kf,"tokens"),kf.forEach(t),$d=n(Us," masqu\xE9s al\xE9atoirement dans le batch d\u2019entr\xE9e, et en cr\xE9ant une colonne "),xo=o(Us,"CODE",{});var qf=r(xo);wd=n(qf,"labels"),qf.forEach(t),xd=n(Us,", nous fournissons la v\xE9rit\xE9 de base pour notre mod\xE8le de langage \xE0 apprendre."),Us.forEach(t),_i=_(e),Et=o(e,"P",{});var Tl=r(Et);Ed=n(Tl,"Appliquons maintenant "),Eo=o(Tl,"CODE",{});var $f=r(Eo);jd=n($f,"group_texts()"),$f.forEach(t),yd=n(Tl," \xE0 nos jeux de donn\xE9es tokenis\xE9s en utilisant notre fid\xE8le fonction "),jo=o(Tl,"CODE",{});var wf=r(jo);zd=n(wf,"Dataset.map()"),wf.forEach(t),Md=n(Tl," :"),Tl.forEach(t),fi=_(e),x(nn.$$.fragment,e),hi=_(e),x(ln.$$.fragment,e),gi=_(e),Ae=o(e,"P",{});var is=r(Ae);Cd=n(is,"Vous pouvez voir que le regroupement puis le d\xE9coupage des textes a produit beaucoup plus d\u2019exemples que nos 25 000 exemples initiaux pour les divisions "),yo=o(is,"CODE",{});var xf=r(yo);Pd=n(xf,"train"),xf.forEach(t),Dd=n(is," et "),zo=o(is,"CODE",{});var Ef=r(zo);Ad=n(Ef,"test"),Ef.forEach(t),Sd=n(is,". C\u2019est parce que nous avons maintenant des exemples impliquant des \u201D"),Mo=o(is,"EM",{});var jf=r(Mo);Td=n(jf,"tokens"),jf.forEach(t),Nd=n(is," contigus\u201D qui s\u2019\xE9tendent sur plusieurs exemples du corpus original. Vous pouvez le voir explicitement en cherchant les "),Co=o(is,"EM",{});var yf=r(Co);Od=n(yf,"tokens"),yf.forEach(t),Kd=n(is," sp\xE9ciaux "),Po=o(is,"CODE",{});var zf=r(Po);Ld=n(zf,"[SEP]"),zf.forEach(t),Rd=n(is," et "),Do=o(is,"CODE",{});var Mf=r(Do);Bd=n(Mf,"[CLS]"),Mf.forEach(t),Fd=n(is," dans l\u2019un des "),Ao=o(is,"EM",{});var Cf=r(Ao);Hd=n(Cf,"chunks"),Cf.forEach(t),Id=n(is," :"),is.forEach(t),vi=_(e),x(on.$$.fragment,e),bi=_(e),x(rn.$$.fragment,e),ki=_(e),ll=o(e,"P",{});var Pf=r(ll);Ud=n(Pf,"Dans cet exemple, vous pouvez voir deux critiques de films qui se chevauchent, l\u2019une sur un film de lyc\xE9e et l\u2019autre sur les sans-abri. Voyons \xE9galement \xE0 quoi ressemblent les \xE9tiquettes pour la mod\xE9lisation du langage masqu\xE9 :"),Pf.forEach(t),qi=_(e),x(un.$$.fragment,e),$i=_(e),x(pn.$$.fragment,e),wi=_(e),Rs=o(e,"P",{});var qa=r(Rs);Gd=n(qa,"Comme pr\xE9vu par notre fonction "),So=o(qa,"CODE",{});var Df=r(So);Vd=n(Df,"group_texts()"),Df.forEach(t),Wd=n(qa," ci-dessus, cela semble identique aux "),To=o(qa,"CODE",{});var Af=r(To);Jd=n(Af,"input_ids"),Af.forEach(t),Yd=n(qa," d\xE9cod\xE9s - mais alors comment notre mod\xE8le peut-il apprendre quoi que ce soit ? Il nous manque une \xE9tape cl\xE9 : ins\xE9rer des "),No=o(qa,"EM",{});var Sf=r(No);Qd=n(Sf,"tokens"),Sf.forEach(t),Xd=n(qa," \xE0 des positions al\xE9atoires dans les entr\xE9es ! Voyons comment nous pouvons le faire \xE0 la vol\xE9e pendant le r\xE9glage fin en utilisant un collateur de donn\xE9es sp\xE9cial."),qa.forEach(t),xi=_(e),Rt=o(e,"H2",{class:!0});var wu=r(Rt);ua=o(wu,"A",{id:!0,class:!0,href:!0});var Tf=r(ua);Oo=o(Tf,"SPAN",{});var Nf=r(Oo);x(dn.$$.fragment,Nf),Nf.forEach(t),Tf.forEach(t),Zd=_(wu),mn=o(wu,"SPAN",{});var xu=r(mn);em=n(xu,"*Finetuning* de DistilBERT avec l'API "),Ko=o(xu,"CODE",{});var Of=r(Ko);sm=n(Of,"Trainer"),Of.forEach(t),tm=n(xu,"."),xu.forEach(t),wu.forEach(t),Ei=_(e),me=o(e,"P",{});var Se=r(me);am=n(Se,"Le "),Lo=o(Se,"EM",{});var Kf=r(Lo);nm=n(Kf,"finetuning"),Kf.forEach(t),lm=n(Se," d\u2019un mod\xE8le de langage masqu\xE9 est presque identique au "),Ro=o(Se,"EM",{});var Lf=r(Ro);om=n(Lf,"finetuning"),Lf.forEach(t),rm=n(Se," d\u2019un mod\xE8le de classification de s\xE9quences, comme nous l\u2019avons fait dans le "),ol=o(Se,"A",{href:!0});var Rf=r(ol);im=n(Rf,"Chapitre 3"),Rf.forEach(t),um=n(Se,". La seule diff\xE9rence est que nous avons besoin d\u2019un collecteur de donn\xE9es sp\xE9cial qui peut masquer de mani\xE8re al\xE9atoire certains des "),Bo=o(Se,"EM",{});var Bf=r(Bo);pm=n(Bf,"tokens"),Bf.forEach(t),dm=n(Se," dans chaque lot de textes. Heureusement, \u{1F917} "),Fo=o(Se,"EM",{});var Ff=r(Fo);mm=n(Ff,"Transformers"),Ff.forEach(t),cm=n(Se," est livr\xE9 pr\xE9par\xE9 avec un "),Ho=o(Se,"CODE",{});var Hf=r(Ho);_m=n(Hf,"DataCollatorForLanguageModeling"),Hf.forEach(t),fm=n(Se," d\xE9di\xE9 \xE0 cette t\xE2che. Nous devons juste lui passer le "),Io=o(Se,"EM",{});var If=r(Io);hm=n(If,"tokenizer"),If.forEach(t),gm=n(Se," et un argument "),Uo=o(Se,"CODE",{});var Uf=r(Uo);vm=n(Uf,"mlm_probability"),Uf.forEach(t),bm=n(Se," qui sp\xE9cifie quelle fraction des "),Go=o(Se,"EM",{});var Gf=r(Go);km=n(Gf,"tokens"),Gf.forEach(t),qm=n(Se," \xE0 masquer. Nous choisirons 15%, qui est la quantit\xE9 utilis\xE9e pour BERT et un choix commun dans la litt\xE9rature :"),Se.forEach(t),ji=_(e),x(cn.$$.fragment,e),yi=_(e),Bs=o(e,"P",{});var $a=r(Bs);$m=n($a,"Pour voir comment le masquage al\xE9atoire fonctionne, nous allons donner quelques exemples au compilateur de donn\xE9es. Puisqu\u2019il s\u2019attend \xE0 une liste de "),Vo=o($a,"CODE",{});var Vf=r(Vo);wm=n(Vf,"dict"),Vf.forEach(t),xm=n($a,"s, o\xF9 chaque "),Wo=o($a,"CODE",{});var Wf=r(Wo);Em=n(Wf,"dict"),Wf.forEach(t),jm=n($a," repr\xE9sente un seul morceau de texte contigu, nous it\xE9rons d\u2019abord sur le jeu de donn\xE9es avant de nourrir le lot au collateur. Nous supprimons la cl\xE9 "),Jo=o($a,"CODE",{});var Jf=r(Jo);ym=n(Jf,'"word_ids"'),Jf.forEach(t),zm=n($a," pour ce collateur de donn\xE9es car il ne l\u2019attend pas :"),$a.forEach(t),zi=_(e),x(_n.$$.fragment,e),Mi=_(e),x(fn.$$.fragment,e),Ci=_(e),ks=o(e,"P",{});var zt=r(ks);Mm=n(zt,"Super, \xE7a a march\xE9 ! Nous pouvons voir que le "),Yo=o(zt,"EM",{});var Yf=r(Yo);Cm=n(Yf,"token"),Yf.forEach(t),Pm=_(zt),Qo=o(zt,"CODE",{});var Qf=r(Qo);Dm=n(Qf,"[MASK]"),Qf.forEach(t),Am=n(zt," a \xE9t\xE9 ins\xE9r\xE9 de fa\xE7on al\xE9atoire \xE0 diff\xE9rents endroits dans notre texte. Ce seront les "),Xo=o(zt,"EM",{});var Xf=r(Xo);Sm=n(Xf,"tokens"),Xf.forEach(t),Tm=n(zt," que notre mod\xE8le devra pr\xE9dire pendant l\u2019entra\xEEnement. Et la beaut\xE9 du collecteur de donn\xE9es est qu\u2019il va rendre al\xE9atoire l\u2019insertion du "),Zo=o(zt,"CODE",{});var Zf=r(Zo);Nm=n(Zf,"[MASK]"),Zf.forEach(t),Om=n(zt," \xE0 chaque lot !"),zt.forEach(t),Pi=_(e),x(pa.$$.fragment,e),Di=_(e),cs&&cs.l(e),rl=_(e),qs=o(e,"P",{});var Mt=r(qs);Km=n(Mt,"Lors de l\u2019entra\xEEnement des mod\xE8les pour la mod\xE9lisation du langage masqu\xE9, une technique qui peut \xEAtre utilis\xE9e est de masquer des mots entiers ensemble, et pas seulement des "),er=o(Mt,"EM",{});var eh=r(er);Lm=n(eh,"tokens"),eh.forEach(t),Rm=n(Mt," individuels. Cette approche est appel\xE9e "),sr=o(Mt,"EM",{});var sh=r(sr);Bm=n(sh,"masquage de mots entiers"),sh.forEach(t),Fm=n(Mt,". Si nous voulons utiliser le masquage de mots entiers, nous devons construire nous-m\xEAmes un collateur de donn\xE9es. Un collateur de donn\xE9es est simplement une fonction qui prend une liste d\u2019\xE9chantillons et les convertit en un lot, alors faisons-le maintenant ! Nous utiliserons les IDs des mots calcul\xE9s plus t\xF4t pour faire une correspondance entre les indices des mots et les "),tr=o(Mt,"EM",{});var th=r(tr);Hm=n(th,"tokens"),th.forEach(t),Im=n(Mt," correspondants, puis nous d\xE9ciderons al\xE9atoirement quels mots masquer et appliquerons ce masque sur les entr\xE9es. Notez que les \xE9tiquettes sont toutes "),ar=o(Mt,"CODE",{});var ah=r(ar);Um=n(ah,"-100"),ah.forEach(t),Gm=n(Mt," sauf celles qui correspondent aux mots masqu\xE9s."),Mt.forEach(t),Ai=_(e),at.l(e),il=_(e),ul=o(e,"P",{});var nh=r(ul);Vm=n(nh,"Ensuite, nous pouvons l\u2019essayer sur les m\xEAmes \xE9chantillons que pr\xE9c\xE9demment :"),nh.forEach(t),Si=_(e),x(hn.$$.fragment,e),Ti=_(e),x(gn.$$.fragment,e),Ni=_(e),x(da.$$.fragment,e),Oi=_(e),Fs=o(e,"P",{});var wa=r(Fs);Wm=n(wa,"Maintenant que nous avons deux collateurs de donn\xE9es, le reste des \xE9tapes de mise au point est standard. L\u2019entra\xEEnement peut prendre un certain temps sur Google Colab si vous n\u2019avez pas la chance d\u2019avoir un mythique GPU P100 \u{1F62D}, donc nous allons d\u2019abord r\xE9duire la taille de l\u2019ensemble d\u2019entra\xEEnement \xE0 quelques milliers d\u2019exemples. Ne vous inqui\xE9tez pas, nous obtiendrons quand m\xEAme un mod\xE8le de langage assez d\xE9cent ! Un moyen rapide de r\xE9duire la taille d\u2019un jeu de donn\xE9es dans \u{1F917} "),nr=o(wa,"EM",{});var lh=r(nr);Jm=n(lh,"Datasets"),lh.forEach(t),Ym=n(wa," est la fonction "),lr=o(wa,"CODE",{});var oh=r(lr);Qm=n(oh,"Dataset.train_test_split()"),oh.forEach(t),Xm=n(wa," que nous avons vue au "),pl=o(wa,"A",{href:!0});var rh=r(pl);Zm=n(rh,"Chapitre 5"),rh.forEach(t),ec=n(wa," :"),wa.forEach(t),Ki=_(e),x(vn.$$.fragment,e),Li=_(e),x(bn.$$.fragment,e),Ri=_(e),$s=o(e,"P",{});var Ct=r($s);sc=n(Ct,"Cela a automatiquement cr\xE9\xE9 de nouvelles divisions "),or=o(Ct,"CODE",{});var ih=r(or);tc=n(ih,"train"),ih.forEach(t),ac=n(Ct," et "),rr=o(Ct,"CODE",{});var uh=r(rr);nc=n(uh,"test"),uh.forEach(t),lc=n(Ct,", avec la taille de l\u2019ensemble d\u2019entra\xEEnement fix\xE9e \xE0 10.000 exemples et la validation fix\xE9e \xE0 10% de cela. N\u2019h\xE9sitez pas \xE0 augmenter cela si vous avez un GPU puissant ! La prochaine chose que nous devons faire est de nous connecter au "),ir=o(Ct,"EM",{});var ph=r(ir);oc=n(ph,"Hub"),ph.forEach(t),rc=n(Ct,". Si vous ex\xE9cutez ce code dans un "),ur=o(Ct,"EM",{});var dh=r(ur);ic=n(dh,"notebook"),dh.forEach(t),uc=n(Ct,", vous pouvez le faire avec la fonction utilitaire suivante :"),Ct.forEach(t),Bi=_(e),x(kn.$$.fragment,e),Fi=_(e),ma=o(e,"P",{});var Eu=r(ma);pc=n(Eu,"qui affichera un "),pr=o(Eu,"EM",{});var mh=r(pr);dc=n(mh,"widget"),mh.forEach(t),mc=n(Eu," o\xF9 vous pourrez saisir vos informations d\u2019identification. Alternativement, vous pouvez ex\xE9cuter :"),Eu.forEach(t),Hi=_(e),x(qn.$$.fragment,e),Ii=_(e),dl=o(e,"P",{});var ch=r(dl);cc=n(ch,"dans votre terminal pr\xE9f\xE9r\xE9 et connectez-vous l\xE0."),ch.forEach(t),Ui=_(e),lt.l(e),ml=_(e),Bt=o(e,"H3",{class:!0});var ju=r(Bt);ca=o(ju,"A",{id:!0,class:!0,href:!0});var _h=r(ca);dr=o(_h,"SPAN",{});var fh=r(dr);x($n.$$.fragment,fh),fh.forEach(t),_h.forEach(t),_c=_(ju),mr=o(ju,"SPAN",{});var hh=r(mr);fc=n(hh,"Perplexit\xE9 pour les mod\xE8les de langage"),hh.forEach(t),ju.forEach(t),Gi=_(e),x(wn.$$.fragment,e),Vi=_(e),cl=o(e,"P",{});var gh=r(cl);hc=n(gh,"Contrairement \xE0 d\u2019autres t\xE2ches, comme la classification de textes ou la r\xE9ponse \xE0 des questions, sur lesquelles nous disposons d\u2019un corpus \xE9tiquet\xE9 pour nous entra\xEEner, la mod\xE9lisation du langage ne s\u2019appuie sur aucune \xE9tiquette explicite. Alors comment d\xE9terminer ce qui fait un bon mod\xE8le de langage ? Comme pour la fonction de correction automatique de votre t\xE9l\xE9phone, un bon mod\xE8le de langage est celui qui attribue des probabilit\xE9s \xE9lev\xE9es aux phrases grammaticalement correctes et des probabilit\xE9s faibles aux phrases absurdes. Pour vous donner une meilleure id\xE9e de ce \xE0 quoi cela ressemble, vous pouvez trouver en ligne des s\xE9ries enti\xE8res de \u201Crat\xE9s d\u2019autocorrection\u201D, o\xF9 le mod\xE8le du t\xE9l\xE9phone d\u2019une personne a produit des compl\xE9ments plut\xF4t amusants (et souvent inappropri\xE9s) !"),gh.forEach(t),Wi=_(e),rt.l(e),_l=_(e),x(xn.$$.fragment,e),Ji=_(e),fl=o(e,"P",{});var vh=r(fl);gc=n(vh,"Un score de perplexit\xE9 plus faible signifie un meilleur mod\xE8le de langue, et nous pouvons voir ici que notre mod\xE8le de d\xE9part a une valeur assez \xE9lev\xE9e. Voyons si nous pouvons la r\xE9duire en l\u2019affinant ! Pour ce faire, nous commen\xE7ons par ex\xE9cuter la boucle d\u2019entra\xEEnement :"),vh.forEach(t),Yi=_(e),ut.l(e),hl=_(e),gl=o(e,"P",{});var bh=r(gl);vc=n(bh,"et ensuite calculer la perplexit\xE9 r\xE9sultante sur l\u2019ensemble de test comme pr\xE9c\xE9demment :"),bh.forEach(t),Qi=_(e),dt.l(e),vl=_(e),x(En.$$.fragment,e),Xi=_(e),bl=o(e,"P",{});var kh=r(bl);bc=n(kh,"Joli. C\u2019est une r\xE9duction consid\xE9rable de la perplexit\xE9, ce qui nous indique que le mod\xE8le a appris quelque chose sur le domaine des critiques de films !"),kh.forEach(t),Zi=_(e),Le&&Le.l(e),kl=_(e),x(_a.$$.fragment,e),eu=_(e),Re&&Re.l(e),ql=_(e),Ft=o(e,"H3",{class:!0});var yu=r(Ft);fa=o(yu,"A",{id:!0,class:!0,href:!0});var qh=r(fa);cr=o(qh,"SPAN",{});var $h=r(cr);x(jn.$$.fragment,$h),$h.forEach(t),qh.forEach(t),kc=_(yu),_r=o(yu,"SPAN",{});var wh=r(_r);qc=n(wh,"Utilisation de notre mod\xE8le *finetun\xE9*"),wh.forEach(t),yu.forEach(t),su=_(e),ls=o(e,"P",{});var Gs=r(ls);$c=n(Gs,"Vous pouvez interagir avec votre mod\xE8le affin\xE9 soit en utilisant son "),fr=o(Gs,"EM",{});var xh=r(fr);wc=n(xh,"widget"),xh.forEach(t),xc=n(Gs," sur le "),hr=o(Gs,"EM",{});var Eh=r(hr);Ec=n(Eh,"Hub"),Eh.forEach(t),jc=n(Gs,", soit localement avec le "),gr=o(Gs,"CODE",{});var jh=r(gr);yc=n(jh,"pipeline"),jh.forEach(t),zc=n(Gs," de \u{1F917} "),vr=o(Gs,"EM",{});var yh=r(vr);Mc=n(yh,"Transformers"),yh.forEach(t),Cc=n(Gs,". Utilisons cette derni\xE8re pour t\xE9l\xE9charger notre mod\xE8le en utilisant le pipeline "),br=o(Gs,"CODE",{});var zh=r(br);Pc=n(zh,"fill-mask"),zh.forEach(t),Dc=n(Gs," :"),Gs.forEach(t),tu=_(e),x(yn.$$.fragment,e),au=_(e),$l=o(e,"P",{});var Mh=r($l);Ac=n(Mh,"Nous pouvons ensuite alimenter le pipeline avec notre exemple de texte \u201CC\u2019est un grand [MASK]\u201D et voir quelles sont les 5 premi\xE8res pr\xE9dictions :"),Mh.forEach(t),nu=_(e),x(zn.$$.fragment,e),lu=_(e),x(Mn.$$.fragment,e),ou=_(e),wl=o(e,"P",{});var Ch=r(wl);Sc=n(Ch,"Notre mod\xE8le a clairement adapt\xE9 ses pond\xE9rations pour pr\xE9dire les mots qui sont plus fortement associ\xE9s aux films !"),Ch.forEach(t),ru=_(e),x(Cn.$$.fragment,e),iu=_(e),jt=o(e,"P",{});var Nl=r(jt);Tc=n(Nl,"Ceci conclut notre premi\xE8re exp\xE9rience d\u2019entra\xEEnement d\u2019un mod\xE8le de langage. Dans la "),xl=o(Nl,"A",{href:!0});var Ph=r(xl);Nc=n(Ph,"section 6"),Ph.forEach(t),Oc=n(Nl,", vous apprendrez comment entra\xEEner un mod\xE8le autor\xE9gressif comme GPT-2 \xE0 partir de z\xE9ro ; allez-y si vous voulez voir comment vous pouvez pr\xE9-entra\xEEner votre propre "),kr=o(Nl,"EM",{});var Dh=r(kr);Kc=n(Dh,"transformer"),Dh.forEach(t),Lc=n(Nl," !"),Nl.forEach(t),uu=_(e),x(ha.$$.fragment,e),this.h()},h(){y(i,"name","hf:doc:metadata"),y(i,"content",JSON.stringify(_g)),y(b,"id","finetuner-un-modle-de-langage-masqu"),y(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(b,"href","#finetuner-un-modle-de-langage-masqu"),y(P,"class","relative group"),y(ye,"href","https://arxiv.org/abs/1801.06146"),y(ye,"rel","nofollow"),y(Ne,"class","block dark:hidden"),yr(Ne.src,Ve="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg")||y(Ne,"src",Ve),y(Ne,"alt","ULMFiT."),y(Ce,"class","hidden dark:block"),yr(Ce.src,he="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg")||y(Ce,"src",he),y(Ce,"alt","ULMFiT."),y(fe,"class","flex justify-center"),y(ke,"href","https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D."),y(ke,"rel","nofollow"),yr(J.src,te="https://hf.space/gradioiframe/course-demos/distilbert-base-uncased-finetuned-imdb/+")||y(J,"src",te),y(J,"frameborder","0"),y(J,"height","300"),y(J,"title","Gradio app"),y(J,"class","block dark:hidden container p-0 flex-grow space-iframe"),y(J,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),y(J,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),yr(ae.src,Ms="https://hf.space/gradioiframe/course-demos/distilbert-base-uncased-finetuned-imdb-darkmode/+")||y(ae,"src",Ms),y(ae,"frameborder","0"),y(ae,"height","300"),y(ae,"title","Gradio app"),y(ae,"class","hidden dark:block container p-0 flex-grow space-iframe"),y(ae,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),y(ae,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),y(Ke,"id","choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu"),y(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ke,"href","#choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu"),y(Fe,"class","relative group"),y(js,"href","https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads"),y(js,"rel","nofollow"),yr(Ys.src,Ma="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png")||y(Ys,"src",Ma),y(Ys,"alt","Hub models."),y(Ys,"width","80%"),y(Js,"class","flex justify-center"),y(As,"href","https://huggingface.co/distilbert-base-uncased"),y(As,"rel","nofollow"),y(ys,"href","https://en.wikipedia.org/wiki/Knowledge_distillation"),y(ys,"rel","nofollow"),y(Xs,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/ch05.html"),y(Xs,"rel","nofollow"),y(gt,"href","https://huggingface.co/datasets/wikipedia"),y(gt,"rel","nofollow"),y(vt,"href","https://huggingface.co/datasets/bookcorpus"),y(vt,"rel","nofollow"),y(Zt,"id","le-jeu-de-donnes"),y(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Zt,"href","#le-jeu-de-donnes"),y(Kt,"class","relative group"),y(Oa,"href","https://huggingface.co/datasets/imdb"),y(Oa,"rel","nofollow"),y(el,"href","/course/fr/chapter3"),y(ta,"id","prtraitement-des-donnes"),y(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(ta,"href","#prtraitement-des-donnes"),y(Lt,"class","relative group"),y(sl,"href","/course/fr/chapter6/3"),y(ua,"id","finetuning-de-distilbert-avec-lapi-trainer"),y(ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(ua,"href","#finetuning-de-distilbert-avec-lapi-trainer"),y(Rt,"class","relative group"),y(ol,"href","/course/fr/chapter3"),y(pl,"href","/course/fr/chapter5"),y(ca,"id","perplexit-pour-les-modles-de-langage"),y(ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(ca,"href","#perplexit-pour-les-modles-de-langage"),y(Bt,"class","relative group"),y(fa,"id","utilisation-de-notre-modle-finetun"),y(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(fa,"href","#utilisation-de-notre-modle-finetun"),y(Ft,"class","relative group"),y(xl,"href","/course/fr/chapter7/section6")},m(e,u){s(document.head,i),p(e,h,u),E(d,e,u),p(e,$,u),p(e,P,u),s(P,b),s(b,k),E(M,k,null),s(P,f),s(P,C),s(C,O),p(e,K,u),Pn[L].m(e,u),p(e,S,u),p(e,A,u),s(A,H),s(A,U),s(U,q),s(A,T),s(A,Y),s(Y,Z),s(A,Q),s(A,G),s(G,W),s(A,ne),p(e,V,u),p(e,I,u),s(I,re),s(I,X),s(X,qe),s(I,le),s(I,Ee),s(Ee,Be),s(I,ie),p(e,$e,u),p(e,ue,u),s(ue,je),s(ue,ce),s(ce,Xe),s(ue,ve),s(ue,Te),s(Te,Ue),s(ue,Ge),s(ue,ye),s(ye,_e),s(ue,ze),s(ue,de),s(de,pe),s(ue,be),p(e,Me,u),p(e,fe,u),s(fe,Ne),s(fe,we),s(fe,Ce),p(e,We,u),p(e,N,u),s(N,se),s(N,ke),s(ke,oe),s(N,Oe),s(N,Ze),s(Ze,Je),s(N,R),p(e,ee,u),p(e,J,u),p(e,us,u),p(e,ae,u),p(e,es,u),p(e,ss,u),s(ss,Ye),p(e,ge,u),E(xe,e,u),p(e,Cs,u),E(Pe,e,u),p(e,Vs,u),p(e,Fe,u),s(Fe,Ke),s(Ke,Ps),E(ps,Ps,null),s(Fe,He),s(Fe,xs),s(xs,Es),p(e,It,u),p(e,ds,u),s(ds,Ws),s(ds,mt),s(mt,_s),s(ds,ja),s(ds,js),s(js,ya),s(ds,za),p(e,ct,u),p(e,Js,u),s(Js,Ys),p(e,Ds,u),p(e,Qe,u),s(Qe,Qs),s(Qe,As),s(As,fs),s(Qe,Ca),s(Qe,ys),s(ys,At),s(At,Pa),s(Qe,St),s(Qe,Xs),s(Xs,Tt),s(Tt,Ut),s(Qe,Zs),p(e,Nt,u),Dn[Ie].m(e,u),p(e,et,u),p(e,ts,u),s(ts,Gt),s(ts,zs),s(zs,Vt),s(ts,_t),p(e,Wt,u),E(Ss,e,u),p(e,m,u),p(e,z,u),s(z,Bn),s(z,ft),s(ft,Fn),s(z,Hn),s(z,Jt),s(Jt,ht),s(z,In),s(z,gt),s(gt,hs),s(z,Un),s(z,vt),s(vt,Gn),s(z,Vn),s(z,bt),s(bt,Wn),s(z,Ts),s(z,Yt),s(Yt,Jn),s(z,Yn),s(z,kt),s(kt,Qn),s(z,Qt),p(e,Da,u),E(Ns,e,u),p(e,Aa,u),p(e,st,u),s(st,qt),s(st,Xt),s(Xt,Xn),s(st,$t),p(e,Sa,u),An[gs].m(e,u),p(e,Ot,u),E(Ta,e,u),p(e,zr,u),p(e,Zn,u),s(Zn,Mu),p(e,Mr,u),p(e,Kt,u),s(Kt,Zt),s(Zt,Ol),E(Na,Ol,null),s(Kt,Cu),s(Kt,Kl),s(Kl,Pu),p(e,Cr,u),p(e,bs,u),s(bs,Du),s(bs,Oa),s(Oa,Au),s(bs,Su),s(bs,Ll),s(Ll,Tu),s(bs,Nu),s(bs,Rl),s(Rl,Ou),s(bs,Ku),s(bs,Bl),s(Bl,Lu),s(bs,Ru),p(e,Pr,u),E(Ka,e,u),p(e,Dr,u),E(La,e,u),p(e,Ar,u),p(e,as,u),s(as,Bu),s(as,Fl),s(Fl,Fu),s(as,Hu),s(as,Hl),s(Hl,Iu),s(as,Uu),s(as,Il),s(Il,Gu),s(as,Vu),s(as,Ul),s(Ul,Wu),s(as,Ju),s(as,Gl),s(Gl,Yu),s(as,Qu),p(e,Sr,u),E(Ra,e,u),p(e,Tr,u),E(Ba,e,u),p(e,Nr,u),p(e,wt,u),s(wt,Xu),s(wt,Vl),s(Vl,Zu),s(wt,ep),s(wt,Wl),s(Wl,sp),s(wt,tp),p(e,Or,u),E(ea,e,u),p(e,Kr,u),p(e,sa,u),s(sa,ap),s(sa,el),s(el,np),s(sa,lp),p(e,Lr,u),p(e,Lt,u),s(Lt,ta),s(ta,Jl),E(Fa,Jl,null),s(Lt,op),s(Lt,Yl),s(Yl,rp),p(e,Rr,u),E(Ha,e,u),p(e,Br,u),p(e,aa,u),s(aa,ip),s(aa,Ql),s(Ql,up),s(aa,pp),p(e,Fr,u),p(e,De,u),s(De,dp),s(De,Xl),s(Xl,mp),s(De,cp),s(De,Zl),s(Zl,_p),s(De,fp),s(De,eo),s(eo,hp),s(De,gp),s(De,so),s(so,vp),s(De,bp),s(De,sl),s(sl,kp),s(De,qp),s(De,to),s(to,$p),s(De,wp),s(De,ao),s(ao,xp),s(De,Ep),p(e,Hr,u),E(Ia,e,u),p(e,Ir,u),E(Ua,e,u),p(e,Ur,u),p(e,Os,u),s(Os,jp),s(Os,no),s(no,yp),s(Os,zp),s(Os,lo),s(lo,Mp),s(Os,Cp),s(Os,oo),s(oo,Pp),s(Os,Dp),p(e,Gr,u),p(e,Ks,u),s(Ks,Ap),s(Ks,ro),s(ro,Sp),s(Ks,Tp),s(Ks,io),s(io,Np),s(Ks,Op),s(Ks,uo),s(uo,Kp),s(Ks,Lp),p(e,Vr,u),E(Ga,e,u),p(e,Wr,u),E(Va,e,u),p(e,Jr,u),p(e,xt,u),s(xt,Rp),s(xt,po),s(po,Bp),s(xt,Fp),s(xt,mo),s(mo,Hp),s(xt,Ip),p(e,Yr,u),E(na,e,u),p(e,Qr,u),p(e,tl,u),s(tl,Up),p(e,Xr,u),E(Wa,e,u),p(e,Zr,u),E(la,e,u),p(e,ei,u),p(e,oa,u),s(oa,Gp),s(oa,co),s(co,Vp),s(oa,Wp),p(e,si,u),E(Ja,e,u),p(e,ti,u),E(Ya,e,u),p(e,ai,u),p(e,al,u),s(al,Jp),p(e,ni,u),E(Qa,e,u),p(e,li,u),E(Xa,e,u),p(e,oi,u),p(e,Ls,u),s(Ls,Yp),s(Ls,_o),s(_o,Qp),s(Ls,Xp),s(Ls,fo),s(fo,Zp),s(Ls,ed),s(Ls,ho),s(ho,sd),s(Ls,td),p(e,ri,u),E(Za,e,u),p(e,ii,u),E(en,e,u),p(e,ui,u),p(e,ra,u),s(ra,ad),s(ra,go),s(go,nd),s(ra,ld),p(e,pi,u),p(e,ia,u),s(ia,sn),s(sn,od),s(sn,vo),s(vo,rd),s(sn,id),s(ia,ud),s(ia,tn),s(tn,pd),s(tn,bo),s(bo,dd),s(tn,md),p(e,di,u),p(e,nl,u),s(nl,cd),p(e,mi,u),E(an,e,u),p(e,ci,u),p(e,ns,u),s(ns,_d),s(ns,ko),s(ko,fd),s(ns,hd),s(ns,qo),s(qo,gd),s(ns,vd),s(ns,$o),s($o,bd),s(ns,kd),s(ns,wo),s(wo,qd),s(ns,$d),s(ns,xo),s(xo,wd),s(ns,xd),p(e,_i,u),p(e,Et,u),s(Et,Ed),s(Et,Eo),s(Eo,jd),s(Et,yd),s(Et,jo),s(jo,zd),s(Et,Md),p(e,fi,u),E(nn,e,u),p(e,hi,u),E(ln,e,u),p(e,gi,u),p(e,Ae,u),s(Ae,Cd),s(Ae,yo),s(yo,Pd),s(Ae,Dd),s(Ae,zo),s(zo,Ad),s(Ae,Sd),s(Ae,Mo),s(Mo,Td),s(Ae,Nd),s(Ae,Co),s(Co,Od),s(Ae,Kd),s(Ae,Po),s(Po,Ld),s(Ae,Rd),s(Ae,Do),s(Do,Bd),s(Ae,Fd),s(Ae,Ao),s(Ao,Hd),s(Ae,Id),p(e,vi,u),E(on,e,u),p(e,bi,u),E(rn,e,u),p(e,ki,u),p(e,ll,u),s(ll,Ud),p(e,qi,u),E(un,e,u),p(e,$i,u),E(pn,e,u),p(e,wi,u),p(e,Rs,u),s(Rs,Gd),s(Rs,So),s(So,Vd),s(Rs,Wd),s(Rs,To),s(To,Jd),s(Rs,Yd),s(Rs,No),s(No,Qd),s(Rs,Xd),p(e,xi,u),p(e,Rt,u),s(Rt,ua),s(ua,Oo),E(dn,Oo,null),s(Rt,Zd),s(Rt,mn),s(mn,em),s(mn,Ko),s(Ko,sm),s(mn,tm),p(e,Ei,u),p(e,me,u),s(me,am),s(me,Lo),s(Lo,nm),s(me,lm),s(me,Ro),s(Ro,om),s(me,rm),s(me,ol),s(ol,im),s(me,um),s(me,Bo),s(Bo,pm),s(me,dm),s(me,Fo),s(Fo,mm),s(me,cm),s(me,Ho),s(Ho,_m),s(me,fm),s(me,Io),s(Io,hm),s(me,gm),s(me,Uo),s(Uo,vm),s(me,bm),s(me,Go),s(Go,km),s(me,qm),p(e,ji,u),E(cn,e,u),p(e,yi,u),p(e,Bs,u),s(Bs,$m),s(Bs,Vo),s(Vo,wm),s(Bs,xm),s(Bs,Wo),s(Wo,Em),s(Bs,jm),s(Bs,Jo),s(Jo,ym),s(Bs,zm),p(e,zi,u),E(_n,e,u),p(e,Mi,u),E(fn,e,u),p(e,Ci,u),p(e,ks,u),s(ks,Mm),s(ks,Yo),s(Yo,Cm),s(ks,Pm),s(ks,Qo),s(Qo,Dm),s(ks,Am),s(ks,Xo),s(Xo,Sm),s(ks,Tm),s(ks,Zo),s(Zo,Nm),s(ks,Om),p(e,Pi,u),E(pa,e,u),p(e,Di,u),cs&&cs.m(e,u),p(e,rl,u),p(e,qs,u),s(qs,Km),s(qs,er),s(er,Lm),s(qs,Rm),s(qs,sr),s(sr,Bm),s(qs,Fm),s(qs,tr),s(tr,Hm),s(qs,Im),s(qs,ar),s(ar,Um),s(qs,Gm),p(e,Ai,u),Sn[tt].m(e,u),p(e,il,u),p(e,ul,u),s(ul,Vm),p(e,Si,u),E(hn,e,u),p(e,Ti,u),E(gn,e,u),p(e,Ni,u),E(da,e,u),p(e,Oi,u),p(e,Fs,u),s(Fs,Wm),s(Fs,nr),s(nr,Jm),s(Fs,Ym),s(Fs,lr),s(lr,Qm),s(Fs,Xm),s(Fs,pl),s(pl,Zm),s(Fs,ec),p(e,Ki,u),E(vn,e,u),p(e,Li,u),E(bn,e,u),p(e,Ri,u),p(e,$s,u),s($s,sc),s($s,or),s(or,tc),s($s,ac),s($s,rr),s(rr,nc),s($s,lc),s($s,ir),s(ir,oc),s($s,rc),s($s,ur),s(ur,ic),s($s,uc),p(e,Bi,u),E(kn,e,u),p(e,Fi,u),p(e,ma,u),s(ma,pc),s(ma,pr),s(pr,dc),s(ma,mc),p(e,Hi,u),E(qn,e,u),p(e,Ii,u),p(e,dl,u),s(dl,cc),p(e,Ui,u),Tn[nt].m(e,u),p(e,ml,u),p(e,Bt,u),s(Bt,ca),s(ca,dr),E($n,dr,null),s(Bt,_c),s(Bt,mr),s(mr,fc),p(e,Gi,u),E(wn,e,u),p(e,Vi,u),p(e,cl,u),s(cl,hc),p(e,Wi,u),Nn[ot].m(e,u),p(e,_l,u),E(xn,e,u),p(e,Ji,u),p(e,fl,u),s(fl,gc),p(e,Yi,u),On[it].m(e,u),p(e,hl,u),p(e,gl,u),s(gl,vc),p(e,Qi,u),Kn[pt].m(e,u),p(e,vl,u),E(En,e,u),p(e,Xi,u),p(e,bl,u),s(bl,bc),p(e,Zi,u),Le&&Le.m(e,u),p(e,kl,u),E(_a,e,u),p(e,eu,u),Re&&Re.m(e,u),p(e,ql,u),p(e,Ft,u),s(Ft,fa),s(fa,cr),E(jn,cr,null),s(Ft,kc),s(Ft,_r),s(_r,qc),p(e,su,u),p(e,ls,u),s(ls,$c),s(ls,fr),s(fr,wc),s(ls,xc),s(ls,hr),s(hr,Ec),s(ls,jc),s(ls,gr),s(gr,yc),s(ls,zc),s(ls,vr),s(vr,Mc),s(ls,Cc),s(ls,br),s(br,Pc),s(ls,Dc),p(e,tu,u),E(yn,e,u),p(e,au,u),p(e,$l,u),s($l,Ac),p(e,nu,u),E(zn,e,u),p(e,lu,u),E(Mn,e,u),p(e,ou,u),p(e,wl,u),s(wl,Sc),p(e,ru,u),E(Cn,e,u),p(e,iu,u),p(e,jt,u),s(jt,Tc),s(jt,xl),s(xl,Nc),s(jt,Oc),s(jt,kr),s(kr,Kc),s(jt,Lc),p(e,uu,u),E(ha,e,u),pu=!0},p(e,[u]){const Ln={};u&1&&(Ln.fw=e[0]),d.$set(Ln);let El=L;L=Bc(e),L!==El&&(Dt(),v(Pn[El],1,1,()=>{Pn[El]=null}),Pt(),F=Pn[L],F||(F=Pn[L]=Rc[L](e),F.c()),g(F,1),F.m(S.parentNode,S));const qr={};u&2&&(qr.$$scope={dirty:u,ctx:e}),Pe.$set(qr);let jl=Ie;Ie=Hc(e),Ie!==jl&&(Dt(),v(Dn[jl],1,1,()=>{Dn[jl]=null}),Pt(),ms=Dn[Ie],ms||(ms=Dn[Ie]=Fc[Ie](e),ms.c()),g(ms,1),ms.m(et.parentNode,et));let Hs=gs;gs=Uc(e),gs!==Hs&&(Dt(),v(An[Hs],1,1,()=>{An[Hs]=null}),Pt(),vs=An[gs],vs||(vs=An[gs]=Ic[gs](e),vs.c()),g(vs,1),vs.m(Ot.parentNode,Ot));const $r={};u&2&&($r.$$scope={dirty:u,ctx:e}),ea.$set($r);const wr={};u&2&&(wr.$$scope={dirty:u,ctx:e}),na.$set(wr);const xr={};u&2&&(xr.$$scope={dirty:u,ctx:e}),la.$set(xr);const Ht={};u&2&&(Ht.$$scope={dirty:u,ctx:e}),pa.$set(Ht),e[0]==="pt"?cs||(cs=Ah(),cs.c(),cs.m(rl.parentNode,rl)):cs&&(cs.d(1),cs=null);let yl=tt;tt=Vc(e),tt!==yl&&(Dt(),v(Sn[yl],1,1,()=>{Sn[yl]=null}),Pt(),at=Sn[tt],at||(at=Sn[tt]=Gc[tt](e),at.c()),g(at,1),at.m(il.parentNode,il));const Er={};u&2&&(Er.$$scope={dirty:u,ctx:e}),da.$set(Er);let ws=nt;nt=Jc(e),nt!==ws&&(Dt(),v(Tn[ws],1,1,()=>{Tn[ws]=null}),Pt(),lt=Tn[nt],lt||(lt=Tn[nt]=Wc[nt](e),lt.c()),g(lt,1),lt.m(ml.parentNode,ml));let zl=ot;ot=Qc(e),ot!==zl&&(Dt(),v(Nn[zl],1,1,()=>{Nn[zl]=null}),Pt(),rt=Nn[ot],rt||(rt=Nn[ot]=Yc[ot](e),rt.c()),g(rt,1),rt.m(_l.parentNode,_l));let Ml=it;it=Zc(e),it!==Ml&&(Dt(),v(On[Ml],1,1,()=>{On[Ml]=null}),Pt(),ut=On[it],ut||(ut=On[it]=Xc[it](e),ut.c()),g(ut,1),ut.m(hl.parentNode,hl));let Cl=pt;pt=s_(e),pt!==Cl&&(Dt(),v(Kn[Cl],1,1,()=>{Kn[Cl]=null}),Pt(),dt=Kn[pt],dt||(dt=Kn[pt]=e_[pt](e),dt.c()),g(dt,1),dt.m(vl.parentNode,vl)),e[0]==="pt"?Le?u&1&&g(Le,1):(Le=Sh(),Le.c(),g(Le,1),Le.m(kl.parentNode,kl)):Le&&(Dt(),v(Le,1,1,()=>{Le=null}),Pt());const jr={};u&2&&(jr.$$scope={dirty:u,ctx:e}),_a.$set(jr),e[0]==="pt"?Re?u&1&&g(Re,1):(Re=Th(),Re.c(),g(Re,1),Re.m(ql.parentNode,ql)):Re&&(Dt(),v(Re,1,1,()=>{Re=null}),Pt());const Rn={};u&2&&(Rn.$$scope={dirty:u,ctx:e}),ha.$set(Rn)},i(e){pu||(g(d.$$.fragment,e),g(M.$$.fragment,e),g(F),g(xe.$$.fragment,e),g(Pe.$$.fragment,e),g(ps.$$.fragment,e),g(ms),g(Ss.$$.fragment,e),g(Ns.$$.fragment,e),g(vs),g(Ta.$$.fragment,e),g(Na.$$.fragment,e),g(Ka.$$.fragment,e),g(La.$$.fragment,e),g(Ra.$$.fragment,e),g(Ba.$$.fragment,e),g(ea.$$.fragment,e),g(Fa.$$.fragment,e),g(Ha.$$.fragment,e),g(Ia.$$.fragment,e),g(Ua.$$.fragment,e),g(Ga.$$.fragment,e),g(Va.$$.fragment,e),g(na.$$.fragment,e),g(Wa.$$.fragment,e),g(la.$$.fragment,e),g(Ja.$$.fragment,e),g(Ya.$$.fragment,e),g(Qa.$$.fragment,e),g(Xa.$$.fragment,e),g(Za.$$.fragment,e),g(en.$$.fragment,e),g(an.$$.fragment,e),g(nn.$$.fragment,e),g(ln.$$.fragment,e),g(on.$$.fragment,e),g(rn.$$.fragment,e),g(un.$$.fragment,e),g(pn.$$.fragment,e),g(dn.$$.fragment,e),g(cn.$$.fragment,e),g(_n.$$.fragment,e),g(fn.$$.fragment,e),g(pa.$$.fragment,e),g(at),g(hn.$$.fragment,e),g(gn.$$.fragment,e),g(da.$$.fragment,e),g(vn.$$.fragment,e),g(bn.$$.fragment,e),g(kn.$$.fragment,e),g(qn.$$.fragment,e),g(lt),g($n.$$.fragment,e),g(wn.$$.fragment,e),g(rt),g(xn.$$.fragment,e),g(ut),g(dt),g(En.$$.fragment,e),g(Le),g(_a.$$.fragment,e),g(Re),g(jn.$$.fragment,e),g(yn.$$.fragment,e),g(zn.$$.fragment,e),g(Mn.$$.fragment,e),g(Cn.$$.fragment,e),g(ha.$$.fragment,e),pu=!0)},o(e){v(d.$$.fragment,e),v(M.$$.fragment,e),v(F),v(xe.$$.fragment,e),v(Pe.$$.fragment,e),v(ps.$$.fragment,e),v(ms),v(Ss.$$.fragment,e),v(Ns.$$.fragment,e),v(vs),v(Ta.$$.fragment,e),v(Na.$$.fragment,e),v(Ka.$$.fragment,e),v(La.$$.fragment,e),v(Ra.$$.fragment,e),v(Ba.$$.fragment,e),v(ea.$$.fragment,e),v(Fa.$$.fragment,e),v(Ha.$$.fragment,e),v(Ia.$$.fragment,e),v(Ua.$$.fragment,e),v(Ga.$$.fragment,e),v(Va.$$.fragment,e),v(na.$$.fragment,e),v(Wa.$$.fragment,e),v(la.$$.fragment,e),v(Ja.$$.fragment,e),v(Ya.$$.fragment,e),v(Qa.$$.fragment,e),v(Xa.$$.fragment,e),v(Za.$$.fragment,e),v(en.$$.fragment,e),v(an.$$.fragment,e),v(nn.$$.fragment,e),v(ln.$$.fragment,e),v(on.$$.fragment,e),v(rn.$$.fragment,e),v(un.$$.fragment,e),v(pn.$$.fragment,e),v(dn.$$.fragment,e),v(cn.$$.fragment,e),v(_n.$$.fragment,e),v(fn.$$.fragment,e),v(pa.$$.fragment,e),v(at),v(hn.$$.fragment,e),v(gn.$$.fragment,e),v(da.$$.fragment,e),v(vn.$$.fragment,e),v(bn.$$.fragment,e),v(kn.$$.fragment,e),v(qn.$$.fragment,e),v(lt),v($n.$$.fragment,e),v(wn.$$.fragment,e),v(rt),v(xn.$$.fragment,e),v(ut),v(dt),v(En.$$.fragment,e),v(Le),v(_a.$$.fragment,e),v(Re),v(jn.$$.fragment,e),v(yn.$$.fragment,e),v(zn.$$.fragment,e),v(Mn.$$.fragment,e),v(Cn.$$.fragment,e),v(ha.$$.fragment,e),pu=!1},d(e){t(i),e&&t(h),j(d,e),e&&t($),e&&t(P),j(M),e&&t(K),Pn[L].d(e),e&&t(S),e&&t(A),e&&t(V),e&&t(I),e&&t($e),e&&t(ue),e&&t(Me),e&&t(fe),e&&t(We),e&&t(N),e&&t(ee),e&&t(J),e&&t(us),e&&t(ae),e&&t(es),e&&t(ss),e&&t(ge),j(xe,e),e&&t(Cs),j(Pe,e),e&&t(Vs),e&&t(Fe),j(ps),e&&t(It),e&&t(ds),e&&t(ct),e&&t(Js),e&&t(Ds),e&&t(Qe),e&&t(Nt),Dn[Ie].d(e),e&&t(et),e&&t(ts),e&&t(Wt),j(Ss,e),e&&t(m),e&&t(z),e&&t(Da),j(Ns,e),e&&t(Aa),e&&t(st),e&&t(Sa),An[gs].d(e),e&&t(Ot),j(Ta,e),e&&t(zr),e&&t(Zn),e&&t(Mr),e&&t(Kt),j(Na),e&&t(Cr),e&&t(bs),e&&t(Pr),j(Ka,e),e&&t(Dr),j(La,e),e&&t(Ar),e&&t(as),e&&t(Sr),j(Ra,e),e&&t(Tr),j(Ba,e),e&&t(Nr),e&&t(wt),e&&t(Or),j(ea,e),e&&t(Kr),e&&t(sa),e&&t(Lr),e&&t(Lt),j(Fa),e&&t(Rr),j(Ha,e),e&&t(Br),e&&t(aa),e&&t(Fr),e&&t(De),e&&t(Hr),j(Ia,e),e&&t(Ir),j(Ua,e),e&&t(Ur),e&&t(Os),e&&t(Gr),e&&t(Ks),e&&t(Vr),j(Ga,e),e&&t(Wr),j(Va,e),e&&t(Jr),e&&t(xt),e&&t(Yr),j(na,e),e&&t(Qr),e&&t(tl),e&&t(Xr),j(Wa,e),e&&t(Zr),j(la,e),e&&t(ei),e&&t(oa),e&&t(si),j(Ja,e),e&&t(ti),j(Ya,e),e&&t(ai),e&&t(al),e&&t(ni),j(Qa,e),e&&t(li),j(Xa,e),e&&t(oi),e&&t(Ls),e&&t(ri),j(Za,e),e&&t(ii),j(en,e),e&&t(ui),e&&t(ra),e&&t(pi),e&&t(ia),e&&t(di),e&&t(nl),e&&t(mi),j(an,e),e&&t(ci),e&&t(ns),e&&t(_i),e&&t(Et),e&&t(fi),j(nn,e),e&&t(hi),j(ln,e),e&&t(gi),e&&t(Ae),e&&t(vi),j(on,e),e&&t(bi),j(rn,e),e&&t(ki),e&&t(ll),e&&t(qi),j(un,e),e&&t($i),j(pn,e),e&&t(wi),e&&t(Rs),e&&t(xi),e&&t(Rt),j(dn),e&&t(Ei),e&&t(me),e&&t(ji),j(cn,e),e&&t(yi),e&&t(Bs),e&&t(zi),j(_n,e),e&&t(Mi),j(fn,e),e&&t(Ci),e&&t(ks),e&&t(Pi),j(pa,e),e&&t(Di),cs&&cs.d(e),e&&t(rl),e&&t(qs),e&&t(Ai),Sn[tt].d(e),e&&t(il),e&&t(ul),e&&t(Si),j(hn,e),e&&t(Ti),j(gn,e),e&&t(Ni),j(da,e),e&&t(Oi),e&&t(Fs),e&&t(Ki),j(vn,e),e&&t(Li),j(bn,e),e&&t(Ri),e&&t($s),e&&t(Bi),j(kn,e),e&&t(Fi),e&&t(ma),e&&t(Hi),j(qn,e),e&&t(Ii),e&&t(dl),e&&t(Ui),Tn[nt].d(e),e&&t(ml),e&&t(Bt),j($n),e&&t(Gi),j(wn,e),e&&t(Vi),e&&t(cl),e&&t(Wi),Nn[ot].d(e),e&&t(_l),j(xn,e),e&&t(Ji),e&&t(fl),e&&t(Yi),On[it].d(e),e&&t(hl),e&&t(gl),e&&t(Qi),Kn[pt].d(e),e&&t(vl),j(En,e),e&&t(Xi),e&&t(bl),e&&t(Zi),Le&&Le.d(e),e&&t(kl),j(_a,e),e&&t(eu),Re&&Re.d(e),e&&t(ql),e&&t(Ft),j(jn),e&&t(su),e&&t(ls),e&&t(tu),j(yn,e),e&&t(au),e&&t($l),e&&t(nu),j(zn,e),e&&t(lu),j(Mn,e),e&&t(ou),e&&t(wl),e&&t(ru),j(Cn,e),e&&t(iu),e&&t(jt),e&&t(uu),j(ha,e)}}}const _g={local:"finetuner-un-modle-de-langage-masqu",sections:[{local:"choix-dun-modle-prentran-pour-la-modlisation-du-langage-masqu",title:"Choix d'un mod\xE8le pr\xE9-entra\xEEn\xE9 pour la mod\xE9lisation du langage masqu\xE9"},{local:"le-jeu-de-donnes",title:"Le jeu de donn\xE9es"},{local:"prtraitement-des-donnes",title:"Pr\xE9traitement des donn\xE9es"},{local:"finetuning-de-distilbert-avec-lapi-trainer",sections:[{local:"perplexit-pour-les-modles-de-langage",title:"Perplexit\xE9 pour les mod\xE8les de langage"}],title:"*Finetuning* de DistilBERT avec l'API `Trainer`."},{local:"finetuning-de-distilbert-avec-accelerate",sections:[{local:"utilisation-de-notre-modle-finetun",title:"Utilisation de notre mod\xE8le *finetun\xE9*"}],title:"*Finetuning* de DistilBERT avec \u{1F917} Accelerate"}],title:"*Finetuner* un mod\xE8le de langage masqu\xE9"};function fg(B,i,h){let d="pt";return Bh(()=>{const $=new URLSearchParams(window.location.search);h(0,d=$.get("fw")||"pt")}),[d]}class wg extends Oh{constructor(i){super();Kh(this,i,fg,cg,Lh,{})}}export{wg as default,_g as metadata};
