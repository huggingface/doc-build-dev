import{S as oi,i as ti,s as ri,e as i,k as m,w as d,t,M as ni,c as l,d as s,m as u,a as p,x as c,h as r,b as v,G as a,g as n,y as f,q as h,o as g,B as x,v as ii}from"../../chunks/vendor-hf-doc-builder.js";import{T as li}from"../../chunks/Tip-hf-doc-builder.js";import{Y as pi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ko}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as _}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as mi}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function ui(Zs){let $,M;return{c(){$=i("p"),M=t("\u26A0\uFE0F Treinar um tokenizador n\xE3o \xE9 o mesmo que treinar um modelo! O treinamento de um modelo usa o gradiente descendente estoc\xE1stico para fazer a perda um pouquinho menor a cada batch. Portanto, \xE9 aleat\xF3rio por natureza (o que significa que voc\xEA deve definir seeds para obter o mesmo resultado quando estiver fazendo o mesmo treino novamente). Treinar um tokenizador \xE9 um processo estat\xEDstico que tenta identificar que subpalavras s\xE3o as melhores para escolher dependendo do algoritmo de tokeniza\xE7\xE3o. Portanto, este processo \xE9 determin\xEDstico, o que significa que voc\xEA ter\xE1 sempre o mesmo resultado quando for treinar com o mesmo algoritmo no mesmo corpus.")},l(z){$=l(z,"P",{});var w=p($);M=r(w,"\u26A0\uFE0F Treinar um tokenizador n\xE3o \xE9 o mesmo que treinar um modelo! O treinamento de um modelo usa o gradiente descendente estoc\xE1stico para fazer a perda um pouquinho menor a cada batch. Portanto, \xE9 aleat\xF3rio por natureza (o que significa que voc\xEA deve definir seeds para obter o mesmo resultado quando estiver fazendo o mesmo treino novamente). Treinar um tokenizador \xE9 um processo estat\xEDstico que tenta identificar que subpalavras s\xE3o as melhores para escolher dependendo do algoritmo de tokeniza\xE7\xE3o. Portanto, este processo \xE9 determin\xEDstico, o que significa que voc\xEA ter\xE1 sempre o mesmo resultado quando for treinar com o mesmo algoritmo no mesmo corpus."),w.forEach(s)},m(z,w){n(z,$,w),a($,M)},d(z){z&&s($)}}}function di(Zs){let $,M,z,w,ps,Z,bo,ms,$o,Ws,W,ea,A,jo,Le,zo,qo,us,wo,Eo,sa,ee,aa,R,oa,G,H,ds,se,Po,cs,Ao,ta,V,yo,fs,Co,Do,ra,q,Oo,ae,To,So,hs,Lo,No,oe,Go,Uo,te,Io,Mo,na,re,ia,Ne,Ro,la,ne,pa,ie,ma,y,Ho,gs,Vo,Bo,xs,Ko,Qo,ua,le,da,Ge,Yo,ca,pe,fa,Ue,Fo,ha,Ie,Xo,ga,me,xa,Me,Jo,va,ue,_a,C,Zo,vs,Wo,et,_s,st,at,ka,Re,ot,ba,de,$a,He,tt,ja,ce,za,Ve,rt,qa,fe,wa,D,nt,ks,it,lt,bs,pt,mt,Ea,he,Pa,Be,ut,Aa,U,B,$s,ge,dt,js,ct,ya,Ke,ft,Ca,xe,Da,Qe,ht,Oa,Ye,gt,Ta,ve,Sa,_e,La,E,xt,zs,vt,_t,qs,kt,bt,ws,$t,jt,Na,K,zt,Es,qt,wt,Ga,ke,Ua,Fe,Et,Ia,O,Pt,Ps,At,yt,be,Ct,Dt,Ma,Q,Ot,Xe,Tt,St,Ra,T,Lt,$e,Nt,Gt,As,Ut,It,Ha,je,Va,ze,Ba,j,Mt,ys,Rt,Ht,Cs,Vt,Bt,Ds,Kt,Qt,Os,Yt,Ft,Ts,Xt,Jt,Ka,qe,Qa,we,Ya,Je,Zt,Fa,Ee,Xa,Pe,Ja,k,Wt,Ss,er,sr,Ls,ar,or,Ns,tr,rr,Gs,nr,ir,Us,lr,pr,Is,mr,ur,Ms,dr,cr,Rs,fr,hr,Hs,gr,xr,Vs,vr,Za,I,Y,Bs,Ae,_r,Ks,kr,Wa,F,br,Qs,$r,jr,eo,ye,so,X,zr,Ys,qr,wr,ao,Ce,oo,Ze,Er,to,De,ro,We,Pr,no,Oe,io,S,Ar,Fs,yr,Cr,Xs,Dr,Or,lo,Te,po,L,Tr,es,Sr,Lr,Js,Nr,Gr,mo;return Z=new ko({}),W=new mi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"}]}}),ee=new pi({props:{id:"DJimQynXZsQ"}}),R=new li({props:{warning:!0,$$slots:{default:[ui]},$$scope:{ctx:Zs}}}),se=new ko({}),re=new _({props:{code:`from datasets import load_dataset

# Isto pode levar alguns minutos para carregar, ent\xE3o pegue um copo de caf\xE9 enquanto espera!
raw_datasets = load_dataset("code_search_net", "python")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Isto pode levar alguns minutos para carregar, ent\xE3o pegue um copo de caf\xE9 enquanto espera!</span>
raw_datasets = load_dataset(<span class="hljs-string">&quot;code_search_net&quot;</span>, <span class="hljs-string">&quot;python&quot;</span>)`}}),ne=new _({props:{code:'raw_datasets["train"]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]'}}),ie=new _({props:{code:`Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;repository_name&#x27;</span>, <span class="hljs-string">&#x27;func_path_in_repository&#x27;</span>, <span class="hljs-string">&#x27;func_name&#x27;</span>, <span class="hljs-string">&#x27;whole_func_string&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_string&#x27;</span>, <span class="hljs-string">&#x27;func_code_tokens&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_string&#x27;</span>, <span class="hljs-string">&#x27;func_documentation_tokens&#x27;</span>, <span class="hljs-string">&#x27;split_name&#x27;</span>, 
      <span class="hljs-string">&#x27;func_code_url&#x27;</span>
    ],
    num_rows: <span class="hljs-number">412178</span>
})`}}),le=new _({props:{code:'print(raw_datasets["train"][123456]["whole_func_string"])',highlighted:'<span class="hljs-built_in">print</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">123456</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>])'}}),pe=new _({props:{code:`def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_simple_responses</span>(<span class="hljs-params">
      self, timeout_ms=<span class="hljs-literal">None</span>, info_cb=DEFAULT_MESSAGE_CALLBACK</span>):
    <span class="hljs-string">&quot;&quot;&quot;Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet&#x27;s message.
    &quot;&quot;&quot;</span>
    <span class="hljs-keyword">return</span> self._accept_responses(<span class="hljs-string">&#x27;OKAY&#x27;</span>, info_cb, timeout_ms=timeout_ms)`}}),me=new _({props:{code:`# N\xE3o remova o coment\xE1rio da linha abaixo, a menos que o teu dataset seja pequeno!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]`,highlighted:`<span class="hljs-comment"># N\xE3o remova o coment\xE1rio da linha abaixo, a menos que o teu dataset seja pequeno!</span>
<span class="hljs-comment"># training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]</span>`}}),ue=new _({props:{code:`training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)`,highlighted:`training_corpus = (
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
)`}}),de=new _({props:{code:`gen = (i for i in range(10))
print(list(gen))
print(list(gen))`,highlighted:`gen = (i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(gen))`}}),ce=new _({props:{code:`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]`,highlighted:`[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]
[]`}}),fe=new _({props:{code:`def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">return</span> (
        raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;whole_func_string&quot;</span>]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]), <span class="hljs-number">1000</span>)
    )


training_corpus = get_training_corpus()`}}),he=new _({props:{code:`def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
    <span class="hljs-keyword">for</span> start_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        samples = dataset[start_idx : start_idx + <span class="hljs-number">1000</span>]
        <span class="hljs-keyword">yield</span> samples[<span class="hljs-string">&quot;whole_func_string&quot;</span>]`}}),ge=new ko({}),xe=new _({props:{code:`from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)`}}),ve=new _({props:{code:`example = '''def add_numbers(a, b):
    """Add the two numbers \`a\` and \`b\`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens`,highlighted:`example = <span class="hljs-string">&#x27;&#x27;&#x27;def add_numbers(a, b):
    &quot;&quot;&quot;Add the two numbers \`a\` and \`b\`.&quot;&quot;&quot;
    return a + b&#x27;&#x27;&#x27;</span>

tokens = old_tokenizer.tokenize(example)
tokens`}}),_e=new _({props:{code:`['def', '\u0120add', '_', 'n', 'umbers', '(', 'a', ',', '\u0120b', '):', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two',
 '\u0120numbers', '\u0120\`', 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`', '."', '""', '\u010A', '\u0120', '\u0120', '\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;umbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>,\n <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;.&quot;&#x27;</span>, <span class="hljs-string">&#x27;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),ke=new _({props:{code:"tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)",highlighted:'tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="hljs-number">52000</span>)'}}),je=new _({props:{code:`tokens = tokenizer.tokenize(example)
tokens`,highlighted:`tokens = tokenizer.tokenize(example)
tokens`}}),ze=new _({props:{code:`['def', '\u0120add', '_', 'numbers', '(', 'a', ',', '\u0120b', '):', '\u010A\u0120\u0120\u0120', '\u0120"""', 'Add', '\u0120the', '\u0120two', '\u0120numbers', '\u0120\`',
 'a', '\`', '\u0120and', '\u0120\`', 'b', '\`."""', '\u010A\u0120\u0120\u0120', '\u0120return', '\u0120a', '\u0120+', '\u0120b']`,highlighted:'[<span class="hljs-string">&#x27;def&#x27;</span>, <span class="hljs-string">&#x27;\u0120add&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;numbers&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;Add&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;\u0120two&#x27;</span>, <span class="hljs-string">&#x27;\u0120numbers&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>,\n <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;`&#x27;</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>, <span class="hljs-string">&#x27;\u0120`&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;`.&quot;&quot;&quot;&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120b&#x27;</span>]'}}),qe=new _({props:{code:`print(len(tokens))
print(len(old_tokenizer.tokenize(example)))`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(tokens))
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(old_tokenizer.tokenize(example)))`}}),we=new _({props:{code:`27
36`,highlighted:`<span class="hljs-number">27</span>
<span class="hljs-number">36</span>`}}),Ee=new _({props:{code:`example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)`,highlighted:`example = <span class="hljs-string">&quot;&quot;&quot;class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    &quot;&quot;&quot;</span>
tokenizer.tokenize(example)`}}),Pe=new _({props:{code:`['class', '\u0120Linear', 'Layer', '():', '\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'init', '__(', 'self', ',', '\u0120input', '_', 'size', ',',
 '\u0120output', '_', 'size', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'weight', '\u0120=', '\u0120torch', '.', 'randn', '(', 'input', '_',
 'size', ',', '\u0120output', '_', 'size', ')', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120', '\u0120self', '.', 'bias', '\u0120=', '\u0120torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', '\u010A\u010A\u0120\u0120\u0120', '\u0120def', '\u0120__', 'call', '__(', 'self', ',', '\u0120x', '):', '\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120',
 '\u0120return', '\u0120x', '\u0120@', '\u0120self', '.', 'weights', '\u0120+', '\u0120self', '.', 'bias', '\u010A\u0120\u0120\u0120\u0120']`,highlighted:`[<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;\u0120Linear&#x27;</span>, <span class="hljs-string">&#x27;Layer&#x27;</span>, <span class="hljs-string">&#x27;():&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;init&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weight&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;randn&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;input&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>,
 <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u0120=&#x27;</span>, <span class="hljs-string">&#x27;\u0120torch&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;zeros&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>,
 <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u010A\u0120\u0120\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120def&#x27;</span>, <span class="hljs-string">&#x27;\u0120__&#x27;</span>, <span class="hljs-string">&#x27;call&#x27;</span>, <span class="hljs-string">&#x27;__(&#x27;</span>, <span class="hljs-string">&#x27;self&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;):&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120return&#x27;</span>, <span class="hljs-string">&#x27;\u0120x&#x27;</span>, <span class="hljs-string">&#x27;\u0120@&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;weights&#x27;</span>, <span class="hljs-string">&#x27;\u0120+&#x27;</span>, <span class="hljs-string">&#x27;\u0120self&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;\u010A\u0120\u0120\u0120\u0120&#x27;</span>]`}}),Ae=new ko({}),ye=new _({props:{code:'tokenizer.save_pretrained("code-search-net-tokenizer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),Ce=new _({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),De=new _({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),Oe=new _({props:{code:'tokenizer.push_to_hub("code-search-net-tokenizer")',highlighted:'tokenizer.push_to_hub(<span class="hljs-string">&quot;code-search-net-tokenizer&quot;</span>)'}}),Te=new _({props:{code:`# Substitua "huggingface-course" abaixo pelo seu namespace real para usar seu pr\xF3prio tokenizador
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")`,highlighted:`<span class="hljs-comment"># Substitua &quot;huggingface-course&quot; abaixo pelo seu namespace real para usar seu pr\xF3prio tokenizador</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)`}}),{c(){$=i("meta"),M=m(),z=i("h1"),w=i("a"),ps=i("span"),d(Z.$$.fragment),bo=m(),ms=i("span"),$o=t("Treinando um novo tokenizador"),Ws=m(),d(W.$$.fragment),ea=m(),A=i("p"),jo=t("Se um modelo de linguagem n\xE3o estiver dispon\xEDvel no idioma que voc\xEA estiver interessado, ou se o seu corpus for muito diferente do que o seu modelo de linguagem foi treinado, voc\xEA muito provavelmente desejar\xE1 retreinar o modelo do zero usando um tokenizador adaptado para seus dados. Isto exigir\xE1 um treinamento de um novo tokenizador para seu conjunto de dados. Mas o que isso exatamente significa? Quando observamos os tokenizadores pela primeira vez no "),Le=i("a"),zo=t("Cap\xEDtulo 2"),qo=t(", n\xF3s vimos que a maioria dos modelos Transformer usa um algoritmo de tokeniza\xE7\xE3o de subpalavras. Para identificar quais subpalavras s\xE3o de interesse e que ocorrem mais frequentemente no corpus em quest\xE3o, o tokenizador precisa dar uma boa olhada em todos os textos no corpus \u2014 processo que chamamos de "),us=i("em"),wo=t("treinamento"),Eo=t(". As regras exatas que governam o treinamento dependem do tipo de tokenizador usado, e veremos os tr\xEAs algoritmos principais mais adiante neste cap\xEDtulo."),sa=m(),d(ee.$$.fragment),aa=m(),d(R.$$.fragment),oa=m(),G=i("h2"),H=i("a"),ds=i("span"),d(se.$$.fragment),Po=m(),cs=i("span"),Ao=t("Montando um corpus"),ta=m(),V=i("p"),yo=t("Existe uma API muito simples em \u{1F917} Transformers que voc\xEA pode usar para treinar um novo tokenizador com as mesmas caracter\xEDsticas de um j\xE1 existente: "),fs=i("code"),Co=t("AutoTokenizer.train_new_from_iterator()"),Do=t(". Para ver isso em a\xE7\xE3o, vamos supor que queremos treinar o GPT-2 do zero, mas em um idioma diferente do ingl\xEAs. Nossa primeira tarefa ser\xE1 obter muitos dados de um idioma em um corpus de treinamento. Para prover exemplos que todo mundo ser\xE1 capaz de entender, n\xE3o usaremos um idioma como russo ou chin\xEAs aqui, mas sim in idiome ingl\xEAs especializado: c\xF3digo Python."),ra=m(),q=i("p"),Oo=t("A biblioteca "),ae=i("a"),To=t("\u{1F917} Datasets"),So=t(" pode nos ajudar a montar um corpus de c\xF3digos em Python. N\xF3s usaremos a fun\xE7\xE3o usual "),hs=i("code"),Lo=t("load_dataset()"),No=t(" para baixar e armazenar em cache o dataset "),oe=i("a"),Go=t("CodeSearchNet"),Uo=t(". Este dataset foi criado para o "),te=i("a"),Io=t("CodeSearchNet challenge"),Mo=t(" e cont\xE9m milh\xF5es de fun\xE7\xF5es de bibliotecas de c\xF3digo aberto no GitHub em diferentes linguagens de programa\xE7\xE3o. Aqui, n\xF3s iremos carregar a parte Python deste dataset:"),na=m(),d(re.$$.fragment),ia=m(),Ne=i("p"),Ro=t("Podemos dar uma olhada na divis\xE3o de treinamento para ver a quais colunas temos acesso:"),la=m(),d(ne.$$.fragment),pa=m(),d(ie.$$.fragment),ma=m(),y=i("p"),Ho=t("Podemos ver que o conjunto de dados separa as docstrings do c\xF3digo e sugere uma tokeniza\xE7\xE3o de ambos. Aqui, usaremos apenas a coluna "),gs=i("code"),Vo=t("whole_func_string"),Bo=t(" para treinar o nosso tokenizador. Podemos observar um exemplo de uma dessas fun\xE7\xF5es indexando na divis\xE3o "),xs=i("code"),Ko=t("train"),Qo=t(":"),ua=m(),d(le.$$.fragment),da=m(),Ge=i("p"),Yo=t("Que deve resultar na seguinte sa\xEDda:"),ca=m(),d(pe.$$.fragment),fa=m(),Ue=i("p"),Fo=t("A primeira coisa que precisamos fazer \xE9 transformar o dataset em um _iterator) de listas de textos \u2014 por exemplo, uma lista de lista de textos. Usando lista de textos ir\xE1 habilitar o nosso tokenizador para funcionar mais rapidamente (treinando em lotes de textos ao inv\xE9s de processar individualmente os textos, um por vez), e deve ser um iterador se quisermos evitar ter tudo na mem\xF3ria de uma vez. Se o teu corpus for grande, voc\xEA vai querer aproveitar o fato de que \u{1F917} Datasets n\xE3o carrega tudo na mem\xF3ria RAM, mas armazena os elementos do dataset no disco."),ha=m(),Ie=i("p"),Xo=t("Executar o trecho abaixo criaria uma lista de listas de 1000 textos cada, mas carregaria tudo na mem\xF3ria:"),ga=m(),d(me.$$.fragment),xa=m(),Me=i("p"),Jo=t("Usando um Python generator, n\xF3s podemos evitar que o Python carregue tudo na mem\xF3ria at\xE9 que realmente seja necess\xE1rio. Para criar tal generator, voc\xEA precisa apenas substituir os colchetes por par\xEAnteses:"),va=m(),d(ue.$$.fragment),_a=m(),C=i("p"),Zo=t("Esta linha de c\xF3digo n\xE3o busca nenhum elemento no dataset; ele apenas cria um objeto que voc\xEA pode usar em um o loop "),vs=i("code"),Wo=t("for"),et=t(" do Python. Os textos s\xF3 ser\xE3o carregados quando voc\xEA precisar deles (ou seja, quando voc\xEA estiver na etapa do loop "),_s=i("code"),st=t("for"),at=t(" que os requer), e apenas 1000 textos por vez ser\xE3o carregados. Desse modo, voc\xEA n\xE3o esgotar\xE1 toda a sua mem\xF3ria, mesmo se voc\xEA estiver processando um grande dataset."),ka=m(),Re=i("p"),ot=t("O problema com um objeto gerador \xE9 que ele s\xF3 pode ser usado uma vez. Ent\xE3o, em vez de nos dar a lista dos primeiros 10 d\xEDgitos duas vezes:"),ba=m(),d(de.$$.fragment),$a=m(),He=i("p"),tt=t("n\xF3s obtemos uma vez e, em seguida, uma lista vazia:"),ja=m(),d(ce.$$.fragment),za=m(),Ve=i("p"),rt=t("\xC9 por isso que definomos uma fun\xE7\xE3o que retorna um gerador:"),qa=m(),d(fe.$$.fragment),wa=m(),D=i("p"),nt=t("Voc\xEA tamb\xE9m pode definir o seu gerador dentro de um loop "),ks=i("code"),it=t("for"),lt=t(" ao usar o comando "),bs=i("code"),pt=t("yield"),mt=t(":"),Ea=m(),d(he.$$.fragment),Pa=m(),Be=i("p"),ut=t("que ir\xE1 produzir exatamente o mesmo gerador de antes, mas permite que voc\xEA use uma l\xF3gica mais complexa do que voc\xEA pode em um list comprehension."),Aa=m(),U=i("h2"),B=i("a"),$s=i("span"),d(ge.$$.fragment),dt=m(),js=i("span"),ct=t("Treinando um novo tokenizador"),ya=m(),Ke=i("p"),ft=t("Agora que n\xF3s temos o nosso corpus na forma de um iterador de lotes de texto, estamos prontos para treinar um novo tokenizador. Para fazer isso, primeiramente n\xF3s precisamos carregar o tokenizador que queremos emparelhar com o nosso modelo (neste caso, GPT-2):"),Ca=m(),d(xe.$$.fragment),Da=m(),Qe=i("p"),ht=t("Por mais que iremos treinar um novo tokenizador, \xE9 uma boa ideia fazer isso para evitar come\xE7ar do zero. Dessa forma, n\xE3o precisaremos especificar nada sobre o algoritmo de tokeniza\xE7\xE3o ou tokens especiais que queremos usar; nosso novo tokenizador ser\xE1 exatamente igual ao GPT-2, e a \xFAnica coisa que ir\xE1 mudar \xE9 o vocabul\xE1rio, que ser\xE1 determinado pelo treinamento em nosso corpus."),Oa=m(),Ye=i("p"),gt=t("Primeiramente, vamos dar uma olhada em como o tokenizador trataria um exemplo de fun\xE7\xE3o:"),Ta=m(),d(ve.$$.fragment),Sa=m(),d(_e.$$.fragment),La=m(),E=i("p"),xt=t("O tokenizador possui alguns s\xEDmbolos especiais, como "),zs=i("code"),vt=t("\u0120"),_t=t(" e "),qs=i("code"),kt=t("\u010A"),bt=t(", que denotam espa\xE7os e novas linhas, respectivamente. Como podemos observar, isso n\xE3o \xE9 t\xE3o eficiente: o tokenizador retorna tokens individuais para cada espa\xE7o, quando poderia agrupar n\xEDveis de indenta\xE7\xE3o (j\xE1 que ter conjuntos de quatro ou oito espa\xE7os ser\xE1 muito comum no c\xF3digo). O tokenizador tamb\xE9m dividiu o nome da fun\xE7\xE3o de uma forma um pouco estranha, n\xE3o sendo usado para ver palavras com o caractere "),ws=i("code"),$t=t("_"),jt=t("."),Na=m(),K=i("p"),zt=t("Vamos treinar um novo tokenizador e ver se isso resolve esses problemas. Para isso, iremos utilizar o m\xE9todo "),Es=i("code"),qt=t("train_new_from_iterator()"),wt=t(":"),Ga=m(),d(ke.$$.fragment),Ua=m(),Fe=i("p"),Et=t("Este comando pode demorar um pouco se o seu corpus for muito grande, mas para este dataset contendo 1.6 GB de textos \xE9 extremamente r\xE1pido (1 minuto e 16 segundos em uma CPU AMD Ryzen 9 3900X com 12 n\xFAcleos)."),Ia=m(),O=i("p"),Pt=t("Observe que "),Ps=i("code"),At=t("AutoTokenizer.train_new_from_iterator()"),yt=t(" funciona apenas se o tokenizador que voc\xEA estiver usando \xE9 um tokenizador \u201Cr\xE1pido\u201D. Como voc\xEA ver\xE1 na pr\xF3xima se\xE7\xE3o, a biblioteca \u{1F917} Transformers cont\xE9m dois tipos de tokenizers: alguns s\xE3o escritos puramente em Python e outros (os mais r\xE1pidos) s\xE3o apoiados pela biblioteca \u{1F917} Tokenizers, que \xE9 escrita na linguagem de programa\xE7\xE3o "),be=i("a"),Ct=t("Rust"),Dt=t(". Python \xE9 a linguagem de programa\xE7\xE3o mais utilizada para Ci\xEAncia de Dados e aplica\xE7\xF5es em Deep Learning, mas quando algo precisa ser paralelizado para ser r\xE1pido, \xE9 preciso ser escrito em uma outra linguagem. Por exemplo, as multiplica\xE7\xF5es de matrizes que est\xE3o na base de modelos de computa\xE7\xE3o s\xE3o escritos em CUDA, uma biblioteca em C otimizada para GPUs."),Ma=m(),Q=i("p"),Ot=t("Treinar um tokenizador totalmente novo usando apenas Python seria terrivelmente lento, e \xE9 por isso que n\xF3s desenvolvemos a biblioteca \u{1F917} Tokenizers. Observe que, assim como voc\xEA n\xE3o precisou aprender a linguagem CUDA para ser capaz de executar seu modelo em um lote de entradas em uma GPU, voc\xEA n\xE3o precisar\xE1 aprender Rust para usar o tokenizador r\xE1pido. A biblioteca \u{1F917} Tokenizers fornece liga\xE7\xE7\xF5es para muitos m\xE9todos que internamente chamam algum trecho de c\xF3digo em Rust; por exemplo, para paralelizar o treinamento do seu novo tokenizador ou, como vimos no "),Xe=i("a"),Tt=t("Chapter 3"),St=t(", a tokeniza\xE7\xE3o de um lote de entradas."),Ra=m(),T=i("p"),Lt=t("A maioria dos modelos Transformer possui um tokenizador r\xE1pido dispon\xEDvel (existem algumas exce\xE7\xF5es que voc\xEA pode checar "),$e=i("a"),Nt=t("aqui"),Gt=t("), e a API "),As=i("code"),Ut=t("AutoTokenizer"),It=t(" sempre seleciona o tokenizador r\xE1pido para voc\xEA se estiver dispon\xEDvel. Na pr\xF3xima se\xE7\xE3o, veremos alguns dos outros recursos especiais que os tokenizers r\xE1pidos possuem, que ser\xE3o realmente \xFAteis para tarefas como classifica\xE7\xE3o de tokens e resposta a perguntas. Antes de aprofundarmos nisso, no entanto, vamos experimentar o nosso novo tokenizador no exemplo anterior:"),Ha=m(),d(je.$$.fragment),Va=m(),d(ze.$$.fragment),Ba=m(),j=i("p"),Mt=t("Aqui vemos novamente os s\xEDmbolos especiais "),ys=i("code"),Rt=t("\u0120"),Ht=t(" and "),Cs=i("code"),Vt=t("\u010A"),Bt=t(" que denotam espa\xE7os e novas linhas, mas tamb\xE9m podemos observar que o nosso tokenizador aprendeu alguns tokens que s\xE3o altamente espec\xEDficos em um corpus de fun\xE7\xF5es em Python: por exemplo, existe um token "),Ds=i("code"),Kt=t("\u010A\u0120\u0120\u0120"),Qt=t(" que representa uma indenta\xE7\xE3o, e um token "),Os=i("code"),Yt=t('\u0120"""'),Ft=t(" que representa as tr\xEAs aspas que come\xE7am uma docstring. O tokenizador tamb\xE9m divide corretamente o nome da fun\xE7\xE3o em "),Ts=i("code"),Xt=t("_"),Jt=t(". Esta \xE9 uma representa\xE7\xE3o bastante compacta; comparativamente, usando o tokenizador em ingl\xEAs no mesmo exemplo nos dar\xE1 uma frase mais longa:"),Ka=m(),d(qe.$$.fragment),Qa=m(),d(we.$$.fragment),Ya=m(),Je=i("p"),Zt=t("Vejamos outro exemplo:"),Fa=m(),d(Ee.$$.fragment),Xa=m(),d(Pe.$$.fragment),Ja=m(),k=i("p"),Wt=t("Al\xE9m do token correpondente a uma indenta\xE7\xE3o, aqui podemos ver um token para uma indenta\xE7\xE3o dupla: "),Ss=i("code"),er=t("\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),sr=t(". Palavras especiais em Python, como "),Ls=i("code"),ar=t("class"),or=t(", "),Ns=i("code"),tr=t("init"),rr=t(", "),Gs=i("code"),nr=t("call"),ir=t(", "),Us=i("code"),lr=t("self"),pr=t(", e "),Is=i("code"),mr=t("return"),ur=t(" s\xE3o tokenizadas como um token, e podemos ver que al\xE9m de dividir em "),Ms=i("code"),dr=t("_"),cr=t(" e "),Rs=i("code"),fr=t("."),hr=t(", o tokenizador divide corretamente at\xE9 mesmo nomes em CamelCase: "),Hs=i("code"),gr=t("LinearLayer"),xr=t(" \xE9 tokenizado como "),Vs=i("code"),vr=t('["\u0120Linear", "Layer"]'),Za=m(),I=i("h2"),Y=i("a"),Bs=i("span"),d(Ae.$$.fragment),_r=m(),Ks=i("span"),kr=t("Salvando o tokenizador"),Wa=m(),F=i("p"),br=t("Para garantir que podemos us\xE1-lo mais tarde, precisamos salvar nosso novo tokenizador. Assim como \xE9 utilizado para modelos, isso \xE9 feito com o m\xE9todo "),Qs=i("code"),$r=t("save_pretrained()"),jr=t(":"),eo=m(),d(ye.$$.fragment),so=m(),X=i("p"),zr=t("Isso ir\xE1 criar uma nova pasta chamada "),Ys=i("em"),qr=t("code-search-net-tokenizer"),wr=t(", que ir\xE1 conter todos os arquivos que o tokenizador precisa para ser recarregado. Se voc\xEA quiser compartilhar este tokenizador com outros colegas e amigos, voc\xEA pode carreg\xE1-lo no Hub fazendo login em sua conta. Se voc\xEA estiver trabalhando em um notebook, h\xE1 uma fun\xE7\xE3o conveniente para ajud\xE1-lo com isso:"),ao=m(),d(Ce.$$.fragment),oo=m(),Ze=i("p"),Er=t("Isso exibir\xE1 um widget onde voc\xEA pode inserir suas credenciais de login do Hugging Face. Se voc\xEA n\xE3o estiver trabalhando em um notebook, basta digitar a seguinte linha em seu terminal:"),to=m(),d(De.$$.fragment),ro=m(),We=i("p"),Pr=t("Depois de voc\xEA logar, voc\xEA pode enviar seu tokenizador executando o seguinte comando:"),no=m(),d(Oe.$$.fragment),io=m(),S=i("p"),Ar=t("Isso criar\xE1 um novo reposit\xF3rio em seu namespace com o nome "),Fs=i("code"),yr=t("code-search-net-tokenizer"),Cr=t(", contendo o arquivo do tokenizador. Voc\xEA pode ent\xE3o carregar o tokenizador de qualquer lugar com o m\xE9todo "),Xs=i("code"),Dr=t("from_pretrained()"),Or=t(":"),lo=m(),d(Te.$$.fragment),po=m(),L=i("p"),Tr=t("Agora voc\xEA est\xE1 pronto para treinar um modelo de linguagem do zero e ajust\xE1-lo para sua tarefa! Chegaremos a isso no "),es=i("a"),Sr=t("Chapter 7"),Lr=t(", mas primeiro, no resto do cap\xEDtulo daremos uma olhada sobre tokenizers r\xE1pidos e explorar em detalhes o que realmente acontece quando chamamos o m\xE9todo "),Js=i("code"),Nr=t("train_new_from_iterator()"),Gr=t("."),this.h()},l(e){const o=ni('[data-svelte="svelte-1phssyn"]',document.head);$=l(o,"META",{name:!0,content:!0}),o.forEach(s),M=u(e),z=l(e,"H1",{class:!0});var Se=p(z);w=l(Se,"A",{id:!0,class:!0,href:!0});var Ur=p(w);ps=l(Ur,"SPAN",{});var Ir=p(ps);c(Z.$$.fragment,Ir),Ir.forEach(s),Ur.forEach(s),bo=u(Se),ms=l(Se,"SPAN",{});var Mr=p(ms);$o=r(Mr,"Treinando um novo tokenizador"),Mr.forEach(s),Se.forEach(s),Ws=u(e),c(W.$$.fragment,e),ea=u(e),A=l(e,"P",{});var ss=p(A);jo=r(ss,"Se um modelo de linguagem n\xE3o estiver dispon\xEDvel no idioma que voc\xEA estiver interessado, ou se o seu corpus for muito diferente do que o seu modelo de linguagem foi treinado, voc\xEA muito provavelmente desejar\xE1 retreinar o modelo do zero usando um tokenizador adaptado para seus dados. Isto exigir\xE1 um treinamento de um novo tokenizador para seu conjunto de dados. Mas o que isso exatamente significa? Quando observamos os tokenizadores pela primeira vez no "),Le=l(ss,"A",{href:!0});var Rr=p(Le);zo=r(Rr,"Cap\xEDtulo 2"),Rr.forEach(s),qo=r(ss,", n\xF3s vimos que a maioria dos modelos Transformer usa um algoritmo de tokeniza\xE7\xE3o de subpalavras. Para identificar quais subpalavras s\xE3o de interesse e que ocorrem mais frequentemente no corpus em quest\xE3o, o tokenizador precisa dar uma boa olhada em todos os textos no corpus \u2014 processo que chamamos de "),us=l(ss,"EM",{});var Hr=p(us);wo=r(Hr,"treinamento"),Hr.forEach(s),Eo=r(ss,". As regras exatas que governam o treinamento dependem do tipo de tokenizador usado, e veremos os tr\xEAs algoritmos principais mais adiante neste cap\xEDtulo."),ss.forEach(s),sa=u(e),c(ee.$$.fragment,e),aa=u(e),c(R.$$.fragment,e),oa=u(e),G=l(e,"H2",{class:!0});var uo=p(G);H=l(uo,"A",{id:!0,class:!0,href:!0});var Vr=p(H);ds=l(Vr,"SPAN",{});var Br=p(ds);c(se.$$.fragment,Br),Br.forEach(s),Vr.forEach(s),Po=u(uo),cs=l(uo,"SPAN",{});var Kr=p(cs);Ao=r(Kr,"Montando um corpus"),Kr.forEach(s),uo.forEach(s),ta=u(e),V=l(e,"P",{});var co=p(V);yo=r(co,"Existe uma API muito simples em \u{1F917} Transformers que voc\xEA pode usar para treinar um novo tokenizador com as mesmas caracter\xEDsticas de um j\xE1 existente: "),fs=l(co,"CODE",{});var Qr=p(fs);Co=r(Qr,"AutoTokenizer.train_new_from_iterator()"),Qr.forEach(s),Do=r(co,". Para ver isso em a\xE7\xE3o, vamos supor que queremos treinar o GPT-2 do zero, mas em um idioma diferente do ingl\xEAs. Nossa primeira tarefa ser\xE1 obter muitos dados de um idioma em um corpus de treinamento. Para prover exemplos que todo mundo ser\xE1 capaz de entender, n\xE3o usaremos um idioma como russo ou chin\xEAs aqui, mas sim in idiome ingl\xEAs especializado: c\xF3digo Python."),co.forEach(s),ra=u(e),q=l(e,"P",{});var N=p(q);Oo=r(N,"A biblioteca "),ae=l(N,"A",{href:!0,rel:!0});var Yr=p(ae);To=r(Yr,"\u{1F917} Datasets"),Yr.forEach(s),So=r(N," pode nos ajudar a montar um corpus de c\xF3digos em Python. N\xF3s usaremos a fun\xE7\xE3o usual "),hs=l(N,"CODE",{});var Fr=p(hs);Lo=r(Fr,"load_dataset()"),Fr.forEach(s),No=r(N," para baixar e armazenar em cache o dataset "),oe=l(N,"A",{href:!0,rel:!0});var Xr=p(oe);Go=r(Xr,"CodeSearchNet"),Xr.forEach(s),Uo=r(N,". Este dataset foi criado para o "),te=l(N,"A",{href:!0,rel:!0});var Jr=p(te);Io=r(Jr,"CodeSearchNet challenge"),Jr.forEach(s),Mo=r(N," e cont\xE9m milh\xF5es de fun\xE7\xF5es de bibliotecas de c\xF3digo aberto no GitHub em diferentes linguagens de programa\xE7\xE3o. Aqui, n\xF3s iremos carregar a parte Python deste dataset:"),N.forEach(s),na=u(e),c(re.$$.fragment,e),ia=u(e),Ne=l(e,"P",{});var Zr=p(Ne);Ro=r(Zr,"Podemos dar uma olhada na divis\xE3o de treinamento para ver a quais colunas temos acesso:"),Zr.forEach(s),la=u(e),c(ne.$$.fragment,e),pa=u(e),c(ie.$$.fragment,e),ma=u(e),y=l(e,"P",{});var as=p(y);Ho=r(as,"Podemos ver que o conjunto de dados separa as docstrings do c\xF3digo e sugere uma tokeniza\xE7\xE3o de ambos. Aqui, usaremos apenas a coluna "),gs=l(as,"CODE",{});var Wr=p(gs);Vo=r(Wr,"whole_func_string"),Wr.forEach(s),Bo=r(as," para treinar o nosso tokenizador. Podemos observar um exemplo de uma dessas fun\xE7\xF5es indexando na divis\xE3o "),xs=l(as,"CODE",{});var en=p(xs);Ko=r(en,"train"),en.forEach(s),Qo=r(as,":"),as.forEach(s),ua=u(e),c(le.$$.fragment,e),da=u(e),Ge=l(e,"P",{});var sn=p(Ge);Yo=r(sn,"Que deve resultar na seguinte sa\xEDda:"),sn.forEach(s),ca=u(e),c(pe.$$.fragment,e),fa=u(e),Ue=l(e,"P",{});var an=p(Ue);Fo=r(an,"A primeira coisa que precisamos fazer \xE9 transformar o dataset em um _iterator) de listas de textos \u2014 por exemplo, uma lista de lista de textos. Usando lista de textos ir\xE1 habilitar o nosso tokenizador para funcionar mais rapidamente (treinando em lotes de textos ao inv\xE9s de processar individualmente os textos, um por vez), e deve ser um iterador se quisermos evitar ter tudo na mem\xF3ria de uma vez. Se o teu corpus for grande, voc\xEA vai querer aproveitar o fato de que \u{1F917} Datasets n\xE3o carrega tudo na mem\xF3ria RAM, mas armazena os elementos do dataset no disco."),an.forEach(s),ha=u(e),Ie=l(e,"P",{});var on=p(Ie);Xo=r(on,"Executar o trecho abaixo criaria uma lista de listas de 1000 textos cada, mas carregaria tudo na mem\xF3ria:"),on.forEach(s),ga=u(e),c(me.$$.fragment,e),xa=u(e),Me=l(e,"P",{});var tn=p(Me);Jo=r(tn,"Usando um Python generator, n\xF3s podemos evitar que o Python carregue tudo na mem\xF3ria at\xE9 que realmente seja necess\xE1rio. Para criar tal generator, voc\xEA precisa apenas substituir os colchetes por par\xEAnteses:"),tn.forEach(s),va=u(e),c(ue.$$.fragment,e),_a=u(e),C=l(e,"P",{});var os=p(C);Zo=r(os,"Esta linha de c\xF3digo n\xE3o busca nenhum elemento no dataset; ele apenas cria um objeto que voc\xEA pode usar em um o loop "),vs=l(os,"CODE",{});var rn=p(vs);Wo=r(rn,"for"),rn.forEach(s),et=r(os," do Python. Os textos s\xF3 ser\xE3o carregados quando voc\xEA precisar deles (ou seja, quando voc\xEA estiver na etapa do loop "),_s=l(os,"CODE",{});var nn=p(_s);st=r(nn,"for"),nn.forEach(s),at=r(os," que os requer), e apenas 1000 textos por vez ser\xE3o carregados. Desse modo, voc\xEA n\xE3o esgotar\xE1 toda a sua mem\xF3ria, mesmo se voc\xEA estiver processando um grande dataset."),os.forEach(s),ka=u(e),Re=l(e,"P",{});var ln=p(Re);ot=r(ln,"O problema com um objeto gerador \xE9 que ele s\xF3 pode ser usado uma vez. Ent\xE3o, em vez de nos dar a lista dos primeiros 10 d\xEDgitos duas vezes:"),ln.forEach(s),ba=u(e),c(de.$$.fragment,e),$a=u(e),He=l(e,"P",{});var pn=p(He);tt=r(pn,"n\xF3s obtemos uma vez e, em seguida, uma lista vazia:"),pn.forEach(s),ja=u(e),c(ce.$$.fragment,e),za=u(e),Ve=l(e,"P",{});var mn=p(Ve);rt=r(mn,"\xC9 por isso que definomos uma fun\xE7\xE3o que retorna um gerador:"),mn.forEach(s),qa=u(e),c(fe.$$.fragment,e),wa=u(e),D=l(e,"P",{});var ts=p(D);nt=r(ts,"Voc\xEA tamb\xE9m pode definir o seu gerador dentro de um loop "),ks=l(ts,"CODE",{});var un=p(ks);it=r(un,"for"),un.forEach(s),lt=r(ts," ao usar o comando "),bs=l(ts,"CODE",{});var dn=p(bs);pt=r(dn,"yield"),dn.forEach(s),mt=r(ts,":"),ts.forEach(s),Ea=u(e),c(he.$$.fragment,e),Pa=u(e),Be=l(e,"P",{});var cn=p(Be);ut=r(cn,"que ir\xE1 produzir exatamente o mesmo gerador de antes, mas permite que voc\xEA use uma l\xF3gica mais complexa do que voc\xEA pode em um list comprehension."),cn.forEach(s),Aa=u(e),U=l(e,"H2",{class:!0});var fo=p(U);B=l(fo,"A",{id:!0,class:!0,href:!0});var fn=p(B);$s=l(fn,"SPAN",{});var hn=p($s);c(ge.$$.fragment,hn),hn.forEach(s),fn.forEach(s),dt=u(fo),js=l(fo,"SPAN",{});var gn=p(js);ct=r(gn,"Treinando um novo tokenizador"),gn.forEach(s),fo.forEach(s),ya=u(e),Ke=l(e,"P",{});var xn=p(Ke);ft=r(xn,"Agora que n\xF3s temos o nosso corpus na forma de um iterador de lotes de texto, estamos prontos para treinar um novo tokenizador. Para fazer isso, primeiramente n\xF3s precisamos carregar o tokenizador que queremos emparelhar com o nosso modelo (neste caso, GPT-2):"),xn.forEach(s),Ca=u(e),c(xe.$$.fragment,e),Da=u(e),Qe=l(e,"P",{});var vn=p(Qe);ht=r(vn,"Por mais que iremos treinar um novo tokenizador, \xE9 uma boa ideia fazer isso para evitar come\xE7ar do zero. Dessa forma, n\xE3o precisaremos especificar nada sobre o algoritmo de tokeniza\xE7\xE3o ou tokens especiais que queremos usar; nosso novo tokenizador ser\xE1 exatamente igual ao GPT-2, e a \xFAnica coisa que ir\xE1 mudar \xE9 o vocabul\xE1rio, que ser\xE1 determinado pelo treinamento em nosso corpus."),vn.forEach(s),Oa=u(e),Ye=l(e,"P",{});var _n=p(Ye);gt=r(_n,"Primeiramente, vamos dar uma olhada em como o tokenizador trataria um exemplo de fun\xE7\xE3o:"),_n.forEach(s),Ta=u(e),c(ve.$$.fragment,e),Sa=u(e),c(_e.$$.fragment,e),La=u(e),E=l(e,"P",{});var J=p(E);xt=r(J,"O tokenizador possui alguns s\xEDmbolos especiais, como "),zs=l(J,"CODE",{});var kn=p(zs);vt=r(kn,"\u0120"),kn.forEach(s),_t=r(J," e "),qs=l(J,"CODE",{});var bn=p(qs);kt=r(bn,"\u010A"),bn.forEach(s),bt=r(J,", que denotam espa\xE7os e novas linhas, respectivamente. Como podemos observar, isso n\xE3o \xE9 t\xE3o eficiente: o tokenizador retorna tokens individuais para cada espa\xE7o, quando poderia agrupar n\xEDveis de indenta\xE7\xE3o (j\xE1 que ter conjuntos de quatro ou oito espa\xE7os ser\xE1 muito comum no c\xF3digo). O tokenizador tamb\xE9m dividiu o nome da fun\xE7\xE3o de uma forma um pouco estranha, n\xE3o sendo usado para ver palavras com o caractere "),ws=l(J,"CODE",{});var $n=p(ws);$t=r($n,"_"),$n.forEach(s),jt=r(J,"."),J.forEach(s),Na=u(e),K=l(e,"P",{});var ho=p(K);zt=r(ho,"Vamos treinar um novo tokenizador e ver se isso resolve esses problemas. Para isso, iremos utilizar o m\xE9todo "),Es=l(ho,"CODE",{});var jn=p(Es);qt=r(jn,"train_new_from_iterator()"),jn.forEach(s),wt=r(ho,":"),ho.forEach(s),Ga=u(e),c(ke.$$.fragment,e),Ua=u(e),Fe=l(e,"P",{});var zn=p(Fe);Et=r(zn,"Este comando pode demorar um pouco se o seu corpus for muito grande, mas para este dataset contendo 1.6 GB de textos \xE9 extremamente r\xE1pido (1 minuto e 16 segundos em uma CPU AMD Ryzen 9 3900X com 12 n\xFAcleos)."),zn.forEach(s),Ia=u(e),O=l(e,"P",{});var rs=p(O);Pt=r(rs,"Observe que "),Ps=l(rs,"CODE",{});var qn=p(Ps);At=r(qn,"AutoTokenizer.train_new_from_iterator()"),qn.forEach(s),yt=r(rs," funciona apenas se o tokenizador que voc\xEA estiver usando \xE9 um tokenizador \u201Cr\xE1pido\u201D. Como voc\xEA ver\xE1 na pr\xF3xima se\xE7\xE3o, a biblioteca \u{1F917} Transformers cont\xE9m dois tipos de tokenizers: alguns s\xE3o escritos puramente em Python e outros (os mais r\xE1pidos) s\xE3o apoiados pela biblioteca \u{1F917} Tokenizers, que \xE9 escrita na linguagem de programa\xE7\xE3o "),be=l(rs,"A",{href:!0,rel:!0});var wn=p(be);Ct=r(wn,"Rust"),wn.forEach(s),Dt=r(rs,". Python \xE9 a linguagem de programa\xE7\xE3o mais utilizada para Ci\xEAncia de Dados e aplica\xE7\xF5es em Deep Learning, mas quando algo precisa ser paralelizado para ser r\xE1pido, \xE9 preciso ser escrito em uma outra linguagem. Por exemplo, as multiplica\xE7\xF5es de matrizes que est\xE3o na base de modelos de computa\xE7\xE3o s\xE3o escritos em CUDA, uma biblioteca em C otimizada para GPUs."),rs.forEach(s),Ma=u(e),Q=l(e,"P",{});var go=p(Q);Ot=r(go,"Treinar um tokenizador totalmente novo usando apenas Python seria terrivelmente lento, e \xE9 por isso que n\xF3s desenvolvemos a biblioteca \u{1F917} Tokenizers. Observe que, assim como voc\xEA n\xE3o precisou aprender a linguagem CUDA para ser capaz de executar seu modelo em um lote de entradas em uma GPU, voc\xEA n\xE3o precisar\xE1 aprender Rust para usar o tokenizador r\xE1pido. A biblioteca \u{1F917} Tokenizers fornece liga\xE7\xE7\xF5es para muitos m\xE9todos que internamente chamam algum trecho de c\xF3digo em Rust; por exemplo, para paralelizar o treinamento do seu novo tokenizador ou, como vimos no "),Xe=l(go,"A",{href:!0});var En=p(Xe);Tt=r(En,"Chapter 3"),En.forEach(s),St=r(go,", a tokeniza\xE7\xE3o de um lote de entradas."),go.forEach(s),Ra=u(e),T=l(e,"P",{});var ns=p(T);Lt=r(ns,"A maioria dos modelos Transformer possui um tokenizador r\xE1pido dispon\xEDvel (existem algumas exce\xE7\xF5es que voc\xEA pode checar "),$e=l(ns,"A",{href:!0,rel:!0});var Pn=p($e);Nt=r(Pn,"aqui"),Pn.forEach(s),Gt=r(ns,"), e a API "),As=l(ns,"CODE",{});var An=p(As);Ut=r(An,"AutoTokenizer"),An.forEach(s),It=r(ns," sempre seleciona o tokenizador r\xE1pido para voc\xEA se estiver dispon\xEDvel. Na pr\xF3xima se\xE7\xE3o, veremos alguns dos outros recursos especiais que os tokenizers r\xE1pidos possuem, que ser\xE3o realmente \xFAteis para tarefas como classifica\xE7\xE3o de tokens e resposta a perguntas. Antes de aprofundarmos nisso, no entanto, vamos experimentar o nosso novo tokenizador no exemplo anterior:"),ns.forEach(s),Ha=u(e),c(je.$$.fragment,e),Va=u(e),c(ze.$$.fragment,e),Ba=u(e),j=l(e,"P",{});var P=p(j);Mt=r(P,"Aqui vemos novamente os s\xEDmbolos especiais "),ys=l(P,"CODE",{});var yn=p(ys);Rt=r(yn,"\u0120"),yn.forEach(s),Ht=r(P," and "),Cs=l(P,"CODE",{});var Cn=p(Cs);Vt=r(Cn,"\u010A"),Cn.forEach(s),Bt=r(P," que denotam espa\xE7os e novas linhas, mas tamb\xE9m podemos observar que o nosso tokenizador aprendeu alguns tokens que s\xE3o altamente espec\xEDficos em um corpus de fun\xE7\xF5es em Python: por exemplo, existe um token "),Ds=l(P,"CODE",{});var Dn=p(Ds);Kt=r(Dn,"\u010A\u0120\u0120\u0120"),Dn.forEach(s),Qt=r(P," que representa uma indenta\xE7\xE3o, e um token "),Os=l(P,"CODE",{});var On=p(Os);Yt=r(On,'\u0120"""'),On.forEach(s),Ft=r(P," que representa as tr\xEAs aspas que come\xE7am uma docstring. O tokenizador tamb\xE9m divide corretamente o nome da fun\xE7\xE3o em "),Ts=l(P,"CODE",{});var Tn=p(Ts);Xt=r(Tn,"_"),Tn.forEach(s),Jt=r(P,". Esta \xE9 uma representa\xE7\xE3o bastante compacta; comparativamente, usando o tokenizador em ingl\xEAs no mesmo exemplo nos dar\xE1 uma frase mais longa:"),P.forEach(s),Ka=u(e),c(qe.$$.fragment,e),Qa=u(e),c(we.$$.fragment,e),Ya=u(e),Je=l(e,"P",{});var Sn=p(Je);Zt=r(Sn,"Vejamos outro exemplo:"),Sn.forEach(s),Fa=u(e),c(Ee.$$.fragment,e),Xa=u(e),c(Pe.$$.fragment,e),Ja=u(e),k=l(e,"P",{});var b=p(k);Wt=r(b,"Al\xE9m do token correpondente a uma indenta\xE7\xE3o, aqui podemos ver um token para uma indenta\xE7\xE3o dupla: "),Ss=l(b,"CODE",{});var Ln=p(Ss);er=r(Ln,"\u010A\u0120\u0120\u0120\u0120\u0120\u0120\u0120"),Ln.forEach(s),sr=r(b,". Palavras especiais em Python, como "),Ls=l(b,"CODE",{});var Nn=p(Ls);ar=r(Nn,"class"),Nn.forEach(s),or=r(b,", "),Ns=l(b,"CODE",{});var Gn=p(Ns);tr=r(Gn,"init"),Gn.forEach(s),rr=r(b,", "),Gs=l(b,"CODE",{});var Un=p(Gs);nr=r(Un,"call"),Un.forEach(s),ir=r(b,", "),Us=l(b,"CODE",{});var In=p(Us);lr=r(In,"self"),In.forEach(s),pr=r(b,", e "),Is=l(b,"CODE",{});var Mn=p(Is);mr=r(Mn,"return"),Mn.forEach(s),ur=r(b," s\xE3o tokenizadas como um token, e podemos ver que al\xE9m de dividir em "),Ms=l(b,"CODE",{});var Rn=p(Ms);dr=r(Rn,"_"),Rn.forEach(s),cr=r(b," e "),Rs=l(b,"CODE",{});var Hn=p(Rs);fr=r(Hn,"."),Hn.forEach(s),hr=r(b,", o tokenizador divide corretamente at\xE9 mesmo nomes em CamelCase: "),Hs=l(b,"CODE",{});var Vn=p(Hs);gr=r(Vn,"LinearLayer"),Vn.forEach(s),xr=r(b," \xE9 tokenizado como "),Vs=l(b,"CODE",{});var Bn=p(Vs);vr=r(Bn,'["\u0120Linear", "Layer"]'),Bn.forEach(s),b.forEach(s),Za=u(e),I=l(e,"H2",{class:!0});var xo=p(I);Y=l(xo,"A",{id:!0,class:!0,href:!0});var Kn=p(Y);Bs=l(Kn,"SPAN",{});var Qn=p(Bs);c(Ae.$$.fragment,Qn),Qn.forEach(s),Kn.forEach(s),_r=u(xo),Ks=l(xo,"SPAN",{});var Yn=p(Ks);kr=r(Yn,"Salvando o tokenizador"),Yn.forEach(s),xo.forEach(s),Wa=u(e),F=l(e,"P",{});var vo=p(F);br=r(vo,"Para garantir que podemos us\xE1-lo mais tarde, precisamos salvar nosso novo tokenizador. Assim como \xE9 utilizado para modelos, isso \xE9 feito com o m\xE9todo "),Qs=l(vo,"CODE",{});var Fn=p(Qs);$r=r(Fn,"save_pretrained()"),Fn.forEach(s),jr=r(vo,":"),vo.forEach(s),eo=u(e),c(ye.$$.fragment,e),so=u(e),X=l(e,"P",{});var _o=p(X);zr=r(_o,"Isso ir\xE1 criar uma nova pasta chamada "),Ys=l(_o,"EM",{});var Xn=p(Ys);qr=r(Xn,"code-search-net-tokenizer"),Xn.forEach(s),wr=r(_o,", que ir\xE1 conter todos os arquivos que o tokenizador precisa para ser recarregado. Se voc\xEA quiser compartilhar este tokenizador com outros colegas e amigos, voc\xEA pode carreg\xE1-lo no Hub fazendo login em sua conta. Se voc\xEA estiver trabalhando em um notebook, h\xE1 uma fun\xE7\xE3o conveniente para ajud\xE1-lo com isso:"),_o.forEach(s),ao=u(e),c(Ce.$$.fragment,e),oo=u(e),Ze=l(e,"P",{});var Jn=p(Ze);Er=r(Jn,"Isso exibir\xE1 um widget onde voc\xEA pode inserir suas credenciais de login do Hugging Face. Se voc\xEA n\xE3o estiver trabalhando em um notebook, basta digitar a seguinte linha em seu terminal:"),Jn.forEach(s),to=u(e),c(De.$$.fragment,e),ro=u(e),We=l(e,"P",{});var Zn=p(We);Pr=r(Zn,"Depois de voc\xEA logar, voc\xEA pode enviar seu tokenizador executando o seguinte comando:"),Zn.forEach(s),no=u(e),c(Oe.$$.fragment,e),io=u(e),S=l(e,"P",{});var is=p(S);Ar=r(is,"Isso criar\xE1 um novo reposit\xF3rio em seu namespace com o nome "),Fs=l(is,"CODE",{});var Wn=p(Fs);yr=r(Wn,"code-search-net-tokenizer"),Wn.forEach(s),Cr=r(is,", contendo o arquivo do tokenizador. Voc\xEA pode ent\xE3o carregar o tokenizador de qualquer lugar com o m\xE9todo "),Xs=l(is,"CODE",{});var ei=p(Xs);Dr=r(ei,"from_pretrained()"),ei.forEach(s),Or=r(is,":"),is.forEach(s),lo=u(e),c(Te.$$.fragment,e),po=u(e),L=l(e,"P",{});var ls=p(L);Tr=r(ls,"Agora voc\xEA est\xE1 pronto para treinar um modelo de linguagem do zero e ajust\xE1-lo para sua tarefa! Chegaremos a isso no "),es=l(ls,"A",{href:!0});var si=p(es);Sr=r(si,"Chapter 7"),si.forEach(s),Lr=r(ls,", mas primeiro, no resto do cap\xEDtulo daremos uma olhada sobre tokenizers r\xE1pidos e explorar em detalhes o que realmente acontece quando chamamos o m\xE9todo "),Js=l(ls,"CODE",{});var ai=p(Js);Nr=r(ai,"train_new_from_iterator()"),ai.forEach(s),Gr=r(ls,"."),ls.forEach(s),this.h()},h(){v($,"name","hf:doc:metadata"),v($,"content",JSON.stringify(ci)),v(w,"id","treinando-um-novo-tokenizador"),v(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(w,"href","#treinando-um-novo-tokenizador"),v(z,"class","relative group"),v(Le,"href","/course/chapter2"),v(H,"id","montando-um-corpus"),v(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(H,"href","#montando-um-corpus"),v(G,"class","relative group"),v(ae,"href","https://github.com/huggingface/datasets"),v(ae,"rel","nofollow"),v(oe,"href","https://huggingface.co/datasets/code_search_net"),v(oe,"rel","nofollow"),v(te,"href","https://wandb.ai/github/CodeSearchNet/benchmark"),v(te,"rel","nofollow"),v(B,"id","treinando-um-novo-tokenizador"),v(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(B,"href","#treinando-um-novo-tokenizador"),v(U,"class","relative group"),v(be,"href","https://www.rust-lang.org"),v(be,"rel","nofollow"),v(Xe,"href","/course/chapter3"),v($e,"href","https://huggingface.co/transformers/#supported-frameworks"),v($e,"rel","nofollow"),v(Y,"id","salvando-o-tokenizador"),v(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Y,"href","#salvando-o-tokenizador"),v(I,"class","relative group"),v(es,"href","/course/chapter7")},m(e,o){a(document.head,$),n(e,M,o),n(e,z,o),a(z,w),a(w,ps),f(Z,ps,null),a(z,bo),a(z,ms),a(ms,$o),n(e,Ws,o),f(W,e,o),n(e,ea,o),n(e,A,o),a(A,jo),a(A,Le),a(Le,zo),a(A,qo),a(A,us),a(us,wo),a(A,Eo),n(e,sa,o),f(ee,e,o),n(e,aa,o),f(R,e,o),n(e,oa,o),n(e,G,o),a(G,H),a(H,ds),f(se,ds,null),a(G,Po),a(G,cs),a(cs,Ao),n(e,ta,o),n(e,V,o),a(V,yo),a(V,fs),a(fs,Co),a(V,Do),n(e,ra,o),n(e,q,o),a(q,Oo),a(q,ae),a(ae,To),a(q,So),a(q,hs),a(hs,Lo),a(q,No),a(q,oe),a(oe,Go),a(q,Uo),a(q,te),a(te,Io),a(q,Mo),n(e,na,o),f(re,e,o),n(e,ia,o),n(e,Ne,o),a(Ne,Ro),n(e,la,o),f(ne,e,o),n(e,pa,o),f(ie,e,o),n(e,ma,o),n(e,y,o),a(y,Ho),a(y,gs),a(gs,Vo),a(y,Bo),a(y,xs),a(xs,Ko),a(y,Qo),n(e,ua,o),f(le,e,o),n(e,da,o),n(e,Ge,o),a(Ge,Yo),n(e,ca,o),f(pe,e,o),n(e,fa,o),n(e,Ue,o),a(Ue,Fo),n(e,ha,o),n(e,Ie,o),a(Ie,Xo),n(e,ga,o),f(me,e,o),n(e,xa,o),n(e,Me,o),a(Me,Jo),n(e,va,o),f(ue,e,o),n(e,_a,o),n(e,C,o),a(C,Zo),a(C,vs),a(vs,Wo),a(C,et),a(C,_s),a(_s,st),a(C,at),n(e,ka,o),n(e,Re,o),a(Re,ot),n(e,ba,o),f(de,e,o),n(e,$a,o),n(e,He,o),a(He,tt),n(e,ja,o),f(ce,e,o),n(e,za,o),n(e,Ve,o),a(Ve,rt),n(e,qa,o),f(fe,e,o),n(e,wa,o),n(e,D,o),a(D,nt),a(D,ks),a(ks,it),a(D,lt),a(D,bs),a(bs,pt),a(D,mt),n(e,Ea,o),f(he,e,o),n(e,Pa,o),n(e,Be,o),a(Be,ut),n(e,Aa,o),n(e,U,o),a(U,B),a(B,$s),f(ge,$s,null),a(U,dt),a(U,js),a(js,ct),n(e,ya,o),n(e,Ke,o),a(Ke,ft),n(e,Ca,o),f(xe,e,o),n(e,Da,o),n(e,Qe,o),a(Qe,ht),n(e,Oa,o),n(e,Ye,o),a(Ye,gt),n(e,Ta,o),f(ve,e,o),n(e,Sa,o),f(_e,e,o),n(e,La,o),n(e,E,o),a(E,xt),a(E,zs),a(zs,vt),a(E,_t),a(E,qs),a(qs,kt),a(E,bt),a(E,ws),a(ws,$t),a(E,jt),n(e,Na,o),n(e,K,o),a(K,zt),a(K,Es),a(Es,qt),a(K,wt),n(e,Ga,o),f(ke,e,o),n(e,Ua,o),n(e,Fe,o),a(Fe,Et),n(e,Ia,o),n(e,O,o),a(O,Pt),a(O,Ps),a(Ps,At),a(O,yt),a(O,be),a(be,Ct),a(O,Dt),n(e,Ma,o),n(e,Q,o),a(Q,Ot),a(Q,Xe),a(Xe,Tt),a(Q,St),n(e,Ra,o),n(e,T,o),a(T,Lt),a(T,$e),a($e,Nt),a(T,Gt),a(T,As),a(As,Ut),a(T,It),n(e,Ha,o),f(je,e,o),n(e,Va,o),f(ze,e,o),n(e,Ba,o),n(e,j,o),a(j,Mt),a(j,ys),a(ys,Rt),a(j,Ht),a(j,Cs),a(Cs,Vt),a(j,Bt),a(j,Ds),a(Ds,Kt),a(j,Qt),a(j,Os),a(Os,Yt),a(j,Ft),a(j,Ts),a(Ts,Xt),a(j,Jt),n(e,Ka,o),f(qe,e,o),n(e,Qa,o),f(we,e,o),n(e,Ya,o),n(e,Je,o),a(Je,Zt),n(e,Fa,o),f(Ee,e,o),n(e,Xa,o),f(Pe,e,o),n(e,Ja,o),n(e,k,o),a(k,Wt),a(k,Ss),a(Ss,er),a(k,sr),a(k,Ls),a(Ls,ar),a(k,or),a(k,Ns),a(Ns,tr),a(k,rr),a(k,Gs),a(Gs,nr),a(k,ir),a(k,Us),a(Us,lr),a(k,pr),a(k,Is),a(Is,mr),a(k,ur),a(k,Ms),a(Ms,dr),a(k,cr),a(k,Rs),a(Rs,fr),a(k,hr),a(k,Hs),a(Hs,gr),a(k,xr),a(k,Vs),a(Vs,vr),n(e,Za,o),n(e,I,o),a(I,Y),a(Y,Bs),f(Ae,Bs,null),a(I,_r),a(I,Ks),a(Ks,kr),n(e,Wa,o),n(e,F,o),a(F,br),a(F,Qs),a(Qs,$r),a(F,jr),n(e,eo,o),f(ye,e,o),n(e,so,o),n(e,X,o),a(X,zr),a(X,Ys),a(Ys,qr),a(X,wr),n(e,ao,o),f(Ce,e,o),n(e,oo,o),n(e,Ze,o),a(Ze,Er),n(e,to,o),f(De,e,o),n(e,ro,o),n(e,We,o),a(We,Pr),n(e,no,o),f(Oe,e,o),n(e,io,o),n(e,S,o),a(S,Ar),a(S,Fs),a(Fs,yr),a(S,Cr),a(S,Xs),a(Xs,Dr),a(S,Or),n(e,lo,o),f(Te,e,o),n(e,po,o),n(e,L,o),a(L,Tr),a(L,es),a(es,Sr),a(L,Lr),a(L,Js),a(Js,Nr),a(L,Gr),mo=!0},p(e,[o]){const Se={};o&2&&(Se.$$scope={dirty:o,ctx:e}),R.$set(Se)},i(e){mo||(h(Z.$$.fragment,e),h(W.$$.fragment,e),h(ee.$$.fragment,e),h(R.$$.fragment,e),h(se.$$.fragment,e),h(re.$$.fragment,e),h(ne.$$.fragment,e),h(ie.$$.fragment,e),h(le.$$.fragment,e),h(pe.$$.fragment,e),h(me.$$.fragment,e),h(ue.$$.fragment,e),h(de.$$.fragment,e),h(ce.$$.fragment,e),h(fe.$$.fragment,e),h(he.$$.fragment,e),h(ge.$$.fragment,e),h(xe.$$.fragment,e),h(ve.$$.fragment,e),h(_e.$$.fragment,e),h(ke.$$.fragment,e),h(je.$$.fragment,e),h(ze.$$.fragment,e),h(qe.$$.fragment,e),h(we.$$.fragment,e),h(Ee.$$.fragment,e),h(Pe.$$.fragment,e),h(Ae.$$.fragment,e),h(ye.$$.fragment,e),h(Ce.$$.fragment,e),h(De.$$.fragment,e),h(Oe.$$.fragment,e),h(Te.$$.fragment,e),mo=!0)},o(e){g(Z.$$.fragment,e),g(W.$$.fragment,e),g(ee.$$.fragment,e),g(R.$$.fragment,e),g(se.$$.fragment,e),g(re.$$.fragment,e),g(ne.$$.fragment,e),g(ie.$$.fragment,e),g(le.$$.fragment,e),g(pe.$$.fragment,e),g(me.$$.fragment,e),g(ue.$$.fragment,e),g(de.$$.fragment,e),g(ce.$$.fragment,e),g(fe.$$.fragment,e),g(he.$$.fragment,e),g(ge.$$.fragment,e),g(xe.$$.fragment,e),g(ve.$$.fragment,e),g(_e.$$.fragment,e),g(ke.$$.fragment,e),g(je.$$.fragment,e),g(ze.$$.fragment,e),g(qe.$$.fragment,e),g(we.$$.fragment,e),g(Ee.$$.fragment,e),g(Pe.$$.fragment,e),g(Ae.$$.fragment,e),g(ye.$$.fragment,e),g(Ce.$$.fragment,e),g(De.$$.fragment,e),g(Oe.$$.fragment,e),g(Te.$$.fragment,e),mo=!1},d(e){s($),e&&s(M),e&&s(z),x(Z),e&&s(Ws),x(W,e),e&&s(ea),e&&s(A),e&&s(sa),x(ee,e),e&&s(aa),x(R,e),e&&s(oa),e&&s(G),x(se),e&&s(ta),e&&s(V),e&&s(ra),e&&s(q),e&&s(na),x(re,e),e&&s(ia),e&&s(Ne),e&&s(la),x(ne,e),e&&s(pa),x(ie,e),e&&s(ma),e&&s(y),e&&s(ua),x(le,e),e&&s(da),e&&s(Ge),e&&s(ca),x(pe,e),e&&s(fa),e&&s(Ue),e&&s(ha),e&&s(Ie),e&&s(ga),x(me,e),e&&s(xa),e&&s(Me),e&&s(va),x(ue,e),e&&s(_a),e&&s(C),e&&s(ka),e&&s(Re),e&&s(ba),x(de,e),e&&s($a),e&&s(He),e&&s(ja),x(ce,e),e&&s(za),e&&s(Ve),e&&s(qa),x(fe,e),e&&s(wa),e&&s(D),e&&s(Ea),x(he,e),e&&s(Pa),e&&s(Be),e&&s(Aa),e&&s(U),x(ge),e&&s(ya),e&&s(Ke),e&&s(Ca),x(xe,e),e&&s(Da),e&&s(Qe),e&&s(Oa),e&&s(Ye),e&&s(Ta),x(ve,e),e&&s(Sa),x(_e,e),e&&s(La),e&&s(E),e&&s(Na),e&&s(K),e&&s(Ga),x(ke,e),e&&s(Ua),e&&s(Fe),e&&s(Ia),e&&s(O),e&&s(Ma),e&&s(Q),e&&s(Ra),e&&s(T),e&&s(Ha),x(je,e),e&&s(Va),x(ze,e),e&&s(Ba),e&&s(j),e&&s(Ka),x(qe,e),e&&s(Qa),x(we,e),e&&s(Ya),e&&s(Je),e&&s(Fa),x(Ee,e),e&&s(Xa),x(Pe,e),e&&s(Ja),e&&s(k),e&&s(Za),e&&s(I),x(Ae),e&&s(Wa),e&&s(F),e&&s(eo),x(ye,e),e&&s(so),e&&s(X),e&&s(ao),x(Ce,e),e&&s(oo),e&&s(Ze),e&&s(to),x(De,e),e&&s(ro),e&&s(We),e&&s(no),x(Oe,e),e&&s(io),e&&s(S),e&&s(lo),x(Te,e),e&&s(po),e&&s(L)}}}const ci={local:"treinando-um-novo-tokenizador",sections:[{local:"montando-um-corpus",title:"Montando um corpus"},{local:"treinando-um-novo-tokenizador",title:"Treinando um novo tokenizador"},{local:"salvando-o-tokenizador",title:"Salvando o tokenizador"}],title:"Treinando um novo tokenizador"};function fi(Zs){return ii(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bi extends oi{constructor($){super();ti(this,$,fi,di,ri,{})}}export{bi as default,ci as metadata};
