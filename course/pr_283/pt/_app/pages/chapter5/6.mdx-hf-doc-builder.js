import{S as Pm,i as Im,s as zm,e as n,k as d,w as x,t,M as Nm,c as r,d as a,m as p,x as E,a as l,h as o,b as w,N as Am,f as Ao,G as s,g as m,y as q,o as g,p as So,q as b,B as k,v as Fm,n as Oo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Sm}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Rm}from"../../chunks/Youtube-hf-doc-builder.js";import{I as _t}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as O}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Om}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Hm}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Mm(L){let c,$;return c=new Om({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"}]}}),{c(){x(c.$$.fragment)},l(f){E(c.$$.fragment,f)},m(f,j){q(c,f,j),$=!0},i(f){$||(b(c.$$.fragment,f),$=!0)},o(f){g(c.$$.fragment,f),$=!1},d(f){k(c,f)}}}function Lm(L){let c,$;return c=new Om({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"}]}}),{c(){x(c.$$.fragment)},l(f){E(c.$$.fragment,f)},m(f,j){q(c,f,j),$=!0},i(f){$||(b(c.$$.fragment,f),$=!0)},o(f){g(c.$$.fragment,f),$=!1},d(f){k(c,f)}}}function Um(L){let c,$,f,j,h,v,P,_,A,y,M,I,S,z,N,R,V,C,F,B;return{c(){c=n("p"),$=t("\u270F\uFE0F "),f=n("strong"),j=t("Experimente!"),h=t(" Veja se voc\xEA pode usar "),v=n("code"),P=t("Dataset.map()"),_=t(" para explodir a coluna "),A=n("code"),y=t("comments"),M=t(" de "),I=n("code"),S=t("issues_dataset"),z=d(),N=n("em"),R=t("sem"),V=t(" recorrer ao uso de Pandas. Isso \xE9 um pouco complicado; voc\xEA pode achar \xFAtil para esta tarefa a se\xE7\xE3o "),C=n("a"),F=t("\u201CMapeamento em lote\u201D"),B=t(" da documenta\xE7\xE3o do \u{1F917} Dataset."),this.h()},l(U){c=r(U,"P",{});var D=l(c);$=o(D,"\u270F\uFE0F "),f=r(D,"STRONG",{});var G=l(f);j=o(G,"Experimente!"),G.forEach(a),h=o(D," Veja se voc\xEA pode usar "),v=r(D,"CODE",{});var u=l(v);P=o(u,"Dataset.map()"),u.forEach(a),_=o(D," para explodir a coluna "),A=r(D,"CODE",{});var T=l(A);y=o(T,"comments"),T.forEach(a),M=o(D," de "),I=r(D,"CODE",{});var H=l(I);S=o(H,"issues_dataset"),H.forEach(a),z=p(D),N=r(D,"EM",{});var W=l(N);R=o(W,"sem"),W.forEach(a),V=o(D," recorrer ao uso de Pandas. Isso \xE9 um pouco complicado; voc\xEA pode achar \xFAtil para esta tarefa a se\xE7\xE3o "),C=r(D,"A",{href:!0,rel:!0});var ne=l(C);F=o(ne,"\u201CMapeamento em lote\u201D"),ne.forEach(a),B=o(D," da documenta\xE7\xE3o do \u{1F917} Dataset."),D.forEach(a),this.h()},h(){w(C,"href","https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping"),w(C,"rel","nofollow")},m(U,D){m(U,c,D),s(c,$),s(c,f),s(f,j),s(c,h),s(c,v),s(v,P),s(c,_),s(c,A),s(A,y),s(c,M),s(c,I),s(I,S),s(c,z),s(c,N),s(N,R),s(c,V),s(c,C),s(C,F),s(c,B)},d(U){U&&a(c)}}}function Vm(L){let c,$,f,j,h,v,P,_,A,y,M,I,S,z,N,R,V;return c=new O({props:{code:`from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){x(c.$$.fragment),$=d(),f=n("p"),j=t("Observe que definimos "),h=n("code"),v=t("from_pt=True"),P=t(" como um argumento do m\xE9todo "),_=n("code"),A=t("from_pretrained()"),y=t(". Isso ocorre porque o checkpoint "),M=n("code"),I=t("multi-qa-mpnet-base-dot-v1"),S=t(" s\xF3 tem pesos PyTorch, portanto, definir "),z=n("code"),N=t("from_pt=True"),R=t(" ir\xE1 convert\xEA-los automaticamente para o formato TensorFlow para n\xF3s. Como voc\xEA pode ver, \xE9 muito simples alternar entre frameworks no \u{1F917} Transformers!")},l(C){E(c.$$.fragment,C),$=p(C),f=r(C,"P",{});var F=l(f);j=o(F,"Observe que definimos "),h=r(F,"CODE",{});var B=l(h);v=o(B,"from_pt=True"),B.forEach(a),P=o(F," como um argumento do m\xE9todo "),_=r(F,"CODE",{});var U=l(_);A=o(U,"from_pretrained()"),U.forEach(a),y=o(F,". Isso ocorre porque o checkpoint "),M=r(F,"CODE",{});var D=l(M);I=o(D,"multi-qa-mpnet-base-dot-v1"),D.forEach(a),S=o(F," s\xF3 tem pesos PyTorch, portanto, definir "),z=r(F,"CODE",{});var G=l(z);N=o(G,"from_pt=True"),G.forEach(a),R=o(F," ir\xE1 convert\xEA-los automaticamente para o formato TensorFlow para n\xF3s. Como voc\xEA pode ver, \xE9 muito simples alternar entre frameworks no \u{1F917} Transformers!"),F.forEach(a)},m(C,F){q(c,C,F),m(C,$,F),m(C,f,F),s(f,j),s(f,h),s(h,v),s(f,P),s(f,_),s(_,A),s(f,y),s(f,M),s(M,I),s(f,S),s(f,z),s(z,N),s(f,R),V=!0},i(C){V||(b(c.$$.fragment,C),V=!0)},o(C){g(c.$$.fragment,C),V=!1},d(C){k(c,C),C&&a($),C&&a(f)}}}function Gm(L){let c,$,f,j,h,v,P;return c=new O({props:{code:`from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`}}),v=new O({props:{code:`import torch

device = torch.device("cuda")
model.to(device)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)
model.to(device)`}}),{c(){x(c.$$.fragment),$=d(),f=n("p"),j=t("Para acelerar o processo de embedding, \xE9 \xFAtil colocar o modelo e as entradas em um dispositivo GPU, ent\xE3o vamos fazer isso agora:"),h=d(),x(v.$$.fragment)},l(_){E(c.$$.fragment,_),$=p(_),f=r(_,"P",{});var A=l(f);j=o(A,"Para acelerar o processo de embedding, \xE9 \xFAtil colocar o modelo e as entradas em um dispositivo GPU, ent\xE3o vamos fazer isso agora:"),A.forEach(a),h=p(_),E(v.$$.fragment,_)},m(_,A){q(c,_,A),m(_,$,A),m(_,f,A),s(f,j),m(_,h,A),q(v,_,A),P=!0},i(_){P||(b(c.$$.fragment,_),b(v.$$.fragment,_),P=!0)},o(_){g(c.$$.fragment,_),g(v.$$.fragment,_),P=!1},d(_){k(c,_),_&&a($),_&&a(f),_&&a(h),k(v,_)}}}function Ym(L){let c,$,f,j,h,v,P,_,A,y,M,I,S,z,N,R,V,C,F,B,U,D,G;return c=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
    )
    encoded_input = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),_=new O({props:{code:"TensorShape([1, 768])",highlighted:'TensorShape([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),D=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){x(c.$$.fragment),$=d(),f=n("p"),j=t("Podemos testar o funcionamento da fun\xE7\xE3o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa\xEDda:"),h=d(),x(v.$$.fragment),P=d(),x(_.$$.fragment),A=d(),y=n("p"),M=t("\xD3timo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens\xF5es! Podemos usar "),I=n("code"),S=t("Dataset.map()"),z=t(" para aplicar nossa fun\xE7\xE3o "),N=n("code"),R=t("get_embeddings()"),V=t(" a cada linha em nosso corpus, ent\xE3o vamos criar uma nova coluna "),C=n("code"),F=t("embeddings"),B=t(" da seguinte forma:"),U=d(),x(D.$$.fragment)},l(u){E(c.$$.fragment,u),$=p(u),f=r(u,"P",{});var T=l(f);j=o(T,"Podemos testar o funcionamento da fun\xE7\xE3o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa\xEDda:"),T.forEach(a),h=p(u),E(v.$$.fragment,u),P=p(u),E(_.$$.fragment,u),A=p(u),y=r(u,"P",{});var H=l(y);M=o(H,"\xD3timo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens\xF5es! Podemos usar "),I=r(H,"CODE",{});var W=l(I);S=o(W,"Dataset.map()"),W.forEach(a),z=o(H," para aplicar nossa fun\xE7\xE3o "),N=r(H,"CODE",{});var ne=l(N);R=o(ne,"get_embeddings()"),ne.forEach(a),V=o(H," a cada linha em nosso corpus, ent\xE3o vamos criar uma nova coluna "),C=r(H,"CODE",{});var me=l(C);F=o(me,"embeddings"),me.forEach(a),B=o(H," da seguinte forma:"),H.forEach(a),U=p(u),E(D.$$.fragment,u)},m(u,T){q(c,u,T),m(u,$,T),m(u,f,T),s(f,j),m(u,h,T),q(v,u,T),m(u,P,T),q(_,u,T),m(u,A,T),m(u,y,T),s(y,M),s(y,I),s(I,S),s(y,z),s(y,N),s(N,R),s(y,V),s(y,C),s(C,F),s(y,B),m(u,U,T),q(D,u,T),G=!0},i(u){G||(b(c.$$.fragment,u),b(v.$$.fragment,u),b(_.$$.fragment,u),b(D.$$.fragment,u),G=!0)},o(u){g(c.$$.fragment,u),g(v.$$.fragment,u),g(_.$$.fragment,u),g(D.$$.fragment,u),G=!1},d(u){k(c,u),u&&a($),u&&a(f),u&&a(h),k(v,u),u&&a(P),k(_,u),u&&a(A),u&&a(y),u&&a(U),k(D,u)}}}function Bm(L){let c,$,f,j,h,v,P,_,A,y,M,I,S,z,N,R,V,C,F,B,U,D,G;return c=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
    )
    encoded_input = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),_=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),D=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){x(c.$$.fragment),$=d(),f=n("p"),j=t("Podemos testar o funcionamento da fun\xE7\xE3o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa\xEDda:"),h=d(),x(v.$$.fragment),P=d(),x(_.$$.fragment),A=d(),y=n("p"),M=t("\xD3timo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens\xF5es! Podemos usar "),I=n("code"),S=t("Dataset.map()"),z=t(" para aplicar nossa fun\xE7\xE3o "),N=n("code"),R=t("get_embeddings()"),V=t(" a cada linha em nosso corpus, ent\xE3o vamos criar uma nova coluna "),C=n("code"),F=t("embeddings"),B=t(" da seguinte forma:"),U=d(),x(D.$$.fragment)},l(u){E(c.$$.fragment,u),$=p(u),f=r(u,"P",{});var T=l(f);j=o(T,"Podemos testar o funcionamento da fun\xE7\xE3o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa\xEDda:"),T.forEach(a),h=p(u),E(v.$$.fragment,u),P=p(u),E(_.$$.fragment,u),A=p(u),y=r(u,"P",{});var H=l(y);M=o(H,"\xD3timo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens\xF5es! Podemos usar "),I=r(H,"CODE",{});var W=l(I);S=o(W,"Dataset.map()"),W.forEach(a),z=o(H," para aplicar nossa fun\xE7\xE3o "),N=r(H,"CODE",{});var ne=l(N);R=o(ne,"get_embeddings()"),ne.forEach(a),V=o(H," a cada linha em nosso corpus, ent\xE3o vamos criar uma nova coluna "),C=r(H,"CODE",{});var me=l(C);F=o(me,"embeddings"),me.forEach(a),B=o(H," da seguinte forma:"),H.forEach(a),U=p(u),E(D.$$.fragment,u)},m(u,T){q(c,u,T),m(u,$,T),m(u,f,T),s(f,j),m(u,h,T),q(v,u,T),m(u,P,T),q(_,u,T),m(u,A,T),m(u,y,T),s(y,M),s(y,I),s(I,S),s(y,z),s(y,N),s(N,R),s(y,V),s(y,C),s(C,F),s(y,B),m(u,U,T),q(D,u,T),G=!0},i(u){G||(b(c.$$.fragment,u),b(v.$$.fragment,u),b(_.$$.fragment,u),b(D.$$.fragment,u),G=!0)},o(u){g(c.$$.fragment,u),g(v.$$.fragment,u),g(_.$$.fragment,u),g(D.$$.fragment,u),G=!1},d(u){k(c,u),u&&a($),u&&a(f),u&&a(h),k(v,u),u&&a(P),k(_,u),u&&a(A),u&&a(y),u&&a(U),k(D,u)}}}function Wm(L){let c,$,f,j;return c=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`}}),f=new O({props:{code:"(1, 768)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">768</span>)'}}),{c(){x(c.$$.fragment),$=d(),x(f.$$.fragment)},l(h){E(c.$$.fragment,h),$=p(h),E(f.$$.fragment,h)},m(h,v){q(c,h,v),m(h,$,v),q(f,h,v),j=!0},i(h){j||(b(c.$$.fragment,h),b(f.$$.fragment,h),j=!0)},o(h){g(c.$$.fragment,h),g(f.$$.fragment,h),j=!1},d(h){k(c,h),h&&a($),k(f,h)}}}function Qm(L){let c,$,f,j;return c=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`}}),f=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),{c(){x(c.$$.fragment),$=d(),x(f.$$.fragment)},l(h){E(c.$$.fragment,h),$=p(h),E(f.$$.fragment,h)},m(h,v){q(c,h,v),m(h,$,v),q(f,h,v),j=!0},i(h){j||(b(c.$$.fragment,h),b(f.$$.fragment,h),j=!0)},o(h){g(c.$$.fragment,h),g(f.$$.fragment,h),j=!1},d(h){k(c,h),h&&a($),k(f,h)}}}function Jm(L){let c,$,f,j,h,v,P,_,A,y,M;return{c(){c=n("p"),$=t("\u270F\uFE0F "),f=n("strong"),j=t("Experimente!"),h=t(" Crie sua pr\xF3pria consulta e veja se consegue encontrar uma resposta nos documentos recuperados. Voc\xEA pode ter que aumentar o par\xE2metro "),v=n("code"),P=t("k"),_=t(" em "),A=n("code"),y=t("Dataset.get_nearest_examples()"),M=t(" para ampliar a pesquisa.")},l(I){c=r(I,"P",{});var S=l(c);$=o(S,"\u270F\uFE0F "),f=r(S,"STRONG",{});var z=l(f);j=o(z,"Experimente!"),z.forEach(a),h=o(S," Crie sua pr\xF3pria consulta e veja se consegue encontrar uma resposta nos documentos recuperados. Voc\xEA pode ter que aumentar o par\xE2metro "),v=r(S,"CODE",{});var N=l(v);P=o(N,"k"),N.forEach(a),_=o(S," em "),A=r(S,"CODE",{});var R=l(A);y=o(R,"Dataset.get_nearest_examples()"),R.forEach(a),M=o(S," para ampliar a pesquisa."),S.forEach(a)},m(I,S){m(I,c,S),s(c,$),s(c,f),s(f,j),s(c,h),s(c,v),s(v,P),s(c,_),s(c,A),s(A,y),s(c,M)},d(I){I&&a(c)}}}function Xm(L){let c,$,f,j,h,v,P,_,A,y,M,I,S,z,N,R,V,C,F,B,U,D,G,u,T,H,W,ne,me,Po,gt,ge,Io,Ts,zo,No,aa,Fo,Ro,bt,As,Ho,vt,Te,Ye,Rl,Mo,Be,Hl,$t,Ae,Pe,ta,We,Lo,oa,Uo,xt,Ss,Vo,Et,Qe,qt,be,Go,na,Yo,Bo,Os,Wo,Qo,kt,Je,wt,Xe,jt,Q,Jo,ra,Xo,Ko,la,Zo,en,ia,sn,an,ma,tn,on,da,nn,rn,yt,Ke,Dt,Ze,Ct,J,ln,pa,mn,dn,ca,pn,cn,ua,un,fn,fa,hn,_n,ha,gn,bn,Tt,es,At,ss,St,K,vn,_a,$n,xn,ga,En,qn,as,ba,kn,wn,va,jn,yn,Ot,ts,Pt,Ie,Dn,$a,Cn,Tn,It,os,zt,ns,Nt,ze,An,xa,Sn,On,Ft,rs,Rt,ee,Ea,Z,Ht,Pn,qa,In,zn,ka,Nn,Fn,wa,Rn,Hn,ja,Mn,Ln,de,se,ya,Un,Vn,Da,Gn,Yn,Ca,Bn,Wn,Ta,Qn,Jn,Aa,Xn,Kn,ae,Sa,Zn,er,Oa,sr,ar,Pa,tr,or,Ia,nr,rr,za,lr,ir,te,Na,mr,dr,Fa,pr,cr,Ra,ur,fr,Ha,hr,_r,Ma,gr,br,oe,La,vr,$r,Ua,xr,Er,Va,qr,kr,Ga,wr,jr,Ya,yr,Mt,re,Dr,Ba,Cr,Tr,Wa,Ar,Sr,Qa,Or,Pr,Lt,ls,Ut,is,Vt,Ps,Ir,Gt,Ne,Yt,Fe,zr,Ja,Nr,Fr,Bt,ms,Wt,Is,Rr,Qt,ds,Jt,ps,Xt,ve,Hr,Xa,Mr,Lr,Ka,Ur,Vr,Kt,cs,Zt,zs,Gr,eo,Se,Re,Za,us,Yr,et,Br,so,Y,Wr,Ns,Qr,Jr,st,Xr,Kr,at,Zr,el,fs,sl,al,tt,tl,ol,hs,nl,rl,ot,ll,il,ao,pe,ce,Fs,$e,ml,nt,dl,pl,rt,cl,ul,to,_s,oo,Rs,fl,no,ue,fe,Hs,Ms,hl,ro,Oe,He,lt,gs,_l,it,gl,lo,xe,bl,mt,vl,$l,bs,xl,El,io,Ee,ql,dt,kl,wl,pt,jl,yl,mo,vs,po,Me,Dl,ct,Cl,Tl,co,he,_e,Ls,Us,Al,uo,$s,fo,qe,Sl,ut,Ol,Pl,ft,Il,zl,ho,xs,_o,Vs,Nl,go,Es,bo,qs,vo,Gs,Fl,$o,Le,xo;f=new Hm({props:{fw:L[0]}}),_=new _t({});const Ml=[Lm,Mm],ks=[];function Ll(e,i){return e[0]==="pt"?0:1}S=Ll(L),z=ks[S]=Ml[S](L),D=new Rm({props:{id:"OATCgQtNX2o"}}),W=new _t({}),We=new _t({}),Qe=new O({props:{code:`from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-hf-doc-builder.jsonl",
    repo_type="dataset",
)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_url

data_files = hf_hub_url(
    repo_id=<span class="hljs-string">&quot;lewtun/github-issues&quot;</span>,
    filename=<span class="hljs-string">&quot;datasets-issues-with-hf-doc-builder.jsonl&quot;</span>,
    repo_type=<span class="hljs-string">&quot;dataset&quot;</span>,
)`}}),Je=new O({props:{code:`from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

issues_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
issues_dataset`}}),Xe=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">2855</span>
})`}}),Ke=new O({props:{code:`issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset`,highlighted:`issues_dataset = issues_dataset.<span class="hljs-built_in">filter</span>(
    <span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-string">&quot;is_pull_request&quot;</span>] == <span class="hljs-literal">False</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>]) &gt; <span class="hljs-number">0</span>)
)
issues_dataset`}}),Ze=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),es=new O({props:{code:`columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`,highlighted:`columns = issues_dataset.column_names
columns_to_keep = [<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;body&quot;</span>, <span class="hljs-string">&quot;html_url&quot;</span>, <span class="hljs-string">&quot;comments&quot;</span>]
columns_to_remove = <span class="hljs-built_in">set</span>(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`}}),ss=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),ts=new O({props:{code:`issues_dataset.set_format("pandas")
df = issues_dataset[:]`,highlighted:`issues_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
df = issues_dataset[:]`}}),os=new O({props:{code:'df["comments"][0].tolist()',highlighted:'df[<span class="hljs-string">&quot;comments&quot;</span>][<span class="hljs-number">0</span>].tolist()'}}),ns=new O({props:{code:`['the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',
 'cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']`,highlighted:`[<span class="hljs-string">&#x27;the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(&quot;glue&quot;, data_args.task_name, cache_dir=model_args.cache_dir)&#x27;</span>,
 <span class="hljs-string">&#x27;Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?&#x27;</span>,
 <span class="hljs-string">&#x27;cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002&#x27;</span>,
 <span class="hljs-string">&#x27;I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...&#x27;</span>]`}}),rs=new O({props:{code:`comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)`,highlighted:`comments_df = df.explode(<span class="hljs-string">&quot;comments&quot;</span>, ignore_index=<span class="hljs-literal">True</span>)
comments_df.head(<span class="hljs-number">4</span>)`}}),ls=new O({props:{code:`from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`}}),is=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">2842</span>
})`}}),Ne=new Sm({props:{$$slots:{default:[Um]},$$scope:{ctx:L}}}),ms=new O({props:{code:`comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;comment_length&quot;</span>: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>].split())}
)`}}),ds=new O({props:{code:`comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;comment_length&quot;</span>] &gt; <span class="hljs-number">15</span>)
comments_dataset`}}),ps=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;comment_length&#x27;</span>],
    num_rows: <span class="hljs-number">2098</span>
})`}}),cs=new O({props:{code:`def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \\n "
        + examples["body"]
        + " \\n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">concatenate_text</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;text&quot;</span>: examples[<span class="hljs-string">&quot;title&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;body&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;comments&quot;</span>]
    }


comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(concatenate_text)`}}),us=new _t({});const Ul=[Gm,Vm],ws=[];function Vl(e,i){return e[0]==="pt"?0:1}pe=Vl(L),ce=ws[pe]=Ul[pe](L),_s=new O({props:{code:`def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">cls_pooling</span>(<span class="hljs-params">model_output</span>):
    <span class="hljs-keyword">return</span> model_output.last_hidden_state[:, <span class="hljs-number">0</span>]`}});const Gl=[Bm,Ym],js=[];function Yl(e,i){return e[0]==="pt"?0:1}ue=Yl(L),fe=js[ue]=Gl[ue](L),gs=new _t({}),vs=new O({props:{code:'embeddings_dataset.add_faiss_index(column="embeddings")',highlighted:'embeddings_dataset.add_faiss_index(column=<span class="hljs-string">&quot;embeddings&quot;</span>)'}});const Bl=[Qm,Wm],ys=[];function Wl(e,i){return e[0]==="pt"?0:1}return he=Wl(L),_e=ys[he]=Bl[he](L),$s=new O({props:{code:`scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)`,highlighted:`scores, samples = embeddings_dataset.get_nearest_examples(
    <span class="hljs-string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="hljs-number">5</span>
)`}}),xs=new O({props:{code:`import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df[<span class="hljs-string">&quot;scores&quot;</span>] = scores
samples_df.sort_values(<span class="hljs-string">&quot;scores&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)`}}),Es=new O({props:{code:`for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()`,highlighted:`<span class="hljs-keyword">for</span> _, row <span class="hljs-keyword">in</span> samples_df.iterrows():
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;COMMENT: <span class="hljs-subst">{row.comments}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;SCORE: <span class="hljs-subst">{row.scores}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;TITLE: <span class="hljs-subst">{row.title}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;URL: <span class="hljs-subst">{row.html_url}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=&quot;</span> * <span class="hljs-number">50</span>)
    <span class="hljs-built_in">print</span>()`}}),qs=new O({props:{code:`"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset("text", data_files=data_files)
\\\`\\\`\\\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset("./my_dataset")
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> \`\`\`
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> \`\`\`
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset(&quot;text&quot;, data_files=data_files)
\\\`\\\`\\\`

We&#x27;ll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.

Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)

I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.

----------

&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset(&quot;./my_dataset&quot;)
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I&#x27;m looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine
&gt;
&gt; 1. (online machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_dataset(...)
&gt;
&gt; data.save_to_disk(/YOUR/DATASET/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt; 2. copy the dir from online to the offline machine
&gt;
&gt; 3. (offline machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_from_disk(/SAVED/DATA/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt;
&gt;
&gt; HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
&quot;&quot;&quot;</span>`}}),Le=new Sm({props:{$$slots:{default:[Jm]},$$scope:{ctx:L}}}),{c(){c=n("meta"),$=d(),x(f.$$.fragment),j=d(),h=n("h1"),v=n("a"),P=n("span"),x(_.$$.fragment),A=d(),y=n("span"),M=t("Busca sem\xE2ntica com o FAISS"),I=d(),z.c(),N=d(),R=n("p"),V=t("Na "),C=n("a"),F=t("se\xE7\xE3o 5"),B=t(", criamos um conjunto de dados de issues e coment\xE1rios do GitHub do reposit\xF3rio \u{1F917} Datasets. Nesta se\xE7\xE3o, usaremos essas informa\xE7\xF5es para construir um mecanismo de pesquisa que pode nos ajudar a encontrar respostas para nossas perguntas mais urgentes sobre a biblioteca!"),U=d(),x(D.$$.fragment),G=d(),u=n("h2"),T=n("a"),H=n("span"),x(W.$$.fragment),ne=d(),me=n("span"),Po=t("Usando embeddings para pesquisa sem\xE2ntica"),gt=d(),ge=n("p"),Io=t("Como vimos no "),Ts=n("a"),zo=t("Cap\xEDtulo 1"),No=t(", os modelos de linguagem baseados em Transformer representam cada token em um intervalo de texto como um "),aa=n("em"),Fo=t("vetor de incorpora\xE7\xE3o"),Ro=t(". Acontece que \xE9 poss\xEDvel \u201Cagrupar\u201D as incorpora\xE7\xF5es individuais para criar uma representa\xE7\xE3o vetorial para frases inteiras, par\xE1grafos ou (em alguns casos) documentos. Essas incorpora\xE7\xF5es podem ser usadas para encontrar documentos semelhantes no corpus calculando a similaridade do produto escalar (ou alguma outra m\xE9trica de similaridade) entre cada incorpora\xE7\xE3o e retornando os documentos com maior sobreposi\xE7\xE3o."),bt=d(),As=n("p"),Ho=t("Nesta se\xE7\xE3o, usaremos embeddings para desenvolver um mecanismo de pesquisa sem\xE2ntica. Esses mecanismos de pesquisa oferecem v\xE1rias vantagens sobre as abordagens convencionais que se baseiam na correspond\xEAncia de palavras-chave em uma consulta com os documentos."),vt=d(),Te=n("div"),Ye=n("img"),Mo=d(),Be=n("img"),$t=d(),Ae=n("h2"),Pe=n("a"),ta=n("span"),x(We.$$.fragment),Lo=d(),oa=n("span"),Uo=t("Carregando e preparando o conjunto de dados"),xt=d(),Ss=n("p"),Vo=t("A primeira coisa que precisamos fazer \xE9 baixar nosso conjunto de dados de issues do GitHub, ent\xE3o vamos usar a biblioteca \u{1F917} Hub para resolver a URL onde nosso arquivo est\xE1 armazenado no Hugging Face Hub:"),Et=d(),x(Qe.$$.fragment),qt=d(),be=n("p"),Go=t("Com a URL armazenada em "),na=n("code"),Yo=t("data_files"),Bo=t(", podemos carregar o conjunto de dados remoto usando o m\xE9todo apresentado na "),Os=n("a"),Wo=t("se\xE7\xE3o 2"),Qo=t(":"),kt=d(),x(Je.$$.fragment),wt=d(),x(Xe.$$.fragment),jt=d(),Q=n("p"),Jo=t("Aqui n\xF3s especificamos a divis\xE3o padr\xE3o "),ra=n("code"),Xo=t("train"),Ko=t(" em "),la=n("code"),Zo=t("load_dataset()"),en=t(", ent\xE3o ele retorna um "),ia=n("code"),sn=t("Dataset"),an=t(" em vez de um "),ma=n("code"),tn=t("DatasetDict"),on=t(". A primeira ordem de neg\xF3cios \xE9 filtrar os pull request, pois elas tendem a ser raramente usadas para responder a consultas de usu\xE1rios e introduzir\xE3o ru\xEDdo em nosso mecanismo de pesquisa. Como j\xE1 deve ser familiar, podemos usar a fun\xE7\xE3o "),da=n("code"),nn=t("Dataset.filter()"),rn=t(" para excluir essas linhas em nosso conjunto de dados. Enquanto estamos nisso, tamb\xE9m vamos filtrar as linhas sem coment\xE1rios, pois elas n\xE3o fornecem respostas \xE0s consultas dos usu\xE1rios:"),yt=d(),x(Ke.$$.fragment),Dt=d(),x(Ze.$$.fragment),Ct=d(),J=n("p"),ln=t("Podemos ver que h\xE1 muitas colunas em nosso conjunto de dados, a maioria das quais n\xE3o precisamos para construir nosso mecanismo de pesquisa. De uma perspectiva de pesquisa, as colunas mais informativas s\xE3o "),pa=n("code"),mn=t("title"),dn=t(", "),ca=n("code"),pn=t("body"),cn=t(" e "),ua=n("code"),un=t("comments"),fn=t(", enquanto "),fa=n("code"),hn=t("html_url"),_n=t(" nos fornece um link de volta para a issue de origem. Vamos usar a fun\xE7\xE3o "),ha=n("code"),gn=t("Dataset.remove_columns()"),bn=t(" para descartar o resto:"),Tt=d(),x(es.$$.fragment),At=d(),x(ss.$$.fragment),St=d(),K=n("p"),vn=t("Para criar nossos embeddings, aumentaremos cada coment\xE1rio com o t\xEDtulo e o corpo da issue, pois esses campos geralmente incluem informa\xE7\xF5es contextuais \xFAteis. Como nossa coluna "),_a=n("code"),$n=t("comments"),xn=t(" \xE9 atualmente uma lista de coment\xE1rios para cada issue, precisamos \u201Cexplodir\u201D a coluna para que cada linha consista em uma tupla "),ga=n("code"),En=t("(html_url, title, body, comment)"),qn=t(". No Pandas podemos fazer isso com a fun\xE7\xE3o "),as=n("a"),ba=n("code"),kn=t("DataFrame.explode()"),wn=t(", que cria uma nova linha para cada elemento em uma coluna semelhante a uma lista, enquanto replica todos os outros valores de coluna. Para ver isso em a\xE7\xE3o, vamos primeiro mudar para o formato "),va=n("code"),jn=t("DataFrame"),yn=t(" do Pandas:"),Ot=d(),x(ts.$$.fragment),Pt=d(),Ie=n("p"),Dn=t("Se inspecionarmos a primeira linha neste "),$a=n("code"),Cn=t("DataFrame"),Tn=t(", podemos ver que h\xE1 quatro coment\xE1rios associados a esta issue:"),It=d(),x(os.$$.fragment),zt=d(),x(ns.$$.fragment),Nt=d(),ze=n("p"),An=t("Quando explodimos "),xa=n("code"),Sn=t("df"),On=t(", esperamos obter uma linha para cada um desses coment\xE1rios. Vamos verificar se \xE9 o caso:"),Ft=d(),x(rs.$$.fragment),Rt=d(),ee=n("table"),Ea=n("thead"),Z=n("tr"),Ht=n("th"),Pn=d(),qa=n("th"),In=t("html_url"),zn=d(),ka=n("th"),Nn=t("title"),Fn=d(),wa=n("th"),Rn=t("comments"),Hn=d(),ja=n("th"),Mn=t("body"),Ln=d(),de=n("tbody"),se=n("tr"),ya=n("th"),Un=t("0"),Vn=d(),Da=n("td"),Gn=t("https://github.com/huggingface/datasets/issues/2787"),Yn=d(),Ca=n("td"),Bn=t("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Wn=d(),Ta=n("td"),Qn=t("the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Jn=d(),Aa=n("td"),Xn=t("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Kn=d(),ae=n("tr"),Sa=n("th"),Zn=t("1"),er=d(),Oa=n("td"),sr=t("https://github.com/huggingface/datasets/issues/2787"),ar=d(),Pa=n("td"),tr=t("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),or=d(),Ia=n("td"),nr=t("Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),rr=d(),za=n("td"),lr=t("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),ir=d(),te=n("tr"),Na=n("th"),mr=t("2"),dr=d(),Fa=n("td"),pr=t("https://github.com/huggingface/datasets/issues/2787"),cr=d(),Ra=n("td"),ur=t("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),fr=d(),Ha=n("td"),hr=t("cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),_r=d(),Ma=n("td"),gr=t("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),br=d(),oe=n("tr"),La=n("th"),vr=t("3"),$r=d(),Ua=n("td"),xr=t("https://github.com/huggingface/datasets/issues/2787"),Er=d(),Va=n("td"),qr=t("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),kr=d(),Ga=n("td"),wr=t("I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),jr=d(),Ya=n("td"),yr=t("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Mt=d(),re=n("p"),Dr=t("\xD3timo, podemos ver que as linhas foram replicadas, com a coluna "),Ba=n("code"),Cr=t("comments"),Tr=t(" contendo os coment\xE1rios individuais! Agora que terminamos com o Pandas, podemos voltar rapidamente para um "),Wa=n("code"),Ar=t("Dataset"),Sr=t(" carregando o "),Qa=n("code"),Or=t("DataFrame"),Pr=t(" na mem\xF3ria"),Lt=d(),x(ls.$$.fragment),Ut=d(),x(is.$$.fragment),Vt=d(),Ps=n("p"),Ir=t("Ok, isso nos deu alguns milhares de coment\xE1rios para trabalhar!"),Gt=d(),x(Ne.$$.fragment),Yt=d(),Fe=n("p"),zr=t("Agora que temos um coment\xE1rio por linha, vamos criar uma nova coluna "),Ja=n("code"),Nr=t("comments_length"),Fr=t(" que cont\xE9m o n\xFAmero de palavras por coment\xE1rio:"),Bt=d(),x(ms.$$.fragment),Wt=d(),Is=n("p"),Rr=t("Podemos usar essa nova coluna para filtrar coment\xE1rios curtos, que normalmente incluem coisas como \u201Ccc @lewtun\u201D ou \u201CObrigado!\u201D que n\xE3o s\xE3o relevantes para o nosso motor de busca. N\xE3o h\xE1 um n\xFAmero preciso para selecionar o filtro, mas cerca de 15 palavras parece um bom come\xE7o:"),Qt=d(),x(ds.$$.fragment),Jt=d(),x(ps.$$.fragment),Xt=d(),ve=n("p"),Hr=t("Depois de limpar um pouco nosso conjunto de dados, vamos concatenar o t\xEDtulo, a descri\xE7\xE3o e os coment\xE1rios da issue em uma nova coluna "),Xa=n("code"),Mr=t("text"),Lr=t(". Como de costume, escreveremos uma fun\xE7\xE3o simples que podemos passar para "),Ka=n("code"),Ur=t("Dataset.map()"),Vr=t(":"),Kt=d(),x(cs.$$.fragment),Zt=d(),zs=n("p"),Gr=t("Finalmente estamos prontos para criar alguns embeddings! Vamos dar uma olhada."),eo=d(),Se=n("h2"),Re=n("a"),Za=n("span"),x(us.$$.fragment),Yr=d(),et=n("span"),Br=t("Criando embeddings de texto"),so=d(),Y=n("p"),Wr=t("Vimos no "),Ns=n("a"),Qr=t("Cap\xEDtulo 2"),Jr=t(" que podemos obter tokens embeddings usando a classe "),st=n("code"),Xr=t("AutoModel"),Kr=t(". Tudo o que precisamos fazer \xE9 escolher um checkpoint adequado para carregar o modelo. Felizmente, existe uma biblioteca chamada "),at=n("code"),Zr=t("sentence-transformers"),el=t(" dedicada \xE0 cria\xE7\xE3o de embeddings. Conforme descrito na "),fs=n("a"),sl=t("documenta\xE7\xE3o da biblioteca"),al=t(", nosso caso de uso \xE9 um exemplo de "),tt=n("em"),tl=t("asymmetric semantic search"),ol=t(" porque temos uma consulta curta cuja resposta gostar\xEDamos de encontrar em um documento mais longo, como um coment\xE1rio da issue. A \xFAtil "),hs=n("a"),nl=t("tabela de vis\xE3o geral do modelo"),rl=t(" na documenta\xE7\xE3o indica que o checkpoint "),ot=n("code"),ll=t("multi-qa-mpnet-base-dot-v1"),il=t(" tem o melhor desempenho para pesquisa sem\xE2ntica, ent\xE3o usaremos isso para nosso aplicativo. Tamb\xE9m carregaremos o tokenizer usando o mesmo checkpoint:"),ao=d(),ce.c(),Fs=d(),$e=n("p"),ml=t("Como mencionamos anteriormente, gostar\xEDamos de representar cada entrada em nosso corpus de issues do GitHub como um \xFAnico vetor, portanto, precisamos \u201Cpool\u201D ou calcular a m\xE9dia de nossas incorpora\xE7\xF5es de token de alguma forma. Uma abordagem popular \xE9 realizar "),nt=n("em"),dl=t("CLS pooling"),pl=t(" nas sa\xEDdas do nosso modelo, onde simplesmente coletamos o \xFAltimo estado oculto para o token especial "),rt=n("code"),cl=t("[CLS]"),ul=t(". A fun\xE7\xE3o a seguir faz o truque para n\xF3s:"),to=d(),x(_s.$$.fragment),oo=d(),Rs=n("p"),fl=t("Em seguida, criaremos uma fun\xE7\xE3o auxiliar que tokenizar\xE1 uma lista de documentos, colocar\xE1 os tensores na GPU, os alimentar\xE1 no modelo e, finalmente, aplicar\xE1 o agrupamento CLS \xE0s sa\xEDdas:"),no=d(),fe.c(),Hs=d(),Ms=n("p"),hl=t("Observe que convertemos os embeddings em arrays NumPy \u2014 isso porque \u{1F917} Datasets requer esse formato quando tentamos index\xE1-los com FAISS, o que faremos a seguir."),ro=d(),Oe=n("h2"),He=n("a"),lt=n("span"),x(gs.$$.fragment),_l=d(),it=n("span"),gl=t("Usando FAISS para busca de similaridade"),lo=d(),xe=n("p"),bl=t("Agora que temos um conjunto de dados de embeddings, precisamos de alguma maneira de pesquis\xE1-los. Para fazer isso, usaremos uma estrutura de dados especial em \u{1F917} Datasets chamada "),mt=n("em"),vl=t("FAISS index"),$l=t(". "),bs=n("a"),xl=t("FAISS"),El=t(" (abrevia\xE7\xE3o de Facebook AI Similarity Search) \xE9 uma biblioteca que fornece algoritmos eficientes para pesquisar rapidamente e agrupar vetores de incorpora\xE7\xE3o."),io=d(),Ee=n("p"),ql=t("A id\xE9ia b\xE1sica por tr\xE1s do FAISS \xE9 criar uma estrutura de dados especial chamada "),dt=n("em"),kl=t("index"),wl=t(" que permite descobrir quais embeddings s\xE3o semelhantes a um embedding de entrada. Criar um \xEDndice FAISS em \u{1F917} Datasets \xE9 simples \u2014 usamos a fun\xE7\xE3o "),pt=n("code"),jl=t("Dataset.add_faiss_index()"),yl=t(" e especificamos qual coluna do nosso conjunto de dados gostar\xEDamos de indexar:"),mo=d(),x(vs.$$.fragment),po=d(),Me=n("p"),Dl=t("Agora podemos realizar consultas neste \xEDndice fazendo uma pesquisa do vizinho mais pr\xF3ximo com a fun\xE7\xE3o "),ct=n("code"),Cl=t("Dataset.get_nearest_examples()"),Tl=t(". Vamos testar isso primeiro incorporando uma pergunta da seguinte forma:"),co=d(),_e.c(),Ls=d(),Us=n("p"),Al=t("Assim como com os documentos, agora temos um vetor de 768 dimens\xF5es representando a consulta, que podemos comparar com todo o corpus para encontrar os embeddings mais semelhantes:"),uo=d(),x($s.$$.fragment),fo=d(),qe=n("p"),Sl=t("A fun\xE7\xE3o "),ut=n("code"),Ol=t("Dataset.get_nearest_examples()"),Pl=t(" retorna uma tupla de pontua\xE7\xF5es que classificam a sobreposi\xE7\xE3o entre a consulta e o documento e um conjunto correspondente de amostras (aqui, as 5 melhores correspond\xEAncias). Vamos colet\xE1-los em um "),ft=n("code"),Il=t("pandas.DataFrame"),zl=t(" para que possamos classific\xE1-los facilmente:"),ho=d(),x(xs.$$.fragment),_o=d(),Vs=n("p"),Nl=t("Agora podemos iterar nas primeiras linhas para ver como nossa consulta correspondeu aos coment\xE1rios dispon\xEDveis:"),go=d(),x(Es.$$.fragment),bo=d(),x(qs.$$.fragment),vo=d(),Gs=n("p"),Fl=t("Nada mal! Nosso segundo resultado parece corresponder \xE0 consulta."),$o=d(),x(Le.$$.fragment),this.h()},l(e){const i=Nm('[data-svelte="svelte-1phssyn"]',document.head);c=r(i,"META",{name:!0,content:!0}),i.forEach(a),$=p(e),E(f.$$.fragment,e),j=p(e),h=r(e,"H1",{class:!0});var Ds=l(h);v=r(Ds,"A",{id:!0,class:!0,href:!0});var Ys=l(v);P=r(Ys,"SPAN",{});var ht=l(P);E(_.$$.fragment,ht),ht.forEach(a),Ys.forEach(a),A=p(Ds),y=r(Ds,"SPAN",{});var Bs=l(y);M=o(Bs,"Busca sem\xE2ntica com o FAISS"),Bs.forEach(a),Ds.forEach(a),I=p(e),z.l(e),N=p(e),R=r(e,"P",{});var Ue=l(R);V=o(Ue,"Na "),C=r(Ue,"A",{href:!0});var Ws=l(C);F=o(Ws,"se\xE7\xE3o 5"),Ws.forEach(a),B=o(Ue,", criamos um conjunto de dados de issues e coment\xE1rios do GitHub do reposit\xF3rio \u{1F917} Datasets. Nesta se\xE7\xE3o, usaremos essas informa\xE7\xF5es para construir um mecanismo de pesquisa que pode nos ajudar a encontrar respostas para nossas perguntas mais urgentes sobre a biblioteca!"),Ue.forEach(a),U=p(e),E(D.$$.fragment,e),G=p(e),u=r(e,"H2",{class:!0});var Cs=l(u);T=r(Cs,"A",{id:!0,class:!0,href:!0});var Ql=l(T);H=r(Ql,"SPAN",{});var Jl=l(H);E(W.$$.fragment,Jl),Jl.forEach(a),Ql.forEach(a),ne=p(Cs),me=r(Cs,"SPAN",{});var Xl=l(me);Po=o(Xl,"Usando embeddings para pesquisa sem\xE2ntica"),Xl.forEach(a),Cs.forEach(a),gt=p(e),ge=r(e,"P",{});var Qs=l(ge);Io=o(Qs,"Como vimos no "),Ts=r(Qs,"A",{href:!0});var Kl=l(Ts);zo=o(Kl,"Cap\xEDtulo 1"),Kl.forEach(a),No=o(Qs,", os modelos de linguagem baseados em Transformer representam cada token em um intervalo de texto como um "),aa=r(Qs,"EM",{});var Zl=l(aa);Fo=o(Zl,"vetor de incorpora\xE7\xE3o"),Zl.forEach(a),Ro=o(Qs,". Acontece que \xE9 poss\xEDvel \u201Cagrupar\u201D as incorpora\xE7\xF5es individuais para criar uma representa\xE7\xE3o vetorial para frases inteiras, par\xE1grafos ou (em alguns casos) documentos. Essas incorpora\xE7\xF5es podem ser usadas para encontrar documentos semelhantes no corpus calculando a similaridade do produto escalar (ou alguma outra m\xE9trica de similaridade) entre cada incorpora\xE7\xE3o e retornando os documentos com maior sobreposi\xE7\xE3o."),Qs.forEach(a),bt=p(e),As=r(e,"P",{});var ei=l(As);Ho=o(ei,"Nesta se\xE7\xE3o, usaremos embeddings para desenvolver um mecanismo de pesquisa sem\xE2ntica. Esses mecanismos de pesquisa oferecem v\xE1rias vantagens sobre as abordagens convencionais que se baseiam na correspond\xEAncia de palavras-chave em uma consulta com os documentos."),ei.forEach(a),vt=p(e),Te=r(e,"DIV",{class:!0});var Eo=l(Te);Ye=r(Eo,"IMG",{class:!0,src:!0,alt:!0}),Mo=p(Eo),Be=r(Eo,"IMG",{class:!0,src:!0,alt:!0}),Eo.forEach(a),$t=p(e),Ae=r(e,"H2",{class:!0});var qo=l(Ae);Pe=r(qo,"A",{id:!0,class:!0,href:!0});var si=l(Pe);ta=r(si,"SPAN",{});var ai=l(ta);E(We.$$.fragment,ai),ai.forEach(a),si.forEach(a),Lo=p(qo),oa=r(qo,"SPAN",{});var ti=l(oa);Uo=o(ti,"Carregando e preparando o conjunto de dados"),ti.forEach(a),qo.forEach(a),xt=p(e),Ss=r(e,"P",{});var oi=l(Ss);Vo=o(oi,"A primeira coisa que precisamos fazer \xE9 baixar nosso conjunto de dados de issues do GitHub, ent\xE3o vamos usar a biblioteca \u{1F917} Hub para resolver a URL onde nosso arquivo est\xE1 armazenado no Hugging Face Hub:"),oi.forEach(a),Et=p(e),E(Qe.$$.fragment,e),qt=p(e),be=r(e,"P",{});var Js=l(be);Go=o(Js,"Com a URL armazenada em "),na=r(Js,"CODE",{});var ni=l(na);Yo=o(ni,"data_files"),ni.forEach(a),Bo=o(Js,", podemos carregar o conjunto de dados remoto usando o m\xE9todo apresentado na "),Os=r(Js,"A",{href:!0});var ri=l(Os);Wo=o(ri,"se\xE7\xE3o 2"),ri.forEach(a),Qo=o(Js,":"),Js.forEach(a),kt=p(e),E(Je.$$.fragment,e),wt=p(e),E(Xe.$$.fragment,e),jt=p(e),Q=r(e,"P",{});var le=l(Q);Jo=o(le,"Aqui n\xF3s especificamos a divis\xE3o padr\xE3o "),ra=r(le,"CODE",{});var li=l(ra);Xo=o(li,"train"),li.forEach(a),Ko=o(le," em "),la=r(le,"CODE",{});var ii=l(la);Zo=o(ii,"load_dataset()"),ii.forEach(a),en=o(le,", ent\xE3o ele retorna um "),ia=r(le,"CODE",{});var mi=l(ia);sn=o(mi,"Dataset"),mi.forEach(a),an=o(le," em vez de um "),ma=r(le,"CODE",{});var di=l(ma);tn=o(di,"DatasetDict"),di.forEach(a),on=o(le,". A primeira ordem de neg\xF3cios \xE9 filtrar os pull request, pois elas tendem a ser raramente usadas para responder a consultas de usu\xE1rios e introduzir\xE3o ru\xEDdo em nosso mecanismo de pesquisa. Como j\xE1 deve ser familiar, podemos usar a fun\xE7\xE3o "),da=r(le,"CODE",{});var pi=l(da);nn=o(pi,"Dataset.filter()"),pi.forEach(a),rn=o(le," para excluir essas linhas em nosso conjunto de dados. Enquanto estamos nisso, tamb\xE9m vamos filtrar as linhas sem coment\xE1rios, pois elas n\xE3o fornecem respostas \xE0s consultas dos usu\xE1rios:"),le.forEach(a),yt=p(e),E(Ke.$$.fragment,e),Dt=p(e),E(Ze.$$.fragment,e),Ct=p(e),J=r(e,"P",{});var ie=l(J);ln=o(ie,"Podemos ver que h\xE1 muitas colunas em nosso conjunto de dados, a maioria das quais n\xE3o precisamos para construir nosso mecanismo de pesquisa. De uma perspectiva de pesquisa, as colunas mais informativas s\xE3o "),pa=r(ie,"CODE",{});var ci=l(pa);mn=o(ci,"title"),ci.forEach(a),dn=o(ie,", "),ca=r(ie,"CODE",{});var ui=l(ca);pn=o(ui,"body"),ui.forEach(a),cn=o(ie," e "),ua=r(ie,"CODE",{});var fi=l(ua);un=o(fi,"comments"),fi.forEach(a),fn=o(ie,", enquanto "),fa=r(ie,"CODE",{});var hi=l(fa);hn=o(hi,"html_url"),hi.forEach(a),_n=o(ie," nos fornece um link de volta para a issue de origem. Vamos usar a fun\xE7\xE3o "),ha=r(ie,"CODE",{});var _i=l(ha);gn=o(_i,"Dataset.remove_columns()"),_i.forEach(a),bn=o(ie," para descartar o resto:"),ie.forEach(a),Tt=p(e),E(es.$$.fragment,e),At=p(e),E(ss.$$.fragment,e),St=p(e),K=r(e,"P",{});var ke=l(K);vn=o(ke,"Para criar nossos embeddings, aumentaremos cada coment\xE1rio com o t\xEDtulo e o corpo da issue, pois esses campos geralmente incluem informa\xE7\xF5es contextuais \xFAteis. Como nossa coluna "),_a=r(ke,"CODE",{});var gi=l(_a);$n=o(gi,"comments"),gi.forEach(a),xn=o(ke," \xE9 atualmente uma lista de coment\xE1rios para cada issue, precisamos \u201Cexplodir\u201D a coluna para que cada linha consista em uma tupla "),ga=r(ke,"CODE",{});var bi=l(ga);En=o(bi,"(html_url, title, body, comment)"),bi.forEach(a),qn=o(ke,". No Pandas podemos fazer isso com a fun\xE7\xE3o "),as=r(ke,"A",{href:!0,rel:!0});var vi=l(as);ba=r(vi,"CODE",{});var $i=l(ba);kn=o($i,"DataFrame.explode()"),$i.forEach(a),vi.forEach(a),wn=o(ke,", que cria uma nova linha para cada elemento em uma coluna semelhante a uma lista, enquanto replica todos os outros valores de coluna. Para ver isso em a\xE7\xE3o, vamos primeiro mudar para o formato "),va=r(ke,"CODE",{});var xi=l(va);jn=o(xi,"DataFrame"),xi.forEach(a),yn=o(ke," do Pandas:"),ke.forEach(a),Ot=p(e),E(ts.$$.fragment,e),Pt=p(e),Ie=r(e,"P",{});var ko=l(Ie);Dn=o(ko,"Se inspecionarmos a primeira linha neste "),$a=r(ko,"CODE",{});var Ei=l($a);Cn=o(Ei,"DataFrame"),Ei.forEach(a),Tn=o(ko,", podemos ver que h\xE1 quatro coment\xE1rios associados a esta issue:"),ko.forEach(a),It=p(e),E(os.$$.fragment,e),zt=p(e),E(ns.$$.fragment,e),Nt=p(e),ze=r(e,"P",{});var wo=l(ze);An=o(wo,"Quando explodimos "),xa=r(wo,"CODE",{});var qi=l(xa);Sn=o(qi,"df"),qi.forEach(a),On=o(wo,", esperamos obter uma linha para cada um desses coment\xE1rios. Vamos verificar se \xE9 o caso:"),wo.forEach(a),Ft=p(e),E(rs.$$.fragment,e),Rt=p(e),ee=r(e,"TABLE",{border:!0,class:!0,style:!0});var jo=l(ee);Ea=r(jo,"THEAD",{});var ki=l(Ea);Z=r(ki,"TR",{style:!0});var we=l(Z);Ht=r(we,"TH",{}),l(Ht).forEach(a),Pn=p(we),qa=r(we,"TH",{});var wi=l(qa);In=o(wi,"html_url"),wi.forEach(a),zn=p(we),ka=r(we,"TH",{});var ji=l(ka);Nn=o(ji,"title"),ji.forEach(a),Fn=p(we),wa=r(we,"TH",{});var yi=l(wa);Rn=o(yi,"comments"),yi.forEach(a),Hn=p(we),ja=r(we,"TH",{});var Di=l(ja);Mn=o(Di,"body"),Di.forEach(a),we.forEach(a),ki.forEach(a),Ln=p(jo),de=r(jo,"TBODY",{});var Ve=l(de);se=r(Ve,"TR",{});var je=l(se);ya=r(je,"TH",{});var Ci=l(ya);Un=o(Ci,"0"),Ci.forEach(a),Vn=p(je),Da=r(je,"TD",{});var Ti=l(Da);Gn=o(Ti,"https://github.com/huggingface/datasets/issues/2787"),Ti.forEach(a),Yn=p(je),Ca=r(je,"TD",{});var Ai=l(Ca);Bn=o(Ai,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Ai.forEach(a),Wn=p(je),Ta=r(je,"TD",{});var Si=l(Ta);Qn=o(Si,"the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Si.forEach(a),Jn=p(je),Aa=r(je,"TD",{});var Oi=l(Aa);Xn=o(Oi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Oi.forEach(a),je.forEach(a),Kn=p(Ve),ae=r(Ve,"TR",{});var ye=l(ae);Sa=r(ye,"TH",{});var Pi=l(Sa);Zn=o(Pi,"1"),Pi.forEach(a),er=p(ye),Oa=r(ye,"TD",{});var Ii=l(Oa);sr=o(Ii,"https://github.com/huggingface/datasets/issues/2787"),Ii.forEach(a),ar=p(ye),Pa=r(ye,"TD",{});var zi=l(Pa);tr=o(zi,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),zi.forEach(a),or=p(ye),Ia=r(ye,"TD",{});var Ni=l(Ia);nr=o(Ni,"Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),Ni.forEach(a),rr=p(ye),za=r(ye,"TD",{});var Fi=l(za);lr=o(Fi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Fi.forEach(a),ye.forEach(a),ir=p(Ve),te=r(Ve,"TR",{});var De=l(te);Na=r(De,"TH",{});var Ri=l(Na);mr=o(Ri,"2"),Ri.forEach(a),dr=p(De),Fa=r(De,"TD",{});var Hi=l(Fa);pr=o(Hi,"https://github.com/huggingface/datasets/issues/2787"),Hi.forEach(a),cr=p(De),Ra=r(De,"TD",{});var Mi=l(Ra);ur=o(Mi,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Mi.forEach(a),fr=p(De),Ha=r(De,"TD",{});var Li=l(Ha);hr=o(Li,"cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),Li.forEach(a),_r=p(De),Ma=r(De,"TD",{});var Ui=l(Ma);gr=o(Ui,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Ui.forEach(a),De.forEach(a),br=p(Ve),oe=r(Ve,"TR",{});var Ce=l(oe);La=r(Ce,"TH",{});var Vi=l(La);vr=o(Vi,"3"),Vi.forEach(a),$r=p(Ce),Ua=r(Ce,"TD",{});var Gi=l(Ua);xr=o(Gi,"https://github.com/huggingface/datasets/issues/2787"),Gi.forEach(a),Er=p(Ce),Va=r(Ce,"TD",{});var Yi=l(Va);qr=o(Yi,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Yi.forEach(a),kr=p(Ce),Ga=r(Ce,"TD",{});var Bi=l(Ga);wr=o(Bi,"I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Bi.forEach(a),jr=p(Ce),Ya=r(Ce,"TD",{});var Wi=l(Ya);yr=o(Wi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Wi.forEach(a),Ce.forEach(a),Ve.forEach(a),jo.forEach(a),Mt=p(e),re=r(e,"P",{});var Ge=l(re);Dr=o(Ge,"\xD3timo, podemos ver que as linhas foram replicadas, com a coluna "),Ba=r(Ge,"CODE",{});var Qi=l(Ba);Cr=o(Qi,"comments"),Qi.forEach(a),Tr=o(Ge," contendo os coment\xE1rios individuais! Agora que terminamos com o Pandas, podemos voltar rapidamente para um "),Wa=r(Ge,"CODE",{});var Ji=l(Wa);Ar=o(Ji,"Dataset"),Ji.forEach(a),Sr=o(Ge," carregando o "),Qa=r(Ge,"CODE",{});var Xi=l(Qa);Or=o(Xi,"DataFrame"),Xi.forEach(a),Pr=o(Ge," na mem\xF3ria"),Ge.forEach(a),Lt=p(e),E(ls.$$.fragment,e),Ut=p(e),E(is.$$.fragment,e),Vt=p(e),Ps=r(e,"P",{});var Ki=l(Ps);Ir=o(Ki,"Ok, isso nos deu alguns milhares de coment\xE1rios para trabalhar!"),Ki.forEach(a),Gt=p(e),E(Ne.$$.fragment,e),Yt=p(e),Fe=r(e,"P",{});var yo=l(Fe);zr=o(yo,"Agora que temos um coment\xE1rio por linha, vamos criar uma nova coluna "),Ja=r(yo,"CODE",{});var Zi=l(Ja);Nr=o(Zi,"comments_length"),Zi.forEach(a),Fr=o(yo," que cont\xE9m o n\xFAmero de palavras por coment\xE1rio:"),yo.forEach(a),Bt=p(e),E(ms.$$.fragment,e),Wt=p(e),Is=r(e,"P",{});var em=l(Is);Rr=o(em,"Podemos usar essa nova coluna para filtrar coment\xE1rios curtos, que normalmente incluem coisas como \u201Ccc @lewtun\u201D ou \u201CObrigado!\u201D que n\xE3o s\xE3o relevantes para o nosso motor de busca. N\xE3o h\xE1 um n\xFAmero preciso para selecionar o filtro, mas cerca de 15 palavras parece um bom come\xE7o:"),em.forEach(a),Qt=p(e),E(ds.$$.fragment,e),Jt=p(e),E(ps.$$.fragment,e),Xt=p(e),ve=r(e,"P",{});var Xs=l(ve);Hr=o(Xs,"Depois de limpar um pouco nosso conjunto de dados, vamos concatenar o t\xEDtulo, a descri\xE7\xE3o e os coment\xE1rios da issue em uma nova coluna "),Xa=r(Xs,"CODE",{});var sm=l(Xa);Mr=o(sm,"text"),sm.forEach(a),Lr=o(Xs,". Como de costume, escreveremos uma fun\xE7\xE3o simples que podemos passar para "),Ka=r(Xs,"CODE",{});var am=l(Ka);Ur=o(am,"Dataset.map()"),am.forEach(a),Vr=o(Xs,":"),Xs.forEach(a),Kt=p(e),E(cs.$$.fragment,e),Zt=p(e),zs=r(e,"P",{});var tm=l(zs);Gr=o(tm,"Finalmente estamos prontos para criar alguns embeddings! Vamos dar uma olhada."),tm.forEach(a),eo=p(e),Se=r(e,"H2",{class:!0});var Do=l(Se);Re=r(Do,"A",{id:!0,class:!0,href:!0});var om=l(Re);Za=r(om,"SPAN",{});var nm=l(Za);E(us.$$.fragment,nm),nm.forEach(a),om.forEach(a),Yr=p(Do),et=r(Do,"SPAN",{});var rm=l(et);Br=o(rm,"Criando embeddings de texto"),rm.forEach(a),Do.forEach(a),so=p(e),Y=r(e,"P",{});var X=l(Y);Wr=o(X,"Vimos no "),Ns=r(X,"A",{href:!0});var lm=l(Ns);Qr=o(lm,"Cap\xEDtulo 2"),lm.forEach(a),Jr=o(X," que podemos obter tokens embeddings usando a classe "),st=r(X,"CODE",{});var im=l(st);Xr=o(im,"AutoModel"),im.forEach(a),Kr=o(X,". Tudo o que precisamos fazer \xE9 escolher um checkpoint adequado para carregar o modelo. Felizmente, existe uma biblioteca chamada "),at=r(X,"CODE",{});var mm=l(at);Zr=o(mm,"sentence-transformers"),mm.forEach(a),el=o(X," dedicada \xE0 cria\xE7\xE3o de embeddings. Conforme descrito na "),fs=r(X,"A",{href:!0,rel:!0});var dm=l(fs);sl=o(dm,"documenta\xE7\xE3o da biblioteca"),dm.forEach(a),al=o(X,", nosso caso de uso \xE9 um exemplo de "),tt=r(X,"EM",{});var pm=l(tt);tl=o(pm,"asymmetric semantic search"),pm.forEach(a),ol=o(X," porque temos uma consulta curta cuja resposta gostar\xEDamos de encontrar em um documento mais longo, como um coment\xE1rio da issue. A \xFAtil "),hs=r(X,"A",{href:!0,rel:!0});var cm=l(hs);nl=o(cm,"tabela de vis\xE3o geral do modelo"),cm.forEach(a),rl=o(X," na documenta\xE7\xE3o indica que o checkpoint "),ot=r(X,"CODE",{});var um=l(ot);ll=o(um,"multi-qa-mpnet-base-dot-v1"),um.forEach(a),il=o(X," tem o melhor desempenho para pesquisa sem\xE2ntica, ent\xE3o usaremos isso para nosso aplicativo. Tamb\xE9m carregaremos o tokenizer usando o mesmo checkpoint:"),X.forEach(a),ao=p(e),ce.l(e),Fs=p(e),$e=r(e,"P",{});var Ks=l($e);ml=o(Ks,"Como mencionamos anteriormente, gostar\xEDamos de representar cada entrada em nosso corpus de issues do GitHub como um \xFAnico vetor, portanto, precisamos \u201Cpool\u201D ou calcular a m\xE9dia de nossas incorpora\xE7\xF5es de token de alguma forma. Uma abordagem popular \xE9 realizar "),nt=r(Ks,"EM",{});var fm=l(nt);dl=o(fm,"CLS pooling"),fm.forEach(a),pl=o(Ks," nas sa\xEDdas do nosso modelo, onde simplesmente coletamos o \xFAltimo estado oculto para o token especial "),rt=r(Ks,"CODE",{});var hm=l(rt);cl=o(hm,"[CLS]"),hm.forEach(a),ul=o(Ks,". A fun\xE7\xE3o a seguir faz o truque para n\xF3s:"),Ks.forEach(a),to=p(e),E(_s.$$.fragment,e),oo=p(e),Rs=r(e,"P",{});var _m=l(Rs);fl=o(_m,"Em seguida, criaremos uma fun\xE7\xE3o auxiliar que tokenizar\xE1 uma lista de documentos, colocar\xE1 os tensores na GPU, os alimentar\xE1 no modelo e, finalmente, aplicar\xE1 o agrupamento CLS \xE0s sa\xEDdas:"),_m.forEach(a),no=p(e),fe.l(e),Hs=p(e),Ms=r(e,"P",{});var gm=l(Ms);hl=o(gm,"Observe que convertemos os embeddings em arrays NumPy \u2014 isso porque \u{1F917} Datasets requer esse formato quando tentamos index\xE1-los com FAISS, o que faremos a seguir."),gm.forEach(a),ro=p(e),Oe=r(e,"H2",{class:!0});var Co=l(Oe);He=r(Co,"A",{id:!0,class:!0,href:!0});var bm=l(He);lt=r(bm,"SPAN",{});var vm=l(lt);E(gs.$$.fragment,vm),vm.forEach(a),bm.forEach(a),_l=p(Co),it=r(Co,"SPAN",{});var $m=l(it);gl=o($m,"Usando FAISS para busca de similaridade"),$m.forEach(a),Co.forEach(a),lo=p(e),xe=r(e,"P",{});var Zs=l(xe);bl=o(Zs,"Agora que temos um conjunto de dados de embeddings, precisamos de alguma maneira de pesquis\xE1-los. Para fazer isso, usaremos uma estrutura de dados especial em \u{1F917} Datasets chamada "),mt=r(Zs,"EM",{});var xm=l(mt);vl=o(xm,"FAISS index"),xm.forEach(a),$l=o(Zs,". "),bs=r(Zs,"A",{href:!0,rel:!0});var Em=l(bs);xl=o(Em,"FAISS"),Em.forEach(a),El=o(Zs," (abrevia\xE7\xE3o de Facebook AI Similarity Search) \xE9 uma biblioteca que fornece algoritmos eficientes para pesquisar rapidamente e agrupar vetores de incorpora\xE7\xE3o."),Zs.forEach(a),io=p(e),Ee=r(e,"P",{});var ea=l(Ee);ql=o(ea,"A id\xE9ia b\xE1sica por tr\xE1s do FAISS \xE9 criar uma estrutura de dados especial chamada "),dt=r(ea,"EM",{});var qm=l(dt);kl=o(qm,"index"),qm.forEach(a),wl=o(ea," que permite descobrir quais embeddings s\xE3o semelhantes a um embedding de entrada. Criar um \xEDndice FAISS em \u{1F917} Datasets \xE9 simples \u2014 usamos a fun\xE7\xE3o "),pt=r(ea,"CODE",{});var km=l(pt);jl=o(km,"Dataset.add_faiss_index()"),km.forEach(a),yl=o(ea," e especificamos qual coluna do nosso conjunto de dados gostar\xEDamos de indexar:"),ea.forEach(a),mo=p(e),E(vs.$$.fragment,e),po=p(e),Me=r(e,"P",{});var To=l(Me);Dl=o(To,"Agora podemos realizar consultas neste \xEDndice fazendo uma pesquisa do vizinho mais pr\xF3ximo com a fun\xE7\xE3o "),ct=r(To,"CODE",{});var wm=l(ct);Cl=o(wm,"Dataset.get_nearest_examples()"),wm.forEach(a),Tl=o(To,". Vamos testar isso primeiro incorporando uma pergunta da seguinte forma:"),To.forEach(a),co=p(e),_e.l(e),Ls=p(e),Us=r(e,"P",{});var jm=l(Us);Al=o(jm,"Assim como com os documentos, agora temos um vetor de 768 dimens\xF5es representando a consulta, que podemos comparar com todo o corpus para encontrar os embeddings mais semelhantes:"),jm.forEach(a),uo=p(e),E($s.$$.fragment,e),fo=p(e),qe=r(e,"P",{});var sa=l(qe);Sl=o(sa,"A fun\xE7\xE3o "),ut=r(sa,"CODE",{});var ym=l(ut);Ol=o(ym,"Dataset.get_nearest_examples()"),ym.forEach(a),Pl=o(sa," retorna uma tupla de pontua\xE7\xF5es que classificam a sobreposi\xE7\xE3o entre a consulta e o documento e um conjunto correspondente de amostras (aqui, as 5 melhores correspond\xEAncias). Vamos colet\xE1-los em um "),ft=r(sa,"CODE",{});var Dm=l(ft);Il=o(Dm,"pandas.DataFrame"),Dm.forEach(a),zl=o(sa," para que possamos classific\xE1-los facilmente:"),sa.forEach(a),ho=p(e),E(xs.$$.fragment,e),_o=p(e),Vs=r(e,"P",{});var Cm=l(Vs);Nl=o(Cm,"Agora podemos iterar nas primeiras linhas para ver como nossa consulta correspondeu aos coment\xE1rios dispon\xEDveis:"),Cm.forEach(a),go=p(e),E(Es.$$.fragment,e),bo=p(e),E(qs.$$.fragment,e),vo=p(e),Gs=r(e,"P",{});var Tm=l(Gs);Fl=o(Tm,"Nada mal! Nosso segundo resultado parece corresponder \xE0 consulta."),Tm.forEach(a),$o=p(e),E(Le.$$.fragment,e),this.h()},h(){w(c,"name","hf:doc:metadata"),w(c,"content",JSON.stringify(Km)),w(v,"id","busca-semntica-com-o-faiss"),w(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(v,"href","#busca-semntica-com-o-faiss"),w(h,"class","relative group"),w(C,"href","/course/chapter5/5"),w(T,"id","usando-embeddings-para-pesquisa-semntica"),w(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(T,"href","#usando-embeddings-para-pesquisa-semntica"),w(u,"class","relative group"),w(Ts,"href","/course/chapter1"),w(Ye,"class","block dark:hidden"),Am(Ye.src,Rl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg")||w(Ye,"src",Rl),w(Ye,"alt","Semantic search."),w(Be,"class","hidden dark:block"),Am(Be.src,Hl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg")||w(Be,"src",Hl),w(Be,"alt","Semantic search."),w(Te,"class","flex justify-center"),w(Pe,"id","carregando-e-preparando-o-conjunto-de-dados"),w(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Pe,"href","#carregando-e-preparando-o-conjunto-de-dados"),w(Ae,"class","relative group"),w(Os,"href","/course/chapter5/2"),w(as,"href","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"),w(as,"rel","nofollow"),Ao(Z,"text-align","right"),w(ee,"border","1"),w(ee,"class","dataframe"),Ao(ee,"table-layout","fixed"),Ao(ee,"word-wrap","break-word"),Ao(ee,"width","100%"),w(Re,"id","criando-embeddings-de-texto"),w(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Re,"href","#criando-embeddings-de-texto"),w(Se,"class","relative group"),w(Ns,"href","/course/chapter2"),w(fs,"href","https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),w(fs,"rel","nofollow"),w(hs,"href","https://www.sbert.net/docs/pretrained_models.html#model-overview"),w(hs,"rel","nofollow"),w(He,"id","usando-faiss-para-busca-de-similaridade"),w(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(He,"href","#usando-faiss-para-busca-de-similaridade"),w(Oe,"class","relative group"),w(bs,"href","https://faiss.ai/"),w(bs,"rel","nofollow")},m(e,i){s(document.head,c),m(e,$,i),q(f,e,i),m(e,j,i),m(e,h,i),s(h,v),s(v,P),q(_,P,null),s(h,A),s(h,y),s(y,M),m(e,I,i),ks[S].m(e,i),m(e,N,i),m(e,R,i),s(R,V),s(R,C),s(C,F),s(R,B),m(e,U,i),q(D,e,i),m(e,G,i),m(e,u,i),s(u,T),s(T,H),q(W,H,null),s(u,ne),s(u,me),s(me,Po),m(e,gt,i),m(e,ge,i),s(ge,Io),s(ge,Ts),s(Ts,zo),s(ge,No),s(ge,aa),s(aa,Fo),s(ge,Ro),m(e,bt,i),m(e,As,i),s(As,Ho),m(e,vt,i),m(e,Te,i),s(Te,Ye),s(Te,Mo),s(Te,Be),m(e,$t,i),m(e,Ae,i),s(Ae,Pe),s(Pe,ta),q(We,ta,null),s(Ae,Lo),s(Ae,oa),s(oa,Uo),m(e,xt,i),m(e,Ss,i),s(Ss,Vo),m(e,Et,i),q(Qe,e,i),m(e,qt,i),m(e,be,i),s(be,Go),s(be,na),s(na,Yo),s(be,Bo),s(be,Os),s(Os,Wo),s(be,Qo),m(e,kt,i),q(Je,e,i),m(e,wt,i),q(Xe,e,i),m(e,jt,i),m(e,Q,i),s(Q,Jo),s(Q,ra),s(ra,Xo),s(Q,Ko),s(Q,la),s(la,Zo),s(Q,en),s(Q,ia),s(ia,sn),s(Q,an),s(Q,ma),s(ma,tn),s(Q,on),s(Q,da),s(da,nn),s(Q,rn),m(e,yt,i),q(Ke,e,i),m(e,Dt,i),q(Ze,e,i),m(e,Ct,i),m(e,J,i),s(J,ln),s(J,pa),s(pa,mn),s(J,dn),s(J,ca),s(ca,pn),s(J,cn),s(J,ua),s(ua,un),s(J,fn),s(J,fa),s(fa,hn),s(J,_n),s(J,ha),s(ha,gn),s(J,bn),m(e,Tt,i),q(es,e,i),m(e,At,i),q(ss,e,i),m(e,St,i),m(e,K,i),s(K,vn),s(K,_a),s(_a,$n),s(K,xn),s(K,ga),s(ga,En),s(K,qn),s(K,as),s(as,ba),s(ba,kn),s(K,wn),s(K,va),s(va,jn),s(K,yn),m(e,Ot,i),q(ts,e,i),m(e,Pt,i),m(e,Ie,i),s(Ie,Dn),s(Ie,$a),s($a,Cn),s(Ie,Tn),m(e,It,i),q(os,e,i),m(e,zt,i),q(ns,e,i),m(e,Nt,i),m(e,ze,i),s(ze,An),s(ze,xa),s(xa,Sn),s(ze,On),m(e,Ft,i),q(rs,e,i),m(e,Rt,i),m(e,ee,i),s(ee,Ea),s(Ea,Z),s(Z,Ht),s(Z,Pn),s(Z,qa),s(qa,In),s(Z,zn),s(Z,ka),s(ka,Nn),s(Z,Fn),s(Z,wa),s(wa,Rn),s(Z,Hn),s(Z,ja),s(ja,Mn),s(ee,Ln),s(ee,de),s(de,se),s(se,ya),s(ya,Un),s(se,Vn),s(se,Da),s(Da,Gn),s(se,Yn),s(se,Ca),s(Ca,Bn),s(se,Wn),s(se,Ta),s(Ta,Qn),s(se,Jn),s(se,Aa),s(Aa,Xn),s(de,Kn),s(de,ae),s(ae,Sa),s(Sa,Zn),s(ae,er),s(ae,Oa),s(Oa,sr),s(ae,ar),s(ae,Pa),s(Pa,tr),s(ae,or),s(ae,Ia),s(Ia,nr),s(ae,rr),s(ae,za),s(za,lr),s(de,ir),s(de,te),s(te,Na),s(Na,mr),s(te,dr),s(te,Fa),s(Fa,pr),s(te,cr),s(te,Ra),s(Ra,ur),s(te,fr),s(te,Ha),s(Ha,hr),s(te,_r),s(te,Ma),s(Ma,gr),s(de,br),s(de,oe),s(oe,La),s(La,vr),s(oe,$r),s(oe,Ua),s(Ua,xr),s(oe,Er),s(oe,Va),s(Va,qr),s(oe,kr),s(oe,Ga),s(Ga,wr),s(oe,jr),s(oe,Ya),s(Ya,yr),m(e,Mt,i),m(e,re,i),s(re,Dr),s(re,Ba),s(Ba,Cr),s(re,Tr),s(re,Wa),s(Wa,Ar),s(re,Sr),s(re,Qa),s(Qa,Or),s(re,Pr),m(e,Lt,i),q(ls,e,i),m(e,Ut,i),q(is,e,i),m(e,Vt,i),m(e,Ps,i),s(Ps,Ir),m(e,Gt,i),q(Ne,e,i),m(e,Yt,i),m(e,Fe,i),s(Fe,zr),s(Fe,Ja),s(Ja,Nr),s(Fe,Fr),m(e,Bt,i),q(ms,e,i),m(e,Wt,i),m(e,Is,i),s(Is,Rr),m(e,Qt,i),q(ds,e,i),m(e,Jt,i),q(ps,e,i),m(e,Xt,i),m(e,ve,i),s(ve,Hr),s(ve,Xa),s(Xa,Mr),s(ve,Lr),s(ve,Ka),s(Ka,Ur),s(ve,Vr),m(e,Kt,i),q(cs,e,i),m(e,Zt,i),m(e,zs,i),s(zs,Gr),m(e,eo,i),m(e,Se,i),s(Se,Re),s(Re,Za),q(us,Za,null),s(Se,Yr),s(Se,et),s(et,Br),m(e,so,i),m(e,Y,i),s(Y,Wr),s(Y,Ns),s(Ns,Qr),s(Y,Jr),s(Y,st),s(st,Xr),s(Y,Kr),s(Y,at),s(at,Zr),s(Y,el),s(Y,fs),s(fs,sl),s(Y,al),s(Y,tt),s(tt,tl),s(Y,ol),s(Y,hs),s(hs,nl),s(Y,rl),s(Y,ot),s(ot,ll),s(Y,il),m(e,ao,i),ws[pe].m(e,i),m(e,Fs,i),m(e,$e,i),s($e,ml),s($e,nt),s(nt,dl),s($e,pl),s($e,rt),s(rt,cl),s($e,ul),m(e,to,i),q(_s,e,i),m(e,oo,i),m(e,Rs,i),s(Rs,fl),m(e,no,i),js[ue].m(e,i),m(e,Hs,i),m(e,Ms,i),s(Ms,hl),m(e,ro,i),m(e,Oe,i),s(Oe,He),s(He,lt),q(gs,lt,null),s(Oe,_l),s(Oe,it),s(it,gl),m(e,lo,i),m(e,xe,i),s(xe,bl),s(xe,mt),s(mt,vl),s(xe,$l),s(xe,bs),s(bs,xl),s(xe,El),m(e,io,i),m(e,Ee,i),s(Ee,ql),s(Ee,dt),s(dt,kl),s(Ee,wl),s(Ee,pt),s(pt,jl),s(Ee,yl),m(e,mo,i),q(vs,e,i),m(e,po,i),m(e,Me,i),s(Me,Dl),s(Me,ct),s(ct,Cl),s(Me,Tl),m(e,co,i),ys[he].m(e,i),m(e,Ls,i),m(e,Us,i),s(Us,Al),m(e,uo,i),q($s,e,i),m(e,fo,i),m(e,qe,i),s(qe,Sl),s(qe,ut),s(ut,Ol),s(qe,Pl),s(qe,ft),s(ft,Il),s(qe,zl),m(e,ho,i),q(xs,e,i),m(e,_o,i),m(e,Vs,i),s(Vs,Nl),m(e,go,i),q(Es,e,i),m(e,bo,i),q(qs,e,i),m(e,vo,i),m(e,Gs,i),s(Gs,Fl),m(e,$o,i),q(Le,e,i),xo=!0},p(e,[i]){const Ds={};i&1&&(Ds.fw=e[0]),f.$set(Ds);let Ys=S;S=Ll(e),S!==Ys&&(Oo(),g(ks[Ys],1,1,()=>{ks[Ys]=null}),So(),z=ks[S],z||(z=ks[S]=Ml[S](e),z.c()),b(z,1),z.m(N.parentNode,N));const ht={};i&2&&(ht.$$scope={dirty:i,ctx:e}),Ne.$set(ht);let Bs=pe;pe=Vl(e),pe!==Bs&&(Oo(),g(ws[Bs],1,1,()=>{ws[Bs]=null}),So(),ce=ws[pe],ce||(ce=ws[pe]=Ul[pe](e),ce.c()),b(ce,1),ce.m(Fs.parentNode,Fs));let Ue=ue;ue=Yl(e),ue!==Ue&&(Oo(),g(js[Ue],1,1,()=>{js[Ue]=null}),So(),fe=js[ue],fe||(fe=js[ue]=Gl[ue](e),fe.c()),b(fe,1),fe.m(Hs.parentNode,Hs));let Ws=he;he=Wl(e),he!==Ws&&(Oo(),g(ys[Ws],1,1,()=>{ys[Ws]=null}),So(),_e=ys[he],_e||(_e=ys[he]=Bl[he](e),_e.c()),b(_e,1),_e.m(Ls.parentNode,Ls));const Cs={};i&2&&(Cs.$$scope={dirty:i,ctx:e}),Le.$set(Cs)},i(e){xo||(b(f.$$.fragment,e),b(_.$$.fragment,e),b(z),b(D.$$.fragment,e),b(W.$$.fragment,e),b(We.$$.fragment,e),b(Qe.$$.fragment,e),b(Je.$$.fragment,e),b(Xe.$$.fragment,e),b(Ke.$$.fragment,e),b(Ze.$$.fragment,e),b(es.$$.fragment,e),b(ss.$$.fragment,e),b(ts.$$.fragment,e),b(os.$$.fragment,e),b(ns.$$.fragment,e),b(rs.$$.fragment,e),b(ls.$$.fragment,e),b(is.$$.fragment,e),b(Ne.$$.fragment,e),b(ms.$$.fragment,e),b(ds.$$.fragment,e),b(ps.$$.fragment,e),b(cs.$$.fragment,e),b(us.$$.fragment,e),b(ce),b(_s.$$.fragment,e),b(fe),b(gs.$$.fragment,e),b(vs.$$.fragment,e),b(_e),b($s.$$.fragment,e),b(xs.$$.fragment,e),b(Es.$$.fragment,e),b(qs.$$.fragment,e),b(Le.$$.fragment,e),xo=!0)},o(e){g(f.$$.fragment,e),g(_.$$.fragment,e),g(z),g(D.$$.fragment,e),g(W.$$.fragment,e),g(We.$$.fragment,e),g(Qe.$$.fragment,e),g(Je.$$.fragment,e),g(Xe.$$.fragment,e),g(Ke.$$.fragment,e),g(Ze.$$.fragment,e),g(es.$$.fragment,e),g(ss.$$.fragment,e),g(ts.$$.fragment,e),g(os.$$.fragment,e),g(ns.$$.fragment,e),g(rs.$$.fragment,e),g(ls.$$.fragment,e),g(is.$$.fragment,e),g(Ne.$$.fragment,e),g(ms.$$.fragment,e),g(ds.$$.fragment,e),g(ps.$$.fragment,e),g(cs.$$.fragment,e),g(us.$$.fragment,e),g(ce),g(_s.$$.fragment,e),g(fe),g(gs.$$.fragment,e),g(vs.$$.fragment,e),g(_e),g($s.$$.fragment,e),g(xs.$$.fragment,e),g(Es.$$.fragment,e),g(qs.$$.fragment,e),g(Le.$$.fragment,e),xo=!1},d(e){a(c),e&&a($),k(f,e),e&&a(j),e&&a(h),k(_),e&&a(I),ks[S].d(e),e&&a(N),e&&a(R),e&&a(U),k(D,e),e&&a(G),e&&a(u),k(W),e&&a(gt),e&&a(ge),e&&a(bt),e&&a(As),e&&a(vt),e&&a(Te),e&&a($t),e&&a(Ae),k(We),e&&a(xt),e&&a(Ss),e&&a(Et),k(Qe,e),e&&a(qt),e&&a(be),e&&a(kt),k(Je,e),e&&a(wt),k(Xe,e),e&&a(jt),e&&a(Q),e&&a(yt),k(Ke,e),e&&a(Dt),k(Ze,e),e&&a(Ct),e&&a(J),e&&a(Tt),k(es,e),e&&a(At),k(ss,e),e&&a(St),e&&a(K),e&&a(Ot),k(ts,e),e&&a(Pt),e&&a(Ie),e&&a(It),k(os,e),e&&a(zt),k(ns,e),e&&a(Nt),e&&a(ze),e&&a(Ft),k(rs,e),e&&a(Rt),e&&a(ee),e&&a(Mt),e&&a(re),e&&a(Lt),k(ls,e),e&&a(Ut),k(is,e),e&&a(Vt),e&&a(Ps),e&&a(Gt),k(Ne,e),e&&a(Yt),e&&a(Fe),e&&a(Bt),k(ms,e),e&&a(Wt),e&&a(Is),e&&a(Qt),k(ds,e),e&&a(Jt),k(ps,e),e&&a(Xt),e&&a(ve),e&&a(Kt),k(cs,e),e&&a(Zt),e&&a(zs),e&&a(eo),e&&a(Se),k(us),e&&a(so),e&&a(Y),e&&a(ao),ws[pe].d(e),e&&a(Fs),e&&a($e),e&&a(to),k(_s,e),e&&a(oo),e&&a(Rs),e&&a(no),js[ue].d(e),e&&a(Hs),e&&a(Ms),e&&a(ro),e&&a(Oe),k(gs),e&&a(lo),e&&a(xe),e&&a(io),e&&a(Ee),e&&a(mo),k(vs,e),e&&a(po),e&&a(Me),e&&a(co),ys[he].d(e),e&&a(Ls),e&&a(Us),e&&a(uo),k($s,e),e&&a(fo),e&&a(qe),e&&a(ho),k(xs,e),e&&a(_o),e&&a(Vs),e&&a(go),k(Es,e),e&&a(bo),k(qs,e),e&&a(vo),e&&a(Gs),e&&a($o),k(Le,e)}}}const Km={local:"busca-semntica-com-o-faiss",sections:[{local:"usando-embeddings-para-pesquisa-semntica",title:"Usando embeddings para pesquisa sem\xE2ntica"},{local:"carregando-e-preparando-o-conjunto-de-dados",title:"Carregando e preparando o conjunto de dados"},{local:"criando-embeddings-de-texto",title:"Criando embeddings de texto"},{local:"usando-faiss-para-busca-de-similaridade",title:"Usando FAISS para busca de similaridade"}],title:"Busca sem\xE2ntica com o FAISS"};function Zm(L,c,$){let f="pt";return Fm(()=>{const j=new URLSearchParams(window.location.search);$(0,f=j.get("fw")||"pt")}),[f]}class ld extends Pm{constructor(c){super();Im(this,c,Zm,Xm,zm,{})}}export{ld as default,Km as metadata};
