import{S as Op,i as Ap,s as Ip,e as r,t,k as m,w as $,c as p,a as c,h as n,d as s,m as f,x as j,g as d,G as a,y as E,q as g,o as v,B as x,l as Pp,M as Lp,b as y,p as Ca,v as Np,n as ya}from"../../chunks/vendor-hf-doc-builder.js";import{T as Wl}from"../../chunks/Tip-hf-doc-builder.js";import{Y as zt}from"../../chunks/Youtube-hf-doc-builder.js";import{I as In}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Sp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Fp}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Mp(w){let i,u;return i=new Sp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Rp(w){let i,u;return i=new Sp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Qp(w){let i,u,o,b,k,h,_,C;return _=new T({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># Same as before</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># This is new</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){i=r("p"),u=t("Continuando l\u2019esempio del "),o=r("a"),b=t("capitolo precedente"),k=t(", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in TensorFlow:"),h=m(),$(_.$$.fragment),this.h()},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"Continuando l\u2019esempio del "),o=p(q,"A",{href:!0});var I=c(o);b=n(I,"capitolo precedente"),I.forEach(s),k=n(q,", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in TensorFlow:"),q.forEach(s),h=f(z),j(_.$$.fragment,z),this.h()},h(){y(o,"href","/course/chapter2")},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),d(z,h,q),E(_,z,q),C=!0},i(z){C||(g(_.$$.fragment,z),C=!0)},o(z){v(_.$$.fragment,z),C=!1},d(z){z&&s(i),z&&s(h),x(_,z)}}}function Bp(w){let i,u,o,b,k,h,_,C;return _=new T({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># Same as before</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># This is new</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){i=r("p"),u=t("Continuando l\u2019esempio del "),o=r("a"),b=t("capitolo precedente"),k=t(", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in Pytorch:"),h=m(),$(_.$$.fragment),this.h()},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"Continuando l\u2019esempio del "),o=p(q,"A",{href:!0});var I=c(o);b=n(I,"capitolo precedente"),I.forEach(s),k=n(q,", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in Pytorch:"),q.forEach(s),h=f(z),j(_.$$.fragment,z),this.h()},h(){y(o,"href","/course/chapter2")},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),d(z,h,q),E(_,z,q),C=!0},i(z){C||(g(_.$$.fragment,z),C=!0)},o(z){v(_.$$.fragment,z),C=!1},d(z){z&&s(i),z&&s(h),x(_,z)}}}function Hp(w){let i,u;return i=new zt({props:{id:"W_gMJF0xomE"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Wp(w){let i,u;return i=new zt({props:{id:"_BZearw7f0w"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Up(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Da provare!"),k=t(" Quali sono le label dell\u2019elemento 15 del training set, e 87 del validation set?")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var C=c(o);b=n(C,"Da provare!"),C.forEach(s),k=n(_," Quali sono le label dell\u2019elemento 15 del training set, e 87 del validation set?"),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function Gp(w){let i,u;return i=new zt({props:{id:"P-rZWqcB6CE"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Vp(w){let i,u;return i=new zt({props:{id:"0u3ioSwev3s"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Jp(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Da provare!"),k=t(" Prendere l\u2019element 15 del training set e tokenizzare le due frasi sia separatamente, sia come coppia. Qual \xE8 la differenza tra i due risultati?")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var C=c(o);b=n(C,"Da provare!"),C.forEach(s),k=n(_," Prendere l\u2019element 15 del training set e tokenizzare le due frasi sia separatamente, sia come coppia. Qual \xE8 la differenza tra i due risultati?"),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function Kp(w){let i,u,o,b,k,h,_,C;return{c(){i=r("p"),u=t("La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=r("em"),b=t("collate function"),k=t(" ("),h=r("em"),_=t("funzione di raccolta"),C=t("). Il  collator di default \xE8 la funzione che converte semplicemente i campioni in tf.Tensor e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9.")},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=p(q,"EM",{});var I=c(o);b=n(I,"collate function"),I.forEach(s),k=n(q," ("),h=p(q,"EM",{});var L=c(h);_=n(L,"funzione di raccolta"),L.forEach(s),C=n(q,"). Il  collator di default \xE8 la funzione che converte semplicemente i campioni in tf.Tensor e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9."),q.forEach(s)},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,C)},d(z){z&&s(i)}}}function Yp(w){let i,u,o,b,k,h,_,C,z,q,I;return{c(){i=r("p"),u=t("La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=r("em"),b=t("collate function"),k=t(" ("),h=r("em"),_=t("funzione di raccolta"),C=t("). \xC8 uno dei parametri che si possono passare quando si costruisce un "),z=r("code"),q=t("DataLoader"),I=t(", e il default \xE8 la funzione che converte semplicemente i campioni in tensori PyTorch e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9.")},l(L){i=p(L,"P",{});var D=c(i);u=n(D,"La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=p(D,"EM",{});var F=c(o);b=n(F,"collate function"),F.forEach(s),k=n(D," ("),h=p(D,"EM",{});var H=c(h);_=n(H,"funzione di raccolta"),H.forEach(s),C=n(D,"). \xC8 uno dei parametri che si possono passare quando si costruisce un "),z=p(D,"CODE",{});var O=c(z);q=n(O,"DataLoader"),O.forEach(s),I=n(D,", e il default \xE8 la funzione che converte semplicemente i campioni in tensori PyTorch e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9."),D.forEach(s)},m(L,D){d(L,i,D),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,C),a(i,z),a(z,q),a(i,I)},d(L){L&&s(i)}}}function Zp(w){let i,u;return i=new T({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function Xp(w){let i,u;return i=new T({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function ec(w){let i,u,o,b,k;return i=new T({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment),u=m(),o=r("p"),b=t("Ottimo! Adesso che siamo passati dal testo grezzo a dei batch che il modello \xE8 in grado di trattare, siamo pronti per affinarlo!")},l(h){j(i.$$.fragment,h),u=f(h),o=p(h,"P",{});var _=c(o);b=n(_,"Ottimo! Adesso che siamo passati dal testo grezzo a dei batch che il modello \xE8 in grado di trattare, siamo pronti per affinarlo!"),_.forEach(s)},m(h,_){E(i,h,_),d(h,u,_),d(h,o,_),a(o,b),k=!0},i(h){k||(g(i.$$.fragment,h),k=!0)},o(h){v(i.$$.fragment,h),k=!1},d(h){x(i,h),h&&s(u),h&&s(o)}}}function ac(w){let i,u;return i=new T({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function sc(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Da provare!"),k=t(" Replicare il preprocessing sul dataset GLUE SST-2. \xC8 leggermente diverso poiche \xE8 composto da frasi singole e non da coppie di frasi, ma il resto della procedura dovrebbe essere simile. Per una sfida pi\xF9 complessa, provare a scrivere una funzione di preprocessing che funzioni per qualsiasi dei compiti in GLUE.")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var C=c(o);b=n(C,"Da provare!"),C.forEach(s),k=n(_," Replicare il preprocessing sul dataset GLUE SST-2. \xC8 leggermente diverso poiche \xE8 composto da frasi singole e non da coppie di frasi, ma il resto della procedura dovrebbe essere simile. Per una sfida pi\xF9 complessa, provare a scrivere una funzione di preprocessing che funzioni per qualsiasi dei compiti in GLUE."),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function Tp(w){let i,u,o,b,k,h,_,C,z,q,I,L,D,F,H,O,R,K,re,Ce;return O=new T({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){i=r("p"),u=t("Ora che sono stati definiti un dataset e un collator (raccoglitore) di dati, dobbiamo metterli insieme. Si potrebbero caricare e raccogliere i batch manualmente, ma significherebbe molto lavoro e probabilmente una bassa performance. Invece, vi \xE8 un metodo semplice che offre una soluzione con buona performance a questo problema: "),o=r("code"),b=t("to_tf_dataset()"),k=t(". Questo impacchetter\xE0 il dataset con "),h=r("code"),_=t("tf.data.Dataset"),C=t(`, con una collate function opzionale.
`),z=r("code"),q=t("tf.data.Dataset"),I=t(" \xE8 un formato nativo di TensorFlow che Keras pu\xF2 utilizzare durante "),L=r("code"),D=t("model.fit()"),F=t(", cosicch\xE9 questo metodo converte immediatamente un \u{1F917} Dataset in un formato pronto per l\u2019addestramento. Vediamolo in azione col nostro dataset!"),H=m(),$(O.$$.fragment),R=m(),K=r("p"),re=t("Fine! Questi dataset verranno utilizzati nelle prossime lezioni, dove l\u2019addestramento sar\xE0 reso piacevolmente immediato dopo tutto questo duro lavoro di preprocessing.")},l(P){i=p(P,"P",{});var S=c(i);u=n(S,"Ora che sono stati definiti un dataset e un collator (raccoglitore) di dati, dobbiamo metterli insieme. Si potrebbero caricare e raccogliere i batch manualmente, ma significherebbe molto lavoro e probabilmente una bassa performance. Invece, vi \xE8 un metodo semplice che offre una soluzione con buona performance a questo problema: "),o=p(S,"CODE",{});var Da=c(o);b=n(Da,"to_tf_dataset()"),Da.forEach(s),k=n(S,". Questo impacchetter\xE0 il dataset con "),h=p(S,"CODE",{});var pe=c(h);_=n(pe,"tf.data.Dataset"),pe.forEach(s),C=n(S,`, con una collate function opzionale.
`),z=p(S,"CODE",{});var Pa=c(z);q=n(Pa,"tf.data.Dataset"),Pa.forEach(s),I=n(S," \xE8 un formato nativo di TensorFlow che Keras pu\xF2 utilizzare durante "),L=p(S,"CODE",{});var Ta=c(L);D=n(Ta,"model.fit()"),Ta.forEach(s),F=n(S,", cosicch\xE9 questo metodo converte immediatamente un \u{1F917} Dataset in un formato pronto per l\u2019addestramento. Vediamolo in azione col nostro dataset!"),S.forEach(s),H=f(P),j(O.$$.fragment,P),R=f(P),K=p(P,"P",{});var We=c(K);re=n(We,"Fine! Questi dataset verranno utilizzati nelle prossime lezioni, dove l\u2019addestramento sar\xE0 reso piacevolmente immediato dopo tutto questo duro lavoro di preprocessing."),We.forEach(s)},m(P,S){d(P,i,S),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,C),a(i,z),a(z,q),a(i,I),a(i,L),a(L,D),a(i,F),d(P,H,S),E(O,P,S),d(P,R,S),d(P,K,S),a(K,re),Ce=!0},i(P){Ce||(g(O.$$.fragment,P),Ce=!0)},o(P){v(O.$$.fragment,P),Ce=!1},d(P){P&&s(i),P&&s(H),x(O,P),P&&s(R),P&&s(K)}}}function tc(w){let i,u,o,b,k,h,_,C,z,q,I,L,D,F,H,O,R,K,re,Ce,P,S,Da,pe,Pa,Ta,We,Ee,ye,rs,Ue,Ln,ps,Nn,kt,ee,ae,Sa,Y,Fn,Ge,Mn,Rn,Ve,Qn,Bn,Je,Hn,Wn,$t,Oa,Un,jt,Ke,Et,Ye,xt,Q,Gn,cs,Vn,Jn,ds,Kn,Yn,us,Zn,Xn,ms,ei,ai,fs,si,ti,qt,ce,ni,hs,ii,oi,_s,li,ri,wt,De,pi,bs,ci,di,Ct,Ze,yt,Xe,Dt,de,ui,gs,mi,fi,vs,hi,_i,Pt,ea,Tt,aa,St,B,bi,zs,gi,vi,ks,zi,ki,$s,$i,ji,js,Ei,xi,Es,qi,wi,Ot,Pe,At,xe,Te,xs,sa,Ci,qs,yi,It,se,te,Aa,Ia,Di,Lt,ta,Nt,La,Pi,Ft,na,Mt,ia,Rt,W,Ti,Na,Si,Oi,ws,Ai,Ii,Cs,Li,Ni,ys,Fi,Mi,Qt,Se,Bt,Oe,Ri,Ds,Qi,Bi,Ht,oa,Wt,Fa,Hi,Ut,la,Gt,ue,Wi,Ps,Ui,Gi,Ts,Vi,Ji,Vt,ra,Jt,U,Ki,Ss,Yi,Zi,Os,Xi,eo,As,ao,so,Is,to,no,Kt,me,io,Ls,oo,lo,Ns,ro,po,Yt,G,co,Fs,uo,mo,Ma,fo,ho,Ms,_o,bo,Rs,go,vo,Zt,Ra,zo,Xt,Ae,ko,Qs,$o,jo,en,fe,Eo,Qa,xo,qo,Ba,wo,Co,an,pa,sn,V,yo,Bs,Do,Po,Hs,To,So,Ws,Oo,Ao,ca,Io,Lo,tn,he,No,da,Us,Fo,Mo,Gs,Ro,Qo,nn,ua,on,A,Bo,Vs,Ho,Wo,Js,Uo,Go,Ks,Vo,Jo,Ys,Ko,Yo,Zs,Zo,Xo,Xs,el,al,et,sl,tl,ma,nl,il,ln,Ie,ol,at,ll,rl,rn,_e,pl,st,cl,dl,tt,ul,ml,pn,fa,cn,Ha,fl,dn,ha,un,be,hl,nt,_l,bl,it,gl,vl,mn,J,zl,ot,kl,$l,lt,jl,El,rt,xl,ql,pt,wl,Cl,fn,Le,yl,ct,Dl,Pl,hn,qe,Ne,dt,_a,Tl,ut,Sl,_n,ba,bn,Wa,Fe,Ol,mt,Al,Il,gn,ne,ie,Ua,ge,Ll,ft,Nl,Fl,ht,Ml,Rl,vn,ga,zn,va,kn,Me,Ql,_t,Bl,Hl,$n,za,jn,oe,le,Ga,Re,En,Va,xn;o=new Fp({props:{fw:w[0]}}),C=new In({});const Ul=[Rp,Mp],ka=[];function Gl(e,l){return e[0]==="pt"?0:1}D=Gl(w),F=ka[D]=Ul[D](w);const Vl=[Bp,Qp],$a=[];function Jl(e,l){return e[0]==="pt"?0:1}O=Jl(w),R=$a[O]=Vl[O](w),Ue=new In({});const Kl=[Wp,Hp],ja=[];function Yl(e,l){return e[0]==="pt"?0:1}ee=Yl(w),ae=ja[ee]=Kl[ee](w),Ke=new T({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),Ye=new T({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),Ze=new T({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),Xe=new T({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>}`}}),ea=new T({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),aa=new T({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Pe=new Wl({props:{$$slots:{default:[Up]},$$scope:{ctx:w}}}),sa=new In({});const Zl=[Vp,Gp],Ea=[];function Xl(e,l){return e[0]==="pt"?0:1}se=Xl(w),te=Ea[se]=Zl[se](w),ta=new T({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),na=new T({props:{code:`inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>)
inputs`}}),ia=new T({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Se=new Wl({props:{$$slots:{default:[Jp]},$$scope:{ctx:w}}}),oa=new T({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),la=new T({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),ra=new T({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),pa=new T({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),ua=new T({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),fa=new T({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),ha=new T({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),_a=new In({}),ba=new zt({props:{id:"7q5NyFT8REg"}});function er(e,l){return e[0]==="pt"?Yp:Kp}let qn=er(w),we=qn(w);const ar=[Xp,Zp],xa=[];function sr(e,l){return e[0]==="pt"?0:1}ne=sr(w),ie=xa[ne]=ar[ne](w),ga=new T({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),va=new T({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),za=new T({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const tr=[ac,ec],qa=[];function nr(e,l){return e[0]==="tf"?0:1}oe=nr(w),le=qa[oe]=tr[oe](w),Re=new Wl({props:{$$slots:{default:[sc]},$$scope:{ctx:w}}});let N=w[0]==="tf"&&Tp();return{c(){i=r("meta"),u=m(),$(o.$$.fragment),b=m(),k=r("h1"),h=r("a"),_=r("span"),$(C.$$.fragment),z=m(),q=r("span"),I=t("Processare i dati"),L=m(),F.c(),H=m(),R.c(),K=m(),re=r("p"),Ce=t("Ovviamente, addestrare il modello su due frasi non porter\xE0 a dei risultati molto buoni. Per ottenere risultati migliori, si deve preparare un dataset pi\xF9 grande."),P=m(),S=r("p"),Da=t("In questa sezione verr\xE0 usato come esempio il dataset MRPC (Microsoft Research Paraphrase Corpus), presentato nell\u2019"),pe=r("a"),Pa=t("articolo"),Ta=t(` di William B. Dolan e Chris Brockett. Il dataset contiene 5801 coppie di
frasi, con una label che indica se l\u2019una \xE8 una parafrasi dell\u2019altra (i.e. se hanno lo stesso significato). \xC8 stato selezionato per
questo capitolo perch\xE9 \xE8 un dataset piccolo, con cui \xE8 facile sperimentare durante l\u2019addestramento.`),We=m(),Ee=r("h3"),ye=r("a"),rs=r("span"),$(Ue.$$.fragment),Ln=m(),ps=r("span"),Nn=t("Caricare un dataset dall'Hub"),kt=m(),ae.c(),Sa=m(),Y=r("p"),Fn=t("L\u2019Hub non contiene solo modelli; contiene anche molti dataset in tante lingue diverse. I dataset possono essere esplorati "),Ge=r("a"),Mn=t("qui"),Rn=t(", ed \xE8 consigliato tentare di caricare e processare un nuovo dataset dopo aver completato questa sezione (cfr. la "),Ve=r("a"),Qn=t("documentazione"),Bn=t("). Per ora, focalizziamoci sul dataset MRPC! Questo \xE8 uno dei 10 dataset che fanno parte del "),Je=r("a"),Hn=t("GLUE benchmark"),Wn=t(", che \xE8 un benchmark accademico usato per misurare la performance di modelli ML su 10 compiti di classificazione del testo."),$t=m(),Oa=r("p"),Un=t("La libreria \u{1F917} Datasets fornisce un comando molto semplice per scaricare e mettere nella cache un dataset sull\u2019Hub. Il dataset MRPC pu\xF2 essere scaricato cos\xEC:"),jt=m(),$(Ke.$$.fragment),Et=m(),$(Ye.$$.fragment),xt=m(),Q=r("p"),Gn=t("Il risultato \xE8 un oggetto di tipo "),cs=r("code"),Vn=t("DatasetDict"),Jn=t(" che contiene il training set, il validation set, e il test set. Ciascuno di questi contiene svariate colonne, ("),ds=r("code"),Kn=t("sentence1"),Yn=t(", "),us=r("code"),Zn=t("sentence2"),Xn=t(", "),ms=r("code"),ei=t("label"),ai=t(", e "),fs=r("code"),si=t("idx"),ti=t(") a un numero variabile di righe, corrispondenti al numero di elementi in ogni set (quindi, vi sono 3668 coppie di frasi nel training set, 408 nel validation set, e 1725 nel test set)."),qt=m(),ce=r("p"),ni=t("Questo comando scarica il dataset e lo mette in cache, in "),hs=r("em"),ii=t("~/.cache/huggingface/dataset"),oi=t(" secondo l\u2019impostazione predefinita. Nel Capitolo 2 \xE8 stato spiegato come personalizzare la cartella di cache valorizzando la variabile d\u2019ambiente "),_s=r("code"),li=t("HF_HOME"),ri=t("."),wt=m(),De=r("p"),pi=t("Ogni coppia di frasi nell\u2019ogetto "),bs=r("code"),ci=t("raw_datasets"),di=t(" pu\xF2 essere ottenuto tramite indexing, come in un dizionario:"),Ct=m(),$(Ze.$$.fragment),yt=m(),$(Xe.$$.fragment),Dt=m(),de=r("p"),ui=t("Le label sono gi\xE0 numeri interi, quindi non \xE8 necessario alcun preprocessing. Per sapere a quale numero corrisponde quale tipo di label, si possono analizzare le "),gs=r("code"),mi=t("features"),fi=t(" del "),vs=r("code"),hi=t("raw_train_dataset"),_i=t(". Ci\xF2 permette di capire la tipologia di ogni colonna:"),Pt=m(),$(ea.$$.fragment),Tt=m(),$(aa.$$.fragment),St=m(),B=r("p"),bi=t("Dietro le quinte, "),zs=r("code"),gi=t("label"),vi=t(" \xE8 del tipo ClassLabel"),ks=r("code"),zi=t(", e la corrispondenza tra i numeri e i nomi delle label \xE8 contenuta nella cartella *names*. "),ki=t("0"),$s=r("code"),$i=t("corrisponde a"),ji=t("not_equivalent"),js=r("code"),Ei=t("(significato diverso), e"),xi=t("1"),Es=r("code"),qi=t("corrisponde a"),wi=t("equivalent` (stesso significato)."),Ot=m(),$(Pe.$$.fragment),At=m(),xe=r("h3"),Te=r("a"),xs=r("span"),$(sa.$$.fragment),Ci=m(),qs=r("span"),yi=t("Preprocessing del dataset"),It=m(),te.c(),Aa=m(),Ia=r("p"),Di=t("Per preprocessare il dataset, \xE8 necessario convertire il testo in numeri comprensibili al modello. Come dimostrato nel [capitolo precedente]/course/chapter2), ci\xF2 viene fatto con un tokenizzatore. Il tokenizer prende come input sia una frase sia una lista di frasi, quindi \xE8 possibile effettuare la tokenizzazione di tutte le prime e seconde frasi di ogni coppia in questo modo:"),Lt=m(),$(ta.$$.fragment),Nt=m(),La=r("p"),Pi=t("Tuttavia, non si possono semplicemente passare al modello due frasi e sperare di predire se l\u2019una \xE8 una parafrasi dell\u2019altra o no. Bisogna gestire le due frasi come una coppia, e applicare il preprocessing necessario. Fortunatamente, il tokenizzatore pu\xF2 anche prendere come input una coppia di frasi e prepararla nel formato atteso dal modello BERT:"),Ft=m(),$(na.$$.fragment),Mt=m(),$(ia.$$.fragment),Rt=m(),W=r("p"),Ti=t("Sono state gi\xE0 discusse nel "),Na=r("a"),Si=t("Capitolo 2"),Oi=t(" le chiavi "),ws=r("code"),Ai=t("input_ids"),Ii=t(" e "),Cs=r("code"),Li=t("attention_mask"),Ni=t(", ma il discorso su "),ys=r("code"),Fi=t("token_type_ids"),Mi=t(" era stato rimandato. In questo esempio, ci\xF2 pu\xF2 essere usato per indicare al modello quale parte dell\u2019input \xE8 la prima frase, e quale la seconda."),Qt=m(),$(Se.$$.fragment),Bt=m(),Oe=r("p"),Ri=t("Decodificando gli ID in "),Ds=r("code"),Qi=t("input_ids"),Bi=t(" per ritrasformarli in parole:"),Ht=m(),$(oa.$$.fragment),Wt=m(),Fa=r("p"),Hi=t("si ottiene:"),Ut=m(),$(la.$$.fragment),Gt=m(),ue=r("p"),Wi=t("Perci\xF2 \xE8 chiaro che il modello si aspetta gli input nella forma "),Ps=r("code"),Ui=t("[CLS] frase1 [SEP] frase2 [SEP]"),Gi=t(" quando vi sono due frasi. Allineando con "),Ts=r("code"),Vi=t("token_type_ids"),Ji=t(" si ottiene:"),Vt=m(),$(ra.$$.fragment),Jt=m(),U=r("p"),Ki=t("Le parti dell\u2019input corrispondenti a "),Ss=r("code"),Yi=t("[CLS] frase1 [SEP]"),Zi=t(" hanno tutte un token type ID di "),Os=r("code"),Xi=t("0"),eo=t(", mentre le altre parti, corrispondenti quindi a "),As=r("code"),ao=t("frase2 [SEP]"),so=t(", hanno tutte un token type ID di "),Is=r("code"),to=t("1"),no=t("."),Kt=m(),me=r("p"),io=t("Da notare che se viene selezionato un altro checkpoint, gli input tokenizzati non conterranno necessariamente i "),Ls=r("code"),oo=t("token_type_ids"),lo=t(" (ad esempio, non si ottengono usando un modello DistilBERT). I "),Ns=r("code"),ro=t("token_type_ids"),po=t(" si ottengono solo quando il modello saprebbe che farne, avendole gi\xE0 viste in fase di pre-addestramento."),Yt=m(),G=r("p"),co=t("In questo caso, BERT \xE8 stato pre-addestrato con i token type IDs, e in aggiunta all\u2019obiettivo di "),Fs=r("em"),uo=t("[masked language modeling - check with Caterina]"),mo=t(" di cui si era parlato nel "),Ma=r("a"),fo=t("Capitolo 1"),ho=t(", vi \xE8 un altro obiettivo che si chiama "),Ms=r("em"),_o=t("next sentence prediction"),bo=t(" ("),Rs=r("em"),go=t("predire la prossima frase"),vo=t("). Lo scopo di questo task \xE8 modellizzare la relazione tra coppie di frasi."),Zt=m(),Ra=r("p"),zo=t("Durante un task di next sentence prediction, il modello riceve una coppia di frasi (con token mascherati in maniera aleatoria) e deve predire se la seconda segue la prima. Per rendere il task meno banale, la met\xE0 delle volte le frasi si susseguono nel documento da cui erano state estratte originariamente, l\u2019altra met\xE0 delle volte le frasi provengono da due documenti diversi."),Xt=m(),Ae=r("p"),ko=t("In generale, non bisogna preoccuparsi se i "),Qs=r("code"),$o=t("token_type_ids"),jo=t(" sono presenti o no negli input tokenizzati: finch\xE9 viene usato lo stesso checkpoint per il tokenizer e il modello, tutto andr\xE0 bene poich\xE9 il tokenizer sa cosa fornire al modello."),en=m(),fe=r("p"),Eo=t("Ora che abbiamo visto come il tokenizer pu\xF2 gestire una coppia di frasi, possiamo usarlo per tokenizzare l\u2019intero dataset: come nel "),Qa=r("a"),xo=t("capitolo precedente"),qo=t(", si pu\xF2 fornire al tokenizer una lista di coppie di frasi dando prima la lista delle prime frasi, e poi la lista delle seconde frasi. Questo approcchio \xE8 anche compatibile le opzioni di padding e truncation gi\xE0 viste nel "),Ba=r("a"),wo=t("Capitolo 2"),Co=t(". Perci\xF2, un modo per preprocessare il dataset di addestramento \xE8:"),an=m(),$(pa.$$.fragment),sn=m(),V=r("p"),yo=t("Questo metodo funziona, ma ha lo svantaggio di restituire un dizionario (avente "),Bs=r("code"),Do=t("input_ids"),Po=t(", "),Hs=r("code"),To=t("attention_mask"),So=t(", e "),Ws=r("code"),Oo=t("token_type_ids"),Ao=t(" come chiave, e delle liste di liste come valori). Oltretutto, questo metodo funziona solo se si ha a disposizione RAM sufficiente per contenere l\u2019intero dataset durante la tokenizzazione (mentre i dataset dalla libreria \u{1F917} Datasets sono file "),ca=r("a"),Io=t("Apache Arrow"),Lo=t(" archiviati su disco, perci\xF2 in memoria vengono caricati solo i campioni richiesti)."),tn=m(),he=r("p"),No=t("Per tenere i dati come dataset, utilizzare il metodo "),da=r("a"),Us=r("code"),Fo=t("Dataset.map()"),Mo=t(". Ci\xF2 permette anche della flessibilit\xE0 extra, qualora fosse necessario del preprocessing aggiuntivo oltre alla tokenizzazione. Il metodo "),Gs=r("code"),Ro=t("map()"),Qo=t(" applica una funziona ad ogni elemento del dataset, perci\xF2 bisogna definire una funzione che tokenizzi gli input:"),nn=m(),$(ua.$$.fragment),on=m(),A=r("p"),Bo=t("Questa funzione riceve un dizionario (come gli element del nostro dataset) e restituisce un nuovo dizionario con input_ids"),Vs=r("code"),Ho=t(", "),Wo=t("attention_mask"),Js=r("code"),Uo=t(", e "),Go=t("token_type_ids"),Ks=r("code"),Vo=t("come chiavi. Funziona anche se il dizionario"),Jo=t("example"),Ys=r("code"),Ko=t("contiene svariati campioni (ad una chiave corrisponde una lista di frasi) poich\xE9 il"),Yo=t("tokenizer"),Zs=r("code"),Zo=t("funziona con liste di coppie di frasi, come gi\xE0 visto. Ci\xF2 permette di usare l'opzione"),Xo=t("batched=True"),Xs=r("code"),el=t("nella chiamata a"),al=t("map()"),et=r("code"),sl=t(", che accelerer\xE0 di molto la tokenizzazione. Il "),tl=t("tokenizer` si appoggia ad un tokenizer scritto in Rust della libreria "),ma=r("a"),nl=t("\u{1F917} Tokenizers"),il=t(". Questo tokenizer pu\xF2 essere molto veloce, ma solo se gli vengono forniti molti input insieme."),ln=m(),Ie=r("p"),ol=t("Per ora non ci siamo preoccupati del parametro "),at=r("code"),ll=t("padding"),rl=t(" nella nostra funzione di tokenizzazione. Questo perch\xE9 il padding di tutti i campioni fino a lunghezza massima non \xE8 efficiente: \xE8 meglio fare il padding dei campioni quando stiamo assemblando una batch, poich\xE9 in quel momento \xE8 necessario il padding solo fino alla lunghezza massima nel batch, non la lunghezza massima nell\u2019intero dataset. Ci\xF2 permette di risparmiare molto tempo e potenza di calcolo nel caso in cui gli input abbiano lunghezze molto varie!"),rn=m(),_e=r("p"),pl=t("Ecco come si applica la funzione di tokenizzazione sull\u2019intero dataset. Bisogna usare "),st=r("code"),cl=t("batched=True"),dl=t(" nella chiamata a "),tt=r("code"),ul=t("map"),ml=t(" in modo tale che la funzione venga applicata a vari elementi del dataset insieme, e non ad ogni elemento separatamente. Ci\xF2 permette un preprocessing pi\xF9 rapido."),pn=m(),$(fa.$$.fragment),cn=m(),Ha=r("p"),fl=t("La libreria \u{1F917} Datasets aggiunge nuovi campi ai dataset, uno per ogni chiave nel dizionario restituito dalla funzione di preprocessing:"),dn=m(),$(ha.$$.fragment),un=m(),be=r("p"),hl=t("Si pu\xF2 anche applicare il multiprocessing durante il preprocessing con la funzione "),nt=r("code"),_l=t("map()"),bl=t(" utilizzando il parametro "),it=r("code"),gl=t("num_proc"),vl=t(". Ci\xF2 non \xE8 stato dimostrato qui perch\xE9 la libreria \u{1F917} Tokenizers gi\xE0 utilizza vari thread per tokenizzare i campioni pi\xF9 rapidamente, ma nel caso in cui non venga usato un tokenizer rapido di questa libreria, ci\xF2 potrebbe velocizzare il preprocessing."),mn=m(),J=r("p"),zl=t("La funzione tokenize_function"),ot=r("code"),kl=t("restituisce un dizionario con"),$l=t("input_ids"),lt=r("code"),jl=t(", "),El=t("attention_mask"),rt=r("code"),xl=t(", e "),ql=t("token_type_ids"),pt=r("code"),wl=t("come chiavi, quindi quei tre campi vengono aggiunti a tutti gli *[split - check!]* del dataset. Si possono anche cambiare i campi esistenti nel caso in cui la funzione di preprocessing restituisca un nuovo valore per una chiave gi\xE0 esistente nel dataset a cui viene applicato"),Cl=t("map()`."),fn=m(),Le=r("p"),yl=t("L\u2019ultima cosa da fare \xE8 il padding di tutti i campioni alla lunghezza dell\u2019elemento pi\xF9 lungo quando sono inseriti in una batch \u2014 una tecnica che si chiama "),ct=r("em"),Dl=t("dynamic padding"),Pl=t("."),hn=m(),qe=r("h3"),Ne=r("a"),dt=r("span"),$(_a.$$.fragment),Tl=m(),ut=r("span"),Sl=t("Dynamic padding"),_n=m(),$(ba.$$.fragment),bn=m(),we.c(),Wa=m(),Fe=r("p"),Ol=t("In pratica, bisogna definire una collate function che applichi la giusta quantit\xE0 di padding agli elementi del dataset in una stessa batch. Fortunatamente, la libreria \u{1F917} Transformers fornisce questa funziona tramite "),mt=r("code"),Al=t("DataCollatorWithPadding"),Il=t(". Essa prende in input un tokenizer quando viene istanziata (per individuare quale token da usare per il padding, e se il modello si aspetta padding a sinistra o a destra dell\u2019input) e far\xE0 tutto il necessario:"),gn=m(),ie.c(),Ua=m(),ge=r("p"),Ll=t("Per testare questo nuovo gioco, analizziamo alcuni campioni dal set di addestramento da raggruppare in un batch. Adesso togliamo le colonne idx"),ft=r("code"),Nl=t(", "),Fl=t("sentence1"),ht=r("code"),Ml=t(", e "),Rl=t("sentence2` poich\xE9 non saranno necessarie e contengono stringhe (e non si possono creare tensori con stringhe), e controlliamo le lunghezze di ogni elemento nel batch:"),vn=m(),$(ga.$$.fragment),zn=m(),$(va.$$.fragment),kn=m(),Me=r("p"),Ql=t("Nulla di sorprendente, i campioni hanno lunghezza variabile da 32 a 67. Il padding dinamico significa che i campioni in questo batch dovrebbero tutti ricevere un padding fino alla lunghezza di 67, il massimo nel batch. Senza padding dinamico, tutti i campioni dovrebbero ricevere un padding fino alla lunghezza massima nell\u2019intero dataset, o la lunghezza massima processabile dal modello. Bisogna controllare che il "),_t=r("code"),Bl=t("data_collator"),Hl=t(" stia applicando un padding dinamico al batch in maniera corretta:"),$n=m(),$(za.$$.fragment),jn=m(),le.c(),Ga=m(),$(Re.$$.fragment),En=m(),N&&N.c(),Va=Pp(),this.h()},l(e){const l=Lp('[data-svelte="svelte-1phssyn"]',document.head);i=p(l,"META",{name:!0,content:!0}),l.forEach(s),u=f(e),j(o.$$.fragment,e),b=f(e),k=p(e,"H1",{class:!0});var wa=c(k);h=p(wa,"A",{id:!0,class:!0,href:!0});var Ja=c(h);_=p(Ja,"SPAN",{});var Ka=c(_);j(C.$$.fragment,Ka),Ka.forEach(s),Ja.forEach(s),z=f(wa),q=p(wa,"SPAN",{});var Ya=c(q);I=n(Ya,"Processare i dati"),Ya.forEach(s),wa.forEach(s),L=f(e),F.l(e),H=f(e),R.l(e),K=f(e),re=p(e,"P",{});var bt=c(re);Ce=n(bt,"Ovviamente, addestrare il modello su due frasi non porter\xE0 a dei risultati molto buoni. Per ottenere risultati migliori, si deve preparare un dataset pi\xF9 grande."),bt.forEach(s),P=f(e),S=p(e,"P",{});var Qe=c(S);Da=n(Qe,"In questa sezione verr\xE0 usato come esempio il dataset MRPC (Microsoft Research Paraphrase Corpus), presentato nell\u2019"),pe=p(Qe,"A",{href:!0,rel:!0});var gt=c(pe);Pa=n(gt,"articolo"),gt.forEach(s),Ta=n(Qe,` di William B. Dolan e Chris Brockett. Il dataset contiene 5801 coppie di
frasi, con una label che indica se l\u2019una \xE8 una parafrasi dell\u2019altra (i.e. se hanno lo stesso significato). \xC8 stato selezionato per
questo capitolo perch\xE9 \xE8 un dataset piccolo, con cui \xE8 facile sperimentare durante l\u2019addestramento.`),Qe.forEach(s),We=f(e),Ee=p(e,"H3",{class:!0});var Be=c(Ee);ye=p(Be,"A",{id:!0,class:!0,href:!0});var Za=c(ye);rs=p(Za,"SPAN",{});var vt=c(rs);j(Ue.$$.fragment,vt),vt.forEach(s),Za.forEach(s),Ln=f(Be),ps=p(Be,"SPAN",{});var ir=c(ps);Nn=n(ir,"Caricare un dataset dall'Hub"),ir.forEach(s),Be.forEach(s),kt=f(e),ae.l(e),Sa=f(e),Y=p(e,"P",{});var He=c(Y);Fn=n(He,"L\u2019Hub non contiene solo modelli; contiene anche molti dataset in tante lingue diverse. I dataset possono essere esplorati "),Ge=p(He,"A",{href:!0,rel:!0});var or=c(Ge);Mn=n(or,"qui"),or.forEach(s),Rn=n(He,", ed \xE8 consigliato tentare di caricare e processare un nuovo dataset dopo aver completato questa sezione (cfr. la "),Ve=p(He,"A",{href:!0,rel:!0});var lr=c(Ve);Qn=n(lr,"documentazione"),lr.forEach(s),Bn=n(He,"). Per ora, focalizziamoci sul dataset MRPC! Questo \xE8 uno dei 10 dataset che fanno parte del "),Je=p(He,"A",{href:!0,rel:!0});var rr=c(Je);Hn=n(rr,"GLUE benchmark"),rr.forEach(s),Wn=n(He,", che \xE8 un benchmark accademico usato per misurare la performance di modelli ML su 10 compiti di classificazione del testo."),He.forEach(s),$t=f(e),Oa=p(e,"P",{});var pr=c(Oa);Un=n(pr,"La libreria \u{1F917} Datasets fornisce un comando molto semplice per scaricare e mettere nella cache un dataset sull\u2019Hub. Il dataset MRPC pu\xF2 essere scaricato cos\xEC:"),pr.forEach(s),jt=f(e),j(Ke.$$.fragment,e),Et=f(e),j(Ye.$$.fragment,e),xt=f(e),Q=p(e,"P",{});var Z=c(Q);Gn=n(Z,"Il risultato \xE8 un oggetto di tipo "),cs=p(Z,"CODE",{});var cr=c(cs);Vn=n(cr,"DatasetDict"),cr.forEach(s),Jn=n(Z," che contiene il training set, il validation set, e il test set. Ciascuno di questi contiene svariate colonne, ("),ds=p(Z,"CODE",{});var dr=c(ds);Kn=n(dr,"sentence1"),dr.forEach(s),Yn=n(Z,", "),us=p(Z,"CODE",{});var ur=c(us);Zn=n(ur,"sentence2"),ur.forEach(s),Xn=n(Z,", "),ms=p(Z,"CODE",{});var mr=c(ms);ei=n(mr,"label"),mr.forEach(s),ai=n(Z,", e "),fs=p(Z,"CODE",{});var fr=c(fs);si=n(fr,"idx"),fr.forEach(s),ti=n(Z,") a un numero variabile di righe, corrispondenti al numero di elementi in ogni set (quindi, vi sono 3668 coppie di frasi nel training set, 408 nel validation set, e 1725 nel test set)."),Z.forEach(s),qt=f(e),ce=p(e,"P",{});var Xa=c(ce);ni=n(Xa,"Questo comando scarica il dataset e lo mette in cache, in "),hs=p(Xa,"EM",{});var hr=c(hs);ii=n(hr,"~/.cache/huggingface/dataset"),hr.forEach(s),oi=n(Xa," secondo l\u2019impostazione predefinita. Nel Capitolo 2 \xE8 stato spiegato come personalizzare la cartella di cache valorizzando la variabile d\u2019ambiente "),_s=p(Xa,"CODE",{});var _r=c(_s);li=n(_r,"HF_HOME"),_r.forEach(s),ri=n(Xa,"."),Xa.forEach(s),wt=f(e),De=p(e,"P",{});var wn=c(De);pi=n(wn,"Ogni coppia di frasi nell\u2019ogetto "),bs=p(wn,"CODE",{});var br=c(bs);ci=n(br,"raw_datasets"),br.forEach(s),di=n(wn," pu\xF2 essere ottenuto tramite indexing, come in un dizionario:"),wn.forEach(s),Ct=f(e),j(Ze.$$.fragment,e),yt=f(e),j(Xe.$$.fragment,e),Dt=f(e),de=p(e,"P",{});var es=c(de);ui=n(es,"Le label sono gi\xE0 numeri interi, quindi non \xE8 necessario alcun preprocessing. Per sapere a quale numero corrisponde quale tipo di label, si possono analizzare le "),gs=p(es,"CODE",{});var gr=c(gs);mi=n(gr,"features"),gr.forEach(s),fi=n(es," del "),vs=p(es,"CODE",{});var vr=c(vs);hi=n(vr,"raw_train_dataset"),vr.forEach(s),_i=n(es,". Ci\xF2 permette di capire la tipologia di ogni colonna:"),es.forEach(s),Pt=f(e),j(ea.$$.fragment,e),Tt=f(e),j(aa.$$.fragment,e),St=f(e),B=p(e,"P",{});var X=c(B);bi=n(X,"Dietro le quinte, "),zs=p(X,"CODE",{});var zr=c(zs);gi=n(zr,"label"),zr.forEach(s),vi=n(X," \xE8 del tipo ClassLabel"),ks=p(X,"CODE",{});var kr=c(ks);zi=n(kr,", e la corrispondenza tra i numeri e i nomi delle label \xE8 contenuta nella cartella *names*. "),kr.forEach(s),ki=n(X,"0"),$s=p(X,"CODE",{});var $r=c($s);$i=n($r,"corrisponde a"),$r.forEach(s),ji=n(X,"not_equivalent"),js=p(X,"CODE",{});var jr=c(js);Ei=n(jr,"(significato diverso), e"),jr.forEach(s),xi=n(X,"1"),Es=p(X,"CODE",{});var Er=c(Es);qi=n(Er,"corrisponde a"),Er.forEach(s),wi=n(X,"equivalent` (stesso significato)."),X.forEach(s),Ot=f(e),j(Pe.$$.fragment,e),At=f(e),xe=p(e,"H3",{class:!0});var Cn=c(xe);Te=p(Cn,"A",{id:!0,class:!0,href:!0});var xr=c(Te);xs=p(xr,"SPAN",{});var qr=c(xs);j(sa.$$.fragment,qr),qr.forEach(s),xr.forEach(s),Ci=f(Cn),qs=p(Cn,"SPAN",{});var wr=c(qs);yi=n(wr,"Preprocessing del dataset"),wr.forEach(s),Cn.forEach(s),It=f(e),te.l(e),Aa=f(e),Ia=p(e,"P",{});var Cr=c(Ia);Di=n(Cr,"Per preprocessare il dataset, \xE8 necessario convertire il testo in numeri comprensibili al modello. Come dimostrato nel [capitolo precedente]/course/chapter2), ci\xF2 viene fatto con un tokenizzatore. Il tokenizer prende come input sia una frase sia una lista di frasi, quindi \xE8 possibile effettuare la tokenizzazione di tutte le prime e seconde frasi di ogni coppia in questo modo:"),Cr.forEach(s),Lt=f(e),j(ta.$$.fragment,e),Nt=f(e),La=p(e,"P",{});var yr=c(La);Pi=n(yr,"Tuttavia, non si possono semplicemente passare al modello due frasi e sperare di predire se l\u2019una \xE8 una parafrasi dell\u2019altra o no. Bisogna gestire le due frasi come una coppia, e applicare il preprocessing necessario. Fortunatamente, il tokenizzatore pu\xF2 anche prendere come input una coppia di frasi e prepararla nel formato atteso dal modello BERT:"),yr.forEach(s),Ft=f(e),j(na.$$.fragment,e),Mt=f(e),j(ia.$$.fragment,e),Rt=f(e),W=p(e,"P",{});var ve=c(W);Ti=n(ve,"Sono state gi\xE0 discusse nel "),Na=p(ve,"A",{href:!0});var Dr=c(Na);Si=n(Dr,"Capitolo 2"),Dr.forEach(s),Oi=n(ve," le chiavi "),ws=p(ve,"CODE",{});var Pr=c(ws);Ai=n(Pr,"input_ids"),Pr.forEach(s),Ii=n(ve," e "),Cs=p(ve,"CODE",{});var Tr=c(Cs);Li=n(Tr,"attention_mask"),Tr.forEach(s),Ni=n(ve,", ma il discorso su "),ys=p(ve,"CODE",{});var Sr=c(ys);Fi=n(Sr,"token_type_ids"),Sr.forEach(s),Mi=n(ve," era stato rimandato. In questo esempio, ci\xF2 pu\xF2 essere usato per indicare al modello quale parte dell\u2019input \xE8 la prima frase, e quale la seconda."),ve.forEach(s),Qt=f(e),j(Se.$$.fragment,e),Bt=f(e),Oe=p(e,"P",{});var yn=c(Oe);Ri=n(yn,"Decodificando gli ID in "),Ds=p(yn,"CODE",{});var Or=c(Ds);Qi=n(Or,"input_ids"),Or.forEach(s),Bi=n(yn," per ritrasformarli in parole:"),yn.forEach(s),Ht=f(e),j(oa.$$.fragment,e),Wt=f(e),Fa=p(e,"P",{});var Ar=c(Fa);Hi=n(Ar,"si ottiene:"),Ar.forEach(s),Ut=f(e),j(la.$$.fragment,e),Gt=f(e),ue=p(e,"P",{});var as=c(ue);Wi=n(as,"Perci\xF2 \xE8 chiaro che il modello si aspetta gli input nella forma "),Ps=p(as,"CODE",{});var Ir=c(Ps);Ui=n(Ir,"[CLS] frase1 [SEP] frase2 [SEP]"),Ir.forEach(s),Gi=n(as," quando vi sono due frasi. Allineando con "),Ts=p(as,"CODE",{});var Lr=c(Ts);Vi=n(Lr,"token_type_ids"),Lr.forEach(s),Ji=n(as," si ottiene:"),as.forEach(s),Vt=f(e),j(ra.$$.fragment,e),Jt=f(e),U=p(e,"P",{});var ze=c(U);Ki=n(ze,"Le parti dell\u2019input corrispondenti a "),Ss=p(ze,"CODE",{});var Nr=c(Ss);Yi=n(Nr,"[CLS] frase1 [SEP]"),Nr.forEach(s),Zi=n(ze," hanno tutte un token type ID di "),Os=p(ze,"CODE",{});var Fr=c(Os);Xi=n(Fr,"0"),Fr.forEach(s),eo=n(ze,", mentre le altre parti, corrispondenti quindi a "),As=p(ze,"CODE",{});var Mr=c(As);ao=n(Mr,"frase2 [SEP]"),Mr.forEach(s),so=n(ze,", hanno tutte un token type ID di "),Is=p(ze,"CODE",{});var Rr=c(Is);to=n(Rr,"1"),Rr.forEach(s),no=n(ze,"."),ze.forEach(s),Kt=f(e),me=p(e,"P",{});var ss=c(me);io=n(ss,"Da notare che se viene selezionato un altro checkpoint, gli input tokenizzati non conterranno necessariamente i "),Ls=p(ss,"CODE",{});var Qr=c(Ls);oo=n(Qr,"token_type_ids"),Qr.forEach(s),lo=n(ss," (ad esempio, non si ottengono usando un modello DistilBERT). I "),Ns=p(ss,"CODE",{});var Br=c(Ns);ro=n(Br,"token_type_ids"),Br.forEach(s),po=n(ss," si ottengono solo quando il modello saprebbe che farne, avendole gi\xE0 viste in fase di pre-addestramento."),ss.forEach(s),Yt=f(e),G=p(e,"P",{});var ke=c(G);co=n(ke,"In questo caso, BERT \xE8 stato pre-addestrato con i token type IDs, e in aggiunta all\u2019obiettivo di "),Fs=p(ke,"EM",{});var Hr=c(Fs);uo=n(Hr,"[masked language modeling - check with Caterina]"),Hr.forEach(s),mo=n(ke," di cui si era parlato nel "),Ma=p(ke,"A",{href:!0});var Wr=c(Ma);fo=n(Wr,"Capitolo 1"),Wr.forEach(s),ho=n(ke,", vi \xE8 un altro obiettivo che si chiama "),Ms=p(ke,"EM",{});var Ur=c(Ms);_o=n(Ur,"next sentence prediction"),Ur.forEach(s),bo=n(ke," ("),Rs=p(ke,"EM",{});var Gr=c(Rs);go=n(Gr,"predire la prossima frase"),Gr.forEach(s),vo=n(ke,"). Lo scopo di questo task \xE8 modellizzare la relazione tra coppie di frasi."),ke.forEach(s),Zt=f(e),Ra=p(e,"P",{});var Vr=c(Ra);zo=n(Vr,"Durante un task di next sentence prediction, il modello riceve una coppia di frasi (con token mascherati in maniera aleatoria) e deve predire se la seconda segue la prima. Per rendere il task meno banale, la met\xE0 delle volte le frasi si susseguono nel documento da cui erano state estratte originariamente, l\u2019altra met\xE0 delle volte le frasi provengono da due documenti diversi."),Vr.forEach(s),Xt=f(e),Ae=p(e,"P",{});var Dn=c(Ae);ko=n(Dn,"In generale, non bisogna preoccuparsi se i "),Qs=p(Dn,"CODE",{});var Jr=c(Qs);$o=n(Jr,"token_type_ids"),Jr.forEach(s),jo=n(Dn," sono presenti o no negli input tokenizzati: finch\xE9 viene usato lo stesso checkpoint per il tokenizer e il modello, tutto andr\xE0 bene poich\xE9 il tokenizer sa cosa fornire al modello."),Dn.forEach(s),en=f(e),fe=p(e,"P",{});var ts=c(fe);Eo=n(ts,"Ora che abbiamo visto come il tokenizer pu\xF2 gestire una coppia di frasi, possiamo usarlo per tokenizzare l\u2019intero dataset: come nel "),Qa=p(ts,"A",{href:!0});var Kr=c(Qa);xo=n(Kr,"capitolo precedente"),Kr.forEach(s),qo=n(ts,", si pu\xF2 fornire al tokenizer una lista di coppie di frasi dando prima la lista delle prime frasi, e poi la lista delle seconde frasi. Questo approcchio \xE8 anche compatibile le opzioni di padding e truncation gi\xE0 viste nel "),Ba=p(ts,"A",{href:!0});var Yr=c(Ba);wo=n(Yr,"Capitolo 2"),Yr.forEach(s),Co=n(ts,". Perci\xF2, un modo per preprocessare il dataset di addestramento \xE8:"),ts.forEach(s),an=f(e),j(pa.$$.fragment,e),sn=f(e),V=p(e,"P",{});var $e=c(V);yo=n($e,"Questo metodo funziona, ma ha lo svantaggio di restituire un dizionario (avente "),Bs=p($e,"CODE",{});var Zr=c(Bs);Do=n(Zr,"input_ids"),Zr.forEach(s),Po=n($e,", "),Hs=p($e,"CODE",{});var Xr=c(Hs);To=n(Xr,"attention_mask"),Xr.forEach(s),So=n($e,", e "),Ws=p($e,"CODE",{});var ep=c(Ws);Oo=n(ep,"token_type_ids"),ep.forEach(s),Ao=n($e," come chiave, e delle liste di liste come valori). Oltretutto, questo metodo funziona solo se si ha a disposizione RAM sufficiente per contenere l\u2019intero dataset durante la tokenizzazione (mentre i dataset dalla libreria \u{1F917} Datasets sono file "),ca=p($e,"A",{href:!0,rel:!0});var ap=c(ca);Io=n(ap,"Apache Arrow"),ap.forEach(s),Lo=n($e," archiviati su disco, perci\xF2 in memoria vengono caricati solo i campioni richiesti)."),$e.forEach(s),tn=f(e),he=p(e,"P",{});var ns=c(he);No=n(ns,"Per tenere i dati come dataset, utilizzare il metodo "),da=p(ns,"A",{href:!0,rel:!0});var sp=c(da);Us=p(sp,"CODE",{});var tp=c(Us);Fo=n(tp,"Dataset.map()"),tp.forEach(s),sp.forEach(s),Mo=n(ns,". Ci\xF2 permette anche della flessibilit\xE0 extra, qualora fosse necessario del preprocessing aggiuntivo oltre alla tokenizzazione. Il metodo "),Gs=p(ns,"CODE",{});var np=c(Gs);Ro=n(np,"map()"),np.forEach(s),Qo=n(ns," applica una funziona ad ogni elemento del dataset, perci\xF2 bisogna definire una funzione che tokenizzi gli input:"),ns.forEach(s),nn=f(e),j(ua.$$.fragment,e),on=f(e),A=p(e,"P",{});var M=c(A);Bo=n(M,"Questa funzione riceve un dizionario (come gli element del nostro dataset) e restituisce un nuovo dizionario con input_ids"),Vs=p(M,"CODE",{});var ip=c(Vs);Ho=n(ip,", "),ip.forEach(s),Wo=n(M,"attention_mask"),Js=p(M,"CODE",{});var op=c(Js);Uo=n(op,", e "),op.forEach(s),Go=n(M,"token_type_ids"),Ks=p(M,"CODE",{});var lp=c(Ks);Vo=n(lp,"come chiavi. Funziona anche se il dizionario"),lp.forEach(s),Jo=n(M,"example"),Ys=p(M,"CODE",{});var rp=c(Ys);Ko=n(rp,"contiene svariati campioni (ad una chiave corrisponde una lista di frasi) poich\xE9 il"),rp.forEach(s),Yo=n(M,"tokenizer"),Zs=p(M,"CODE",{});var pp=c(Zs);Zo=n(pp,"funziona con liste di coppie di frasi, come gi\xE0 visto. Ci\xF2 permette di usare l'opzione"),pp.forEach(s),Xo=n(M,"batched=True"),Xs=p(M,"CODE",{});var cp=c(Xs);el=n(cp,"nella chiamata a"),cp.forEach(s),al=n(M,"map()"),et=p(M,"CODE",{});var dp=c(et);sl=n(dp,", che accelerer\xE0 di molto la tokenizzazione. Il "),dp.forEach(s),tl=n(M,"tokenizer` si appoggia ad un tokenizer scritto in Rust della libreria "),ma=p(M,"A",{href:!0,rel:!0});var up=c(ma);nl=n(up,"\u{1F917} Tokenizers"),up.forEach(s),il=n(M,". Questo tokenizer pu\xF2 essere molto veloce, ma solo se gli vengono forniti molti input insieme."),M.forEach(s),ln=f(e),Ie=p(e,"P",{});var Pn=c(Ie);ol=n(Pn,"Per ora non ci siamo preoccupati del parametro "),at=p(Pn,"CODE",{});var mp=c(at);ll=n(mp,"padding"),mp.forEach(s),rl=n(Pn," nella nostra funzione di tokenizzazione. Questo perch\xE9 il padding di tutti i campioni fino a lunghezza massima non \xE8 efficiente: \xE8 meglio fare il padding dei campioni quando stiamo assemblando una batch, poich\xE9 in quel momento \xE8 necessario il padding solo fino alla lunghezza massima nel batch, non la lunghezza massima nell\u2019intero dataset. Ci\xF2 permette di risparmiare molto tempo e potenza di calcolo nel caso in cui gli input abbiano lunghezze molto varie!"),Pn.forEach(s),rn=f(e),_e=p(e,"P",{});var is=c(_e);pl=n(is,"Ecco come si applica la funzione di tokenizzazione sull\u2019intero dataset. Bisogna usare "),st=p(is,"CODE",{});var fp=c(st);cl=n(fp,"batched=True"),fp.forEach(s),dl=n(is," nella chiamata a "),tt=p(is,"CODE",{});var hp=c(tt);ul=n(hp,"map"),hp.forEach(s),ml=n(is," in modo tale che la funzione venga applicata a vari elementi del dataset insieme, e non ad ogni elemento separatamente. Ci\xF2 permette un preprocessing pi\xF9 rapido."),is.forEach(s),pn=f(e),j(fa.$$.fragment,e),cn=f(e),Ha=p(e,"P",{});var _p=c(Ha);fl=n(_p,"La libreria \u{1F917} Datasets aggiunge nuovi campi ai dataset, uno per ogni chiave nel dizionario restituito dalla funzione di preprocessing:"),_p.forEach(s),dn=f(e),j(ha.$$.fragment,e),un=f(e),be=p(e,"P",{});var os=c(be);hl=n(os,"Si pu\xF2 anche applicare il multiprocessing durante il preprocessing con la funzione "),nt=p(os,"CODE",{});var bp=c(nt);_l=n(bp,"map()"),bp.forEach(s),bl=n(os," utilizzando il parametro "),it=p(os,"CODE",{});var gp=c(it);gl=n(gp,"num_proc"),gp.forEach(s),vl=n(os,". Ci\xF2 non \xE8 stato dimostrato qui perch\xE9 la libreria \u{1F917} Tokenizers gi\xE0 utilizza vari thread per tokenizzare i campioni pi\xF9 rapidamente, ma nel caso in cui non venga usato un tokenizer rapido di questa libreria, ci\xF2 potrebbe velocizzare il preprocessing."),os.forEach(s),mn=f(e),J=p(e,"P",{});var je=c(J);zl=n(je,"La funzione tokenize_function"),ot=p(je,"CODE",{});var vp=c(ot);kl=n(vp,"restituisce un dizionario con"),vp.forEach(s),$l=n(je,"input_ids"),lt=p(je,"CODE",{});var zp=c(lt);jl=n(zp,", "),zp.forEach(s),El=n(je,"attention_mask"),rt=p(je,"CODE",{});var kp=c(rt);xl=n(kp,", e "),kp.forEach(s),ql=n(je,"token_type_ids"),pt=p(je,"CODE",{});var $p=c(pt);wl=n($p,"come chiavi, quindi quei tre campi vengono aggiunti a tutti gli *[split - check!]* del dataset. Si possono anche cambiare i campi esistenti nel caso in cui la funzione di preprocessing restituisca un nuovo valore per una chiave gi\xE0 esistente nel dataset a cui viene applicato"),$p.forEach(s),Cl=n(je,"map()`."),je.forEach(s),fn=f(e),Le=p(e,"P",{});var Tn=c(Le);yl=n(Tn,"L\u2019ultima cosa da fare \xE8 il padding di tutti i campioni alla lunghezza dell\u2019elemento pi\xF9 lungo quando sono inseriti in una batch \u2014 una tecnica che si chiama "),ct=p(Tn,"EM",{});var jp=c(ct);Dl=n(jp,"dynamic padding"),jp.forEach(s),Pl=n(Tn,"."),Tn.forEach(s),hn=f(e),qe=p(e,"H3",{class:!0});var Sn=c(qe);Ne=p(Sn,"A",{id:!0,class:!0,href:!0});var Ep=c(Ne);dt=p(Ep,"SPAN",{});var xp=c(dt);j(_a.$$.fragment,xp),xp.forEach(s),Ep.forEach(s),Tl=f(Sn),ut=p(Sn,"SPAN",{});var qp=c(ut);Sl=n(qp,"Dynamic padding"),qp.forEach(s),Sn.forEach(s),_n=f(e),j(ba.$$.fragment,e),bn=f(e),we.l(e),Wa=f(e),Fe=p(e,"P",{});var On=c(Fe);Ol=n(On,"In pratica, bisogna definire una collate function che applichi la giusta quantit\xE0 di padding agli elementi del dataset in una stessa batch. Fortunatamente, la libreria \u{1F917} Transformers fornisce questa funziona tramite "),mt=p(On,"CODE",{});var wp=c(mt);Al=n(wp,"DataCollatorWithPadding"),wp.forEach(s),Il=n(On,". Essa prende in input un tokenizer quando viene istanziata (per individuare quale token da usare per il padding, e se il modello si aspetta padding a sinistra o a destra dell\u2019input) e far\xE0 tutto il necessario:"),On.forEach(s),gn=f(e),ie.l(e),Ua=f(e),ge=p(e,"P",{});var ls=c(ge);Ll=n(ls,"Per testare questo nuovo gioco, analizziamo alcuni campioni dal set di addestramento da raggruppare in un batch. Adesso togliamo le colonne idx"),ft=p(ls,"CODE",{});var Cp=c(ft);Nl=n(Cp,", "),Cp.forEach(s),Fl=n(ls,"sentence1"),ht=p(ls,"CODE",{});var yp=c(ht);Ml=n(yp,", e "),yp.forEach(s),Rl=n(ls,"sentence2` poich\xE9 non saranno necessarie e contengono stringhe (e non si possono creare tensori con stringhe), e controlliamo le lunghezze di ogni elemento nel batch:"),ls.forEach(s),vn=f(e),j(ga.$$.fragment,e),zn=f(e),j(va.$$.fragment,e),kn=f(e),Me=p(e,"P",{});var An=c(Me);Ql=n(An,"Nulla di sorprendente, i campioni hanno lunghezza variabile da 32 a 67. Il padding dinamico significa che i campioni in questo batch dovrebbero tutti ricevere un padding fino alla lunghezza di 67, il massimo nel batch. Senza padding dinamico, tutti i campioni dovrebbero ricevere un padding fino alla lunghezza massima nell\u2019intero dataset, o la lunghezza massima processabile dal modello. Bisogna controllare che il "),_t=p(An,"CODE",{});var Dp=c(_t);Bl=n(Dp,"data_collator"),Dp.forEach(s),Hl=n(An," stia applicando un padding dinamico al batch in maniera corretta:"),An.forEach(s),$n=f(e),j(za.$$.fragment,e),jn=f(e),le.l(e),Ga=f(e),j(Re.$$.fragment,e),En=f(e),N&&N.l(e),Va=Pp(),this.h()},h(){y(i,"name","hf:doc:metadata"),y(i,"content",JSON.stringify(nc)),y(h,"id","processare-i-dati"),y(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(h,"href","#processare-i-dati"),y(k,"class","relative group"),y(pe,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),y(pe,"rel","nofollow"),y(ye,"id","caricare-un-dataset-dallhub"),y(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(ye,"href","#caricare-un-dataset-dallhub"),y(Ee,"class","relative group"),y(Ge,"href","https://huggingface.co/datasets"),y(Ge,"rel","nofollow"),y(Ve,"href","https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub"),y(Ve,"rel","nofollow"),y(Je,"href","https://gluebenchmark.com/"),y(Je,"rel","nofollow"),y(Te,"id","preprocessing-del-dataset"),y(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Te,"href","#preprocessing-del-dataset"),y(xe,"class","relative group"),y(Na,"href","/course/chapter2"),y(Ma,"href","/course/chapter1"),y(Qa,"href","/course/chapter2"),y(Ba,"href","/course/chapter2"),y(ca,"href","https://arrow.apache.org/"),y(ca,"rel","nofollow"),y(da,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),y(da,"rel","nofollow"),y(ma,"href","https://github.com/huggingface/tokenizers"),y(ma,"rel","nofollow"),y(Ne,"id","dynamic-padding"),y(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ne,"href","#dynamic-padding"),y(qe,"class","relative group")},m(e,l){a(document.head,i),d(e,u,l),E(o,e,l),d(e,b,l),d(e,k,l),a(k,h),a(h,_),E(C,_,null),a(k,z),a(k,q),a(q,I),d(e,L,l),ka[D].m(e,l),d(e,H,l),$a[O].m(e,l),d(e,K,l),d(e,re,l),a(re,Ce),d(e,P,l),d(e,S,l),a(S,Da),a(S,pe),a(pe,Pa),a(S,Ta),d(e,We,l),d(e,Ee,l),a(Ee,ye),a(ye,rs),E(Ue,rs,null),a(Ee,Ln),a(Ee,ps),a(ps,Nn),d(e,kt,l),ja[ee].m(e,l),d(e,Sa,l),d(e,Y,l),a(Y,Fn),a(Y,Ge),a(Ge,Mn),a(Y,Rn),a(Y,Ve),a(Ve,Qn),a(Y,Bn),a(Y,Je),a(Je,Hn),a(Y,Wn),d(e,$t,l),d(e,Oa,l),a(Oa,Un),d(e,jt,l),E(Ke,e,l),d(e,Et,l),E(Ye,e,l),d(e,xt,l),d(e,Q,l),a(Q,Gn),a(Q,cs),a(cs,Vn),a(Q,Jn),a(Q,ds),a(ds,Kn),a(Q,Yn),a(Q,us),a(us,Zn),a(Q,Xn),a(Q,ms),a(ms,ei),a(Q,ai),a(Q,fs),a(fs,si),a(Q,ti),d(e,qt,l),d(e,ce,l),a(ce,ni),a(ce,hs),a(hs,ii),a(ce,oi),a(ce,_s),a(_s,li),a(ce,ri),d(e,wt,l),d(e,De,l),a(De,pi),a(De,bs),a(bs,ci),a(De,di),d(e,Ct,l),E(Ze,e,l),d(e,yt,l),E(Xe,e,l),d(e,Dt,l),d(e,de,l),a(de,ui),a(de,gs),a(gs,mi),a(de,fi),a(de,vs),a(vs,hi),a(de,_i),d(e,Pt,l),E(ea,e,l),d(e,Tt,l),E(aa,e,l),d(e,St,l),d(e,B,l),a(B,bi),a(B,zs),a(zs,gi),a(B,vi),a(B,ks),a(ks,zi),a(B,ki),a(B,$s),a($s,$i),a(B,ji),a(B,js),a(js,Ei),a(B,xi),a(B,Es),a(Es,qi),a(B,wi),d(e,Ot,l),E(Pe,e,l),d(e,At,l),d(e,xe,l),a(xe,Te),a(Te,xs),E(sa,xs,null),a(xe,Ci),a(xe,qs),a(qs,yi),d(e,It,l),Ea[se].m(e,l),d(e,Aa,l),d(e,Ia,l),a(Ia,Di),d(e,Lt,l),E(ta,e,l),d(e,Nt,l),d(e,La,l),a(La,Pi),d(e,Ft,l),E(na,e,l),d(e,Mt,l),E(ia,e,l),d(e,Rt,l),d(e,W,l),a(W,Ti),a(W,Na),a(Na,Si),a(W,Oi),a(W,ws),a(ws,Ai),a(W,Ii),a(W,Cs),a(Cs,Li),a(W,Ni),a(W,ys),a(ys,Fi),a(W,Mi),d(e,Qt,l),E(Se,e,l),d(e,Bt,l),d(e,Oe,l),a(Oe,Ri),a(Oe,Ds),a(Ds,Qi),a(Oe,Bi),d(e,Ht,l),E(oa,e,l),d(e,Wt,l),d(e,Fa,l),a(Fa,Hi),d(e,Ut,l),E(la,e,l),d(e,Gt,l),d(e,ue,l),a(ue,Wi),a(ue,Ps),a(Ps,Ui),a(ue,Gi),a(ue,Ts),a(Ts,Vi),a(ue,Ji),d(e,Vt,l),E(ra,e,l),d(e,Jt,l),d(e,U,l),a(U,Ki),a(U,Ss),a(Ss,Yi),a(U,Zi),a(U,Os),a(Os,Xi),a(U,eo),a(U,As),a(As,ao),a(U,so),a(U,Is),a(Is,to),a(U,no),d(e,Kt,l),d(e,me,l),a(me,io),a(me,Ls),a(Ls,oo),a(me,lo),a(me,Ns),a(Ns,ro),a(me,po),d(e,Yt,l),d(e,G,l),a(G,co),a(G,Fs),a(Fs,uo),a(G,mo),a(G,Ma),a(Ma,fo),a(G,ho),a(G,Ms),a(Ms,_o),a(G,bo),a(G,Rs),a(Rs,go),a(G,vo),d(e,Zt,l),d(e,Ra,l),a(Ra,zo),d(e,Xt,l),d(e,Ae,l),a(Ae,ko),a(Ae,Qs),a(Qs,$o),a(Ae,jo),d(e,en,l),d(e,fe,l),a(fe,Eo),a(fe,Qa),a(Qa,xo),a(fe,qo),a(fe,Ba),a(Ba,wo),a(fe,Co),d(e,an,l),E(pa,e,l),d(e,sn,l),d(e,V,l),a(V,yo),a(V,Bs),a(Bs,Do),a(V,Po),a(V,Hs),a(Hs,To),a(V,So),a(V,Ws),a(Ws,Oo),a(V,Ao),a(V,ca),a(ca,Io),a(V,Lo),d(e,tn,l),d(e,he,l),a(he,No),a(he,da),a(da,Us),a(Us,Fo),a(he,Mo),a(he,Gs),a(Gs,Ro),a(he,Qo),d(e,nn,l),E(ua,e,l),d(e,on,l),d(e,A,l),a(A,Bo),a(A,Vs),a(Vs,Ho),a(A,Wo),a(A,Js),a(Js,Uo),a(A,Go),a(A,Ks),a(Ks,Vo),a(A,Jo),a(A,Ys),a(Ys,Ko),a(A,Yo),a(A,Zs),a(Zs,Zo),a(A,Xo),a(A,Xs),a(Xs,el),a(A,al),a(A,et),a(et,sl),a(A,tl),a(A,ma),a(ma,nl),a(A,il),d(e,ln,l),d(e,Ie,l),a(Ie,ol),a(Ie,at),a(at,ll),a(Ie,rl),d(e,rn,l),d(e,_e,l),a(_e,pl),a(_e,st),a(st,cl),a(_e,dl),a(_e,tt),a(tt,ul),a(_e,ml),d(e,pn,l),E(fa,e,l),d(e,cn,l),d(e,Ha,l),a(Ha,fl),d(e,dn,l),E(ha,e,l),d(e,un,l),d(e,be,l),a(be,hl),a(be,nt),a(nt,_l),a(be,bl),a(be,it),a(it,gl),a(be,vl),d(e,mn,l),d(e,J,l),a(J,zl),a(J,ot),a(ot,kl),a(J,$l),a(J,lt),a(lt,jl),a(J,El),a(J,rt),a(rt,xl),a(J,ql),a(J,pt),a(pt,wl),a(J,Cl),d(e,fn,l),d(e,Le,l),a(Le,yl),a(Le,ct),a(ct,Dl),a(Le,Pl),d(e,hn,l),d(e,qe,l),a(qe,Ne),a(Ne,dt),E(_a,dt,null),a(qe,Tl),a(qe,ut),a(ut,Sl),d(e,_n,l),E(ba,e,l),d(e,bn,l),we.m(e,l),d(e,Wa,l),d(e,Fe,l),a(Fe,Ol),a(Fe,mt),a(mt,Al),a(Fe,Il),d(e,gn,l),xa[ne].m(e,l),d(e,Ua,l),d(e,ge,l),a(ge,Ll),a(ge,ft),a(ft,Nl),a(ge,Fl),a(ge,ht),a(ht,Ml),a(ge,Rl),d(e,vn,l),E(ga,e,l),d(e,zn,l),E(va,e,l),d(e,kn,l),d(e,Me,l),a(Me,Ql),a(Me,_t),a(_t,Bl),a(Me,Hl),d(e,$n,l),E(za,e,l),d(e,jn,l),qa[oe].m(e,l),d(e,Ga,l),E(Re,e,l),d(e,En,l),N&&N.m(e,l),d(e,Va,l),xn=!0},p(e,[l]){const wa={};l&1&&(wa.fw=e[0]),o.$set(wa);let Ja=D;D=Gl(e),D!==Ja&&(ya(),v(ka[Ja],1,1,()=>{ka[Ja]=null}),Ca(),F=ka[D],F||(F=ka[D]=Ul[D](e),F.c()),g(F,1),F.m(H.parentNode,H));let Ka=O;O=Jl(e),O!==Ka&&(ya(),v($a[Ka],1,1,()=>{$a[Ka]=null}),Ca(),R=$a[O],R||(R=$a[O]=Vl[O](e),R.c()),g(R,1),R.m(K.parentNode,K));let Ya=ee;ee=Yl(e),ee!==Ya&&(ya(),v(ja[Ya],1,1,()=>{ja[Ya]=null}),Ca(),ae=ja[ee],ae||(ae=ja[ee]=Kl[ee](e),ae.c()),g(ae,1),ae.m(Sa.parentNode,Sa));const bt={};l&2&&(bt.$$scope={dirty:l,ctx:e}),Pe.$set(bt);let Qe=se;se=Xl(e),se!==Qe&&(ya(),v(Ea[Qe],1,1,()=>{Ea[Qe]=null}),Ca(),te=Ea[se],te||(te=Ea[se]=Zl[se](e),te.c()),g(te,1),te.m(Aa.parentNode,Aa));const gt={};l&2&&(gt.$$scope={dirty:l,ctx:e}),Se.$set(gt),qn!==(qn=er(e))&&(we.d(1),we=qn(e),we&&(we.c(),we.m(Wa.parentNode,Wa)));let Be=ne;ne=sr(e),ne!==Be&&(ya(),v(xa[Be],1,1,()=>{xa[Be]=null}),Ca(),ie=xa[ne],ie||(ie=xa[ne]=ar[ne](e),ie.c()),g(ie,1),ie.m(Ua.parentNode,Ua));let Za=oe;oe=nr(e),oe!==Za&&(ya(),v(qa[Za],1,1,()=>{qa[Za]=null}),Ca(),le=qa[oe],le||(le=qa[oe]=tr[oe](e),le.c()),g(le,1),le.m(Ga.parentNode,Ga));const vt={};l&2&&(vt.$$scope={dirty:l,ctx:e}),Re.$set(vt),e[0]==="tf"?N?l&1&&g(N,1):(N=Tp(),N.c(),g(N,1),N.m(Va.parentNode,Va)):N&&(ya(),v(N,1,1,()=>{N=null}),Ca())},i(e){xn||(g(o.$$.fragment,e),g(C.$$.fragment,e),g(F),g(R),g(Ue.$$.fragment,e),g(ae),g(Ke.$$.fragment,e),g(Ye.$$.fragment,e),g(Ze.$$.fragment,e),g(Xe.$$.fragment,e),g(ea.$$.fragment,e),g(aa.$$.fragment,e),g(Pe.$$.fragment,e),g(sa.$$.fragment,e),g(te),g(ta.$$.fragment,e),g(na.$$.fragment,e),g(ia.$$.fragment,e),g(Se.$$.fragment,e),g(oa.$$.fragment,e),g(la.$$.fragment,e),g(ra.$$.fragment,e),g(pa.$$.fragment,e),g(ua.$$.fragment,e),g(fa.$$.fragment,e),g(ha.$$.fragment,e),g(_a.$$.fragment,e),g(ba.$$.fragment,e),g(ie),g(ga.$$.fragment,e),g(va.$$.fragment,e),g(za.$$.fragment,e),g(le),g(Re.$$.fragment,e),g(N),xn=!0)},o(e){v(o.$$.fragment,e),v(C.$$.fragment,e),v(F),v(R),v(Ue.$$.fragment,e),v(ae),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(ea.$$.fragment,e),v(aa.$$.fragment,e),v(Pe.$$.fragment,e),v(sa.$$.fragment,e),v(te),v(ta.$$.fragment,e),v(na.$$.fragment,e),v(ia.$$.fragment,e),v(Se.$$.fragment,e),v(oa.$$.fragment,e),v(la.$$.fragment,e),v(ra.$$.fragment,e),v(pa.$$.fragment,e),v(ua.$$.fragment,e),v(fa.$$.fragment,e),v(ha.$$.fragment,e),v(_a.$$.fragment,e),v(ba.$$.fragment,e),v(ie),v(ga.$$.fragment,e),v(va.$$.fragment,e),v(za.$$.fragment,e),v(le),v(Re.$$.fragment,e),v(N),xn=!1},d(e){s(i),e&&s(u),x(o,e),e&&s(b),e&&s(k),x(C),e&&s(L),ka[D].d(e),e&&s(H),$a[O].d(e),e&&s(K),e&&s(re),e&&s(P),e&&s(S),e&&s(We),e&&s(Ee),x(Ue),e&&s(kt),ja[ee].d(e),e&&s(Sa),e&&s(Y),e&&s($t),e&&s(Oa),e&&s(jt),x(Ke,e),e&&s(Et),x(Ye,e),e&&s(xt),e&&s(Q),e&&s(qt),e&&s(ce),e&&s(wt),e&&s(De),e&&s(Ct),x(Ze,e),e&&s(yt),x(Xe,e),e&&s(Dt),e&&s(de),e&&s(Pt),x(ea,e),e&&s(Tt),x(aa,e),e&&s(St),e&&s(B),e&&s(Ot),x(Pe,e),e&&s(At),e&&s(xe),x(sa),e&&s(It),Ea[se].d(e),e&&s(Aa),e&&s(Ia),e&&s(Lt),x(ta,e),e&&s(Nt),e&&s(La),e&&s(Ft),x(na,e),e&&s(Mt),x(ia,e),e&&s(Rt),e&&s(W),e&&s(Qt),x(Se,e),e&&s(Bt),e&&s(Oe),e&&s(Ht),x(oa,e),e&&s(Wt),e&&s(Fa),e&&s(Ut),x(la,e),e&&s(Gt),e&&s(ue),e&&s(Vt),x(ra,e),e&&s(Jt),e&&s(U),e&&s(Kt),e&&s(me),e&&s(Yt),e&&s(G),e&&s(Zt),e&&s(Ra),e&&s(Xt),e&&s(Ae),e&&s(en),e&&s(fe),e&&s(an),x(pa,e),e&&s(sn),e&&s(V),e&&s(tn),e&&s(he),e&&s(nn),x(ua,e),e&&s(on),e&&s(A),e&&s(ln),e&&s(Ie),e&&s(rn),e&&s(_e),e&&s(pn),x(fa,e),e&&s(cn),e&&s(Ha),e&&s(dn),x(ha,e),e&&s(un),e&&s(be),e&&s(mn),e&&s(J),e&&s(fn),e&&s(Le),e&&s(hn),e&&s(qe),x(_a),e&&s(_n),x(ba,e),e&&s(bn),we.d(e),e&&s(Wa),e&&s(Fe),e&&s(gn),xa[ne].d(e),e&&s(Ua),e&&s(ge),e&&s(vn),x(ga,e),e&&s(zn),x(va,e),e&&s(kn),e&&s(Me),e&&s($n),x(za,e),e&&s(jn),qa[oe].d(e),e&&s(Ga),x(Re,e),e&&s(En),N&&N.d(e),e&&s(Va)}}}const nc={local:"processare-i-dati",sections:[{local:"caricare-un-dataset-dallhub",title:"Caricare un dataset dall'Hub"},{local:"preprocessing-del-dataset",title:"Preprocessing del dataset"},{local:"dynamic-padding",title:"Dynamic padding"}],title:"Processare i dati"};function ic(w,i,u){let o="pt";return Np(()=>{const b=new URLSearchParams(window.location.search);u(0,o=b.get("fw")||"pt")}),[o]}class mc extends Op{constructor(i){super();Ap(this,i,ic,tc,Ip,{})}}export{mc as default,nc as metadata};
