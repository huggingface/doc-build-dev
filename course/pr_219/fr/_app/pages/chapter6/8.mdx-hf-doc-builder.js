import{S as Eq,i as qq,s as jq,e as r,k as u,w as m,t as n,M as xq,c as l,d as t,m as c,a,x as d,h as o,b as z,N as $q,G as s,g as p,y as f,q as k,o as v,B as _,v as bq}from"../../chunks/vendor-hf-doc-builder.js";import{T as gq}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Pq}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ti}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as h}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as wq}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Tq(ni){let $,Pe,te,le,qe,je,Fs,xe,Us,es,ne,ss,we,Ve,ts,R,be,Rs,Ws,ge,Is,Gs,oe,Vs,Xe;return{c(){$=r("p"),Pe=r("strong"),te=n("Pour aller plus loin"),le=n(" Si vous testez les deux versions des normalisateurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=r("code"),je=n('u"\\u0085"'),Fs=n(` vous remarquerez s\xFBrement que ces deux normalisateurs ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),xe=r("code"),Us=n("normalizers.Sequence"),es=n(", nous n\u2019avons pas inclus les remplacements Regex que le "),ne=r("code"),ss=n("BertNormalizer"),we=n(" requiert quand l\u2019argument "),Ve=r("code"),ts=n("clean_text"),R=n(" est mis \xE0 "),be=r("code"),Rs=n("True"),Ws=n(" ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),ge=r("code"),Is=n("BertNormalizer"),Gs=n(" en ajoutant deux "),oe=r("code"),Vs=n("normalizers.Replace"),Xe=n(" \xE0 la s\xE9quence de normalisation.")},l(Ke){$=l(Ke,"P",{});var x=a($);Pe=l(x,"STRONG",{});var wn=a(Pe);te=o(wn,"Pour aller plus loin"),wn.forEach(t),le=o(x," Si vous testez les deux versions des normalisateurs pr\xE9c\xE9dents sur une cha\xEEne contenant le caract\xE8re unicode "),qe=l(x,"CODE",{});var ae=a(qe);je=o(ae,'u"\\u0085"'),ae.forEach(t),Fs=o(x,` vous remarquerez s\xFBrement que ces deux normalisateurs ne sont pas exactement \xE9quivalents.
Pour ne pas trop compliquer la version avec `),xe=l(x,"CODE",{});var Tn=a(xe);Us=o(Tn,"normalizers.Sequence"),Tn.forEach(t),es=o(x,", nous n\u2019avons pas inclus les remplacements Regex que le "),ne=l(x,"CODE",{});var ns=a(ne);ss=o(ns,"BertNormalizer"),ns.forEach(t),we=o(x," requiert quand l\u2019argument "),Ve=l(x,"CODE",{});var Cn=a(Ve);ts=o(Cn,"clean_text"),Cn.forEach(t),R=o(x," est mis \xE0 "),be=l(x,"CODE",{});var Dn=a(be);Rs=o(Dn,"True"),Dn.forEach(t),Ws=o(x," ce qui est le comportement par d\xE9faut. Mais ne vous inqui\xE9tez pas : il est possible d\u2019obtenir exactement la m\xEAme normalisation sans utiliser le tr\xE8s pratique "),ge=l(x,"CODE",{});var os=a(ge);Is=o(os,"BertNormalizer"),os.forEach(t),Gs=o(x," en ajoutant deux "),oe=l(x,"CODE",{});var Ln=a(oe);Vs=o(Ln,"normalizers.Replace"),Ln.forEach(t),Xe=o(x," \xE0 la s\xE9quence de normalisation."),x.forEach(t)},m(Ke,x){p(Ke,$,x),s($,Pe),s(Pe,te),s($,le),s($,qe),s(qe,je),s($,Fs),s($,xe),s(xe,Us),s($,es),s($,ne),s(ne,ss),s($,we),s($,Ve),s(Ve,ts),s($,R),s($,be),s(be,Rs),s($,Ws),s($,ge),s(ge,Is),s($,Gs),s($,oe),s(oe,Vs),s($,Xe)},d(Ke){Ke&&t($)}}}function Cq(ni){let $,Pe,te,le,qe,je,Fs,xe,Us,es,ne,ss,we,Ve,ts,R,be,Rs,Ws,ge,Is,Gs,oe,Vs,Xe,Ke,x,wn,ae,Tn,ns,Cn,Dn,os,Ln,vc,oi,On,_c,ri,He,Xs,Fz,hc,Ks,Uz,li,W,zc,io,$c,Ec,po,qc,jc,uo,xc,bc,yn,gc,Pc,co,wc,Tc,ai,Hs,ii,rs,Cc,mo,Dc,Lc,pi,I,Te,fo,Oc,yc,ko,Mc,Sc,Ys,Nc,Bc,Ac,Ce,vo,Fc,Uc,_o,Rc,Wc,Js,Ic,Gc,Vc,G,ho,Xc,Kc,zo,Hc,Yc,$o,Jc,Zc,Eo,Qc,em,qo,sm,tm,Zs,nm,om,rm,De,jo,lm,am,xo,im,pm,Qs,um,cm,mm,Le,bo,dm,fm,go,km,vm,et,_m,hm,zm,Oe,Po,$m,Em,wo,qm,jm,st,xm,bm,ui,ls,gm,tt,Pm,wm,ci,Ye,as,To,nt,Tm,Co,Cm,mi,ie,Dm,Do,Lm,Om,Mn,ym,Mm,ot,Sm,Nm,di,rt,fi,ye,Bm,Lo,Am,Fm,Oo,Um,Rm,ki,is,Wm,yo,Im,Gm,vi,lt,_i,pe,Vm,Mo,Xm,Km,So,Hm,Ym,No,Jm,Zm,hi,Je,ps,Bo,at,Qm,Ao,ed,zi,b,sd,Fo,td,nd,Uo,od,rd,Ro,ld,ad,Wo,id,pd,Io,ud,cd,Go,md,dd,Vo,fd,kd,Xo,vd,_d,$i,Me,hd,Ko,zd,$d,Ho,Ed,qd,Ei,it,qi,ue,jd,Yo,xd,bd,Jo,gd,Pd,Zo,wd,Td,ji,g,Cd,Qo,Dd,Ld,er,Od,yd,sr,Md,Sd,tr,Nd,Bd,nr,Ad,Fd,or,Ud,Rd,rr,Wd,Id,lr,Gd,Vd,xi,pt,bi,V,Xd,ar,Kd,Hd,ir,Yd,Jd,pr,Zd,Qd,ur,ef,sf,cr,tf,nf,gi,ut,Pi,Se,of,mr,rf,lf,dr,af,pf,wi,Ne,uf,fr,cf,mf,kr,df,ff,Ti,ct,Ci,mt,Di,us,Li,cs,kf,vr,vf,_f,Oi,dt,yi,Sn,hf,Mi,ft,Si,ms,zf,_r,$f,Ef,Ni,kt,Bi,vt,Ai,ds,qf,hr,jf,xf,Fi,_t,Ui,ht,Ri,fs,bf,zr,gf,Pf,Wi,zt,Ii,$t,Gi,ce,wf,$r,Tf,Cf,Er,Df,Lf,qr,Of,yf,Vi,Et,Xi,D,Mf,jr,Sf,Nf,xr,Bf,Af,br,Ff,Uf,gr,Rf,Wf,Pr,If,Gf,wr,Vf,Xf,Ki,Nn,Kf,Hi,qt,Yi,Be,Hf,Tr,Yf,Jf,Cr,Zf,Qf,Ji,jt,Zi,Ae,ek,Dr,sk,tk,Lr,nk,ok,Qi,xt,ep,bt,sp,q,rk,Or,lk,ak,yr,ik,pk,Mr,uk,ck,Sr,mk,dk,Nr,fk,kk,Br,vk,_k,Ar,hk,zk,Fr,$k,Ek,Ur,qk,jk,Rr,xk,bk,tp,P,gk,Wr,Pk,wk,Ir,Tk,Ck,Gr,Dk,Lk,Vr,Ok,yk,Xr,Mk,Sk,Kr,Nk,Bk,Hr,Ak,Fk,Yr,Uk,Rk,np,gt,op,Pt,rp,L,Wk,Jr,Ik,Gk,Zr,Vk,Xk,Qr,Kk,Hk,el,Yk,Jk,sl,Zk,Qk,tl,ev,sv,lp,ks,tv,nl,nv,ov,ap,wt,ip,vs,rv,ol,lv,av,pp,Bn,iv,up,Tt,cp,Ct,mp,An,pv,dp,Dt,fp,Lt,kp,_s,uv,rl,cv,mv,vp,Ot,_p,hs,dv,ll,fv,kv,hp,yt,zp,Mt,$p,zs,vv,al,_v,hv,Ep,St,qp,Fe,zv,il,$v,Ev,pl,qv,jv,jp,Nt,xp,O,xv,ul,bv,gv,cl,Pv,wv,ml,Tv,Cv,dl,Dv,Lv,fl,Ov,yv,kl,Mv,Sv,bp,E,Nv,vl,Bv,Av,_l,Fv,Uv,hl,Rv,Wv,zl,Iv,Gv,$l,Vv,Xv,El,Kv,Hv,ql,Yv,Jv,jl,Zv,Qv,xl,e_,s_,bl,t_,n_,gl,o_,Pl,r_,l_,gp,Bt,Pp,me,a_,wl,i_,p_,Tl,u_,c_,Cl,m_,d_,wp,At,Tp,y,f_,Dl,k_,v_,Ll,__,h_,Ol,z_,$_,yl,E_,q_,Ml,j_,x_,Sl,b_,g_,Cp,Ue,P_,Nl,w_,T_,Bl,C_,D_,Dp,Ze,$s,Al,Ft,L_,Fl,O_,Lp,de,y_,Ul,M_,S_,Rl,N_,B_,Wl,A_,F_,Op,Ut,yp,fe,U_,Il,R_,W_,Gl,I_,G_,Vl,V_,X_,Mp,Fn,K_,Sp,Rt,Np,Es,H_,Xl,Y_,J_,Bp,Wt,Ap,It,Fp,Re,Z_,Kl,Q_,e2,Hl,s2,t2,Up,Gt,Rp,M,n2,Yl,o2,r2,Jl,l2,a2,Zl,i2,p2,Ql,u2,c2,ea,m2,d2,sa,f2,k2,Wp,qs,v2,ta,_2,h2,Ip,Vt,Gp,Un,z2,Vp,Xt,Xp,Kt,Kp,js,$2,na,E2,q2,Hp,Ht,Yp,X,j2,oa,x2,b2,ra,g2,P2,la,w2,T2,aa,C2,D2,ia,L2,O2,Jp,Yt,Zp,Jt,Qp,Rn,y2,eu,Zt,su,Wn,M2,tu,Qt,nu,en,ou,ke,S2,pa,N2,B2,ua,A2,F2,ca,U2,R2,ru,sn,lu,In,W2,au,tn,iu,We,I2,ma,G2,V2,da,X2,K2,pu,Qe,xs,fa,nn,H2,ka,Y2,uu,K,J2,va,Z2,Q2,_a,eh,sh,ha,th,nh,za,oh,rh,cu,on,mu,Gn,lh,du,bs,ah,$a,ih,ph,fu,rn,ku,ve,uh,Ea,ch,mh,qa,dh,fh,ja,kh,vh,vu,_e,_h,xa,hh,zh,ba,$h,Eh,ga,qh,jh,_u,ln,hu,Vn,xh,zu,an,$u,pn,Eu,gs,bh,Pa,gh,Ph,qu,un,ju,S,wh,wa,Th,Ch,Ta,Dh,Lh,Ca,Oh,yh,Da,Mh,Sh,La,Nh,Bh,Oa,Ah,Fh,xu,Ps,Uh,ya,Rh,Wh,bu,cn,gu,Xn,Ih,Pu,mn,wu,dn,Tu,w,Gh,Ma,Vh,Xh,Sa,Kh,Hh,Na,Yh,Jh,Ba,Zh,Qh,Aa,ez,sz,Fa,tz,nz,Ua,oz,rz,Ra,lz,az,Cu,fn,Du,kn,Lu,Kn,iz,Ou,vn,yu,Hn,pz,Mu,_n,Su,hn,Nu,ws,uz,Wa,cz,mz,Bu,zn,Au,T,dz,Ia,fz,kz,Ga,vz,_z,Va,hz,zz,Xa,$z,Ez,Ka,qz,jz,Ha,xz,bz,Ya,gz,Pz,Ja,wz,Tz,Fu,$n,Uu,Yn,Cz,Ru,En,Wu,H,Dz,Za,Lz,Oz,Qa,yz,Mz,ei,Sz,Nz,si,Bz,Az,Iu;return je=new ti({}),ne=new wq({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"}]}}),Hs=new Pq({props:{id:"MR8tZm5ViWU"}}),nt=new ti({}),rt=new h({props:{code:`from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, name=<span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        <span class="hljs-keyword">yield</span> dataset[i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;text&quot;</span>]`}}),lt=new h({props:{code:`with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\\n")`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset)):
        f.write(dataset[i][<span class="hljs-string">&quot;text&quot;</span>] + <span class="hljs-string">&quot;\\n&quot;</span>)`}}),at=new ti({}),it=new h({props:{code:`from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),pt=new h({props:{code:"tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)",highlighted:'tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="hljs-literal">True</span>)'}}),ut=new h({props:{code:`tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`,highlighted:`tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`}}),ct=new h({props:{code:'print(tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),mt=new h({props:{code:"hello how are u?",highlighted:"hello how are u?"}}),us=new gq({props:{$$slots:{default:[Tq]},$$scope:{ctx:ni}}}),dt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"}}),ft=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"}}),kt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)'}}),vt=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),_t=new h({props:{code:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),ht=new h({props:{code:`[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]`,highlighted:'[(<span class="hljs-string">&quot;Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">28</span>))]'}}),zt=new h({props:{code:`pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")`,highlighted:`pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),$t=new h({props:{code:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Et=new h({props:{code:`special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
trainer = trainers.WordPieceTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens)`}}),qt=new h({props:{code:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)",highlighted:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"}}),jt=new h({props:{code:`tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>)
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),xt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),bt=new h({props:{code:`['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']`,highlighted:'[<span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),gt=new h({props:{code:`cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Pt=new h({props:{code:"(2, 3)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)'}}),wt=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,
    pair=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="hljs-string">&quot;[SEP]&quot;</span>, sep_token_id)],
)`}}),Tt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Ct=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']`,highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),Dt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),Lt=new h({props:{code:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;pair&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;sentences&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Ot=new h({props:{code:'tokenizer.decoder = decoders.WordPiece(prefix="##")',highlighted:'tokenizer.decoder = decoders.WordPiece(prefix=<span class="hljs-string">&quot;##&quot;</span>)'}}),yt=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),Mt=new h({props:{code:`"let's test this tokenizer... on a pair of sentences." # Testons ce tokenizer... sur une paire de phrases.`,highlighted:'<span class="hljs-string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer... sur une paire de phrases.</span>'}}),St=new h({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Nt=new h({props:{code:'new_tokenizer = Tokenizer.from_file("tokenizer.json")',highlighted:'new_tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),Bt=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    <span class="hljs-comment"># tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively</span>
    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,
    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,
    cls_token=<span class="hljs-string">&quot;[CLS]&quot;</span>,
    sep_token=<span class="hljs-string">&quot;[SEP]&quot;</span>,
    mask_token=<span class="hljs-string">&quot;[MASK]&quot;</span>,
)`}}),At=new h({props:{code:`from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`}}),Ft=new ti({}),Ut=new h({props:{code:"tokenizer = Tokenizer(models.BPE())",highlighted:"tokenizer = Tokenizer(models.BPE())"}}),Rt=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)",highlighted:'tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>)'}}),Wt=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)'}}),It=new h({props:{code:`[('Let', (0, 3)), ("'s", (3, 5)), ('\u0120test', (5, 10)), ('\u0120pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;s&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u0120test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120pre&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
 (<span class="hljs-string">&#x27;tokenization&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;!&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Gt=new h({props:{code:`trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`trainer = trainers.BpeTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),Vt=new h({props:{code:`tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.BPE()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Xt=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Kt=new h({props:{code:`['L', 'et', "'", 's', '\u0120test', '\u0120this', '\u0120to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-string">&#x27;et&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u0120test&#x27;</span>, <span class="hljs-string">&#x27;\u0120this&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Ht=new h({props:{code:"tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)",highlighted:'tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="hljs-literal">False</span>)'}}),Yt=new h({props:{code:`sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]`,highlighted:`sentence = <span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[<span class="hljs-number">4</span>]
sentence[start:end]`}}),Jt=new h({props:{code:"' test'",highlighted:'<span class="hljs-string">&#x27; test&#x27;</span>'}}),Zt=new h({props:{code:"tokenizer.decoder = decoders.ByteLevel()",highlighted:"tokenizer.decoder = decoders.ByteLevel()"}}),Qt=new h({props:{code:"tokenizer.decode(encoding.ids)",highlighted:"tokenizer.decode(encoding.ids)"}}),en=new h({props:{code:`"Let's test this tokenizer." # Testons ce tokenizer`,highlighted:'<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span> <span class="hljs-comment"># Testons ce tokenizer</span>'}}),sn=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
)`}}),tn=new h({props:{code:`from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`}}),nn=new ti({}),on=new h({props:{code:"tokenizer = Tokenizer(models.Unigram())",highlighted:"tokenizer = Tokenizer(models.Unigram())"}}),rn=new h({props:{code:`from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("\`\`", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Regex

tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [
        normalizers.Replace(<span class="hljs-string">&quot;\`\`&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.Replace(<span class="hljs-string">&quot;&#x27;&#x27;&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(<span class="hljs-string">&quot; {2,}&quot;</span>), <span class="hljs-string">&quot; &quot;</span>),
    ]
)`}}),ln=new h({props:{code:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"}}),an=new h({props:{code:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)'}}),pn=new h({props:{code:`[("\u2581Let's", (0, 5)), ('\u2581test', (5, 10)), ('\u2581the', (10, 14)), ('\u2581pre-tokenizer!', (14, 29))]`,highlighted:'[(<span class="hljs-string">&quot;\u2581Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u2581test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581the&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581pre-tokenizer!&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">29</span>))]'}}),un=new h({props:{code:`special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`,highlighted:`special_tokens = [<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>]
trainer = trainers.UnigramTrainer(
    vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens, unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),cn=new h({props:{code:`tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)`,highlighted:`tokenizer.model = models.Unigram()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),mn=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),dn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.']`,highlighted:'[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),fn=new h({props:{code:`cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),kn=new h({props:{code:"0 1",highlighted:'<span class="hljs-number">0</span> <span class="hljs-number">1</span>'}}),vn=new h({props:{code:`tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,
    pair=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],
)`}}),_n=new h({props:{code:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences!&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),hn=new h({props:{code:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.', '.', '.', '<sep>', '\u2581', 'on', '\u2581', 'a', '\u2581pair', 
  '\u2581of', '\u2581sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]`,highlighted:`[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;\u2581pair&#x27;</span>, 
  <span class="hljs-string">&#x27;\u2581of&#x27;</span>, <span class="hljs-string">&#x27;\u2581sentence&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;cls&gt;&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]`}}),zn=new h({props:{code:"tokenizer.decoder = decoders.Metaspace()",highlighted:"tokenizer.decoder = decoders.Metaspace()"}}),$n=new h({props:{code:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>,
    unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>,
    pad_token=<span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>,
    cls_token=<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>,
    sep_token=<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>,
    mask_token=<span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>,
    padding_side=<span class="hljs-string">&quot;left&quot;</span>,
)`}}),En=new h({props:{code:`from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`}}),{c(){$=r("meta"),Pe=u(),te=r("h1"),le=r("a"),qe=r("span"),m(je.$$.fragment),Fs=u(),xe=r("span"),Us=n("Construction d'un *tokenizer*, bloc par bloc"),es=u(),m(ne.$$.fragment),ss=u(),we=r("p"),Ve=n("Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),ts=u(),R=r("ul"),be=r("li"),Rs=n("normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.)"),Ws=u(),ge=r("li"),Is=n("pr\xE9-tok\xE9nisation (division de l\u2019entr\xE9e en mots)"),Gs=u(),oe=r("li"),Vs=n("passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),Xe=r("em"),Ke=n("tokens"),x=n(")"),wn=u(),ae=r("li"),Tn=n("post-traitement (ajout des tokens sp\xE9ciaux du "),ns=r("em"),Cn=n("tokenizer"),Dn=n(", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),os=r("em"),Ln=n("token"),vc=n(")."),oi=u(),On=r("p"),_c=n("Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),ri=u(),He=r("div"),Xs=r("img"),hc=u(),Ks=r("img"),li=u(),W=r("p"),zc=n("La biblioth\xE8que \u{1F917} "),io=r("em"),$c=n("Tokenizers"),Ec=n(" a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes, que vous pouvez m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),po=r("em"),qc=n("tokenizer"),jc=n(" \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),uo=r("em"),xc=n("tokenizer"),bc=n(" \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),yn=r("a"),gc=n("section 2"),Pc=n(". Vous serez alors en mesure de construire n\u2019importe quel type de "),co=r("em"),wc=n("tokenizer"),Tc=n(" auquel vous pouvez penser !"),ai=u(),m(Hs.$$.fragment),ii=u(),rs=r("p"),Cc=n("Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),mo=r("code"),Dc=n("Tokenizer"),Lc=n(" avec les blocs de construction regroup\xE9s en sous-modules :"),pi=u(),I=r("ul"),Te=r("li"),fo=r("code"),Oc=n("normalizers"),yc=n(" contient tous les types de "),ko=r("code"),Mc=n("Normalizer"),Sc=n(" que vous pouvez utiliser (liste compl\xE8te "),Ys=r("a"),Nc=n("ici"),Bc=n("),"),Ac=u(),Ce=r("li"),vo=r("code"),Fc=n("pre_tokenizers"),Uc=n(" contient tous les types de "),_o=r("code"),Rc=n("PreTokenizer"),Wc=n(" que vous pouvez utiliser (liste compl\xE8te "),Js=r("a"),Ic=n("ici"),Gc=n("),"),Vc=u(),G=r("li"),ho=r("code"),Xc=n("models"),Kc=n(" contient les diff\xE9rents types de "),zo=r("code"),Hc=n("Model"),Yc=n(" que vous pouvez utiliser, comme "),$o=r("code"),Jc=n("BPE"),Zc=n(", "),Eo=r("code"),Qc=n("WordPiece"),em=n(", et "),qo=r("code"),sm=n("Unigram"),tm=n(" (liste compl\xE8te "),Zs=r("a"),nm=n("ici"),om=n("),"),rm=u(),De=r("li"),jo=r("code"),lm=n("trainers"),am=n(" contient tous les diff\xE9rents types de "),xo=r("code"),im=n("Trainer"),pm=n(" que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Qs=r("a"),um=n("ici"),cm=n("),"),mm=u(),Le=r("li"),bo=r("code"),dm=n("post_processors"),fm=n(" contient les diff\xE9rents types de "),go=r("code"),km=n("PostProcessor"),vm=n(" que vous pouvez utiliser (liste compl\xE8te "),et=r("a"),_m=n("ici"),hm=n("),"),zm=u(),Oe=r("li"),Po=r("code"),$m=n("decoders"),Em=n(" contient les diff\xE9rents types de "),wo=r("code"),qm=n("Decoder"),jm=n(" que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),st=r("a"),xm=n("ici"),bm=n(")."),ui=u(),ls=r("p"),gm=n("Vous pouvez trouver la liste compl\xE8te des blocs de construction "),tt=r("a"),Pm=n("ici"),wm=n("."),ci=u(),Ye=r("h2"),as=r("a"),To=r("span"),m(nt.$$.fragment),Tm=u(),Co=r("span"),Cm=n("Acquisition d'un corpus"),mi=u(),ie=r("p"),Dm=n("Pour entra\xEEner notre nouveau "),Do=r("em"),Lm=n("tokenizer"),Om=n(", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir le corpus sont similaires \xE0 celles que nous avons suivies au "),Mn=r("a"),ym=n("d\xE9but de ce chapitre"),Mm=n(", mais cette fois nous utiliserons le jeu de donn\xE9es "),ot=r("a"),Sm=n("WikiText-2"),Nm=n(" :"),di=u(),m(rt.$$.fragment),fi=u(),ye=r("p"),Bm=n("La fonction "),Lo=r("code"),Am=n("get_training_corpus()"),Fm=n(" est un g\xE9n\xE9rateur qui donnera des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Oo=r("em"),Um=n("tokenizer"),Rm=n("."),ki=u(),is=r("p"),Wm=n("\u{1F917} "),yo=r("em"),Im=n("Tokenizers"),Gm=n(" peuvent aussi \xEAtre entra\xEEn\xE9s directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes/entr\xE9es de WikiText-2 que nous pouvons utiliser localement :"),vi=u(),m(lt.$$.fragment),_i=u(),pe=r("p"),Vm=n("Ensuite, nous vous montrerons comment construire vos propres "),Mo=r("em"),Xm=n("tokenizers"),Km=n(" BERT, GPT-2 et XLNet, bloc par bloc. Cela nous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),So=r("em"),Hm=n("WordPiece"),Ym=n(", BPE et "),No=r("em"),Jm=n("Unigram"),Zm=n(". Commen\xE7ons par BERT !"),hi=u(),Je=r("h2"),ps=r("a"),Bo=r("span"),m(at.$$.fragment),Qm=u(),Ao=r("span"),ed=n("Construire un tokenizer *WordPiece* \xE0 partir de z\xE9ro"),zi=u(),b=r("p"),sd=n("Pour construire un "),Fo=r("em"),td=n("tokenizer"),nd=n(" avec la biblioth\xE8que \u{1F917} "),Uo=r("em"),od=n("Tokenizers"),rd=n(", nous commen\xE7ons par instancier un objet "),Ro=r("code"),ld=n("Tokenizer"),ad=n(" avec un "),Wo=r("code"),id=n("model"),pd=n(", puis nous d\xE9finissons ses attributs "),Io=r("code"),ud=n("normalizer"),cd=n(", "),Go=r("code"),md=n("pre_tokenizer"),dd=n(", "),Vo=r("code"),fd=n("post_processor"),kd=n(", et "),Xo=r("code"),vd=n("decoder"),_d=n(" aux valeurs que nous voulons."),$i=u(),Me=r("p"),hd=n("Pour cet exemple, nous allons cr\xE9er un "),Ko=r("code"),zd=n("Tokenizer"),$d=n(" avec un mod\xE8le "),Ho=r("em"),Ed=n("WordPiece"),qd=n(" :"),Ei=u(),m(it.$$.fragment),qi=u(),ue=r("p"),jd=n("Nous devons sp\xE9cifier le "),Yo=r("code"),xd=n("unk_token"),bd=n(" pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),Jo=r("code"),gd=n("vocab"),Pd=n(" de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),Zo=r("code"),wd=n("max_input_chars_per_word"),Td=n(", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),ji=u(),g=r("p"),Cd=n("La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation, donc commen\xE7ons par cela. Puisque BERT est largement utilis\xE9, il y a un "),Qo=r("code"),Dd=n("BertNormalizer"),Ld=n(" avec les options classiques que nous pouvons d\xE9finir pour BERT : "),er=r("code"),Od=n("lowercase"),yd=n(" et "),sr=r("code"),Md=n("strip_accents"),Sd=n(", qui sont auto-explicatifs ; "),tr=r("code"),Nd=n("clean_text"),Bd=n(" pour enlever tous les caract\xE8res de contr\xF4le et remplacer les espaces r\xE9p\xE9t\xE9s par un seul ; et "),nr=r("code"),Ad=n("handle_chinese_chars"),Fd=n(", qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),or=r("em"),Ud=n("tokenizer"),Rd=u(),rr=r("code"),Wd=n("bert-base-uncased"),Id=n(", nous pouvons simplement d\xE9finir ce "),lr=r("em"),Gd=n("normalizer"),Vd=n(" :"),xi=u(),m(pt.$$.fragment),bi=u(),V=r("p"),Xd=n("En g\xE9n\xE9ral, cependant, lorsque vous construisez un nouveau "),ar=r("em"),Kd=n("tokenizer"),Hd=n(", vous n\u2019aurez pas acc\xE8s \xE0 un normalisateur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),ir=r("em"),Yd=n("Tokenizers"),Jd=n(". Donc voyons comment cr\xE9er le normalisateur BERT manuellement. La biblioth\xE8que fournit un normaliseur "),pr=r("code"),Zd=n("Lowercase"),Qd=n(" et un normaliseur "),ur=r("code"),ef=n("StripAccents"),sf=n(", et vous pouvez composer plusieurs normaliseurs en utilisant une "),cr=r("code"),tf=n("Sequence"),nf=n(" :"),gi=u(),m(ut.$$.fragment),Pi=u(),Se=r("p"),of=n("Nous utilisons \xE9galement un normaliseur Unicode "),mr=r("code"),rf=n("NFD"),lf=n(", car sinon le normalisateur "),dr=r("code"),af=n("StripAccents"),pf=n(" ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),wi=u(),Ne=r("p"),uf=n("Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),fr=r("code"),cf=n("normalize_str()"),mf=n(" du "),kr=r("code"),df=n("normalizer"),ff=n(" pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),Ti=u(),m(ct.$$.fragment),Ci=u(),m(mt.$$.fragment),Di=u(),m(us.$$.fragment),Li=u(),cs=r("p"),kf=n("L\u2019\xE9tape suivante est la pr\xE9-tokenalisation. Encore une fois, il y a un "),vr=r("code"),vf=n("BertPreTokenizer"),_f=n(" pr\xE9construit que nous pouvons utiliser :"),Oi=u(),m(dt.$$.fragment),yi=u(),Sn=r("p"),hf=n("Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),Mi=u(),m(ft.$$.fragment),Si=u(),ms=r("p"),zf=n("Notez que le pr\xE9-tokenizer "),_r=r("code"),$f=n("Whitespace"),Ef=n(" divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement, donc techniquement il divise sur les espaces et la ponctuation :"),Ni=u(),m(kt.$$.fragment),Bi=u(),m(vt.$$.fragment),Ai=u(),ds=r("p"),qf=n("Si vous voulez seulement s\xE9parer sur les espaces, vous devriez utiliser le pr\xE9-tokenizer "),hr=r("code"),jf=n("WhitespaceSplit"),xf=n(" \xE0 la place :"),Fi=u(),m(_t.$$.fragment),Ui=u(),m(ht.$$.fragment),Ri=u(),fs=r("p"),bf=n("Comme pour les normaliseurs, vous pouvez utiliser une "),zr=r("code"),gf=n("Sequence"),Pf=n(" pour composer plusieurs pr\xE9-tokenizers :"),Wi=u(),m(zt.$$.fragment),Ii=u(),m($t.$$.fragment),Gi=u(),ce=r("p"),wf=n("L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),$r=r("code"),Tf=n("WordPieceTrainer"),Cf=n(". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Er=r("em"),Df=n("Tokenizers"),Lf=n(" est que vous devez lui passer tous les "),qr=r("em"),Of=n("tokens"),yf=n(" sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire, puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Vi=u(),m(Et.$$.fragment),Xi=u(),D=r("p"),Mf=n("En plus de sp\xE9cifier la "),jr=r("code"),Sf=n("vocab_size"),Nf=n(" et les "),xr=r("code"),Bf=n("special_tokens"),Af=n(", nous pouvons d\xE9finir la "),br=r("code"),Ff=n("min_frequency"),Uf=n(" (le nombre de fois qu\u2019un "),gr=r("em"),Rf=n("token"),Wf=n(" doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Pr=r("code"),If=n("continuing_subword_prefix"),Gf=n(" (si nous voulons utiliser quelque chose de diff\xE9rent de "),wr=r("code"),Vf=n("##"),Xf=n(")."),Ki=u(),Nn=r("p"),Kf=n("Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),Hi=u(),m(qt.$$.fragment),Yi=u(),Be=r("p"),Hf=n("Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),Tr=r("em"),Yf=n("tokenizer"),Jf=n(", qui ressemblerait \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Cr=r("code"),Zf=n("WordPiece"),Qf=n(" vide au pr\xE9alable) :"),Ji=u(),m(jt.$$.fragment),Zi=u(),Ae=r("p"),ek=n("Dans les deux cas, nous pouvons ensuite tester le "),Dr=r("em"),sk=n("tokenizer"),tk=n(" sur un texte en appelant la m\xE9thode "),Lr=r("code"),nk=n("encode()"),ok=n(" :"),Qi=u(),m(xt.$$.fragment),ep=u(),m(bt.$$.fragment),sp=u(),q=r("p"),rk=n("Le "),Or=r("code"),lk=n("encodage"),ak=n(" obtenu est un "),yr=r("code"),ik=n("Encoding"),pk=n(", qui contient toutes les sorties n\xE9cessaires du "),Mr=r("em"),uk=n("tokenizer"),ck=n(" dans ses diff\xE9rents attributs : "),Sr=r("code"),mk=n("ids"),dk=n(", "),Nr=r("code"),fk=n("type_ids"),kk=n(", "),Br=r("code"),vk=n("tokens"),_k=n(", "),Ar=r("code"),hk=n("offsets"),zk=n(", "),Fr=r("code"),$k=n("attention_mask"),Ek=n(", "),Ur=r("code"),qk=n("special_tokens_mask"),jk=n(", et "),Rr=r("code"),xk=n("overflowing"),bk=n("."),tp=u(),P=r("p"),gk=n("La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Wr=r("em"),Pk=n("token"),wk=u(),Ir=r("code"),Tk=n("[CLS]"),Ck=n(" au d\xE9but et le "),Gr=r("em"),Dk=n("token"),Lk=u(),Vr=r("code"),Ok=n("[SEP]"),yk=n(" \xE0 la fin (ou apr\xE8s chaque phrase, si nous avons une paire de phrases). Nous utiliserons un "),Xr=r("code"),Mk=n("TemplateProcessor"),Sk=n(" pour cela, mais d\u2019abord nous devons conna\xEEtre les ID des "),Kr=r("em"),Nk=n("tokens"),Bk=u(),Hr=r("code"),Ak=n("[CLS]"),Fk=n(" et "),Yr=r("code"),Uk=n("[SEP]"),Rk=n(" dans le vocabulaire :"),np=u(),m(gt.$$.fragment),op=u(),m(Pt.$$.fragment),rp=u(),L=r("p"),Wk=n("Pour \xE9crire le mod\xE8le pour le "),Jr=r("code"),Ik=n("TemplateProcessor"),Gk=n(", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),Zr=r("em"),Vk=n("tokens"),Xk=n(" sp\xE9ciaux que nous voulons utiliser ; la premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),Qr=r("code"),Kk=n("$A"),Hk=n(", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),el=r("code"),Yk=n("$B"),Jk=n(". Pour chacun de ces \xE9l\xE9ments ("),sl=r("em"),Zk=n("tokens"),Qk=n(" sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019ID du type de "),tl=r("em"),ev=n("token"),sv=n(" correspondant apr\xE8s un deux-points."),lp=u(),ks=r("p"),tv=n("Le "),nl=r("em"),nv=n("template"),ov=n(" classique de BERT est donc d\xE9fini comme suit :"),ap=u(),m(wt.$$.fragment),ip=u(),vs=r("p"),rv=n("Notez que nous devons transmettre les ID des jetons sp\xE9ciaux, afin que le "),ol=r("em"),lv=n("tokenizer"),av=n(" puisse les convertir correctement en leurs ID."),pp=u(),Bn=r("p"),iv=n("Une fois que cela est ajout\xE9, revenir \xE0 notre exemple pr\xE9c\xE9dent donnera :"),up=u(),m(Tt.$$.fragment),cp=u(),m(Ct.$$.fragment),mp=u(),An=r("p"),pv=n("Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),dp=u(),m(Dt.$$.fragment),fp=u(),m(Lt.$$.fragment),kp=u(),_s=r("p"),uv=n("Nous avons presque fini de construire ce "),rl=r("em"),cv=n("tokenizer"),mv=n(" \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),vp=u(),m(Ot.$$.fragment),_p=u(),hs=r("p"),dv=n("Testons-le sur notre pr\xE9c\xE9dent "),ll=r("code"),fv=n("encoding"),kv=n(" :"),hp=u(),m(yt.$$.fragment),zp=u(),m(Mt.$$.fragment),$p=u(),zs=r("p"),vv=n("G\xE9nial ! Nous pouvons enregistrer notre "),al=r("em"),_v=n("tokenizer"),hv=n(" dans un seul fichier JSON comme ceci :"),Ep=u(),m(St.$$.fragment),qp=u(),Fe=r("p"),zv=n("Nous pouvons alors recharger ce fichier dans un objet "),il=r("code"),$v=n("Tokenizer"),Ev=n(" avec la m\xE9thode "),pl=r("code"),qv=n("from_file()"),jv=n(" :"),jp=u(),m(Nt.$$.fragment),xp=u(),O=r("p"),xv=n("Pour utiliser ce "),ul=r("em"),bv=n("tokenizer"),gv=n(" dans \u{1F917} "),cl=r("em"),Pv=n("Transformers"),wv=n(", nous devons l\u2019envelopper dans un "),ml=r("code"),Tv=n("PreTrainedTokenizerFast"),Cv=n(". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),dl=r("em"),Dv=n("tokenizer"),Lv=n(" correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),fl=r("code"),Ov=n("BertTokenizerFast"),yv=n("). Si vous appliquez cette le\xE7on pour construire un tout nouveau "),kl=r("em"),Mv=n("tokenizer"),Sv=n(", vous devrez utiliser la premi\xE8re option."),bp=u(),E=r("p"),Nv=n("Pour envelopper le "),vl=r("em"),Bv=n("tokenizer"),Av=n(" dans un "),_l=r("code"),Fv=n("PreTrainedTokenizerFast"),Uv=n(", nous pouvons soit passer le "),hl=r("em"),Rv=n("tokenizer"),Wv=n("que nous avons construit comme un "),zl=r("code"),Iv=n("tokenizer_object"),Gv=n(", soit passer le fichier de "),$l=r("em"),Vv=n("tokenizer"),Xv=n(" que nous avons sauvegard\xE9 comme "),El=r("code"),Kv=n("tokenizer_file"),Hv=n(". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),ql=r("em"),Yv=n("tokens"),Jv=n(" sp\xE9ciaux, car cette classe ne peut pas d\xE9duire de l\u2019objet "),jl=r("code"),Zv=n("tokenizer"),Qv=n(" quel "),xl=r("em"),e_=n("token"),s_=n(" est le "),bl=r("em"),t_=n("token"),n_=n(" de masque, le "),gl=r("em"),o_=n("token"),Pl=r("code"),r_=n("[CLS]"),l_=n(", etc :"),gp=u(),m(Bt.$$.fragment),Pp=u(),me=r("p"),a_=n("Si vous utilisez une classe de "),wl=r("em"),i_=n("tokenizer"),p_=n(" sp\xE9cifique (comme "),Tl=r("code"),u_=n("BertTokenizerFast"),c_=n("), vous aurez seulement besoin de sp\xE9cifier les "),Cl=r("em"),m_=n("tokens"),d_=n(" sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),wp=u(),m(At.$$.fragment),Tp=u(),y=r("p"),f_=n("Vous pouvez ensuite utiliser ce "),Dl=r("em"),k_=n("tokenizer"),v_=n(" comme n\u2019importe quel autre "),Ll=r("em"),__=n("tokenizer"),h_=n(" de \u{1F917} "),Ol=r("em"),z_=n("Transformers"),$_=n(". Vous pouvez le sauvegarder avec la m\xE9thode "),yl=r("code"),E_=n("save_pretrained()"),q_=n(", ou le t\xE9l\xE9charger sur le "),Ml=r("em"),j_=n("Hub"),x_=n(" avec la m\xE9thode "),Sl=r("code"),b_=n("push_to_hub()"),g_=n("."),Cp=u(),Ue=r("p"),P_=n("Maintenant que nous avons vu comment construire un "),Nl=r("em"),w_=n("tokenizer WordPiece"),T_=n(", faisons de m\xEAme pour un "),Bl=r("em"),C_=n("tokenizer"),D_=n(" BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes, et nous ne soulignerons que les diff\xE9rences."),Dp=u(),Ze=r("h2"),$s=r("a"),Al=r("span"),m(Ft.$$.fragment),L_=u(),Fl=r("span"),O_=n("Construire un *tokenizer* BPE \xE0 partir de z\xE9ro"),Lp=u(),de=r("p"),y_=n("Construisons maintenant un "),Ul=r("em"),M_=n("tokenizer"),S_=n(" BPE. Comme pour le "),Rl=r("em"),N_=n("tokenizer"),B_=n(" BERT, nous commen\xE7ons par initialiser un "),Wl=r("code"),A_=n("Tokenizer"),F_=n(" avec un mod\xE8le BPE :"),Op=u(),m(Ut.$$.fragment),yp=u(),fe=r("p"),U_=n("Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Il=r("code"),R_=n("vocab"),W_=n(" et le "),Gl=r("code"),I_=n("merges"),G_=n(" dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Vl=r("code"),V_=n("unk_token"),X_=n(" parce que GPT-2 utilise un BPE au niveau de l\u2019octet, ce qui ne le n\xE9cessite pas."),Mp=u(),Fn=r("p"),K_=n("GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9-tok\xE9nisation :"),Sp=u(),m(Rt.$$.fragment),Np=u(),Es=r("p"),H_=n("L\u2019option que nous avons ajout\xE9e \xE0 "),Xl=r("code"),Y_=n("ByteLevel"),J_=n(" ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9-tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),Bp=u(),m(Wt.$$.fragment),Ap=u(),m(It.$$.fragment),Fp=u(),Re=r("p"),Z_=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour GPT-2, le seul "),Kl=r("em"),Q_=n("token"),e2=n(" sp\xE9cial est le "),Hl=r("em"),s2=n("token"),t2=n(" de fin de texte :"),Up=u(),m(Gt.$$.fragment),Rp=u(),M=r("p"),n2=n("Comme avec le "),Yl=r("code"),o2=n("WordPieceTrainer"),r2=n(", ainsi que le "),Jl=r("code"),l2=n("vocab_size"),a2=n(" et le "),Zl=r("code"),i2=n("special_tokens"),p2=n(", nous pouvons sp\xE9cifier la "),Ql=r("code"),u2=n("min_frequency"),c2=n(" si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ea=r("code"),m2=n("</w>"),d2=n("), nous pouvons le d\xE9finir avec "),sa=r("code"),f2=n("end_of_word_suffix"),k2=n("."),Wp=u(),qs=r("p"),v2=n("Ce "),ta=r("em"),_2=n("tokenizer"),h2=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),Ip=u(),m(Vt.$$.fragment),Gp=u(),Un=r("p"),z2=n("Regardons la tokenisation d\u2019un exemple de texte :"),Vp=u(),m(Xt.$$.fragment),Xp=u(),m(Kt.$$.fragment),Kp=u(),js=r("p"),$2=n("Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),na=r("em"),E2=n("tokenizer"),q2=n(" du GPT-2 comme suit :"),Hp=u(),m(Ht.$$.fragment),Yp=u(),X=r("p"),j2=n("L\u2019option "),oa=r("code"),x2=n("trim_offsets = False"),b2=n(" indique au post-processeur que nous devons laisser les "),ra=r("em"),g2=n("offsets"),P2=n(" des "),la=r("em"),w2=n("tokens"),T2=n(" qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),aa=r("em"),C2=n("offsets"),D2=n(" pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du token). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),ia=r("code"),L2=n("'\u0120test'"),O2=n(" est le token \xE0 l\u2019index 4 :"),Jp=u(),m(Yt.$$.fragment),Zp=u(),m(Jt.$$.fragment),Qp=u(),Rn=r("p"),y2=n("Enfin, nous ajoutons un d\xE9codeur de niveau octet :"),eu=u(),m(Zt.$$.fragment),su=u(),Wn=r("p"),M2=n("et nous pourrons v\xE9rifier qu\u2019il fonctionne correctement :"),tu=u(),m(Qt.$$.fragment),nu=u(),m(en.$$.fragment),ou=u(),ke=r("p"),S2=n("Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),pa=r("code"),N2=n("PreTrainedTokenizerFast"),B2=n(" ou un "),ua=r("code"),A2=n("GPT2TokenizerFast"),F2=n(" si nous voulons l\u2019utiliser dans \u{1F917} "),ca=r("em"),U2=n("Transformers"),R2=n(" :"),ru=u(),m(sn.$$.fragment),lu=u(),In=r("p"),W2=n("ou :"),au=u(),m(tn.$$.fragment),iu=u(),We=r("p"),I2=n("Comme dernier exemple, nous allons vous montrer comment construire un "),ma=r("em"),G2=n("tokenizer"),V2=u(),da=r("em"),X2=n("Unigram"),K2=n(" \xE0 partir de z\xE9ro."),pu=u(),Qe=r("h2"),xs=r("a"),fa=r("span"),m(nn.$$.fragment),H2=u(),ka=r("span"),Y2=n("Construire un *tokenizer* *Unigram* \xE0 partir de rien."),uu=u(),K=r("p"),J2=n("Construisons maintenant un "),va=r("em"),Z2=n("tokenizer"),Q2=n(" XLNet. Comme pour les "),_a=r("em"),eh=n("tokenizers"),sh=n(" pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ha=r("code"),th=n("Tokenizer"),nh=n(" avec un mod\xE8le "),za=r("em"),oh=n("Unigram"),rh=n(" :"),cu=u(),m(on.$$.fragment),mu=u(),Gn=r("p"),lh=n("Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),du=u(),bs=r("p"),ah=n("Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),$a=r("em"),ih=n("SentencePiece"),ph=n(") :"),fu=u(),m(rn.$$.fragment),ku=u(),ve=r("p"),uh=n("Cela remplace "),Ea=r("code"),ch=n("\u201C"),mh=n(" et "),qa=r("code"),dh=n("\u201D"),fh=n(" avec "),ja=r("code"),kh=n("\u201D"),vh=n(" et toute s\xE9quence de deux espaces ou plus par un seul espace, ainsi que la suppression des accents dans les textes \xE0 cat\xE9goriser."),vu=u(),_e=r("p"),_h=n("Le pr\xE9-"),xa=r("em"),hh=n("tokenizer"),zh=n(" \xE0 utiliser pour tout "),ba=r("em"),$h=n("tokenizer SentencePiece"),Eh=n(" est "),ga=r("code"),qh=n("Metaspace"),jh=n(" :"),_u=u(),m(ln.$$.fragment),hu=u(),Vn=r("p"),xh=n("Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9-tok\xE9nisation d\u2019un exemple de texte comme pr\xE9c\xE9demment :"),zu=u(),m(an.$$.fragment),$u=u(),m(pn.$$.fragment),Eu=u(),gs=r("p"),bh=n("Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Pa=r("em"),gh=n("tokens"),Ph=n(" sp\xE9ciaux :"),qu=u(),m(un.$$.fragment),ju=u(),S=r("p"),wh=n("Un argument tr\xE8s important \xE0 ne pas oublier pour le "),wa=r("code"),Th=n("UnigramTrainer"),Ch=n(" est le "),Ta=r("code"),Dh=n("unk_token"),Lh=n(". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Ca=r("em"),Oh=n("Unigram"),yh=n(", comme le "),Da=r("code"),Mh=n("shrinking_factor"),Sh=n(" pour chaque \xE9tape o\xF9 nous enlevons des "),La=r("em"),Nh=n("tokens"),Bh=n(" (par d\xE9faut 0.75) ou le "),Oa=r("code"),Ah=n("max_piece_length"),Fh=n(" pour sp\xE9cifier la longueur maximale d\u2019un token donn\xE9 (par d\xE9faut 16)."),xu=u(),Ps=r("p"),Uh=n("Ce "),ya=r("em"),Rh=n("tokenizer"),Wh=n(" peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),bu=u(),m(cn.$$.fragment),gu=u(),Xn=r("p"),Ih=n("Regardons la tokenisation d\u2019un exemple de texte :"),Pu=u(),m(mn.$$.fragment),wu=u(),m(dn.$$.fragment),Tu=u(),w=r("p"),Gh=n("Une particularit\xE9 de XLNet est qu\u2019il place le "),Ma=r("em"),Vh=n("token"),Xh=u(),Sa=r("code"),Kh=n("<cls>"),Hh=n(" \xE0 la fin de la phrase, avec un type ID de 2 (pour le distinguer des autres "),Na=r("em"),Yh=n("tokens"),Jh=n("). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ba=r("em"),Zh=n("tokens"),Qh=n(" sp\xE9ciaux et les IDs de type de "),Aa=r("em"),ez=n("token"),sz=n(" avec un mod\xE8le, comme pour BERT, mais d\u2019abord nous devons obtenir les IDs des "),Fa=r("em"),tz=n("tokens"),nz=u(),Ua=r("code"),oz=n("<cls>"),rz=n(" et "),Ra=r("code"),lz=n("<sep>"),az=n(" :"),Cu=u(),m(fn.$$.fragment),Du=u(),m(kn.$$.fragment),Lu=u(),Kn=r("p"),iz=n("Le mod\xE8le ressemble \xE0 ceci :"),Ou=u(),m(vn.$$.fragment),yu=u(),Hn=r("p"),pz=n("Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),Mu=u(),m(_n.$$.fragment),Su=u(),m(hn.$$.fragment),Nu=u(),ws=r("p"),uz=n("Enfin, nous ajoutons un d\xE9codeur "),Wa=r("code"),cz=n("Metaspace"),mz=n(" :"),Bu=u(),m(zn.$$.fragment),Au=u(),T=r("p"),dz=n("et on en a fini avec ce "),Ia=r("em"),fz=n("tokenizer"),kz=n(" ! On peut sauvegarder le "),Ga=r("em"),vz=n("tokenizer"),_z=n(" comme avant, et l\u2019envelopper dans un "),Va=r("code"),hz=n("PreTrainedTokenizerFast"),zz=n(" ou "),Xa=r("code"),$z=n("XLNetTokenizerFast"),Ez=n(" si on veut l\u2019utiliser dans \u{1F917} "),Ka=r("em"),qz=n("Transformers"),jz=n(". Une chose \xE0 noter lors de l\u2019utilisation de "),Ha=r("code"),xz=n("PreTrainedTokenizerFast"),bz=n(" est qu\u2019en plus des "),Ya=r("em"),gz=n("tokens"),Pz=n(" sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),Ja=r("em"),wz=n("Transformers"),Tz=n(" de remplir \xE0 gauche :"),Fu=u(),m($n.$$.fragment),Uu=u(),Yn=r("p"),Cz=n("Ou alternativement :"),Ru=u(),m(En.$$.fragment),Wu=u(),H=r("p"),Dz=n("Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),Za=r("em"),Lz=n("tokenizers"),Oz=n(" existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),Qa=r("em"),yz=n("tokenizer"),Mz=n(" que vous voulez avec la biblioth\xE8que \u{1F917} "),ei=r("em"),Sz=n("Tokenizers"),Nz=n(" et pouvoir l\u2019utiliser dans \u{1F917} "),si=r("em"),Bz=n("Transformers"),Az=n("."),this.h()},l(e){const i=xq('[data-svelte="svelte-1phssyn"]',document.head);$=l(i,"META",{name:!0,content:!0}),i.forEach(t),Pe=c(e),te=l(e,"H1",{class:!0});var qn=a(te);le=l(qn,"A",{id:!0,class:!0,href:!0});var Rz=a(le);qe=l(Rz,"SPAN",{});var Wz=a(qe);d(je.$$.fragment,Wz),Wz.forEach(t),Rz.forEach(t),Fs=c(qn),xe=l(qn,"SPAN",{});var Iz=a(xe);Us=o(Iz,"Construction d'un *tokenizer*, bloc par bloc"),Iz.forEach(t),qn.forEach(t),es=c(e),d(ne.$$.fragment,e),ss=c(e),we=l(e,"P",{});var Gz=a(we);Ve=o(Gz,"Comme nous l\u2019avons vu dans les sections pr\xE9c\xE9dentes, la tokenisation comprend plusieurs \xE9tapes :"),Gz.forEach(t),ts=c(e),R=l(e,"UL",{});var Ts=a(R);be=l(Ts,"LI",{});var Vz=a(be);Rs=o(Vz,"normalisation (tout nettoyage du texte jug\xE9 n\xE9cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.)"),Vz.forEach(t),Ws=c(Ts),ge=l(Ts,"LI",{});var Xz=a(ge);Is=o(Xz,"pr\xE9-tok\xE9nisation (division de l\u2019entr\xE9e en mots)"),Xz.forEach(t),Gs=c(Ts),oe=l(Ts,"LI",{});var Gu=a(oe);Vs=o(Gu,"passage de l\u2019entr\xE9e dans le mod\xE8le (utilisation des mots pr\xE9tok\xE9nis\xE9s pour produire une s\xE9quence de "),Xe=l(Gu,"EM",{});var Kz=a(Xe);Ke=o(Kz,"tokens"),Kz.forEach(t),x=o(Gu,")"),Gu.forEach(t),wn=c(Ts),ae=l(Ts,"LI",{});var Jn=a(ae);Tn=o(Jn,"post-traitement (ajout des tokens sp\xE9ciaux du "),ns=l(Jn,"EM",{});var Hz=a(ns);Cn=o(Hz,"tokenizer"),Hz.forEach(t),Dn=o(Jn,", g\xE9n\xE9ration du masque d\u2019attention et des identifiants du type de "),os=l(Jn,"EM",{});var Yz=a(os);Ln=o(Yz,"token"),Yz.forEach(t),vc=o(Jn,")."),Jn.forEach(t),Ts.forEach(t),oi=c(e),On=l(e,"P",{});var Jz=a(On);_c=o(Jz,"Pour m\xE9moire, voici un autre aper\xE7u du processus global :"),Jz.forEach(t),ri=c(e),He=l(e,"DIV",{class:!0});var Vu=a(He);Xs=l(Vu,"IMG",{class:!0,src:!0,alt:!0}),hc=c(Vu),Ks=l(Vu,"IMG",{class:!0,src:!0,alt:!0}),Vu.forEach(t),li=c(e),W=l(e,"P",{});var he=a(W);zc=o(he,"La biblioth\xE8que \u{1F917} "),io=l(he,"EM",{});var Zz=a(io);$c=o(Zz,"Tokenizers"),Zz.forEach(t),Ec=o(he," a \xE9t\xE9 construite pour fournir plusieurs options pour chacune de ces \xE9tapes, que vous pouvez m\xE9langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un "),po=l(he,"EM",{});var Qz=a(po);qc=o(Qz,"tokenizer"),Qz.forEach(t),jc=o(he," \xE0 partir de z\xE9ro, par opposition \xE0 entra\xEEner un nouveau "),uo=l(he,"EM",{});var e$=a(uo);xc=o(e$,"tokenizer"),e$.forEach(t),bc=o(he," \xE0 partir d\u2019un ancien, comme nous l\u2019avons fait dans "),yn=l(he,"A",{href:!0});var s$=a(yn);gc=o(s$,"section 2"),s$.forEach(t),Pc=o(he,". Vous serez alors en mesure de construire n\u2019importe quel type de "),co=l(he,"EM",{});var t$=a(co);wc=o(t$,"tokenizer"),t$.forEach(t),Tc=o(he," auquel vous pouvez penser !"),he.forEach(t),ai=c(e),d(Hs.$$.fragment,e),ii=c(e),rs=l(e,"P",{});var Xu=a(rs);Cc=o(Xu,"Plus pr\xE9cis\xE9ment, la biblioth\xE8que est construite autour d\u2019une classe centrale "),mo=l(Xu,"CODE",{});var n$=a(mo);Dc=o(n$,"Tokenizer"),n$.forEach(t),Lc=o(Xu," avec les blocs de construction regroup\xE9s en sous-modules :"),Xu.forEach(t),pi=c(e),I=l(e,"UL",{});var ze=a(I);Te=l(ze,"LI",{});var jn=a(Te);fo=l(jn,"CODE",{});var o$=a(fo);Oc=o(o$,"normalizers"),o$.forEach(t),yc=o(jn," contient tous les types de "),ko=l(jn,"CODE",{});var r$=a(ko);Mc=o(r$,"Normalizer"),r$.forEach(t),Sc=o(jn," que vous pouvez utiliser (liste compl\xE8te "),Ys=l(jn,"A",{href:!0,rel:!0});var l$=a(Ys);Nc=o(l$,"ici"),l$.forEach(t),Bc=o(jn,"),"),jn.forEach(t),Ac=c(ze),Ce=l(ze,"LI",{});var xn=a(Ce);vo=l(xn,"CODE",{});var a$=a(vo);Fc=o(a$,"pre_tokenizers"),a$.forEach(t),Uc=o(xn," contient tous les types de "),_o=l(xn,"CODE",{});var i$=a(_o);Rc=o(i$,"PreTokenizer"),i$.forEach(t),Wc=o(xn," que vous pouvez utiliser (liste compl\xE8te "),Js=l(xn,"A",{href:!0,rel:!0});var p$=a(Js);Ic=o(p$,"ici"),p$.forEach(t),Gc=o(xn,"),"),xn.forEach(t),Vc=c(ze),G=l(ze,"LI",{});var re=a(G);ho=l(re,"CODE",{});var u$=a(ho);Xc=o(u$,"models"),u$.forEach(t),Kc=o(re," contient les diff\xE9rents types de "),zo=l(re,"CODE",{});var c$=a(zo);Hc=o(c$,"Model"),c$.forEach(t),Yc=o(re," que vous pouvez utiliser, comme "),$o=l(re,"CODE",{});var m$=a($o);Jc=o(m$,"BPE"),m$.forEach(t),Zc=o(re,", "),Eo=l(re,"CODE",{});var d$=a(Eo);Qc=o(d$,"WordPiece"),d$.forEach(t),em=o(re,", et "),qo=l(re,"CODE",{});var f$=a(qo);sm=o(f$,"Unigram"),f$.forEach(t),tm=o(re," (liste compl\xE8te "),Zs=l(re,"A",{href:!0,rel:!0});var k$=a(Zs);nm=o(k$,"ici"),k$.forEach(t),om=o(re,"),"),re.forEach(t),rm=c(ze),De=l(ze,"LI",{});var bn=a(De);jo=l(bn,"CODE",{});var v$=a(jo);lm=o(v$,"trainers"),v$.forEach(t),am=o(bn," contient tous les diff\xE9rents types de "),xo=l(bn,"CODE",{});var _$=a(xo);im=o(_$,"Trainer"),_$.forEach(t),pm=o(bn," que vous pouvez utiliser pour entra\xEEner votre mod\xE8le sur un corpus (un par type de mod\xE8le ; liste compl\xE8te "),Qs=l(bn,"A",{href:!0,rel:!0});var h$=a(Qs);um=o(h$,"ici"),h$.forEach(t),cm=o(bn,"),"),bn.forEach(t),mm=c(ze),Le=l(ze,"LI",{});var gn=a(Le);bo=l(gn,"CODE",{});var z$=a(bo);dm=o(z$,"post_processors"),z$.forEach(t),fm=o(gn," contient les diff\xE9rents types de "),go=l(gn,"CODE",{});var $$=a(go);km=o($$,"PostProcessor"),$$.forEach(t),vm=o(gn," que vous pouvez utiliser (liste compl\xE8te "),et=l(gn,"A",{href:!0,rel:!0});var E$=a(et);_m=o(E$,"ici"),E$.forEach(t),hm=o(gn,"),"),gn.forEach(t),zm=c(ze),Oe=l(ze,"LI",{});var Pn=a(Oe);Po=l(Pn,"CODE",{});var q$=a(Po);$m=o(q$,"decoders"),q$.forEach(t),Em=o(Pn," contient les diff\xE9rents types de "),wo=l(Pn,"CODE",{});var j$=a(wo);qm=o(j$,"Decoder"),j$.forEach(t),jm=o(Pn," que vous pouvez utiliser pour d\xE9coder les sorties de tokenization (liste compl\xE8te "),st=l(Pn,"A",{href:!0,rel:!0});var x$=a(st);xm=o(x$,"ici"),x$.forEach(t),bm=o(Pn,")."),Pn.forEach(t),ze.forEach(t),ui=c(e),ls=l(e,"P",{});var Ku=a(ls);gm=o(Ku,"Vous pouvez trouver la liste compl\xE8te des blocs de construction "),tt=l(Ku,"A",{href:!0,rel:!0});var b$=a(tt);Pm=o(b$,"ici"),b$.forEach(t),wm=o(Ku,"."),Ku.forEach(t),ci=c(e),Ye=l(e,"H2",{class:!0});var Hu=a(Ye);as=l(Hu,"A",{id:!0,class:!0,href:!0});var g$=a(as);To=l(g$,"SPAN",{});var P$=a(To);d(nt.$$.fragment,P$),P$.forEach(t),g$.forEach(t),Tm=c(Hu),Co=l(Hu,"SPAN",{});var w$=a(Co);Cm=o(w$,"Acquisition d'un corpus"),w$.forEach(t),Hu.forEach(t),mi=c(e),ie=l(e,"P",{});var Cs=a(ie);Dm=o(Cs,"Pour entra\xEEner notre nouveau "),Do=l(Cs,"EM",{});var T$=a(Do);Lm=o(T$,"tokenizer"),T$.forEach(t),Om=o(Cs,", nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les \xE9tapes pour acqu\xE9rir le corpus sont similaires \xE0 celles que nous avons suivies au "),Mn=l(Cs,"A",{href:!0});var C$=a(Mn);ym=o(C$,"d\xE9but de ce chapitre"),C$.forEach(t),Mm=o(Cs,", mais cette fois nous utiliserons le jeu de donn\xE9es "),ot=l(Cs,"A",{href:!0,rel:!0});var D$=a(ot);Sm=o(D$,"WikiText-2"),D$.forEach(t),Nm=o(Cs," :"),Cs.forEach(t),di=c(e),d(rt.$$.fragment,e),fi=c(e),ye=l(e,"P",{});var Zn=a(ye);Bm=o(Zn,"La fonction "),Lo=l(Zn,"CODE",{});var L$=a(Lo);Am=o(L$,"get_training_corpus()"),L$.forEach(t),Fm=o(Zn," est un g\xE9n\xE9rateur qui donnera des batchs de 1 000 textes, que nous utiliserons pour entra\xEEner le "),Oo=l(Zn,"EM",{});var O$=a(Oo);Um=o(O$,"tokenizer"),O$.forEach(t),Rm=o(Zn,"."),Zn.forEach(t),ki=c(e),is=l(e,"P",{});var Yu=a(is);Wm=o(Yu,"\u{1F917} "),yo=l(Yu,"EM",{});var y$=a(yo);Im=o(y$,"Tokenizers"),y$.forEach(t),Gm=o(Yu," peuvent aussi \xEAtre entra\xEEn\xE9s directement sur des fichiers texte. Voici comment nous pouvons g\xE9n\xE9rer un fichier texte contenant tous les textes/entr\xE9es de WikiText-2 que nous pouvons utiliser localement :"),Yu.forEach(t),vi=c(e),d(lt.$$.fragment,e),_i=c(e),pe=l(e,"P",{});var Ds=a(pe);Vm=o(Ds,"Ensuite, nous vous montrerons comment construire vos propres "),Mo=l(Ds,"EM",{});var M$=a(Mo);Xm=o(M$,"tokenizers"),M$.forEach(t),Km=o(Ds," BERT, GPT-2 et XLNet, bloc par bloc. Cela nous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : "),So=l(Ds,"EM",{});var S$=a(So);Hm=o(S$,"WordPiece"),S$.forEach(t),Ym=o(Ds,", BPE et "),No=l(Ds,"EM",{});var N$=a(No);Jm=o(N$,"Unigram"),N$.forEach(t),Zm=o(Ds,". Commen\xE7ons par BERT !"),Ds.forEach(t),hi=c(e),Je=l(e,"H2",{class:!0});var Ju=a(Je);ps=l(Ju,"A",{id:!0,class:!0,href:!0});var B$=a(ps);Bo=l(B$,"SPAN",{});var A$=a(Bo);d(at.$$.fragment,A$),A$.forEach(t),B$.forEach(t),Qm=c(Ju),Ao=l(Ju,"SPAN",{});var F$=a(Ao);ed=o(F$,"Construire un tokenizer *WordPiece* \xE0 partir de z\xE9ro"),F$.forEach(t),Ju.forEach(t),zi=c(e),b=l(e,"P",{});var N=a(b);sd=o(N,"Pour construire un "),Fo=l(N,"EM",{});var U$=a(Fo);td=o(U$,"tokenizer"),U$.forEach(t),nd=o(N," avec la biblioth\xE8que \u{1F917} "),Uo=l(N,"EM",{});var R$=a(Uo);od=o(R$,"Tokenizers"),R$.forEach(t),rd=o(N,", nous commen\xE7ons par instancier un objet "),Ro=l(N,"CODE",{});var W$=a(Ro);ld=o(W$,"Tokenizer"),W$.forEach(t),ad=o(N," avec un "),Wo=l(N,"CODE",{});var I$=a(Wo);id=o(I$,"model"),I$.forEach(t),pd=o(N,", puis nous d\xE9finissons ses attributs "),Io=l(N,"CODE",{});var G$=a(Io);ud=o(G$,"normalizer"),G$.forEach(t),cd=o(N,", "),Go=l(N,"CODE",{});var V$=a(Go);md=o(V$,"pre_tokenizer"),V$.forEach(t),dd=o(N,", "),Vo=l(N,"CODE",{});var X$=a(Vo);fd=o(X$,"post_processor"),X$.forEach(t),kd=o(N,", et "),Xo=l(N,"CODE",{});var K$=a(Xo);vd=o(K$,"decoder"),K$.forEach(t),_d=o(N," aux valeurs que nous voulons."),N.forEach(t),$i=c(e),Me=l(e,"P",{});var Qn=a(Me);hd=o(Qn,"Pour cet exemple, nous allons cr\xE9er un "),Ko=l(Qn,"CODE",{});var H$=a(Ko);zd=o(H$,"Tokenizer"),H$.forEach(t),$d=o(Qn," avec un mod\xE8le "),Ho=l(Qn,"EM",{});var Y$=a(Ho);Ed=o(Y$,"WordPiece"),Y$.forEach(t),qd=o(Qn," :"),Qn.forEach(t),Ei=c(e),d(it.$$.fragment,e),qi=c(e),ue=l(e,"P",{});var Ls=a(ue);jd=o(Ls,"Nous devons sp\xE9cifier le "),Yo=l(Ls,"CODE",{});var J$=a(Yo);xd=o(J$,"unk_token"),J$.forEach(t),bd=o(Ls," pour que le mod\xE8le sache quoi retourner lorsqu\u2019il rencontre des caract\xE8res qu\u2019il n\u2019a pas vu auparavant. D\u2019autres arguments que nous pouvons d\xE9finir ici incluent le "),Jo=l(Ls,"CODE",{});var Z$=a(Jo);gd=o(Z$,"vocab"),Z$.forEach(t),Pd=o(Ls," de notre mod\xE8le (nous allons entra\xEEner le mod\xE8le, donc nous n\u2019avons pas besoin de le d\xE9finir) et "),Zo=l(Ls,"CODE",{});var Q$=a(Zo);wd=o(Q$,"max_input_chars_per_word"),Q$.forEach(t),Td=o(Ls,", qui sp\xE9cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass\xE9e seront s\xE9par\xE9s)."),Ls.forEach(t),ji=c(e),g=l(e,"P",{});var B=a(g);Cd=o(B,"La premi\xE8re \xE9tape de la tok\xE9nisation est la normalisation, donc commen\xE7ons par cela. Puisque BERT est largement utilis\xE9, il y a un "),Qo=l(B,"CODE",{});var eE=a(Qo);Dd=o(eE,"BertNormalizer"),eE.forEach(t),Ld=o(B," avec les options classiques que nous pouvons d\xE9finir pour BERT : "),er=l(B,"CODE",{});var sE=a(er);Od=o(sE,"lowercase"),sE.forEach(t),yd=o(B," et "),sr=l(B,"CODE",{});var tE=a(sr);Md=o(tE,"strip_accents"),tE.forEach(t),Sd=o(B,", qui sont auto-explicatifs ; "),tr=l(B,"CODE",{});var nE=a(tr);Nd=o(nE,"clean_text"),nE.forEach(t),Bd=o(B," pour enlever tous les caract\xE8res de contr\xF4le et remplacer les espaces r\xE9p\xE9t\xE9s par un seul ; et "),nr=l(B,"CODE",{});var oE=a(nr);Ad=o(oE,"handle_chinese_chars"),oE.forEach(t),Fd=o(B,", qui place des espaces autour des caract\xE8res chinois. Pour reproduire le "),or=l(B,"EM",{});var rE=a(or);Ud=o(rE,"tokenizer"),rE.forEach(t),Rd=c(B),rr=l(B,"CODE",{});var lE=a(rr);Wd=o(lE,"bert-base-uncased"),lE.forEach(t),Id=o(B,", nous pouvons simplement d\xE9finir ce "),lr=l(B,"EM",{});var aE=a(lr);Gd=o(aE,"normalizer"),aE.forEach(t),Vd=o(B," :"),B.forEach(t),xi=c(e),d(pt.$$.fragment,e),bi=c(e),V=l(e,"P",{});var $e=a(V);Xd=o($e,"En g\xE9n\xE9ral, cependant, lorsque vous construisez un nouveau "),ar=l($e,"EM",{});var iE=a(ar);Kd=o(iE,"tokenizer"),iE.forEach(t),Hd=o($e,", vous n\u2019aurez pas acc\xE8s \xE0 un normalisateur aussi pratique d\xE9j\xE0 impl\xE9ment\xE9 dans la biblioth\xE8que \u{1F917} "),ir=l($e,"EM",{});var pE=a(ir);Yd=o(pE,"Tokenizers"),pE.forEach(t),Jd=o($e,". Donc voyons comment cr\xE9er le normalisateur BERT manuellement. La biblioth\xE8que fournit un normaliseur "),pr=l($e,"CODE",{});var uE=a(pr);Zd=o(uE,"Lowercase"),uE.forEach(t),Qd=o($e," et un normaliseur "),ur=l($e,"CODE",{});var cE=a(ur);ef=o(cE,"StripAccents"),cE.forEach(t),sf=o($e,", et vous pouvez composer plusieurs normaliseurs en utilisant une "),cr=l($e,"CODE",{});var mE=a(cr);tf=o(mE,"Sequence"),mE.forEach(t),nf=o($e," :"),$e.forEach(t),gi=c(e),d(ut.$$.fragment,e),Pi=c(e),Se=l(e,"P",{});var eo=a(Se);of=o(eo,"Nous utilisons \xE9galement un normaliseur Unicode "),mr=l(eo,"CODE",{});var dE=a(mr);rf=o(dE,"NFD"),dE.forEach(t),lf=o(eo,", car sinon le normalisateur "),dr=l(eo,"CODE",{});var fE=a(dr);af=o(fE,"StripAccents"),fE.forEach(t),pf=o(eo," ne reconna\xEEtra pas correctement les caract\xE8res accentu\xE9s et ne les supprimera donc pas."),eo.forEach(t),wi=c(e),Ne=l(e,"P",{});var so=a(Ne);uf=o(so,"Comme nous l\u2019avons vu pr\xE9c\xE9demment, nous pouvons utiliser la m\xE9thode "),fr=l(so,"CODE",{});var kE=a(fr);cf=o(kE,"normalize_str()"),kE.forEach(t),mf=o(so," du "),kr=l(so,"CODE",{});var vE=a(kr);df=o(vE,"normalizer"),vE.forEach(t),ff=o(so," pour v\xE9rifier les effets qu\u2019il a sur un texte donn\xE9 :"),so.forEach(t),Ti=c(e),d(ct.$$.fragment,e),Ci=c(e),d(mt.$$.fragment,e),Di=c(e),d(us.$$.fragment,e),Li=c(e),cs=l(e,"P",{});var Zu=a(cs);kf=o(Zu,"L\u2019\xE9tape suivante est la pr\xE9-tokenalisation. Encore une fois, il y a un "),vr=l(Zu,"CODE",{});var _E=a(vr);vf=o(_E,"BertPreTokenizer"),_E.forEach(t),_f=o(Zu," pr\xE9construit que nous pouvons utiliser :"),Zu.forEach(t),Oi=c(e),d(dt.$$.fragment,e),yi=c(e),Sn=l(e,"P",{});var hE=a(Sn);hf=o(hE,"Ou nous pouvons le construire \xE0 partir de z\xE9ro :"),hE.forEach(t),Mi=c(e),d(ft.$$.fragment,e),Si=c(e),ms=l(e,"P",{});var Qu=a(ms);zf=o(Qu,"Notez que le pr\xE9-tokenizer "),_r=l(Qu,"CODE",{});var zE=a(_r);$f=o(zE,"Whitespace"),zE.forEach(t),Ef=o(Qu," divise sur les espaces et tous les caract\xE8res qui ne sont pas des lettres, des chiffres ou le caract\xE8re de soulignement, donc techniquement il divise sur les espaces et la ponctuation :"),Qu.forEach(t),Ni=c(e),d(kt.$$.fragment,e),Bi=c(e),d(vt.$$.fragment,e),Ai=c(e),ds=l(e,"P",{});var ec=a(ds);qf=o(ec,"Si vous voulez seulement s\xE9parer sur les espaces, vous devriez utiliser le pr\xE9-tokenizer "),hr=l(ec,"CODE",{});var $E=a(hr);jf=o($E,"WhitespaceSplit"),$E.forEach(t),xf=o(ec," \xE0 la place :"),ec.forEach(t),Fi=c(e),d(_t.$$.fragment,e),Ui=c(e),d(ht.$$.fragment,e),Ri=c(e),fs=l(e,"P",{});var sc=a(fs);bf=o(sc,"Comme pour les normaliseurs, vous pouvez utiliser une "),zr=l(sc,"CODE",{});var EE=a(zr);gf=o(EE,"Sequence"),EE.forEach(t),Pf=o(sc," pour composer plusieurs pr\xE9-tokenizers :"),sc.forEach(t),Wi=c(e),d(zt.$$.fragment,e),Ii=c(e),d($t.$$.fragment,e),Gi=c(e),ce=l(e,"P",{});var Os=a(ce);wf=o(Os,"L\u2019\xE9tape suivante dans le pipeline de tok\xE9nisation est de faire passer les entr\xE9es par le mod\xE8le. Nous avons d\xE9j\xE0 sp\xE9cifi\xE9 notre mod\xE8le dans l\u2019initialisation, mais nous devons encore l\u2019entra\xEEner, ce qui n\xE9cessitera un "),$r=l(Os,"CODE",{});var qE=a($r);Tf=o(qE,"WordPieceTrainer"),qE.forEach(t),Cf=o(Os,". La principale chose \xE0 retenir lors de l\u2019instanciation d\u2019un entra\xEEneur dans \u{1F917} "),Er=l(Os,"EM",{});var jE=a(Er);Df=o(jE,"Tokenizers"),jE.forEach(t),Lf=o(Os," est que vous devez lui passer tous les "),qr=l(Os,"EM",{});var xE=a(qr);Of=o(xE,"tokens"),xE.forEach(t),yf=o(Os," sp\xE9ciaux que vous avez l\u2019intention d\u2019utiliser. Sinon il ne les ajoutera pas au vocabulaire, puisqu\u2019ils ne sont pas dans le corpus d\u2019entra\xEEnement :"),Os.forEach(t),Vi=c(e),d(Et.$$.fragment,e),Xi=c(e),D=l(e,"P",{});var Y=a(D);Mf=o(Y,"En plus de sp\xE9cifier la "),jr=l(Y,"CODE",{});var bE=a(jr);Sf=o(bE,"vocab_size"),bE.forEach(t),Nf=o(Y," et les "),xr=l(Y,"CODE",{});var gE=a(xr);Bf=o(gE,"special_tokens"),gE.forEach(t),Af=o(Y,", nous pouvons d\xE9finir la "),br=l(Y,"CODE",{});var PE=a(br);Ff=o(PE,"min_frequency"),PE.forEach(t),Uf=o(Y," (le nombre de fois qu\u2019un "),gr=l(Y,"EM",{});var wE=a(gr);Rf=o(wE,"token"),wE.forEach(t),Wf=o(Y," doit appara\xEEtre pour \xEAtre inclus dans le vocabulaire) ou changer le "),Pr=l(Y,"CODE",{});var TE=a(Pr);If=o(TE,"continuing_subword_prefix"),TE.forEach(t),Gf=o(Y," (si nous voulons utiliser quelque chose de diff\xE9rent de "),wr=l(Y,"CODE",{});var CE=a(wr);Vf=o(CE,"##"),CE.forEach(t),Xf=o(Y,")."),Y.forEach(t),Ki=c(e),Nn=l(e,"P",{});var DE=a(Nn);Kf=o(DE,"Pour entra\xEEner notre mod\xE8le en utilisant l\u2019it\xE9rateur que nous avons d\xE9fini plus t\xF4t, il suffit d\u2019ex\xE9cuter cette commande :"),DE.forEach(t),Hi=c(e),d(qt.$$.fragment,e),Yi=c(e),Be=l(e,"P",{});var to=a(Be);Hf=o(to,"Nous pouvons \xE9galement utiliser des fichiers texte pour entra\xEEner notre "),Tr=l(to,"EM",{});var LE=a(Tr);Yf=o(LE,"tokenizer"),LE.forEach(t),Jf=o(to,", qui ressemblerait \xE0 ceci (nous r\xE9initialisons le mod\xE8le avec un "),Cr=l(to,"CODE",{});var OE=a(Cr);Zf=o(OE,"WordPiece"),OE.forEach(t),Qf=o(to," vide au pr\xE9alable) :"),to.forEach(t),Ji=c(e),d(jt.$$.fragment,e),Zi=c(e),Ae=l(e,"P",{});var no=a(Ae);ek=o(no,"Dans les deux cas, nous pouvons ensuite tester le "),Dr=l(no,"EM",{});var yE=a(Dr);sk=o(yE,"tokenizer"),yE.forEach(t),tk=o(no," sur un texte en appelant la m\xE9thode "),Lr=l(no,"CODE",{});var ME=a(Lr);nk=o(ME,"encode()"),ME.forEach(t),ok=o(no," :"),no.forEach(t),Qi=c(e),d(xt.$$.fragment,e),ep=c(e),d(bt.$$.fragment,e),sp=c(e),q=l(e,"P",{});var C=a(q);rk=o(C,"Le "),Or=l(C,"CODE",{});var SE=a(Or);lk=o(SE,"encodage"),SE.forEach(t),ak=o(C," obtenu est un "),yr=l(C,"CODE",{});var NE=a(yr);ik=o(NE,"Encoding"),NE.forEach(t),pk=o(C,", qui contient toutes les sorties n\xE9cessaires du "),Mr=l(C,"EM",{});var BE=a(Mr);uk=o(BE,"tokenizer"),BE.forEach(t),ck=o(C," dans ses diff\xE9rents attributs : "),Sr=l(C,"CODE",{});var AE=a(Sr);mk=o(AE,"ids"),AE.forEach(t),dk=o(C,", "),Nr=l(C,"CODE",{});var FE=a(Nr);fk=o(FE,"type_ids"),FE.forEach(t),kk=o(C,", "),Br=l(C,"CODE",{});var UE=a(Br);vk=o(UE,"tokens"),UE.forEach(t),_k=o(C,", "),Ar=l(C,"CODE",{});var RE=a(Ar);hk=o(RE,"offsets"),RE.forEach(t),zk=o(C,", "),Fr=l(C,"CODE",{});var WE=a(Fr);$k=o(WE,"attention_mask"),WE.forEach(t),Ek=o(C,", "),Ur=l(C,"CODE",{});var IE=a(Ur);qk=o(IE,"special_tokens_mask"),IE.forEach(t),jk=o(C,", et "),Rr=l(C,"CODE",{});var GE=a(Rr);xk=o(GE,"overflowing"),GE.forEach(t),bk=o(C,"."),C.forEach(t),tp=c(e),P=l(e,"P",{});var A=a(P);gk=o(A,"La derni\xE8re \xE9tape du pipeline de tok\xE9nisation est le post-traitement. Nous devons ajouter le "),Wr=l(A,"EM",{});var VE=a(Wr);Pk=o(VE,"token"),VE.forEach(t),wk=c(A),Ir=l(A,"CODE",{});var XE=a(Ir);Tk=o(XE,"[CLS]"),XE.forEach(t),Ck=o(A," au d\xE9but et le "),Gr=l(A,"EM",{});var KE=a(Gr);Dk=o(KE,"token"),KE.forEach(t),Lk=c(A),Vr=l(A,"CODE",{});var HE=a(Vr);Ok=o(HE,"[SEP]"),HE.forEach(t),yk=o(A," \xE0 la fin (ou apr\xE8s chaque phrase, si nous avons une paire de phrases). Nous utiliserons un "),Xr=l(A,"CODE",{});var YE=a(Xr);Mk=o(YE,"TemplateProcessor"),YE.forEach(t),Sk=o(A," pour cela, mais d\u2019abord nous devons conna\xEEtre les ID des "),Kr=l(A,"EM",{});var JE=a(Kr);Nk=o(JE,"tokens"),JE.forEach(t),Bk=c(A),Hr=l(A,"CODE",{});var ZE=a(Hr);Ak=o(ZE,"[CLS]"),ZE.forEach(t),Fk=o(A," et "),Yr=l(A,"CODE",{});var QE=a(Yr);Uk=o(QE,"[SEP]"),QE.forEach(t),Rk=o(A," dans le vocabulaire :"),A.forEach(t),np=c(e),d(gt.$$.fragment,e),op=c(e),d(Pt.$$.fragment,e),rp=c(e),L=l(e,"P",{});var J=a(L);Wk=o(J,"Pour \xE9crire le mod\xE8le pour le "),Jr=l(J,"CODE",{});var e1=a(Jr);Ik=o(e1,"TemplateProcessor"),e1.forEach(t),Gk=o(J,", nous devons sp\xE9cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous \xE9crivons les "),Zr=l(J,"EM",{});var s1=a(Zr);Vk=o(s1,"tokens"),s1.forEach(t),Xk=o(J," sp\xE9ciaux que nous voulons utiliser ; la premi\xE8re (ou unique) phrase est repr\xE9sent\xE9e par "),Qr=l(J,"CODE",{});var t1=a(Qr);Kk=o(t1,"$A"),t1.forEach(t),Hk=o(J,", alors que la deuxi\xE8me phrase (si on code une paire) est repr\xE9sent\xE9e par "),el=l(J,"CODE",{});var n1=a(el);Yk=o(n1,"$B"),n1.forEach(t),Jk=o(J,". Pour chacun de ces \xE9l\xE9ments ("),sl=l(J,"EM",{});var o1=a(sl);Zk=o(o1,"tokens"),o1.forEach(t),Qk=o(J," sp\xE9ciaux et phrases), nous sp\xE9cifions \xE9galement l\u2019ID du type de "),tl=l(J,"EM",{});var r1=a(tl);ev=o(r1,"token"),r1.forEach(t),sv=o(J," correspondant apr\xE8s un deux-points."),J.forEach(t),lp=c(e),ks=l(e,"P",{});var tc=a(ks);tv=o(tc,"Le "),nl=l(tc,"EM",{});var l1=a(nl);nv=o(l1,"template"),l1.forEach(t),ov=o(tc," classique de BERT est donc d\xE9fini comme suit :"),tc.forEach(t),ap=c(e),d(wt.$$.fragment,e),ip=c(e),vs=l(e,"P",{});var nc=a(vs);rv=o(nc,"Notez que nous devons transmettre les ID des jetons sp\xE9ciaux, afin que le "),ol=l(nc,"EM",{});var a1=a(ol);lv=o(a1,"tokenizer"),a1.forEach(t),av=o(nc," puisse les convertir correctement en leurs ID."),nc.forEach(t),pp=c(e),Bn=l(e,"P",{});var i1=a(Bn);iv=o(i1,"Une fois que cela est ajout\xE9, revenir \xE0 notre exemple pr\xE9c\xE9dent donnera :"),i1.forEach(t),up=c(e),d(Tt.$$.fragment,e),cp=c(e),d(Ct.$$.fragment,e),mp=c(e),An=l(e,"P",{});var p1=a(An);pv=o(p1,"Et sur une paire de phrases, on obtient le bon r\xE9sultat :"),p1.forEach(t),dp=c(e),d(Dt.$$.fragment,e),fp=c(e),d(Lt.$$.fragment,e),kp=c(e),_s=l(e,"P",{});var oc=a(_s);uv=o(oc,"Nous avons presque fini de construire ce "),rl=l(oc,"EM",{});var u1=a(rl);cv=o(u1,"tokenizer"),u1.forEach(t),mv=o(oc," \xE0 partir de z\xE9ro. La derni\xE8re \xE9tape consiste \xE0 inclure un d\xE9codeur :"),oc.forEach(t),vp=c(e),d(Ot.$$.fragment,e),_p=c(e),hs=l(e,"P",{});var rc=a(hs);dv=o(rc,"Testons-le sur notre pr\xE9c\xE9dent "),ll=l(rc,"CODE",{});var c1=a(ll);fv=o(c1,"encoding"),c1.forEach(t),kv=o(rc," :"),rc.forEach(t),hp=c(e),d(yt.$$.fragment,e),zp=c(e),d(Mt.$$.fragment,e),$p=c(e),zs=l(e,"P",{});var lc=a(zs);vv=o(lc,"G\xE9nial ! Nous pouvons enregistrer notre "),al=l(lc,"EM",{});var m1=a(al);_v=o(m1,"tokenizer"),m1.forEach(t),hv=o(lc," dans un seul fichier JSON comme ceci :"),lc.forEach(t),Ep=c(e),d(St.$$.fragment,e),qp=c(e),Fe=l(e,"P",{});var oo=a(Fe);zv=o(oo,"Nous pouvons alors recharger ce fichier dans un objet "),il=l(oo,"CODE",{});var d1=a(il);$v=o(d1,"Tokenizer"),d1.forEach(t),Ev=o(oo," avec la m\xE9thode "),pl=l(oo,"CODE",{});var f1=a(pl);qv=o(f1,"from_file()"),f1.forEach(t),jv=o(oo," :"),oo.forEach(t),jp=c(e),d(Nt.$$.fragment,e),xp=c(e),O=l(e,"P",{});var Z=a(O);xv=o(Z,"Pour utiliser ce "),ul=l(Z,"EM",{});var k1=a(ul);bv=o(k1,"tokenizer"),k1.forEach(t),gv=o(Z," dans \u{1F917} "),cl=l(Z,"EM",{});var v1=a(cl);Pv=o(v1,"Transformers"),v1.forEach(t),wv=o(Z,", nous devons l\u2019envelopper dans un "),ml=l(Z,"CODE",{});var _1=a(ml);Tv=o(_1,"PreTrainedTokenizerFast"),_1.forEach(t),Cv=o(Z,". Nous pouvons soit utiliser la classe g\xE9n\xE9rique, soit, si notre "),dl=l(Z,"EM",{});var h1=a(dl);Dv=o(h1,"tokenizer"),h1.forEach(t),Lv=o(Z," correspond \xE0 un mod\xE8le existant, utiliser cette classe (ici, "),fl=l(Z,"CODE",{});var z1=a(fl);Ov=o(z1,"BertTokenizerFast"),z1.forEach(t),yv=o(Z,"). Si vous appliquez cette le\xE7on pour construire un tout nouveau "),kl=l(Z,"EM",{});var $1=a(kl);Mv=o($1,"tokenizer"),$1.forEach(t),Sv=o(Z,", vous devrez utiliser la premi\xE8re option."),Z.forEach(t),bp=c(e),E=l(e,"P",{});var j=a(E);Nv=o(j,"Pour envelopper le "),vl=l(j,"EM",{});var E1=a(vl);Bv=o(E1,"tokenizer"),E1.forEach(t),Av=o(j," dans un "),_l=l(j,"CODE",{});var q1=a(_l);Fv=o(q1,"PreTrainedTokenizerFast"),q1.forEach(t),Uv=o(j,", nous pouvons soit passer le "),hl=l(j,"EM",{});var j1=a(hl);Rv=o(j1,"tokenizer"),j1.forEach(t),Wv=o(j,"que nous avons construit comme un "),zl=l(j,"CODE",{});var x1=a(zl);Iv=o(x1,"tokenizer_object"),x1.forEach(t),Gv=o(j,", soit passer le fichier de "),$l=l(j,"EM",{});var b1=a($l);Vv=o(b1,"tokenizer"),b1.forEach(t),Xv=o(j," que nous avons sauvegard\xE9 comme "),El=l(j,"CODE",{});var g1=a(El);Kv=o(g1,"tokenizer_file"),g1.forEach(t),Hv=o(j,". Ce qu\u2019il faut retenir, c\u2019est que nous devons d\xE9finir manuellement tous les "),ql=l(j,"EM",{});var P1=a(ql);Yv=o(P1,"tokens"),P1.forEach(t),Jv=o(j," sp\xE9ciaux, car cette classe ne peut pas d\xE9duire de l\u2019objet "),jl=l(j,"CODE",{});var w1=a(jl);Zv=o(w1,"tokenizer"),w1.forEach(t),Qv=o(j," quel "),xl=l(j,"EM",{});var T1=a(xl);e_=o(T1,"token"),T1.forEach(t),s_=o(j," est le "),bl=l(j,"EM",{});var C1=a(bl);t_=o(C1,"token"),C1.forEach(t),n_=o(j," de masque, le "),gl=l(j,"EM",{});var D1=a(gl);o_=o(D1,"token"),D1.forEach(t),Pl=l(j,"CODE",{});var L1=a(Pl);r_=o(L1,"[CLS]"),L1.forEach(t),l_=o(j,", etc :"),j.forEach(t),gp=c(e),d(Bt.$$.fragment,e),Pp=c(e),me=l(e,"P",{});var ys=a(me);a_=o(ys,"Si vous utilisez une classe de "),wl=l(ys,"EM",{});var O1=a(wl);i_=o(O1,"tokenizer"),O1.forEach(t),p_=o(ys," sp\xE9cifique (comme "),Tl=l(ys,"CODE",{});var y1=a(Tl);u_=o(y1,"BertTokenizerFast"),y1.forEach(t),c_=o(ys,"), vous aurez seulement besoin de sp\xE9cifier les "),Cl=l(ys,"EM",{});var M1=a(Cl);m_=o(M1,"tokens"),M1.forEach(t),d_=o(ys," sp\xE9ciaux qui sont diff\xE9rents de ceux par d\xE9faut (ici, aucun) :"),ys.forEach(t),wp=c(e),d(At.$$.fragment,e),Tp=c(e),y=l(e,"P",{});var Q=a(y);f_=o(Q,"Vous pouvez ensuite utiliser ce "),Dl=l(Q,"EM",{});var S1=a(Dl);k_=o(S1,"tokenizer"),S1.forEach(t),v_=o(Q," comme n\u2019importe quel autre "),Ll=l(Q,"EM",{});var N1=a(Ll);__=o(N1,"tokenizer"),N1.forEach(t),h_=o(Q," de \u{1F917} "),Ol=l(Q,"EM",{});var B1=a(Ol);z_=o(B1,"Transformers"),B1.forEach(t),$_=o(Q,". Vous pouvez le sauvegarder avec la m\xE9thode "),yl=l(Q,"CODE",{});var A1=a(yl);E_=o(A1,"save_pretrained()"),A1.forEach(t),q_=o(Q,", ou le t\xE9l\xE9charger sur le "),Ml=l(Q,"EM",{});var F1=a(Ml);j_=o(F1,"Hub"),F1.forEach(t),x_=o(Q," avec la m\xE9thode "),Sl=l(Q,"CODE",{});var U1=a(Sl);b_=o(U1,"push_to_hub()"),U1.forEach(t),g_=o(Q,"."),Q.forEach(t),Cp=c(e),Ue=l(e,"P",{});var ro=a(Ue);P_=o(ro,"Maintenant que nous avons vu comment construire un "),Nl=l(ro,"EM",{});var R1=a(Nl);w_=o(R1,"tokenizer WordPiece"),R1.forEach(t),T_=o(ro,", faisons de m\xEAme pour un "),Bl=l(ro,"EM",{});var W1=a(Bl);C_=o(W1,"tokenizer"),W1.forEach(t),D_=o(ro," BPE. Nous irons un peu plus vite puisque vous connaissez toutes les \xE9tapes, et nous ne soulignerons que les diff\xE9rences."),ro.forEach(t),Dp=c(e),Ze=l(e,"H2",{class:!0});var ac=a(Ze);$s=l(ac,"A",{id:!0,class:!0,href:!0});var I1=a($s);Al=l(I1,"SPAN",{});var G1=a(Al);d(Ft.$$.fragment,G1),G1.forEach(t),I1.forEach(t),L_=c(ac),Fl=l(ac,"SPAN",{});var V1=a(Fl);O_=o(V1,"Construire un *tokenizer* BPE \xE0 partir de z\xE9ro"),V1.forEach(t),ac.forEach(t),Lp=c(e),de=l(e,"P",{});var Ms=a(de);y_=o(Ms,"Construisons maintenant un "),Ul=l(Ms,"EM",{});var X1=a(Ul);M_=o(X1,"tokenizer"),X1.forEach(t),S_=o(Ms," BPE. Comme pour le "),Rl=l(Ms,"EM",{});var K1=a(Rl);N_=o(K1,"tokenizer"),K1.forEach(t),B_=o(Ms," BERT, nous commen\xE7ons par initialiser un "),Wl=l(Ms,"CODE",{});var H1=a(Wl);A_=o(H1,"Tokenizer"),H1.forEach(t),F_=o(Ms," avec un mod\xE8le BPE :"),Ms.forEach(t),Op=c(e),d(Ut.$$.fragment,e),yp=c(e),fe=l(e,"P",{});var Ss=a(fe);U_=o(Ss,"Comme pour BERT, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le "),Il=l(Ss,"CODE",{});var Y1=a(Il);R_=o(Y1,"vocab"),Y1.forEach(t),W_=o(Ss," et le "),Gl=l(Ss,"CODE",{});var J1=a(Gl);I_=o(J1,"merges"),J1.forEach(t),G_=o(Ss," dans ce cas), mais puisque nous allons nous entra\xEEner \xE0 partir de z\xE9ro, nous n\u2019avons pas besoin de le faire. Nous n\u2019avons pas non plus besoin de sp\xE9cifier un "),Vl=l(Ss,"CODE",{});var Z1=a(Vl);V_=o(Z1,"unk_token"),Z1.forEach(t),X_=o(Ss," parce que GPT-2 utilise un BPE au niveau de l\u2019octet, ce qui ne le n\xE9cessite pas."),Ss.forEach(t),Mp=c(e),Fn=l(e,"P",{});var Q1=a(Fn);K_=o(Q1,"GPT-2 n\u2019utilise pas de normaliseur, donc nous sautons cette \xE9tape et allons directement \xE0 la pr\xE9-tok\xE9nisation :"),Q1.forEach(t),Sp=c(e),d(Rt.$$.fragment,e),Np=c(e),Es=l(e,"P",{});var ic=a(Es);H_=o(ic,"L\u2019option que nous avons ajout\xE9e \xE0 "),Xl=l(ic,"CODE",{});var e7=a(Xl);Y_=o(e7,"ByteLevel"),e7.forEach(t),J_=o(ic," ici est de ne pas ajouter d\u2019espace en d\xE9but de phrase (ce qui est le cas par d\xE9faut). Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9-tok\xE9nisation d\u2019un texte d\u2019exemple comme avant :"),ic.forEach(t),Bp=c(e),d(Wt.$$.fragment,e),Ap=c(e),d(It.$$.fragment,e),Fp=c(e),Re=l(e,"P",{});var lo=a(Re);Z_=o(lo,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. Pour GPT-2, le seul "),Kl=l(lo,"EM",{});var s7=a(Kl);Q_=o(s7,"token"),s7.forEach(t),e2=o(lo," sp\xE9cial est le "),Hl=l(lo,"EM",{});var t7=a(Hl);s2=o(t7,"token"),t7.forEach(t),t2=o(lo," de fin de texte :"),lo.forEach(t),Up=c(e),d(Gt.$$.fragment,e),Rp=c(e),M=l(e,"P",{});var ee=a(M);n2=o(ee,"Comme avec le "),Yl=l(ee,"CODE",{});var n7=a(Yl);o2=o(n7,"WordPieceTrainer"),n7.forEach(t),r2=o(ee,", ainsi que le "),Jl=l(ee,"CODE",{});var o7=a(Jl);l2=o(o7,"vocab_size"),o7.forEach(t),a2=o(ee," et le "),Zl=l(ee,"CODE",{});var r7=a(Zl);i2=o(r7,"special_tokens"),r7.forEach(t),p2=o(ee,", nous pouvons sp\xE9cifier la "),Ql=l(ee,"CODE",{});var l7=a(Ql);u2=o(l7,"min_frequency"),l7.forEach(t),c2=o(ee," si nous le voulons, ou si nous avons un suffixe de fin de mot (comme "),ea=l(ee,"CODE",{});var a7=a(ea);m2=o(a7,"</w>"),a7.forEach(t),d2=o(ee,"), nous pouvons le d\xE9finir avec "),sa=l(ee,"CODE",{});var i7=a(sa);f2=o(i7,"end_of_word_suffix"),i7.forEach(t),k2=o(ee,"."),ee.forEach(t),Wp=c(e),qs=l(e,"P",{});var pc=a(qs);v2=o(pc,"Ce "),ta=l(pc,"EM",{});var p7=a(ta);_2=o(p7,"tokenizer"),p7.forEach(t),h2=o(pc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),pc.forEach(t),Ip=c(e),d(Vt.$$.fragment,e),Gp=c(e),Un=l(e,"P",{});var u7=a(Un);z2=o(u7,"Regardons la tokenisation d\u2019un exemple de texte :"),u7.forEach(t),Vp=c(e),d(Xt.$$.fragment,e),Xp=c(e),d(Kt.$$.fragment,e),Kp=c(e),js=l(e,"P",{});var uc=a(js);$2=o(uc,"Nous appliquons le post-traitement au niveau de l\u2019octet pour le "),na=l(uc,"EM",{});var c7=a(na);E2=o(c7,"tokenizer"),c7.forEach(t),q2=o(uc," du GPT-2 comme suit :"),uc.forEach(t),Hp=c(e),d(Ht.$$.fragment,e),Yp=c(e),X=l(e,"P",{});var Ee=a(X);j2=o(Ee,"L\u2019option "),oa=l(Ee,"CODE",{});var m7=a(oa);x2=o(m7,"trim_offsets = False"),m7.forEach(t),b2=o(Ee," indique au post-processeur que nous devons laisser les "),ra=l(Ee,"EM",{});var d7=a(ra);g2=o(d7,"offsets"),d7.forEach(t),P2=o(Ee," des "),la=l(Ee,"EM",{});var f7=a(la);w2=o(f7,"tokens"),f7.forEach(t),T2=o(Ee," qui commencent par \u2018\u0120\u2019 tels quels : de cette fa\xE7on, le d\xE9but des "),aa=l(Ee,"EM",{});var k7=a(aa);C2=o(k7,"offsets"),k7.forEach(t),D2=o(Ee," pointera sur l\u2019espace avant le mot, et non sur le premier caract\xE8re du mot (puisque l\u2019espace fait techniquement partie du token). Regardons le r\xE9sultat avec le texte que nous venons de coder, o\xF9 "),ia=l(Ee,"CODE",{});var v7=a(ia);L2=o(v7,"'\u0120test'"),v7.forEach(t),O2=o(Ee," est le token \xE0 l\u2019index 4 :"),Ee.forEach(t),Jp=c(e),d(Yt.$$.fragment,e),Zp=c(e),d(Jt.$$.fragment,e),Qp=c(e),Rn=l(e,"P",{});var _7=a(Rn);y2=o(_7,"Enfin, nous ajoutons un d\xE9codeur de niveau octet :"),_7.forEach(t),eu=c(e),d(Zt.$$.fragment,e),su=c(e),Wn=l(e,"P",{});var h7=a(Wn);M2=o(h7,"et nous pourrons v\xE9rifier qu\u2019il fonctionne correctement :"),h7.forEach(t),tu=c(e),d(Qt.$$.fragment,e),nu=c(e),d(en.$$.fragment,e),ou=c(e),ke=l(e,"P",{});var Ns=a(ke);S2=o(Ns,"Super ! Maintenant que nous avons termin\xE9, nous pouvons sauvegarder le tokenizer comme avant, et l\u2019envelopper dans un "),pa=l(Ns,"CODE",{});var z7=a(pa);N2=o(z7,"PreTrainedTokenizerFast"),z7.forEach(t),B2=o(Ns," ou un "),ua=l(Ns,"CODE",{});var $7=a(ua);A2=o($7,"GPT2TokenizerFast"),$7.forEach(t),F2=o(Ns," si nous voulons l\u2019utiliser dans \u{1F917} "),ca=l(Ns,"EM",{});var E7=a(ca);U2=o(E7,"Transformers"),E7.forEach(t),R2=o(Ns," :"),Ns.forEach(t),ru=c(e),d(sn.$$.fragment,e),lu=c(e),In=l(e,"P",{});var q7=a(In);W2=o(q7,"ou :"),q7.forEach(t),au=c(e),d(tn.$$.fragment,e),iu=c(e),We=l(e,"P",{});var ao=a(We);I2=o(ao,"Comme dernier exemple, nous allons vous montrer comment construire un "),ma=l(ao,"EM",{});var j7=a(ma);G2=o(j7,"tokenizer"),j7.forEach(t),V2=c(ao),da=l(ao,"EM",{});var x7=a(da);X2=o(x7,"Unigram"),x7.forEach(t),K2=o(ao," \xE0 partir de z\xE9ro."),ao.forEach(t),pu=c(e),Qe=l(e,"H2",{class:!0});var cc=a(Qe);xs=l(cc,"A",{id:!0,class:!0,href:!0});var b7=a(xs);fa=l(b7,"SPAN",{});var g7=a(fa);d(nn.$$.fragment,g7),g7.forEach(t),b7.forEach(t),H2=c(cc),ka=l(cc,"SPAN",{});var P7=a(ka);Y2=o(P7,"Construire un *tokenizer* *Unigram* \xE0 partir de rien."),P7.forEach(t),cc.forEach(t),uu=c(e),K=l(e,"P",{});var Ie=a(K);J2=o(Ie,"Construisons maintenant un "),va=l(Ie,"EM",{});var w7=a(va);Z2=o(w7,"tokenizer"),w7.forEach(t),Q2=o(Ie," XLNet. Comme pour les "),_a=l(Ie,"EM",{});var T7=a(_a);eh=o(T7,"tokenizers"),T7.forEach(t),sh=o(Ie," pr\xE9c\xE9dents, nous commen\xE7ons par initialiser un "),ha=l(Ie,"CODE",{});var C7=a(ha);th=o(C7,"Tokenizer"),C7.forEach(t),nh=o(Ie," avec un mod\xE8le "),za=l(Ie,"EM",{});var D7=a(za);oh=o(D7,"Unigram"),D7.forEach(t),rh=o(Ie," :"),Ie.forEach(t),cu=c(e),d(on.$$.fragment,e),mu=c(e),Gn=l(e,"P",{});var L7=a(Gn);lh=o(L7,"Encore une fois, nous pourrions initialiser ce mod\xE8le avec un vocabulaire si nous en avions un."),L7.forEach(t),du=c(e),bs=l(e,"P",{});var mc=a(bs);ah=o(mc,"Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de "),$a=l(mc,"EM",{});var O7=a($a);ih=o(O7,"SentencePiece"),O7.forEach(t),ph=o(mc,") :"),mc.forEach(t),fu=c(e),d(rn.$$.fragment,e),ku=c(e),ve=l(e,"P",{});var Bs=a(ve);uh=o(Bs,"Cela remplace "),Ea=l(Bs,"CODE",{});var y7=a(Ea);ch=o(y7,"\u201C"),y7.forEach(t),mh=o(Bs," et "),qa=l(Bs,"CODE",{});var M7=a(qa);dh=o(M7,"\u201D"),M7.forEach(t),fh=o(Bs," avec "),ja=l(Bs,"CODE",{});var S7=a(ja);kh=o(S7,"\u201D"),S7.forEach(t),vh=o(Bs," et toute s\xE9quence de deux espaces ou plus par un seul espace, ainsi que la suppression des accents dans les textes \xE0 cat\xE9goriser."),Bs.forEach(t),vu=c(e),_e=l(e,"P",{});var As=a(_e);_h=o(As,"Le pr\xE9-"),xa=l(As,"EM",{});var N7=a(xa);hh=o(N7,"tokenizer"),N7.forEach(t),zh=o(As," \xE0 utiliser pour tout "),ba=l(As,"EM",{});var B7=a(ba);$h=o(B7,"tokenizer SentencePiece"),B7.forEach(t),Eh=o(As," est "),ga=l(As,"CODE",{});var A7=a(ga);qh=o(A7,"Metaspace"),A7.forEach(t),jh=o(As," :"),As.forEach(t),_u=c(e),d(ln.$$.fragment,e),hu=c(e),Vn=l(e,"P",{});var F7=a(Vn);xh=o(F7,"Nous pouvons jeter un coup d\u2019oeil \xE0 la pr\xE9-tok\xE9nisation d\u2019un exemple de texte comme pr\xE9c\xE9demment :"),F7.forEach(t),zu=c(e),d(an.$$.fragment,e),$u=c(e),d(pn.$$.fragment,e),Eu=c(e),gs=l(e,"P",{});var dc=a(gs);bh=o(dc,"Vient ensuite le mod\xE8le, qui doit \xEAtre entra\xEEn\xE9. XLNet poss\xE8de un certain nombre de "),Pa=l(dc,"EM",{});var U7=a(Pa);gh=o(U7,"tokens"),U7.forEach(t),Ph=o(dc," sp\xE9ciaux :"),dc.forEach(t),qu=c(e),d(un.$$.fragment,e),ju=c(e),S=l(e,"P",{});var se=a(S);wh=o(se,"Un argument tr\xE8s important \xE0 ne pas oublier pour le "),wa=l(se,"CODE",{});var R7=a(wa);Th=o(R7,"UnigramTrainer"),R7.forEach(t),Ch=o(se," est le "),Ta=l(se,"CODE",{});var W7=a(Ta);Dh=o(W7,"unk_token"),W7.forEach(t),Lh=o(se,". Nous pouvons aussi passer d\u2019autres arguments sp\xE9cifiques \xE0 l\u2019algorithme "),Ca=l(se,"EM",{});var I7=a(Ca);Oh=o(I7,"Unigram"),I7.forEach(t),yh=o(se,", comme le "),Da=l(se,"CODE",{});var G7=a(Da);Mh=o(G7,"shrinking_factor"),G7.forEach(t),Sh=o(se," pour chaque \xE9tape o\xF9 nous enlevons des "),La=l(se,"EM",{});var V7=a(La);Nh=o(V7,"tokens"),V7.forEach(t),Bh=o(se," (par d\xE9faut 0.75) ou le "),Oa=l(se,"CODE",{});var X7=a(Oa);Ah=o(X7,"max_piece_length"),X7.forEach(t),Fh=o(se," pour sp\xE9cifier la longueur maximale d\u2019un token donn\xE9 (par d\xE9faut 16)."),se.forEach(t),xu=c(e),Ps=l(e,"P",{});var fc=a(Ps);Uh=o(fc,"Ce "),ya=l(fc,"EM",{});var K7=a(ya);Rh=o(K7,"tokenizer"),K7.forEach(t),Wh=o(fc," peut aussi \xEAtre entra\xEEn\xE9 sur des fichiers texte :"),fc.forEach(t),bu=c(e),d(cn.$$.fragment,e),gu=c(e),Xn=l(e,"P",{});var H7=a(Xn);Ih=o(H7,"Regardons la tokenisation d\u2019un exemple de texte :"),H7.forEach(t),Pu=c(e),d(mn.$$.fragment,e),wu=c(e),d(dn.$$.fragment,e),Tu=c(e),w=l(e,"P",{});var F=a(w);Gh=o(F,"Une particularit\xE9 de XLNet est qu\u2019il place le "),Ma=l(F,"EM",{});var Y7=a(Ma);Vh=o(Y7,"token"),Y7.forEach(t),Xh=c(F),Sa=l(F,"CODE",{});var J7=a(Sa);Kh=o(J7,"<cls>"),J7.forEach(t),Hh=o(F," \xE0 la fin de la phrase, avec un type ID de 2 (pour le distinguer des autres "),Na=l(F,"EM",{});var Z7=a(Na);Yh=o(Z7,"tokens"),Z7.forEach(t),Jh=o(F,"). Le r\xE9sultat est un remplissage \xE0 gauche. Nous pouvons traiter tous les "),Ba=l(F,"EM",{});var Q7=a(Ba);Zh=o(Q7,"tokens"),Q7.forEach(t),Qh=o(F," sp\xE9ciaux et les IDs de type de "),Aa=l(F,"EM",{});var eq=a(Aa);ez=o(eq,"token"),eq.forEach(t),sz=o(F," avec un mod\xE8le, comme pour BERT, mais d\u2019abord nous devons obtenir les IDs des "),Fa=l(F,"EM",{});var sq=a(Fa);tz=o(sq,"tokens"),sq.forEach(t),nz=c(F),Ua=l(F,"CODE",{});var tq=a(Ua);oz=o(tq,"<cls>"),tq.forEach(t),rz=o(F," et "),Ra=l(F,"CODE",{});var nq=a(Ra);lz=o(nq,"<sep>"),nq.forEach(t),az=o(F," :"),F.forEach(t),Cu=c(e),d(fn.$$.fragment,e),Du=c(e),d(kn.$$.fragment,e),Lu=c(e),Kn=l(e,"P",{});var oq=a(Kn);iz=o(oq,"Le mod\xE8le ressemble \xE0 ceci :"),oq.forEach(t),Ou=c(e),d(vn.$$.fragment,e),yu=c(e),Hn=l(e,"P",{});var rq=a(Hn);pz=o(rq,"Et nous pouvons tester son fonctionnement en codant une paire de phrases :"),rq.forEach(t),Mu=c(e),d(_n.$$.fragment,e),Su=c(e),d(hn.$$.fragment,e),Nu=c(e),ws=l(e,"P",{});var kc=a(ws);uz=o(kc,"Enfin, nous ajoutons un d\xE9codeur "),Wa=l(kc,"CODE",{});var lq=a(Wa);cz=o(lq,"Metaspace"),lq.forEach(t),mz=o(kc," :"),kc.forEach(t),Bu=c(e),d(zn.$$.fragment,e),Au=c(e),T=l(e,"P",{});var U=a(T);dz=o(U,"et on en a fini avec ce "),Ia=l(U,"EM",{});var aq=a(Ia);fz=o(aq,"tokenizer"),aq.forEach(t),kz=o(U," ! On peut sauvegarder le "),Ga=l(U,"EM",{});var iq=a(Ga);vz=o(iq,"tokenizer"),iq.forEach(t),_z=o(U," comme avant, et l\u2019envelopper dans un "),Va=l(U,"CODE",{});var pq=a(Va);hz=o(pq,"PreTrainedTokenizerFast"),pq.forEach(t),zz=o(U," ou "),Xa=l(U,"CODE",{});var uq=a(Xa);$z=o(uq,"XLNetTokenizerFast"),uq.forEach(t),Ez=o(U," si on veut l\u2019utiliser dans \u{1F917} "),Ka=l(U,"EM",{});var cq=a(Ka);qz=o(cq,"Transformers"),cq.forEach(t),jz=o(U,". Une chose \xE0 noter lors de l\u2019utilisation de "),Ha=l(U,"CODE",{});var mq=a(Ha);xz=o(mq,"PreTrainedTokenizerFast"),mq.forEach(t),bz=o(U," est qu\u2019en plus des "),Ya=l(U,"EM",{});var dq=a(Ya);gz=o(dq,"tokens"),dq.forEach(t),Pz=o(U," sp\xE9ciaux, nous devons dire \xE0 la biblioth\xE8que \u{1F917} "),Ja=l(U,"EM",{});var fq=a(Ja);wz=o(fq,"Transformers"),fq.forEach(t),Tz=o(U," de remplir \xE0 gauche :"),U.forEach(t),Fu=c(e),d($n.$$.fragment,e),Uu=c(e),Yn=l(e,"P",{});var kq=a(Yn);Cz=o(kq,"Ou alternativement :"),kq.forEach(t),Ru=c(e),d(En.$$.fragment,e),Wu=c(e),H=l(e,"P",{});var Ge=a(H);Dz=o(Ge,"Maintenant que vous avez vu comment les diff\xE9rentes briques sont utilis\xE9es pour construire des "),Za=l(Ge,"EM",{});var vq=a(Za);Lz=o(vq,"tokenizers"),vq.forEach(t),Oz=o(Ge," existants, vous devriez \xEAtre capable d\u2019\xE9crire n\u2019importe quel "),Qa=l(Ge,"EM",{});var _q=a(Qa);yz=o(_q,"tokenizer"),_q.forEach(t),Mz=o(Ge," que vous voulez avec la biblioth\xE8que \u{1F917} "),ei=l(Ge,"EM",{});var hq=a(ei);Sz=o(hq,"Tokenizers"),hq.forEach(t),Nz=o(Ge," et pouvoir l\u2019utiliser dans \u{1F917} "),si=l(Ge,"EM",{});var zq=a(si);Bz=o(zq,"Transformers"),zq.forEach(t),Az=o(Ge,"."),Ge.forEach(t),this.h()},h(){z($,"name","hf:doc:metadata"),z($,"content",JSON.stringify(Dq)),z(le,"id","construction-dun-tokenizer-bloc-par-bloc"),z(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(le,"href","#construction-dun-tokenizer-bloc-par-bloc"),z(te,"class","relative group"),z(Xs,"class","block dark:hidden"),$q(Xs.src,Fz="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||z(Xs,"src",Fz),z(Xs,"alt","The tokenization pipeline."),z(Ks,"class","hidden dark:block"),$q(Ks.src,Uz="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||z(Ks,"src",Uz),z(Ks,"alt","The tokenization pipeline."),z(He,"class","flex justify-center"),z(yn,"href","/course/fr/chapter6/2"),z(Ys,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"),z(Ys,"rel","nofollow"),z(Js,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers"),z(Js,"rel","nofollow"),z(Zs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models"),z(Zs,"rel","nofollow"),z(Qs,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers"),z(Qs,"rel","nofollow"),z(et,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors"),z(et,"rel","nofollow"),z(st,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders"),z(st,"rel","nofollow"),z(tt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html"),z(tt,"rel","nofollow"),z(as,"id","acquisition-dun-corpus"),z(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(as,"href","#acquisition-dun-corpus"),z(Ye,"class","relative group"),z(Mn,"href","/course/fr/chapter6/2"),z(ot,"href","https://huggingface.co/datasets/wikitext"),z(ot,"rel","nofollow"),z(ps,"id","construire-un-tokenizer-wordpiece-partir-de-zro"),z(ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(ps,"href","#construire-un-tokenizer-wordpiece-partir-de-zro"),z(Je,"class","relative group"),z($s,"id","construire-un-tokenizer-bpe-partir-de-zro"),z($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z($s,"href","#construire-un-tokenizer-bpe-partir-de-zro"),z(Ze,"class","relative group"),z(xs,"id","construire-un-tokenizer-unigram-partir-de-rien"),z(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(xs,"href","#construire-un-tokenizer-unigram-partir-de-rien"),z(Qe,"class","relative group")},m(e,i){s(document.head,$),p(e,Pe,i),p(e,te,i),s(te,le),s(le,qe),f(je,qe,null),s(te,Fs),s(te,xe),s(xe,Us),p(e,es,i),f(ne,e,i),p(e,ss,i),p(e,we,i),s(we,Ve),p(e,ts,i),p(e,R,i),s(R,be),s(be,Rs),s(R,Ws),s(R,ge),s(ge,Is),s(R,Gs),s(R,oe),s(oe,Vs),s(oe,Xe),s(Xe,Ke),s(oe,x),s(R,wn),s(R,ae),s(ae,Tn),s(ae,ns),s(ns,Cn),s(ae,Dn),s(ae,os),s(os,Ln),s(ae,vc),p(e,oi,i),p(e,On,i),s(On,_c),p(e,ri,i),p(e,He,i),s(He,Xs),s(He,hc),s(He,Ks),p(e,li,i),p(e,W,i),s(W,zc),s(W,io),s(io,$c),s(W,Ec),s(W,po),s(po,qc),s(W,jc),s(W,uo),s(uo,xc),s(W,bc),s(W,yn),s(yn,gc),s(W,Pc),s(W,co),s(co,wc),s(W,Tc),p(e,ai,i),f(Hs,e,i),p(e,ii,i),p(e,rs,i),s(rs,Cc),s(rs,mo),s(mo,Dc),s(rs,Lc),p(e,pi,i),p(e,I,i),s(I,Te),s(Te,fo),s(fo,Oc),s(Te,yc),s(Te,ko),s(ko,Mc),s(Te,Sc),s(Te,Ys),s(Ys,Nc),s(Te,Bc),s(I,Ac),s(I,Ce),s(Ce,vo),s(vo,Fc),s(Ce,Uc),s(Ce,_o),s(_o,Rc),s(Ce,Wc),s(Ce,Js),s(Js,Ic),s(Ce,Gc),s(I,Vc),s(I,G),s(G,ho),s(ho,Xc),s(G,Kc),s(G,zo),s(zo,Hc),s(G,Yc),s(G,$o),s($o,Jc),s(G,Zc),s(G,Eo),s(Eo,Qc),s(G,em),s(G,qo),s(qo,sm),s(G,tm),s(G,Zs),s(Zs,nm),s(G,om),s(I,rm),s(I,De),s(De,jo),s(jo,lm),s(De,am),s(De,xo),s(xo,im),s(De,pm),s(De,Qs),s(Qs,um),s(De,cm),s(I,mm),s(I,Le),s(Le,bo),s(bo,dm),s(Le,fm),s(Le,go),s(go,km),s(Le,vm),s(Le,et),s(et,_m),s(Le,hm),s(I,zm),s(I,Oe),s(Oe,Po),s(Po,$m),s(Oe,Em),s(Oe,wo),s(wo,qm),s(Oe,jm),s(Oe,st),s(st,xm),s(Oe,bm),p(e,ui,i),p(e,ls,i),s(ls,gm),s(ls,tt),s(tt,Pm),s(ls,wm),p(e,ci,i),p(e,Ye,i),s(Ye,as),s(as,To),f(nt,To,null),s(Ye,Tm),s(Ye,Co),s(Co,Cm),p(e,mi,i),p(e,ie,i),s(ie,Dm),s(ie,Do),s(Do,Lm),s(ie,Om),s(ie,Mn),s(Mn,ym),s(ie,Mm),s(ie,ot),s(ot,Sm),s(ie,Nm),p(e,di,i),f(rt,e,i),p(e,fi,i),p(e,ye,i),s(ye,Bm),s(ye,Lo),s(Lo,Am),s(ye,Fm),s(ye,Oo),s(Oo,Um),s(ye,Rm),p(e,ki,i),p(e,is,i),s(is,Wm),s(is,yo),s(yo,Im),s(is,Gm),p(e,vi,i),f(lt,e,i),p(e,_i,i),p(e,pe,i),s(pe,Vm),s(pe,Mo),s(Mo,Xm),s(pe,Km),s(pe,So),s(So,Hm),s(pe,Ym),s(pe,No),s(No,Jm),s(pe,Zm),p(e,hi,i),p(e,Je,i),s(Je,ps),s(ps,Bo),f(at,Bo,null),s(Je,Qm),s(Je,Ao),s(Ao,ed),p(e,zi,i),p(e,b,i),s(b,sd),s(b,Fo),s(Fo,td),s(b,nd),s(b,Uo),s(Uo,od),s(b,rd),s(b,Ro),s(Ro,ld),s(b,ad),s(b,Wo),s(Wo,id),s(b,pd),s(b,Io),s(Io,ud),s(b,cd),s(b,Go),s(Go,md),s(b,dd),s(b,Vo),s(Vo,fd),s(b,kd),s(b,Xo),s(Xo,vd),s(b,_d),p(e,$i,i),p(e,Me,i),s(Me,hd),s(Me,Ko),s(Ko,zd),s(Me,$d),s(Me,Ho),s(Ho,Ed),s(Me,qd),p(e,Ei,i),f(it,e,i),p(e,qi,i),p(e,ue,i),s(ue,jd),s(ue,Yo),s(Yo,xd),s(ue,bd),s(ue,Jo),s(Jo,gd),s(ue,Pd),s(ue,Zo),s(Zo,wd),s(ue,Td),p(e,ji,i),p(e,g,i),s(g,Cd),s(g,Qo),s(Qo,Dd),s(g,Ld),s(g,er),s(er,Od),s(g,yd),s(g,sr),s(sr,Md),s(g,Sd),s(g,tr),s(tr,Nd),s(g,Bd),s(g,nr),s(nr,Ad),s(g,Fd),s(g,or),s(or,Ud),s(g,Rd),s(g,rr),s(rr,Wd),s(g,Id),s(g,lr),s(lr,Gd),s(g,Vd),p(e,xi,i),f(pt,e,i),p(e,bi,i),p(e,V,i),s(V,Xd),s(V,ar),s(ar,Kd),s(V,Hd),s(V,ir),s(ir,Yd),s(V,Jd),s(V,pr),s(pr,Zd),s(V,Qd),s(V,ur),s(ur,ef),s(V,sf),s(V,cr),s(cr,tf),s(V,nf),p(e,gi,i),f(ut,e,i),p(e,Pi,i),p(e,Se,i),s(Se,of),s(Se,mr),s(mr,rf),s(Se,lf),s(Se,dr),s(dr,af),s(Se,pf),p(e,wi,i),p(e,Ne,i),s(Ne,uf),s(Ne,fr),s(fr,cf),s(Ne,mf),s(Ne,kr),s(kr,df),s(Ne,ff),p(e,Ti,i),f(ct,e,i),p(e,Ci,i),f(mt,e,i),p(e,Di,i),f(us,e,i),p(e,Li,i),p(e,cs,i),s(cs,kf),s(cs,vr),s(vr,vf),s(cs,_f),p(e,Oi,i),f(dt,e,i),p(e,yi,i),p(e,Sn,i),s(Sn,hf),p(e,Mi,i),f(ft,e,i),p(e,Si,i),p(e,ms,i),s(ms,zf),s(ms,_r),s(_r,$f),s(ms,Ef),p(e,Ni,i),f(kt,e,i),p(e,Bi,i),f(vt,e,i),p(e,Ai,i),p(e,ds,i),s(ds,qf),s(ds,hr),s(hr,jf),s(ds,xf),p(e,Fi,i),f(_t,e,i),p(e,Ui,i),f(ht,e,i),p(e,Ri,i),p(e,fs,i),s(fs,bf),s(fs,zr),s(zr,gf),s(fs,Pf),p(e,Wi,i),f(zt,e,i),p(e,Ii,i),f($t,e,i),p(e,Gi,i),p(e,ce,i),s(ce,wf),s(ce,$r),s($r,Tf),s(ce,Cf),s(ce,Er),s(Er,Df),s(ce,Lf),s(ce,qr),s(qr,Of),s(ce,yf),p(e,Vi,i),f(Et,e,i),p(e,Xi,i),p(e,D,i),s(D,Mf),s(D,jr),s(jr,Sf),s(D,Nf),s(D,xr),s(xr,Bf),s(D,Af),s(D,br),s(br,Ff),s(D,Uf),s(D,gr),s(gr,Rf),s(D,Wf),s(D,Pr),s(Pr,If),s(D,Gf),s(D,wr),s(wr,Vf),s(D,Xf),p(e,Ki,i),p(e,Nn,i),s(Nn,Kf),p(e,Hi,i),f(qt,e,i),p(e,Yi,i),p(e,Be,i),s(Be,Hf),s(Be,Tr),s(Tr,Yf),s(Be,Jf),s(Be,Cr),s(Cr,Zf),s(Be,Qf),p(e,Ji,i),f(jt,e,i),p(e,Zi,i),p(e,Ae,i),s(Ae,ek),s(Ae,Dr),s(Dr,sk),s(Ae,tk),s(Ae,Lr),s(Lr,nk),s(Ae,ok),p(e,Qi,i),f(xt,e,i),p(e,ep,i),f(bt,e,i),p(e,sp,i),p(e,q,i),s(q,rk),s(q,Or),s(Or,lk),s(q,ak),s(q,yr),s(yr,ik),s(q,pk),s(q,Mr),s(Mr,uk),s(q,ck),s(q,Sr),s(Sr,mk),s(q,dk),s(q,Nr),s(Nr,fk),s(q,kk),s(q,Br),s(Br,vk),s(q,_k),s(q,Ar),s(Ar,hk),s(q,zk),s(q,Fr),s(Fr,$k),s(q,Ek),s(q,Ur),s(Ur,qk),s(q,jk),s(q,Rr),s(Rr,xk),s(q,bk),p(e,tp,i),p(e,P,i),s(P,gk),s(P,Wr),s(Wr,Pk),s(P,wk),s(P,Ir),s(Ir,Tk),s(P,Ck),s(P,Gr),s(Gr,Dk),s(P,Lk),s(P,Vr),s(Vr,Ok),s(P,yk),s(P,Xr),s(Xr,Mk),s(P,Sk),s(P,Kr),s(Kr,Nk),s(P,Bk),s(P,Hr),s(Hr,Ak),s(P,Fk),s(P,Yr),s(Yr,Uk),s(P,Rk),p(e,np,i),f(gt,e,i),p(e,op,i),f(Pt,e,i),p(e,rp,i),p(e,L,i),s(L,Wk),s(L,Jr),s(Jr,Ik),s(L,Gk),s(L,Zr),s(Zr,Vk),s(L,Xk),s(L,Qr),s(Qr,Kk),s(L,Hk),s(L,el),s(el,Yk),s(L,Jk),s(L,sl),s(sl,Zk),s(L,Qk),s(L,tl),s(tl,ev),s(L,sv),p(e,lp,i),p(e,ks,i),s(ks,tv),s(ks,nl),s(nl,nv),s(ks,ov),p(e,ap,i),f(wt,e,i),p(e,ip,i),p(e,vs,i),s(vs,rv),s(vs,ol),s(ol,lv),s(vs,av),p(e,pp,i),p(e,Bn,i),s(Bn,iv),p(e,up,i),f(Tt,e,i),p(e,cp,i),f(Ct,e,i),p(e,mp,i),p(e,An,i),s(An,pv),p(e,dp,i),f(Dt,e,i),p(e,fp,i),f(Lt,e,i),p(e,kp,i),p(e,_s,i),s(_s,uv),s(_s,rl),s(rl,cv),s(_s,mv),p(e,vp,i),f(Ot,e,i),p(e,_p,i),p(e,hs,i),s(hs,dv),s(hs,ll),s(ll,fv),s(hs,kv),p(e,hp,i),f(yt,e,i),p(e,zp,i),f(Mt,e,i),p(e,$p,i),p(e,zs,i),s(zs,vv),s(zs,al),s(al,_v),s(zs,hv),p(e,Ep,i),f(St,e,i),p(e,qp,i),p(e,Fe,i),s(Fe,zv),s(Fe,il),s(il,$v),s(Fe,Ev),s(Fe,pl),s(pl,qv),s(Fe,jv),p(e,jp,i),f(Nt,e,i),p(e,xp,i),p(e,O,i),s(O,xv),s(O,ul),s(ul,bv),s(O,gv),s(O,cl),s(cl,Pv),s(O,wv),s(O,ml),s(ml,Tv),s(O,Cv),s(O,dl),s(dl,Dv),s(O,Lv),s(O,fl),s(fl,Ov),s(O,yv),s(O,kl),s(kl,Mv),s(O,Sv),p(e,bp,i),p(e,E,i),s(E,Nv),s(E,vl),s(vl,Bv),s(E,Av),s(E,_l),s(_l,Fv),s(E,Uv),s(E,hl),s(hl,Rv),s(E,Wv),s(E,zl),s(zl,Iv),s(E,Gv),s(E,$l),s($l,Vv),s(E,Xv),s(E,El),s(El,Kv),s(E,Hv),s(E,ql),s(ql,Yv),s(E,Jv),s(E,jl),s(jl,Zv),s(E,Qv),s(E,xl),s(xl,e_),s(E,s_),s(E,bl),s(bl,t_),s(E,n_),s(E,gl),s(gl,o_),s(E,Pl),s(Pl,r_),s(E,l_),p(e,gp,i),f(Bt,e,i),p(e,Pp,i),p(e,me,i),s(me,a_),s(me,wl),s(wl,i_),s(me,p_),s(me,Tl),s(Tl,u_),s(me,c_),s(me,Cl),s(Cl,m_),s(me,d_),p(e,wp,i),f(At,e,i),p(e,Tp,i),p(e,y,i),s(y,f_),s(y,Dl),s(Dl,k_),s(y,v_),s(y,Ll),s(Ll,__),s(y,h_),s(y,Ol),s(Ol,z_),s(y,$_),s(y,yl),s(yl,E_),s(y,q_),s(y,Ml),s(Ml,j_),s(y,x_),s(y,Sl),s(Sl,b_),s(y,g_),p(e,Cp,i),p(e,Ue,i),s(Ue,P_),s(Ue,Nl),s(Nl,w_),s(Ue,T_),s(Ue,Bl),s(Bl,C_),s(Ue,D_),p(e,Dp,i),p(e,Ze,i),s(Ze,$s),s($s,Al),f(Ft,Al,null),s(Ze,L_),s(Ze,Fl),s(Fl,O_),p(e,Lp,i),p(e,de,i),s(de,y_),s(de,Ul),s(Ul,M_),s(de,S_),s(de,Rl),s(Rl,N_),s(de,B_),s(de,Wl),s(Wl,A_),s(de,F_),p(e,Op,i),f(Ut,e,i),p(e,yp,i),p(e,fe,i),s(fe,U_),s(fe,Il),s(Il,R_),s(fe,W_),s(fe,Gl),s(Gl,I_),s(fe,G_),s(fe,Vl),s(Vl,V_),s(fe,X_),p(e,Mp,i),p(e,Fn,i),s(Fn,K_),p(e,Sp,i),f(Rt,e,i),p(e,Np,i),p(e,Es,i),s(Es,H_),s(Es,Xl),s(Xl,Y_),s(Es,J_),p(e,Bp,i),f(Wt,e,i),p(e,Ap,i),f(It,e,i),p(e,Fp,i),p(e,Re,i),s(Re,Z_),s(Re,Kl),s(Kl,Q_),s(Re,e2),s(Re,Hl),s(Hl,s2),s(Re,t2),p(e,Up,i),f(Gt,e,i),p(e,Rp,i),p(e,M,i),s(M,n2),s(M,Yl),s(Yl,o2),s(M,r2),s(M,Jl),s(Jl,l2),s(M,a2),s(M,Zl),s(Zl,i2),s(M,p2),s(M,Ql),s(Ql,u2),s(M,c2),s(M,ea),s(ea,m2),s(M,d2),s(M,sa),s(sa,f2),s(M,k2),p(e,Wp,i),p(e,qs,i),s(qs,v2),s(qs,ta),s(ta,_2),s(qs,h2),p(e,Ip,i),f(Vt,e,i),p(e,Gp,i),p(e,Un,i),s(Un,z2),p(e,Vp,i),f(Xt,e,i),p(e,Xp,i),f(Kt,e,i),p(e,Kp,i),p(e,js,i),s(js,$2),s(js,na),s(na,E2),s(js,q2),p(e,Hp,i),f(Ht,e,i),p(e,Yp,i),p(e,X,i),s(X,j2),s(X,oa),s(oa,x2),s(X,b2),s(X,ra),s(ra,g2),s(X,P2),s(X,la),s(la,w2),s(X,T2),s(X,aa),s(aa,C2),s(X,D2),s(X,ia),s(ia,L2),s(X,O2),p(e,Jp,i),f(Yt,e,i),p(e,Zp,i),f(Jt,e,i),p(e,Qp,i),p(e,Rn,i),s(Rn,y2),p(e,eu,i),f(Zt,e,i),p(e,su,i),p(e,Wn,i),s(Wn,M2),p(e,tu,i),f(Qt,e,i),p(e,nu,i),f(en,e,i),p(e,ou,i),p(e,ke,i),s(ke,S2),s(ke,pa),s(pa,N2),s(ke,B2),s(ke,ua),s(ua,A2),s(ke,F2),s(ke,ca),s(ca,U2),s(ke,R2),p(e,ru,i),f(sn,e,i),p(e,lu,i),p(e,In,i),s(In,W2),p(e,au,i),f(tn,e,i),p(e,iu,i),p(e,We,i),s(We,I2),s(We,ma),s(ma,G2),s(We,V2),s(We,da),s(da,X2),s(We,K2),p(e,pu,i),p(e,Qe,i),s(Qe,xs),s(xs,fa),f(nn,fa,null),s(Qe,H2),s(Qe,ka),s(ka,Y2),p(e,uu,i),p(e,K,i),s(K,J2),s(K,va),s(va,Z2),s(K,Q2),s(K,_a),s(_a,eh),s(K,sh),s(K,ha),s(ha,th),s(K,nh),s(K,za),s(za,oh),s(K,rh),p(e,cu,i),f(on,e,i),p(e,mu,i),p(e,Gn,i),s(Gn,lh),p(e,du,i),p(e,bs,i),s(bs,ah),s(bs,$a),s($a,ih),s(bs,ph),p(e,fu,i),f(rn,e,i),p(e,ku,i),p(e,ve,i),s(ve,uh),s(ve,Ea),s(Ea,ch),s(ve,mh),s(ve,qa),s(qa,dh),s(ve,fh),s(ve,ja),s(ja,kh),s(ve,vh),p(e,vu,i),p(e,_e,i),s(_e,_h),s(_e,xa),s(xa,hh),s(_e,zh),s(_e,ba),s(ba,$h),s(_e,Eh),s(_e,ga),s(ga,qh),s(_e,jh),p(e,_u,i),f(ln,e,i),p(e,hu,i),p(e,Vn,i),s(Vn,xh),p(e,zu,i),f(an,e,i),p(e,$u,i),f(pn,e,i),p(e,Eu,i),p(e,gs,i),s(gs,bh),s(gs,Pa),s(Pa,gh),s(gs,Ph),p(e,qu,i),f(un,e,i),p(e,ju,i),p(e,S,i),s(S,wh),s(S,wa),s(wa,Th),s(S,Ch),s(S,Ta),s(Ta,Dh),s(S,Lh),s(S,Ca),s(Ca,Oh),s(S,yh),s(S,Da),s(Da,Mh),s(S,Sh),s(S,La),s(La,Nh),s(S,Bh),s(S,Oa),s(Oa,Ah),s(S,Fh),p(e,xu,i),p(e,Ps,i),s(Ps,Uh),s(Ps,ya),s(ya,Rh),s(Ps,Wh),p(e,bu,i),f(cn,e,i),p(e,gu,i),p(e,Xn,i),s(Xn,Ih),p(e,Pu,i),f(mn,e,i),p(e,wu,i),f(dn,e,i),p(e,Tu,i),p(e,w,i),s(w,Gh),s(w,Ma),s(Ma,Vh),s(w,Xh),s(w,Sa),s(Sa,Kh),s(w,Hh),s(w,Na),s(Na,Yh),s(w,Jh),s(w,Ba),s(Ba,Zh),s(w,Qh),s(w,Aa),s(Aa,ez),s(w,sz),s(w,Fa),s(Fa,tz),s(w,nz),s(w,Ua),s(Ua,oz),s(w,rz),s(w,Ra),s(Ra,lz),s(w,az),p(e,Cu,i),f(fn,e,i),p(e,Du,i),f(kn,e,i),p(e,Lu,i),p(e,Kn,i),s(Kn,iz),p(e,Ou,i),f(vn,e,i),p(e,yu,i),p(e,Hn,i),s(Hn,pz),p(e,Mu,i),f(_n,e,i),p(e,Su,i),f(hn,e,i),p(e,Nu,i),p(e,ws,i),s(ws,uz),s(ws,Wa),s(Wa,cz),s(ws,mz),p(e,Bu,i),f(zn,e,i),p(e,Au,i),p(e,T,i),s(T,dz),s(T,Ia),s(Ia,fz),s(T,kz),s(T,Ga),s(Ga,vz),s(T,_z),s(T,Va),s(Va,hz),s(T,zz),s(T,Xa),s(Xa,$z),s(T,Ez),s(T,Ka),s(Ka,qz),s(T,jz),s(T,Ha),s(Ha,xz),s(T,bz),s(T,Ya),s(Ya,gz),s(T,Pz),s(T,Ja),s(Ja,wz),s(T,Tz),p(e,Fu,i),f($n,e,i),p(e,Uu,i),p(e,Yn,i),s(Yn,Cz),p(e,Ru,i),f(En,e,i),p(e,Wu,i),p(e,H,i),s(H,Dz),s(H,Za),s(Za,Lz),s(H,Oz),s(H,Qa),s(Qa,yz),s(H,Mz),s(H,ei),s(ei,Sz),s(H,Nz),s(H,si),s(si,Bz),s(H,Az),Iu=!0},p(e,[i]){const qn={};i&2&&(qn.$$scope={dirty:i,ctx:e}),us.$set(qn)},i(e){Iu||(k(je.$$.fragment,e),k(ne.$$.fragment,e),k(Hs.$$.fragment,e),k(nt.$$.fragment,e),k(rt.$$.fragment,e),k(lt.$$.fragment,e),k(at.$$.fragment,e),k(it.$$.fragment,e),k(pt.$$.fragment,e),k(ut.$$.fragment,e),k(ct.$$.fragment,e),k(mt.$$.fragment,e),k(us.$$.fragment,e),k(dt.$$.fragment,e),k(ft.$$.fragment,e),k(kt.$$.fragment,e),k(vt.$$.fragment,e),k(_t.$$.fragment,e),k(ht.$$.fragment,e),k(zt.$$.fragment,e),k($t.$$.fragment,e),k(Et.$$.fragment,e),k(qt.$$.fragment,e),k(jt.$$.fragment,e),k(xt.$$.fragment,e),k(bt.$$.fragment,e),k(gt.$$.fragment,e),k(Pt.$$.fragment,e),k(wt.$$.fragment,e),k(Tt.$$.fragment,e),k(Ct.$$.fragment,e),k(Dt.$$.fragment,e),k(Lt.$$.fragment,e),k(Ot.$$.fragment,e),k(yt.$$.fragment,e),k(Mt.$$.fragment,e),k(St.$$.fragment,e),k(Nt.$$.fragment,e),k(Bt.$$.fragment,e),k(At.$$.fragment,e),k(Ft.$$.fragment,e),k(Ut.$$.fragment,e),k(Rt.$$.fragment,e),k(Wt.$$.fragment,e),k(It.$$.fragment,e),k(Gt.$$.fragment,e),k(Vt.$$.fragment,e),k(Xt.$$.fragment,e),k(Kt.$$.fragment,e),k(Ht.$$.fragment,e),k(Yt.$$.fragment,e),k(Jt.$$.fragment,e),k(Zt.$$.fragment,e),k(Qt.$$.fragment,e),k(en.$$.fragment,e),k(sn.$$.fragment,e),k(tn.$$.fragment,e),k(nn.$$.fragment,e),k(on.$$.fragment,e),k(rn.$$.fragment,e),k(ln.$$.fragment,e),k(an.$$.fragment,e),k(pn.$$.fragment,e),k(un.$$.fragment,e),k(cn.$$.fragment,e),k(mn.$$.fragment,e),k(dn.$$.fragment,e),k(fn.$$.fragment,e),k(kn.$$.fragment,e),k(vn.$$.fragment,e),k(_n.$$.fragment,e),k(hn.$$.fragment,e),k(zn.$$.fragment,e),k($n.$$.fragment,e),k(En.$$.fragment,e),Iu=!0)},o(e){v(je.$$.fragment,e),v(ne.$$.fragment,e),v(Hs.$$.fragment,e),v(nt.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(at.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ut.$$.fragment,e),v(ct.$$.fragment,e),v(mt.$$.fragment,e),v(us.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(kt.$$.fragment,e),v(vt.$$.fragment,e),v(_t.$$.fragment,e),v(ht.$$.fragment,e),v(zt.$$.fragment,e),v($t.$$.fragment,e),v(Et.$$.fragment,e),v(qt.$$.fragment,e),v(jt.$$.fragment,e),v(xt.$$.fragment,e),v(bt.$$.fragment,e),v(gt.$$.fragment,e),v(Pt.$$.fragment,e),v(wt.$$.fragment,e),v(Tt.$$.fragment,e),v(Ct.$$.fragment,e),v(Dt.$$.fragment,e),v(Lt.$$.fragment,e),v(Ot.$$.fragment,e),v(yt.$$.fragment,e),v(Mt.$$.fragment,e),v(St.$$.fragment,e),v(Nt.$$.fragment,e),v(Bt.$$.fragment,e),v(At.$$.fragment,e),v(Ft.$$.fragment,e),v(Ut.$$.fragment,e),v(Rt.$$.fragment,e),v(Wt.$$.fragment,e),v(It.$$.fragment,e),v(Gt.$$.fragment,e),v(Vt.$$.fragment,e),v(Xt.$$.fragment,e),v(Kt.$$.fragment,e),v(Ht.$$.fragment,e),v(Yt.$$.fragment,e),v(Jt.$$.fragment,e),v(Zt.$$.fragment,e),v(Qt.$$.fragment,e),v(en.$$.fragment,e),v(sn.$$.fragment,e),v(tn.$$.fragment,e),v(nn.$$.fragment,e),v(on.$$.fragment,e),v(rn.$$.fragment,e),v(ln.$$.fragment,e),v(an.$$.fragment,e),v(pn.$$.fragment,e),v(un.$$.fragment,e),v(cn.$$.fragment,e),v(mn.$$.fragment,e),v(dn.$$.fragment,e),v(fn.$$.fragment,e),v(kn.$$.fragment,e),v(vn.$$.fragment,e),v(_n.$$.fragment,e),v(hn.$$.fragment,e),v(zn.$$.fragment,e),v($n.$$.fragment,e),v(En.$$.fragment,e),Iu=!1},d(e){t($),e&&t(Pe),e&&t(te),_(je),e&&t(es),_(ne,e),e&&t(ss),e&&t(we),e&&t(ts),e&&t(R),e&&t(oi),e&&t(On),e&&t(ri),e&&t(He),e&&t(li),e&&t(W),e&&t(ai),_(Hs,e),e&&t(ii),e&&t(rs),e&&t(pi),e&&t(I),e&&t(ui),e&&t(ls),e&&t(ci),e&&t(Ye),_(nt),e&&t(mi),e&&t(ie),e&&t(di),_(rt,e),e&&t(fi),e&&t(ye),e&&t(ki),e&&t(is),e&&t(vi),_(lt,e),e&&t(_i),e&&t(pe),e&&t(hi),e&&t(Je),_(at),e&&t(zi),e&&t(b),e&&t($i),e&&t(Me),e&&t(Ei),_(it,e),e&&t(qi),e&&t(ue),e&&t(ji),e&&t(g),e&&t(xi),_(pt,e),e&&t(bi),e&&t(V),e&&t(gi),_(ut,e),e&&t(Pi),e&&t(Se),e&&t(wi),e&&t(Ne),e&&t(Ti),_(ct,e),e&&t(Ci),_(mt,e),e&&t(Di),_(us,e),e&&t(Li),e&&t(cs),e&&t(Oi),_(dt,e),e&&t(yi),e&&t(Sn),e&&t(Mi),_(ft,e),e&&t(Si),e&&t(ms),e&&t(Ni),_(kt,e),e&&t(Bi),_(vt,e),e&&t(Ai),e&&t(ds),e&&t(Fi),_(_t,e),e&&t(Ui),_(ht,e),e&&t(Ri),e&&t(fs),e&&t(Wi),_(zt,e),e&&t(Ii),_($t,e),e&&t(Gi),e&&t(ce),e&&t(Vi),_(Et,e),e&&t(Xi),e&&t(D),e&&t(Ki),e&&t(Nn),e&&t(Hi),_(qt,e),e&&t(Yi),e&&t(Be),e&&t(Ji),_(jt,e),e&&t(Zi),e&&t(Ae),e&&t(Qi),_(xt,e),e&&t(ep),_(bt,e),e&&t(sp),e&&t(q),e&&t(tp),e&&t(P),e&&t(np),_(gt,e),e&&t(op),_(Pt,e),e&&t(rp),e&&t(L),e&&t(lp),e&&t(ks),e&&t(ap),_(wt,e),e&&t(ip),e&&t(vs),e&&t(pp),e&&t(Bn),e&&t(up),_(Tt,e),e&&t(cp),_(Ct,e),e&&t(mp),e&&t(An),e&&t(dp),_(Dt,e),e&&t(fp),_(Lt,e),e&&t(kp),e&&t(_s),e&&t(vp),_(Ot,e),e&&t(_p),e&&t(hs),e&&t(hp),_(yt,e),e&&t(zp),_(Mt,e),e&&t($p),e&&t(zs),e&&t(Ep),_(St,e),e&&t(qp),e&&t(Fe),e&&t(jp),_(Nt,e),e&&t(xp),e&&t(O),e&&t(bp),e&&t(E),e&&t(gp),_(Bt,e),e&&t(Pp),e&&t(me),e&&t(wp),_(At,e),e&&t(Tp),e&&t(y),e&&t(Cp),e&&t(Ue),e&&t(Dp),e&&t(Ze),_(Ft),e&&t(Lp),e&&t(de),e&&t(Op),_(Ut,e),e&&t(yp),e&&t(fe),e&&t(Mp),e&&t(Fn),e&&t(Sp),_(Rt,e),e&&t(Np),e&&t(Es),e&&t(Bp),_(Wt,e),e&&t(Ap),_(It,e),e&&t(Fp),e&&t(Re),e&&t(Up),_(Gt,e),e&&t(Rp),e&&t(M),e&&t(Wp),e&&t(qs),e&&t(Ip),_(Vt,e),e&&t(Gp),e&&t(Un),e&&t(Vp),_(Xt,e),e&&t(Xp),_(Kt,e),e&&t(Kp),e&&t(js),e&&t(Hp),_(Ht,e),e&&t(Yp),e&&t(X),e&&t(Jp),_(Yt,e),e&&t(Zp),_(Jt,e),e&&t(Qp),e&&t(Rn),e&&t(eu),_(Zt,e),e&&t(su),e&&t(Wn),e&&t(tu),_(Qt,e),e&&t(nu),_(en,e),e&&t(ou),e&&t(ke),e&&t(ru),_(sn,e),e&&t(lu),e&&t(In),e&&t(au),_(tn,e),e&&t(iu),e&&t(We),e&&t(pu),e&&t(Qe),_(nn),e&&t(uu),e&&t(K),e&&t(cu),_(on,e),e&&t(mu),e&&t(Gn),e&&t(du),e&&t(bs),e&&t(fu),_(rn,e),e&&t(ku),e&&t(ve),e&&t(vu),e&&t(_e),e&&t(_u),_(ln,e),e&&t(hu),e&&t(Vn),e&&t(zu),_(an,e),e&&t($u),_(pn,e),e&&t(Eu),e&&t(gs),e&&t(qu),_(un,e),e&&t(ju),e&&t(S),e&&t(xu),e&&t(Ps),e&&t(bu),_(cn,e),e&&t(gu),e&&t(Xn),e&&t(Pu),_(mn,e),e&&t(wu),_(dn,e),e&&t(Tu),e&&t(w),e&&t(Cu),_(fn,e),e&&t(Du),_(kn,e),e&&t(Lu),e&&t(Kn),e&&t(Ou),_(vn,e),e&&t(yu),e&&t(Hn),e&&t(Mu),_(_n,e),e&&t(Su),_(hn,e),e&&t(Nu),e&&t(ws),e&&t(Bu),_(zn,e),e&&t(Au),e&&t(T),e&&t(Fu),_($n,e),e&&t(Uu),e&&t(Yn),e&&t(Ru),_(En,e),e&&t(Wu),e&&t(H)}}}const Dq={local:"construction-dun-tokenizer-bloc-par-bloc",sections:[{local:"acquisition-dun-corpus",title:"Acquisition d'un corpus"},{local:"construire-un-tokenizer-wordpiece-partir-de-zro",title:"Construire un tokenizer *WordPiece* \xE0 partir de z\xE9ro"},{local:"construire-un-tokenizer-bpe-partir-de-zro",title:"Construire un *tokenizer* BPE \xE0 partir de z\xE9ro"},{local:"construire-un-tokenizer-unigram-partir-de-rien",title:"Construire un *tokenizer* *Unigram* \xE0 partir de rien."}],title:"Construction d'un *tokenizer*, bloc par bloc"};function Lq(ni){return bq(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Aq extends Eq{constructor($){super();qq(this,$,Lq,Cq,jq,{})}}export{Aq as default,Dq as metadata};
