import{S as cs,i as ms,s as ps,e as u,k as _,w as S,t as i,M as us,c as h,d as s,m as w,x as W,a as b,h as l,b as x,N as os,G as r,g as p,y as N,o as M,p as ns,q as E,B,v as hs,n as is}from"../../chunks/vendor-hf-doc-builder.js";import{T as fs}from"../../chunks/Tip-hf-doc-builder.js";import{I as ds}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as ls}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as bs}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function ks(C){let o,c;return o=new ls({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section2_tf.ipynb"}]}}),{c(){S(o.$$.fragment)},l(t){W(o.$$.fragment,t)},m(t,$){N(o,t,$),c=!0},i(t){c||(E(o.$$.fragment,t),c=!0)},o(t){M(o.$$.fragment,t),c=!1},d(t){B(o,t)}}}function gs(C){let o,c;return o=new ls({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section2_pt.ipynb"}]}}),{c(){S(o.$$.fragment)},l(t){W(o.$$.fragment,t)},m(t,$){N(o,t,$),c=!0},i(t){c||(E(o.$$.fragment,t),c=!0)},o(t){M(o.$$.fragment,t),c=!1},d(t){B(o,t)}}}function _s(C){let o,c,t,$,f,g,A,y,D,v,O,q,k,d,j;return o=new ne({props:{code:`from transformers import CamembertTokenizer, TFCamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = TFCamembertForMaskedLM.from_pretrained("camembert-base")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CamembertTokenizer, TFCamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)
model = TFCamembertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)`}}),d=new ne({props:{code:`from transformers import AutoTokenizer, TFAutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = TFAutoModelForMaskedLM.from_pretrained("camembert-base")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)
model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)`}}),{c(){S(o.$$.fragment),c=_(),t=u("p"),$=i("However, we recommend using the "),f=u("a"),g=u("code"),A=i("TFAuto*"),y=i(" classes"),D=i(" instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the "),v=u("code"),O=i("TFAuto*"),q=i(" classes makes switching checkpoints simple:"),k=_(),S(d.$$.fragment),this.h()},l(a){W(o.$$.fragment,a),c=w(a),t=h(a,"P",{});var m=b(t);$=l(m,"However, we recommend using the "),f=h(m,"A",{href:!0,rel:!0});var F=b(f);g=h(F,"CODE",{});var L=b(g);A=l(L,"TFAuto*"),L.forEach(s),y=l(F," classes"),F.forEach(s),D=l(m," instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the "),v=h(m,"CODE",{});var G=b(v);O=l(G,"TFAuto*"),G.forEach(s),q=l(m," classes makes switching checkpoints simple:"),m.forEach(s),k=w(a),W(d.$$.fragment,a),this.h()},h(){x(f,"href","https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes"),x(f,"rel","nofollow")},m(a,m){N(o,a,m),p(a,c,m),p(a,t,m),r(t,$),r(t,f),r(f,g),r(g,A),r(f,y),r(t,D),r(t,v),r(v,O),r(t,q),p(a,k,m),N(d,a,m),j=!0},i(a){j||(E(o.$$.fragment,a),E(d.$$.fragment,a),j=!0)},o(a){M(o.$$.fragment,a),M(d.$$.fragment,a),j=!1},d(a){B(o,a),a&&s(c),a&&s(t),a&&s(k),B(d,a)}}}function ws(C){let o,c,t,$,f,g,A,y,D,v,O,q,k,d,j;return o=new ne({props:{code:`from transformers import CamembertTokenizer, CamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = CamembertForMaskedLM.from_pretrained("camembert-base")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CamembertTokenizer, CamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)
model = CamembertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)`}}),d=new ne({props:{code:`from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = AutoModelForMaskedLM.from_pretrained("camembert-base")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)
model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;camembert-base&quot;</span>)`}}),{c(){S(o.$$.fragment),c=_(),t=u("p"),$=i("However, we recommend using the "),f=u("a"),g=u("code"),A=i("Auto*"),y=i(" classes"),D=i(" instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the "),v=u("code"),O=i("Auto*"),q=i(" classes makes switching checkpoints simple:"),k=_(),S(d.$$.fragment),this.h()},l(a){W(o.$$.fragment,a),c=w(a),t=h(a,"P",{});var m=b(t);$=l(m,"However, we recommend using the "),f=h(m,"A",{href:!0,rel:!0});var F=b(f);g=h(F,"CODE",{});var L=b(g);A=l(L,"Auto*"),L.forEach(s),y=l(F," classes"),F.forEach(s),D=l(m," instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the "),v=h(m,"CODE",{});var G=b(v);O=l(G,"Auto*"),G.forEach(s),q=l(m," classes makes switching checkpoints simple:"),m.forEach(s),k=w(a),W(d.$$.fragment,a),this.h()},h(){x(f,"href","https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes"),x(f,"rel","nofollow")},m(a,m){N(o,a,m),p(a,c,m),p(a,t,m),r(t,$),r(t,f),r(f,g),r(g,A),r(f,y),r(t,D),r(t,v),r(v,O),r(t,q),p(a,k,m),N(d,a,m),j=!0},i(a){j||(E(o.$$.fragment,a),E(d.$$.fragment,a),j=!0)},o(a){M(o.$$.fragment,a),M(d.$$.fragment,a),j=!1},d(a){B(o,a),a&&s(c),a&&s(t),a&&s(k),B(d,a)}}}function $s(C){let o;return{c(){o=i("When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.")},l(c){o=l(c,"When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.")},m(c,t){p(c,o,t)},d(c){c&&s(o)}}}function xs(C){let o,c,t,$,f,g,A,y,D,v,O,q,k,d,j,a,m,F,L,G,de,V,Y,Ne,be,z,je,ie,ye,Me,le,Ee,Te,ce,Ce,Ae,ke,J,ge,K,_e,T,qe,me,Fe,Le,pe,ze,De,ue,Oe,He,he,Pe,Se,we,Q,X,Be,$e,te,We,xe,H,P,ae,R,ve;t=new bs({props:{fw:C[0]}}),y=new ds({});const Ie=[gs,ks],Z=[];function Ge(e,n){return e[0]==="pt"?0:1}k=Ge(C),d=Z[k]=Ie[k](C),J=new ne({props:{code:`from transformers import pipeline

camembert_fill_mask = pipeline("fill-mask", model="camembert-base")
results = camembert_fill_mask("Le camembert est <mask> :)")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

camembert_fill_mask = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;camembert-base&quot;</span>)
results = camembert_fill_mask(<span class="hljs-string">&quot;Le camembert est &lt;mask&gt; :)&quot;</span>)`}}),K=new ne({props:{code:`[
  {'sequence': 'Le camembert est d\xE9licieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'd\xE9licieux'}, 
  {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'}, 
  {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'}, 
  {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'}, 
  {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'}
]`,highlighted:`[
  {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Le camembert est d\xE9licieux :)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.49091005325317383</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">7200</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;d\xE9licieux&#x27;</span>}, 
  {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Le camembert est excellent :)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.1055697426199913</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">2183</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;excellent&#x27;</span>}, 
  {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Le camembert est succulent :)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.03453313186764717</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">26202</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;succulent&#x27;</span>}, 
  {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Le camembert est meilleur :)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0330314114689827</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">528</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;meilleur&#x27;</span>}, 
  {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Le camembert est parfait :)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.03007650189101696</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">1654</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;parfait&#x27;</span>}
]`}});const Re=[ws,_s],ee=[];function Ue(e,n){return e[0]==="pt"?0:1}return H=Ue(C),P=ee[H]=Re[H](C),R=new fs({props:{$$slots:{default:[$s]},$$scope:{ctx:C}}}),{c(){o=u("meta"),c=_(),S(t.$$.fragment),$=_(),f=u("h1"),g=u("a"),A=u("span"),S(y.$$.fragment),D=_(),v=u("span"),O=i("Using pretrained models"),q=_(),d.c(),j=_(),a=u("p"),m=i("The Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code. Let\u2019s take a look at how to actually use one of these models, and how to contribute back to the community."),F=_(),L=u("p"),G=i("Let\u2019s say we\u2019re looking for a French-based model that can perform mask filling."),de=_(),V=u("div"),Y=u("img"),be=_(),z=u("p"),je=i("We select the "),ie=u("code"),ye=i("camembert-base"),Me=i(" checkpoint to try it out. The identifier "),le=u("code"),Ee=i("camembert-base"),Te=i(" is all we need to start using it! As you\u2019ve seen in previous chapters, we can instantiate it using the "),ce=u("code"),Ce=i("pipeline()"),Ae=i(" function:"),ke=_(),S(J.$$.fragment),ge=_(),S(K.$$.fragment),_e=_(),T=u("p"),qe=i("As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it\u2019s going to be used for. For example, here we are loading the "),me=u("code"),Fe=i("camembert-base"),Le=i(" checkpoint in the "),pe=u("code"),ze=i("fill-mask"),De=i(" pipeline, which is completely fine. But if we were to load this checkpoint in the "),ue=u("code"),Oe=i("text-classification"),He=i(" pipeline, the results would not make any sense because the head of "),he=u("code"),Pe=i("camembert-base"),Se=i(" is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:"),we=_(),Q=u("div"),X=u("img"),$e=_(),te=u("p"),We=i("You can also instantiate the checkpoint using the model architecture directly:"),xe=_(),P.c(),ae=_(),S(R.$$.fragment),this.h()},l(e){const n=us('[data-svelte="svelte-1phssyn"]',document.head);o=h(n,"META",{name:!0,content:!0}),n.forEach(s),c=w(e),W(t.$$.fragment,e),$=w(e),f=h(e,"H1",{class:!0});var se=b(f);g=h(se,"A",{id:!0,class:!0,href:!0});var re=b(g);A=h(re,"SPAN",{});var oe=b(A);W(y.$$.fragment,oe),oe.forEach(s),re.forEach(s),D=w(se),v=h(se,"SPAN",{});var fe=b(v);O=l(fe,"Using pretrained models"),fe.forEach(s),se.forEach(s),q=w(e),d.l(e),j=w(e),a=h(e,"P",{});var Ve=b(a);m=l(Ve,"The Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code. Let\u2019s take a look at how to actually use one of these models, and how to contribute back to the community."),Ve.forEach(s),F=w(e),L=h(e,"P",{});var Ye=b(L);G=l(Ye,"Let\u2019s say we\u2019re looking for a French-based model that can perform mask filling."),Ye.forEach(s),de=w(e),V=h(e,"DIV",{class:!0});var Je=b(V);Y=h(Je,"IMG",{src:!0,alt:!0,width:!0}),Je.forEach(s),be=w(e),z=h(e,"P",{});var U=b(z);je=l(U,"We select the "),ie=h(U,"CODE",{});var Ke=b(ie);ye=l(Ke,"camembert-base"),Ke.forEach(s),Me=l(U," checkpoint to try it out. The identifier "),le=h(U,"CODE",{});var Qe=b(le);Ee=l(Qe,"camembert-base"),Qe.forEach(s),Te=l(U," is all we need to start using it! As you\u2019ve seen in previous chapters, we can instantiate it using the "),ce=h(U,"CODE",{});var Xe=b(ce);Ce=l(Xe,"pipeline()"),Xe.forEach(s),Ae=l(U," function:"),U.forEach(s),ke=w(e),W(J.$$.fragment,e),ge=w(e),W(K.$$.fragment,e),_e=w(e),T=h(e,"P",{});var I=b(T);qe=l(I,"As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it\u2019s going to be used for. For example, here we are loading the "),me=h(I,"CODE",{});var Ze=b(me);Fe=l(Ze,"camembert-base"),Ze.forEach(s),Le=l(I," checkpoint in the "),pe=h(I,"CODE",{});var es=b(pe);ze=l(es,"fill-mask"),es.forEach(s),De=l(I," pipeline, which is completely fine. But if we were to load this checkpoint in the "),ue=h(I,"CODE",{});var ss=b(ue);Oe=l(ss,"text-classification"),ss.forEach(s),He=l(I," pipeline, the results would not make any sense because the head of "),he=h(I,"CODE",{});var ts=b(he);Pe=l(ts,"camembert-base"),ts.forEach(s),Se=l(I," is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:"),I.forEach(s),we=w(e),Q=h(e,"DIV",{class:!0});var as=b(Q);X=h(as,"IMG",{src:!0,alt:!0,width:!0}),as.forEach(s),$e=w(e),te=h(e,"P",{});var rs=b(te);We=l(rs,"You can also instantiate the checkpoint using the model architecture directly:"),rs.forEach(s),xe=w(e),P.l(e),ae=w(e),W(R.$$.fragment,e),this.h()},h(){x(o,"name","hf:doc:metadata"),x(o,"content",JSON.stringify(vs)),x(g,"id","using-pretrained-models"),x(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),x(g,"href","#using-pretrained-models"),x(f,"class","relative group"),os(Y.src,Ne="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/camembert.gif")||x(Y,"src",Ne),x(Y,"alt","Selecting the Camembert model."),x(Y,"width","80%"),x(V,"class","flex justify-center"),os(X.src,Be="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png")||x(X,"src",Be),x(X,"alt","The task selector on the web interface."),x(X,"width","80%"),x(Q,"class","flex justify-center")},m(e,n){r(document.head,o),p(e,c,n),N(t,e,n),p(e,$,n),p(e,f,n),r(f,g),r(g,A),N(y,A,null),r(f,D),r(f,v),r(v,O),p(e,q,n),Z[k].m(e,n),p(e,j,n),p(e,a,n),r(a,m),p(e,F,n),p(e,L,n),r(L,G),p(e,de,n),p(e,V,n),r(V,Y),p(e,be,n),p(e,z,n),r(z,je),r(z,ie),r(ie,ye),r(z,Me),r(z,le),r(le,Ee),r(z,Te),r(z,ce),r(ce,Ce),r(z,Ae),p(e,ke,n),N(J,e,n),p(e,ge,n),N(K,e,n),p(e,_e,n),p(e,T,n),r(T,qe),r(T,me),r(me,Fe),r(T,Le),r(T,pe),r(pe,ze),r(T,De),r(T,ue),r(ue,Oe),r(T,He),r(T,he),r(he,Pe),r(T,Se),p(e,we,n),p(e,Q,n),r(Q,X),p(e,$e,n),p(e,te,n),r(te,We),p(e,xe,n),ee[H].m(e,n),p(e,ae,n),N(R,e,n),ve=!0},p(e,[n]){const se={};n&1&&(se.fw=e[0]),t.$set(se);let re=k;k=Ge(e),k!==re&&(is(),M(Z[re],1,1,()=>{Z[re]=null}),ns(),d=Z[k],d||(d=Z[k]=Ie[k](e),d.c()),E(d,1),d.m(j.parentNode,j));let oe=H;H=Ue(e),H!==oe&&(is(),M(ee[oe],1,1,()=>{ee[oe]=null}),ns(),P=ee[H],P||(P=ee[H]=Re[H](e),P.c()),E(P,1),P.m(ae.parentNode,ae));const fe={};n&2&&(fe.$$scope={dirty:n,ctx:e}),R.$set(fe)},i(e){ve||(E(t.$$.fragment,e),E(y.$$.fragment,e),E(d),E(J.$$.fragment,e),E(K.$$.fragment,e),E(P),E(R.$$.fragment,e),ve=!0)},o(e){M(t.$$.fragment,e),M(y.$$.fragment,e),M(d),M(J.$$.fragment,e),M(K.$$.fragment,e),M(P),M(R.$$.fragment,e),ve=!1},d(e){s(o),e&&s(c),B(t,e),e&&s($),e&&s(f),B(y),e&&s(q),Z[k].d(e),e&&s(j),e&&s(a),e&&s(F),e&&s(L),e&&s(de),e&&s(V),e&&s(be),e&&s(z),e&&s(ke),B(J,e),e&&s(ge),B(K,e),e&&s(_e),e&&s(T),e&&s(we),e&&s(Q),e&&s($e),e&&s(te),e&&s(xe),ee[H].d(e),e&&s(ae),B(R,e)}}}const vs={local:"using-pretrained-models",title:"Using pretrained models"};function js(C,o,c){let t="pt";return hs(()=>{const $=new URLSearchParams(window.location.search);c(0,t=$.get("fw")||"pt")}),[t]}class qs extends cs{constructor(o){super();ms(this,o,js,xs,ps,{})}}export{qs as default,vs as metadata};
