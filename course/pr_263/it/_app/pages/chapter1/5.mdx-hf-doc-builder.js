import{S as Se,i as xe,s as ke,e as a,k as d,w as ge,t as m,M as Ce,c as l,d as o,m as f,a as r,x as be,h as p,b as n,G as t,g as c,y as qe,L as Ne,q as Pe,o as Be,B as Me,v as Qe}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ue}from"../../chunks/Youtube-hf-doc-builder.js";import{I as De}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Je(de){let h,C,_,v,b,z,O,q,F,N,w,Q,E,K,P,V,W,U,R,X,D,T,Z,J,g,ee,Y,s,B,$,oe,te,M,A,ae,le,S,y,re,ie,x,I,ne,se,k,L,ce,j;return z=new De({}),w=new Ue({props:{id:"MUqNwgPjJvQ"}}),{c(){h=a("meta"),C=d(),_=a("h1"),v=a("a"),b=a("span"),ge(z.$$.fragment),O=d(),q=a("span"),F=m("Modelli encoder"),N=d(),ge(w.$$.fragment),Q=d(),E=a("p"),K=m("I modelli encoder utilizzano solo l\u2019encoder di un modello Transformer. In ogni fase, gli attention layer hanno accesso a tutte le parole della frase di partenza. Questi modelli sono spesso caratterizzati come aventi attenzione \u201Cbi-direzionale\u201D e chiamati "),P=a("em"),V=m("auto-encoding models"),W=m("."),U=d(),R=a("p"),X=m("Solitamente, il pre-addestramento di questi modelli consiste nel corrompere una determinata frase (ad esempio, nascondendone casualmente alcune parole) e incaricare il modello di ritrovare o ricostruire la frase di partenza."),D=d(),T=a("p"),Z=m("I modelli encoder sono particolarmente appropriati per compiti che richiedono la comprensione di frasi intere, quali la classificazione di frasi, riconoscimento delle entit\xE0 nominate (e in senso pi\xF9 ampio, la classificazione di parole), e l\u2019estrazione di risposte da un contesto."),J=d(),g=a("p"),ee=m("Alcuni esempi di modelli di questo tipo includono:"),Y=d(),s=a("ul"),B=a("li"),$=a("a"),oe=m("ALBERT"),te=d(),M=a("li"),A=a("a"),ae=m("BERT"),le=d(),S=a("li"),y=a("a"),re=m("DistilBERT"),ie=d(),x=a("li"),I=a("a"),ne=m("ELECTRA"),se=d(),k=a("li"),L=a("a"),ce=m("RoBERTa"),this.h()},l(e){const i=Ce('[data-svelte="svelte-1phssyn"]',document.head);h=l(i,"META",{name:!0,content:!0}),i.forEach(o),C=f(e),_=l(e,"H1",{class:!0});var G=r(_);v=l(G,"A",{id:!0,class:!0,href:!0});var me=r(v);b=l(me,"SPAN",{});var fe=r(b);be(z.$$.fragment,fe),fe.forEach(o),me.forEach(o),O=f(G),q=l(G,"SPAN",{});var pe=r(q);F=p(pe,"Modelli encoder"),pe.forEach(o),G.forEach(o),N=f(e),be(w.$$.fragment,e),Q=f(e),E=l(e,"P",{});var H=r(E);K=p(H,"I modelli encoder utilizzano solo l\u2019encoder di un modello Transformer. In ogni fase, gli attention layer hanno accesso a tutte le parole della frase di partenza. Questi modelli sono spesso caratterizzati come aventi attenzione \u201Cbi-direzionale\u201D e chiamati "),P=l(H,"EM",{});var ue=r(P);V=p(ue,"auto-encoding models"),ue.forEach(o),W=p(H,"."),H.forEach(o),U=f(e),R=l(e,"P",{});var he=r(R);X=p(he,"Solitamente, il pre-addestramento di questi modelli consiste nel corrompere una determinata frase (ad esempio, nascondendone casualmente alcune parole) e incaricare il modello di ritrovare o ricostruire la frase di partenza."),he.forEach(o),D=f(e),T=l(e,"P",{});var _e=r(T);Z=p(_e,"I modelli encoder sono particolarmente appropriati per compiti che richiedono la comprensione di frasi intere, quali la classificazione di frasi, riconoscimento delle entit\xE0 nominate (e in senso pi\xF9 ampio, la classificazione di parole), e l\u2019estrazione di risposte da un contesto."),_e.forEach(o),J=f(e),g=l(e,"P",{});var ve=r(g);ee=p(ve,"Alcuni esempi di modelli di questo tipo includono:"),ve.forEach(o),Y=f(e),s=l(e,"UL",{});var u=r(s);B=l(u,"LI",{});var Ee=r(B);$=l(Ee,"A",{href:!0,rel:!0});var ze=r($);oe=p(ze,"ALBERT"),ze.forEach(o),Ee.forEach(o),te=f(u),M=l(u,"LI",{});var we=r(M);A=l(we,"A",{href:!0,rel:!0});var $e=r(A);ae=p($e,"BERT"),$e.forEach(o),we.forEach(o),le=f(u),S=l(u,"LI",{});var Ae=r(S);y=l(Ae,"A",{href:!0,rel:!0});var ye=r(y);re=p(ye,"DistilBERT"),ye.forEach(o),Ae.forEach(o),ie=f(u),x=l(u,"LI",{});var Ie=r(x);I=l(Ie,"A",{href:!0,rel:!0});var Le=r(I);ne=p(Le,"ELECTRA"),Le.forEach(o),Ie.forEach(o),se=f(u),k=l(u,"LI",{});var Re=r(k);L=l(Re,"A",{href:!0,rel:!0});var Te=r(L);ce=p(Te,"RoBERTa"),Te.forEach(o),Re.forEach(o),u.forEach(o),this.h()},h(){n(h,"name","hf:doc:metadata"),n(h,"content",JSON.stringify(Ye)),n(v,"id","modelli-encoder"),n(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(v,"href","#modelli-encoder"),n(_,"class","relative group"),n($,"href","https://huggingface.co/transformers/model_doc/albert.html"),n($,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/bert.html"),n(A,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),n(y,"rel","nofollow"),n(I,"href","https://huggingface.co/transformers/model_doc/electra.html"),n(I,"rel","nofollow"),n(L,"href","https://huggingface.co/transformers/model_doc/roberta.html"),n(L,"rel","nofollow")},m(e,i){t(document.head,h),c(e,C,i),c(e,_,i),t(_,v),t(v,b),qe(z,b,null),t(_,O),t(_,q),t(q,F),c(e,N,i),qe(w,e,i),c(e,Q,i),c(e,E,i),t(E,K),t(E,P),t(P,V),t(E,W),c(e,U,i),c(e,R,i),t(R,X),c(e,D,i),c(e,T,i),t(T,Z),c(e,J,i),c(e,g,i),t(g,ee),c(e,Y,i),c(e,s,i),t(s,B),t(B,$),t($,oe),t(s,te),t(s,M),t(M,A),t(A,ae),t(s,le),t(s,S),t(S,y),t(y,re),t(s,ie),t(s,x),t(x,I),t(I,ne),t(s,se),t(s,k),t(k,L),t(L,ce),j=!0},p:Ne,i(e){j||(Pe(z.$$.fragment,e),Pe(w.$$.fragment,e),j=!0)},o(e){Be(z.$$.fragment,e),Be(w.$$.fragment,e),j=!1},d(e){o(h),e&&o(C),e&&o(_),Me(z),e&&o(N),Me(w,e),e&&o(Q),e&&o(E),e&&o(U),e&&o(R),e&&o(D),e&&o(T),e&&o(J),e&&o(g),e&&o(Y),e&&o(s)}}}const Ye={local:"modelli-encoder",title:"Modelli encoder"};function je(de){return Qe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fe extends Se{constructor(h){super();xe(this,h,je,Je,ke,{})}}export{Fe as default,Ye as metadata};
