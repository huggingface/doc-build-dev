import{S as Rp,i as Gp,s as Mp,e as l,k as c,w as b,t as o,M as Fp,c as n,d as i,m as u,a as r,x as w,h as s,b as d,N as Up,G as a,g as p,y as E,q as x,o as q,B as j,v as Qp}from"../../chunks/vendor-hf-doc-builder.js";import{T as Me}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Vp}from"../../chunks/Youtube-hf-doc-builder.js";import{I as D}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Bp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Wp(S){let m,_,f,z,$,h,g,v,P;return{c(){m=o("\u{1F440} Lo vedi il pulsante "),_=l("em"),f=o("Open in Colab"),z=o(` in alto a destra? Cliccalo per aprire il blocco note Colab di Google che contiene tutti gli esempi di codice di questa sezione. Ritroverai il pulsante in ogni sezione che contiene esempi di codice. 
`),$=l("p"),h=o("Se intendi compilare gli esempi localmente, ti consigliamo di dare un occhio alla sezione "),g=l("a"),v=o("setup"),P=o("."),this.h()},l(y){m=s(y,"\u{1F440} Lo vedi il pulsante "),_=n(y,"EM",{});var I=r(_);f=s(I,"Open in Colab"),I.forEach(i),z=s(y,` in alto a destra? Cliccalo per aprire il blocco note Colab di Google che contiene tutti gli esempi di codice di questa sezione. Ritroverai il pulsante in ogni sezione che contiene esempi di codice. 
`),$=n(y,"P",{});var A=r($);h=s(A,"Se intendi compilare gli esempi localmente, ti consigliamo di dare un occhio alla sezione "),g=n(A,"A",{href:!0});var k=r(g);v=s(k,"setup"),k.forEach(i),P=s(A,"."),A.forEach(i),this.h()},h(){d(g,"href","/course/chapter0")},m(y,I){p(y,m,I),p(y,_,I),a(_,f),p(y,z,I),p(y,$,I),a($,h),a($,g),a(g,v),a($,P)},d(y){y&&i(m),y&&i(_),y&&i(z),y&&i($)}}}function Yp(S){let m,_,f,z,$,h,g;return{c(){m=o("\u26A0\uFE0F L'Hugging Face Hub non si limitata ai soli modelli Transformer. Chiunque pu\xF2 condividere qualsiasi tipo di modello o dataset ("),_=l("em"),f=o("insieme di dati"),z=o(")! "),$=l("a"),h=o("Crea un profilo huggingface.co"),g=o(" per approfittare di tutte le funzioni disponibili!"),this.h()},l(v){m=s(v,"\u26A0\uFE0F L'Hugging Face Hub non si limitata ai soli modelli Transformer. Chiunque pu\xF2 condividere qualsiasi tipo di modello o dataset ("),_=n(v,"EM",{});var P=r(_);f=s(P,"insieme di dati"),P.forEach(i),z=s(v,")! "),$=n(v,"A",{href:!0});var y=r($);h=s(y,"Crea un profilo huggingface.co"),y.forEach(i),g=s(v," per approfittare di tutte le funzioni disponibili!"),this.h()},h(){d($,"href","https://huggingface.co/join")},m(v,P){p(v,m,P),p(v,_,P),a(_,f),p(v,z,P),p(v,$,P),a($,h),p(v,g,P)},d(v){v&&i(m),v&&i(_),v&&i(z),v&&i($),v&&i(g)}}}function Jp(S){let m,_,f,z,$;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Divertiti creando sequenze ed etichette e osserva come si comporta il modello.")},l(h){m=n(h,"P",{});var g=r(m);_=s(g,"\u270F\uFE0F "),f=n(g,"STRONG",{});var v=r(f);z=s(v,"Provaci anche tu!"),v.forEach(i),$=s(g," Divertiti creando sequenze ed etichette e osserva come si comporta il modello."),g.forEach(i)},m(h,g){p(h,m,g),a(m,_),a(m,f),a(f,z),a(m,$)},d(h){h&&i(m)}}}function Zp(S){let m,_,f,z,$,h,g,v,P,y,I;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Usa gli argomenti "),h=l("code"),g=o("num_return_sequences"),v=o(" e "),P=l("code"),y=o("max_length"),I=o(" per generare due frasi di 15 parole ciascuna.")},l(A){m=n(A,"P",{});var k=r(m);_=s(k,"\u270F\uFE0F "),f=n(k,"STRONG",{});var W=r(f);z=s(W,"Provaci anche tu!"),W.forEach(i),$=s(k," Usa gli argomenti "),h=n(k,"CODE",{});var R=r(h);g=s(R,"num_return_sequences"),R.forEach(i),v=s(k," e "),P=n(k,"CODE",{});var Y=r(P);y=s(Y,"max_length"),Y.forEach(i),I=s(k," per generare due frasi di 15 parole ciascuna."),k.forEach(i)},m(A,k){p(A,m,k),a(m,_),a(m,f),a(f,z),a(m,$),a(m,h),a(h,g),a(m,v),a(m,P),a(P,y),a(m,I)},d(A){A&&i(m)}}}function Kp(S){let m,_,f,z,$;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Usa i filtri per trovare un modello di generazione testuale per un\u2019altra lingua. Sentiti libero/a di divertirti con il widget e usalo in una pipeline!")},l(h){m=n(h,"P",{});var g=r(m);_=s(g,"\u270F\uFE0F "),f=n(g,"STRONG",{});var v=r(f);z=s(v,"Provaci anche tu!"),v.forEach(i),$=s(g," Usa i filtri per trovare un modello di generazione testuale per un\u2019altra lingua. Sentiti libero/a di divertirti con il widget e usalo in una pipeline!"),g.forEach(i)},m(h,g){p(h,m,g),a(m,_),a(m,f),a(f,z),a(m,$)},d(h){h&&i(m)}}}function Xp(S){let m,_,f,z,$,h,g,v,P,y,I;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Cerca il modello "),h=l("code"),g=o("bert-base-cased"),v=o(" nell\u2019Hub e identifica la sua mask word nel widget dell\u2019Inference API. Cosa predice questo modello per la frase nel nostro esempio "),P=l("code"),y=o("pipeline"),I=o(" qui sopra?")},l(A){m=n(A,"P",{});var k=r(m);_=s(k,"\u270F\uFE0F "),f=n(k,"STRONG",{});var W=r(f);z=s(W,"Provaci anche tu!"),W.forEach(i),$=s(k," Cerca il modello "),h=n(k,"CODE",{});var R=r(h);g=s(R,"bert-base-cased"),R.forEach(i),v=s(k," nell\u2019Hub e identifica la sua mask word nel widget dell\u2019Inference API. Cosa predice questo modello per la frase nel nostro esempio "),P=n(k,"CODE",{});var Y=r(P);y=s(Y,"pipeline"),Y.forEach(i),I=s(k," qui sopra?"),k.forEach(i)},m(A,k){p(A,m,k),a(m,_),a(m,f),a(f,z),a(m,$),a(m,h),a(h,g),a(m,v),a(m,P),a(P,y),a(m,I)},d(A){A&&i(m)}}}function ec(S){let m,_,f,z,$;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Nel Model Hub, cerca un modello capace di effettuare part-of-speech tagging (comunemente abbreviato come POS) in inglese. Cosa predice il modello per la frase nell\u2019esempio qui sopra?")},l(h){m=n(h,"P",{});var g=r(m);_=s(g,"\u270F\uFE0F "),f=n(g,"STRONG",{});var v=r(f);z=s(v,"Provaci anche tu!"),v.forEach(i),$=s(g," Nel Model Hub, cerca un modello capace di effettuare part-of-speech tagging (comunemente abbreviato come POS) in inglese. Cosa predice il modello per la frase nell\u2019esempio qui sopra?"),g.forEach(i)},m(h,g){p(h,m,g),a(m,_),a(m,f),a(f,z),a(m,$)},d(h){h&&i(m)}}}function ic(S){let m,_,f,z,$;return{c(){m=l("p"),_=o("\u270F\uFE0F "),f=l("strong"),z=o("Provaci anche tu!"),$=o(" Cerca modelli di traduzione in altre lingue e prova a tradurre la frase precedente in un paio di lingue diverse.")},l(h){m=n(h,"P",{});var g=r(m);_=s(g,"\u270F\uFE0F "),f=n(g,"STRONG",{});var v=r(f);z=s(v,"Provaci anche tu!"),v.forEach(i),$=s(g," Cerca modelli di traduzione in altre lingue e prova a tradurre la frase precedente in un paio di lingue diverse."),g.forEach(i)},m(h,g){p(h,m,g),a(m,_),a(m,f),a(f,z),a(m,$)},d(h){h&&i(m)}}}function ac(S){let m,_,f,z,$,h,g,v,P,y,I,A,k,W,R,Y,rs,vt,ne,$t,J,re,Zi,Fe,ps,Ki,cs,_t,Ti,us,zt,pe,Jn,bt,G,ms,Ue,ds,fs,Qe,hs,gs,wt,ce,Et,Si,vs,xt,Z,ue,Xi,Ve,$s,ea,_s,qt,Be,jt,me,zs,ia,bs,ws,kt,We,yt,Ye,Pt,Oi,Es,It,Je,Ct,Ze,At,de,xs,aa,qs,js,Tt,Hi,ks,St,M,ta,ys,Ps,oa,Is,Cs,sa,As,Ot,fe,Ts,Ke,Ss,Os,Ht,C,Li,la,Hs,Ls,Ns,na,ra,Ds,Rs,he,pa,Gs,Ms,ca,Fs,Us,Qs,ua,ma,Vs,Bs,da,fa,Ws,Ys,ha,ga,Js,Zs,va,$a,Ks,Xs,_a,za,el,il,ba,wa,al,Lt,Ni,tl,Nt,K,ge,Ea,Xe,ol,xa,sl,Dt,ve,ll,qa,nl,rl,Rt,ei,Gt,ii,Mt,$e,pl,ja,cl,ul,Ft,_e,Ut,X,ze,ka,ai,ml,ya,dl,Qt,be,fl,Pa,hl,gl,Vt,ti,Bt,oi,Wt,F,vl,Ia,$l,_l,Ca,zl,bl,Yt,we,Jt,ee,Ee,Aa,si,wl,Ta,El,Zt,U,xl,li,ql,jl,ni,kl,yl,Kt,xe,Pl,ri,Sa,Il,Cl,Xt,pi,eo,ci,io,Di,Al,ao,Ri,Tl,to,qe,oo,ie,je,Oa,ui,Sl,Ha,Ol,so,ke,Hl,mi,Ll,Nl,lo,ye,Dl,di,Rl,Gl,no,ae,Pe,La,fi,Ml,Na,Fl,ro,Ie,Ul,Da,Ql,Vl,po,hi,co,gi,uo,N,Bl,Ra,Wl,Yl,Ga,Jl,Zl,Ma,Kl,Xl,mo,Ce,fo,te,Ae,Fa,vi,en,Ua,an,ho,Te,tn,Qa,on,sn,go,$i,vo,_i,$o,Gi,ln,_o,O,nn,Va,rn,pn,Ba,cn,un,Wa,mn,dn,Ya,fn,hn,Ja,gn,vn,Za,$n,_n,zo,Se,bo,oe,Oe,Ka,zi,zn,Xa,bn,wo,He,wn,et,En,xn,Eo,bi,xo,wi,qo,Mi,qn,jo,se,Le,it,Ei,jn,at,kn,ko,Fi,yn,yo,xi,Po,qi,Io,Q,Pn,tt,In,Cn,ot,An,Tn,Co,le,Ne,st,ji,Sn,lt,On,Ao,V,Hn,nt,Ln,Nn,ki,Dn,Rn,To,yi,So,Pi,Oo,B,Gn,rt,Mn,Fn,pt,Un,Qn,Ho,De,Lo,Re,Vn,ct,Bn,Wn,No;return h=new D({}),I=new Bp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"}]}}),ne=new Me({props:{$$slots:{default:[Wp]},$$scope:{ctx:S}}}),Fe=new D({}),ce=new Me({props:{$$slots:{default:[Yp]},$$scope:{ctx:S}}}),Ve=new D({}),Be=new Vp({props:{id:"tiZFewofSLM"}}),We=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)
classifier(<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>)`}}),Ye=new T({props:{code:"[{'label': 'POSITIVE', 'score': 0.9598047137260437}]",highlighted:'[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>}]'}}),Je=new T({props:{code:`classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)`,highlighted:`classifier(
    [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;I hate this so much!&quot;</span>]
)`}}),Ze=new T({props:{code:`[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]`,highlighted:`[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>},
 {<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994558095932007</span>}]`}}),Xe=new D({}),ei=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;zero-shot-classification&quot;</span>)
classifier(
    <span class="hljs-string">&quot;This is a course about the Transformers library&quot;</span>,
    candidate_labels=[<span class="hljs-string">&quot;education&quot;</span>, <span class="hljs-string">&quot;politics&quot;</span>, <span class="hljs-string">&quot;business&quot;</span>],
)`}}),ii=new T({props:{code:`{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}`,highlighted:`{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;This is a course about the Transformers library&#x27;</span>,
 <span class="hljs-string">&#x27;labels&#x27;</span>: [<span class="hljs-string">&#x27;education&#x27;</span>, <span class="hljs-string">&#x27;business&#x27;</span>, <span class="hljs-string">&#x27;politics&#x27;</span>],
 <span class="hljs-string">&#x27;scores&#x27;</span>: [<span class="hljs-number">0.8445963859558105</span>, <span class="hljs-number">0.111976258456707</span>, <span class="hljs-number">0.043427448719739914</span>]}`}}),_e=new Me({props:{$$slots:{default:[Jp]},$$scope:{ctx:S}}}),ai=new D({}),ti=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

generator = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>)
generator(<span class="hljs-string">&quot;In this course, we will teach you how to&quot;</span>)`}}),oi=new T({props:{code:`[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows \u2014 data flows of various types, as seen by the '
                    'HTTP'}]`,highlighted:`[{<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;In this course, we will teach you how to understand and use &#x27;</span>
                    <span class="hljs-string">&#x27;data flow and data interchange when handling user data. We &#x27;</span>
                    <span class="hljs-string">&#x27;will be working with one or more of the most commonly used &#x27;</span>
                    <span class="hljs-string">&#x27;data flows \u2014 data flows of various types, as seen by the &#x27;</span>
                    <span class="hljs-string">&#x27;HTTP&#x27;</span>}]`}}),we=new Me({props:{$$slots:{default:[Zp]},$$scope:{ctx:S}}}),si=new D({}),pi=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

generator = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;distilgpt2&quot;</span>)
generator(
    <span class="hljs-string">&quot;In this course, we will teach you how to&quot;</span>,
    max_length=<span class="hljs-number">30</span>,
    num_return_sequences=<span class="hljs-number">2</span>,
)`}}),ci=new T({props:{code:`[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]`,highlighted:`[{<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;In this course, we will teach you how to manipulate the world and &#x27;</span>
                    <span class="hljs-string">&#x27;move your mental and physical capabilities to your advantage.&#x27;</span>},
 {<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;In this course, we will teach you how to become an expert and &#x27;</span>
                    <span class="hljs-string">&#x27;practice realtime, and with a hands on experience on both real &#x27;</span>
                    <span class="hljs-string">&#x27;time and real&#x27;</span>}]`}}),qe=new Me({props:{$$slots:{default:[Kp]},$$scope:{ctx:S}}}),ui=new D({}),fi=new D({}),hi=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

unmasker = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>)
unmasker(<span class="hljs-string">&quot;This course will teach you all about &lt;mask&gt; models.&quot;</span>, top_k=<span class="hljs-number">2</span>)`}}),gi=new T({props:{code:`[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]`,highlighted:`[{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;This course will teach you all about mathematical models.&#x27;</span>,
  <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.19619831442832947</span>,
  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">30412</span>,
  <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27; mathematical&#x27;</span>},
 {<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;This course will teach you all about computational models.&#x27;</span>,
  <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.04052725434303284</span>,
  <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">38163</span>,
  <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27; computational&#x27;</span>}]`}}),Ce=new Me({props:{$$slots:{default:[Xp]},$$scope:{ctx:S}}}),vi=new D({}),$i=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

ner = pipeline(<span class="hljs-string">&quot;ner&quot;</span>, grouped_entities=<span class="hljs-literal">True</span>)
ner(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),_i=new T({props:{code:`[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99816</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>}, 
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>}, 
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}
]`}}),Se=new Me({props:{$$slots:{default:[ec]},$$scope:{ctx:S}}}),zi=new D({}),bi=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>)
question_answerer(
    question=<span class="hljs-string">&quot;Where do I work?&quot;</span>,
    context=<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;</span>,
)`}}),wi=new T({props:{code:"{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}",highlighted:'{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.6385916471481323</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>, <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>}'}}),Ei=new D({}),xi=new T({props:{code:`
`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>)
summarizer(
    <span class="hljs-string">&quot;&quot;&quot;
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
&quot;&quot;&quot;</span>
)`}}),qi=new T({props:{code:`[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]`,highlighted:`[{<span class="hljs-string">&#x27;summary_text&#x27;</span>: <span class="hljs-string">&#x27; America has changed dramatically during recent years . The &#x27;</span>
                  <span class="hljs-string">&#x27;number of engineering graduates in the U.S. has declined in &#x27;</span>
                  <span class="hljs-string">&#x27;traditional engineering disciplines such as mechanical, civil &#x27;</span>
                  <span class="hljs-string">&#x27;, electrical, chemical, and aeronautical engineering . Rapidly &#x27;</span>
                  <span class="hljs-string">&#x27;developing economies such as China and India, as well as other &#x27;</span>
                  <span class="hljs-string">&#x27;industrial countries in Europe and Asia, continue to encourage &#x27;</span>
                  <span class="hljs-string">&#x27;and advance engineering .&#x27;</span>}]`}}),ji=new D({}),yi=new T({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

translator = pipeline(<span class="hljs-string">&quot;translation&quot;</span>, model=<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-fr-en&quot;</span>)
translator(<span class="hljs-string">&quot;Ce cours est produit par Hugging Face.&quot;</span>)`}}),Pi=new T({props:{code:"[{'translation_text': 'This course is produced by Hugging Face.'}]",highlighted:'[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;This course is produced by Hugging Face.&#x27;</span>}]'}}),De=new Me({props:{$$slots:{default:[ic]},$$scope:{ctx:S}}}),{c(){m=l("meta"),_=c(),f=l("h1"),z=l("a"),$=l("span"),b(h.$$.fragment),g=c(),v=l("span"),P=o("Cosa fanno i Transformer?"),y=c(),b(I.$$.fragment),A=c(),k=l("p"),W=o("In questa sezione, vedremo di cosa sono capaci i modelli Transformer e useremo il nostro primo strumento della libreria \u{1F917} Transformer: la funzione "),R=l("code"),Y=o("pipeline()"),rs=o("."),vt=c(),b(ne.$$.fragment),$t=c(),J=l("h2"),re=l("a"),Zi=l("span"),b(Fe.$$.fragment),ps=c(),Ki=l("span"),cs=o("I Transformer sono ovunque!"),_t=c(),Ti=l("p"),us=o("I modelli Transformer sono utilizzati per eseguire qualsiasi compito di NLP, come ad esempio quelli menzionati nelle sezioni precedenti. Ecco alcune delle aziende e organizzazioni che utilizzano Hugging Face e i modelli Transformer, e contribuiscono a loro volta alla comunit\xE0 condividendo i propri modelli:"),zt=c(),pe=l("img"),bt=c(),G=l("p"),ms=o("La "),Ue=l("a"),ds=o("libreria \u{1F917} Transformer"),fs=o(" fornisce la funzionalit\xE0 per creare e utilizzare questi modelli condivisi. Il "),Qe=l("a"),hs=o("Model Hub"),gs=o(" contiene migliaia di modelli pre-addestrati che possono essere scaricati e usati liberamente. Puoi anche caricare i tuoi modelli nell\u2019Hub!"),wt=c(),b(ce.$$.fragment),Et=c(),Si=l("p"),vs=o("Prima di scoprire come funzionino i modelli Transformer dietro le quinte, vediamo qualche esempio di come questi possano essere utilizzati per risolvere alcuni problemi interessanti di NLP."),xt=c(),Z=l("h2"),ue=l("a"),Xi=l("span"),b(Ve.$$.fragment),$s=c(),ea=l("span"),_s=o("Lavorare con le pipeline"),qt=c(),b(Be.$$.fragment),jt=c(),me=l("p"),zs=o("L\u2019oggetto pi\xF9 basilare della libreria \u{1F917} Transformer \xE8 la funzione "),ia=l("code"),bs=o("pipeline()"),ws=o(". Questa connette un modello con tutte le fasi necessarie di preprocessing e postprocessing, permettendoci cos\xEC di fornire un qualsiasi testo come input diretto e ottenere una risposta intelligibile:"),kt=c(),b(We.$$.fragment),yt=c(),b(Ye.$$.fragment),Pt=c(),Oi=l("p"),Es=o("\xC8 anche possibile lavorare su pi\xF9 frasi!"),It=c(),b(Je.$$.fragment),Ct=c(),b(Ze.$$.fragment),At=c(),de=l("p"),xs=o("Per default, questa pipeline seleziona un preciso modello pre-addestrato che \xE8 stato affinato per il sentiment analysis in inglese. Quando creiamo l\u2019oggetto "),aa=l("code"),qs=o("classifier"),js=o(", il modello viene scaricato e memorizzato nella cache. Se inizializziamo di nuovo il comando, verr\xE0 utilizzato il modello salvato nella cache e non ci sar\xE0 quindi bisogno di scaricare di nuovo il modello."),Tt=c(),Hi=l("p"),ks=o("Tre passaggi principali sono coinvolti quando passiamo del testo in un pipeline:"),St=c(),M=l("ol"),ta=l("li"),ys=o("Il testo \xE8 pre-elaborato in un formato che il modello pu\xF2 capire."),Ps=c(),oa=l("li"),Is=o("Gli input pre-elaborati vengono passati al modello."),Cs=c(),sa=l("li"),As=o("Le previsioni del modello sono post-elaborate in un formato accessibile all\u2019utilizzatore."),Ot=c(),fe=l("p"),Ts=o("Tra le "),Ke=l("a"),Ss=o("pipeline disponibili"),Os=o(" al momento ci sono:"),Ht=c(),C=l("ul"),Li=l("li"),la=l("code"),Hs=o("feature-extraction"),Ls=o(" (per ottenere la rappresentazione vettoriale di un testo)"),Ns=c(),na=l("li"),ra=l("code"),Ds=o("fill-mask"),Rs=c(),he=l("li"),pa=l("code"),Gs=o("ner"),Ms=o(" (riconoscimento delle entit\xE0 nominate, "),ca=l("em"),Fs=o("named entity recognition"),Us=o(")"),Qs=c(),ua=l("li"),ma=l("code"),Vs=o("question-answering"),Bs=c(),da=l("li"),fa=l("code"),Ws=o("sentiment-analysis"),Ys=c(),ha=l("li"),ga=l("code"),Js=o("summarization"),Zs=c(),va=l("li"),$a=l("code"),Ks=o("text-generation"),Xs=c(),_a=l("li"),za=l("code"),el=o("translation"),il=c(),ba=l("li"),wa=l("code"),al=o("zero-shot-classification"),Lt=c(),Ni=l("p"),tl=o("Proviamo a vederne alcune!"),Nt=c(),K=l("h2"),ge=l("a"),Ea=l("span"),b(Xe.$$.fragment),ol=c(),xa=l("span"),sl=o("Classificazione zero-shot"),Dt=c(),ve=l("p"),ll=o("Cominceremo con l\u2019affrontare un compito impegnativo che consiste nella classificazione di testi non etichettati. Si tratta di uno scenario comune in molti progetti pratici perch\xE9 l\u2019annotazione testuale richiede tempo e competenza settoriale. In questo caso d\u2019uso, la pipeline "),qa=l("code"),nl=o("zero-shot-classification"),rl=o(" \xE8 molto potente e permette di specificare le etichette da utilizzare per la classificazione, in modo da non dover fare affidamento sulle etichette del modello pre-addestrato. Abbiamo gi\xE0 visto come il modello riesca a classificare una frase utilizzando le etichette \u2018positiva\u2019 e \u2018negativa\u2019, ma \xE8 anche possibile classificare testi utilizzando una qualsiasi serie di etichette di nostra scelta."),Rt=c(),b(ei.$$.fragment),Gt=c(),b(ii.$$.fragment),Mt=c(),$e=l("p"),pl=o("Questa pipeline si chiama "),ja=l("em"),cl=o("zero-shot"),ul=o(" perch\xE9 non hai bisogno di affinare il modello usando i tuoi dati per poterlo utilizzare. \xC8 direttamente in grado di generare una previsione probabilistica per qualsiasi lista di etichette tu voglia!"),Ft=c(),b(_e.$$.fragment),Ut=c(),X=l("h2"),ze=l("a"),ka=l("span"),b(ai.$$.fragment),ml=c(),ya=l("span"),dl=o("Generazione di testi"),Qt=c(),be=l("p"),fl=o("Vediamo ora come utilizzare la pipeline per generare testi. L\u2019idea \xE8 di fornire un prompt ("),Pa=l("em"),hl=o("richiesta"),gl=o(") che verr\xE0 auto-completato dal modello, il quale generer\xE0 il testo mancante. Si tratta di un compito simile alla funzione di scrittura facilitata che troviamo al giorno d\u2019oggi in molti cellulari. La generazione di testi presenta una componente arbitraria, per cui non essere sorpreso/a se non ottieni gli stessi risultati che mostriamo qui sotto."),Vt=c(),b(ti.$$.fragment),Bt=c(),b(oi.$$.fragment),Wt=c(),F=l("p"),vl=o("Usando l\u2019argomento "),Ia=l("code"),$l=o("num_return_sequences"),_l=o(" puoi controllare quante sequenze diverse vengono generate e, con l\u2019argomento "),Ca=l("code"),zl=o("max_length"),bl=o(", la lunghezza totale dell\u2019output testuale."),Yt=c(),b(we.$$.fragment),Jt=c(),ee=l("h2"),Ee=l("a"),Aa=l("span"),b(si.$$.fragment),wl=c(),Ta=l("span"),El=o("Utilizzo di un qualsiasi modello dell'Hub in una pipeline"),Zt=c(),U=l("p"),xl=o("Gli esempi precedenti utilizzavano il modello di default per il compito dato, ma puoi anche scegliere un modello particolare dell\u2019Hub da utilizzare in una pipeline per un compito specifico, come ad esempio la generazione testuale. Vai al "),li=l("a"),ql=o("Model Hub"),jl=o(" e clicca sull\u2019etichetta corrispondente a destra, in modo da mostrare solo i modelli supportati per il compito in questione. Dovresti ritrovarti in una pagina come "),ni=l("a"),kl=o("questa"),yl=o("."),Kt=c(),xe=l("p"),Pl=o("Proviamo il modello "),ri=l("a"),Sa=l("code"),Il=o("distilgpt2"),Cl=o("! Ecco come caricarlo nella pipeline usata in precedenza:"),Xt=c(),b(pi.$$.fragment),eo=c(),b(ci.$$.fragment),io=c(),Di=l("p"),Al=o("Puoi affinare la ricerca di un modello cliccando sulle etichette corrispondenti alle lingue, e scegliere in seguito un modello che generi testo in un\u2019altra lingua. Il Model Hub contiene anche checkpoint per modelli multilingue che supportano numerose lingue."),ao=c(),Ri=l("p"),Tl=o("Quando avrai selezionato un modello cliccando su di esso, vedrai che esiste un widget che ti permette di provarlo direttamente online. In questo modo, puoi testare velocemente le capacit\xE0 del modello prima di scaricarlo."),to=c(),b(qe.$$.fragment),oo=c(),ie=l("h3"),je=l("a"),Oa=l("span"),b(ui.$$.fragment),Sl=c(),Ha=l("span"),Ol=o("La Inference API"),so=c(),ke=l("p"),Hl=o("Tutti i modelli possono essere testati direttamente attraverso il tuo browser utilizzando l\u2019Inference API che trovi nel "),mi=l("a"),Ll=o("sito"),Nl=o(" di Hugging Face. Puoi divertirti con il modello direttamente in questa pagina, inserendo testo personalizzato e osservando come il modello processi i dati fornitigli."),lo=c(),ye=l("p"),Dl=o("La Inference API che alimenta il widget \xE8 disponibile anche come prodotto a pagamento, il che \xE8 comodo se ne hai bisogno per i tuoi flussi di lavoro. Vedi la "),di=l("a"),Rl=o("pagina dei prezzi"),Gl=o(" per maggiori informazioni."),no=c(),ae=l("h2"),Pe=l("a"),La=l("span"),b(fi.$$.fragment),Ml=c(),Na=l("span"),Fl=o("Mask filling"),ro=c(),Ie=l("p"),Ul=o("La prossima pipeline che proverai \xE8 "),Da=l("code"),Ql=o("fill-mask"),Vl=o(". L\u2019idea di questo compito \xE8 di completare gli spazi bianchi in un dato testo:"),po=c(),b(hi.$$.fragment),co=c(),b(gi.$$.fragment),uo=c(),N=l("p"),Bl=o("L\u2019argomento "),Ra=l("code"),Wl=o("top_k"),Yl=o(" gestisce il numero di possibilit\xE0 che vuoi mostrare. Nota che qui il modello inserisce la "),Ga=l("code"),Jl=o("<mask>"),Zl=o(" word speciale, la quale viene spesso chiamata "),Ma=l("em"),Kl=o("mask token"),Xl=o(". Altri modelli di tipo mask-filling potrebbero avere mask token diversi, quindi \xE8 sempre bene verificare quale sia la corretta mask word quando esploriamo nuovi modelli. Un modo per verificarla consiste nel trovare la mask word utilizzata nel widget."),mo=c(),b(Ce.$$.fragment),fo=c(),te=l("h2"),Ae=l("a"),Fa=l("span"),b(vi.$$.fragment),en=c(),Ua=l("span"),an=o("Riconoscimento delle entit\xE0 nominate"),ho=c(),Te=l("p"),tn=o("Il riconoscimento delle entit\xE0 nominate ("),Qa=l("em"),on=o("Named entity recognition"),sn=o(", NER) \xE8 un compito in cui il modello deve determinare quali parti dell\u2019input testuale corrispondono a entit\xE0 quali persone, localit\xE0, o organizzazioni. Guardiamo a un esempio:"),go=c(),b($i.$$.fragment),vo=c(),b(_i.$$.fragment),$o=c(),Gi=l("p"),ln=o("Qui il modello ha correttamente identificato che Sylvain \xE8 una persona (PER), Hugging Face un\u2019organizzazione (ORG), e Brooklyn una localit\xE0 (LOC)."),_o=c(),O=l("p"),nn=o("Passiamo l\u2019opzione "),Va=l("code"),rn=o("grouped_entities=True"),pn=o(" nella funzione di creazione della pipeline per raggruppare le parti frasali che corrispondono alla stessa entit\xE0: qui il modello raggruppa correttamente \u201CHugging\u201D e \u201CFace\u201D come singola organizzazione, nonostante il nome sia formato da pi\xF9 parole. A dire il vero, come vedremo nel prossimo capitolo, il preprocessing divide perfino alcune parole in parti pi\xF9 piccole. Ad esempio, "),Ba=l("code"),cn=o("Sylvain"),un=o(" viene suddiviso in quattro parti: "),Wa=l("code"),mn=o("S"),dn=o(", "),Ya=l("code"),fn=o("##yl"),hn=o(", "),Ja=l("code"),gn=o("##va"),vn=o(", and "),Za=l("code"),$n=o("##in"),_n=o(". Al momento del post-processing, la pipeline raggruppa le parti con successo."),zo=c(),b(Se.$$.fragment),bo=c(),oe=l("h2"),Oe=l("a"),Ka=l("span"),b(zi.$$.fragment),zn=c(),Xa=l("span"),bn=o("Risposta a domande"),wo=c(),He=l("p"),wn=o("La pipeline "),et=l("code"),En=o("question-answering"),xn=o(" risponde a domande utilizzando informazioni da un contesto prestabilito:"),Eo=c(),b(bi.$$.fragment),xo=c(),b(wi.$$.fragment),qo=c(),Mi=l("p"),qn=o("Nota che questa pipeline non genera risposte ma estrae informazioni da un contesto fornito."),jo=c(),se=l("h2"),Le=l("a"),it=l("span"),b(Ei.$$.fragment),jn=c(),at=l("span"),kn=o("Riassunto"),ko=c(),Fi=l("p"),yn=o("Quello del riassunto \xE8 un compito che trasforma un testo in un testo pi\xF9 breve, conservando tutti (o quasi) gli argomenti pi\xF9 importanti del testo di partenza. Ecco un esempio:"),yo=c(),b(xi.$$.fragment),Po=c(),b(qi.$$.fragment),Io=c(),Q=l("p"),Pn=o("Come nella generazione di testi, puoi specificare un "),tt=l("code"),In=o("max_length"),Cn=o(" o "),ot=l("code"),An=o("min_length"),Tn=o(" per il testo da generare."),Co=c(),le=l("h2"),Ne=l("a"),st=l("span"),b(ji.$$.fragment),Sn=c(),lt=l("span"),On=o("Traduzione"),Ao=c(),V=l("p"),Hn=o("Per compiti di traduzione, puoi utilizzare un modello di default indicando la coppia linguistica nel nome del compito (come ad esempio "),nt=l("code"),Ln=o('"translation_en_to_fr"'),Nn=o("), anche se il metodo pi\xF9 semplice \xE8 di scegliere il modello che desideri utilizzare dal "),ki=l("a"),Dn=o("Model Hub"),Rn=o(". Qui in seguito traduciamo dal francese all\u2019inglese:"),To=c(),b(yi.$$.fragment),So=c(),b(Pi.$$.fragment),Oo=c(),B=l("p"),Gn=o("Come per le funzioni di generazione testuale e riassunto, \xE8 possibile specificare un "),rt=l("code"),Mn=o("max_length"),Fn=o(" o un "),pt=l("code"),Un=o("min_length"),Qn=o(" per il risultato."),Ho=c(),b(De.$$.fragment),Lo=c(),Re=l("p"),Vn=o("Finora abbiamo mostrato pipeline a solo scopo dimostrativo. Tali pipeline sono state programmate per compiti ben specifici e non sono in grado di eseguire variazioni di questi ultimi. Nel prossimo capitolo, imparerai cosa si nasconde dentro la funzione "),ct=l("code"),Bn=o("pipeline()"),Wn=o(" e come personalizzarne il comportamento."),this.h()},l(e){const t=Fp('[data-svelte="svelte-1phssyn"]',document.head);m=n(t,"META",{name:!0,content:!0}),t.forEach(i),_=u(e),f=n(e,"H1",{class:!0});var Ii=r(f);z=n(Ii,"A",{id:!0,class:!0,href:!0});var ut=r(z);$=n(ut,"SPAN",{});var mt=r($);w(h.$$.fragment,mt),mt.forEach(i),ut.forEach(i),g=u(Ii),v=n(Ii,"SPAN",{});var dt=r(v);P=s(dt,"Cosa fanno i Transformer?"),dt.forEach(i),Ii.forEach(i),y=u(e),w(I.$$.fragment,e),A=u(e),k=n(e,"P",{});var Ci=r(k);W=s(Ci,"In questa sezione, vedremo di cosa sono capaci i modelli Transformer e useremo il nostro primo strumento della libreria \u{1F917} Transformer: la funzione "),R=n(Ci,"CODE",{});var ft=r(R);Y=s(ft,"pipeline()"),ft.forEach(i),rs=s(Ci,"."),Ci.forEach(i),vt=u(e),w(ne.$$.fragment,e),$t=u(e),J=n(e,"H2",{class:!0});var Ai=r(J);re=n(Ai,"A",{id:!0,class:!0,href:!0});var ht=r(re);Zi=n(ht,"SPAN",{});var Zn=r(Zi);w(Fe.$$.fragment,Zn),Zn.forEach(i),ht.forEach(i),ps=u(Ai),Ki=n(Ai,"SPAN",{});var Kn=r(Ki);cs=s(Kn,"I Transformer sono ovunque!"),Kn.forEach(i),Ai.forEach(i),_t=u(e),Ti=n(e,"P",{});var Xn=r(Ti);us=s(Xn,"I modelli Transformer sono utilizzati per eseguire qualsiasi compito di NLP, come ad esempio quelli menzionati nelle sezioni precedenti. Ecco alcune delle aziende e organizzazioni che utilizzano Hugging Face e i modelli Transformer, e contribuiscono a loro volta alla comunit\xE0 condividendo i propri modelli:"),Xn.forEach(i),zt=u(e),pe=n(e,"IMG",{src:!0,alt:!0,width:!0}),bt=u(e),G=n(e,"P",{});var Ui=r(G);ms=s(Ui,"La "),Ue=n(Ui,"A",{href:!0,rel:!0});var er=r(Ue);ds=s(er,"libreria \u{1F917} Transformer"),er.forEach(i),fs=s(Ui," fornisce la funzionalit\xE0 per creare e utilizzare questi modelli condivisi. Il "),Qe=n(Ui,"A",{href:!0,rel:!0});var ir=r(Qe);hs=s(ir,"Model Hub"),ir.forEach(i),gs=s(Ui," contiene migliaia di modelli pre-addestrati che possono essere scaricati e usati liberamente. Puoi anche caricare i tuoi modelli nell\u2019Hub!"),Ui.forEach(i),wt=u(e),w(ce.$$.fragment,e),Et=u(e),Si=n(e,"P",{});var ar=r(Si);vs=s(ar,"Prima di scoprire come funzionino i modelli Transformer dietro le quinte, vediamo qualche esempio di come questi possano essere utilizzati per risolvere alcuni problemi interessanti di NLP."),ar.forEach(i),xt=u(e),Z=n(e,"H2",{class:!0});var Do=r(Z);ue=n(Do,"A",{id:!0,class:!0,href:!0});var tr=r(ue);Xi=n(tr,"SPAN",{});var or=r(Xi);w(Ve.$$.fragment,or),or.forEach(i),tr.forEach(i),$s=u(Do),ea=n(Do,"SPAN",{});var sr=r(ea);_s=s(sr,"Lavorare con le pipeline"),sr.forEach(i),Do.forEach(i),qt=u(e),w(Be.$$.fragment,e),jt=u(e),me=n(e,"P",{});var Ro=r(me);zs=s(Ro,"L\u2019oggetto pi\xF9 basilare della libreria \u{1F917} Transformer \xE8 la funzione "),ia=n(Ro,"CODE",{});var lr=r(ia);bs=s(lr,"pipeline()"),lr.forEach(i),ws=s(Ro,". Questa connette un modello con tutte le fasi necessarie di preprocessing e postprocessing, permettendoci cos\xEC di fornire un qualsiasi testo come input diretto e ottenere una risposta intelligibile:"),Ro.forEach(i),kt=u(e),w(We.$$.fragment,e),yt=u(e),w(Ye.$$.fragment,e),Pt=u(e),Oi=n(e,"P",{});var nr=r(Oi);Es=s(nr,"\xC8 anche possibile lavorare su pi\xF9 frasi!"),nr.forEach(i),It=u(e),w(Je.$$.fragment,e),Ct=u(e),w(Ze.$$.fragment,e),At=u(e),de=n(e,"P",{});var Go=r(de);xs=s(Go,"Per default, questa pipeline seleziona un preciso modello pre-addestrato che \xE8 stato affinato per il sentiment analysis in inglese. Quando creiamo l\u2019oggetto "),aa=n(Go,"CODE",{});var rr=r(aa);qs=s(rr,"classifier"),rr.forEach(i),js=s(Go,", il modello viene scaricato e memorizzato nella cache. Se inizializziamo di nuovo il comando, verr\xE0 utilizzato il modello salvato nella cache e non ci sar\xE0 quindi bisogno di scaricare di nuovo il modello."),Go.forEach(i),Tt=u(e),Hi=n(e,"P",{});var pr=r(Hi);ks=s(pr,"Tre passaggi principali sono coinvolti quando passiamo del testo in un pipeline:"),pr.forEach(i),St=u(e),M=n(e,"OL",{});var Qi=r(M);ta=n(Qi,"LI",{});var cr=r(ta);ys=s(cr,"Il testo \xE8 pre-elaborato in un formato che il modello pu\xF2 capire."),cr.forEach(i),Ps=u(Qi),oa=n(Qi,"LI",{});var ur=r(oa);Is=s(ur,"Gli input pre-elaborati vengono passati al modello."),ur.forEach(i),Cs=u(Qi),sa=n(Qi,"LI",{});var mr=r(sa);As=s(mr,"Le previsioni del modello sono post-elaborate in un formato accessibile all\u2019utilizzatore."),mr.forEach(i),Qi.forEach(i),Ot=u(e),fe=n(e,"P",{});var Mo=r(fe);Ts=s(Mo,"Tra le "),Ke=n(Mo,"A",{href:!0,rel:!0});var dr=r(Ke);Ss=s(dr,"pipeline disponibili"),dr.forEach(i),Os=s(Mo," al momento ci sono:"),Mo.forEach(i),Ht=u(e),C=n(e,"UL",{});var H=r(C);Li=n(H,"LI",{});var Yn=r(Li);la=n(Yn,"CODE",{});var fr=r(la);Hs=s(fr,"feature-extraction"),fr.forEach(i),Ls=s(Yn," (per ottenere la rappresentazione vettoriale di un testo)"),Yn.forEach(i),Ns=u(H),na=n(H,"LI",{});var hr=r(na);ra=n(hr,"CODE",{});var gr=r(ra);Ds=s(gr,"fill-mask"),gr.forEach(i),hr.forEach(i),Rs=u(H),he=n(H,"LI",{});var gt=r(he);pa=n(gt,"CODE",{});var vr=r(pa);Gs=s(vr,"ner"),vr.forEach(i),Ms=s(gt," (riconoscimento delle entit\xE0 nominate, "),ca=n(gt,"EM",{});var $r=r(ca);Fs=s($r,"named entity recognition"),$r.forEach(i),Us=s(gt,")"),gt.forEach(i),Qs=u(H),ua=n(H,"LI",{});var _r=r(ua);ma=n(_r,"CODE",{});var zr=r(ma);Vs=s(zr,"question-answering"),zr.forEach(i),_r.forEach(i),Bs=u(H),da=n(H,"LI",{});var br=r(da);fa=n(br,"CODE",{});var wr=r(fa);Ws=s(wr,"sentiment-analysis"),wr.forEach(i),br.forEach(i),Ys=u(H),ha=n(H,"LI",{});var Er=r(ha);ga=n(Er,"CODE",{});var xr=r(ga);Js=s(xr,"summarization"),xr.forEach(i),Er.forEach(i),Zs=u(H),va=n(H,"LI",{});var qr=r(va);$a=n(qr,"CODE",{});var jr=r($a);Ks=s(jr,"text-generation"),jr.forEach(i),qr.forEach(i),Xs=u(H),_a=n(H,"LI",{});var kr=r(_a);za=n(kr,"CODE",{});var yr=r(za);el=s(yr,"translation"),yr.forEach(i),kr.forEach(i),il=u(H),ba=n(H,"LI",{});var Pr=r(ba);wa=n(Pr,"CODE",{});var Ir=r(wa);al=s(Ir,"zero-shot-classification"),Ir.forEach(i),Pr.forEach(i),H.forEach(i),Lt=u(e),Ni=n(e,"P",{});var Cr=r(Ni);tl=s(Cr,"Proviamo a vederne alcune!"),Cr.forEach(i),Nt=u(e),K=n(e,"H2",{class:!0});var Fo=r(K);ge=n(Fo,"A",{id:!0,class:!0,href:!0});var Ar=r(ge);Ea=n(Ar,"SPAN",{});var Tr=r(Ea);w(Xe.$$.fragment,Tr),Tr.forEach(i),Ar.forEach(i),ol=u(Fo),xa=n(Fo,"SPAN",{});var Sr=r(xa);sl=s(Sr,"Classificazione zero-shot"),Sr.forEach(i),Fo.forEach(i),Dt=u(e),ve=n(e,"P",{});var Uo=r(ve);ll=s(Uo,"Cominceremo con l\u2019affrontare un compito impegnativo che consiste nella classificazione di testi non etichettati. Si tratta di uno scenario comune in molti progetti pratici perch\xE9 l\u2019annotazione testuale richiede tempo e competenza settoriale. In questo caso d\u2019uso, la pipeline "),qa=n(Uo,"CODE",{});var Or=r(qa);nl=s(Or,"zero-shot-classification"),Or.forEach(i),rl=s(Uo," \xE8 molto potente e permette di specificare le etichette da utilizzare per la classificazione, in modo da non dover fare affidamento sulle etichette del modello pre-addestrato. Abbiamo gi\xE0 visto come il modello riesca a classificare una frase utilizzando le etichette \u2018positiva\u2019 e \u2018negativa\u2019, ma \xE8 anche possibile classificare testi utilizzando una qualsiasi serie di etichette di nostra scelta."),Uo.forEach(i),Rt=u(e),w(ei.$$.fragment,e),Gt=u(e),w(ii.$$.fragment,e),Mt=u(e),$e=n(e,"P",{});var Qo=r($e);pl=s(Qo,"Questa pipeline si chiama "),ja=n(Qo,"EM",{});var Hr=r(ja);cl=s(Hr,"zero-shot"),Hr.forEach(i),ul=s(Qo," perch\xE9 non hai bisogno di affinare il modello usando i tuoi dati per poterlo utilizzare. \xC8 direttamente in grado di generare una previsione probabilistica per qualsiasi lista di etichette tu voglia!"),Qo.forEach(i),Ft=u(e),w(_e.$$.fragment,e),Ut=u(e),X=n(e,"H2",{class:!0});var Vo=r(X);ze=n(Vo,"A",{id:!0,class:!0,href:!0});var Lr=r(ze);ka=n(Lr,"SPAN",{});var Nr=r(ka);w(ai.$$.fragment,Nr),Nr.forEach(i),Lr.forEach(i),ml=u(Vo),ya=n(Vo,"SPAN",{});var Dr=r(ya);dl=s(Dr,"Generazione di testi"),Dr.forEach(i),Vo.forEach(i),Qt=u(e),be=n(e,"P",{});var Bo=r(be);fl=s(Bo,"Vediamo ora come utilizzare la pipeline per generare testi. L\u2019idea \xE8 di fornire un prompt ("),Pa=n(Bo,"EM",{});var Rr=r(Pa);hl=s(Rr,"richiesta"),Rr.forEach(i),gl=s(Bo,") che verr\xE0 auto-completato dal modello, il quale generer\xE0 il testo mancante. Si tratta di un compito simile alla funzione di scrittura facilitata che troviamo al giorno d\u2019oggi in molti cellulari. La generazione di testi presenta una componente arbitraria, per cui non essere sorpreso/a se non ottieni gli stessi risultati che mostriamo qui sotto."),Bo.forEach(i),Vt=u(e),w(ti.$$.fragment,e),Bt=u(e),w(oi.$$.fragment,e),Wt=u(e),F=n(e,"P",{});var Vi=r(F);vl=s(Vi,"Usando l\u2019argomento "),Ia=n(Vi,"CODE",{});var Gr=r(Ia);$l=s(Gr,"num_return_sequences"),Gr.forEach(i),_l=s(Vi," puoi controllare quante sequenze diverse vengono generate e, con l\u2019argomento "),Ca=n(Vi,"CODE",{});var Mr=r(Ca);zl=s(Mr,"max_length"),Mr.forEach(i),bl=s(Vi,", la lunghezza totale dell\u2019output testuale."),Vi.forEach(i),Yt=u(e),w(we.$$.fragment,e),Jt=u(e),ee=n(e,"H2",{class:!0});var Wo=r(ee);Ee=n(Wo,"A",{id:!0,class:!0,href:!0});var Fr=r(Ee);Aa=n(Fr,"SPAN",{});var Ur=r(Aa);w(si.$$.fragment,Ur),Ur.forEach(i),Fr.forEach(i),wl=u(Wo),Ta=n(Wo,"SPAN",{});var Qr=r(Ta);El=s(Qr,"Utilizzo di un qualsiasi modello dell'Hub in una pipeline"),Qr.forEach(i),Wo.forEach(i),Zt=u(e),U=n(e,"P",{});var Bi=r(U);xl=s(Bi,"Gli esempi precedenti utilizzavano il modello di default per il compito dato, ma puoi anche scegliere un modello particolare dell\u2019Hub da utilizzare in una pipeline per un compito specifico, come ad esempio la generazione testuale. Vai al "),li=n(Bi,"A",{href:!0,rel:!0});var Vr=r(li);ql=s(Vr,"Model Hub"),Vr.forEach(i),jl=s(Bi," e clicca sull\u2019etichetta corrispondente a destra, in modo da mostrare solo i modelli supportati per il compito in questione. Dovresti ritrovarti in una pagina come "),ni=n(Bi,"A",{href:!0,rel:!0});var Br=r(ni);kl=s(Br,"questa"),Br.forEach(i),yl=s(Bi,"."),Bi.forEach(i),Kt=u(e),xe=n(e,"P",{});var Yo=r(xe);Pl=s(Yo,"Proviamo il modello "),ri=n(Yo,"A",{href:!0,rel:!0});var Wr=r(ri);Sa=n(Wr,"CODE",{});var Yr=r(Sa);Il=s(Yr,"distilgpt2"),Yr.forEach(i),Wr.forEach(i),Cl=s(Yo,"! Ecco come caricarlo nella pipeline usata in precedenza:"),Yo.forEach(i),Xt=u(e),w(pi.$$.fragment,e),eo=u(e),w(ci.$$.fragment,e),io=u(e),Di=n(e,"P",{});var Jr=r(Di);Al=s(Jr,"Puoi affinare la ricerca di un modello cliccando sulle etichette corrispondenti alle lingue, e scegliere in seguito un modello che generi testo in un\u2019altra lingua. Il Model Hub contiene anche checkpoint per modelli multilingue che supportano numerose lingue."),Jr.forEach(i),ao=u(e),Ri=n(e,"P",{});var Zr=r(Ri);Tl=s(Zr,"Quando avrai selezionato un modello cliccando su di esso, vedrai che esiste un widget che ti permette di provarlo direttamente online. In questo modo, puoi testare velocemente le capacit\xE0 del modello prima di scaricarlo."),Zr.forEach(i),to=u(e),w(qe.$$.fragment,e),oo=u(e),ie=n(e,"H3",{class:!0});var Jo=r(ie);je=n(Jo,"A",{id:!0,class:!0,href:!0});var Kr=r(je);Oa=n(Kr,"SPAN",{});var Xr=r(Oa);w(ui.$$.fragment,Xr),Xr.forEach(i),Kr.forEach(i),Sl=u(Jo),Ha=n(Jo,"SPAN",{});var ep=r(Ha);Ol=s(ep,"La Inference API"),ep.forEach(i),Jo.forEach(i),so=u(e),ke=n(e,"P",{});var Zo=r(ke);Hl=s(Zo,"Tutti i modelli possono essere testati direttamente attraverso il tuo browser utilizzando l\u2019Inference API che trovi nel "),mi=n(Zo,"A",{href:!0,rel:!0});var ip=r(mi);Ll=s(ip,"sito"),ip.forEach(i),Nl=s(Zo," di Hugging Face. Puoi divertirti con il modello direttamente in questa pagina, inserendo testo personalizzato e osservando come il modello processi i dati fornitigli."),Zo.forEach(i),lo=u(e),ye=n(e,"P",{});var Ko=r(ye);Dl=s(Ko,"La Inference API che alimenta il widget \xE8 disponibile anche come prodotto a pagamento, il che \xE8 comodo se ne hai bisogno per i tuoi flussi di lavoro. Vedi la "),di=n(Ko,"A",{href:!0,rel:!0});var ap=r(di);Rl=s(ap,"pagina dei prezzi"),ap.forEach(i),Gl=s(Ko," per maggiori informazioni."),Ko.forEach(i),no=u(e),ae=n(e,"H2",{class:!0});var Xo=r(ae);Pe=n(Xo,"A",{id:!0,class:!0,href:!0});var tp=r(Pe);La=n(tp,"SPAN",{});var op=r(La);w(fi.$$.fragment,op),op.forEach(i),tp.forEach(i),Ml=u(Xo),Na=n(Xo,"SPAN",{});var sp=r(Na);Fl=s(sp,"Mask filling"),sp.forEach(i),Xo.forEach(i),ro=u(e),Ie=n(e,"P",{});var es=r(Ie);Ul=s(es,"La prossima pipeline che proverai \xE8 "),Da=n(es,"CODE",{});var lp=r(Da);Ql=s(lp,"fill-mask"),lp.forEach(i),Vl=s(es,". L\u2019idea di questo compito \xE8 di completare gli spazi bianchi in un dato testo:"),es.forEach(i),po=u(e),w(hi.$$.fragment,e),co=u(e),w(gi.$$.fragment,e),uo=u(e),N=n(e,"P",{});var Ge=r(N);Bl=s(Ge,"L\u2019argomento "),Ra=n(Ge,"CODE",{});var np=r(Ra);Wl=s(np,"top_k"),np.forEach(i),Yl=s(Ge," gestisce il numero di possibilit\xE0 che vuoi mostrare. Nota che qui il modello inserisce la "),Ga=n(Ge,"CODE",{});var rp=r(Ga);Jl=s(rp,"<mask>"),rp.forEach(i),Zl=s(Ge," word speciale, la quale viene spesso chiamata "),Ma=n(Ge,"EM",{});var pp=r(Ma);Kl=s(pp,"mask token"),pp.forEach(i),Xl=s(Ge,". Altri modelli di tipo mask-filling potrebbero avere mask token diversi, quindi \xE8 sempre bene verificare quale sia la corretta mask word quando esploriamo nuovi modelli. Un modo per verificarla consiste nel trovare la mask word utilizzata nel widget."),Ge.forEach(i),mo=u(e),w(Ce.$$.fragment,e),fo=u(e),te=n(e,"H2",{class:!0});var is=r(te);Ae=n(is,"A",{id:!0,class:!0,href:!0});var cp=r(Ae);Fa=n(cp,"SPAN",{});var up=r(Fa);w(vi.$$.fragment,up),up.forEach(i),cp.forEach(i),en=u(is),Ua=n(is,"SPAN",{});var mp=r(Ua);an=s(mp,"Riconoscimento delle entit\xE0 nominate"),mp.forEach(i),is.forEach(i),ho=u(e),Te=n(e,"P",{});var as=r(Te);tn=s(as,"Il riconoscimento delle entit\xE0 nominate ("),Qa=n(as,"EM",{});var dp=r(Qa);on=s(dp,"Named entity recognition"),dp.forEach(i),sn=s(as,", NER) \xE8 un compito in cui il modello deve determinare quali parti dell\u2019input testuale corrispondono a entit\xE0 quali persone, localit\xE0, o organizzazioni. Guardiamo a un esempio:"),as.forEach(i),go=u(e),w($i.$$.fragment,e),vo=u(e),w(_i.$$.fragment,e),$o=u(e),Gi=n(e,"P",{});var fp=r(Gi);ln=s(fp,"Qui il modello ha correttamente identificato che Sylvain \xE8 una persona (PER), Hugging Face un\u2019organizzazione (ORG), e Brooklyn una localit\xE0 (LOC)."),fp.forEach(i),_o=u(e),O=n(e,"P",{});var L=r(O);nn=s(L,"Passiamo l\u2019opzione "),Va=n(L,"CODE",{});var hp=r(Va);rn=s(hp,"grouped_entities=True"),hp.forEach(i),pn=s(L," nella funzione di creazione della pipeline per raggruppare le parti frasali che corrispondono alla stessa entit\xE0: qui il modello raggruppa correttamente \u201CHugging\u201D e \u201CFace\u201D come singola organizzazione, nonostante il nome sia formato da pi\xF9 parole. A dire il vero, come vedremo nel prossimo capitolo, il preprocessing divide perfino alcune parole in parti pi\xF9 piccole. Ad esempio, "),Ba=n(L,"CODE",{});var gp=r(Ba);cn=s(gp,"Sylvain"),gp.forEach(i),un=s(L," viene suddiviso in quattro parti: "),Wa=n(L,"CODE",{});var vp=r(Wa);mn=s(vp,"S"),vp.forEach(i),dn=s(L,", "),Ya=n(L,"CODE",{});var $p=r(Ya);fn=s($p,"##yl"),$p.forEach(i),hn=s(L,", "),Ja=n(L,"CODE",{});var _p=r(Ja);gn=s(_p,"##va"),_p.forEach(i),vn=s(L,", and "),Za=n(L,"CODE",{});var zp=r(Za);$n=s(zp,"##in"),zp.forEach(i),_n=s(L,". Al momento del post-processing, la pipeline raggruppa le parti con successo."),L.forEach(i),zo=u(e),w(Se.$$.fragment,e),bo=u(e),oe=n(e,"H2",{class:!0});var ts=r(oe);Oe=n(ts,"A",{id:!0,class:!0,href:!0});var bp=r(Oe);Ka=n(bp,"SPAN",{});var wp=r(Ka);w(zi.$$.fragment,wp),wp.forEach(i),bp.forEach(i),zn=u(ts),Xa=n(ts,"SPAN",{});var Ep=r(Xa);bn=s(Ep,"Risposta a domande"),Ep.forEach(i),ts.forEach(i),wo=u(e),He=n(e,"P",{});var os=r(He);wn=s(os,"La pipeline "),et=n(os,"CODE",{});var xp=r(et);En=s(xp,"question-answering"),xp.forEach(i),xn=s(os," risponde a domande utilizzando informazioni da un contesto prestabilito:"),os.forEach(i),Eo=u(e),w(bi.$$.fragment,e),xo=u(e),w(wi.$$.fragment,e),qo=u(e),Mi=n(e,"P",{});var qp=r(Mi);qn=s(qp,"Nota che questa pipeline non genera risposte ma estrae informazioni da un contesto fornito."),qp.forEach(i),jo=u(e),se=n(e,"H2",{class:!0});var ss=r(se);Le=n(ss,"A",{id:!0,class:!0,href:!0});var jp=r(Le);it=n(jp,"SPAN",{});var kp=r(it);w(Ei.$$.fragment,kp),kp.forEach(i),jp.forEach(i),jn=u(ss),at=n(ss,"SPAN",{});var yp=r(at);kn=s(yp,"Riassunto"),yp.forEach(i),ss.forEach(i),ko=u(e),Fi=n(e,"P",{});var Pp=r(Fi);yn=s(Pp,"Quello del riassunto \xE8 un compito che trasforma un testo in un testo pi\xF9 breve, conservando tutti (o quasi) gli argomenti pi\xF9 importanti del testo di partenza. Ecco un esempio:"),Pp.forEach(i),yo=u(e),w(xi.$$.fragment,e),Po=u(e),w(qi.$$.fragment,e),Io=u(e),Q=n(e,"P",{});var Wi=r(Q);Pn=s(Wi,"Come nella generazione di testi, puoi specificare un "),tt=n(Wi,"CODE",{});var Ip=r(tt);In=s(Ip,"max_length"),Ip.forEach(i),Cn=s(Wi," o "),ot=n(Wi,"CODE",{});var Cp=r(ot);An=s(Cp,"min_length"),Cp.forEach(i),Tn=s(Wi," per il testo da generare."),Wi.forEach(i),Co=u(e),le=n(e,"H2",{class:!0});var ls=r(le);Ne=n(ls,"A",{id:!0,class:!0,href:!0});var Ap=r(Ne);st=n(Ap,"SPAN",{});var Tp=r(st);w(ji.$$.fragment,Tp),Tp.forEach(i),Ap.forEach(i),Sn=u(ls),lt=n(ls,"SPAN",{});var Sp=r(lt);On=s(Sp,"Traduzione"),Sp.forEach(i),ls.forEach(i),Ao=u(e),V=n(e,"P",{});var Yi=r(V);Hn=s(Yi,"Per compiti di traduzione, puoi utilizzare un modello di default indicando la coppia linguistica nel nome del compito (come ad esempio "),nt=n(Yi,"CODE",{});var Op=r(nt);Ln=s(Op,'"translation_en_to_fr"'),Op.forEach(i),Nn=s(Yi,"), anche se il metodo pi\xF9 semplice \xE8 di scegliere il modello che desideri utilizzare dal "),ki=n(Yi,"A",{href:!0,rel:!0});var Hp=r(ki);Dn=s(Hp,"Model Hub"),Hp.forEach(i),Rn=s(Yi,". Qui in seguito traduciamo dal francese all\u2019inglese:"),Yi.forEach(i),To=u(e),w(yi.$$.fragment,e),So=u(e),w(Pi.$$.fragment,e),Oo=u(e),B=n(e,"P",{});var Ji=r(B);Gn=s(Ji,"Come per le funzioni di generazione testuale e riassunto, \xE8 possibile specificare un "),rt=n(Ji,"CODE",{});var Lp=r(rt);Mn=s(Lp,"max_length"),Lp.forEach(i),Fn=s(Ji," o un "),pt=n(Ji,"CODE",{});var Np=r(pt);Un=s(Np,"min_length"),Np.forEach(i),Qn=s(Ji," per il risultato."),Ji.forEach(i),Ho=u(e),w(De.$$.fragment,e),Lo=u(e),Re=n(e,"P",{});var ns=r(Re);Vn=s(ns,"Finora abbiamo mostrato pipeline a solo scopo dimostrativo. Tali pipeline sono state programmate per compiti ben specifici e non sono in grado di eseguire variazioni di questi ultimi. Nel prossimo capitolo, imparerai cosa si nasconde dentro la funzione "),ct=n(ns,"CODE",{});var Dp=r(ct);Bn=s(Dp,"pipeline()"),Dp.forEach(i),Wn=s(ns," e come personalizzarne il comportamento."),ns.forEach(i),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(tc)),d(z,"id","cosa-fanno-i-transformer"),d(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(z,"href","#cosa-fanno-i-transformer"),d(f,"class","relative group"),d(re,"id","i-transformer-sono-ovunque"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#i-transformer-sono-ovunque"),d(J,"class","relative group"),Up(pe.src,Jn="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG")||d(pe,"src",Jn),d(pe,"alt","Companies using Hugging Face"),d(pe,"width","100%"),d(Ue,"href","https://github.com/huggingface/transformers"),d(Ue,"rel","nofollow"),d(Qe,"href","https://huggingface.co/models"),d(Qe,"rel","nofollow"),d(ue,"id","lavorare-con-le-pipeline"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#lavorare-con-le-pipeline"),d(Z,"class","relative group"),d(Ke,"href","https://huggingface.co/transformers/main_classes/pipelines.html"),d(Ke,"rel","nofollow"),d(ge,"id","classificazione-zeroshot"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#classificazione-zeroshot"),d(K,"class","relative group"),d(ze,"id","generazione-di-testi"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#generazione-di-testi"),d(X,"class","relative group"),d(Ee,"id","utilizzo-di-un-qualsiasi-modello-dellhub-in-una-pipeline"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#utilizzo-di-un-qualsiasi-modello-dellhub-in-una-pipeline"),d(ee,"class","relative group"),d(li,"href","https://huggingface.co/models"),d(li,"rel","nofollow"),d(ni,"href","https://huggingface.co/models?pipeline_tag=text-generation"),d(ni,"rel","nofollow"),d(ri,"href","https://huggingface.co/distilgpt2"),d(ri,"rel","nofollow"),d(je,"id","la-inference-api"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#la-inference-api"),d(ie,"class","relative group"),d(mi,"href","https://huggingface.co/"),d(mi,"rel","nofollow"),d(di,"href","https://huggingface.co/pricing"),d(di,"rel","nofollow"),d(Pe,"id","mask-filling"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#mask-filling"),d(ae,"class","relative group"),d(Ae,"id","riconoscimento-delle-entit-nominate"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#riconoscimento-delle-entit-nominate"),d(te,"class","relative group"),d(Oe,"id","risposta-a-domande"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#risposta-a-domande"),d(oe,"class","relative group"),d(Le,"id","riassunto"),d(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Le,"href","#riassunto"),d(se,"class","relative group"),d(Ne,"id","traduzione"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#traduzione"),d(le,"class","relative group"),d(ki,"href","https://huggingface.co/models"),d(ki,"rel","nofollow")},m(e,t){a(document.head,m),p(e,_,t),p(e,f,t),a(f,z),a(z,$),E(h,$,null),a(f,g),a(f,v),a(v,P),p(e,y,t),E(I,e,t),p(e,A,t),p(e,k,t),a(k,W),a(k,R),a(R,Y),a(k,rs),p(e,vt,t),E(ne,e,t),p(e,$t,t),p(e,J,t),a(J,re),a(re,Zi),E(Fe,Zi,null),a(J,ps),a(J,Ki),a(Ki,cs),p(e,_t,t),p(e,Ti,t),a(Ti,us),p(e,zt,t),p(e,pe,t),p(e,bt,t),p(e,G,t),a(G,ms),a(G,Ue),a(Ue,ds),a(G,fs),a(G,Qe),a(Qe,hs),a(G,gs),p(e,wt,t),E(ce,e,t),p(e,Et,t),p(e,Si,t),a(Si,vs),p(e,xt,t),p(e,Z,t),a(Z,ue),a(ue,Xi),E(Ve,Xi,null),a(Z,$s),a(Z,ea),a(ea,_s),p(e,qt,t),E(Be,e,t),p(e,jt,t),p(e,me,t),a(me,zs),a(me,ia),a(ia,bs),a(me,ws),p(e,kt,t),E(We,e,t),p(e,yt,t),E(Ye,e,t),p(e,Pt,t),p(e,Oi,t),a(Oi,Es),p(e,It,t),E(Je,e,t),p(e,Ct,t),E(Ze,e,t),p(e,At,t),p(e,de,t),a(de,xs),a(de,aa),a(aa,qs),a(de,js),p(e,Tt,t),p(e,Hi,t),a(Hi,ks),p(e,St,t),p(e,M,t),a(M,ta),a(ta,ys),a(M,Ps),a(M,oa),a(oa,Is),a(M,Cs),a(M,sa),a(sa,As),p(e,Ot,t),p(e,fe,t),a(fe,Ts),a(fe,Ke),a(Ke,Ss),a(fe,Os),p(e,Ht,t),p(e,C,t),a(C,Li),a(Li,la),a(la,Hs),a(Li,Ls),a(C,Ns),a(C,na),a(na,ra),a(ra,Ds),a(C,Rs),a(C,he),a(he,pa),a(pa,Gs),a(he,Ms),a(he,ca),a(ca,Fs),a(he,Us),a(C,Qs),a(C,ua),a(ua,ma),a(ma,Vs),a(C,Bs),a(C,da),a(da,fa),a(fa,Ws),a(C,Ys),a(C,ha),a(ha,ga),a(ga,Js),a(C,Zs),a(C,va),a(va,$a),a($a,Ks),a(C,Xs),a(C,_a),a(_a,za),a(za,el),a(C,il),a(C,ba),a(ba,wa),a(wa,al),p(e,Lt,t),p(e,Ni,t),a(Ni,tl),p(e,Nt,t),p(e,K,t),a(K,ge),a(ge,Ea),E(Xe,Ea,null),a(K,ol),a(K,xa),a(xa,sl),p(e,Dt,t),p(e,ve,t),a(ve,ll),a(ve,qa),a(qa,nl),a(ve,rl),p(e,Rt,t),E(ei,e,t),p(e,Gt,t),E(ii,e,t),p(e,Mt,t),p(e,$e,t),a($e,pl),a($e,ja),a(ja,cl),a($e,ul),p(e,Ft,t),E(_e,e,t),p(e,Ut,t),p(e,X,t),a(X,ze),a(ze,ka),E(ai,ka,null),a(X,ml),a(X,ya),a(ya,dl),p(e,Qt,t),p(e,be,t),a(be,fl),a(be,Pa),a(Pa,hl),a(be,gl),p(e,Vt,t),E(ti,e,t),p(e,Bt,t),E(oi,e,t),p(e,Wt,t),p(e,F,t),a(F,vl),a(F,Ia),a(Ia,$l),a(F,_l),a(F,Ca),a(Ca,zl),a(F,bl),p(e,Yt,t),E(we,e,t),p(e,Jt,t),p(e,ee,t),a(ee,Ee),a(Ee,Aa),E(si,Aa,null),a(ee,wl),a(ee,Ta),a(Ta,El),p(e,Zt,t),p(e,U,t),a(U,xl),a(U,li),a(li,ql),a(U,jl),a(U,ni),a(ni,kl),a(U,yl),p(e,Kt,t),p(e,xe,t),a(xe,Pl),a(xe,ri),a(ri,Sa),a(Sa,Il),a(xe,Cl),p(e,Xt,t),E(pi,e,t),p(e,eo,t),E(ci,e,t),p(e,io,t),p(e,Di,t),a(Di,Al),p(e,ao,t),p(e,Ri,t),a(Ri,Tl),p(e,to,t),E(qe,e,t),p(e,oo,t),p(e,ie,t),a(ie,je),a(je,Oa),E(ui,Oa,null),a(ie,Sl),a(ie,Ha),a(Ha,Ol),p(e,so,t),p(e,ke,t),a(ke,Hl),a(ke,mi),a(mi,Ll),a(ke,Nl),p(e,lo,t),p(e,ye,t),a(ye,Dl),a(ye,di),a(di,Rl),a(ye,Gl),p(e,no,t),p(e,ae,t),a(ae,Pe),a(Pe,La),E(fi,La,null),a(ae,Ml),a(ae,Na),a(Na,Fl),p(e,ro,t),p(e,Ie,t),a(Ie,Ul),a(Ie,Da),a(Da,Ql),a(Ie,Vl),p(e,po,t),E(hi,e,t),p(e,co,t),E(gi,e,t),p(e,uo,t),p(e,N,t),a(N,Bl),a(N,Ra),a(Ra,Wl),a(N,Yl),a(N,Ga),a(Ga,Jl),a(N,Zl),a(N,Ma),a(Ma,Kl),a(N,Xl),p(e,mo,t),E(Ce,e,t),p(e,fo,t),p(e,te,t),a(te,Ae),a(Ae,Fa),E(vi,Fa,null),a(te,en),a(te,Ua),a(Ua,an),p(e,ho,t),p(e,Te,t),a(Te,tn),a(Te,Qa),a(Qa,on),a(Te,sn),p(e,go,t),E($i,e,t),p(e,vo,t),E(_i,e,t),p(e,$o,t),p(e,Gi,t),a(Gi,ln),p(e,_o,t),p(e,O,t),a(O,nn),a(O,Va),a(Va,rn),a(O,pn),a(O,Ba),a(Ba,cn),a(O,un),a(O,Wa),a(Wa,mn),a(O,dn),a(O,Ya),a(Ya,fn),a(O,hn),a(O,Ja),a(Ja,gn),a(O,vn),a(O,Za),a(Za,$n),a(O,_n),p(e,zo,t),E(Se,e,t),p(e,bo,t),p(e,oe,t),a(oe,Oe),a(Oe,Ka),E(zi,Ka,null),a(oe,zn),a(oe,Xa),a(Xa,bn),p(e,wo,t),p(e,He,t),a(He,wn),a(He,et),a(et,En),a(He,xn),p(e,Eo,t),E(bi,e,t),p(e,xo,t),E(wi,e,t),p(e,qo,t),p(e,Mi,t),a(Mi,qn),p(e,jo,t),p(e,se,t),a(se,Le),a(Le,it),E(Ei,it,null),a(se,jn),a(se,at),a(at,kn),p(e,ko,t),p(e,Fi,t),a(Fi,yn),p(e,yo,t),E(xi,e,t),p(e,Po,t),E(qi,e,t),p(e,Io,t),p(e,Q,t),a(Q,Pn),a(Q,tt),a(tt,In),a(Q,Cn),a(Q,ot),a(ot,An),a(Q,Tn),p(e,Co,t),p(e,le,t),a(le,Ne),a(Ne,st),E(ji,st,null),a(le,Sn),a(le,lt),a(lt,On),p(e,Ao,t),p(e,V,t),a(V,Hn),a(V,nt),a(nt,Ln),a(V,Nn),a(V,ki),a(ki,Dn),a(V,Rn),p(e,To,t),E(yi,e,t),p(e,So,t),E(Pi,e,t),p(e,Oo,t),p(e,B,t),a(B,Gn),a(B,rt),a(rt,Mn),a(B,Fn),a(B,pt),a(pt,Un),a(B,Qn),p(e,Ho,t),E(De,e,t),p(e,Lo,t),p(e,Re,t),a(Re,Vn),a(Re,ct),a(ct,Bn),a(Re,Wn),No=!0},p(e,[t]){const Ii={};t&2&&(Ii.$$scope={dirty:t,ctx:e}),ne.$set(Ii);const ut={};t&2&&(ut.$$scope={dirty:t,ctx:e}),ce.$set(ut);const mt={};t&2&&(mt.$$scope={dirty:t,ctx:e}),_e.$set(mt);const dt={};t&2&&(dt.$$scope={dirty:t,ctx:e}),we.$set(dt);const Ci={};t&2&&(Ci.$$scope={dirty:t,ctx:e}),qe.$set(Ci);const ft={};t&2&&(ft.$$scope={dirty:t,ctx:e}),Ce.$set(ft);const Ai={};t&2&&(Ai.$$scope={dirty:t,ctx:e}),Se.$set(Ai);const ht={};t&2&&(ht.$$scope={dirty:t,ctx:e}),De.$set(ht)},i(e){No||(x(h.$$.fragment,e),x(I.$$.fragment,e),x(ne.$$.fragment,e),x(Fe.$$.fragment,e),x(ce.$$.fragment,e),x(Ve.$$.fragment,e),x(Be.$$.fragment,e),x(We.$$.fragment,e),x(Ye.$$.fragment,e),x(Je.$$.fragment,e),x(Ze.$$.fragment,e),x(Xe.$$.fragment,e),x(ei.$$.fragment,e),x(ii.$$.fragment,e),x(_e.$$.fragment,e),x(ai.$$.fragment,e),x(ti.$$.fragment,e),x(oi.$$.fragment,e),x(we.$$.fragment,e),x(si.$$.fragment,e),x(pi.$$.fragment,e),x(ci.$$.fragment,e),x(qe.$$.fragment,e),x(ui.$$.fragment,e),x(fi.$$.fragment,e),x(hi.$$.fragment,e),x(gi.$$.fragment,e),x(Ce.$$.fragment,e),x(vi.$$.fragment,e),x($i.$$.fragment,e),x(_i.$$.fragment,e),x(Se.$$.fragment,e),x(zi.$$.fragment,e),x(bi.$$.fragment,e),x(wi.$$.fragment,e),x(Ei.$$.fragment,e),x(xi.$$.fragment,e),x(qi.$$.fragment,e),x(ji.$$.fragment,e),x(yi.$$.fragment,e),x(Pi.$$.fragment,e),x(De.$$.fragment,e),No=!0)},o(e){q(h.$$.fragment,e),q(I.$$.fragment,e),q(ne.$$.fragment,e),q(Fe.$$.fragment,e),q(ce.$$.fragment,e),q(Ve.$$.fragment,e),q(Be.$$.fragment,e),q(We.$$.fragment,e),q(Ye.$$.fragment,e),q(Je.$$.fragment,e),q(Ze.$$.fragment,e),q(Xe.$$.fragment,e),q(ei.$$.fragment,e),q(ii.$$.fragment,e),q(_e.$$.fragment,e),q(ai.$$.fragment,e),q(ti.$$.fragment,e),q(oi.$$.fragment,e),q(we.$$.fragment,e),q(si.$$.fragment,e),q(pi.$$.fragment,e),q(ci.$$.fragment,e),q(qe.$$.fragment,e),q(ui.$$.fragment,e),q(fi.$$.fragment,e),q(hi.$$.fragment,e),q(gi.$$.fragment,e),q(Ce.$$.fragment,e),q(vi.$$.fragment,e),q($i.$$.fragment,e),q(_i.$$.fragment,e),q(Se.$$.fragment,e),q(zi.$$.fragment,e),q(bi.$$.fragment,e),q(wi.$$.fragment,e),q(Ei.$$.fragment,e),q(xi.$$.fragment,e),q(qi.$$.fragment,e),q(ji.$$.fragment,e),q(yi.$$.fragment,e),q(Pi.$$.fragment,e),q(De.$$.fragment,e),No=!1},d(e){i(m),e&&i(_),e&&i(f),j(h),e&&i(y),j(I,e),e&&i(A),e&&i(k),e&&i(vt),j(ne,e),e&&i($t),e&&i(J),j(Fe),e&&i(_t),e&&i(Ti),e&&i(zt),e&&i(pe),e&&i(bt),e&&i(G),e&&i(wt),j(ce,e),e&&i(Et),e&&i(Si),e&&i(xt),e&&i(Z),j(Ve),e&&i(qt),j(Be,e),e&&i(jt),e&&i(me),e&&i(kt),j(We,e),e&&i(yt),j(Ye,e),e&&i(Pt),e&&i(Oi),e&&i(It),j(Je,e),e&&i(Ct),j(Ze,e),e&&i(At),e&&i(de),e&&i(Tt),e&&i(Hi),e&&i(St),e&&i(M),e&&i(Ot),e&&i(fe),e&&i(Ht),e&&i(C),e&&i(Lt),e&&i(Ni),e&&i(Nt),e&&i(K),j(Xe),e&&i(Dt),e&&i(ve),e&&i(Rt),j(ei,e),e&&i(Gt),j(ii,e),e&&i(Mt),e&&i($e),e&&i(Ft),j(_e,e),e&&i(Ut),e&&i(X),j(ai),e&&i(Qt),e&&i(be),e&&i(Vt),j(ti,e),e&&i(Bt),j(oi,e),e&&i(Wt),e&&i(F),e&&i(Yt),j(we,e),e&&i(Jt),e&&i(ee),j(si),e&&i(Zt),e&&i(U),e&&i(Kt),e&&i(xe),e&&i(Xt),j(pi,e),e&&i(eo),j(ci,e),e&&i(io),e&&i(Di),e&&i(ao),e&&i(Ri),e&&i(to),j(qe,e),e&&i(oo),e&&i(ie),j(ui),e&&i(so),e&&i(ke),e&&i(lo),e&&i(ye),e&&i(no),e&&i(ae),j(fi),e&&i(ro),e&&i(Ie),e&&i(po),j(hi,e),e&&i(co),j(gi,e),e&&i(uo),e&&i(N),e&&i(mo),j(Ce,e),e&&i(fo),e&&i(te),j(vi),e&&i(ho),e&&i(Te),e&&i(go),j($i,e),e&&i(vo),j(_i,e),e&&i($o),e&&i(Gi),e&&i(_o),e&&i(O),e&&i(zo),j(Se,e),e&&i(bo),e&&i(oe),j(zi),e&&i(wo),e&&i(He),e&&i(Eo),j(bi,e),e&&i(xo),j(wi,e),e&&i(qo),e&&i(Mi),e&&i(jo),e&&i(se),j(Ei),e&&i(ko),e&&i(Fi),e&&i(yo),j(xi,e),e&&i(Po),j(qi,e),e&&i(Io),e&&i(Q),e&&i(Co),e&&i(le),j(ji),e&&i(Ao),e&&i(V),e&&i(To),j(yi,e),e&&i(So),j(Pi,e),e&&i(Oo),e&&i(B),e&&i(Ho),j(De,e),e&&i(Lo),e&&i(Re)}}}const tc={local:"cosa-fanno-i-transformer",sections:[{local:"i-transformer-sono-ovunque",title:"I Transformer sono ovunque!"},{local:"lavorare-con-le-pipeline",title:"Lavorare con le pipeline"},{local:"classificazione-zeroshot",title:"Classificazione zero-shot"},{local:"generazione-di-testi",title:"Generazione di testi"},{local:"utilizzo-di-un-qualsiasi-modello-dellhub-in-una-pipeline",sections:[{local:"la-inference-api",title:"La Inference API"}],title:"Utilizzo di un qualsiasi modello dell'Hub in una pipeline"},{local:"mask-filling",title:"Mask filling"},{local:"riconoscimento-delle-entit-nominate",title:"Riconoscimento delle entit\xE0 nominate"},{local:"risposta-a-domande",title:"Risposta a domande"},{local:"riassunto",title:"Riassunto"},{local:"traduzione",title:"Traduzione"}],title:"Cosa fanno i Transformer?"};function oc(S){return Qp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uc extends Rp{constructor(m){super();Gp(this,m,oc,ac,Mp,{})}}export{uc as default,tc as metadata};
