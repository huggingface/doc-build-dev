import{S as Wd,i as Ud,s as Bd,e as o,t as n,k as m,w as x,c as l,a as r,h as a,d as t,m as f,x as z,g as c,F as s,y as w,q,o as j,B as y,l as Hd,M as Gd,b as D,p as Ws,v as Jd,n as Us}from"../../chunks/vendor-1e8b365d.js";import{T as jp}from"../../chunks/Tip-62b14c6e.js";import{Y as qa}from"../../chunks/Youtube-c2a8cc39.js";import{I as So}from"../../chunks/IconCopyLink-483c28ba.js";import{C as M}from"../../chunks/CodeBlock-e5764662.js";import{D as Vd}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Qd}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Kd(C){let i,d;return i=new Vd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Yd(C){let i,d;return i=new Vd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function Zd(C){let i,d,u,_,g,v,b,$;return b=new M({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# M\xEAme chose que pr\xE9c\xE9demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Ceci est nouveau
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># M\xEAme chose que pr\xE9c\xE9demment</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Ce cours est incroyable !</span>
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># Ceci est nouveau</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){i=o("p"),d=n("En continuant avec l\u2019exemple du "),u=o("a"),_=n("chapitre pr\xE9c\xE9dent"),g=n(", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec TensorFlow :"),v=m(),x(b.$$.fragment),this.h()},l(h){i=l(h,"P",{});var k=r(i);d=a(k,"En continuant avec l\u2019exemple du "),u=l(k,"A",{href:!0});var S=r(u);_=a(S,"chapitre pr\xE9c\xE9dent"),S.forEach(t),g=a(k,", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec TensorFlow :"),k.forEach(t),v=f(h),z(b.$$.fragment,h),this.h()},h(){D(u,"href","/course/fr/chapter2")},m(h,k){c(h,i,k),s(i,d),s(i,u),s(u,_),s(i,g),c(h,v,k),w(b,h,k),$=!0},i(h){$||(q(b.$$.fragment,h),$=!0)},o(h){j(b.$$.fragment,h),$=!1},d(h){h&&t(i),h&&t(v),y(b,h)}}}function Xd(C){let i,d,u,_,g,v,b,$;return b=new M({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# M\xEAme chose que pr\xE9c\xE9demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Ceci est nouveau
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># M\xEAme chose que pr\xE9c\xE9demment</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,  <span class="hljs-comment"># Ce cours est incroyable !</span>
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Ceci est nouveau</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){i=o("p"),d=n("En continuant avec l\u2019exemple du "),u=o("a"),_=n("chapitre pr\xE9c\xE9dent"),g=n(", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec PyTorch :"),v=m(),x(b.$$.fragment),this.h()},l(h){i=l(h,"P",{});var k=r(i);d=a(k,"En continuant avec l\u2019exemple du "),u=l(k,"A",{href:!0});var S=r(u);_=a(S,"chapitre pr\xE9c\xE9dent"),S.forEach(t),g=a(k,", voici comment entra\xEEner un classifieur de s\xE9quences sur un batch avec PyTorch :"),k.forEach(t),v=f(h),z(b.$$.fragment,h),this.h()},h(){D(u,"href","/course/fr/chapter2")},m(h,k){c(h,i,k),s(i,d),s(i,u),s(u,_),s(i,g),c(h,v,k),w(b,h,k),$=!0},i(h){$||(q(b.$$.fragment,h),$=!0)},o(h){j(b.$$.fragment,h),$=!1},d(h){h&&t(i),h&&t(v),y(b,h)}}}function em(C){let i,d;return i=new qa({props:{id:"W_gMJF0xomE"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function sm(C){let i,d;return i=new qa({props:{id:"_BZearw7f0w"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function tm(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Regardez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et l\u2019\xE9l\xE9ment 87 de l\u2019ensemble de validation. Quelles sont leurs \xE9tiquettes ?")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Regardez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et l\u2019\xE9l\xE9ment 87 de l\u2019ensemble de validation. Quelles sont leurs \xE9tiquettes ?"),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function nm(C){let i,d;return i=new qa({props:{id:"P-rZWqcB6CE"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function am(C){let i,d;return i=new qa({props:{id:"0u3ioSwev3s"}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function om(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Prenez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et tokenisez les deux phrases s\xE9par\xE9ment et par paire. Quelle est la diff\xE9rence entre les deux r\xE9sultats ?")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Prenez l\u2019\xE9l\xE9ment 15 de l\u2019ensemble d\u2019entra\xEEnement et tokenisez les deux phrases s\xE9par\xE9ment et par paire. Quelle est la diff\xE9rence entre les deux r\xE9sultats ?"),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function lm(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R;return{c(){i=o("p"),d=n("La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=o("em"),_=n("fonction de rassemblement"),g=n(". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=o("code"),b=n("DataLoader"),$=n(", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en type tf.Tensor et les concat\xE9ner (r\xE9cursivement si les \xE9l\xE9ments sont des listes, des "),h=o("em"),k=n("tuples"),S=n(" ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=o("em"),O=n("padding"),L=n(", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=o("em"),F=n("padding"),R=n(" suppl\xE9mentaire.")},l(H){i=l(H,"P",{});var E=r(i);d=a(E,"La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=l(E,"EM",{});var ae=r(u);_=a(ae,"fonction de rassemblement"),ae.forEach(t),g=a(E,". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=l(E,"CODE",{});var Q=r(v);b=a(Q,"DataLoader"),Q.forEach(t),$=a(E,", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en type tf.Tensor et les concat\xE9ner (r\xE9cursivement si les \xE9l\xE9ments sont des listes, des "),h=l(E,"EM",{});var W=r(h);k=a(W,"tuples"),W.forEach(t),S=a(E," ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=l(E,"EM",{});var oe=r(I);O=a(oe,"padding"),oe.forEach(t),L=a(E,", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=l(E,"EM",{});var P=r(A);F=a(P,"padding"),P.forEach(t),R=a(E," suppl\xE9mentaire."),E.forEach(t)},m(H,E){c(H,i,E),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R)},d(H){H&&t(i)}}}function rm(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R;return{c(){i=o("p"),d=n("La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=o("em"),_=n("fonction de rassemblement"),g=n(". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=o("code"),b=n("DataLoader"),$=n(", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en tenseurs PyTorch et les concat\xE9ner (r\xE9cursivement si vos \xE9l\xE9ments sont des listes, des "),h=o("em"),k=n("tuples"),S=n(" ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=o("em"),O=n("padding"),L=n(", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=o("em"),F=n("padding"),R=n(" suppl\xE9mentaire.")},l(H){i=l(H,"P",{});var E=r(i);d=a(E,"La fonction qui est responsable de l\u2019assemblage des \xE9chantillons dans un batch est appel\xE9e "),u=l(E,"EM",{});var ae=r(u);_=a(ae,"fonction de rassemblement"),ae.forEach(t),g=a(E,". C\u2019est un argument que vous pouvez passer quand vous construisez un "),v=l(E,"CODE",{});var Q=r(v);b=a(Q,"DataLoader"),Q.forEach(t),$=a(E,", la valeur par d\xE9faut \xE9tant une fonction qui va juste convertir vos \xE9chantillons en tenseurs PyTorch et les concat\xE9ner (r\xE9cursivement si vos \xE9l\xE9ments sont des listes, des "),h=l(E,"EM",{});var W=r(h);k=a(W,"tuples"),W.forEach(t),S=a(E," ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr\xE9es que nous avons ne seront pas toutes de la m\xEAme taille. Nous avons d\xE9lib\xE9r\xE9ment report\xE9 le "),I=l(E,"EM",{});var oe=r(I);O=a(oe,"padding"),oe.forEach(t),L=a(E,", pour ne l\u2019appliquer que si n\xE9cessaire sur chaque batch et \xE9viter d\u2019avoir des entr\xE9es trop longues avec beaucoup de remplissage. Cela acc\xE9l\xE8re consid\xE9rablement l\u2019entra\xEEnement, mais notez que si vous vous entra\xEEnez sur un TPU, cela peut poser des probl\xE8mes. En effet, les TPU pr\xE9f\xE8rent les formes fixes, m\xEAme si cela n\xE9cessite un "),A=l(E,"EM",{});var P=r(A);F=a(P,"padding"),P.forEach(t),R=a(E," suppl\xE9mentaire."),E.forEach(t)},m(H,E){c(H,i,E),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R)},d(H){H&&t(i)}}}function im(C){let i,d;return i=new M({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function um(C){let i,d;return i=new M({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function pm(C){let i,d,u,_,g,v,b,$;return i=new M({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){x(i.$$.fragment),d=m(),u=o("p"),_=n("C\u2019est beau ! Maintenant que nous sommes pass\xE9s du texte brut \xE0 des batchs que notre mod\xE8le peut traiter, nous sommes pr\xEAts \xE0 le "),g=o("em"),v=n("finetuner"),b=n(" !")},l(h){z(i.$$.fragment,h),d=f(h),u=l(h,"P",{});var k=r(u);_=a(k,"C\u2019est beau ! Maintenant que nous sommes pass\xE9s du texte brut \xE0 des batchs que notre mod\xE8le peut traiter, nous sommes pr\xEAts \xE0 le "),g=l(k,"EM",{});var S=r(g);v=a(S,"finetuner"),S.forEach(t),b=a(k," !"),k.forEach(t)},m(h,k){w(i,h,k),c(h,d,k),c(h,u,k),s(u,_),s(u,g),s(g,v),s(u,b),$=!0},i(h){$||(q(i.$$.fragment,h),$=!0)},o(h){j(i.$$.fragment,h),$=!1},d(h){y(i,h),h&&t(d),h&&t(u)}}}function cm(C){let i,d;return i=new M({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){x(i.$$.fragment)},l(u){z(i.$$.fragment,u)},m(u,_){w(i,u,_),d=!0},i(u){d||(q(i.$$.fragment,u),d=!0)},o(u){j(i.$$.fragment,u),d=!1},d(u){y(i,u)}}}function dm(C){let i,d,u,_,g;return{c(){i=o("p"),d=n("\u270F\uFE0F "),u=o("strong"),_=n("Essayez !"),g=n(" Reproduisez le pr\xE9traitement sur le jeu de donn\xE9es GLUE SST-2. C\u2019est un peu diff\xE9rent puisqu\u2019il est compos\xE9 de phrases simples au lieu de paires, mais le reste de ce que nous avons fait devrait \xEAtre identique. Pour un d\xE9fi plus difficile, essayez d\u2019\xE9crire une fonction de pr\xE9traitement qui fonctionne sur toutes les t\xE2ches GLUE.")},l(v){i=l(v,"P",{});var b=r(i);d=a(b,"\u270F\uFE0F "),u=l(b,"STRONG",{});var $=r(u);_=a($,"Essayez !"),$.forEach(t),g=a(b," Reproduisez le pr\xE9traitement sur le jeu de donn\xE9es GLUE SST-2. C\u2019est un peu diff\xE9rent puisqu\u2019il est compos\xE9 de phrases simples au lieu de paires, mais le reste de ce que nous avons fait devrait \xEAtre identique. Pour un d\xE9fi plus difficile, essayez d\u2019\xE9crire une fonction de pr\xE9traitement qui fonctionne sur toutes les t\xE2ches GLUE."),b.forEach(t)},m(v,b){c(v,i,b),s(i,d),s(i,u),s(u,_),s(i,g)},d(v){v&&t(i)}}}function Id(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R,H,E,ae,Q,W,oe;return E=new M({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){i=o("p"),d=n("Maintenant que nous disposons de notre jeu de donn\xE9es et d\u2019un collecteur de donn\xE9es, nous devons les assembler. Nous pourrions charger manuellement des batchs et les assembler mais c\u2019est beaucoup de travail et probablement pas tr\xE8s performant non plus. A la place, il existe une m\xE9thode simple qui offre une solution performante \xE0 ce probl\xE8me : "),u=o("code"),_=n("to_tf_dataset()"),g=n(". Cela va envelopper un "),v=o("code"),b=n("tf.data.Dataset"),$=n(" autour de votre jeu de donn\xE9es, avec une fonction de collation optionnelle. "),h=o("code"),k=n("tf.data.Dataset"),S=n(" est un format natif de TensorFlow que Keras peut utiliser pour "),I=o("code"),O=n("model.fit()"),L=n(", donc cette seule m\xE9thode convertit imm\xE9diatement un "),A=o("em"),F=n("dataset"),R=n(" en un format pr\xEAt pour l\u2019entra\xEEnement. Voyons cela en action avec notre jeu de donn\xE9es !"),H=m(),x(E.$$.fragment),ae=m(),Q=o("p"),W=n("Et c\u2019est tout ! Nous pouvons utiliser ces jeux de donn\xE9es dans le prochain cours, o\xF9 l\u2019entra\xEEnement sera agr\xE9ablement simple apr\xE8s tout le dur travail de pr\xE9traitement des donn\xE9es.")},l(P){i=l(P,"P",{});var V=r(i);d=a(V,"Maintenant que nous disposons de notre jeu de donn\xE9es et d\u2019un collecteur de donn\xE9es, nous devons les assembler. Nous pourrions charger manuellement des batchs et les assembler mais c\u2019est beaucoup de travail et probablement pas tr\xE8s performant non plus. A la place, il existe une m\xE9thode simple qui offre une solution performante \xE0 ce probl\xE8me : "),u=l(V,"CODE",{});var Bs=r(u);_=a(Bs,"to_tf_dataset()"),Bs.forEach(t),g=a(V,". Cela va envelopper un "),v=l(V,"CODE",{});var Ce=r(v);b=a(Ce,"tf.data.Dataset"),Ce.forEach(t),$=a(V," autour de votre jeu de donn\xE9es, avec une fonction de collation optionnelle. "),h=l(V,"CODE",{});var Gs=r(h);k=a(Gs,"tf.data.Dataset"),Gs.forEach(t),S=a(V," est un format natif de TensorFlow que Keras peut utiliser pour "),I=l(V,"CODE",{});var Js=r(I);O=a(Js,"model.fit()"),Js.forEach(t),L=a(V,", donc cette seule m\xE9thode convertit imm\xE9diatement un "),A=l(V,"EM",{});var us=r(A);F=a(us,"dataset"),us.forEach(t),R=a(V," en un format pr\xEAt pour l\u2019entra\xEEnement. Voyons cela en action avec notre jeu de donn\xE9es !"),V.forEach(t),H=f(P),z(E.$$.fragment,P),ae=f(P),Q=l(P,"P",{});var pe=r(Q);W=a(pe,"Et c\u2019est tout ! Nous pouvons utiliser ces jeux de donn\xE9es dans le prochain cours, o\xF9 l\u2019entra\xEEnement sera agr\xE9ablement simple apr\xE8s tout le dur travail de pr\xE9traitement des donn\xE9es."),pe.forEach(t)},m(P,V){c(P,i,V),s(i,d),s(i,u),s(u,_),s(i,g),s(i,v),s(v,b),s(i,$),s(i,h),s(h,k),s(i,S),s(i,I),s(I,O),s(i,L),s(i,A),s(A,F),s(i,R),c(P,H,V),w(E,P,V),c(P,ae,V),c(P,Q,V),s(Q,W),oe=!0},i(P){oe||(q(E.$$.fragment,P),oe=!0)},o(P){j(E.$$.fragment,P),oe=!1},d(P){P&&t(i),P&&t(H),y(E,P),P&&t(ae),P&&t(Q)}}}function mm(C){let i,d,u,_,g,v,b,$,h,k,S,I,O,L,A,F,R,H,E,ae,Q,W,oe,P,V,Bs,Ce,Gs,Js,us,pe,We,jt,ps,Ao,Qs,No,gt,Oo,ja,ge,ke,Ks,K,Lo,kt,Fo,Ro,cs,Ho,Io,ds,Vo,Wo,Ue,Et,Uo,Bo,Go,$t,Jo,Qo,ga,De,Ko,xt,Yo,Zo,zt,Xo,el,ka,ms,Ea,fs,$a,Y,sl,wt,tl,nl,yt,al,ol,Ct,ll,rl,Dt,il,ul,Pt,pl,cl,xa,Pe,dl,Tt,ml,fl,Mt,hl,_l,za,Be,vl,St,bl,ql,wa,hs,ya,_s,Ca,Te,jl,At,gl,kl,Nt,El,$l,Da,vs,Pa,bs,Ta,U,xl,Ot,zl,wl,Lt,yl,Cl,Ft,Dl,Pl,Rt,Tl,Ml,Ht,Sl,Al,It,Nl,Ol,Vt,Ll,Fl,Ma,Ge,Sa,He,Je,Wt,qs,Rl,Ut,Hl,Aa,Ee,$e,Ys,ce,Il,Zs,Vl,Wl,Bt,Ul,Bl,Gt,Gl,Jl,Na,js,Oa,Qe,Ql,Jt,Kl,Yl,La,gs,Fa,ks,Ra,le,Zl,Qt,Xl,er,Kt,sr,tr,Xs,nr,ar,Yt,or,lr,Ha,Ke,Ia,Ye,rr,Zt,ir,ur,Va,Es,Wa,et,pr,Ua,$s,Ba,Me,cr,Xt,dr,mr,en,fr,hr,Ga,xs,Ja,J,_r,sn,vr,br,tn,qr,jr,nn,gr,kr,an,Er,$r,on,xr,zr,ln,wr,yr,Qa,Se,Cr,rn,Dr,Pr,un,Tr,Mr,Ka,de,Sr,pn,Ar,Nr,st,Or,Lr,cn,Fr,Rr,Ya,Ze,Hr,dn,Ir,Vr,Za,re,Wr,mn,Ur,Br,fn,Gr,Jr,hn,Qr,Kr,_n,Yr,Zr,Xa,ie,Xr,vn,ei,si,tt,ti,ni,bn,ai,oi,nt,li,ri,eo,zs,so,Z,ii,qn,ui,pi,jn,ci,di,gn,mi,fi,kn,hi,_i,ws,vi,bi,to,Ae,qi,ys,En,ji,gi,$n,ki,Ei,no,Cs,ao,T,$i,xn,xi,zi,zn,wi,yi,wn,Ci,Di,yn,Pi,Ti,Cn,Mi,Si,Dn,Ai,Ni,Pn,Oi,Li,Tn,Fi,Ri,Mn,Hi,Ii,Xe,Vi,Sn,Wi,Ui,An,Bi,Gi,oo,me,Ji,Nn,Qi,Ki,On,Yi,Zi,Ln,Xi,eu,lo,Ne,su,Fn,tu,nu,Rn,au,ou,ro,Ds,io,es,lu,Hn,ru,iu,uo,Ps,po,X,uu,In,pu,cu,Vn,du,mu,Wn,fu,hu,Un,_u,vu,Bn,bu,qu,co,ee,ju,Gn,gu,ku,Jn,Eu,$u,Qn,xu,zu,Kn,wu,yu,Yn,Cu,Du,mo,ss,Pu,Zn,Tu,Mu,fo,Ie,ts,Xn,Ts,Su,at,ea,Au,Nu,ho,Ms,_o,ot,B,Ou,sa,Lu,Fu,ta,Ru,Hu,na,Iu,Vu,aa,Wu,Uu,oa,Bu,Gu,la,Ju,Qu,ra,Ku,Yu,vo,xe,ze,lt,se,Zu,ia,Xu,ep,ua,sp,tp,pa,np,ap,ca,op,lp,da,rp,ip,bo,Ss,qo,As,jo,fe,up,ma,pp,cp,fa,dp,mp,ha,fp,hp,go,Ns,ko,we,ye,rt,ns,Eo,it,$o;u=new Qd({props:{fw:C[0]}}),$=new So({});const gp=[Yd,Kd],Os=[];function kp(e,p){return e[0]==="pt"?0:1}O=kp(C),L=Os[O]=gp[O](C);const Ep=[Xd,Zd],Ls=[];function $p(e,p){return e[0]==="pt"?0:1}F=$p(C),R=Ls[F]=Ep[F](C),ps=new So({});const xp=[sm,em],Fs=[];function zp(e,p){return e[0]==="pt"?0:1}ge=zp(C),ke=Fs[ge]=xp[ge](C),ms=new M({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),fs=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),hs=new M({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),_s=new M({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 
 # Amrozi a accus\xE9 son fr\xE8re, qu'il a appel\xE9 \xAB le t\xE9moin \xBB, de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'} 
 # Se r\xE9f\xE9rant \xE0 lui uniquement comme \xAB le t\xE9moin \xBB, Amrozi a accus\xE9 son fr\xE8re de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>, 
 <span class="hljs-comment"># Amrozi a accus\xE9 son fr\xE8re, qu&#x27;il a appel\xE9 \xAB le t\xE9moin \xBB, de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.</span>
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>} 
 <span class="hljs-comment"># Se r\xE9f\xE9rant \xE0 lui uniquement comme \xAB le t\xE9moin \xBB, Amrozi a accus\xE9 son fr\xE8re de d\xE9former d\xE9lib\xE9r\xE9ment son t\xE9moignage.</span>`}}),vs=new M({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),bs=new M({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Ge=new jp({props:{$$slots:{default:[tm]},$$scope:{ctx:C}}}),qs=new So({});const wp=[am,nm],Rs=[];function yp(e,p){return e[0]==="pt"?0:1}Ee=yp(C),$e=Rs[Ee]=wp[Ee](C),js=new M({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),gs=new M({props:{code:`inputs = tokenizer(
    "This is the first sentence.", "This is the second one."
)  # "C'est la premi\xE8re phrase.", "C'est la deuxi\xE8me."
inputs`,highlighted:`inputs = tokenizer(
    <span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>
)  <span class="hljs-comment"># &quot;C&#x27;est la premi\xE8re phrase.&quot;, &quot;C&#x27;est la deuxi\xE8me.&quot;</span>
inputs`}}),ks=new M({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Ke=new jp({props:{$$slots:{default:[om]},$$scope:{ctx:C}}}),Es=new M({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),$s=new M({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),xs=new M({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),zs=new M({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),Cs=new M({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),Ds=new M({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),Ps=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),Ts=new So({}),Ms=new qa({props:{id:"7q5NyFT8REg"}});function Cp(e,p){return e[0]==="pt"?rm:lm}let xo=Cp(C),Ve=xo(C);const Dp=[um,im],Hs=[];function Pp(e,p){return e[0]==="pt"?0:1}xe=Pp(C),ze=Hs[xe]=Dp[xe](C),Ss=new M({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),As=new M({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),Ns=new M({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const Tp=[cm,pm],Is=[];function Mp(e,p){return e[0]==="tf"?0:1}we=Mp(C),ye=Is[we]=Tp[we](C),ns=new jp({props:{$$slots:{default:[dm]},$$scope:{ctx:C}}});let G=C[0]==="tf"&&Id();return{c(){i=o("meta"),d=m(),x(u.$$.fragment),_=m(),g=o("h1"),v=o("a"),b=o("span"),x($.$$.fragment),h=m(),k=o("span"),S=n("Pr\xE9parer les donn\xE9es"),I=m(),L.c(),A=m(),R.c(),H=m(),E=o("p"),ae=n("Evidemment, entra\xEEner un mod\xE8le avec seulement deux phrases ne va pas donner de bons r\xE9sultats. Pour obtenir de meilleurs r\xE9sultats, vous allez avoir \xE0 pr\xE9parer un plus grand jeu de donn\xE9es."),Q=m(),W=o("p"),oe=n("Dans cette section, nous allons utiliser comme exemple le jeu de donn\xE9es MRPC ("),P=o("em"),V=n("Microsoft Research Paraphrase Corpus"),Bs=n(") pr\xE9sent\xE9 dans un "),Ce=o("a"),Gs=n("papier"),Js=n(" par William B. Dolan et Chris Brockett. Ce jeu de donn\xE9es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m\xEAme signification). Nous l\u2019avons choisi pour ce chapitre parce que c\u2019est un petit jeu de donn\xE9es et cela rend donc simples les exp\xE9riences d\u2019entra\xEEnement sur ce jeu de donn\xE9es."),us=m(),pe=o("h3"),We=o("a"),jt=o("span"),x(ps.$$.fragment),Ao=m(),Qs=o("span"),No=n("Charger un jeu de donn\xE9es depuis le "),gt=o("i"),Oo=n("Hub"),ja=m(),ke.c(),Ks=m(),K=o("p"),Lo=n("Le "),kt=o("em"),Fo=n("Hub"),Ro=n(" ne contient pas seulement des mod\xE8les mais aussi plusieurs jeux de donn\xE9es dans un tas de langues diff\xE9rentes. Vous pouvez explorer les jeux de donn\xE9es "),cs=o("a"),Ho=n("ici"),Io=n(" et nous vous conseillons d\u2019essayer de charger un nouveau jeu de donn\xE9es une fois que vous avez \xE9tudi\xE9 cette section (voir la documentation g\xE9n\xE9rale "),ds=o("a"),Vo=n("ici"),Wo=n("). Mais pour l\u2019instant, concentrons-nous sur le jeu de donn\xE9es MRPC ! Il s\u2019agit de l\u2019un des 10 jeux de donn\xE9es qui constituent le "),Ue=o("a"),Et=o("em"),Uo=n("benchmark"),Bo=n(" GLUE"),Go=n(" qui est un "),$t=o("em"),Jo=n("benchmark"),Qo=n(" acad\xE9mique utilis\xE9 pour mesurer les performances des mod\xE8les d\u2019apprentissage automatique sur 10 diff\xE9rentes t\xE2ches de classification de textes."),ga=m(),De=o("p"),Ko=n("La biblioth\xE8que \u{1F917} "),xt=o("em"),Yo=n("Datasets"),Zo=n(" propose une commande tr\xE8s simple pour t\xE9l\xE9charger et mettre en cache un jeu de donn\xE9es \xE0 partir du "),zt=o("em"),Xo=n("Hub"),el=n(". On peut t\xE9l\xE9charger le jeu de donn\xE9es MRPC comme ceci :"),ka=m(),x(ms.$$.fragment),Ea=m(),x(fs.$$.fragment),$a=m(),Y=o("p"),sl=n("Comme vous le voyez, on obtient un objet de type "),wt=o("code"),tl=n("DatasetDict"),nl=n(" qui contient le jeu de donn\xE9es d\u2019entra\xEEnement, celui de validation et celui de test. Chacun d\u2019eux contient plusieurs colonnes ("),yt=o("code"),al=n("sentence1"),ol=n(", "),Ct=o("code"),ll=n("sentence2"),rl=n(", "),Dt=o("code"),il=n("label"),ul=n(" et "),Pt=o("code"),pl=n("idx"),cl=n(") et une variable nombre de lignes qui contient le nombre d\u2019\xE9l\xE9ments dans chaque jeu de donn\xE9es (il y a donc 3.668 paires de phrases dans le jeu d\u2019entra\xEEnement, 408 dans celui de validation et 1.725 dans celui de test)."),xa=m(),Pe=o("p"),dl=n("Cette commande t\xE9l\xE9charge et met en cache le jeu de donn\xE9es dans "),Tt=o("em"),ml=n("~/.cache/huggingface/dataset"),fl=n(". Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d\u2019environnement "),Mt=o("code"),hl=n("HF_HOME"),_l=n("."),za=m(),Be=o("p"),vl=n("Nous pouvons acc\xE9der \xE0 chaque paire de phrase de notre objet "),St=o("code"),bl=n("raw_datasets"),ql=n(" par les indices, comme avec un dictionnaire :"),wa=m(),x(hs.$$.fragment),ya=m(),x(_s.$$.fragment),Ca=m(),Te=o("p"),jl=n("Nous pouvons voir que les \xE9tiquettes sont d\xE9j\xE0 des entiers, donc nous n\u2019aurons pas \xE0 faire de pr\xE9traitement ici. Pour savoir quel entier correspond \xE0 quel label, nous pouvons inspecter les "),At=o("code"),gl=n("features"),kl=n(" de notre "),Nt=o("code"),El=n("raw_train_dataset"),$l=n(". Cela nous indiquera le type de chaque colonne :"),Da=m(),x(vs.$$.fragment),Pa=m(),x(bs.$$.fragment),Ta=m(),U=o("p"),xl=n("En r\xE9alit\xE9, "),Ot=o("code"),zl=n("label"),wl=n(" est de type "),Lt=o("code"),yl=n("ClassLabel"),Cl=n(" et la correspondance des entiers aux noms des labels est enregistr\xE9e le dossier "),Ft=o("em"),Dl=n("names"),Pl=n(". "),Rt=o("code"),Tl=n("0"),Ml=n(" correspond \xE0  "),Ht=o("code"),Sl=n("not_equivalent"),Al=n(" et "),It=o("code"),Nl=n("1"),Ol=n(" correspond \xE0 "),Vt=o("code"),Ll=n("equivalent"),Fl=n("."),Ma=m(),x(Ge.$$.fragment),Sa=m(),He=o("h3"),Je=o("a"),Wt=o("span"),x(qs.$$.fragment),Rl=m(),Ut=o("span"),Hl=n("Pr\xE9traitement d'un jeu de donn\xE9es"),Aa=m(),$e.c(),Ys=m(),ce=o("p"),Il=n("Pour pr\xE9traiter le jeu de donn\xE9es, nous devons convertir le texte en chiffres compr\xE9hensibles par le mod\xE8le. Comme vous l\u2019avez vu dans le "),Zs=o("a"),Vl=n("chapitre pr\xE9c\xE9dent"),Wl=n(", cette conversion est effectu\xE9e par un "),Bt=o("em"),Ul=n("tokenizer"),Bl=n(". Nous pouvons fournir au "),Gt=o("em"),Gl=n("tokenizer"),Jl=n(" une phrase ou une liste de phrases, de sorte que nous pouvons directement tokeniser toutes les premi\xE8res phrases et toutes les secondes phrases de chaque paire comme ceci :"),Na=m(),x(js.$$.fragment),Oa=m(),Qe=o("p"),Ql=n("Cependant, nous ne pouvons pas simplement passer deux s\xE9quences au mod\xE8le et obtenir une pr\xE9diction pour savoir si les deux phrases sont des paraphrases ou non. Nous devons traiter les deux s\xE9quences comme une paire, et appliquer le pr\xE9traitement appropri\xE9. Heureusement, le "),Jt=o("em"),Kl=n("tokenizer"),Yl=n(" peut \xE9galement prendre une paire de s\xE9quences et la pr\xE9parer de la mani\xE8re attendue par notre mod\xE8le BERT :"),La=m(),x(gs.$$.fragment),Fa=m(),x(ks.$$.fragment),Ra=m(),le=o("p"),Zl=n("Nous avons discut\xE9 des cl\xE9s "),Qt=o("code"),Xl=n("input_ids"),er=n(" et "),Kt=o("code"),sr=n("attention_mask"),tr=n(" dans le "),Xs=o("a"),nr=n("chapitre 2"),ar=n(", mais nous avons laiss\xE9 de c\xF4t\xE9 les "),Yt=o("code"),or=n("token_type_ids"),lr=n(". Dans cet exemple, c\u2019est ce qui indique au mod\xE8le quelle partie de l\u2019entr\xE9e est la premi\xE8re phrase et quelle partie est la deuxi\xE8me phrase."),Ha=m(),x(Ke.$$.fragment),Ia=m(),Ye=o("p"),rr=n("Si on d\xE9code les IDs dans "),Zt=o("code"),ir=n("input_ids"),ur=n(" en mots :"),Va=m(),x(Es.$$.fragment),Wa=m(),et=o("p"),pr=n("nous aurons :"),Ua=m(),x($s.$$.fragment),Ba=m(),Me=o("p"),cr=n("Nous voyons donc que le mod\xE8le s\u2019attend \xE0 ce que les entr\xE9es soient de la forme "),Xt=o("code"),dr=n("[CLS] phrase1 [SEP] phrase2 [SEP]"),mr=n(" lorsqu\u2019il y a deux phrases. En alignant cela avec les "),en=o("code"),fr=n("token_type_ids"),hr=n(", on obtient :"),Ga=m(),x(xs.$$.fragment),Ja=m(),J=o("p"),_r=n("Comme vous pouvez le voir, les parties de l\u2019entr\xE9e correspondant \xE0 "),sn=o("code"),vr=n("[CLS] sentence1 [SEP]"),br=n(" ont toutes un "),tn=o("em"),qr=n("token"),jr=n(" de type ID de "),nn=o("code"),gr=n("0"),kr=n(", tandis que les autres parties, correspondant \xE0 "),an=o("code"),Er=n("sentence2 [SEP]"),$r=n(", ont toutes un "),on=o("em"),xr=n("token"),zr=n(" de type ID de "),ln=o("code"),wr=n("1"),yr=n("."),Qa=m(),Se=o("p"),Cr=n("Notez que si vous choisissez un autre "),rn=o("em"),Dr=n("checkpoint"),Pr=n(", vous n\u2019aurez pas n\xE9cessairement les "),un=o("code"),Tr=n("token_type_ids"),Mr=n(" dans vos entr\xE9es tokenis\xE9es (par exemple, ils ne sont pas retourn\xE9s si vous utilisez un mod\xE8le DistilBERT). Ils ne sont retourn\xE9s que lorsque le mod\xE8le sait quoi faire avec eux, parce qu\u2019il les a vus pendant son pr\xE9-entra\xEEnement."),Ka=m(),de=o("p"),Sr=n("Ici, BERT est pr\xE9-entra\xEEn\xE9 avec les "),pn=o("em"),Ar=n("tokens"),Nr=n(" de type ID et en plus de l\u2019objectif de mod\xE9lisation du langage masqu\xE9 dont nous avons abord\xE9 dans "),st=o("a"),Or=n("chapitre 1"),Lr=n(", il a un objectif suppl\xE9mentaire appel\xE9 "),cn=o("em"),Fr=n("pr\xE9diction de la phrase suivante"),Rr=n(". Le but de cette t\xE2che est de mod\xE9liser la relation entre des paires de phrases."),Ya=m(),Ze=o("p"),Hr=n("Avec la pr\xE9diction de la phrase suivante, on fournit au mod\xE8le des paires de phrases (avec des "),dn=o("em"),Ir=n("tokens"),Vr=n(" masqu\xE9s de mani\xE8re al\xE9atoire) et on lui demande de pr\xE9dire si la deuxi\xE8me phrase suit la premi\xE8re. Pour rendre la t\xE2che non triviale, la moiti\xE9 du temps, les phrases se suivent dans le document d\u2019origine dont elles ont \xE9t\xE9 extraites, et l\u2019autre moiti\xE9 du temps, les deux phrases proviennent de deux documents diff\xE9rents."),Za=m(),re=o("p"),Wr=n("En g\xE9n\xE9ral, vous n\u2019avez pas besoin de vous inqui\xE9ter de savoir s\u2019il y a ou non des "),mn=o("code"),Ur=n("token_type_ids"),Br=n(" dans vos entr\xE9es tokenis\xE9es : tant que vous utilisez le m\xEAme "),fn=o("em"),Gr=n("checkpoint"),Jr=n(" pour le "),hn=o("em"),Qr=n("tokenizer"),Kr=n(" et le mod\xE8le, tout ira bien puisque le "),_n=o("em"),Yr=n("tokenizer"),Zr=n(" sait quoi fournir \xE0 son mod\xE8le."),Xa=m(),ie=o("p"),Xr=n("Maintenant que nous avons vu comment notre "),vn=o("em"),ei=n("tokenizer"),si=n(" peut traiter une paire de phrases, nous pouvons l\u2019utiliser pour tokeniser l\u2019ensemble de notre jeu de donn\xE9es : comme dans le "),tt=o("a"),ti=n("chapitre pr\xE9c\xE9dent"),ni=n(", nous pouvons fournir au "),bn=o("em"),ai=n("tokenizer"),oi=n(" une liste de paires de phrases en lui donnant la liste des premi\xE8res phrases, puis la liste des secondes phrases. Ceci est \xE9galement compatible avec les options de remplissage et de troncature que nous avons vues dans le "),nt=o("a"),li=n("chapitre 2"),ri=n(". Voici donc une fa\xE7on de pr\xE9traiter le jeu de donn\xE9es d\u2019entra\xEEnement :"),eo=m(),x(zs.$$.fragment),so=m(),Z=o("p"),ii=n("Cela fonctionne bien, mais a l\u2019inconv\xE9nient de retourner un dictionnaire (avec nos cl\xE9s, "),qn=o("code"),ui=n("input_ids"),pi=n(", "),jn=o("code"),ci=n("attention_mask"),di=n(", et "),gn=o("code"),mi=n("token_type_ids"),fi=n(", et des valeurs qui sont des listes de listes). Cela ne fonctionnera \xE9galement que si vous avez assez de RAM pour stocker l\u2019ensemble de votre jeu de donn\xE9es pendant la tokenisation (alors que les jeux de donn\xE9es de la biblioth\xE8que \u{1F917} "),kn=o("em"),hi=n("Datasets"),_i=n(" sont des fichiers "),ws=o("a"),vi=n("Apache Arrow"),bi=n(" stock\xE9s sur le disque, vous ne gardez donc en m\xE9moire que les \xE9chantillons que vous demandez)."),to=m(),Ae=o("p"),qi=n("Pour conserver les donn\xE9es sous forme de jeu de donn\xE9es, nous utiliserons la m\xE9thode "),ys=o("a"),En=o("code"),ji=n("Dataset.map()"),gi=n(". Cela nous permet \xE9galement une certaine flexibilit\xE9, si nous avons besoin d\u2019un pr\xE9traitement plus pouss\xE9 que la simple tokenisation. La m\xE9thode "),$n=o("code"),ki=n("map()"),Ei=n(" fonctionne en appliquant une fonction sur chaque \xE9l\xE9ment de l\u2019ensemble de donn\xE9es, donc d\xE9finissons une fonction qui tokenise nos entr\xE9es :"),no=m(),x(Cs.$$.fragment),ao=m(),T=o("p"),$i=n("Cette fonction prend un dictionnaire (comme les \xE9l\xE9ments de notre jeu de donn\xE9es) et retourne un nouveau dictionnaire avec les cl\xE9s "),xn=o("code"),xi=n("input_ids"),zi=n(", "),zn=o("code"),wi=n("attention_mask"),yi=n(", et "),wn=o("code"),Ci=n("token_type_ids"),Di=n(". Notez que cela fonctionne \xE9galement si le dictionnaire "),yn=o("code"),Pi=n("example"),Ti=n(" contient plusieurs \xE9chantillons (chaque cl\xE9 \xE9tant une liste de phrases) puisque le "),Cn=o("code"),Mi=n("tokenizer"),Si=n(" travaille sur des listes de paires de phrases, comme vu pr\xE9c\xE9demment. Cela nous permettra d\u2019utiliser l\u2019option "),Dn=o("code"),Ai=n("batched=True"),Ni=n(" dans notre appel \xE0 "),Pn=o("code"),Oi=n("map()"),Li=n(", ce qui acc\xE9l\xE9rera grandement la tok\xE9nisation. Le "),Tn=o("code"),Fi=n("tokenizer"),Ri=n(" est soutenu par un "),Mn=o("em"),Hi=n("tokenizer"),Ii=n(" \xE9crit en Rust \xE0 partir de la biblioth\xE8que "),Xe=o("a"),Vi=n("\u{1F917} "),Sn=o("em"),Wi=n("Tokenizers"),Ui=n(". Ce "),An=o("em"),Bi=n("tokenizer"),Gi=n(" peut \xEAtre tr\xE8s rapide, mais seulement si on lui donne beaucoup d\u2019entr\xE9es en m\xEAme temps."),oo=m(),me=o("p"),Ji=n("Notez que nous avons laiss\xE9 l\u2019argument "),Nn=o("code"),Qi=n("padding"),Ki=n(" hors de notre fonction de "),On=o("em"),Yi=n("tokenizer"),Zi=n(" pour le moment. C\u2019est parce que le "),Ln=o("em"),Xi=n("padding"),eu=n(" de tous les \xE9chantillons \xE0 la longueur maximale n\u2019est pas efficace : il est pr\xE9f\xE9rable de remplir les \xE9chantillons lorsque nous construisons un batch, car alors nous avons seulement besoin de remplir \xE0 la longueur maximale dans ce batch, et non la longueur maximale dans l\u2019ensemble des donn\xE9es. Cela peut permettre de gagner beaucoup de temps et de puissance de traitement lorsque les entr\xE9es ont des longueurs tr\xE8s variables !"),lo=m(),Ne=o("p"),su=n("Voici comment nous appliquons la fonction de tokenization sur tous nos jeux de donn\xE9es en m\xEAme temps. Nous utilisons "),Fn=o("code"),tu=n("batched=True"),nu=n(" dans notre appel \xE0 "),Rn=o("code"),au=n("map"),ou=n(" pour que la fonction soit appliqu\xE9e \xE0 plusieurs \xE9l\xE9ments de notre jeu de donn\xE9es en une fois, et non \xE0 chaque \xE9l\xE9ment s\xE9par\xE9ment. Cela permet un pr\xE9traitement plus rapide."),ro=m(),x(Ds.$$.fragment),io=m(),es=o("p"),lu=n("La fa\xE7on dont la biblioth\xE8que \u{1F917} "),Hn=o("em"),ru=n("Datasets"),iu=n(" applique ce traitement consiste \xE0 ajouter de nouveaux champs aux jeux de donn\xE9es, un pour chaque cl\xE9 du dictionnaire renvoy\xE9 par la fonction de pr\xE9traitement :"),uo=m(),x(Ps.$$.fragment),po=m(),X=o("p"),uu=n("Vous pouvez m\xEAme utiliser le multitraitement lorsque vous appliquez votre fonction de pr\xE9traitement avec "),In=o("code"),pu=n("map()"),cu=n(" en passant un argument "),Vn=o("code"),du=n("num_proc"),mu=n(". Nous ne l\u2019avons pas fait ici parce que la biblioth\xE8que \u{1F917} "),Wn=o("em"),fu=n("Tokenizers"),hu=n(" utilise d\xE9j\xE0 plusieurs "),Un=o("em"),_u=n("threads"),vu=n(" pour tokeniser nos \xE9chantillons plus rapidement, mais si vous n\u2019utilisez pas un "),Bn=o("em"),bu=n("tokenizer"),qu=n(" rapide soutenu par cette biblioth\xE8que, cela pourrait acc\xE9l\xE9rer votre pr\xE9traitement."),co=m(),ee=o("p"),ju=n("Notre "),Gn=o("code"),gu=n("tokenize_function"),ku=n(" retourne un dictionnaire avec les cl\xE9s "),Jn=o("code"),Eu=n("input_ids"),$u=n(", "),Qn=o("code"),xu=n("attention_mask"),zu=n(", et "),Kn=o("code"),wu=n("token_type_ids"),yu=n(", donc ces trois champs sont ajout\xE9s \xE0 toutes les divisions de notre jeu de donn\xE9es. Notez que nous aurions \xE9galement pu modifier des champs existants si notre fonction de pr\xE9traitement avait retourn\xE9 une nouvelle valeur pour une cl\xE9 existante dans l\u2019ensemble de donn\xE9es auquel nous avons appliqu\xE9 "),Yn=o("code"),Cu=n("map()"),Du=n("."),mo=m(),ss=o("p"),Pu=n("La derni\xE8re chose que nous devrons faire est de remplir tous les exemples \xE0 la longueur de l\u2019\xE9l\xE9ment le plus long lorsque nous regroupons les \xE9l\xE9ments, une technique que nous appelons le "),Zn=o("em"),Tu=n("padding dynamique"),Mu=n("."),fo=m(),Ie=o("h3"),ts=o("a"),Xn=o("span"),x(Ts.$$.fragment),Su=m(),at=o("span"),ea=o("i"),Au=n("Padding"),Nu=n(" dynamique"),ho=m(),x(Ms.$$.fragment),_o=m(),Ve.c(),ot=m(),B=o("p"),Ou=n("Pour faire cela en pratique, nous devons d\xE9finir une fonction de rassemblement qui appliquera la bonne quantit\xE9 de "),sa=o("em"),Lu=n("padding"),Fu=n(" aux \xE9l\xE9ments du jeu de donn\xE9es que nous voulons regrouper. Heureusement, la biblioth\xE8que \u{1F917} "),ta=o("em"),Ru=n("Transformers"),Hu=n(" nous fournit une telle fonction via "),na=o("code"),Iu=n("DataCollatorWithPadding"),Vu=n(". Elle prend un "),aa=o("em"),Wu=n("tokenizer"),Uu=n(" lorsque vous l\u2019instanciez (pour savoir quel "),oa=o("em"),Bu=n("token"),Gu=n(" de "),la=o("em"),Ju=n("padding"),Qu=n(" utiliser et si le mod\xE8le s\u2019attend \xE0 ce que le "),ra=o("em"),Ku=n("padding"),Yu=n(" soit \xE0 gauche ou \xE0 droite des entr\xE9es) et fera tout ce dont vous avez besoin :"),vo=m(),ze.c(),lt=m(),se=o("p"),Zu=n("Pour tester notre nouveau jouet, prenons quelques \xE9l\xE9ments de notre jeu d\u2019entra\xEEnement avec lesquels nous allons former un batch. Ici, on supprime les colonnes "),ia=o("code"),Xu=n("idx"),ep=n(", "),ua=o("code"),sp=n("sentence1"),tp=n(" et "),pa=o("code"),np=n("sentence2"),ap=n(" puisque nous n\u2019en aurons pas besoin et qu\u2019elles contiennent des "),ca=o("em"),op=n("strings"),lp=n(" (et nous ne pouvons pas cr\xE9er des tenseurs avec des "),da=o("em"),rp=n("strings"),ip=n(") et on regarde la longueur de chaque entr\xE9e du batch :"),bo=m(),x(Ss.$$.fragment),qo=m(),x(As.$$.fragment),jo=m(),fe=o("p"),up=n("Sans surprise, nous obtenons des \xE9chantillons de longueur variable, de 32 \xE0 67. Le "),ma=o("em"),pp=n("padding"),cp=n(" dynamique signifie que les \xE9chantillons de ce batch doivent tous \xEAtre rembourr\xE9s \xE0 une longueur de 67, la longueur maximale dans le batch. Sans le "),fa=o("em"),dp=n("padding"),mp=n(" dynamique, tous les \xE9chantillons devraient \xEAtre rembourr\xE9s \xE0 la longueur maximale du jeu de donn\xE9es entier, ou \xE0 la longueur maximale que le mod\xE8le peut accepter. V\xE9rifions \xE0 nouveau que notre "),ha=o("code"),fp=n("data_collator"),hp=n(" rembourre dynamiquement le batch correctement :"),go=m(),x(Ns.$$.fragment),ko=m(),ye.c(),rt=m(),x(ns.$$.fragment),Eo=m(),G&&G.c(),it=Hd(),this.h()},l(e){const p=Gd('[data-svelte="svelte-1phssyn"]',document.head);i=l(p,"META",{name:!0,content:!0}),p.forEach(t),d=f(e),z(u.$$.fragment,e),_=f(e),g=l(e,"H1",{class:!0});var Vs=r(g);v=l(Vs,"A",{id:!0,class:!0,href:!0});var ut=r(v);b=l(ut,"SPAN",{});var pt=r(b);z($.$$.fragment,pt),pt.forEach(t),ut.forEach(t),h=f(Vs),k=l(Vs,"SPAN",{});var ct=r(k);S=a(ct,"Pr\xE9parer les donn\xE9es"),ct.forEach(t),Vs.forEach(t),I=f(e),L.l(e),A=f(e),R.l(e),H=f(e),E=l(e,"P",{});var _a=r(E);ae=a(_a,"Evidemment, entra\xEEner un mod\xE8le avec seulement deux phrases ne va pas donner de bons r\xE9sultats. Pour obtenir de meilleurs r\xE9sultats, vous allez avoir \xE0 pr\xE9parer un plus grand jeu de donn\xE9es."),_a.forEach(t),Q=f(e),W=l(e,"P",{});var Oe=r(W);oe=a(Oe,"Dans cette section, nous allons utiliser comme exemple le jeu de donn\xE9es MRPC ("),P=l(Oe,"EM",{});var va=r(P);V=a(va,"Microsoft Research Paraphrase Corpus"),va.forEach(t),Bs=a(Oe,") pr\xE9sent\xE9 dans un "),Ce=l(Oe,"A",{href:!0,rel:!0});var dt=r(Ce);Gs=a(dt,"papier"),dt.forEach(t),Js=a(Oe," par William B. Dolan et Chris Brockett. Ce jeu de donn\xE9es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m\xEAme signification). Nous l\u2019avons choisi pour ce chapitre parce que c\u2019est un petit jeu de donn\xE9es et cela rend donc simples les exp\xE9riences d\u2019entra\xEEnement sur ce jeu de donn\xE9es."),Oe.forEach(t),us=f(e),pe=l(e,"H3",{class:!0});var as=r(pe);We=l(as,"A",{id:!0,class:!0,href:!0});var ba=r(We);jt=l(ba,"SPAN",{});var Sp=r(jt);z(ps.$$.fragment,Sp),Sp.forEach(t),ba.forEach(t),Ao=f(as),Qs=l(as,"SPAN",{});var _p=r(Qs);No=a(_p,"Charger un jeu de donn\xE9es depuis le "),gt=l(_p,"I",{});var Ap=r(gt);Oo=a(Ap,"Hub"),Ap.forEach(t),_p.forEach(t),as.forEach(t),ja=f(e),ke.l(e),Ks=f(e),K=l(e,"P",{});var he=r(K);Lo=a(he,"Le "),kt=l(he,"EM",{});var Np=r(kt);Fo=a(Np,"Hub"),Np.forEach(t),Ro=a(he," ne contient pas seulement des mod\xE8les mais aussi plusieurs jeux de donn\xE9es dans un tas de langues diff\xE9rentes. Vous pouvez explorer les jeux de donn\xE9es "),cs=l(he,"A",{href:!0,rel:!0});var Op=r(cs);Ho=a(Op,"ici"),Op.forEach(t),Io=a(he," et nous vous conseillons d\u2019essayer de charger un nouveau jeu de donn\xE9es une fois que vous avez \xE9tudi\xE9 cette section (voir la documentation g\xE9n\xE9rale "),ds=l(he,"A",{href:!0,rel:!0});var Lp=r(ds);Vo=a(Lp,"ici"),Lp.forEach(t),Wo=a(he,"). Mais pour l\u2019instant, concentrons-nous sur le jeu de donn\xE9es MRPC ! Il s\u2019agit de l\u2019un des 10 jeux de donn\xE9es qui constituent le "),Ue=l(he,"A",{href:!0,rel:!0});var vp=r(Ue);Et=l(vp,"EM",{});var Fp=r(Et);Uo=a(Fp,"benchmark"),Fp.forEach(t),Bo=a(vp," GLUE"),vp.forEach(t),Go=a(he," qui est un "),$t=l(he,"EM",{});var Rp=r($t);Jo=a(Rp,"benchmark"),Rp.forEach(t),Qo=a(he," acad\xE9mique utilis\xE9 pour mesurer les performances des mod\xE8les d\u2019apprentissage automatique sur 10 diff\xE9rentes t\xE2ches de classification de textes."),he.forEach(t),ga=f(e),De=l(e,"P",{});var mt=r(De);Ko=a(mt,"La biblioth\xE8que \u{1F917} "),xt=l(mt,"EM",{});var Hp=r(xt);Yo=a(Hp,"Datasets"),Hp.forEach(t),Zo=a(mt," propose une commande tr\xE8s simple pour t\xE9l\xE9charger et mettre en cache un jeu de donn\xE9es \xE0 partir du "),zt=l(mt,"EM",{});var Ip=r(zt);Xo=a(Ip,"Hub"),Ip.forEach(t),el=a(mt,". On peut t\xE9l\xE9charger le jeu de donn\xE9es MRPC comme ceci :"),mt.forEach(t),ka=f(e),z(ms.$$.fragment,e),Ea=f(e),z(fs.$$.fragment,e),$a=f(e),Y=l(e,"P",{});var _e=r(Y);sl=a(_e,"Comme vous le voyez, on obtient un objet de type "),wt=l(_e,"CODE",{});var Vp=r(wt);tl=a(Vp,"DatasetDict"),Vp.forEach(t),nl=a(_e," qui contient le jeu de donn\xE9es d\u2019entra\xEEnement, celui de validation et celui de test. Chacun d\u2019eux contient plusieurs colonnes ("),yt=l(_e,"CODE",{});var Wp=r(yt);al=a(Wp,"sentence1"),Wp.forEach(t),ol=a(_e,", "),Ct=l(_e,"CODE",{});var Up=r(Ct);ll=a(Up,"sentence2"),Up.forEach(t),rl=a(_e,", "),Dt=l(_e,"CODE",{});var Bp=r(Dt);il=a(Bp,"label"),Bp.forEach(t),ul=a(_e," et "),Pt=l(_e,"CODE",{});var Gp=r(Pt);pl=a(Gp,"idx"),Gp.forEach(t),cl=a(_e,") et une variable nombre de lignes qui contient le nombre d\u2019\xE9l\xE9ments dans chaque jeu de donn\xE9es (il y a donc 3.668 paires de phrases dans le jeu d\u2019entra\xEEnement, 408 dans celui de validation et 1.725 dans celui de test)."),_e.forEach(t),xa=f(e),Pe=l(e,"P",{});var ft=r(Pe);dl=a(ft,"Cette commande t\xE9l\xE9charge et met en cache le jeu de donn\xE9es dans "),Tt=l(ft,"EM",{});var Jp=r(Tt);ml=a(Jp,"~/.cache/huggingface/dataset"),Jp.forEach(t),fl=a(ft,". Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d\u2019environnement "),Mt=l(ft,"CODE",{});var Qp=r(Mt);hl=a(Qp,"HF_HOME"),Qp.forEach(t),_l=a(ft,"."),ft.forEach(t),za=f(e),Be=l(e,"P",{});var zo=r(Be);vl=a(zo,"Nous pouvons acc\xE9der \xE0 chaque paire de phrase de notre objet "),St=l(zo,"CODE",{});var Kp=r(St);bl=a(Kp,"raw_datasets"),Kp.forEach(t),ql=a(zo," par les indices, comme avec un dictionnaire :"),zo.forEach(t),wa=f(e),z(hs.$$.fragment,e),ya=f(e),z(_s.$$.fragment,e),Ca=f(e),Te=l(e,"P",{});var ht=r(Te);jl=a(ht,"Nous pouvons voir que les \xE9tiquettes sont d\xE9j\xE0 des entiers, donc nous n\u2019aurons pas \xE0 faire de pr\xE9traitement ici. Pour savoir quel entier correspond \xE0 quel label, nous pouvons inspecter les "),At=l(ht,"CODE",{});var Yp=r(At);gl=a(Yp,"features"),Yp.forEach(t),kl=a(ht," de notre "),Nt=l(ht,"CODE",{});var Zp=r(Nt);El=a(Zp,"raw_train_dataset"),Zp.forEach(t),$l=a(ht,". Cela nous indiquera le type de chaque colonne :"),ht.forEach(t),Da=f(e),z(vs.$$.fragment,e),Pa=f(e),z(bs.$$.fragment,e),Ta=f(e),U=l(e,"P",{});var te=r(U);xl=a(te,"En r\xE9alit\xE9, "),Ot=l(te,"CODE",{});var Xp=r(Ot);zl=a(Xp,"label"),Xp.forEach(t),wl=a(te," est de type "),Lt=l(te,"CODE",{});var ec=r(Lt);yl=a(ec,"ClassLabel"),ec.forEach(t),Cl=a(te," et la correspondance des entiers aux noms des labels est enregistr\xE9e le dossier "),Ft=l(te,"EM",{});var sc=r(Ft);Dl=a(sc,"names"),sc.forEach(t),Pl=a(te,". "),Rt=l(te,"CODE",{});var tc=r(Rt);Tl=a(tc,"0"),tc.forEach(t),Ml=a(te," correspond \xE0  "),Ht=l(te,"CODE",{});var nc=r(Ht);Sl=a(nc,"not_equivalent"),nc.forEach(t),Al=a(te," et "),It=l(te,"CODE",{});var ac=r(It);Nl=a(ac,"1"),ac.forEach(t),Ol=a(te," correspond \xE0 "),Vt=l(te,"CODE",{});var oc=r(Vt);Ll=a(oc,"equivalent"),oc.forEach(t),Fl=a(te,"."),te.forEach(t),Ma=f(e),z(Ge.$$.fragment,e),Sa=f(e),He=l(e,"H3",{class:!0});var wo=r(He);Je=l(wo,"A",{id:!0,class:!0,href:!0});var lc=r(Je);Wt=l(lc,"SPAN",{});var rc=r(Wt);z(qs.$$.fragment,rc),rc.forEach(t),lc.forEach(t),Rl=f(wo),Ut=l(wo,"SPAN",{});var ic=r(Ut);Hl=a(ic,"Pr\xE9traitement d'un jeu de donn\xE9es"),ic.forEach(t),wo.forEach(t),Aa=f(e),$e.l(e),Ys=f(e),ce=l(e,"P",{});var os=r(ce);Il=a(os,"Pour pr\xE9traiter le jeu de donn\xE9es, nous devons convertir le texte en chiffres compr\xE9hensibles par le mod\xE8le. Comme vous l\u2019avez vu dans le "),Zs=l(os,"A",{href:!0});var uc=r(Zs);Vl=a(uc,"chapitre pr\xE9c\xE9dent"),uc.forEach(t),Wl=a(os,", cette conversion est effectu\xE9e par un "),Bt=l(os,"EM",{});var pc=r(Bt);Ul=a(pc,"tokenizer"),pc.forEach(t),Bl=a(os,". Nous pouvons fournir au "),Gt=l(os,"EM",{});var cc=r(Gt);Gl=a(cc,"tokenizer"),cc.forEach(t),Jl=a(os," une phrase ou une liste de phrases, de sorte que nous pouvons directement tokeniser toutes les premi\xE8res phrases et toutes les secondes phrases de chaque paire comme ceci :"),os.forEach(t),Na=f(e),z(js.$$.fragment,e),Oa=f(e),Qe=l(e,"P",{});var yo=r(Qe);Ql=a(yo,"Cependant, nous ne pouvons pas simplement passer deux s\xE9quences au mod\xE8le et obtenir une pr\xE9diction pour savoir si les deux phrases sont des paraphrases ou non. Nous devons traiter les deux s\xE9quences comme une paire, et appliquer le pr\xE9traitement appropri\xE9. Heureusement, le "),Jt=l(yo,"EM",{});var dc=r(Jt);Kl=a(dc,"tokenizer"),dc.forEach(t),Yl=a(yo," peut \xE9galement prendre une paire de s\xE9quences et la pr\xE9parer de la mani\xE8re attendue par notre mod\xE8le BERT :"),yo.forEach(t),La=f(e),z(gs.$$.fragment,e),Fa=f(e),z(ks.$$.fragment,e),Ra=f(e),le=l(e,"P",{});var Le=r(le);Zl=a(Le,"Nous avons discut\xE9 des cl\xE9s "),Qt=l(Le,"CODE",{});var mc=r(Qt);Xl=a(mc,"input_ids"),mc.forEach(t),er=a(Le," et "),Kt=l(Le,"CODE",{});var fc=r(Kt);sr=a(fc,"attention_mask"),fc.forEach(t),tr=a(Le," dans le "),Xs=l(Le,"A",{href:!0});var hc=r(Xs);nr=a(hc,"chapitre 2"),hc.forEach(t),ar=a(Le,", mais nous avons laiss\xE9 de c\xF4t\xE9 les "),Yt=l(Le,"CODE",{});var _c=r(Yt);or=a(_c,"token_type_ids"),_c.forEach(t),lr=a(Le,". Dans cet exemple, c\u2019est ce qui indique au mod\xE8le quelle partie de l\u2019entr\xE9e est la premi\xE8re phrase et quelle partie est la deuxi\xE8me phrase."),Le.forEach(t),Ha=f(e),z(Ke.$$.fragment,e),Ia=f(e),Ye=l(e,"P",{});var Co=r(Ye);rr=a(Co,"Si on d\xE9code les IDs dans "),Zt=l(Co,"CODE",{});var vc=r(Zt);ir=a(vc,"input_ids"),vc.forEach(t),ur=a(Co," en mots :"),Co.forEach(t),Va=f(e),z(Es.$$.fragment,e),Wa=f(e),et=l(e,"P",{});var bc=r(et);pr=a(bc,"nous aurons :"),bc.forEach(t),Ua=f(e),z($s.$$.fragment,e),Ba=f(e),Me=l(e,"P",{});var _t=r(Me);cr=a(_t,"Nous voyons donc que le mod\xE8le s\u2019attend \xE0 ce que les entr\xE9es soient de la forme "),Xt=l(_t,"CODE",{});var qc=r(Xt);dr=a(qc,"[CLS] phrase1 [SEP] phrase2 [SEP]"),qc.forEach(t),mr=a(_t," lorsqu\u2019il y a deux phrases. En alignant cela avec les "),en=l(_t,"CODE",{});var jc=r(en);fr=a(jc,"token_type_ids"),jc.forEach(t),hr=a(_t,", on obtient :"),_t.forEach(t),Ga=f(e),z(xs.$$.fragment,e),Ja=f(e),J=l(e,"P",{});var ue=r(J);_r=a(ue,"Comme vous pouvez le voir, les parties de l\u2019entr\xE9e correspondant \xE0 "),sn=l(ue,"CODE",{});var gc=r(sn);vr=a(gc,"[CLS] sentence1 [SEP]"),gc.forEach(t),br=a(ue," ont toutes un "),tn=l(ue,"EM",{});var kc=r(tn);qr=a(kc,"token"),kc.forEach(t),jr=a(ue," de type ID de "),nn=l(ue,"CODE",{});var Ec=r(nn);gr=a(Ec,"0"),Ec.forEach(t),kr=a(ue,", tandis que les autres parties, correspondant \xE0 "),an=l(ue,"CODE",{});var $c=r(an);Er=a($c,"sentence2 [SEP]"),$c.forEach(t),$r=a(ue,", ont toutes un "),on=l(ue,"EM",{});var xc=r(on);xr=a(xc,"token"),xc.forEach(t),zr=a(ue," de type ID de "),ln=l(ue,"CODE",{});var zc=r(ln);wr=a(zc,"1"),zc.forEach(t),yr=a(ue,"."),ue.forEach(t),Qa=f(e),Se=l(e,"P",{});var vt=r(Se);Cr=a(vt,"Notez que si vous choisissez un autre "),rn=l(vt,"EM",{});var wc=r(rn);Dr=a(wc,"checkpoint"),wc.forEach(t),Pr=a(vt,", vous n\u2019aurez pas n\xE9cessairement les "),un=l(vt,"CODE",{});var yc=r(un);Tr=a(yc,"token_type_ids"),yc.forEach(t),Mr=a(vt," dans vos entr\xE9es tokenis\xE9es (par exemple, ils ne sont pas retourn\xE9s si vous utilisez un mod\xE8le DistilBERT). Ils ne sont retourn\xE9s que lorsque le mod\xE8le sait quoi faire avec eux, parce qu\u2019il les a vus pendant son pr\xE9-entra\xEEnement."),vt.forEach(t),Ka=f(e),de=l(e,"P",{});var ls=r(de);Sr=a(ls,"Ici, BERT est pr\xE9-entra\xEEn\xE9 avec les "),pn=l(ls,"EM",{});var Cc=r(pn);Ar=a(Cc,"tokens"),Cc.forEach(t),Nr=a(ls," de type ID et en plus de l\u2019objectif de mod\xE9lisation du langage masqu\xE9 dont nous avons abord\xE9 dans "),st=l(ls,"A",{href:!0});var Dc=r(st);Or=a(Dc,"chapitre 1"),Dc.forEach(t),Lr=a(ls,", il a un objectif suppl\xE9mentaire appel\xE9 "),cn=l(ls,"EM",{});var Pc=r(cn);Fr=a(Pc,"pr\xE9diction de la phrase suivante"),Pc.forEach(t),Rr=a(ls,". Le but de cette t\xE2che est de mod\xE9liser la relation entre des paires de phrases."),ls.forEach(t),Ya=f(e),Ze=l(e,"P",{});var Do=r(Ze);Hr=a(Do,"Avec la pr\xE9diction de la phrase suivante, on fournit au mod\xE8le des paires de phrases (avec des "),dn=l(Do,"EM",{});var Tc=r(dn);Ir=a(Tc,"tokens"),Tc.forEach(t),Vr=a(Do," masqu\xE9s de mani\xE8re al\xE9atoire) et on lui demande de pr\xE9dire si la deuxi\xE8me phrase suit la premi\xE8re. Pour rendre la t\xE2che non triviale, la moiti\xE9 du temps, les phrases se suivent dans le document d\u2019origine dont elles ont \xE9t\xE9 extraites, et l\u2019autre moiti\xE9 du temps, les deux phrases proviennent de deux documents diff\xE9rents."),Do.forEach(t),Za=f(e),re=l(e,"P",{});var Fe=r(re);Wr=a(Fe,"En g\xE9n\xE9ral, vous n\u2019avez pas besoin de vous inqui\xE9ter de savoir s\u2019il y a ou non des "),mn=l(Fe,"CODE",{});var Mc=r(mn);Ur=a(Mc,"token_type_ids"),Mc.forEach(t),Br=a(Fe," dans vos entr\xE9es tokenis\xE9es : tant que vous utilisez le m\xEAme "),fn=l(Fe,"EM",{});var Sc=r(fn);Gr=a(Sc,"checkpoint"),Sc.forEach(t),Jr=a(Fe," pour le "),hn=l(Fe,"EM",{});var Ac=r(hn);Qr=a(Ac,"tokenizer"),Ac.forEach(t),Kr=a(Fe," et le mod\xE8le, tout ira bien puisque le "),_n=l(Fe,"EM",{});var Nc=r(_n);Yr=a(Nc,"tokenizer"),Nc.forEach(t),Zr=a(Fe," sait quoi fournir \xE0 son mod\xE8le."),Fe.forEach(t),Xa=f(e),ie=l(e,"P",{});var Re=r(ie);Xr=a(Re,"Maintenant que nous avons vu comment notre "),vn=l(Re,"EM",{});var Oc=r(vn);ei=a(Oc,"tokenizer"),Oc.forEach(t),si=a(Re," peut traiter une paire de phrases, nous pouvons l\u2019utiliser pour tokeniser l\u2019ensemble de notre jeu de donn\xE9es : comme dans le "),tt=l(Re,"A",{href:!0});var Lc=r(tt);ti=a(Lc,"chapitre pr\xE9c\xE9dent"),Lc.forEach(t),ni=a(Re,", nous pouvons fournir au "),bn=l(Re,"EM",{});var Fc=r(bn);ai=a(Fc,"tokenizer"),Fc.forEach(t),oi=a(Re," une liste de paires de phrases en lui donnant la liste des premi\xE8res phrases, puis la liste des secondes phrases. Ceci est \xE9galement compatible avec les options de remplissage et de troncature que nous avons vues dans le "),nt=l(Re,"A",{href:!0});var Rc=r(nt);li=a(Rc,"chapitre 2"),Rc.forEach(t),ri=a(Re,". Voici donc une fa\xE7on de pr\xE9traiter le jeu de donn\xE9es d\u2019entra\xEEnement :"),Re.forEach(t),eo=f(e),z(zs.$$.fragment,e),so=f(e),Z=l(e,"P",{});var ve=r(Z);ii=a(ve,"Cela fonctionne bien, mais a l\u2019inconv\xE9nient de retourner un dictionnaire (avec nos cl\xE9s, "),qn=l(ve,"CODE",{});var Hc=r(qn);ui=a(Hc,"input_ids"),Hc.forEach(t),pi=a(ve,", "),jn=l(ve,"CODE",{});var Ic=r(jn);ci=a(Ic,"attention_mask"),Ic.forEach(t),di=a(ve,", et "),gn=l(ve,"CODE",{});var Vc=r(gn);mi=a(Vc,"token_type_ids"),Vc.forEach(t),fi=a(ve,", et des valeurs qui sont des listes de listes). Cela ne fonctionnera \xE9galement que si vous avez assez de RAM pour stocker l\u2019ensemble de votre jeu de donn\xE9es pendant la tokenisation (alors que les jeux de donn\xE9es de la biblioth\xE8que \u{1F917} "),kn=l(ve,"EM",{});var Wc=r(kn);hi=a(Wc,"Datasets"),Wc.forEach(t),_i=a(ve," sont des fichiers "),ws=l(ve,"A",{href:!0,rel:!0});var Uc=r(ws);vi=a(Uc,"Apache Arrow"),Uc.forEach(t),bi=a(ve," stock\xE9s sur le disque, vous ne gardez donc en m\xE9moire que les \xE9chantillons que vous demandez)."),ve.forEach(t),to=f(e),Ae=l(e,"P",{});var bt=r(Ae);qi=a(bt,"Pour conserver les donn\xE9es sous forme de jeu de donn\xE9es, nous utiliserons la m\xE9thode "),ys=l(bt,"A",{href:!0,rel:!0});var Bc=r(ys);En=l(Bc,"CODE",{});var Gc=r(En);ji=a(Gc,"Dataset.map()"),Gc.forEach(t),Bc.forEach(t),gi=a(bt,". Cela nous permet \xE9galement une certaine flexibilit\xE9, si nous avons besoin d\u2019un pr\xE9traitement plus pouss\xE9 que la simple tokenisation. La m\xE9thode "),$n=l(bt,"CODE",{});var Jc=r($n);ki=a(Jc,"map()"),Jc.forEach(t),Ei=a(bt," fonctionne en appliquant une fonction sur chaque \xE9l\xE9ment de l\u2019ensemble de donn\xE9es, donc d\xE9finissons une fonction qui tokenise nos entr\xE9es :"),bt.forEach(t),no=f(e),z(Cs.$$.fragment,e),ao=f(e),T=l(e,"P",{});var N=r(T);$i=a(N,"Cette fonction prend un dictionnaire (comme les \xE9l\xE9ments de notre jeu de donn\xE9es) et retourne un nouveau dictionnaire avec les cl\xE9s "),xn=l(N,"CODE",{});var Qc=r(xn);xi=a(Qc,"input_ids"),Qc.forEach(t),zi=a(N,", "),zn=l(N,"CODE",{});var Kc=r(zn);wi=a(Kc,"attention_mask"),Kc.forEach(t),yi=a(N,", et "),wn=l(N,"CODE",{});var Yc=r(wn);Ci=a(Yc,"token_type_ids"),Yc.forEach(t),Di=a(N,". Notez que cela fonctionne \xE9galement si le dictionnaire "),yn=l(N,"CODE",{});var Zc=r(yn);Pi=a(Zc,"example"),Zc.forEach(t),Ti=a(N," contient plusieurs \xE9chantillons (chaque cl\xE9 \xE9tant une liste de phrases) puisque le "),Cn=l(N,"CODE",{});var Xc=r(Cn);Mi=a(Xc,"tokenizer"),Xc.forEach(t),Si=a(N," travaille sur des listes de paires de phrases, comme vu pr\xE9c\xE9demment. Cela nous permettra d\u2019utiliser l\u2019option "),Dn=l(N,"CODE",{});var ed=r(Dn);Ai=a(ed,"batched=True"),ed.forEach(t),Ni=a(N," dans notre appel \xE0 "),Pn=l(N,"CODE",{});var sd=r(Pn);Oi=a(sd,"map()"),sd.forEach(t),Li=a(N,", ce qui acc\xE9l\xE9rera grandement la tok\xE9nisation. Le "),Tn=l(N,"CODE",{});var td=r(Tn);Fi=a(td,"tokenizer"),td.forEach(t),Ri=a(N," est soutenu par un "),Mn=l(N,"EM",{});var nd=r(Mn);Hi=a(nd,"tokenizer"),nd.forEach(t),Ii=a(N," \xE9crit en Rust \xE0 partir de la biblioth\xE8que "),Xe=l(N,"A",{href:!0,rel:!0});var bp=r(Xe);Vi=a(bp,"\u{1F917} "),Sn=l(bp,"EM",{});var ad=r(Sn);Wi=a(ad,"Tokenizers"),ad.forEach(t),bp.forEach(t),Ui=a(N,". Ce "),An=l(N,"EM",{});var od=r(An);Bi=a(od,"tokenizer"),od.forEach(t),Gi=a(N," peut \xEAtre tr\xE8s rapide, mais seulement si on lui donne beaucoup d\u2019entr\xE9es en m\xEAme temps."),N.forEach(t),oo=f(e),me=l(e,"P",{});var rs=r(me);Ji=a(rs,"Notez que nous avons laiss\xE9 l\u2019argument "),Nn=l(rs,"CODE",{});var ld=r(Nn);Qi=a(ld,"padding"),ld.forEach(t),Ki=a(rs," hors de notre fonction de "),On=l(rs,"EM",{});var rd=r(On);Yi=a(rd,"tokenizer"),rd.forEach(t),Zi=a(rs," pour le moment. C\u2019est parce que le "),Ln=l(rs,"EM",{});var id=r(Ln);Xi=a(id,"padding"),id.forEach(t),eu=a(rs," de tous les \xE9chantillons \xE0 la longueur maximale n\u2019est pas efficace : il est pr\xE9f\xE9rable de remplir les \xE9chantillons lorsque nous construisons un batch, car alors nous avons seulement besoin de remplir \xE0 la longueur maximale dans ce batch, et non la longueur maximale dans l\u2019ensemble des donn\xE9es. Cela peut permettre de gagner beaucoup de temps et de puissance de traitement lorsque les entr\xE9es ont des longueurs tr\xE8s variables !"),rs.forEach(t),lo=f(e),Ne=l(e,"P",{});var qt=r(Ne);su=a(qt,"Voici comment nous appliquons la fonction de tokenization sur tous nos jeux de donn\xE9es en m\xEAme temps. Nous utilisons "),Fn=l(qt,"CODE",{});var ud=r(Fn);tu=a(ud,"batched=True"),ud.forEach(t),nu=a(qt," dans notre appel \xE0 "),Rn=l(qt,"CODE",{});var pd=r(Rn);au=a(pd,"map"),pd.forEach(t),ou=a(qt," pour que la fonction soit appliqu\xE9e \xE0 plusieurs \xE9l\xE9ments de notre jeu de donn\xE9es en une fois, et non \xE0 chaque \xE9l\xE9ment s\xE9par\xE9ment. Cela permet un pr\xE9traitement plus rapide."),qt.forEach(t),ro=f(e),z(Ds.$$.fragment,e),io=f(e),es=l(e,"P",{});var Po=r(es);lu=a(Po,"La fa\xE7on dont la biblioth\xE8que \u{1F917} "),Hn=l(Po,"EM",{});var cd=r(Hn);ru=a(cd,"Datasets"),cd.forEach(t),iu=a(Po," applique ce traitement consiste \xE0 ajouter de nouveaux champs aux jeux de donn\xE9es, un pour chaque cl\xE9 du dictionnaire renvoy\xE9 par la fonction de pr\xE9traitement :"),Po.forEach(t),uo=f(e),z(Ps.$$.fragment,e),po=f(e),X=l(e,"P",{});var be=r(X);uu=a(be,"Vous pouvez m\xEAme utiliser le multitraitement lorsque vous appliquez votre fonction de pr\xE9traitement avec "),In=l(be,"CODE",{});var dd=r(In);pu=a(dd,"map()"),dd.forEach(t),cu=a(be," en passant un argument "),Vn=l(be,"CODE",{});var md=r(Vn);du=a(md,"num_proc"),md.forEach(t),mu=a(be,". Nous ne l\u2019avons pas fait ici parce que la biblioth\xE8que \u{1F917} "),Wn=l(be,"EM",{});var fd=r(Wn);fu=a(fd,"Tokenizers"),fd.forEach(t),hu=a(be," utilise d\xE9j\xE0 plusieurs "),Un=l(be,"EM",{});var hd=r(Un);_u=a(hd,"threads"),hd.forEach(t),vu=a(be," pour tokeniser nos \xE9chantillons plus rapidement, mais si vous n\u2019utilisez pas un "),Bn=l(be,"EM",{});var _d=r(Bn);bu=a(_d,"tokenizer"),_d.forEach(t),qu=a(be," rapide soutenu par cette biblioth\xE8que, cela pourrait acc\xE9l\xE9rer votre pr\xE9traitement."),be.forEach(t),co=f(e),ee=l(e,"P",{});var qe=r(ee);ju=a(qe,"Notre "),Gn=l(qe,"CODE",{});var vd=r(Gn);gu=a(vd,"tokenize_function"),vd.forEach(t),ku=a(qe," retourne un dictionnaire avec les cl\xE9s "),Jn=l(qe,"CODE",{});var bd=r(Jn);Eu=a(bd,"input_ids"),bd.forEach(t),$u=a(qe,", "),Qn=l(qe,"CODE",{});var qd=r(Qn);xu=a(qd,"attention_mask"),qd.forEach(t),zu=a(qe,", et "),Kn=l(qe,"CODE",{});var jd=r(Kn);wu=a(jd,"token_type_ids"),jd.forEach(t),yu=a(qe,", donc ces trois champs sont ajout\xE9s \xE0 toutes les divisions de notre jeu de donn\xE9es. Notez que nous aurions \xE9galement pu modifier des champs existants si notre fonction de pr\xE9traitement avait retourn\xE9 une nouvelle valeur pour une cl\xE9 existante dans l\u2019ensemble de donn\xE9es auquel nous avons appliqu\xE9 "),Yn=l(qe,"CODE",{});var gd=r(Yn);Cu=a(gd,"map()"),gd.forEach(t),Du=a(qe,"."),qe.forEach(t),mo=f(e),ss=l(e,"P",{});var To=r(ss);Pu=a(To,"La derni\xE8re chose que nous devrons faire est de remplir tous les exemples \xE0 la longueur de l\u2019\xE9l\xE9ment le plus long lorsque nous regroupons les \xE9l\xE9ments, une technique que nous appelons le "),Zn=l(To,"EM",{});var kd=r(Zn);Tu=a(kd,"padding dynamique"),kd.forEach(t),Mu=a(To,"."),To.forEach(t),fo=f(e),Ie=l(e,"H3",{class:!0});var Mo=r(Ie);ts=l(Mo,"A",{id:!0,class:!0,href:!0});var Ed=r(ts);Xn=l(Ed,"SPAN",{});var $d=r(Xn);z(Ts.$$.fragment,$d),$d.forEach(t),Ed.forEach(t),Su=f(Mo),at=l(Mo,"SPAN",{});var qp=r(at);ea=l(qp,"I",{});var xd=r(ea);Au=a(xd,"Padding"),xd.forEach(t),Nu=a(qp," dynamique"),qp.forEach(t),Mo.forEach(t),ho=f(e),z(Ms.$$.fragment,e),_o=f(e),Ve.l(e),ot=f(e),B=l(e,"P",{});var ne=r(B);Ou=a(ne,"Pour faire cela en pratique, nous devons d\xE9finir une fonction de rassemblement qui appliquera la bonne quantit\xE9 de "),sa=l(ne,"EM",{});var zd=r(sa);Lu=a(zd,"padding"),zd.forEach(t),Fu=a(ne," aux \xE9l\xE9ments du jeu de donn\xE9es que nous voulons regrouper. Heureusement, la biblioth\xE8que \u{1F917} "),ta=l(ne,"EM",{});var wd=r(ta);Ru=a(wd,"Transformers"),wd.forEach(t),Hu=a(ne," nous fournit une telle fonction via "),na=l(ne,"CODE",{});var yd=r(na);Iu=a(yd,"DataCollatorWithPadding"),yd.forEach(t),Vu=a(ne,". Elle prend un "),aa=l(ne,"EM",{});var Cd=r(aa);Wu=a(Cd,"tokenizer"),Cd.forEach(t),Uu=a(ne," lorsque vous l\u2019instanciez (pour savoir quel "),oa=l(ne,"EM",{});var Dd=r(oa);Bu=a(Dd,"token"),Dd.forEach(t),Gu=a(ne," de "),la=l(ne,"EM",{});var Pd=r(la);Ju=a(Pd,"padding"),Pd.forEach(t),Qu=a(ne," utiliser et si le mod\xE8le s\u2019attend \xE0 ce que le "),ra=l(ne,"EM",{});var Td=r(ra);Ku=a(Td,"padding"),Td.forEach(t),Yu=a(ne," soit \xE0 gauche ou \xE0 droite des entr\xE9es) et fera tout ce dont vous avez besoin :"),ne.forEach(t),vo=f(e),ze.l(e),lt=f(e),se=l(e,"P",{});var je=r(se);Zu=a(je,"Pour tester notre nouveau jouet, prenons quelques \xE9l\xE9ments de notre jeu d\u2019entra\xEEnement avec lesquels nous allons former un batch. Ici, on supprime les colonnes "),ia=l(je,"CODE",{});var Md=r(ia);Xu=a(Md,"idx"),Md.forEach(t),ep=a(je,", "),ua=l(je,"CODE",{});var Sd=r(ua);sp=a(Sd,"sentence1"),Sd.forEach(t),tp=a(je," et "),pa=l(je,"CODE",{});var Ad=r(pa);np=a(Ad,"sentence2"),Ad.forEach(t),ap=a(je," puisque nous n\u2019en aurons pas besoin et qu\u2019elles contiennent des "),ca=l(je,"EM",{});var Nd=r(ca);op=a(Nd,"strings"),Nd.forEach(t),lp=a(je," (et nous ne pouvons pas cr\xE9er des tenseurs avec des "),da=l(je,"EM",{});var Od=r(da);rp=a(Od,"strings"),Od.forEach(t),ip=a(je,") et on regarde la longueur de chaque entr\xE9e du batch :"),je.forEach(t),bo=f(e),z(Ss.$$.fragment,e),qo=f(e),z(As.$$.fragment,e),jo=f(e),fe=l(e,"P",{});var is=r(fe);up=a(is,"Sans surprise, nous obtenons des \xE9chantillons de longueur variable, de 32 \xE0 67. Le "),ma=l(is,"EM",{});var Ld=r(ma);pp=a(Ld,"padding"),Ld.forEach(t),cp=a(is," dynamique signifie que les \xE9chantillons de ce batch doivent tous \xEAtre rembourr\xE9s \xE0 une longueur de 67, la longueur maximale dans le batch. Sans le "),fa=l(is,"EM",{});var Fd=r(fa);dp=a(Fd,"padding"),Fd.forEach(t),mp=a(is," dynamique, tous les \xE9chantillons devraient \xEAtre rembourr\xE9s \xE0 la longueur maximale du jeu de donn\xE9es entier, ou \xE0 la longueur maximale que le mod\xE8le peut accepter. V\xE9rifions \xE0 nouveau que notre "),ha=l(is,"CODE",{});var Rd=r(ha);fp=a(Rd,"data_collator"),Rd.forEach(t),hp=a(is," rembourre dynamiquement le batch correctement :"),is.forEach(t),go=f(e),z(Ns.$$.fragment,e),ko=f(e),ye.l(e),rt=f(e),z(ns.$$.fragment,e),Eo=f(e),G&&G.l(e),it=Hd(),this.h()},h(){D(i,"name","hf:doc:metadata"),D(i,"content",JSON.stringify(fm)),D(v,"id","prparer-les-donnes"),D(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(v,"href","#prparer-les-donnes"),D(g,"class","relative group"),D(Ce,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),D(Ce,"rel","nofollow"),D(We,"id","charger-un-jeu-de-donnes-depuis-le-ihubi"),D(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(We,"href","#charger-un-jeu-de-donnes-depuis-le-ihubi"),D(pe,"class","relative group"),D(cs,"href","https://huggingface.co/datasets"),D(cs,"rel","nofollow"),D(ds,"href","https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub"),D(ds,"rel","nofollow"),D(Ue,"href","https://gluebenchmark.com/"),D(Ue,"rel","nofollow"),D(Je,"id","prtraitement-dun-jeu-de-donnes"),D(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Je,"href","#prtraitement-dun-jeu-de-donnes"),D(He,"class","relative group"),D(Zs,"href","/course/fr/chapter2"),D(Xs,"href","/course/fr/chapter2"),D(st,"href","/course/fr/chapter1"),D(tt,"href","/course/fr/chapter2"),D(nt,"href","/course/fr/chapter2"),D(ws,"href","https://arrow.apache.org/"),D(ws,"rel","nofollow"),D(ys,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),D(ys,"rel","nofollow"),D(Xe,"href","https://github.com/huggingface/tokenizers"),D(Xe,"rel","nofollow"),D(ts,"id","ipaddingi-dynamique"),D(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ts,"href","#ipaddingi-dynamique"),D(Ie,"class","relative group")},m(e,p){s(document.head,i),c(e,d,p),w(u,e,p),c(e,_,p),c(e,g,p),s(g,v),s(v,b),w($,b,null),s(g,h),s(g,k),s(k,S),c(e,I,p),Os[O].m(e,p),c(e,A,p),Ls[F].m(e,p),c(e,H,p),c(e,E,p),s(E,ae),c(e,Q,p),c(e,W,p),s(W,oe),s(W,P),s(P,V),s(W,Bs),s(W,Ce),s(Ce,Gs),s(W,Js),c(e,us,p),c(e,pe,p),s(pe,We),s(We,jt),w(ps,jt,null),s(pe,Ao),s(pe,Qs),s(Qs,No),s(Qs,gt),s(gt,Oo),c(e,ja,p),Fs[ge].m(e,p),c(e,Ks,p),c(e,K,p),s(K,Lo),s(K,kt),s(kt,Fo),s(K,Ro),s(K,cs),s(cs,Ho),s(K,Io),s(K,ds),s(ds,Vo),s(K,Wo),s(K,Ue),s(Ue,Et),s(Et,Uo),s(Ue,Bo),s(K,Go),s(K,$t),s($t,Jo),s(K,Qo),c(e,ga,p),c(e,De,p),s(De,Ko),s(De,xt),s(xt,Yo),s(De,Zo),s(De,zt),s(zt,Xo),s(De,el),c(e,ka,p),w(ms,e,p),c(e,Ea,p),w(fs,e,p),c(e,$a,p),c(e,Y,p),s(Y,sl),s(Y,wt),s(wt,tl),s(Y,nl),s(Y,yt),s(yt,al),s(Y,ol),s(Y,Ct),s(Ct,ll),s(Y,rl),s(Y,Dt),s(Dt,il),s(Y,ul),s(Y,Pt),s(Pt,pl),s(Y,cl),c(e,xa,p),c(e,Pe,p),s(Pe,dl),s(Pe,Tt),s(Tt,ml),s(Pe,fl),s(Pe,Mt),s(Mt,hl),s(Pe,_l),c(e,za,p),c(e,Be,p),s(Be,vl),s(Be,St),s(St,bl),s(Be,ql),c(e,wa,p),w(hs,e,p),c(e,ya,p),w(_s,e,p),c(e,Ca,p),c(e,Te,p),s(Te,jl),s(Te,At),s(At,gl),s(Te,kl),s(Te,Nt),s(Nt,El),s(Te,$l),c(e,Da,p),w(vs,e,p),c(e,Pa,p),w(bs,e,p),c(e,Ta,p),c(e,U,p),s(U,xl),s(U,Ot),s(Ot,zl),s(U,wl),s(U,Lt),s(Lt,yl),s(U,Cl),s(U,Ft),s(Ft,Dl),s(U,Pl),s(U,Rt),s(Rt,Tl),s(U,Ml),s(U,Ht),s(Ht,Sl),s(U,Al),s(U,It),s(It,Nl),s(U,Ol),s(U,Vt),s(Vt,Ll),s(U,Fl),c(e,Ma,p),w(Ge,e,p),c(e,Sa,p),c(e,He,p),s(He,Je),s(Je,Wt),w(qs,Wt,null),s(He,Rl),s(He,Ut),s(Ut,Hl),c(e,Aa,p),Rs[Ee].m(e,p),c(e,Ys,p),c(e,ce,p),s(ce,Il),s(ce,Zs),s(Zs,Vl),s(ce,Wl),s(ce,Bt),s(Bt,Ul),s(ce,Bl),s(ce,Gt),s(Gt,Gl),s(ce,Jl),c(e,Na,p),w(js,e,p),c(e,Oa,p),c(e,Qe,p),s(Qe,Ql),s(Qe,Jt),s(Jt,Kl),s(Qe,Yl),c(e,La,p),w(gs,e,p),c(e,Fa,p),w(ks,e,p),c(e,Ra,p),c(e,le,p),s(le,Zl),s(le,Qt),s(Qt,Xl),s(le,er),s(le,Kt),s(Kt,sr),s(le,tr),s(le,Xs),s(Xs,nr),s(le,ar),s(le,Yt),s(Yt,or),s(le,lr),c(e,Ha,p),w(Ke,e,p),c(e,Ia,p),c(e,Ye,p),s(Ye,rr),s(Ye,Zt),s(Zt,ir),s(Ye,ur),c(e,Va,p),w(Es,e,p),c(e,Wa,p),c(e,et,p),s(et,pr),c(e,Ua,p),w($s,e,p),c(e,Ba,p),c(e,Me,p),s(Me,cr),s(Me,Xt),s(Xt,dr),s(Me,mr),s(Me,en),s(en,fr),s(Me,hr),c(e,Ga,p),w(xs,e,p),c(e,Ja,p),c(e,J,p),s(J,_r),s(J,sn),s(sn,vr),s(J,br),s(J,tn),s(tn,qr),s(J,jr),s(J,nn),s(nn,gr),s(J,kr),s(J,an),s(an,Er),s(J,$r),s(J,on),s(on,xr),s(J,zr),s(J,ln),s(ln,wr),s(J,yr),c(e,Qa,p),c(e,Se,p),s(Se,Cr),s(Se,rn),s(rn,Dr),s(Se,Pr),s(Se,un),s(un,Tr),s(Se,Mr),c(e,Ka,p),c(e,de,p),s(de,Sr),s(de,pn),s(pn,Ar),s(de,Nr),s(de,st),s(st,Or),s(de,Lr),s(de,cn),s(cn,Fr),s(de,Rr),c(e,Ya,p),c(e,Ze,p),s(Ze,Hr),s(Ze,dn),s(dn,Ir),s(Ze,Vr),c(e,Za,p),c(e,re,p),s(re,Wr),s(re,mn),s(mn,Ur),s(re,Br),s(re,fn),s(fn,Gr),s(re,Jr),s(re,hn),s(hn,Qr),s(re,Kr),s(re,_n),s(_n,Yr),s(re,Zr),c(e,Xa,p),c(e,ie,p),s(ie,Xr),s(ie,vn),s(vn,ei),s(ie,si),s(ie,tt),s(tt,ti),s(ie,ni),s(ie,bn),s(bn,ai),s(ie,oi),s(ie,nt),s(nt,li),s(ie,ri),c(e,eo,p),w(zs,e,p),c(e,so,p),c(e,Z,p),s(Z,ii),s(Z,qn),s(qn,ui),s(Z,pi),s(Z,jn),s(jn,ci),s(Z,di),s(Z,gn),s(gn,mi),s(Z,fi),s(Z,kn),s(kn,hi),s(Z,_i),s(Z,ws),s(ws,vi),s(Z,bi),c(e,to,p),c(e,Ae,p),s(Ae,qi),s(Ae,ys),s(ys,En),s(En,ji),s(Ae,gi),s(Ae,$n),s($n,ki),s(Ae,Ei),c(e,no,p),w(Cs,e,p),c(e,ao,p),c(e,T,p),s(T,$i),s(T,xn),s(xn,xi),s(T,zi),s(T,zn),s(zn,wi),s(T,yi),s(T,wn),s(wn,Ci),s(T,Di),s(T,yn),s(yn,Pi),s(T,Ti),s(T,Cn),s(Cn,Mi),s(T,Si),s(T,Dn),s(Dn,Ai),s(T,Ni),s(T,Pn),s(Pn,Oi),s(T,Li),s(T,Tn),s(Tn,Fi),s(T,Ri),s(T,Mn),s(Mn,Hi),s(T,Ii),s(T,Xe),s(Xe,Vi),s(Xe,Sn),s(Sn,Wi),s(T,Ui),s(T,An),s(An,Bi),s(T,Gi),c(e,oo,p),c(e,me,p),s(me,Ji),s(me,Nn),s(Nn,Qi),s(me,Ki),s(me,On),s(On,Yi),s(me,Zi),s(me,Ln),s(Ln,Xi),s(me,eu),c(e,lo,p),c(e,Ne,p),s(Ne,su),s(Ne,Fn),s(Fn,tu),s(Ne,nu),s(Ne,Rn),s(Rn,au),s(Ne,ou),c(e,ro,p),w(Ds,e,p),c(e,io,p),c(e,es,p),s(es,lu),s(es,Hn),s(Hn,ru),s(es,iu),c(e,uo,p),w(Ps,e,p),c(e,po,p),c(e,X,p),s(X,uu),s(X,In),s(In,pu),s(X,cu),s(X,Vn),s(Vn,du),s(X,mu),s(X,Wn),s(Wn,fu),s(X,hu),s(X,Un),s(Un,_u),s(X,vu),s(X,Bn),s(Bn,bu),s(X,qu),c(e,co,p),c(e,ee,p),s(ee,ju),s(ee,Gn),s(Gn,gu),s(ee,ku),s(ee,Jn),s(Jn,Eu),s(ee,$u),s(ee,Qn),s(Qn,xu),s(ee,zu),s(ee,Kn),s(Kn,wu),s(ee,yu),s(ee,Yn),s(Yn,Cu),s(ee,Du),c(e,mo,p),c(e,ss,p),s(ss,Pu),s(ss,Zn),s(Zn,Tu),s(ss,Mu),c(e,fo,p),c(e,Ie,p),s(Ie,ts),s(ts,Xn),w(Ts,Xn,null),s(Ie,Su),s(Ie,at),s(at,ea),s(ea,Au),s(at,Nu),c(e,ho,p),w(Ms,e,p),c(e,_o,p),Ve.m(e,p),c(e,ot,p),c(e,B,p),s(B,Ou),s(B,sa),s(sa,Lu),s(B,Fu),s(B,ta),s(ta,Ru),s(B,Hu),s(B,na),s(na,Iu),s(B,Vu),s(B,aa),s(aa,Wu),s(B,Uu),s(B,oa),s(oa,Bu),s(B,Gu),s(B,la),s(la,Ju),s(B,Qu),s(B,ra),s(ra,Ku),s(B,Yu),c(e,vo,p),Hs[xe].m(e,p),c(e,lt,p),c(e,se,p),s(se,Zu),s(se,ia),s(ia,Xu),s(se,ep),s(se,ua),s(ua,sp),s(se,tp),s(se,pa),s(pa,np),s(se,ap),s(se,ca),s(ca,op),s(se,lp),s(se,da),s(da,rp),s(se,ip),c(e,bo,p),w(Ss,e,p),c(e,qo,p),w(As,e,p),c(e,jo,p),c(e,fe,p),s(fe,up),s(fe,ma),s(ma,pp),s(fe,cp),s(fe,fa),s(fa,dp),s(fe,mp),s(fe,ha),s(ha,fp),s(fe,hp),c(e,go,p),w(Ns,e,p),c(e,ko,p),Is[we].m(e,p),c(e,rt,p),w(ns,e,p),c(e,Eo,p),G&&G.m(e,p),c(e,it,p),$o=!0},p(e,[p]){const Vs={};p&1&&(Vs.fw=e[0]),u.$set(Vs);let ut=O;O=kp(e),O!==ut&&(Us(),j(Os[ut],1,1,()=>{Os[ut]=null}),Ws(),L=Os[O],L||(L=Os[O]=gp[O](e),L.c()),q(L,1),L.m(A.parentNode,A));let pt=F;F=$p(e),F!==pt&&(Us(),j(Ls[pt],1,1,()=>{Ls[pt]=null}),Ws(),R=Ls[F],R||(R=Ls[F]=Ep[F](e),R.c()),q(R,1),R.m(H.parentNode,H));let ct=ge;ge=zp(e),ge!==ct&&(Us(),j(Fs[ct],1,1,()=>{Fs[ct]=null}),Ws(),ke=Fs[ge],ke||(ke=Fs[ge]=xp[ge](e),ke.c()),q(ke,1),ke.m(Ks.parentNode,Ks));const _a={};p&2&&(_a.$$scope={dirty:p,ctx:e}),Ge.$set(_a);let Oe=Ee;Ee=yp(e),Ee!==Oe&&(Us(),j(Rs[Oe],1,1,()=>{Rs[Oe]=null}),Ws(),$e=Rs[Ee],$e||($e=Rs[Ee]=wp[Ee](e),$e.c()),q($e,1),$e.m(Ys.parentNode,Ys));const va={};p&2&&(va.$$scope={dirty:p,ctx:e}),Ke.$set(va),xo!==(xo=Cp(e))&&(Ve.d(1),Ve=xo(e),Ve&&(Ve.c(),Ve.m(ot.parentNode,ot)));let dt=xe;xe=Pp(e),xe!==dt&&(Us(),j(Hs[dt],1,1,()=>{Hs[dt]=null}),Ws(),ze=Hs[xe],ze||(ze=Hs[xe]=Dp[xe](e),ze.c()),q(ze,1),ze.m(lt.parentNode,lt));let as=we;we=Mp(e),we!==as&&(Us(),j(Is[as],1,1,()=>{Is[as]=null}),Ws(),ye=Is[we],ye||(ye=Is[we]=Tp[we](e),ye.c()),q(ye,1),ye.m(rt.parentNode,rt));const ba={};p&2&&(ba.$$scope={dirty:p,ctx:e}),ns.$set(ba),e[0]==="tf"?G?p&1&&q(G,1):(G=Id(),G.c(),q(G,1),G.m(it.parentNode,it)):G&&(Us(),j(G,1,1,()=>{G=null}),Ws())},i(e){$o||(q(u.$$.fragment,e),q($.$$.fragment,e),q(L),q(R),q(ps.$$.fragment,e),q(ke),q(ms.$$.fragment,e),q(fs.$$.fragment,e),q(hs.$$.fragment,e),q(_s.$$.fragment,e),q(vs.$$.fragment,e),q(bs.$$.fragment,e),q(Ge.$$.fragment,e),q(qs.$$.fragment,e),q($e),q(js.$$.fragment,e),q(gs.$$.fragment,e),q(ks.$$.fragment,e),q(Ke.$$.fragment,e),q(Es.$$.fragment,e),q($s.$$.fragment,e),q(xs.$$.fragment,e),q(zs.$$.fragment,e),q(Cs.$$.fragment,e),q(Ds.$$.fragment,e),q(Ps.$$.fragment,e),q(Ts.$$.fragment,e),q(Ms.$$.fragment,e),q(ze),q(Ss.$$.fragment,e),q(As.$$.fragment,e),q(Ns.$$.fragment,e),q(ye),q(ns.$$.fragment,e),q(G),$o=!0)},o(e){j(u.$$.fragment,e),j($.$$.fragment,e),j(L),j(R),j(ps.$$.fragment,e),j(ke),j(ms.$$.fragment,e),j(fs.$$.fragment,e),j(hs.$$.fragment,e),j(_s.$$.fragment,e),j(vs.$$.fragment,e),j(bs.$$.fragment,e),j(Ge.$$.fragment,e),j(qs.$$.fragment,e),j($e),j(js.$$.fragment,e),j(gs.$$.fragment,e),j(ks.$$.fragment,e),j(Ke.$$.fragment,e),j(Es.$$.fragment,e),j($s.$$.fragment,e),j(xs.$$.fragment,e),j(zs.$$.fragment,e),j(Cs.$$.fragment,e),j(Ds.$$.fragment,e),j(Ps.$$.fragment,e),j(Ts.$$.fragment,e),j(Ms.$$.fragment,e),j(ze),j(Ss.$$.fragment,e),j(As.$$.fragment,e),j(Ns.$$.fragment,e),j(ye),j(ns.$$.fragment,e),j(G),$o=!1},d(e){t(i),e&&t(d),y(u,e),e&&t(_),e&&t(g),y($),e&&t(I),Os[O].d(e),e&&t(A),Ls[F].d(e),e&&t(H),e&&t(E),e&&t(Q),e&&t(W),e&&t(us),e&&t(pe),y(ps),e&&t(ja),Fs[ge].d(e),e&&t(Ks),e&&t(K),e&&t(ga),e&&t(De),e&&t(ka),y(ms,e),e&&t(Ea),y(fs,e),e&&t($a),e&&t(Y),e&&t(xa),e&&t(Pe),e&&t(za),e&&t(Be),e&&t(wa),y(hs,e),e&&t(ya),y(_s,e),e&&t(Ca),e&&t(Te),e&&t(Da),y(vs,e),e&&t(Pa),y(bs,e),e&&t(Ta),e&&t(U),e&&t(Ma),y(Ge,e),e&&t(Sa),e&&t(He),y(qs),e&&t(Aa),Rs[Ee].d(e),e&&t(Ys),e&&t(ce),e&&t(Na),y(js,e),e&&t(Oa),e&&t(Qe),e&&t(La),y(gs,e),e&&t(Fa),y(ks,e),e&&t(Ra),e&&t(le),e&&t(Ha),y(Ke,e),e&&t(Ia),e&&t(Ye),e&&t(Va),y(Es,e),e&&t(Wa),e&&t(et),e&&t(Ua),y($s,e),e&&t(Ba),e&&t(Me),e&&t(Ga),y(xs,e),e&&t(Ja),e&&t(J),e&&t(Qa),e&&t(Se),e&&t(Ka),e&&t(de),e&&t(Ya),e&&t(Ze),e&&t(Za),e&&t(re),e&&t(Xa),e&&t(ie),e&&t(eo),y(zs,e),e&&t(so),e&&t(Z),e&&t(to),e&&t(Ae),e&&t(no),y(Cs,e),e&&t(ao),e&&t(T),e&&t(oo),e&&t(me),e&&t(lo),e&&t(Ne),e&&t(ro),y(Ds,e),e&&t(io),e&&t(es),e&&t(uo),y(Ps,e),e&&t(po),e&&t(X),e&&t(co),e&&t(ee),e&&t(mo),e&&t(ss),e&&t(fo),e&&t(Ie),y(Ts),e&&t(ho),y(Ms,e),e&&t(_o),Ve.d(e),e&&t(ot),e&&t(B),e&&t(vo),Hs[xe].d(e),e&&t(lt),e&&t(se),e&&t(bo),y(Ss,e),e&&t(qo),y(As,e),e&&t(jo),e&&t(fe),e&&t(go),y(Ns,e),e&&t(ko),Is[we].d(e),e&&t(rt),y(ns,e),e&&t(Eo),G&&G.d(e),e&&t(it)}}}const fm={local:"prparer-les-donnes",sections:[{local:"charger-un-jeu-de-donnes-depuis-le-ihubi",title:"Charger un jeu de donn\xE9es depuis le <i>Hub</i>"},{local:"prtraitement-dun-jeu-de-donnes",title:"Pr\xE9traitement d'un jeu de donn\xE9es"},{local:"ipaddingi-dynamique",title:"<i>Padding</i> dynamique"}],title:"Pr\xE9parer les donn\xE9es"};function hm(C,i,d){let u="pt";return Jd(()=>{const _=new URLSearchParams(window.location.search);d(0,u=_.get("fw")||"pt")}),[u]}class Em extends Wd{constructor(i){super();Ud(this,i,hm,mm,Bd,{})}}export{Em as default,fm as metadata};
