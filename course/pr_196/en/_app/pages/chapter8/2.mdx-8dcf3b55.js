import{S as Mi,i as Fi,s as Gi,e as i,k as p,w as d,t as r,M as Qi,c as l,d as t,m as h,a as u,x as m,h as n,b as f,N as Js,F as o,g as a,y as _,q as g,o as w,B as b,v as Li}from"../../chunks/vendor-1e8b365d.js";import{T as ga}from"../../chunks/Tip-62b14c6e.js";import{Y as wa}from"../../chunks/Youtube-c2a8cc39.js";import{I as ba}from"../../chunks/IconCopyLink-483c28ba.js";import{C as y}from"../../chunks/CodeBlock-e5764662.js";import{D as Wi}from"../../chunks/DocNotebookDropdown-37d928d3.js";function Yi(U){let c,E;return{c(){c=i("p"),E=r("\u{1F6A8} See that blue box around \u201C6 frames\u201D in the traceback from Google Colab? That\u2019s a special feature of Colab, which  compresses the traceback into \u201Cframes.\u201D If you can\u2019t seem to find the source of an error, make sure you expand the full traceback by clicking on those two little arrows.")},l(k){c=l(k,"P",{});var $=u(c);E=n($,"\u{1F6A8} See that blue box around \u201C6 frames\u201D in the traceback from Google Colab? That\u2019s a special feature of Colab, which  compresses the traceback into \u201Cframes.\u201D If you can\u2019t seem to find the source of an error, make sure you expand the full traceback by clicking on those two little arrows."),$.forEach(t)},m(k,$){a(k,c,$),o(c,E)},d(k){k&&t(c)}}}function Ri(U){let c,E,k,$,P,v,T,D,H,X,N;return{c(){c=i("p"),E=r("\u{1F4A1} If you encounter an error message that is difficult to understand, just copy and paste the message into the Google or "),k=i("a"),$=r("Stack Overflow"),P=r(" search bar (yes, really!). There\u2019s a good chance that you\u2019re not the first person to encounter the error, and this is a good way to find solutions that others in the community have posted. For example, searching for "),v=i("code"),T=r("OSError: Can't load config for"),D=r(" on Stack Overflow gives several "),H=i("a"),X=r("hits"),N=r(" that could be used as a starting point for solving the problem."),this.h()},l(M){c=l(M,"P",{});var q=u(c);E=n(q,"\u{1F4A1} If you encounter an error message that is difficult to understand, just copy and paste the message into the Google or "),k=l(q,"A",{href:!0,rel:!0});var it=u(k);$=n(it,"Stack Overflow"),it.forEach(t),P=n(q," search bar (yes, really!). There\u2019s a good chance that you\u2019re not the first person to encounter the error, and this is a good way to find solutions that others in the community have posted. For example, searching for "),v=l(q,"CODE",{});var V=u(v);T=n(V,"OSError: Can't load config for"),V.forEach(t),D=n(q," on Stack Overflow gives several "),H=l(q,"A",{href:!0,rel:!0});var lt=u(H);X=n(lt,"hits"),lt.forEach(t),N=n(q," that could be used as a starting point for solving the problem."),q.forEach(t),this.h()},h(){f(k,"href","https://stackoverflow.com/"),f(k,"rel","nofollow"),f(H,"href","https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+"),f(H,"rel","nofollow")},m(M,q){a(M,c,q),o(c,E),o(c,k),o(k,$),o(c,P),o(c,v),o(v,T),o(c,D),o(c,H),o(H,X),o(c,N)},d(M){M&&t(c)}}}function Vi(U){let c,E,k,$,P;return{c(){c=i("p"),E=r("\u{1F6A8} The approach we\u2019re taking here is not foolproof, since our colleague may have tweaked the configuration of "),k=i("code"),$=r("distilbert-base-uncased"),P=r(" before fine-tuning the model. In real life, we\u2019d want to check with them first, but for the purposes of this section we\u2019ll assume they used the default configuration.")},l(v){c=l(v,"P",{});var T=u(c);E=n(T,"\u{1F6A8} The approach we\u2019re taking here is not foolproof, since our colleague may have tweaked the configuration of "),k=l(T,"CODE",{});var D=u(k);$=n(D,"distilbert-base-uncased"),D.forEach(t),P=n(T," before fine-tuning the model. In real life, we\u2019d want to check with them first, but for the purposes of this section we\u2019ll assume they used the default configuration."),T.forEach(t)},m(v,T){a(v,c,T),o(c,E),o(c,k),o(k,$),o(c,P)},d(v){v&&t(c)}}}function Bi(U){let c,E,k,$,P,v,T,D,H,X,N,M,q,it,V,lt,Us,bo,de,ko,F,Xs,me,Ks,Zs,_e,er,tr,yo,ge,$o,ut,or,vo,we,qo,K,sr,At,rr,nr,Eo,be,xo,Z,ar,Pt,ir,lr,jo,B,ee,Dt,ke,ur,It,pr,To,pt,hr,Co,ht,ye,fr,ft,cr,dr,Ao,te,mr,Ot,_r,gr,Po,$e,Do,ve,Io,G,wr,zt,br,kr,St,yr,$r,Oo,qe,Ee,ka,zo,C,vr,Ht,qr,Er,Nt,xr,jr,ct,Tr,Cr,Mt,Ar,Pr,So,oe,Ho,Q,Dr,Ft,Ir,Or,Gt,zr,Sr,No,xe,Mo,se,Fo,dt,Hr,Go,je,Te,ya,Qo,mt,Nr,Lo,Ce,Ae,$a,Wo,_t,Mr,Yo,Pe,Ro,De,Vo,L,Fr,Qt,Gr,Qr,Lt,Lr,Wr,Bo,Ie,Jo,Oe,Uo,x,Yr,Wt,Rr,Vr,Yt,Br,Jr,ze,Rt,Ur,Xr,gt,Kr,Zr,Vt,en,tn,Xo,Se,Ko,re,Zo,ne,on,Bt,sn,rn,es,He,ts,ae,nn,Jt,an,ln,os,Ne,ss,Me,rs,wt,un,ns,I,Fe,pn,Ut,hn,fn,cn,Xt,dn,mn,Kt,_n,gn,Ge,wn,Zt,bn,kn,as,bt,yn,is,J,ie,eo,Qe,$n,to,vn,ls,W,qn,oo,En,xn,so,jn,Tn,us,Le,ps,kt,Cn,hs,We,fs,le,An,yt,Pn,Dn,cs,Ye,ds,Re,ms,$t,In,_s,Ve,gs,vt,On,ws,Be,bs,A,zn,ro,Sn,Hn,no,Nn,Mn,ao,Fn,Gn,io,Qn,Ln,ks,Je,ys,Ue,$s,ue,Wn,lo,Yn,Rn,vs,Xe,qs,Ke,Es,j,Vn,uo,Bn,Jn,qt,Un,Xn,po,Kn,Zn,ho,ea,ta,fo,oa,sa,xs,Ze,js,O,ra,co,na,aa,mo,ia,la,et,ua,pa,Ts,tt,ot,va,Cs,pe,ha,_o,fa,ca,As,st,Ps,rt,Ds,he,da,nt,ma,_a,Is;return v=new ba({}),N=new Wi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"}]}}),de=new wa({props:{id:"DQ-CpJn6Rc4"}}),ge=new y({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),we=new y({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),be=new y({props:{code:`from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()`,highlighted:`<span class="hljs-keyword">from</span> distutils.dir_util <span class="hljs-keyword">import</span> copy_tree
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, snapshot_download, create_repo, get_full_repo_name


<span class="hljs-keyword">def</span> <span class="hljs-title function_">copy_repository_template</span>():
    <span class="hljs-comment"># Clone the repo and extract the local path</span>
    template_repo_id = <span class="hljs-string">&quot;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&quot;</span>
    commit_hash = <span class="hljs-string">&quot;be3eaffc28669d7932492681cd5f3e8905e358b4&quot;</span>
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    <span class="hljs-comment"># Create an empty repo on the Hub</span>
    model_name = template_repo_id.split(<span class="hljs-string">&quot;/&quot;</span>)[<span class="hljs-number">1</span>]
    create_repo(model_name, exist_ok=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Clone the empty repo</span>
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    <span class="hljs-comment"># Copy files</span>
    copy_tree(template_repo_dir, new_repo_dir)
    <span class="hljs-comment"># Push to Hub</span>
    repo.push_to_hub()`}}),ke=new ba({}),$e=new y({props:{code:`from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

model_checkpoint = get_full_repo_name(<span class="hljs-string">&quot;distillbert-base-uncased-finetuned-squad-d5716d28&quot;</span>)
reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint)`}}),ve=new y({props:{code:`"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
OSError: Can&#x27;t load config for &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27;. Make sure that:

- &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),oe=new ga({props:{$$slots:{default:[Yi]},$$scope:{ctx:U}}}),xe=new y({props:{code:`"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
Make sure that:

- &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),se=new ga({props:{$$slots:{default:[Ri]},$$scope:{ctx:U}}}),Pe=new y({props:{code:`model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)`,highlighted:`model_checkpoint = get_full_repo_name(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-squad-d5716d28&quot;</span>)
reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint)`}}),De=new y({props:{code:`"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
OSError: Can&#x27;t load config for &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27;. Make sure that:

- &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),Ie=new y({props:{code:`from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> list_repo_files

list_repo_files(repo_id=model_checkpoint)`}}),Oe=new y({props:{code:"['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']",highlighted:'[<span class="hljs-string">&#x27;.gitattributes&#x27;</span>, <span class="hljs-string">&#x27;README.md&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model.bin&#x27;</span>, <span class="hljs-string">&#x27;special_tokens_map.json&#x27;</span>, <span class="hljs-string">&#x27;tokenizer_config.json&#x27;</span>, <span class="hljs-string">&#x27;training_args.bin&#x27;</span>, <span class="hljs-string">&#x27;vocab.txt&#x27;</span>]'}}),Se=new y({props:{code:`from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

pretrained_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
config = AutoConfig.from_pretrained(pretrained_checkpoint)`}}),re=new ga({props:{warning:!0,$$slots:{default:[Vi]},$$scope:{ctx:U}}}),He=new y({props:{code:'config.push_to_hub(model_checkpoint, commit_message="Add config.json")',highlighted:'config.push_to_hub(model_checkpoint, commit_message=<span class="hljs-string">&quot;Add config.json&quot;</span>)'}}),Ne=new y({props:{code:`reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

\u{1F917} Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)`,highlighted:`reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint, revision=<span class="hljs-string">&quot;main&quot;</span>)

context = <span class="hljs-string">r&quot;&quot;&quot;
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

\u{1F917} Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
&quot;&quot;&quot;</span>

question = <span class="hljs-string">&quot;What is extractive question answering?&quot;</span>
reader(question=question, context=context)`}}),Me=new y({props:{code:`{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.38669535517692566</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">34</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">95</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;the task of extracting an answer from a text given a question&#x27;</span>}`}}),Qe=new ba({}),Le=new y({props:{code:`tokenizer = reader.tokenizer
model = reader.model`,highlighted:`tokenizer = reader.tokenizer
model = reader.model`}}),We=new y({props:{code:'question = "Which frameworks can I use?"',highlighted:'question = <span class="hljs-string">&quot;Which frameworks can I use?&quot;</span>'}}),Ye=new y({props:{code:`import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")`,highlighted:`<span class="hljs-keyword">import</span> torch

inputs = tokenizer(question, context, add_special_tokens=<span class="hljs-literal">True</span>)
input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
<span class="hljs-comment"># Get the most likely beginning of answer with the argmax of the score</span>
answer_start = torch.argmax(answer_start_scores)
<span class="hljs-comment"># Get the most likely end of answer with the argmax of the score</span>
answer_end = torch.argmax(answer_end_scores) + <span class="hljs-number">1</span>
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{question}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Answer: <span class="hljs-subst">{answer}</span>&quot;</span>)`}}),Re=new y({props:{code:`"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in &lt;module&gt;
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs[&quot;input_ids&quot;]
----&gt; 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--&gt; 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError(&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;)
    472         elif input_ids is not None:
--&gt; 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: &#x27;list&#x27; object has no attribute &#x27;size&#x27;
&quot;&quot;&quot;</span>`}}),Ve=new wa({props:{id:"rSPyvPw0p9k"}}),Be=new wa({props:{id:"5PkZ4rbHL6c"}}),Je=new y({props:{code:'inputs["input_ids"][:5]',highlighted:'inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][:<span class="hljs-number">5</span>]'}}),Ue=new y({props:{code:"[101, 2029, 7705, 2015, 2064]",highlighted:'[<span class="hljs-number">101</span>, <span class="hljs-number">2029</span>, <span class="hljs-number">7705</span>, <span class="hljs-number">2015</span>, <span class="hljs-number">2064</span>]'}}),Xe=new y({props:{code:'type(inputs["input_ids"])',highlighted:'<span class="hljs-built_in">type</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),Ke=new y({props:{code:"list",highlighted:'<span class="hljs-built_in">list</span>'}}),Ze=new y({props:{code:`~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'`,highlighted:`~<span class="hljs-regexp">/miniconda3/</span>envs<span class="hljs-regexp">/huggingface/</span>lib<span class="hljs-regexp">/python3.8/</span>site-packages<span class="hljs-regexp">/transformers/m</span>odels<span class="hljs-regexp">/distilbert/m</span>odeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    <span class="hljs-number">471</span>             raise ValueError(<span class="hljs-string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)
    <span class="hljs-number">472</span>         elif input_ids is not None:
--&gt; <span class="hljs-number">473</span>             input_shape = input_ids.<span class="hljs-keyword">size</span>()
    <span class="hljs-number">474</span>         elif inputs_embeds is not None:
    <span class="hljs-number">475</span>             input_shape = inputs_embeds.<span class="hljs-keyword">size</span>()[:-<span class="hljs-number">1</span>]

AttributeError: <span class="hljs-string">&#x27;list&#x27;</span> object has no attribute <span class="hljs-string">&#x27;size&#x27;</span>`}}),st=new y({props:{code:`inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")`,highlighted:`inputs = tokenizer(question, context, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
<span class="hljs-comment"># Get the most likely beginning of answer with the argmax of the score</span>
answer_start = torch.argmax(answer_start_scores)
<span class="hljs-comment"># Get the most likely end of answer with the argmax of the score</span>
answer_end = torch.argmax(answer_end_scores) + <span class="hljs-number">1</span>
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{question}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Answer: <span class="hljs-subst">{answer}</span>&quot;</span>)`}}),rt=new y({props:{code:`"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
&quot;&quot;&quot;</span>`}}),{c(){c=i("meta"),E=p(),k=i("h1"),$=i("a"),P=i("span"),d(v.$$.fragment),T=p(),D=i("span"),H=r("What to do when you get an error"),X=p(),d(N.$$.fragment),M=p(),q=i("p"),it=r("In this section we\u2019ll look at some common errors that can occur when you\u2019re trying to generate predictions from your freshly tuned Transformer model. This will prepare you for "),V=i("a"),lt=r("section 4"),Us=r(", where we\u2019ll explore how to debug the training phase itself."),bo=p(),d(de.$$.fragment),ko=p(),F=i("p"),Xs=r("We\u2019ve prepared a "),me=i("a"),Ks=r("template model repository"),Zs=r(" for this section, and if you want to run the code in this chapter you\u2019ll first need to copy the model to your account on the "),_e=i("a"),er=r("Hugging Face Hub"),tr=r(". To do so, first log in by running either the following in a Jupyter notebook:"),yo=p(),d(ge.$$.fragment),$o=p(),ut=i("p"),or=r("or the following in your favorite terminal:"),vo=p(),d(we.$$.fragment),qo=p(),K=i("p"),sr=r("This will prompt you to enter your username and password, and will save a token under "),At=i("em"),rr=r("~/.cache/huggingface/"),nr=r(". Once you\u2019ve logged in, you can copy the template repository with the following function:"),Eo=p(),d(be.$$.fragment),xo=p(),Z=i("p"),ar=r("Now when you call "),Pt=i("code"),ir=r("copy_repository_template()"),lr=r(", it will create a copy of the template repository under your account."),jo=p(),B=i("h2"),ee=i("a"),Dt=i("span"),d(ke.$$.fragment),ur=p(),It=i("span"),pr=r("Debugging the pipeline from \u{1F917} Transformers"),To=p(),pt=i("p"),hr=r("To kick off our journey into the wonderful world of debugging Transformer models, consider the following scenario: you\u2019re working with a colleague on a question answering project to help the customers of an e-commerce website find answers about consumer products. Your colleague shoots you a message like:"),Co=p(),ht=i("blockquote"),ye=i("p"),fr=r("G\u2019day! I just ran an experiment using the techniques in "),ft=i("a"),cr=r("Chapter 7"),dr=r(" of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is \u201Clewtun/distillbert-base-uncased-finetuned-squad-d5716d28\u201D. Feel free to test it out :)"),Ao=p(),te=i("p"),mr=r("and the first thing you think of is to load the model using the "),Ot=i("code"),_r=r("pipeline"),gr=r(" from \u{1F917} Transformers:"),Po=p(),d($e.$$.fragment),Do=p(),d(ve.$$.fragment),Io=p(),G=i("p"),wr=r("Oh no, something seems to have gone wrong! If you\u2019re new to programming, these kind of errors can seem a bit cryptic at first (what even is an "),zt=i("code"),br=r("OSError"),kr=r("?!). The error displayed here is just the last part of a much larger error report called a "),St=i("em"),yr=r("Python traceback"),$r=r(" (aka stack trace). For example, if you\u2019re running this code on Google Colab, you should see something like the following screenshot:"),Oo=p(),qe=i("div"),Ee=i("img"),zo=p(),C=i("p"),vr=r("There\u2019s a lot of information contained in these reports, so let\u2019s walk through the key parts together. The first thing to note is that tracebacks should be read "),Ht=i("em"),qr=r("from bottom to top"),Er=r(". This might sound weird if you\u2019re used to reading English text from top to bottom, but it reflects the fact that the traceback shows the sequence of function calls that the "),Nt=i("code"),xr=r("pipeline"),jr=r(" makes when downloading the model and tokenizer. (Check out "),ct=i("a"),Tr=r("Chapter 2"),Cr=r(" for more details on how the "),Mt=i("code"),Ar=r("pipeline"),Pr=r(" works under the hood.)"),So=p(),d(oe.$$.fragment),Ho=p(),Q=i("p"),Dr=r("This means that the last line of the traceback indicates the last error message and gives the name of the exception that was raised. In this case, the exception type is "),Ft=i("code"),Ir=r("OSError"),Or=r(", which indicates a system-related error. If we read the accompanying error message, we can see that there seems to be a problem with the model\u2019s "),Gt=i("em"),zr=r("config.json"),Sr=r(" file, and we\u2019re given two suggestions to fix it:"),No=p(),d(xe.$$.fragment),Mo=p(),d(se.$$.fragment),Fo=p(),dt=i("p"),Hr=r("The first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub\u2019s search bar:"),Go=p(),je=i("div"),Te=i("img"),Qo=p(),mt=i("p"),Nr=r("Hmm, it indeed looks like our colleague\u2019s model is not on the Hub\u2026 aha, but there\u2019s a typo in the name of the model! DistilBERT only has one \u201Cl\u201D in its name, so let\u2019s fix that and look for \u201Clewtun/distilbert-base-uncased-finetuned-squad-d5716d28\u201D instead:"),Lo=p(),Ce=i("div"),Ae=i("img"),Wo=p(),_t=i("p"),Mr=r("Okay, this got a hit. Now let\u2019s try to download the model again with the correct model ID:"),Yo=p(),d(Pe.$$.fragment),Ro=p(),d(De.$$.fragment),Vo=p(),L=i("p"),Fr=r("Argh, foiled again \u2014 welcome to the daily life of a machine learning engineer! Since we\u2019ve fixed the model ID, the problem must lie in the repository itself. A quick way to access the contents of a repository on the \u{1F917} Hub is via the "),Qt=i("code"),Gr=r("list_repo_files()"),Qr=r(" function of the "),Lt=i("code"),Lr=r("huggingface_hub"),Wr=r(" library:"),Bo=p(),d(Ie.$$.fragment),Jo=p(),d(Oe.$$.fragment),Uo=p(),x=i("p"),Yr=r("Interesting \u2014 there doesn\u2019t seem to be a "),Wt=i("em"),Rr=r("config.json"),Vr=r(" file in the repository! No wonder our "),Yt=i("code"),Br=r("pipeline"),Jr=r(" couldn\u2019t load the model; our colleague must have forgotten to push this file to the Hub after they fine-tuned it. In this case, the problem seems pretty straightforward to fix: we could ask them to add the file, or, since we can see from the model ID that the pretrained model used was "),ze=i("a"),Rt=i("code"),Ur=r("distilbert-base-uncased"),Xr=r(", we can download the config for this model and push it to our repo to see if that resolves the problem. Let\u2019s try that. Using the techniques we learned in "),gt=i("a"),Kr=r("Chapter 2"),Zr=r(", we can download the model\u2019s configuration with the "),Vt=i("code"),en=r("AutoConfig"),tn=r(" class:"),Xo=p(),d(Se.$$.fragment),Ko=p(),d(re.$$.fragment),Zo=p(),ne=i("p"),on=r("We can then push this to our model repository with the configuration\u2019s "),Bt=i("code"),sn=r("push_to_hub()"),rn=r(" function:"),es=p(),d(He.$$.fragment),ts=p(),ae=i("p"),nn=r("Now we can test if this worked by loading the model from the latest commit on the "),Jt=i("code"),an=r("main"),ln=r(" branch:"),os=p(),d(Ne.$$.fragment),ss=p(),d(Me.$$.fragment),rs=p(),wt=i("p"),un=r("Woohoo, it worked! Let\u2019s recap what you\u2019ve just learned:"),ns=p(),I=i("ul"),Fe=i("li"),pn=r("The error messages in Python are known as "),Ut=i("em"),hn=r("tracebacks"),fn=r(" and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem."),cn=p(),Xt=i("li"),dn=r("If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs."),mn=p(),Kt=i("li"),_n=r("If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue."),gn=p(),Ge=i("li"),wn=r("The "),Zt=i("code"),bn=r("huggingface_hub"),kn=r(`
// \u{1F917} Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.`),as=p(),bt=i("p"),yn=r("Now that you know how to debug a pipeline, let\u2019s take a look at a trickier example in the forward pass of the model itself."),is=p(),J=i("h2"),ie=i("a"),eo=i("span"),d(Qe.$$.fragment),$n=p(),to=i("span"),vn=r("Debugging the forward pass of your model"),ls=p(),W=i("p"),qn=r("Although the "),oo=i("code"),En=r("pipeline"),xn=r(" is great for most applications where you need to quickly generate predictions, sometimes you\u2019ll need to access the model\u2019s logits (say, if you have some custom post-processing that you\u2019d like to apply). To see what can go wrong in this case, let\u2019s first grab the model and tokenizer from our "),so=i("code"),jn=r("pipeline"),Tn=r(":"),us=p(),d(Le.$$.fragment),ps=p(),kt=i("p"),Cn=r("Next we need a question, so let\u2019s see if our favorite frameworks are supported:"),hs=p(),d(We.$$.fragment),fs=p(),le=i("p"),An=r("As we saw in "),yt=i("a"),Pn=r("Chapter 7"),Dn=r(", the usual steps we need to take are tokenizing the inputs, extracting the logits of the start and end tokens, and then decoding the answer span:"),cs=p(),d(Ye.$$.fragment),ds=p(),d(Re.$$.fragment),ms=p(),$t=i("p"),In=r("Oh dear, it looks like we have a bug in our code! But we\u2019re not afraid of a little debugging. You can use the Python debugger in a notebook:"),_s=p(),d(Ve.$$.fragment),gs=p(),vt=i("p"),On=r("or in a terminal:"),ws=p(),d(Be.$$.fragment),bs=p(),A=i("p"),zn=r("Here, reading the error message tells us that "),ro=i("code"),Sn=r("'list' object has no attribute 'size'"),Hn=r(", and we can see a "),no=i("code"),Nn=r("-->"),Mn=r(" arrow pointing to the line where the problem was raised in "),ao=i("code"),Fn=r("model(**inputs)"),Gn=r(".You can debug this interactively using the Python debugger, but for now we\u2019ll simply print out a slice of "),io=i("code"),Qn=r("inputs"),Ln=r(" to see what we have:"),ks=p(),d(Je.$$.fragment),ys=p(),d(Ue.$$.fragment),$s=p(),ue=i("p"),Wn=r("This certainly looks like an ordinary Python "),lo=i("code"),Yn=r("list"),Rn=r(", but let\u2019s double-check the type:"),vs=p(),d(Xe.$$.fragment),qs=p(),d(Ke.$$.fragment),Es=p(),j=i("p"),Vn=r("Yep, that\u2019s a Python "),uo=i("code"),Bn=r("list"),Jn=r(" for sure. So what went wrong? Recall from "),qt=i("a"),Un=r("Chapter 2"),Xn=r(" that the "),po=i("code"),Kn=r("AutoModelForXxx"),Zn=r(" classes in \u{1F917} Transformers operate on "),ho=i("em"),ea=r("tensors"),ta=r(" (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using "),fo=i("code"),oa=r("Tensor.size()"),sa=r(" in, say, PyTorch. Let\u2019s take another look at the traceback, to see which line triggered the exception:"),xs=p(),d(Ze.$$.fragment),js=p(),O=i("p"),ra=r("It looks like our code tried to call "),co=i("code"),na=r("input_ids.size()"),aa=r(", but this clearly won\u2019t work for a Python "),mo=i("code"),ia=r("list"),la=r(", which is just a container. How can we solve this problem? Searching for the error message on Stack Overflow gives quite a few relevant "),et=i("a"),ua=r("hits"),pa=r(". Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below:"),Ts=p(),tt=i("div"),ot=i("img"),Cs=p(),pe=i("p"),ha=r("The answer recommends that we add "),_o=i("code"),fa=r("return_tensors='pt'"),ca=r(" to the tokenizer, so let\u2019s see if that works for us:"),As=p(),d(st.$$.fragment),Ps=p(),d(rt.$$.fragment),Ds=p(),he=i("p"),da=r("Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won\u2019t always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the "),nt=i("a"),ma=r("Hugging Face forums"),_a=r(" that can help you out! In the next section, we\u2019ll take a look at how you can craft good forum questions that are likely to get answered."),this.h()},l(e){const s=Qi('[data-svelte="svelte-1phssyn"]',document.head);c=l(s,"META",{name:!0,content:!0}),s.forEach(t),E=h(e),k=l(e,"H1",{class:!0});var at=u(k);$=l(at,"A",{id:!0,class:!0,href:!0});var go=u($);P=l(go,"SPAN",{});var wo=u(P);m(v.$$.fragment,wo),wo.forEach(t),go.forEach(t),T=h(at),D=l(at,"SPAN",{});var qa=u(D);H=n(qa,"What to do when you get an error"),qa.forEach(t),at.forEach(t),X=h(e),m(N.$$.fragment,e),M=h(e),q=l(e,"P",{});var Os=u(q);it=n(Os,"In this section we\u2019ll look at some common errors that can occur when you\u2019re trying to generate predictions from your freshly tuned Transformer model. This will prepare you for "),V=l(Os,"A",{href:!0});var Ea=u(V);lt=n(Ea,"section 4"),Ea.forEach(t),Us=n(Os,", where we\u2019ll explore how to debug the training phase itself."),Os.forEach(t),bo=h(e),m(de.$$.fragment,e),ko=h(e),F=l(e,"P",{});var Et=u(F);Xs=n(Et,"We\u2019ve prepared a "),me=l(Et,"A",{href:!0,rel:!0});var xa=u(me);Ks=n(xa,"template model repository"),xa.forEach(t),Zs=n(Et," for this section, and if you want to run the code in this chapter you\u2019ll first need to copy the model to your account on the "),_e=l(Et,"A",{href:!0,rel:!0});var ja=u(_e);er=n(ja,"Hugging Face Hub"),ja.forEach(t),tr=n(Et,". To do so, first log in by running either the following in a Jupyter notebook:"),Et.forEach(t),yo=h(e),m(ge.$$.fragment,e),$o=h(e),ut=l(e,"P",{});var Ta=u(ut);or=n(Ta,"or the following in your favorite terminal:"),Ta.forEach(t),vo=h(e),m(we.$$.fragment,e),qo=h(e),K=l(e,"P",{});var zs=u(K);sr=n(zs,"This will prompt you to enter your username and password, and will save a token under "),At=l(zs,"EM",{});var Ca=u(At);rr=n(Ca,"~/.cache/huggingface/"),Ca.forEach(t),nr=n(zs,". Once you\u2019ve logged in, you can copy the template repository with the following function:"),zs.forEach(t),Eo=h(e),m(be.$$.fragment,e),xo=h(e),Z=l(e,"P",{});var Ss=u(Z);ar=n(Ss,"Now when you call "),Pt=l(Ss,"CODE",{});var Aa=u(Pt);ir=n(Aa,"copy_repository_template()"),Aa.forEach(t),lr=n(Ss,", it will create a copy of the template repository under your account."),Ss.forEach(t),jo=h(e),B=l(e,"H2",{class:!0});var Hs=u(B);ee=l(Hs,"A",{id:!0,class:!0,href:!0});var Pa=u(ee);Dt=l(Pa,"SPAN",{});var Da=u(Dt);m(ke.$$.fragment,Da),Da.forEach(t),Pa.forEach(t),ur=h(Hs),It=l(Hs,"SPAN",{});var Ia=u(It);pr=n(Ia,"Debugging the pipeline from \u{1F917} Transformers"),Ia.forEach(t),Hs.forEach(t),To=h(e),pt=l(e,"P",{});var Oa=u(pt);hr=n(Oa,"To kick off our journey into the wonderful world of debugging Transformer models, consider the following scenario: you\u2019re working with a colleague on a question answering project to help the customers of an e-commerce website find answers about consumer products. Your colleague shoots you a message like:"),Oa.forEach(t),Co=h(e),ht=l(e,"BLOCKQUOTE",{});var za=u(ht);ye=l(za,"P",{});var Ns=u(ye);fr=n(Ns,"G\u2019day! I just ran an experiment using the techniques in "),ft=l(Ns,"A",{href:!0});var Sa=u(ft);cr=n(Sa,"Chapter 7"),Sa.forEach(t),dr=n(Ns," of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is \u201Clewtun/distillbert-base-uncased-finetuned-squad-d5716d28\u201D. Feel free to test it out :)"),Ns.forEach(t),za.forEach(t),Ao=h(e),te=l(e,"P",{});var Ms=u(te);mr=n(Ms,"and the first thing you think of is to load the model using the "),Ot=l(Ms,"CODE",{});var Ha=u(Ot);_r=n(Ha,"pipeline"),Ha.forEach(t),gr=n(Ms," from \u{1F917} Transformers:"),Ms.forEach(t),Po=h(e),m($e.$$.fragment,e),Do=h(e),m(ve.$$.fragment,e),Io=h(e),G=l(e,"P",{});var xt=u(G);wr=n(xt,"Oh no, something seems to have gone wrong! If you\u2019re new to programming, these kind of errors can seem a bit cryptic at first (what even is an "),zt=l(xt,"CODE",{});var Na=u(zt);br=n(Na,"OSError"),Na.forEach(t),kr=n(xt,"?!). The error displayed here is just the last part of a much larger error report called a "),St=l(xt,"EM",{});var Ma=u(St);yr=n(Ma,"Python traceback"),Ma.forEach(t),$r=n(xt," (aka stack trace). For example, if you\u2019re running this code on Google Colab, you should see something like the following screenshot:"),xt.forEach(t),Oo=h(e),qe=l(e,"DIV",{class:!0});var Fa=u(qe);Ee=l(Fa,"IMG",{src:!0,alt:!0,width:!0}),Fa.forEach(t),zo=h(e),C=l(e,"P",{});var Y=u(C);vr=n(Y,"There\u2019s a lot of information contained in these reports, so let\u2019s walk through the key parts together. The first thing to note is that tracebacks should be read "),Ht=l(Y,"EM",{});var Ga=u(Ht);qr=n(Ga,"from bottom to top"),Ga.forEach(t),Er=n(Y,". This might sound weird if you\u2019re used to reading English text from top to bottom, but it reflects the fact that the traceback shows the sequence of function calls that the "),Nt=l(Y,"CODE",{});var Qa=u(Nt);xr=n(Qa,"pipeline"),Qa.forEach(t),jr=n(Y," makes when downloading the model and tokenizer. (Check out "),ct=l(Y,"A",{href:!0});var La=u(ct);Tr=n(La,"Chapter 2"),La.forEach(t),Cr=n(Y," for more details on how the "),Mt=l(Y,"CODE",{});var Wa=u(Mt);Ar=n(Wa,"pipeline"),Wa.forEach(t),Pr=n(Y," works under the hood.)"),Y.forEach(t),So=h(e),m(oe.$$.fragment,e),Ho=h(e),Q=l(e,"P",{});var jt=u(Q);Dr=n(jt,"This means that the last line of the traceback indicates the last error message and gives the name of the exception that was raised. In this case, the exception type is "),Ft=l(jt,"CODE",{});var Ya=u(Ft);Ir=n(Ya,"OSError"),Ya.forEach(t),Or=n(jt,", which indicates a system-related error. If we read the accompanying error message, we can see that there seems to be a problem with the model\u2019s "),Gt=l(jt,"EM",{});var Ra=u(Gt);zr=n(Ra,"config.json"),Ra.forEach(t),Sr=n(jt," file, and we\u2019re given two suggestions to fix it:"),jt.forEach(t),No=h(e),m(xe.$$.fragment,e),Mo=h(e),m(se.$$.fragment,e),Fo=h(e),dt=l(e,"P",{});var Va=u(dt);Hr=n(Va,"The first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub\u2019s search bar:"),Va.forEach(t),Go=h(e),je=l(e,"DIV",{class:!0});var Ba=u(je);Te=l(Ba,"IMG",{src:!0,alt:!0,width:!0}),Ba.forEach(t),Qo=h(e),mt=l(e,"P",{});var Ja=u(mt);Nr=n(Ja,"Hmm, it indeed looks like our colleague\u2019s model is not on the Hub\u2026 aha, but there\u2019s a typo in the name of the model! DistilBERT only has one \u201Cl\u201D in its name, so let\u2019s fix that and look for \u201Clewtun/distilbert-base-uncased-finetuned-squad-d5716d28\u201D instead:"),Ja.forEach(t),Lo=h(e),Ce=l(e,"DIV",{class:!0});var Ua=u(Ce);Ae=l(Ua,"IMG",{src:!0,alt:!0,width:!0}),Ua.forEach(t),Wo=h(e),_t=l(e,"P",{});var Xa=u(_t);Mr=n(Xa,"Okay, this got a hit. Now let\u2019s try to download the model again with the correct model ID:"),Xa.forEach(t),Yo=h(e),m(Pe.$$.fragment,e),Ro=h(e),m(De.$$.fragment,e),Vo=h(e),L=l(e,"P",{});var Tt=u(L);Fr=n(Tt,"Argh, foiled again \u2014 welcome to the daily life of a machine learning engineer! Since we\u2019ve fixed the model ID, the problem must lie in the repository itself. A quick way to access the contents of a repository on the \u{1F917} Hub is via the "),Qt=l(Tt,"CODE",{});var Ka=u(Qt);Gr=n(Ka,"list_repo_files()"),Ka.forEach(t),Qr=n(Tt," function of the "),Lt=l(Tt,"CODE",{});var Za=u(Lt);Lr=n(Za,"huggingface_hub"),Za.forEach(t),Wr=n(Tt," library:"),Tt.forEach(t),Bo=h(e),m(Ie.$$.fragment,e),Jo=h(e),m(Oe.$$.fragment,e),Uo=h(e),x=l(e,"P",{});var z=u(x);Yr=n(z,"Interesting \u2014 there doesn\u2019t seem to be a "),Wt=l(z,"EM",{});var ei=u(Wt);Rr=n(ei,"config.json"),ei.forEach(t),Vr=n(z," file in the repository! No wonder our "),Yt=l(z,"CODE",{});var ti=u(Yt);Br=n(ti,"pipeline"),ti.forEach(t),Jr=n(z," couldn\u2019t load the model; our colleague must have forgotten to push this file to the Hub after they fine-tuned it. In this case, the problem seems pretty straightforward to fix: we could ask them to add the file, or, since we can see from the model ID that the pretrained model used was "),ze=l(z,"A",{href:!0,rel:!0});var oi=u(ze);Rt=l(oi,"CODE",{});var si=u(Rt);Ur=n(si,"distilbert-base-uncased"),si.forEach(t),oi.forEach(t),Xr=n(z,", we can download the config for this model and push it to our repo to see if that resolves the problem. Let\u2019s try that. Using the techniques we learned in "),gt=l(z,"A",{href:!0});var ri=u(gt);Kr=n(ri,"Chapter 2"),ri.forEach(t),Zr=n(z,", we can download the model\u2019s configuration with the "),Vt=l(z,"CODE",{});var ni=u(Vt);en=n(ni,"AutoConfig"),ni.forEach(t),tn=n(z," class:"),z.forEach(t),Xo=h(e),m(Se.$$.fragment,e),Ko=h(e),m(re.$$.fragment,e),Zo=h(e),ne=l(e,"P",{});var Fs=u(ne);on=n(Fs,"We can then push this to our model repository with the configuration\u2019s "),Bt=l(Fs,"CODE",{});var ai=u(Bt);sn=n(ai,"push_to_hub()"),ai.forEach(t),rn=n(Fs," function:"),Fs.forEach(t),es=h(e),m(He.$$.fragment,e),ts=h(e),ae=l(e,"P",{});var Gs=u(ae);nn=n(Gs,"Now we can test if this worked by loading the model from the latest commit on the "),Jt=l(Gs,"CODE",{});var ii=u(Jt);an=n(ii,"main"),ii.forEach(t),ln=n(Gs," branch:"),Gs.forEach(t),os=h(e),m(Ne.$$.fragment,e),ss=h(e),m(Me.$$.fragment,e),rs=h(e),wt=l(e,"P",{});var li=u(wt);un=n(li,"Woohoo, it worked! Let\u2019s recap what you\u2019ve just learned:"),li.forEach(t),ns=h(e),I=l(e,"UL",{});var fe=u(I);Fe=l(fe,"LI",{});var Qs=u(Fe);pn=n(Qs,"The error messages in Python are known as "),Ut=l(Qs,"EM",{});var ui=u(Ut);hn=n(ui,"tracebacks"),ui.forEach(t),fn=n(Qs," and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem."),Qs.forEach(t),cn=h(fe),Xt=l(fe,"LI",{});var pi=u(Xt);dn=n(pi,"If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs."),pi.forEach(t),mn=h(fe),Kt=l(fe,"LI",{});var hi=u(Kt);_n=n(hi,"If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue."),hi.forEach(t),gn=h(fe),Ge=l(fe,"LI",{});var Ls=u(Ge);wn=n(Ls,"The "),Zt=l(Ls,"CODE",{});var fi=u(Zt);bn=n(fi,"huggingface_hub"),fi.forEach(t),kn=n(Ls,`
// \u{1F917} Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.`),Ls.forEach(t),fe.forEach(t),as=h(e),bt=l(e,"P",{});var ci=u(bt);yn=n(ci,"Now that you know how to debug a pipeline, let\u2019s take a look at a trickier example in the forward pass of the model itself."),ci.forEach(t),is=h(e),J=l(e,"H2",{class:!0});var Ws=u(J);ie=l(Ws,"A",{id:!0,class:!0,href:!0});var di=u(ie);eo=l(di,"SPAN",{});var mi=u(eo);m(Qe.$$.fragment,mi),mi.forEach(t),di.forEach(t),$n=h(Ws),to=l(Ws,"SPAN",{});var _i=u(to);vn=n(_i,"Debugging the forward pass of your model"),_i.forEach(t),Ws.forEach(t),ls=h(e),W=l(e,"P",{});var Ct=u(W);qn=n(Ct,"Although the "),oo=l(Ct,"CODE",{});var gi=u(oo);En=n(gi,"pipeline"),gi.forEach(t),xn=n(Ct," is great for most applications where you need to quickly generate predictions, sometimes you\u2019ll need to access the model\u2019s logits (say, if you have some custom post-processing that you\u2019d like to apply). To see what can go wrong in this case, let\u2019s first grab the model and tokenizer from our "),so=l(Ct,"CODE",{});var wi=u(so);jn=n(wi,"pipeline"),wi.forEach(t),Tn=n(Ct,":"),Ct.forEach(t),us=h(e),m(Le.$$.fragment,e),ps=h(e),kt=l(e,"P",{});var bi=u(kt);Cn=n(bi,"Next we need a question, so let\u2019s see if our favorite frameworks are supported:"),bi.forEach(t),hs=h(e),m(We.$$.fragment,e),fs=h(e),le=l(e,"P",{});var Ys=u(le);An=n(Ys,"As we saw in "),yt=l(Ys,"A",{href:!0});var ki=u(yt);Pn=n(ki,"Chapter 7"),ki.forEach(t),Dn=n(Ys,", the usual steps we need to take are tokenizing the inputs, extracting the logits of the start and end tokens, and then decoding the answer span:"),Ys.forEach(t),cs=h(e),m(Ye.$$.fragment,e),ds=h(e),m(Re.$$.fragment,e),ms=h(e),$t=l(e,"P",{});var yi=u($t);In=n(yi,"Oh dear, it looks like we have a bug in our code! But we\u2019re not afraid of a little debugging. You can use the Python debugger in a notebook:"),yi.forEach(t),_s=h(e),m(Ve.$$.fragment,e),gs=h(e),vt=l(e,"P",{});var $i=u(vt);On=n($i,"or in a terminal:"),$i.forEach(t),ws=h(e),m(Be.$$.fragment,e),bs=h(e),A=l(e,"P",{});var R=u(A);zn=n(R,"Here, reading the error message tells us that "),ro=l(R,"CODE",{});var vi=u(ro);Sn=n(vi,"'list' object has no attribute 'size'"),vi.forEach(t),Hn=n(R,", and we can see a "),no=l(R,"CODE",{});var qi=u(no);Nn=n(qi,"-->"),qi.forEach(t),Mn=n(R," arrow pointing to the line where the problem was raised in "),ao=l(R,"CODE",{});var Ei=u(ao);Fn=n(Ei,"model(**inputs)"),Ei.forEach(t),Gn=n(R,".You can debug this interactively using the Python debugger, but for now we\u2019ll simply print out a slice of "),io=l(R,"CODE",{});var xi=u(io);Qn=n(xi,"inputs"),xi.forEach(t),Ln=n(R," to see what we have:"),R.forEach(t),ks=h(e),m(Je.$$.fragment,e),ys=h(e),m(Ue.$$.fragment,e),$s=h(e),ue=l(e,"P",{});var Rs=u(ue);Wn=n(Rs,"This certainly looks like an ordinary Python "),lo=l(Rs,"CODE",{});var ji=u(lo);Yn=n(ji,"list"),ji.forEach(t),Rn=n(Rs,", but let\u2019s double-check the type:"),Rs.forEach(t),vs=h(e),m(Xe.$$.fragment,e),qs=h(e),m(Ke.$$.fragment,e),Es=h(e),j=l(e,"P",{});var S=u(j);Vn=n(S,"Yep, that\u2019s a Python "),uo=l(S,"CODE",{});var Ti=u(uo);Bn=n(Ti,"list"),Ti.forEach(t),Jn=n(S," for sure. So what went wrong? Recall from "),qt=l(S,"A",{href:!0});var Ci=u(qt);Un=n(Ci,"Chapter 2"),Ci.forEach(t),Xn=n(S," that the "),po=l(S,"CODE",{});var Ai=u(po);Kn=n(Ai,"AutoModelForXxx"),Ai.forEach(t),Zn=n(S," classes in \u{1F917} Transformers operate on "),ho=l(S,"EM",{});var Pi=u(ho);ea=n(Pi,"tensors"),Pi.forEach(t),ta=n(S," (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using "),fo=l(S,"CODE",{});var Di=u(fo);oa=n(Di,"Tensor.size()"),Di.forEach(t),sa=n(S," in, say, PyTorch. Let\u2019s take another look at the traceback, to see which line triggered the exception:"),S.forEach(t),xs=h(e),m(Ze.$$.fragment,e),js=h(e),O=l(e,"P",{});var ce=u(O);ra=n(ce,"It looks like our code tried to call "),co=l(ce,"CODE",{});var Ii=u(co);na=n(Ii,"input_ids.size()"),Ii.forEach(t),aa=n(ce,", but this clearly won\u2019t work for a Python "),mo=l(ce,"CODE",{});var Oi=u(mo);ia=n(Oi,"list"),Oi.forEach(t),la=n(ce,", which is just a container. How can we solve this problem? Searching for the error message on Stack Overflow gives quite a few relevant "),et=l(ce,"A",{href:!0,rel:!0});var zi=u(et);ua=n(zi,"hits"),zi.forEach(t),pa=n(ce,". Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below:"),ce.forEach(t),Ts=h(e),tt=l(e,"DIV",{class:!0});var Si=u(tt);ot=l(Si,"IMG",{src:!0,alt:!0,width:!0}),Si.forEach(t),Cs=h(e),pe=l(e,"P",{});var Vs=u(pe);ha=n(Vs,"The answer recommends that we add "),_o=l(Vs,"CODE",{});var Hi=u(_o);fa=n(Hi,"return_tensors='pt'"),Hi.forEach(t),ca=n(Vs," to the tokenizer, so let\u2019s see if that works for us:"),Vs.forEach(t),As=h(e),m(st.$$.fragment,e),Ps=h(e),m(rt.$$.fragment,e),Ds=h(e),he=l(e,"P",{});var Bs=u(he);da=n(Bs,"Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won\u2019t always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the "),nt=l(Bs,"A",{href:!0,rel:!0});var Ni=u(nt);ma=n(Ni,"Hugging Face forums"),Ni.forEach(t),_a=n(Bs," that can help you out! In the next section, we\u2019ll take a look at how you can craft good forum questions that are likely to get answered."),Bs.forEach(t),this.h()},h(){f(c,"name","hf:doc:metadata"),f(c,"content",JSON.stringify(Ji)),f($,"id","what-to-do-when-you-get-an-error"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#what-to-do-when-you-get-an-error"),f(k,"class","relative group"),f(V,"href","/course/chapter8/section4"),f(me,"href","https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"),f(me,"rel","nofollow"),f(_e,"href","https://huggingface.co"),f(_e,"rel","nofollow"),f(ee,"id","debugging-the-pipeline-from-transformers"),f(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ee,"href","#debugging-the-pipeline-from-transformers"),f(B,"class","relative group"),f(ft,"href","/course/chapter7/7"),Js(Ee.src,ka="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png")||f(Ee,"src",ka),f(Ee,"alt","A Python traceback."),f(Ee,"width","100%"),f(qe,"class","flex justify-center"),f(ct,"href","/course/chapter2"),Js(Te.src,ya="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png")||f(Te,"src",ya),f(Te,"alt","The wrong model name."),f(Te,"width","100%"),f(je,"class","flex justify-center"),Js(Ae.src,$a="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png")||f(Ae,"src",$a),f(Ae,"alt","The right model name."),f(Ae,"width","100%"),f(Ce,"class","flex justify-center"),f(ze,"href","https://huggingface.co/distilbert-base-uncased"),f(ze,"rel","nofollow"),f(gt,"href","/course/chapter2"),f(ie,"id","debugging-the-forward-pass-of-your-model"),f(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ie,"href","#debugging-the-forward-pass-of-your-model"),f(J,"class","relative group"),f(yt,"href","/course/chapter7"),f(qt,"href","/course/chapter2"),f(et,"href","https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f"),f(et,"rel","nofollow"),Js(ot.src,va="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png")||f(ot,"src",va),f(ot,"alt","An answer from Stack Overflow."),f(ot,"width","100%"),f(tt,"class","flex justify-center"),f(nt,"href","https://discuss.huggingface.co/"),f(nt,"rel","nofollow")},m(e,s){o(document.head,c),a(e,E,s),a(e,k,s),o(k,$),o($,P),_(v,P,null),o(k,T),o(k,D),o(D,H),a(e,X,s),_(N,e,s),a(e,M,s),a(e,q,s),o(q,it),o(q,V),o(V,lt),o(q,Us),a(e,bo,s),_(de,e,s),a(e,ko,s),a(e,F,s),o(F,Xs),o(F,me),o(me,Ks),o(F,Zs),o(F,_e),o(_e,er),o(F,tr),a(e,yo,s),_(ge,e,s),a(e,$o,s),a(e,ut,s),o(ut,or),a(e,vo,s),_(we,e,s),a(e,qo,s),a(e,K,s),o(K,sr),o(K,At),o(At,rr),o(K,nr),a(e,Eo,s),_(be,e,s),a(e,xo,s),a(e,Z,s),o(Z,ar),o(Z,Pt),o(Pt,ir),o(Z,lr),a(e,jo,s),a(e,B,s),o(B,ee),o(ee,Dt),_(ke,Dt,null),o(B,ur),o(B,It),o(It,pr),a(e,To,s),a(e,pt,s),o(pt,hr),a(e,Co,s),a(e,ht,s),o(ht,ye),o(ye,fr),o(ye,ft),o(ft,cr),o(ye,dr),a(e,Ao,s),a(e,te,s),o(te,mr),o(te,Ot),o(Ot,_r),o(te,gr),a(e,Po,s),_($e,e,s),a(e,Do,s),_(ve,e,s),a(e,Io,s),a(e,G,s),o(G,wr),o(G,zt),o(zt,br),o(G,kr),o(G,St),o(St,yr),o(G,$r),a(e,Oo,s),a(e,qe,s),o(qe,Ee),a(e,zo,s),a(e,C,s),o(C,vr),o(C,Ht),o(Ht,qr),o(C,Er),o(C,Nt),o(Nt,xr),o(C,jr),o(C,ct),o(ct,Tr),o(C,Cr),o(C,Mt),o(Mt,Ar),o(C,Pr),a(e,So,s),_(oe,e,s),a(e,Ho,s),a(e,Q,s),o(Q,Dr),o(Q,Ft),o(Ft,Ir),o(Q,Or),o(Q,Gt),o(Gt,zr),o(Q,Sr),a(e,No,s),_(xe,e,s),a(e,Mo,s),_(se,e,s),a(e,Fo,s),a(e,dt,s),o(dt,Hr),a(e,Go,s),a(e,je,s),o(je,Te),a(e,Qo,s),a(e,mt,s),o(mt,Nr),a(e,Lo,s),a(e,Ce,s),o(Ce,Ae),a(e,Wo,s),a(e,_t,s),o(_t,Mr),a(e,Yo,s),_(Pe,e,s),a(e,Ro,s),_(De,e,s),a(e,Vo,s),a(e,L,s),o(L,Fr),o(L,Qt),o(Qt,Gr),o(L,Qr),o(L,Lt),o(Lt,Lr),o(L,Wr),a(e,Bo,s),_(Ie,e,s),a(e,Jo,s),_(Oe,e,s),a(e,Uo,s),a(e,x,s),o(x,Yr),o(x,Wt),o(Wt,Rr),o(x,Vr),o(x,Yt),o(Yt,Br),o(x,Jr),o(x,ze),o(ze,Rt),o(Rt,Ur),o(x,Xr),o(x,gt),o(gt,Kr),o(x,Zr),o(x,Vt),o(Vt,en),o(x,tn),a(e,Xo,s),_(Se,e,s),a(e,Ko,s),_(re,e,s),a(e,Zo,s),a(e,ne,s),o(ne,on),o(ne,Bt),o(Bt,sn),o(ne,rn),a(e,es,s),_(He,e,s),a(e,ts,s),a(e,ae,s),o(ae,nn),o(ae,Jt),o(Jt,an),o(ae,ln),a(e,os,s),_(Ne,e,s),a(e,ss,s),_(Me,e,s),a(e,rs,s),a(e,wt,s),o(wt,un),a(e,ns,s),a(e,I,s),o(I,Fe),o(Fe,pn),o(Fe,Ut),o(Ut,hn),o(Fe,fn),o(I,cn),o(I,Xt),o(Xt,dn),o(I,mn),o(I,Kt),o(Kt,_n),o(I,gn),o(I,Ge),o(Ge,wn),o(Ge,Zt),o(Zt,bn),o(Ge,kn),a(e,as,s),a(e,bt,s),o(bt,yn),a(e,is,s),a(e,J,s),o(J,ie),o(ie,eo),_(Qe,eo,null),o(J,$n),o(J,to),o(to,vn),a(e,ls,s),a(e,W,s),o(W,qn),o(W,oo),o(oo,En),o(W,xn),o(W,so),o(so,jn),o(W,Tn),a(e,us,s),_(Le,e,s),a(e,ps,s),a(e,kt,s),o(kt,Cn),a(e,hs,s),_(We,e,s),a(e,fs,s),a(e,le,s),o(le,An),o(le,yt),o(yt,Pn),o(le,Dn),a(e,cs,s),_(Ye,e,s),a(e,ds,s),_(Re,e,s),a(e,ms,s),a(e,$t,s),o($t,In),a(e,_s,s),_(Ve,e,s),a(e,gs,s),a(e,vt,s),o(vt,On),a(e,ws,s),_(Be,e,s),a(e,bs,s),a(e,A,s),o(A,zn),o(A,ro),o(ro,Sn),o(A,Hn),o(A,no),o(no,Nn),o(A,Mn),o(A,ao),o(ao,Fn),o(A,Gn),o(A,io),o(io,Qn),o(A,Ln),a(e,ks,s),_(Je,e,s),a(e,ys,s),_(Ue,e,s),a(e,$s,s),a(e,ue,s),o(ue,Wn),o(ue,lo),o(lo,Yn),o(ue,Rn),a(e,vs,s),_(Xe,e,s),a(e,qs,s),_(Ke,e,s),a(e,Es,s),a(e,j,s),o(j,Vn),o(j,uo),o(uo,Bn),o(j,Jn),o(j,qt),o(qt,Un),o(j,Xn),o(j,po),o(po,Kn),o(j,Zn),o(j,ho),o(ho,ea),o(j,ta),o(j,fo),o(fo,oa),o(j,sa),a(e,xs,s),_(Ze,e,s),a(e,js,s),a(e,O,s),o(O,ra),o(O,co),o(co,na),o(O,aa),o(O,mo),o(mo,ia),o(O,la),o(O,et),o(et,ua),o(O,pa),a(e,Ts,s),a(e,tt,s),o(tt,ot),a(e,Cs,s),a(e,pe,s),o(pe,ha),o(pe,_o),o(_o,fa),o(pe,ca),a(e,As,s),_(st,e,s),a(e,Ps,s),_(rt,e,s),a(e,Ds,s),a(e,he,s),o(he,da),o(he,nt),o(nt,ma),o(he,_a),Is=!0},p(e,[s]){const at={};s&2&&(at.$$scope={dirty:s,ctx:e}),oe.$set(at);const go={};s&2&&(go.$$scope={dirty:s,ctx:e}),se.$set(go);const wo={};s&2&&(wo.$$scope={dirty:s,ctx:e}),re.$set(wo)},i(e){Is||(g(v.$$.fragment,e),g(N.$$.fragment,e),g(de.$$.fragment,e),g(ge.$$.fragment,e),g(we.$$.fragment,e),g(be.$$.fragment,e),g(ke.$$.fragment,e),g($e.$$.fragment,e),g(ve.$$.fragment,e),g(oe.$$.fragment,e),g(xe.$$.fragment,e),g(se.$$.fragment,e),g(Pe.$$.fragment,e),g(De.$$.fragment,e),g(Ie.$$.fragment,e),g(Oe.$$.fragment,e),g(Se.$$.fragment,e),g(re.$$.fragment,e),g(He.$$.fragment,e),g(Ne.$$.fragment,e),g(Me.$$.fragment,e),g(Qe.$$.fragment,e),g(Le.$$.fragment,e),g(We.$$.fragment,e),g(Ye.$$.fragment,e),g(Re.$$.fragment,e),g(Ve.$$.fragment,e),g(Be.$$.fragment,e),g(Je.$$.fragment,e),g(Ue.$$.fragment,e),g(Xe.$$.fragment,e),g(Ke.$$.fragment,e),g(Ze.$$.fragment,e),g(st.$$.fragment,e),g(rt.$$.fragment,e),Is=!0)},o(e){w(v.$$.fragment,e),w(N.$$.fragment,e),w(de.$$.fragment,e),w(ge.$$.fragment,e),w(we.$$.fragment,e),w(be.$$.fragment,e),w(ke.$$.fragment,e),w($e.$$.fragment,e),w(ve.$$.fragment,e),w(oe.$$.fragment,e),w(xe.$$.fragment,e),w(se.$$.fragment,e),w(Pe.$$.fragment,e),w(De.$$.fragment,e),w(Ie.$$.fragment,e),w(Oe.$$.fragment,e),w(Se.$$.fragment,e),w(re.$$.fragment,e),w(He.$$.fragment,e),w(Ne.$$.fragment,e),w(Me.$$.fragment,e),w(Qe.$$.fragment,e),w(Le.$$.fragment,e),w(We.$$.fragment,e),w(Ye.$$.fragment,e),w(Re.$$.fragment,e),w(Ve.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ue.$$.fragment,e),w(Xe.$$.fragment,e),w(Ke.$$.fragment,e),w(Ze.$$.fragment,e),w(st.$$.fragment,e),w(rt.$$.fragment,e),Is=!1},d(e){t(c),e&&t(E),e&&t(k),b(v),e&&t(X),b(N,e),e&&t(M),e&&t(q),e&&t(bo),b(de,e),e&&t(ko),e&&t(F),e&&t(yo),b(ge,e),e&&t($o),e&&t(ut),e&&t(vo),b(we,e),e&&t(qo),e&&t(K),e&&t(Eo),b(be,e),e&&t(xo),e&&t(Z),e&&t(jo),e&&t(B),b(ke),e&&t(To),e&&t(pt),e&&t(Co),e&&t(ht),e&&t(Ao),e&&t(te),e&&t(Po),b($e,e),e&&t(Do),b(ve,e),e&&t(Io),e&&t(G),e&&t(Oo),e&&t(qe),e&&t(zo),e&&t(C),e&&t(So),b(oe,e),e&&t(Ho),e&&t(Q),e&&t(No),b(xe,e),e&&t(Mo),b(se,e),e&&t(Fo),e&&t(dt),e&&t(Go),e&&t(je),e&&t(Qo),e&&t(mt),e&&t(Lo),e&&t(Ce),e&&t(Wo),e&&t(_t),e&&t(Yo),b(Pe,e),e&&t(Ro),b(De,e),e&&t(Vo),e&&t(L),e&&t(Bo),b(Ie,e),e&&t(Jo),b(Oe,e),e&&t(Uo),e&&t(x),e&&t(Xo),b(Se,e),e&&t(Ko),b(re,e),e&&t(Zo),e&&t(ne),e&&t(es),b(He,e),e&&t(ts),e&&t(ae),e&&t(os),b(Ne,e),e&&t(ss),b(Me,e),e&&t(rs),e&&t(wt),e&&t(ns),e&&t(I),e&&t(as),e&&t(bt),e&&t(is),e&&t(J),b(Qe),e&&t(ls),e&&t(W),e&&t(us),b(Le,e),e&&t(ps),e&&t(kt),e&&t(hs),b(We,e),e&&t(fs),e&&t(le),e&&t(cs),b(Ye,e),e&&t(ds),b(Re,e),e&&t(ms),e&&t($t),e&&t(_s),b(Ve,e),e&&t(gs),e&&t(vt),e&&t(ws),b(Be,e),e&&t(bs),e&&t(A),e&&t(ks),b(Je,e),e&&t(ys),b(Ue,e),e&&t($s),e&&t(ue),e&&t(vs),b(Xe,e),e&&t(qs),b(Ke,e),e&&t(Es),e&&t(j),e&&t(xs),b(Ze,e),e&&t(js),e&&t(O),e&&t(Ts),e&&t(tt),e&&t(Cs),e&&t(pe),e&&t(As),b(st,e),e&&t(Ps),b(rt,e),e&&t(Ds),e&&t(he)}}}const Ji={local:"what-to-do-when-you-get-an-error",sections:[{local:"debugging-the-pipeline-from-transformers",title:"Debugging the pipeline from \u{1F917} Transformers"},{local:"debugging-the-forward-pass-of-your-model",title:"Debugging the forward pass of your model"}],title:"What to do when you get an error"};function Ui(U){return Li(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sl extends Mi{constructor(c){super();Fi(this,c,Ui,Bi,Gi,{})}}export{sl as default,Ji as metadata};
