import{S as Qc,i as Wc,s as Zc,e as o,k as u,w as h,t as l,M as Kc,c as s,d as r,m as p,a as t,x as g,h as n,b as c,N as m,G as a,g as d,y as E,L as eu,q as b,o as _,B as q,v as au}from"../../chunks/vendor-hf-doc-builder.js";import{Y as wi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as N}from"../../chunks/IconCopyLink-hf-doc-builder.js";function ru(Ai){let S,_o,R,re,er,Te,st,ar,tt,qo,ha,lt,yo,D,oe,rr,Pe,nt,or,it,jo,ga,dt,To,B,$e,Ii,ct,ke,xi,Po,se,ut,we,pt,mt,$o,f,sr,te,tr,ft,vt,Ae,ht,gt,Et,lr,le,nr,bt,_t,Ie,qt,yt,jt,ir,ne,dr,Tt,Pt,xe,$t,kt,wt,cr,ie,ur,At,It,Me,xt,Mt,Lt,pr,j,mr,zt,Gt,Le,Nt,St,ze,Rt,Dt,Bt,fr,T,vr,Ot,Ct,Ge,Vt,Ht,hr,Ut,Yt,ko,Ea,Ft,wo,P,Ne,Jt,gr,Xt,Qt,Wt,Se,Zt,Er,Kt,el,al,Re,rl,br,ol,sl,Ao,ba,tl,Io,O,de,_r,De,ll,qr,nl,xo,ce,il,yr,dl,cl,Mo,$,ul,jr,pl,ml,Tr,fl,vl,Lo,k,hl,Pr,gl,El,$r,bl,_l,zo,C,Be,Mi,ql,Oe,Li,Go,ue,yl,kr,jl,Tl,No,V,Ce,zi,Pl,Ve,Gi,So,H,pe,wr,He,$l,Ar,kl,Ro,_a,wl,Do,Ue,Ye,Ni,Bo,qa,Al,Oo,U,Fe,Si,Il,Je,Ri,Co,Xe,Vo,ya,xl,Ho,ja,Ml,Uo,Ta,Ll,Yo,Y,me,Ir,Qe,zl,xr,Gl,Fo,We,Jo,fe,Nl,Mr,Sl,Rl,Xo,F,Ze,Di,Dl,Ke,Bi,Qo,Pa,Bl,Wo,w,Ol,Lr,Cl,Vl,zr,Hl,Ul,Zo,A,Gr,Yl,Fl,Nr,Jl,Xl,Sr,Ql,Ko,ve,Wl,Rr,Zl,Kl,es,J,ea,Oi,en,aa,Ci,as,$a,an,rs,ka,rn,os,X,he,Dr,ra,on,Br,sn,ss,wa,tn,ts,oa,ls,Q,ge,Or,sa,ln,Cr,nn,ns,Aa,dn,is,Ee,Ia,Vr,cn,un,pn,xa,Hr,mn,fn,ds,W,ta,Vi,vn,la,Hi,cs,Ma,hn,us,I,La,Ur,gn,En,bn,za,Yr,_n,qn,yn,be,Fr,jn,Tn,Jr,Pn,$n,ps,Ga,kn,ms,Z,_e,Xr,na,wn,Qr,An,fs,x,In,Wr,xn,Mn,ia,Ln,zn,vs,Na,Gn,hs,Sa,Nn,gs,Ra,Sn,Es,K,qe,Zr,da,Rn,Kr,Dn,bs,Da,Bn,_s,Ba,On,qs,Oa,Cn,ys,ee,ca,Ui,Vn,ua,Yi,js,Ca,Hn,Ts,ye,Un,eo,Yn,Fn,Ps,ae,je,ao,pa,Jn,ro,Xn,$s,v,Qn,oo,Wn,Zn,so,Kn,ei,to,ai,ri,lo,oi,si,ks,M,Va,no,ti,li,ni,Ha,io,ii,di,ci,L,co,ui,pi,uo,mi,fi,po,vi,hi,ws,z,gi,mo,Ei,bi,fo,_i,qi,As;return Te=new N({}),Pe=new N({}),De=new N({}),He=new N({}),Xe=new wi({props:{id:"ftWlj4FBHTg"}}),Qe=new N({}),We=new wi({props:{id:"BqqfQnyjmgg"}}),ra=new N({}),oa=new wi({props:{id:"H39Z_720T5s"}}),sa=new N({}),na=new N({}),da=new N({}),pa=new N({}),{c(){S=o("meta"),_o=u(),R=o("h1"),re=o("a"),er=o("span"),h(Te.$$.fragment),st=u(),ar=o("span"),tt=l("\xBFC\xF3mo funcionan los Transformadores?"),qo=u(),ha=o("p"),lt=l("En esta secci\xF3n, daremos una mirada de alto nivel a la arquitectura de los Transformadores."),yo=u(),D=o("h2"),oe=o("a"),rr=o("span"),h(Pe.$$.fragment),nt=u(),or=o("span"),it=l("Un poco de historia sobre los Transformadores"),jo=u(),ga=o("p"),dt=l("Estos son algunos hitos en la (corta) historia de los Transformadores:"),To=u(),B=o("div"),$e=o("img"),ct=u(),ke=o("img"),Po=u(),se=o("p"),ut=l("La "),we=o("a"),pt=l("arquitectura de los Transformadores"),mt=l(" fue presentada por primera vez en junio de 2017. El trabajo original se enfocaba en tareas de traducci\xF3n. A esto le sigu\xF3 la introducci\xF3n de numerosos modelos influyentes, que incluyen:"),$o=u(),f=o("ul"),sr=o("li"),te=o("p"),tr=o("strong"),ft=l("Junio de 2018"),vt=l(": "),Ae=o("a"),ht=l("GPT"),gt=l(", el primer modelo de Transformadores preentrenados, que fue usado para ajustar varias tareas de PLN y obtuvo resultados de vanguardia"),Et=u(),lr=o("li"),le=o("p"),nr=o("strong"),bt=l("Octubre de 2018"),_t=l(": "),Ie=o("a"),qt=l("BERT"),yt=l(", otro gran modelo preentrenado, dise\xF1ado para producir mejores res\xFAmenes de oraciones (\xA1m\xE1s sobre esto en el siguiente cap\xEDtulo!)"),jt=u(),ir=o("li"),ne=o("p"),dr=o("strong"),Tt=l("Febrero de 2019"),Pt=l(": "),xe=o("a"),$t=l("GPT-2"),kt=l(", una versi\xF3n mejorada (y m\xE1s grande) de GPT, que no se liber\xF3 inmediatamente al p\xFAblico por consideraciones \xE9ticas"),wt=u(),cr=o("li"),ie=o("p"),ur=o("strong"),At=l("Octubre de 2019"),It=l(": "),Me=o("a"),xt=l("DistilBERT"),Mt=l(", una versi\xF3n destilada de BERT que es 60% m\xE1s r\xE1pida, 40% m\xE1s ligera en memoria y que retiene el 97% del desempe\xF1o de BERT"),Lt=u(),pr=o("li"),j=o("p"),mr=o("strong"),zt=l("Octubre de 2019"),Gt=l(": "),Le=o("a"),Nt=l("BART"),St=l(" y "),ze=o("a"),Rt=l("T5"),Dt=l(", dos grandes modelos preentrenados usando la misma arquitectura del modelo original de Transformador (los primeros en hacerlo)"),Bt=u(),fr=o("li"),T=o("p"),vr=o("strong"),Ot=l("Mayo de 2020"),Ct=l(", "),Ge=o("a"),Vt=l("GPT-3"),Ht=l(", una versi\xF3n a\xFAn m\xE1s grande de GPT-2 con buen desempe\xF1o en una gran variedad de tareas sin la necesidad de ajustes (llamado "),hr=o("em"),Ut=l("zero-shot learning"),Yt=l(")"),ko=u(),Ea=o("p"),Ft=l("Esta lista est\xE1 lejos de ser exhaustiva y solo pretende resaltar algunos de los diferentes modelos de Transformadores. De manera general, estos pueden agruparse en tres categor\xEDas:"),wo=u(),P=o("ul"),Ne=o("li"),Jt=l("Parecidos a GPT (tambi\xE9n llamados modelos "),gr=o("em"),Xt=l("auto-regressive"),Qt=l(")"),Wt=u(),Se=o("li"),Zt=l("Parecidos a BERT (tambi\xE9n llamados modelos "),Er=o("em"),Kt=l("auto-encoding"),el=l(")"),al=u(),Re=o("li"),rl=l("Parecidos a BART/T5 (tambi\xE9n llamados modelos "),br=o("em"),ol=l("sequence-to-sequence"),sl=l(")"),Ao=u(),ba=o("p"),tl=l("Vamos a entrar en estas familias de modelos a profundidad m\xE1s adelante."),Io=u(),O=o("h2"),de=o("a"),_r=o("span"),h(De.$$.fragment),ll=u(),qr=o("span"),nl=l("Los Transformadores son modelos de lenguaje"),xo=u(),ce=o("p"),il=l("Todos los modelos de Transformadores mencionados con anterioridad (GPT, BERT, BART, T5, etc.) han sido entrenados como "),yr=o("em"),dl=l("modelos de lenguaje"),cl=l(". Esto significa que han sido entrenados con grandes cantidades de texto crudo de una manera auto-supervisada. El aprendizaje auto-supervisado es un tipo de entrenamiento en el que el objetivo se computa autom\xE1ticamente de las entradas del modelo. \xA1Esto significa que no necesitan humanos que etiqueten los datos!"),Mo=u(),$=o("p"),ul=l("Este tipo de modelos desarrolla un entendimiento estad\xEDstico del lenguaje sobre el que fue entrenado, pero no es muy \xFAtil para tareas pr\xE1cticas espec\xEDficas. Por lo anterior, el modelo general preentrenado pasa por un proceso llamado "),jr=o("em"),pl=l("transferencia de aprendizaje"),ml=l(" (o "),Tr=o("em"),fl=l("transfer learning"),vl=l(" en Ingl\xE9s). Durante este proceso, el modelo se ajusta de una forma supervisada \u2014 esto es, usando etiquetas hechas por humanos \u2014 para una tarea dada."),Lo=u(),k=o("p"),hl=l("Un ejemplo de una tarea es predecir la palabra siguiente en una oraci\xF3n con base en las "),Pr=o("em"),gl=l("n"),El=l(" palabras previas. Esto se denomina "),$r=o("em"),bl=l("modelado de lenguaje causal"),_l=l(" porque la salida depende de las entradas pasadas y presentes, pero no en las futuras."),zo=u(),C=o("div"),Be=o("img"),ql=u(),Oe=o("img"),Go=u(),ue=o("p"),yl=l("Otro ejemplo es el "),kr=o("em"),jl=l("modelado de leguaje oculto"),Tl=l(", en el que el modelo predice una palabra oculta en la oraci\xF3n."),No=u(),V=o("div"),Ce=o("img"),Pl=u(),Ve=o("img"),So=u(),H=o("h2"),pe=o("a"),wr=o("span"),h(He.$$.fragment),$l=u(),Ar=o("span"),kl=l("Los Transformadores son modelos grandes"),Ro=u(),_a=o("p"),wl=l("Excepto algunos casos at\xEDpicos (como DistilBERT), la estrategia general para mejorar el desempe\xF1o es incrementar el tama\xF1o de los modelos, as\xED como la cantidad de datos con los que est\xE1n preentrenados."),Do=u(),Ue=o("div"),Ye=o("img"),Bo=u(),qa=o("p"),Al=l("Desafortunadamente, entrenar un modelo, especialmente uno grande, requiere de grandes cantidades de datos. Esto se vuelve muy costoso en t\xE9rminos de tiempo y recursos de computaci\xF3n, que se traduce incluso en impacto ambiental, como se puede ver en la siguiente gr\xE1fica."),Oo=u(),U=o("div"),Fe=o("img"),Il=u(),Je=o("img"),Co=u(),h(Xe.$$.fragment),Vo=u(),ya=o("p"),xl=l("Esto es ilustrativo para un proyecto que busca un modelo (muy grande), liderado por un equipo que intenta de manera consciente reducir el impacto ambiental del preentrenamiento. La huella de ejecutar muchas pruebas para encontrar los mejores hiperpar\xE1metros es a\xFAn mayor."),Ho=u(),ja=o("p"),Ml=l("Ahora imag\xEDnate si cada vez que un equipo de investigaci\xF3n, una organizaci\xF3n estudiantil o una compa\xF1\xEDa intentaran entrenar un modelo, tuvieran que hacerlo desde cero. \xA1Esto implicar\xEDa costos globales enormes e innecesarios!"),Uo=u(),Ta=o("p"),Ll=l("Esta es la raz\xF3n por la que compartir modelos de lenguaje es fundamental: compartir los pesos entrenados y construir sobre los existentes reduce el costo general y la huella de carbono de la comunidad."),Yo=u(),Y=o("h2"),me=o("a"),Ir=o("span"),h(Qe.$$.fragment),zl=u(),xr=o("span"),Gl=l("Transferencia de aprendizaje (*Transfer learning*)"),Fo=u(),h(We.$$.fragment),Jo=u(),fe=o("p"),Nl=l("El "),Mr=o("em"),Sl=l("preentrenamiento"),Rl=l(" es el acto de entrenar un modelo desde cero: los pesos se inicializan de manera aleat\xF3ria y el entrenamiento empieza sin un conocimiento previo."),Xo=u(),F=o("div"),Ze=o("img"),Dl=u(),Ke=o("img"),Qo=u(),Pa=o("p"),Bl=l("Este preentrenamiento se hace usualmente sobre grandes cantidades de datos. Por lo anterior, requiere un gran corpus de datos y el entrenamiento puede tomar varias semanas."),Wo=u(),w=o("p"),Ol=l("Por su parte, el ajuste (o "),Lr=o("em"),Cl=l("fine-tuning"),Vl=l(") es el entrenamiento realizado "),zr=o("strong"),Hl=l("despu\xE9s"),Ul=l(" de que el modelo ha sido preentrenado. Para hacer el ajuste, comienzas con un modelo de lenguaje preentrenado y luego realizas un aprendizaje adicional con un conjunto de datos espec\xEDficos para tu tarea. Pero entonces \u2014 \xBFpor qu\xE9 no entrenar directamente para la tarea final? Hay un par de razones:"),Zo=u(),A=o("ul"),Gr=o("li"),Yl=l("El modelo preentrenado ya est\xE1 entrenado con un conjunto de datos parecido al conjunto de datos de ajuste. De esta manera, el proceso de ajuste puede hacer uso del conocimiento adquirido por el modelo inicial durante el preentrenamiento (por ejemplo, para problemas de PLN, el modelo preentrenado tendr\xE1 alg\xFAn tipo de entendimiento estad\xEDstico del idioma que est\xE1s usando para tu tarea)."),Fl=u(),Nr=o("li"),Jl=l("Dado que el modelo preentrenado fue entrenado con muchos datos, el ajuste requerir\xE1 menos datos para tener resultados decentes."),Xl=u(),Sr=o("li"),Ql=l("Por la misma raz\xF3n, la cantidad de tiempo y recursos necesarios para tener buenos resultados es mucho menor."),Ko=u(),ve=o("p"),Wl=l("Por ejemplo, se podr\xEDa aprovechar un modelo preentrenado en Ingl\xE9s y despu\xE9s ajustarlo con un corpus arXiv, teniendo como resultado un modelo basado en investigaci\xF3n cient\xEDfica. El ajuste solo requerir\xE1 una cantidad limitada de datos: el conocimiento que el modelo preentrenado ha adquirido se \u201Ctransfiere\u201D, de ah\xED el t\xE9rmino "),Rr=o("em"),Zl=l("transferencia de aprendizaje"),Kl=l("."),es=u(),J=o("div"),ea=o("img"),en=u(),aa=o("img"),as=u(),$a=o("p"),an=l("De ese modo, el ajuste de un modelo tendr\xE1 menos costos de tiempo, de datos, financieros y ambientales. Adem\xE1s es m\xE1s r\xE1pido y m\xE1s f\xE1cil de iterar en diferentes esquemas de ajuste, dado que el entrenamiento es menos restrictivo que un preentrenamiento completo."),rs=u(),ka=o("p"),rn=l("Este proceso tambi\xE9n conseguir\xE1 mejores resultados que entrenar desde cero (a menos que tengas una gran cantidad de datos), raz\xF3n por la cual siempre deber\xEDas intentar aprovechar un modelo preentrenado \u2014 uno que est\xE9 tan cerca como sea posible a la tarea respectiva \u2014 y ajustarlo."),os=u(),X=o("h2"),he=o("a"),Dr=o("span"),h(ra.$$.fragment),on=u(),Br=o("span"),sn=l("Arquitectura general"),ss=u(),wa=o("p"),tn=l("En esta secci\xF3n, revisaremos la arquitectura general del Transformador. No te preocupes si no entiendes algunos de los conceptos; hay secciones detalladas m\xE1s adelante para cada uno de los componentes."),ts=u(),h(oa.$$.fragment),ls=u(),Q=o("h2"),ge=o("a"),Or=o("span"),h(sa.$$.fragment),ln=u(),Cr=o("span"),nn=l("Introducci\xF3n"),ns=u(),Aa=o("p"),dn=l("El modelo est\xE1 compuesto por dos bloques:"),is=u(),Ee=o("ul"),Ia=o("li"),Vr=o("strong"),cn=l("Codificador (izquierda)"),un=l(": El codificador recibe una entrada y construye una representaci\xF3n de \xE9sta (sus caracter\xEDsticas). Esto significa que el modelo est\xE1 optimizado para conseguir un entendimiento a partir de la entrada."),pn=u(),xa=o("li"),Hr=o("strong"),mn=l("Decodificador (derecha)"),fn=l(": El decodificador usa la representac\xF3n del codificador (caracter\xEDsticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est\xE1 optimizado para generar salidas."),ds=u(),W=o("div"),ta=o("img"),vn=u(),la=o("img"),cs=u(),Ma=o("p"),hn=l("Cada una de estas partes puede ser usada de manera independiente, dependiendo de la tarea:"),us=u(),I=o("ul"),La=o("li"),Ur=o("strong"),gn=l("Modelos con solo codificadores"),En=l(": Buenos para las tareas que requieren el entendimiento de la entrada, como la clasificaci\xF3n de oraciones y reconocimiento de entidades nombradas."),bn=u(),za=o("li"),Yr=o("strong"),_n=l("Modelos con solo decodificadores"),qn=l(": Buenos para tareas generativas como la generaci\xF3n de textos."),yn=u(),be=o("li"),Fr=o("strong"),jn=l("Modelos con codificadores y decodificadores"),Tn=l(" o "),Jr=o("strong"),Pn=l("Modelos secuencia a secuencia"),$n=l(": Buenos para tareas generativas que requieren una entrada, como la traducci\xF3n o resumen."),ps=u(),Ga=o("p"),kn=l("Vamos a abordar estas arquitecturas de manera independiente en secciones posteriores."),ms=u(),Z=o("h2"),_e=o("a"),Xr=o("span"),h(na.$$.fragment),wn=u(),Qr=o("span"),An=l("Capas de atenci\xF3n"),fs=u(),x=o("p"),In=l("Una caracter\xEDstica clave de los Transformadores es que est\xE1n construidos con capas especiales llamadas "),Wr=o("em"),xn=l("capas de atenci\xF3n"),Mn=l(". De hecho, el t\xEDtulo del trabajo que introdujo la arquitectura de los Transformadores fue "),ia=o("a"),Ln=l("\u201CAttention Is All You Need\u201D"),zn=l(". Vamos a explorar los detalles de las capas de atenci\xF3n m\xE1s adelante en el curso; por ahora, todo lo que tienes que saber es que esta capa va a indicarle al modelo que tiene que prestar especial atenci\xF3n a ciertas partes de la oraci\xF3n que le pasaste (y m\xE1s o menos ignorar las dem\xE1s), cuando trabaje con la representaci\xF3n de cada palabra."),vs=u(),Na=o("p"),Gn=l("Para poner esto en contexto, piensa en la tarea de traducir texto de Ingl\xE9s a Franc\xE9s. Dada la entrada \u201CYou like this course\u201D, un modelo de traducci\xF3n necesitar\xE1 tener en cuenta la palabra adyacente \u201CYou\u201D para obtener la traducci\xF3n correcta de la palabra \u201Clike\u201D, porque en Franc\xE9s el verbo \u201Clike\u201D se conjuga de manera distinta dependiendo del sujeto. Sin embargo, el resto de la oraci\xF3n no es \xFAtil para la traducci\xF3n de esa palabra. En la misma l\xEDnea, al traducir \u201Cthis\u201D, el modelo tambi\xE9n deber\xE1 prestar atenci\xF3n a la palabra \u201Ccourse\u201D, porque \u201Cthis\u201D se traduce de manera distinta dependiendo de si el nombre asociado es masculino o femenino. De nuevo, las otras palabras en la oraci\xF3n no van a importar para la traducci\xF3n de \u201Cthis\u201D. Con oraciones (y reglas gramaticales) m\xE1s complejas, el modelo deber\xE1 prestar especial atenci\xF3n a palabras que pueden aparecer m\xE1s lejos en la oraci\xF3n para traducir correctamente cada palabra."),hs=u(),Sa=o("p"),Nn=l("El mismo concepto aplica para cualquier tarea asociada con lenguaje natural: una palabra por si misma tiene un significado, pero ese significado est\xE1 afectado profundamente por el contexto, que puede ser cualquier palabra (o palabras) antes o despu\xE9s de la palabra que est\xE1 siendo estudiada."),gs=u(),Ra=o("p"),Sn=l("Ahora que tienes una idea de qu\xE9 son las capas de atenci\xF3n, echemos un vistazo m\xE1s de cerca a la arquitectura del Transformador."),Es=u(),K=o("h2"),qe=o("a"),Zr=o("span"),h(da.$$.fragment),Rn=u(),Kr=o("span"),Dn=l("La arquitectura original"),bs=u(),Da=o("p"),Bn=l("La arquitectura del Transformador fue dise\xF1ada originalmente para traducci\xF3n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un idioma dado, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo. En el codificador, las capas de atenci\xF3n pueden usar todas las palabras en una oraci\xF3n (dado que, como vimos, la traducci\xF3n de una palabra dada puede ser dependiente de lo que est\xE1 antes y despu\xE9s en la oraci\xF3n). Por su parte, el decodificador trabaja de manera secuencial y s\xF3lo le puede prestar atenci\xF3n a las palabras en la oraci\xF3n que ya ha traducido (es decir, s\xF3lo las palabras antes de que la palabra se ha generado). Por ejemplo, cuando hemos predecido las primeras tres palabras del objetivo de traducci\xF3n se las damos al decodificador, que luego usa todas las entradas del codificador para intentar predecir la cuarta palabra."),_s=u(),Ba=o("p"),On=l("Para acelerar el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), al decodificador se le alimenta el objetivo completo, pero no puede usar palabras futuras (si tuviera acceso a la palabra en la posici\xF3n 2 cuando trata de predecir la palabra en la posici\xF3n 2, \xA1el problema no ser\xEDa muy dificil!). Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci\xF3n s\xF3lo tendr\xEDa acceso a las palabras en las posiciones 1 a 3."),qs=u(),Oa=o("p"),Cn=l("La arquitectura original del Transformador se ve\xEDa as\xED, con el codificador a la izquierda y el decodificador a la derecha:"),ys=u(),ee=o("div"),ca=o("img"),Vn=u(),ua=o("img"),js=u(),Ca=o("p"),Hn=l("Observa que la primera capa de atenci\xF3n en un bloque de decodificador presta atenci\xF3n a todas las entradas (pasadas) al decodificador, mientras que la segunda capa de atenci\xF3n usa la salida del codificador. De esta manera puede acceder a toda la oraci\xF3n de entrada para predecir de mejor manera la palabra actual. Esto es muy \xFAtil dado que diferentes idiomas pueden tener reglas gramaticales que ponen las palabras en \xF3rden distinto o alg\xFAn contexto que se provee despu\xE9s puede ser \xFAtil para determinar la mejor traducci\xF3n de una palabra dada."),Ts=u(),ye=o("p"),Un=l("La "),eo=o("em"),Yn=l("m\xE1scara de atenci\xF3n"),Fn=l(" tambi\xE9n se puede usar en el codificador/decodificador para evitar que el modelo preste atenci\xF3n a algunas palabras especiales \u2014por ejemplo, la palabra especial de relleno que hace que todas las entradas sean de la misma longitud cuando se agrupan oraciones."),Ps=u(),ae=o("h2"),je=o("a"),ao=o("span"),h(pa.$$.fragment),Jn=u(),ro=o("span"),Xn=l("Arquitecturas vs. puntos de control"),$s=u(),v=o("p"),Qn=l("A medida que estudiemos a profundidad los Transformadores, ver\xE1s menciones a "),oo=o("em"),Wn=l("arquitecturas"),Zn=l(", "),so=o("em"),Kn=l("puntos de control"),ei=l(" ("),to=o("em"),ai=l("checkpoints"),ri=l(") y "),lo=o("em"),oi=l("modelos"),si=l(". Estos t\xE9rminos tienen significados ligeramentes diferentes:"),ks=u(),M=o("ul"),Va=o("li"),no=o("strong"),ti=l("Arquitecturas"),li=l(": Este es el esqueleto del modelo \u2014 la definici\xF3n de cada capa y cada operaci\xF3n que sucede al interior del modelo."),ni=u(),Ha=o("li"),io=o("strong"),ii=l("Puntos de control"),di=l(": Estos son los pesos que ser\xE1n cargados en una arquitectura dada."),ci=u(),L=o("li"),co=o("strong"),ui=l("Modelo"),pi=l(": Esta es un t\xE9rmino sombrilla que no es tan preciso como \u201Carquitectura\u201D o \u201Cpunto de control\u201D y puede significar ambas cosas. Este curso especificar\xE1 "),uo=o("em"),mi=l("arquitectura"),fi=l(" o "),po=o("em"),vi=l("punto de control"),hi=l(" cuando sea relevante para evitar ambig\xFCedades."),ws=u(),z=o("p"),gi=l("Por ejemplo, mientras que BERT es una arquitectura, "),mo=o("code"),Ei=l("bert-base-cased"),bi=l(" - un conjunto de pesos entrenados por el equipo de Google para la primera versi\xF3n de BERT - es un punto de control. Sin embargo, se podr\xEDa decir \u201Cel modelo BERT\u201D y \u201Cel modelo "),fo=o("code"),_i=l("bert-base-cased"),qi=l("\u201C."),this.h()},l(e){const i=Kc('[data-svelte="svelte-1phssyn"]',document.head);S=s(i,"META",{name:!0,content:!0}),i.forEach(r),_o=p(e),R=s(e,"H1",{class:!0});var Is=t(R);re=s(Is,"A",{id:!0,class:!0,href:!0});var Fi=t(re);er=s(Fi,"SPAN",{});var Ji=t(er);g(Te.$$.fragment,Ji),Ji.forEach(r),Fi.forEach(r),st=p(Is),ar=s(Is,"SPAN",{});var Xi=t(ar);tt=n(Xi,"\xBFC\xF3mo funcionan los Transformadores?"),Xi.forEach(r),Is.forEach(r),qo=p(e),ha=s(e,"P",{});var Qi=t(ha);lt=n(Qi,"En esta secci\xF3n, daremos una mirada de alto nivel a la arquitectura de los Transformadores."),Qi.forEach(r),yo=p(e),D=s(e,"H2",{class:!0});var xs=t(D);oe=s(xs,"A",{id:!0,class:!0,href:!0});var Wi=t(oe);rr=s(Wi,"SPAN",{});var Zi=t(rr);g(Pe.$$.fragment,Zi),Zi.forEach(r),Wi.forEach(r),nt=p(xs),or=s(xs,"SPAN",{});var Ki=t(or);it=n(Ki,"Un poco de historia sobre los Transformadores"),Ki.forEach(r),xs.forEach(r),jo=p(e),ga=s(e,"P",{});var ed=t(ga);dt=n(ed,"Estos son algunos hitos en la (corta) historia de los Transformadores:"),ed.forEach(r),To=p(e),B=s(e,"DIV",{class:!0});var Ms=t(B);$e=s(Ms,"IMG",{class:!0,src:!0,alt:!0}),ct=p(Ms),ke=s(Ms,"IMG",{class:!0,src:!0,alt:!0}),Ms.forEach(r),Po=p(e),se=s(e,"P",{});var Ls=t(se);ut=n(Ls,"La "),we=s(Ls,"A",{href:!0,rel:!0});var ad=t(we);pt=n(ad,"arquitectura de los Transformadores"),ad.forEach(r),mt=n(Ls," fue presentada por primera vez en junio de 2017. El trabajo original se enfocaba en tareas de traducci\xF3n. A esto le sigu\xF3 la introducci\xF3n de numerosos modelos influyentes, que incluyen:"),Ls.forEach(r),$o=p(e),f=s(e,"UL",{});var y=t(f);sr=s(y,"LI",{});var rd=t(sr);te=s(rd,"P",{});var vo=t(te);tr=s(vo,"STRONG",{});var od=t(tr);ft=n(od,"Junio de 2018"),od.forEach(r),vt=n(vo,": "),Ae=s(vo,"A",{href:!0,rel:!0});var sd=t(Ae);ht=n(sd,"GPT"),sd.forEach(r),gt=n(vo,", el primer modelo de Transformadores preentrenados, que fue usado para ajustar varias tareas de PLN y obtuvo resultados de vanguardia"),vo.forEach(r),rd.forEach(r),Et=p(y),lr=s(y,"LI",{});var td=t(lr);le=s(td,"P",{});var ho=t(le);nr=s(ho,"STRONG",{});var ld=t(nr);bt=n(ld,"Octubre de 2018"),ld.forEach(r),_t=n(ho,": "),Ie=s(ho,"A",{href:!0,rel:!0});var nd=t(Ie);qt=n(nd,"BERT"),nd.forEach(r),yt=n(ho,", otro gran modelo preentrenado, dise\xF1ado para producir mejores res\xFAmenes de oraciones (\xA1m\xE1s sobre esto en el siguiente cap\xEDtulo!)"),ho.forEach(r),td.forEach(r),jt=p(y),ir=s(y,"LI",{});var id=t(ir);ne=s(id,"P",{});var go=t(ne);dr=s(go,"STRONG",{});var dd=t(dr);Tt=n(dd,"Febrero de 2019"),dd.forEach(r),Pt=n(go,": "),xe=s(go,"A",{href:!0,rel:!0});var cd=t(xe);$t=n(cd,"GPT-2"),cd.forEach(r),kt=n(go,", una versi\xF3n mejorada (y m\xE1s grande) de GPT, que no se liber\xF3 inmediatamente al p\xFAblico por consideraciones \xE9ticas"),go.forEach(r),id.forEach(r),wt=p(y),cr=s(y,"LI",{});var ud=t(cr);ie=s(ud,"P",{});var Eo=t(ie);ur=s(Eo,"STRONG",{});var pd=t(ur);At=n(pd,"Octubre de 2019"),pd.forEach(r),It=n(Eo,": "),Me=s(Eo,"A",{href:!0,rel:!0});var md=t(Me);xt=n(md,"DistilBERT"),md.forEach(r),Mt=n(Eo,", una versi\xF3n destilada de BERT que es 60% m\xE1s r\xE1pida, 40% m\xE1s ligera en memoria y que retiene el 97% del desempe\xF1o de BERT"),Eo.forEach(r),ud.forEach(r),Lt=p(y),pr=s(y,"LI",{});var fd=t(pr);j=s(fd,"P",{});var ma=t(j);mr=s(ma,"STRONG",{});var vd=t(mr);zt=n(vd,"Octubre de 2019"),vd.forEach(r),Gt=n(ma,": "),Le=s(ma,"A",{href:!0,rel:!0});var hd=t(Le);Nt=n(hd,"BART"),hd.forEach(r),St=n(ma," y "),ze=s(ma,"A",{href:!0,rel:!0});var gd=t(ze);Rt=n(gd,"T5"),gd.forEach(r),Dt=n(ma,", dos grandes modelos preentrenados usando la misma arquitectura del modelo original de Transformador (los primeros en hacerlo)"),ma.forEach(r),fd.forEach(r),Bt=p(y),fr=s(y,"LI",{});var Ed=t(fr);T=s(Ed,"P",{});var fa=t(T);vr=s(fa,"STRONG",{});var bd=t(vr);Ot=n(bd,"Mayo de 2020"),bd.forEach(r),Ct=n(fa,", "),Ge=s(fa,"A",{href:!0,rel:!0});var _d=t(Ge);Vt=n(_d,"GPT-3"),_d.forEach(r),Ht=n(fa,", una versi\xF3n a\xFAn m\xE1s grande de GPT-2 con buen desempe\xF1o en una gran variedad de tareas sin la necesidad de ajustes (llamado "),hr=s(fa,"EM",{});var qd=t(hr);Ut=n(qd,"zero-shot learning"),qd.forEach(r),Yt=n(fa,")"),fa.forEach(r),Ed.forEach(r),y.forEach(r),ko=p(e),Ea=s(e,"P",{});var yd=t(Ea);Ft=n(yd,"Esta lista est\xE1 lejos de ser exhaustiva y solo pretende resaltar algunos de los diferentes modelos de Transformadores. De manera general, estos pueden agruparse en tres categor\xEDas:"),yd.forEach(r),wo=p(e),P=s(e,"UL",{});var Ua=t(P);Ne=s(Ua,"LI",{});var zs=t(Ne);Jt=n(zs,"Parecidos a GPT (tambi\xE9n llamados modelos "),gr=s(zs,"EM",{});var jd=t(gr);Xt=n(jd,"auto-regressive"),jd.forEach(r),Qt=n(zs,")"),zs.forEach(r),Wt=p(Ua),Se=s(Ua,"LI",{});var Gs=t(Se);Zt=n(Gs,"Parecidos a BERT (tambi\xE9n llamados modelos "),Er=s(Gs,"EM",{});var Td=t(Er);Kt=n(Td,"auto-encoding"),Td.forEach(r),el=n(Gs,")"),Gs.forEach(r),al=p(Ua),Re=s(Ua,"LI",{});var Ns=t(Re);rl=n(Ns,"Parecidos a BART/T5 (tambi\xE9n llamados modelos "),br=s(Ns,"EM",{});var Pd=t(br);ol=n(Pd,"sequence-to-sequence"),Pd.forEach(r),sl=n(Ns,")"),Ns.forEach(r),Ua.forEach(r),Ao=p(e),ba=s(e,"P",{});var $d=t(ba);tl=n($d,"Vamos a entrar en estas familias de modelos a profundidad m\xE1s adelante."),$d.forEach(r),Io=p(e),O=s(e,"H2",{class:!0});var Ss=t(O);de=s(Ss,"A",{id:!0,class:!0,href:!0});var kd=t(de);_r=s(kd,"SPAN",{});var wd=t(_r);g(De.$$.fragment,wd),wd.forEach(r),kd.forEach(r),ll=p(Ss),qr=s(Ss,"SPAN",{});var Ad=t(qr);nl=n(Ad,"Los Transformadores son modelos de lenguaje"),Ad.forEach(r),Ss.forEach(r),xo=p(e),ce=s(e,"P",{});var Rs=t(ce);il=n(Rs,"Todos los modelos de Transformadores mencionados con anterioridad (GPT, BERT, BART, T5, etc.) han sido entrenados como "),yr=s(Rs,"EM",{});var Id=t(yr);dl=n(Id,"modelos de lenguaje"),Id.forEach(r),cl=n(Rs,". Esto significa que han sido entrenados con grandes cantidades de texto crudo de una manera auto-supervisada. El aprendizaje auto-supervisado es un tipo de entrenamiento en el que el objetivo se computa autom\xE1ticamente de las entradas del modelo. \xA1Esto significa que no necesitan humanos que etiqueten los datos!"),Rs.forEach(r),Mo=p(e),$=s(e,"P",{});var Ya=t($);ul=n(Ya,"Este tipo de modelos desarrolla un entendimiento estad\xEDstico del lenguaje sobre el que fue entrenado, pero no es muy \xFAtil para tareas pr\xE1cticas espec\xEDficas. Por lo anterior, el modelo general preentrenado pasa por un proceso llamado "),jr=s(Ya,"EM",{});var xd=t(jr);pl=n(xd,"transferencia de aprendizaje"),xd.forEach(r),ml=n(Ya," (o "),Tr=s(Ya,"EM",{});var Md=t(Tr);fl=n(Md,"transfer learning"),Md.forEach(r),vl=n(Ya," en Ingl\xE9s). Durante este proceso, el modelo se ajusta de una forma supervisada \u2014 esto es, usando etiquetas hechas por humanos \u2014 para una tarea dada."),Ya.forEach(r),Lo=p(e),k=s(e,"P",{});var Fa=t(k);hl=n(Fa,"Un ejemplo de una tarea es predecir la palabra siguiente en una oraci\xF3n con base en las "),Pr=s(Fa,"EM",{});var Ld=t(Pr);gl=n(Ld,"n"),Ld.forEach(r),El=n(Fa," palabras previas. Esto se denomina "),$r=s(Fa,"EM",{});var zd=t($r);bl=n(zd,"modelado de lenguaje causal"),zd.forEach(r),_l=n(Fa," porque la salida depende de las entradas pasadas y presentes, pero no en las futuras."),Fa.forEach(r),zo=p(e),C=s(e,"DIV",{class:!0});var Ds=t(C);Be=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),ql=p(Ds),Oe=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),Ds.forEach(r),Go=p(e),ue=s(e,"P",{});var Bs=t(ue);yl=n(Bs,"Otro ejemplo es el "),kr=s(Bs,"EM",{});var Gd=t(kr);jl=n(Gd,"modelado de leguaje oculto"),Gd.forEach(r),Tl=n(Bs,", en el que el modelo predice una palabra oculta en la oraci\xF3n."),Bs.forEach(r),No=p(e),V=s(e,"DIV",{class:!0});var Os=t(V);Ce=s(Os,"IMG",{class:!0,src:!0,alt:!0}),Pl=p(Os),Ve=s(Os,"IMG",{class:!0,src:!0,alt:!0}),Os.forEach(r),So=p(e),H=s(e,"H2",{class:!0});var Cs=t(H);pe=s(Cs,"A",{id:!0,class:!0,href:!0});var Nd=t(pe);wr=s(Nd,"SPAN",{});var Sd=t(wr);g(He.$$.fragment,Sd),Sd.forEach(r),Nd.forEach(r),$l=p(Cs),Ar=s(Cs,"SPAN",{});var Rd=t(Ar);kl=n(Rd,"Los Transformadores son modelos grandes"),Rd.forEach(r),Cs.forEach(r),Ro=p(e),_a=s(e,"P",{});var Dd=t(_a);wl=n(Dd,"Excepto algunos casos at\xEDpicos (como DistilBERT), la estrategia general para mejorar el desempe\xF1o es incrementar el tama\xF1o de los modelos, as\xED como la cantidad de datos con los que est\xE1n preentrenados."),Dd.forEach(r),Do=p(e),Ue=s(e,"DIV",{class:!0});var Bd=t(Ue);Ye=s(Bd,"IMG",{src:!0,alt:!0,width:!0}),Bd.forEach(r),Bo=p(e),qa=s(e,"P",{});var Od=t(qa);Al=n(Od,"Desafortunadamente, entrenar un modelo, especialmente uno grande, requiere de grandes cantidades de datos. Esto se vuelve muy costoso en t\xE9rminos de tiempo y recursos de computaci\xF3n, que se traduce incluso en impacto ambiental, como se puede ver en la siguiente gr\xE1fica."),Od.forEach(r),Oo=p(e),U=s(e,"DIV",{class:!0});var Vs=t(U);Fe=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),Il=p(Vs),Je=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),Vs.forEach(r),Co=p(e),g(Xe.$$.fragment,e),Vo=p(e),ya=s(e,"P",{});var Cd=t(ya);xl=n(Cd,"Esto es ilustrativo para un proyecto que busca un modelo (muy grande), liderado por un equipo que intenta de manera consciente reducir el impacto ambiental del preentrenamiento. La huella de ejecutar muchas pruebas para encontrar los mejores hiperpar\xE1metros es a\xFAn mayor."),Cd.forEach(r),Ho=p(e),ja=s(e,"P",{});var Vd=t(ja);Ml=n(Vd,"Ahora imag\xEDnate si cada vez que un equipo de investigaci\xF3n, una organizaci\xF3n estudiantil o una compa\xF1\xEDa intentaran entrenar un modelo, tuvieran que hacerlo desde cero. \xA1Esto implicar\xEDa costos globales enormes e innecesarios!"),Vd.forEach(r),Uo=p(e),Ta=s(e,"P",{});var Hd=t(Ta);Ll=n(Hd,"Esta es la raz\xF3n por la que compartir modelos de lenguaje es fundamental: compartir los pesos entrenados y construir sobre los existentes reduce el costo general y la huella de carbono de la comunidad."),Hd.forEach(r),Yo=p(e),Y=s(e,"H2",{class:!0});var Hs=t(Y);me=s(Hs,"A",{id:!0,class:!0,href:!0});var Ud=t(me);Ir=s(Ud,"SPAN",{});var Yd=t(Ir);g(Qe.$$.fragment,Yd),Yd.forEach(r),Ud.forEach(r),zl=p(Hs),xr=s(Hs,"SPAN",{});var Fd=t(xr);Gl=n(Fd,"Transferencia de aprendizaje (*Transfer learning*)"),Fd.forEach(r),Hs.forEach(r),Fo=p(e),g(We.$$.fragment,e),Jo=p(e),fe=s(e,"P",{});var Us=t(fe);Nl=n(Us,"El "),Mr=s(Us,"EM",{});var Jd=t(Mr);Sl=n(Jd,"preentrenamiento"),Jd.forEach(r),Rl=n(Us," es el acto de entrenar un modelo desde cero: los pesos se inicializan de manera aleat\xF3ria y el entrenamiento empieza sin un conocimiento previo."),Us.forEach(r),Xo=p(e),F=s(e,"DIV",{class:!0});var Ys=t(F);Ze=s(Ys,"IMG",{class:!0,src:!0,alt:!0}),Dl=p(Ys),Ke=s(Ys,"IMG",{class:!0,src:!0,alt:!0}),Ys.forEach(r),Qo=p(e),Pa=s(e,"P",{});var Xd=t(Pa);Bl=n(Xd,"Este preentrenamiento se hace usualmente sobre grandes cantidades de datos. Por lo anterior, requiere un gran corpus de datos y el entrenamiento puede tomar varias semanas."),Xd.forEach(r),Wo=p(e),w=s(e,"P",{});var Ja=t(w);Ol=n(Ja,"Por su parte, el ajuste (o "),Lr=s(Ja,"EM",{});var Qd=t(Lr);Cl=n(Qd,"fine-tuning"),Qd.forEach(r),Vl=n(Ja,") es el entrenamiento realizado "),zr=s(Ja,"STRONG",{});var Wd=t(zr);Hl=n(Wd,"despu\xE9s"),Wd.forEach(r),Ul=n(Ja," de que el modelo ha sido preentrenado. Para hacer el ajuste, comienzas con un modelo de lenguaje preentrenado y luego realizas un aprendizaje adicional con un conjunto de datos espec\xEDficos para tu tarea. Pero entonces \u2014 \xBFpor qu\xE9 no entrenar directamente para la tarea final? Hay un par de razones:"),Ja.forEach(r),Zo=p(e),A=s(e,"UL",{});var Xa=t(A);Gr=s(Xa,"LI",{});var Zd=t(Gr);Yl=n(Zd,"El modelo preentrenado ya est\xE1 entrenado con un conjunto de datos parecido al conjunto de datos de ajuste. De esta manera, el proceso de ajuste puede hacer uso del conocimiento adquirido por el modelo inicial durante el preentrenamiento (por ejemplo, para problemas de PLN, el modelo preentrenado tendr\xE1 alg\xFAn tipo de entendimiento estad\xEDstico del idioma que est\xE1s usando para tu tarea)."),Zd.forEach(r),Fl=p(Xa),Nr=s(Xa,"LI",{});var Kd=t(Nr);Jl=n(Kd,"Dado que el modelo preentrenado fue entrenado con muchos datos, el ajuste requerir\xE1 menos datos para tener resultados decentes."),Kd.forEach(r),Xl=p(Xa),Sr=s(Xa,"LI",{});var ec=t(Sr);Ql=n(ec,"Por la misma raz\xF3n, la cantidad de tiempo y recursos necesarios para tener buenos resultados es mucho menor."),ec.forEach(r),Xa.forEach(r),Ko=p(e),ve=s(e,"P",{});var Fs=t(ve);Wl=n(Fs,"Por ejemplo, se podr\xEDa aprovechar un modelo preentrenado en Ingl\xE9s y despu\xE9s ajustarlo con un corpus arXiv, teniendo como resultado un modelo basado en investigaci\xF3n cient\xEDfica. El ajuste solo requerir\xE1 una cantidad limitada de datos: el conocimiento que el modelo preentrenado ha adquirido se \u201Ctransfiere\u201D, de ah\xED el t\xE9rmino "),Rr=s(Fs,"EM",{});var ac=t(Rr);Zl=n(ac,"transferencia de aprendizaje"),ac.forEach(r),Kl=n(Fs,"."),Fs.forEach(r),es=p(e),J=s(e,"DIV",{class:!0});var Js=t(J);ea=s(Js,"IMG",{class:!0,src:!0,alt:!0}),en=p(Js),aa=s(Js,"IMG",{class:!0,src:!0,alt:!0}),Js.forEach(r),as=p(e),$a=s(e,"P",{});var rc=t($a);an=n(rc,"De ese modo, el ajuste de un modelo tendr\xE1 menos costos de tiempo, de datos, financieros y ambientales. Adem\xE1s es m\xE1s r\xE1pido y m\xE1s f\xE1cil de iterar en diferentes esquemas de ajuste, dado que el entrenamiento es menos restrictivo que un preentrenamiento completo."),rc.forEach(r),rs=p(e),ka=s(e,"P",{});var oc=t(ka);rn=n(oc,"Este proceso tambi\xE9n conseguir\xE1 mejores resultados que entrenar desde cero (a menos que tengas una gran cantidad de datos), raz\xF3n por la cual siempre deber\xEDas intentar aprovechar un modelo preentrenado \u2014 uno que est\xE9 tan cerca como sea posible a la tarea respectiva \u2014 y ajustarlo."),oc.forEach(r),os=p(e),X=s(e,"H2",{class:!0});var Xs=t(X);he=s(Xs,"A",{id:!0,class:!0,href:!0});var sc=t(he);Dr=s(sc,"SPAN",{});var tc=t(Dr);g(ra.$$.fragment,tc),tc.forEach(r),sc.forEach(r),on=p(Xs),Br=s(Xs,"SPAN",{});var lc=t(Br);sn=n(lc,"Arquitectura general"),lc.forEach(r),Xs.forEach(r),ss=p(e),wa=s(e,"P",{});var nc=t(wa);tn=n(nc,"En esta secci\xF3n, revisaremos la arquitectura general del Transformador. No te preocupes si no entiendes algunos de los conceptos; hay secciones detalladas m\xE1s adelante para cada uno de los componentes."),nc.forEach(r),ts=p(e),g(oa.$$.fragment,e),ls=p(e),Q=s(e,"H2",{class:!0});var Qs=t(Q);ge=s(Qs,"A",{id:!0,class:!0,href:!0});var ic=t(ge);Or=s(ic,"SPAN",{});var dc=t(Or);g(sa.$$.fragment,dc),dc.forEach(r),ic.forEach(r),ln=p(Qs),Cr=s(Qs,"SPAN",{});var cc=t(Cr);nn=n(cc,"Introducci\xF3n"),cc.forEach(r),Qs.forEach(r),ns=p(e),Aa=s(e,"P",{});var uc=t(Aa);dn=n(uc,"El modelo est\xE1 compuesto por dos bloques:"),uc.forEach(r),is=p(e),Ee=s(e,"UL",{});var Ws=t(Ee);Ia=s(Ws,"LI",{});var yi=t(Ia);Vr=s(yi,"STRONG",{});var pc=t(Vr);cn=n(pc,"Codificador (izquierda)"),pc.forEach(r),un=n(yi,": El codificador recibe una entrada y construye una representaci\xF3n de \xE9sta (sus caracter\xEDsticas). Esto significa que el modelo est\xE1 optimizado para conseguir un entendimiento a partir de la entrada."),yi.forEach(r),pn=p(Ws),xa=s(Ws,"LI",{});var ji=t(xa);Hr=s(ji,"STRONG",{});var mc=t(Hr);mn=n(mc,"Decodificador (derecha)"),mc.forEach(r),fn=n(ji,": El decodificador usa la representac\xF3n del codificador (caracter\xEDsticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est\xE1 optimizado para generar salidas."),ji.forEach(r),Ws.forEach(r),ds=p(e),W=s(e,"DIV",{class:!0});var Zs=t(W);ta=s(Zs,"IMG",{class:!0,src:!0,alt:!0}),vn=p(Zs),la=s(Zs,"IMG",{class:!0,src:!0,alt:!0}),Zs.forEach(r),cs=p(e),Ma=s(e,"P",{});var fc=t(Ma);hn=n(fc,"Cada una de estas partes puede ser usada de manera independiente, dependiendo de la tarea:"),fc.forEach(r),us=p(e),I=s(e,"UL",{});var Qa=t(I);La=s(Qa,"LI",{});var Ti=t(La);Ur=s(Ti,"STRONG",{});var vc=t(Ur);gn=n(vc,"Modelos con solo codificadores"),vc.forEach(r),En=n(Ti,": Buenos para las tareas que requieren el entendimiento de la entrada, como la clasificaci\xF3n de oraciones y reconocimiento de entidades nombradas."),Ti.forEach(r),bn=p(Qa),za=s(Qa,"LI",{});var Pi=t(za);Yr=s(Pi,"STRONG",{});var hc=t(Yr);_n=n(hc,"Modelos con solo decodificadores"),hc.forEach(r),qn=n(Pi,": Buenos para tareas generativas como la generaci\xF3n de textos."),Pi.forEach(r),yn=p(Qa),be=s(Qa,"LI",{});var bo=t(be);Fr=s(bo,"STRONG",{});var gc=t(Fr);jn=n(gc,"Modelos con codificadores y decodificadores"),gc.forEach(r),Tn=n(bo," o "),Jr=s(bo,"STRONG",{});var Ec=t(Jr);Pn=n(Ec,"Modelos secuencia a secuencia"),Ec.forEach(r),$n=n(bo,": Buenos para tareas generativas que requieren una entrada, como la traducci\xF3n o resumen."),bo.forEach(r),Qa.forEach(r),ps=p(e),Ga=s(e,"P",{});var bc=t(Ga);kn=n(bc,"Vamos a abordar estas arquitecturas de manera independiente en secciones posteriores."),bc.forEach(r),ms=p(e),Z=s(e,"H2",{class:!0});var Ks=t(Z);_e=s(Ks,"A",{id:!0,class:!0,href:!0});var _c=t(_e);Xr=s(_c,"SPAN",{});var qc=t(Xr);g(na.$$.fragment,qc),qc.forEach(r),_c.forEach(r),wn=p(Ks),Qr=s(Ks,"SPAN",{});var yc=t(Qr);An=n(yc,"Capas de atenci\xF3n"),yc.forEach(r),Ks.forEach(r),fs=p(e),x=s(e,"P",{});var Wa=t(x);In=n(Wa,"Una caracter\xEDstica clave de los Transformadores es que est\xE1n construidos con capas especiales llamadas "),Wr=s(Wa,"EM",{});var jc=t(Wr);xn=n(jc,"capas de atenci\xF3n"),jc.forEach(r),Mn=n(Wa,". De hecho, el t\xEDtulo del trabajo que introdujo la arquitectura de los Transformadores fue "),ia=s(Wa,"A",{href:!0,rel:!0});var Tc=t(ia);Ln=n(Tc,"\u201CAttention Is All You Need\u201D"),Tc.forEach(r),zn=n(Wa,". Vamos a explorar los detalles de las capas de atenci\xF3n m\xE1s adelante en el curso; por ahora, todo lo que tienes que saber es que esta capa va a indicarle al modelo que tiene que prestar especial atenci\xF3n a ciertas partes de la oraci\xF3n que le pasaste (y m\xE1s o menos ignorar las dem\xE1s), cuando trabaje con la representaci\xF3n de cada palabra."),Wa.forEach(r),vs=p(e),Na=s(e,"P",{});var Pc=t(Na);Gn=n(Pc,"Para poner esto en contexto, piensa en la tarea de traducir texto de Ingl\xE9s a Franc\xE9s. Dada la entrada \u201CYou like this course\u201D, un modelo de traducci\xF3n necesitar\xE1 tener en cuenta la palabra adyacente \u201CYou\u201D para obtener la traducci\xF3n correcta de la palabra \u201Clike\u201D, porque en Franc\xE9s el verbo \u201Clike\u201D se conjuga de manera distinta dependiendo del sujeto. Sin embargo, el resto de la oraci\xF3n no es \xFAtil para la traducci\xF3n de esa palabra. En la misma l\xEDnea, al traducir \u201Cthis\u201D, el modelo tambi\xE9n deber\xE1 prestar atenci\xF3n a la palabra \u201Ccourse\u201D, porque \u201Cthis\u201D se traduce de manera distinta dependiendo de si el nombre asociado es masculino o femenino. De nuevo, las otras palabras en la oraci\xF3n no van a importar para la traducci\xF3n de \u201Cthis\u201D. Con oraciones (y reglas gramaticales) m\xE1s complejas, el modelo deber\xE1 prestar especial atenci\xF3n a palabras que pueden aparecer m\xE1s lejos en la oraci\xF3n para traducir correctamente cada palabra."),Pc.forEach(r),hs=p(e),Sa=s(e,"P",{});var $c=t(Sa);Nn=n($c,"El mismo concepto aplica para cualquier tarea asociada con lenguaje natural: una palabra por si misma tiene un significado, pero ese significado est\xE1 afectado profundamente por el contexto, que puede ser cualquier palabra (o palabras) antes o despu\xE9s de la palabra que est\xE1 siendo estudiada."),$c.forEach(r),gs=p(e),Ra=s(e,"P",{});var kc=t(Ra);Sn=n(kc,"Ahora que tienes una idea de qu\xE9 son las capas de atenci\xF3n, echemos un vistazo m\xE1s de cerca a la arquitectura del Transformador."),kc.forEach(r),Es=p(e),K=s(e,"H2",{class:!0});var et=t(K);qe=s(et,"A",{id:!0,class:!0,href:!0});var wc=t(qe);Zr=s(wc,"SPAN",{});var Ac=t(Zr);g(da.$$.fragment,Ac),Ac.forEach(r),wc.forEach(r),Rn=p(et),Kr=s(et,"SPAN",{});var Ic=t(Kr);Dn=n(Ic,"La arquitectura original"),Ic.forEach(r),et.forEach(r),bs=p(e),Da=s(e,"P",{});var xc=t(Da);Bn=n(xc,"La arquitectura del Transformador fue dise\xF1ada originalmente para traducci\xF3n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un idioma dado, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo. En el codificador, las capas de atenci\xF3n pueden usar todas las palabras en una oraci\xF3n (dado que, como vimos, la traducci\xF3n de una palabra dada puede ser dependiente de lo que est\xE1 antes y despu\xE9s en la oraci\xF3n). Por su parte, el decodificador trabaja de manera secuencial y s\xF3lo le puede prestar atenci\xF3n a las palabras en la oraci\xF3n que ya ha traducido (es decir, s\xF3lo las palabras antes de que la palabra se ha generado). Por ejemplo, cuando hemos predecido las primeras tres palabras del objetivo de traducci\xF3n se las damos al decodificador, que luego usa todas las entradas del codificador para intentar predecir la cuarta palabra."),xc.forEach(r),_s=p(e),Ba=s(e,"P",{});var Mc=t(Ba);On=n(Mc,"Para acelerar el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), al decodificador se le alimenta el objetivo completo, pero no puede usar palabras futuras (si tuviera acceso a la palabra en la posici\xF3n 2 cuando trata de predecir la palabra en la posici\xF3n 2, \xA1el problema no ser\xEDa muy dificil!). Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci\xF3n s\xF3lo tendr\xEDa acceso a las palabras en las posiciones 1 a 3."),Mc.forEach(r),qs=p(e),Oa=s(e,"P",{});var Lc=t(Oa);Cn=n(Lc,"La arquitectura original del Transformador se ve\xEDa as\xED, con el codificador a la izquierda y el decodificador a la derecha:"),Lc.forEach(r),ys=p(e),ee=s(e,"DIV",{class:!0});var at=t(ee);ca=s(at,"IMG",{class:!0,src:!0,alt:!0}),Vn=p(at),ua=s(at,"IMG",{class:!0,src:!0,alt:!0}),at.forEach(r),js=p(e),Ca=s(e,"P",{});var zc=t(Ca);Hn=n(zc,"Observa que la primera capa de atenci\xF3n en un bloque de decodificador presta atenci\xF3n a todas las entradas (pasadas) al decodificador, mientras que la segunda capa de atenci\xF3n usa la salida del codificador. De esta manera puede acceder a toda la oraci\xF3n de entrada para predecir de mejor manera la palabra actual. Esto es muy \xFAtil dado que diferentes idiomas pueden tener reglas gramaticales que ponen las palabras en \xF3rden distinto o alg\xFAn contexto que se provee despu\xE9s puede ser \xFAtil para determinar la mejor traducci\xF3n de una palabra dada."),zc.forEach(r),Ts=p(e),ye=s(e,"P",{});var rt=t(ye);Un=n(rt,"La "),eo=s(rt,"EM",{});var Gc=t(eo);Yn=n(Gc,"m\xE1scara de atenci\xF3n"),Gc.forEach(r),Fn=n(rt," tambi\xE9n se puede usar en el codificador/decodificador para evitar que el modelo preste atenci\xF3n a algunas palabras especiales \u2014por ejemplo, la palabra especial de relleno que hace que todas las entradas sean de la misma longitud cuando se agrupan oraciones."),rt.forEach(r),Ps=p(e),ae=s(e,"H2",{class:!0});var ot=t(ae);je=s(ot,"A",{id:!0,class:!0,href:!0});var Nc=t(je);ao=s(Nc,"SPAN",{});var Sc=t(ao);g(pa.$$.fragment,Sc),Sc.forEach(r),Nc.forEach(r),Jn=p(ot),ro=s(ot,"SPAN",{});var Rc=t(ro);Xn=n(Rc,"Arquitecturas vs. puntos de control"),Rc.forEach(r),ot.forEach(r),$s=p(e),v=s(e,"P",{});var G=t(v);Qn=n(G,"A medida que estudiemos a profundidad los Transformadores, ver\xE1s menciones a "),oo=s(G,"EM",{});var Dc=t(oo);Wn=n(Dc,"arquitecturas"),Dc.forEach(r),Zn=n(G,", "),so=s(G,"EM",{});var Bc=t(so);Kn=n(Bc,"puntos de control"),Bc.forEach(r),ei=n(G," ("),to=s(G,"EM",{});var Oc=t(to);ai=n(Oc,"checkpoints"),Oc.forEach(r),ri=n(G,") y "),lo=s(G,"EM",{});var Cc=t(lo);oi=n(Cc,"modelos"),Cc.forEach(r),si=n(G,". Estos t\xE9rminos tienen significados ligeramentes diferentes:"),G.forEach(r),ks=p(e),M=s(e,"UL",{});var Za=t(M);Va=s(Za,"LI",{});var $i=t(Va);no=s($i,"STRONG",{});var Vc=t(no);ti=n(Vc,"Arquitecturas"),Vc.forEach(r),li=n($i,": Este es el esqueleto del modelo \u2014 la definici\xF3n de cada capa y cada operaci\xF3n que sucede al interior del modelo."),$i.forEach(r),ni=p(Za),Ha=s(Za,"LI",{});var ki=t(Ha);io=s(ki,"STRONG",{});var Hc=t(io);ii=n(Hc,"Puntos de control"),Hc.forEach(r),di=n(ki,": Estos son los pesos que ser\xE1n cargados en una arquitectura dada."),ki.forEach(r),ci=p(Za),L=s(Za,"LI",{});var va=t(L);co=s(va,"STRONG",{});var Uc=t(co);ui=n(Uc,"Modelo"),Uc.forEach(r),pi=n(va,": Esta es un t\xE9rmino sombrilla que no es tan preciso como \u201Carquitectura\u201D o \u201Cpunto de control\u201D y puede significar ambas cosas. Este curso especificar\xE1 "),uo=s(va,"EM",{});var Yc=t(uo);mi=n(Yc,"arquitectura"),Yc.forEach(r),fi=n(va," o "),po=s(va,"EM",{});var Fc=t(po);vi=n(Fc,"punto de control"),Fc.forEach(r),hi=n(va," cuando sea relevante para evitar ambig\xFCedades."),va.forEach(r),Za.forEach(r),ws=p(e),z=s(e,"P",{});var Ka=t(z);gi=n(Ka,"Por ejemplo, mientras que BERT es una arquitectura, "),mo=s(Ka,"CODE",{});var Jc=t(mo);Ei=n(Jc,"bert-base-cased"),Jc.forEach(r),bi=n(Ka," - un conjunto de pesos entrenados por el equipo de Google para la primera versi\xF3n de BERT - es un punto de control. Sin embargo, se podr\xEDa decir \u201Cel modelo BERT\u201D y \u201Cel modelo "),fo=s(Ka,"CODE",{});var Xc=t(fo);_i=n(Xc,"bert-base-cased"),Xc.forEach(r),qi=n(Ka,"\u201C."),Ka.forEach(r),this.h()},h(){c(S,"name","hf:doc:metadata"),c(S,"content",JSON.stringify(ou)),c(re,"id","cmo-funcionan-los-transformadores"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#cmo-funcionan-los-transformadores"),c(R,"class","relative group"),c(oe,"id","un-poco-de-historia-sobre-los-transformadores"),c(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oe,"href","#un-poco-de-historia-sobre-los-transformadores"),c(D,"class","relative group"),c($e,"class","block dark:hidden"),m($e.src,Ii="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c($e,"src",Ii),c($e,"alt","A brief chronology of Transformers models."),c(ke,"class","hidden dark:block"),m(ke.src,xi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(ke,"src",xi),c(ke,"alt","A brief chronology of Transformers models."),c(B,"class","flex justify-center"),c(we,"href","https://arxiv.org/abs/1706.03762"),c(we,"rel","nofollow"),c(Ae,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Ae,"rel","nofollow"),c(Ie,"href","https://arxiv.org/abs/1810.04805"),c(Ie,"rel","nofollow"),c(xe,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(xe,"rel","nofollow"),c(Me,"href","https://arxiv.org/abs/1910.01108"),c(Me,"rel","nofollow"),c(Le,"href","https://arxiv.org/abs/1910.13461"),c(Le,"rel","nofollow"),c(ze,"href","https://arxiv.org/abs/1910.10683"),c(ze,"rel","nofollow"),c(Ge,"href","https://arxiv.org/abs/2005.14165"),c(Ge,"rel","nofollow"),c(de,"id","los-transformadores-son-modelos-de-lenguaje"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#los-transformadores-son-modelos-de-lenguaje"),c(O,"class","relative group"),c(Be,"class","block dark:hidden"),m(Be.src,Mi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(Be,"src",Mi),c(Be,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(Oe,"class","hidden dark:block"),m(Oe.src,Li="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(Oe,"src",Li),c(Oe,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(C,"class","flex justify-center"),c(Ce,"class","block dark:hidden"),m(Ce.src,zi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(Ce,"src",zi),c(Ce,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(Ve,"class","hidden dark:block"),m(Ve.src,Gi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(Ve,"src",Gi),c(Ve,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(V,"class","flex justify-center"),c(pe,"id","los-transformadores-son-modelos-grandes"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#los-transformadores-son-modelos-grandes"),c(H,"class","relative group"),m(Ye.src,Ni="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(Ye,"src",Ni),c(Ye,"alt","Number of parameters of recent Transformers models"),c(Ye,"width","90%"),c(Ue,"class","flex justify-center"),c(Fe,"class","block dark:hidden"),m(Fe.src,Si="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(Fe,"src",Si),c(Fe,"alt","The carbon footprint of a large language model."),c(Je,"class","hidden dark:block"),m(Je.src,Ri="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(Je,"src",Ri),c(Je,"alt","The carbon footprint of a large language model."),c(U,"class","flex justify-center"),c(me,"id","transferencia-de-aprendizaje-transfer-learning"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transferencia-de-aprendizaje-transfer-learning"),c(Y,"class","relative group"),c(Ze,"class","block dark:hidden"),m(Ze.src,Di="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(Ze,"src",Di),c(Ze,"alt","The pretraining of a language model is costly in both time and money."),c(Ke,"class","hidden dark:block"),m(Ke.src,Bi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(Ke,"src",Bi),c(Ke,"alt","The pretraining of a language model is costly in both time and money."),c(F,"class","flex justify-center"),c(ea,"class","block dark:hidden"),m(ea.src,Oi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(ea,"src",Oi),c(ea,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(aa,"class","hidden dark:block"),m(aa.src,Ci="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(aa,"src",Ci),c(aa,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(J,"class","flex justify-center"),c(he,"id","arquitectura-general"),c(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(he,"href","#arquitectura-general"),c(X,"class","relative group"),c(ge,"id","introduccin"),c(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ge,"href","#introduccin"),c(Q,"class","relative group"),c(ta,"class","block dark:hidden"),m(ta.src,Vi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(ta,"src",Vi),c(ta,"alt","Architecture of a Transformers models"),c(la,"class","hidden dark:block"),m(la.src,Hi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(la,"src",Hi),c(la,"alt","Architecture of a Transformers models"),c(W,"class","flex justify-center"),c(_e,"id","capas-de-atencin"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#capas-de-atencin"),c(Z,"class","relative group"),c(ia,"href","https://arxiv.org/abs/1706.03762"),c(ia,"rel","nofollow"),c(qe,"id","la-arquitectura-original"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#la-arquitectura-original"),c(K,"class","relative group"),c(ca,"class","block dark:hidden"),m(ca.src,Ui="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(ca,"src",Ui),c(ca,"alt","Architecture of a Transformers models"),c(ua,"class","hidden dark:block"),m(ua.src,Yi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(ua,"src",Yi),c(ua,"alt","Architecture of a Transformers models"),c(ee,"class","flex justify-center"),c(je,"id","arquitecturas-vs-puntos-de-control"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#arquitecturas-vs-puntos-de-control"),c(ae,"class","relative group")},m(e,i){a(document.head,S),d(e,_o,i),d(e,R,i),a(R,re),a(re,er),E(Te,er,null),a(R,st),a(R,ar),a(ar,tt),d(e,qo,i),d(e,ha,i),a(ha,lt),d(e,yo,i),d(e,D,i),a(D,oe),a(oe,rr),E(Pe,rr,null),a(D,nt),a(D,or),a(or,it),d(e,jo,i),d(e,ga,i),a(ga,dt),d(e,To,i),d(e,B,i),a(B,$e),a(B,ct),a(B,ke),d(e,Po,i),d(e,se,i),a(se,ut),a(se,we),a(we,pt),a(se,mt),d(e,$o,i),d(e,f,i),a(f,sr),a(sr,te),a(te,tr),a(tr,ft),a(te,vt),a(te,Ae),a(Ae,ht),a(te,gt),a(f,Et),a(f,lr),a(lr,le),a(le,nr),a(nr,bt),a(le,_t),a(le,Ie),a(Ie,qt),a(le,yt),a(f,jt),a(f,ir),a(ir,ne),a(ne,dr),a(dr,Tt),a(ne,Pt),a(ne,xe),a(xe,$t),a(ne,kt),a(f,wt),a(f,cr),a(cr,ie),a(ie,ur),a(ur,At),a(ie,It),a(ie,Me),a(Me,xt),a(ie,Mt),a(f,Lt),a(f,pr),a(pr,j),a(j,mr),a(mr,zt),a(j,Gt),a(j,Le),a(Le,Nt),a(j,St),a(j,ze),a(ze,Rt),a(j,Dt),a(f,Bt),a(f,fr),a(fr,T),a(T,vr),a(vr,Ot),a(T,Ct),a(T,Ge),a(Ge,Vt),a(T,Ht),a(T,hr),a(hr,Ut),a(T,Yt),d(e,ko,i),d(e,Ea,i),a(Ea,Ft),d(e,wo,i),d(e,P,i),a(P,Ne),a(Ne,Jt),a(Ne,gr),a(gr,Xt),a(Ne,Qt),a(P,Wt),a(P,Se),a(Se,Zt),a(Se,Er),a(Er,Kt),a(Se,el),a(P,al),a(P,Re),a(Re,rl),a(Re,br),a(br,ol),a(Re,sl),d(e,Ao,i),d(e,ba,i),a(ba,tl),d(e,Io,i),d(e,O,i),a(O,de),a(de,_r),E(De,_r,null),a(O,ll),a(O,qr),a(qr,nl),d(e,xo,i),d(e,ce,i),a(ce,il),a(ce,yr),a(yr,dl),a(ce,cl),d(e,Mo,i),d(e,$,i),a($,ul),a($,jr),a(jr,pl),a($,ml),a($,Tr),a(Tr,fl),a($,vl),d(e,Lo,i),d(e,k,i),a(k,hl),a(k,Pr),a(Pr,gl),a(k,El),a(k,$r),a($r,bl),a(k,_l),d(e,zo,i),d(e,C,i),a(C,Be),a(C,ql),a(C,Oe),d(e,Go,i),d(e,ue,i),a(ue,yl),a(ue,kr),a(kr,jl),a(ue,Tl),d(e,No,i),d(e,V,i),a(V,Ce),a(V,Pl),a(V,Ve),d(e,So,i),d(e,H,i),a(H,pe),a(pe,wr),E(He,wr,null),a(H,$l),a(H,Ar),a(Ar,kl),d(e,Ro,i),d(e,_a,i),a(_a,wl),d(e,Do,i),d(e,Ue,i),a(Ue,Ye),d(e,Bo,i),d(e,qa,i),a(qa,Al),d(e,Oo,i),d(e,U,i),a(U,Fe),a(U,Il),a(U,Je),d(e,Co,i),E(Xe,e,i),d(e,Vo,i),d(e,ya,i),a(ya,xl),d(e,Ho,i),d(e,ja,i),a(ja,Ml),d(e,Uo,i),d(e,Ta,i),a(Ta,Ll),d(e,Yo,i),d(e,Y,i),a(Y,me),a(me,Ir),E(Qe,Ir,null),a(Y,zl),a(Y,xr),a(xr,Gl),d(e,Fo,i),E(We,e,i),d(e,Jo,i),d(e,fe,i),a(fe,Nl),a(fe,Mr),a(Mr,Sl),a(fe,Rl),d(e,Xo,i),d(e,F,i),a(F,Ze),a(F,Dl),a(F,Ke),d(e,Qo,i),d(e,Pa,i),a(Pa,Bl),d(e,Wo,i),d(e,w,i),a(w,Ol),a(w,Lr),a(Lr,Cl),a(w,Vl),a(w,zr),a(zr,Hl),a(w,Ul),d(e,Zo,i),d(e,A,i),a(A,Gr),a(Gr,Yl),a(A,Fl),a(A,Nr),a(Nr,Jl),a(A,Xl),a(A,Sr),a(Sr,Ql),d(e,Ko,i),d(e,ve,i),a(ve,Wl),a(ve,Rr),a(Rr,Zl),a(ve,Kl),d(e,es,i),d(e,J,i),a(J,ea),a(J,en),a(J,aa),d(e,as,i),d(e,$a,i),a($a,an),d(e,rs,i),d(e,ka,i),a(ka,rn),d(e,os,i),d(e,X,i),a(X,he),a(he,Dr),E(ra,Dr,null),a(X,on),a(X,Br),a(Br,sn),d(e,ss,i),d(e,wa,i),a(wa,tn),d(e,ts,i),E(oa,e,i),d(e,ls,i),d(e,Q,i),a(Q,ge),a(ge,Or),E(sa,Or,null),a(Q,ln),a(Q,Cr),a(Cr,nn),d(e,ns,i),d(e,Aa,i),a(Aa,dn),d(e,is,i),d(e,Ee,i),a(Ee,Ia),a(Ia,Vr),a(Vr,cn),a(Ia,un),a(Ee,pn),a(Ee,xa),a(xa,Hr),a(Hr,mn),a(xa,fn),d(e,ds,i),d(e,W,i),a(W,ta),a(W,vn),a(W,la),d(e,cs,i),d(e,Ma,i),a(Ma,hn),d(e,us,i),d(e,I,i),a(I,La),a(La,Ur),a(Ur,gn),a(La,En),a(I,bn),a(I,za),a(za,Yr),a(Yr,_n),a(za,qn),a(I,yn),a(I,be),a(be,Fr),a(Fr,jn),a(be,Tn),a(be,Jr),a(Jr,Pn),a(be,$n),d(e,ps,i),d(e,Ga,i),a(Ga,kn),d(e,ms,i),d(e,Z,i),a(Z,_e),a(_e,Xr),E(na,Xr,null),a(Z,wn),a(Z,Qr),a(Qr,An),d(e,fs,i),d(e,x,i),a(x,In),a(x,Wr),a(Wr,xn),a(x,Mn),a(x,ia),a(ia,Ln),a(x,zn),d(e,vs,i),d(e,Na,i),a(Na,Gn),d(e,hs,i),d(e,Sa,i),a(Sa,Nn),d(e,gs,i),d(e,Ra,i),a(Ra,Sn),d(e,Es,i),d(e,K,i),a(K,qe),a(qe,Zr),E(da,Zr,null),a(K,Rn),a(K,Kr),a(Kr,Dn),d(e,bs,i),d(e,Da,i),a(Da,Bn),d(e,_s,i),d(e,Ba,i),a(Ba,On),d(e,qs,i),d(e,Oa,i),a(Oa,Cn),d(e,ys,i),d(e,ee,i),a(ee,ca),a(ee,Vn),a(ee,ua),d(e,js,i),d(e,Ca,i),a(Ca,Hn),d(e,Ts,i),d(e,ye,i),a(ye,Un),a(ye,eo),a(eo,Yn),a(ye,Fn),d(e,Ps,i),d(e,ae,i),a(ae,je),a(je,ao),E(pa,ao,null),a(ae,Jn),a(ae,ro),a(ro,Xn),d(e,$s,i),d(e,v,i),a(v,Qn),a(v,oo),a(oo,Wn),a(v,Zn),a(v,so),a(so,Kn),a(v,ei),a(v,to),a(to,ai),a(v,ri),a(v,lo),a(lo,oi),a(v,si),d(e,ks,i),d(e,M,i),a(M,Va),a(Va,no),a(no,ti),a(Va,li),a(M,ni),a(M,Ha),a(Ha,io),a(io,ii),a(Ha,di),a(M,ci),a(M,L),a(L,co),a(co,ui),a(L,pi),a(L,uo),a(uo,mi),a(L,fi),a(L,po),a(po,vi),a(L,hi),d(e,ws,i),d(e,z,i),a(z,gi),a(z,mo),a(mo,Ei),a(z,bi),a(z,fo),a(fo,_i),a(z,qi),As=!0},p:eu,i(e){As||(b(Te.$$.fragment,e),b(Pe.$$.fragment,e),b(De.$$.fragment,e),b(He.$$.fragment,e),b(Xe.$$.fragment,e),b(Qe.$$.fragment,e),b(We.$$.fragment,e),b(ra.$$.fragment,e),b(oa.$$.fragment,e),b(sa.$$.fragment,e),b(na.$$.fragment,e),b(da.$$.fragment,e),b(pa.$$.fragment,e),As=!0)},o(e){_(Te.$$.fragment,e),_(Pe.$$.fragment,e),_(De.$$.fragment,e),_(He.$$.fragment,e),_(Xe.$$.fragment,e),_(Qe.$$.fragment,e),_(We.$$.fragment,e),_(ra.$$.fragment,e),_(oa.$$.fragment,e),_(sa.$$.fragment,e),_(na.$$.fragment,e),_(da.$$.fragment,e),_(pa.$$.fragment,e),As=!1},d(e){r(S),e&&r(_o),e&&r(R),q(Te),e&&r(qo),e&&r(ha),e&&r(yo),e&&r(D),q(Pe),e&&r(jo),e&&r(ga),e&&r(To),e&&r(B),e&&r(Po),e&&r(se),e&&r($o),e&&r(f),e&&r(ko),e&&r(Ea),e&&r(wo),e&&r(P),e&&r(Ao),e&&r(ba),e&&r(Io),e&&r(O),q(De),e&&r(xo),e&&r(ce),e&&r(Mo),e&&r($),e&&r(Lo),e&&r(k),e&&r(zo),e&&r(C),e&&r(Go),e&&r(ue),e&&r(No),e&&r(V),e&&r(So),e&&r(H),q(He),e&&r(Ro),e&&r(_a),e&&r(Do),e&&r(Ue),e&&r(Bo),e&&r(qa),e&&r(Oo),e&&r(U),e&&r(Co),q(Xe,e),e&&r(Vo),e&&r(ya),e&&r(Ho),e&&r(ja),e&&r(Uo),e&&r(Ta),e&&r(Yo),e&&r(Y),q(Qe),e&&r(Fo),q(We,e),e&&r(Jo),e&&r(fe),e&&r(Xo),e&&r(F),e&&r(Qo),e&&r(Pa),e&&r(Wo),e&&r(w),e&&r(Zo),e&&r(A),e&&r(Ko),e&&r(ve),e&&r(es),e&&r(J),e&&r(as),e&&r($a),e&&r(rs),e&&r(ka),e&&r(os),e&&r(X),q(ra),e&&r(ss),e&&r(wa),e&&r(ts),q(oa,e),e&&r(ls),e&&r(Q),q(sa),e&&r(ns),e&&r(Aa),e&&r(is),e&&r(Ee),e&&r(ds),e&&r(W),e&&r(cs),e&&r(Ma),e&&r(us),e&&r(I),e&&r(ps),e&&r(Ga),e&&r(ms),e&&r(Z),q(na),e&&r(fs),e&&r(x),e&&r(vs),e&&r(Na),e&&r(hs),e&&r(Sa),e&&r(gs),e&&r(Ra),e&&r(Es),e&&r(K),q(da),e&&r(bs),e&&r(Da),e&&r(_s),e&&r(Ba),e&&r(qs),e&&r(Oa),e&&r(ys),e&&r(ee),e&&r(js),e&&r(Ca),e&&r(Ts),e&&r(ye),e&&r(Ps),e&&r(ae),q(pa),e&&r($s),e&&r(v),e&&r(ks),e&&r(M),e&&r(ws),e&&r(z)}}}const ou={local:"cmo-funcionan-los-transformadores",sections:[{local:"un-poco-de-historia-sobre-los-transformadores",title:"Un poco de historia sobre los Transformadores"},{local:"los-transformadores-son-modelos-de-lenguaje",title:"Los Transformadores son modelos de lenguaje"},{local:"los-transformadores-son-modelos-grandes",title:"Los Transformadores son modelos grandes"},{local:"transferencia-de-aprendizaje-transfer-learning",title:"Transferencia de aprendizaje (*Transfer learning*)"},{local:"arquitectura-general",title:"Arquitectura general"},{local:"introduccin",title:"Introducci\xF3n"},{local:"capas-de-atencin",title:"Capas de atenci\xF3n"},{local:"la-arquitectura-original",title:"La arquitectura original"},{local:"arquitecturas-vs-puntos-de-control",title:"Arquitecturas vs. puntos de control"}],title:"\xBFC\xF3mo funcionan los Transformadores?"};function su(Ai){return au(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class iu extends Qc{constructor(S){super();Wc(this,S,su,ru,Zc,{})}}export{iu as default,ou as metadata};
