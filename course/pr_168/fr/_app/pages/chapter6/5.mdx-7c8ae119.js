import{S as ho,i as mo,s as jo,e as r,k as u,w as m,t,M as go,c as o,d as e,m as c,a as i,x as d,h as l,b as T,F as n,g as p,y as j,q as g,o as x,B as q,v as xo}from"../../chunks/vendor-1e8b365d.js";import{T as _e}from"../../chunks/Tip-62b14c6e.js";import{Y as qo}from"../../chunks/Youtube-c2a8cc39.js";import{I as ot}from"../../chunks/IconCopyLink-483c28ba.js";import{C as $}from"../../chunks/CodeBlock-e5764662.js";import{D as bo}from"../../chunks/DocNotebookDropdown-37d928d3.js";function $o(O){let f,_;return{c(){f=r("p"),_=t("\u{1F4A1} Cette section couvre le BPE en profondeur, allant jusqu\u2019\xE0 montrer une impl\xE9mentation compl\xE8te. Vous pouvez passer directement \xE0 la fin si vous souhaitez simplement avoir un aper\xE7u g\xE9n\xE9ral de l\u2019algorithme de tokenisation.")},l(h){f=o(h,"P",{});var b=i(f);_=l(b,"\u{1F4A1} Cette section couvre le BPE en profondeur, allant jusqu\u2019\xE0 montrer une impl\xE9mentation compl\xE8te. Vous pouvez passer directement \xE0 la fin si vous souhaitez simplement avoir un aper\xE7u g\xE9n\xE9ral de l\u2019algorithme de tokenisation."),b.forEach(e)},m(h,b){p(h,f,b),n(f,_)},d(h){h&&e(f)}}}function vo(O){let f,_,h,b,E,v,k,z;return{c(){f=r("p"),_=t("Les "),h=r("em"),b=t("tokenizer"),E=t(" de GPT-2 et de RoBERTa (qui sont assez similaires) ont une fa\xE7on intelligente de g\xE9rer ce probl\xE8me : ils ne consid\xE8rent pas les mots comme \xE9tant \xE9crits avec des caract\xE8res Unicode, mais avec des octets. De cette fa\xE7on, le vocabulaire de base a une petite taille (256), mais tous les caract\xE8res auxquels vous pouvez penser seront inclus et ne finiront pas par \xEAtre convertis en un token inconnu. Cette astuce est appel\xE9e "),v=r("em"),k=t("byte-level BPE"),z=t(".")},l(N){f=o(N,"P",{});var w=i(f);_=l(w,"Les "),h=o(w,"EM",{});var M=i(h);b=l(M,"tokenizer"),M.forEach(e),E=l(w," de GPT-2 et de RoBERTa (qui sont assez similaires) ont une fa\xE7on intelligente de g\xE9rer ce probl\xE8me : ils ne consid\xE8rent pas les mots comme \xE9tant \xE9crits avec des caract\xE8res Unicode, mais avec des octets. De cette fa\xE7on, le vocabulaire de base a une petite taille (256), mais tous les caract\xE8res auxquels vous pouvez penser seront inclus et ne finiront pas par \xEAtre convertis en un token inconnu. Cette astuce est appel\xE9e "),v=o(w,"EM",{});var U=i(v);k=l(U,"byte-level BPE"),U.forEach(e),z=l(w,"."),w.forEach(e)},m(N,w){p(N,f,w),n(f,_),n(f,h),n(h,b),n(f,E),n(f,v),n(v,k),n(f,z)},d(N){N&&e(f)}}}function _o(O){let f,_,h,b,E;return{c(){f=r("p"),_=t("\u270F\uFE0F "),h=r("strong"),b=t("A votre tour !"),E=t(" A votre avis, quelle sera la prochaine r\xE8gle de fusion ?")},l(v){f=o(v,"P",{});var k=i(f);_=l(k,"\u270F\uFE0F "),h=o(k,"STRONG",{});var z=i(h);b=l(z,"A votre tour !"),z.forEach(e),E=l(k," A votre avis, quelle sera la prochaine r\xE8gle de fusion ?"),k.forEach(e)},m(v,k){p(v,f,k),n(f,_),n(f,h),n(h,b),n(f,E)},d(v){v&&e(f)}}}function ko(O){let f,_,h,b,E;return{c(){f=r("p"),_=t("\u270F\uFE0F "),h=r("strong"),b=t("A votre tour !"),E=t(" Comment pensez-vous que le mot \u201Cunhug\u201D\u201C sera tokenized ?")},l(v){f=o(v,"P",{});var k=i(f);_=l(k,"\u270F\uFE0F "),h=o(k,"STRONG",{});var z=i(h);b=l(z,"A votre tour !"),z.forEach(e),E=l(k," Comment pensez-vous que le mot \u201Cunhug\u201D\u201C sera tokenized ?"),k.forEach(e)},m(v,k){p(v,f,k),n(f,_),n(f,h),n(h,b),n(f,E)},d(v){v&&e(f)}}}function wo(O){let f,_,h,b,E,v,k,z;return{c(){f=r("p"),_=t("\u{1F4A1} Utiliser "),h=r("code"),b=t("train_new_from_iterator()"),E=t(" sur le m\xEAme corpus ne donnera pas exactement le m\xEAme vocabulaire. C\u2019est parce que lorsqu\u2019il y a un choix de la paire la plus fr\xE9quente, nous avons s\xE9lectionn\xE9 la premi\xE8re rencontr\xE9e, alors que la biblioth\xE8que \u{1F917} "),v=r("em"),k=t("Tokenizers"),z=t(" s\xE9lectionne la premi\xE8re en fonction de ses identifiants internes.")},l(N){f=o(N,"P",{});var w=i(f);_=l(w,"\u{1F4A1} Utiliser "),h=o(w,"CODE",{});var M=i(h);b=l(M,"train_new_from_iterator()"),M.forEach(e),E=l(w," sur le m\xEAme corpus ne donnera pas exactement le m\xEAme vocabulaire. C\u2019est parce que lorsqu\u2019il y a un choix de la paire la plus fr\xE9quente, nous avons s\xE9lectionn\xE9 la premi\xE8re rencontr\xE9e, alors que la biblioth\xE8que \u{1F917} "),v=o(w,"EM",{});var U=i(v);k=l(U,"Tokenizers"),U.forEach(e),z=l(w," s\xE9lectionne la premi\xE8re en fonction de ses identifiants internes."),w.forEach(e)},m(N,w){p(N,f,w),n(f,_),n(f,h),n(h,b),n(f,E),n(f,v),n(v,k),n(f,z)},d(N){N&&e(f)}}}function Eo(O){let f,_;return{c(){f=r("p"),_=t("\u26A0\uFE0F Notre impl\xE9mentation lancera une erreur s\u2019il y a un caract\xE8re inconnu puisque nous n\u2019avons rien fait pour les g\xE9rer. GPT-2 n\u2019a pas r\xE9ellement de jeton inconnu (il est impossible d\u2019obtenir un caract\xE8re inconnu en utilisant le BPE au niveau de l\u2019octet), mais cela pourrait arriver ici parce que nous n\u2019avons pas inclus tous les octets possibles dans le vocabulaire initial. Cet aspect du BPE d\xE9passe le cadre de cette section, nous avons donc laiss\xE9 les d\xE9tails de c\xF4t\xE9.")},l(h){f=o(h,"P",{});var b=i(f);_=l(b,"\u26A0\uFE0F Notre impl\xE9mentation lancera une erreur s\u2019il y a un caract\xE8re inconnu puisque nous n\u2019avons rien fait pour les g\xE9rer. GPT-2 n\u2019a pas r\xE9ellement de jeton inconnu (il est impossible d\u2019obtenir un caract\xE8re inconnu en utilisant le BPE au niveau de l\u2019octet), mais cela pourrait arriver ici parce que nous n\u2019avons pas inclus tous les octets possibles dans le vocabulaire initial. Cet aspect du BPE d\xE9passe le cadre de cette section, nous avons donc laiss\xE9 les d\xE9tails de c\xF4t\xE9."),b.forEach(e)},m(h,b){p(h,f,b),n(f,_)},d(h){h&&e(f)}}}function yo(O){let f,_,h,b,E,v,k,z,N,w,M,U,gs,ke,it,ut,Dn,xs,Nn,Q,On,W,X,we,qs,ct,Ee,ft,Bn,ee,ht,Mn,bs,Ln,F,mt,ye,dt,jt,Pe,gt,xt,Hn,ss,Sn,L,qt,ze,bt,$t,Ce,vt,_t,Te,kt,wt,Gn,H,Et,Ae,yt,Pt,De,zt,Ct,Ne,Tt,At,In,ne,Dt,Rn,$s,Vn,C,Nt,Oe,Ot,Bt,Be,Mt,Lt,Me,Ht,St,Le,Gt,It,He,Rt,Vt,Se,Ut,Ft,Un,vs,Fn,y,Kt,Ge,Jt,Wt,Ie,Yt,Zt,Re,Qt,Xt,Ve,sl,el,Ue,nl,al,Fe,tl,ll,Ke,pl,rl,Kn,S,ol,Je,il,ul,We,cl,fl,Ye,hl,ml,Jn,_s,Wn,G,dl,Ze,jl,gl,Qe,xl,ql,Xe,bl,$l,Yn,ks,Zn,I,vl,sn,_l,kl,en,wl,El,nn,yl,Pl,Qn,ws,Xn,ae,zl,sa,es,ea,Y,ns,an,Es,Cl,tn,Tl,na,te,Al,aa,R,ln,Dl,Nl,pn,Ol,Bl,rn,Ml,Ll,on,Hl,ta,le,Sl,la,ys,pa,P,Gl,un,Il,Rl,cn,Vl,Ul,fn,Fl,Kl,hn,Jl,Wl,mn,Yl,Zl,dn,Ql,Xl,jn,sp,ep,ra,as,oa,Z,ts,gn,Ps,np,xn,ap,ia,pe,tp,ua,re,lp,ca,zs,fa,V,pp,qn,rp,op,bn,ip,up,$n,cp,fp,ha,Cs,ma,oe,hp,da,Ts,ja,As,ga,ie,mp,xa,Ds,qa,Ns,ba,K,dp,vn,jp,gp,_n,xp,qp,$a,Os,va,ue,bp,_a,Bs,ka,ce,$p,wa,Ms,Ea,fe,vp,ya,Ls,Pa,Hs,za,he,_p,Ca,Ss,Ta,Gs,Aa,J,kp,kn,wp,Ep,wn,yp,Pp,Da,Is,Na,ls,zp,En,Cp,Tp,Oa,Rs,Ba,me,Ap,Ma,Vs,La,Us,Ha,de,Dp,Sa,Fs,Ga,ps,Np,yn,Op,Bp,Ia,Ks,Ra,Js,Va,rs,Mp,Pn,Lp,Hp,Ua,Ws,Fa,Ys,Ka,os,Ja,je,Sp,Wa,Zs,Ya,ge,Gp,Za,Qs,Qa,Xs,Xa,is,st,xe,Ip,et;return v=new ot({}),M=new bo({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"}]}}),xs=new qo({props:{id:"HEikzVL-lZU"}}),Q=new _e({props:{$$slots:{default:[$o]},$$scope:{ctx:O}}}),qs=new ot({}),bs=new $({props:{code:'"hug", "pug", "pun", "bun", "hugs" # "c\xE2lin", "carlin", "jeu de mots", "brioche", "c\xE2lins"...',highlighted:'<span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-string">&quot;pug&quot;</span>, <span class="hljs-string">&quot;pun&quot;</span>, <span class="hljs-string">&quot;bun&quot;</span>, <span class="hljs-string">&quot;hugs&quot;</span> <span class="hljs-meta"># <span class="hljs-string">&quot;c\xE2lin&quot;</span>, <span class="hljs-string">&quot;carlin&quot;</span>, <span class="hljs-string">&quot;jeu de mots&quot;</span>, <span class="hljs-string">&quot;brioche&quot;</span>, <span class="hljs-string">&quot;c\xE2lins&quot;</span>...</span>'}}),ss=new _e({props:{$$slots:{default:[vo]},$$scope:{ctx:O}}}),$s=new $({props:{code:'("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),vs=new $({props:{code:'("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)',highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),_s=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),ks=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),ws=new $({props:{code:`Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)`,highlighted:`<span class="hljs-symbol">Vocabulary:</span> [<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;hug&quot;</span>]
<span class="hljs-symbol">Corpus:</span> (<span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-number">10</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;hug&quot;</span> <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-number">5</span>)`}}),es=new _e({props:{$$slots:{default:[_o]},$$scope:{ctx:O}}}),Es=new ot({}),ys=new $({props:{code:`("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"`,highlighted:`<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;ug&quot;</span>
<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;un&quot;</span>
<span class="hljs-function"><span class="hljs-params">(<span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>)</span> -&gt;</span> <span class="hljs-string">&quot;hug&quot;</span>`}}),as=new _e({props:{$$slots:{default:[ko]},$$scope:{ctx:O}}}),Ps=new ot({}),zs=new $({props:{code:`corpus = [
    "This is the Hugging Face course.",  # C'est le cours d'Hugging Face.
    "This chapter is about tokenization.",  # This chapter is about tokenization
    "This section shows several tokenizer algorithms.",  # Cette section pr\xE9sente plusieurs algorithmes de *tokenizer*.
    "Hopefully, you will be able to understand how they are trained and generate tokens.",  # Avec un peu de chance, vous serez en mesure de comprendre comment ils sont entra\xEEn\xE9s et g\xE9n\xE8rent des *tokens*.
]`,highlighted:`corpus = [
    <span class="hljs-string">&quot;This is the Hugging Face course.&quot;</span>,  <span class="hljs-comment"># C&#x27;est le cours d&#x27;Hugging Face.</span>
    <span class="hljs-string">&quot;This chapter is about tokenization.&quot;</span>,  <span class="hljs-comment"># This chapter is about tokenization</span>
    <span class="hljs-string">&quot;This section shows several tokenizer algorithms.&quot;</span>,  <span class="hljs-comment"># Cette section pr\xE9sente plusieurs algorithmes de *tokenizer*.</span>
    <span class="hljs-string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,  <span class="hljs-comment"># Avec un peu de chance, vous serez en mesure de comprendre comment ils sont entra\xEEn\xE9s et g\xE9n\xE8rent des *tokens*.</span>
]`}}),Cs=new $({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)`}}),Ts=new $({props:{code:`from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)`,highlighted:`<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict

word_freqs = defaultdict(<span class="hljs-built_in">int</span>)

<span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> words_with_offsets]
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_words:
        word_freqs[word] += <span class="hljs-number">1</span>

<span class="hljs-built_in">print</span>(word_freqs)`}}),As=new $({props:{code:`defaultdict(int, {'This': 3, '\u0120is': 2, '\u0120the': 1, '\u0120Hugging': 1, '\u0120Face': 1, '\u0120Course': 1, '.': 4, '\u0120chapter': 1,
    '\u0120about': 1, '\u0120tokenization': 1, '\u0120section': 1, '\u0120shows': 1, '\u0120several': 1, '\u0120tokenizer': 1, '\u0120algorithms': 1,
    'Hopefully': 1, ',': 1, '\u0120you': 1, '\u0120will': 1, '\u0120be': 1, '\u0120able': 1, '\u0120to': 1, '\u0120understand': 1, '\u0120how': 1,
    '\u0120they': 1, '\u0120are': 1, '\u0120trained': 1, '\u0120and': 1, '\u0120generate': 1, '\u0120tokens': 1})`,highlighted:`defaultdict(<span class="hljs-built_in">int</span>, {<span class="hljs-string">&#x27;This&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Hugging&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Face&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120Course&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;.&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;\u0120chapter&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;\u0120about&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokenization&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120section&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120shows&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120several&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokenizer&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120algorithms&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;Hopefully&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;,&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120you&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120will&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120be&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120able&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120understand&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120how&#x27;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&#x27;\u0120they&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120are&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120trained&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120and&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120generate&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;\u0120tokens&#x27;</span>: <span class="hljs-number">1</span>})`}}),Ds=new $({props:{code:`alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)`,highlighted:`alphabet = []

<span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys():
    <span class="hljs-keyword">for</span> letter <span class="hljs-keyword">in</span> word:
        <span class="hljs-keyword">if</span> letter <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> alphabet:
            alphabet.append(letter)
alphabet.sort()

<span class="hljs-built_in">print</span>(alphabet)`}}),Ns=new $({props:{code:`[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', '\u0120']`,highlighted:`[ <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;F&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>,
  <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;z&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>]`}}),Os=new $({props:{code:'vocab = ["<|endoftext|>"] + alphabet.copy()',highlighted:'vocab = [<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>] + alphabet.copy()'}}),Bs=new $({props:{code:"splits = {word: [c for c in word] for word in word_freqs.keys()}",highlighted:'splits = {word: [c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys()}'}}),Ms=new $({props:{code:`def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_pair_freqs</span>(<span class="hljs-params">splits</span>):
    pair_freqs = defaultdict(<span class="hljs-built_in">int</span>)
    <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> word_freqs.items():
        split = splits[word]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">continue</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>):
            pair = (split[i], split[i + <span class="hljs-number">1</span>])
            pair_freqs[pair] += freq
    <span class="hljs-keyword">return</span> pair_freqs`}}),Ls=new $({props:{code:`pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break`,highlighted:`pair_freqs = compute_pair_freqs(splits)

<span class="hljs-keyword">for</span> i, key <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pair_freqs.keys()):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span>: <span class="hljs-subst">{pair_freqs[key]}</span>&quot;</span>)
    <span class="hljs-keyword">if</span> i &gt;= <span class="hljs-number">5</span>:
        <span class="hljs-keyword">break</span>`}}),Hs=new $({props:{code:`('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('\u0120', 'i'): 2
('\u0120', 't'): 7
('t', 'h'): 3`,highlighted:`(<span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-number">3</span>
(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-number">3</span>
(<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>): <span class="hljs-number">5</span>
(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-number">2</span>
(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>): <span class="hljs-number">7</span>
(<span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-number">3</span>`}}),Ss=new $({props:{code:`best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)`,highlighted:`best_pair = <span class="hljs-string">&quot;&quot;</span>
max_freq = <span class="hljs-literal">None</span>

<span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freqs.items():
    <span class="hljs-keyword">if</span> max_freq <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> max_freq &lt; freq:
        best_pair = pair
        max_freq = freq

<span class="hljs-built_in">print</span>(best_pair, max_freq)`}}),Gs=new $({props:{code:"('\u0120', 't') 7",highlighted:'(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>) <span class="hljs-number">7</span>'}}),Is=new $({props:{code:`merges = {("\u0120", "t"): "\u0120t"}
vocab.append("\u0120t")`,highlighted:`merges = {(<span class="hljs-string">&quot;\u0120&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>): <span class="hljs-string">&quot;\u0120t&quot;</span>}
vocab.append(<span class="hljs-string">&quot;\u0120t&quot;</span>)`}}),Rs=new $({props:{code:`def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_pair</span>(<span class="hljs-params">a, b, splits</span>):
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs:
        split = splits[word]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">continue</span>

        i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>:
            <span class="hljs-keyword">if</span> split[i] == a <span class="hljs-keyword">and</span> split[i + <span class="hljs-number">1</span>] == b:
                split = split[:i] + [a + b] + split[i + <span class="hljs-number">2</span> :]
            <span class="hljs-keyword">else</span>:
                i += <span class="hljs-number">1</span>
        splits[word] = split
    <span class="hljs-keyword">return</span> splits`}}),Vs=new $({props:{code:`splits = merge_pair("\u0120", "t", splits)
print(splits["\u0120trained"])`,highlighted:`splits = merge_pair(<span class="hljs-string">&quot;\u0120&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, splits)
<span class="hljs-built_in">print</span>(splits[<span class="hljs-string">&quot;\u0120trained&quot;</span>])`}}),Us=new $({props:{code:"['\u0120t', 'r', 'a', 'i', 'n', 'e', 'd']",highlighted:'[<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]'}}),Fs=new $({props:{code:`vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])`,highlighted:`vocab_size = <span class="hljs-number">50</span>

<span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(vocab) &lt; vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = <span class="hljs-string">&quot;&quot;</span>
    max_freq = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freqs.items():
        <span class="hljs-keyword">if</span> max_freq <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> max_freq &lt; freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>]
    vocab.append(best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>])`}}),Ks=new $({props:{code:"print(merges)",highlighted:'<span class="hljs-built_in">print</span>(merges)'}}),Js=new $({props:{code:`{('\u0120', 't'): '\u0120t', ('i', 's'): 'is', ('e', 'r'): 'er', ('\u0120', 'a'): '\u0120a', ('\u0120t', 'o'): '\u0120to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('\u0120to', 'k'): '\u0120tok',
 ('\u0120tok', 'en'): '\u0120token', ('n', 'd'): 'nd', ('\u0120', 'is'): '\u0120is', ('\u0120t', 'h'): '\u0120th', ('\u0120th', 'e'): '\u0120the',
 ('i', 'n'): 'in', ('\u0120a', 'b'): '\u0120ab', ('\u0120token', 'i'): '\u0120tokeni'}`,highlighted:`{(<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>): <span class="hljs-string">&#x27;\u0120t&#x27;</span>, (<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>): <span class="hljs-string">&#x27;is&#x27;</span>, (<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>): <span class="hljs-string">&#x27;er&#x27;</span>, (<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>): <span class="hljs-string">&#x27;\u0120a&#x27;</span>, (<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>): <span class="hljs-string">&#x27;\u0120to&#x27;</span>, (<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>): <span class="hljs-string">&#x27;en&#x27;</span>,
 (<span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-string">&#x27;Th&#x27;</span>, (<span class="hljs-string">&#x27;Th&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>): <span class="hljs-string">&#x27;This&#x27;</span>, (<span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>): <span class="hljs-string">&#x27;ou&#x27;</span>, (<span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>): <span class="hljs-string">&#x27;se&#x27;</span>, (<span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>): <span class="hljs-string">&#x27;\u0120tok&#x27;</span>,
 (<span class="hljs-string">&#x27;\u0120tok&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>): <span class="hljs-string">&#x27;\u0120token&#x27;</span>, (<span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>): <span class="hljs-string">&#x27;nd&#x27;</span>, (<span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>): <span class="hljs-string">&#x27;\u0120is&#x27;</span>, (<span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>): <span class="hljs-string">&#x27;\u0120th&#x27;</span>, (<span class="hljs-string">&#x27;\u0120th&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>): <span class="hljs-string">&#x27;\u0120the&#x27;</span>,
 (<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>): <span class="hljs-string">&#x27;in&#x27;</span>, (<span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>): <span class="hljs-string">&#x27;\u0120ab&#x27;</span>, (<span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>): <span class="hljs-string">&#x27;\u0120tokeni&#x27;</span>}`}}),Ws=new $({props:{code:"print(vocab)",highlighted:'<span class="hljs-built_in">print</span>(vocab)'}}),Ys=new $({props:{code:`['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', '\u0120', '\u0120t', 'is', 'er', '\u0120a', '\u0120to', 'en', 'Th', 'This', 'ou', 'se',
 '\u0120tok', '\u0120token', 'nd', '\u0120is', '\u0120th', '\u0120the', 'in', '\u0120ab', '\u0120tokeni']`,highlighted:`[<span class="hljs-string">&#x27;&lt;|endoftext|&gt;&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;F&#x27;</span>, <span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>,
 <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;z&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;\u0120t&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, <span class="hljs-string">&#x27;Th&#x27;</span>, <span class="hljs-string">&#x27;This&#x27;</span>, <span class="hljs-string">&#x27;ou&#x27;</span>, <span class="hljs-string">&#x27;se&#x27;</span>,
 <span class="hljs-string">&#x27;\u0120tok&#x27;</span>, <span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;nd&#x27;</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>, <span class="hljs-string">&#x27;\u0120th&#x27;</span>, <span class="hljs-string">&#x27;\u0120the&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;\u0120ab&#x27;</span>, <span class="hljs-string">&#x27;\u0120tokeni&#x27;</span>]`}}),os=new _e({props:{$$slots:{default:[wo]},$$scope:{ctx:O}}}),Zs=new $({props:{code:`def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">text</span>):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> pre_tokenize_result]
    splits = [[l <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> pre_tokenized_text]
    <span class="hljs-keyword">for</span> pair, merge <span class="hljs-keyword">in</span> merges.items():
        <span class="hljs-keyword">for</span> idx, split <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(splits):
            i = <span class="hljs-number">0</span>
            <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>:
                <span class="hljs-keyword">if</span> split[i] == pair[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> split[i + <span class="hljs-number">1</span>] == pair[<span class="hljs-number">1</span>]:
                    split = split[:i] + [merge] + split[i + <span class="hljs-number">2</span> :]
                <span class="hljs-keyword">else</span>:
                    i += <span class="hljs-number">1</span>
            splits[idx] = split

    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(splits, [])`}}),Qs=new $({props:{code:'tokenize("This is not a token.")',highlighted:'tokenize(<span class="hljs-string">&quot;This is not a token.&quot;</span>)'}}),Xs=new $({props:{code:"['This', '\u0120is', '\u0120', 'n', 'o', 't', '\u0120a', '\u0120token', '.']",highlighted:'[<span class="hljs-string">&#x27;This&#x27;</span>, <span class="hljs-string">&#x27;\u0120is&#x27;</span>, <span class="hljs-string">&#x27;\u0120&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;\u0120a&#x27;</span>, <span class="hljs-string">&#x27;\u0120token&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),is=new _e({props:{warning:!0,$$slots:{default:[Eo]},$$scope:{ctx:O}}}),{c(){f=r("meta"),_=u(),h=r("h1"),b=r("a"),E=r("span"),m(v.$$.fragment),k=u(),z=r("span"),N=t("Tok\xE9nisation *Byte-Pair Encoding*"),w=u(),m(M.$$.fragment),U=u(),gs=r("p"),ke=r("em"),it=t("Byte-Pair Encoding"),ut=t(" (BPE) a \xE9t\xE9 initialement d\xE9velopp\xE9 en tant qu\u2019algorithme de compression de textes, puis utilis\xE9 par OpenAI pour la tokenisation lors du pr\xE9-entra\xEEnement du mod\xE8le GPT. Il est utilis\xE9 par de nombreux mod\xE8les Transformer, dont GPT, GPT-2, RoBERTa, BART et DeBERTa."),Dn=u(),m(xs.$$.fragment),Nn=u(),m(Q.$$.fragment),On=u(),W=r("h2"),X=r("a"),we=r("span"),m(qs.$$.fragment),ct=u(),Ee=r("span"),ft=t("Algorithme d'entra\xEEnement"),Bn=u(),ee=r("p"),ht=t("L\u2019entra\xEEnement du BPE commence par le calcul de l\u2019ensemble unique de mots utilis\xE9s dans le corpus (apr\xE8s les \xE9tapes de normalisation et de pr\xE9-tok\xE9nisation), puis la construction du vocabulaire en prenant tous les symboles utilis\xE9s pour \xE9crire ces mots. A titre d\u2019exemple tr\xE8s simple, disons que notre corpus utilise ces cinq mots :"),Mn=u(),m(bs.$$.fragment),Ln=u(),F=r("p"),mt=t("Le vocabulaire de base sera alors "),ye=r("code"),dt=t('["b", "g", "h", "n", "p", "s", "u"]'),jt=t(". Dans le monde r\xE9el, ce vocabulaire de base contiendra au moins tous les caract\xE8res ASCII, et probablement aussi quelques caract\xE8res Unicode. Si un exemple que vous tokenisez utilise un caract\xE8re qui n\u2019est pas dans le corpus d\u2019entra\xEEnement, ce caract\xE8re sera converti en "),Pe=r("em"),gt=t("token"),xt=t(" inconnu. C\u2019est l\u2019une des raisons pour lesquelles de nombreux mod\xE8les de NLP sont tr\xE8s mauvais dans l\u2019analyse de contenus contenant des emojis, par exemple."),Hn=u(),m(ss.$$.fragment),Sn=u(),L=r("p"),qt=t("Apr\xE8s avoir obtenu ce vocabulaire de base, nous ajoutons de nouveaux "),ze=r("em"),bt=t("tokens"),$t=t(" jusqu\u2019\xE0 ce que la taille souhait\xE9e du vocabulaire soit atteinte en apprenant les "),Ce=r("em"),vt=t("merges"),_t=t(", qui sont des r\xE8gles permettant de fusionner deux \xE9l\xE9ments du vocabulaire existant pour en cr\xE9er un nouveau. Ainsi, au d\xE9but, ces fusions cr\xE9eront des "),Te=r("em"),kt=t("tokens"),wt=t(" de deux caract\xE8res, puis, au fur et \xE0 mesure de l\u2019entra\xEEnement, des sous-mots plus longs."),Gn=u(),H=r("p"),Et=t("\xC0 chaque \xE9tape de l\u2019entra\xEEnement du "),Ae=r("em"),yt=t("tokenizer"),Pt=t(", l\u2019algorithme BPE recherche la paire la plus fr\xE9quente de "),De=r("em"),zt=t("tokens"),Ct=t(" existants (par \u201Cpaire\u201D, nous entendons ici deux "),Ne=r("em"),Tt=t("tokens"),At=t(" cons\xE9cutifs dans un mot). Cette paire la plus fr\xE9quente est celle qui sera fusionn\xE9e, et nous rin\xE7ons et r\xE9p\xE9tons pour l\u2019\xE9tape suivante."),In=u(),ne=r("p"),Dt=t("Pour revenir \xE0 notre exemple pr\xE9c\xE9dent, supposons que les mots ont les fr\xE9quences suivantes :"),Rn=u(),m($s.$$.fragment),Vn=u(),C=r("p"),Nt=t("ce qui veut dire que "),Oe=r("code"),Ot=t('"hug"'),Bt=t(" \xE9tait pr\xE9sent 10 fois dans le corpus, "),Be=r("code"),Mt=t('"pug"'),Lt=t(" 5 fois, "),Me=r("code"),Ht=t('"pun"'),St=t(" 12 fois, "),Le=r("code"),Gt=t('"bun"'),It=t(" 4 fois, et "),He=r("code"),Rt=t('"hugs"'),Vt=t("\u201D 5 fois. Nous commen\xE7ons l\u2019entra\xEEnement en divisant chaque mot en caract\xE8res (ceux qui forment notre vocabulaire initial) afin de voir chaque mot comme une liste de "),Se=r("em"),Ut=t("tokens"),Ft=t(" :"),Un=u(),m(vs.$$.fragment),Fn=u(),y=r("p"),Kt=t("Ensuite, nous regardons les paires. La paire "),Ge=r("code"),Jt=t('("h", "u")'),Wt=t(" est pr\xE9sente dans les mots "),Ie=r("code"),Yt=t('"hug"'),Zt=t(" et "),Re=r("code"),Qt=t('"hugs"'),Xt=t(", donc 15 fois au total dans le corpus. Ce n\u2019est pas la paire la plus fr\xE9quente, cependant : cet honneur revient \xE0 "),Ve=r("code"),sl=t('("u", "g")'),el=t(", qui est pr\xE9sent dans "),Ue=r("code"),nl=t('"hug"'),al=t(", "),Fe=r("code"),tl=t('"pug"'),ll=t(", et "),Ke=r("code"),pl=t('"hugs"'),rl=t(", pour un grand total de 20 fois dans le vocabulaire."),Kn=u(),S=r("p"),ol=t("Ainsi, la premi\xE8re r\xE8gle de fusion apprise par le "),Je=r("em"),il=t("tokenizer"),ul=t(" est "),We=r("code"),cl=t('("u", "g") -> "ug"'),fl=t(", ce qui signifie que "),Ye=r("code"),hl=t('"ug"'),ml=t(" sera ajout\xE9 au vocabulaire, et que la paire devra \xEAtre fusionn\xE9e dans tous les mots du corpus. A la fin de cette \xE9tape, le vocabulaire et le corpus ressemblent \xE0 ceci :"),Jn=u(),m(_s.$$.fragment),Wn=u(),G=r("p"),dl=t("Nous avons maintenant quelques paires qui aboutissent \xE0 un token de plus de deux caract\xE8res : la paire "),Ze=r("code"),jl=t('("h", "ug")'),gl=t(", par exemple (pr\xE9sente 15 fois dans le corpus). La paire la plus fr\xE9quente \xE0 ce stade est "),Qe=r("code"),xl=t('("u", "n")'),ql=t(", cependant, pr\xE9sente 16 fois dans le corpus, donc la deuxi\xE8me r\xE8gle de fusion apprise est "),Xe=r("code"),bl=t('("u", "n") -> "un"'),$l=t(". Ajouter cela au vocabulaire et fusionner toutes les occurrences existantes nous conduit \xE0 :"),Yn=u(),m(ks.$$.fragment),Zn=u(),I=r("p"),vl=t("Maintenant la paire la plus fr\xE9quente est "),sn=r("code"),_l=t('("h", "ug")'),kl=t(", donc nous apprenons la r\xE8gle de fusion "),en=r("code"),wl=t('("h", "ug") -> "hug"'),El=t(", ce qui nous donne notre premier "),nn=r("em"),yl=t("token"),Pl=t(" de trois lettres. Apr\xE8s la fusion, le corpus ressemble \xE0 ceci :"),Qn=u(),m(ws.$$.fragment),Xn=u(),ae=r("p"),zl=t("Et nous continuons ainsi jusqu\u2019\xE0 ce que nous atteignions la taille de vocabulaire souhait\xE9e."),sa=u(),m(es.$$.fragment),ea=u(),Y=r("h2"),ns=r("a"),an=r("span"),m(Es.$$.fragment),Cl=u(),tn=r("span"),Tl=t("Algorithme de tokenisation"),na=u(),te=r("p"),Al=t("La tokenisation suit de pr\xE8s le processus d\u2019entra\xEEnement, dans le sens o\xF9 les nouvelles entr\xE9es sont tokenis\xE9es en appliquant les \xE9tapes suivantes :"),aa=u(),R=r("ol"),ln=r("li"),Dl=t("Normalisation"),Nl=u(),pn=r("li"),Ol=t("Pr\xE9-tok\xE9nisation."),Bl=u(),rn=r("li"),Ml=t("D\xE9coupage des mots en caract\xE8res individuels"),Ll=u(),on=r("li"),Hl=t("Application des r\xE8gles de fusion apprises dans l\u2019ordre sur ces divisions."),ta=u(),le=r("p"),Sl=t("Prenons l\u2019exemple que nous avons utilis\xE9 pendant l\u2019entra\xEEnement, avec les trois r\xE8gles de fusion apprises :"),la=u(),m(ys.$$.fragment),pa=u(),P=r("p"),Gl=t("Le mot \u201D bug \u201D sera traduit par \u201D [\u201Cb\u201D, \u201Cug\u201D]"),un=r("code"),Il=t('". Par contre, le mot " mug " sera traduit par " ["[UNK]", "ug"]'),Rl=t(" \u201D puisque la lettre \u201D m \u201D ne fait pas partie du vocabulaire de base. De la m\xEAme fa\xE7on, le mot \u201D thug \u201D"),cn=r("code"),Vl=t('sera tokenis\xE9 comme "["[UNK]", "hug"]'),Ul=t(" : la lettre "),fn=r("code"),Fl=t('"t"'),Kl=t(" n\u2019est pas dans le vocabulaire de base, et l\u2019application des r\xE8gles de fusion r\xE9sulte d\u2019abord en la fusion de "),hn=r("code"),Jl=t('"u"'),Wl=t(" et "),mn=r("code"),Yl=t('"g"'),Zl=t(" et ensuite en la fusion de "),dn=r("code"),Ql=t('"hu"'),Xl=t(" et "),jn=r("code"),sp=t('"g"'),ep=t("."),ra=u(),m(as.$$.fragment),oa=u(),Z=r("h2"),ts=r("a"),gn=r("span"),m(Ps.$$.fragment),np=u(),xn=r("span"),ap=t("Mise en \u0153uvre du BPE"),ia=u(),pe=r("p"),tp=t("Voyons maintenant une impl\xE9mentation de l\u2019algorithme BPE. Il ne s\u2019agira pas d\u2019une version optimis\xE9e que vous pourrez utiliser sur un grand corpus ; nous voulons simplement vous montrer le code afin que vous puissiez comprendre un peu mieux l\u2019algorithme."),ua=u(),re=r("p"),lp=t("Tout d\u2019abord, nous avons besoin d\u2019un corpus, alors cr\xE9ons un corpus simple avec quelques phrases :"),ca=u(),m(zs.$$.fragment),fa=u(),V=r("p"),pp=t("Ensuite, nous devons pr\xE9-tokeniser ce corpus en mots. Puisque nous r\xE9pliquons un "),qn=r("em"),rp=t("tokenizer"),op=t(" BPE (comme GPT-2), nous utiliserons le "),bn=r("em"),ip=t("tokenizer"),up=u(),$n=r("code"),cp=t("gpt2"),fp=t(" pour la pr\xE9-tok\xE9nisation :"),ha=u(),m(Cs.$$.fragment),ma=u(),oe=r("p"),hp=t("Ensuite, nous calculons les fr\xE9quences de chaque mot dans le corpus comme nous le faisons pour la pr\xE9-tok\xE9nisation :"),da=u(),m(Ts.$$.fragment),ja=u(),m(As.$$.fragment),ga=u(),ie=r("p"),mp=t("L\u2019\xE9tape suivante consiste \xE0 calculer le vocabulaire de base, form\xE9 par tous les caract\xE8res utilis\xE9s dans le corpus :"),xa=u(),m(Ds.$$.fragment),qa=u(),m(Ns.$$.fragment),ba=u(),K=r("p"),dp=t("Nous ajoutons \xE9galement les "),vn=r("em"),jp=t("tokens"),gp=t(" sp\xE9ciaux utilis\xE9s par le mod\xE8le au d\xE9but de ce vocabulaire. Dans le cas de GPT-2, le seul token sp\xE9cial est "),_n=r("code"),xp=t('"<|endoftext|>"'),qp=t(" :"),$a=u(),m(Os.$$.fragment),va=u(),ue=r("p"),bp=t("Nous devons maintenant diviser chaque mot en caract\xE8res individuels, pour pouvoir commencer l\u2019entra\xEEnement :"),_a=u(),m(Bs.$$.fragment),ka=u(),ce=r("p"),$p=t("Maintenant que nous sommes pr\xEAts pour l\u2019entra\xEEnement, \xE9crivons une fonction qui calcule la fr\xE9quence de chaque paire. Nous devrons l\u2019utiliser \xE0 chaque \xE9tape de l\u2019entra\xEEnement :"),wa=u(),m(Ms.$$.fragment),Ea=u(),fe=r("p"),vp=t("Jetons un coup d\u2019\u0153il \xE0 une partie de ce dictionnaire apr\xE8s les premi\xE8res divisions :"),ya=u(),m(Ls.$$.fragment),Pa=u(),m(Hs.$$.fragment),za=u(),he=r("p"),_p=t("Maintenant, trouver la paire la plus fr\xE9quente ne demande qu\u2019une rapide boucle :"),Ca=u(),m(Ss.$$.fragment),Ta=u(),m(Gs.$$.fragment),Aa=u(),J=r("p"),kp=t("Donc la premi\xE8re fusion \xE0 apprendre est "),kn=r("code"),wp=t("('\u0120', 't') -> '\u0120t'"),Ep=t(", et on ajoute "),wn=r("code"),yp=t("'\u0120t'"),Pp=t(" au vocabulaire :"),Da=u(),m(Is.$$.fragment),Na=u(),ls=r("p"),zp=t("Pour continuer, nous devons appliquer cette fusion dans notre dictionnaire "),En=r("code"),Cp=t("splits"),Tp=t(". \xC9crivons une autre fonction pour cela :"),Oa=u(),m(Rs.$$.fragment),Ba=u(),me=r("p"),Ap=t("Et nous pouvons regarder le r\xE9sultat de la premi\xE8re fusion :"),Ma=u(),m(Vs.$$.fragment),La=u(),m(Us.$$.fragment),Ha=u(),de=r("p"),Dp=t("Maintenant, nous avons tout ce dont nous avons besoin pour boucler jusqu\u2019\xE0 ce que nous ayons appris toutes les fusions que nous voulons. Visons une taille de vocabulaire de 50 :"),Sa=u(),m(Fs.$$.fragment),Ga=u(),ps=r("p"),Np=t("En cons\xE9quence, nous avons appris 19 r\xE8gles de fusion (le vocabulaire initial avait une taille de 31 : 30 caract\xE8res dans l\u2019alphabet, plus le "),yn=r("em"),Op=t("token"),Bp=t(" sp\xE9cial) :"),Ia=u(),m(Ks.$$.fragment),Ra=u(),m(Js.$$.fragment),Va=u(),rs=r("p"),Mp=t("Et le vocabulaire est compos\xE9 du "),Pn=r("em"),Lp=t("token"),Hp=t(" sp\xE9cial, de l\u2019alphabet initial, et de tous les r\xE9sultats des fusions :"),Ua=u(),m(Ws.$$.fragment),Fa=u(),m(Ys.$$.fragment),Ka=u(),m(os.$$.fragment),Ja=u(),je=r("p"),Sp=t("Pour tokeniser un nouveau texte, on le pr\xE9-tokenise, on le divise, puis on applique toutes les r\xE8gles de fusion apprises :"),Wa=u(),m(Zs.$$.fragment),Ya=u(),ge=r("p"),Gp=t("Nous pouvons essayer cela sur n\u2019importe quel texte compos\xE9 de caract\xE8res de l\u2019alphabet :"),Za=u(),m(Qs.$$.fragment),Qa=u(),m(Xs.$$.fragment),Xa=u(),m(is.$$.fragment),st=u(),xe=r("p"),Ip=t("C\u2019est tout pour l\u2019algorithme BPE ! Ensuite, nous allons nous int\xE9resser \xE0 WordPiece."),this.h()},l(s){const a=go('[data-svelte="svelte-1phssyn"]',document.head);f=o(a,"META",{name:!0,content:!0}),a.forEach(e),_=c(s),h=o(s,"H1",{class:!0});var se=i(h);b=o(se,"A",{id:!0,class:!0,href:!0});var zn=i(b);E=o(zn,"SPAN",{});var Cn=i(E);d(v.$$.fragment,Cn),Cn.forEach(e),zn.forEach(e),k=c(se),z=o(se,"SPAN",{});var Tn=i(z);N=l(Tn,"Tok\xE9nisation *Byte-Pair Encoding*"),Tn.forEach(e),se.forEach(e),w=c(s),d(M.$$.fragment,s),U=c(s),gs=o(s,"P",{});var qe=i(gs);ke=o(qe,"EM",{});var An=i(ke);it=l(An,"Byte-Pair Encoding"),An.forEach(e),ut=l(qe," (BPE) a \xE9t\xE9 initialement d\xE9velopp\xE9 en tant qu\u2019algorithme de compression de textes, puis utilis\xE9 par OpenAI pour la tokenisation lors du pr\xE9-entra\xEEnement du mod\xE8le GPT. Il est utilis\xE9 par de nombreux mod\xE8les Transformer, dont GPT, GPT-2, RoBERTa, BART et DeBERTa."),qe.forEach(e),Dn=c(s),d(xs.$$.fragment,s),Nn=c(s),d(Q.$$.fragment,s),On=c(s),W=o(s,"H2",{class:!0});var nt=i(W);X=o(nt,"A",{id:!0,class:!0,href:!0});var Rp=i(X);we=o(Rp,"SPAN",{});var Vp=i(we);d(qs.$$.fragment,Vp),Vp.forEach(e),Rp.forEach(e),ct=c(nt),Ee=o(nt,"SPAN",{});var Up=i(Ee);ft=l(Up,"Algorithme d'entra\xEEnement"),Up.forEach(e),nt.forEach(e),Bn=c(s),ee=o(s,"P",{});var Fp=i(ee);ht=l(Fp,"L\u2019entra\xEEnement du BPE commence par le calcul de l\u2019ensemble unique de mots utilis\xE9s dans le corpus (apr\xE8s les \xE9tapes de normalisation et de pr\xE9-tok\xE9nisation), puis la construction du vocabulaire en prenant tous les symboles utilis\xE9s pour \xE9crire ces mots. A titre d\u2019exemple tr\xE8s simple, disons que notre corpus utilise ces cinq mots :"),Fp.forEach(e),Mn=c(s),d(bs.$$.fragment,s),Ln=c(s),F=o(s,"P",{});var be=i(F);mt=l(be,"Le vocabulaire de base sera alors "),ye=o(be,"CODE",{});var Kp=i(ye);dt=l(Kp,'["b", "g", "h", "n", "p", "s", "u"]'),Kp.forEach(e),jt=l(be,". Dans le monde r\xE9el, ce vocabulaire de base contiendra au moins tous les caract\xE8res ASCII, et probablement aussi quelques caract\xE8res Unicode. Si un exemple que vous tokenisez utilise un caract\xE8re qui n\u2019est pas dans le corpus d\u2019entra\xEEnement, ce caract\xE8re sera converti en "),Pe=o(be,"EM",{});var Jp=i(Pe);gt=l(Jp,"token"),Jp.forEach(e),xt=l(be," inconnu. C\u2019est l\u2019une des raisons pour lesquelles de nombreux mod\xE8les de NLP sont tr\xE8s mauvais dans l\u2019analyse de contenus contenant des emojis, par exemple."),be.forEach(e),Hn=c(s),d(ss.$$.fragment,s),Sn=c(s),L=o(s,"P",{});var us=i(L);qt=l(us,"Apr\xE8s avoir obtenu ce vocabulaire de base, nous ajoutons de nouveaux "),ze=o(us,"EM",{});var Wp=i(ze);bt=l(Wp,"tokens"),Wp.forEach(e),$t=l(us," jusqu\u2019\xE0 ce que la taille souhait\xE9e du vocabulaire soit atteinte en apprenant les "),Ce=o(us,"EM",{});var Yp=i(Ce);vt=l(Yp,"merges"),Yp.forEach(e),_t=l(us,", qui sont des r\xE8gles permettant de fusionner deux \xE9l\xE9ments du vocabulaire existant pour en cr\xE9er un nouveau. Ainsi, au d\xE9but, ces fusions cr\xE9eront des "),Te=o(us,"EM",{});var Zp=i(Te);kt=l(Zp,"tokens"),Zp.forEach(e),wt=l(us," de deux caract\xE8res, puis, au fur et \xE0 mesure de l\u2019entra\xEEnement, des sous-mots plus longs."),us.forEach(e),Gn=c(s),H=o(s,"P",{});var cs=i(H);Et=l(cs,"\xC0 chaque \xE9tape de l\u2019entra\xEEnement du "),Ae=o(cs,"EM",{});var Qp=i(Ae);yt=l(Qp,"tokenizer"),Qp.forEach(e),Pt=l(cs,", l\u2019algorithme BPE recherche la paire la plus fr\xE9quente de "),De=o(cs,"EM",{});var Xp=i(De);zt=l(Xp,"tokens"),Xp.forEach(e),Ct=l(cs," existants (par \u201Cpaire\u201D, nous entendons ici deux "),Ne=o(cs,"EM",{});var sr=i(Ne);Tt=l(sr,"tokens"),sr.forEach(e),At=l(cs," cons\xE9cutifs dans un mot). Cette paire la plus fr\xE9quente est celle qui sera fusionn\xE9e, et nous rin\xE7ons et r\xE9p\xE9tons pour l\u2019\xE9tape suivante."),cs.forEach(e),In=c(s),ne=o(s,"P",{});var er=i(ne);Dt=l(er,"Pour revenir \xE0 notre exemple pr\xE9c\xE9dent, supposons que les mots ont les fr\xE9quences suivantes :"),er.forEach(e),Rn=c(s),d($s.$$.fragment,s),Vn=c(s),C=o(s,"P",{});var B=i(C);Nt=l(B,"ce qui veut dire que "),Oe=o(B,"CODE",{});var nr=i(Oe);Ot=l(nr,'"hug"'),nr.forEach(e),Bt=l(B," \xE9tait pr\xE9sent 10 fois dans le corpus, "),Be=o(B,"CODE",{});var ar=i(Be);Mt=l(ar,'"pug"'),ar.forEach(e),Lt=l(B," 5 fois, "),Me=o(B,"CODE",{});var tr=i(Me);Ht=l(tr,'"pun"'),tr.forEach(e),St=l(B," 12 fois, "),Le=o(B,"CODE",{});var lr=i(Le);Gt=l(lr,'"bun"'),lr.forEach(e),It=l(B," 4 fois, et "),He=o(B,"CODE",{});var pr=i(He);Rt=l(pr,'"hugs"'),pr.forEach(e),Vt=l(B,"\u201D 5 fois. Nous commen\xE7ons l\u2019entra\xEEnement en divisant chaque mot en caract\xE8res (ceux qui forment notre vocabulaire initial) afin de voir chaque mot comme une liste de "),Se=o(B,"EM",{});var rr=i(Se);Ut=l(rr,"tokens"),rr.forEach(e),Ft=l(B," :"),B.forEach(e),Un=c(s),d(vs.$$.fragment,s),Fn=c(s),y=o(s,"P",{});var A=i(y);Kt=l(A,"Ensuite, nous regardons les paires. La paire "),Ge=o(A,"CODE",{});var or=i(Ge);Jt=l(or,'("h", "u")'),or.forEach(e),Wt=l(A," est pr\xE9sente dans les mots "),Ie=o(A,"CODE",{});var ir=i(Ie);Yt=l(ir,'"hug"'),ir.forEach(e),Zt=l(A," et "),Re=o(A,"CODE",{});var ur=i(Re);Qt=l(ur,'"hugs"'),ur.forEach(e),Xt=l(A,", donc 15 fois au total dans le corpus. Ce n\u2019est pas la paire la plus fr\xE9quente, cependant : cet honneur revient \xE0 "),Ve=o(A,"CODE",{});var cr=i(Ve);sl=l(cr,'("u", "g")'),cr.forEach(e),el=l(A,", qui est pr\xE9sent dans "),Ue=o(A,"CODE",{});var fr=i(Ue);nl=l(fr,'"hug"'),fr.forEach(e),al=l(A,", "),Fe=o(A,"CODE",{});var hr=i(Fe);tl=l(hr,'"pug"'),hr.forEach(e),ll=l(A,", et "),Ke=o(A,"CODE",{});var mr=i(Ke);pl=l(mr,'"hugs"'),mr.forEach(e),rl=l(A,", pour un grand total de 20 fois dans le vocabulaire."),A.forEach(e),Kn=c(s),S=o(s,"P",{});var fs=i(S);ol=l(fs,"Ainsi, la premi\xE8re r\xE8gle de fusion apprise par le "),Je=o(fs,"EM",{});var dr=i(Je);il=l(dr,"tokenizer"),dr.forEach(e),ul=l(fs," est "),We=o(fs,"CODE",{});var jr=i(We);cl=l(jr,'("u", "g") -> "ug"'),jr.forEach(e),fl=l(fs,", ce qui signifie que "),Ye=o(fs,"CODE",{});var gr=i(Ye);hl=l(gr,'"ug"'),gr.forEach(e),ml=l(fs," sera ajout\xE9 au vocabulaire, et que la paire devra \xEAtre fusionn\xE9e dans tous les mots du corpus. A la fin de cette \xE9tape, le vocabulaire et le corpus ressemblent \xE0 ceci :"),fs.forEach(e),Jn=c(s),d(_s.$$.fragment,s),Wn=c(s),G=o(s,"P",{});var hs=i(G);dl=l(hs,"Nous avons maintenant quelques paires qui aboutissent \xE0 un token de plus de deux caract\xE8res : la paire "),Ze=o(hs,"CODE",{});var xr=i(Ze);jl=l(xr,'("h", "ug")'),xr.forEach(e),gl=l(hs,", par exemple (pr\xE9sente 15 fois dans le corpus). La paire la plus fr\xE9quente \xE0 ce stade est "),Qe=o(hs,"CODE",{});var qr=i(Qe);xl=l(qr,'("u", "n")'),qr.forEach(e),ql=l(hs,", cependant, pr\xE9sente 16 fois dans le corpus, donc la deuxi\xE8me r\xE8gle de fusion apprise est "),Xe=o(hs,"CODE",{});var br=i(Xe);bl=l(br,'("u", "n") -> "un"'),br.forEach(e),$l=l(hs,". Ajouter cela au vocabulaire et fusionner toutes les occurrences existantes nous conduit \xE0 :"),hs.forEach(e),Yn=c(s),d(ks.$$.fragment,s),Zn=c(s),I=o(s,"P",{});var ms=i(I);vl=l(ms,"Maintenant la paire la plus fr\xE9quente est "),sn=o(ms,"CODE",{});var $r=i(sn);_l=l($r,'("h", "ug")'),$r.forEach(e),kl=l(ms,", donc nous apprenons la r\xE8gle de fusion "),en=o(ms,"CODE",{});var vr=i(en);wl=l(vr,'("h", "ug") -> "hug"'),vr.forEach(e),El=l(ms,", ce qui nous donne notre premier "),nn=o(ms,"EM",{});var _r=i(nn);yl=l(_r,"token"),_r.forEach(e),Pl=l(ms," de trois lettres. Apr\xE8s la fusion, le corpus ressemble \xE0 ceci :"),ms.forEach(e),Qn=c(s),d(ws.$$.fragment,s),Xn=c(s),ae=o(s,"P",{});var kr=i(ae);zl=l(kr,"Et nous continuons ainsi jusqu\u2019\xE0 ce que nous atteignions la taille de vocabulaire souhait\xE9e."),kr.forEach(e),sa=c(s),d(es.$$.fragment,s),ea=c(s),Y=o(s,"H2",{class:!0});var at=i(Y);ns=o(at,"A",{id:!0,class:!0,href:!0});var wr=i(ns);an=o(wr,"SPAN",{});var Er=i(an);d(Es.$$.fragment,Er),Er.forEach(e),wr.forEach(e),Cl=c(at),tn=o(at,"SPAN",{});var yr=i(tn);Tl=l(yr,"Algorithme de tokenisation"),yr.forEach(e),at.forEach(e),na=c(s),te=o(s,"P",{});var Pr=i(te);Al=l(Pr,"La tokenisation suit de pr\xE8s le processus d\u2019entra\xEEnement, dans le sens o\xF9 les nouvelles entr\xE9es sont tokenis\xE9es en appliquant les \xE9tapes suivantes :"),Pr.forEach(e),aa=c(s),R=o(s,"OL",{});var ds=i(R);ln=o(ds,"LI",{});var zr=i(ln);Dl=l(zr,"Normalisation"),zr.forEach(e),Nl=c(ds),pn=o(ds,"LI",{});var Cr=i(pn);Ol=l(Cr,"Pr\xE9-tok\xE9nisation."),Cr.forEach(e),Bl=c(ds),rn=o(ds,"LI",{});var Tr=i(rn);Ml=l(Tr,"D\xE9coupage des mots en caract\xE8res individuels"),Tr.forEach(e),Ll=c(ds),on=o(ds,"LI",{});var Ar=i(on);Hl=l(Ar,"Application des r\xE8gles de fusion apprises dans l\u2019ordre sur ces divisions."),Ar.forEach(e),ds.forEach(e),ta=c(s),le=o(s,"P",{});var Dr=i(le);Sl=l(Dr,"Prenons l\u2019exemple que nous avons utilis\xE9 pendant l\u2019entra\xEEnement, avec les trois r\xE8gles de fusion apprises :"),Dr.forEach(e),la=c(s),d(ys.$$.fragment,s),pa=c(s),P=o(s,"P",{});var D=i(P);Gl=l(D,"Le mot \u201D bug \u201D sera traduit par \u201D [\u201Cb\u201D, \u201Cug\u201D]"),un=o(D,"CODE",{});var Nr=i(un);Il=l(Nr,'". Par contre, le mot " mug " sera traduit par " ["[UNK]", "ug"]'),Nr.forEach(e),Rl=l(D," \u201D puisque la lettre \u201D m \u201D ne fait pas partie du vocabulaire de base. De la m\xEAme fa\xE7on, le mot \u201D thug \u201D"),cn=o(D,"CODE",{});var Or=i(cn);Vl=l(Or,'sera tokenis\xE9 comme "["[UNK]", "hug"]'),Or.forEach(e),Ul=l(D," : la lettre "),fn=o(D,"CODE",{});var Br=i(fn);Fl=l(Br,'"t"'),Br.forEach(e),Kl=l(D," n\u2019est pas dans le vocabulaire de base, et l\u2019application des r\xE8gles de fusion r\xE9sulte d\u2019abord en la fusion de "),hn=o(D,"CODE",{});var Mr=i(hn);Jl=l(Mr,'"u"'),Mr.forEach(e),Wl=l(D," et "),mn=o(D,"CODE",{});var Lr=i(mn);Yl=l(Lr,'"g"'),Lr.forEach(e),Zl=l(D," et ensuite en la fusion de "),dn=o(D,"CODE",{});var Hr=i(dn);Ql=l(Hr,'"hu"'),Hr.forEach(e),Xl=l(D," et "),jn=o(D,"CODE",{});var Sr=i(jn);sp=l(Sr,'"g"'),Sr.forEach(e),ep=l(D,"."),D.forEach(e),ra=c(s),d(as.$$.fragment,s),oa=c(s),Z=o(s,"H2",{class:!0});var tt=i(Z);ts=o(tt,"A",{id:!0,class:!0,href:!0});var Gr=i(ts);gn=o(Gr,"SPAN",{});var Ir=i(gn);d(Ps.$$.fragment,Ir),Ir.forEach(e),Gr.forEach(e),np=c(tt),xn=o(tt,"SPAN",{});var Rr=i(xn);ap=l(Rr,"Mise en \u0153uvre du BPE"),Rr.forEach(e),tt.forEach(e),ia=c(s),pe=o(s,"P",{});var Vr=i(pe);tp=l(Vr,"Voyons maintenant une impl\xE9mentation de l\u2019algorithme BPE. Il ne s\u2019agira pas d\u2019une version optimis\xE9e que vous pourrez utiliser sur un grand corpus ; nous voulons simplement vous montrer le code afin que vous puissiez comprendre un peu mieux l\u2019algorithme."),Vr.forEach(e),ua=c(s),re=o(s,"P",{});var Ur=i(re);lp=l(Ur,"Tout d\u2019abord, nous avons besoin d\u2019un corpus, alors cr\xE9ons un corpus simple avec quelques phrases :"),Ur.forEach(e),ca=c(s),d(zs.$$.fragment,s),fa=c(s),V=o(s,"P",{});var js=i(V);pp=l(js,"Ensuite, nous devons pr\xE9-tokeniser ce corpus en mots. Puisque nous r\xE9pliquons un "),qn=o(js,"EM",{});var Fr=i(qn);rp=l(Fr,"tokenizer"),Fr.forEach(e),op=l(js," BPE (comme GPT-2), nous utiliserons le "),bn=o(js,"EM",{});var Kr=i(bn);ip=l(Kr,"tokenizer"),Kr.forEach(e),up=c(js),$n=o(js,"CODE",{});var Jr=i($n);cp=l(Jr,"gpt2"),Jr.forEach(e),fp=l(js," pour la pr\xE9-tok\xE9nisation :"),js.forEach(e),ha=c(s),d(Cs.$$.fragment,s),ma=c(s),oe=o(s,"P",{});var Wr=i(oe);hp=l(Wr,"Ensuite, nous calculons les fr\xE9quences de chaque mot dans le corpus comme nous le faisons pour la pr\xE9-tok\xE9nisation :"),Wr.forEach(e),da=c(s),d(Ts.$$.fragment,s),ja=c(s),d(As.$$.fragment,s),ga=c(s),ie=o(s,"P",{});var Yr=i(ie);mp=l(Yr,"L\u2019\xE9tape suivante consiste \xE0 calculer le vocabulaire de base, form\xE9 par tous les caract\xE8res utilis\xE9s dans le corpus :"),Yr.forEach(e),xa=c(s),d(Ds.$$.fragment,s),qa=c(s),d(Ns.$$.fragment,s),ba=c(s),K=o(s,"P",{});var $e=i(K);dp=l($e,"Nous ajoutons \xE9galement les "),vn=o($e,"EM",{});var Zr=i(vn);jp=l(Zr,"tokens"),Zr.forEach(e),gp=l($e," sp\xE9ciaux utilis\xE9s par le mod\xE8le au d\xE9but de ce vocabulaire. Dans le cas de GPT-2, le seul token sp\xE9cial est "),_n=o($e,"CODE",{});var Qr=i(_n);xp=l(Qr,'"<|endoftext|>"'),Qr.forEach(e),qp=l($e," :"),$e.forEach(e),$a=c(s),d(Os.$$.fragment,s),va=c(s),ue=o(s,"P",{});var Xr=i(ue);bp=l(Xr,"Nous devons maintenant diviser chaque mot en caract\xE8res individuels, pour pouvoir commencer l\u2019entra\xEEnement :"),Xr.forEach(e),_a=c(s),d(Bs.$$.fragment,s),ka=c(s),ce=o(s,"P",{});var so=i(ce);$p=l(so,"Maintenant que nous sommes pr\xEAts pour l\u2019entra\xEEnement, \xE9crivons une fonction qui calcule la fr\xE9quence de chaque paire. Nous devrons l\u2019utiliser \xE0 chaque \xE9tape de l\u2019entra\xEEnement :"),so.forEach(e),wa=c(s),d(Ms.$$.fragment,s),Ea=c(s),fe=o(s,"P",{});var eo=i(fe);vp=l(eo,"Jetons un coup d\u2019\u0153il \xE0 une partie de ce dictionnaire apr\xE8s les premi\xE8res divisions :"),eo.forEach(e),ya=c(s),d(Ls.$$.fragment,s),Pa=c(s),d(Hs.$$.fragment,s),za=c(s),he=o(s,"P",{});var no=i(he);_p=l(no,"Maintenant, trouver la paire la plus fr\xE9quente ne demande qu\u2019une rapide boucle :"),no.forEach(e),Ca=c(s),d(Ss.$$.fragment,s),Ta=c(s),d(Gs.$$.fragment,s),Aa=c(s),J=o(s,"P",{});var ve=i(J);kp=l(ve,"Donc la premi\xE8re fusion \xE0 apprendre est "),kn=o(ve,"CODE",{});var ao=i(kn);wp=l(ao,"('\u0120', 't') -> '\u0120t'"),ao.forEach(e),Ep=l(ve,", et on ajoute "),wn=o(ve,"CODE",{});var to=i(wn);yp=l(to,"'\u0120t'"),to.forEach(e),Pp=l(ve," au vocabulaire :"),ve.forEach(e),Da=c(s),d(Is.$$.fragment,s),Na=c(s),ls=o(s,"P",{});var lt=i(ls);zp=l(lt,"Pour continuer, nous devons appliquer cette fusion dans notre dictionnaire "),En=o(lt,"CODE",{});var lo=i(En);Cp=l(lo,"splits"),lo.forEach(e),Tp=l(lt,". \xC9crivons une autre fonction pour cela :"),lt.forEach(e),Oa=c(s),d(Rs.$$.fragment,s),Ba=c(s),me=o(s,"P",{});var po=i(me);Ap=l(po,"Et nous pouvons regarder le r\xE9sultat de la premi\xE8re fusion :"),po.forEach(e),Ma=c(s),d(Vs.$$.fragment,s),La=c(s),d(Us.$$.fragment,s),Ha=c(s),de=o(s,"P",{});var ro=i(de);Dp=l(ro,"Maintenant, nous avons tout ce dont nous avons besoin pour boucler jusqu\u2019\xE0 ce que nous ayons appris toutes les fusions que nous voulons. Visons une taille de vocabulaire de 50 :"),ro.forEach(e),Sa=c(s),d(Fs.$$.fragment,s),Ga=c(s),ps=o(s,"P",{});var pt=i(ps);Np=l(pt,"En cons\xE9quence, nous avons appris 19 r\xE8gles de fusion (le vocabulaire initial avait une taille de 31 : 30 caract\xE8res dans l\u2019alphabet, plus le "),yn=o(pt,"EM",{});var oo=i(yn);Op=l(oo,"token"),oo.forEach(e),Bp=l(pt," sp\xE9cial) :"),pt.forEach(e),Ia=c(s),d(Ks.$$.fragment,s),Ra=c(s),d(Js.$$.fragment,s),Va=c(s),rs=o(s,"P",{});var rt=i(rs);Mp=l(rt,"Et le vocabulaire est compos\xE9 du "),Pn=o(rt,"EM",{});var io=i(Pn);Lp=l(io,"token"),io.forEach(e),Hp=l(rt," sp\xE9cial, de l\u2019alphabet initial, et de tous les r\xE9sultats des fusions :"),rt.forEach(e),Ua=c(s),d(Ws.$$.fragment,s),Fa=c(s),d(Ys.$$.fragment,s),Ka=c(s),d(os.$$.fragment,s),Ja=c(s),je=o(s,"P",{});var uo=i(je);Sp=l(uo,"Pour tokeniser un nouveau texte, on le pr\xE9-tokenise, on le divise, puis on applique toutes les r\xE8gles de fusion apprises :"),uo.forEach(e),Wa=c(s),d(Zs.$$.fragment,s),Ya=c(s),ge=o(s,"P",{});var co=i(ge);Gp=l(co,"Nous pouvons essayer cela sur n\u2019importe quel texte compos\xE9 de caract\xE8res de l\u2019alphabet :"),co.forEach(e),Za=c(s),d(Qs.$$.fragment,s),Qa=c(s),d(Xs.$$.fragment,s),Xa=c(s),d(is.$$.fragment,s),st=c(s),xe=o(s,"P",{});var fo=i(xe);Ip=l(fo,"C\u2019est tout pour l\u2019algorithme BPE ! Ensuite, nous allons nous int\xE9resser \xE0 WordPiece."),fo.forEach(e),this.h()},h(){T(f,"name","hf:doc:metadata"),T(f,"content",JSON.stringify(Po)),T(b,"id","toknisation-bytepair-encoding"),T(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(b,"href","#toknisation-bytepair-encoding"),T(h,"class","relative group"),T(X,"id","algorithme-dentranement"),T(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(X,"href","#algorithme-dentranement"),T(W,"class","relative group"),T(ns,"id","algorithme-de-tokenisation"),T(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(ns,"href","#algorithme-de-tokenisation"),T(Y,"class","relative group"),T(ts,"id","mise-en-uvre-du-bpe"),T(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(ts,"href","#mise-en-uvre-du-bpe"),T(Z,"class","relative group")},m(s,a){n(document.head,f),p(s,_,a),p(s,h,a),n(h,b),n(b,E),j(v,E,null),n(h,k),n(h,z),n(z,N),p(s,w,a),j(M,s,a),p(s,U,a),p(s,gs,a),n(gs,ke),n(ke,it),n(gs,ut),p(s,Dn,a),j(xs,s,a),p(s,Nn,a),j(Q,s,a),p(s,On,a),p(s,W,a),n(W,X),n(X,we),j(qs,we,null),n(W,ct),n(W,Ee),n(Ee,ft),p(s,Bn,a),p(s,ee,a),n(ee,ht),p(s,Mn,a),j(bs,s,a),p(s,Ln,a),p(s,F,a),n(F,mt),n(F,ye),n(ye,dt),n(F,jt),n(F,Pe),n(Pe,gt),n(F,xt),p(s,Hn,a),j(ss,s,a),p(s,Sn,a),p(s,L,a),n(L,qt),n(L,ze),n(ze,bt),n(L,$t),n(L,Ce),n(Ce,vt),n(L,_t),n(L,Te),n(Te,kt),n(L,wt),p(s,Gn,a),p(s,H,a),n(H,Et),n(H,Ae),n(Ae,yt),n(H,Pt),n(H,De),n(De,zt),n(H,Ct),n(H,Ne),n(Ne,Tt),n(H,At),p(s,In,a),p(s,ne,a),n(ne,Dt),p(s,Rn,a),j($s,s,a),p(s,Vn,a),p(s,C,a),n(C,Nt),n(C,Oe),n(Oe,Ot),n(C,Bt),n(C,Be),n(Be,Mt),n(C,Lt),n(C,Me),n(Me,Ht),n(C,St),n(C,Le),n(Le,Gt),n(C,It),n(C,He),n(He,Rt),n(C,Vt),n(C,Se),n(Se,Ut),n(C,Ft),p(s,Un,a),j(vs,s,a),p(s,Fn,a),p(s,y,a),n(y,Kt),n(y,Ge),n(Ge,Jt),n(y,Wt),n(y,Ie),n(Ie,Yt),n(y,Zt),n(y,Re),n(Re,Qt),n(y,Xt),n(y,Ve),n(Ve,sl),n(y,el),n(y,Ue),n(Ue,nl),n(y,al),n(y,Fe),n(Fe,tl),n(y,ll),n(y,Ke),n(Ke,pl),n(y,rl),p(s,Kn,a),p(s,S,a),n(S,ol),n(S,Je),n(Je,il),n(S,ul),n(S,We),n(We,cl),n(S,fl),n(S,Ye),n(Ye,hl),n(S,ml),p(s,Jn,a),j(_s,s,a),p(s,Wn,a),p(s,G,a),n(G,dl),n(G,Ze),n(Ze,jl),n(G,gl),n(G,Qe),n(Qe,xl),n(G,ql),n(G,Xe),n(Xe,bl),n(G,$l),p(s,Yn,a),j(ks,s,a),p(s,Zn,a),p(s,I,a),n(I,vl),n(I,sn),n(sn,_l),n(I,kl),n(I,en),n(en,wl),n(I,El),n(I,nn),n(nn,yl),n(I,Pl),p(s,Qn,a),j(ws,s,a),p(s,Xn,a),p(s,ae,a),n(ae,zl),p(s,sa,a),j(es,s,a),p(s,ea,a),p(s,Y,a),n(Y,ns),n(ns,an),j(Es,an,null),n(Y,Cl),n(Y,tn),n(tn,Tl),p(s,na,a),p(s,te,a),n(te,Al),p(s,aa,a),p(s,R,a),n(R,ln),n(ln,Dl),n(R,Nl),n(R,pn),n(pn,Ol),n(R,Bl),n(R,rn),n(rn,Ml),n(R,Ll),n(R,on),n(on,Hl),p(s,ta,a),p(s,le,a),n(le,Sl),p(s,la,a),j(ys,s,a),p(s,pa,a),p(s,P,a),n(P,Gl),n(P,un),n(un,Il),n(P,Rl),n(P,cn),n(cn,Vl),n(P,Ul),n(P,fn),n(fn,Fl),n(P,Kl),n(P,hn),n(hn,Jl),n(P,Wl),n(P,mn),n(mn,Yl),n(P,Zl),n(P,dn),n(dn,Ql),n(P,Xl),n(P,jn),n(jn,sp),n(P,ep),p(s,ra,a),j(as,s,a),p(s,oa,a),p(s,Z,a),n(Z,ts),n(ts,gn),j(Ps,gn,null),n(Z,np),n(Z,xn),n(xn,ap),p(s,ia,a),p(s,pe,a),n(pe,tp),p(s,ua,a),p(s,re,a),n(re,lp),p(s,ca,a),j(zs,s,a),p(s,fa,a),p(s,V,a),n(V,pp),n(V,qn),n(qn,rp),n(V,op),n(V,bn),n(bn,ip),n(V,up),n(V,$n),n($n,cp),n(V,fp),p(s,ha,a),j(Cs,s,a),p(s,ma,a),p(s,oe,a),n(oe,hp),p(s,da,a),j(Ts,s,a),p(s,ja,a),j(As,s,a),p(s,ga,a),p(s,ie,a),n(ie,mp),p(s,xa,a),j(Ds,s,a),p(s,qa,a),j(Ns,s,a),p(s,ba,a),p(s,K,a),n(K,dp),n(K,vn),n(vn,jp),n(K,gp),n(K,_n),n(_n,xp),n(K,qp),p(s,$a,a),j(Os,s,a),p(s,va,a),p(s,ue,a),n(ue,bp),p(s,_a,a),j(Bs,s,a),p(s,ka,a),p(s,ce,a),n(ce,$p),p(s,wa,a),j(Ms,s,a),p(s,Ea,a),p(s,fe,a),n(fe,vp),p(s,ya,a),j(Ls,s,a),p(s,Pa,a),j(Hs,s,a),p(s,za,a),p(s,he,a),n(he,_p),p(s,Ca,a),j(Ss,s,a),p(s,Ta,a),j(Gs,s,a),p(s,Aa,a),p(s,J,a),n(J,kp),n(J,kn),n(kn,wp),n(J,Ep),n(J,wn),n(wn,yp),n(J,Pp),p(s,Da,a),j(Is,s,a),p(s,Na,a),p(s,ls,a),n(ls,zp),n(ls,En),n(En,Cp),n(ls,Tp),p(s,Oa,a),j(Rs,s,a),p(s,Ba,a),p(s,me,a),n(me,Ap),p(s,Ma,a),j(Vs,s,a),p(s,La,a),j(Us,s,a),p(s,Ha,a),p(s,de,a),n(de,Dp),p(s,Sa,a),j(Fs,s,a),p(s,Ga,a),p(s,ps,a),n(ps,Np),n(ps,yn),n(yn,Op),n(ps,Bp),p(s,Ia,a),j(Ks,s,a),p(s,Ra,a),j(Js,s,a),p(s,Va,a),p(s,rs,a),n(rs,Mp),n(rs,Pn),n(Pn,Lp),n(rs,Hp),p(s,Ua,a),j(Ws,s,a),p(s,Fa,a),j(Ys,s,a),p(s,Ka,a),j(os,s,a),p(s,Ja,a),p(s,je,a),n(je,Sp),p(s,Wa,a),j(Zs,s,a),p(s,Ya,a),p(s,ge,a),n(ge,Gp),p(s,Za,a),j(Qs,s,a),p(s,Qa,a),j(Xs,s,a),p(s,Xa,a),j(is,s,a),p(s,st,a),p(s,xe,a),n(xe,Ip),et=!0},p(s,[a]){const se={};a&2&&(se.$$scope={dirty:a,ctx:s}),Q.$set(se);const zn={};a&2&&(zn.$$scope={dirty:a,ctx:s}),ss.$set(zn);const Cn={};a&2&&(Cn.$$scope={dirty:a,ctx:s}),es.$set(Cn);const Tn={};a&2&&(Tn.$$scope={dirty:a,ctx:s}),as.$set(Tn);const qe={};a&2&&(qe.$$scope={dirty:a,ctx:s}),os.$set(qe);const An={};a&2&&(An.$$scope={dirty:a,ctx:s}),is.$set(An)},i(s){et||(g(v.$$.fragment,s),g(M.$$.fragment,s),g(xs.$$.fragment,s),g(Q.$$.fragment,s),g(qs.$$.fragment,s),g(bs.$$.fragment,s),g(ss.$$.fragment,s),g($s.$$.fragment,s),g(vs.$$.fragment,s),g(_s.$$.fragment,s),g(ks.$$.fragment,s),g(ws.$$.fragment,s),g(es.$$.fragment,s),g(Es.$$.fragment,s),g(ys.$$.fragment,s),g(as.$$.fragment,s),g(Ps.$$.fragment,s),g(zs.$$.fragment,s),g(Cs.$$.fragment,s),g(Ts.$$.fragment,s),g(As.$$.fragment,s),g(Ds.$$.fragment,s),g(Ns.$$.fragment,s),g(Os.$$.fragment,s),g(Bs.$$.fragment,s),g(Ms.$$.fragment,s),g(Ls.$$.fragment,s),g(Hs.$$.fragment,s),g(Ss.$$.fragment,s),g(Gs.$$.fragment,s),g(Is.$$.fragment,s),g(Rs.$$.fragment,s),g(Vs.$$.fragment,s),g(Us.$$.fragment,s),g(Fs.$$.fragment,s),g(Ks.$$.fragment,s),g(Js.$$.fragment,s),g(Ws.$$.fragment,s),g(Ys.$$.fragment,s),g(os.$$.fragment,s),g(Zs.$$.fragment,s),g(Qs.$$.fragment,s),g(Xs.$$.fragment,s),g(is.$$.fragment,s),et=!0)},o(s){x(v.$$.fragment,s),x(M.$$.fragment,s),x(xs.$$.fragment,s),x(Q.$$.fragment,s),x(qs.$$.fragment,s),x(bs.$$.fragment,s),x(ss.$$.fragment,s),x($s.$$.fragment,s),x(vs.$$.fragment,s),x(_s.$$.fragment,s),x(ks.$$.fragment,s),x(ws.$$.fragment,s),x(es.$$.fragment,s),x(Es.$$.fragment,s),x(ys.$$.fragment,s),x(as.$$.fragment,s),x(Ps.$$.fragment,s),x(zs.$$.fragment,s),x(Cs.$$.fragment,s),x(Ts.$$.fragment,s),x(As.$$.fragment,s),x(Ds.$$.fragment,s),x(Ns.$$.fragment,s),x(Os.$$.fragment,s),x(Bs.$$.fragment,s),x(Ms.$$.fragment,s),x(Ls.$$.fragment,s),x(Hs.$$.fragment,s),x(Ss.$$.fragment,s),x(Gs.$$.fragment,s),x(Is.$$.fragment,s),x(Rs.$$.fragment,s),x(Vs.$$.fragment,s),x(Us.$$.fragment,s),x(Fs.$$.fragment,s),x(Ks.$$.fragment,s),x(Js.$$.fragment,s),x(Ws.$$.fragment,s),x(Ys.$$.fragment,s),x(os.$$.fragment,s),x(Zs.$$.fragment,s),x(Qs.$$.fragment,s),x(Xs.$$.fragment,s),x(is.$$.fragment,s),et=!1},d(s){e(f),s&&e(_),s&&e(h),q(v),s&&e(w),q(M,s),s&&e(U),s&&e(gs),s&&e(Dn),q(xs,s),s&&e(Nn),q(Q,s),s&&e(On),s&&e(W),q(qs),s&&e(Bn),s&&e(ee),s&&e(Mn),q(bs,s),s&&e(Ln),s&&e(F),s&&e(Hn),q(ss,s),s&&e(Sn),s&&e(L),s&&e(Gn),s&&e(H),s&&e(In),s&&e(ne),s&&e(Rn),q($s,s),s&&e(Vn),s&&e(C),s&&e(Un),q(vs,s),s&&e(Fn),s&&e(y),s&&e(Kn),s&&e(S),s&&e(Jn),q(_s,s),s&&e(Wn),s&&e(G),s&&e(Yn),q(ks,s),s&&e(Zn),s&&e(I),s&&e(Qn),q(ws,s),s&&e(Xn),s&&e(ae),s&&e(sa),q(es,s),s&&e(ea),s&&e(Y),q(Es),s&&e(na),s&&e(te),s&&e(aa),s&&e(R),s&&e(ta),s&&e(le),s&&e(la),q(ys,s),s&&e(pa),s&&e(P),s&&e(ra),q(as,s),s&&e(oa),s&&e(Z),q(Ps),s&&e(ia),s&&e(pe),s&&e(ua),s&&e(re),s&&e(ca),q(zs,s),s&&e(fa),s&&e(V),s&&e(ha),q(Cs,s),s&&e(ma),s&&e(oe),s&&e(da),q(Ts,s),s&&e(ja),q(As,s),s&&e(ga),s&&e(ie),s&&e(xa),q(Ds,s),s&&e(qa),q(Ns,s),s&&e(ba),s&&e(K),s&&e($a),q(Os,s),s&&e(va),s&&e(ue),s&&e(_a),q(Bs,s),s&&e(ka),s&&e(ce),s&&e(wa),q(Ms,s),s&&e(Ea),s&&e(fe),s&&e(ya),q(Ls,s),s&&e(Pa),q(Hs,s),s&&e(za),s&&e(he),s&&e(Ca),q(Ss,s),s&&e(Ta),q(Gs,s),s&&e(Aa),s&&e(J),s&&e(Da),q(Is,s),s&&e(Na),s&&e(ls),s&&e(Oa),q(Rs,s),s&&e(Ba),s&&e(me),s&&e(Ma),q(Vs,s),s&&e(La),q(Us,s),s&&e(Ha),s&&e(de),s&&e(Sa),q(Fs,s),s&&e(Ga),s&&e(ps),s&&e(Ia),q(Ks,s),s&&e(Ra),q(Js,s),s&&e(Va),s&&e(rs),s&&e(Ua),q(Ws,s),s&&e(Fa),q(Ys,s),s&&e(Ka),q(os,s),s&&e(Ja),s&&e(je),s&&e(Wa),q(Zs,s),s&&e(Ya),s&&e(ge),s&&e(Za),q(Qs,s),s&&e(Qa),q(Xs,s),s&&e(Xa),q(is,s),s&&e(st),s&&e(xe)}}}const Po={local:"toknisation-bytepair-encoding",sections:[{local:"algorithme-dentranement",title:"Algorithme d'entra\xEEnement"},{local:"algorithme-de-tokenisation",title:"Algorithme de tokenisation"},{local:"mise-en-uvre-du-bpe",title:"Mise en \u0153uvre du BPE"}],title:"Tok\xE9nisation *Byte-Pair Encoding*"};function zo(O){return xo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bo extends ho{constructor(f){super();mo(this,f,zo,yo,jo,{})}}export{Bo as default,Po as metadata};
