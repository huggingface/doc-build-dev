import{S as mi,i as gi,s as wi,e as l,k as p,w as u,t as r,M as bi,c as o,d as s,m as c,a as d,x as h,h as a,b,G as n,g as i,y as f,q as m,o as g,B as w,v as _i}from"../../chunks/vendor-hf-doc-builder.js";import{D as vi,Y as hi,T as fi}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{I as Zs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as v}from"../../chunks/CodeBlock-hf-doc-builder.js";function $i(Ze){let _,z,$,E,M;return{c(){_=l("p"),z=r("\u270F\uFE0F "),$=l("strong"),E=r("Probier es selbt!"),M=r(" \xC4ndere die vorherige Trainingsschleife, um dein Modell auf dem SST-2-Datensatz fein zu tunen.")},l(j){_=o(j,"P",{});var S=d(_);z=a(S,"\u270F\uFE0F "),$=o(S,"STRONG",{});var L=d($);E=a(L,"Probier es selbt!"),L.forEach(s),M=a(S," \xC4ndere die vorherige Trainingsschleife, um dein Modell auf dem SST-2-Datensatz fein zu tunen."),S.forEach(s)},m(j,S){i(j,_,S),n(_,z),n(_,$),n($,E),n(_,M)},d(j){j&&s(_)}}}function ki(Ze){let _;return{c(){_=r('\u26A0\uFE0F Um von dem Geschwindigkeitsvorteil der Cloud TPUs zu profitieren, empfehlen wir, deine Samples mit den Argumenten `padding="max_length"` und `max_length` des Tokenizers auf eine feste L\xE4nge aufzuf\xFCllen.')},l(z){_=a(z,'\u26A0\uFE0F Um von dem Geschwindigkeitsvorteil der Cloud TPUs zu profitieren, empfehlen wir, deine Samples mit den Argumenten `padding="max_length"` und `max_length` des Tokenizers auf eine feste L\xE4nge aufzuf\xFCllen.')},m(z,$){i(z,_,$)},d(z){z&&s(_)}}}function zi(Ze){let _,z,$,E,M,j,S,L,ut,Hs,oe,Fs,de,Ns,G,ht,ts,ft,mt,Is,pe,Vs,x,Z,rs,ce,gt,as,wt,Ks,P,bt,is,_t,vt,ls,$t,kt,Rs,C,O,zt,os,jt,Et,ds,Dt,yt,At,q,Tt,ps,qt,St,cs,Pt,Ct,us,Mt,xt,Ot,hs,Bt,Js,H,Wt,fs,Ut,Lt,Ys,ue,Qs,He,Gt,Xs,he,en,Fe,Zt,sn,fe,nn,Ne,Ht,tn,me,rn,ge,an,F,Ft,ms,Nt,It,ln,Ie,Vt,on,we,dn,Ve,Kt,pn,be,cn,_e,un,N,Rt,gs,Jt,Yt,hn,D,Qt,ws,Xt,er,bs,sr,nr,ve,tr,rr,fn,$e,mn,I,ar,_s,ir,lr,gn,ke,wn,ze,bn,B,V,vs,je,or,$s,dr,_n,K,pr,ks,cr,ur,vn,Ee,$n,De,kn,R,hr,zs,fr,mr,zn,ye,jn,Ke,gr,En,W,J,js,Ae,wr,Es,br,Dn,y,_r,Ds,vr,$r,ys,kr,zr,As,jr,Er,yn,Te,An,qe,Tn,Re,Dr,qn,Y,Sn,U,Q,Ts,Se,yr,qs,Ar,Pn,Pe,Cn,X,Tr,Ce,qr,Sr,Mn,Me,xn,Je,Pr,On,xe,Bn,A,Cr,Ss,Mr,xr,Ps,Or,Br,Cs,Wr,Ur,Wn,k,Lr,Ms,Gr,Zr,xs,Hr,Fr,Os,Nr,Ir,Bs,Vr,Kr,Ws,Rr,Jr,Un,ee,Ln,Ye,Yr,Gn,Oe,Zn,se,Qr,Us,Xr,ea,Hn,Be,Fn,Qe,sa,Nn,We,In,Xe,na,Vn,ne,ta,Ls,ra,aa,Kn,Ue,Rn,te,ia,Le,la,oa,Jn;return j=new Zs({}),oe=new vi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"}]}}),de=new hi({props:{id:"Dh9CL8fyG80"}}),pe=new v({props:{code:`



`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),ce=new Zs({}),ue=new v({props:{code:`tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names`,highlighted:`tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>, <span class="hljs-string">&quot;idx&quot;</span>])
tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)
tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names`}}),he=new v({props:{code:'["attention_mask", "input_ids", "labels", "token_type_ids"]',highlighted:'[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>]'}}),fe=new v({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>], shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], batch_size=<span class="hljs-number">8</span>, collate_fn=data_collator
)`}}),me=new v({props:{code:`for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
    <span class="hljs-keyword">break</span>
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}}),ge=new v({props:{code:`{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">65</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">65</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">65</span>])}`}}),we=new v({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)`}}),be=new v({props:{code:`outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)`,highlighted:`outputs = model(**batch)
<span class="hljs-built_in">print</span>(outputs.loss, outputs.logits.shape)`}}),_e=new v({props:{code:"tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])",highlighted:'tensor(<span class="hljs-number">0.5441</span>, grad_fn=&lt;NllLossBackward&gt;) torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">2</span>])'}}),$e=new v({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`}}),ke=new v({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_epochs = <span class="hljs-number">3</span>
num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)
lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)
<span class="hljs-built_in">print</span>(num_training_steps)`}}),ze=new v({props:{code:"1377",highlighted:'<span class="hljs-number">1377</span>'}}),je=new Zs({}),Ee=new v({props:{code:"",highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
model.to(device)
device`}}),De=new v({props:{code:"device(type='cuda')",highlighted:'device(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;cuda&#x27;</span>)'}}),ye=new v({props:{code:`

`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)`}}),Ae=new Zs({}),Te=new v({props:{code:`

`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
model.<span class="hljs-built_in">eval</span>()
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">&quot;labels&quot;</span>])

metric.compute()`}}),qe=new v({props:{code:"{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.8431372549019608</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.8907849829351535</span>}'}}),Y=new fi({props:{$$slots:{default:[$i]},$$scope:{ctx:Ze}}}),Se=new Zs({}),Pe=new hi({props:{id:"s7dy8QRgjJ0"}}),Me=new v({props:{code:`




`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
model.to(device)

num_epochs = <span class="hljs-number">3</span>
num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)
lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)`}}),xe=new v({props:{code:`






`,highlighted:`<span class="hljs-addition">+ from accelerate import Accelerator</span>
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

<span class="hljs-addition">+ accelerator = Accelerator()</span>

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

<span class="hljs-deletion">- device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)</span>
<span class="hljs-deletion">- model.to(device)</span>

<span class="hljs-addition">+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(</span>
<span class="hljs-addition">+     train_dataloader, eval_dataloader, model, optimizer</span>
<span class="hljs-addition">+ )</span>

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      &quot;linear&quot;,
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
<span class="hljs-deletion">-         batch = {k: v.to(device) for k, v in batch.items()}</span>
          outputs = model(**batch)
          loss = outputs.loss
<span class="hljs-deletion">-         loss.backward()</span>
<span class="hljs-addition">+         accelerator.backward(loss)</span>

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)`}}),ee=new fi({props:{$$slots:{default:[ki]},$$scope:{ctx:Ze}}}),Oe=new v({props:{code:`





`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = <span class="hljs-number">3</span>
num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dl)
lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)`}}),Be=new v({props:{code:"accelerate config",highlighted:"accelerate config"}}),We=new v({props:{code:"accelerate launch train.py",highlighted:'accelerate <span class="hljs-built_in">launch</span> train.py'}}),Ue=new v({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> notebook_launcher

notebook_launcher(training_function)`}}),{c(){_=l("meta"),z=p(),$=l("h1"),E=l("a"),M=l("span"),u(j.$$.fragment),S=p(),L=l("span"),ut=r("Komplettes Training"),Hs=p(),u(oe.$$.fragment),Fs=p(),u(de.$$.fragment),Ns=p(),G=l("p"),ht=r("In diesem Abschnitt befassen wir uns damit, wie wir die gleichen Ergebnisse wie im letzten Abschnitt erzielen k\xF6nnen, ohne die Klasse "),ts=l("code"),ft=r("Trainer"),mt=r(" zu verwenden. Auch hier gehen wir davon aus, dass du die Datenverarbeitung in Abschnitt 2 durchgef\xFChrt hast. Hier ist eine kurze Zusammenfassung mit allem, was du brauchst:"),Is=p(),u(pe.$$.fragment),Vs=p(),x=l("h3"),Z=l("a"),rs=l("span"),u(ce.$$.fragment),gt=p(),as=l("span"),wt=r("Vorbereitung f\xFCr das Training"),Ks=p(),P=l("p"),bt=r("Bevor wir unsere Trainingsschleife schreiben, m\xFCssen wir noch einige Objekte definieren. Zun\xE4chst m\xFCssen wir die Datalader definieren, mit denen wir \xFCber die Batches iterieren werden. Doch bevor wir diese Dataloader definieren k\xF6nnen, m\xFCssen wir unsere "),is=l("code"),_t=r("tokenized_datasets"),vt=r(" nachbearbeiten, um einige Dinge zu erledigen, die der "),ls=l("code"),$t=r("Trainer"),kt=r(" automatisch f\xFCr uns erledigt hat. Konkret hei\xDFt das, dass wir:"),Rs=p(),C=l("ul"),O=l("li"),zt=r("Die Spalten entfernen, die Werte enthalten, die das Modell nicht erwartet (wie die Spalten "),os=l("code"),jt=r("sentence1"),Et=r(" und "),ds=l("code"),Dt=r("sentence2"),yt=r(")."),At=p(),q=l("li"),Tt=r("Die Spalte "),ps=l("code"),qt=r("label"),St=r(" in "),cs=l("code"),Pt=r("labels"),Ct=r(" umbenennen (weil das Modell erwartet, dass das Argument "),us=l("code"),Mt=r("labels"),xt=r(" hei\xDFt)."),Ot=p(),hs=l("li"),Bt=r("Das Format der Datens\xE4tze anpassen, so dass sie PyTorch-Tensoren statt Listen zur\xFCckgeben."),Js=p(),H=l("p"),Wt=r("Das "),fs=l("code"),Ut=r("tokenized_datasets"),Lt=r(" hat eine Methode f\xFCr jeden dieser Schritte:"),Ys=p(),u(ue.$$.fragment),Qs=p(),He=l("p"),Gt=r("Anschlie\xDFend k\xF6nnen wir \xFCberpr\xFCfen, ob der Output nur Spalten enth\xE4lt, die unser Modell akzeptiert:"),Xs=p(),u(he.$$.fragment),en=p(),Fe=l("p"),Zt=r("Jetzt k\xF6nnen wir ganz einfach unsere Dataloader definieren:"),sn=p(),u(fe.$$.fragment),nn=p(),Ne=l("p"),Ht=r("Um sicher zu gehen, \xFCberpr\xFCfen wir ein Batch auf Fehler in der Datenverarbeitung:"),tn=p(),u(me.$$.fragment),rn=p(),u(ge.$$.fragment),an=p(),F=l("p"),Ft=r("Beachte, dass die Dimensionen der Tensoren wahrscheinlich etwas anders aussehen werden, da wir f\xFCr den Trainingsdatenlader "),ms=l("code"),Nt=r("shuffle=True"),It=r(" eingestellt haben und innerhalb des Batches auf die maximale L\xE4nge auff\xFCllen."),ln=p(),Ie=l("p"),Vt=r("Da wir nun mit der Datenvorverarbeitung fertig sind (ein zufriedenstellendes aber schwer erreichbares Ziel f\xFCr jeden ML-Experten), k\xF6nnen wir uns nun dem Modell zuwenden. Wir instanziieren es genauso wie im vorherigen Abschnitt:"),on=p(),u(we.$$.fragment),dn=p(),Ve=l("p"),Kt=r("Als weitere Sicherheitsma\xDFnahme \xFCbergeben wir unseren Batch an das Modell, um sicherzustellen, dass beim Training alles glatt l\xE4uft:"),pn=p(),u(be.$$.fragment),cn=p(),u(_e.$$.fragment),un=p(),N=l("p"),Rt=r("Alle \u{1F917} Transformer Modelle geben den Verlust zur\xFCck, wenn "),gs=l("code"),Jt=r("labels"),Yt=r(" angegeben werden, und wir erhalten zus\xE4tzlich die Logits (zwei f\xFCr jede Eingabe in unserem Batch, also einen Tensor der Gr\xF6\xDFe 8 x 2)."),hn=p(),D=l("p"),Qt=r("Wir sind fast so weit, unsere Trainingsschleife zu schreiben! Es fehlen nur noch zwei Dinge: ein Optimierer und ein Scheduler f\xFCr die Lernrate. Da wir versuchen, das zu wiederholen, was der "),ws=l("code"),Xt=r("Trainer"),er=r(" automatisch gemacht hat, werden wir die gleichen Standardwerte verwenden. Der Optimierer, den der "),bs=l("code"),sr=r("Trainer"),nr=r(" verwendet, hei\xDFt \u201CAdamW\u201D und ist gr\xF6\xDFtenteils derselbe wie Adam, abgesehen von einer Abwandlung f\xFCr die \u201CWeight Decay Regularization\u201D (siehe [\u201CDecoupled Weight Decay Regularization\u201D] ("),ve=l("a"),tr=r("https://arxiv.org/abs/1711.05101"),rr=r(") von Ilya Loshchilov und Frank Hutter):"),fn=p(),u($e.$$.fragment),mn=p(),I=l("p"),ar=r("Der standardm\xE4\xDFig verwendete Scheduler f\xFCr die Lernrate ist ein linearer Abstieg vom Maximalwert (5e-5) auf 0. Um ihn richtig zu definieren, m\xFCssen wir die Anzahl der Trainingsschritte kennen, d.h. die Anzahl der Epochen, die die Trainingsschleife durchlaufen soll, multipliziert mit der Anzahl der Trainingsbatches (der L\xE4nge unseres Trainingsdatenordners). Der "),_s=l("code"),ir=r("Trainer"),lr=r(" verwendet standardm\xE4\xDFig drei Epochen, woran wir uns hier orientieren werden:"),gn=p(),u(ke.$$.fragment),wn=p(),u(ze.$$.fragment),bn=p(),B=l("h3"),V=l("a"),vs=l("span"),u(je.$$.fragment),or=p(),$s=l("span"),dr=r("Die Trainingsschleife"),_n=p(),K=l("p"),pr=r("Ein letzter Hinweis: Wir wollen die GPU zum Training nutzen, wenn wir Zugang zu einer haben (auf einer CPU kann das Training mehrere Stunden statt ein paar Minuten dauern). Dazu definieren wir "),ks=l("code"),cr=r("device"),ur=r(" als Ger\xE4t auf dem wir unser Modell und unsere Batches speichern:"),vn=p(),u(Ee.$$.fragment),$n=p(),u(De.$$.fragment),kn=p(),R=l("p"),hr=r("Wir sind jetzt bereit f\xFCr das Training! Um ein Gef\xFChl daf\xFCr zu bekommen, wann das Training abgeschlossen sein wird, f\xFCgen wir mit der Bibliothek "),zs=l("code"),fr=r("tqdm"),mr=r(" einen Fortschrittsbalken \xFCber die Anzahl der Trainingsschritte ein:"),zn=p(),u(ye.$$.fragment),jn=p(),Ke=l("p"),gr=r("Der Kern der Trainingsschleife sieht \xE4hnlich aus wie in der Einleitung. Da wir keine Berichte angefordert haben, gibt die Trainingsschleife nichts \xFCber die Performance des Modells zur\xFCck. Daf\xFCr m\xFCssen wir eine Evaluationsschleife einf\xFCgen."),En=p(),W=l("h3"),J=l("a"),js=l("span"),u(Ae.$$.fragment),wr=p(),Es=l("span"),br=r("Die Evaluationsschleife"),Dn=p(),y=l("p"),_r=r("Wie schon zuvor verwenden wir eine Metrik, die von der \u{1F917} Datasets-Bibliothek bereitgestellt wird. Wir haben bereits die Methode "),Ds=l("code"),vr=r("metric.compute()"),$r=r(" gesehen, aber Metriken k\xF6nnen auch Batches f\xFCr uns akkumulieren, wenn wir die Vorhersageschleife mit der Methode "),ys=l("code"),kr=r("add_batch()"),zr=r(" durchlaufen. Sobald wir alle Batches gesammelt haben, k\xF6nnen wir das Endergebnis mit der Methode "),As=l("code"),jr=r("metric.compute()"),Er=r(" ermitteln. So implementierst du all das in eine Evaluationsschleife:"),yn=p(),u(Te.$$.fragment),An=p(),u(qe.$$.fragment),Tn=p(),Re=l("p"),Dr=r("Auch hier werden deine Ergebnisse wegen der Zuf\xE4lligkeit bei der Initialisierung des Modellkopfes und der Datenverteilung etwas anders ausfallen, aber sie sollten in etwa gleich sein."),qn=p(),u(Y.$$.fragment),Sn=p(),U=l("h3"),Q=l("a"),Ts=l("span"),u(Se.$$.fragment),yr=p(),qs=l("span"),Ar=r("Verbessere deine Trainingsschleife mit \u{1F917} Accelerate"),Pn=p(),u(Pe.$$.fragment),Cn=p(),X=l("p"),Tr=r("Die Trainingsschleife, die wir zuvor definiert haben, funktioniert gut auf einer einzelnen CPU oder GPU. Aber mit der Bibliothek "),Ce=l("a"),qr=r("\u{1F917} Accelerate"),Sr=r(" k\xF6nnen wir mit wenigen Anpassungen verteiltes Training auf mehreren GPUs oder TPUs implementieren. Beginnend mit der Erstellung der Trainings- und Validierungsdaten, sieht unsere manuelle Trainingsschleife nun folgenderma\xDFen aus:"),Mn=p(),u(Me.$$.fragment),xn=p(),Je=l("p"),Pr=r("Und hier sind die \xC4nderungen:"),On=p(),u(xe.$$.fragment),Bn=p(),A=l("p"),Cr=r("Die erste Zeile, die hinzugef\xFCgt werden muss, ist die Import-Zeile. Die zweite Zeile instanziiert ein "),Ss=l("code"),Mr=r("Accelerator"),xr=r("-Objekt, das die Hardware analysiert und die richtige verteilte Umgebung initialisiert. Accelerate k\xFCmmert sich um die Anordnung der Ger\xE4te, du kannst also die Zeilen entfernen, die das Modell auf dem Ger\xE4t platzieren (oder, wenn du das m\xF6chtest, sie so \xE4ndern, dass sie "),Ps=l("code"),Or=r("accelerator.device"),Br=r(" anstelle von "),Cs=l("code"),Wr=r("device"),Ur=r(" verwenden)."),Wn=p(),k=l("p"),Lr=r("Der Hauptteil der Arbeit wird dann in der Zeile erledigt, die die Dataloader, das Modell und den Optimierer an "),Ms=l("code"),Gr=r("accelerator.prepare()"),Zr=r(" sendet. Dadurch werden diese Objekte in den richtigen Container verpackt, damit das verteilte Training wie vorgesehen funktioniert. Die verbleibenden \xC4nderungen sind das Entfernen der Zeile, die das Batch auf dem Ger\xE4t mit "),xs=l("code"),Hr=r("device"),Fr=r(" ablegt (wenn du das beibehalten willst, kannst du es einfach in "),Os=l("code"),Nr=r("accelerator.device"),Ir=r(" \xE4ndern) und das Ersetzen von "),Bs=l("code"),Vr=r("loss.backward()"),Kr=r(" durch "),Ws=l("code"),Rr=r("accelerator.backward(loss)"),Jr=r("."),Un=p(),u(ee.$$.fragment),Ln=p(),Ye=l("p"),Yr=r("Wenn du damit experimentieren m\xF6chtest, siehst du hier, wie die komplette Trainingsschleife mit \u{1F917} Accelerate aussieht:"),Gn=p(),u(Oe.$$.fragment),Zn=p(),se=l("p"),Qr=r("Wenn dies in das Script "),Us=l("code"),Xr=r("train.py"),ea=r(" eingef\xFCgt wird, kann das Script auf jeder Art von verteilter Hardware ausgef\xFChrt werden. Um es auf deiner verteilten Hardware auszuprobieren, f\xFChre den folgenden Befehl aus:"),Hn=p(),u(Be.$$.fragment),Fn=p(),Qe=l("p"),sa=r("Du wirst dann aufgefordert werden, einige Fragen zu beantworten und die Antworten in eine Konfigurationsdatei zu schreiben, die von diesem Befehl verwendet wird:"),Nn=p(),u(We.$$.fragment),In=p(),Xe=l("p"),na=r("Damit wird das verteilte Training gestartet."),Vn=p(),ne=l("p"),ta=r("Wenn du das in einem Notebook ausprobieren m\xF6chtest (z. B. um es mit TPUs auf Colab zu testen), f\xFCge den Code einfach in eine "),Ls=l("code"),ra=r("training_function()"),aa=r(" ein und f\xFChre eine letzte Zelle mit aus:"),Kn=p(),u(Ue.$$.fragment),Rn=p(),te=l("p"),ia=r("Weitere Beispiele findest du in dem "),Le=l("a"),la=r("\u{1F917} Accelerate Repo"),oa=r("."),this.h()},l(e){const t=bi('[data-svelte="svelte-1phssyn"]',document.head);_=o(t,"META",{name:!0,content:!0}),t.forEach(s),z=c(e),$=o(e,"H1",{class:!0});var Ge=d($);E=o(Ge,"A",{id:!0,class:!0,href:!0});var Gs=d(E);M=o(Gs,"SPAN",{});var da=d(M);h(j.$$.fragment,da),da.forEach(s),Gs.forEach(s),S=c(Ge),L=o(Ge,"SPAN",{});var pa=d(L);ut=a(pa,"Komplettes Training"),pa.forEach(s),Ge.forEach(s),Hs=c(e),h(oe.$$.fragment,e),Fs=c(e),h(de.$$.fragment,e),Ns=c(e),G=o(e,"P",{});var Yn=d(G);ht=a(Yn,"In diesem Abschnitt befassen wir uns damit, wie wir die gleichen Ergebnisse wie im letzten Abschnitt erzielen k\xF6nnen, ohne die Klasse "),ts=o(Yn,"CODE",{});var ca=d(ts);ft=a(ca,"Trainer"),ca.forEach(s),mt=a(Yn," zu verwenden. Auch hier gehen wir davon aus, dass du die Datenverarbeitung in Abschnitt 2 durchgef\xFChrt hast. Hier ist eine kurze Zusammenfassung mit allem, was du brauchst:"),Yn.forEach(s),Is=c(e),h(pe.$$.fragment,e),Vs=c(e),x=o(e,"H3",{class:!0});var Qn=d(x);Z=o(Qn,"A",{id:!0,class:!0,href:!0});var ua=d(Z);rs=o(ua,"SPAN",{});var ha=d(rs);h(ce.$$.fragment,ha),ha.forEach(s),ua.forEach(s),gt=c(Qn),as=o(Qn,"SPAN",{});var fa=d(as);wt=a(fa,"Vorbereitung f\xFCr das Training"),fa.forEach(s),Qn.forEach(s),Ks=c(e),P=o(e,"P",{});var es=d(P);bt=a(es,"Bevor wir unsere Trainingsschleife schreiben, m\xFCssen wir noch einige Objekte definieren. Zun\xE4chst m\xFCssen wir die Datalader definieren, mit denen wir \xFCber die Batches iterieren werden. Doch bevor wir diese Dataloader definieren k\xF6nnen, m\xFCssen wir unsere "),is=o(es,"CODE",{});var ma=d(is);_t=a(ma,"tokenized_datasets"),ma.forEach(s),vt=a(es," nachbearbeiten, um einige Dinge zu erledigen, die der "),ls=o(es,"CODE",{});var ga=d(ls);$t=a(ga,"Trainer"),ga.forEach(s),kt=a(es," automatisch f\xFCr uns erledigt hat. Konkret hei\xDFt das, dass wir:"),es.forEach(s),Rs=c(e),C=o(e,"UL",{});var ss=d(C);O=o(ss,"LI",{});var ns=d(O);zt=a(ns,"Die Spalten entfernen, die Werte enthalten, die das Modell nicht erwartet (wie die Spalten "),os=o(ns,"CODE",{});var wa=d(os);jt=a(wa,"sentence1"),wa.forEach(s),Et=a(ns," und "),ds=o(ns,"CODE",{});var ba=d(ds);Dt=a(ba,"sentence2"),ba.forEach(s),yt=a(ns,")."),ns.forEach(s),At=c(ss),q=o(ss,"LI",{});var re=d(q);Tt=a(re,"Die Spalte "),ps=o(re,"CODE",{});var _a=d(ps);qt=a(_a,"label"),_a.forEach(s),St=a(re," in "),cs=o(re,"CODE",{});var va=d(cs);Pt=a(va,"labels"),va.forEach(s),Ct=a(re," umbenennen (weil das Modell erwartet, dass das Argument "),us=o(re,"CODE",{});var $a=d(us);Mt=a($a,"labels"),$a.forEach(s),xt=a(re," hei\xDFt)."),re.forEach(s),Ot=c(ss),hs=o(ss,"LI",{});var ka=d(hs);Bt=a(ka,"Das Format der Datens\xE4tze anpassen, so dass sie PyTorch-Tensoren statt Listen zur\xFCckgeben."),ka.forEach(s),ss.forEach(s),Js=c(e),H=o(e,"P",{});var Xn=d(H);Wt=a(Xn,"Das "),fs=o(Xn,"CODE",{});var za=d(fs);Ut=a(za,"tokenized_datasets"),za.forEach(s),Lt=a(Xn," hat eine Methode f\xFCr jeden dieser Schritte:"),Xn.forEach(s),Ys=c(e),h(ue.$$.fragment,e),Qs=c(e),He=o(e,"P",{});var ja=d(He);Gt=a(ja,"Anschlie\xDFend k\xF6nnen wir \xFCberpr\xFCfen, ob der Output nur Spalten enth\xE4lt, die unser Modell akzeptiert:"),ja.forEach(s),Xs=c(e),h(he.$$.fragment,e),en=c(e),Fe=o(e,"P",{});var Ea=d(Fe);Zt=a(Ea,"Jetzt k\xF6nnen wir ganz einfach unsere Dataloader definieren:"),Ea.forEach(s),sn=c(e),h(fe.$$.fragment,e),nn=c(e),Ne=o(e,"P",{});var Da=d(Ne);Ht=a(Da,"Um sicher zu gehen, \xFCberpr\xFCfen wir ein Batch auf Fehler in der Datenverarbeitung:"),Da.forEach(s),tn=c(e),h(me.$$.fragment,e),rn=c(e),h(ge.$$.fragment,e),an=c(e),F=o(e,"P",{});var et=d(F);Ft=a(et,"Beachte, dass die Dimensionen der Tensoren wahrscheinlich etwas anders aussehen werden, da wir f\xFCr den Trainingsdatenlader "),ms=o(et,"CODE",{});var ya=d(ms);Nt=a(ya,"shuffle=True"),ya.forEach(s),It=a(et," eingestellt haben und innerhalb des Batches auf die maximale L\xE4nge auff\xFCllen."),et.forEach(s),ln=c(e),Ie=o(e,"P",{});var Aa=d(Ie);Vt=a(Aa,"Da wir nun mit der Datenvorverarbeitung fertig sind (ein zufriedenstellendes aber schwer erreichbares Ziel f\xFCr jeden ML-Experten), k\xF6nnen wir uns nun dem Modell zuwenden. Wir instanziieren es genauso wie im vorherigen Abschnitt:"),Aa.forEach(s),on=c(e),h(we.$$.fragment,e),dn=c(e),Ve=o(e,"P",{});var Ta=d(Ve);Kt=a(Ta,"Als weitere Sicherheitsma\xDFnahme \xFCbergeben wir unseren Batch an das Modell, um sicherzustellen, dass beim Training alles glatt l\xE4uft:"),Ta.forEach(s),pn=c(e),h(be.$$.fragment,e),cn=c(e),h(_e.$$.fragment,e),un=c(e),N=o(e,"P",{});var st=d(N);Rt=a(st,"Alle \u{1F917} Transformer Modelle geben den Verlust zur\xFCck, wenn "),gs=o(st,"CODE",{});var qa=d(gs);Jt=a(qa,"labels"),qa.forEach(s),Yt=a(st," angegeben werden, und wir erhalten zus\xE4tzlich die Logits (zwei f\xFCr jede Eingabe in unserem Batch, also einen Tensor der Gr\xF6\xDFe 8 x 2)."),st.forEach(s),hn=c(e),D=o(e,"P",{});var ae=d(D);Qt=a(ae,"Wir sind fast so weit, unsere Trainingsschleife zu schreiben! Es fehlen nur noch zwei Dinge: ein Optimierer und ein Scheduler f\xFCr die Lernrate. Da wir versuchen, das zu wiederholen, was der "),ws=o(ae,"CODE",{});var Sa=d(ws);Xt=a(Sa,"Trainer"),Sa.forEach(s),er=a(ae," automatisch gemacht hat, werden wir die gleichen Standardwerte verwenden. Der Optimierer, den der "),bs=o(ae,"CODE",{});var Pa=d(bs);sr=a(Pa,"Trainer"),Pa.forEach(s),nr=a(ae," verwendet, hei\xDFt \u201CAdamW\u201D und ist gr\xF6\xDFtenteils derselbe wie Adam, abgesehen von einer Abwandlung f\xFCr die \u201CWeight Decay Regularization\u201D (siehe [\u201CDecoupled Weight Decay Regularization\u201D] ("),ve=o(ae,"A",{href:!0,rel:!0});var Ca=d(ve);tr=a(Ca,"https://arxiv.org/abs/1711.05101"),Ca.forEach(s),rr=a(ae,") von Ilya Loshchilov und Frank Hutter):"),ae.forEach(s),fn=c(e),h($e.$$.fragment,e),mn=c(e),I=o(e,"P",{});var nt=d(I);ar=a(nt,"Der standardm\xE4\xDFig verwendete Scheduler f\xFCr die Lernrate ist ein linearer Abstieg vom Maximalwert (5e-5) auf 0. Um ihn richtig zu definieren, m\xFCssen wir die Anzahl der Trainingsschritte kennen, d.h. die Anzahl der Epochen, die die Trainingsschleife durchlaufen soll, multipliziert mit der Anzahl der Trainingsbatches (der L\xE4nge unseres Trainingsdatenordners). Der "),_s=o(nt,"CODE",{});var Ma=d(_s);ir=a(Ma,"Trainer"),Ma.forEach(s),lr=a(nt," verwendet standardm\xE4\xDFig drei Epochen, woran wir uns hier orientieren werden:"),nt.forEach(s),gn=c(e),h(ke.$$.fragment,e),wn=c(e),h(ze.$$.fragment,e),bn=c(e),B=o(e,"H3",{class:!0});var tt=d(B);V=o(tt,"A",{id:!0,class:!0,href:!0});var xa=d(V);vs=o(xa,"SPAN",{});var Oa=d(vs);h(je.$$.fragment,Oa),Oa.forEach(s),xa.forEach(s),or=c(tt),$s=o(tt,"SPAN",{});var Ba=d($s);dr=a(Ba,"Die Trainingsschleife"),Ba.forEach(s),tt.forEach(s),_n=c(e),K=o(e,"P",{});var rt=d(K);pr=a(rt,"Ein letzter Hinweis: Wir wollen die GPU zum Training nutzen, wenn wir Zugang zu einer haben (auf einer CPU kann das Training mehrere Stunden statt ein paar Minuten dauern). Dazu definieren wir "),ks=o(rt,"CODE",{});var Wa=d(ks);cr=a(Wa,"device"),Wa.forEach(s),ur=a(rt," als Ger\xE4t auf dem wir unser Modell und unsere Batches speichern:"),rt.forEach(s),vn=c(e),h(Ee.$$.fragment,e),$n=c(e),h(De.$$.fragment,e),kn=c(e),R=o(e,"P",{});var at=d(R);hr=a(at,"Wir sind jetzt bereit f\xFCr das Training! Um ein Gef\xFChl daf\xFCr zu bekommen, wann das Training abgeschlossen sein wird, f\xFCgen wir mit der Bibliothek "),zs=o(at,"CODE",{});var Ua=d(zs);fr=a(Ua,"tqdm"),Ua.forEach(s),mr=a(at," einen Fortschrittsbalken \xFCber die Anzahl der Trainingsschritte ein:"),at.forEach(s),zn=c(e),h(ye.$$.fragment,e),jn=c(e),Ke=o(e,"P",{});var La=d(Ke);gr=a(La,"Der Kern der Trainingsschleife sieht \xE4hnlich aus wie in der Einleitung. Da wir keine Berichte angefordert haben, gibt die Trainingsschleife nichts \xFCber die Performance des Modells zur\xFCck. Daf\xFCr m\xFCssen wir eine Evaluationsschleife einf\xFCgen."),La.forEach(s),En=c(e),W=o(e,"H3",{class:!0});var it=d(W);J=o(it,"A",{id:!0,class:!0,href:!0});var Ga=d(J);js=o(Ga,"SPAN",{});var Za=d(js);h(Ae.$$.fragment,Za),Za.forEach(s),Ga.forEach(s),wr=c(it),Es=o(it,"SPAN",{});var Ha=d(Es);br=a(Ha,"Die Evaluationsschleife"),Ha.forEach(s),it.forEach(s),Dn=c(e),y=o(e,"P",{});var ie=d(y);_r=a(ie,"Wie schon zuvor verwenden wir eine Metrik, die von der \u{1F917} Datasets-Bibliothek bereitgestellt wird. Wir haben bereits die Methode "),Ds=o(ie,"CODE",{});var Fa=d(Ds);vr=a(Fa,"metric.compute()"),Fa.forEach(s),$r=a(ie," gesehen, aber Metriken k\xF6nnen auch Batches f\xFCr uns akkumulieren, wenn wir die Vorhersageschleife mit der Methode "),ys=o(ie,"CODE",{});var Na=d(ys);kr=a(Na,"add_batch()"),Na.forEach(s),zr=a(ie," durchlaufen. Sobald wir alle Batches gesammelt haben, k\xF6nnen wir das Endergebnis mit der Methode "),As=o(ie,"CODE",{});var Ia=d(As);jr=a(Ia,"metric.compute()"),Ia.forEach(s),Er=a(ie," ermitteln. So implementierst du all das in eine Evaluationsschleife:"),ie.forEach(s),yn=c(e),h(Te.$$.fragment,e),An=c(e),h(qe.$$.fragment,e),Tn=c(e),Re=o(e,"P",{});var Va=d(Re);Dr=a(Va,"Auch hier werden deine Ergebnisse wegen der Zuf\xE4lligkeit bei der Initialisierung des Modellkopfes und der Datenverteilung etwas anders ausfallen, aber sie sollten in etwa gleich sein."),Va.forEach(s),qn=c(e),h(Y.$$.fragment,e),Sn=c(e),U=o(e,"H3",{class:!0});var lt=d(U);Q=o(lt,"A",{id:!0,class:!0,href:!0});var Ka=d(Q);Ts=o(Ka,"SPAN",{});var Ra=d(Ts);h(Se.$$.fragment,Ra),Ra.forEach(s),Ka.forEach(s),yr=c(lt),qs=o(lt,"SPAN",{});var Ja=d(qs);Ar=a(Ja,"Verbessere deine Trainingsschleife mit \u{1F917} Accelerate"),Ja.forEach(s),lt.forEach(s),Pn=c(e),h(Pe.$$.fragment,e),Cn=c(e),X=o(e,"P",{});var ot=d(X);Tr=a(ot,"Die Trainingsschleife, die wir zuvor definiert haben, funktioniert gut auf einer einzelnen CPU oder GPU. Aber mit der Bibliothek "),Ce=o(ot,"A",{href:!0,rel:!0});var Ya=d(Ce);qr=a(Ya,"\u{1F917} Accelerate"),Ya.forEach(s),Sr=a(ot," k\xF6nnen wir mit wenigen Anpassungen verteiltes Training auf mehreren GPUs oder TPUs implementieren. Beginnend mit der Erstellung der Trainings- und Validierungsdaten, sieht unsere manuelle Trainingsschleife nun folgenderma\xDFen aus:"),ot.forEach(s),Mn=c(e),h(Me.$$.fragment,e),xn=c(e),Je=o(e,"P",{});var Qa=d(Je);Pr=a(Qa,"Und hier sind die \xC4nderungen:"),Qa.forEach(s),On=c(e),h(xe.$$.fragment,e),Bn=c(e),A=o(e,"P",{});var le=d(A);Cr=a(le,"Die erste Zeile, die hinzugef\xFCgt werden muss, ist die Import-Zeile. Die zweite Zeile instanziiert ein "),Ss=o(le,"CODE",{});var Xa=d(Ss);Mr=a(Xa,"Accelerator"),Xa.forEach(s),xr=a(le,"-Objekt, das die Hardware analysiert und die richtige verteilte Umgebung initialisiert. Accelerate k\xFCmmert sich um die Anordnung der Ger\xE4te, du kannst also die Zeilen entfernen, die das Modell auf dem Ger\xE4t platzieren (oder, wenn du das m\xF6chtest, sie so \xE4ndern, dass sie "),Ps=o(le,"CODE",{});var ei=d(Ps);Or=a(ei,"accelerator.device"),ei.forEach(s),Br=a(le," anstelle von "),Cs=o(le,"CODE",{});var si=d(Cs);Wr=a(si,"device"),si.forEach(s),Ur=a(le," verwenden)."),le.forEach(s),Wn=c(e),k=o(e,"P",{});var T=d(k);Lr=a(T,"Der Hauptteil der Arbeit wird dann in der Zeile erledigt, die die Dataloader, das Modell und den Optimierer an "),Ms=o(T,"CODE",{});var ni=d(Ms);Gr=a(ni,"accelerator.prepare()"),ni.forEach(s),Zr=a(T," sendet. Dadurch werden diese Objekte in den richtigen Container verpackt, damit das verteilte Training wie vorgesehen funktioniert. Die verbleibenden \xC4nderungen sind das Entfernen der Zeile, die das Batch auf dem Ger\xE4t mit "),xs=o(T,"CODE",{});var ti=d(xs);Hr=a(ti,"device"),ti.forEach(s),Fr=a(T," ablegt (wenn du das beibehalten willst, kannst du es einfach in "),Os=o(T,"CODE",{});var ri=d(Os);Nr=a(ri,"accelerator.device"),ri.forEach(s),Ir=a(T," \xE4ndern) und das Ersetzen von "),Bs=o(T,"CODE",{});var ai=d(Bs);Vr=a(ai,"loss.backward()"),ai.forEach(s),Kr=a(T," durch "),Ws=o(T,"CODE",{});var ii=d(Ws);Rr=a(ii,"accelerator.backward(loss)"),ii.forEach(s),Jr=a(T,"."),T.forEach(s),Un=c(e),h(ee.$$.fragment,e),Ln=c(e),Ye=o(e,"P",{});var li=d(Ye);Yr=a(li,"Wenn du damit experimentieren m\xF6chtest, siehst du hier, wie die komplette Trainingsschleife mit \u{1F917} Accelerate aussieht:"),li.forEach(s),Gn=c(e),h(Oe.$$.fragment,e),Zn=c(e),se=o(e,"P",{});var dt=d(se);Qr=a(dt,"Wenn dies in das Script "),Us=o(dt,"CODE",{});var oi=d(Us);Xr=a(oi,"train.py"),oi.forEach(s),ea=a(dt," eingef\xFCgt wird, kann das Script auf jeder Art von verteilter Hardware ausgef\xFChrt werden. Um es auf deiner verteilten Hardware auszuprobieren, f\xFChre den folgenden Befehl aus:"),dt.forEach(s),Hn=c(e),h(Be.$$.fragment,e),Fn=c(e),Qe=o(e,"P",{});var di=d(Qe);sa=a(di,"Du wirst dann aufgefordert werden, einige Fragen zu beantworten und die Antworten in eine Konfigurationsdatei zu schreiben, die von diesem Befehl verwendet wird:"),di.forEach(s),Nn=c(e),h(We.$$.fragment,e),In=c(e),Xe=o(e,"P",{});var pi=d(Xe);na=a(pi,"Damit wird das verteilte Training gestartet."),pi.forEach(s),Vn=c(e),ne=o(e,"P",{});var pt=d(ne);ta=a(pt,"Wenn du das in einem Notebook ausprobieren m\xF6chtest (z. B. um es mit TPUs auf Colab zu testen), f\xFCge den Code einfach in eine "),Ls=o(pt,"CODE",{});var ci=d(Ls);ra=a(ci,"training_function()"),ci.forEach(s),aa=a(pt," ein und f\xFChre eine letzte Zelle mit aus:"),pt.forEach(s),Kn=c(e),h(Ue.$$.fragment,e),Rn=c(e),te=o(e,"P",{});var ct=d(te);ia=a(ct,"Weitere Beispiele findest du in dem "),Le=o(ct,"A",{href:!0,rel:!0});var ui=d(Le);la=a(ui,"\u{1F917} Accelerate Repo"),ui.forEach(s),oa=a(ct,"."),ct.forEach(s),this.h()},h(){b(_,"name","hf:doc:metadata"),b(_,"content",JSON.stringify(ji)),b(E,"id","komplettes-training"),b(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(E,"href","#komplettes-training"),b($,"class","relative group"),b(Z,"id","vorbereitung-fr-das-training"),b(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Z,"href","#vorbereitung-fr-das-training"),b(x,"class","relative group"),b(ve,"href","https://arxiv.org/abs/1711.05101"),b(ve,"rel","nofollow"),b(V,"id","die-trainingsschleife"),b(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(V,"href","#die-trainingsschleife"),b(B,"class","relative group"),b(J,"id","die-evaluationsschleife"),b(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(J,"href","#die-evaluationsschleife"),b(W,"class","relative group"),b(Q,"id","verbessere-deine-trainingsschleife-mit-accelerate"),b(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Q,"href","#verbessere-deine-trainingsschleife-mit-accelerate"),b(U,"class","relative group"),b(Ce,"href","https://github.com/huggingface/accelerate"),b(Ce,"rel","nofollow"),b(Le,"href","https://github.com/huggingface/accelerate/tree/main/examples"),b(Le,"rel","nofollow")},m(e,t){n(document.head,_),i(e,z,t),i(e,$,t),n($,E),n(E,M),f(j,M,null),n($,S),n($,L),n(L,ut),i(e,Hs,t),f(oe,e,t),i(e,Fs,t),f(de,e,t),i(e,Ns,t),i(e,G,t),n(G,ht),n(G,ts),n(ts,ft),n(G,mt),i(e,Is,t),f(pe,e,t),i(e,Vs,t),i(e,x,t),n(x,Z),n(Z,rs),f(ce,rs,null),n(x,gt),n(x,as),n(as,wt),i(e,Ks,t),i(e,P,t),n(P,bt),n(P,is),n(is,_t),n(P,vt),n(P,ls),n(ls,$t),n(P,kt),i(e,Rs,t),i(e,C,t),n(C,O),n(O,zt),n(O,os),n(os,jt),n(O,Et),n(O,ds),n(ds,Dt),n(O,yt),n(C,At),n(C,q),n(q,Tt),n(q,ps),n(ps,qt),n(q,St),n(q,cs),n(cs,Pt),n(q,Ct),n(q,us),n(us,Mt),n(q,xt),n(C,Ot),n(C,hs),n(hs,Bt),i(e,Js,t),i(e,H,t),n(H,Wt),n(H,fs),n(fs,Ut),n(H,Lt),i(e,Ys,t),f(ue,e,t),i(e,Qs,t),i(e,He,t),n(He,Gt),i(e,Xs,t),f(he,e,t),i(e,en,t),i(e,Fe,t),n(Fe,Zt),i(e,sn,t),f(fe,e,t),i(e,nn,t),i(e,Ne,t),n(Ne,Ht),i(e,tn,t),f(me,e,t),i(e,rn,t),f(ge,e,t),i(e,an,t),i(e,F,t),n(F,Ft),n(F,ms),n(ms,Nt),n(F,It),i(e,ln,t),i(e,Ie,t),n(Ie,Vt),i(e,on,t),f(we,e,t),i(e,dn,t),i(e,Ve,t),n(Ve,Kt),i(e,pn,t),f(be,e,t),i(e,cn,t),f(_e,e,t),i(e,un,t),i(e,N,t),n(N,Rt),n(N,gs),n(gs,Jt),n(N,Yt),i(e,hn,t),i(e,D,t),n(D,Qt),n(D,ws),n(ws,Xt),n(D,er),n(D,bs),n(bs,sr),n(D,nr),n(D,ve),n(ve,tr),n(D,rr),i(e,fn,t),f($e,e,t),i(e,mn,t),i(e,I,t),n(I,ar),n(I,_s),n(_s,ir),n(I,lr),i(e,gn,t),f(ke,e,t),i(e,wn,t),f(ze,e,t),i(e,bn,t),i(e,B,t),n(B,V),n(V,vs),f(je,vs,null),n(B,or),n(B,$s),n($s,dr),i(e,_n,t),i(e,K,t),n(K,pr),n(K,ks),n(ks,cr),n(K,ur),i(e,vn,t),f(Ee,e,t),i(e,$n,t),f(De,e,t),i(e,kn,t),i(e,R,t),n(R,hr),n(R,zs),n(zs,fr),n(R,mr),i(e,zn,t),f(ye,e,t),i(e,jn,t),i(e,Ke,t),n(Ke,gr),i(e,En,t),i(e,W,t),n(W,J),n(J,js),f(Ae,js,null),n(W,wr),n(W,Es),n(Es,br),i(e,Dn,t),i(e,y,t),n(y,_r),n(y,Ds),n(Ds,vr),n(y,$r),n(y,ys),n(ys,kr),n(y,zr),n(y,As),n(As,jr),n(y,Er),i(e,yn,t),f(Te,e,t),i(e,An,t),f(qe,e,t),i(e,Tn,t),i(e,Re,t),n(Re,Dr),i(e,qn,t),f(Y,e,t),i(e,Sn,t),i(e,U,t),n(U,Q),n(Q,Ts),f(Se,Ts,null),n(U,yr),n(U,qs),n(qs,Ar),i(e,Pn,t),f(Pe,e,t),i(e,Cn,t),i(e,X,t),n(X,Tr),n(X,Ce),n(Ce,qr),n(X,Sr),i(e,Mn,t),f(Me,e,t),i(e,xn,t),i(e,Je,t),n(Je,Pr),i(e,On,t),f(xe,e,t),i(e,Bn,t),i(e,A,t),n(A,Cr),n(A,Ss),n(Ss,Mr),n(A,xr),n(A,Ps),n(Ps,Or),n(A,Br),n(A,Cs),n(Cs,Wr),n(A,Ur),i(e,Wn,t),i(e,k,t),n(k,Lr),n(k,Ms),n(Ms,Gr),n(k,Zr),n(k,xs),n(xs,Hr),n(k,Fr),n(k,Os),n(Os,Nr),n(k,Ir),n(k,Bs),n(Bs,Vr),n(k,Kr),n(k,Ws),n(Ws,Rr),n(k,Jr),i(e,Un,t),f(ee,e,t),i(e,Ln,t),i(e,Ye,t),n(Ye,Yr),i(e,Gn,t),f(Oe,e,t),i(e,Zn,t),i(e,se,t),n(se,Qr),n(se,Us),n(Us,Xr),n(se,ea),i(e,Hn,t),f(Be,e,t),i(e,Fn,t),i(e,Qe,t),n(Qe,sa),i(e,Nn,t),f(We,e,t),i(e,In,t),i(e,Xe,t),n(Xe,na),i(e,Vn,t),i(e,ne,t),n(ne,ta),n(ne,Ls),n(Ls,ra),n(ne,aa),i(e,Kn,t),f(Ue,e,t),i(e,Rn,t),i(e,te,t),n(te,ia),n(te,Le),n(Le,la),n(te,oa),Jn=!0},p(e,[t]){const Ge={};t&2&&(Ge.$$scope={dirty:t,ctx:e}),Y.$set(Ge);const Gs={};t&2&&(Gs.$$scope={dirty:t,ctx:e}),ee.$set(Gs)},i(e){Jn||(m(j.$$.fragment,e),m(oe.$$.fragment,e),m(de.$$.fragment,e),m(pe.$$.fragment,e),m(ce.$$.fragment,e),m(ue.$$.fragment,e),m(he.$$.fragment,e),m(fe.$$.fragment,e),m(me.$$.fragment,e),m(ge.$$.fragment,e),m(we.$$.fragment,e),m(be.$$.fragment,e),m(_e.$$.fragment,e),m($e.$$.fragment,e),m(ke.$$.fragment,e),m(ze.$$.fragment,e),m(je.$$.fragment,e),m(Ee.$$.fragment,e),m(De.$$.fragment,e),m(ye.$$.fragment,e),m(Ae.$$.fragment,e),m(Te.$$.fragment,e),m(qe.$$.fragment,e),m(Y.$$.fragment,e),m(Se.$$.fragment,e),m(Pe.$$.fragment,e),m(Me.$$.fragment,e),m(xe.$$.fragment,e),m(ee.$$.fragment,e),m(Oe.$$.fragment,e),m(Be.$$.fragment,e),m(We.$$.fragment,e),m(Ue.$$.fragment,e),Jn=!0)},o(e){g(j.$$.fragment,e),g(oe.$$.fragment,e),g(de.$$.fragment,e),g(pe.$$.fragment,e),g(ce.$$.fragment,e),g(ue.$$.fragment,e),g(he.$$.fragment,e),g(fe.$$.fragment,e),g(me.$$.fragment,e),g(ge.$$.fragment,e),g(we.$$.fragment,e),g(be.$$.fragment,e),g(_e.$$.fragment,e),g($e.$$.fragment,e),g(ke.$$.fragment,e),g(ze.$$.fragment,e),g(je.$$.fragment,e),g(Ee.$$.fragment,e),g(De.$$.fragment,e),g(ye.$$.fragment,e),g(Ae.$$.fragment,e),g(Te.$$.fragment,e),g(qe.$$.fragment,e),g(Y.$$.fragment,e),g(Se.$$.fragment,e),g(Pe.$$.fragment,e),g(Me.$$.fragment,e),g(xe.$$.fragment,e),g(ee.$$.fragment,e),g(Oe.$$.fragment,e),g(Be.$$.fragment,e),g(We.$$.fragment,e),g(Ue.$$.fragment,e),Jn=!1},d(e){s(_),e&&s(z),e&&s($),w(j),e&&s(Hs),w(oe,e),e&&s(Fs),w(de,e),e&&s(Ns),e&&s(G),e&&s(Is),w(pe,e),e&&s(Vs),e&&s(x),w(ce),e&&s(Ks),e&&s(P),e&&s(Rs),e&&s(C),e&&s(Js),e&&s(H),e&&s(Ys),w(ue,e),e&&s(Qs),e&&s(He),e&&s(Xs),w(he,e),e&&s(en),e&&s(Fe),e&&s(sn),w(fe,e),e&&s(nn),e&&s(Ne),e&&s(tn),w(me,e),e&&s(rn),w(ge,e),e&&s(an),e&&s(F),e&&s(ln),e&&s(Ie),e&&s(on),w(we,e),e&&s(dn),e&&s(Ve),e&&s(pn),w(be,e),e&&s(cn),w(_e,e),e&&s(un),e&&s(N),e&&s(hn),e&&s(D),e&&s(fn),w($e,e),e&&s(mn),e&&s(I),e&&s(gn),w(ke,e),e&&s(wn),w(ze,e),e&&s(bn),e&&s(B),w(je),e&&s(_n),e&&s(K),e&&s(vn),w(Ee,e),e&&s($n),w(De,e),e&&s(kn),e&&s(R),e&&s(zn),w(ye,e),e&&s(jn),e&&s(Ke),e&&s(En),e&&s(W),w(Ae),e&&s(Dn),e&&s(y),e&&s(yn),w(Te,e),e&&s(An),w(qe,e),e&&s(Tn),e&&s(Re),e&&s(qn),w(Y,e),e&&s(Sn),e&&s(U),w(Se),e&&s(Pn),w(Pe,e),e&&s(Cn),e&&s(X),e&&s(Mn),w(Me,e),e&&s(xn),e&&s(Je),e&&s(On),w(xe,e),e&&s(Bn),e&&s(A),e&&s(Wn),e&&s(k),e&&s(Un),w(ee,e),e&&s(Ln),e&&s(Ye),e&&s(Gn),w(Oe,e),e&&s(Zn),e&&s(se),e&&s(Hn),w(Be,e),e&&s(Fn),e&&s(Qe),e&&s(Nn),w(We,e),e&&s(In),e&&s(Xe),e&&s(Vn),e&&s(ne),e&&s(Kn),w(Ue,e),e&&s(Rn),e&&s(te)}}}const ji={local:"komplettes-training",sections:[{local:"vorbereitung-fr-das-training",title:"Vorbereitung f\xFCr das Training"},{local:"die-trainingsschleife",title:"Die Trainingsschleife"},{local:"die-evaluationsschleife",title:"Die Evaluationsschleife"},{local:"verbessere-deine-trainingsschleife-mit-accelerate",title:"Verbessere deine Trainingsschleife mit \u{1F917} Accelerate"}],title:"Komplettes Training"};function Ei(Ze){return _i(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qi extends mi{constructor(_){super();gi(this,_,Ei,zi,wi,{})}}export{qi as default,ji as metadata};
