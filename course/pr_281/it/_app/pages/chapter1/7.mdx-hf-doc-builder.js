import{S as xe,i as Le,s as Se,e as a,k as u,w as ye,t as s,M as Be,c as l,d as t,m as p,a as i,x as Te,h as d,b as n,G as o,g as c,y as Ie,L as Re,q as ke,o as Me,B as Pe,v as Ne}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ce}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ue}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Ye(ce){let f,R,h,_,k,z,D,M,F,N,E,C,v,Q,P,V,W,U,g,X,w,Z,ee,Y,T,te,G,I,oe,H,m,x,$,ae,le,L,b,ie,re,S,A,ne,se,B,y,de,J;return z=new Ue({}),E=new Ce({props:{id:"0_4KEb08xrE"}}),{c(){f=a("meta"),R=u(),h=a("h1"),_=a("a"),k=a("span"),ye(z.$$.fragment),D=u(),M=a("span"),F=s("Modelli sequence-to-sequence"),N=u(),ye(E.$$.fragment),C=u(),v=a("p"),Q=s("I modelli encoder-decoder (detti anche modelli "),P=a("em"),V=s("sequence-to-sequence"),W=s(") utilizzano entrambi i componenti dell\u2019architettura Transformer. Ad ogni passaggio, gli attention layer dell\u2019encoder hanno accesso a tutte le parole della frase iniziale, mentre gli attention layer del decoder possono solo accedere alle parole che precedono linearmente una data parola nell\u2019input."),U=u(),g=a("p"),X=s("Il pre-addestramento di questi modelli pu\xF2 essere fatto utilizzando gli obiettivi dei modelli encoder o decoder, anche se solitamente include un livello di complessit\xE0 maggiore. Ad esempio, "),w=a("a"),Z=s("T5"),ee=s(" \xE8 pre-addestrato rimpiazzando porzioni random di testo (che possono contenere pi\xF9 di una parola) con una speciale mask word, con l\u2019obiettivo di predirre il testo rimpiazzato dalla mask word stessa."),Y=u(),T=a("p"),te=s("I modelli sequence-to-sequence sono pi\xF9 adatti ai compiti che hanno a che fare con la generazione di nuove frasi sulla base di un input preciso, come il riassunto, la traduzione, o la generazione di risposte a domande."),G=u(),I=a("p"),oe=s("Tra i rappresentanti di questa famiglia di modelli ci sono:"),H=u(),m=a("ul"),x=a("li"),$=a("a"),ae=s("BART"),le=u(),L=a("li"),b=a("a"),ie=s("mBART"),re=u(),S=a("li"),A=a("a"),ne=s("Marian"),se=u(),B=a("li"),y=a("a"),de=s("T5"),this.h()},l(e){const r=Be('[data-svelte="svelte-1phssyn"]',document.head);f=l(r,"META",{name:!0,content:!0}),r.forEach(t),R=p(e),h=l(e,"H1",{class:!0});var K=i(h);_=l(K,"A",{id:!0,class:!0,href:!0});var me=i(_);k=l(me,"SPAN",{});var ue=i(k);Te(z.$$.fragment,ue),ue.forEach(t),me.forEach(t),D=p(K),M=l(K,"SPAN",{});var pe=i(M);F=d(pe,"Modelli sequence-to-sequence"),pe.forEach(t),K.forEach(t),N=p(e),Te(E.$$.fragment,e),C=p(e),v=l(e,"P",{});var O=i(v);Q=d(O,"I modelli encoder-decoder (detti anche modelli "),P=l(O,"EM",{});var fe=i(P);V=d(fe,"sequence-to-sequence"),fe.forEach(t),W=d(O,") utilizzano entrambi i componenti dell\u2019architettura Transformer. Ad ogni passaggio, gli attention layer dell\u2019encoder hanno accesso a tutte le parole della frase iniziale, mentre gli attention layer del decoder possono solo accedere alle parole che precedono linearmente una data parola nell\u2019input."),O.forEach(t),U=p(e),g=l(e,"P",{});var j=i(g);X=d(j,"Il pre-addestramento di questi modelli pu\xF2 essere fatto utilizzando gli obiettivi dei modelli encoder o decoder, anche se solitamente include un livello di complessit\xE0 maggiore. Ad esempio, "),w=l(j,"A",{href:!0,rel:!0});var he=i(w);Z=d(he,"T5"),he.forEach(t),ee=d(j," \xE8 pre-addestrato rimpiazzando porzioni random di testo (che possono contenere pi\xF9 di una parola) con una speciale mask word, con l\u2019obiettivo di predirre il testo rimpiazzato dalla mask word stessa."),j.forEach(t),Y=p(e),T=l(e,"P",{});var _e=i(T);te=d(_e,"I modelli sequence-to-sequence sono pi\xF9 adatti ai compiti che hanno a che fare con la generazione di nuove frasi sulla base di un input preciso, come il riassunto, la traduzione, o la generazione di risposte a domande."),_e.forEach(t),G=p(e),I=l(e,"P",{});var ve=i(I);oe=d(ve,"Tra i rappresentanti di questa famiglia di modelli ci sono:"),ve.forEach(t),H=p(e),m=l(e,"UL",{});var q=i(m);x=l(q,"LI",{});var ge=i(x);$=l(ge,"A",{href:!0,rel:!0});var qe=i($);ae=d(qe,"BART"),qe.forEach(t),ge.forEach(t),le=p(q),L=l(q,"LI",{});var ze=i(L);b=l(ze,"A",{href:!0,rel:!0});var Ee=i(b);ie=d(Ee,"mBART"),Ee.forEach(t),ze.forEach(t),re=p(q),S=l(q,"LI",{});var we=i(S);A=l(we,"A",{href:!0,rel:!0});var $e=i(A);ne=d($e,"Marian"),$e.forEach(t),we.forEach(t),se=p(q),B=l(q,"LI",{});var be=i(B);y=l(be,"A",{href:!0,rel:!0});var Ae=i(y);de=d(Ae,"T5"),Ae.forEach(t),be.forEach(t),q.forEach(t),this.h()},h(){n(f,"name","hf:doc:metadata"),n(f,"content",JSON.stringify(Ge)),n(_,"id","modelli-sequencetosequence"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#modelli-sequencetosequence"),n(h,"class","relative group"),n(w,"href","https://huggingface.co/t5-base"),n(w,"rel","nofollow"),n($,"href","https://huggingface.co/transformers/model_doc/bart.html"),n($,"rel","nofollow"),n(b,"href","https://huggingface.co/transformers/model_doc/mbart.html"),n(b,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/marian.html"),n(A,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/t5.html"),n(y,"rel","nofollow")},m(e,r){o(document.head,f),c(e,R,r),c(e,h,r),o(h,_),o(_,k),Ie(z,k,null),o(h,D),o(h,M),o(M,F),c(e,N,r),Ie(E,e,r),c(e,C,r),c(e,v,r),o(v,Q),o(v,P),o(P,V),o(v,W),c(e,U,r),c(e,g,r),o(g,X),o(g,w),o(w,Z),o(g,ee),c(e,Y,r),c(e,T,r),o(T,te),c(e,G,r),c(e,I,r),o(I,oe),c(e,H,r),c(e,m,r),o(m,x),o(x,$),o($,ae),o(m,le),o(m,L),o(L,b),o(b,ie),o(m,re),o(m,S),o(S,A),o(A,ne),o(m,se),o(m,B),o(B,y),o(y,de),J=!0},p:Re,i(e){J||(ke(z.$$.fragment,e),ke(E.$$.fragment,e),J=!0)},o(e){Me(z.$$.fragment,e),Me(E.$$.fragment,e),J=!1},d(e){t(f),e&&t(R),e&&t(h),Pe(z),e&&t(N),Pe(E,e),e&&t(C),e&&t(v),e&&t(U),e&&t(g),e&&t(Y),e&&t(T),e&&t(G),e&&t(I),e&&t(H),e&&t(m)}}}const Ge={local:"modelli-sequencetosequence",title:"Modelli sequence-to-sequence"};function He(ce){return Ne(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class je extends xe{constructor(f){super();Le(this,f,He,Ye,Se,{})}}export{je as default,Ge as metadata};
