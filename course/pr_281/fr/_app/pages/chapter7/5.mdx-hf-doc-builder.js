import{S as l1,i as o1,s as i1,e as r,t as a,k as m,w as g,c as l,a as o,h as n,d as t,m as c,x as b,b as f,g as i,G as e,y as q,q as v,o as _,B as j,U as Zb,M as u1,V as e1,N as Io,p as Go,v as p1,n as Uo}from"../../chunks/vendor-hf-doc-builder.js";import{T as nr}from"../../chunks/Tip-hf-doc-builder.js";import{Y as xv}from"../../chunks/Youtube-hf-doc-builder.js";import{I as St}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as a1}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as m1}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function c1(te){let d,E;return d=new a1({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"}]}}),{c(){g(d.$$.fragment)},l(h){b(d.$$.fragment,h)},m(h,y){q(d,h,y),E=!0},i(h){E||(v(d.$$.fragment,h),E=!0)},o(h){_(d.$$.fragment,h),E=!1},d(h){j(d,h)}}}function d1(te){let d,E;return d=new a1({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"}]}}),{c(){g(d.$$.fragment)},l(h){b(d.$$.fragment,h)},m(h,y){q(d,h,y),E=!0},i(h){E||(v(d.$$.fragment,h),E=!0)},o(h){_(d.$$.fragment,h),E=!1},d(h){j(d,h)}}}function h1(te){let d,E,h,y,L,k,T,M,P,S,A;return{c(){d=r("p"),E=a("\u270F\uFE0F "),h=r("strong"),y=a("Essayez !"),L=a(" Changez la graine al\xE9atoire dans la commande "),k=r("code"),T=a("Dataset.shuffle()"),M=a(" pour explorer d\u2019autres critiques dans le corpus. Si vous parlez espagnol, jetez un coup d\u2019\u0153il  \xE0 certaines des critiques dans "),P=r("code"),S=a("spanish_dataset"),A=a(" pour voir si les titres semblent aussi \xEAtre des r\xE9sum\xE9s raisonnables.")},l(O){d=l(O,"P",{});var w=o(d);E=n(w,"\u270F\uFE0F "),h=l(w,"STRONG",{});var I=o(h);y=n(I,"Essayez !"),I.forEach(t),L=n(w," Changez la graine al\xE9atoire dans la commande "),k=l(w,"CODE",{});var D=o(k);T=n(D,"Dataset.shuffle()"),D.forEach(t),M=n(w," pour explorer d\u2019autres critiques dans le corpus. Si vous parlez espagnol, jetez un coup d\u2019\u0153il  \xE0 certaines des critiques dans "),P=l(w,"CODE",{});var N=o(P);S=n(N,"spanish_dataset"),N.forEach(t),A=n(w," pour voir si les titres semblent aussi \xEAtre des r\xE9sum\xE9s raisonnables."),w.forEach(t)},m(O,w){i(O,d,w),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M),e(d,P),e(P,S),e(d,A)},d(O){O&&t(d)}}}function f1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I;return{c(){d=r("p"),E=a("\u270F\uFE0F "),h=r("strong"),y=a("Essayez !"),L=a(" Une fois que vous aurez termin\xE9 cette section, comparez le mT5 \xE0 mBART en "),k=r("em"),T=a("finetunant"),M=a(" ce dernier avec les m\xEAmes techniques. Pour des points bonus, vous pouvez aussi essayer de "),P=r("em"),S=a("finetuner"),A=a(" le T5 uniquement sur les critiques anglaises. Puisque le T5 a un pr\xE9fixe sp\xE9cial, vous devrez ajouter "),O=r("code"),w=a("summarize:"),I=a(" aux entr\xE9es dans les \xE9tapes de pr\xE9traitement ci-dessous.")},l(D){d=l(D,"P",{});var N=o(d);E=n(N,"\u270F\uFE0F "),h=l(N,"STRONG",{});var oe=o(h);y=n(oe,"Essayez !"),oe.forEach(t),L=n(N," Une fois que vous aurez termin\xE9 cette section, comparez le mT5 \xE0 mBART en "),k=l(N,"EM",{});var W=o(k);T=n(W,"finetunant"),W.forEach(t),M=n(N," ce dernier avec les m\xEAmes techniques. Pour des points bonus, vous pouvez aussi essayer de "),P=l(N,"EM",{});var re=o(P);S=n(re,"finetuner"),re.forEach(t),A=n(N," le T5 uniquement sur les critiques anglaises. Puisque le T5 a un pr\xE9fixe sp\xE9cial, vous devrez ajouter "),O=l(N,"CODE",{});var F=o(O);w=n(F,"summarize:"),F.forEach(t),I=n(N," aux entr\xE9es dans les \xE9tapes de pr\xE9traitement ci-dessous."),N.forEach(t)},m(D,N){i(D,d,N),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M),e(d,P),e(P,S),e(d,A),e(d,O),e(O,w),e(d,I)},d(D){D&&t(d)}}}function v1(te){let d,E,h,y,L;return{c(){d=r("p"),E=a("\u{1F4A1} Aux premiers stades de vos projets de NLP, une bonne pratique consiste \xE0 entra\xEEner une classe de \xAB petits \xBB mod\xE8les sur un petit \xE9chantillon de donn\xE9es. Cela vous permet de d\xE9boguer et d\u2019it\xE9rer plus rapidement vers un flux de travail de bout en bout. Une fois que vous avez confiance dans les r\xE9sultats, vous pouvez toujours faire \xE9voluer le mod\xE8le en changeant simplement le "),h=r("em"),y=a("checkpoint"),L=a(" du mod\xE8le !")},l(k){d=l(k,"P",{});var T=o(d);E=n(T,"\u{1F4A1} Aux premiers stades de vos projets de NLP, une bonne pratique consiste \xE0 entra\xEEner une classe de \xAB petits \xBB mod\xE8les sur un petit \xE9chantillon de donn\xE9es. Cela vous permet de d\xE9boguer et d\u2019it\xE9rer plus rapidement vers un flux de travail de bout en bout. Une fois que vous avez confiance dans les r\xE9sultats, vous pouvez toujours faire \xE9voluer le mod\xE8le en changeant simplement le "),h=l(T,"EM",{});var M=o(h);y=n(M,"checkpoint"),M.forEach(t),L=n(T," du mod\xE8le !"),T.forEach(t)},m(k,T){i(k,d,T),e(d,E),e(d,h),e(h,y),e(d,L)},d(k){k&&t(d)}}}function _1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F;return{c(){d=r("p"),E=a("\u{1F4A1} Vous avez peut-\xEAtre remarqu\xE9 que nous avons utilis\xE9 "),h=r("code"),y=a("batched=True"),L=a(" dans notre fonction "),k=r("code"),T=a("Dataset.map()"),M=a(" ci-dessus. Cela permet de coder les exemples par lots de 1 000 (par d\xE9faut) et d\u2019utiliser les capacit\xE9s de "),P=r("em"),S=a("multithreading"),A=a(" des "),O=r("em"),w=a("tokenizers"),I=a(" rapides de \u{1F917} "),D=r("em"),N=a("Transformers"),oe=a(". Lorsque cela est possible, essayez d\u2019utiliser "),W=r("code"),re=a("batched=True"),F=a(" pour tirer le meilleur parti de votre pr\xE9traitement !")},l(ee){d=l(ee,"P",{});var G=o(d);E=n(G,"\u{1F4A1} Vous avez peut-\xEAtre remarqu\xE9 que nous avons utilis\xE9 "),h=l(G,"CODE",{});var he=o(h);y=n(he,"batched=True"),he.forEach(t),L=n(G," dans notre fonction "),k=l(G,"CODE",{});var K=o(k);T=n(K,"Dataset.map()"),K.forEach(t),M=n(G," ci-dessus. Cela permet de coder les exemples par lots de 1 000 (par d\xE9faut) et d\u2019utiliser les capacit\xE9s de "),P=l(G,"EM",{});var ie=o(P);S=n(ie,"multithreading"),ie.forEach(t),A=n(G," des "),O=l(G,"EM",{});var ne=o(O);w=n(ne,"tokenizers"),ne.forEach(t),I=n(G," rapides de \u{1F917} "),D=l(G,"EM",{});var Q=o(D);N=n(Q,"Transformers"),Q.forEach(t),oe=n(G,". Lorsque cela est possible, essayez d\u2019utiliser "),W=l(G,"CODE",{});var R=o(W);re=n(R,"batched=True"),R.forEach(t),F=n(G," pour tirer le meilleur parti de votre pr\xE9traitement !"),G.forEach(t)},m(ee,G){i(ee,d,G),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M),e(d,P),e(P,S),e(d,A),e(d,O),e(O,w),e(d,I),e(d,D),e(D,N),e(d,oe),e(d,W),e(W,re),e(d,F)},d(ee){ee&&t(d)}}}function g1(te){let d,E,h,y,L,k,T;return{c(){d=r("p"),E=a("\u{1F64B} Ne vous inqui\xE9tez pas si c\u2019est la premi\xE8re fois que vous entendez parler de pr\xE9cision et de rappel. Nous allons parcourir ensemble quelques exemples explicites pour que tout soit clair. Ces m\xE9triques sont g\xE9n\xE9ralement rencontr\xE9es dans les t\xE2ches de classification, donc si vous voulez comprendre comment la pr\xE9cision et le rappel sont d\xE9finis dans ce contexte, nous vous recommandons de consulter les "),h=r("a"),y=a("guides de "),L=r("code"),k=a("scikit-learn"),T=a("."),this.h()},l(M){d=l(M,"P",{});var P=o(d);E=n(P,"\u{1F64B} Ne vous inqui\xE9tez pas si c\u2019est la premi\xE8re fois que vous entendez parler de pr\xE9cision et de rappel. Nous allons parcourir ensemble quelques exemples explicites pour que tout soit clair. Ces m\xE9triques sont g\xE9n\xE9ralement rencontr\xE9es dans les t\xE2ches de classification, donc si vous voulez comprendre comment la pr\xE9cision et le rappel sont d\xE9finis dans ce contexte, nous vous recommandons de consulter les "),h=l(P,"A",{href:!0,rel:!0});var S=o(h);y=n(S,"guides de "),L=l(S,"CODE",{});var A=o(L);k=n(A,"scikit-learn"),A.forEach(t),S.forEach(t),T=n(P,"."),P.forEach(t),this.h()},h(){f(h,"href","https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"),f(h,"rel","nofollow")},m(M,P){i(M,d,P),e(d,E),e(d,h),e(h,y),e(h,L),e(L,k),e(d,T)},d(M){M&&t(d)}}}function b1(te){let d,E,h,y,L,k,T,M;return{c(){d=r("p"),E=a("\u270F\uFE0F "),h=r("strong"),y=a("Essayez !"),L=a(" Cr\xE9ez votre propre exemple de r\xE9sum\xE9 g\xE9n\xE9r\xE9 et de r\xE9f\xE9rence et voyez si les scores ROUGE obtenus correspondent \xE0 un calcul manuel bas\xE9 sur les formules de pr\xE9cision et de rappel. Pour des points bonus, divisez le texte en bigrammes et comparez la pr\xE9cision et le rappel pour la m\xE9trique "),k=r("code"),T=a("rouge2"),M=a(".")},l(P){d=l(P,"P",{});var S=o(d);E=n(S,"\u270F\uFE0F "),h=l(S,"STRONG",{});var A=o(h);y=n(A,"Essayez !"),A.forEach(t),L=n(S," Cr\xE9ez votre propre exemple de r\xE9sum\xE9 g\xE9n\xE9r\xE9 et de r\xE9f\xE9rence et voyez si les scores ROUGE obtenus correspondent \xE0 un calcul manuel bas\xE9 sur les formules de pr\xE9cision et de rappel. Pour des points bonus, divisez le texte en bigrammes et comparez la pr\xE9cision et le rappel pour la m\xE9trique "),k=l(S,"CODE",{});var O=o(k);T=n(O,"rouge2"),O.forEach(t),M=n(S,"."),S.forEach(t)},m(P,S){i(P,d,S),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M)},d(P){P&&t(d)}}}function q1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q;return y=new St({}),ne=new C({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){d=r("h2"),E=r("a"),h=r("span"),g(y.$$.fragment),L=m(),k=r("span"),T=r("i"),M=a("Finetuning"),P=a(" de mT5 avec Keras"),S=m(),A=r("p"),O=a("Le "),w=r("em"),I=a("finetuning"),D=a(" d\u2019un mod\xE8le pour le r\xE9sum\xE9 est tr\xE8s similaire aux autres t\xE2ches que nous avons couvertes dans ce chapitre. La premi\xE8re chose \xE0 faire est de charger le mod\xE8le pr\xE9-entra\xEEn\xE9 \xE0 partir du "),N=r("em"),oe=a("checkpoint"),W=m(),re=r("code"),F=a("mt5-small"),ee=a(". Puisque la compression est une t\xE2che de s\xE9quence \xE0 s\xE9quence, nous pouvons charger le mod\xE8le avec la classe "),G=r("code"),he=a("TFAutoModelForSeq2SeqLM"),K=a(", qui t\xE9l\xE9chargera automatiquement et mettra en cache les poids :"),ie=m(),g(ne.$$.fragment),this.h()},l(R){d=l(R,"H2",{class:!0});var se=o(d);E=l(se,"A",{id:!0,class:!0,href:!0});var B=o(E);h=l(B,"SPAN",{});var ae=o(h);b(y.$$.fragment,ae),ae.forEach(t),B.forEach(t),L=c(se),k=l(se,"SPAN",{});var Y=o(k);T=l(Y,"I",{});var ge=o(T);M=n(ge,"Finetuning"),ge.forEach(t),P=n(Y," de mT5 avec Keras"),Y.forEach(t),se.forEach(t),S=c(R),A=l(R,"P",{});var Z=o(A);O=n(Z,"Le "),w=l(Z,"EM",{});var ke=o(w);I=n(ke,"finetuning"),ke.forEach(t),D=n(Z," d\u2019un mod\xE8le pour le r\xE9sum\xE9 est tr\xE8s similaire aux autres t\xE2ches que nous avons couvertes dans ce chapitre. La premi\xE8re chose \xE0 faire est de charger le mod\xE8le pr\xE9-entra\xEEn\xE9 \xE0 partir du "),N=l(Z,"EM",{});var me=o(N);oe=n(me,"checkpoint"),me.forEach(t),W=c(Z),re=l(Z,"CODE",{});var le=o(re);F=n(le,"mt5-small"),le.forEach(t),ee=n(Z,". Puisque la compression est une t\xE2che de s\xE9quence \xE0 s\xE9quence, nous pouvons charger le mod\xE8le avec la classe "),G=l(Z,"CODE",{});var fe=o(G);he=n(fe,"TFAutoModelForSeq2SeqLM"),fe.forEach(t),K=n(Z,", qui t\xE9l\xE9chargera automatiquement et mettra en cache les poids :"),Z.forEach(t),ie=c(R),b(ne.$$.fragment,R),this.h()},h(){f(E,"id","ifinetuningi-de-mt5-avec-keras"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#ifinetuningi-de-mt5-avec-keras"),f(d,"class","relative group")},m(R,se){i(R,d,se),e(d,E),e(E,h),q(y,h,null),e(d,L),e(d,k),e(k,T),e(T,M),e(k,P),i(R,S,se),i(R,A,se),e(A,O),e(A,w),e(w,I),e(A,D),e(A,N),e(N,oe),e(A,W),e(A,re),e(re,F),e(A,ee),e(A,G),e(G,he),e(A,K),i(R,ie,se),q(ne,R,se),Q=!0},i(R){Q||(v(y.$$.fragment,R),v(ne.$$.fragment,R),Q=!0)},o(R){_(y.$$.fragment,R),_(ne.$$.fragment,R),Q=!1},d(R){R&&t(d),j(y),R&&t(S),R&&t(A),R&&t(ie),j(ne,R)}}}function j1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se;return y=new St({}),R=new C({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){d=r("h2"),E=r("a"),h=r("span"),g(y.$$.fragment),L=m(),k=r("span"),T=r("i"),M=a("Finetuning"),P=a(" de mT5 avec l'API "),S=r("code"),A=a("Trainer"),O=m(),w=r("p"),I=a("Le "),D=r("em"),N=a("finetuning"),oe=a(" d\u2019un mod\xE8le pour le r\xE9sum\xE9 est tr\xE8s similaire aux autres t\xE2ches que nous avons couvertes dans ce chapitre. La premi\xE8re chose \xE0 faire est de charger le mod\xE8le pr\xE9-entra\xEEn\xE9 \xE0 partir du "),W=r("em"),re=a("checkpoint"),F=m(),ee=r("code"),G=a("mt5-small"),he=a(". Puisque la compression est une t\xE2che de s\xE9quence \xE0 s\xE9quence, nous pouvons charger le mod\xE8le avec la classe "),K=r("code"),ie=a("AutoModelForSeq2SeqLM"),ne=a(", qui t\xE9l\xE9chargera automatiquement et mettra en cache les poids :"),Q=m(),g(R.$$.fragment),this.h()},l(B){d=l(B,"H2",{class:!0});var ae=o(d);E=l(ae,"A",{id:!0,class:!0,href:!0});var Y=o(E);h=l(Y,"SPAN",{});var ge=o(h);b(y.$$.fragment,ge),ge.forEach(t),Y.forEach(t),L=c(ae),k=l(ae,"SPAN",{});var Z=o(k);T=l(Z,"I",{});var ke=o(T);M=n(ke,"Finetuning"),ke.forEach(t),P=n(Z," de mT5 avec l'API "),S=l(Z,"CODE",{});var me=o(S);A=n(me,"Trainer"),me.forEach(t),Z.forEach(t),ae.forEach(t),O=c(B),w=l(B,"P",{});var le=o(w);I=n(le,"Le "),D=l(le,"EM",{});var fe=o(D);N=n(fe,"finetuning"),fe.forEach(t),oe=n(le," d\u2019un mod\xE8le pour le r\xE9sum\xE9 est tr\xE8s similaire aux autres t\xE2ches que nous avons couvertes dans ce chapitre. La premi\xE8re chose \xE0 faire est de charger le mod\xE8le pr\xE9-entra\xEEn\xE9 \xE0 partir du "),W=l(le,"EM",{});var ce=o(W);re=n(ce,"checkpoint"),ce.forEach(t),F=c(le),ee=l(le,"CODE",{});var H=o(ee);G=n(H,"mt5-small"),H.forEach(t),he=n(le,". Puisque la compression est une t\xE2che de s\xE9quence \xE0 s\xE9quence, nous pouvons charger le mod\xE8le avec la classe "),K=l(le,"CODE",{});var Ce=o(K);ie=n(Ce,"AutoModelForSeq2SeqLM"),Ce.forEach(t),ne=n(le,", qui t\xE9l\xE9chargera automatiquement et mettra en cache les poids :"),le.forEach(t),Q=c(B),b(R.$$.fragment,B),this.h()},h(){f(E,"id","ifinetuningi-de-mt5-avec-lapi-trainer"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#ifinetuningi-de-mt5-avec-lapi-trainer"),f(d,"class","relative group")},m(B,ae){i(B,d,ae),e(d,E),e(E,h),q(y,h,null),e(d,L),e(d,k),e(k,T),e(T,M),e(k,P),e(k,S),e(S,A),i(B,O,ae),i(B,w,ae),e(w,I),e(w,D),e(D,N),e(w,oe),e(w,W),e(W,re),e(w,F),e(w,ee),e(ee,G),e(w,he),e(w,K),e(K,ie),e(w,ne),i(B,Q,ae),q(R,B,ae),se=!0},i(B){se||(v(y.$$.fragment,B),v(R.$$.fragment,B),se=!0)},o(B){_(y.$$.fragment,B),_(R.$$.fragment,B),se=!1},d(B){B&&t(d),j(y),B&&t(O),B&&t(w),B&&t(Q),j(R,B)}}}function $1(te){let d,E,h,y,L,k,T,M;return{c(){d=r("p"),E=a("\u{1F4A1} Si vous vous demandez pourquoi vous ne voyez aucun avertissement concernant le "),h=r("em"),y=a("finetuning"),L=a(" du mod\xE8le sur une t\xE2che en aval, c\u2019est parce que pour les t\xE2ches de s\xE9quence \xE0 s\xE9quence, nous conservons tous les poids du r\xE9seau. Comparez cela \xE0 notre mod\xE8le de classification de texte du "),k=r("a"),T=a("chapitre 3"),M=a(" o\xF9 la t\xEAte du mod\xE8le pr\xE9-entra\xEEn\xE9 a \xE9t\xE9 remplac\xE9e par un r\xE9seau initialis\xE9 de mani\xE8re al\xE9atoire."),this.h()},l(P){d=l(P,"P",{});var S=o(d);E=n(S,"\u{1F4A1} Si vous vous demandez pourquoi vous ne voyez aucun avertissement concernant le "),h=l(S,"EM",{});var A=o(h);y=n(A,"finetuning"),A.forEach(t),L=n(S," du mod\xE8le sur une t\xE2che en aval, c\u2019est parce que pour les t\xE2ches de s\xE9quence \xE0 s\xE9quence, nous conservons tous les poids du r\xE9seau. Comparez cela \xE0 notre mod\xE8le de classification de texte du "),k=l(S,"A",{href:!0});var O=o(k);T=n(O,"chapitre 3"),O.forEach(t),M=n(S," o\xF9 la t\xEAte du mod\xE8le pr\xE9-entra\xEEn\xE9 a \xE9t\xE9 remplac\xE9e par un r\xE9seau initialis\xE9 de mani\xE8re al\xE9atoire."),S.forEach(t),this.h()},h(){f(k,"href","/course/fr/chapter3")},m(P,S){i(P,d,S),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M)},d(P){P&&t(d)}}}function s1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se,B,ae,Y,ge,Z,ke,me,le,fe,ce,H,Ce,X,be,Te,Ne,x,J,Pe,De,qe,Ie,je,Ve,Ge,ss,ye,$e,ms,ts,Se,as,cs,Le,Ls,ns,bs,de,ds,ve,qs,_e,js,rs,$s,xs,hs,We,$,V,Ms,ls,Es,os,xe,Ke,As,Qe,ue,Ue;return w=new C({props:{code:`from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# La perte d'entra\xEEnement \xE0 chaque \xE9poque
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments

batch_size = <span class="hljs-number">8</span>
num_train_epochs = <span class="hljs-number">8</span>
<span class="hljs-comment"># La perte d&#x27;entra\xEEnement \xE0 chaque \xE9poque</span>
logging_steps = <span class="hljs-built_in">len</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">5.6e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=<span class="hljs-number">0.01</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    num_train_epochs=num_train_epochs,
    predict_with_generate=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
    push_to_hub=<span class="hljs-literal">True</span>,
)`}}),ue=new C({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # D\xE9coder les r\xE9sum\xE9s g\xE9n\xE9r\xE9s en texte
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Remplacer -100 dans les \xE9tiquettes car nous ne pouvons pas les d\xE9coder
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # D\xE9coder les r\xE9sum\xE9s de r\xE9f\xE9rence en texte
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE attend une nouvelle ligne apr\xE8s chaque phrase
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Calcul des scores ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extraire les scores m\xE9dians
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-comment"># D\xE9coder les r\xE9sum\xE9s g\xE9n\xE9r\xE9s en texte</span>
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Remplacer -100 dans les \xE9tiquettes car nous ne pouvons pas les d\xE9coder</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    <span class="hljs-comment"># D\xE9coder les r\xE9sum\xE9s de r\xE9f\xE9rence en texte</span>
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># ROUGE attend une nouvelle ligne apr\xE8s chaque phrase</span>
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    <span class="hljs-comment"># Calcul des scores ROUGE</span>
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Extraire les scores m\xE9dians</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-keyword">return</span> {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),{c(){d=r("p"),E=a("Nous aurons besoin de g\xE9n\xE9rer des r\xE9sum\xE9s afin de calculer les scores ROUGE pendant l\u2019entra\xEEnement. Heureusement, \u{1F917} "),h=r("em"),y=a("Transformers"),L=a(" fournit des classes d\xE9di\xE9es "),k=r("code"),T=a("Seq2SeqTrainingArguments"),M=a(" et "),P=r("code"),S=a("Seq2SeqTrainer"),A=a(" qui peuvent faire cela pour nous automatiquement ! Pour voir comment cela fonctionne, d\xE9finissons d\u2019abord les hyperparam\xE8tres et autres arguments pour nos exp\xE9riences :"),O=m(),g(w.$$.fragment),I=m(),D=r("p"),N=a("Ici, l\u2019argument "),oe=r("code"),W=a("predict_with_generate"),re=a(" a \xE9t\xE9 d\xE9fini pour indiquer que nous devons g\xE9n\xE9rer des r\xE9sum\xE9s pendant l\u2019\xE9valuation afin de pouvoir calculer les scores ROUGE pour chaque \xE9poque. Comme discut\xE9 au "),F=r("a"),ee=a("chapitre 1"),G=a(", le d\xE9codeur effectue l\u2019inf\xE9rence en pr\xE9disant les "),he=r("em"),K=a("tokens"),ie=a(" un par un, et ceci est impl\xE9ment\xE9 par la m\xE9thode "),ne=r("code"),Q=a("generate()"),R=a(". D\xE9finir "),se=r("code"),B=a("predict_with_generate=True"),ae=a(" indique au "),Y=r("code"),ge=a("Seq2SeqTrainer"),Z=a(" d\u2019utiliser cette m\xE9thode pour l\u2019\xE9valuation. Nous avons \xE9galement ajust\xE9 certains des hyperparam\xE8tres par d\xE9faut, comme le taux d\u2019apprentissage, le nombre d\u2019\xE9poques, et le taux de d\xE9croissance des poids, et nous avons r\xE9gl\xE9 l\u2019option "),ke=r("code"),me=a("save_total_limit"),le=a(" pour ne sauvegarder que jusqu\u2019\xE0 trois "),fe=r("em"),ce=a("checkpoints"),H=a(" pendant l\u2019entra\xEEnement. C\u2019est parce que m\xEAme la plus petite version de mT5 utilise environ 1 Go d\u2019espace disque, et nous pouvons gagner un peu de place en limitant le nombre de copies que nous sauvegardons."),Ce=m(),X=r("p"),be=a("L\u2019argument "),Te=r("code"),Ne=a("push_to_hub=True"),x=a(" nous permettra de pousser le mod\xE8le vers le "),J=r("em"),Pe=a("Hub"),De=a(" apr\xE8s l\u2019entra\xEEnement. Vous trouverez le d\xE9p\xF4t sous votre profil utilisateur dans l\u2019emplacement d\xE9fini par "),qe=r("code"),Ie=a("output_dir"),je=a(". Notez que vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),Ve=r("code"),Ge=a("hub_model_id"),ss=a(" (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ye=r("a"),$e=r("code"),ms=a("huggingface-course"),ts=a(", nous avons ajout\xE9 "),Se=r("code"),as=a('hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),cs=a(" \xE0 "),Le=r("code"),Ls=a("Seq2SeqTrainingArguments"),ns=a("."),bs=m(),de=r("p"),ds=a("La prochaine chose que nous devons faire est de fournir \xE0 "),ve=r("code"),qs=a("Seq2SeqTrainer"),_e=a(" une fonction "),js=r("code"),rs=a("compute_metrics()"),$s=a(" afin que nous puissions \xE9valuer notre mod\xE8le pendant l\u2019entra\xEEnement. Pour le r\xE9sum\xE9, c\u2019est un peu plus compliqu\xE9 que de simplement appeler "),xs=r("code"),hs=a("rouge_score.compute()"),We=a(" sur les pr\xE9dictions du mod\xE8le, puisque nous devons "),$=r("em"),V=a("d\xE9coder"),Ms=a(" les sorties et les \xE9tiquettes en texte avant de pouvoir calculer les scores ROUGE. La fonction suivante fait exactement cela, et utilise \xE9galement la fonction "),ls=r("code"),Es=a("sent_tokenize()"),os=a(" de "),xe=r("code"),Ke=a("nltk"),As=a(" pour s\xE9parer les phrases du r\xE9sum\xE9 avec des nouvelles lignes :"),Qe=m(),g(ue.$$.fragment),this.h()},l(U){d=l(U,"P",{});var pe=o(d);E=n(pe,"Nous aurons besoin de g\xE9n\xE9rer des r\xE9sum\xE9s afin de calculer les scores ROUGE pendant l\u2019entra\xEEnement. Heureusement, \u{1F917} "),h=l(pe,"EM",{});var et=o(h);y=n(et,"Transformers"),et.forEach(t),L=n(pe," fournit des classes d\xE9di\xE9es "),k=l(pe,"CODE",{});var Os=o(k);T=n(Os,"Seq2SeqTrainingArguments"),Os.forEach(t),M=n(pe," et "),P=l(pe,"CODE",{});var Ns=o(P);S=n(Ns,"Seq2SeqTrainer"),Ns.forEach(t),A=n(pe," qui peuvent faire cela pour nous automatiquement ! Pour voir comment cela fonctionne, d\xE9finissons d\u2019abord les hyperparam\xE8tres et autres arguments pour nos exp\xE9riences :"),pe.forEach(t),O=c(U),b(w.$$.fragment,U),I=c(U),D=l(U,"P",{});var Ee=o(D);N=n(Ee,"Ici, l\u2019argument "),oe=l(Ee,"CODE",{});var Is=o(oe);W=n(Is,"predict_with_generate"),Is.forEach(t),re=n(Ee," a \xE9t\xE9 d\xE9fini pour indiquer que nous devons g\xE9n\xE9rer des r\xE9sum\xE9s pendant l\u2019\xE9valuation afin de pouvoir calculer les scores ROUGE pour chaque \xE9poque. Comme discut\xE9 au "),F=l(Ee,"A",{href:!0});var ks=o(F);ee=n(ks,"chapitre 1"),ks.forEach(t),G=n(Ee,", le d\xE9codeur effectue l\u2019inf\xE9rence en pr\xE9disant les "),he=l(Ee,"EM",{});var Je=o(he);K=n(Je,"tokens"),Je.forEach(t),ie=n(Ee," un par un, et ceci est impl\xE9ment\xE9 par la m\xE9thode "),ne=l(Ee,"CODE",{});var Rs=o(ne);Q=n(Rs,"generate()"),Rs.forEach(t),R=n(Ee,". D\xE9finir "),se=l(Ee,"CODE",{});var Be=o(se);B=n(Be,"predict_with_generate=True"),Be.forEach(t),ae=n(Ee," indique au "),Y=l(Ee,"CODE",{});var ut=o(Y);ge=n(ut,"Seq2SeqTrainer"),ut.forEach(t),Z=n(Ee," d\u2019utiliser cette m\xE9thode pour l\u2019\xE9valuation. Nous avons \xE9galement ajust\xE9 certains des hyperparam\xE8tres par d\xE9faut, comme le taux d\u2019apprentissage, le nombre d\u2019\xE9poques, et le taux de d\xE9croissance des poids, et nous avons r\xE9gl\xE9 l\u2019option "),ke=l(Ee,"CODE",{});var Me=o(ke);me=n(Me,"save_total_limit"),Me.forEach(t),le=n(Ee," pour ne sauvegarder que jusqu\u2019\xE0 trois "),fe=l(Ee,"EM",{});var aa=o(fe);ce=n(aa,"checkpoints"),aa.forEach(t),H=n(Ee," pendant l\u2019entra\xEEnement. C\u2019est parce que m\xEAme la plus petite version de mT5 utilise environ 1 Go d\u2019espace disque, et nous pouvons gagner un peu de place en limitant le nombre de copies que nous sauvegardons."),Ee.forEach(t),Ce=c(U),X=l(U,"P",{});var ze=o(X);be=n(ze,"L\u2019argument "),Te=l(ze,"CODE",{});var Gs=o(Te);Ne=n(Gs,"push_to_hub=True"),Gs.forEach(t),x=n(ze," nous permettra de pousser le mod\xE8le vers le "),J=l(ze,"EM",{});var Lt=o(J);Pe=n(Lt,"Hub"),Lt.forEach(t),De=n(ze," apr\xE8s l\u2019entra\xEEnement. Vous trouverez le d\xE9p\xF4t sous votre profil utilisateur dans l\u2019emplacement d\xE9fini par "),qe=l(ze,"CODE",{});var is=o(qe);Ie=n(is,"output_dir"),is.forEach(t),je=n(ze,". Notez que vous pouvez sp\xE9cifier le nom du d\xE9p\xF4t vers lequel vous voulez pousser avec l\u2019argument "),Ve=l(ze,"CODE",{});var na=o(Ve);Ge=n(na,"hub_model_id"),na.forEach(t),ss=n(ze," (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss\xE9 le mod\xE8le vers l\u2019organisation "),ye=l(ze,"A",{href:!0,rel:!0});var pt=o(ye);$e=l(pt,"CODE",{});var st=o($e);ms=n(st,"huggingface-course"),st.forEach(t),pt.forEach(t),ts=n(ze,", nous avons ajout\xE9 "),Se=l(ze,"CODE",{});var ra=o(Se);as=n(ra,'hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),ra.forEach(t),cs=n(ze," \xE0 "),Le=l(ze,"CODE",{});var mt=o(Le);Ls=n(mt,"Seq2SeqTrainingArguments"),mt.forEach(t),ns=n(ze,"."),ze.forEach(t),bs=c(U),de=l(U,"P",{});var Ae=o(de);ds=n(Ae,"La prochaine chose que nous devons faire est de fournir \xE0 "),ve=l(Ae,"CODE",{});var la=o(ve);qs=n(la,"Seq2SeqTrainer"),la.forEach(t),_e=n(Ae," une fonction "),js=l(Ae,"CODE",{});var Mt=o(js);rs=n(Mt,"compute_metrics()"),Mt.forEach(t),$s=n(Ae," afin que nous puissions \xE9valuer notre mod\xE8le pendant l\u2019entra\xEEnement. Pour le r\xE9sum\xE9, c\u2019est un peu plus compliqu\xE9 que de simplement appeler "),xs=l(Ae,"CODE",{});var fs=o(xs);hs=n(fs,"rouge_score.compute()"),fs.forEach(t),We=n(Ae," sur les pr\xE9dictions du mod\xE8le, puisque nous devons "),$=l(Ae,"EM",{});var Us=o($);V=n(Us,"d\xE9coder"),Us.forEach(t),Ms=n(Ae," les sorties et les \xE9tiquettes en texte avant de pouvoir calculer les scores ROUGE. La fonction suivante fait exactement cela, et utilise \xE9galement la fonction "),ls=l(Ae,"CODE",{});var tt=o(ls);Es=n(tt,"sent_tokenize()"),tt.forEach(t),os=n(Ae," de "),xe=l(Ae,"CODE",{});var Re=o(xe);Ke=n(Re,"nltk"),Re.forEach(t),As=n(Ae," pour s\xE9parer les phrases du r\xE9sum\xE9 avec des nouvelles lignes :"),Ae.forEach(t),Qe=c(U),b(ue.$$.fragment,U),this.h()},h(){f(F,"href","/course/fr/chapter1"),f(ye,"href","https://huggingface.co/huggingface-course"),f(ye,"rel","nofollow")},m(U,pe){i(U,d,pe),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M),e(d,P),e(P,S),e(d,A),i(U,O,pe),q(w,U,pe),i(U,I,pe),i(U,D,pe),e(D,N),e(D,oe),e(oe,W),e(D,re),e(D,F),e(F,ee),e(D,G),e(D,he),e(he,K),e(D,ie),e(D,ne),e(ne,Q),e(D,R),e(D,se),e(se,B),e(D,ae),e(D,Y),e(Y,ge),e(D,Z),e(D,ke),e(ke,me),e(D,le),e(D,fe),e(fe,ce),e(D,H),i(U,Ce,pe),i(U,X,pe),e(X,be),e(X,Te),e(Te,Ne),e(X,x),e(X,J),e(J,Pe),e(X,De),e(X,qe),e(qe,Ie),e(X,je),e(X,Ve),e(Ve,Ge),e(X,ss),e(X,ye),e(ye,$e),e($e,ms),e(X,ts),e(X,Se),e(Se,as),e(X,cs),e(X,Le),e(Le,Ls),e(X,ns),i(U,bs,pe),i(U,de,pe),e(de,ds),e(de,ve),e(ve,qs),e(de,_e),e(de,js),e(js,rs),e(de,$s),e(de,xs),e(xs,hs),e(de,We),e(de,$),e($,V),e(de,Ms),e(de,ls),e(ls,Es),e(de,os),e(de,xe),e(xe,Ke),e(de,As),i(U,Qe,pe),q(ue,U,pe),Ue=!0},i(U){Ue||(v(w.$$.fragment,U),v(ue.$$.fragment,U),Ue=!0)},o(U){_(w.$$.fragment,U),_(ue.$$.fragment,U),Ue=!1},d(U){U&&t(d),U&&t(O),j(w,U),U&&t(I),U&&t(D),U&&t(Ce),U&&t(X),U&&t(bs),U&&t(de),U&&t(Qe),j(ue,U)}}}function x1(te){let d,E;return d=new C({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){g(d.$$.fragment)},l(h){b(d.$$.fragment,h)},m(h,y){q(d,h,y),E=!0},i(h){E||(v(d.$$.fragment,h),E=!0)},o(h){_(d.$$.fragment,h),E=!1},d(h){j(d,h)}}}function E1(te){let d,E;return d=new C({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`}}),{c(){g(d.$$.fragment)},l(h){b(d.$$.fragment,h)},m(h,y){q(d,h,y),E=!0},i(h){E||(v(d.$$.fragment,h),E=!0)},o(h){_(d.$$.fragment,h),E=!1},d(h){j(d,h)}}}function k1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se,B,ae,Y,ge,Z,ke,me,le,fe,ce,H,Ce,X,be,Te,Ne;return w=new C({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">8</span>,
)
tf_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
)`}}),W=new C({props:{code:`from transformers import create_optimizer
import tensorflow as tf

# Le nombre d'\xE9tapes d'entra\xEEnement est le nombre d'\xE9chantillons dans le jeu de donn\xE9es, divis\xE9 par la taille du batch,
# puis multipli\xE9 par le nombre total d'\xE9poques. Notez que le jeu de donn\xE9es tf_train_dataset est ici un tf.data.Dataset,
# et non le jeu de donn\xE9es original donc son len() est d\xE9j\xE0 num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Entra\xEEner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># Le nombre d&#x27;\xE9tapes d&#x27;entra\xEEnement est le nombre d&#x27;\xE9chantillons dans le jeu de donn\xE9es, divis\xE9 par la taille du batch,</span>
<span class="hljs-comment"># puis multipli\xE9 par le nombre total d&#x27;\xE9poques. Notez que le jeu de donn\xE9es tf_train_dataset est ici un tf.data.Dataset,</span>
<span class="hljs-comment"># et non le jeu de donn\xE9es original donc son len() est d\xE9j\xE0 num_samples // batch_size.</span>
num_train_epochs = <span class="hljs-number">8</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5.6e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Entra\xEEner en mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),Y=new C({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>, tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=<span class="hljs-number">8</span>
)`}}),le=new C({props:{code:`from tqdm import tqdm
import numpy as np

all_preds = []
all_labels = []
for batch in tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = batch["labels"].numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`,highlighted:`<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

all_preds = []
all_labels = []
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`}}),X=new C({props:{code:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}`,highlighted:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
)
result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
{k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),Te=new C({props:{code:"{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}",highlighted:'{&#x27;rouge1&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rouge2&#x27;: <span class="hljs-number">25.4386</span>, &#x27;rougeL&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rougeLsum&#x27;: <span class="hljs-number">31.4815</span>}'}}),{c(){d=r("p"),E=a("Nous sommes presque pr\xEAts \xE0 nous entra\xEEner ! Nous devons juste convertir nos jeux de donn\xE9es en "),h=r("code"),y=a("tf.data.Dataset"),L=a(" en utilisant le collateur de donn\xE9es que nous avons d\xE9fini ci-dessus, puis utiliser "),k=r("code"),T=a("compile()"),M=a(" et "),P=r("code"),S=a("fit()"),A=a(". D\u2019abord, les jeux de donn\xE9es :"),O=m(),g(w.$$.fragment),I=m(),D=r("p"),N=a("Maintenant, nous d\xE9finissons nos hyperparam\xE8tres d\u2019entra\xEEnement et nous compilons :"),oe=m(),g(W.$$.fragment),re=m(),F=r("p"),ee=a("Et enfin, nous "),G=r("em"),he=a("finetunons"),K=a(" le mod\xE8le. Nous utilisons un "),ie=r("code"),ne=a("PushToHubCallback"),Q=a(" pour sauvegarder le mod\xE8le sur le "),R=r("em"),se=a("Hub"),B=a(" apr\xE8s chaque \xE9poque, ce qui nous permettra de l\u2019utiliser pour l\u2019inf\xE9rence plus tard :"),ae=m(),g(Y.$$.fragment),ge=m(),Z=r("p"),ke=a("Nous avons obtenu quelques valeurs de perte pendant l\u2019entra\xEEnement, mais nous aimerions vraiment voir les m\xE9triques ROUGE que nous avons calcul\xE9es plus t\xF4t. Pour obtenir ces m\xE9triques, nous devons g\xE9n\xE9rer les sorties du mod\xE8le et les convertir en cha\xEEnes de caract\xE8res. Construisons quelques listes d\u2019\xE9tiquettes et de pr\xE9dictions pour comparer la m\xE9trique ROUGE (notez que si vous obtenez des erreurs d\u2019importation pour cette section, vous pouvez avoir besoin de \u201Cpip install tqdm\u201D) :"),me=m(),g(le.$$.fragment),fe=m(),ce=r("p"),H=a("Une fois que nous avons nos listes d\u2019\xE9tiquettes et de cha\xEEnes de pr\xE9diction, le calcul du score ROUGE est facile :"),Ce=m(),g(X.$$.fragment),be=m(),g(Te.$$.fragment)},l(x){d=l(x,"P",{});var J=o(d);E=n(J,"Nous sommes presque pr\xEAts \xE0 nous entra\xEEner ! Nous devons juste convertir nos jeux de donn\xE9es en "),h=l(J,"CODE",{});var Pe=o(h);y=n(Pe,"tf.data.Dataset"),Pe.forEach(t),L=n(J," en utilisant le collateur de donn\xE9es que nous avons d\xE9fini ci-dessus, puis utiliser "),k=l(J,"CODE",{});var De=o(k);T=n(De,"compile()"),De.forEach(t),M=n(J," et "),P=l(J,"CODE",{});var qe=o(P);S=n(qe,"fit()"),qe.forEach(t),A=n(J,". D\u2019abord, les jeux de donn\xE9es :"),J.forEach(t),O=c(x),b(w.$$.fragment,x),I=c(x),D=l(x,"P",{});var Ie=o(D);N=n(Ie,"Maintenant, nous d\xE9finissons nos hyperparam\xE8tres d\u2019entra\xEEnement et nous compilons :"),Ie.forEach(t),oe=c(x),b(W.$$.fragment,x),re=c(x),F=l(x,"P",{});var je=o(F);ee=n(je,"Et enfin, nous "),G=l(je,"EM",{});var Ve=o(G);he=n(Ve,"finetunons"),Ve.forEach(t),K=n(je," le mod\xE8le. Nous utilisons un "),ie=l(je,"CODE",{});var Ge=o(ie);ne=n(Ge,"PushToHubCallback"),Ge.forEach(t),Q=n(je," pour sauvegarder le mod\xE8le sur le "),R=l(je,"EM",{});var ss=o(R);se=n(ss,"Hub"),ss.forEach(t),B=n(je," apr\xE8s chaque \xE9poque, ce qui nous permettra de l\u2019utiliser pour l\u2019inf\xE9rence plus tard :"),je.forEach(t),ae=c(x),b(Y.$$.fragment,x),ge=c(x),Z=l(x,"P",{});var ye=o(Z);ke=n(ye,"Nous avons obtenu quelques valeurs de perte pendant l\u2019entra\xEEnement, mais nous aimerions vraiment voir les m\xE9triques ROUGE que nous avons calcul\xE9es plus t\xF4t. Pour obtenir ces m\xE9triques, nous devons g\xE9n\xE9rer les sorties du mod\xE8le et les convertir en cha\xEEnes de caract\xE8res. Construisons quelques listes d\u2019\xE9tiquettes et de pr\xE9dictions pour comparer la m\xE9trique ROUGE (notez que si vous obtenez des erreurs d\u2019importation pour cette section, vous pouvez avoir besoin de \u201Cpip install tqdm\u201D) :"),ye.forEach(t),me=c(x),b(le.$$.fragment,x),fe=c(x),ce=l(x,"P",{});var $e=o(ce);H=n($e,"Une fois que nous avons nos listes d\u2019\xE9tiquettes et de cha\xEEnes de pr\xE9diction, le calcul du score ROUGE est facile :"),$e.forEach(t),Ce=c(x),b(X.$$.fragment,x),be=c(x),b(Te.$$.fragment,x)},m(x,J){i(x,d,J),e(d,E),e(d,h),e(h,y),e(d,L),e(d,k),e(k,T),e(d,M),e(d,P),e(P,S),e(d,A),i(x,O,J),q(w,x,J),i(x,I,J),i(x,D,J),e(D,N),i(x,oe,J),q(W,x,J),i(x,re,J),i(x,F,J),e(F,ee),e(F,G),e(G,he),e(F,K),e(F,ie),e(ie,ne),e(F,Q),e(F,R),e(R,se),e(F,B),i(x,ae,J),q(Y,x,J),i(x,ge,J),i(x,Z,J),e(Z,ke),i(x,me,J),q(le,x,J),i(x,fe,J),i(x,ce,J),e(ce,H),i(x,Ce,J),q(X,x,J),i(x,be,J),q(Te,x,J),Ne=!0},i(x){Ne||(v(w.$$.fragment,x),v(W.$$.fragment,x),v(Y.$$.fragment,x),v(le.$$.fragment,x),v(X.$$.fragment,x),v(Te.$$.fragment,x),Ne=!0)},o(x){_(w.$$.fragment,x),_(W.$$.fragment,x),_(Y.$$.fragment,x),_(le.$$.fragment,x),_(X.$$.fragment,x),_(Te.$$.fragment,x),Ne=!1},d(x){x&&t(d),x&&t(O),j(w,x),x&&t(I),x&&t(D),x&&t(oe),j(W,x),x&&t(re),x&&t(F),x&&t(ae),j(Y,x),x&&t(ge),x&&t(Z),x&&t(me),j(le,x),x&&t(fe),x&&t(ce),x&&t(Ce),j(X,x),x&&t(be),j(Te,x)}}}function w1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se,B,ae,Y,ge,Z,ke,me,le,fe,ce,H,Ce,X,be,Te,Ne,x,J,Pe,De,qe,Ie,je,Ve,Ge,ss,ye,$e,ms,ts,Se,as,cs,Le,Ls,ns,bs,de,ds,ve,qs,_e,js,rs,$s,xs,hs,We;return T=new C({props:{code:`from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`}}),O=new C({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),F=new C({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),G=new C({props:{code:`{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}`,highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">3.028524398803711</span>,
 <span class="hljs-string">&#x27;eval_rouge1&#x27;</span>: <span class="hljs-number">16.9728</span>,
 <span class="hljs-string">&#x27;eval_rouge2&#x27;</span>: <span class="hljs-number">8.2969</span>,
 <span class="hljs-string">&#x27;eval_rougeL&#x27;</span>: <span class="hljs-number">16.8366</span>,
 <span class="hljs-string">&#x27;eval_rougeLsum&#x27;</span>: <span class="hljs-number">16.851</span>,
 <span class="hljs-string">&#x27;eval_gen_len&#x27;</span>: <span class="hljs-number">10.1597</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">6.1054</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">38.982</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">4.914</span>}`}}),me=new C({props:{code:'trainer.push_to_hub(commit_message="Training complete", tags="summarization")',highlighted:'trainer.push_to_hub(<span class="hljs-attribute">commit_message</span>=<span class="hljs-string">&quot;Training complete&quot;</span>, <span class="hljs-attribute">tags</span>=<span class="hljs-string">&quot;summarization&quot;</span>)'}}),fe=new C({props:{code:"'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0&#x27;</span>'}}),{c(){d=r("p"),E=a("Nous avons enfin tous les ingr\xE9dients dont nous avons besoin pour l\u2019entra\xEEnement ! Nous devons maintenant simplement instancier le "),h=r("code"),y=a("Seq2SeqTrainer"),L=a(" avec les arguments :"),k=m(),g(T.$$.fragment),M=m(),P=r("p"),S=a("et lancer notre course d\u2019entra\xEEnement :"),A=m(),g(O.$$.fragment),w=m(),I=r("p"),D=a("Pendant l\u2019entra\xEEnement, vous devriez voir la perte d\u2019entra\xEEnement diminuer et les scores ROUGE augmenter \xE0 chaque \xE9poque. Une fois l\u2019entra\xEEnement termin\xE9, vous pouvez voir les scores ROUGE finaux en ex\xE9cutant "),N=r("code"),oe=a("Trainer.evaluate()"),W=a(" :"),re=m(),g(F.$$.fragment),ee=m(),g(G.$$.fragment),he=m(),K=r("p"),ie=a("D\u2019apr\xE8s les scores, nous pouvons voir que notre mod\xE8le a largement surpass\xE9 notre "),ne=r("em"),Q=a("baseline"),R=m(),se=r("em"),B=a("lead-3"),ae=a(". Bien ! La derni\xE8re chose \xE0 faire est de pousser les poids du mod\xE8le vers le "),Y=r("em"),ge=a("Hub"),Z=a(", comme suit :"),ke=m(),g(me.$$.fragment),le=m(),g(fe.$$.fragment),ce=m(),H=r("p"),Ce=a("Ceci sauvegardera le "),X=r("em"),be=a("checkpoint"),Te=a(" et les fichiers de configuration dans "),Ne=r("code"),x=a("output_dir"),J=a(", avant de t\xE9l\xE9charger tous les fichiers sur le "),Pe=r("em"),De=a("Hub"),qe=a(". En sp\xE9cifiant l\u2019argument "),Ie=r("code"),je=a("tags"),Ve=a(", nous nous assurons \xE9galement que le "),Ge=r("em"),ss=a("widget"),ye=a(" sur le "),$e=r("em"),ms=a("Hub"),ts=a(" sera celui d\u2019un pipeline de r\xE9sum\xE9 au lieu de celui de la g\xE9n\xE9ration de texte par d\xE9faut associ\xE9 \xE0 l\u2019architecture mT5 (pour plus d\u2019informations sur les balises de mod\xE8le, voir la "),Se=r("a"),as=a("documentation du "),cs=r("em"),Le=a("Hub"),Ls=a("). La sortie de "),ns=r("code"),bs=a("trainer.push_to_hub()"),de=a(" est une URL vers le hash du commit Git, donc vous pouvez facilement voir les changements qui ont \xE9t\xE9 faits au d\xE9p\xF4t de mod\xE8le !"),ds=m(),ve=r("p"),qs=a("Pour conclure cette section, voyons comment nous pouvons \xE9galement "),_e=r("em"),js=a("finetuner"),rs=a(" mT5 en utilisant les fonctionnalit\xE9s de bas niveau fournies par \u{1F917} "),$s=r("em"),xs=a("Accelerate"),hs=a("."),this.h()},l($){d=l($,"P",{});var V=o(d);E=n(V,"Nous avons enfin tous les ingr\xE9dients dont nous avons besoin pour l\u2019entra\xEEnement ! Nous devons maintenant simplement instancier le "),h=l(V,"CODE",{});var Ms=o(h);y=n(Ms,"Seq2SeqTrainer"),Ms.forEach(t),L=n(V," avec les arguments :"),V.forEach(t),k=c($),b(T.$$.fragment,$),M=c($),P=l($,"P",{});var ls=o(P);S=n(ls,"et lancer notre course d\u2019entra\xEEnement :"),ls.forEach(t),A=c($),b(O.$$.fragment,$),w=c($),I=l($,"P",{});var Es=o(I);D=n(Es,"Pendant l\u2019entra\xEEnement, vous devriez voir la perte d\u2019entra\xEEnement diminuer et les scores ROUGE augmenter \xE0 chaque \xE9poque. Une fois l\u2019entra\xEEnement termin\xE9, vous pouvez voir les scores ROUGE finaux en ex\xE9cutant "),N=l(Es,"CODE",{});var os=o(N);oe=n(os,"Trainer.evaluate()"),os.forEach(t),W=n(Es," :"),Es.forEach(t),re=c($),b(F.$$.fragment,$),ee=c($),b(G.$$.fragment,$),he=c($),K=l($,"P",{});var xe=o(K);ie=n(xe,"D\u2019apr\xE8s les scores, nous pouvons voir que notre mod\xE8le a largement surpass\xE9 notre "),ne=l(xe,"EM",{});var Ke=o(ne);Q=n(Ke,"baseline"),Ke.forEach(t),R=c(xe),se=l(xe,"EM",{});var As=o(se);B=n(As,"lead-3"),As.forEach(t),ae=n(xe,". Bien ! La derni\xE8re chose \xE0 faire est de pousser les poids du mod\xE8le vers le "),Y=l(xe,"EM",{});var Qe=o(Y);ge=n(Qe,"Hub"),Qe.forEach(t),Z=n(xe,", comme suit :"),xe.forEach(t),ke=c($),b(me.$$.fragment,$),le=c($),b(fe.$$.fragment,$),ce=c($),H=l($,"P",{});var ue=o(H);Ce=n(ue,"Ceci sauvegardera le "),X=l(ue,"EM",{});var Ue=o(X);be=n(Ue,"checkpoint"),Ue.forEach(t),Te=n(ue," et les fichiers de configuration dans "),Ne=l(ue,"CODE",{});var U=o(Ne);x=n(U,"output_dir"),U.forEach(t),J=n(ue,", avant de t\xE9l\xE9charger tous les fichiers sur le "),Pe=l(ue,"EM",{});var pe=o(Pe);De=n(pe,"Hub"),pe.forEach(t),qe=n(ue,". En sp\xE9cifiant l\u2019argument "),Ie=l(ue,"CODE",{});var et=o(Ie);je=n(et,"tags"),et.forEach(t),Ve=n(ue,", nous nous assurons \xE9galement que le "),Ge=l(ue,"EM",{});var Os=o(Ge);ss=n(Os,"widget"),Os.forEach(t),ye=n(ue," sur le "),$e=l(ue,"EM",{});var Ns=o($e);ms=n(Ns,"Hub"),Ns.forEach(t),ts=n(ue," sera celui d\u2019un pipeline de r\xE9sum\xE9 au lieu de celui de la g\xE9n\xE9ration de texte par d\xE9faut associ\xE9 \xE0 l\u2019architecture mT5 (pour plus d\u2019informations sur les balises de mod\xE8le, voir la "),Se=l(ue,"A",{href:!0,rel:!0});var Ee=o(Se);as=n(Ee,"documentation du "),cs=l(Ee,"EM",{});var Is=o(cs);Le=n(Is,"Hub"),Is.forEach(t),Ee.forEach(t),Ls=n(ue,"). La sortie de "),ns=l(ue,"CODE",{});var ks=o(ns);bs=n(ks,"trainer.push_to_hub()"),ks.forEach(t),de=n(ue," est une URL vers le hash du commit Git, donc vous pouvez facilement voir les changements qui ont \xE9t\xE9 faits au d\xE9p\xF4t de mod\xE8le !"),ue.forEach(t),ds=c($),ve=l($,"P",{});var Je=o(ve);qs=n(Je,"Pour conclure cette section, voyons comment nous pouvons \xE9galement "),_e=l(Je,"EM",{});var Rs=o(_e);js=n(Rs,"finetuner"),Rs.forEach(t),rs=n(Je," mT5 en utilisant les fonctionnalit\xE9s de bas niveau fournies par \u{1F917} "),$s=l(Je,"EM",{});var Be=o($s);xs=n(Be,"Accelerate"),Be.forEach(t),hs=n(Je,"."),Je.forEach(t),this.h()},h(){f(Se,"href","https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined"),f(Se,"rel","nofollow")},m($,V){i($,d,V),e(d,E),e(d,h),e(h,y),e(d,L),i($,k,V),q(T,$,V),i($,M,V),i($,P,V),e(P,S),i($,A,V),q(O,$,V),i($,w,V),i($,I,V),e(I,D),e(I,N),e(N,oe),e(I,W),i($,re,V),q(F,$,V),i($,ee,V),q(G,$,V),i($,he,V),i($,K,V),e(K,ie),e(K,ne),e(ne,Q),e(K,R),e(K,se),e(se,B),e(K,ae),e(K,Y),e(Y,ge),e(K,Z),i($,ke,V),q(me,$,V),i($,le,V),q(fe,$,V),i($,ce,V),i($,H,V),e(H,Ce),e(H,X),e(X,be),e(H,Te),e(H,Ne),e(Ne,x),e(H,J),e(H,Pe),e(Pe,De),e(H,qe),e(H,Ie),e(Ie,je),e(H,Ve),e(H,Ge),e(Ge,ss),e(H,ye),e(H,$e),e($e,ms),e(H,ts),e(H,Se),e(Se,as),e(Se,cs),e(cs,Le),e(H,Ls),e(H,ns),e(ns,bs),e(H,de),i($,ds,V),i($,ve,V),e(ve,qs),e(ve,_e),e(_e,js),e(ve,rs),e(ve,$s),e($s,xs),e(ve,hs),We=!0},i($){We||(v(T.$$.fragment,$),v(O.$$.fragment,$),v(F.$$.fragment,$),v(G.$$.fragment,$),v(me.$$.fragment,$),v(fe.$$.fragment,$),We=!0)},o($){_(T.$$.fragment,$),_(O.$$.fragment,$),_(F.$$.fragment,$),_(G.$$.fragment,$),_(me.$$.fragment,$),_(fe.$$.fragment,$),We=!1},d($){$&&t(d),$&&t(k),j(T,$),$&&t(M),$&&t(P),$&&t(A),j(O,$),$&&t(w),$&&t(I),$&&t(re),j(F,$),$&&t(ee),j(G,$),$&&t(he),$&&t(K),$&&t(ke),j(me,$),$&&t(le),j(fe,$),$&&t(ce),$&&t(H),$&&t(ds),$&&t(ve)}}}function t1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se,B,ae,Y,ge,Z,ke,me,le,fe,ce,H,Ce,X,be,Te,Ne,x,J,Pe,De,qe,Ie,je,Ve,Ge,ss,ye,$e,ms,ts,Se,as,cs,Le,Ls,ns,bs,de,ds,ve,qs,_e,js,rs,$s,xs,hs,We,$,V,Ms,ls,Es,os,xe,Ke,As,Qe,ue,Ue,U,pe,et,Os,Ns,Ee,Is,ks,Je,Rs,Be,ut,Me,aa,ze,Gs,Lt,is,na,pt,st,ra,mt,Ae,la,Mt,fs,Us,tt,Re,rr,At,lr,or,oa,Ot,mn,at,cn,ia,Ma,dn,Nt,ua,Hs,ir,pa,Rt,hn,ct,ur,fn,dt,Fs,ys,ma,He,pr,It,mr,vn,Js,cr,Aa,Oa,dr,_n,vs,Gt,hr,ca,Ut,gn,Ht,ht,da,Na,bn,nt,Ft,qn,fr,Jt,Ye,jn,Bs,vr,Ra,Ia,_r,$n,rt,xn,Vs,Ga,Ua,gr,Ha,Fa,br,Ja,En,lt,ot,ft,Bt,kn,it,wn,vt,ws,_t,Ba,Va,qr,ha,yn;return y=new St({}),Z=new St({}),Pe=new C({props:{code:'tokenized_datasets.set_format("torch")',highlighted:'tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)'}}),ye=new C({props:{code:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",highlighted:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"}}),as=new C({props:{code:`from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

batch_size = <span class="hljs-number">8</span>
train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=batch_size
)`}}),ve=new C({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`}}),We=new C({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),V=new nr({props:{$$slots:{default:[y1]},$$scope:{ctx:te}}}),Be=new C({props:{code:`from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">10</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),Gs=new C({props:{code:`def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE attend une nouvelle ligne apr\xE8s chaque phrase
    preds = ["\\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [label.strip() <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-comment"># ROUGE attend une nouvelle ligne apr\xE8s chaque phrase</span>
    preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(pred)) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(label)) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-keyword">return</span> preds, labels`}}),Ot=new C({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;test-bert-finetuned-squad-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),at=new C({props:{code:"'lewtun/mt5-finetuned-amazon-en-es-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/mt5-finetuned-amazon-en-es-accelerate&#x27;</span>'}}),Nt=new C({props:{code:`from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = <span class="hljs-string">&quot;results-mt5-finetuned-squad-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),He=new St({}),it=new C({props:{code:`from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra\xEEnement
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"], attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # Si nous n'avons pas rempli la longueur maximale, nous devons \xE9galement remplir les \xE9tiquettes
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Remplacer -100 dans les \xE9tiquettes car nous ne pouvons pas les d\xE9coder
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Calculer les m\xE9triques
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Sauvegarder et t\xE9l\xE9charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Entra\xEEnement</span>
    model.train()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch[<span class="hljs-string">&quot;input_ids&quot;</span>], attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )
            labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

            <span class="hljs-comment"># Si nous n&#x27;avons pas rempli la longueur maximale, nous devons \xE9galement remplir les \xE9tiquettes</span>
            labels = accelerator.pad_across_processes(
                batch[<span class="hljs-string">&quot;labels&quot;</span>], dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            <span class="hljs-comment"># Remplacer -100 dans les \xE9tiquettes car nous ne pouvons pas les d\xE9coder</span>
            labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(generated_tokens, <span class="hljs-built_in">tuple</span>):
                generated_tokens = generated_tokens[<span class="hljs-number">0</span>]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    <span class="hljs-comment"># Calculer les m\xE9triques</span>
    result = rouge_score.compute()
    <span class="hljs-comment"># Extract the median ROUGE scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch}</span>:&quot;</span>, result)

    <span class="hljs-comment"># Sauvegarder et t\xE9l\xE9charger</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),vt=new C({props:{code:`Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}`,highlighted:`Epoch <span class="hljs-number">0</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">5.6351</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">1.1625</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">5.4866</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">5.5005</span>}
Epoch <span class="hljs-number">1</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">9.8646</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.4106</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">9.9439</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">9.9306</span>}
Epoch <span class="hljs-number">2</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.0872</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.3273</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.0508</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">10.9468</span>}
Epoch <span class="hljs-number">3</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.8587</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">4.8167</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.7986</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">11.7518</span>}
Epoch <span class="hljs-number">4</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9842</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.5887</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.7546</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.7029</span>}
Epoch <span class="hljs-number">5</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.4628</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.4598</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.312</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.2913</span>}
Epoch <span class="hljs-number">6</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9131</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.8914</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.6896</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.5701</span>}
Epoch <span class="hljs-number">7</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.3079</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.2994</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.1536</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.1194</span>}
Epoch <span class="hljs-number">8</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.96</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.5998</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.9123</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.7744</span>}
Epoch <span class="hljs-number">9</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">14.1192</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">7.0059</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">14.1172</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.9509</span>}`}}),{c(){d=r("h2"),E=r("a"),h=r("span"),g(y.$$.fragment),L=m(),k=r("span"),T=r("i"),M=a("Finetuning"),P=a(" de mT5 avec \u{1F917} "),S=r("i"),A=a("Accelerate"),O=m(),w=r("p"),I=a("Le "),D=r("em"),N=a("finetuning"),oe=a(" de notre mod\xE8le avec \u{1F917} "),W=r("em"),re=a("Accelerate"),F=a(" est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte que nous avons rencontr\xE9 dans le "),ee=r("a"),G=a("chapitre 3"),he=a(". Les principales diff\xE9rences seront la n\xE9cessit\xE9 de g\xE9n\xE9rer explicitement nos r\xE9sum\xE9s pendant l\u2019entra\xEEnement et de d\xE9finir comment nous calculons les scores ROUGE (rappelons que le "),K=r("code"),ie=a("Seq2SeqTrainer"),ne=a(" s\u2019est occup\xE9 de la g\xE9n\xE9ration pour nous). Voyons comment nous pouvons mettre en \u0153uvre ces deux exigences dans \u{1F917} "),Q=r("em"),R=a("Accelerate"),se=a(" !"),B=m(),ae=r("h3"),Y=r("a"),ge=r("span"),g(Z.$$.fragment),ke=m(),me=r("span"),le=a("Pr\xE9parer tout pour l'entra\xEEnement"),fe=m(),ce=r("p"),H=a("La premi\xE8re chose que nous devons faire est de cr\xE9er un "),Ce=r("code"),X=a("DataLoader"),be=a(" pour chacun de nos \xE9chantillons. Puisque les chargeurs de donn\xE9es PyTorch attendent des batchs de tenseurs, nous devons d\xE9finir le format \xE0 "),Te=r("code"),Ne=a('"torch"'),x=a(" dans nos jeux de donn\xE9es :"),J=m(),g(Pe.$$.fragment),De=m(),qe=r("p"),Ie=a("Maintenant que nous avons des jeux de donn\xE9es constitu\xE9s uniquement de tenseurs, la prochaine chose \xE0 faire est d\u2019instancier \xE0 nouveau le "),je=r("code"),Ve=a("DataCollatorForSeq2Seq"),Ge=a(". Pour cela, nous devons fournir une nouvelle version du mod\xE8le, donc chargeons-le \xE0 nouveau depuis notre cache :"),ss=m(),g(ye.$$.fragment),$e=m(),ms=r("p"),ts=a("Nous pouvons ensuite instancier le collateur de donn\xE9es et l\u2019utiliser pour d\xE9finir nos chargeurs de donn\xE9es :"),Se=m(),g(as.$$.fragment),cs=m(),Le=r("p"),Ls=a("La prochaine chose \xE0 faire est de d\xE9finir l\u2019optimiseur que nous voulons utiliser. Comme dans nos autres exemples, nous allons utiliser "),ns=r("code"),bs=a("AdamW"),de=a(", qui fonctionne bien pour la plupart des probl\xE8mes :"),ds=m(),g(ve.$$.fragment),qs=m(),_e=r("p"),js=a("Enfin, nous introduisons notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es dans la m\xE9thode "),rs=r("code"),$s=a("accelerator.prepare()"),xs=a(" :"),hs=m(),g(We.$$.fragment),$=m(),g(V.$$.fragment),Ms=m(),ls=r("p"),Es=a("Maintenant que nous avons pr\xE9par\xE9 nos objets, il reste trois choses \xE0 faire :"),os=m(),xe=r("ul"),Ke=r("li"),As=a("d\xE9finir le programmeur du taux d\u2019apprentissage,"),Qe=m(),ue=r("li"),Ue=a("impl\xE9menter une fonction pour post-traiter les r\xE9sum\xE9s pour l\u2019\xE9valuation,"),U=m(),pe=r("li"),et=a("cr\xE9er un d\xE9p\xF4t sur le "),Os=r("em"),Ns=a("Hub"),Ee=a(" vers lequel nous pouvons pousser notre mod\xE8le."),Is=m(),ks=r("p"),Je=a("Pour le programmeur de taux d\u2019apprentissage, nous utiliserons le programmeur lin\xE9aire standard des sections pr\xE9c\xE9dentes :"),Rs=m(),g(Be.$$.fragment),ut=m(),Me=r("p"),aa=a("Pour le post-traitement, nous avons besoin d\u2019une fonction qui divise les r\xE9sum\xE9s g\xE9n\xE9r\xE9s en phrases s\xE9par\xE9es par des nouvelles lignes. C\u2019est le format attendu par la m\xE9trique ROUGE et nous pouvons y parvenir avec le bout de code suivant :"),ze=m(),g(Gs.$$.fragment),Lt=m(),is=r("p"),na=a("Cela devrait vous sembler familier si vous vous rappelez comment nous avons d\xE9fini la fonction "),pt=r("code"),st=a("compute_metrics()"),ra=a(" du "),mt=r("code"),Ae=a("Seq2SeqTrainer"),la=a("."),Mt=m(),fs=r("p"),Us=a("Enfin, nous devons cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),tt=r("em"),Re=a("Hub"),rr=a(". Pour cela, nous pouvons utiliser la biblioth\xE8que \u{1F917} "),At=r("em"),lr=a("Hub"),or=a(", qui porte le nom appropri\xE9. Nous avons juste besoin de d\xE9finir un nom pour notre d\xE9p\xF4t, et la biblioth\xE8que a une fonction utilitaire pour combiner l\u2019identifiant du d\xE9p\xF4t avec le profil de l\u2019utilisateur :"),oa=m(),g(Ot.$$.fragment),mn=m(),g(at.$$.fragment),cn=m(),ia=r("p"),Ma=a("Nous pouvons maintenant utiliser ce nom de d\xE9p\xF4t pour cloner une version locale dans notre r\xE9pertoire de r\xE9sultats qui stockera les artefacts d\u2019entra\xEEnement :"),dn=m(),g(Nt.$$.fragment),ua=m(),Hs=r("p"),ir=a("Cela nous permettra de pousser les artefacts vers le "),pa=r("em"),Rt=a("Hub"),hn=a(" en appelant la m\xE9thode "),ct=r("code"),ur=a("repo.push_to_hub()"),fn=a(" pendant l\u2019entra\xEEnement ! Concluons maintenant notre analyse en \xE9crivant la boucle d\u2019entra\xEEnement."),dt=m(),Fs=r("h3"),ys=r("a"),ma=r("span"),g(He.$$.fragment),pr=m(),It=r("span"),mr=a("Boucle d'entra\xEEnement"),vn=m(),Js=r("p"),cr=a("La boucle d\u2019entra\xEEnement pour le r\xE9sum\xE9 est assez similaire aux autres exemples \u{1F917} "),Aa=r("em"),Oa=a("Accelerate"),dr=a(" que nous avons rencontr\xE9s et est grossi\xE8rement divis\xE9e en quatre \xE9tapes principales :"),_n=m(),vs=r("ol"),Gt=r("li"),hr=a("entra\xEEner le mod\xE8le en it\xE9rant sur tous les exemples dans "),ca=r("code"),Ut=a("train_dataloader"),gn=a(" pour chaque \xE9poque,"),Ht=m(),ht=r("li"),da=a("g\xE9n\xE9rer les r\xE9sum\xE9s du mod\xE8le \xE0 la fin de chaque \xE9poque, en g\xE9n\xE9rant d\u2019abord les "),Na=r("em"),bn=a("tokens"),nt=a(" puis en les d\xE9codant (ainsi que les r\xE9sum\xE9s de r\xE9f\xE9rence) en texte,"),Ft=m(),qn=r("li"),fr=a("calculer les scores ROUGE en utilisant les m\xEAmes techniques que nous avons vues pr\xE9c\xE9demment,"),Jt=m(),Ye=r("li"),jn=a("sauvegarder les "),Bs=r("em"),vr=a("checkpoints"),Ra=a(" et pousser le tout vers le "),Ia=r("em"),_r=a("Hub"),$n=a(". Ici, nous nous appuyons sur l\u2019argument "),rt=r("code"),xn=a("blocking=False"),Vs=a(" de l\u2019objet "),Ga=r("code"),Ua=a("Repository"),gr=a(" afin de pouvoir pousser les "),Ha=r("em"),Fa=a("checkpoints"),br=a(" par \xE9poque de mani\xE8re "),Ja=r("em"),En=a("asynchrone"),lt=a(". Cela nous permet de poursuivre l\u2019entra\xEEnement sans avoir \xE0 attendre le t\xE9l\xE9chargement quelque peu lent associ\xE9 \xE0 un mod\xE8le de la taille d\u20191 Go !"),ot=m(),ft=r("p"),Bt=a("Ces \xE9tapes peuvent \xEAtre vues dans le bloc de code suivant :"),kn=m(),g(it.$$.fragment),wn=m(),g(vt.$$.fragment),ws=m(),_t=r("p"),Ba=a("Et c\u2019est tout ! Une fois que vous l\u2019aurez ex\xE9cut\xE9, vous aurez un mod\xE8le et des r\xE9sultats assez similaires \xE0 ceux que nous avons obtenus avec le "),Va=r("code"),qr=a("Trainer"),ha=a("."),this.h()},l(p){d=l(p,"H2",{class:!0});var z=o(d);E=l(z,"A",{id:!0,class:!0,href:!0});var gl=o(E);h=l(gl,"SPAN",{});var bl=o(h);b(y.$$.fragment,bl),bl.forEach(t),gl.forEach(t),L=c(z),k=l(z,"SPAN",{});var Wa=o(k);T=l(Wa,"I",{});var Vt=o(T);M=n(Vt,"Finetuning"),Vt.forEach(t),P=n(Wa," de mT5 avec \u{1F917} "),S=l(Wa,"I",{});var zn=o(S);A=n(zn,"Accelerate"),zn.forEach(t),Wa.forEach(t),z.forEach(t),O=c(p),w=l(p,"P",{});var Fe=o(w);I=n(Fe,"Le "),D=l(Fe,"EM",{});var Ka=o(D);N=n(Ka,"finetuning"),Ka.forEach(t),oe=n(Fe," de notre mod\xE8le avec \u{1F917} "),W=l(Fe,"EM",{});var Tn=o(W);re=n(Tn,"Accelerate"),Tn.forEach(t),F=n(Fe," est tr\xE8s similaire \xE0 l\u2019exemple de classification de texte que nous avons rencontr\xE9 dans le "),ee=l(Fe,"A",{href:!0});var ql=o(ee);G=n(ql,"chapitre 3"),ql.forEach(t),he=n(Fe,". Les principales diff\xE9rences seront la n\xE9cessit\xE9 de g\xE9n\xE9rer explicitement nos r\xE9sum\xE9s pendant l\u2019entra\xEEnement et de d\xE9finir comment nous calculons les scores ROUGE (rappelons que le "),K=l(Fe,"CODE",{});var jl=o(K);ie=n(jl,"Seq2SeqTrainer"),jl.forEach(t),ne=n(Fe," s\u2019est occup\xE9 de la g\xE9n\xE9ration pour nous). Voyons comment nous pouvons mettre en \u0153uvre ces deux exigences dans \u{1F917} "),Q=l(Fe,"EM",{});var Cn=o(Q);R=n(Cn,"Accelerate"),Cn.forEach(t),se=n(Fe," !"),Fe.forEach(t),B=c(p),ae=l(p,"H3",{class:!0});var Pn=o(ae);Y=l(Pn,"A",{id:!0,class:!0,href:!0});var $l=o(Y);ge=l($l,"SPAN",{});var Qa=o(ge);b(Z.$$.fragment,Qa),Qa.forEach(t),$l.forEach(t),ke=c(Pn),me=l(Pn,"SPAN",{});var xl=o(me);le=n(xl,"Pr\xE9parer tout pour l'entra\xEEnement"),xl.forEach(t),Pn.forEach(t),fe=c(p),ce=l(p,"P",{});var fa=o(ce);H=n(fa,"La premi\xE8re chose que nous devons faire est de cr\xE9er un "),Ce=l(fa,"CODE",{});var _s=o(Ce);X=n(_s,"DataLoader"),_s.forEach(t),be=n(fa," pour chacun de nos \xE9chantillons. Puisque les chargeurs de donn\xE9es PyTorch attendent des batchs de tenseurs, nous devons d\xE9finir le format \xE0 "),Te=l(fa,"CODE",{});var gt=o(Te);Ne=n(gt,'"torch"'),gt.forEach(t),x=n(fa," dans nos jeux de donn\xE9es :"),fa.forEach(t),J=c(p),b(Pe.$$.fragment,p),De=c(p),qe=l(p,"P",{});var Wt=o(qe);Ie=n(Wt,"Maintenant que nous avons des jeux de donn\xE9es constitu\xE9s uniquement de tenseurs, la prochaine chose \xE0 faire est d\u2019instancier \xE0 nouveau le "),je=l(Wt,"CODE",{});var va=o(je);Ve=n(va,"DataCollatorForSeq2Seq"),va.forEach(t),Ge=n(Wt,". Pour cela, nous devons fournir une nouvelle version du mod\xE8le, donc chargeons-le \xE0 nouveau depuis notre cache :"),Wt.forEach(t),ss=c(p),b(ye.$$.fragment,p),$e=c(p),ms=l(p,"P",{});var El=o(ms);ts=n(El,"Nous pouvons ensuite instancier le collateur de donn\xE9es et l\u2019utiliser pour d\xE9finir nos chargeurs de donn\xE9es :"),El.forEach(t),Se=c(p),b(as.$$.fragment,p),cs=c(p),Le=l(p,"P",{});var Dn=o(Le);Ls=n(Dn,"La prochaine chose \xE0 faire est de d\xE9finir l\u2019optimiseur que nous voulons utiliser. Comme dans nos autres exemples, nous allons utiliser "),ns=l(Dn,"CODE",{});var _a=o(ns);bs=n(_a,"AdamW"),_a.forEach(t),de=n(Dn,", qui fonctionne bien pour la plupart des probl\xE8mes :"),Dn.forEach(t),ds=c(p),b(ve.$$.fragment,p),qs=c(p),_e=l(p,"P",{});var Sn=o(_e);js=n(Sn,"Enfin, nous introduisons notre mod\xE8le, notre optimiseur et nos chargeurs de donn\xE9es dans la m\xE9thode "),rs=l(Sn,"CODE",{});var Ln=o(rs);$s=n(Ln,"accelerator.prepare()"),Ln.forEach(t),xs=n(Sn," :"),Sn.forEach(t),hs=c(p),b(We.$$.fragment,p),$=c(p),b(V.$$.fragment,p),Ms=c(p),ls=l(p,"P",{});var kl=o(ls);Es=n(kl,"Maintenant que nous avons pr\xE9par\xE9 nos objets, il reste trois choses \xE0 faire :"),kl.forEach(t),os=c(p),xe=l(p,"UL",{});var ga=o(xe);Ke=l(ga,"LI",{});var wl=o(Ke);As=n(wl,"d\xE9finir le programmeur du taux d\u2019apprentissage,"),wl.forEach(t),Qe=c(ga),ue=l(ga,"LI",{});var Ya=o(ue);Ue=n(Ya,"impl\xE9menter une fonction pour post-traiter les r\xE9sum\xE9s pour l\u2019\xE9valuation,"),Ya.forEach(t),U=c(ga),pe=l(ga,"LI",{});var Mn=o(pe);et=n(Mn,"cr\xE9er un d\xE9p\xF4t sur le "),Os=l(Mn,"EM",{});var yl=o(Os);Ns=n(yl,"Hub"),yl.forEach(t),Ee=n(Mn," vers lequel nous pouvons pousser notre mod\xE8le."),Mn.forEach(t),ga.forEach(t),Is=c(p),ks=l(p,"P",{});var bt=o(ks);Je=n(bt,"Pour le programmeur de taux d\u2019apprentissage, nous utiliserons le programmeur lin\xE9aire standard des sections pr\xE9c\xE9dentes :"),bt.forEach(t),Rs=c(p),b(Be.$$.fragment,p),ut=c(p),Me=l(p,"P",{});var Xa=o(Me);aa=n(Xa,"Pour le post-traitement, nous avons besoin d\u2019une fonction qui divise les r\xE9sum\xE9s g\xE9n\xE9r\xE9s en phrases s\xE9par\xE9es par des nouvelles lignes. C\u2019est le format attendu par la m\xE9trique ROUGE et nous pouvons y parvenir avec le bout de code suivant :"),Xa.forEach(t),ze=c(p),b(Gs.$$.fragment,p),Lt=c(p),is=l(p,"P",{});var Ws=o(is);na=n(Ws,"Cela devrait vous sembler familier si vous vous rappelez comment nous avons d\xE9fini la fonction "),pt=l(Ws,"CODE",{});var zl=o(pt);st=n(zl,"compute_metrics()"),zl.forEach(t),ra=n(Ws," du "),mt=l(Ws,"CODE",{});var Tl=o(mt);Ae=n(Tl,"Seq2SeqTrainer"),Tl.forEach(t),la=n(Ws,"."),Ws.forEach(t),Mt=c(p),fs=l(p,"P",{});var Ks=o(fs);Us=n(Ks,"Enfin, nous devons cr\xE9er un d\xE9p\xF4t de mod\xE8les sur le "),tt=l(Ks,"EM",{});var Cl=o(tt);Re=n(Cl,"Hub"),Cl.forEach(t),rr=n(Ks,". Pour cela, nous pouvons utiliser la biblioth\xE8que \u{1F917} "),At=l(Ks,"EM",{});var An=o(At);lr=n(An,"Hub"),An.forEach(t),or=n(Ks,", qui porte le nom appropri\xE9. Nous avons juste besoin de d\xE9finir un nom pour notre d\xE9p\xF4t, et la biblioth\xE8que a une fonction utilitaire pour combiner l\u2019identifiant du d\xE9p\xF4t avec le profil de l\u2019utilisateur :"),Ks.forEach(t),oa=c(p),b(Ot.$$.fragment,p),mn=c(p),b(at.$$.fragment,p),cn=c(p),ia=l(p,"P",{});var Pl=o(ia);Ma=n(Pl,"Nous pouvons maintenant utiliser ce nom de d\xE9p\xF4t pour cloner une version locale dans notre r\xE9pertoire de r\xE9sultats qui stockera les artefacts d\u2019entra\xEEnement :"),Pl.forEach(t),dn=c(p),b(Nt.$$.fragment,p),ua=c(p),Hs=l(p,"P",{});var ba=o(Hs);ir=n(ba,"Cela nous permettra de pousser les artefacts vers le "),pa=l(ba,"EM",{});var Dl=o(pa);Rt=n(Dl,"Hub"),Dl.forEach(t),hn=n(ba," en appelant la m\xE9thode "),ct=l(ba,"CODE",{});var Za=o(ct);ur=n(Za,"repo.push_to_hub()"),Za.forEach(t),fn=n(ba," pendant l\u2019entra\xEEnement ! Concluons maintenant notre analyse en \xE9crivant la boucle d\u2019entra\xEEnement."),ba.forEach(t),dt=c(p),Fs=l(p,"H3",{class:!0});var On=o(Fs);ys=l(On,"A",{id:!0,class:!0,href:!0});var Sl=o(ys);ma=l(Sl,"SPAN",{});var qt=o(ma);b(He.$$.fragment,qt),qt.forEach(t),Sl.forEach(t),pr=c(On),It=l(On,"SPAN",{});var en=o(It);mr=n(en,"Boucle d'entra\xEEnement"),en.forEach(t),On.forEach(t),vn=c(p),Js=l(p,"P",{});var jt=o(Js);cr=n(jt,"La boucle d\u2019entra\xEEnement pour le r\xE9sum\xE9 est assez similaire aux autres exemples \u{1F917} "),Aa=l(jt,"EM",{});var Ll=o(Aa);Oa=n(Ll,"Accelerate"),Ll.forEach(t),dr=n(jt," que nous avons rencontr\xE9s et est grossi\xE8rement divis\xE9e en quatre \xE9tapes principales :"),jt.forEach(t),_n=c(p),vs=l(p,"OL",{});var $t=o(vs);Gt=l($t,"LI",{});var Qs=o(Gt);hr=n(Qs,"entra\xEEner le mod\xE8le en it\xE9rant sur tous les exemples dans "),ca=l(Qs,"CODE",{});var Ml=o(ca);Ut=n(Ml,"train_dataloader"),Ml.forEach(t),gn=n(Qs," pour chaque \xE9poque,"),Qs.forEach(t),Ht=c($t),ht=l($t,"LI",{});var qa=o(ht);da=n(qa,"g\xE9n\xE9rer les r\xE9sum\xE9s du mod\xE8le \xE0 la fin de chaque \xE9poque, en g\xE9n\xE9rant d\u2019abord les "),Na=l(qa,"EM",{});var Al=o(Na);bn=n(Al,"tokens"),Al.forEach(t),nt=n(qa," puis en les d\xE9codant (ainsi que les r\xE9sum\xE9s de r\xE9f\xE9rence) en texte,"),qa.forEach(t),Ft=c($t),qn=l($t,"LI",{});var Ol=o(qn);fr=n(Ol,"calculer les scores ROUGE en utilisant les m\xEAmes techniques que nous avons vues pr\xE9c\xE9demment,"),Ol.forEach(t),Jt=c($t),Ye=l($t,"LI",{});var Xe=o(Ye);jn=n(Xe,"sauvegarder les "),Bs=l(Xe,"EM",{});var Nl=o(Bs);vr=n(Nl,"checkpoints"),Nl.forEach(t),Ra=n(Xe," et pousser le tout vers le "),Ia=l(Xe,"EM",{});var Rl=o(Ia);_r=n(Rl,"Hub"),Rl.forEach(t),$n=n(Xe,". Ici, nous nous appuyons sur l\u2019argument "),rt=l(Xe,"CODE",{});var Il=o(rt);xn=n(Il,"blocking=False"),Il.forEach(t),Vs=n(Xe," de l\u2019objet "),Ga=l(Xe,"CODE",{});var sn=o(Ga);Ua=n(sn,"Repository"),sn.forEach(t),gr=n(Xe," afin de pouvoir pousser les "),Ha=l(Xe,"EM",{});var Gl=o(Ha);Fa=n(Gl,"checkpoints"),Gl.forEach(t),br=n(Xe," par \xE9poque de mani\xE8re "),Ja=l(Xe,"EM",{});var Ul=o(Ja);En=n(Ul,"asynchrone"),Ul.forEach(t),lt=n(Xe,". Cela nous permet de poursuivre l\u2019entra\xEEnement sans avoir \xE0 attendre le t\xE9l\xE9chargement quelque peu lent associ\xE9 \xE0 un mod\xE8le de la taille d\u20191 Go !"),Xe.forEach(t),$t.forEach(t),ot=c(p),ft=l(p,"P",{});var xt=o(ft);Bt=n(xt,"Ces \xE9tapes peuvent \xEAtre vues dans le bloc de code suivant :"),xt.forEach(t),kn=c(p),b(it.$$.fragment,p),wn=c(p),b(vt.$$.fragment,p),ws=c(p),_t=l(p,"P",{});var Kt=o(_t);Ba=n(Kt,"Et c\u2019est tout ! Une fois que vous l\u2019aurez ex\xE9cut\xE9, vous aurez un mod\xE8le et des r\xE9sultats assez similaires \xE0 ceux que nous avons obtenus avec le "),Va=l(Kt,"CODE",{});var ja=o(Va);qr=n(ja,"Trainer"),ja.forEach(t),ha=n(Kt,"."),Kt.forEach(t),this.h()},h(){f(E,"id","ifinetuningi-de-mt5-avec-iacceleratei"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#ifinetuningi-de-mt5-avec-iacceleratei"),f(d,"class","relative group"),f(ee,"href","/course/fr/chapter3"),f(Y,"id","prparer-tout-pour-lentranement"),f(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Y,"href","#prparer-tout-pour-lentranement"),f(ae,"class","relative group"),f(ys,"id","boucle-dentranement"),f(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ys,"href","#boucle-dentranement"),f(Fs,"class","relative group")},m(p,z){i(p,d,z),e(d,E),e(E,h),q(y,h,null),e(d,L),e(d,k),e(k,T),e(T,M),e(k,P),e(k,S),e(S,A),i(p,O,z),i(p,w,z),e(w,I),e(w,D),e(D,N),e(w,oe),e(w,W),e(W,re),e(w,F),e(w,ee),e(ee,G),e(w,he),e(w,K),e(K,ie),e(w,ne),e(w,Q),e(Q,R),e(w,se),i(p,B,z),i(p,ae,z),e(ae,Y),e(Y,ge),q(Z,ge,null),e(ae,ke),e(ae,me),e(me,le),i(p,fe,z),i(p,ce,z),e(ce,H),e(ce,Ce),e(Ce,X),e(ce,be),e(ce,Te),e(Te,Ne),e(ce,x),i(p,J,z),q(Pe,p,z),i(p,De,z),i(p,qe,z),e(qe,Ie),e(qe,je),e(je,Ve),e(qe,Ge),i(p,ss,z),q(ye,p,z),i(p,$e,z),i(p,ms,z),e(ms,ts),i(p,Se,z),q(as,p,z),i(p,cs,z),i(p,Le,z),e(Le,Ls),e(Le,ns),e(ns,bs),e(Le,de),i(p,ds,z),q(ve,p,z),i(p,qs,z),i(p,_e,z),e(_e,js),e(_e,rs),e(rs,$s),e(_e,xs),i(p,hs,z),q(We,p,z),i(p,$,z),q(V,p,z),i(p,Ms,z),i(p,ls,z),e(ls,Es),i(p,os,z),i(p,xe,z),e(xe,Ke),e(Ke,As),e(xe,Qe),e(xe,ue),e(ue,Ue),e(xe,U),e(xe,pe),e(pe,et),e(pe,Os),e(Os,Ns),e(pe,Ee),i(p,Is,z),i(p,ks,z),e(ks,Je),i(p,Rs,z),q(Be,p,z),i(p,ut,z),i(p,Me,z),e(Me,aa),i(p,ze,z),q(Gs,p,z),i(p,Lt,z),i(p,is,z),e(is,na),e(is,pt),e(pt,st),e(is,ra),e(is,mt),e(mt,Ae),e(is,la),i(p,Mt,z),i(p,fs,z),e(fs,Us),e(fs,tt),e(tt,Re),e(fs,rr),e(fs,At),e(At,lr),e(fs,or),i(p,oa,z),q(Ot,p,z),i(p,mn,z),q(at,p,z),i(p,cn,z),i(p,ia,z),e(ia,Ma),i(p,dn,z),q(Nt,p,z),i(p,ua,z),i(p,Hs,z),e(Hs,ir),e(Hs,pa),e(pa,Rt),e(Hs,hn),e(Hs,ct),e(ct,ur),e(Hs,fn),i(p,dt,z),i(p,Fs,z),e(Fs,ys),e(ys,ma),q(He,ma,null),e(Fs,pr),e(Fs,It),e(It,mr),i(p,vn,z),i(p,Js,z),e(Js,cr),e(Js,Aa),e(Aa,Oa),e(Js,dr),i(p,_n,z),i(p,vs,z),e(vs,Gt),e(Gt,hr),e(Gt,ca),e(ca,Ut),e(Gt,gn),e(vs,Ht),e(vs,ht),e(ht,da),e(ht,Na),e(Na,bn),e(ht,nt),e(vs,Ft),e(vs,qn),e(qn,fr),e(vs,Jt),e(vs,Ye),e(Ye,jn),e(Ye,Bs),e(Bs,vr),e(Ye,Ra),e(Ye,Ia),e(Ia,_r),e(Ye,$n),e(Ye,rt),e(rt,xn),e(Ye,Vs),e(Ye,Ga),e(Ga,Ua),e(Ye,gr),e(Ye,Ha),e(Ha,Fa),e(Ye,br),e(Ye,Ja),e(Ja,En),e(Ye,lt),i(p,ot,z),i(p,ft,z),e(ft,Bt),i(p,kn,z),q(it,p,z),i(p,wn,z),q(vt,p,z),i(p,ws,z),i(p,_t,z),e(_t,Ba),e(_t,Va),e(Va,qr),e(_t,ha),yn=!0},i(p){yn||(v(y.$$.fragment,p),v(Z.$$.fragment,p),v(Pe.$$.fragment,p),v(ye.$$.fragment,p),v(as.$$.fragment,p),v(ve.$$.fragment,p),v(We.$$.fragment,p),v(V.$$.fragment,p),v(Be.$$.fragment,p),v(Gs.$$.fragment,p),v(Ot.$$.fragment,p),v(at.$$.fragment,p),v(Nt.$$.fragment,p),v(He.$$.fragment,p),v(it.$$.fragment,p),v(vt.$$.fragment,p),yn=!0)},o(p){_(y.$$.fragment,p),_(Z.$$.fragment,p),_(Pe.$$.fragment,p),_(ye.$$.fragment,p),_(as.$$.fragment,p),_(ve.$$.fragment,p),_(We.$$.fragment,p),_(V.$$.fragment,p),_(Be.$$.fragment,p),_(Gs.$$.fragment,p),_(Ot.$$.fragment,p),_(at.$$.fragment,p),_(Nt.$$.fragment,p),_(He.$$.fragment,p),_(it.$$.fragment,p),_(vt.$$.fragment,p),yn=!1},d(p){p&&t(d),j(y),p&&t(O),p&&t(w),p&&t(B),p&&t(ae),j(Z),p&&t(fe),p&&t(ce),p&&t(J),j(Pe,p),p&&t(De),p&&t(qe),p&&t(ss),j(ye,p),p&&t($e),p&&t(ms),p&&t(Se),j(as,p),p&&t(cs),p&&t(Le),p&&t(ds),j(ve,p),p&&t(qs),p&&t(_e),p&&t(hs),j(We,p),p&&t($),j(V,p),p&&t(Ms),p&&t(ls),p&&t(os),p&&t(xe),p&&t(Is),p&&t(ks),p&&t(Rs),j(Be,p),p&&t(ut),p&&t(Me),p&&t(ze),j(Gs,p),p&&t(Lt),p&&t(is),p&&t(Mt),p&&t(fs),p&&t(oa),j(Ot,p),p&&t(mn),j(at,p),p&&t(cn),p&&t(ia),p&&t(dn),j(Nt,p),p&&t(ua),p&&t(Hs),p&&t(dt),p&&t(Fs),j(He),p&&t(vn),p&&t(Js),p&&t(_n),p&&t(vs),p&&t(ot),p&&t(ft),p&&t(kn),j(it,p),p&&t(wn),j(vt,p),p&&t(ws),p&&t(_t)}}}function y1(te){let d,E,h,y,L;return{c(){d=r("p"),E=a("\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),h=r("a"),y=a("chapitre 3"),L=a(" pour plus de d\xE9tails."),this.h()},l(k){d=l(k,"P",{});var T=o(d);E=n(T,"\u{1F6A8} Si vous vous entra\xEEnez sur un TPU, vous devrez d\xE9placer tout le code ci-dessus dans une fonction d\u2019entra\xEEnement d\xE9di\xE9e. Voir le "),h=l(T,"A",{href:!0});var M=o(h);y=n(M,"chapitre 3"),M.forEach(t),L=n(T," pour plus de d\xE9tails."),T.forEach(t),this.h()},h(){f(h,"href","/course/fr/chapter3")},m(k,T){i(k,d,T),e(d,E),e(d,h),e(h,y),e(d,L)},d(k){k&&t(d)}}}function z1(te){let d,E,h,y,L,k,T,M,P,S,A,O,w,I,D,N,oe,W,re,F,ee,G,he,K,ie,ne,Q,R,se,B,ae,Y,ge,Z,ke,me,le,fe,ce,H,Ce,X,be,Te,Ne,x,J,Pe,De,qe,Ie,je,Ve,Ge,ss,ye,$e,ms,ts,Se,as,cs,Le,Ls,ns,bs,de,ds,ve,qs,_e,js,rs,$s,xs,hs,We,$,V,Ms,ls,Es,os,xe,Ke,As,Qe,ue,Ue,U,pe,et,Os,Ns,Ee,Is,ks,Je,Rs,Be,ut,Me,aa,ze,Gs,Lt,is,na,pt,st,ra,mt,Ae,la,Mt,fs,Us,tt,Re,rr,At,lr,or,oa,Ot,mn,at,cn,ia,Ma,dn,Nt,ua,Hs,ir,pa,Rt,hn,ct,ur,fn,dt,Fs,ys,ma,He,pr,It,mr,vn,Js,cr,Aa,Oa,dr,_n,vs,Gt,hr,ca,Ut,gn,Ht,ht,da,Na,bn,nt,Ft,qn,fr,Jt,Ye,jn,Bs,vr,Ra,Ia,_r,$n,rt,xn,Vs,Ga,Ua,gr,Ha,Fa,br,Ja,En,lt,ot,ft,Bt,kn,it,wn,vt,ws,_t,Ba,Va,qr,ha,yn,p,z,gl,bl,Wa,Vt,zn,Fe,Ka,Tn,ql,jl,Cn,Pn,$l,Qa,xl,fa,_s,gt,Wt,va,El,Dn,_a,Sn,Ln,kl,ga,wl,Ya,Mn,yl,bt,Xa,Ws,zl,Tl,Ks,Cl,An,Pl,ba,Dl,Za,On,Sl,qt,en,jt,Ll,$t,Qs,Ml,qa,Al,Ol,Xe,Nl,Rl,Il,sn,Gl,Ul,xt,Kt,ja,Om,Nm,Ho,Rm,Im,Hl,Gm,Um,tn,Fl,jr,Hm,Fm,$r,Jm,Fo,Bm,Vm,Wm,Jl,Km,Qm,an,Bl,xr,Ym,Xm,Jo,Zm,ec,Vl,sc,Nu,$a,tc,Bo,ac,nc,Vo,rc,lc,Ru,Et,oc,Wo,ic,uc,Ko,pc,mc,Qo,cc,dc,Iu,nn,Er,Ev,hc,kr,kv,Gu,Wl,fc,Uu,Nn,Hu,rn,Rn,Yo,wr,vc,Xo,_c,Fu,yr,Ju,zs,gc,Zo,bc,qc,ei,jc,$c,si,xc,Ec,ti,kc,wc,ai,yc,zc,Bu,zr,Vu,In,Wu,Gn,Tc,ni,Cc,Pc,Ku,Tr,Qu,Cr,Yu,Ze,Dc,ri,Sc,Lc,li,Mc,Ac,oi,Oc,Nc,Kl,Rc,Ic,ii,Gc,Uc,ui,Hc,Fc,pi,Jc,Bc,Xu,Pr,Zu,Dr,ep,Ts,Vc,mi,Wc,Kc,ci,Qc,Yc,di,Xc,Zc,hi,ed,sd,Ql,td,ad,sp,xa,nd,fi,rd,ld,vi,od,id,tp,Sr,ap,Ys,ud,_i,pd,md,gi,cd,dd,bi,hd,fd,qi,vd,_d,np,Ea,gd,ji,bd,qd,$i,jd,$d,rp,Lr,lp,Yl,xd,op,Un,ip,ln,Hn,xi,Mr,Ed,Ei,kd,up,Ar,pp,Xl,wd,mp,ka,yd,Or,zd,Td,ki,Cd,Pd,cp,Nr,dp,wa,Dd,wi,Sd,Ld,yi,Md,Ad,hp,Fn,fp,Rr,Od,vp,n1='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">i</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi></mrow><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">l</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">r</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">r</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mi mathvariant="normal">f</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Recall} = \\frac{\\mathrm{Nombre\\,de\\,mots\\,qui\\,se\\,chevauchent}}{\\mathrm{Nombre\\, total\\, de\\, mots\\, dans\\, le\\, r\xE9sum\xE9\\, de\\, r\xE9ference}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Recall</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Nombre</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">de</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">mots</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">dans</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">le</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">r</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mord mathrm">sum</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">de</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">r</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mord mathrm">ference</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Nombre</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">de</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">mots</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">qui</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">se</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">chevauchent</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',_p,Ir,Nd,gp,r1='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">i</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi></mrow><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">l</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">r</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mtext>\u2009</mtext><mi mathvariant="normal">g</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mi mathvariant="normal">n</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover><mi mathvariant="normal">r</mi><mover accent="true"><mi mathvariant="normal">e</mi><mo>\u02CA</mo></mover></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Precision} = \\frac{\\mathrm{Nombre\\,de\\,mots\\,qui\\,se\\,chevauchent}}{\\mathrm{Nombre\\, total\\, de\\, mots\\, dans\\, le\\, r\xE9sum\xE9\\, g\xE9n\xE9r\xE9}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm">Precision</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Nombre</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">de</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">mots</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">dans</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">le</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">r</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mord mathrm">sum</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mord mathrm">n</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span><span class="mord mathrm">r</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathrm">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mathrm">\u02CA</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Nombre</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">de</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">mots</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">qui</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">se</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">chevauchent</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',bp,kt,Rd,zi,Id,Gd,Ti,Ud,Hd,Ci,Fd,Jd,qp,Gr,jp,Zl,Bd,$p,Ur,xp,Jn,Vd,Pi,Wd,Kd,Ep,Hr,kp,Fr,wp,es,Qd,Di,Yd,Xd,Si,Zd,eh,Li,sh,th,Mi,ah,nh,Ai,rh,lh,Oi,oh,ih,Ni,uh,ph,yp,Jr,zp,Br,Tp,Cs,mh,Ri,ch,dh,Ii,hh,fh,Gi,vh,_h,Ui,gh,bh,Hi,qh,jh,Cp,Bn,Pp,Vn,$h,Fi,xh,Eh,Dp,on,Wn,Ji,Vr,kh,Bi,wh,Sp,gs,yh,Vi,zh,Th,Wi,Ch,Ph,Ki,Dh,Sh,Qi,Lh,Mh,Yi,Ah,Oh,Xi,Nh,Rh,Lp,Wr,Mp,eo,Ih,Ap,Kr,Op,ya,Gh,Zi,Uh,Hh,eu,Fh,Jh,Np,Qr,Rp,Yr,Ip,so,Bh,Gp,Xr,Up,to,Vh,Hp,Zr,Fp,el,Jp,Ps,Wh,su,Kh,Qh,tu,Yh,Xh,au,Zh,ef,nu,sf,tf,ru,af,nf,Bp,Qt,Yt,ao,Kn,Vp,za,rf,lu,lf,of,ou,uf,pf,Wp,sl,Kp,Qn,mf,iu,cf,df,Qp,tl,Yp,no,Ta,hf,uu,ff,vf,ro,_f,gf,Xp,Xs,bf,pu,qf,jf,mu,$f,xf,cu,Ef,kf,du,wf,yf,Zp,Xt,Zt,lo,oo,zf,em,al,sm,Ca,Tf,hu,Cf,Pf,fu,Df,Sf,tm,nl,am,rl,nm,we,Lf,vu,Mf,Af,_u,Of,Nf,gu,Rf,If,bu,Gf,Uf,qu,Hf,Ff,ju,Jf,Bf,$u,Vf,Wf,xu,Kf,Qf,Eu,Yf,Xf,ku,Zf,ev,wu,sv,tv,rm,ea,sa,io,uo,un,Yn,yu,ll,av,po,nv,zu,rv,lm,wt,lv,Tu,ov,iv,Cu,uv,pv,Pu,mv,cv,om,ol,im,mo,dv,um,il,pm,co,hv,mm,ul,cm,pl,dm,Xn,fv,Du,vv,_v,hm,ml,fm,cl,vm,ho,gv,_m,fo,bv,gm;h=new m1({props:{fw:te[0]}}),M=new St({});const wv=[d1,c1],dl=[];function yv(s,u){return s[0]==="pt"?0:1}w=yv(te),I=dl[w]=wv[w](te),ie=new xv({props:{id:"yHnr5Dk2zCI"}}),je=new St({}),de=new C({props:{code:`from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

spanish_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;es&quot;</span>)
english_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>)
english_dataset`}}),ve=new C({props:{code:`DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">200000</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
})`}}),os=new C({props:{code:`def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_samples</span>(<span class="hljs-params">dataset, num_samples=<span class="hljs-number">3</span>, seed=<span class="hljs-number">42</span></span>):
    sample = dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=seed).select(<span class="hljs-built_in">range</span>(num_samples))
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> sample:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt; Title: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_title&#x27;</span>]}</span>&#x27;&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt; Review: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_body&#x27;</span>]}</span>&#x27;&quot;</span>)


show_samples(english_dataset)`}}),Ke=new C({props:{code:`'>> Title: Worked in front position, not rear'
# Travaill\xE9 en position avant, pas arri\xE8re
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'
# 3 \xE9toiles car ce ne sont pas des freins arri\xE8re comme indiqu\xE9 dans la description de l'article. Au moins, l'adaptateur de montage ne fonctionnait que sur la fourche avant du v\xE9lo pour lequel je l'ai achet\xE9.

'>> Title: meh'
'>> Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'
# Il fait son travail et il est magnifique mais le mien est en train de tomber en morceaux, j'ai d\xFB le recoller avec de la colle chaude.

'>> Title: Can\\'t beat these for the money' 
# On ne peut pas faire mieux pour le prix
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.'
# Je l'ai achet\xE9 pour manipuler diverses pi\xE8ces d'avion et des "trucs" de hangar que je devais organiser ; il a vraiment fait l'affaire. L'unit\xE9 est arriv\xE9e rapidement, \xE9tait bien emball\xE9e et est arriv\xE9e intacte (toujours un bon signe). Il y a cinq supports muraux - trois sur le dessus et deux sur le dessous. Je voulais le monter sur le mur, alors tout ce que j'ai eu \xE0 faire \xE9tait d'enlever les deux couches sup\xE9rieures de tiroirs en plastique, ainsi que les tiroirs d'angle inf\xE9rieurs, de le placer o\xF9 je voulais et de le marquer ; j'ai ensuite utilis\xE9 quelques-uns des nouveaux ancrages muraux \xE0 vis en plastique (la vari\xE9t\xE9 de 50 livres) et il s'est facilement mont\xE9 sur le mur. Certains ont fait remarquer qu'ils voulaient des s\xE9parateurs pour les tiroirs, et qu'ils les ont fabriqu\xE9s. Bonne id\xE9e. Pour ma part, j'avais besoin de quelque chose dont je pouvais voir le contenu \xE0 hauteur des yeux, et je voulais donc des tiroirs plus grands. J'aime aussi le fait qu'il s'agisse du nouveau plastique qui ne se fragilise pas et ne se fend pas comme mes anciens tiroirs en plastique. J'aime la construction enti\xE8rement en plastique. Elle est suffisamment r\xE9sistante pour contenir des pi\xE8ces m\xE9talliques, mais \xE9tant en plastique, elle n'est pas aussi lourde qu'un cadre m\xE9tallique, ce qui permet de la fixer facilement au mur et de la charger d'objets lourds ou l\xE9gers. Aucun probl\xE8me. Pour le prix, c'est imbattable. C'est le meilleur que j'ai achet\xE9 \xE0 ce jour, et j'utilise des versions de ce type depuis plus de quarante ans.`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Worked in front position, not rear&#x27;</span>
<span class="hljs-comment"># Travaill\xE9 en position avant, pas arri\xE8re</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.&#x27;</span>
<span class="hljs-comment"># 3 \xE9toiles car ce ne sont pas des freins arri\xE8re comme indiqu\xE9 dans la description de l&#x27;article. Au moins, l&#x27;adaptateur de montage ne fonctionnait que sur la fourche avant du v\xE9lo pour lequel je l&#x27;ai achet\xE9.</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: meh&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue&#x27;</span>
<span class="hljs-comment"># Il fait son travail et il est magnifique mais le mien est en train de tomber en morceaux, j&#x27;ai d\xFB le recoller avec de la colle chaude.</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Can\\&#x27;t beat these for the money&#x27;</span> 
<span class="hljs-comment"># On ne peut pas faire mieux pour le prix</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Bought this for handling miscellaneous aircraft parts and hanger &quot;stuff&quot; that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\&#x27;t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\&#x27;s heavy duty enough to hold metal parts, but being made of plastic it\\&#x27;s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\&#x27;t beat it. Best one of these I\\&#x27;ve bought to date-- and I\\&#x27;ve been using some version of these for over forty years.&#x27;</span>
<span class="hljs-comment"># Je l&#x27;ai achet\xE9 pour manipuler diverses pi\xE8ces d&#x27;avion et des &quot;trucs&quot; de hangar que je devais organiser ; il a vraiment fait l&#x27;affaire. L&#x27;unit\xE9 est arriv\xE9e rapidement, \xE9tait bien emball\xE9e et est arriv\xE9e intacte (toujours un bon signe). Il y a cinq supports muraux - trois sur le dessus et deux sur le dessous. Je voulais le monter sur le mur, alors tout ce que j&#x27;ai eu \xE0 faire \xE9tait d&#x27;enlever les deux couches sup\xE9rieures de tiroirs en plastique, ainsi que les tiroirs d&#x27;angle inf\xE9rieurs, de le placer o\xF9 je voulais et de le marquer ; j&#x27;ai ensuite utilis\xE9 quelques-uns des nouveaux ancrages muraux \xE0 vis en plastique (la vari\xE9t\xE9 de 50 livres) et il s&#x27;est facilement mont\xE9 sur le mur. Certains ont fait remarquer qu&#x27;ils voulaient des s\xE9parateurs pour les tiroirs, et qu&#x27;ils les ont fabriqu\xE9s. Bonne id\xE9e. Pour ma part, j&#x27;avais besoin de quelque chose dont je pouvais voir le contenu \xE0 hauteur des yeux, et je voulais donc des tiroirs plus grands. J&#x27;aime aussi le fait qu&#x27;il s&#x27;agisse du nouveau plastique qui ne se fragilise pas et ne se fend pas comme mes anciens tiroirs en plastique. J&#x27;aime la construction enti\xE8rement en plastique. Elle est suffisamment r\xE9sistante pour contenir des pi\xE8ces m\xE9talliques, mais \xE9tant en plastique, elle n&#x27;est pas aussi lourde qu&#x27;un cadre m\xE9tallique, ce qui permet de la fixer facilement au mur et de la charger d&#x27;objets lourds ou l\xE9gers. Aucun probl\xE8me. Pour le prix, c&#x27;est imbattable. C&#x27;est le meilleur que j&#x27;ai achet\xE9 \xE0 ce jour, et j&#x27;utilise des versions de ce type depuis plus de quarante ans.</span>`}}),Qe=new nr({props:{$$slots:{default:[h1]},$$scope:{ctx:te}}}),Je=new C({props:{code:`english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Afficher le compte des 20 premiers produits
english_df["product_category"].value_counts()[:20]`,highlighted:`english_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
english_df = english_dataset[<span class="hljs-string">&quot;train&quot;</span>][:]
<span class="hljs-comment"># Afficher le compte des 20 premiers produits</span>
english_df[<span class="hljs-string">&quot;product_category&quot;</span>].value_counts()[:<span class="hljs-number">20</span>]`}}),Be=new C({props:{code:`home                      17679   # maison
apparel                   15951   # v\xEAtements
wireless                  15717   # sans fil
other                     13418   # autres
beauty                    12091   # beaut\xE9
drugstore                 11730   # pharmacie
kitchen                   10382   # cuisine
toy                        8745   # jouets
sports                     8277   # sports
automotive                 7506   # automobile
lawn_and_garden            7327   # pelouse_et_jardin
home_improvement           7136   # am\xE9lioration_de_la_maison
pet_products               7082   # produits_pour_animaux_de_compagnie
digital_ebook_purchase     6749   # achat_de_livres_num\xE9riques 
pc                         6401   # ordinateur_personnel
electronics                6186   # \xE9lectronique
office_product             5521   # produits_de_bureau 
shoes                      5197   # chaussures 
grocery                    4730   # \xE9picerie
book                       3756   # livre
Name: product_category, dtype: int64`,highlighted:`home                      <span class="hljs-number">17679</span>   <span class="hljs-comment"># maison</span>
apparel                   <span class="hljs-number">15951</span>   <span class="hljs-comment"># v\xEAtements</span>
wireless                  <span class="hljs-number">15717</span>   <span class="hljs-comment"># sans fil</span>
other                     <span class="hljs-number">13418</span>   <span class="hljs-comment"># autres</span>
beauty                    <span class="hljs-number">12091</span>   <span class="hljs-comment"># beaut\xE9</span>
drugstore                 <span class="hljs-number">11730</span>   <span class="hljs-comment"># pharmacie</span>
kitchen                   <span class="hljs-number">10382</span>   <span class="hljs-comment"># cuisine</span>
toy                        <span class="hljs-number">8745</span>   <span class="hljs-comment"># jouets</span>
sports                     <span class="hljs-number">8277</span>   <span class="hljs-comment"># sports</span>
automotive                 <span class="hljs-number">7506</span>   <span class="hljs-comment"># automobile</span>
lawn_and_garden            <span class="hljs-number">7327</span>   <span class="hljs-comment"># pelouse_et_jardin</span>
home_improvement           <span class="hljs-number">7136</span>   <span class="hljs-comment"># am\xE9lioration_de_la_maison</span>
pet_products               <span class="hljs-number">7082</span>   <span class="hljs-comment"># produits_pour_animaux_de_compagnie</span>
digital_ebook_purchase     <span class="hljs-number">6749</span>   <span class="hljs-comment"># achat_de_livres_num\xE9riques </span>
pc                         <span class="hljs-number">6401</span>   <span class="hljs-comment"># ordinateur_personnel</span>
electronics                <span class="hljs-number">6186</span>   <span class="hljs-comment"># \xE9lectronique</span>
office_product             <span class="hljs-number">5521</span>   <span class="hljs-comment"># produits_de_bureau </span>
shoes                      <span class="hljs-number">5197</span>   <span class="hljs-comment"># chaussures </span>
grocery                    <span class="hljs-number">4730</span>   <span class="hljs-comment"># \xE9picerie</span>
book                       <span class="hljs-number">3756</span>   <span class="hljs-comment"># livre</span>
Name: product_category, dtype: int64`}}),Us=new C({props:{code:`def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_books</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> (
        example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;book&quot;</span>
        <span class="hljs-keyword">or</span> example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;digital_ebook_purchase&quot;</span>
    )`}}),Rt=new C({props:{code:"english_dataset.reset_format()",highlighted:"english_dataset.reset_format()"}}),dt=new C({props:{code:`spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)`,highlighted:`spanish_books = spanish_dataset.<span class="hljs-built_in">filter</span>(filter_books)
english_books = english_dataset.<span class="hljs-built_in">filter</span>(filter_books)
show_samples(english_books)`}}),ys=new C({props:{code:`'>> Title: I\\'m dissapointed.' 
# Je suis d\xE9\xE7u
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.'
# Je suppose que j'avais de plus grandes attentes pour ce livre d'apr\xE8s les critiques. Je pensais vraiment que j'allais au moins l'aimer. L'id\xE9e de l'intrigue \xE9tait g\xE9niale. J'aimais Ash, mais \xE7a n'allait nulle part. La plus grande partie du livre \xE9tait consacr\xE9e \xE0 leur \xE9mission de radio et aux conversations avec les auditeurs. Je voulais que l'auteur creuse plus profond\xE9ment pour que nous puissions vraiment conna\xEEtre les personnages. Tout ce que nous savons de Grace, c'est qu'elle est s\xE9duisante, qu'elle est latino et qu'elle est une sorte de garce. Je suis d\xE9\xE7ue.

'>> Title: Good art, good price, poor design' 
# Un bon art, un bon prix, un mauvais design
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'
# J'ai eu le calendrier DC Vintage ces deux derni\xE8res ann\xE9es, mais il \xE9tait en rupture de stock pour toujours cette ann\xE9e et j'ai vu qu'ils avaient r\xE9duit les dimensions sans raison valable. Celui-ci a de bons choix artistiques mais le design a le pli qui traverse l'image, donc c'est moins esth\xE9tique, surtout si vous voulez garder une image \xE0 accrocher. Pour le prix, c'est un bon calendrier.

'>> Title: Helpful'
# Utile
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
# Presque tous les conseils sont utiles et. Je me consid\xE8re comme un utilisateur interm\xE9diaire \xE0 avanc\xE9 de OneNote. Je le recommande vivement.`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: I\\&#x27;m dissapointed.&#x27;</span> 
<span class="hljs-comment"># Je suis d\xE9\xE7u</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I guess I had higher expectations for this book from the reviews. I really thought I\\&#x27;d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\&#x27;m dissapointed.&#x27;</span>
<span class="hljs-comment"># Je suppose que j&#x27;avais de plus grandes attentes pour ce livre d&#x27;apr\xE8s les critiques. Je pensais vraiment que j&#x27;allais au moins l&#x27;aimer. L&#x27;id\xE9e de l&#x27;intrigue \xE9tait g\xE9niale. J&#x27;aimais Ash, mais \xE7a n&#x27;allait nulle part. La plus grande partie du livre \xE9tait consacr\xE9e \xE0 leur \xE9mission de radio et aux conversations avec les auditeurs. Je voulais que l&#x27;auteur creuse plus profond\xE9ment pour que nous puissions vraiment conna\xEEtre les personnages. Tout ce que nous savons de Grace, c&#x27;est qu&#x27;elle est s\xE9duisante, qu&#x27;elle est latino et qu&#x27;elle est une sorte de garce. Je suis d\xE9\xE7ue.</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Good art, good price, poor design&#x27;</span> 
<span class="hljs-comment"># Un bon art, un bon prix, un mauvais design</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\&#x27;s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar&#x27;</span>
<span class="hljs-comment"># J&#x27;ai eu le calendrier DC Vintage ces deux derni\xE8res ann\xE9es, mais il \xE9tait en rupture de stock pour toujours cette ann\xE9e et j&#x27;ai vu qu&#x27;ils avaient r\xE9duit les dimensions sans raison valable. Celui-ci a de bons choix artistiques mais le design a le pli qui traverse l&#x27;image, donc c&#x27;est moins esth\xE9tique, surtout si vous voulez garder une image \xE0 accrocher. Pour le prix, c&#x27;est un bon calendrier.</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Helpful&#x27;</span>
<span class="hljs-comment"># Utile</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.&#x27;</span>
<span class="hljs-comment"># Presque tous les conseils sont utiles et. Je me consid\xE8re comme un utilisateur interm\xE9diaire \xE0 avanc\xE9 de OneNote. Je le recommande vivement.</span>`}}),Ut=new C({props:{code:`from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Quelques exemples
show_samples(books_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

<span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># Quelques exemples</span>
show_samples(books_dataset)`}}),Ht=new C({props:{code:`'>> Title: Easy to follow!!!!' 
# Facile \xE0 suivre!!!!
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'
# J'ai ador\xE9 The dash diet weight loss Solution. Jamais faim. Je recommande ce r\xE9gime. Les menus sont \xE9galement bien arrondis. Essayez-le. Il contient beaucoup d'informations, merci.

'>> Title: PARCIALMENTE DA\xD1ADO' 
# PARTIELLEMENT ENDOMMAG\xC9
'>> Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).'
# Il est arriv\xE9 le jour pr\xE9vu, avec d'autres livres que j'avais command\xE9s, mais la bo\xEEte est arriv\xE9e en mauvais \xE9tat, ce qui a endommag\xE9 les coins des livres car ils \xE9taient livr\xE9s sans protection (doublure).

'>> Title: no lo he podido descargar' 
# Je n'ai pas pu le t\xE9l\xE9charger
'>> Review: igual que el anterior' 
# m\xEAme chose que ci-dessus`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Easy to follow!!!!&#x27;</span> 
<span class="hljs-comment"># Facile \xE0 suivre!!!!</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.&#x27;</span>
<span class="hljs-comment"># J&#x27;ai ador\xE9 The dash diet weight loss Solution. Jamais faim. Je recommande ce r\xE9gime. Les menus sont \xE9galement bien arrondis. Essayez-le. Il contient beaucoup d&#x27;informations, merci.</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: PARCIALMENTE DA\xD1ADO&#x27;</span> 
<span class="hljs-comment"># PARTIELLEMENT ENDOMMAG\xC9</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).&#x27;</span>
<span class="hljs-comment"># Il est arriv\xE9 le jour pr\xE9vu, avec d&#x27;autres livres que j&#x27;avais command\xE9s, mais la bo\xEEte est arriv\xE9e en mauvais \xE9tat, ce qui a endommag\xE9 les coins des livres car ils \xE9taient livr\xE9s sans protection (doublure).</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: no lo he podido descargar&#x27;</span> 
<span class="hljs-comment"># Je n&#x27;ai pas pu le t\xE9l\xE9charger</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: igual que el anterior&#x27;</span> 
<span class="hljs-comment"># m\xEAme chose que ci-dessus</span>`}}),rt=new C({props:{code:'books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)',highlighted:'books_dataset = books_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;review_title&quot;</span>].split()) &gt; <span class="hljs-number">2</span>)'}}),Bt=new St({}),Nn=new nr({props:{$$slots:{default:[f1]},$$scope:{ctx:te}}}),wr=new St({}),yr=new xv({props:{id:"1m7BerpSq8A"}}),zr=new C({props:{code:`from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;google/mt5-small&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}}),In=new nr({props:{$$slots:{default:[v1]},$$scope:{ctx:te}}}),Tr=new C({props:{code:`inputs = tokenizer(
    "I loved reading the Hunger Games!"
)  # J'ai ador\xE9 lire les Hunger Games !
inputs`,highlighted:`inputs = tokenizer(
    <span class="hljs-string">&quot;I loved reading the Hunger Games!&quot;</span>
)  <span class="hljs-comment"># J&#x27;ai ador\xE9 lire les Hunger Games !</span>
inputs`}}),Cr=new C({props:{code:"{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">336</span>, <span class="hljs-number">259</span>, <span class="hljs-number">28387</span>, <span class="hljs-number">11807</span>, <span class="hljs-number">287</span>, <span class="hljs-number">62893</span>, <span class="hljs-number">295</span>, <span class="hljs-number">12507</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}'}}),Pr=new C({props:{code:"tokenizer.convert_ids_to_tokens(inputs.input_ids)",highlighted:"tokenizer.convert_ids_to_tokens(inputs.input_ids)"}}),Dr=new C({props:{code:"['\u2581I', '\u2581', 'loved', '\u2581reading', '\u2581the', '\u2581Hung', 'er', '\u2581Games', '</s>']",highlighted:'[<span class="hljs-string">&#x27;\u2581I&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;loved&#x27;</span>, <span class="hljs-string">&#x27;\u2581reading&#x27;</span>, <span class="hljs-string">&#x27;\u2581the&#x27;</span>, <span class="hljs-string">&#x27;\u2581Hung&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;\u2581Games&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]'}}),Sr=new C({props:{code:`max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"], max_length=max_input_length, truncation=True
    )
    # Configurer le tokenizer pour les cibles
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["review_title"], max_length=max_target_length, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`max_input_length = <span class="hljs-number">512</span>
max_target_length = <span class="hljs-number">30</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    model_inputs = tokenizer(
        examples[<span class="hljs-string">&quot;review_body&quot;</span>], max_length=max_input_length, truncation=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Configurer le tokenizer pour les cibles</span>
    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples[<span class="hljs-string">&quot;review_title&quot;</span>], max_length=max_target_length, truncation=<span class="hljs-literal">True</span>
        )

    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
    <span class="hljs-keyword">return</span> model_inputs`}}),Lr=new C({props:{code:"tokenized_datasets = books_dataset.map(preprocess_function, batched=True)",highlighted:'tokenized_datasets = books_dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),Un=new nr({props:{$$slots:{default:[_1]},$$scope:{ctx:te}}}),Mr=new St({}),Ar=new xv({props:{id:"TMshhnrEXlg"}}),Nr=new C({props:{code:`generated_summary = "I absolutely loved reading the Hunger Games"
# "J'ai absolument ador\xE9 lire les Hunger Games"
reference_summary = "I loved reading the Hunger Games"
# "J'ai ador\xE9 lire les Hunger Games"`,highlighted:`generated_summary = <span class="hljs-string">&quot;I absolutely loved reading the Hunger Games&quot;</span>
<span class="hljs-comment"># &quot;J&#x27;ai absolument ador\xE9 lire les Hunger Games&quot;</span>
reference_summary = <span class="hljs-string">&quot;I loved reading the Hunger Games&quot;</span>
<span class="hljs-comment"># &quot;J&#x27;ai ador\xE9 lire les Hunger Games&quot;</span>`}}),Fn=new nr({props:{$$slots:{default:[g1]},$$scope:{ctx:te}}}),Gr=new C({props:{code:"!pip install rouge_score",highlighted:"!pip install rouge_score"}}),Ur=new C({props:{code:`from datasets import load_metric

rouge_score = load_metric("rouge")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

rouge_score = load_metric(<span class="hljs-string">&quot;rouge&quot;</span>)`}}),Hr=new C({props:{code:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`,highlighted:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`}}),Fr=new C({props:{code:`{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}`,highlighted:`{<span class="hljs-string">&#x27;rouge1&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rouge2&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), mid=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), high=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>)),
 <span class="hljs-string">&#x27;rougeL&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>))}`}}),Jr=new C({props:{code:'scores["rouge1"].mid',highlighted:'scores[<span class="hljs-string">&quot;rouge1&quot;</span>].mid'}}),Br=new C({props:{code:"Score(precision=0.86, recall=1.0, fmeasure=0.92)",highlighted:'Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)'}}),Bn=new nr({props:{$$slots:{default:[b1]},$$scope:{ctx:te}}}),Vr=new St({}),Wr=new C({props:{code:"!pip install nltk",highlighted:"!pip install nltk"}}),Kr=new C({props:{code:`import nltk

nltk.download("punkt")`,highlighted:`<span class="hljs-keyword">import</span> nltk

nltk.download(<span class="hljs-string">&quot;punkt&quot;</span>)`}}),Qr=new C({props:{code:`from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))`,highlighted:`<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> sent_tokenize


<span class="hljs-keyword">def</span> <span class="hljs-title function_">three_sentence_summary</span>(<span class="hljs-params">text</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(text)[:<span class="hljs-number">3</span>])


<span class="hljs-built_in">print</span>(three_sentence_summary(books_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;review_body&quot;</span>]))`}}),Yr=new C({props:{code:`'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.' 
# J'ai grandi en lisant Koontz, et il y a des ann\xE9es, j'ai arr\xEAt\xE9, convaincu que je l'avais "d\xE9pass\xE9"
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.' 
# "Pourtant, quand une amie cherchait un livre \xE0 suspense, je lui ai sugg\xE9r\xE9 Koontz."
'She found Strangers.' 
# Elle a trouv\xE9 Strangers.`,highlighted:`<span class="hljs-string">&#x27;I grew up reading Koontz, and years ago, I stopped,convinced i had &quot;outgrown&quot; him.&#x27;</span> 
<span class="hljs-comment"># J&#x27;ai grandi en lisant Koontz, et il y a des ann\xE9es, j&#x27;ai arr\xEAt\xE9, convaincu que je l&#x27;avais &quot;d\xE9pass\xE9&quot;</span>
<span class="hljs-string">&#x27;Still,when a friend was looking for something suspenseful too read, I suggested Koontz.&#x27;</span> 
<span class="hljs-comment"># &quot;Pourtant, quand une amie cherchait un livre \xE0 suspense, je lui ai sugg\xE9r\xE9 Koontz.&quot;</span>
<span class="hljs-string">&#x27;She found Strangers.&#x27;</span> 
<span class="hljs-comment"># Elle a trouv\xE9 Strangers.</span>`}}),Xr=new C({props:{code:`def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_baseline</span>(<span class="hljs-params">dataset, metric</span>):
    summaries = [three_sentence_summary(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">&quot;review_body&quot;</span>]]
    <span class="hljs-keyword">return</span> metric.compute(predictions=summaries, references=dataset[<span class="hljs-string">&quot;review_title&quot;</span>])`}}),Zr=new C({props:{code:`import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

score = evaluate_baseline(books_dataset[<span class="hljs-string">&quot;validation&quot;</span>], rouge_score)
rouge_names = [<span class="hljs-string">&quot;rouge1&quot;</span>, <span class="hljs-string">&quot;rouge2&quot;</span>, <span class="hljs-string">&quot;rougeL&quot;</span>, <span class="hljs-string">&quot;rougeLsum&quot;</span>]
rouge_dict = <span class="hljs-built_in">dict</span>((rn, <span class="hljs-built_in">round</span>(score[rn].mid.fmeasure * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> rn <span class="hljs-keyword">in</span> rouge_names)
rouge_dict`}}),el=new C({props:{code:"{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}",highlighted:'{<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">16.74</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">8.83</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">15.6</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">15.96</span>}'}});const zv=[j1,q1],hl=[];function Tv(s,u){return s[0]==="pt"?0:1}Qt=Tv(te),Yt=hl[Qt]=zv[Qt](te),Kn=new nr({props:{$$slots:{default:[$1]},$$scope:{ctx:te}}}),sl=new C({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),tl=new C({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});let us=te[0]==="pt"&&s1();const Cv=[E1,x1],fl=[];function Pv(s,u){return s[0]==="pt"?0:1}Xt=Pv(te),Zt=fl[Xt]=Cv[Xt](te),al=new C({props:{code:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)`,highlighted:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset[<span class="hljs-string">&quot;train&quot;</span>].column_names
)`}}),nl=new C({props:{code:`features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)`,highlighted:`features = [tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
data_collator(features)`}}),rl=new C({props:{code:`{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">1494</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">8622</span>,    <span class="hljs-number">390</span>,    <span class="hljs-number">259</span>,    <span class="hljs-number">262</span>,   <span class="hljs-number">2316</span>,   <span class="hljs-number">3435</span>,    <span class="hljs-number">955</span>,
            <span class="hljs-number">772</span>,    <span class="hljs-number">281</span>,    <span class="hljs-number">772</span>,   <span class="hljs-number">1617</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">305</span>,  <span class="hljs-number">14701</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">1385</span>,
           <span class="hljs-number">3031</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">24146</span>,    <span class="hljs-number">332</span>,   <span class="hljs-number">1037</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">43906</span>,    <span class="hljs-number">305</span>,    <span class="hljs-number">336</span>,
            <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>],
        [   <span class="hljs-number">259</span>,  <span class="hljs-number">27531</span>,  <span class="hljs-number">13483</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">7505</span>,    <span class="hljs-number">260</span>, <span class="hljs-number">112240</span>,  <span class="hljs-number">15192</span>,    <span class="hljs-number">305</span>,
          <span class="hljs-number">53198</span>,    <span class="hljs-number">276</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">74060</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">260</span>,    <span class="hljs-number">459</span>,  <span class="hljs-number">25640</span>,    <span class="hljs-number">776</span>,
           <span class="hljs-number">2119</span>,    <span class="hljs-number">336</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">2220</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">18896</span>,    <span class="hljs-number">288</span>,   <span class="hljs-number">4906</span>,    <span class="hljs-number">288</span>,
           <span class="hljs-number">1037</span>,   <span class="hljs-number">3931</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">7083</span>, <span class="hljs-number">101476</span>,   <span class="hljs-number">1143</span>,    <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;labels&#x27;</span>: tensor([[ <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>,  -<span class="hljs-number">100</span>],
        [  <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>,     <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;decoder_input_ids&#x27;</span>: tensor([[    <span class="hljs-number">0</span>,  <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>],
        [    <span class="hljs-number">0</span>,   <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>]])}`}});const Dv=[w1,k1],vl=[];function Sv(s,u){return s[0]==="pt"?0:1}ea=Sv(te),sa=vl[ea]=Dv[ea](te);let ps=te[0]==="pt"&&t1(te);return ll=new St({}),ol=new C({props:{code:`from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

hub_model_id = <span class="hljs-string">&quot;huggingface-course/mt5-small-finetuned-amazon-en-es&quot;</span>
summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>, model=hub_model_id)`}}),il=new C({props:{code:`def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\\n'>>> Title: {title}'")
    print(f"\\n'>>> Summary: {summary}'")`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">idx</span>):
    review = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>]
    title = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_title&quot;</span>]
    summary = summarizer(books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>])[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;summary_text&quot;</span>]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{review}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Title: <span class="hljs-subst">{title}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Summary: <span class="hljs-subst">{summary}</span>&#x27;&quot;</span>)`}}),ul=new C({props:{code:"print_summary(100)",highlighted:'print_summary(<span class="hljs-number">100</span>)'}}),pl=new C({props:{code:`'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.'
# Ce produit n'a rien de sp\xE9cial... le livre est trop petit et rigide et il est difficile d'y \xE9crire. L'\xE9norme autocollant au dos ne se d\xE9tache pas et a l'air super collant. Je n'ach\xE8terai plus jamais ce produit. J'aurais pu simplement acheter un journal dans un magasin \xE0 un dollar et ce serait \xE0 peu pr\xE8s la m\xEAme chose. Il est \xE9galement tr\xE8s cher pour ce qu'il est.

'>>> Title: Not impressed at all... buy something else' 
# Pas du tout impressionn\xE9... achetez autre chose.

'>>> Summary: Nothing special at all about this product' 
# Rien de sp\xE9cial \xE0 propos de ce produit`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.&#x27;</span>
<span class="hljs-comment"># Ce produit n&#x27;a rien de sp\xE9cial... le livre est trop petit et rigide et il est difficile d&#x27;y \xE9crire. L&#x27;\xE9norme autocollant au dos ne se d\xE9tache pas et a l&#x27;air super collant. Je n&#x27;ach\xE8terai plus jamais ce produit. J&#x27;aurais pu simplement acheter un journal dans un magasin \xE0 un dollar et ce serait \xE0 peu pr\xE8s la m\xEAme chose. Il est \xE9galement tr\xE8s cher pour ce qu&#x27;il est.</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Not impressed at all... buy something else&#x27;</span> 
<span class="hljs-comment"># Pas du tout impressionn\xE9... achetez autre chose.</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Nothing special at all about this product&#x27;</span> 
<span class="hljs-comment"># Rien de sp\xE9cial \xE0 propos de ce produit</span>`}}),ml=new C({props:{code:"print_summary(0)",highlighted:'print_summary(<span class="hljs-number">0</span>)'}}),cl=new C({props:{code:`'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada' 
# C'est une trilogie qui se lit tr\xE8s facilement. J'ai aim\xE9, je ne m'attendais pas du tout \xE0 la fin.

'>>> Title: Buena literatura para adolescentes' 
# Bonne litt\xE9rature pour les adolescents

'>>> Summary: Muy facil de leer' 
# Tr\xE8s facile \xE0 lire`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada&#x27;</span> 
<span class="hljs-comment"># C&#x27;est une trilogie qui se lit tr\xE8s facilement. J&#x27;ai aim\xE9, je ne m&#x27;attendais pas du tout \xE0 la fin.</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Buena literatura para adolescentes&#x27;</span> 
<span class="hljs-comment"># Bonne litt\xE9rature pour les adolescents</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Muy facil de leer&#x27;</span> 
<span class="hljs-comment"># Tr\xE8s facile \xE0 lire</span>`}}),{c(){d=r("meta"),E=m(),g(h.$$.fragment),y=m(),L=r("h1"),k=r("a"),T=r("span"),g(M.$$.fragment),P=m(),S=r("span"),A=a("R\xE9sum\xE9 de textes"),O=m(),I.c(),D=m(),N=r("p"),oe=a("Dans cette section, nous allons voir comment les "),W=r("em"),re=a("transformers"),F=a(" peuvent \xEAtre utilis\xE9s pour condenser de longs documents en r\xE9sum\xE9s, une t\xE2che connue sous le nom de "),ee=r("em"),G=a("r\xE9sum\xE9 de texte"),he=a(". Il s\u2019agit de l\u2019une des t\xE2ches de NLP les plus difficiles car elle requiert une s\xE9rie de capacit\xE9s, telles que la compr\xE9hension de longs passages et la g\xE9n\xE9ration d\u2019un texte coh\xE9rent qui capture les sujets principaux d\u2019un document. Cependant, lorsqu\u2019il est bien fait, le r\xE9sum\xE9 de texte est un outil puissant qui peut acc\xE9l\xE9rer divers processus commerciaux en soulageant les experts du domaine de la lecture d\xE9taill\xE9e de longs documents."),K=m(),g(ie.$$.fragment),ne=m(),Q=r("p"),R=a("Bien qu\u2019il existe d\xE9j\xE0 plusieurs mod\xE8les "),se=r("em"),B=a("finetun\xE9s"),ae=a(" pour le r\xE9sum\xE9 sur le "),Y=r("a"),ge=r("em"),Z=a("Hub"),ke=a(", la plupart d\u2019entre eux ne sont adapt\xE9s qu\u2019aux documents en anglais. Ainsi, pour ajouter une touche d\u2019originalit\xE9 \xE0 cette section, nous allons entra\xEEner un mod\xE8le bilingue pour l\u2019anglais et l\u2019espagnol. \xC0 la fin de cette section, vous disposerez d\u2019un "),me=r("a"),le=a("mod\xE8le"),fe=a(" capable de r\xE9sumer les commentaires des clients comme celui pr\xE9sent\xE9 ici :"),ce=m(),H=r("iframe"),X=m(),be=r("iframe"),Ne=m(),x=r("p"),J=a("Comme nous allons le voir, ces r\xE9sum\xE9s sont concis car ils sont appris \xE0 partir des titres que les clients fournissent dans leurs commentaires sur les produits. Commen\xE7ons par constituer un corpus bilingue appropri\xE9 pour cette t\xE2che."),Pe=m(),De=r("h2"),qe=r("a"),Ie=r("span"),g(je.$$.fragment),Ve=m(),Ge=r("span"),ss=a("Pr\xE9paration d'un corpus multilingue"),ye=m(),$e=r("p"),ms=a("Nous allons utiliser le "),ts=r("a"),Se=r("em"),as=a("Multilingual Amazon Reviews Corpus"),cs=a(" pour cr\xE9er notre r\xE9sumeur bilingue. Ce corpus est constitu\xE9 de critiques de produits Amazon en six langues et est g\xE9n\xE9ralement utilis\xE9 pour \xE9valuer les classifieurs multilingues. Cependant, comme chaque critique est accompagn\xE9e d\u2019un titre court, nous pouvons utiliser les titres comme r\xE9sum\xE9s cibles pour l\u2019apprentissage de notre mod\xE8le ! Pour commencer, t\xE9l\xE9chargeons les sous-ensembles anglais et espagnols depuis le "),Le=r("em"),Ls=a("Hub"),ns=a(" :"),bs=m(),g(de.$$.fragment),ds=m(),g(ve.$$.fragment),qs=m(),_e=r("p"),js=a("Comme vous pouvez le voir, pour chaque langue, il y a 200 000 critiques pour la partie entra\xEEnement et 5 000 critiques pour chacune des parties validation et test. Les informations qui nous int\xE9ressent sont contenues dans les colonnes "),rs=r("code"),$s=a("review_body"),xs=a(" et "),hs=r("code"),We=a("review_title"),$=a(". Voyons quelques exemples en cr\xE9ant une fonction simple qui prend un \xE9chantillon al\xE9atoire de l\u2019ensemble d\u2019entra\xEEnement avec les techniques apprises au "),V=r("a"),Ms=a("chapitre 5"),ls=a(" :"),Es=m(),g(os.$$.fragment),xe=m(),g(Ke.$$.fragment),As=m(),g(Qe.$$.fragment),ue=m(),Ue=r("p"),U=a("Cet \xE9chantillon montre la diversit\xE9 des critiques que l\u2019on trouve g\xE9n\xE9ralement en ligne, allant du positif au n\xE9gatif (et tout ce qui se trouve entre les deux !). Bien que l\u2019exemple avec le titre \xAB meh \xBB ne soit pas tr\xE8s informatif, les autres titres semblent \xEAtre des r\xE9sum\xE9s d\xE9cents des critiques. Entra\xEEner un mod\xE8le de r\xE9sum\xE9 sur l\u2019ensemble des 400 000 avis prendrait beaucoup trop de temps sur un seul GPU, nous allons donc nous concentrer sur la g\xE9n\xE9ration de r\xE9sum\xE9s pour un seul domaine de produits. Pour avoir une id\xE9e des domaines parmi lesquels nous pouvons choisir, convertissons "),pe=r("code"),et=a("english_dataset"),Os=a(" en "),Ns=r("code"),Ee=a("pandas.DataFrame"),Is=a(" et calculons le nombre d\u2019avis par cat\xE9gorie de produits :"),ks=m(),g(Je.$$.fragment),Rs=m(),g(Be.$$.fragment),ut=m(),Me=r("p"),aa=a("Les produits les plus populaires du jeu de donn\xE9es anglais concernent les articles m\xE9nagers, les v\xEAtements et l\u2019\xE9lectronique sans fil. Pour rester dans le th\xE8me d\u2019Amazon, nous allons nous concentrer sur le r\xE9sum\xE9 des critiques de livres. Apr\xE8s tout, c\u2019est la raison d\u2019\xEAtre de l\u2019entreprise ! Nous pouvons voir deux cat\xE9gories de produits qui correspondent \xE0 nos besoins ("),ze=r("code"),Gs=a("book"),Lt=a(" et "),is=r("code"),na=a("digital_ebook_purchase"),pt=a("). Nous allons donc filtrer les jeux de donn\xE9es dans les deux langues pour ces produits uniquement. Comme nous l\u2019avons vu dans le "),st=r("a"),ra=a("chapitre 5"),mt=a(", la fonction "),Ae=r("code"),la=a("Dataset.filter()"),Mt=a(" nous permet de d\xE9couper un jeu de donn\xE9es de mani\xE8re tr\xE8s efficace. Nous pouvons donc d\xE9finir une fonction simple pour le faire :"),fs=m(),g(Us.$$.fragment),tt=m(),Re=r("p"),rr=a("Maintenant, lorsque nous appliquons cette fonction \xE0 "),At=r("code"),lr=a("english_dataset"),or=a(" et "),oa=r("code"),Ot=a("spanish_dataset"),mn=a(", le r\xE9sultat ne contient que les lignes impliquant les cat\xE9gories de livres. Avant d\u2019appliquer le filtre, changeons le format de "),at=r("code"),cn=a("english_dataset"),ia=a(" de "),Ma=r("code"),dn=a('"pandas"'),Nt=a(" \xE0 "),ua=r("code"),Hs=a('"arrow"'),ir=a(" :"),pa=m(),g(Rt.$$.fragment),hn=m(),ct=r("p"),ur=a("Nous pouvons ensuite appliquer la fonction de filtrage et, \xE0 titre de v\xE9rification, inspecter un \xE9chantillon de critiques pour voir si elles portent bien sur des livres :"),fn=m(),g(dt.$$.fragment),Fs=m(),g(ys.$$.fragment),ma=m(),He=r("p"),pr=a("D\u2019accord, nous pouvons voir que les critiques ne concernent pas strictement les livres et peuvent se r\xE9f\xE9rer \xE0 des choses comme des calendriers et des applications \xE9lectroniques telles que OneNote. N\xE9anmoins, le domaine semble appropri\xE9 pour entra\xEEner un mod\xE8le de r\xE9sum\xE9. Avant de regarder les diff\xE9rents mod\xE8les qui conviennent \xE0 cette t\xE2che, nous avons une derni\xE8re pr\xE9paration de donn\xE9es \xE0 faire : combiner les critiques anglaises et espagnoles en un seul objet "),It=r("code"),mr=a("DatasetDict"),vn=a(". \u{1F917} "),Js=r("em"),cr=a("Datasets"),Aa=a(" fournit une fonction pratique "),Oa=r("code"),dr=a("concatenate_datasets()"),_n=a(" qui (comme son nom l\u2019indique) va empiler deux objets "),vs=r("code"),Gt=a("Dataset"),hr=a(" l\u2019un sur l\u2019autre. Ainsi, pour cr\xE9er notre jeu de donn\xE9es bilingue, nous allons boucler sur chaque division, concat\xE9ner les jeux de donn\xE9es pour cette division, et m\xE9langer le r\xE9sultat pour s\u2019assurer que notre mod\xE8le ne s\u2019adapte pas trop \xE0 une seule langue :"),ca=m(),g(Ut.$$.fragment),gn=m(),g(Ht.$$.fragment),ht=m(),da=r("p"),Na=a("Cela ressemble certainement \xE0 un m\xE9lange de critiques anglaises et espagnoles ! Maintenant que nous avons un corpus d\u2019entra\xEEnement, une derni\xE8re chose \xE0 v\xE9rifier est la distribution des mots dans les critiques et leurs titres. Ceci est particuli\xE8rement important pour les t\xE2ches de r\xE9sum\xE9, o\xF9 les r\xE9sum\xE9s de r\xE9f\xE9rence courts dans les donn\xE9es peuvent biaiser le mod\xE8le pour qu\u2019il ne produise qu\u2019un ou deux mots dans les r\xE9sum\xE9s g\xE9n\xE9r\xE9s. Les graphiques ci-dessous montrent les distributions de mots, et nous pouvons voir que les titres sont fortement biais\xE9s vers seulement 1 ou 2 mots :"),bn=m(),nt=r("div"),Ft=r("img"),fr=m(),Jt=r("img"),jn=m(),Bs=r("p"),vr=a("Pour y rem\xE9dier, nous allons filtrer les exemples avec des titres tr\xE8s courts afin que notre mod\xE8le puisse produire des r\xE9sum\xE9s plus int\xE9ressants. Puisque nous avons affaire \xE0 des textes anglais et espagnols, nous pouvons utiliser une heuristique grossi\xE8re pour s\xE9parer les titres sur les espaces blancs, puis utiliser notre fid\xE8le m\xE9thode "),Ra=r("code"),Ia=a("Dataset.filter()"),_r=a(" comme suit :"),$n=m(),g(rt.$$.fragment),xn=m(),Vs=r("p"),Ga=a("Maintenant que nous avons pr\xE9par\xE9 notre corpus, voyons quelques "),Ua=r("em"),gr=a("transformers"),Ha=a(" possibles que l\u2019on pourrait "),Fa=r("em"),br=a("finetun\xE9"),Ja=a(" dessus !"),En=m(),lt=r("h2"),ot=r("a"),ft=r("span"),g(Bt.$$.fragment),kn=m(),it=r("span"),wn=a("Mod\xE8les pour le r\xE9sum\xE9 de texte"),vt=m(),ws=r("p"),_t=a("Si vous y pensez, le r\xE9sum\xE9 de texte est une t\xE2che similaire \xE0 la traduction automatique. Nous avons un corps de texte, comme une critique, que nous aimerions \xAB traduire \xBB en une version plus courte qui capture les caract\xE9ristiques saillantes de l\u2019entr\xE9e. En cons\xE9quence, la plupart des "),Ba=r("em"),Va=a("transformers"),qr=a(" pour le r\xE9sum\xE9 adoptent l\u2019architecture encodeur-d\xE9codeur que nous avons rencontr\xE9e pour la premi\xE8re fois dans le "),ha=r("a"),yn=a("chapitre 1"),p=a(", bien qu\u2019il y ait quelques exceptions comme la famille de mod\xE8les GPT qui peut \xE9galement \xEAtre utilis\xE9e pour le r\xE9sum\xE9 dans des contextes peu complexes. Le tableau suivant pr\xE9sente quelques mod\xE8les pr\xE9-entra\xEEn\xE9s populaires qui peuvent \xEAtre "),z=r("em"),gl=a("finetun\xE9s"),bl=a(" pour le r\xE9sum\xE9."),Wa=m(),Vt=r("table"),zn=r("thead"),Fe=r("tr"),Ka=r("th"),Tn=r("em"),ql=a("Transformers"),jl=m(),Cn=r("th"),Pn=a("Description"),$l=m(),Qa=r("th"),xl=a("Multilingue ?"),fa=m(),_s=r("tbody"),gt=r("tr"),Wt=r("td"),va=r("a"),El=a("GPT-2"),Dn=m(),_a=r("td"),Sn=a("Bien qu\u2019il soit entra\xEEn\xE9 comme un mod\xE8le de langage autor\xE9gressif, vous pouvez faire en sorte que le GPT-2 g\xE9n\xE8re des r\xE9sum\xE9s en ajoutant "),Ln=r("code"),kl=a("TL;DR"),ga=a(" \xE0 la fin du texte d\u2019entr\xE9e."),wl=m(),Ya=r("td"),Mn=a("\u274C"),yl=m(),bt=r("tr"),Xa=r("td"),Ws=r("a"),zl=a("PEGASUS"),Tl=m(),Ks=r("td"),Cl=a("Utilise un objectif de pr\xE9-entra\xEEnement pour pr\xE9dire les phrases masqu\xE9es dans les textes \xE0 plusieurs phrases. Cet objectif de pr\xE9-entra\xEEnement est plus proche du r\xE9sum\xE9 que de la mod\xE9lisation du langage standard et obtient des scores \xE9lev\xE9s sur des "),An=r("em"),Pl=a("benchmarks"),ba=a(" populaires."),Dl=m(),Za=r("td"),On=a("\u274C"),Sl=m(),qt=r("tr"),en=r("td"),jt=r("a"),Ll=a("T5"),$t=m(),Qs=r("td"),Ml=a("Une architecture universelle de "),qa=r("em"),Al=a("transformer"),Ol=a(" qui formule toutes les t\xE2ches dans un cadre texte \xE0 texte. Par exemple, le format d\u2019entr\xE9e du mod\xE8le pour r\xE9sumer un document est "),Xe=r("code"),Nl=a("summarize: ARTICLE"),Rl=a("."),Il=m(),sn=r("td"),Gl=a("\u274C"),Ul=m(),xt=r("tr"),Kt=r("td"),ja=r("a"),Om=a("mT5"),Nm=m(),Ho=r("td"),Rm=a("Une version multilingue de T5, pr\xE9-entra\xEEn\xE9e sur le corpus multilingue Common Crawl (mC4), couvrant 101 langues."),Im=m(),Hl=r("td"),Gm=a("\u2705"),Um=m(),tn=r("tr"),Fl=r("td"),jr=r("a"),Hm=a("BART"),Fm=m(),$r=r("td"),Jm=a("Une architecture de "),Fo=r("em"),Bm=a("transformer"),Vm=a(" avec une pile d\u2019encodeurs et de d\xE9codeurs entra\xEEn\xE9s pour reconstruire l\u2019entr\xE9e corrompue qui combine les sch\xE9mas de pr\xE9-entra\xEEnement de BERT et GPT-2."),Wm=m(),Jl=r("td"),Km=a("\u274C"),Qm=m(),an=r("tr"),Bl=r("td"),xr=r("a"),Ym=a("mBART-50"),Xm=m(),Jo=r("td"),Zm=a("Une version multilingue de BART, pr\xE9-entra\xEEn\xE9e sur 50 langues."),ec=m(),Vl=r("td"),sc=a("\u2705"),Nu=m(),$a=r("p"),tc=a("Comme vous pouvez le voir dans ce tableau, la majorit\xE9 des "),Bo=r("em"),ac=a("transformers"),nc=a(" pour le r\xE9sum\xE9 (et en fait la plupart des t\xE2ches de NLP) sont monolingues. C\u2019est une bonne chose si votre t\xE2che se d\xE9roule dans une langue \xAB \xE0 haute ressource \xBB comme l\u2019anglais ou l\u2019allemand, mais moins pour les milliers d\u2019autres langues utilis\xE9es dans le monde. Heureusement, il existe une cat\xE9gorie de "),Vo=r("em"),rc=a("transformers"),lc=a(" multilingues, comme mT5 et mBART, qui viennent \xE0 la rescousse. Ces mod\xE8les sont pr\xE9-entra\xEEn\xE9s en utilisant la mod\xE9lisation du langage mais avec une particularit\xE9 : au lieu d\u2019\xEAtre entra\xEEn\xE9 sur un corpus d\u2019une seule langue, ils sont entra\xEEn\xE9s conjointement sur des textes dans plus de 50 langues !"),Ru=m(),Et=r("p"),oc=a("Nous allons nous concentrer sur mT5, une architecture int\xE9ressante bas\xE9e sur T5 qui a \xE9t\xE9 pr\xE9-entra\xEEn\xE9e dans un cadre texte \xE0 texte. Dans T5, chaque t\xE2che de NLP est formul\xE9e en termes d\u2019un pr\xE9fixe de "),Wo=r("em"),ic=a("prompt"),uc=a(" comme "),Ko=r("code"),pc=a("summarize:"),mc=a(" qui conditionne le mod\xE8le \xE0 adapter le texte g\xE9n\xE9r\xE9 au "),Qo=r("em"),cc=a("prompt"),dc=a(". Comme le montre la figure ci-dessous, cela rend le T5 extr\xEAmement polyvalent car vous pouvez r\xE9soudre de nombreuses t\xE2ches avec un seul mod\xE8le !"),Iu=m(),nn=r("div"),Er=r("img"),hc=m(),kr=r("img"),Gu=m(),Wl=r("p"),fc=a("mT5 n\u2019utilise pas de pr\xE9fixes mais partage une grande partie de la polyvalence de T5 et a l\u2019avantage d\u2019\xEAtre multilingue. Maintenant que nous avons choisi un mod\xE8le, voyons comment pr\xE9parer nos donn\xE9es pour l\u2019entra\xEEnement."),Uu=m(),g(Nn.$$.fragment),Hu=m(),rn=r("h2"),Rn=r("a"),Yo=r("span"),g(wr.$$.fragment),vc=m(),Xo=r("span"),_c=a("Pr\xE9traitement des donn\xE9es"),Fu=m(),g(yr.$$.fragment),Ju=m(),zs=r("p"),gc=a("Notre prochaine t\xE2che est de tokeniser et d\u2019encoder nos critiques et leurs titres. Comme d\u2019habitude, nous commen\xE7ons par charger le "),Zo=r("em"),bc=a("tokenizer"),qc=a(" associ\xE9 au "),ei=r("em"),jc=a("checkpoint"),$c=a(" du mod\xE8le pr\xE9-entra\xEEn\xE9. Nous utiliserons "),si=r("code"),xc=a("mt5-small"),Ec=a(" comme "),ti=r("em"),kc=a("checkpoint"),wc=a(" afin de pouvoir "),ai=r("em"),yc=a("finetuner"),zc=a(" le mod\xE8le en un temps raisonnable :"),Bu=m(),g(zr.$$.fragment),Vu=m(),g(In.$$.fragment),Wu=m(),Gn=r("p"),Tc=a("Testons le "),ni=r("em"),Cc=a("tokenizer"),Pc=a(" de mT5 sur un petit exemple :"),Ku=m(),g(Tr.$$.fragment),Qu=m(),g(Cr.$$.fragment),Yu=m(),Ze=r("p"),Dc=a("Ici nous pouvons voir les familiers "),ri=r("code"),Sc=a("input_ids"),Lc=a(" et "),li=r("code"),Mc=a("attention_mask"),Ac=a(" que nous avons rencontr\xE9s dans nos premi\xE8res exp\xE9riences de "),oi=r("em"),Oc=a("finetuning"),Nc=a(" au "),Kl=r("a"),Rc=a("chapitre 3"),Ic=a(". D\xE9codons ces identifiants d\u2019entr\xE9e avec la fonction "),ii=r("code"),Gc=a("convert_ids_to_tokens()"),Uc=a(" du "),ui=r("em"),Hc=a("tokenizer"),Fc=a(" pour voir \xE0 quel type de "),pi=r("em"),Jc=a("tokenizer"),Bc=a(" nous avons affaire :"),Xu=m(),g(Pr.$$.fragment),Zu=m(),g(Dr.$$.fragment),ep=m(),Ts=r("p"),Vc=a("Le caract\xE8re Unicode sp\xE9cial "),mi=r("code"),Wc=a("\u2581"),Kc=a(" et le "),ci=r("em"),Qc=a("token"),Yc=a(" de fin de s\xE9quence "),di=r("code"),Xc=a("</s>"),Zc=a(" indiquent que nous avons affaire au "),hi=r("em"),ed=a("tokenizer"),sd=a(" de SentencePiece, qui est bas\xE9 sur l\u2019algorithme de segmentation Unigram discut\xE9 dans le "),Ql=r("a"),td=a("chapitre 6"),ad=a(". Unigram est particuli\xE8rement utile pour les corpus multilingues car il permet \xE0 SentencePiece d\u2019\xEAtre agnostique vis-\xE0-vis des accents, de la ponctuation et du fait que de nombreuses langues, comme le japonais, n\u2019ont pas de caract\xE8res d\u2019espacement."),sp=m(),xa=r("p"),nd=a("Pour tokeniser notre corpus, nous devons faire face \xE0 une subtilit\xE9 associ\xE9e au r\xE9sum\xE9 : comme nos \xE9tiquettes sont \xE9galement du texte, il est possible qu\u2019elles d\xE9passent la taille maximale du contexte du mod\xE8le. Cela signifie que nous devons appliquer une troncature \xE0 la fois aux critiques et \xE0 leurs titres pour nous assurer de ne pas transmettre des entr\xE9es trop longues \xE0 notre mod\xE8le. Les tokenizers de \u{1F917} "),fi=r("em"),rd=a("Transformers"),ld=a(" fournissent une fonction tr\xE8s pratique "),vi=r("code"),od=a("as_target_tokenizer()"),id=a(" qui vous permet de tokeniser les \xE9tiquettes en parall\xE8le avec les entr\xE9es. Ceci est typiquement fait en utilisant un gestionnaire de contexte \xE0 l\u2019int\xE9rieur d\u2019une fonction de pr\xE9traitement qui encode d\u2019abord les entr\xE9es, et ensuite encode les \xE9tiquettes comme une colonne s\xE9par\xE9e. Voici un exemple d\u2019une telle fonction pour mT5 :"),tp=m(),g(Sr.$$.fragment),ap=m(),Ys=r("p"),ud=a("Parcourons ce code pour comprendre ce qui se passe. La premi\xE8re chose que nous avons faite est de d\xE9finir des valeurs pour "),_i=r("code"),pd=a("max_input_length"),md=a(" et "),gi=r("code"),cd=a("max_target_length"),dd=a(", qui fixent les limites sup\xE9rieures de la longueur des commentaires et des titres. Comme le corps de la critique est g\xE9n\xE9ralement beaucoup plus long que le titre, nous avons mis ces valeurs \xE0 l\u2019\xE9chelle en cons\xE9quence. Ensuite, dans la "),bi=r("code"),hd=a("preprocess_function()"),fd=a(" elle-m\xEAme, nous pouvons voir que les commentaires sont d\u2019abord tokeniz\xE9s, suivis par les titres avec "),qi=r("code"),vd=a("as_target_tokenizer()"),_d=a("."),np=m(),Ea=r("p"),gd=a("Avec la fonction "),ji=r("code"),bd=a("preprocess_function()"),qd=a(", il est alors simple de tokeniser l\u2019ensemble du corpus en utilisant la fonction pratique "),$i=r("code"),jd=a("Dataset.map()"),$d=a(" que nous avons largement utilis\xE9e dans ce cours :"),rp=m(),g(Lr.$$.fragment),lp=m(),Yl=r("p"),xd=a("Maintenant que le corpus a \xE9t\xE9 pr\xE9trait\xE9, examinons certaines m\xE9triques couramment utilis\xE9es pour le r\xE9sum\xE9. Comme nous allons le voir, il n\u2019existe pas de solution miracle pour mesurer la qualit\xE9 d\u2019un texte g\xE9n\xE9r\xE9 par une machine."),op=m(),g(Un.$$.fragment),ip=m(),ln=r("h2"),Hn=r("a"),xi=r("span"),g(Mr.$$.fragment),Ed=m(),Ei=r("span"),kd=a("M\xE9triques pour le r\xE9sum\xE9 de texte"),up=m(),g(Ar.$$.fragment),pp=m(),Xl=r("p"),wd=a("Par rapport \xE0 la plupart des autres t\xE2ches que nous avons abord\xE9es dans ce cours, la mesure des performances des t\xE2ches de g\xE9n\xE9ration de texte comme le r\xE9sum\xE9 ou la traduction n\u2019est pas aussi simple. Par exemple, pour une critique telle que \xAB J\u2019ai ador\xE9 lire les Hunger Games \xBB, il existe plusieurs r\xE9sum\xE9s valides, comme \xAB J\u2019ai ador\xE9 Hunger Games \xBB ou \xAB Hunger Games est une excellente lecture \xBB. Il est clair que l\u2019application d\u2019une sorte de correspondance exacte entre le r\xE9sum\xE9 g\xE9n\xE9r\xE9 et l\u2019\xE9tiquette n\u2019est pas une bonne solution. En effet, m\xEAme les humains auraient de mauvais r\xE9sultats avec une telle mesure, car nous avons tous notre propre style d\u2019\xE9criture."),mp=m(),ka=r("p"),yd=a("Pour le r\xE9sum\xE9, l\u2019une des m\xE9triques les plus couramment utilis\xE9es est le "),Or=r("a"),zd=a("score ROUGE"),Td=a(" (abr\xE9viation de "),ki=r("em"),Cd=a("Recall-Oriented Understudy for Gisting Evaluation"),Pd=a("). L\u2019id\xE9e de base de cette m\xE9trique est de comparer un r\xE9sum\xE9 g\xE9n\xE9r\xE9 avec un ensemble de r\xE9sum\xE9s de r\xE9f\xE9rence qui sont g\xE9n\xE9ralement cr\xE9\xE9s par des humains. Pour \xEAtre plus pr\xE9cis, supposons que nous voulions comparer les deux r\xE9sum\xE9s suivants :"),cp=m(),g(Nr.$$.fragment),dp=m(),wa=r("p"),Dd=a("Une fa\xE7on de les comparer pourrait \xEAtre de compter le nombre de mots qui se chevauchent, qui dans ce cas serait de 6. Cependant, cette m\xE9thode est un peu grossi\xE8re, c\u2019est pourquoi ROUGE se base sur le calcul des scores de "),wi=r("em"),Sd=a("pr\xE9cision"),Ld=a(" et de "),yi=r("em"),Md=a("rappel"),Ad=a(" pour le chevauchement."),hp=m(),g(Fn.$$.fragment),fp=m(),Rr=r("p"),Od=a(`Pour ROUGE, le rappel mesure la proportion du r\xE9sum\xE9 de r\xE9f\xE9rence qui est captur\xE9e par le r\xE9sum\xE9 g\xE9n\xE9r\xE9. Si nous ne faisons que comparer des mots, le rappel peut \xEAtre calcul\xE9 selon la formule suivante :
`),vp=new Zb,_p=m(),Ir=r("p"),Nd=a(`Pour notre exemple simple ci-dessus, cette formule donne un rappel parfait de 6/6 = 1, c\u2019est-\xE0-dire que tous les mots du r\xE9sum\xE9 de r\xE9f\xE9rence ont \xE9t\xE9 produits par le mod\xE8le. Cela peut sembler g\xE9nial, mais imaginez que le r\xE9sum\xE9 g\xE9n\xE9r\xE9 ait \xE9t\xE9 \xAB J\u2019ai vraiment aim\xE9 lire les Hunger Games toute la nuit \xBB. Le rappel serait \xE9galement parfait, mais le r\xE9sum\xE9 serait sans doute moins bon puisqu\u2019il serait verbeux. Pour traiter ces sc\xE9narios, nous calculons \xE9galement la pr\xE9cision, qui dans le contexte de ROUGE, mesure la proportion du r\xE9sum\xE9 g\xE9n\xE9r\xE9 qui est pertinente :
`),gp=new Zb,bp=m(),kt=r("p"),Rd=a("En appliquant cela \xE0 notre r\xE9sum\xE9 verbeux, on obtient une pr\xE9cision de 6/10 = 0,6, ce qui est consid\xE9rablement moins bon que la pr\xE9cision de 6/7 = 0,86 obtenue par notre r\xE9sum\xE9 plus court. En pratique, la pr\xE9cision et le rappel sont g\xE9n\xE9ralement calcul\xE9s, puis le score F1 (la moyenne harmonique de la pr\xE9cision et du rappel) est indiqu\xE9. Nous pouvons le faire facilement dans \u{1F917} "),zi=r("em"),Id=a("Datasets"),Gd=a(" en installant d\u2019abord le "),Ti=r("em"),Ud=a("package"),Hd=m(),Ci=r("code"),Fd=a("rouge_score"),Jd=a(" :"),qp=m(),g(Gr.$$.fragment),jp=m(),Zl=r("p"),Bd=a("et ensuite charger la m\xE9trique ROUGE comme suit :"),$p=m(),g(Ur.$$.fragment),xp=m(),Jn=r("p"),Vd=a("Ensuite, nous pouvons utiliser la fonction "),Pi=r("code"),Wd=a("rouge_score.compute()"),Kd=a(" pour calculer toutes les m\xE9triques en une seule fois :"),Ep=m(),g(Hr.$$.fragment),kp=m(),g(Fr.$$.fragment),wp=m(),es=r("p"),Qd=a("Whoa, il y a pas mal d\u2019informations dans cette sortie. Qu\u2019est-ce que \xE7a veut dire ? Tout d\u2019abord, \u{1F917} "),Di=r("em"),Yd=a("Datasets"),Xd=a(" calcule des intervalles de confiance pour la pr\xE9cision, le rappel et le score F1. Ce sont les attributs "),Si=r("code"),Zd=a("low"),eh=a(", "),Li=r("code"),sh=a("mid"),th=a(", et "),Mi=r("code"),ah=a("high"),nh=a(" que vous pouvez voir ici. De plus, \u{1F917} "),Ai=r("em"),rh=a("Datasets"),lh=a(" calcule une vari\xE9t\xE9 de scores ROUGE qui sont bas\xE9s sur diff\xE9rents types de granularit\xE9 du texte lors de la comparaison des r\xE9sum\xE9s g\xE9n\xE9r\xE9s et de r\xE9f\xE9rence. La variante "),Oi=r("code"),oh=a("rouge1"),ih=a(" est le chevauchement des unigrammes. C\u2019est juste une fa\xE7on fantaisiste de dire le chevauchement des mots et c\u2019est exactement la m\xE9trique dont nous avons discut\xE9 ci-dessus. Pour v\xE9rifier cela, nous allons extraire la valeur "),Ni=r("code"),uh=a("mid"),ph=a(" de nos scores :"),yp=m(),g(Jr.$$.fragment),zp=m(),g(Br.$$.fragment),Tp=m(),Cs=r("p"),mh=a("Super, les chiffres de pr\xE9cision et de rappel correspondent ! Maintenant, qu\u2019en est-il des autres scores ROUGE ? "),Ri=r("code"),ch=a("rouge2"),dh=a(" mesure le chevauchement entre les bigrammes (chevauchement des paires de mots), tandis que "),Ii=r("code"),hh=a("rougeL"),fh=a(" et "),Gi=r("code"),vh=a("rougeLsum"),_h=a(" mesurent les plus longues s\xE9quences de mots correspondants en recherchant les plus longues sous-souches communes dans les r\xE9sum\xE9s g\xE9n\xE9r\xE9s et de r\xE9f\xE9rence. Le \xAB sum \xBB dans "),Ui=r("code"),gh=a("rougeLsum"),bh=a(" fait r\xE9f\xE9rence au fait que cette m\xE9trique est calcul\xE9e sur un r\xE9sum\xE9 entier, alors que "),Hi=r("code"),qh=a("rougeL"),jh=a(" est calcul\xE9e comme une moyenne sur des phrases individuelles."),Cp=m(),g(Bn.$$.fragment),Pp=m(),Vn=r("p"),$h=a("Nous utiliserons ces scores ROUGE pour suivre les performances de notre mod\xE8le, mais avant cela, faisons ce que tout bon praticien de NLP devrait faire : cr\xE9er une "),Fi=r("em"),xh=a("baseline"),Eh=a(" solide, mais simple !"),Dp=m(),on=r("h3"),Wn=r("a"),Ji=r("span"),g(Vr.$$.fragment),kh=m(),Bi=r("span"),wh=a("Cr\xE9ation d'une base de r\xE9f\xE9rence solide"),Sp=m(),gs=r("p"),yh=a("Une "),Vi=r("em"),zh=a("baseline"),Th=a(" commune pour le r\xE9sum\xE9 de texte consiste \xE0 prendre simplement les trois premi\xE8res phrases d\u2019un article, souvent appel\xE9e la "),Wi=r("em"),Ch=a("baseline"),Ph=m(),Ki=r("em"),Dh=a("lead-3"),Sh=a(". Nous pourrions utiliser les points pour tracker les limites des phrases mais cela \xE9chouera avec des acronymes comme \xAB U.S. \xBB ou \xAB U.N. \xBB. Nous allons donc utiliser la biblioth\xE8que "),Qi=r("code"),Lh=a("nltk"),Mh=a(", qui inclut un meilleur algorithme pour g\xE9rer ces cas. Vous pouvez installer le "),Yi=r("em"),Ah=a("package"),Oh=a(" en utilisant "),Xi=r("code"),Nh=a("pip"),Rh=a(" comme suit :"),Lp=m(),g(Wr.$$.fragment),Mp=m(),eo=r("p"),Ih=a("puis t\xE9l\xE9chargez les r\xE8gles de ponctuation :"),Ap=m(),g(Kr.$$.fragment),Op=m(),ya=r("p"),Gh=a("Ensuite, nous importons le "),Zi=r("em"),Uh=a("tokenizer"),Hh=a(" de "),eu=r("code"),Fh=a("nltk"),Jh=a(" et cr\xE9ons une fonction simple pour extraire les trois premi\xE8res phrases d\u2019une critique. La convention dans le r\xE9sum\xE9 de texte est de s\xE9parer chaque r\xE9sum\xE9 avec une nouvelle ligne, donc nous allons \xE9galement inclure ceci et tester le tout sur un exemple d\u2019entra\xEEnement :"),Np=m(),g(Qr.$$.fragment),Rp=m(),g(Yr.$$.fragment),Ip=m(),so=r("p"),Bh=a("Cela semble fonctionner, alors impl\xE9mentons maintenant une fonction qui extrait ces r\xE9sum\xE9s d\u2019un jeu de donn\xE9es et calcule les scores ROUGE pour la ligne de base :"),Gp=m(),g(Xr.$$.fragment),Up=m(),to=r("p"),Vh=a("Nous pouvons ensuite utiliser cette fonction pour calculer les scores ROUGE sur l\u2019ensemble de validation et les embellir un peu en utilisant Pandas :"),Hp=m(),g(Zr.$$.fragment),Fp=m(),g(el.$$.fragment),Jp=m(),Ps=r("p"),Wh=a("Nous pouvons voir que le score de "),su=r("code"),Kh=a("rouge2"),Qh=a(" est significativement plus bas que le reste. Ceci refl\xE8te probablement le fait que les titres des critiques sont typiquement concis et donc que la "),tu=r("em"),Yh=a("baseline"),Xh=m(),au=r("em"),Zh=a("lead-3"),ef=a(" est trop verbeuse. Maintenant que nous disposons d\u2019une bonne "),nu=r("em"),sf=a("baseline"),tf=a(", concentrons-nous sur le "),ru=r("em"),af=a("finetuning"),nf=a(" du mT5 !"),Bp=m(),Yt.c(),ao=m(),g(Kn.$$.fragment),Vp=m(),za=r("p"),rf=a("La prochaine chose que nous devons faire est de nous connecter au "),lu=r("em"),lf=a("Hub"),of=a(". Si vous ex\xE9cutez ce code dans un "),ou=r("em"),uf=a("notebook"),pf=a(", vous pouvez le faire avec la fonction utilitaire suivante :"),Wp=m(),g(sl.$$.fragment),Kp=m(),Qn=r("p"),mf=a("qui affichera un "),iu=r("em"),cf=a("widget"),df=a(" o\xF9 vous pourrez saisir vos informations d\u2019identification. Vous pouvez \xE9galement ex\xE9cuter cette commande dans votre terminal et vous connecter \xE0 partir de l\xE0 :"),Qp=m(),g(tl.$$.fragment),Yp=m(),us&&us.c(),no=m(),Ta=r("p"),hf=a("Ensuite, nous devons d\xE9finir un collateur de donn\xE9es pour notre t\xE2che de s\xE9quence \xE0 s\xE9quence. Comme mT5 est un "),uu=r("em"),ff=a("transformer"),vf=a(" encodeur-d\xE9codeur, une des subtilit\xE9s de la pr\xE9paration de nos batchs est que, pendant le d\xE9codage, nous devons d\xE9caler les \xE9tiquettes d\u2019une unit\xE9 vers la droite. Ceci est n\xE9cessaire pour garantir que le d\xE9codeur ne voit que les \xE9tiquettes de v\xE9rit\xE9 terrain pr\xE9c\xE9dentes et non les \xE9tiquettes actuelles ou futures, qui seraient faciles \xE0 m\xE9moriser pour le mod\xE8le. Cela ressemble \xE0 la fa\xE7on dont l\u2019auto-attention masqu\xE9e est appliqu\xE9e aux entr\xE9es dans une t\xE2che comme "),ro=r("a"),_f=a("la mod\xE9lisation causale du langage"),gf=a("."),Xp=m(),Xs=r("p"),bf=a("Heureusement, \u{1F917} "),pu=r("em"),qf=a("Transformers"),jf=a(" fournit un collateur "),mu=r("code"),$f=a("DataCollatorForSeq2Seq"),xf=a(" qui rembourrera dynamiquement les entr\xE9es et les \xE9tiquettes pour nous. Pour instancier ce collateur, nous devons simplement fournir le "),cu=r("em"),Ef=a("tokenizer"),kf=a(" et le "),du=r("em"),wf=a("mod\xE8le"),yf=a(" :"),Zp=m(),Zt.c(),lo=m(),oo=r("p"),zf=a("Voyons ce que produit ce collateur lorsqu\u2019on lui donne un petit batch d\u2019exemples. Tout d\u2019abord, nous devons supprimer les colonnes contenant des cha\xEEnes de caract\xE8res, car le collateur ne saura pas comment remplir ces \xE9l\xE9ments :"),em=m(),g(al.$$.fragment),sm=m(),Ca=r("p"),Tf=a("Comme le collateur attend une liste de "),hu=r("code"),Cf=a("dict"),Pf=a(", o\xF9 chaque "),fu=r("code"),Df=a("dict"),Sf=a(" repr\xE9sente un seul exemple du jeu de donn\xE9es, nous devons \xE9galement mettre les donn\xE9es dans le format attendu avant de les transmettre au collateur de donn\xE9es :"),tm=m(),g(nl.$$.fragment),am=m(),g(rl.$$.fragment),nm=m(),we=r("p"),Lf=a("La principale chose \xE0 remarquer ici est que le premier exemple est plus long que le second, donc les "),vu=r("code"),Mf=a("input_ids"),Af=a(" et "),_u=r("code"),Of=a("attention_mask"),Nf=a(" du second exemple ont \xE9t\xE9 compl\xE9t\xE9s sur la droite avec un "),gu=r("em"),Rf=a("token"),If=m(),bu=r("code"),Gf=a("[PAD]"),Uf=a(" (dont l\u2019identifiant est "),qu=r("code"),Hf=a("0"),Ff=a("). De m\xEAme, nous pouvons voir que les "),ju=r("code"),Jf=a("labels"),Bf=a(" ont \xE9t\xE9 compl\xE9t\xE9s par des "),$u=r("code"),Vf=a("-100"),Wf=a(", pour s\u2019assurer que les "),xu=r("em"),Kf=a("tokens"),Qf=a(" de remplissage sont ignor\xE9s par la fonction de perte. Et enfin, nous pouvons voir un nouveau "),Eu=r("code"),Yf=a("decoder_input_ids"),Xf=a(" qui a d\xE9plac\xE9 les \xE9tiquettes vers la droite en ins\xE9rant un "),ku=r("em"),Zf=a("token"),ev=m(),wu=r("code"),sv=a("[PAD]"),tv=a(" dans la premi\xE8re entr\xE9e."),rm=m(),sa.c(),io=m(),ps&&ps.c(),uo=m(),un=r("h2"),Yn=r("a"),yu=r("span"),g(ll.$$.fragment),av=m(),po=r("span"),nv=a("Utilisation de votre mod\xE8le "),zu=r("i"),rv=a("finetun\xE9"),lm=m(),wt=r("p"),lv=a("Une fois que vous avez pouss\xE9 le mod\xE8le vers le "),Tu=r("em"),ov=a("Hub"),iv=a(", vous pouvez jouer avec lui soit via le "),Cu=r("em"),uv=a("widget"),pv=a(" d\u2019inf\xE9rence, soit avec un objet "),Pu=r("code"),mv=a("pipeline"),cv=a(", comme suit :"),om=m(),g(ol.$$.fragment),im=m(),mo=r("p"),dv=a("Nous pouvons alimenter notre pipeline avec quelques exemples de l\u2019ensemble de test (que le mod\xE8le n\u2019a pas vu) pour avoir une id\xE9e de la qualit\xE9 des r\xE9sum\xE9s. Tout d\u2019abord, impl\xE9mentons une fonction simple pour afficher ensemble la critique, le titre et le r\xE9sum\xE9 g\xE9n\xE9r\xE9 :"),um=m(),g(il.$$.fragment),pm=m(),co=r("p"),hv=a("Examinons l\u2019un des exemples anglais que nous recevons :"),mm=m(),g(ul.$$.fragment),cm=m(),g(pl.$$.fragment),dm=m(),Xn=r("p"),fv=a("Ce n\u2019est pas si mal ! Nous pouvons voir que notre mod\xE8le a \xE9t\xE9 capable d\u2019effectuer un r\xE9sum\xE9 "),Du=r("em"),vv=a("abstractif"),_v=a(" en augmentant certaines parties de la critique avec de nouveaux mots. Et peut-\xEAtre que l\u2019aspect le plus cool de notre mod\xE8le est qu\u2019il est bilingue, donc nous pouvons \xE9galement g\xE9n\xE9rer des r\xE9sum\xE9s de critiques en espagnol :"),hm=m(),g(ml.$$.fragment),fm=m(),g(cl.$$.fragment),vm=m(),ho=r("p"),gv=a("Le r\xE9sum\xE9 a \xE9t\xE9 extrait directement de la critique. N\xE9anmoins, cela montre la polyvalence du mod\xE8le mT5 et vous a donn\xE9 un aper\xE7u de ce que c\u2019est que de traiter un corpus multilingue !"),_m=m(),fo=r("p"),bv=a("Ensuite, nous allons nous int\xE9resser \xE0 une t\xE2che un peu plus complexe : entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro."),this.h()},l(s){const u=u1('[data-svelte="svelte-1phssyn"]',document.head);d=l(u,"META",{name:!0,content:!0}),u.forEach(t),E=c(s),b(h.$$.fragment,s),y=c(s),L=l(s,"H1",{class:!0});var _l=o(L);k=l(_l,"A",{id:!0,class:!0,href:!0});var vo=o(k);T=l(vo,"SPAN",{});var Su=o(T);b(M.$$.fragment,Su),Su.forEach(t),vo.forEach(t),P=c(_l),S=l(_l,"SPAN",{});var Lu=o(S);A=n(Lu,"R\xE9sum\xE9 de textes"),Lu.forEach(t),_l.forEach(t),O=c(s),I.l(s),D=c(s),N=l(s,"P",{});var pn=o(N);oe=n(pn,"Dans cette section, nous allons voir comment les "),W=l(pn,"EM",{});var Mu=o(W);re=n(Mu,"transformers"),Mu.forEach(t),F=n(pn," peuvent \xEAtre utilis\xE9s pour condenser de longs documents en r\xE9sum\xE9s, une t\xE2che connue sous le nom de "),ee=l(pn,"EM",{});var Au=o(ee);G=n(Au,"r\xE9sum\xE9 de texte"),Au.forEach(t),he=n(pn,". Il s\u2019agit de l\u2019une des t\xE2ches de NLP les plus difficiles car elle requiert une s\xE9rie de capacit\xE9s, telles que la compr\xE9hension de longs passages et la g\xE9n\xE9ration d\u2019un texte coh\xE9rent qui capture les sujets principaux d\u2019un document. Cependant, lorsqu\u2019il est bien fait, le r\xE9sum\xE9 de texte est un outil puissant qui peut acc\xE9l\xE9rer divers processus commerciaux en soulageant les experts du domaine de la lecture d\xE9taill\xE9e de longs documents."),pn.forEach(t),K=c(s),b(ie.$$.fragment,s),ne=c(s),Q=l(s,"P",{});var ta=o(Q);R=n(ta,"Bien qu\u2019il existe d\xE9j\xE0 plusieurs mod\xE8les "),se=l(ta,"EM",{});var _o=o(se);B=n(_o,"finetun\xE9s"),_o.forEach(t),ae=n(ta," pour le r\xE9sum\xE9 sur le "),Y=l(ta,"A",{href:!0,rel:!0});var Ou=o(Y);ge=l(Ou,"EM",{});var go=o(ge);Z=n(go,"Hub"),go.forEach(t),Ou.forEach(t),ke=n(ta,", la plupart d\u2019entre eux ne sont adapt\xE9s qu\u2019aux documents en anglais. Ainsi, pour ajouter une touche d\u2019originalit\xE9 \xE0 cette section, nous allons entra\xEEner un mod\xE8le bilingue pour l\u2019anglais et l\u2019espagnol. \xC0 la fin de cette section, vous disposerez d\u2019un "),me=l(ta,"A",{href:!0,rel:!0});var bo=o(me);le=n(bo,"mod\xE8le"),bo.forEach(t),fe=n(ta," capable de r\xE9sumer les commentaires des clients comme celui pr\xE9sent\xE9 ici :"),ta.forEach(t),ce=c(s),H=l(s,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),o(H).forEach(t),X=c(s),be=l(s,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),o(be).forEach(t),Ne=c(s),x=l(s,"P",{});var Lv=o(x);J=n(Lv,"Comme nous allons le voir, ces r\xE9sum\xE9s sont concis car ils sont appris \xE0 partir des titres que les clients fournissent dans leurs commentaires sur les produits. Commen\xE7ons par constituer un corpus bilingue appropri\xE9 pour cette t\xE2che."),Lv.forEach(t),Pe=c(s),De=l(s,"H2",{class:!0});var bm=o(De);qe=l(bm,"A",{id:!0,class:!0,href:!0});var Mv=o(qe);Ie=l(Mv,"SPAN",{});var Av=o(Ie);b(je.$$.fragment,Av),Av.forEach(t),Mv.forEach(t),Ve=c(bm),Ge=l(bm,"SPAN",{});var Ov=o(Ge);ss=n(Ov,"Pr\xE9paration d'un corpus multilingue"),Ov.forEach(t),bm.forEach(t),ye=c(s),$e=l(s,"P",{});var qo=o($e);ms=n(qo,"Nous allons utiliser le "),ts=l(qo,"A",{href:!0,rel:!0});var Nv=o(ts);Se=l(Nv,"EM",{});var Rv=o(Se);as=n(Rv,"Multilingual Amazon Reviews Corpus"),Rv.forEach(t),Nv.forEach(t),cs=n(qo," pour cr\xE9er notre r\xE9sumeur bilingue. Ce corpus est constitu\xE9 de critiques de produits Amazon en six langues et est g\xE9n\xE9ralement utilis\xE9 pour \xE9valuer les classifieurs multilingues. Cependant, comme chaque critique est accompagn\xE9e d\u2019un titre court, nous pouvons utiliser les titres comme r\xE9sum\xE9s cibles pour l\u2019apprentissage de notre mod\xE8le ! Pour commencer, t\xE9l\xE9chargeons les sous-ensembles anglais et espagnols depuis le "),Le=l(qo,"EM",{});var Iv=o(Le);Ls=n(Iv,"Hub"),Iv.forEach(t),ns=n(qo," :"),qo.forEach(t),bs=c(s),b(de.$$.fragment,s),ds=c(s),b(ve.$$.fragment,s),qs=c(s),_e=l(s,"P",{});var Zn=o(_e);js=n(Zn,"Comme vous pouvez le voir, pour chaque langue, il y a 200 000 critiques pour la partie entra\xEEnement et 5 000 critiques pour chacune des parties validation et test. Les informations qui nous int\xE9ressent sont contenues dans les colonnes "),rs=l(Zn,"CODE",{});var Gv=o(rs);$s=n(Gv,"review_body"),Gv.forEach(t),xs=n(Zn," et "),hs=l(Zn,"CODE",{});var Uv=o(hs);We=n(Uv,"review_title"),Uv.forEach(t),$=n(Zn,". Voyons quelques exemples en cr\xE9ant une fonction simple qui prend un \xE9chantillon al\xE9atoire de l\u2019ensemble d\u2019entra\xEEnement avec les techniques apprises au "),V=l(Zn,"A",{href:!0});var Hv=o(V);Ms=n(Hv,"chapitre 5"),Hv.forEach(t),ls=n(Zn," :"),Zn.forEach(t),Es=c(s),b(os.$$.fragment,s),xe=c(s),b(Ke.$$.fragment,s),As=c(s),b(Qe.$$.fragment,s),ue=c(s),Ue=l(s,"P",{});var jo=o(Ue);U=n(jo,"Cet \xE9chantillon montre la diversit\xE9 des critiques que l\u2019on trouve g\xE9n\xE9ralement en ligne, allant du positif au n\xE9gatif (et tout ce qui se trouve entre les deux !). Bien que l\u2019exemple avec le titre \xAB meh \xBB ne soit pas tr\xE8s informatif, les autres titres semblent \xEAtre des r\xE9sum\xE9s d\xE9cents des critiques. Entra\xEEner un mod\xE8le de r\xE9sum\xE9 sur l\u2019ensemble des 400 000 avis prendrait beaucoup trop de temps sur un seul GPU, nous allons donc nous concentrer sur la g\xE9n\xE9ration de r\xE9sum\xE9s pour un seul domaine de produits. Pour avoir une id\xE9e des domaines parmi lesquels nous pouvons choisir, convertissons "),pe=l(jo,"CODE",{});var Fv=o(pe);et=n(Fv,"english_dataset"),Fv.forEach(t),Os=n(jo," en "),Ns=l(jo,"CODE",{});var Jv=o(Ns);Ee=n(Jv,"pandas.DataFrame"),Jv.forEach(t),Is=n(jo," et calculons le nombre d\u2019avis par cat\xE9gorie de produits :"),jo.forEach(t),ks=c(s),b(Je.$$.fragment,s),Rs=c(s),b(Be.$$.fragment,s),ut=c(s),Me=l(s,"P",{});var Pa=o(Me);aa=n(Pa,"Les produits les plus populaires du jeu de donn\xE9es anglais concernent les articles m\xE9nagers, les v\xEAtements et l\u2019\xE9lectronique sans fil. Pour rester dans le th\xE8me d\u2019Amazon, nous allons nous concentrer sur le r\xE9sum\xE9 des critiques de livres. Apr\xE8s tout, c\u2019est la raison d\u2019\xEAtre de l\u2019entreprise ! Nous pouvons voir deux cat\xE9gories de produits qui correspondent \xE0 nos besoins ("),ze=l(Pa,"CODE",{});var Bv=o(ze);Gs=n(Bv,"book"),Bv.forEach(t),Lt=n(Pa," et "),is=l(Pa,"CODE",{});var Vv=o(is);na=n(Vv,"digital_ebook_purchase"),Vv.forEach(t),pt=n(Pa,"). Nous allons donc filtrer les jeux de donn\xE9es dans les deux langues pour ces produits uniquement. Comme nous l\u2019avons vu dans le "),st=l(Pa,"A",{href:!0});var Wv=o(st);ra=n(Wv,"chapitre 5"),Wv.forEach(t),mt=n(Pa,", la fonction "),Ae=l(Pa,"CODE",{});var Kv=o(Ae);la=n(Kv,"Dataset.filter()"),Kv.forEach(t),Mt=n(Pa," nous permet de d\xE9couper un jeu de donn\xE9es de mani\xE8re tr\xE8s efficace. Nous pouvons donc d\xE9finir une fonction simple pour le faire :"),Pa.forEach(t),fs=c(s),b(Us.$$.fragment,s),tt=c(s),Re=l(s,"P",{});var yt=o(Re);rr=n(yt,"Maintenant, lorsque nous appliquons cette fonction \xE0 "),At=l(yt,"CODE",{});var Qv=o(At);lr=n(Qv,"english_dataset"),Qv.forEach(t),or=n(yt," et "),oa=l(yt,"CODE",{});var Yv=o(oa);Ot=n(Yv,"spanish_dataset"),Yv.forEach(t),mn=n(yt,", le r\xE9sultat ne contient que les lignes impliquant les cat\xE9gories de livres. Avant d\u2019appliquer le filtre, changeons le format de "),at=l(yt,"CODE",{});var Xv=o(at);cn=n(Xv,"english_dataset"),Xv.forEach(t),ia=n(yt," de "),Ma=l(yt,"CODE",{});var Zv=o(Ma);dn=n(Zv,'"pandas"'),Zv.forEach(t),Nt=n(yt," \xE0 "),ua=l(yt,"CODE",{});var e_=o(ua);Hs=n(e_,'"arrow"'),e_.forEach(t),ir=n(yt," :"),yt.forEach(t),pa=c(s),b(Rt.$$.fragment,s),hn=c(s),ct=l(s,"P",{});var s_=o(ct);ur=n(s_,"Nous pouvons ensuite appliquer la fonction de filtrage et, \xE0 titre de v\xE9rification, inspecter un \xE9chantillon de critiques pour voir si elles portent bien sur des livres :"),s_.forEach(t),fn=c(s),b(dt.$$.fragment,s),Fs=c(s),b(ys.$$.fragment,s),ma=c(s),He=l(s,"P",{});var Da=o(He);pr=n(Da,"D\u2019accord, nous pouvons voir que les critiques ne concernent pas strictement les livres et peuvent se r\xE9f\xE9rer \xE0 des choses comme des calendriers et des applications \xE9lectroniques telles que OneNote. N\xE9anmoins, le domaine semble appropri\xE9 pour entra\xEEner un mod\xE8le de r\xE9sum\xE9. Avant de regarder les diff\xE9rents mod\xE8les qui conviennent \xE0 cette t\xE2che, nous avons une derni\xE8re pr\xE9paration de donn\xE9es \xE0 faire : combiner les critiques anglaises et espagnoles en un seul objet "),It=l(Da,"CODE",{});var t_=o(It);mr=n(t_,"DatasetDict"),t_.forEach(t),vn=n(Da,". \u{1F917} "),Js=l(Da,"EM",{});var a_=o(Js);cr=n(a_,"Datasets"),a_.forEach(t),Aa=n(Da," fournit une fonction pratique "),Oa=l(Da,"CODE",{});var n_=o(Oa);dr=n(n_,"concatenate_datasets()"),n_.forEach(t),_n=n(Da," qui (comme son nom l\u2019indique) va empiler deux objets "),vs=l(Da,"CODE",{});var r_=o(vs);Gt=n(r_,"Dataset"),r_.forEach(t),hr=n(Da," l\u2019un sur l\u2019autre. Ainsi, pour cr\xE9er notre jeu de donn\xE9es bilingue, nous allons boucler sur chaque division, concat\xE9ner les jeux de donn\xE9es pour cette division, et m\xE9langer le r\xE9sultat pour s\u2019assurer que notre mod\xE8le ne s\u2019adapte pas trop \xE0 une seule langue :"),Da.forEach(t),ca=c(s),b(Ut.$$.fragment,s),gn=c(s),b(Ht.$$.fragment,s),ht=c(s),da=l(s,"P",{});var l_=o(da);Na=n(l_,"Cela ressemble certainement \xE0 un m\xE9lange de critiques anglaises et espagnoles ! Maintenant que nous avons un corpus d\u2019entra\xEEnement, une derni\xE8re chose \xE0 v\xE9rifier est la distribution des mots dans les critiques et leurs titres. Ceci est particuli\xE8rement important pour les t\xE2ches de r\xE9sum\xE9, o\xF9 les r\xE9sum\xE9s de r\xE9f\xE9rence courts dans les donn\xE9es peuvent biaiser le mod\xE8le pour qu\u2019il ne produise qu\u2019un ou deux mots dans les r\xE9sum\xE9s g\xE9n\xE9r\xE9s. Les graphiques ci-dessous montrent les distributions de mots, et nous pouvons voir que les titres sont fortement biais\xE9s vers seulement 1 ou 2 mots :"),l_.forEach(t),bn=c(s),nt=l(s,"DIV",{class:!0});var qm=o(nt);Ft=l(qm,"IMG",{class:!0,src:!0,alt:!0}),fr=c(qm),Jt=l(qm,"IMG",{class:!0,src:!0,alt:!0}),qm.forEach(t),jn=c(s),Bs=l(s,"P",{});var jm=o(Bs);vr=n(jm,"Pour y rem\xE9dier, nous allons filtrer les exemples avec des titres tr\xE8s courts afin que notre mod\xE8le puisse produire des r\xE9sum\xE9s plus int\xE9ressants. Puisque nous avons affaire \xE0 des textes anglais et espagnols, nous pouvons utiliser une heuristique grossi\xE8re pour s\xE9parer les titres sur les espaces blancs, puis utiliser notre fid\xE8le m\xE9thode "),Ra=l(jm,"CODE",{});var o_=o(Ra);Ia=n(o_,"Dataset.filter()"),o_.forEach(t),_r=n(jm," comme suit :"),jm.forEach(t),$n=c(s),b(rt.$$.fragment,s),xn=c(s),Vs=l(s,"P",{});var $o=o(Vs);Ga=n($o,"Maintenant que nous avons pr\xE9par\xE9 notre corpus, voyons quelques "),Ua=l($o,"EM",{});var i_=o(Ua);gr=n(i_,"transformers"),i_.forEach(t),Ha=n($o," possibles que l\u2019on pourrait "),Fa=l($o,"EM",{});var u_=o(Fa);br=n(u_,"finetun\xE9"),u_.forEach(t),Ja=n($o," dessus !"),$o.forEach(t),En=c(s),lt=l(s,"H2",{class:!0});var $m=o(lt);ot=l($m,"A",{id:!0,class:!0,href:!0});var p_=o(ot);ft=l(p_,"SPAN",{});var m_=o(ft);b(Bt.$$.fragment,m_),m_.forEach(t),p_.forEach(t),kn=c($m),it=l($m,"SPAN",{});var c_=o(it);wn=n(c_,"Mod\xE8les pour le r\xE9sum\xE9 de texte"),c_.forEach(t),$m.forEach(t),vt=c(s),ws=l(s,"P",{});var er=o(ws);_t=n(er,"Si vous y pensez, le r\xE9sum\xE9 de texte est une t\xE2che similaire \xE0 la traduction automatique. Nous avons un corps de texte, comme une critique, que nous aimerions \xAB traduire \xBB en une version plus courte qui capture les caract\xE9ristiques saillantes de l\u2019entr\xE9e. En cons\xE9quence, la plupart des "),Ba=l(er,"EM",{});var d_=o(Ba);Va=n(d_,"transformers"),d_.forEach(t),qr=n(er," pour le r\xE9sum\xE9 adoptent l\u2019architecture encodeur-d\xE9codeur que nous avons rencontr\xE9e pour la premi\xE8re fois dans le "),ha=l(er,"A",{href:!0});var h_=o(ha);yn=n(h_,"chapitre 1"),h_.forEach(t),p=n(er,", bien qu\u2019il y ait quelques exceptions comme la famille de mod\xE8les GPT qui peut \xE9galement \xEAtre utilis\xE9e pour le r\xE9sum\xE9 dans des contextes peu complexes. Le tableau suivant pr\xE9sente quelques mod\xE8les pr\xE9-entra\xEEn\xE9s populaires qui peuvent \xEAtre "),z=l(er,"EM",{});var f_=o(z);gl=n(f_,"finetun\xE9s"),f_.forEach(t),bl=n(er," pour le r\xE9sum\xE9."),er.forEach(t),Wa=c(s),Vt=l(s,"TABLE",{});var xm=o(Vt);zn=l(xm,"THEAD",{});var v_=o(zn);Fe=l(v_,"TR",{});var xo=o(Fe);Ka=l(xo,"TH",{align:!0});var __=o(Ka);Tn=l(__,"EM",{});var g_=o(Tn);ql=n(g_,"Transformers"),g_.forEach(t),__.forEach(t),jl=c(xo),Cn=l(xo,"TH",{});var b_=o(Cn);Pn=n(b_,"Description"),b_.forEach(t),$l=c(xo),Qa=l(xo,"TH",{align:!0});var q_=o(Qa);xl=n(q_,"Multilingue ?"),q_.forEach(t),xo.forEach(t),v_.forEach(t),fa=c(xm),_s=l(xm,"TBODY",{});var zt=o(_s);gt=l(zt,"TR",{});var Eo=o(gt);Wt=l(Eo,"TD",{align:!0});var j_=o(Wt);va=l(j_,"A",{href:!0,rel:!0});var $_=o(va);El=n($_,"GPT-2"),$_.forEach(t),j_.forEach(t),Dn=c(Eo),_a=l(Eo,"TD",{});var Em=o(_a);Sn=n(Em,"Bien qu\u2019il soit entra\xEEn\xE9 comme un mod\xE8le de langage autor\xE9gressif, vous pouvez faire en sorte que le GPT-2 g\xE9n\xE8re des r\xE9sum\xE9s en ajoutant "),Ln=l(Em,"CODE",{});var x_=o(Ln);kl=n(x_,"TL;DR"),x_.forEach(t),ga=n(Em," \xE0 la fin du texte d\u2019entr\xE9e."),Em.forEach(t),wl=c(Eo),Ya=l(Eo,"TD",{align:!0});var E_=o(Ya);Mn=n(E_,"\u274C"),E_.forEach(t),Eo.forEach(t),yl=c(zt),bt=l(zt,"TR",{});var ko=o(bt);Xa=l(ko,"TD",{align:!0});var k_=o(Xa);Ws=l(k_,"A",{href:!0,rel:!0});var w_=o(Ws);zl=n(w_,"PEGASUS"),w_.forEach(t),k_.forEach(t),Tl=c(ko),Ks=l(ko,"TD",{});var km=o(Ks);Cl=n(km,"Utilise un objectif de pr\xE9-entra\xEEnement pour pr\xE9dire les phrases masqu\xE9es dans les textes \xE0 plusieurs phrases. Cet objectif de pr\xE9-entra\xEEnement est plus proche du r\xE9sum\xE9 que de la mod\xE9lisation du langage standard et obtient des scores \xE9lev\xE9s sur des "),An=l(km,"EM",{});var y_=o(An);Pl=n(y_,"benchmarks"),y_.forEach(t),ba=n(km," populaires."),km.forEach(t),Dl=c(ko),Za=l(ko,"TD",{align:!0});var z_=o(Za);On=n(z_,"\u274C"),z_.forEach(t),ko.forEach(t),Sl=c(zt),qt=l(zt,"TR",{});var wo=o(qt);en=l(wo,"TD",{align:!0});var T_=o(en);jt=l(T_,"A",{href:!0,rel:!0});var C_=o(jt);Ll=n(C_,"T5"),C_.forEach(t),T_.forEach(t),$t=c(wo),Qs=l(wo,"TD",{});var yo=o(Qs);Ml=n(yo,"Une architecture universelle de "),qa=l(yo,"EM",{});var P_=o(qa);Al=n(P_,"transformer"),P_.forEach(t),Ol=n(yo," qui formule toutes les t\xE2ches dans un cadre texte \xE0 texte. Par exemple, le format d\u2019entr\xE9e du mod\xE8le pour r\xE9sumer un document est "),Xe=l(yo,"CODE",{});var D_=o(Xe);Nl=n(D_,"summarize: ARTICLE"),D_.forEach(t),Rl=n(yo,"."),yo.forEach(t),Il=c(wo),sn=l(wo,"TD",{align:!0});var S_=o(sn);Gl=n(S_,"\u274C"),S_.forEach(t),wo.forEach(t),Ul=c(zt),xt=l(zt,"TR",{});var zo=o(xt);Kt=l(zo,"TD",{align:!0});var L_=o(Kt);ja=l(L_,"A",{href:!0,rel:!0});var M_=o(ja);Om=n(M_,"mT5"),M_.forEach(t),L_.forEach(t),Nm=c(zo),Ho=l(zo,"TD",{});var A_=o(Ho);Rm=n(A_,"Une version multilingue de T5, pr\xE9-entra\xEEn\xE9e sur le corpus multilingue Common Crawl (mC4), couvrant 101 langues."),A_.forEach(t),Im=c(zo),Hl=l(zo,"TD",{align:!0});var O_=o(Hl);Gm=n(O_,"\u2705"),O_.forEach(t),zo.forEach(t),Um=c(zt),tn=l(zt,"TR",{});var To=o(tn);Fl=l(To,"TD",{align:!0});var N_=o(Fl);jr=l(N_,"A",{href:!0,rel:!0});var R_=o(jr);Hm=n(R_,"BART"),R_.forEach(t),N_.forEach(t),Fm=c(To),$r=l(To,"TD",{});var wm=o($r);Jm=n(wm,"Une architecture de "),Fo=l(wm,"EM",{});var I_=o(Fo);Bm=n(I_,"transformer"),I_.forEach(t),Vm=n(wm," avec une pile d\u2019encodeurs et de d\xE9codeurs entra\xEEn\xE9s pour reconstruire l\u2019entr\xE9e corrompue qui combine les sch\xE9mas de pr\xE9-entra\xEEnement de BERT et GPT-2."),wm.forEach(t),Wm=c(To),Jl=l(To,"TD",{align:!0});var G_=o(Jl);Km=n(G_,"\u274C"),G_.forEach(t),To.forEach(t),Qm=c(zt),an=l(zt,"TR",{});var Co=o(an);Bl=l(Co,"TD",{align:!0});var U_=o(Bl);xr=l(U_,"A",{href:!0,rel:!0});var H_=o(xr);Ym=n(H_,"mBART-50"),H_.forEach(t),U_.forEach(t),Xm=c(Co),Jo=l(Co,"TD",{});var F_=o(Jo);Zm=n(F_,"Une version multilingue de BART, pr\xE9-entra\xEEn\xE9e sur 50 langues."),F_.forEach(t),ec=c(Co),Vl=l(Co,"TD",{align:!0});var J_=o(Vl);sc=n(J_,"\u2705"),J_.forEach(t),Co.forEach(t),zt.forEach(t),xm.forEach(t),Nu=c(s),$a=l(s,"P",{});var Po=o($a);tc=n(Po,"Comme vous pouvez le voir dans ce tableau, la majorit\xE9 des "),Bo=l(Po,"EM",{});var B_=o(Bo);ac=n(B_,"transformers"),B_.forEach(t),nc=n(Po," pour le r\xE9sum\xE9 (et en fait la plupart des t\xE2ches de NLP) sont monolingues. C\u2019est une bonne chose si votre t\xE2che se d\xE9roule dans une langue \xAB \xE0 haute ressource \xBB comme l\u2019anglais ou l\u2019allemand, mais moins pour les milliers d\u2019autres langues utilis\xE9es dans le monde. Heureusement, il existe une cat\xE9gorie de "),Vo=l(Po,"EM",{});var V_=o(Vo);rc=n(V_,"transformers"),V_.forEach(t),lc=n(Po," multilingues, comme mT5 et mBART, qui viennent \xE0 la rescousse. Ces mod\xE8les sont pr\xE9-entra\xEEn\xE9s en utilisant la mod\xE9lisation du langage mais avec une particularit\xE9 : au lieu d\u2019\xEAtre entra\xEEn\xE9 sur un corpus d\u2019une seule langue, ils sont entra\xEEn\xE9s conjointement sur des textes dans plus de 50 langues !"),Po.forEach(t),Ru=c(s),Et=l(s,"P",{});var sr=o(Et);oc=n(sr,"Nous allons nous concentrer sur mT5, une architecture int\xE9ressante bas\xE9e sur T5 qui a \xE9t\xE9 pr\xE9-entra\xEEn\xE9e dans un cadre texte \xE0 texte. Dans T5, chaque t\xE2che de NLP est formul\xE9e en termes d\u2019un pr\xE9fixe de "),Wo=l(sr,"EM",{});var W_=o(Wo);ic=n(W_,"prompt"),W_.forEach(t),uc=n(sr," comme "),Ko=l(sr,"CODE",{});var K_=o(Ko);pc=n(K_,"summarize:"),K_.forEach(t),mc=n(sr," qui conditionne le mod\xE8le \xE0 adapter le texte g\xE9n\xE9r\xE9 au "),Qo=l(sr,"EM",{});var Q_=o(Qo);cc=n(Q_,"prompt"),Q_.forEach(t),dc=n(sr,". Comme le montre la figure ci-dessous, cela rend le T5 extr\xEAmement polyvalent car vous pouvez r\xE9soudre de nombreuses t\xE2ches avec un seul mod\xE8le !"),sr.forEach(t),Iu=c(s),nn=l(s,"DIV",{class:!0});var ym=o(nn);Er=l(ym,"IMG",{class:!0,src:!0,alt:!0}),hc=c(ym),kr=l(ym,"IMG",{class:!0,src:!0,alt:!0}),ym.forEach(t),Gu=c(s),Wl=l(s,"P",{});var Y_=o(Wl);fc=n(Y_,"mT5 n\u2019utilise pas de pr\xE9fixes mais partage une grande partie de la polyvalence de T5 et a l\u2019avantage d\u2019\xEAtre multilingue. Maintenant que nous avons choisi un mod\xE8le, voyons comment pr\xE9parer nos donn\xE9es pour l\u2019entra\xEEnement."),Y_.forEach(t),Uu=c(s),b(Nn.$$.fragment,s),Hu=c(s),rn=l(s,"H2",{class:!0});var zm=o(rn);Rn=l(zm,"A",{id:!0,class:!0,href:!0});var X_=o(Rn);Yo=l(X_,"SPAN",{});var Z_=o(Yo);b(wr.$$.fragment,Z_),Z_.forEach(t),X_.forEach(t),vc=c(zm),Xo=l(zm,"SPAN",{});var eg=o(Xo);_c=n(eg,"Pr\xE9traitement des donn\xE9es"),eg.forEach(t),zm.forEach(t),Fu=c(s),b(yr.$$.fragment,s),Ju=c(s),zs=l(s,"P",{});var Tt=o(zs);gc=n(Tt,"Notre prochaine t\xE2che est de tokeniser et d\u2019encoder nos critiques et leurs titres. Comme d\u2019habitude, nous commen\xE7ons par charger le "),Zo=l(Tt,"EM",{});var sg=o(Zo);bc=n(sg,"tokenizer"),sg.forEach(t),qc=n(Tt," associ\xE9 au "),ei=l(Tt,"EM",{});var tg=o(ei);jc=n(tg,"checkpoint"),tg.forEach(t),$c=n(Tt," du mod\xE8le pr\xE9-entra\xEEn\xE9. Nous utiliserons "),si=l(Tt,"CODE",{});var ag=o(si);xc=n(ag,"mt5-small"),ag.forEach(t),Ec=n(Tt," comme "),ti=l(Tt,"EM",{});var ng=o(ti);kc=n(ng,"checkpoint"),ng.forEach(t),wc=n(Tt," afin de pouvoir "),ai=l(Tt,"EM",{});var rg=o(ai);yc=n(rg,"finetuner"),rg.forEach(t),zc=n(Tt," le mod\xE8le en un temps raisonnable :"),Tt.forEach(t),Bu=c(s),b(zr.$$.fragment,s),Vu=c(s),b(In.$$.fragment,s),Wu=c(s),Gn=l(s,"P",{});var Tm=o(Gn);Tc=n(Tm,"Testons le "),ni=l(Tm,"EM",{});var lg=o(ni);Cc=n(lg,"tokenizer"),lg.forEach(t),Pc=n(Tm," de mT5 sur un petit exemple :"),Tm.forEach(t),Ku=c(s),b(Tr.$$.fragment,s),Qu=c(s),b(Cr.$$.fragment,s),Yu=c(s),Ze=l(s,"P",{});var Ds=o(Ze);Dc=n(Ds,"Ici nous pouvons voir les familiers "),ri=l(Ds,"CODE",{});var og=o(ri);Sc=n(og,"input_ids"),og.forEach(t),Lc=n(Ds," et "),li=l(Ds,"CODE",{});var ig=o(li);Mc=n(ig,"attention_mask"),ig.forEach(t),Ac=n(Ds," que nous avons rencontr\xE9s dans nos premi\xE8res exp\xE9riences de "),oi=l(Ds,"EM",{});var ug=o(oi);Oc=n(ug,"finetuning"),ug.forEach(t),Nc=n(Ds," au "),Kl=l(Ds,"A",{href:!0});var pg=o(Kl);Rc=n(pg,"chapitre 3"),pg.forEach(t),Ic=n(Ds,". D\xE9codons ces identifiants d\u2019entr\xE9e avec la fonction "),ii=l(Ds,"CODE",{});var mg=o(ii);Gc=n(mg,"convert_ids_to_tokens()"),mg.forEach(t),Uc=n(Ds," du "),ui=l(Ds,"EM",{});var cg=o(ui);Hc=n(cg,"tokenizer"),cg.forEach(t),Fc=n(Ds," pour voir \xE0 quel type de "),pi=l(Ds,"EM",{});var dg=o(pi);Jc=n(dg,"tokenizer"),dg.forEach(t),Bc=n(Ds," nous avons affaire :"),Ds.forEach(t),Xu=c(s),b(Pr.$$.fragment,s),Zu=c(s),b(Dr.$$.fragment,s),ep=c(s),Ts=l(s,"P",{});var Ct=o(Ts);Vc=n(Ct,"Le caract\xE8re Unicode sp\xE9cial "),mi=l(Ct,"CODE",{});var hg=o(mi);Wc=n(hg,"\u2581"),hg.forEach(t),Kc=n(Ct," et le "),ci=l(Ct,"EM",{});var fg=o(ci);Qc=n(fg,"token"),fg.forEach(t),Yc=n(Ct," de fin de s\xE9quence "),di=l(Ct,"CODE",{});var vg=o(di);Xc=n(vg,"</s>"),vg.forEach(t),Zc=n(Ct," indiquent que nous avons affaire au "),hi=l(Ct,"EM",{});var _g=o(hi);ed=n(_g,"tokenizer"),_g.forEach(t),sd=n(Ct," de SentencePiece, qui est bas\xE9 sur l\u2019algorithme de segmentation Unigram discut\xE9 dans le "),Ql=l(Ct,"A",{href:!0});var gg=o(Ql);td=n(gg,"chapitre 6"),gg.forEach(t),ad=n(Ct,". Unigram est particuli\xE8rement utile pour les corpus multilingues car il permet \xE0 SentencePiece d\u2019\xEAtre agnostique vis-\xE0-vis des accents, de la ponctuation et du fait que de nombreuses langues, comme le japonais, n\u2019ont pas de caract\xE8res d\u2019espacement."),Ct.forEach(t),sp=c(s),xa=l(s,"P",{});var Do=o(xa);nd=n(Do,"Pour tokeniser notre corpus, nous devons faire face \xE0 une subtilit\xE9 associ\xE9e au r\xE9sum\xE9 : comme nos \xE9tiquettes sont \xE9galement du texte, il est possible qu\u2019elles d\xE9passent la taille maximale du contexte du mod\xE8le. Cela signifie que nous devons appliquer une troncature \xE0 la fois aux critiques et \xE0 leurs titres pour nous assurer de ne pas transmettre des entr\xE9es trop longues \xE0 notre mod\xE8le. Les tokenizers de \u{1F917} "),fi=l(Do,"EM",{});var bg=o(fi);rd=n(bg,"Transformers"),bg.forEach(t),ld=n(Do," fournissent une fonction tr\xE8s pratique "),vi=l(Do,"CODE",{});var qg=o(vi);od=n(qg,"as_target_tokenizer()"),qg.forEach(t),id=n(Do," qui vous permet de tokeniser les \xE9tiquettes en parall\xE8le avec les entr\xE9es. Ceci est typiquement fait en utilisant un gestionnaire de contexte \xE0 l\u2019int\xE9rieur d\u2019une fonction de pr\xE9traitement qui encode d\u2019abord les entr\xE9es, et ensuite encode les \xE9tiquettes comme une colonne s\xE9par\xE9e. Voici un exemple d\u2019une telle fonction pour mT5 :"),Do.forEach(t),tp=c(s),b(Sr.$$.fragment,s),ap=c(s),Ys=l(s,"P",{});var Sa=o(Ys);ud=n(Sa,"Parcourons ce code pour comprendre ce qui se passe. La premi\xE8re chose que nous avons faite est de d\xE9finir des valeurs pour "),_i=l(Sa,"CODE",{});var jg=o(_i);pd=n(jg,"max_input_length"),jg.forEach(t),md=n(Sa," et "),gi=l(Sa,"CODE",{});var $g=o(gi);cd=n($g,"max_target_length"),$g.forEach(t),dd=n(Sa,", qui fixent les limites sup\xE9rieures de la longueur des commentaires et des titres. Comme le corps de la critique est g\xE9n\xE9ralement beaucoup plus long que le titre, nous avons mis ces valeurs \xE0 l\u2019\xE9chelle en cons\xE9quence. Ensuite, dans la "),bi=l(Sa,"CODE",{});var xg=o(bi);hd=n(xg,"preprocess_function()"),xg.forEach(t),fd=n(Sa," elle-m\xEAme, nous pouvons voir que les commentaires sont d\u2019abord tokeniz\xE9s, suivis par les titres avec "),qi=l(Sa,"CODE",{});var Eg=o(qi);vd=n(Eg,"as_target_tokenizer()"),Eg.forEach(t),_d=n(Sa,"."),Sa.forEach(t),np=c(s),Ea=l(s,"P",{});var So=o(Ea);gd=n(So,"Avec la fonction "),ji=l(So,"CODE",{});var kg=o(ji);bd=n(kg,"preprocess_function()"),kg.forEach(t),qd=n(So,", il est alors simple de tokeniser l\u2019ensemble du corpus en utilisant la fonction pratique "),$i=l(So,"CODE",{});var wg=o($i);jd=n(wg,"Dataset.map()"),wg.forEach(t),$d=n(So," que nous avons largement utilis\xE9e dans ce cours :"),So.forEach(t),rp=c(s),b(Lr.$$.fragment,s),lp=c(s),Yl=l(s,"P",{});var yg=o(Yl);xd=n(yg,"Maintenant que le corpus a \xE9t\xE9 pr\xE9trait\xE9, examinons certaines m\xE9triques couramment utilis\xE9es pour le r\xE9sum\xE9. Comme nous allons le voir, il n\u2019existe pas de solution miracle pour mesurer la qualit\xE9 d\u2019un texte g\xE9n\xE9r\xE9 par une machine."),yg.forEach(t),op=c(s),b(Un.$$.fragment,s),ip=c(s),ln=l(s,"H2",{class:!0});var Cm=o(ln);Hn=l(Cm,"A",{id:!0,class:!0,href:!0});var zg=o(Hn);xi=l(zg,"SPAN",{});var Tg=o(xi);b(Mr.$$.fragment,Tg),Tg.forEach(t),zg.forEach(t),Ed=c(Cm),Ei=l(Cm,"SPAN",{});var Cg=o(Ei);kd=n(Cg,"M\xE9triques pour le r\xE9sum\xE9 de texte"),Cg.forEach(t),Cm.forEach(t),up=c(s),b(Ar.$$.fragment,s),pp=c(s),Xl=l(s,"P",{});var Pg=o(Xl);wd=n(Pg,"Par rapport \xE0 la plupart des autres t\xE2ches que nous avons abord\xE9es dans ce cours, la mesure des performances des t\xE2ches de g\xE9n\xE9ration de texte comme le r\xE9sum\xE9 ou la traduction n\u2019est pas aussi simple. Par exemple, pour une critique telle que \xAB J\u2019ai ador\xE9 lire les Hunger Games \xBB, il existe plusieurs r\xE9sum\xE9s valides, comme \xAB J\u2019ai ador\xE9 Hunger Games \xBB ou \xAB Hunger Games est une excellente lecture \xBB. Il est clair que l\u2019application d\u2019une sorte de correspondance exacte entre le r\xE9sum\xE9 g\xE9n\xE9r\xE9 et l\u2019\xE9tiquette n\u2019est pas une bonne solution. En effet, m\xEAme les humains auraient de mauvais r\xE9sultats avec une telle mesure, car nous avons tous notre propre style d\u2019\xE9criture."),Pg.forEach(t),mp=c(s),ka=l(s,"P",{});var Lo=o(ka);yd=n(Lo,"Pour le r\xE9sum\xE9, l\u2019une des m\xE9triques les plus couramment utilis\xE9es est le "),Or=l(Lo,"A",{href:!0,rel:!0});var Dg=o(Or);zd=n(Dg,"score ROUGE"),Dg.forEach(t),Td=n(Lo," (abr\xE9viation de "),ki=l(Lo,"EM",{});var Sg=o(ki);Cd=n(Sg,"Recall-Oriented Understudy for Gisting Evaluation"),Sg.forEach(t),Pd=n(Lo,"). L\u2019id\xE9e de base de cette m\xE9trique est de comparer un r\xE9sum\xE9 g\xE9n\xE9r\xE9 avec un ensemble de r\xE9sum\xE9s de r\xE9f\xE9rence qui sont g\xE9n\xE9ralement cr\xE9\xE9s par des humains. Pour \xEAtre plus pr\xE9cis, supposons que nous voulions comparer les deux r\xE9sum\xE9s suivants :"),Lo.forEach(t),cp=c(s),b(Nr.$$.fragment,s),dp=c(s),wa=l(s,"P",{});var Mo=o(wa);Dd=n(Mo,"Une fa\xE7on de les comparer pourrait \xEAtre de compter le nombre de mots qui se chevauchent, qui dans ce cas serait de 6. Cependant, cette m\xE9thode est un peu grossi\xE8re, c\u2019est pourquoi ROUGE se base sur le calcul des scores de "),wi=l(Mo,"EM",{});var Lg=o(wi);Sd=n(Lg,"pr\xE9cision"),Lg.forEach(t),Ld=n(Mo," et de "),yi=l(Mo,"EM",{});var Mg=o(yi);Md=n(Mg,"rappel"),Mg.forEach(t),Ad=n(Mo," pour le chevauchement."),Mo.forEach(t),hp=c(s),b(Fn.$$.fragment,s),fp=c(s),Rr=l(s,"P",{});var qv=o(Rr);Od=n(qv,`Pour ROUGE, le rappel mesure la proportion du r\xE9sum\xE9 de r\xE9f\xE9rence qui est captur\xE9e par le r\xE9sum\xE9 g\xE9n\xE9r\xE9. Si nous ne faisons que comparer des mots, le rappel peut \xEAtre calcul\xE9 selon la formule suivante :
`),vp=e1(qv),qv.forEach(t),_p=c(s),Ir=l(s,"P",{});var jv=o(Ir);Nd=n(jv,`Pour notre exemple simple ci-dessus, cette formule donne un rappel parfait de 6/6 = 1, c\u2019est-\xE0-dire que tous les mots du r\xE9sum\xE9 de r\xE9f\xE9rence ont \xE9t\xE9 produits par le mod\xE8le. Cela peut sembler g\xE9nial, mais imaginez que le r\xE9sum\xE9 g\xE9n\xE9r\xE9 ait \xE9t\xE9 \xAB J\u2019ai vraiment aim\xE9 lire les Hunger Games toute la nuit \xBB. Le rappel serait \xE9galement parfait, mais le r\xE9sum\xE9 serait sans doute moins bon puisqu\u2019il serait verbeux. Pour traiter ces sc\xE9narios, nous calculons \xE9galement la pr\xE9cision, qui dans le contexte de ROUGE, mesure la proportion du r\xE9sum\xE9 g\xE9n\xE9r\xE9 qui est pertinente :
`),gp=e1(jv),jv.forEach(t),bp=c(s),kt=l(s,"P",{});var tr=o(kt);Rd=n(tr,"En appliquant cela \xE0 notre r\xE9sum\xE9 verbeux, on obtient une pr\xE9cision de 6/10 = 0,6, ce qui est consid\xE9rablement moins bon que la pr\xE9cision de 6/7 = 0,86 obtenue par notre r\xE9sum\xE9 plus court. En pratique, la pr\xE9cision et le rappel sont g\xE9n\xE9ralement calcul\xE9s, puis le score F1 (la moyenne harmonique de la pr\xE9cision et du rappel) est indiqu\xE9. Nous pouvons le faire facilement dans \u{1F917} "),zi=l(tr,"EM",{});var Ag=o(zi);Id=n(Ag,"Datasets"),Ag.forEach(t),Gd=n(tr," en installant d\u2019abord le "),Ti=l(tr,"EM",{});var Og=o(Ti);Ud=n(Og,"package"),Og.forEach(t),Hd=c(tr),Ci=l(tr,"CODE",{});var Ng=o(Ci);Fd=n(Ng,"rouge_score"),Ng.forEach(t),Jd=n(tr," :"),tr.forEach(t),qp=c(s),b(Gr.$$.fragment,s),jp=c(s),Zl=l(s,"P",{});var Rg=o(Zl);Bd=n(Rg,"et ensuite charger la m\xE9trique ROUGE comme suit :"),Rg.forEach(t),$p=c(s),b(Ur.$$.fragment,s),xp=c(s),Jn=l(s,"P",{});var Pm=o(Jn);Vd=n(Pm,"Ensuite, nous pouvons utiliser la fonction "),Pi=l(Pm,"CODE",{});var Ig=o(Pi);Wd=n(Ig,"rouge_score.compute()"),Ig.forEach(t),Kd=n(Pm," pour calculer toutes les m\xE9triques en une seule fois :"),Pm.forEach(t),Ep=c(s),b(Hr.$$.fragment,s),kp=c(s),b(Fr.$$.fragment,s),wp=c(s),es=l(s,"P",{});var Ss=o(es);Qd=n(Ss,"Whoa, il y a pas mal d\u2019informations dans cette sortie. Qu\u2019est-ce que \xE7a veut dire ? Tout d\u2019abord, \u{1F917} "),Di=l(Ss,"EM",{});var Gg=o(Di);Yd=n(Gg,"Datasets"),Gg.forEach(t),Xd=n(Ss," calcule des intervalles de confiance pour la pr\xE9cision, le rappel et le score F1. Ce sont les attributs "),Si=l(Ss,"CODE",{});var Ug=o(Si);Zd=n(Ug,"low"),Ug.forEach(t),eh=n(Ss,", "),Li=l(Ss,"CODE",{});var Hg=o(Li);sh=n(Hg,"mid"),Hg.forEach(t),th=n(Ss,", et "),Mi=l(Ss,"CODE",{});var Fg=o(Mi);ah=n(Fg,"high"),Fg.forEach(t),nh=n(Ss," que vous pouvez voir ici. De plus, \u{1F917} "),Ai=l(Ss,"EM",{});var Jg=o(Ai);rh=n(Jg,"Datasets"),Jg.forEach(t),lh=n(Ss," calcule une vari\xE9t\xE9 de scores ROUGE qui sont bas\xE9s sur diff\xE9rents types de granularit\xE9 du texte lors de la comparaison des r\xE9sum\xE9s g\xE9n\xE9r\xE9s et de r\xE9f\xE9rence. La variante "),Oi=l(Ss,"CODE",{});var Bg=o(Oi);oh=n(Bg,"rouge1"),Bg.forEach(t),ih=n(Ss," est le chevauchement des unigrammes. C\u2019est juste une fa\xE7on fantaisiste de dire le chevauchement des mots et c\u2019est exactement la m\xE9trique dont nous avons discut\xE9 ci-dessus. Pour v\xE9rifier cela, nous allons extraire la valeur "),Ni=l(Ss,"CODE",{});var Vg=o(Ni);uh=n(Vg,"mid"),Vg.forEach(t),ph=n(Ss," de nos scores :"),Ss.forEach(t),yp=c(s),b(Jr.$$.fragment,s),zp=c(s),b(Br.$$.fragment,s),Tp=c(s),Cs=l(s,"P",{});var Pt=o(Cs);mh=n(Pt,"Super, les chiffres de pr\xE9cision et de rappel correspondent ! Maintenant, qu\u2019en est-il des autres scores ROUGE ? "),Ri=l(Pt,"CODE",{});var Wg=o(Ri);ch=n(Wg,"rouge2"),Wg.forEach(t),dh=n(Pt," mesure le chevauchement entre les bigrammes (chevauchement des paires de mots), tandis que "),Ii=l(Pt,"CODE",{});var Kg=o(Ii);hh=n(Kg,"rougeL"),Kg.forEach(t),fh=n(Pt," et "),Gi=l(Pt,"CODE",{});var Qg=o(Gi);vh=n(Qg,"rougeLsum"),Qg.forEach(t),_h=n(Pt," mesurent les plus longues s\xE9quences de mots correspondants en recherchant les plus longues sous-souches communes dans les r\xE9sum\xE9s g\xE9n\xE9r\xE9s et de r\xE9f\xE9rence. Le \xAB sum \xBB dans "),Ui=l(Pt,"CODE",{});var Yg=o(Ui);gh=n(Yg,"rougeLsum"),Yg.forEach(t),bh=n(Pt," fait r\xE9f\xE9rence au fait que cette m\xE9trique est calcul\xE9e sur un r\xE9sum\xE9 entier, alors que "),Hi=l(Pt,"CODE",{});var Xg=o(Hi);qh=n(Xg,"rougeL"),Xg.forEach(t),jh=n(Pt," est calcul\xE9e comme une moyenne sur des phrases individuelles."),Pt.forEach(t),Cp=c(s),b(Bn.$$.fragment,s),Pp=c(s),Vn=l(s,"P",{});var Dm=o(Vn);$h=n(Dm,"Nous utiliserons ces scores ROUGE pour suivre les performances de notre mod\xE8le, mais avant cela, faisons ce que tout bon praticien de NLP devrait faire : cr\xE9er une "),Fi=l(Dm,"EM",{});var Zg=o(Fi);xh=n(Zg,"baseline"),Zg.forEach(t),Eh=n(Dm," solide, mais simple !"),Dm.forEach(t),Dp=c(s),on=l(s,"H3",{class:!0});var Sm=o(on);Wn=l(Sm,"A",{id:!0,class:!0,href:!0});var eb=o(Wn);Ji=l(eb,"SPAN",{});var sb=o(Ji);b(Vr.$$.fragment,sb),sb.forEach(t),eb.forEach(t),kh=c(Sm),Bi=l(Sm,"SPAN",{});var tb=o(Bi);wh=n(tb,"Cr\xE9ation d'une base de r\xE9f\xE9rence solide"),tb.forEach(t),Sm.forEach(t),Sp=c(s),gs=l(s,"P",{});var Zs=o(gs);yh=n(Zs,"Une "),Vi=l(Zs,"EM",{});var ab=o(Vi);zh=n(ab,"baseline"),ab.forEach(t),Th=n(Zs," commune pour le r\xE9sum\xE9 de texte consiste \xE0 prendre simplement les trois premi\xE8res phrases d\u2019un article, souvent appel\xE9e la "),Wi=l(Zs,"EM",{});var nb=o(Wi);Ch=n(nb,"baseline"),nb.forEach(t),Ph=c(Zs),Ki=l(Zs,"EM",{});var rb=o(Ki);Dh=n(rb,"lead-3"),rb.forEach(t),Sh=n(Zs,". Nous pourrions utiliser les points pour tracker les limites des phrases mais cela \xE9chouera avec des acronymes comme \xAB U.S. \xBB ou \xAB U.N. \xBB. Nous allons donc utiliser la biblioth\xE8que "),Qi=l(Zs,"CODE",{});var lb=o(Qi);Lh=n(lb,"nltk"),lb.forEach(t),Mh=n(Zs,", qui inclut un meilleur algorithme pour g\xE9rer ces cas. Vous pouvez installer le "),Yi=l(Zs,"EM",{});var ob=o(Yi);Ah=n(ob,"package"),ob.forEach(t),Oh=n(Zs," en utilisant "),Xi=l(Zs,"CODE",{});var ib=o(Xi);Nh=n(ib,"pip"),ib.forEach(t),Rh=n(Zs," comme suit :"),Zs.forEach(t),Lp=c(s),b(Wr.$$.fragment,s),Mp=c(s),eo=l(s,"P",{});var ub=o(eo);Ih=n(ub,"puis t\xE9l\xE9chargez les r\xE8gles de ponctuation :"),ub.forEach(t),Ap=c(s),b(Kr.$$.fragment,s),Op=c(s),ya=l(s,"P",{});var Ao=o(ya);Gh=n(Ao,"Ensuite, nous importons le "),Zi=l(Ao,"EM",{});var pb=o(Zi);Uh=n(pb,"tokenizer"),pb.forEach(t),Hh=n(Ao," de "),eu=l(Ao,"CODE",{});var mb=o(eu);Fh=n(mb,"nltk"),mb.forEach(t),Jh=n(Ao," et cr\xE9ons une fonction simple pour extraire les trois premi\xE8res phrases d\u2019une critique. La convention dans le r\xE9sum\xE9 de texte est de s\xE9parer chaque r\xE9sum\xE9 avec une nouvelle ligne, donc nous allons \xE9galement inclure ceci et tester le tout sur un exemple d\u2019entra\xEEnement :"),Ao.forEach(t),Np=c(s),b(Qr.$$.fragment,s),Rp=c(s),b(Yr.$$.fragment,s),Ip=c(s),so=l(s,"P",{});var cb=o(so);Bh=n(cb,"Cela semble fonctionner, alors impl\xE9mentons maintenant une fonction qui extrait ces r\xE9sum\xE9s d\u2019un jeu de donn\xE9es et calcule les scores ROUGE pour la ligne de base :"),cb.forEach(t),Gp=c(s),b(Xr.$$.fragment,s),Up=c(s),to=l(s,"P",{});var db=o(to);Vh=n(db,"Nous pouvons ensuite utiliser cette fonction pour calculer les scores ROUGE sur l\u2019ensemble de validation et les embellir un peu en utilisant Pandas :"),db.forEach(t),Hp=c(s),b(Zr.$$.fragment,s),Fp=c(s),b(el.$$.fragment,s),Jp=c(s),Ps=l(s,"P",{});var Dt=o(Ps);Wh=n(Dt,"Nous pouvons voir que le score de "),su=l(Dt,"CODE",{});var hb=o(su);Kh=n(hb,"rouge2"),hb.forEach(t),Qh=n(Dt," est significativement plus bas que le reste. Ceci refl\xE8te probablement le fait que les titres des critiques sont typiquement concis et donc que la "),tu=l(Dt,"EM",{});var fb=o(tu);Yh=n(fb,"baseline"),fb.forEach(t),Xh=c(Dt),au=l(Dt,"EM",{});var vb=o(au);Zh=n(vb,"lead-3"),vb.forEach(t),ef=n(Dt," est trop verbeuse. Maintenant que nous disposons d\u2019une bonne "),nu=l(Dt,"EM",{});var _b=o(nu);sf=n(_b,"baseline"),_b.forEach(t),tf=n(Dt,", concentrons-nous sur le "),ru=l(Dt,"EM",{});var gb=o(ru);af=n(gb,"finetuning"),gb.forEach(t),nf=n(Dt," du mT5 !"),Dt.forEach(t),Bp=c(s),Yt.l(s),ao=c(s),b(Kn.$$.fragment,s),Vp=c(s),za=l(s,"P",{});var Oo=o(za);rf=n(Oo,"La prochaine chose que nous devons faire est de nous connecter au "),lu=l(Oo,"EM",{});var bb=o(lu);lf=n(bb,"Hub"),bb.forEach(t),of=n(Oo,". Si vous ex\xE9cutez ce code dans un "),ou=l(Oo,"EM",{});var qb=o(ou);uf=n(qb,"notebook"),qb.forEach(t),pf=n(Oo,", vous pouvez le faire avec la fonction utilitaire suivante :"),Oo.forEach(t),Wp=c(s),b(sl.$$.fragment,s),Kp=c(s),Qn=l(s,"P",{});var Lm=o(Qn);mf=n(Lm,"qui affichera un "),iu=l(Lm,"EM",{});var jb=o(iu);cf=n(jb,"widget"),jb.forEach(t),df=n(Lm," o\xF9 vous pourrez saisir vos informations d\u2019identification. Vous pouvez \xE9galement ex\xE9cuter cette commande dans votre terminal et vous connecter \xE0 partir de l\xE0 :"),Lm.forEach(t),Qp=c(s),b(tl.$$.fragment,s),Yp=c(s),us&&us.l(s),no=c(s),Ta=l(s,"P",{});var No=o(Ta);hf=n(No,"Ensuite, nous devons d\xE9finir un collateur de donn\xE9es pour notre t\xE2che de s\xE9quence \xE0 s\xE9quence. Comme mT5 est un "),uu=l(No,"EM",{});var $b=o(uu);ff=n($b,"transformer"),$b.forEach(t),vf=n(No," encodeur-d\xE9codeur, une des subtilit\xE9s de la pr\xE9paration de nos batchs est que, pendant le d\xE9codage, nous devons d\xE9caler les \xE9tiquettes d\u2019une unit\xE9 vers la droite. Ceci est n\xE9cessaire pour garantir que le d\xE9codeur ne voit que les \xE9tiquettes de v\xE9rit\xE9 terrain pr\xE9c\xE9dentes et non les \xE9tiquettes actuelles ou futures, qui seraient faciles \xE0 m\xE9moriser pour le mod\xE8le. Cela ressemble \xE0 la fa\xE7on dont l\u2019auto-attention masqu\xE9e est appliqu\xE9e aux entr\xE9es dans une t\xE2che comme "),ro=l(No,"A",{href:!0});var xb=o(ro);_f=n(xb,"la mod\xE9lisation causale du langage"),xb.forEach(t),gf=n(No,"."),No.forEach(t),Xp=c(s),Xs=l(s,"P",{});var La=o(Xs);bf=n(La,"Heureusement, \u{1F917} "),pu=l(La,"EM",{});var Eb=o(pu);qf=n(Eb,"Transformers"),Eb.forEach(t),jf=n(La," fournit un collateur "),mu=l(La,"CODE",{});var kb=o(mu);$f=n(kb,"DataCollatorForSeq2Seq"),kb.forEach(t),xf=n(La," qui rembourrera dynamiquement les entr\xE9es et les \xE9tiquettes pour nous. Pour instancier ce collateur, nous devons simplement fournir le "),cu=l(La,"EM",{});var wb=o(cu);Ef=n(wb,"tokenizer"),wb.forEach(t),kf=n(La," et le "),du=l(La,"EM",{});var yb=o(du);wf=n(yb,"mod\xE8le"),yb.forEach(t),yf=n(La," :"),La.forEach(t),Zp=c(s),Zt.l(s),lo=c(s),oo=l(s,"P",{});var zb=o(oo);zf=n(zb,"Voyons ce que produit ce collateur lorsqu\u2019on lui donne un petit batch d\u2019exemples. Tout d\u2019abord, nous devons supprimer les colonnes contenant des cha\xEEnes de caract\xE8res, car le collateur ne saura pas comment remplir ces \xE9l\xE9ments :"),zb.forEach(t),em=c(s),b(al.$$.fragment,s),sm=c(s),Ca=l(s,"P",{});var Ro=o(Ca);Tf=n(Ro,"Comme le collateur attend une liste de "),hu=l(Ro,"CODE",{});var Tb=o(hu);Cf=n(Tb,"dict"),Tb.forEach(t),Pf=n(Ro,", o\xF9 chaque "),fu=l(Ro,"CODE",{});var Cb=o(fu);Df=n(Cb,"dict"),Cb.forEach(t),Sf=n(Ro," repr\xE9sente un seul exemple du jeu de donn\xE9es, nous devons \xE9galement mettre les donn\xE9es dans le format attendu avant de les transmettre au collateur de donn\xE9es :"),Ro.forEach(t),tm=c(s),b(nl.$$.fragment,s),am=c(s),b(rl.$$.fragment,s),nm=c(s),we=l(s,"P",{});var Oe=o(we);Lf=n(Oe,"La principale chose \xE0 remarquer ici est que le premier exemple est plus long que le second, donc les "),vu=l(Oe,"CODE",{});var Pb=o(vu);Mf=n(Pb,"input_ids"),Pb.forEach(t),Af=n(Oe," et "),_u=l(Oe,"CODE",{});var Db=o(_u);Of=n(Db,"attention_mask"),Db.forEach(t),Nf=n(Oe," du second exemple ont \xE9t\xE9 compl\xE9t\xE9s sur la droite avec un "),gu=l(Oe,"EM",{});var Sb=o(gu);Rf=n(Sb,"token"),Sb.forEach(t),If=c(Oe),bu=l(Oe,"CODE",{});var Lb=o(bu);Gf=n(Lb,"[PAD]"),Lb.forEach(t),Uf=n(Oe," (dont l\u2019identifiant est "),qu=l(Oe,"CODE",{});var Mb=o(qu);Hf=n(Mb,"0"),Mb.forEach(t),Ff=n(Oe,"). De m\xEAme, nous pouvons voir que les "),ju=l(Oe,"CODE",{});var Ab=o(ju);Jf=n(Ab,"labels"),Ab.forEach(t),Bf=n(Oe," ont \xE9t\xE9 compl\xE9t\xE9s par des "),$u=l(Oe,"CODE",{});var Ob=o($u);Vf=n(Ob,"-100"),Ob.forEach(t),Wf=n(Oe,", pour s\u2019assurer que les "),xu=l(Oe,"EM",{});var Nb=o(xu);Kf=n(Nb,"tokens"),Nb.forEach(t),Qf=n(Oe," de remplissage sont ignor\xE9s par la fonction de perte. Et enfin, nous pouvons voir un nouveau "),Eu=l(Oe,"CODE",{});var Rb=o(Eu);Yf=n(Rb,"decoder_input_ids"),Rb.forEach(t),Xf=n(Oe," qui a d\xE9plac\xE9 les \xE9tiquettes vers la droite en ins\xE9rant un "),ku=l(Oe,"EM",{});var Ib=o(ku);Zf=n(Ib,"token"),Ib.forEach(t),ev=c(Oe),wu=l(Oe,"CODE",{});var Gb=o(wu);sv=n(Gb,"[PAD]"),Gb.forEach(t),tv=n(Oe," dans la premi\xE8re entr\xE9e."),Oe.forEach(t),rm=c(s),sa.l(s),io=c(s),ps&&ps.l(s),uo=c(s),un=l(s,"H2",{class:!0});var Mm=o(un);Yn=l(Mm,"A",{id:!0,class:!0,href:!0});var Ub=o(Yn);yu=l(Ub,"SPAN",{});var Hb=o(yu);b(ll.$$.fragment,Hb),Hb.forEach(t),Ub.forEach(t),av=c(Mm),po=l(Mm,"SPAN",{});var $v=o(po);nv=n($v,"Utilisation de votre mod\xE8le "),zu=l($v,"I",{});var Fb=o(zu);rv=n(Fb,"finetun\xE9"),Fb.forEach(t),$v.forEach(t),Mm.forEach(t),lm=c(s),wt=l(s,"P",{});var ar=o(wt);lv=n(ar,"Une fois que vous avez pouss\xE9 le mod\xE8le vers le "),Tu=l(ar,"EM",{});var Jb=o(Tu);ov=n(Jb,"Hub"),Jb.forEach(t),iv=n(ar,", vous pouvez jouer avec lui soit via le "),Cu=l(ar,"EM",{});var Bb=o(Cu);uv=n(Bb,"widget"),Bb.forEach(t),pv=n(ar," d\u2019inf\xE9rence, soit avec un objet "),Pu=l(ar,"CODE",{});var Vb=o(Pu);mv=n(Vb,"pipeline"),Vb.forEach(t),cv=n(ar,", comme suit :"),ar.forEach(t),om=c(s),b(ol.$$.fragment,s),im=c(s),mo=l(s,"P",{});var Wb=o(mo);dv=n(Wb,"Nous pouvons alimenter notre pipeline avec quelques exemples de l\u2019ensemble de test (que le mod\xE8le n\u2019a pas vu) pour avoir une id\xE9e de la qualit\xE9 des r\xE9sum\xE9s. Tout d\u2019abord, impl\xE9mentons une fonction simple pour afficher ensemble la critique, le titre et le r\xE9sum\xE9 g\xE9n\xE9r\xE9 :"),Wb.forEach(t),um=c(s),b(il.$$.fragment,s),pm=c(s),co=l(s,"P",{});var Kb=o(co);hv=n(Kb,"Examinons l\u2019un des exemples anglais que nous recevons :"),Kb.forEach(t),mm=c(s),b(ul.$$.fragment,s),cm=c(s),b(pl.$$.fragment,s),dm=c(s),Xn=l(s,"P",{});var Am=o(Xn);fv=n(Am,"Ce n\u2019est pas si mal ! Nous pouvons voir que notre mod\xE8le a \xE9t\xE9 capable d\u2019effectuer un r\xE9sum\xE9 "),Du=l(Am,"EM",{});var Qb=o(Du);vv=n(Qb,"abstractif"),Qb.forEach(t),_v=n(Am," en augmentant certaines parties de la critique avec de nouveaux mots. Et peut-\xEAtre que l\u2019aspect le plus cool de notre mod\xE8le est qu\u2019il est bilingue, donc nous pouvons \xE9galement g\xE9n\xE9rer des r\xE9sum\xE9s de critiques en espagnol :"),Am.forEach(t),hm=c(s),b(ml.$$.fragment,s),fm=c(s),b(cl.$$.fragment,s),vm=c(s),ho=l(s,"P",{});var Yb=o(ho);gv=n(Yb,"Le r\xE9sum\xE9 a \xE9t\xE9 extrait directement de la critique. N\xE9anmoins, cela montre la polyvalence du mod\xE8le mT5 et vous a donn\xE9 un aper\xE7u de ce que c\u2019est que de traiter un corpus multilingue !"),Yb.forEach(t),_m=c(s),fo=l(s,"P",{});var Xb=o(fo);bv=n(Xb,"Ensuite, nous allons nous int\xE9resser \xE0 une t\xE2che un peu plus complexe : entra\xEEner un mod\xE8le de langue \xE0 partir de z\xE9ro."),Xb.forEach(t),this.h()},h(){f(d,"name","hf:doc:metadata"),f(d,"content",JSON.stringify(T1)),f(k,"id","rsum-de-textes"),f(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(k,"href","#rsum-de-textes"),f(L,"class","relative group"),f(Y,"href","https://huggingface.co/models?pipeline_tag=summarization&sort=downloads"),f(Y,"rel","nofollow"),f(me,"href","https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es"),f(me,"rel","nofollow"),Io(H.src,Ce="https://hf.space/gradioiframe/course-demos/mt5-small-finetuned-amazon-en-es/+")||f(H,"src",Ce),f(H,"frameborder","0"),f(H,"height","400"),f(H,"title","Gradio app"),f(H,"class","block dark:hidden container p-0 flex-grow space-iframe"),f(H,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),f(H,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Io(be.src,Te="https://hf.space/gradioiframe/course-demos/mt5-small-finetuned-amazon-en-es-darkmode/+")||f(be,"src",Te),f(be,"frameborder","0"),f(be,"height","400"),f(be,"title","Gradio app"),f(be,"class","hidden dark:block container p-0 flex-grow space-iframe"),f(be,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),f(be,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),f(qe,"id","prparation-dun-corpus-multilingue"),f(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(qe,"href","#prparation-dun-corpus-multilingue"),f(De,"class","relative group"),f(ts,"href","https://huggingface.co/datasets/amazon_reviews_multi"),f(ts,"rel","nofollow"),f(V,"href","/course/fr/chapter5"),f(st,"href","/course/fr/chapter5"),f(Ft,"class","block dark:hidden"),Io(Ft.src,qn="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg")||f(Ft,"src",qn),f(Ft,"alt","Word count distributions for the review titles and texts."),f(Jt,"class","hidden dark:block"),Io(Jt.src,Ye="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg")||f(Jt,"src",Ye),f(Jt,"alt","Word count distributions for the review titles and texts."),f(nt,"class","flex justify-center"),f(ot,"id","modles-pour-le-rsum-de-texte"),f(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ot,"href","#modles-pour-le-rsum-de-texte"),f(lt,"class","relative group"),f(ha,"href","/course/fr/chapter1"),f(Ka,"align","center"),f(Qa,"align","center"),f(va,"href","https://huggingface.co/gpt2-xl"),f(va,"rel","nofollow"),f(Wt,"align","center"),f(Ya,"align","center"),f(Ws,"href","https://huggingface.co/google/pegasus-large"),f(Ws,"rel","nofollow"),f(Xa,"align","center"),f(Za,"align","center"),f(jt,"href","https://huggingface.co/t5-base"),f(jt,"rel","nofollow"),f(en,"align","center"),f(sn,"align","center"),f(ja,"href","https://huggingface.co/google/mt5-base"),f(ja,"rel","nofollow"),f(Kt,"align","center"),f(Hl,"align","center"),f(jr,"href","https://huggingface.co/facebook/bart-base"),f(jr,"rel","nofollow"),f(Fl,"align","center"),f(Jl,"align","center"),f(xr,"href","https://huggingface.co/facebook/mbart-large-50"),f(xr,"rel","nofollow"),f(Bl,"align","center"),f(Vl,"align","center"),f(Er,"class","block dark:hidden"),Io(Er.src,Ev="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg")||f(Er,"src",Ev),f(Er,"alt","Different tasks performed by the T5 architecture."),f(kr,"class","hidden dark:block"),Io(kr.src,kv="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg")||f(kr,"src",kv),f(kr,"alt","Different tasks performed by the T5 architecture."),f(nn,"class","flex justify-center"),f(Rn,"id","prtraitement-des-donnes"),f(Rn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Rn,"href","#prtraitement-des-donnes"),f(rn,"class","relative group"),f(Kl,"href","/course/fr/chapter3"),f(Ql,"href","/course/chapter6"),f(Hn,"id","mtriques-pour-le-rsum-de-texte"),f(Hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Hn,"href","#mtriques-pour-le-rsum-de-texte"),f(ln,"class","relative group"),f(Or,"href","https://en.wikipedia.org/wiki/ROUGE_(metric)"),f(Or,"rel","nofollow"),vp.a=null,gp.a=null,f(Wn,"id","cration-dune-base-de-rfrence-solide"),f(Wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Wn,"href","#cration-dune-base-de-rfrence-solide"),f(on,"class","relative group"),f(ro,"href","/course/fr/chapter7/6"),f(Yn,"id","utilisation-de-votre-modle-ifinetuni"),f(Yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Yn,"href","#utilisation-de-votre-modle-ifinetuni"),f(un,"class","relative group")},m(s,u){e(document.head,d),i(s,E,u),q(h,s,u),i(s,y,u),i(s,L,u),e(L,k),e(k,T),q(M,T,null),e(L,P),e(L,S),e(S,A),i(s,O,u),dl[w].m(s,u),i(s,D,u),i(s,N,u),e(N,oe),e(N,W),e(W,re),e(N,F),e(N,ee),e(ee,G),e(N,he),i(s,K,u),q(ie,s,u),i(s,ne,u),i(s,Q,u),e(Q,R),e(Q,se),e(se,B),e(Q,ae),e(Q,Y),e(Y,ge),e(ge,Z),e(Q,ke),e(Q,me),e(me,le),e(Q,fe),i(s,ce,u),i(s,H,u),i(s,X,u),i(s,be,u),i(s,Ne,u),i(s,x,u),e(x,J),i(s,Pe,u),i(s,De,u),e(De,qe),e(qe,Ie),q(je,Ie,null),e(De,Ve),e(De,Ge),e(Ge,ss),i(s,ye,u),i(s,$e,u),e($e,ms),e($e,ts),e(ts,Se),e(Se,as),e($e,cs),e($e,Le),e(Le,Ls),e($e,ns),i(s,bs,u),q(de,s,u),i(s,ds,u),q(ve,s,u),i(s,qs,u),i(s,_e,u),e(_e,js),e(_e,rs),e(rs,$s),e(_e,xs),e(_e,hs),e(hs,We),e(_e,$),e(_e,V),e(V,Ms),e(_e,ls),i(s,Es,u),q(os,s,u),i(s,xe,u),q(Ke,s,u),i(s,As,u),q(Qe,s,u),i(s,ue,u),i(s,Ue,u),e(Ue,U),e(Ue,pe),e(pe,et),e(Ue,Os),e(Ue,Ns),e(Ns,Ee),e(Ue,Is),i(s,ks,u),q(Je,s,u),i(s,Rs,u),q(Be,s,u),i(s,ut,u),i(s,Me,u),e(Me,aa),e(Me,ze),e(ze,Gs),e(Me,Lt),e(Me,is),e(is,na),e(Me,pt),e(Me,st),e(st,ra),e(Me,mt),e(Me,Ae),e(Ae,la),e(Me,Mt),i(s,fs,u),q(Us,s,u),i(s,tt,u),i(s,Re,u),e(Re,rr),e(Re,At),e(At,lr),e(Re,or),e(Re,oa),e(oa,Ot),e(Re,mn),e(Re,at),e(at,cn),e(Re,ia),e(Re,Ma),e(Ma,dn),e(Re,Nt),e(Re,ua),e(ua,Hs),e(Re,ir),i(s,pa,u),q(Rt,s,u),i(s,hn,u),i(s,ct,u),e(ct,ur),i(s,fn,u),q(dt,s,u),i(s,Fs,u),q(ys,s,u),i(s,ma,u),i(s,He,u),e(He,pr),e(He,It),e(It,mr),e(He,vn),e(He,Js),e(Js,cr),e(He,Aa),e(He,Oa),e(Oa,dr),e(He,_n),e(He,vs),e(vs,Gt),e(He,hr),i(s,ca,u),q(Ut,s,u),i(s,gn,u),q(Ht,s,u),i(s,ht,u),i(s,da,u),e(da,Na),i(s,bn,u),i(s,nt,u),e(nt,Ft),e(nt,fr),e(nt,Jt),i(s,jn,u),i(s,Bs,u),e(Bs,vr),e(Bs,Ra),e(Ra,Ia),e(Bs,_r),i(s,$n,u),q(rt,s,u),i(s,xn,u),i(s,Vs,u),e(Vs,Ga),e(Vs,Ua),e(Ua,gr),e(Vs,Ha),e(Vs,Fa),e(Fa,br),e(Vs,Ja),i(s,En,u),i(s,lt,u),e(lt,ot),e(ot,ft),q(Bt,ft,null),e(lt,kn),e(lt,it),e(it,wn),i(s,vt,u),i(s,ws,u),e(ws,_t),e(ws,Ba),e(Ba,Va),e(ws,qr),e(ws,ha),e(ha,yn),e(ws,p),e(ws,z),e(z,gl),e(ws,bl),i(s,Wa,u),i(s,Vt,u),e(Vt,zn),e(zn,Fe),e(Fe,Ka),e(Ka,Tn),e(Tn,ql),e(Fe,jl),e(Fe,Cn),e(Cn,Pn),e(Fe,$l),e(Fe,Qa),e(Qa,xl),e(Vt,fa),e(Vt,_s),e(_s,gt),e(gt,Wt),e(Wt,va),e(va,El),e(gt,Dn),e(gt,_a),e(_a,Sn),e(_a,Ln),e(Ln,kl),e(_a,ga),e(gt,wl),e(gt,Ya),e(Ya,Mn),e(_s,yl),e(_s,bt),e(bt,Xa),e(Xa,Ws),e(Ws,zl),e(bt,Tl),e(bt,Ks),e(Ks,Cl),e(Ks,An),e(An,Pl),e(Ks,ba),e(bt,Dl),e(bt,Za),e(Za,On),e(_s,Sl),e(_s,qt),e(qt,en),e(en,jt),e(jt,Ll),e(qt,$t),e(qt,Qs),e(Qs,Ml),e(Qs,qa),e(qa,Al),e(Qs,Ol),e(Qs,Xe),e(Xe,Nl),e(Qs,Rl),e(qt,Il),e(qt,sn),e(sn,Gl),e(_s,Ul),e(_s,xt),e(xt,Kt),e(Kt,ja),e(ja,Om),e(xt,Nm),e(xt,Ho),e(Ho,Rm),e(xt,Im),e(xt,Hl),e(Hl,Gm),e(_s,Um),e(_s,tn),e(tn,Fl),e(Fl,jr),e(jr,Hm),e(tn,Fm),e(tn,$r),e($r,Jm),e($r,Fo),e(Fo,Bm),e($r,Vm),e(tn,Wm),e(tn,Jl),e(Jl,Km),e(_s,Qm),e(_s,an),e(an,Bl),e(Bl,xr),e(xr,Ym),e(an,Xm),e(an,Jo),e(Jo,Zm),e(an,ec),e(an,Vl),e(Vl,sc),i(s,Nu,u),i(s,$a,u),e($a,tc),e($a,Bo),e(Bo,ac),e($a,nc),e($a,Vo),e(Vo,rc),e($a,lc),i(s,Ru,u),i(s,Et,u),e(Et,oc),e(Et,Wo),e(Wo,ic),e(Et,uc),e(Et,Ko),e(Ko,pc),e(Et,mc),e(Et,Qo),e(Qo,cc),e(Et,dc),i(s,Iu,u),i(s,nn,u),e(nn,Er),e(nn,hc),e(nn,kr),i(s,Gu,u),i(s,Wl,u),e(Wl,fc),i(s,Uu,u),q(Nn,s,u),i(s,Hu,u),i(s,rn,u),e(rn,Rn),e(Rn,Yo),q(wr,Yo,null),e(rn,vc),e(rn,Xo),e(Xo,_c),i(s,Fu,u),q(yr,s,u),i(s,Ju,u),i(s,zs,u),e(zs,gc),e(zs,Zo),e(Zo,bc),e(zs,qc),e(zs,ei),e(ei,jc),e(zs,$c),e(zs,si),e(si,xc),e(zs,Ec),e(zs,ti),e(ti,kc),e(zs,wc),e(zs,ai),e(ai,yc),e(zs,zc),i(s,Bu,u),q(zr,s,u),i(s,Vu,u),q(In,s,u),i(s,Wu,u),i(s,Gn,u),e(Gn,Tc),e(Gn,ni),e(ni,Cc),e(Gn,Pc),i(s,Ku,u),q(Tr,s,u),i(s,Qu,u),q(Cr,s,u),i(s,Yu,u),i(s,Ze,u),e(Ze,Dc),e(Ze,ri),e(ri,Sc),e(Ze,Lc),e(Ze,li),e(li,Mc),e(Ze,Ac),e(Ze,oi),e(oi,Oc),e(Ze,Nc),e(Ze,Kl),e(Kl,Rc),e(Ze,Ic),e(Ze,ii),e(ii,Gc),e(Ze,Uc),e(Ze,ui),e(ui,Hc),e(Ze,Fc),e(Ze,pi),e(pi,Jc),e(Ze,Bc),i(s,Xu,u),q(Pr,s,u),i(s,Zu,u),q(Dr,s,u),i(s,ep,u),i(s,Ts,u),e(Ts,Vc),e(Ts,mi),e(mi,Wc),e(Ts,Kc),e(Ts,ci),e(ci,Qc),e(Ts,Yc),e(Ts,di),e(di,Xc),e(Ts,Zc),e(Ts,hi),e(hi,ed),e(Ts,sd),e(Ts,Ql),e(Ql,td),e(Ts,ad),i(s,sp,u),i(s,xa,u),e(xa,nd),e(xa,fi),e(fi,rd),e(xa,ld),e(xa,vi),e(vi,od),e(xa,id),i(s,tp,u),q(Sr,s,u),i(s,ap,u),i(s,Ys,u),e(Ys,ud),e(Ys,_i),e(_i,pd),e(Ys,md),e(Ys,gi),e(gi,cd),e(Ys,dd),e(Ys,bi),e(bi,hd),e(Ys,fd),e(Ys,qi),e(qi,vd),e(Ys,_d),i(s,np,u),i(s,Ea,u),e(Ea,gd),e(Ea,ji),e(ji,bd),e(Ea,qd),e(Ea,$i),e($i,jd),e(Ea,$d),i(s,rp,u),q(Lr,s,u),i(s,lp,u),i(s,Yl,u),e(Yl,xd),i(s,op,u),q(Un,s,u),i(s,ip,u),i(s,ln,u),e(ln,Hn),e(Hn,xi),q(Mr,xi,null),e(ln,Ed),e(ln,Ei),e(Ei,kd),i(s,up,u),q(Ar,s,u),i(s,pp,u),i(s,Xl,u),e(Xl,wd),i(s,mp,u),i(s,ka,u),e(ka,yd),e(ka,Or),e(Or,zd),e(ka,Td),e(ka,ki),e(ki,Cd),e(ka,Pd),i(s,cp,u),q(Nr,s,u),i(s,dp,u),i(s,wa,u),e(wa,Dd),e(wa,wi),e(wi,Sd),e(wa,Ld),e(wa,yi),e(yi,Md),e(wa,Ad),i(s,hp,u),q(Fn,s,u),i(s,fp,u),i(s,Rr,u),e(Rr,Od),vp.m(n1,Rr),i(s,_p,u),i(s,Ir,u),e(Ir,Nd),gp.m(r1,Ir),i(s,bp,u),i(s,kt,u),e(kt,Rd),e(kt,zi),e(zi,Id),e(kt,Gd),e(kt,Ti),e(Ti,Ud),e(kt,Hd),e(kt,Ci),e(Ci,Fd),e(kt,Jd),i(s,qp,u),q(Gr,s,u),i(s,jp,u),i(s,Zl,u),e(Zl,Bd),i(s,$p,u),q(Ur,s,u),i(s,xp,u),i(s,Jn,u),e(Jn,Vd),e(Jn,Pi),e(Pi,Wd),e(Jn,Kd),i(s,Ep,u),q(Hr,s,u),i(s,kp,u),q(Fr,s,u),i(s,wp,u),i(s,es,u),e(es,Qd),e(es,Di),e(Di,Yd),e(es,Xd),e(es,Si),e(Si,Zd),e(es,eh),e(es,Li),e(Li,sh),e(es,th),e(es,Mi),e(Mi,ah),e(es,nh),e(es,Ai),e(Ai,rh),e(es,lh),e(es,Oi),e(Oi,oh),e(es,ih),e(es,Ni),e(Ni,uh),e(es,ph),i(s,yp,u),q(Jr,s,u),i(s,zp,u),q(Br,s,u),i(s,Tp,u),i(s,Cs,u),e(Cs,mh),e(Cs,Ri),e(Ri,ch),e(Cs,dh),e(Cs,Ii),e(Ii,hh),e(Cs,fh),e(Cs,Gi),e(Gi,vh),e(Cs,_h),e(Cs,Ui),e(Ui,gh),e(Cs,bh),e(Cs,Hi),e(Hi,qh),e(Cs,jh),i(s,Cp,u),q(Bn,s,u),i(s,Pp,u),i(s,Vn,u),e(Vn,$h),e(Vn,Fi),e(Fi,xh),e(Vn,Eh),i(s,Dp,u),i(s,on,u),e(on,Wn),e(Wn,Ji),q(Vr,Ji,null),e(on,kh),e(on,Bi),e(Bi,wh),i(s,Sp,u),i(s,gs,u),e(gs,yh),e(gs,Vi),e(Vi,zh),e(gs,Th),e(gs,Wi),e(Wi,Ch),e(gs,Ph),e(gs,Ki),e(Ki,Dh),e(gs,Sh),e(gs,Qi),e(Qi,Lh),e(gs,Mh),e(gs,Yi),e(Yi,Ah),e(gs,Oh),e(gs,Xi),e(Xi,Nh),e(gs,Rh),i(s,Lp,u),q(Wr,s,u),i(s,Mp,u),i(s,eo,u),e(eo,Ih),i(s,Ap,u),q(Kr,s,u),i(s,Op,u),i(s,ya,u),e(ya,Gh),e(ya,Zi),e(Zi,Uh),e(ya,Hh),e(ya,eu),e(eu,Fh),e(ya,Jh),i(s,Np,u),q(Qr,s,u),i(s,Rp,u),q(Yr,s,u),i(s,Ip,u),i(s,so,u),e(so,Bh),i(s,Gp,u),q(Xr,s,u),i(s,Up,u),i(s,to,u),e(to,Vh),i(s,Hp,u),q(Zr,s,u),i(s,Fp,u),q(el,s,u),i(s,Jp,u),i(s,Ps,u),e(Ps,Wh),e(Ps,su),e(su,Kh),e(Ps,Qh),e(Ps,tu),e(tu,Yh),e(Ps,Xh),e(Ps,au),e(au,Zh),e(Ps,ef),e(Ps,nu),e(nu,sf),e(Ps,tf),e(Ps,ru),e(ru,af),e(Ps,nf),i(s,Bp,u),hl[Qt].m(s,u),i(s,ao,u),q(Kn,s,u),i(s,Vp,u),i(s,za,u),e(za,rf),e(za,lu),e(lu,lf),e(za,of),e(za,ou),e(ou,uf),e(za,pf),i(s,Wp,u),q(sl,s,u),i(s,Kp,u),i(s,Qn,u),e(Qn,mf),e(Qn,iu),e(iu,cf),e(Qn,df),i(s,Qp,u),q(tl,s,u),i(s,Yp,u),us&&us.m(s,u),i(s,no,u),i(s,Ta,u),e(Ta,hf),e(Ta,uu),e(uu,ff),e(Ta,vf),e(Ta,ro),e(ro,_f),e(Ta,gf),i(s,Xp,u),i(s,Xs,u),e(Xs,bf),e(Xs,pu),e(pu,qf),e(Xs,jf),e(Xs,mu),e(mu,$f),e(Xs,xf),e(Xs,cu),e(cu,Ef),e(Xs,kf),e(Xs,du),e(du,wf),e(Xs,yf),i(s,Zp,u),fl[Xt].m(s,u),i(s,lo,u),i(s,oo,u),e(oo,zf),i(s,em,u),q(al,s,u),i(s,sm,u),i(s,Ca,u),e(Ca,Tf),e(Ca,hu),e(hu,Cf),e(Ca,Pf),e(Ca,fu),e(fu,Df),e(Ca,Sf),i(s,tm,u),q(nl,s,u),i(s,am,u),q(rl,s,u),i(s,nm,u),i(s,we,u),e(we,Lf),e(we,vu),e(vu,Mf),e(we,Af),e(we,_u),e(_u,Of),e(we,Nf),e(we,gu),e(gu,Rf),e(we,If),e(we,bu),e(bu,Gf),e(we,Uf),e(we,qu),e(qu,Hf),e(we,Ff),e(we,ju),e(ju,Jf),e(we,Bf),e(we,$u),e($u,Vf),e(we,Wf),e(we,xu),e(xu,Kf),e(we,Qf),e(we,Eu),e(Eu,Yf),e(we,Xf),e(we,ku),e(ku,Zf),e(we,ev),e(we,wu),e(wu,sv),e(we,tv),i(s,rm,u),vl[ea].m(s,u),i(s,io,u),ps&&ps.m(s,u),i(s,uo,u),i(s,un,u),e(un,Yn),e(Yn,yu),q(ll,yu,null),e(un,av),e(un,po),e(po,nv),e(po,zu),e(zu,rv),i(s,lm,u),i(s,wt,u),e(wt,lv),e(wt,Tu),e(Tu,ov),e(wt,iv),e(wt,Cu),e(Cu,uv),e(wt,pv),e(wt,Pu),e(Pu,mv),e(wt,cv),i(s,om,u),q(ol,s,u),i(s,im,u),i(s,mo,u),e(mo,dv),i(s,um,u),q(il,s,u),i(s,pm,u),i(s,co,u),e(co,hv),i(s,mm,u),q(ul,s,u),i(s,cm,u),q(pl,s,u),i(s,dm,u),i(s,Xn,u),e(Xn,fv),e(Xn,Du),e(Du,vv),e(Xn,_v),i(s,hm,u),q(ml,s,u),i(s,fm,u),q(cl,s,u),i(s,vm,u),i(s,ho,u),e(ho,gv),i(s,_m,u),i(s,fo,u),e(fo,bv),gm=!0},p(s,[u]){const _l={};u&1&&(_l.fw=s[0]),h.$set(_l);let vo=w;w=yv(s),w!==vo&&(Uo(),_(dl[vo],1,1,()=>{dl[vo]=null}),Go(),I=dl[w],I||(I=dl[w]=wv[w](s),I.c()),v(I,1),I.m(D.parentNode,D));const Su={};u&2&&(Su.$$scope={dirty:u,ctx:s}),Qe.$set(Su);const Lu={};u&2&&(Lu.$$scope={dirty:u,ctx:s}),Nn.$set(Lu);const pn={};u&2&&(pn.$$scope={dirty:u,ctx:s}),In.$set(pn);const Mu={};u&2&&(Mu.$$scope={dirty:u,ctx:s}),Un.$set(Mu);const Au={};u&2&&(Au.$$scope={dirty:u,ctx:s}),Fn.$set(Au);const ta={};u&2&&(ta.$$scope={dirty:u,ctx:s}),Bn.$set(ta);let _o=Qt;Qt=Tv(s),Qt!==_o&&(Uo(),_(hl[_o],1,1,()=>{hl[_o]=null}),Go(),Yt=hl[Qt],Yt||(Yt=hl[Qt]=zv[Qt](s),Yt.c()),v(Yt,1),Yt.m(ao.parentNode,ao));const Ou={};u&2&&(Ou.$$scope={dirty:u,ctx:s}),Kn.$set(Ou),s[0]==="pt"?us?u&1&&v(us,1):(us=s1(),us.c(),v(us,1),us.m(no.parentNode,no)):us&&(Uo(),_(us,1,1,()=>{us=null}),Go());let go=Xt;Xt=Pv(s),Xt!==go&&(Uo(),_(fl[go],1,1,()=>{fl[go]=null}),Go(),Zt=fl[Xt],Zt||(Zt=fl[Xt]=Cv[Xt](s),Zt.c()),v(Zt,1),Zt.m(lo.parentNode,lo));let bo=ea;ea=Sv(s),ea!==bo&&(Uo(),_(vl[bo],1,1,()=>{vl[bo]=null}),Go(),sa=vl[ea],sa||(sa=vl[ea]=Dv[ea](s),sa.c()),v(sa,1),sa.m(io.parentNode,io)),s[0]==="pt"?ps?u&1&&v(ps,1):(ps=t1(s),ps.c(),v(ps,1),ps.m(uo.parentNode,uo)):ps&&(Uo(),_(ps,1,1,()=>{ps=null}),Go())},i(s){gm||(v(h.$$.fragment,s),v(M.$$.fragment,s),v(I),v(ie.$$.fragment,s),v(je.$$.fragment,s),v(de.$$.fragment,s),v(ve.$$.fragment,s),v(os.$$.fragment,s),v(Ke.$$.fragment,s),v(Qe.$$.fragment,s),v(Je.$$.fragment,s),v(Be.$$.fragment,s),v(Us.$$.fragment,s),v(Rt.$$.fragment,s),v(dt.$$.fragment,s),v(ys.$$.fragment,s),v(Ut.$$.fragment,s),v(Ht.$$.fragment,s),v(rt.$$.fragment,s),v(Bt.$$.fragment,s),v(Nn.$$.fragment,s),v(wr.$$.fragment,s),v(yr.$$.fragment,s),v(zr.$$.fragment,s),v(In.$$.fragment,s),v(Tr.$$.fragment,s),v(Cr.$$.fragment,s),v(Pr.$$.fragment,s),v(Dr.$$.fragment,s),v(Sr.$$.fragment,s),v(Lr.$$.fragment,s),v(Un.$$.fragment,s),v(Mr.$$.fragment,s),v(Ar.$$.fragment,s),v(Nr.$$.fragment,s),v(Fn.$$.fragment,s),v(Gr.$$.fragment,s),v(Ur.$$.fragment,s),v(Hr.$$.fragment,s),v(Fr.$$.fragment,s),v(Jr.$$.fragment,s),v(Br.$$.fragment,s),v(Bn.$$.fragment,s),v(Vr.$$.fragment,s),v(Wr.$$.fragment,s),v(Kr.$$.fragment,s),v(Qr.$$.fragment,s),v(Yr.$$.fragment,s),v(Xr.$$.fragment,s),v(Zr.$$.fragment,s),v(el.$$.fragment,s),v(Yt),v(Kn.$$.fragment,s),v(sl.$$.fragment,s),v(tl.$$.fragment,s),v(us),v(Zt),v(al.$$.fragment,s),v(nl.$$.fragment,s),v(rl.$$.fragment,s),v(sa),v(ps),v(ll.$$.fragment,s),v(ol.$$.fragment,s),v(il.$$.fragment,s),v(ul.$$.fragment,s),v(pl.$$.fragment,s),v(ml.$$.fragment,s),v(cl.$$.fragment,s),gm=!0)},o(s){_(h.$$.fragment,s),_(M.$$.fragment,s),_(I),_(ie.$$.fragment,s),_(je.$$.fragment,s),_(de.$$.fragment,s),_(ve.$$.fragment,s),_(os.$$.fragment,s),_(Ke.$$.fragment,s),_(Qe.$$.fragment,s),_(Je.$$.fragment,s),_(Be.$$.fragment,s),_(Us.$$.fragment,s),_(Rt.$$.fragment,s),_(dt.$$.fragment,s),_(ys.$$.fragment,s),_(Ut.$$.fragment,s),_(Ht.$$.fragment,s),_(rt.$$.fragment,s),_(Bt.$$.fragment,s),_(Nn.$$.fragment,s),_(wr.$$.fragment,s),_(yr.$$.fragment,s),_(zr.$$.fragment,s),_(In.$$.fragment,s),_(Tr.$$.fragment,s),_(Cr.$$.fragment,s),_(Pr.$$.fragment,s),_(Dr.$$.fragment,s),_(Sr.$$.fragment,s),_(Lr.$$.fragment,s),_(Un.$$.fragment,s),_(Mr.$$.fragment,s),_(Ar.$$.fragment,s),_(Nr.$$.fragment,s),_(Fn.$$.fragment,s),_(Gr.$$.fragment,s),_(Ur.$$.fragment,s),_(Hr.$$.fragment,s),_(Fr.$$.fragment,s),_(Jr.$$.fragment,s),_(Br.$$.fragment,s),_(Bn.$$.fragment,s),_(Vr.$$.fragment,s),_(Wr.$$.fragment,s),_(Kr.$$.fragment,s),_(Qr.$$.fragment,s),_(Yr.$$.fragment,s),_(Xr.$$.fragment,s),_(Zr.$$.fragment,s),_(el.$$.fragment,s),_(Yt),_(Kn.$$.fragment,s),_(sl.$$.fragment,s),_(tl.$$.fragment,s),_(us),_(Zt),_(al.$$.fragment,s),_(nl.$$.fragment,s),_(rl.$$.fragment,s),_(sa),_(ps),_(ll.$$.fragment,s),_(ol.$$.fragment,s),_(il.$$.fragment,s),_(ul.$$.fragment,s),_(pl.$$.fragment,s),_(ml.$$.fragment,s),_(cl.$$.fragment,s),gm=!1},d(s){t(d),s&&t(E),j(h,s),s&&t(y),s&&t(L),j(M),s&&t(O),dl[w].d(s),s&&t(D),s&&t(N),s&&t(K),j(ie,s),s&&t(ne),s&&t(Q),s&&t(ce),s&&t(H),s&&t(X),s&&t(be),s&&t(Ne),s&&t(x),s&&t(Pe),s&&t(De),j(je),s&&t(ye),s&&t($e),s&&t(bs),j(de,s),s&&t(ds),j(ve,s),s&&t(qs),s&&t(_e),s&&t(Es),j(os,s),s&&t(xe),j(Ke,s),s&&t(As),j(Qe,s),s&&t(ue),s&&t(Ue),s&&t(ks),j(Je,s),s&&t(Rs),j(Be,s),s&&t(ut),s&&t(Me),s&&t(fs),j(Us,s),s&&t(tt),s&&t(Re),s&&t(pa),j(Rt,s),s&&t(hn),s&&t(ct),s&&t(fn),j(dt,s),s&&t(Fs),j(ys,s),s&&t(ma),s&&t(He),s&&t(ca),j(Ut,s),s&&t(gn),j(Ht,s),s&&t(ht),s&&t(da),s&&t(bn),s&&t(nt),s&&t(jn),s&&t(Bs),s&&t($n),j(rt,s),s&&t(xn),s&&t(Vs),s&&t(En),s&&t(lt),j(Bt),s&&t(vt),s&&t(ws),s&&t(Wa),s&&t(Vt),s&&t(Nu),s&&t($a),s&&t(Ru),s&&t(Et),s&&t(Iu),s&&t(nn),s&&t(Gu),s&&t(Wl),s&&t(Uu),j(Nn,s),s&&t(Hu),s&&t(rn),j(wr),s&&t(Fu),j(yr,s),s&&t(Ju),s&&t(zs),s&&t(Bu),j(zr,s),s&&t(Vu),j(In,s),s&&t(Wu),s&&t(Gn),s&&t(Ku),j(Tr,s),s&&t(Qu),j(Cr,s),s&&t(Yu),s&&t(Ze),s&&t(Xu),j(Pr,s),s&&t(Zu),j(Dr,s),s&&t(ep),s&&t(Ts),s&&t(sp),s&&t(xa),s&&t(tp),j(Sr,s),s&&t(ap),s&&t(Ys),s&&t(np),s&&t(Ea),s&&t(rp),j(Lr,s),s&&t(lp),s&&t(Yl),s&&t(op),j(Un,s),s&&t(ip),s&&t(ln),j(Mr),s&&t(up),j(Ar,s),s&&t(pp),s&&t(Xl),s&&t(mp),s&&t(ka),s&&t(cp),j(Nr,s),s&&t(dp),s&&t(wa),s&&t(hp),j(Fn,s),s&&t(fp),s&&t(Rr),s&&t(_p),s&&t(Ir),s&&t(bp),s&&t(kt),s&&t(qp),j(Gr,s),s&&t(jp),s&&t(Zl),s&&t($p),j(Ur,s),s&&t(xp),s&&t(Jn),s&&t(Ep),j(Hr,s),s&&t(kp),j(Fr,s),s&&t(wp),s&&t(es),s&&t(yp),j(Jr,s),s&&t(zp),j(Br,s),s&&t(Tp),s&&t(Cs),s&&t(Cp),j(Bn,s),s&&t(Pp),s&&t(Vn),s&&t(Dp),s&&t(on),j(Vr),s&&t(Sp),s&&t(gs),s&&t(Lp),j(Wr,s),s&&t(Mp),s&&t(eo),s&&t(Ap),j(Kr,s),s&&t(Op),s&&t(ya),s&&t(Np),j(Qr,s),s&&t(Rp),j(Yr,s),s&&t(Ip),s&&t(so),s&&t(Gp),j(Xr,s),s&&t(Up),s&&t(to),s&&t(Hp),j(Zr,s),s&&t(Fp),j(el,s),s&&t(Jp),s&&t(Ps),s&&t(Bp),hl[Qt].d(s),s&&t(ao),j(Kn,s),s&&t(Vp),s&&t(za),s&&t(Wp),j(sl,s),s&&t(Kp),s&&t(Qn),s&&t(Qp),j(tl,s),s&&t(Yp),us&&us.d(s),s&&t(no),s&&t(Ta),s&&t(Xp),s&&t(Xs),s&&t(Zp),fl[Xt].d(s),s&&t(lo),s&&t(oo),s&&t(em),j(al,s),s&&t(sm),s&&t(Ca),s&&t(tm),j(nl,s),s&&t(am),j(rl,s),s&&t(nm),s&&t(we),s&&t(rm),vl[ea].d(s),s&&t(io),ps&&ps.d(s),s&&t(uo),s&&t(un),j(ll),s&&t(lm),s&&t(wt),s&&t(om),j(ol,s),s&&t(im),s&&t(mo),s&&t(um),j(il,s),s&&t(pm),s&&t(co),s&&t(mm),j(ul,s),s&&t(cm),j(pl,s),s&&t(dm),s&&t(Xn),s&&t(hm),j(ml,s),s&&t(fm),j(cl,s),s&&t(vm),s&&t(ho),s&&t(_m),s&&t(fo)}}}const T1={local:"rsum-de-textes",sections:[{local:"prparation-dun-corpus-multilingue",title:"Pr\xE9paration d'un corpus multilingue"},{local:"modles-pour-le-rsum-de-texte",title:"Mod\xE8les pour le r\xE9sum\xE9 de texte"},{local:"prtraitement-des-donnes",title:"Pr\xE9traitement des donn\xE9es"},{local:"mtriques-pour-le-rsum-de-texte",sections:[{local:"cration-dune-base-de-rfrence-solide",title:"Cr\xE9ation d'une base de r\xE9f\xE9rence solide"}],title:"M\xE9triques pour le r\xE9sum\xE9 de texte"},{local:"ifinetuningi-de-mt5-avec-lapi-trainer",title:"<i>Finetuning</i> de mT5 avec l'API `Trainer`"},{local:"ifinetuningi-de-mt5-avec-keras",title:"<i>Finetuning</i> de mT5 avec Keras"},{local:"ifinetuningi-de-mt5-avec-iacceleratei",sections:[{local:"prparer-tout-pour-lentranement",title:"Pr\xE9parer tout pour l'entra\xEEnement"},{local:"boucle-dentranement",title:"Boucle d'entra\xEEnement"}],title:"<i>Finetuning</i> de mT5 avec \u{1F917} <i>Accelerate</i>"},{local:"utilisation-de-votre-modle-ifinetuni",title:"Utilisation de votre mod\xE8le <i>finetun\xE9</i>"}],title:"R\xE9sum\xE9 de textes"};function C1(te,d,E){let h="pt";return p1(()=>{const y=new URLSearchParams(window.location.search);E(0,h=y.get("fw")||"pt")}),[h]}class N1 extends l1{constructor(d){super();o1(this,d,C1,z1,i1,{})}}export{N1 as default,T1 as metadata};
