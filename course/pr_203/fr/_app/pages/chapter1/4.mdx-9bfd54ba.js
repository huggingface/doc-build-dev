import{S as Np,i as Gp,s as Rp,e as s,k as d,w as g,t as n,M as jp,c as a,d as r,m,a as o,x as E,h as l,b as c,N as h,F as t,g as u,y as _,L as Sp,q,o as b,B as x,v as zp}from"../../chunks/vendor-1e8b365d.js";import{Y as Hc}from"../../chunks/Youtube-c2a8cc39.js";import{I as B}from"../../chunks/IconCopyLink-483c28ba.js";function Bp(Vc){let D,ca,O,fe,Er,Oe,sl,Ue,al,_r,nl,ll,da,he,ol,qr,il,ul,ma,U,ve,br,He,cl,At,dl,xr,ml,pa,ge,pl,Pr,fl,hl,fa,H,Ve,Yc,vl,Ye,Fc,ha,Fe,Ee,gl,$r,El,_l,va,v,kr,P,Mr,ql,bl,Je,xl,Pl,wr,$l,kl,yr,Ml,wl,yl,Lr,_e,Tr,Ll,Tl,Xe,Il,Al,Cl,Ir,qe,Ar,Nl,Gl,Qe,Rl,jl,Sl,Cr,be,Nr,zl,Bl,We,Dl,Ol,Ul,Gr,$,Rr,Hl,Vl,Ze,Yl,Fl,Ke,Jl,Xl,jr,Ql,Wl,Zl,Sr,k,zr,Kl,eo,et,to,ro,Br,so,ao,Dr,no,lo,ga,xe,oo,Or,io,uo,Ea,T,V,co,Ur,mo,po,Hr,fo,ho,vo,Y,go,Vr,Eo,_o,Yr,qo,bo,xo,F,Po,Fr,$o,ko,Jr,Mo,wo,_a,Ct,yo,qa,J,Pe,Xr,tt,Lo,rt,To,Qr,Io,Ao,ba,I,Co,Wr,No,Go,Zr,Ro,jo,xa,$e,So,Kr,zo,Bo,Pa,A,Do,es,Oo,Uo,ts,Ho,Vo,$a,X,st,Jc,Yo,at,Xc,ka,ke,Fo,rs,Jo,Xo,Ma,Q,nt,Qc,Qo,lt,Wc,wa,W,Me,ss,ot,Wo,it,Zo,as,Ko,ei,ya,Nt,ti,La,ut,ct,Zc,Ta,Gt,ri,Ia,Z,dt,Kc,si,mt,ed,Aa,pt,Ca,Rt,ai,Na,jt,ni,Ga,St,li,Ra,K,we,ns,ft,oi,ls,ii,ja,ht,Sa,zt,ui,za,ee,vt,td,ci,gt,rd,Ba,Bt,di,Da,C,mi,os,pi,fi,is,hi,vi,Oa,N,te,gi,us,Ei,_i,cs,qi,bi,xi,Et,Pi,ds,$i,ki,Mi,ms,wi,Ua,G,yi,ps,Li,Ti,fs,Ii,Ai,Ha,re,_t,sd,Ci,qt,ad,Va,R,Ni,hs,Gi,Ri,vs,ji,Si,Ya,ye,zi,gs,Bi,Di,Fa,se,Le,Es,bt,Oi,_s,Ui,Ja,Te,Hi,qs,Vi,Yi,Xa,xt,Qa,ae,Ie,bs,Pt,Fi,xs,Ji,Wa,Dt,Xi,Za,Ae,Ot,Ps,Qi,Wi,Zi,Ut,$s,Ki,eu,Ka,ne,$t,nd,tu,kt,ld,en,Ht,ru,tn,j,Vt,ks,su,au,nu,Yt,Ms,lu,ou,iu,Ce,ws,uu,cu,ys,du,mu,rn,Ft,pu,sn,le,Ne,Ls,Mt,fu,Ts,hu,an,M,vu,Is,gu,Eu,As,_u,qu,wt,Cs,bu,xu,nn,p,Pu,Ns,$u,ku,Gs,Mu,wu,Rs,yu,Lu,js,Tu,Iu,Ss,Au,Cu,zs,Nu,Gu,Bs,Ru,ju,Ds,Su,zu,ln,Jt,Bu,on,Ge,Du,Os,Ou,Uu,un,oe,Re,Us,yt,Hu,Hs,Vu,cn,je,Yu,Vs,Fu,Ju,dn,Xt,Xu,mn,Se,Qu,Ys,Wu,Zu,pn,ie,Lt,od,Ku,Tt,id,fn,Qt,ec,hn,S,tc,Fs,rc,sc,Js,ac,nc,vn,ue,ze,Xs,It,lc,Wt,oc,Qs,ic,gn,Zt,uc,En,Kt,cc,_n,er,dc,qn,tr,mc,bn,z,rr,Ws,pc,fc,hc,sr,Zs,vc,gc,Ec,w,Ks,_c,qc,ea,bc,xc,ta,Pc,$c,ra,kc,Mc,xn,y,wc,sa,yc,Lc,aa,Tc,Ic,na,Ac,Cc,Pn;return Oe=new B({}),He=new B({}),tt=new B({}),ot=new B({}),pt=new Hc({props:{id:"ftWlj4FBHTg"}}),ft=new B({}),ht=new Hc({props:{id:"BqqfQnyjmgg"}}),bt=new B({}),xt=new Hc({props:{id:"H39Z_720T5s"}}),Pt=new B({}),Mt=new B({}),yt=new B({}),It=new B({}),{c(){D=s("meta"),ca=d(),O=s("h1"),fe=s("a"),Er=s("span"),g(Oe.$$.fragment),sl=d(),Ue=s("span"),al=n("Comment fonctionnent les "),_r=s("i"),nl=n("transformers"),ll=n(" ?"),da=d(),he=s("p"),ol=n("Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),qr=s("em"),il=n("transformers"),ul=n("."),ma=d(),U=s("h2"),ve=s("a"),br=s("span"),g(He.$$.fragment),cl=d(),At=s("span"),dl=n("Court historique des "),xr=s("i"),ml=n("transformers"),pa=d(),ge=s("p"),pl=n("Voici quelques dates clefs dans la courte histoire des "),Pr=s("em"),fl=n("transformers"),hl=n(" :"),fa=d(),H=s("div"),Ve=s("img"),vl=d(),Ye=s("img"),ha=d(),Fe=s("p"),Ee=s("a"),gl=n("L\u2019architecture "),$r=s("em"),El=n("Transformer"),_l=n(" a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),va=d(),v=s("ul"),kr=s("li"),P=s("p"),Mr=s("strong"),ql=n("Juin 2018"),bl=n(" : "),Je=s("a"),xl=n("GPT"),Pl=n(", le premier "),wr=s("em"),$l=n("transformer"),kl=n(" pr\xE9-entra\xEEn\xE9 et "),yr=s("em"),Ml=n("finetun\xE9"),wl=n(" sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),yl=d(),Lr=s("li"),_e=s("p"),Tr=s("strong"),Ll=n("Octobre 2018"),Tl=n(" : "),Xe=s("a"),Il=n("BERT"),Al=n(", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),Cl=d(),Ir=s("li"),qe=s("p"),Ar=s("strong"),Nl=n("F\xE9vrier 2019"),Gl=n(" : "),Qe=s("a"),Rl=n("GPT-2"),jl=n(", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),Sl=d(),Cr=s("li"),be=s("p"),Nr=s("strong"),zl=n("Octobre 2019"),Bl=n(" : "),We=s("a"),Dl=n("DistilBERT"),Ol=n(", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),Ul=d(),Gr=s("li"),$=s("p"),Rr=s("strong"),Hl=n("Octobre 2019"),Vl=n(" : "),Ze=s("a"),Yl=n("BART"),Fl=n(" et "),Ke=s("a"),Jl=n("T5"),Xl=n(", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),jr=s("em"),Ql=n("transformer"),Wl=n(" original (les premiers \xE0 faire cela),"),Zl=d(),Sr=s("li"),k=s("p"),zr=s("strong"),Kl=n("Mai 2020"),eo=n(" : "),et=s("a"),to=n("GPT-3"),ro=n(", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Br=s("em"),so=n("finetuning"),ao=n(" (appel\xE9 "),Dr=s("em"),no=n("zero-shot learning"),lo=n(")."),ga=d(),xe=s("p"),oo=n("Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Or=s("em"),io=n("transformers"),uo=n(". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),Ea=d(),T=s("ul"),V=s("li"),co=n("ceux de type GPT (aussi appel\xE9s "),Ur=s("em"),mo=n("transformers"),po=d(),Hr=s("em"),fo=n("autor\xE9gressifs"),ho=n(")"),vo=d(),Y=s("li"),go=n("ceux de type BERT (aussi appel\xE9s "),Vr=s("em"),Eo=n("transformers"),_o=d(),Yr=s("em"),qo=n("auto-encodeurs"),bo=n(")"),xo=d(),F=s("li"),Po=n("ceux de type BART/T5 (aussi appel\xE9s "),Fr=s("em"),$o=n("transformers"),ko=d(),Jr=s("em"),Mo=n("s\xE9quence-\xE0-s\xE9quence"),wo=n(")"),_a=d(),Ct=s("p"),yo=n("Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),qa=d(),J=s("h2"),Pe=s("a"),Xr=s("span"),g(tt.$$.fragment),Lo=d(),rt=s("span"),To=n("Les "),Qr=s("i"),Io=n("transformers"),Ao=n(" sont des mod\xE8les de langage"),ba=d(),I=s("p"),Co=n("Tous les "),Wr=s("em"),No=n("transformers"),Go=n(" mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Zr=s("em"),Ro=n("mod\xE8les de langage"),jo=n(". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),xa=d(),$e=s("p"),So=n("Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),Kr=s("em"),zo=n("finetun\xE9"),Bo=n(" de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),Pa=d(),A=s("p"),Do=n("Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),es=s("em"),Oo=n("n"),Uo=n(" mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),ts=s("em"),Ho=n("mod\xE9lisation causale du langage"),Vo=n(" car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),$a=d(),X=s("div"),st=s("img"),Yo=d(),at=s("img"),ka=d(),ke=s("p"),Fo=n("Un autre exemple est la "),rs=s("em"),Jo=n("mod\xE9lisation du langage masqu\xE9"),Xo=n(", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),Ma=d(),Q=s("div"),nt=s("img"),Qo=d(),lt=s("img"),wa=d(),W=s("h2"),Me=s("a"),ss=s("span"),g(ot.$$.fragment),Wo=d(),it=s("span"),Zo=n("Les "),as=s("i"),Ko=n("transformers"),ei=n(" sont \xE9normes"),ya=d(),Nt=s("p"),ti=n("En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),La=d(),ut=s("div"),ct=s("img"),Ta=d(),Gt=s("p"),ri=n("Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),Ia=d(),Z=s("div"),dt=s("img"),si=d(),mt=s("img"),Aa=d(),g(pt.$$.fragment),Ca=d(),Rt=s("p"),ai=n("L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),Na=d(),jt=s("p"),ni=n("Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),Ga=d(),St=s("p"),li=n("C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),Ra=d(),K=s("h2"),we=s("a"),ns=s("span"),g(ft.$$.fragment),oi=d(),ls=s("span"),ii=n("L'apprentissage par transfert"),ja=d(),g(ht.$$.fragment),Sa=d(),zt=s("p"),ui=n("Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),za=d(),ee=s("div"),vt=s("img"),ci=d(),gt=s("img"),Ba=d(),Bt=s("p"),di=n("Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),Da=d(),C=s("p"),mi=n("Le "),os=s("em"),pi=n("finetuning"),fi=n(", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),is=s("em"),hi=n("finetuning"),vi=n(", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),Oa=d(),N=s("ul"),te=s("li"),gi=n("Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),us=s("em"),Ei=n("finetuning"),_i=n(". Le processus de "),cs=s("em"),qi=n("finetuning"),bi=n(" est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),xi=d(),Et=s("li"),Pi=n("Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),ds=s("em"),$i=n("finetuning"),ki=n(" n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),Mi=d(),ms=s("li"),wi=n("Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),Ua=d(),G=s("p"),yi=n("Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),ps=s("em"),Li=n("finetuner"),Ti=n(" sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),fs=s("em"),Ii=n("finetuning"),Ai=n(" ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),Ha=d(),re=s("div"),_t=s("img"),Ci=d(),qt=s("img"),Va=d(),R=s("p"),Ni=n("Le "),hs=s("em"),Gi=n("finetuning"),Ri=n(" d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),vs=s("em"),ji=n("finetuning"),Si=n(" car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),Ya=d(),ye=s("p"),zi=n("Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),gs=s("em"),Bi=n("finetuner"),Di=n("."),Fa=d(),se=s("h2"),Le=s("a"),Es=s("span"),g(bt.$$.fragment),Oi=d(),_s=s("span"),Ui=n("Architecture g\xE9n\xE9rale"),Ja=d(),Te=s("p"),Hi=n("Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),qs=s("em"),Vi=n("transformers"),Yi=n(". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),Xa=d(),g(xt.$$.fragment),Qa=d(),ae=s("h2"),Ie=s("a"),bs=s("span"),g(Pt.$$.fragment),Fi=d(),xs=s("span"),Ji=n("Introduction"),Wa=d(),Dt=s("p"),Xi=n("Le mod\xE8le est principalement compos\xE9 de deux blocs :"),Za=d(),Ae=s("ul"),Ot=s("li"),Ps=s("strong"),Qi=n("Encodeur (\xE0 gauche)"),Wi=n(" : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),Zi=d(),Ut=s("li"),$s=s("strong"),Ki=n("D\xE9codeur (\xE0 droite)"),eu=n(" : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),Ka=d(),ne=s("div"),$t=s("img"),tu=d(),kt=s("img"),en=d(),Ht=s("p"),ru=n("Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),tn=d(),j=s("ul"),Vt=s("li"),ks=s("strong"),su=n("Mod\xE8les uniquement encodeurs"),au=n(" : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),nu=d(),Yt=s("li"),Ms=s("strong"),lu=n("Mod\xE8les uniquement d\xE9codeurs"),ou=n(" : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),iu=d(),Ce=s("li"),ws=s("strong"),uu=n("Mod\xE8les encodeurs-d\xE9codeurs"),cu=n(" (ou "),ys=s("strong"),du=n("mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),mu=n(") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),rn=d(),Ft=s("p"),pu=n("Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),sn=d(),le=s("h2"),Ne=s("a"),Ls=s("span"),g(Mt.$$.fragment),fu=d(),Ts=s("span"),hu=n("Les couches d'attention"),an=d(),M=s("p"),vu=n("Une caract\xE9ristique cl\xE9 des "),Is=s("em"),gu=n("transformers"),Eu=n(" est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),As=s("em"),_u=n("transformer"),qu=n(" se nomme "),wt=s("a"),Cs=s("em"),bu=n("Attention Is All You Need"),xu=n(" ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),nn=d(),p=s("p"),Pu=n("Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB "),Ns=s("em"),$u=n("You like this course"),ku=n(" \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB "),Gs=s("em"),Mu=n("You"),wu=n(" \xBB pour obtenir la traduction correcte du mot \xAB "),Rs=s("em"),yu=n("like"),Lu=n(" \xBB, car en fran\xE7ais le verbe \xAB "),js=s("em"),Tu=n("like"),Iu=n(" \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB "),Ss=s("em"),Au=n("this"),Cu=n(" \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB "),zs=s("em"),Nu=n("course"),Gu=n(" \xBB car \xAB "),Bs=s("em"),Ru=n("this"),ju=n(" \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB "),Ds=s("em"),Su=n("this"),zu=n(" \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),ln=d(),Jt=s("p"),Bu=n("Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),on=d(),Ge=s("p"),Du=n("Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),Os=s("em"),Ou=n("transformers"),Uu=n("."),un=d(),oe=s("h2"),Re=s("a"),Us=s("span"),g(yt.$$.fragment),Hu=d(),Hs=s("span"),Vu=n("L'architecture originale"),cn=d(),je=s("p"),Yu=n("L\u2019architecture du "),Vs=s("em"),Fu=n("transformer"),Ju=n(" a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),dn=d(),Xt=s("p"),Xu=n("Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),mn=d(),Se=s("p"),Qu=n("L\u2019architecture originale du "),Ys=s("em"),Wu=n("transformer"),Zu=n(" ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),pn=d(),ie=s("div"),Lt=s("img"),Ku=d(),Tt=s("img"),fn=d(),Qt=s("p"),ec=n("Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),hn=d(),S=s("p"),tc=n("Le "),Fs=s("em"),rc=n("masque d\u2019attention"),sc=n(" peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Js=s("em"),ac=n("padding"),nc=n(") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),vn=d(),ue=s("h2"),ze=s("a"),Xs=s("span"),g(It.$$.fragment),lc=d(),Wt=s("span"),oc=n("Architectures contre "),Qs=s("i"),ic=n("checkpoints"),gn=n(`

 
En approfondissant l'\xE9tude des `),Zt=s("i"),uc=n("transformers"),En=n(" dans ce cours, vous verrez des mentions d'"),Kt=s("i"),cc=n("architectures"),_n=n(" et de "),er=s("i"),dc=n("checkpoints"),qn=n(" ainsi que de "),tr=s("i"),mc=n("mod\xE8les"),bn=n(`. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),z=s("ul"),rr=s("li"),Ws=s("strong"),pc=n("Architecture"),fc=n(" : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),hc=d(),sr=s("li"),Zs=s("strong"),vc=n("Checkpoints"),gc=n(" : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),Ec=d(),w=s("li"),Ks=s("strong"),_c=n("Mod\xE8le"),qc=n(" : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),ea=s("em"),bc=n("checkpoint"),xc=n(" \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),ta=s("em"),Pc=n("architecture"),$c=n(" ou "),ra=s("em"),kc=n("checkpoint"),Mc=n(" lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),xn=d(),y=s("p"),wc=n("Par exemple, BERT est une architecture alors que "),sa=s("code"),yc=n("bert-base-cased"),Lc=n(" (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),aa=s("em"),Tc=n("checkpoint"),Ic=n(". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),na=s("code"),Ac=n("bert-base-cased"),Cc=n(" \xBB."),this.h()},l(e){const i=jp('[data-svelte="svelte-1phssyn"]',document.head);D=a(i,"META",{name:!0,content:!0}),i.forEach(r),ca=m(e),O=a(e,"H1",{class:!0});var $n=o(O);fe=a($n,"A",{id:!0,class:!0,href:!0});var ud=o(fe);Er=a(ud,"SPAN",{});var cd=o(Er);E(Oe.$$.fragment,cd),cd.forEach(r),ud.forEach(r),sl=m($n),Ue=a($n,"SPAN",{});var kn=o(Ue);al=l(kn,"Comment fonctionnent les "),_r=a(kn,"I",{});var dd=o(_r);nl=l(dd,"transformers"),dd.forEach(r),ll=l(kn," ?"),kn.forEach(r),$n.forEach(r),da=m(e),he=a(e,"P",{});var Mn=o(he);ol=l(Mn,"Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),qr=a(Mn,"EM",{});var md=o(qr);il=l(md,"transformers"),md.forEach(r),ul=l(Mn,"."),Mn.forEach(r),ma=m(e),U=a(e,"H2",{class:!0});var wn=o(U);ve=a(wn,"A",{id:!0,class:!0,href:!0});var pd=o(ve);br=a(pd,"SPAN",{});var fd=o(br);E(He.$$.fragment,fd),fd.forEach(r),pd.forEach(r),cl=m(wn),At=a(wn,"SPAN",{});var Nc=o(At);dl=l(Nc,"Court historique des "),xr=a(Nc,"I",{});var hd=o(xr);ml=l(hd,"transformers"),hd.forEach(r),Nc.forEach(r),wn.forEach(r),pa=m(e),ge=a(e,"P",{});var yn=o(ge);pl=l(yn,"Voici quelques dates clefs dans la courte histoire des "),Pr=a(yn,"EM",{});var vd=o(Pr);fl=l(vd,"transformers"),vd.forEach(r),hl=l(yn," :"),yn.forEach(r),fa=m(e),H=a(e,"DIV",{class:!0});var Ln=o(H);Ve=a(Ln,"IMG",{class:!0,src:!0,alt:!0}),vl=m(Ln),Ye=a(Ln,"IMG",{class:!0,src:!0,alt:!0}),Ln.forEach(r),ha=m(e),Fe=a(e,"P",{});var Gc=o(Fe);Ee=a(Gc,"A",{href:!0,rel:!0});var Rc=o(Ee);gl=l(Rc,"L\u2019architecture "),$r=a(Rc,"EM",{});var gd=o($r);El=l(gd,"Transformer"),gd.forEach(r),Rc.forEach(r),_l=l(Gc," a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),Gc.forEach(r),va=m(e),v=a(e,"UL",{});var L=o(v);kr=a(L,"LI",{});var Ed=o(kr);P=a(Ed,"P",{});var ce=o(P);Mr=a(ce,"STRONG",{});var _d=o(Mr);ql=l(_d,"Juin 2018"),_d.forEach(r),bl=l(ce," : "),Je=a(ce,"A",{href:!0,rel:!0});var qd=o(Je);xl=l(qd,"GPT"),qd.forEach(r),Pl=l(ce,", le premier "),wr=a(ce,"EM",{});var bd=o(wr);$l=l(bd,"transformer"),bd.forEach(r),kl=l(ce," pr\xE9-entra\xEEn\xE9 et "),yr=a(ce,"EM",{});var xd=o(yr);Ml=l(xd,"finetun\xE9"),xd.forEach(r),wl=l(ce," sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),ce.forEach(r),Ed.forEach(r),yl=m(L),Lr=a(L,"LI",{});var Pd=o(Lr);_e=a(Pd,"P",{});var la=o(_e);Tr=a(la,"STRONG",{});var $d=o(Tr);Ll=l($d,"Octobre 2018"),$d.forEach(r),Tl=l(la," : "),Xe=a(la,"A",{href:!0,rel:!0});var kd=o(Xe);Il=l(kd,"BERT"),kd.forEach(r),Al=l(la,", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),la.forEach(r),Pd.forEach(r),Cl=m(L),Ir=a(L,"LI",{});var Md=o(Ir);qe=a(Md,"P",{});var oa=o(qe);Ar=a(oa,"STRONG",{});var wd=o(Ar);Nl=l(wd,"F\xE9vrier 2019"),wd.forEach(r),Gl=l(oa," : "),Qe=a(oa,"A",{href:!0,rel:!0});var yd=o(Qe);Rl=l(yd,"GPT-2"),yd.forEach(r),jl=l(oa,", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),oa.forEach(r),Md.forEach(r),Sl=m(L),Cr=a(L,"LI",{});var Ld=o(Cr);be=a(Ld,"P",{});var ia=o(be);Nr=a(ia,"STRONG",{});var Td=o(Nr);zl=l(Td,"Octobre 2019"),Td.forEach(r),Bl=l(ia," : "),We=a(ia,"A",{href:!0,rel:!0});var Id=o(We);Dl=l(Id,"DistilBERT"),Id.forEach(r),Ol=l(ia,", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),ia.forEach(r),Ld.forEach(r),Ul=m(L),Gr=a(L,"LI",{});var Ad=o(Gr);$=a(Ad,"P",{});var de=o($);Rr=a(de,"STRONG",{});var Cd=o(Rr);Hl=l(Cd,"Octobre 2019"),Cd.forEach(r),Vl=l(de," : "),Ze=a(de,"A",{href:!0,rel:!0});var Nd=o(Ze);Yl=l(Nd,"BART"),Nd.forEach(r),Fl=l(de," et "),Ke=a(de,"A",{href:!0,rel:!0});var Gd=o(Ke);Jl=l(Gd,"T5"),Gd.forEach(r),Xl=l(de,", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),jr=a(de,"EM",{});var Rd=o(jr);Ql=l(Rd,"transformer"),Rd.forEach(r),Wl=l(de," original (les premiers \xE0 faire cela),"),de.forEach(r),Ad.forEach(r),Zl=m(L),Sr=a(L,"LI",{});var jd=o(Sr);k=a(jd,"P",{});var me=o(k);zr=a(me,"STRONG",{});var Sd=o(zr);Kl=l(Sd,"Mai 2020"),Sd.forEach(r),eo=l(me," : "),et=a(me,"A",{href:!0,rel:!0});var zd=o(et);to=l(zd,"GPT-3"),zd.forEach(r),ro=l(me,", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Br=a(me,"EM",{});var Bd=o(Br);so=l(Bd,"finetuning"),Bd.forEach(r),ao=l(me," (appel\xE9 "),Dr=a(me,"EM",{});var Dd=o(Dr);no=l(Dd,"zero-shot learning"),Dd.forEach(r),lo=l(me,")."),me.forEach(r),jd.forEach(r),L.forEach(r),ga=m(e),xe=a(e,"P",{});var Tn=o(xe);oo=l(Tn,"Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Or=a(Tn,"EM",{});var Od=o(Or);io=l(Od,"transformers"),Od.forEach(r),uo=l(Tn,". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),Tn.forEach(r),Ea=m(e),T=a(e,"UL",{});var ar=o(T);V=a(ar,"LI",{});var nr=o(V);co=l(nr,"ceux de type GPT (aussi appel\xE9s "),Ur=a(nr,"EM",{});var Ud=o(Ur);mo=l(Ud,"transformers"),Ud.forEach(r),po=m(nr),Hr=a(nr,"EM",{});var Hd=o(Hr);fo=l(Hd,"autor\xE9gressifs"),Hd.forEach(r),ho=l(nr,")"),nr.forEach(r),vo=m(ar),Y=a(ar,"LI",{});var lr=o(Y);go=l(lr,"ceux de type BERT (aussi appel\xE9s "),Vr=a(lr,"EM",{});var Vd=o(Vr);Eo=l(Vd,"transformers"),Vd.forEach(r),_o=m(lr),Yr=a(lr,"EM",{});var Yd=o(Yr);qo=l(Yd,"auto-encodeurs"),Yd.forEach(r),bo=l(lr,")"),lr.forEach(r),xo=m(ar),F=a(ar,"LI",{});var or=o(F);Po=l(or,"ceux de type BART/T5 (aussi appel\xE9s "),Fr=a(or,"EM",{});var Fd=o(Fr);$o=l(Fd,"transformers"),Fd.forEach(r),ko=m(or),Jr=a(or,"EM",{});var Jd=o(Jr);Mo=l(Jd,"s\xE9quence-\xE0-s\xE9quence"),Jd.forEach(r),wo=l(or,")"),or.forEach(r),ar.forEach(r),_a=m(e),Ct=a(e,"P",{});var Xd=o(Ct);yo=l(Xd,"Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),Xd.forEach(r),qa=m(e),J=a(e,"H2",{class:!0});var In=o(J);Pe=a(In,"A",{id:!0,class:!0,href:!0});var Qd=o(Pe);Xr=a(Qd,"SPAN",{});var Wd=o(Xr);E(tt.$$.fragment,Wd),Wd.forEach(r),Qd.forEach(r),Lo=m(In),rt=a(In,"SPAN",{});var An=o(rt);To=l(An,"Les "),Qr=a(An,"I",{});var Zd=o(Qr);Io=l(Zd,"transformers"),Zd.forEach(r),Ao=l(An," sont des mod\xE8les de langage"),An.forEach(r),In.forEach(r),ba=m(e),I=a(e,"P",{});var ir=o(I);Co=l(ir,"Tous les "),Wr=a(ir,"EM",{});var Kd=o(Wr);No=l(Kd,"transformers"),Kd.forEach(r),Go=l(ir," mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Zr=a(ir,"EM",{});var em=o(Zr);Ro=l(em,"mod\xE8les de langage"),em.forEach(r),jo=l(ir,". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),ir.forEach(r),xa=m(e),$e=a(e,"P",{});var Cn=o($e);So=l(Cn,"Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),Kr=a(Cn,"EM",{});var tm=o(Kr);zo=l(tm,"finetun\xE9"),tm.forEach(r),Bo=l(Cn," de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),Cn.forEach(r),Pa=m(e),A=a(e,"P",{});var ur=o(A);Do=l(ur,"Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),es=a(ur,"EM",{});var rm=o(es);Oo=l(rm,"n"),rm.forEach(r),Uo=l(ur," mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),ts=a(ur,"EM",{});var sm=o(ts);Ho=l(sm,"mod\xE9lisation causale du langage"),sm.forEach(r),Vo=l(ur," car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),ur.forEach(r),$a=m(e),X=a(e,"DIV",{class:!0});var Nn=o(X);st=a(Nn,"IMG",{class:!0,src:!0,alt:!0}),Yo=m(Nn),at=a(Nn,"IMG",{class:!0,src:!0,alt:!0}),Nn.forEach(r),ka=m(e),ke=a(e,"P",{});var Gn=o(ke);Fo=l(Gn,"Un autre exemple est la "),rs=a(Gn,"EM",{});var am=o(rs);Jo=l(am,"mod\xE9lisation du langage masqu\xE9"),am.forEach(r),Xo=l(Gn,", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),Gn.forEach(r),Ma=m(e),Q=a(e,"DIV",{class:!0});var Rn=o(Q);nt=a(Rn,"IMG",{class:!0,src:!0,alt:!0}),Qo=m(Rn),lt=a(Rn,"IMG",{class:!0,src:!0,alt:!0}),Rn.forEach(r),wa=m(e),W=a(e,"H2",{class:!0});var jn=o(W);Me=a(jn,"A",{id:!0,class:!0,href:!0});var nm=o(Me);ss=a(nm,"SPAN",{});var lm=o(ss);E(ot.$$.fragment,lm),lm.forEach(r),nm.forEach(r),Wo=m(jn),it=a(jn,"SPAN",{});var Sn=o(it);Zo=l(Sn,"Les "),as=a(Sn,"I",{});var om=o(as);Ko=l(om,"transformers"),om.forEach(r),ei=l(Sn," sont \xE9normes"),Sn.forEach(r),jn.forEach(r),ya=m(e),Nt=a(e,"P",{});var im=o(Nt);ti=l(im,"En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),im.forEach(r),La=m(e),ut=a(e,"DIV",{class:!0});var um=o(ut);ct=a(um,"IMG",{src:!0,alt:!0,width:!0}),um.forEach(r),Ta=m(e),Gt=a(e,"P",{});var cm=o(Gt);ri=l(cm,"Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),cm.forEach(r),Ia=m(e),Z=a(e,"DIV",{class:!0});var zn=o(Z);dt=a(zn,"IMG",{class:!0,src:!0,alt:!0}),si=m(zn),mt=a(zn,"IMG",{class:!0,src:!0,alt:!0}),zn.forEach(r),Aa=m(e),E(pt.$$.fragment,e),Ca=m(e),Rt=a(e,"P",{});var dm=o(Rt);ai=l(dm,"L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),dm.forEach(r),Na=m(e),jt=a(e,"P",{});var mm=o(jt);ni=l(mm,"Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),mm.forEach(r),Ga=m(e),St=a(e,"P",{});var pm=o(St);li=l(pm,"C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),pm.forEach(r),Ra=m(e),K=a(e,"H2",{class:!0});var Bn=o(K);we=a(Bn,"A",{id:!0,class:!0,href:!0});var fm=o(we);ns=a(fm,"SPAN",{});var hm=o(ns);E(ft.$$.fragment,hm),hm.forEach(r),fm.forEach(r),oi=m(Bn),ls=a(Bn,"SPAN",{});var vm=o(ls);ii=l(vm,"L'apprentissage par transfert"),vm.forEach(r),Bn.forEach(r),ja=m(e),E(ht.$$.fragment,e),Sa=m(e),zt=a(e,"P",{});var gm=o(zt);ui=l(gm,"Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),gm.forEach(r),za=m(e),ee=a(e,"DIV",{class:!0});var Dn=o(ee);vt=a(Dn,"IMG",{class:!0,src:!0,alt:!0}),ci=m(Dn),gt=a(Dn,"IMG",{class:!0,src:!0,alt:!0}),Dn.forEach(r),Ba=m(e),Bt=a(e,"P",{});var Em=o(Bt);di=l(Em,"Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),Em.forEach(r),Da=m(e),C=a(e,"P",{});var cr=o(C);mi=l(cr,"Le "),os=a(cr,"EM",{});var _m=o(os);pi=l(_m,"finetuning"),_m.forEach(r),fi=l(cr,", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),is=a(cr,"EM",{});var qm=o(is);hi=l(qm,"finetuning"),qm.forEach(r),vi=l(cr,", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),cr.forEach(r),Oa=m(e),N=a(e,"UL",{});var dr=o(N);te=a(dr,"LI",{});var mr=o(te);gi=l(mr,"Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),us=a(mr,"EM",{});var bm=o(us);Ei=l(bm,"finetuning"),bm.forEach(r),_i=l(mr,". Le processus de "),cs=a(mr,"EM",{});var xm=o(cs);qi=l(xm,"finetuning"),xm.forEach(r),bi=l(mr," est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),mr.forEach(r),xi=m(dr),Et=a(dr,"LI",{});var On=o(Et);Pi=l(On,"Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),ds=a(On,"EM",{});var Pm=o(ds);$i=l(Pm,"finetuning"),Pm.forEach(r),ki=l(On," n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),On.forEach(r),Mi=m(dr),ms=a(dr,"LI",{});var $m=o(ms);wi=l($m,"Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),$m.forEach(r),dr.forEach(r),Ua=m(e),G=a(e,"P",{});var pr=o(G);yi=l(pr,"Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),ps=a(pr,"EM",{});var km=o(ps);Li=l(km,"finetuner"),km.forEach(r),Ti=l(pr," sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),fs=a(pr,"EM",{});var Mm=o(fs);Ii=l(Mm,"finetuning"),Mm.forEach(r),Ai=l(pr," ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),pr.forEach(r),Ha=m(e),re=a(e,"DIV",{class:!0});var Un=o(re);_t=a(Un,"IMG",{class:!0,src:!0,alt:!0}),Ci=m(Un),qt=a(Un,"IMG",{class:!0,src:!0,alt:!0}),Un.forEach(r),Va=m(e),R=a(e,"P",{});var fr=o(R);Ni=l(fr,"Le "),hs=a(fr,"EM",{});var wm=o(hs);Gi=l(wm,"finetuning"),wm.forEach(r),Ri=l(fr," d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),vs=a(fr,"EM",{});var ym=o(vs);ji=l(ym,"finetuning"),ym.forEach(r),Si=l(fr," car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),fr.forEach(r),Ya=m(e),ye=a(e,"P",{});var Hn=o(ye);zi=l(Hn,"Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),gs=a(Hn,"EM",{});var Lm=o(gs);Bi=l(Lm,"finetuner"),Lm.forEach(r),Di=l(Hn,"."),Hn.forEach(r),Fa=m(e),se=a(e,"H2",{class:!0});var Vn=o(se);Le=a(Vn,"A",{id:!0,class:!0,href:!0});var Tm=o(Le);Es=a(Tm,"SPAN",{});var Im=o(Es);E(bt.$$.fragment,Im),Im.forEach(r),Tm.forEach(r),Oi=m(Vn),_s=a(Vn,"SPAN",{});var Am=o(_s);Ui=l(Am,"Architecture g\xE9n\xE9rale"),Am.forEach(r),Vn.forEach(r),Ja=m(e),Te=a(e,"P",{});var Yn=o(Te);Hi=l(Yn,"Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),qs=a(Yn,"EM",{});var Cm=o(qs);Vi=l(Cm,"transformers"),Cm.forEach(r),Yi=l(Yn,". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),Yn.forEach(r),Xa=m(e),E(xt.$$.fragment,e),Qa=m(e),ae=a(e,"H2",{class:!0});var Fn=o(ae);Ie=a(Fn,"A",{id:!0,class:!0,href:!0});var Nm=o(Ie);bs=a(Nm,"SPAN",{});var Gm=o(bs);E(Pt.$$.fragment,Gm),Gm.forEach(r),Nm.forEach(r),Fi=m(Fn),xs=a(Fn,"SPAN",{});var Rm=o(xs);Ji=l(Rm,"Introduction"),Rm.forEach(r),Fn.forEach(r),Wa=m(e),Dt=a(e,"P",{});var jm=o(Dt);Xi=l(jm,"Le mod\xE8le est principalement compos\xE9 de deux blocs :"),jm.forEach(r),Za=m(e),Ae=a(e,"UL",{});var Jn=o(Ae);Ot=a(Jn,"LI",{});var jc=o(Ot);Ps=a(jc,"STRONG",{});var Sm=o(Ps);Qi=l(Sm,"Encodeur (\xE0 gauche)"),Sm.forEach(r),Wi=l(jc," : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),jc.forEach(r),Zi=m(Jn),Ut=a(Jn,"LI",{});var Sc=o(Ut);$s=a(Sc,"STRONG",{});var zm=o($s);Ki=l(zm,"D\xE9codeur (\xE0 droite)"),zm.forEach(r),eu=l(Sc," : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),Sc.forEach(r),Jn.forEach(r),Ka=m(e),ne=a(e,"DIV",{class:!0});var Xn=o(ne);$t=a(Xn,"IMG",{class:!0,src:!0,alt:!0}),tu=m(Xn),kt=a(Xn,"IMG",{class:!0,src:!0,alt:!0}),Xn.forEach(r),en=m(e),Ht=a(e,"P",{});var Bm=o(Ht);ru=l(Bm,"Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),Bm.forEach(r),tn=m(e),j=a(e,"UL",{});var hr=o(j);Vt=a(hr,"LI",{});var zc=o(Vt);ks=a(zc,"STRONG",{});var Dm=o(ks);su=l(Dm,"Mod\xE8les uniquement encodeurs"),Dm.forEach(r),au=l(zc," : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),zc.forEach(r),nu=m(hr),Yt=a(hr,"LI",{});var Bc=o(Yt);Ms=a(Bc,"STRONG",{});var Om=o(Ms);lu=l(Om,"Mod\xE8les uniquement d\xE9codeurs"),Om.forEach(r),ou=l(Bc," : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),Bc.forEach(r),iu=m(hr),Ce=a(hr,"LI",{});var ua=o(Ce);ws=a(ua,"STRONG",{});var Um=o(ws);uu=l(Um,"Mod\xE8les encodeurs-d\xE9codeurs"),Um.forEach(r),cu=l(ua," (ou "),ys=a(ua,"STRONG",{});var Hm=o(ys);du=l(Hm,"mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),Hm.forEach(r),mu=l(ua,") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),ua.forEach(r),hr.forEach(r),rn=m(e),Ft=a(e,"P",{});var Vm=o(Ft);pu=l(Vm,"Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),Vm.forEach(r),sn=m(e),le=a(e,"H2",{class:!0});var Qn=o(le);Ne=a(Qn,"A",{id:!0,class:!0,href:!0});var Ym=o(Ne);Ls=a(Ym,"SPAN",{});var Fm=o(Ls);E(Mt.$$.fragment,Fm),Fm.forEach(r),Ym.forEach(r),fu=m(Qn),Ts=a(Qn,"SPAN",{});var Jm=o(Ts);hu=l(Jm,"Les couches d'attention"),Jm.forEach(r),Qn.forEach(r),an=m(e),M=a(e,"P",{});var Be=o(M);vu=l(Be,"Une caract\xE9ristique cl\xE9 des "),Is=a(Be,"EM",{});var Xm=o(Is);gu=l(Xm,"transformers"),Xm.forEach(r),Eu=l(Be," est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),As=a(Be,"EM",{});var Qm=o(As);_u=l(Qm,"transformer"),Qm.forEach(r),qu=l(Be," se nomme "),wt=a(Be,"A",{href:!0,rel:!0});var Wm=o(wt);Cs=a(Wm,"EM",{});var Zm=o(Cs);bu=l(Zm,"Attention Is All You Need"),Zm.forEach(r),Wm.forEach(r),xu=l(Be," ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),Be.forEach(r),nn=m(e),p=a(e,"P",{});var f=o(p);Pu=l(f,"Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB "),Ns=a(f,"EM",{});var Km=o(Ns);$u=l(Km,"You like this course"),Km.forEach(r),ku=l(f," \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB "),Gs=a(f,"EM",{});var ep=o(Gs);Mu=l(ep,"You"),ep.forEach(r),wu=l(f," \xBB pour obtenir la traduction correcte du mot \xAB "),Rs=a(f,"EM",{});var tp=o(Rs);yu=l(tp,"like"),tp.forEach(r),Lu=l(f," \xBB, car en fran\xE7ais le verbe \xAB "),js=a(f,"EM",{});var rp=o(js);Tu=l(rp,"like"),rp.forEach(r),Iu=l(f," \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB "),Ss=a(f,"EM",{});var sp=o(Ss);Au=l(sp,"this"),sp.forEach(r),Cu=l(f," \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB "),zs=a(f,"EM",{});var ap=o(zs);Nu=l(ap,"course"),ap.forEach(r),Gu=l(f," \xBB car \xAB "),Bs=a(f,"EM",{});var np=o(Bs);Ru=l(np,"this"),np.forEach(r),ju=l(f," \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB "),Ds=a(f,"EM",{});var lp=o(Ds);Su=l(lp,"this"),lp.forEach(r),zu=l(f," \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),f.forEach(r),ln=m(e),Jt=a(e,"P",{});var op=o(Jt);Bu=l(op,"Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),op.forEach(r),on=m(e),Ge=a(e,"P",{});var Wn=o(Ge);Du=l(Wn,"Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),Os=a(Wn,"EM",{});var ip=o(Os);Ou=l(ip,"transformers"),ip.forEach(r),Uu=l(Wn,"."),Wn.forEach(r),un=m(e),oe=a(e,"H2",{class:!0});var Zn=o(oe);Re=a(Zn,"A",{id:!0,class:!0,href:!0});var up=o(Re);Us=a(up,"SPAN",{});var cp=o(Us);E(yt.$$.fragment,cp),cp.forEach(r),up.forEach(r),Hu=m(Zn),Hs=a(Zn,"SPAN",{});var dp=o(Hs);Vu=l(dp,"L'architecture originale"),dp.forEach(r),Zn.forEach(r),cn=m(e),je=a(e,"P",{});var Kn=o(je);Yu=l(Kn,"L\u2019architecture du "),Vs=a(Kn,"EM",{});var mp=o(Vs);Fu=l(mp,"transformer"),mp.forEach(r),Ju=l(Kn," a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),Kn.forEach(r),dn=m(e),Xt=a(e,"P",{});var pp=o(Xt);Xu=l(pp,"Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),pp.forEach(r),mn=m(e),Se=a(e,"P",{});var el=o(Se);Qu=l(el,"L\u2019architecture originale du "),Ys=a(el,"EM",{});var fp=o(Ys);Wu=l(fp,"transformer"),fp.forEach(r),Zu=l(el," ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),el.forEach(r),pn=m(e),ie=a(e,"DIV",{class:!0});var tl=o(ie);Lt=a(tl,"IMG",{class:!0,src:!0,alt:!0}),Ku=m(tl),Tt=a(tl,"IMG",{class:!0,src:!0,alt:!0}),tl.forEach(r),fn=m(e),Qt=a(e,"P",{});var hp=o(Qt);ec=l(hp,"Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),hp.forEach(r),hn=m(e),S=a(e,"P",{});var vr=o(S);tc=l(vr,"Le "),Fs=a(vr,"EM",{});var vp=o(Fs);rc=l(vp,"masque d\u2019attention"),vp.forEach(r),sc=l(vr," peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Js=a(vr,"EM",{});var gp=o(Js);ac=l(gp,"padding"),gp.forEach(r),nc=l(vr,") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),vr.forEach(r),vn=m(e),ue=a(e,"H2",{class:!0});var rl=o(ue);ze=a(rl,"A",{id:!0,class:!0,href:!0});var Ep=o(ze);Xs=a(Ep,"SPAN",{});var _p=o(Xs);E(It.$$.fragment,_p),_p.forEach(r),Ep.forEach(r),lc=m(rl),Wt=a(rl,"SPAN",{});var Dc=o(Wt);oc=l(Dc,"Architectures contre "),Qs=a(Dc,"I",{});var qp=o(Qs);ic=l(qp,"checkpoints"),qp.forEach(r),Dc.forEach(r),rl.forEach(r),gn=l(e,`

 
En approfondissant l'\xE9tude des `),Zt=a(e,"I",{});var bp=o(Zt);uc=l(bp,"transformers"),bp.forEach(r),En=l(e," dans ce cours, vous verrez des mentions d'"),Kt=a(e,"I",{});var xp=o(Kt);cc=l(xp,"architectures"),xp.forEach(r),_n=l(e," et de "),er=a(e,"I",{});var Pp=o(er);dc=l(Pp,"checkpoints"),Pp.forEach(r),qn=l(e," ainsi que de "),tr=a(e,"I",{});var $p=o(tr);mc=l($p,"mod\xE8les"),$p.forEach(r),bn=l(e,`. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),z=a(e,"UL",{});var gr=o(z);rr=a(gr,"LI",{});var Oc=o(rr);Ws=a(Oc,"STRONG",{});var kp=o(Ws);pc=l(kp,"Architecture"),kp.forEach(r),fc=l(Oc," : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),Oc.forEach(r),hc=m(gr),sr=a(gr,"LI",{});var Uc=o(sr);Zs=a(Uc,"STRONG",{});var Mp=o(Zs);vc=l(Mp,"Checkpoints"),Mp.forEach(r),gc=l(Uc," : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),Uc.forEach(r),Ec=m(gr),w=a(gr,"LI",{});var pe=o(w);Ks=a(pe,"STRONG",{});var wp=o(Ks);_c=l(wp,"Mod\xE8le"),wp.forEach(r),qc=l(pe," : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),ea=a(pe,"EM",{});var yp=o(ea);bc=l(yp,"checkpoint"),yp.forEach(r),xc=l(pe," \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),ta=a(pe,"EM",{});var Lp=o(ta);Pc=l(Lp,"architecture"),Lp.forEach(r),$c=l(pe," ou "),ra=a(pe,"EM",{});var Tp=o(ra);kc=l(Tp,"checkpoint"),Tp.forEach(r),Mc=l(pe," lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),pe.forEach(r),gr.forEach(r),xn=m(e),y=a(e,"P",{});var De=o(y);wc=l(De,"Par exemple, BERT est une architecture alors que "),sa=a(De,"CODE",{});var Ip=o(sa);yc=l(Ip,"bert-base-cased"),Ip.forEach(r),Lc=l(De," (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),aa=a(De,"EM",{});var Ap=o(aa);Tc=l(Ap,"checkpoint"),Ap.forEach(r),Ic=l(De,". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),na=a(De,"CODE",{});var Cp=o(na);Ac=l(Cp,"bert-base-cased"),Cp.forEach(r),Cc=l(De," \xBB."),De.forEach(r),this.h()},h(){c(D,"name","hf:doc:metadata"),c(D,"content",JSON.stringify(Dp)),c(fe,"id","comment-fonctionnent-les-itransformersi"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#comment-fonctionnent-les-itransformersi"),c(O,"class","relative group"),c(ve,"id","court-historique-des-itransformersi"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#court-historique-des-itransformersi"),c(U,"class","relative group"),c(Ve,"class","block dark:hidden"),h(Ve.src,Yc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(Ve,"src",Yc),c(Ve,"alt","A brief chronology of Transformers models."),c(Ye,"class","hidden dark:block"),h(Ye.src,Fc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(Ye,"src",Fc),c(Ye,"alt","A brief chronology of Transformers models."),c(H,"class","flex justify-center"),c(Ee,"href","https://arxiv.org/abs/1706.03762"),c(Ee,"rel","nofollow"),c(Je,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Je,"rel","nofollow"),c(Xe,"href","https://arxiv.org/abs/1810.04805"),c(Xe,"rel","nofollow"),c(Qe,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(Qe,"rel","nofollow"),c(We,"href","https://arxiv.org/abs/1910.01108"),c(We,"rel","nofollow"),c(Ze,"href","https://arxiv.org/abs/1910.13461"),c(Ze,"rel","nofollow"),c(Ke,"href","https://arxiv.org/abs/1910.10683"),c(Ke,"rel","nofollow"),c(et,"href","https://arxiv.org/abs/2005.14165"),c(et,"rel","nofollow"),c(Pe,"id","les-itransformersi-sont-des-modles-de-langage"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#les-itransformersi-sont-des-modles-de-langage"),c(J,"class","relative group"),c(st,"class","block dark:hidden"),h(st.src,Jc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(st,"src",Jc),c(st,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(at,"class","hidden dark:block"),h(at.src,Xc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(at,"src",Xc),c(at,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(X,"class","flex justify-center"),c(nt,"class","block dark:hidden"),h(nt.src,Qc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(nt,"src",Qc),c(nt,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(lt,"class","hidden dark:block"),h(lt.src,Wc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(lt,"src",Wc),c(lt,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(Q,"class","flex justify-center"),c(Me,"id","les-itransformersi-sont-normes"),c(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Me,"href","#les-itransformersi-sont-normes"),c(W,"class","relative group"),h(ct.src,Zc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(ct,"src",Zc),c(ct,"alt","Number of parameters of recent Transformers models"),c(ct,"width","90%"),c(ut,"class","flex justify-center"),c(dt,"class","block dark:hidden"),h(dt.src,Kc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(dt,"src",Kc),c(dt,"alt","The carbon footprint of a large language model."),c(mt,"class","hidden dark:block"),h(mt.src,ed="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(mt,"src",ed),c(mt,"alt","The carbon footprint of a large language model."),c(Z,"class","flex justify-center"),c(we,"id","lapprentissage-par-transfert"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#lapprentissage-par-transfert"),c(K,"class","relative group"),c(vt,"class","block dark:hidden"),h(vt.src,td="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(vt,"src",td),c(vt,"alt","The pretraining of a language model is costly in both time and money."),c(gt,"class","hidden dark:block"),h(gt.src,rd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(gt,"src",rd),c(gt,"alt","The pretraining of a language model is costly in both time and money."),c(ee,"class","flex justify-center"),c(_t,"class","block dark:hidden"),h(_t.src,sd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(_t,"src",sd),c(_t,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(qt,"class","hidden dark:block"),h(qt.src,ad="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(qt,"src",ad),c(qt,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(re,"class","flex justify-center"),c(Le,"id","architecture-gnrale"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#architecture-gnrale"),c(se,"class","relative group"),c(Ie,"id","introduction"),c(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ie,"href","#introduction"),c(ae,"class","relative group"),c($t,"class","block dark:hidden"),h($t.src,nd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c($t,"src",nd),c($t,"alt","Architecture of a Transformers models"),c(kt,"class","hidden dark:block"),h(kt.src,ld="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(kt,"src",ld),c(kt,"alt","Architecture of a Transformers models"),c(ne,"class","flex justify-center"),c(Ne,"id","les-couches-dattention"),c(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ne,"href","#les-couches-dattention"),c(le,"class","relative group"),c(wt,"href","https://arxiv.org/abs/1706.03762"),c(wt,"rel","nofollow"),c(Re,"id","larchitecture-originale"),c(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Re,"href","#larchitecture-originale"),c(oe,"class","relative group"),c(Lt,"class","block dark:hidden"),h(Lt.src,od="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(Lt,"src",od),c(Lt,"alt","Architecture of a Transformers models"),c(Tt,"class","hidden dark:block"),h(Tt.src,id="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(Tt,"src",id),c(Tt,"alt","Architecture of a Transformers models"),c(ie,"class","flex justify-center"),c(ze,"id","architectures-contre-icheckpointsi"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#architectures-contre-icheckpointsi"),c(ue,"class","relative group")},m(e,i){t(document.head,D),u(e,ca,i),u(e,O,i),t(O,fe),t(fe,Er),_(Oe,Er,null),t(O,sl),t(O,Ue),t(Ue,al),t(Ue,_r),t(_r,nl),t(Ue,ll),u(e,da,i),u(e,he,i),t(he,ol),t(he,qr),t(qr,il),t(he,ul),u(e,ma,i),u(e,U,i),t(U,ve),t(ve,br),_(He,br,null),t(U,cl),t(U,At),t(At,dl),t(At,xr),t(xr,ml),u(e,pa,i),u(e,ge,i),t(ge,pl),t(ge,Pr),t(Pr,fl),t(ge,hl),u(e,fa,i),u(e,H,i),t(H,Ve),t(H,vl),t(H,Ye),u(e,ha,i),u(e,Fe,i),t(Fe,Ee),t(Ee,gl),t(Ee,$r),t($r,El),t(Fe,_l),u(e,va,i),u(e,v,i),t(v,kr),t(kr,P),t(P,Mr),t(Mr,ql),t(P,bl),t(P,Je),t(Je,xl),t(P,Pl),t(P,wr),t(wr,$l),t(P,kl),t(P,yr),t(yr,Ml),t(P,wl),t(v,yl),t(v,Lr),t(Lr,_e),t(_e,Tr),t(Tr,Ll),t(_e,Tl),t(_e,Xe),t(Xe,Il),t(_e,Al),t(v,Cl),t(v,Ir),t(Ir,qe),t(qe,Ar),t(Ar,Nl),t(qe,Gl),t(qe,Qe),t(Qe,Rl),t(qe,jl),t(v,Sl),t(v,Cr),t(Cr,be),t(be,Nr),t(Nr,zl),t(be,Bl),t(be,We),t(We,Dl),t(be,Ol),t(v,Ul),t(v,Gr),t(Gr,$),t($,Rr),t(Rr,Hl),t($,Vl),t($,Ze),t(Ze,Yl),t($,Fl),t($,Ke),t(Ke,Jl),t($,Xl),t($,jr),t(jr,Ql),t($,Wl),t(v,Zl),t(v,Sr),t(Sr,k),t(k,zr),t(zr,Kl),t(k,eo),t(k,et),t(et,to),t(k,ro),t(k,Br),t(Br,so),t(k,ao),t(k,Dr),t(Dr,no),t(k,lo),u(e,ga,i),u(e,xe,i),t(xe,oo),t(xe,Or),t(Or,io),t(xe,uo),u(e,Ea,i),u(e,T,i),t(T,V),t(V,co),t(V,Ur),t(Ur,mo),t(V,po),t(V,Hr),t(Hr,fo),t(V,ho),t(T,vo),t(T,Y),t(Y,go),t(Y,Vr),t(Vr,Eo),t(Y,_o),t(Y,Yr),t(Yr,qo),t(Y,bo),t(T,xo),t(T,F),t(F,Po),t(F,Fr),t(Fr,$o),t(F,ko),t(F,Jr),t(Jr,Mo),t(F,wo),u(e,_a,i),u(e,Ct,i),t(Ct,yo),u(e,qa,i),u(e,J,i),t(J,Pe),t(Pe,Xr),_(tt,Xr,null),t(J,Lo),t(J,rt),t(rt,To),t(rt,Qr),t(Qr,Io),t(rt,Ao),u(e,ba,i),u(e,I,i),t(I,Co),t(I,Wr),t(Wr,No),t(I,Go),t(I,Zr),t(Zr,Ro),t(I,jo),u(e,xa,i),u(e,$e,i),t($e,So),t($e,Kr),t(Kr,zo),t($e,Bo),u(e,Pa,i),u(e,A,i),t(A,Do),t(A,es),t(es,Oo),t(A,Uo),t(A,ts),t(ts,Ho),t(A,Vo),u(e,$a,i),u(e,X,i),t(X,st),t(X,Yo),t(X,at),u(e,ka,i),u(e,ke,i),t(ke,Fo),t(ke,rs),t(rs,Jo),t(ke,Xo),u(e,Ma,i),u(e,Q,i),t(Q,nt),t(Q,Qo),t(Q,lt),u(e,wa,i),u(e,W,i),t(W,Me),t(Me,ss),_(ot,ss,null),t(W,Wo),t(W,it),t(it,Zo),t(it,as),t(as,Ko),t(it,ei),u(e,ya,i),u(e,Nt,i),t(Nt,ti),u(e,La,i),u(e,ut,i),t(ut,ct),u(e,Ta,i),u(e,Gt,i),t(Gt,ri),u(e,Ia,i),u(e,Z,i),t(Z,dt),t(Z,si),t(Z,mt),u(e,Aa,i),_(pt,e,i),u(e,Ca,i),u(e,Rt,i),t(Rt,ai),u(e,Na,i),u(e,jt,i),t(jt,ni),u(e,Ga,i),u(e,St,i),t(St,li),u(e,Ra,i),u(e,K,i),t(K,we),t(we,ns),_(ft,ns,null),t(K,oi),t(K,ls),t(ls,ii),u(e,ja,i),_(ht,e,i),u(e,Sa,i),u(e,zt,i),t(zt,ui),u(e,za,i),u(e,ee,i),t(ee,vt),t(ee,ci),t(ee,gt),u(e,Ba,i),u(e,Bt,i),t(Bt,di),u(e,Da,i),u(e,C,i),t(C,mi),t(C,os),t(os,pi),t(C,fi),t(C,is),t(is,hi),t(C,vi),u(e,Oa,i),u(e,N,i),t(N,te),t(te,gi),t(te,us),t(us,Ei),t(te,_i),t(te,cs),t(cs,qi),t(te,bi),t(N,xi),t(N,Et),t(Et,Pi),t(Et,ds),t(ds,$i),t(Et,ki),t(N,Mi),t(N,ms),t(ms,wi),u(e,Ua,i),u(e,G,i),t(G,yi),t(G,ps),t(ps,Li),t(G,Ti),t(G,fs),t(fs,Ii),t(G,Ai),u(e,Ha,i),u(e,re,i),t(re,_t),t(re,Ci),t(re,qt),u(e,Va,i),u(e,R,i),t(R,Ni),t(R,hs),t(hs,Gi),t(R,Ri),t(R,vs),t(vs,ji),t(R,Si),u(e,Ya,i),u(e,ye,i),t(ye,zi),t(ye,gs),t(gs,Bi),t(ye,Di),u(e,Fa,i),u(e,se,i),t(se,Le),t(Le,Es),_(bt,Es,null),t(se,Oi),t(se,_s),t(_s,Ui),u(e,Ja,i),u(e,Te,i),t(Te,Hi),t(Te,qs),t(qs,Vi),t(Te,Yi),u(e,Xa,i),_(xt,e,i),u(e,Qa,i),u(e,ae,i),t(ae,Ie),t(Ie,bs),_(Pt,bs,null),t(ae,Fi),t(ae,xs),t(xs,Ji),u(e,Wa,i),u(e,Dt,i),t(Dt,Xi),u(e,Za,i),u(e,Ae,i),t(Ae,Ot),t(Ot,Ps),t(Ps,Qi),t(Ot,Wi),t(Ae,Zi),t(Ae,Ut),t(Ut,$s),t($s,Ki),t(Ut,eu),u(e,Ka,i),u(e,ne,i),t(ne,$t),t(ne,tu),t(ne,kt),u(e,en,i),u(e,Ht,i),t(Ht,ru),u(e,tn,i),u(e,j,i),t(j,Vt),t(Vt,ks),t(ks,su),t(Vt,au),t(j,nu),t(j,Yt),t(Yt,Ms),t(Ms,lu),t(Yt,ou),t(j,iu),t(j,Ce),t(Ce,ws),t(ws,uu),t(Ce,cu),t(Ce,ys),t(ys,du),t(Ce,mu),u(e,rn,i),u(e,Ft,i),t(Ft,pu),u(e,sn,i),u(e,le,i),t(le,Ne),t(Ne,Ls),_(Mt,Ls,null),t(le,fu),t(le,Ts),t(Ts,hu),u(e,an,i),u(e,M,i),t(M,vu),t(M,Is),t(Is,gu),t(M,Eu),t(M,As),t(As,_u),t(M,qu),t(M,wt),t(wt,Cs),t(Cs,bu),t(M,xu),u(e,nn,i),u(e,p,i),t(p,Pu),t(p,Ns),t(Ns,$u),t(p,ku),t(p,Gs),t(Gs,Mu),t(p,wu),t(p,Rs),t(Rs,yu),t(p,Lu),t(p,js),t(js,Tu),t(p,Iu),t(p,Ss),t(Ss,Au),t(p,Cu),t(p,zs),t(zs,Nu),t(p,Gu),t(p,Bs),t(Bs,Ru),t(p,ju),t(p,Ds),t(Ds,Su),t(p,zu),u(e,ln,i),u(e,Jt,i),t(Jt,Bu),u(e,on,i),u(e,Ge,i),t(Ge,Du),t(Ge,Os),t(Os,Ou),t(Ge,Uu),u(e,un,i),u(e,oe,i),t(oe,Re),t(Re,Us),_(yt,Us,null),t(oe,Hu),t(oe,Hs),t(Hs,Vu),u(e,cn,i),u(e,je,i),t(je,Yu),t(je,Vs),t(Vs,Fu),t(je,Ju),u(e,dn,i),u(e,Xt,i),t(Xt,Xu),u(e,mn,i),u(e,Se,i),t(Se,Qu),t(Se,Ys),t(Ys,Wu),t(Se,Zu),u(e,pn,i),u(e,ie,i),t(ie,Lt),t(ie,Ku),t(ie,Tt),u(e,fn,i),u(e,Qt,i),t(Qt,ec),u(e,hn,i),u(e,S,i),t(S,tc),t(S,Fs),t(Fs,rc),t(S,sc),t(S,Js),t(Js,ac),t(S,nc),u(e,vn,i),u(e,ue,i),t(ue,ze),t(ze,Xs),_(It,Xs,null),t(ue,lc),t(ue,Wt),t(Wt,oc),t(Wt,Qs),t(Qs,ic),u(e,gn,i),u(e,Zt,i),t(Zt,uc),u(e,En,i),u(e,Kt,i),t(Kt,cc),u(e,_n,i),u(e,er,i),t(er,dc),u(e,qn,i),u(e,tr,i),t(tr,mc),u(e,bn,i),u(e,z,i),t(z,rr),t(rr,Ws),t(Ws,pc),t(rr,fc),t(z,hc),t(z,sr),t(sr,Zs),t(Zs,vc),t(sr,gc),t(z,Ec),t(z,w),t(w,Ks),t(Ks,_c),t(w,qc),t(w,ea),t(ea,bc),t(w,xc),t(w,ta),t(ta,Pc),t(w,$c),t(w,ra),t(ra,kc),t(w,Mc),u(e,xn,i),u(e,y,i),t(y,wc),t(y,sa),t(sa,yc),t(y,Lc),t(y,aa),t(aa,Tc),t(y,Ic),t(y,na),t(na,Ac),t(y,Cc),Pn=!0},p:Sp,i(e){Pn||(q(Oe.$$.fragment,e),q(He.$$.fragment,e),q(tt.$$.fragment,e),q(ot.$$.fragment,e),q(pt.$$.fragment,e),q(ft.$$.fragment,e),q(ht.$$.fragment,e),q(bt.$$.fragment,e),q(xt.$$.fragment,e),q(Pt.$$.fragment,e),q(Mt.$$.fragment,e),q(yt.$$.fragment,e),q(It.$$.fragment,e),Pn=!0)},o(e){b(Oe.$$.fragment,e),b(He.$$.fragment,e),b(tt.$$.fragment,e),b(ot.$$.fragment,e),b(pt.$$.fragment,e),b(ft.$$.fragment,e),b(ht.$$.fragment,e),b(bt.$$.fragment,e),b(xt.$$.fragment,e),b(Pt.$$.fragment,e),b(Mt.$$.fragment,e),b(yt.$$.fragment,e),b(It.$$.fragment,e),Pn=!1},d(e){r(D),e&&r(ca),e&&r(O),x(Oe),e&&r(da),e&&r(he),e&&r(ma),e&&r(U),x(He),e&&r(pa),e&&r(ge),e&&r(fa),e&&r(H),e&&r(ha),e&&r(Fe),e&&r(va),e&&r(v),e&&r(ga),e&&r(xe),e&&r(Ea),e&&r(T),e&&r(_a),e&&r(Ct),e&&r(qa),e&&r(J),x(tt),e&&r(ba),e&&r(I),e&&r(xa),e&&r($e),e&&r(Pa),e&&r(A),e&&r($a),e&&r(X),e&&r(ka),e&&r(ke),e&&r(Ma),e&&r(Q),e&&r(wa),e&&r(W),x(ot),e&&r(ya),e&&r(Nt),e&&r(La),e&&r(ut),e&&r(Ta),e&&r(Gt),e&&r(Ia),e&&r(Z),e&&r(Aa),x(pt,e),e&&r(Ca),e&&r(Rt),e&&r(Na),e&&r(jt),e&&r(Ga),e&&r(St),e&&r(Ra),e&&r(K),x(ft),e&&r(ja),x(ht,e),e&&r(Sa),e&&r(zt),e&&r(za),e&&r(ee),e&&r(Ba),e&&r(Bt),e&&r(Da),e&&r(C),e&&r(Oa),e&&r(N),e&&r(Ua),e&&r(G),e&&r(Ha),e&&r(re),e&&r(Va),e&&r(R),e&&r(Ya),e&&r(ye),e&&r(Fa),e&&r(se),x(bt),e&&r(Ja),e&&r(Te),e&&r(Xa),x(xt,e),e&&r(Qa),e&&r(ae),x(Pt),e&&r(Wa),e&&r(Dt),e&&r(Za),e&&r(Ae),e&&r(Ka),e&&r(ne),e&&r(en),e&&r(Ht),e&&r(tn),e&&r(j),e&&r(rn),e&&r(Ft),e&&r(sn),e&&r(le),x(Mt),e&&r(an),e&&r(M),e&&r(nn),e&&r(p),e&&r(ln),e&&r(Jt),e&&r(on),e&&r(Ge),e&&r(un),e&&r(oe),x(yt),e&&r(cn),e&&r(je),e&&r(dn),e&&r(Xt),e&&r(mn),e&&r(Se),e&&r(pn),e&&r(ie),e&&r(fn),e&&r(Qt),e&&r(hn),e&&r(S),e&&r(vn),e&&r(ue),x(It),e&&r(gn),e&&r(Zt),e&&r(En),e&&r(Kt),e&&r(_n),e&&r(er),e&&r(qn),e&&r(tr),e&&r(bn),e&&r(z),e&&r(xn),e&&r(y)}}}const Dp={local:"comment-fonctionnent-les-itransformersi",sections:[{local:"court-historique-des-itransformersi",title:"Court historique des <i>transformers</i>"},{local:"les-itransformersi-sont-des-modles-de-langage",title:"Les <i>transformers</i> sont des mod\xE8les de langage"},{local:"les-itransformersi-sont-normes",title:"Les <i>transformers</i> sont \xE9normes"},{local:"lapprentissage-par-transfert",title:"L'apprentissage par transfert"},{local:"architecture-gnrale",title:"Architecture g\xE9n\xE9rale"},{local:"introduction",title:"Introduction"},{local:"les-couches-dattention",title:"Les couches d'attention"},{local:"larchitecture-originale",title:"L'architecture originale"},{local:"architectures-contre-icheckpointsi",title:"Architectures contre <i>checkpoints</i>"}],title:"Comment fonctionnent les <i>transformers</i> ?"};function Op(Vc){return zp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yp extends Np{constructor(D){super();Gp(this,D,Op,Bp,Rp,{})}}export{Yp as default,Dp as metadata};
