import{S as Or,i as Dr,s as zr,e as a,k as c,w as re,t as l,N as Fr,c as o,d as r,m as p,a as n,x as ae,h as s,b as i,G as t,g as h,y as oe,L as Ur,q as ne,o as le,B as se,v as Yr}from"../chunks/vendor-hf-doc-builder.js";import{I as ie}from"../chunks/IconCopyLink-hf-doc-builder.js";function Mr(er){let w,ke,y,$,fe,D,tt,he,rt,Se,b,at,z,ot,nt,Ne,_,k,ce,F,lt,pe,st,xe,S,it,U,ft,ht,Le,N,ct,ue,pt,ut,Ce,d,Y,dt,M,mt,vt,gt,de,wt,yt,me,_t,He,u,It,V,At,Et,R,Pt,$t,T,bt,kt,W,St,Nt,qe,I,x,ve,X,xt,ge,Lt,Oe,L,Ct,we,Ht,qt,De,A,C,ye,B,Ot,_e,Dt,ze,H,zt,Ie,Ft,Ut,Fe,E,q,Ae,G,Yt,Ee,Mt,Ue,m,Rt,J,Tt,Wt,j,Xt,Bt,Ye,P,O,Pe,K,Gt,$e,Jt,Me,v,jt,be,Kt,Qt,Q,Vt,Zt,Re;return D=new ie({}),F=new ie({}),X=new ie({}),B=new ie({}),G=new ie({}),K=new ie({}),{c(){w=a("meta"),ke=c(),y=a("h1"),$=a("a"),fe=a("span"),re(D.$$.fragment),tt=c(),he=a("span"),rt=l("Inference API"),Se=c(),b=a("p"),at=l("Please refer to "),z=a("a"),ot=l("Accelerated Inference API Documentation"),nt=l(" for detailed information."),Ne=c(),_=a("h2"),k=a("a"),ce=a("span"),re(F.$$.fragment),lt=c(),pe=a("span"),st=l("What technology do you use to power the inference API?"),xe=c(),S=a("p"),it=l("For \u{1F917} Transformers models, "),U=a("a"),ft=l("Pipelines"),ht=l(" power the API."),Le=c(),N=a("p"),ct=l("On top of "),ue=a("code"),pt=l("Pipelines"),ut=l(" and depending on the model type, there are several production optimizations like:"),Ce=c(),d=a("ul"),Y=a("li"),dt=l("compiling models to optimized intermediary representations (e.g. "),M=a("a"),mt=l("ONNX"),vt=l("),"),gt=c(),de=a("li"),wt=l("maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),yt=c(),me=a("li"),_t=l("scaling the underlying compute infrastructure on the fly depending on the load constraints."),He=c(),u=a("p"),It=l("For models from "),V=a("a"),At=l("other libraries"),Et=l(", the API uses "),R=a("a"),Pt=l("Starlette"),$t=l(" and runs in "),T=a("a"),bt=l("Docker containers"),kt=l(". Each library defines the implementation of "),W=a("a"),St=l("different pipelines"),Nt=l("."),qe=c(),I=a("h2"),x=a("a"),ve=a("span"),re(X.$$.fragment),xt=c(),ge=a("span"),Lt=l("How can I turn off the inference API for my model?"),Oe=c(),L=a("p"),Ct=l("Specify "),we=a("code"),Ht=l("inference: false"),qt=l(" in your model card\u2019s metadata."),De=c(),A=a("h2"),C=a("a"),ye=a("span"),re(B.$$.fragment),Ot=c(),_e=a("span"),Dt=l("Can I send large volumes of requests? Can I get accelerated APIs?"),ze=c(),H=a("p"),zt=l("If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ie=a("code"),Ft=l("api-enterprise at huggingface.co"),Ut=l("."),Fe=c(),E=a("h2"),q=a("a"),Ae=a("span"),re(G.$$.fragment),Yt=c(),Ee=a("span"),Mt=l("How can I see my usage?"),Ue=c(),m=a("p"),Rt=l("You can head to the "),J=a("a"),Tt=l("Inference API dashboard"),Wt=l(". Learn more about it in the "),j=a("a"),Xt=l("Inference API documentation"),Bt=l("."),Ye=c(),P=a("h2"),O=a("a"),Pe=a("span"),re(K.$$.fragment),Gt=c(),$e=a("span"),Jt=l("Is there programmatic access to the Inference API?"),Me=c(),v=a("p"),jt=l("Yes, the "),be=a("code"),Kt=l("huggingface_hub"),Qt=l(" library has a client wrapper documented "),Q=a("a"),Vt=l("here"),Zt=l("."),this.h()},l(e){const f=Fr('[data-svelte="svelte-1phssyn"]',document.head);w=o(f,"META",{name:!0,content:!0}),f.forEach(r),ke=p(e),y=o(e,"H1",{class:!0});var Te=n(y);$=o(Te,"A",{id:!0,class:!0,href:!0});var tr=n($);fe=o(tr,"SPAN",{});var rr=n(fe);ae(D.$$.fragment,rr),rr.forEach(r),tr.forEach(r),tt=p(Te),he=o(Te,"SPAN",{});var ar=n(he);rt=s(ar,"Inference API"),ar.forEach(r),Te.forEach(r),Se=p(e),b=o(e,"P",{});var We=n(b);at=s(We,"Please refer to "),z=o(We,"A",{href:!0,rel:!0});var or=n(z);ot=s(or,"Accelerated Inference API Documentation"),or.forEach(r),nt=s(We," for detailed information."),We.forEach(r),Ne=p(e),_=o(e,"H2",{class:!0});var Xe=n(_);k=o(Xe,"A",{id:!0,class:!0,href:!0});var nr=n(k);ce=o(nr,"SPAN",{});var lr=n(ce);ae(F.$$.fragment,lr),lr.forEach(r),nr.forEach(r),lt=p(Xe),pe=o(Xe,"SPAN",{});var sr=n(pe);st=s(sr,"What technology do you use to power the inference API?"),sr.forEach(r),Xe.forEach(r),xe=p(e),S=o(e,"P",{});var Be=n(S);it=s(Be,"For \u{1F917} Transformers models, "),U=o(Be,"A",{href:!0,rel:!0});var ir=n(U);ft=s(ir,"Pipelines"),ir.forEach(r),ht=s(Be," power the API."),Be.forEach(r),Le=p(e),N=o(e,"P",{});var Ge=n(N);ct=s(Ge,"On top of "),ue=o(Ge,"CODE",{});var fr=n(ue);pt=s(fr,"Pipelines"),fr.forEach(r),ut=s(Ge," and depending on the model type, there are several production optimizations like:"),Ge.forEach(r),Ce=p(e),d=o(e,"UL",{});var Z=n(d);Y=o(Z,"LI",{});var Je=n(Y);dt=s(Je,"compiling models to optimized intermediary representations (e.g. "),M=o(Je,"A",{href:!0,rel:!0});var hr=n(M);mt=s(hr,"ONNX"),hr.forEach(r),vt=s(Je,"),"),Je.forEach(r),gt=p(Z),de=o(Z,"LI",{});var cr=n(de);wt=s(cr,"maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),cr.forEach(r),yt=p(Z),me=o(Z,"LI",{});var pr=n(me);_t=s(pr,"scaling the underlying compute infrastructure on the fly depending on the load constraints."),pr.forEach(r),Z.forEach(r),He=p(e),u=o(e,"P",{});var g=n(u);It=s(g,"For models from "),V=o(g,"A",{href:!0});var ur=n(V);At=s(ur,"other libraries"),ur.forEach(r),Et=s(g,", the API uses "),R=o(g,"A",{href:!0,rel:!0});var dr=n(R);Pt=s(dr,"Starlette"),dr.forEach(r),$t=s(g," and runs in "),T=o(g,"A",{href:!0,rel:!0});var mr=n(T);bt=s(mr,"Docker containers"),mr.forEach(r),kt=s(g,". Each library defines the implementation of "),W=o(g,"A",{href:!0,rel:!0});var vr=n(W);St=s(vr,"different pipelines"),vr.forEach(r),Nt=s(g,"."),g.forEach(r),qe=p(e),I=o(e,"H2",{class:!0});var je=n(I);x=o(je,"A",{id:!0,class:!0,href:!0});var gr=n(x);ve=o(gr,"SPAN",{});var wr=n(ve);ae(X.$$.fragment,wr),wr.forEach(r),gr.forEach(r),xt=p(je),ge=o(je,"SPAN",{});var yr=n(ge);Lt=s(yr,"How can I turn off the inference API for my model?"),yr.forEach(r),je.forEach(r),Oe=p(e),L=o(e,"P",{});var Ke=n(L);Ct=s(Ke,"Specify "),we=o(Ke,"CODE",{});var _r=n(we);Ht=s(_r,"inference: false"),_r.forEach(r),qt=s(Ke," in your model card\u2019s metadata."),Ke.forEach(r),De=p(e),A=o(e,"H2",{class:!0});var Qe=n(A);C=o(Qe,"A",{id:!0,class:!0,href:!0});var Ir=n(C);ye=o(Ir,"SPAN",{});var Ar=n(ye);ae(B.$$.fragment,Ar),Ar.forEach(r),Ir.forEach(r),Ot=p(Qe),_e=o(Qe,"SPAN",{});var Er=n(_e);Dt=s(Er,"Can I send large volumes of requests? Can I get accelerated APIs?"),Er.forEach(r),Qe.forEach(r),ze=p(e),H=o(e,"P",{});var Ve=n(H);zt=s(Ve,"If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ie=o(Ve,"CODE",{});var Pr=n(Ie);Ft=s(Pr,"api-enterprise at huggingface.co"),Pr.forEach(r),Ut=s(Ve,"."),Ve.forEach(r),Fe=p(e),E=o(e,"H2",{class:!0});var Ze=n(E);q=o(Ze,"A",{id:!0,class:!0,href:!0});var $r=n(q);Ae=o($r,"SPAN",{});var br=n(Ae);ae(G.$$.fragment,br),br.forEach(r),$r.forEach(r),Yt=p(Ze),Ee=o(Ze,"SPAN",{});var kr=n(Ee);Mt=s(kr,"How can I see my usage?"),kr.forEach(r),Ze.forEach(r),Ue=p(e),m=o(e,"P",{});var ee=n(m);Rt=s(ee,"You can head to the "),J=o(ee,"A",{href:!0,rel:!0});var Sr=n(J);Tt=s(Sr,"Inference API dashboard"),Sr.forEach(r),Wt=s(ee,". Learn more about it in the "),j=o(ee,"A",{href:!0,rel:!0});var Nr=n(j);Xt=s(Nr,"Inference API documentation"),Nr.forEach(r),Bt=s(ee,"."),ee.forEach(r),Ye=p(e),P=o(e,"H2",{class:!0});var et=n(P);O=o(et,"A",{id:!0,class:!0,href:!0});var xr=n(O);Pe=o(xr,"SPAN",{});var Lr=n(Pe);ae(K.$$.fragment,Lr),Lr.forEach(r),xr.forEach(r),Gt=p(et),$e=o(et,"SPAN",{});var Cr=n($e);Jt=s(Cr,"Is there programmatic access to the Inference API?"),Cr.forEach(r),et.forEach(r),Me=p(e),v=o(e,"P",{});var te=n(v);jt=s(te,"Yes, the "),be=o(te,"CODE",{});var Hr=n(be);Kt=s(Hr,"huggingface_hub"),Hr.forEach(r),Qt=s(te," library has a client wrapper documented "),Q=o(te,"A",{href:!0,rel:!0});var qr=n(Q);Vt=s(qr,"here"),qr.forEach(r),Zt=s(te,"."),te.forEach(r),this.h()},h(){i(w,"name","hf:doc:metadata"),i(w,"content",JSON.stringify(Rr)),i($,"id","inference-api"),i($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i($,"href","#inference-api"),i(y,"class","relative group"),i(z,"href","https://api-inference.huggingface.co/docs/python/html/index.html"),i(z,"rel","nofollow"),i(k,"id","what-technology-do-you-use-to-power-the-inference-api"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#what-technology-do-you-use-to-power-the-inference-api"),i(_,"class","relative group"),i(U,"href","https://huggingface.co/transformers/main_classes/pipelines.html"),i(U,"rel","nofollow"),i(M,"href","https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333"),i(M,"rel","nofollow"),i(V,"href","/docs/hub/libraries"),i(R,"href","https://www.starlette.io"),i(R,"rel","nofollow"),i(T,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images"),i(T,"rel","nofollow"),i(W,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines"),i(W,"rel","nofollow"),i(x,"id","how-can-i-turn-off-the-inference-api-for-my-model"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#how-can-i-turn-off-the-inference-api-for-my-model"),i(I,"class","relative group"),i(C,"id","can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(A,"class","relative group"),i(q,"id","how-can-i-see-my-usage"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-can-i-see-my-usage"),i(E,"class","relative group"),i(J,"href","https://api-inference.huggingface.co/dashboard/"),i(J,"rel","nofollow"),i(j,"href","https://api-inference.huggingface.co/docs/python/html/usage.html#api-usage-dashboard"),i(j,"rel","nofollow"),i(O,"id","is-there-programmatic-access-to-the-inference-api"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#is-there-programmatic-access-to-the-inference-api"),i(P,"class","relative group"),i(Q,"href","https://huggingface.co/docs/huggingface_hub/how-to-inference"),i(Q,"rel","nofollow")},m(e,f){t(document.head,w),h(e,ke,f),h(e,y,f),t(y,$),t($,fe),oe(D,fe,null),t(y,tt),t(y,he),t(he,rt),h(e,Se,f),h(e,b,f),t(b,at),t(b,z),t(z,ot),t(b,nt),h(e,Ne,f),h(e,_,f),t(_,k),t(k,ce),oe(F,ce,null),t(_,lt),t(_,pe),t(pe,st),h(e,xe,f),h(e,S,f),t(S,it),t(S,U),t(U,ft),t(S,ht),h(e,Le,f),h(e,N,f),t(N,ct),t(N,ue),t(ue,pt),t(N,ut),h(e,Ce,f),h(e,d,f),t(d,Y),t(Y,dt),t(Y,M),t(M,mt),t(Y,vt),t(d,gt),t(d,de),t(de,wt),t(d,yt),t(d,me),t(me,_t),h(e,He,f),h(e,u,f),t(u,It),t(u,V),t(V,At),t(u,Et),t(u,R),t(R,Pt),t(u,$t),t(u,T),t(T,bt),t(u,kt),t(u,W),t(W,St),t(u,Nt),h(e,qe,f),h(e,I,f),t(I,x),t(x,ve),oe(X,ve,null),t(I,xt),t(I,ge),t(ge,Lt),h(e,Oe,f),h(e,L,f),t(L,Ct),t(L,we),t(we,Ht),t(L,qt),h(e,De,f),h(e,A,f),t(A,C),t(C,ye),oe(B,ye,null),t(A,Ot),t(A,_e),t(_e,Dt),h(e,ze,f),h(e,H,f),t(H,zt),t(H,Ie),t(Ie,Ft),t(H,Ut),h(e,Fe,f),h(e,E,f),t(E,q),t(q,Ae),oe(G,Ae,null),t(E,Yt),t(E,Ee),t(Ee,Mt),h(e,Ue,f),h(e,m,f),t(m,Rt),t(m,J),t(J,Tt),t(m,Wt),t(m,j),t(j,Xt),t(m,Bt),h(e,Ye,f),h(e,P,f),t(P,O),t(O,Pe),oe(K,Pe,null),t(P,Gt),t(P,$e),t($e,Jt),h(e,Me,f),h(e,v,f),t(v,jt),t(v,be),t(be,Kt),t(v,Qt),t(v,Q),t(Q,Vt),t(v,Zt),Re=!0},p:Ur,i(e){Re||(ne(D.$$.fragment,e),ne(F.$$.fragment,e),ne(X.$$.fragment,e),ne(B.$$.fragment,e),ne(G.$$.fragment,e),ne(K.$$.fragment,e),Re=!0)},o(e){le(D.$$.fragment,e),le(F.$$.fragment,e),le(X.$$.fragment,e),le(B.$$.fragment,e),le(G.$$.fragment,e),le(K.$$.fragment,e),Re=!1},d(e){r(w),e&&r(ke),e&&r(y),se(D),e&&r(Se),e&&r(b),e&&r(Ne),e&&r(_),se(F),e&&r(xe),e&&r(S),e&&r(Le),e&&r(N),e&&r(Ce),e&&r(d),e&&r(He),e&&r(u),e&&r(qe),e&&r(I),se(X),e&&r(Oe),e&&r(L),e&&r(De),e&&r(A),se(B),e&&r(ze),e&&r(H),e&&r(Fe),e&&r(E),se(G),e&&r(Ue),e&&r(m),e&&r(Ye),e&&r(P),se(K),e&&r(Me),e&&r(v)}}}const Rr={local:"inference-api",sections:[{local:"what-technology-do-you-use-to-power-the-inference-api",title:"What technology do you use to power the inference API?"},{local:"how-can-i-turn-off-the-inference-api-for-my-model",title:"How can I turn off the inference API for my model?"},{local:"can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis",title:"Can I send large volumes of requests? Can I get accelerated APIs?"},{local:"how-can-i-see-my-usage",title:"How can I see my usage?"},{local:"is-there-programmatic-access-to-the-inference-api",title:"Is there programmatic access to the Inference API?"}],title:"Inference API"};function Tr(er){return Yr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Br extends Or{constructor(w){super();Dr(this,w,Tr,Mr,zr,{})}}export{Br as default,Rr as metadata};
