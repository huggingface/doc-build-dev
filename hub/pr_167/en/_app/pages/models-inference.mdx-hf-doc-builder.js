import{S as Ur,i as Yr,s as Mr,e as a,k as h,w as ae,t as l,N as Rr,c as o,d as r,m as p,a as n,x as oe,h as s,b as i,G as t,g as c,y as ne,L as Tr,q as le,o as se,B as ie,v as Wr}from"../chunks/vendor-hf-doc-builder.js";import{I as fe}from"../chunks/IconCopyLink-hf-doc-builder.js";function Xr(ar){let y,Ne,_,b,ce,D,rt,he,at,xe,k,ot,z,nt,lt,Ce,E,S,pe,F,st,ue,it,Le,d,ft,de,ct,ht,U,pt,ut,He,N,dt,me,mt,vt,qe,m,Y,gt,M,wt,yt,_t,ve,Et,It,ge,At,Oe,u,Pt,V,$t,bt,R,kt,St,T,Nt,xt,W,Ct,Lt,De,I,x,we,X,Ht,ye,qt,ze,C,Ot,_e,Dt,zt,Fe,A,L,Ee,B,Ft,Ie,Ut,Ue,H,Yt,Ae,Mt,Rt,Ye,P,q,Pe,G,Tt,$e,Wt,Me,v,Xt,J,Bt,Gt,j,Jt,jt,Re,$,O,be,K,Kt,ke,Qt,Te,g,Vt,Se,Zt,er,Q,tr,rr,We;return D=new fe({}),F=new fe({}),X=new fe({}),B=new fe({}),G=new fe({}),K=new fe({}),{c(){y=a("meta"),Ne=h(),_=a("h1"),b=a("a"),ce=a("span"),ae(D.$$.fragment),rt=h(),he=a("span"),at=l("Inference API"),xe=h(),k=a("p"),ot=l("Please refer to "),z=a("a"),nt=l("Accelerated Inference API Documentation"),lt=l(" for detailed information."),Ce=h(),E=a("h2"),S=a("a"),pe=a("span"),ae(F.$$.fragment),st=h(),ue=a("span"),it=l("What technology do you use to power the inference API?"),Le=h(),d=a("p"),ft=l("For \u{1F917} "),de=a("code"),ct=l("Transformers"),ht=l(" models, "),U=a("a"),pt=l("Pipelines"),ut=l("  power the API."),He=h(),N=a("p"),dt=l("On top of "),me=a("code"),mt=l("Pipelines"),vt=l(" and depending on the model type, there are several production optimizations like:"),qe=h(),m=a("ul"),Y=a("li"),gt=l("compiling models to optimized intermediary representations (e.g. "),M=a("a"),wt=l("ONNX"),yt=l("),"),_t=h(),ve=a("li"),Et=l("maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),It=h(),ge=a("li"),At=l("scaling the underlying compute infrastructure on the fly depending on the load constraints."),Oe=h(),u=a("p"),Pt=l("For models from "),V=a("a"),$t=l("other libraries"),bt=l(", the API uses "),R=a("a"),kt=l("Starlette"),St=l(" and runs in "),T=a("a"),Nt=l("Docker containers"),xt=l(". Each library defines the implementation of "),W=a("a"),Ct=l("different pipelines"),Lt=l("."),De=h(),I=a("h2"),x=a("a"),we=a("span"),ae(X.$$.fragment),Ht=h(),ye=a("span"),qt=l("How can I turn off the inference API for my model?"),ze=h(),C=a("p"),Ot=l("Specify "),_e=a("code"),Dt=l("inference: false"),zt=l(" in your model card\u2019s metadata."),Fe=h(),A=a("h2"),L=a("a"),Ee=a("span"),ae(B.$$.fragment),Ft=h(),Ie=a("span"),Ut=l("Can I send large volumes of requests? Can I get accelerated APIs?"),Ue=h(),H=a("p"),Yt=l("If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ae=a("code"),Mt=l("api-enterprise at huggingface.co"),Rt=l("."),Ye=h(),P=a("h2"),q=a("a"),Pe=a("span"),ae(G.$$.fragment),Tt=h(),$e=a("span"),Wt=l("How can I see my usage?"),Me=h(),v=a("p"),Xt=l("You can head to the "),J=a("a"),Bt=l("Inference API dashboard"),Gt=l(". Learn more about it in the "),j=a("a"),Jt=l("Inference API documentation"),jt=l("."),Re=h(),$=a("h2"),O=a("a"),be=a("span"),ae(K.$$.fragment),Kt=h(),ke=a("span"),Qt=l("Is there programmatic access to the Inference API?"),Te=h(),g=a("p"),Vt=l("Yes, the "),Se=a("code"),Zt=l("huggingface_hub"),er=l(" library has a client wrapper documented "),Q=a("a"),tr=l("here"),rr=l("."),this.h()},l(e){const f=Rr('[data-svelte="svelte-1phssyn"]',document.head);y=o(f,"META",{name:!0,content:!0}),f.forEach(r),Ne=p(e),_=o(e,"H1",{class:!0});var Xe=n(_);b=o(Xe,"A",{id:!0,class:!0,href:!0});var or=n(b);ce=o(or,"SPAN",{});var nr=n(ce);oe(D.$$.fragment,nr),nr.forEach(r),or.forEach(r),rt=p(Xe),he=o(Xe,"SPAN",{});var lr=n(he);at=s(lr,"Inference API"),lr.forEach(r),Xe.forEach(r),xe=p(e),k=o(e,"P",{});var Be=n(k);ot=s(Be,"Please refer to "),z=o(Be,"A",{href:!0,rel:!0});var sr=n(z);nt=s(sr,"Accelerated Inference API Documentation"),sr.forEach(r),lt=s(Be," for detailed information."),Be.forEach(r),Ce=p(e),E=o(e,"H2",{class:!0});var Ge=n(E);S=o(Ge,"A",{id:!0,class:!0,href:!0});var ir=n(S);pe=o(ir,"SPAN",{});var fr=n(pe);oe(F.$$.fragment,fr),fr.forEach(r),ir.forEach(r),st=p(Ge),ue=o(Ge,"SPAN",{});var cr=n(ue);it=s(cr,"What technology do you use to power the inference API?"),cr.forEach(r),Ge.forEach(r),Le=p(e),d=o(e,"P",{});var Z=n(d);ft=s(Z,"For \u{1F917} "),de=o(Z,"CODE",{});var hr=n(de);ct=s(hr,"Transformers"),hr.forEach(r),ht=s(Z," models, "),U=o(Z,"A",{href:!0,rel:!0});var pr=n(U);pt=s(pr,"Pipelines"),pr.forEach(r),ut=s(Z,"  power the API."),Z.forEach(r),He=p(e),N=o(e,"P",{});var Je=n(N);dt=s(Je,"On top of "),me=o(Je,"CODE",{});var ur=n(me);mt=s(ur,"Pipelines"),ur.forEach(r),vt=s(Je," and depending on the model type, there are several production optimizations like:"),Je.forEach(r),qe=p(e),m=o(e,"UL",{});var ee=n(m);Y=o(ee,"LI",{});var je=n(Y);gt=s(je,"compiling models to optimized intermediary representations (e.g. "),M=o(je,"A",{href:!0,rel:!0});var dr=n(M);wt=s(dr,"ONNX"),dr.forEach(r),yt=s(je,"),"),je.forEach(r),_t=p(ee),ve=o(ee,"LI",{});var mr=n(ve);Et=s(mr,"maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),mr.forEach(r),It=p(ee),ge=o(ee,"LI",{});var vr=n(ge);At=s(vr,"scaling the underlying compute infrastructure on the fly depending on the load constraints."),vr.forEach(r),ee.forEach(r),Oe=p(e),u=o(e,"P",{});var w=n(u);Pt=s(w,"For models from "),V=o(w,"A",{href:!0});var gr=n(V);$t=s(gr,"other libraries"),gr.forEach(r),bt=s(w,", the API uses "),R=o(w,"A",{href:!0,rel:!0});var wr=n(R);kt=s(wr,"Starlette"),wr.forEach(r),St=s(w," and runs in "),T=o(w,"A",{href:!0,rel:!0});var yr=n(T);Nt=s(yr,"Docker containers"),yr.forEach(r),xt=s(w,". Each library defines the implementation of "),W=o(w,"A",{href:!0,rel:!0});var _r=n(W);Ct=s(_r,"different pipelines"),_r.forEach(r),Lt=s(w,"."),w.forEach(r),De=p(e),I=o(e,"H2",{class:!0});var Ke=n(I);x=o(Ke,"A",{id:!0,class:!0,href:!0});var Er=n(x);we=o(Er,"SPAN",{});var Ir=n(we);oe(X.$$.fragment,Ir),Ir.forEach(r),Er.forEach(r),Ht=p(Ke),ye=o(Ke,"SPAN",{});var Ar=n(ye);qt=s(Ar,"How can I turn off the inference API for my model?"),Ar.forEach(r),Ke.forEach(r),ze=p(e),C=o(e,"P",{});var Qe=n(C);Ot=s(Qe,"Specify "),_e=o(Qe,"CODE",{});var Pr=n(_e);Dt=s(Pr,"inference: false"),Pr.forEach(r),zt=s(Qe," in your model card\u2019s metadata."),Qe.forEach(r),Fe=p(e),A=o(e,"H2",{class:!0});var Ve=n(A);L=o(Ve,"A",{id:!0,class:!0,href:!0});var $r=n(L);Ee=o($r,"SPAN",{});var br=n(Ee);oe(B.$$.fragment,br),br.forEach(r),$r.forEach(r),Ft=p(Ve),Ie=o(Ve,"SPAN",{});var kr=n(Ie);Ut=s(kr,"Can I send large volumes of requests? Can I get accelerated APIs?"),kr.forEach(r),Ve.forEach(r),Ue=p(e),H=o(e,"P",{});var Ze=n(H);Yt=s(Ze,"If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ae=o(Ze,"CODE",{});var Sr=n(Ae);Mt=s(Sr,"api-enterprise at huggingface.co"),Sr.forEach(r),Rt=s(Ze,"."),Ze.forEach(r),Ye=p(e),P=o(e,"H2",{class:!0});var et=n(P);q=o(et,"A",{id:!0,class:!0,href:!0});var Nr=n(q);Pe=o(Nr,"SPAN",{});var xr=n(Pe);oe(G.$$.fragment,xr),xr.forEach(r),Nr.forEach(r),Tt=p(et),$e=o(et,"SPAN",{});var Cr=n($e);Wt=s(Cr,"How can I see my usage?"),Cr.forEach(r),et.forEach(r),Me=p(e),v=o(e,"P",{});var te=n(v);Xt=s(te,"You can head to the "),J=o(te,"A",{href:!0,rel:!0});var Lr=n(J);Bt=s(Lr,"Inference API dashboard"),Lr.forEach(r),Gt=s(te,". Learn more about it in the "),j=o(te,"A",{href:!0,rel:!0});var Hr=n(j);Jt=s(Hr,"Inference API documentation"),Hr.forEach(r),jt=s(te,"."),te.forEach(r),Re=p(e),$=o(e,"H2",{class:!0});var tt=n($);O=o(tt,"A",{id:!0,class:!0,href:!0});var qr=n(O);be=o(qr,"SPAN",{});var Or=n(be);oe(K.$$.fragment,Or),Or.forEach(r),qr.forEach(r),Kt=p(tt),ke=o(tt,"SPAN",{});var Dr=n(ke);Qt=s(Dr,"Is there programmatic access to the Inference API?"),Dr.forEach(r),tt.forEach(r),Te=p(e),g=o(e,"P",{});var re=n(g);Vt=s(re,"Yes, the "),Se=o(re,"CODE",{});var zr=n(Se);Zt=s(zr,"huggingface_hub"),zr.forEach(r),er=s(re," library has a client wrapper documented "),Q=o(re,"A",{href:!0,rel:!0});var Fr=n(Q);tr=s(Fr,"here"),Fr.forEach(r),rr=s(re,"."),re.forEach(r),this.h()},h(){i(y,"name","hf:doc:metadata"),i(y,"content",JSON.stringify(Br)),i(b,"id","inference-api"),i(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(b,"href","#inference-api"),i(_,"class","relative group"),i(z,"href","https://api-inference.huggingface.co/docs/python/html/index.html"),i(z,"rel","nofollow"),i(S,"id","what-technology-do-you-use-to-power-the-inference-api"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#what-technology-do-you-use-to-power-the-inference-api"),i(E,"class","relative group"),i(U,"href","https://huggingface.co/transformers/main_classes/pipelines.html"),i(U,"rel","nofollow"),i(M,"href","https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333"),i(M,"rel","nofollow"),i(V,"href","/docs/hub/libraries"),i(R,"href","https://www.starlette.io"),i(R,"rel","nofollow"),i(T,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images"),i(T,"rel","nofollow"),i(W,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines"),i(W,"rel","nofollow"),i(x,"id","how-can-i-turn-off-the-inference-api-for-my-model"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#how-can-i-turn-off-the-inference-api-for-my-model"),i(I,"class","relative group"),i(L,"id","can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(L,"href","#can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(A,"class","relative group"),i(q,"id","how-can-i-see-my-usage"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-can-i-see-my-usage"),i(P,"class","relative group"),i(J,"href","https://api-inference.huggingface.co/dashboard/"),i(J,"rel","nofollow"),i(j,"href","https://api-inference.huggingface.co/docs/python/html/usage.html#api-usage-dashboard"),i(j,"rel","nofollow"),i(O,"id","is-there-programmatic-access-to-the-inference-api"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#is-there-programmatic-access-to-the-inference-api"),i($,"class","relative group"),i(Q,"href","https://huggingface.co/docs/huggingface_hub/how-to-inference"),i(Q,"rel","nofollow")},m(e,f){t(document.head,y),c(e,Ne,f),c(e,_,f),t(_,b),t(b,ce),ne(D,ce,null),t(_,rt),t(_,he),t(he,at),c(e,xe,f),c(e,k,f),t(k,ot),t(k,z),t(z,nt),t(k,lt),c(e,Ce,f),c(e,E,f),t(E,S),t(S,pe),ne(F,pe,null),t(E,st),t(E,ue),t(ue,it),c(e,Le,f),c(e,d,f),t(d,ft),t(d,de),t(de,ct),t(d,ht),t(d,U),t(U,pt),t(d,ut),c(e,He,f),c(e,N,f),t(N,dt),t(N,me),t(me,mt),t(N,vt),c(e,qe,f),c(e,m,f),t(m,Y),t(Y,gt),t(Y,M),t(M,wt),t(Y,yt),t(m,_t),t(m,ve),t(ve,Et),t(m,It),t(m,ge),t(ge,At),c(e,Oe,f),c(e,u,f),t(u,Pt),t(u,V),t(V,$t),t(u,bt),t(u,R),t(R,kt),t(u,St),t(u,T),t(T,Nt),t(u,xt),t(u,W),t(W,Ct),t(u,Lt),c(e,De,f),c(e,I,f),t(I,x),t(x,we),ne(X,we,null),t(I,Ht),t(I,ye),t(ye,qt),c(e,ze,f),c(e,C,f),t(C,Ot),t(C,_e),t(_e,Dt),t(C,zt),c(e,Fe,f),c(e,A,f),t(A,L),t(L,Ee),ne(B,Ee,null),t(A,Ft),t(A,Ie),t(Ie,Ut),c(e,Ue,f),c(e,H,f),t(H,Yt),t(H,Ae),t(Ae,Mt),t(H,Rt),c(e,Ye,f),c(e,P,f),t(P,q),t(q,Pe),ne(G,Pe,null),t(P,Tt),t(P,$e),t($e,Wt),c(e,Me,f),c(e,v,f),t(v,Xt),t(v,J),t(J,Bt),t(v,Gt),t(v,j),t(j,Jt),t(v,jt),c(e,Re,f),c(e,$,f),t($,O),t(O,be),ne(K,be,null),t($,Kt),t($,ke),t(ke,Qt),c(e,Te,f),c(e,g,f),t(g,Vt),t(g,Se),t(Se,Zt),t(g,er),t(g,Q),t(Q,tr),t(g,rr),We=!0},p:Tr,i(e){We||(le(D.$$.fragment,e),le(F.$$.fragment,e),le(X.$$.fragment,e),le(B.$$.fragment,e),le(G.$$.fragment,e),le(K.$$.fragment,e),We=!0)},o(e){se(D.$$.fragment,e),se(F.$$.fragment,e),se(X.$$.fragment,e),se(B.$$.fragment,e),se(G.$$.fragment,e),se(K.$$.fragment,e),We=!1},d(e){r(y),e&&r(Ne),e&&r(_),ie(D),e&&r(xe),e&&r(k),e&&r(Ce),e&&r(E),ie(F),e&&r(Le),e&&r(d),e&&r(He),e&&r(N),e&&r(qe),e&&r(m),e&&r(Oe),e&&r(u),e&&r(De),e&&r(I),ie(X),e&&r(ze),e&&r(C),e&&r(Fe),e&&r(A),ie(B),e&&r(Ue),e&&r(H),e&&r(Ye),e&&r(P),ie(G),e&&r(Me),e&&r(v),e&&r(Re),e&&r($),ie(K),e&&r(Te),e&&r(g)}}}const Br={local:"inference-api",sections:[{local:"what-technology-do-you-use-to-power-the-inference-api",title:"What technology do you use to power the inference API?"},{local:"how-can-i-turn-off-the-inference-api-for-my-model",title:"How can I turn off the inference API for my model?"},{local:"can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis",title:"Can I send large volumes of requests? Can I get accelerated APIs?"},{local:"how-can-i-see-my-usage",title:"How can I see my usage?"},{local:"is-there-programmatic-access-to-the-inference-api",title:"Is there programmatic access to the Inference API?"}],title:"Inference API"};function Gr(ar){return Wr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kr extends Ur{constructor(y){super();Yr(this,y,Gr,Xr,Mr,{})}}export{Kr as default,Br as metadata};
