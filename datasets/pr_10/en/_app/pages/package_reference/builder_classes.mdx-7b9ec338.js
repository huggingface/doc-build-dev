import{S as Nl,i as jl,s as Rl,e as s,k as d,w as p,t as o,M as Cl,c as n,d as t,m as l,a as r,x as f,h as i,b as c,F as e,g as v,y as u,L as kl,q as g,o as h,B as m,v as Ol}from"../../chunks/vendor-8138ceec.js";import{D as b}from"../../chunks/Docstring-1fc0f9df.js";import{C as qt}from"../../chunks/CodeBlock-fc89709f.js";import{I as Al}from"../../chunks/IconCopyLink-2dd3a6ac.js";function Ll(td){let q,fs,H,ne,Ht,Ee,en,Gt,tn,us,P,an,ft,sn,nn,ut,rn,on,gs,$,De,dn,Wt,ln,cn,gt,Xt,pn,fn,un,G,ht,zt,gn,hn,mn,mt,_t,_n,vn,bn,re,vt,$n,wn,Jt,En,Dn,xn,N,Kt,yn,Tn,Yt,In,Bn,Qt,Sn,Nn,Zt,jn,Rn,Cn,oe,xe,kn,ea,On,An,ie,ye,Ln,ta,Pn,Vn,de,Te,Fn,aa,Mn,Un,le,Ie,qn,sa,Hn,Gn,ce,Be,Wn,na,Xn,hs,k,Se,zn,ra,Jn,Kn,V,oa,Yn,Qn,ia,Zn,er,da,tr,ar,ms,W,Ne,sr,la,nr,_s,X,je,rr,ca,or,vs,S,Re,ir,Ce,dr,bt,lr,cr,pr,ke,fr,$t,ur,gr,hr,F,Oe,mr,pa,_r,vr,z,fa,br,$r,ua,wr,Er,ga,Dr,bs,E,Ae,xr,pe,Le,yr,ha,Tr,Ir,j,Pe,Br,ma,Sr,Nr,_a,jr,Rr,Ve,Cr,fe,Fe,kr,Me,Or,va,Ar,Lr,Pr,ue,Ue,Vr,ba,Fr,Mr,ge,qe,Ur,$a,qr,Hr,he,He,Gr,wa,Wr,Xr,me,Ge,zr,Ea,Jr,$s,B,We,Kr,wt,Da,Yr,Qr,Zr,Xe,eo,xa,to,ao,so,ya,no,ro,ze,Ta,J,ws,oo,Ia,io,lo,Ba,co,po,K,Y,Et,Sa,fo,uo,go,Na,ho,mo,ja,_o,vo,Q,Ra,Ca,bo,$o,ka,wo,Eo,Oa,Do,xo,Z,Aa,La,yo,To,Pa,Io,Bo,Va,So,Es,O,Je,No,Fa,jo,Ro,ee,Co,Ma,ko,Oo,Ua,Ao,Lo,Ds,y,Ke,Po,Dt,qa,Vo,Fo,Mo,Ha,Uo,qo,A,xt,Ga,Ho,Go,Wo,yt,Wa,Xo,zo,Jo,Tt,Xa,Ko,Yo,Qo,It,za,Zo,ei,ti,Bt,ai,Ja,si,ni,Ye,ri,Ka,oi,ii,xs,w,Qe,di,Ya,li,ci,Qa,pi,fi,Ze,ui,Za,gi,hi,et,mi,es,_i,vi,tt,bi,ts,$i,wi,at,ys,te,st,Ei,as,Di,Ts,T,nt,xi,ss,yi,Ti,ns,Ii,Bi,rt,Si,_e,ot,Ni,rs,ji,Ri,M,it,Ci,os,ki,Oi,is,Ai,Is,ae,dt,Li,ds,Pi,Bs,L,lt,Vi,ls,Fi,Mi,ve,ct,Ui,cs,qi,Ss;return Ee=new Al({}),De=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L177"}}),xe=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L744",parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L487",parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}]}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L320"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L328"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L482"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L1012"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L1172"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L1110"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L81",parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}]}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/builder.py#L120"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new qt({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ue=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L310",parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}]}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L326",parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}]}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/splits.py#L549",parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}]}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/splits.py#L303"}}),Ze=new qt({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new qt({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new qt({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new qt({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/splits.py#L372"}}),nt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/arrow_reader.py#L456"}}),rt=new qt({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),ot=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/arrow_reader.py#L536",parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],returnDescription:`
<p>ReadInstruction instance.</p>
`}}),it=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),dt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/file_utils.py#L153",parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}]}}),lt=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/version.py#L30",parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}]}}),ct=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_10/src/datasets/utils/version.py#L92"}}),{c(){q=s("meta"),fs=d(),H=s("h1"),ne=s("a"),Ht=s("span"),p(Ee.$$.fragment),en=d(),Gt=s("span"),tn=o("Builder classes"),us=d(),P=s("p"),an=o("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=s("a"),sn=o("DatasetBuilder"),nn=o(" and "),ut=s("a"),rn=o("BuilderConfig"),on=o("."),gs=d(),$=s("div"),p(De.$$.fragment),dn=d(),Wt=s("p"),ln=o("Abstract base class for all datasets."),cn=d(),gt=s("p"),Xt=s("em"),pn=o("DatasetBuilder"),fn=o(" has 3 key methods:"),un=d(),G=s("ul"),ht=s("li"),zt=s("code"),gn=o("datasets.DatasetBuilder.info"),hn=o(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),mn=d(),mt=s("li"),_t=s("a"),_n=o("datasets.DatasetBuilder.download_and_prepare()"),vn=o(`: Downloads the source data
and writes it to disk.`),bn=d(),re=s("li"),vt=s("a"),$n=o("datasets.DatasetBuilder.as_dataset()"),wn=o(": Generates a "),Jt=s("em"),En=o("Dataset"),Dn=o("."),xn=d(),N=s("p"),Kt=s("strong"),yn=o("Configuration"),Tn=o(": Some "),Yt=s("em"),In=o("DatasetBuilder"),Bn=o(`s expose multiple variants of the
dataset by defining a `),Qt=s("em"),Sn=o("datasets.BuilderConfig"),Nn=o(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=s("code"),jn=o("datasets.DatasetBuilder.builder_configs()"),Rn=o("."),Cn=d(),oe=s("div"),p(xe.$$.fragment),kn=d(),ea=s("p"),On=o("Return a Dataset for the specified split."),An=d(),ie=s("div"),p(ye.$$.fragment),Ln=d(),ta=s("p"),Pn=o("Downloads and prepares dataset for reading."),Vn=d(),de=s("div"),p(Te.$$.fragment),Fn=d(),aa=s("p"),Mn=o("Empty dict if doesn\u2019t exist"),Un=d(),le=s("div"),p(Ie.$$.fragment),qn=d(),sa=s("p"),Hn=o("Empty DatasetInfo if doesn\u2019t exist"),Gn=d(),ce=s("div"),p(Be.$$.fragment),Wn=d(),na=s("p"),Xn=o("Return the path of the module of this class or subclass."),hs=d(),k=s("div"),p(Se.$$.fragment),zn=d(),ra=s("p"),Jn=o("Base class for datasets with data generation based on dict generators."),Kn=d(),V=s("p"),oa=s("code"),Yn=o("GeneratorBasedBuilder"),Qn=o(` is a convenience class that abstracts away much
of the data writing and reading of `),ia=s("code"),Zn=o("DatasetBuilder"),er=o(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=s("code"),tr=o("_split_generators"),ar=o("). See the method docstrings for details."),ms=d(),W=s("div"),p(Ne.$$.fragment),sr=d(),la=s("p"),nr=o("Beam based Builder."),_s=d(),X=s("div"),p(je.$$.fragment),rr=d(),ca=s("p"),or=o("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=d(),S=s("div"),p(Re.$$.fragment),ir=d(),Ce=s("p"),dr=o("Base class for "),bt=s("a"),lr=o("DatasetBuilder"),cr=o(" data configuration."),pr=d(),ke=s("p"),fr=o(`DatasetBuilder subclasses with data configuration options should subclass
`),$t=s("a"),ur=o("BuilderConfig"),gr=o(" and add their own properties."),hr=d(),F=s("div"),p(Oe.$$.fragment),mr=d(),pa=s("p"),_r=o(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),vr=d(),z=s("ul"),fa=s("li"),br=o("the config kwargs that can be used to overwrite attributes"),$r=d(),ua=s("li"),wr=o("the custom features used to write the dataset"),Er=d(),ga=s("li"),Dr=o(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=d(),E=s("div"),p(Ae.$$.fragment),xr=d(),pe=s("div"),p(Le.$$.fragment),yr=d(),ha=s("p"),Tr=o("Download given url(s)."),Ir=d(),j=s("div"),p(Pe.$$.fragment),Br=d(),ma=s("p"),Sr=o("Download and extract given url_or_urls."),Nr=d(),_a=s("p"),jr=o("Is roughly equivalent to:"),Rr=d(),p(Ve.$$.fragment),Cr=d(),fe=s("div"),p(Fe.$$.fragment),kr=d(),Me=s("p"),Or=o("Download given urls(s) by calling "),va=s("code"),Ar=o("custom_download"),Lr=o("."),Pr=d(),ue=s("div"),p(Ue.$$.fragment),Vr=d(),ba=s("p"),Fr=o("Extract given path(s)."),Mr=d(),ge=s("div"),p(qe.$$.fragment),Ur=d(),$a=s("p"),qr=o("Iterate over files within an archive."),Hr=d(),he=s("div"),p(He.$$.fragment),Gr=d(),wa=s("p"),Wr=o("Iterate over file paths."),Xr=d(),me=s("div"),p(Ge.$$.fragment),zr=d(),Ea=s("p"),Jr=o("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=d(),B=s("div"),p(We.$$.fragment),Kr=d(),wt=s("p"),Da=s("code"),Yr=o("Enum"),Qr=o(" for how to treat pre-existing downloads and data."),Zr=d(),Xe=s("p"),eo=o("The default mode is "),xa=s("code"),to=o("REUSE_DATASET_IF_EXISTS"),ao=o(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),so=d(),ya=s("p"),no=o("The generations modes:"),ro=d(),ze=s("table"),Ta=s("thead"),J=s("tr"),ws=s("th"),oo=d(),Ia=s("th"),io=o("Downloads"),lo=d(),Ba=s("th"),co=o("Dataset"),po=d(),K=s("tbody"),Y=s("tr"),Et=s("td"),Sa=s("code"),fo=o("REUSE_DATASET_IF_EXISTS"),uo=o(" (default)"),go=d(),Na=s("td"),ho=o("Reuse"),mo=d(),ja=s("td"),_o=o("Reuse"),vo=d(),Q=s("tr"),Ra=s("td"),Ca=s("code"),bo=o("REUSE_CACHE_IF_EXISTS"),$o=d(),ka=s("td"),wo=o("Reuse"),Eo=d(),Oa=s("td"),Do=o("Fresh"),xo=d(),Z=s("tr"),Aa=s("td"),La=s("code"),yo=o("FORCE_REDOWNLOAD"),To=d(),Pa=s("td"),Io=o("Fresh"),Bo=d(),Va=s("td"),So=o("Fresh"),Es=d(),O=s("div"),p(Je.$$.fragment),No=d(),Fa=s("p"),jo=o("Defines the split information for the generator."),Ro=d(),ee=s("p"),Co=o(`This should be used as returned value of
`),Ma=s("code"),ko=o("GeneratorBasedBuilder._split_generators()"),Oo=o(`.
See `),Ua=s("code"),Ao=o("GeneratorBasedBuilder._split_generators()"),Lo=o(` for more info and example
of usage.`),Ds=d(),y=s("div"),p(Ke.$$.fragment),Po=d(),Dt=s("p"),qa=s("code"),Vo=o("Enum"),Fo=o(" for dataset splits."),Mo=d(),Ha=s("p"),Uo=o(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),qo=d(),A=s("ul"),xt=s("li"),Ga=s("code"),Ho=o("TRAIN"),Go=o(": the training data."),Wo=d(),yt=s("li"),Wa=s("code"),Xo=o("VALIDATION"),zo=o(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Jo=d(),Tt=s("li"),Xa=s("code"),Ko=o("TEST"),Yo=o(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qo=d(),It=s("li"),za=s("code"),Zo=o("ALL"),ei=o(": the union of all defined dataset splits."),ti=d(),Bt=s("p"),ai=o("Note: All splits, including compositions inherit from "),Ja=s("code"),si=o("datasets.SplitBase"),ni=d(),Ye=s("p"),ri=o("See the :doc:"),Ka=s("code"),oi=o("guide on splits </loading>"),ii=o(" for more information."),xs=d(),w=s("div"),p(Qe.$$.fragment),di=d(),Ya=s("p"),li=o("Descriptor corresponding to a named split (train, test, \u2026)."),ci=d(),Qa=s("p"),pi=o("Example:"),fi=d(),p(Ze.$$.fragment),ui=d(),Za=s("p"),gi=o(`Warning:
A split cannot be added twice, so the following will fail:`),hi=d(),p(et.$$.fragment),mi=d(),es=s("p"),_i=o(`Warning:
The slices can be applied only one time. So the following are valid:`),vi=d(),p(tt.$$.fragment),bi=d(),ts=s("p"),$i=o("But not:"),wi=d(),p(at.$$.fragment),ys=d(),te=s("div"),p(st.$$.fragment),Ei=d(),as=s("p"),Di=o("Split corresponding to the union of all defined dataset splits."),Ts=d(),T=s("div"),p(nt.$$.fragment),xi=d(),ss=s("p"),yi=o("Reading instruction for a dataset."),Ti=d(),ns=s("p"),Ii=o("Examples:"),Bi=d(),p(rt.$$.fragment),Si=d(),_e=s("div"),p(ot.$$.fragment),Ni=d(),rs=s("p"),ji=o("Creates a ReadInstruction instance out of a string spec."),Ri=d(),M=s("div"),p(it.$$.fragment),Ci=d(),os=s("p"),ki=o("Translate instruction into a list of absolute instructions."),Oi=d(),is=s("p"),Ai=o("Those absolute instructions are then to be added together."),Is=d(),ae=s("div"),p(dt.$$.fragment),Li=d(),ds=s("p"),Pi=o("Configuration for our cached path manager."),Bs=d(),L=s("div"),p(lt.$$.fragment),Vi=d(),ls=s("p"),Fi=o("Dataset version MAJOR.MINOR.PATCH."),Mi=d(),ve=s("div"),p(ct.$$.fragment),Ui=d(),cs=s("p"),qi=o("Returns True if other_version matches."),this.h()},l(a){const _=Cl('[data-svelte="svelte-1phssyn"]',document.head);q=n(_,"META",{name:!0,content:!0}),_.forEach(t),fs=l(a),H=n(a,"H1",{class:!0});var Ns=r(H);ne=n(Ns,"A",{id:!0,class:!0,href:!0});var ad=r(ne);Ht=n(ad,"SPAN",{});var sd=r(Ht);f(Ee.$$.fragment,sd),sd.forEach(t),ad.forEach(t),en=l(Ns),Gt=n(Ns,"SPAN",{});var nd=r(Gt);tn=i(nd,"Builder classes"),nd.forEach(t),Ns.forEach(t),us=l(a),P=n(a,"P",{});var St=r(P);an=i(St,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=n(St,"A",{href:!0});var rd=r(ft);sn=i(rd,"DatasetBuilder"),rd.forEach(t),nn=i(St," and "),ut=n(St,"A",{href:!0});var od=r(ut);rn=i(od,"BuilderConfig"),od.forEach(t),on=i(St,"."),St.forEach(t),gs=l(a),$=n(a,"DIV",{class:!0});var D=r($);f(De.$$.fragment,D),dn=l(D),Wt=n(D,"P",{});var id=r(Wt);ln=i(id,"Abstract base class for all datasets."),id.forEach(t),cn=l(D),gt=n(D,"P",{});var Hi=r(gt);Xt=n(Hi,"EM",{});var dd=r(Xt);pn=i(dd,"DatasetBuilder"),dd.forEach(t),fn=i(Hi," has 3 key methods:"),Hi.forEach(t),un=l(D),G=n(D,"UL",{});var Nt=r(G);ht=n(Nt,"LI",{});var Gi=r(ht);zt=n(Gi,"CODE",{});var ld=r(zt);gn=i(ld,"datasets.DatasetBuilder.info"),ld.forEach(t),hn=i(Gi,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Gi.forEach(t),mn=l(Nt),mt=n(Nt,"LI",{});var Wi=r(mt);_t=n(Wi,"A",{href:!0});var cd=r(_t);_n=i(cd,"datasets.DatasetBuilder.download_and_prepare()"),cd.forEach(t),vn=i(Wi,`: Downloads the source data
and writes it to disk.`),Wi.forEach(t),bn=l(Nt),re=n(Nt,"LI",{});var ps=r(re);vt=n(ps,"A",{href:!0});var pd=r(vt);$n=i(pd,"datasets.DatasetBuilder.as_dataset()"),pd.forEach(t),wn=i(ps,": Generates a "),Jt=n(ps,"EM",{});var fd=r(Jt);En=i(fd,"Dataset"),fd.forEach(t),Dn=i(ps,"."),ps.forEach(t),Nt.forEach(t),xn=l(D),N=n(D,"P",{});var se=r(N);Kt=n(se,"STRONG",{});var ud=r(Kt);yn=i(ud,"Configuration"),ud.forEach(t),Tn=i(se,": Some "),Yt=n(se,"EM",{});var gd=r(Yt);In=i(gd,"DatasetBuilder"),gd.forEach(t),Bn=i(se,`s expose multiple variants of the
dataset by defining a `),Qt=n(se,"EM",{});var hd=r(Qt);Sn=i(hd,"datasets.BuilderConfig"),hd.forEach(t),Nn=i(se,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=n(se,"CODE",{});var md=r(Zt);jn=i(md,"datasets.DatasetBuilder.builder_configs()"),md.forEach(t),Rn=i(se,"."),se.forEach(t),Cn=l(D),oe=n(D,"DIV",{class:!0});var js=r(oe);f(xe.$$.fragment,js),kn=l(js),ea=n(js,"P",{});var _d=r(ea);On=i(_d,"Return a Dataset for the specified split."),_d.forEach(t),js.forEach(t),An=l(D),ie=n(D,"DIV",{class:!0});var Rs=r(ie);f(ye.$$.fragment,Rs),Ln=l(Rs),ta=n(Rs,"P",{});var vd=r(ta);Pn=i(vd,"Downloads and prepares dataset for reading."),vd.forEach(t),Rs.forEach(t),Vn=l(D),de=n(D,"DIV",{class:!0});var Cs=r(de);f(Te.$$.fragment,Cs),Fn=l(Cs),aa=n(Cs,"P",{});var bd=r(aa);Mn=i(bd,"Empty dict if doesn\u2019t exist"),bd.forEach(t),Cs.forEach(t),Un=l(D),le=n(D,"DIV",{class:!0});var ks=r(le);f(Ie.$$.fragment,ks),qn=l(ks),sa=n(ks,"P",{});var $d=r(sa);Hn=i($d,"Empty DatasetInfo if doesn\u2019t exist"),$d.forEach(t),ks.forEach(t),Gn=l(D),ce=n(D,"DIV",{class:!0});var Os=r(ce);f(Be.$$.fragment,Os),Wn=l(Os),na=n(Os,"P",{});var wd=r(na);Xn=i(wd,"Return the path of the module of this class or subclass."),wd.forEach(t),Os.forEach(t),D.forEach(t),hs=l(a),k=n(a,"DIV",{class:!0});var jt=r(k);f(Se.$$.fragment,jt),zn=l(jt),ra=n(jt,"P",{});var Ed=r(ra);Jn=i(Ed,"Base class for datasets with data generation based on dict generators."),Ed.forEach(t),Kn=l(jt),V=n(jt,"P",{});var pt=r(V);oa=n(pt,"CODE",{});var Dd=r(oa);Yn=i(Dd,"GeneratorBasedBuilder"),Dd.forEach(t),Qn=i(pt,` is a convenience class that abstracts away much
of the data writing and reading of `),ia=n(pt,"CODE",{});var xd=r(ia);Zn=i(xd,"DatasetBuilder"),xd.forEach(t),er=i(pt,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=n(pt,"CODE",{});var yd=r(da);tr=i(yd,"_split_generators"),yd.forEach(t),ar=i(pt,"). See the method docstrings for details."),pt.forEach(t),jt.forEach(t),ms=l(a),W=n(a,"DIV",{class:!0});var As=r(W);f(Ne.$$.fragment,As),sr=l(As),la=n(As,"P",{});var Td=r(la);nr=i(Td,"Beam based Builder."),Td.forEach(t),As.forEach(t),_s=l(a),X=n(a,"DIV",{class:!0});var Ls=r(X);f(je.$$.fragment,Ls),rr=l(Ls),ca=n(Ls,"P",{});var Id=r(ca);or=i(Id,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Id.forEach(t),Ls.forEach(t),vs=l(a),S=n(a,"DIV",{class:!0});var be=r(S);f(Re.$$.fragment,be),ir=l(be),Ce=n(be,"P",{});var Ps=r(Ce);dr=i(Ps,"Base class for "),bt=n(Ps,"A",{href:!0});var Bd=r(bt);lr=i(Bd,"DatasetBuilder"),Bd.forEach(t),cr=i(Ps," data configuration."),Ps.forEach(t),pr=l(be),ke=n(be,"P",{});var Vs=r(ke);fr=i(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),$t=n(Vs,"A",{href:!0});var Sd=r($t);ur=i(Sd,"BuilderConfig"),Sd.forEach(t),gr=i(Vs," and add their own properties."),Vs.forEach(t),hr=l(be),F=n(be,"DIV",{class:!0});var Rt=r(F);f(Oe.$$.fragment,Rt),mr=l(Rt),pa=n(Rt,"P",{});var Nd=r(pa);_r=i(Nd,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Nd.forEach(t),vr=l(Rt),z=n(Rt,"UL",{});var Ct=r(z);fa=n(Ct,"LI",{});var jd=r(fa);br=i(jd,"the config kwargs that can be used to overwrite attributes"),jd.forEach(t),$r=l(Ct),ua=n(Ct,"LI",{});var Rd=r(ua);wr=i(Rd,"the custom features used to write the dataset"),Rd.forEach(t),Er=l(Ct),ga=n(Ct,"LI",{});var Cd=r(ga);Dr=i(Cd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Cd.forEach(t),Ct.forEach(t),Rt.forEach(t),be.forEach(t),bs=l(a),E=n(a,"DIV",{class:!0});var I=r(E);f(Ae.$$.fragment,I),xr=l(I),pe=n(I,"DIV",{class:!0});var Fs=r(pe);f(Le.$$.fragment,Fs),yr=l(Fs),ha=n(Fs,"P",{});var kd=r(ha);Tr=i(kd,"Download given url(s)."),kd.forEach(t),Fs.forEach(t),Ir=l(I),j=n(I,"DIV",{class:!0});var $e=r(j);f(Pe.$$.fragment,$e),Br=l($e),ma=n($e,"P",{});var Od=r(ma);Sr=i(Od,"Download and extract given url_or_urls."),Od.forEach(t),Nr=l($e),_a=n($e,"P",{});var Ad=r(_a);jr=i(Ad,"Is roughly equivalent to:"),Ad.forEach(t),Rr=l($e),f(Ve.$$.fragment,$e),$e.forEach(t),Cr=l(I),fe=n(I,"DIV",{class:!0});var Ms=r(fe);f(Fe.$$.fragment,Ms),kr=l(Ms),Me=n(Ms,"P",{});var Us=r(Me);Or=i(Us,"Download given urls(s) by calling "),va=n(Us,"CODE",{});var Ld=r(va);Ar=i(Ld,"custom_download"),Ld.forEach(t),Lr=i(Us,"."),Us.forEach(t),Ms.forEach(t),Pr=l(I),ue=n(I,"DIV",{class:!0});var qs=r(ue);f(Ue.$$.fragment,qs),Vr=l(qs),ba=n(qs,"P",{});var Pd=r(ba);Fr=i(Pd,"Extract given path(s)."),Pd.forEach(t),qs.forEach(t),Mr=l(I),ge=n(I,"DIV",{class:!0});var Hs=r(ge);f(qe.$$.fragment,Hs),Ur=l(Hs),$a=n(Hs,"P",{});var Vd=r($a);qr=i(Vd,"Iterate over files within an archive."),Vd.forEach(t),Hs.forEach(t),Hr=l(I),he=n(I,"DIV",{class:!0});var Gs=r(he);f(He.$$.fragment,Gs),Gr=l(Gs),wa=n(Gs,"P",{});var Fd=r(wa);Wr=i(Fd,"Iterate over file paths."),Fd.forEach(t),Gs.forEach(t),Xr=l(I),me=n(I,"DIV",{class:!0});var Ws=r(me);f(Ge.$$.fragment,Ws),zr=l(Ws),Ea=n(Ws,"P",{});var Md=r(Ea);Jr=i(Md,"Ship the files using Beam FileSystems to the pipeline temp dir."),Md.forEach(t),Ws.forEach(t),I.forEach(t),$s=l(a),B=n(a,"DIV",{class:!0});var U=r(B);f(We.$$.fragment,U),Kr=l(U),wt=n(U,"P",{});var Xi=r(wt);Da=n(Xi,"CODE",{});var Ud=r(Da);Yr=i(Ud,"Enum"),Ud.forEach(t),Qr=i(Xi," for how to treat pre-existing downloads and data."),Xi.forEach(t),Zr=l(U),Xe=n(U,"P",{});var Xs=r(Xe);eo=i(Xs,"The default mode is "),xa=n(Xs,"CODE",{});var qd=r(xa);to=i(qd,"REUSE_DATASET_IF_EXISTS"),qd.forEach(t),ao=i(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),so=l(U),ya=n(U,"P",{});var Hd=r(ya);no=i(Hd,"The generations modes:"),Hd.forEach(t),ro=l(U),ze=n(U,"TABLE",{});var zs=r(ze);Ta=n(zs,"THEAD",{});var Gd=r(Ta);J=n(Gd,"TR",{});var kt=r(J);ws=n(kt,"TH",{}),r(ws).forEach(t),oo=l(kt),Ia=n(kt,"TH",{});var Wd=r(Ia);io=i(Wd,"Downloads"),Wd.forEach(t),lo=l(kt),Ba=n(kt,"TH",{});var Xd=r(Ba);co=i(Xd,"Dataset"),Xd.forEach(t),kt.forEach(t),Gd.forEach(t),po=l(zs),K=n(zs,"TBODY",{});var Ot=r(K);Y=n(Ot,"TR",{});var At=r(Y);Et=n(At,"TD",{});var zi=r(Et);Sa=n(zi,"CODE",{});var zd=r(Sa);fo=i(zd,"REUSE_DATASET_IF_EXISTS"),zd.forEach(t),uo=i(zi," (default)"),zi.forEach(t),go=l(At),Na=n(At,"TD",{});var Jd=r(Na);ho=i(Jd,"Reuse"),Jd.forEach(t),mo=l(At),ja=n(At,"TD",{});var Kd=r(ja);_o=i(Kd,"Reuse"),Kd.forEach(t),At.forEach(t),vo=l(Ot),Q=n(Ot,"TR",{});var Lt=r(Q);Ra=n(Lt,"TD",{});var Yd=r(Ra);Ca=n(Yd,"CODE",{});var Qd=r(Ca);bo=i(Qd,"REUSE_CACHE_IF_EXISTS"),Qd.forEach(t),Yd.forEach(t),$o=l(Lt),ka=n(Lt,"TD",{});var Zd=r(ka);wo=i(Zd,"Reuse"),Zd.forEach(t),Eo=l(Lt),Oa=n(Lt,"TD",{});var el=r(Oa);Do=i(el,"Fresh"),el.forEach(t),Lt.forEach(t),xo=l(Ot),Z=n(Ot,"TR",{});var Pt=r(Z);Aa=n(Pt,"TD",{});var tl=r(Aa);La=n(tl,"CODE",{});var al=r(La);yo=i(al,"FORCE_REDOWNLOAD"),al.forEach(t),tl.forEach(t),To=l(Pt),Pa=n(Pt,"TD",{});var sl=r(Pa);Io=i(sl,"Fresh"),sl.forEach(t),Bo=l(Pt),Va=n(Pt,"TD",{});var nl=r(Va);So=i(nl,"Fresh"),nl.forEach(t),Pt.forEach(t),Ot.forEach(t),zs.forEach(t),U.forEach(t),Es=l(a),O=n(a,"DIV",{class:!0});var Vt=r(O);f(Je.$$.fragment,Vt),No=l(Vt),Fa=n(Vt,"P",{});var rl=r(Fa);jo=i(rl,"Defines the split information for the generator."),rl.forEach(t),Ro=l(Vt),ee=n(Vt,"P",{});var Ft=r(ee);Co=i(Ft,`This should be used as returned value of
`),Ma=n(Ft,"CODE",{});var ol=r(Ma);ko=i(ol,"GeneratorBasedBuilder._split_generators()"),ol.forEach(t),Oo=i(Ft,`.
See `),Ua=n(Ft,"CODE",{});var il=r(Ua);Ao=i(il,"GeneratorBasedBuilder._split_generators()"),il.forEach(t),Lo=i(Ft,` for more info and example
of usage.`),Ft.forEach(t),Vt.forEach(t),Ds=l(a),y=n(a,"DIV",{class:!0});var R=r(y);f(Ke.$$.fragment,R),Po=l(R),Dt=n(R,"P",{});var Ji=r(Dt);qa=n(Ji,"CODE",{});var dl=r(qa);Vo=i(dl,"Enum"),dl.forEach(t),Fo=i(Ji," for dataset splits."),Ji.forEach(t),Mo=l(R),Ha=n(R,"P",{});var ll=r(Ha);Uo=i(ll,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),ll.forEach(t),qo=l(R),A=n(R,"UL",{});var we=r(A);xt=n(we,"LI",{});var Ki=r(xt);Ga=n(Ki,"CODE",{});var cl=r(Ga);Ho=i(cl,"TRAIN"),cl.forEach(t),Go=i(Ki,": the training data."),Ki.forEach(t),Wo=l(we),yt=n(we,"LI",{});var Yi=r(yt);Wa=n(Yi,"CODE",{});var pl=r(Wa);Xo=i(pl,"VALIDATION"),pl.forEach(t),zo=i(Yi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Yi.forEach(t),Jo=l(we),Tt=n(we,"LI",{});var Qi=r(Tt);Xa=n(Qi,"CODE",{});var fl=r(Xa);Ko=i(fl,"TEST"),fl.forEach(t),Yo=i(Qi,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qi.forEach(t),Qo=l(we),It=n(we,"LI",{});var Zi=r(It);za=n(Zi,"CODE",{});var ul=r(za);Zo=i(ul,"ALL"),ul.forEach(t),ei=i(Zi,": the union of all defined dataset splits."),Zi.forEach(t),we.forEach(t),ti=l(R),Bt=n(R,"P",{});var ed=r(Bt);ai=i(ed,"Note: All splits, including compositions inherit from "),Ja=n(ed,"CODE",{});var gl=r(Ja);si=i(gl,"datasets.SplitBase"),gl.forEach(t),ed.forEach(t),ni=l(R),Ye=n(R,"P",{});var Js=r(Ye);ri=i(Js,"See the :doc:"),Ka=n(Js,"CODE",{});var hl=r(Ka);oi=i(hl,"guide on splits </loading>"),hl.forEach(t),ii=i(Js," for more information."),Js.forEach(t),R.forEach(t),xs=l(a),w=n(a,"DIV",{class:!0});var x=r(w);f(Qe.$$.fragment,x),di=l(x),Ya=n(x,"P",{});var ml=r(Ya);li=i(ml,"Descriptor corresponding to a named split (train, test, \u2026)."),ml.forEach(t),ci=l(x),Qa=n(x,"P",{});var _l=r(Qa);pi=i(_l,"Example:"),_l.forEach(t),fi=l(x),f(Ze.$$.fragment,x),ui=l(x),Za=n(x,"P",{});var vl=r(Za);gi=i(vl,`Warning:
A split cannot be added twice, so the following will fail:`),vl.forEach(t),hi=l(x),f(et.$$.fragment,x),mi=l(x),es=n(x,"P",{});var bl=r(es);_i=i(bl,`Warning:
The slices can be applied only one time. So the following are valid:`),bl.forEach(t),vi=l(x),f(tt.$$.fragment,x),bi=l(x),ts=n(x,"P",{});var $l=r(ts);$i=i($l,"But not:"),$l.forEach(t),wi=l(x),f(at.$$.fragment,x),x.forEach(t),ys=l(a),te=n(a,"DIV",{class:!0});var Ks=r(te);f(st.$$.fragment,Ks),Ei=l(Ks),as=n(Ks,"P",{});var wl=r(as);Di=i(wl,"Split corresponding to the union of all defined dataset splits."),wl.forEach(t),Ks.forEach(t),Ts=l(a),T=n(a,"DIV",{class:!0});var C=r(T);f(nt.$$.fragment,C),xi=l(C),ss=n(C,"P",{});var El=r(ss);yi=i(El,"Reading instruction for a dataset."),El.forEach(t),Ti=l(C),ns=n(C,"P",{});var Dl=r(ns);Ii=i(Dl,"Examples:"),Dl.forEach(t),Bi=l(C),f(rt.$$.fragment,C),Si=l(C),_e=n(C,"DIV",{class:!0});var Ys=r(_e);f(ot.$$.fragment,Ys),Ni=l(Ys),rs=n(Ys,"P",{});var xl=r(rs);ji=i(xl,"Creates a ReadInstruction instance out of a string spec."),xl.forEach(t),Ys.forEach(t),Ri=l(C),M=n(C,"DIV",{class:!0});var Mt=r(M);f(it.$$.fragment,Mt),Ci=l(Mt),os=n(Mt,"P",{});var yl=r(os);ki=i(yl,"Translate instruction into a list of absolute instructions."),yl.forEach(t),Oi=l(Mt),is=n(Mt,"P",{});var Tl=r(is);Ai=i(Tl,"Those absolute instructions are then to be added together."),Tl.forEach(t),Mt.forEach(t),C.forEach(t),Is=l(a),ae=n(a,"DIV",{class:!0});var Qs=r(ae);f(dt.$$.fragment,Qs),Li=l(Qs),ds=n(Qs,"P",{});var Il=r(ds);Pi=i(Il,"Configuration for our cached path manager."),Il.forEach(t),Qs.forEach(t),Bs=l(a),L=n(a,"DIV",{class:!0});var Ut=r(L);f(lt.$$.fragment,Ut),Vi=l(Ut),ls=n(Ut,"P",{});var Bl=r(ls);Fi=i(Bl,"Dataset version MAJOR.MINOR.PATCH."),Bl.forEach(t),Mi=l(Ut),ve=n(Ut,"DIV",{class:!0});var Zs=r(ve);f(ct.$$.fragment,Zs),Ui=l(Zs),cs=n(Zs,"P",{});var Sl=r(cs);qi=i(Sl,"Returns True if other_version matches."),Sl.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){c(q,"name","hf:doc:metadata"),c(q,"content",JSON.stringify(Pl)),c(ne,"id","datasets.DatasetBuilder"),c(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ne,"href","#datasets.DatasetBuilder"),c(H,"class","relative group"),c(ft,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(ut,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.BuilderConfig"),c(_t,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),c(vt,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),c(oe,"class","docstring"),c(ie,"class","docstring"),c(de,"class","docstring"),c(le,"class","docstring"),c(ce,"class","docstring"),c($,"class","docstring"),c(k,"class","docstring"),c(W,"class","docstring"),c(X,"class","docstring"),c(bt,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.DatasetBuilder"),c($t,"href","/docs/datasets/pr_10/en/package_reference/builder_classes#datasets.BuilderConfig"),c(F,"class","docstring"),c(S,"class","docstring"),c(pe,"class","docstring"),c(j,"class","docstring"),c(fe,"class","docstring"),c(ue,"class","docstring"),c(ge,"class","docstring"),c(he,"class","docstring"),c(me,"class","docstring"),c(E,"class","docstring"),c(B,"class","docstring"),c(O,"class","docstring"),c(y,"class","docstring"),c(w,"class","docstring"),c(te,"class","docstring"),c(_e,"class","docstring"),c(M,"class","docstring"),c(T,"class","docstring"),c(ae,"class","docstring"),c(ve,"class","docstring"),c(L,"class","docstring")},m(a,_){e(document.head,q),v(a,fs,_),v(a,H,_),e(H,ne),e(ne,Ht),u(Ee,Ht,null),e(H,en),e(H,Gt),e(Gt,tn),v(a,us,_),v(a,P,_),e(P,an),e(P,ft),e(ft,sn),e(P,nn),e(P,ut),e(ut,rn),e(P,on),v(a,gs,_),v(a,$,_),u(De,$,null),e($,dn),e($,Wt),e(Wt,ln),e($,cn),e($,gt),e(gt,Xt),e(Xt,pn),e(gt,fn),e($,un),e($,G),e(G,ht),e(ht,zt),e(zt,gn),e(ht,hn),e(G,mn),e(G,mt),e(mt,_t),e(_t,_n),e(mt,vn),e(G,bn),e(G,re),e(re,vt),e(vt,$n),e(re,wn),e(re,Jt),e(Jt,En),e(re,Dn),e($,xn),e($,N),e(N,Kt),e(Kt,yn),e(N,Tn),e(N,Yt),e(Yt,In),e(N,Bn),e(N,Qt),e(Qt,Sn),e(N,Nn),e(N,Zt),e(Zt,jn),e(N,Rn),e($,Cn),e($,oe),u(xe,oe,null),e(oe,kn),e(oe,ea),e(ea,On),e($,An),e($,ie),u(ye,ie,null),e(ie,Ln),e(ie,ta),e(ta,Pn),e($,Vn),e($,de),u(Te,de,null),e(de,Fn),e(de,aa),e(aa,Mn),e($,Un),e($,le),u(Ie,le,null),e(le,qn),e(le,sa),e(sa,Hn),e($,Gn),e($,ce),u(Be,ce,null),e(ce,Wn),e(ce,na),e(na,Xn),v(a,hs,_),v(a,k,_),u(Se,k,null),e(k,zn),e(k,ra),e(ra,Jn),e(k,Kn),e(k,V),e(V,oa),e(oa,Yn),e(V,Qn),e(V,ia),e(ia,Zn),e(V,er),e(V,da),e(da,tr),e(V,ar),v(a,ms,_),v(a,W,_),u(Ne,W,null),e(W,sr),e(W,la),e(la,nr),v(a,_s,_),v(a,X,_),u(je,X,null),e(X,rr),e(X,ca),e(ca,or),v(a,vs,_),v(a,S,_),u(Re,S,null),e(S,ir),e(S,Ce),e(Ce,dr),e(Ce,bt),e(bt,lr),e(Ce,cr),e(S,pr),e(S,ke),e(ke,fr),e(ke,$t),e($t,ur),e(ke,gr),e(S,hr),e(S,F),u(Oe,F,null),e(F,mr),e(F,pa),e(pa,_r),e(F,vr),e(F,z),e(z,fa),e(fa,br),e(z,$r),e(z,ua),e(ua,wr),e(z,Er),e(z,ga),e(ga,Dr),v(a,bs,_),v(a,E,_),u(Ae,E,null),e(E,xr),e(E,pe),u(Le,pe,null),e(pe,yr),e(pe,ha),e(ha,Tr),e(E,Ir),e(E,j),u(Pe,j,null),e(j,Br),e(j,ma),e(ma,Sr),e(j,Nr),e(j,_a),e(_a,jr),e(j,Rr),u(Ve,j,null),e(E,Cr),e(E,fe),u(Fe,fe,null),e(fe,kr),e(fe,Me),e(Me,Or),e(Me,va),e(va,Ar),e(Me,Lr),e(E,Pr),e(E,ue),u(Ue,ue,null),e(ue,Vr),e(ue,ba),e(ba,Fr),e(E,Mr),e(E,ge),u(qe,ge,null),e(ge,Ur),e(ge,$a),e($a,qr),e(E,Hr),e(E,he),u(He,he,null),e(he,Gr),e(he,wa),e(wa,Wr),e(E,Xr),e(E,me),u(Ge,me,null),e(me,zr),e(me,Ea),e(Ea,Jr),v(a,$s,_),v(a,B,_),u(We,B,null),e(B,Kr),e(B,wt),e(wt,Da),e(Da,Yr),e(wt,Qr),e(B,Zr),e(B,Xe),e(Xe,eo),e(Xe,xa),e(xa,to),e(Xe,ao),e(B,so),e(B,ya),e(ya,no),e(B,ro),e(B,ze),e(ze,Ta),e(Ta,J),e(J,ws),e(J,oo),e(J,Ia),e(Ia,io),e(J,lo),e(J,Ba),e(Ba,co),e(ze,po),e(ze,K),e(K,Y),e(Y,Et),e(Et,Sa),e(Sa,fo),e(Et,uo),e(Y,go),e(Y,Na),e(Na,ho),e(Y,mo),e(Y,ja),e(ja,_o),e(K,vo),e(K,Q),e(Q,Ra),e(Ra,Ca),e(Ca,bo),e(Q,$o),e(Q,ka),e(ka,wo),e(Q,Eo),e(Q,Oa),e(Oa,Do),e(K,xo),e(K,Z),e(Z,Aa),e(Aa,La),e(La,yo),e(Z,To),e(Z,Pa),e(Pa,Io),e(Z,Bo),e(Z,Va),e(Va,So),v(a,Es,_),v(a,O,_),u(Je,O,null),e(O,No),e(O,Fa),e(Fa,jo),e(O,Ro),e(O,ee),e(ee,Co),e(ee,Ma),e(Ma,ko),e(ee,Oo),e(ee,Ua),e(Ua,Ao),e(ee,Lo),v(a,Ds,_),v(a,y,_),u(Ke,y,null),e(y,Po),e(y,Dt),e(Dt,qa),e(qa,Vo),e(Dt,Fo),e(y,Mo),e(y,Ha),e(Ha,Uo),e(y,qo),e(y,A),e(A,xt),e(xt,Ga),e(Ga,Ho),e(xt,Go),e(A,Wo),e(A,yt),e(yt,Wa),e(Wa,Xo),e(yt,zo),e(A,Jo),e(A,Tt),e(Tt,Xa),e(Xa,Ko),e(Tt,Yo),e(A,Qo),e(A,It),e(It,za),e(za,Zo),e(It,ei),e(y,ti),e(y,Bt),e(Bt,ai),e(Bt,Ja),e(Ja,si),e(y,ni),e(y,Ye),e(Ye,ri),e(Ye,Ka),e(Ka,oi),e(Ye,ii),v(a,xs,_),v(a,w,_),u(Qe,w,null),e(w,di),e(w,Ya),e(Ya,li),e(w,ci),e(w,Qa),e(Qa,pi),e(w,fi),u(Ze,w,null),e(w,ui),e(w,Za),e(Za,gi),e(w,hi),u(et,w,null),e(w,mi),e(w,es),e(es,_i),e(w,vi),u(tt,w,null),e(w,bi),e(w,ts),e(ts,$i),e(w,wi),u(at,w,null),v(a,ys,_),v(a,te,_),u(st,te,null),e(te,Ei),e(te,as),e(as,Di),v(a,Ts,_),v(a,T,_),u(nt,T,null),e(T,xi),e(T,ss),e(ss,yi),e(T,Ti),e(T,ns),e(ns,Ii),e(T,Bi),u(rt,T,null),e(T,Si),e(T,_e),u(ot,_e,null),e(_e,Ni),e(_e,rs),e(rs,ji),e(T,Ri),e(T,M),u(it,M,null),e(M,Ci),e(M,os),e(os,ki),e(M,Oi),e(M,is),e(is,Ai),v(a,Is,_),v(a,ae,_),u(dt,ae,null),e(ae,Li),e(ae,ds),e(ds,Pi),v(a,Bs,_),v(a,L,_),u(lt,L,null),e(L,Vi),e(L,ls),e(ls,Fi),e(L,Mi),e(L,ve),u(ct,ve,null),e(ve,Ui),e(ve,cs),e(cs,qi),Ss=!0},p:kl,i(a){Ss||(g(Ee.$$.fragment,a),g(De.$$.fragment,a),g(xe.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(Ue.$$.fragment,a),g(qe.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(nt.$$.fragment,a),g(rt.$$.fragment,a),g(ot.$$.fragment,a),g(it.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(ct.$$.fragment,a),Ss=!0)},o(a){h(Ee.$$.fragment,a),h(De.$$.fragment,a),h(xe.$$.fragment,a),h(ye.$$.fragment,a),h(Te.$$.fragment,a),h(Ie.$$.fragment,a),h(Be.$$.fragment,a),h(Se.$$.fragment,a),h(Ne.$$.fragment,a),h(je.$$.fragment,a),h(Re.$$.fragment,a),h(Oe.$$.fragment,a),h(Ae.$$.fragment,a),h(Le.$$.fragment,a),h(Pe.$$.fragment,a),h(Ve.$$.fragment,a),h(Fe.$$.fragment,a),h(Ue.$$.fragment,a),h(qe.$$.fragment,a),h(He.$$.fragment,a),h(Ge.$$.fragment,a),h(We.$$.fragment,a),h(Je.$$.fragment,a),h(Ke.$$.fragment,a),h(Qe.$$.fragment,a),h(Ze.$$.fragment,a),h(et.$$.fragment,a),h(tt.$$.fragment,a),h(at.$$.fragment,a),h(st.$$.fragment,a),h(nt.$$.fragment,a),h(rt.$$.fragment,a),h(ot.$$.fragment,a),h(it.$$.fragment,a),h(dt.$$.fragment,a),h(lt.$$.fragment,a),h(ct.$$.fragment,a),Ss=!1},d(a){t(q),a&&t(fs),a&&t(H),m(Ee),a&&t(us),a&&t(P),a&&t(gs),a&&t($),m(De),m(xe),m(ye),m(Te),m(Ie),m(Be),a&&t(hs),a&&t(k),m(Se),a&&t(ms),a&&t(W),m(Ne),a&&t(_s),a&&t(X),m(je),a&&t(vs),a&&t(S),m(Re),m(Oe),a&&t(bs),a&&t(E),m(Ae),m(Le),m(Pe),m(Ve),m(Fe),m(Ue),m(qe),m(He),m(Ge),a&&t($s),a&&t(B),m(We),a&&t(Es),a&&t(O),m(Je),a&&t(Ds),a&&t(y),m(Ke),a&&t(xs),a&&t(w),m(Qe),m(Ze),m(et),m(tt),m(at),a&&t(ys),a&&t(te),m(st),a&&t(Ts),a&&t(T),m(nt),m(rt),m(ot),m(it),a&&t(Is),a&&t(ae),m(dt),a&&t(Bs),a&&t(L),m(lt),m(ct)}}}const Pl={local:"datasets.DatasetBuilder",title:"Builder classes"};function Vl(td){return Ol(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hl extends Nl{constructor(q){super();jl(this,q,Vl,Ll,Rl,{})}}export{Hl as default,Pl as metadata};
