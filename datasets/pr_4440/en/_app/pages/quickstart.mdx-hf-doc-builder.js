import{S as Vp,i as Bp,s as Yp,e as r,k as f,w as k,t,M as Jp,c as l,d as s,m as d,a as o,x,h as n,b as h,G as e,g as u,y as E,q,o as A,B as T,v as Qp,L as Qs}from"../chunks/vendor-hf-doc-builder.js";import{T as Kp}from"../chunks/Tip-hf-doc-builder.js";import{I as Js}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as N}from"../chunks/CodeBlock-hf-doc-builder.js";import{C as Zp}from"../chunks/CodeBlockFw-hf-doc-builder.js";import{F as co,M as Ks}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function Xp(F){let i,g,c,m,$;return{c(){i=r("p"),g=t("For more detailed information about the \u{1F917} Datasets library, check out "),c=r("a"),m=t("Chapter 5"),$=t(" of the Hugging Face course! It covers other important topics such as loading remote or local datasets, tools for cleaning up a dataset and creating your own dataset."),this.h()},l(j){i=l(j,"P",{});var S=o(i);g=n(S,"For more detailed information about the \u{1F917} Datasets library, check out "),c=l(S,"A",{href:!0,rel:!0});var P=o(c);m=n(P,"Chapter 5"),P.forEach(s),$=n(S," of the Hugging Face course! It covers other important topics such as loading remote or local datasets, tools for cleaning up a dataset and creating your own dataset."),S.forEach(s),this.h()},h(){h(c,"href","https://huggingface.co/course/chapter5/1?fw=pt"),h(c,"rel","nofollow")},m(j,S){u(j,i,S),e(i,g),e(i,c),e(c,m),e(i,$)},d(j){j&&s(i)}}}function ai(F){let i,g,c,m,$,j,S,P,C,_,y,b,w,z,O;return z=new N({props:{code:`import torch

dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "labels"])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)`}}),{c(){i=r("p"),g=t("Use the "),c=r("a"),m=t("set_format()"),$=t(" function to set the dataset format to "),j=r("code"),S=t("torch"),P=t(" and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in "),C=r("a"),_=r("code"),y=t("torch.utils.data.DataLoader"),b=t(":"),w=f(),k(z.$$.fragment),this.h()},l(v){i=l(v,"P",{});var D=o(i);g=n(D,"Use the "),c=l(D,"A",{href:!0});var ta=o(c);m=n(ta,"set_format()"),ta.forEach(s),$=n(D," function to set the dataset format to "),j=l(D,"CODE",{});var K=o(j);S=n(K,"torch"),K.forEach(s),P=n(D," and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in "),C=l(D,"A",{href:!0,rel:!0});var G=o(C);_=l(G,"CODE",{});var I=o(_);y=n(I,"torch.utils.data.DataLoader"),I.forEach(s),G.forEach(s),b=n(D,":"),D.forEach(s),w=d(v),x(z.$$.fragment,v),this.h()},h(){h(c,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.set_format"),h(C,"href","https://alband.github.io/doc_view/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader"),h(C,"rel","nofollow")},m(v,D){u(v,i,D),e(i,g),e(i,c),e(c,m),e(i,$),e(i,j),e(j,S),e(i,P),e(i,C),e(C,_),e(_,y),e(i,b),u(v,w,D),E(z,v,D),O=!0},p:Qs,i(v){O||(q(z.$$.fragment,v),O=!0)},o(v){A(z.$$.fragment,v),O=!1},d(v){v&&s(i),v&&s(w),T(z,v)}}}function ei(F){let i,g;return i=new Ks({props:{$$slots:{default:[ai]},$$scope:{ctx:F}}}),{c(){k(i.$$.fragment)},l(c){x(i.$$.fragment,c)},m(c,m){E(i,c,m),g=!0},p(c,m){const $={};m&2&&($.$$scope={dirty:m,ctx:c}),i.$set($)},i(c){g||(q(i.$$.fragment,c),g=!0)},o(c){A(i.$$.fragment,c),g=!1},d(c){T(i,c)}}}function si(F){let i,g,c,m,$,j,S,P,C,_,y;return _=new N({props:{code:`import tensorflow as tf
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
tf_dataset = dataset.to_tf_dataset(
    columns=["input_ids", "token_type_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=2,
    collate_fn=data_collator,
    shuffle=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = dataset.to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>],
<span class="hljs-meta">... </span>    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>)`}}),{c(){i=r("p"),g=t("Use the "),c=r("a"),m=t("to_tf_dataset()"),$=t(" function to set the dataset format to be compatible with TensorFlow. You\u2019ll also need to import a "),j=r("a"),S=t("data collator"),P=t(" from \u{1F917} Transformers to combine the varying sequence lengths into a single batch of equal lengths:"),C=f(),k(_.$$.fragment),this.h()},l(b){i=l(b,"P",{});var w=o(i);g=n(w,"Use the "),c=l(w,"A",{href:!0});var z=o(c);m=n(z,"to_tf_dataset()"),z.forEach(s),$=n(w," function to set the dataset format to be compatible with TensorFlow. You\u2019ll also need to import a "),j=l(w,"A",{href:!0,rel:!0});var O=o(j);S=n(O,"data collator"),O.forEach(s),P=n(w," from \u{1F917} Transformers to combine the varying sequence lengths into a single batch of equal lengths:"),w.forEach(s),C=d(b),x(_.$$.fragment,b),this.h()},h(){h(c,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),h(j,"href","https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding"),h(j,"rel","nofollow")},m(b,w){u(b,i,w),e(i,g),e(i,c),e(c,m),e(i,$),e(i,j),e(j,S),e(i,P),u(b,C,w),E(_,b,w),y=!0},p:Qs,i(b){y||(q(_.$$.fragment,b),y=!0)},o(b){A(_.$$.fragment,b),y=!1},d(b){b&&s(i),b&&s(C),T(_,b)}}}function ti(F){let i,g;return i=new Ks({props:{$$slots:{default:[si]},$$scope:{ctx:F}}}),{c(){k(i.$$.fragment)},l(c){x(i.$$.fragment,c)},m(c,m){E(i,c,m),g=!0},p(c,m){const $={};m&2&&($.$$scope={dirty:m,ctx:c}),i.$set($)},i(c){g||(q(i.$$.fragment,c),g=!0)},o(c){A(i.$$.fragment,c),g=!1},d(c){T(i,c)}}}function ni(F){let i,g,c,m,$,j,S,P,C,_,y,b,w,z,O;return z=new N({props:{code:`from torch.utils.data import DataLoader

dataset.set_format(type="torch", columns=["input_values", "labels"])
dataloader = DataLoader(dataset, batch_size=4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_values&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">4</span>)`}}),{c(){i=r("p"),g=t("Use the "),c=r("a"),m=t("set_format()"),$=t(" function to set the dataset format to "),j=r("code"),S=t("torch"),P=t(" and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in "),C=r("a"),_=r("code"),y=t("torch.utils.data.DataLoader"),b=t(":"),w=f(),k(z.$$.fragment),this.h()},l(v){i=l(v,"P",{});var D=o(i);g=n(D,"Use the "),c=l(D,"A",{href:!0});var ta=o(c);m=n(ta,"set_format()"),ta.forEach(s),$=n(D," function to set the dataset format to "),j=l(D,"CODE",{});var K=o(j);S=n(K,"torch"),K.forEach(s),P=n(D," and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in "),C=l(D,"A",{href:!0,rel:!0});var G=o(C);_=l(G,"CODE",{});var I=o(_);y=n(I,"torch.utils.data.DataLoader"),I.forEach(s),G.forEach(s),b=n(D,":"),D.forEach(s),w=d(v),x(z.$$.fragment,v),this.h()},h(){h(c,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.set_format"),h(C,"href","https://alband.github.io/doc_view/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader"),h(C,"rel","nofollow")},m(v,D){u(v,i,D),e(i,g),e(i,c),e(c,m),e(i,$),e(i,j),e(j,S),e(i,P),e(i,C),e(C,_),e(_,y),e(i,b),u(v,w,D),E(z,v,D),O=!0},p:Qs,i(v){O||(q(z.$$.fragment,v),O=!0)},o(v){A(z.$$.fragment,v),O=!1},d(v){v&&s(i),v&&s(w),T(z,v)}}}function ri(F){let i,g;return i=new Ks({props:{$$slots:{default:[ni]},$$scope:{ctx:F}}}),{c(){k(i.$$.fragment)},l(c){x(i.$$.fragment,c)},m(c,m){E(i,c,m),g=!0},p(c,m){const $={};m&2&&($.$$scope={dirty:m,ctx:c}),i.$set($)},i(c){g||(q(i.$$.fragment,c),g=!0)},o(c){A(i.$$.fragment,c),g=!1},d(c){T(i,c)}}}function li(F){let i,g,c,m,$,j,S,P,C,_,y;return _=new N({props:{code:`import tensorflow as tf

tf_dataset = dataset.to_tf_dataset(
    columns=["input_values"],
    label_cols=["labels"],
    batch_size=4,
    shuffle=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = dataset.to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;input_values&quot;</span>],
<span class="hljs-meta">... </span>    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>)`}}),{c(){i=r("p"),g=t("Use the "),c=r("a"),m=t("to_tf_dataset()"),$=t(" function to set the dataset format to be compatible with TensorFlow. You\u2019ll also need to import a "),j=r("a"),S=t("data collator"),P=t(" from \u{1F917} Transformers to combine the varying sequence lengths into a single batch of equal lengths:"),C=f(),k(_.$$.fragment),this.h()},l(b){i=l(b,"P",{});var w=o(i);g=n(w,"Use the "),c=l(w,"A",{href:!0});var z=o(c);m=n(z,"to_tf_dataset()"),z.forEach(s),$=n(w," function to set the dataset format to be compatible with TensorFlow. You\u2019ll also need to import a "),j=l(w,"A",{href:!0,rel:!0});var O=o(j);S=n(O,"data collator"),O.forEach(s),P=n(w," from \u{1F917} Transformers to combine the varying sequence lengths into a single batch of equal lengths:"),w.forEach(s),C=d(b),x(_.$$.fragment,b),this.h()},h(){h(c,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),h(j,"href","https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding"),h(j,"rel","nofollow")},m(b,w){u(b,i,w),e(i,g),e(i,c),e(c,m),e(i,$),e(i,j),e(j,S),e(i,P),u(b,C,w),E(_,b,w),y=!0},p:Qs,i(b){y||(q(_.$$.fragment,b),y=!0)},o(b){A(_.$$.fragment,b),y=!1},d(b){b&&s(i),b&&s(C),T(_,b)}}}function oi(F){let i,g;return i=new Ks({props:{$$slots:{default:[li]},$$scope:{ctx:F}}}),{c(){k(i.$$.fragment)},l(c){x(i.$$.fragment,c)},m(c,m){E(i,c,m),g=!0},p(c,m){const $={};m&2&&($.$$scope={dirty:m,ctx:c}),i.$set($)},i(c){g||(q(i.$$.fragment,c),g=!0)},o(c){A(i.$$.fragment,c),g=!1},d(c){T(i,c)}}}function pi(F){let i,g,c,m,$,j,S,P,C;return P=new N({props:{code:`from torch.utils.data import DataLoader

def collate_fn(examples):
    images = []
    labels = []
    for example in examples:
        images.append((example["pixel_values"]))
        labels.append(example["labels"])
        
    pixel_values = torch.stack(images)
    labels = torch.tensor(labels)
    return {"pixel_values": pixel_values, "labels": labels}
dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    images = []
<span class="hljs-meta">... </span>    labels = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples:
<span class="hljs-meta">... </span>        images.append((example[<span class="hljs-string">&quot;pixel_values&quot;</span>]))
<span class="hljs-meta">... </span>        labels.append(example[<span class="hljs-string">&quot;labels&quot;</span>])
<span class="hljs-meta">... </span>        
<span class="hljs-meta">... </span>    pixel_values = torch.stack(images)
<span class="hljs-meta">... </span>    labels = torch.tensor(labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;pixel_values&quot;</span>: pixel_values, <span class="hljs-string">&quot;labels&quot;</span>: labels}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=<span class="hljs-number">4</span>)`}}),{c(){i=r("p"),g=t("Wrap the dataset in "),c=r("a"),m=r("code"),$=t("torch.utils.data.DataLoader"),j=t(". You\u2019ll also need to create a collate function to collate the samples into batches:"),S=f(),k(P.$$.fragment),this.h()},l(_){i=l(_,"P",{});var y=o(i);g=n(y,"Wrap the dataset in "),c=l(y,"A",{href:!0,rel:!0});var b=o(c);m=l(b,"CODE",{});var w=o(m);$=n(w,"torch.utils.data.DataLoader"),w.forEach(s),b.forEach(s),j=n(y,". You\u2019ll also need to create a collate function to collate the samples into batches:"),y.forEach(s),S=d(_),x(P.$$.fragment,_),this.h()},h(){h(c,"href","https://alband.github.io/doc_view/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader"),h(c,"rel","nofollow")},m(_,y){u(_,i,y),e(i,g),e(i,c),e(c,m),e(m,$),e(i,j),u(_,S,y),E(P,_,y),C=!0},p:Qs,i(_){C||(q(P.$$.fragment,_),C=!0)},o(_){A(P.$$.fragment,_),C=!1},d(_){_&&s(i),_&&s(S),T(P,_)}}}function ii(F){let i,g;return i=new Ks({props:{$$slots:{default:[pi]},$$scope:{ctx:F}}}),{c(){k(i.$$.fragment)},l(c){x(i.$$.fragment,c)},m(c,m){E(i,c,m),g=!0},p(c,m){const $={};m&2&&($.$$scope={dirty:m,ctx:c}),i.$set($)},i(c){g||(q(i.$$.fragment,c),g=!0)},o(c){A(i.$$.fragment,c),g=!1},d(c){T(i,c)}}}function ci(F){let i,g,c,m,$,j,S,P,C,_,y,b,w,z,O,v,D,ta,K,G,I,na,xe,cn,hn,Ee,un,fn,ra,qe,dn,mn,Ae,gn,_n,la,Te,bn,jn,Ce,$n,Zs,va,Xs,Pe,wn,at,Sa,et,ya,vn,De,yn,kn,st,za,tt,ka,xn,Se,En,qn,nt,Fa,rt,oa,xa,Ze,Na,An,Xe,Tn,lt,Z,Cn,Oa,Pn,Dn,Ia,Sn,zn,ot,pa,as,Fn,Nn,ze,On,In,pt,Ra,it,V,es,Rn,Ln,La,Mn,Gn,Ma,Hn,Un,ct,Ga,ht,H,ss,Wn,Vn,ts,Bn,Yn,ns,Jn,Qn,rs,Kn,Zn,ut,Ea,Xn,Fe,ar,er,ft,Ha,dt,U,ls,sr,tr,os,nr,rr,ps,lr,or,Ua,pr,ir,mt,Wa,gt,Va,is,cr,hr,_t,qa,bt,ia,cs,ur,fr,Ba,dr,mr,jt,ca,Aa,hs,Ya,gr,us,_r,$t,X,br,Ja,jr,$r,Qa,wr,vr,wt,ha,fs,yr,kr,Ne,xr,Er,vt,Ka,yt,B,ds,qr,Ar,Za,Tr,Cr,Xa,Pr,Dr,kt,ae,xt,R,ms,Sr,zr,ee,Fr,Nr,gs,Or,Ir,Oe,Rr,Lr,Ie,Mr,Gr,Et,se,qt,W,_s,Hr,Ur,bs,Wr,Vr,js,Br,Yr,$s,Jr,Qr,At,aa,Kr,ws,Zr,Xr,Re,al,el,Tt,te,Ct,L,vs,sl,tl,Le,nl,rl,ys,ll,ol,ks,pl,il,ne,cl,hl,Pt,re,Dt,le,xs,ul,fl,St,Ta,zt,ua,Es,dl,ml,oe,gl,_l,Ft,fa,Ca,qs,pe,bl,As,jl,Nt,ea,$l,ie,wl,vl,ce,yl,kl,Ot,da,Ts,xl,El,Me,ql,Al,It,he,Rt,M,Cs,Tl,Cl,ue,Pl,Dl,fe,Sl,zl,de,Fl,Nl,me,Ol,Il,Lt,ge,Mt,ma,Ps,Rl,Ll,Ds,Ml,Gl,Gt,_e,Ht,ga,Ss,Hl,Ul,Ge,Wl,Vl,Ut,be,Wt,je,zs,Bl,Yl,Vt,Pa,Bt,_a,Fs,Jl,Ql,$e,Kl,Zl,Yt,ba,Da,Ns,we,Xl,Os,ao,Jt,He,eo,Qt,sa,so,Ue,to,no,We,ro,lo,Kt;return j=new Js({}),va=new Kp({props:{$$slots:{default:[Xp]},$$scope:{ctx:F}}}),Sa=new N({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),za=new N({props:{code:"pip install datasets[audio]",highlighted:"pip install datasets[audio]"}}),Fa=new N({props:{code:"pip install datasets[vision]",highlighted:"pip install datasets[vision]"}}),Na=new Js({}),Ra=new N({props:{code:`from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ga=new Zp({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}}),Ha=new N({props:{code:`def encode(examples):
    return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length")

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;sentence1&quot;</span>], examples[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),Wa=new N({props:{code:'dataset = dataset.map(lambda examples: {"labels": examples["label"]}, batched=True)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&quot;labels&quot;</span>: examples[<span class="hljs-string">&quot;label&quot;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),qa=new co({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ti],pytorch:[ei]},$$scope:{ctx:F}}}),Ya=new Js({}),Ka=new N({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", "en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, <span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),ae=new N({props:{code:`from transformers import AutoModelForAudioClassification, AutoFeatureExtractor

model = AutoModelForAudioClassification.from_pretrained("facebook/wav2vec2-base")
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification, AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),se=new N({props:{code:`dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset[0]["audio"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),te=new N({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs

dataset = dataset.map(preprocess_function, batched=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)`}}),re=new N({props:{code:'dataset = dataset.rename_column("intent_class", "labels")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)'}}),Ta=new co({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[oi],pytorch:[ri]},$$scope:{ctx:F}}}),pe=new Js({}),he=new N({props:{code:`from datasets import load_dataset, Image

dataset = load_dataset("beans", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),ge=new N({props:{code:`from torchvision.transforms import Compose, ColorJitter, ToTensor

jitter = Compose(
    [ColorJitter(brightness=0.5, hue=0.5), ToTensor()]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = Compose(
<span class="hljs-meta">... </span>    [ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor()]
<span class="hljs-meta">... </span>)`}}),_e=new N({props:{code:`def transforms(examples):
    examples["pixel_values"] = [jitter(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [jitter(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),be=new N({props:{code:"dataset = dataset.with_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_transform(transforms)'}}),Pa=new co({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[ii]},$$scope:{ctx:F}}}),we=new Js({}),{c(){i=r("meta"),g=f(),c=r("h1"),m=r("a"),$=r("span"),k(j.$$.fragment),S=f(),P=r("span"),C=t("Quickstart"),_=f(),y=r("p"),b=t("This quickstart is intended for developers ready to dive into the code and see an end-to-end example of how to integrate \u{1F917} Datasets into their model training workflow. If you are a beginner or a non-developer, we recommend starting with our "),w=r("a"),z=t("tutorials"),O=t(", where you will get a more thorough introduction."),v=f(),D=r("p"),ta=t("Each dataset is unique, and depending on the task, some datasets may require additional steps to prepare the data. But you can always use \u{1F917} Datasets tools to load and process a dataset. Choose the type of dataset you want to work with, and let\u2019s get started!"),K=f(),G=r("div"),I=r("div"),na=r("a"),xe=r("div"),cn=t("Audio"),hn=f(),Ee=r("p"),un=t("Resample an audio dataset and get it ready for a model to classify what type of banking issue a speaker is calling about."),fn=f(),ra=r("a"),qe=r("div"),dn=t("Vision"),mn=f(),Ae=r("p"),gn=t("Apply data augmentation to an image dataset and get it ready for a model to diagnose disease in bean plants."),_n=f(),la=r("a"),Te=r("div"),bn=t("NLP"),jn=f(),Ce=r("p"),$n=t("Tokenize a dataset and train a model to determine whether a pair of sentences have the same meaning."),Zs=f(),k(va.$$.fragment),Xs=f(),Pe=r("p"),wn=t("Start by installing \u{1F917} Datasets:"),at=f(),k(Sa.$$.fragment),et=f(),ya=r("p"),vn=t("To work with audio datasets, install the "),De=r("a"),yn=t("Audio"),kn=t(" feature:"),st=f(),k(za.$$.fragment),tt=f(),ka=r("p"),xn=t("To work with image datasets, install the "),Se=r("a"),En=t("Image"),qn=t(" feature:"),nt=f(),k(Fa.$$.fragment),rt=f(),oa=r("h2"),xa=r("a"),Ze=r("span"),k(Na.$$.fragment),An=f(),Xe=r("span"),Tn=t("NLP"),lt=f(),Z=r("p"),Cn=t("The fastest and easiest way to get started is by loading an existing dataset from the "),Oa=r("a"),Pn=t("Hugging Face Hub"),Dn=t(". There are thousands of datasets to choose from, spanning many tasks. For the quickstart, you\u2019ll load the "),Ia=r("a"),Sn=t("Microsoft Research Paraphrase Corpus (MRPC)"),zn=t(" training dataset to train a model to determine whether a pair of sentences mean the same thing. As with all text datasets, you\u2019ll need to tokenize the dataset to prepare it for training."),ot=f(),pa=r("p"),as=r("strong"),Fn=t("1"),Nn=t(". Load the MRPC dataset by providing the "),ze=r("a"),On=t("load_dataset()"),In=t(" function with the dataset name, dataset configuration (not all datasets will have a configuration), and dataset split:"),pt=f(),k(Ra.$$.fragment),it=f(),V=r("p"),es=r("strong"),Rn=t("2"),Ln=t(". Next, load a pretrained "),La=r("a"),Mn=t("BERT"),Gn=t(" model and its corresponding tokenizer from the "),Ma=r("a"),Hn=t("\u{1F917} Transformers"),Un=t(" library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),ct=f(),k(Ga.$$.fragment),ht=f(),H=r("p"),ss=r("strong"),Wn=t("3"),Vn=t(". Tokenize the dataset with the tokenizer you just loaded. You should also truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: "),ts=r("code"),Bn=t("input_ids"),Yn=t(", "),ns=r("code"),Jn=t("token_type_ids"),Qn=t(", and an "),rs=r("code"),Kn=t("attention_mask"),Zn=t(". These are the inputs to your model."),ut=f(),Ea=r("p"),Xn=t("Use the "),Fe=r("a"),ar=t("map()"),er=t(" function to speed up processing by applying your tokenization function to batches of examples in the dataset:"),ft=f(),k(Ha.$$.fragment),dt=f(),U=r("p"),ls=r("strong"),sr=t("4"),tr=t(". Rename the "),os=r("code"),nr=t("label"),rr=t(" column to "),ps=r("code"),lr=t("labels"),or=t(", which is the expected input name in "),Ua=r("a"),pr=t("BertForSequenceClassification"),ir=t(":"),mt=f(),k(Wa.$$.fragment),gt=f(),Va=r("p"),is=r("strong"),cr=t("5"),hr=t(". Set the dataset format according to the machine learning framework you\u2019re using."),_t=f(),k(qa.$$.fragment),bt=f(),ia=r("p"),cs=r("strong"),ur=t("6"),fr=t(". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),Ba=r("a"),dr=t("fine-tuning tutorial"),mr=t(" for an example of how to train a model on your dataset."),jt=f(),ca=r("h2"),Aa=r("a"),hs=r("span"),k(Ya.$$.fragment),gr=f(),us=r("span"),_r=t("Audio"),$t=f(),X=r("p"),br=t("Audio datasets are loaded just like text datasets. However, an audio dataset is preprocessed a bit differently. Instead of a tokenizer, you\u2019ll need a "),Ja=r("a"),jr=t("feature extractor"),$r=t(". An audio input may also require resampling its sampling rate to match the sampling rate of the pretrained model you\u2019re using. In this quickstart, you\u2019ll prepare the "),Qa=r("a"),wr=t("MInDS-14"),vr=t(" dataset for a model train on and classify the banking issue a customer is having."),wt=f(),ha=r("p"),fs=r("strong"),yr=t("1"),kr=t(". Load the MInDS-14 dataset by providing the "),Ne=r("a"),xr=t("load_dataset()"),Er=t(" function with the dataset name, dataset configuration (not all datasets will have a configuration), and a dataset split:"),vt=f(),k(Ka.$$.fragment),yt=f(),B=r("p"),ds=r("strong"),qr=t("2"),Ar=t(". Next, load a pretrained "),Za=r("a"),Tr=t("Wav2Vec2"),Cr=t(" model and its corresponding feature extractor from the "),Xa=r("a"),Pr=t("\u{1F917} Transformers"),Dr=t(" library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),kt=f(),k(ae.$$.fragment),xt=f(),R=r("p"),ms=r("strong"),Sr=t("3"),zr=t(". The "),ee=r("a"),Fr=t("MInDS-14"),Nr=t(" dataset card indicates the sampling rate is 8kHz, but the Wav2Vec2 model was pretrained on a sampling rate of 16kHZ. You\u2019ll need to upsample the "),gs=r("code"),Or=t("audio"),Ir=t(" column with the "),Oe=r("a"),Rr=t("cast_column()"),Lr=t(" function and "),Ie=r("a"),Mr=t("Audio"),Gr=t(" feature to match the model\u2019s sampling rate."),Et=f(),k(se.$$.fragment),qt=f(),W=r("p"),_s=r("strong"),Hr=t("4"),Ur=t(". Create a function that will preprocess the audio "),bs=r("code"),Wr=t("array"),Vr=t(" with the feature extractor, and truncate and pad the sequences into tidy rectangular tensors. The most important thing to remember here is to call the audio "),js=r("code"),Br=t("array"),Yr=t(" in the feature extractor since the "),$s=r("code"),Jr=t("array"),Qr=t(" - the actual speech signal - is the model input."),At=f(),aa=r("p"),Kr=t("Once you have the "),ws=r("code"),Zr=t("preprocess_function"),Xr=t(", use the "),Re=r("a"),al=t("map()"),el=t(" function to speed up processing by applying the function to batches of examples in the dataset."),Tt=f(),k(te.$$.fragment),Ct=f(),L=r("p"),vs=r("strong"),sl=t("5"),tl=t(". Use the "),Le=r("a"),nl=t("rename_column()"),rl=t(" function to rename the "),ys=r("code"),ll=t("intent_class"),ol=t(" column to "),ks=r("code"),pl=t("labels"),il=t(", which is the expected input name in "),ne=r("a"),cl=t("Wav2Vec2ForSequenceClassification"),hl=t(":"),Pt=f(),k(re.$$.fragment),Dt=f(),le=r("p"),xs=r("strong"),ul=t("6"),fl=t(". Set the dataset format according to the machine learning framework you\u2019re using."),St=f(),k(Ta.$$.fragment),zt=f(),ua=r("p"),Es=r("strong"),dl=t("7"),ml=t(". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),oe=r("a"),gl=t("audio classification guide"),_l=t(" for an example of how to train a model on an audio dataset."),Ft=f(),fa=r("h2"),Ca=r("a"),qs=r("span"),k(pe.$$.fragment),bl=f(),As=r("span"),jl=t("Vision"),Nt=f(),ea=r("p"),$l=t("Image datasets are loaded just like text datasets. However, instead of a tokenizer, you\u2019ll need a "),ie=r("a"),wl=t("feature extractor"),vl=t(" to preprocess the dataset. An image input is a sequence of pixel values that represent an image. Applying some data augmentation to an image in computer vision is common to make the model more robust against overfitting. You\u2019re free to use any data augmentation library you want, and then you can apply the augmentations to your dataset with \u{1F917} Datasets. In this quickstart, you\u2019ll load the "),ce=r("a"),yl=t("Beans"),kl=t(" dataset and get it ready for the model to train on and identify disease from the leaf images."),Ot=f(),da=r("p"),Ts=r("strong"),xl=t("1"),El=t(". Load the Beans dataset by providing the "),Me=r("a"),ql=t("load_dataset()"),Al=t(" function with the dataset name and a dataset split:"),It=f(),k(he.$$.fragment),Rt=f(),M=r("p"),Cs=r("strong"),Tl=t("2"),Cl=t(". Now you can add some data augmentations to your dataset with any library ("),ue=r("a"),Pl=t("Albumentations"),Dl=t(", "),fe=r("a"),Sl=t("imgaug"),zl=t(", "),de=r("a"),Fl=t("Kornia"),Nl=t(") you like. Here, you\u2019ll use "),me=r("a"),Ol=t("torchvision"),Il=t(" to randomly change the color properties of an image:"),Lt=f(),k(ge.$$.fragment),Mt=f(),ma=r("p"),Ps=r("strong"),Rl=t("3"),Ll=t(". Create a function to apply your transform to your dataset and generate your model input: "),Ds=r("code"),Ml=t("pixel_values"),Gl=t("."),Gt=f(),k(_e.$$.fragment),Ht=f(),ga=r("p"),Ss=r("strong"),Hl=t("4"),Ul=t(". Use the "),Ge=r("a"),Wl=t("with_transform()"),Vl=t(" function to apply the data augmentations on-the-fly:"),Ut=f(),k(be.$$.fragment),Wt=f(),je=r("p"),zs=r("strong"),Bl=t("5"),Yl=t(". Set the dataset format according to the machine learning framework you\u2019re using."),Vt=f(),k(Pa.$$.fragment),Bt=f(),_a=r("p"),Fs=r("strong"),Jl=t("6"),Ql=t(". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),$e=r("a"),Kl=t("image classification guide"),Zl=t(" for an example of how to train a model on an image dataset."),Yt=f(),ba=r("h2"),Da=r("a"),Ns=r("span"),k(we.$$.fragment),Xl=f(),Os=r("span"),ao=t("What's next?"),Jt=f(),He=r("p"),eo=t("This completes the \u{1F917} Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on."),Qt=f(),sa=r("p"),so=t("For your next steps, take a look at our "),Ue=r("a"),to=t("How-to guides"),no=t(" and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If you\u2019re interested in learning more about \u{1F917} Datasets core concepts, grab a cup of coffee and read our "),We=r("a"),ro=t("Conceptual Guides"),lo=t("!"),this.h()},l(a){const p=Jp('[data-svelte="svelte-1phssyn"]',document.head);i=l(p,"META",{name:!0,content:!0}),p.forEach(s),g=d(a),c=l(a,"H1",{class:!0});var ve=o(c);m=l(ve,"A",{id:!0,class:!0,href:!0});var Is=o(m);$=l(Is,"SPAN",{});var Rs=o($);x(j.$$.fragment,Rs),Rs.forEach(s),Is.forEach(s),S=d(ve),P=l(ve,"SPAN",{});var Ls=o(P);C=n(Ls,"Quickstart"),Ls.forEach(s),ve.forEach(s),_=d(a),y=l(a,"P",{});var Zt=o(y);b=n(Zt,"This quickstart is intended for developers ready to dive into the code and see an end-to-end example of how to integrate \u{1F917} Datasets into their model training workflow. If you are a beginner or a non-developer, we recommend starting with our "),w=l(Zt,"A",{href:!0});var ho=o(w);z=n(ho,"tutorials"),ho.forEach(s),O=n(Zt,", where you will get a more thorough introduction."),Zt.forEach(s),v=d(a),D=l(a,"P",{});var uo=o(D);ta=n(uo,"Each dataset is unique, and depending on the task, some datasets may require additional steps to prepare the data. But you can always use \u{1F917} Datasets tools to load and process a dataset. Choose the type of dataset you want to work with, and let\u2019s get started!"),uo.forEach(s),K=d(a),G=l(a,"DIV",{class:!0});var fo=o(G);I=l(fo,"DIV",{class:!0});var Ve=o(I);na=l(Ve,"A",{class:!0,href:!0});var Xt=o(na);xe=l(Xt,"DIV",{class:!0});var mo=o(xe);cn=n(mo,"Audio"),mo.forEach(s),hn=d(Xt),Ee=l(Xt,"P",{class:!0});var go=o(Ee);un=n(go,"Resample an audio dataset and get it ready for a model to classify what type of banking issue a speaker is calling about."),go.forEach(s),Xt.forEach(s),fn=d(Ve),ra=l(Ve,"A",{class:!0,href:!0});var an=o(ra);qe=l(an,"DIV",{class:!0});var _o=o(qe);dn=n(_o,"Vision"),_o.forEach(s),mn=d(an),Ae=l(an,"P",{class:!0});var bo=o(Ae);gn=n(bo,"Apply data augmentation to an image dataset and get it ready for a model to diagnose disease in bean plants."),bo.forEach(s),an.forEach(s),_n=d(Ve),la=l(Ve,"A",{class:!0,href:!0});var en=o(la);Te=l(en,"DIV",{class:!0});var jo=o(Te);bn=n(jo,"NLP"),jo.forEach(s),jn=d(en),Ce=l(en,"P",{class:!0});var $o=o(Ce);$n=n($o,"Tokenize a dataset and train a model to determine whether a pair of sentences have the same meaning."),$o.forEach(s),en.forEach(s),Ve.forEach(s),fo.forEach(s),Zs=d(a),x(va.$$.fragment,a),Xs=d(a),Pe=l(a,"P",{});var wo=o(Pe);wn=n(wo,"Start by installing \u{1F917} Datasets:"),wo.forEach(s),at=d(a),x(Sa.$$.fragment,a),et=d(a),ya=l(a,"P",{});var sn=o(ya);vn=n(sn,"To work with audio datasets, install the "),De=l(sn,"A",{href:!0});var vo=o(De);yn=n(vo,"Audio"),vo.forEach(s),kn=n(sn," feature:"),sn.forEach(s),st=d(a),x(za.$$.fragment,a),tt=d(a),ka=l(a,"P",{});var tn=o(ka);xn=n(tn,"To work with image datasets, install the "),Se=l(tn,"A",{href:!0});var yo=o(Se);En=n(yo,"Image"),yo.forEach(s),qn=n(tn," feature:"),tn.forEach(s),nt=d(a),x(Fa.$$.fragment,a),rt=d(a),oa=l(a,"H2",{class:!0});var nn=o(oa);xa=l(nn,"A",{id:!0,class:!0,href:!0});var ko=o(xa);Ze=l(ko,"SPAN",{});var xo=o(Ze);x(Na.$$.fragment,xo),xo.forEach(s),ko.forEach(s),An=d(nn),Xe=l(nn,"SPAN",{});var Eo=o(Xe);Tn=n(Eo,"NLP"),Eo.forEach(s),nn.forEach(s),lt=d(a),Z=l(a,"P",{});var Be=o(Z);Cn=n(Be,"The fastest and easiest way to get started is by loading an existing dataset from the "),Oa=l(Be,"A",{href:!0,rel:!0});var qo=o(Oa);Pn=n(qo,"Hugging Face Hub"),qo.forEach(s),Dn=n(Be,". There are thousands of datasets to choose from, spanning many tasks. For the quickstart, you\u2019ll load the "),Ia=l(Be,"A",{href:!0,rel:!0});var Ao=o(Ia);Sn=n(Ao,"Microsoft Research Paraphrase Corpus (MRPC)"),Ao.forEach(s),zn=n(Be," training dataset to train a model to determine whether a pair of sentences mean the same thing. As with all text datasets, you\u2019ll need to tokenize the dataset to prepare it for training."),Be.forEach(s),ot=d(a),pa=l(a,"P",{});var Ms=o(pa);as=l(Ms,"STRONG",{});var To=o(as);Fn=n(To,"1"),To.forEach(s),Nn=n(Ms,". Load the MRPC dataset by providing the "),ze=l(Ms,"A",{href:!0});var Co=o(ze);On=n(Co,"load_dataset()"),Co.forEach(s),In=n(Ms," function with the dataset name, dataset configuration (not all datasets will have a configuration), and dataset split:"),Ms.forEach(s),pt=d(a),x(Ra.$$.fragment,a),it=d(a),V=l(a,"P",{});var ye=o(V);es=l(ye,"STRONG",{});var Po=o(es);Rn=n(Po,"2"),Po.forEach(s),Ln=n(ye,". Next, load a pretrained "),La=l(ye,"A",{href:!0,rel:!0});var Do=o(La);Mn=n(Do,"BERT"),Do.forEach(s),Gn=n(ye," model and its corresponding tokenizer from the "),Ma=l(ye,"A",{href:!0,rel:!0});var So=o(Ma);Hn=n(So,"\u{1F917} Transformers"),So.forEach(s),Un=n(ye," library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),ye.forEach(s),ct=d(a),x(Ga.$$.fragment,a),ht=d(a),H=l(a,"P",{});var ja=o(H);ss=l(ja,"STRONG",{});var zo=o(ss);Wn=n(zo,"3"),zo.forEach(s),Vn=n(ja,". Tokenize the dataset with the tokenizer you just loaded. You should also truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: "),ts=l(ja,"CODE",{});var Fo=o(ts);Bn=n(Fo,"input_ids"),Fo.forEach(s),Yn=n(ja,", "),ns=l(ja,"CODE",{});var No=o(ns);Jn=n(No,"token_type_ids"),No.forEach(s),Qn=n(ja,", and an "),rs=l(ja,"CODE",{});var Oo=o(rs);Kn=n(Oo,"attention_mask"),Oo.forEach(s),Zn=n(ja,". These are the inputs to your model."),ja.forEach(s),ut=d(a),Ea=l(a,"P",{});var rn=o(Ea);Xn=n(rn,"Use the "),Fe=l(rn,"A",{href:!0});var Io=o(Fe);ar=n(Io,"map()"),Io.forEach(s),er=n(rn," function to speed up processing by applying your tokenization function to batches of examples in the dataset:"),rn.forEach(s),ft=d(a),x(Ha.$$.fragment,a),dt=d(a),U=l(a,"P",{});var $a=o(U);ls=l($a,"STRONG",{});var Ro=o(ls);sr=n(Ro,"4"),Ro.forEach(s),tr=n($a,". Rename the "),os=l($a,"CODE",{});var Lo=o(os);nr=n(Lo,"label"),Lo.forEach(s),rr=n($a," column to "),ps=l($a,"CODE",{});var Mo=o(ps);lr=n(Mo,"labels"),Mo.forEach(s),or=n($a,", which is the expected input name in "),Ua=l($a,"A",{href:!0,rel:!0});var Go=o(Ua);pr=n(Go,"BertForSequenceClassification"),Go.forEach(s),ir=n($a,":"),$a.forEach(s),mt=d(a),x(Wa.$$.fragment,a),gt=d(a),Va=l(a,"P",{});var oo=o(Va);is=l(oo,"STRONG",{});var Ho=o(is);cr=n(Ho,"5"),Ho.forEach(s),hr=n(oo,". Set the dataset format according to the machine learning framework you\u2019re using."),oo.forEach(s),_t=d(a),x(qa.$$.fragment,a),bt=d(a),ia=l(a,"P",{});var Gs=o(ia);cs=l(Gs,"STRONG",{});var Uo=o(cs);ur=n(Uo,"6"),Uo.forEach(s),fr=n(Gs,". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),Ba=l(Gs,"A",{href:!0,rel:!0});var Wo=o(Ba);dr=n(Wo,"fine-tuning tutorial"),Wo.forEach(s),mr=n(Gs," for an example of how to train a model on your dataset."),Gs.forEach(s),jt=d(a),ca=l(a,"H2",{class:!0});var ln=o(ca);Aa=l(ln,"A",{id:!0,class:!0,href:!0});var Vo=o(Aa);hs=l(Vo,"SPAN",{});var Bo=o(hs);x(Ya.$$.fragment,Bo),Bo.forEach(s),Vo.forEach(s),gr=d(ln),us=l(ln,"SPAN",{});var Yo=o(us);_r=n(Yo,"Audio"),Yo.forEach(s),ln.forEach(s),$t=d(a),X=l(a,"P",{});var Ye=o(X);br=n(Ye,"Audio datasets are loaded just like text datasets. However, an audio dataset is preprocessed a bit differently. Instead of a tokenizer, you\u2019ll need a "),Ja=l(Ye,"A",{href:!0,rel:!0});var Jo=o(Ja);jr=n(Jo,"feature extractor"),Jo.forEach(s),$r=n(Ye,". An audio input may also require resampling its sampling rate to match the sampling rate of the pretrained model you\u2019re using. In this quickstart, you\u2019ll prepare the "),Qa=l(Ye,"A",{href:!0,rel:!0});var Qo=o(Qa);wr=n(Qo,"MInDS-14"),Qo.forEach(s),vr=n(Ye," dataset for a model train on and classify the banking issue a customer is having."),Ye.forEach(s),wt=d(a),ha=l(a,"P",{});var Hs=o(ha);fs=l(Hs,"STRONG",{});var Ko=o(fs);yr=n(Ko,"1"),Ko.forEach(s),kr=n(Hs,". Load the MInDS-14 dataset by providing the "),Ne=l(Hs,"A",{href:!0});var Zo=o(Ne);xr=n(Zo,"load_dataset()"),Zo.forEach(s),Er=n(Hs," function with the dataset name, dataset configuration (not all datasets will have a configuration), and a dataset split:"),Hs.forEach(s),vt=d(a),x(Ka.$$.fragment,a),yt=d(a),B=l(a,"P",{});var ke=o(B);ds=l(ke,"STRONG",{});var Xo=o(ds);qr=n(Xo,"2"),Xo.forEach(s),Ar=n(ke,". Next, load a pretrained "),Za=l(ke,"A",{href:!0,rel:!0});var ap=o(Za);Tr=n(ap,"Wav2Vec2"),ap.forEach(s),Cr=n(ke," model and its corresponding feature extractor from the "),Xa=l(ke,"A",{href:!0,rel:!0});var ep=o(Xa);Pr=n(ep,"\u{1F917} Transformers"),ep.forEach(s),Dr=n(ke," library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),ke.forEach(s),kt=d(a),x(ae.$$.fragment,a),xt=d(a),R=l(a,"P",{});var Y=o(R);ms=l(Y,"STRONG",{});var sp=o(ms);Sr=n(sp,"3"),sp.forEach(s),zr=n(Y,". The "),ee=l(Y,"A",{href:!0,rel:!0});var tp=o(ee);Fr=n(tp,"MInDS-14"),tp.forEach(s),Nr=n(Y," dataset card indicates the sampling rate is 8kHz, but the Wav2Vec2 model was pretrained on a sampling rate of 16kHZ. You\u2019ll need to upsample the "),gs=l(Y,"CODE",{});var np=o(gs);Or=n(np,"audio"),np.forEach(s),Ir=n(Y," column with the "),Oe=l(Y,"A",{href:!0});var rp=o(Oe);Rr=n(rp,"cast_column()"),rp.forEach(s),Lr=n(Y," function and "),Ie=l(Y,"A",{href:!0});var lp=o(Ie);Mr=n(lp,"Audio"),lp.forEach(s),Gr=n(Y," feature to match the model\u2019s sampling rate."),Y.forEach(s),Et=d(a),x(se.$$.fragment,a),qt=d(a),W=l(a,"P",{});var wa=o(W);_s=l(wa,"STRONG",{});var op=o(_s);Hr=n(op,"4"),op.forEach(s),Ur=n(wa,". Create a function that will preprocess the audio "),bs=l(wa,"CODE",{});var pp=o(bs);Wr=n(pp,"array"),pp.forEach(s),Vr=n(wa," with the feature extractor, and truncate and pad the sequences into tidy rectangular tensors. The most important thing to remember here is to call the audio "),js=l(wa,"CODE",{});var ip=o(js);Br=n(ip,"array"),ip.forEach(s),Yr=n(wa," in the feature extractor since the "),$s=l(wa,"CODE",{});var cp=o($s);Jr=n(cp,"array"),cp.forEach(s),Qr=n(wa," - the actual speech signal - is the model input."),wa.forEach(s),At=d(a),aa=l(a,"P",{});var Je=o(aa);Kr=n(Je,"Once you have the "),ws=l(Je,"CODE",{});var hp=o(ws);Zr=n(hp,"preprocess_function"),hp.forEach(s),Xr=n(Je,", use the "),Re=l(Je,"A",{href:!0});var up=o(Re);al=n(up,"map()"),up.forEach(s),el=n(Je," function to speed up processing by applying the function to batches of examples in the dataset."),Je.forEach(s),Tt=d(a),x(te.$$.fragment,a),Ct=d(a),L=l(a,"P",{});var J=o(L);vs=l(J,"STRONG",{});var fp=o(vs);sl=n(fp,"5"),fp.forEach(s),tl=n(J,". Use the "),Le=l(J,"A",{href:!0});var dp=o(Le);nl=n(dp,"rename_column()"),dp.forEach(s),rl=n(J," function to rename the "),ys=l(J,"CODE",{});var mp=o(ys);ll=n(mp,"intent_class"),mp.forEach(s),ol=n(J," column to "),ks=l(J,"CODE",{});var gp=o(ks);pl=n(gp,"labels"),gp.forEach(s),il=n(J,", which is the expected input name in "),ne=l(J,"A",{href:!0,rel:!0});var _p=o(ne);cl=n(_p,"Wav2Vec2ForSequenceClassification"),_p.forEach(s),hl=n(J,":"),J.forEach(s),Pt=d(a),x(re.$$.fragment,a),Dt=d(a),le=l(a,"P",{});var po=o(le);xs=l(po,"STRONG",{});var bp=o(xs);ul=n(bp,"6"),bp.forEach(s),fl=n(po,". Set the dataset format according to the machine learning framework you\u2019re using."),po.forEach(s),St=d(a),x(Ta.$$.fragment,a),zt=d(a),ua=l(a,"P",{});var Us=o(ua);Es=l(Us,"STRONG",{});var jp=o(Es);dl=n(jp,"7"),jp.forEach(s),ml=n(Us,". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),oe=l(Us,"A",{href:!0,rel:!0});var $p=o(oe);gl=n($p,"audio classification guide"),$p.forEach(s),_l=n(Us," for an example of how to train a model on an audio dataset."),Us.forEach(s),Ft=d(a),fa=l(a,"H2",{class:!0});var on=o(fa);Ca=l(on,"A",{id:!0,class:!0,href:!0});var wp=o(Ca);qs=l(wp,"SPAN",{});var vp=o(qs);x(pe.$$.fragment,vp),vp.forEach(s),wp.forEach(s),bl=d(on),As=l(on,"SPAN",{});var yp=o(As);jl=n(yp,"Vision"),yp.forEach(s),on.forEach(s),Nt=d(a),ea=l(a,"P",{});var Qe=o(ea);$l=n(Qe,"Image datasets are loaded just like text datasets. However, instead of a tokenizer, you\u2019ll need a "),ie=l(Qe,"A",{href:!0,rel:!0});var kp=o(ie);wl=n(kp,"feature extractor"),kp.forEach(s),vl=n(Qe," to preprocess the dataset. An image input is a sequence of pixel values that represent an image. Applying some data augmentation to an image in computer vision is common to make the model more robust against overfitting. You\u2019re free to use any data augmentation library you want, and then you can apply the augmentations to your dataset with \u{1F917} Datasets. In this quickstart, you\u2019ll load the "),ce=l(Qe,"A",{href:!0,rel:!0});var xp=o(ce);yl=n(xp,"Beans"),xp.forEach(s),kl=n(Qe," dataset and get it ready for the model to train on and identify disease from the leaf images."),Qe.forEach(s),Ot=d(a),da=l(a,"P",{});var Ws=o(da);Ts=l(Ws,"STRONG",{});var Ep=o(Ts);xl=n(Ep,"1"),Ep.forEach(s),El=n(Ws,". Load the Beans dataset by providing the "),Me=l(Ws,"A",{href:!0});var qp=o(Me);ql=n(qp,"load_dataset()"),qp.forEach(s),Al=n(Ws," function with the dataset name and a dataset split:"),Ws.forEach(s),It=d(a),x(he.$$.fragment,a),Rt=d(a),M=l(a,"P",{});var Q=o(M);Cs=l(Q,"STRONG",{});var Ap=o(Cs);Tl=n(Ap,"2"),Ap.forEach(s),Cl=n(Q,". Now you can add some data augmentations to your dataset with any library ("),ue=l(Q,"A",{href:!0,rel:!0});var Tp=o(ue);Pl=n(Tp,"Albumentations"),Tp.forEach(s),Dl=n(Q,", "),fe=l(Q,"A",{href:!0,rel:!0});var Cp=o(fe);Sl=n(Cp,"imgaug"),Cp.forEach(s),zl=n(Q,", "),de=l(Q,"A",{href:!0,rel:!0});var Pp=o(de);Fl=n(Pp,"Kornia"),Pp.forEach(s),Nl=n(Q,") you like. Here, you\u2019ll use "),me=l(Q,"A",{href:!0,rel:!0});var Dp=o(me);Ol=n(Dp,"torchvision"),Dp.forEach(s),Il=n(Q," to randomly change the color properties of an image:"),Q.forEach(s),Lt=d(a),x(ge.$$.fragment,a),Mt=d(a),ma=l(a,"P",{});var Vs=o(ma);Ps=l(Vs,"STRONG",{});var Sp=o(Ps);Rl=n(Sp,"3"),Sp.forEach(s),Ll=n(Vs,". Create a function to apply your transform to your dataset and generate your model input: "),Ds=l(Vs,"CODE",{});var zp=o(Ds);Ml=n(zp,"pixel_values"),zp.forEach(s),Gl=n(Vs,"."),Vs.forEach(s),Gt=d(a),x(_e.$$.fragment,a),Ht=d(a),ga=l(a,"P",{});var Bs=o(ga);Ss=l(Bs,"STRONG",{});var Fp=o(Ss);Hl=n(Fp,"4"),Fp.forEach(s),Ul=n(Bs,". Use the "),Ge=l(Bs,"A",{href:!0});var Np=o(Ge);Wl=n(Np,"with_transform()"),Np.forEach(s),Vl=n(Bs," function to apply the data augmentations on-the-fly:"),Bs.forEach(s),Ut=d(a),x(be.$$.fragment,a),Wt=d(a),je=l(a,"P",{});var io=o(je);zs=l(io,"STRONG",{});var Op=o(zs);Bl=n(Op,"5"),Op.forEach(s),Yl=n(io,". Set the dataset format according to the machine learning framework you\u2019re using."),io.forEach(s),Vt=d(a),x(Pa.$$.fragment,a),Bt=d(a),_a=l(a,"P",{});var Ys=o(_a);Fs=l(Ys,"STRONG",{});var Ip=o(Fs);Jl=n(Ip,"6"),Ip.forEach(s),Ql=n(Ys,". Start training with your machine learning framework! Check out the \u{1F917} Transformers "),$e=l(Ys,"A",{href:!0,rel:!0});var Rp=o($e);Kl=n(Rp,"image classification guide"),Rp.forEach(s),Zl=n(Ys," for an example of how to train a model on an image dataset."),Ys.forEach(s),Yt=d(a),ba=l(a,"H2",{class:!0});var pn=o(ba);Da=l(pn,"A",{id:!0,class:!0,href:!0});var Lp=o(Da);Ns=l(Lp,"SPAN",{});var Mp=o(Ns);x(we.$$.fragment,Mp),Mp.forEach(s),Lp.forEach(s),Xl=d(pn),Os=l(pn,"SPAN",{});var Gp=o(Os);ao=n(Gp,"What's next?"),Gp.forEach(s),pn.forEach(s),Jt=d(a),He=l(a,"P",{});var Hp=o(He);eo=n(Hp,"This completes the \u{1F917} Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on."),Hp.forEach(s),Qt=d(a),sa=l(a,"P",{});var Ke=o(sa);so=n(Ke,"For your next steps, take a look at our "),Ue=l(Ke,"A",{href:!0});var Up=o(Ue);to=n(Up,"How-to guides"),Up.forEach(s),no=n(Ke," and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If you\u2019re interested in learning more about \u{1F917} Datasets core concepts, grab a cup of coffee and read our "),We=l(Ke,"A",{href:!0});var Wp=o(We);ro=n(Wp,"Conceptual Guides"),Wp.forEach(s),lo=n(Ke,"!"),Ke.forEach(s),this.h()},h(){h(i,"name","hf:doc:metadata"),h(i,"content",JSON.stringify(hi)),h(m,"id","quickstart"),h(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(m,"href","#quickstart"),h(c,"class","relative group"),h(w,"href","./tutorial"),h(xe,"class","w-full text-center bg-gradient-to-r from-violet-300 via-sky-400 to-green-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),h(Ee,"class","text-gray-700"),h(na,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),h(na,"href","/docs/datasets/quickstart#nlp"),h(qe,"class","w-full text-center bg-gradient-to-r from-rose-300 via-purple-400 to-sky-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),h(Ae,"class","text-gray-700"),h(ra,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),h(ra,"href","/docs/datasets/quickstart#audio"),h(Te,"class","w-full text-center bg-gradient-to-r from-orange-300 via-red-400 to-violet-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),h(Ce,"class","text-gray-700"),h(la,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),h(la,"href","/docs/datasets/quickstart#vision"),h(I,"class","w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3 md:gap-y-4 md:gap-x-5"),h(G,"class","mt-4"),h(De,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Audio"),h(Se,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Image"),h(xa,"id","nlp"),h(xa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xa,"href","#nlp"),h(oa,"class","relative group"),h(Oa,"href","https://huggingface.co/datasets"),h(Oa,"rel","nofollow"),h(Ia,"href","https://huggingface.co/datasets/glue/viewer/mrpc"),h(Ia,"rel","nofollow"),h(ze,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),h(La,"href","https://huggingface.co/bert-base-uncased"),h(La,"rel","nofollow"),h(Ma,"href","https://huggingface.co/transformers/"),h(Ma,"rel","nofollow"),h(Fe,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.map"),h(Ua,"href","https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification"),h(Ua,"rel","nofollow"),h(Ba,"href","https://huggingface.co/docs/transformers/training"),h(Ba,"rel","nofollow"),h(Aa,"id","audio"),h(Aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Aa,"href","#audio"),h(ca,"class","relative group"),h(Ja,"href","https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor"),h(Ja,"rel","nofollow"),h(Qa,"href","https://huggingface.co/datasets/PolyAI/minds14"),h(Qa,"rel","nofollow"),h(Ne,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),h(Za,"href","https://huggingface.co/facebook/wav2vec2-base"),h(Za,"rel","nofollow"),h(Xa,"href","https://huggingface.co/transformers/"),h(Xa,"rel","nofollow"),h(ee,"href","https://huggingface.co/datasets/PolyAI/minds14"),h(ee,"rel","nofollow"),h(Oe,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.cast_column"),h(Ie,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Audio"),h(Re,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.map"),h(Le,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.rename_column"),h(ne,"href","https://huggingface.co/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),h(ne,"rel","nofollow"),h(oe,"href","https://huggingface.co/docs/transformers/tasks/audio_classification"),h(oe,"rel","nofollow"),h(Ca,"id","vision"),h(Ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ca,"href","#vision"),h(fa,"class","relative group"),h(ie,"href","https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor"),h(ie,"rel","nofollow"),h(ce,"href","https://huggingface.co/datasets/beans"),h(ce,"rel","nofollow"),h(Me,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),h(ue,"href","https://albumentations.ai/"),h(ue,"rel","nofollow"),h(fe,"href","https://imgaug.readthedocs.io/en/latest/"),h(fe,"rel","nofollow"),h(de,"href","https://kornia.readthedocs.io/en/latest/"),h(de,"rel","nofollow"),h(me,"href","https://pytorch.org/vision/stable/transforms.html"),h(me,"rel","nofollow"),h(Ge,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.with_transform"),h($e,"href","https://huggingface.co/docs/transformers/tasks/image_classification"),h($e,"rel","nofollow"),h(Da,"id","whats-next"),h(Da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Da,"href","#whats-next"),h(ba,"class","relative group"),h(Ue,"href","./how_to"),h(We,"href","./about_arrow")},m(a,p){e(document.head,i),u(a,g,p),u(a,c,p),e(c,m),e(m,$),E(j,$,null),e(c,S),e(c,P),e(P,C),u(a,_,p),u(a,y,p),e(y,b),e(y,w),e(w,z),e(y,O),u(a,v,p),u(a,D,p),e(D,ta),u(a,K,p),u(a,G,p),e(G,I),e(I,na),e(na,xe),e(xe,cn),e(na,hn),e(na,Ee),e(Ee,un),e(I,fn),e(I,ra),e(ra,qe),e(qe,dn),e(ra,mn),e(ra,Ae),e(Ae,gn),e(I,_n),e(I,la),e(la,Te),e(Te,bn),e(la,jn),e(la,Ce),e(Ce,$n),u(a,Zs,p),E(va,a,p),u(a,Xs,p),u(a,Pe,p),e(Pe,wn),u(a,at,p),E(Sa,a,p),u(a,et,p),u(a,ya,p),e(ya,vn),e(ya,De),e(De,yn),e(ya,kn),u(a,st,p),E(za,a,p),u(a,tt,p),u(a,ka,p),e(ka,xn),e(ka,Se),e(Se,En),e(ka,qn),u(a,nt,p),E(Fa,a,p),u(a,rt,p),u(a,oa,p),e(oa,xa),e(xa,Ze),E(Na,Ze,null),e(oa,An),e(oa,Xe),e(Xe,Tn),u(a,lt,p),u(a,Z,p),e(Z,Cn),e(Z,Oa),e(Oa,Pn),e(Z,Dn),e(Z,Ia),e(Ia,Sn),e(Z,zn),u(a,ot,p),u(a,pa,p),e(pa,as),e(as,Fn),e(pa,Nn),e(pa,ze),e(ze,On),e(pa,In),u(a,pt,p),E(Ra,a,p),u(a,it,p),u(a,V,p),e(V,es),e(es,Rn),e(V,Ln),e(V,La),e(La,Mn),e(V,Gn),e(V,Ma),e(Ma,Hn),e(V,Un),u(a,ct,p),E(Ga,a,p),u(a,ht,p),u(a,H,p),e(H,ss),e(ss,Wn),e(H,Vn),e(H,ts),e(ts,Bn),e(H,Yn),e(H,ns),e(ns,Jn),e(H,Qn),e(H,rs),e(rs,Kn),e(H,Zn),u(a,ut,p),u(a,Ea,p),e(Ea,Xn),e(Ea,Fe),e(Fe,ar),e(Ea,er),u(a,ft,p),E(Ha,a,p),u(a,dt,p),u(a,U,p),e(U,ls),e(ls,sr),e(U,tr),e(U,os),e(os,nr),e(U,rr),e(U,ps),e(ps,lr),e(U,or),e(U,Ua),e(Ua,pr),e(U,ir),u(a,mt,p),E(Wa,a,p),u(a,gt,p),u(a,Va,p),e(Va,is),e(is,cr),e(Va,hr),u(a,_t,p),E(qa,a,p),u(a,bt,p),u(a,ia,p),e(ia,cs),e(cs,ur),e(ia,fr),e(ia,Ba),e(Ba,dr),e(ia,mr),u(a,jt,p),u(a,ca,p),e(ca,Aa),e(Aa,hs),E(Ya,hs,null),e(ca,gr),e(ca,us),e(us,_r),u(a,$t,p),u(a,X,p),e(X,br),e(X,Ja),e(Ja,jr),e(X,$r),e(X,Qa),e(Qa,wr),e(X,vr),u(a,wt,p),u(a,ha,p),e(ha,fs),e(fs,yr),e(ha,kr),e(ha,Ne),e(Ne,xr),e(ha,Er),u(a,vt,p),E(Ka,a,p),u(a,yt,p),u(a,B,p),e(B,ds),e(ds,qr),e(B,Ar),e(B,Za),e(Za,Tr),e(B,Cr),e(B,Xa),e(Xa,Pr),e(B,Dr),u(a,kt,p),E(ae,a,p),u(a,xt,p),u(a,R,p),e(R,ms),e(ms,Sr),e(R,zr),e(R,ee),e(ee,Fr),e(R,Nr),e(R,gs),e(gs,Or),e(R,Ir),e(R,Oe),e(Oe,Rr),e(R,Lr),e(R,Ie),e(Ie,Mr),e(R,Gr),u(a,Et,p),E(se,a,p),u(a,qt,p),u(a,W,p),e(W,_s),e(_s,Hr),e(W,Ur),e(W,bs),e(bs,Wr),e(W,Vr),e(W,js),e(js,Br),e(W,Yr),e(W,$s),e($s,Jr),e(W,Qr),u(a,At,p),u(a,aa,p),e(aa,Kr),e(aa,ws),e(ws,Zr),e(aa,Xr),e(aa,Re),e(Re,al),e(aa,el),u(a,Tt,p),E(te,a,p),u(a,Ct,p),u(a,L,p),e(L,vs),e(vs,sl),e(L,tl),e(L,Le),e(Le,nl),e(L,rl),e(L,ys),e(ys,ll),e(L,ol),e(L,ks),e(ks,pl),e(L,il),e(L,ne),e(ne,cl),e(L,hl),u(a,Pt,p),E(re,a,p),u(a,Dt,p),u(a,le,p),e(le,xs),e(xs,ul),e(le,fl),u(a,St,p),E(Ta,a,p),u(a,zt,p),u(a,ua,p),e(ua,Es),e(Es,dl),e(ua,ml),e(ua,oe),e(oe,gl),e(ua,_l),u(a,Ft,p),u(a,fa,p),e(fa,Ca),e(Ca,qs),E(pe,qs,null),e(fa,bl),e(fa,As),e(As,jl),u(a,Nt,p),u(a,ea,p),e(ea,$l),e(ea,ie),e(ie,wl),e(ea,vl),e(ea,ce),e(ce,yl),e(ea,kl),u(a,Ot,p),u(a,da,p),e(da,Ts),e(Ts,xl),e(da,El),e(da,Me),e(Me,ql),e(da,Al),u(a,It,p),E(he,a,p),u(a,Rt,p),u(a,M,p),e(M,Cs),e(Cs,Tl),e(M,Cl),e(M,ue),e(ue,Pl),e(M,Dl),e(M,fe),e(fe,Sl),e(M,zl),e(M,de),e(de,Fl),e(M,Nl),e(M,me),e(me,Ol),e(M,Il),u(a,Lt,p),E(ge,a,p),u(a,Mt,p),u(a,ma,p),e(ma,Ps),e(Ps,Rl),e(ma,Ll),e(ma,Ds),e(Ds,Ml),e(ma,Gl),u(a,Gt,p),E(_e,a,p),u(a,Ht,p),u(a,ga,p),e(ga,Ss),e(Ss,Hl),e(ga,Ul),e(ga,Ge),e(Ge,Wl),e(ga,Vl),u(a,Ut,p),E(be,a,p),u(a,Wt,p),u(a,je,p),e(je,zs),e(zs,Bl),e(je,Yl),u(a,Vt,p),E(Pa,a,p),u(a,Bt,p),u(a,_a,p),e(_a,Fs),e(Fs,Jl),e(_a,Ql),e(_a,$e),e($e,Kl),e(_a,Zl),u(a,Yt,p),u(a,ba,p),e(ba,Da),e(Da,Ns),E(we,Ns,null),e(ba,Xl),e(ba,Os),e(Os,ao),u(a,Jt,p),u(a,He,p),e(He,eo),u(a,Qt,p),u(a,sa,p),e(sa,so),e(sa,Ue),e(Ue,to),e(sa,no),e(sa,We),e(We,ro),e(sa,lo),Kt=!0},p(a,[p]){const ve={};p&2&&(ve.$$scope={dirty:p,ctx:a}),va.$set(ve);const Is={};p&2&&(Is.$$scope={dirty:p,ctx:a}),qa.$set(Is);const Rs={};p&2&&(Rs.$$scope={dirty:p,ctx:a}),Ta.$set(Rs);const Ls={};p&2&&(Ls.$$scope={dirty:p,ctx:a}),Pa.$set(Ls)},i(a){Kt||(q(j.$$.fragment,a),q(va.$$.fragment,a),q(Sa.$$.fragment,a),q(za.$$.fragment,a),q(Fa.$$.fragment,a),q(Na.$$.fragment,a),q(Ra.$$.fragment,a),q(Ga.$$.fragment,a),q(Ha.$$.fragment,a),q(Wa.$$.fragment,a),q(qa.$$.fragment,a),q(Ya.$$.fragment,a),q(Ka.$$.fragment,a),q(ae.$$.fragment,a),q(se.$$.fragment,a),q(te.$$.fragment,a),q(re.$$.fragment,a),q(Ta.$$.fragment,a),q(pe.$$.fragment,a),q(he.$$.fragment,a),q(ge.$$.fragment,a),q(_e.$$.fragment,a),q(be.$$.fragment,a),q(Pa.$$.fragment,a),q(we.$$.fragment,a),Kt=!0)},o(a){A(j.$$.fragment,a),A(va.$$.fragment,a),A(Sa.$$.fragment,a),A(za.$$.fragment,a),A(Fa.$$.fragment,a),A(Na.$$.fragment,a),A(Ra.$$.fragment,a),A(Ga.$$.fragment,a),A(Ha.$$.fragment,a),A(Wa.$$.fragment,a),A(qa.$$.fragment,a),A(Ya.$$.fragment,a),A(Ka.$$.fragment,a),A(ae.$$.fragment,a),A(se.$$.fragment,a),A(te.$$.fragment,a),A(re.$$.fragment,a),A(Ta.$$.fragment,a),A(pe.$$.fragment,a),A(he.$$.fragment,a),A(ge.$$.fragment,a),A(_e.$$.fragment,a),A(be.$$.fragment,a),A(Pa.$$.fragment,a),A(we.$$.fragment,a),Kt=!1},d(a){s(i),a&&s(g),a&&s(c),T(j),a&&s(_),a&&s(y),a&&s(v),a&&s(D),a&&s(K),a&&s(G),a&&s(Zs),T(va,a),a&&s(Xs),a&&s(Pe),a&&s(at),T(Sa,a),a&&s(et),a&&s(ya),a&&s(st),T(za,a),a&&s(tt),a&&s(ka),a&&s(nt),T(Fa,a),a&&s(rt),a&&s(oa),T(Na),a&&s(lt),a&&s(Z),a&&s(ot),a&&s(pa),a&&s(pt),T(Ra,a),a&&s(it),a&&s(V),a&&s(ct),T(Ga,a),a&&s(ht),a&&s(H),a&&s(ut),a&&s(Ea),a&&s(ft),T(Ha,a),a&&s(dt),a&&s(U),a&&s(mt),T(Wa,a),a&&s(gt),a&&s(Va),a&&s(_t),T(qa,a),a&&s(bt),a&&s(ia),a&&s(jt),a&&s(ca),T(Ya),a&&s($t),a&&s(X),a&&s(wt),a&&s(ha),a&&s(vt),T(Ka,a),a&&s(yt),a&&s(B),a&&s(kt),T(ae,a),a&&s(xt),a&&s(R),a&&s(Et),T(se,a),a&&s(qt),a&&s(W),a&&s(At),a&&s(aa),a&&s(Tt),T(te,a),a&&s(Ct),a&&s(L),a&&s(Pt),T(re,a),a&&s(Dt),a&&s(le),a&&s(St),T(Ta,a),a&&s(zt),a&&s(ua),a&&s(Ft),a&&s(fa),T(pe),a&&s(Nt),a&&s(ea),a&&s(Ot),a&&s(da),a&&s(It),T(he,a),a&&s(Rt),a&&s(M),a&&s(Lt),T(ge,a),a&&s(Mt),a&&s(ma),a&&s(Gt),T(_e,a),a&&s(Ht),a&&s(ga),a&&s(Ut),T(be,a),a&&s(Wt),a&&s(je),a&&s(Vt),T(Pa,a),a&&s(Bt),a&&s(_a),a&&s(Yt),a&&s(ba),T(we),a&&s(Jt),a&&s(He),a&&s(Qt),a&&s(sa)}}}const hi={local:"quickstart",sections:[{local:"nlp",title:"NLP"},{local:"audio",title:"Audio"},{local:"vision",title:"Vision"},{local:"whats-next",title:"What's next?"}],title:"Quickstart"};function ui(F){return Qp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $i extends Vp{constructor(i){super();Bp(this,i,ui,ci,Yp,{})}}export{$i as default,hi as metadata};
