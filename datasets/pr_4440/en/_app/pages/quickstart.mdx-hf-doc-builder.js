import{S as Cp,i as Pp,s as Sp,e as r,k as u,w as b,t,M as Dp,c as l,d as e,m as h,a as o,x as j,h as n,b as c,G as a,g as i,y as _,q as y,o as w,B as v,v as zp,L as Ap}from"../chunks/vendor-hf-doc-builder.js";import{T as Ip}from"../chunks/Tip-hf-doc-builder.js";import{I as Ie}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../chunks/CodeBlock-hf-doc-builder.js";import{C as qp}from"../chunks/CodeBlockFw-hf-doc-builder.js";import{F as Np,M as Tp}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function Op(F){let m,$,d,g,E;return{c(){m=r("p"),$=t("For more detailed information about the \u{1F917} Datasets library, check out "),d=r("a"),g=t("Chapter 5"),E=t(" of the Hugging Face course! It covers other important topics such as loading remote or local datasets, tools for cleaning up a dataset and creating your own dataset."),this.h()},l(q){m=l(q,"P",{});var A=o(m);$=n(A,"For more detailed information about the \u{1F917} Datasets library, check out "),d=l(A,"A",{href:!0,rel:!0});var x=o(d);g=n(x,"Chapter 5"),x.forEach(e),E=n(A," of the Hugging Face course! It covers other important topics such as loading remote or local datasets, tools for cleaning up a dataset and creating your own dataset."),A.forEach(e),this.h()},h(){c(d,"href","https://huggingface.co/course/chapter5/1?fw=pt"),c(d,"rel","nofollow")},m(q,A){i(q,m,A),a(m,$),a(m,d),a(d,g),a(m,E)},d(q){q&&e(m)}}}function Fp(F){let m,$,d,g,E,q,A,x,D;return x=new T({props:{code:`import torch

dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "labels"])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)`}}),{c(){m=r("p"),$=t("Wrap the dataset in "),d=r("a"),g=r("code"),E=t("torch.utils.data.DataLoader"),q=t(":"),A=u(),b(x.$$.fragment),this.h()},l(f){m=l(f,"P",{});var k=o(m);$=n(k,"Wrap the dataset in "),d=l(k,"A",{href:!0,rel:!0});var K=o(d);g=l(K,"CODE",{});var O=o(g);E=n(O,"torch.utils.data.DataLoader"),O.forEach(e),K.forEach(e),q=n(k,":"),k.forEach(e),A=h(f),j(x.$$.fragment,f),this.h()},h(){c(d,"href","https://alband.github.io/doc_view/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader"),c(d,"rel","nofollow")},m(f,k){i(f,m,k),a(m,$),a(m,d),a(d,g),a(g,E),a(m,q),i(f,A,k),_(x,f,k),D=!0},p:Ap,i(f){D||(y(x.$$.fragment,f),D=!0)},o(f){w(x.$$.fragment,f),D=!1},d(f){f&&e(m),f&&e(A),v(x,f)}}}function Rp(F){let m,$;return m=new Tp({props:{$$slots:{default:[Fp]},$$scope:{ctx:F}}}),{c(){b(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,g){_(m,d,g),$=!0},p(d,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:d}),m.$set(E)},i(d){$||(y(m.$$.fragment,d),$=!0)},o(d){w(m.$$.fragment,d),$=!1},d(d){v(m,d)}}}function Mp(F){let m,$,d,g,E,q,A,x,D;return x=new T({props:{code:`import tensorflow as tf

dataset.set_format(type="tensorflow", columns=["input_ids", "token_type_ids", "attention_mask", "labels"])
features = {x: dataset[x] for x in ["input_ids", "token_type_ids", "attention_mask"]}
tf_dataset = tf.data.Dataset.from_tensor_slices((features, dataset["labels"])).batch(32)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;tensorflow&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>features = {x: dataset[x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;labels&quot;</span>])).batch(<span class="hljs-number">32</span>)`}}),{c(){m=r("p"),$=t("Wrap the dataset in "),d=r("a"),g=r("code"),E=t("tf.data.Dataset"),q=t(":"),A=u(),b(x.$$.fragment),this.h()},l(f){m=l(f,"P",{});var k=o(m);$=n(k,"Wrap the dataset in "),d=l(k,"A",{href:!0,rel:!0});var K=o(d);g=l(K,"CODE",{});var O=o(g);E=n(O,"tf.data.Dataset"),O.forEach(e),K.forEach(e),q=n(k,":"),k.forEach(e),A=h(f),j(x.$$.fragment,f),this.h()},h(){c(d,"href","https://www.tensorflow.org/api_docs/python/tf/data/Dataset"),c(d,"rel","nofollow")},m(f,k){i(f,m,k),a(m,$),a(m,d),a(d,g),a(g,E),a(m,q),i(f,A,k),_(x,f,k),D=!0},p:Ap,i(f){D||(y(x.$$.fragment,f),D=!0)},o(f){w(x.$$.fragment,f),D=!1},d(f){f&&e(m),f&&e(A),v(x,f)}}}function Hp(F){let m,$;return m=new Tp({props:{$$slots:{default:[Mp]},$$scope:{ctx:F}}}),{c(){b(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,g){_(m,d,g),$=!0},p(d,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:d}),m.$set(E)},i(d){$||(y(m.$$.fragment,d),$=!0)},o(d){w(m.$$.fragment,d),$=!1},d(d){v(m,d)}}}function Gp(F){let m,$,d,g,E,q,A,x,D,f,k,K,O,Ut,Yt,Ne,ma,Jt,Oe,vs,B,Z,fa,Qt,Kt,ga,Zt,Xt,X,ba,sn,an,ja,en,tn,ss,_a,nn,rn,ya,ln,Fe,ds,Re,wa,on,Me,$s,He,ms,pn,va,cn,un,Ge,ks,Le,fs,hn,$a,dn,mn,Ve,xs,Be,as,gs,Ba,Es,fn,Wa,gn,We,W,bn,qs,jn,_n,As,yn,wn,Ue,es,Ua,vn,$n,ka,kn,xn,Ye,Ts,Je,R,Ya,En,qn,Cs,An,Tn,Ps,Cn,Pn,Qe,Ss,Ke,z,Ja,Sn,Dn,Qa,zn,In,Ka,Nn,On,Za,Fn,Rn,Ze,bs,Mn,xa,Hn,Gn,Xe,Ds,st,I,Xa,Ln,Vn,se,Bn,Wn,ae,Un,Yn,zs,Jn,Qn,at,Is,et,M,ee,Kn,Zn,Ea,Xn,sr,qa,ar,er,tt,Aa,tr,nt,js,rt,Ns,te,nr,rr,lt,Os,ot,ts,_s,ne,Fs,lr,re,or,pt,U,pr,Rs,ir,cr,Ms,ur,hr,it,ns,le,dr,mr,Ta,fr,gr,ct,Hs,ut,H,oe,br,jr,Gs,_r,yr,Ls,wr,vr,ht,Vs,dt,C,pe,$r,kr,Bs,xr,Er,ie,qr,Ar,Ca,Tr,Cr,Pa,Pr,Sr,mt,Ws,ft,N,ce,Dr,zr,ue,Ir,Nr,he,Or,Fr,de,Rr,Mr,gt,Y,Hr,me,Gr,Lr,Sa,Vr,Br,bt,Us,jt,P,fe,Wr,Ur,Da,Yr,Jr,ge,Qr,Kr,be,Zr,Xr,Ys,sl,al,_t,Js,yt,Qs,je,el,tl,wt,rs,ys,_e,Ks,nl,ye,rl,vt,J,ll,Zs,ol,pl,Xs,il,cl,$t,ls,we,ul,hl,za,dl,ml,kt,sa,xt,S,ve,fl,gl,aa,bl,jl,ea,_l,yl,ta,wl,vl,na,$l,kl,Et,ra,qt,os,$e,xl,El,ke,ql,Al,At,la,Tt,ps,xe,Tl,Cl,Ia,Pl,Sl,Ct,oa,Pt,pa,Ee,Dl,zl,St,is,ws,qe,ia,Il,Ae,Nl,Dt,Na,Ol,zt,Q,Fl,Oa,Rl,Ml,Fa,Hl,Gl,It;return q=new Ie({}),ds=new Ip({props:{$$slots:{default:[Op]},$$scope:{ctx:F}}}),$s=new T({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),ks=new T({props:{code:"pip install datasets[audio]",highlighted:"pip install datasets[audio]"}}),xs=new T({props:{code:"pip install datasets[vision]",highlighted:"pip install datasets[vision]"}}),Es=new Ie({}),Ts=new T({props:{code:`from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ss=new qp({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}}),Ds=new T({props:{code:`def encode(examples):
    return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length")

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;sentence1&quot;</span>], examples[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),Is=new T({props:{code:'dataset = dataset.map(lambda examples: {"labels": examples["label"]}, batched=True)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&quot;labels&quot;</span>: examples[<span class="hljs-string">&quot;label&quot;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),js=new Np({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Hp],pytorch:[Rp]},$$scope:{ctx:F}}}),Os=new qp({props:{group1:{id:"pt",code:`from tqdm import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    for i, batch in enumerate(tqdm(dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader)):
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:`loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss=loss_fn, metrics=["accuracy"])
model.fit(tfdataset, epochs=3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">3e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss_fn, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tfdataset, epochs=<span class="hljs-number">3</span>)`}}}),Fs=new Ie({}),Hs=new T({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", "en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, <span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Vs=new T({props:{code:`from transformers import AutoModelForAudioClassification, AutoFeatureExtractor

model = AutoModelForAudioClassification.from_pretrained("facebook/wav2vec2-base")
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification, AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),Ws=new T({props:{code:`dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset[0]["audio"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),Us=new T({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs

dataset = dataset.map(preprocess_function, batched=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)`}}),Js=new T({props:{code:'dataset = dataset.rename_column("intent_class", "labels")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)'}}),Ks=new Ie({}),sa=new T({props:{code:`from datasets import load_dataset, Image

dataset = load_dataset("beans", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),ra=new T({props:{code:`from torchvision.transforms import Compose, ColorJitter, ToTensor

jitter = Compose(
    [
         ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.5),
         ToTensor(),
    ]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = Compose(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>         ColorJitter(brightness=<span class="hljs-number">0.25</span>, contrast=<span class="hljs-number">0.25</span>, saturation=<span class="hljs-number">0.25</span>, hue=<span class="hljs-number">0.5</span>),
<span class="hljs-meta">... </span>         ToTensor(),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)`}}),la=new T({props:{code:`def transforms(examples):
    examples["pixel_values"] = [jitter(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [jitter(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),oa=new T({props:{code:"dataset = dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.set_transform(transforms)'}}),ia=new Ie({}),{c(){m=r("meta"),$=u(),d=r("h1"),g=r("a"),E=r("span"),b(q.$$.fragment),A=u(),x=r("span"),D=t("Quickstart"),f=u(),k=r("p"),K=t("This quickstart is intended for developers ready to dive into the code and see an end-to-end example of how to integrate \u{1F917} Datasets into their model training workflow. If you are a beginner or a non-developer, we recommend starting with our "),O=r("a"),Ut=t("tutorials"),Yt=t(", where you will get a more thorough introduction."),Ne=u(),ma=r("p"),Jt=t("Each dataset is unique, and depending on the task, some datasets may require additional steps to prepare the data. But you can always use \u{1F917} Datasets tools to load and process a dataset. Choose the type of dataset you want to work with, and let\u2019s get started!"),Oe=u(),vs=r("div"),B=r("div"),Z=r("a"),fa=r("div"),Qt=t("NLP"),Kt=u(),ga=r("p"),Zt=t("Tokenize a dataset and train a model to determine whether a pair of sentences have the same meaning."),Xt=u(),X=r("a"),ba=r("div"),sn=t("Audio"),an=u(),ja=r("p"),en=t("Resample an audio dataset and get it ready for a model to classify what type of banking issue a speaker is calling about."),tn=u(),ss=r("a"),_a=r("div"),nn=t("Vision"),rn=u(),ya=r("p"),ln=t("Apply a data augmentation to an image dataset and get it ready for a model to diagnose disease in bean plants."),Fe=u(),b(ds.$$.fragment),Re=u(),wa=r("p"),on=t("Start by installing \u{1F917} Datasets:"),Me=u(),b($s.$$.fragment),He=u(),ms=r("p"),pn=t("To work with audio datasets, install the "),va=r("a"),cn=t("Audio"),un=t(" feature:"),Ge=u(),b(ks.$$.fragment),Le=u(),fs=r("p"),hn=t("To work with image datasets, install the "),$a=r("a"),dn=t("Image"),mn=t(" feature:"),Ve=u(),b(xs.$$.fragment),Be=u(),as=r("h2"),gs=r("a"),Ba=r("span"),b(Es.$$.fragment),fn=u(),Wa=r("span"),gn=t("NLP"),We=u(),W=r("p"),bn=t("The fastest and easiest way to get started is by loading an existing dataset from the "),qs=r("a"),jn=t("Hugging Face Hub"),_n=t(". There are thousands of datasets to choose from, spanning many tasks. In this guide, you\u2019ll load the "),As=r("a"),yn=t("Microsoft Research Paraphrase Corpus (MRPC)"),wn=t(" training dataset to train a model to determine whether a pair of sentences mean the same thing. As with all text datasets, you\u2019ll need to tokenize the dataset to prepare it for training."),Ue=u(),es=r("p"),Ua=r("strong"),vn=t("1"),$n=t(". Load the MRPC dataset by providing the "),ka=r("a"),kn=t("load_dataset()"),xn=t(" function with the dataset name, dataset configuration (not all datasets will have a configuration), and dataset split:"),Ye=u(),b(Ts.$$.fragment),Je=u(),R=r("p"),Ya=r("strong"),En=t("2"),qn=t(". Next, load a pretrained "),Cs=r("a"),An=t("BERT"),Tn=t(" model and its corresponding tokenizer from the "),Ps=r("a"),Cn=t("\u{1F917} Transformers"),Pn=t(" library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),Qe=u(),b(Ss.$$.fragment),Ke=u(),z=r("p"),Ja=r("strong"),Sn=t("3"),Dn=t(". Tokenize the dataset with the tokenizer you just loaded. You should also truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: "),Qa=r("code"),zn=t("input_ids"),In=t(", "),Ka=r("code"),Nn=t("token_type_ids"),On=t(", and an "),Za=r("code"),Fn=t("attention_mask"),Rn=t(". These are the inputs to your model."),Ze=u(),bs=r("p"),Mn=t("Use the "),xa=r("a"),Hn=t("map()"),Gn=t(" function to speed up processing by applying your tokenization function to batches of examples in the dataset:"),Xe=u(),b(Ds.$$.fragment),st=u(),I=r("p"),Xa=r("strong"),Ln=t("4"),Vn=t(". Rename the "),se=r("code"),Bn=t("label"),Wn=t(" column to "),ae=r("code"),Un=t("labels"),Yn=t(", which is the expected input name in "),zs=r("a"),Jn=t("BertForSequenceClassification"),Qn=t(":"),at=u(),b(Is.$$.fragment),et=u(),M=r("p"),ee=r("strong"),Kn=t("5"),Zn=t(". Use the "),Ea=r("a"),Xn=t("set_format()"),sr=t(" function to set the dataset format according to the machine learning framework you\u2019re using and specify the columns you want to format. The "),qa=r("a"),ar=t("set_format()"),er=t(" function applies the formatting on-the-fly."),tt=u(),Aa=r("p"),tr=t("After setting the format, create a dataset using your framework\u2019s class or API:"),nt=u(),b(js.$$.fragment),rt=u(),Ns=r("p"),te=r("strong"),nr=t("6"),rr=t(". Start training with the machine learning framework you\u2019re using!"),lt=u(),b(Os.$$.fragment),ot=u(),ts=r("h2"),_s=r("a"),ne=r("span"),b(Fs.$$.fragment),lr=u(),re=r("span"),or=t("Audio"),pt=u(),U=r("p"),pr=t("Audio datasets are loaded just like text datasets. However, an audio dataset is preprocessed a bit differently. Instead of a tokenizer, you\u2019ll need a "),Rs=r("a"),ir=t("feature extractor"),cr=t(". An audio input may also require resampling its sampling rate to match the sampling rate of the pretrained model you\u2019re using. In this guide, you\u2019ll prepare the "),Ms=r("a"),ur=t("MInDS-14"),hr=t(" dataset for a model train on and classify the banking issue a customer is having."),it=u(),ns=r("p"),le=r("strong"),dr=t("1"),mr=t(". Load the MInDS-14 dataset by providing the "),Ta=r("a"),fr=t("load_dataset()"),gr=t(" function with the dataset name, dataset configuration (not all datasets will have a configuration), and a dataset split:"),ct=u(),b(Hs.$$.fragment),ut=u(),H=r("p"),oe=r("strong"),br=t("2"),jr=t(". Next, load a pretrained "),Gs=r("a"),_r=t("Wav2Vec2"),yr=t(" model and its corresponding feature extractor from the "),Ls=r("a"),wr=t("\u{1F917} Transformers"),vr=t(" library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),ht=u(),b(Vs.$$.fragment),dt=u(),C=r("p"),pe=r("strong"),$r=t("3"),kr=t(". The "),Bs=r("a"),xr=t("MInDS-14"),Er=t(" dataset card indicates the sampling rate is 8kHz, but the Wav2Vec2 model was pretrained on a sampling rate of 16kHZ. You\u2019ll need to upsample the "),ie=r("code"),qr=t("audio"),Ar=t(" column with the "),Ca=r("a"),Tr=t("cast_column()"),Cr=t(" function and "),Pa=r("a"),Pr=t("Audio"),Sr=t(" feature to match the model\u2019s sampling rate."),mt=u(),b(Ws.$$.fragment),ft=u(),N=r("p"),ce=r("strong"),Dr=t("4"),zr=t(". Create a function that will preprocess the audio "),ue=r("code"),Ir=t("array"),Nr=t(" with the feature extractor, and truncate and pad the sequences into tidy rectangular tensors. The most important thing to remember here is to call the audio "),he=r("code"),Or=t("array"),Fr=t(" in the feature extractor since the "),de=r("code"),Rr=t("array"),Mr=t(" - the actual speech signal - is the model input."),gt=u(),Y=r("p"),Hr=t("Once you have the "),me=r("code"),Gr=t("preprocess_function"),Lr=t(", use the "),Sa=r("a"),Vr=t("map()"),Br=t(" function to speed up processing by applying the function to batches of examples in the dataset."),bt=u(),b(Us.$$.fragment),jt=u(),P=r("p"),fe=r("strong"),Wr=t("5"),Ur=t(". Use the "),Da=r("a"),Yr=t("rename_column()"),Jr=t(" function to rename the "),ge=r("code"),Qr=t("intent_class"),Kr=t(" column to "),be=r("code"),Zr=t("labels"),Xr=t(", which is the expected input name in "),Ys=r("a"),sl=t("Wav2Vec2ForSequenceClassification"),al=t(":"),_t=u(),b(Js.$$.fragment),yt=u(),Qs=r("p"),je=r("strong"),el=t("6"),tl=t(". Create a dataset using your framework\u2019s class or API, and start training!"),wt=u(),rs=r("h2"),ys=r("a"),_e=r("span"),b(Ks.$$.fragment),nl=u(),ye=r("span"),rl=t("Vision"),vt=u(),J=r("p"),ll=t("Image datasets are loaded just like text datasets. However, instead of a tokenizer, you\u2019ll need a "),Zs=r("a"),ol=t("feature extractor"),pl=t(" to preprocess the dataset. An image input is a sequence of pixel values that represent an image. Applying some data augmentation to an image in computer vision is common to make the model more robust against overfitting. You\u2019re free to use any data augmentation library you want, and then you can apply the augmentations to your dataset with \u{1F917} Datasets. In this guide, you\u2019ll load the "),Xs=r("a"),il=t("Beans"),cl=t(" dataset and get it ready for the model to train on and identify disease from the leaf images."),$t=u(),ls=r("p"),we=r("strong"),ul=t("1"),hl=t(". Load the Beans dataset by providing the "),za=r("a"),dl=t("load_dataset()"),ml=t(" function with the dataset name and a dataset split:"),kt=u(),b(sa.$$.fragment),xt=u(),S=r("p"),ve=r("strong"),fl=t("2"),gl=t(". Now you can add some data augmentations to your dataset with any library ("),aa=r("a"),bl=t("Albumentations"),jl=t(", "),ea=r("a"),_l=t("imgaug"),yl=t(", "),ta=r("a"),wl=t("Kornia"),vl=t(") you like. Here, you\u2019ll use "),na=r("a"),$l=t("torchvision"),kl=t(" to randomly change the color properties of an image:"),Et=u(),b(ra.$$.fragment),qt=u(),os=r("p"),$e=r("strong"),xl=t("3"),El=t(". Create a function to apply your transform to your dataset and generate your model input: "),ke=r("code"),ql=t("pixel_values"),Al=t("."),At=u(),b(la.$$.fragment),Tt=u(),ps=r("p"),xe=r("strong"),Tl=t("4"),Cl=t(". Use the "),Ia=r("a"),Pl=t("set_transform()"),Sl=t(" function to apply the transformation on-the-fly to your dataset:"),Ct=u(),b(oa.$$.fragment),Pt=u(),pa=r("p"),Ee=r("strong"),Dl=t("5"),zl=t(". Create a dataset using your framework\u2019s class or API, and start training!"),St=u(),is=r("h2"),ws=r("a"),qe=r("span"),b(ia.$$.fragment),Il=u(),Ae=r("span"),Nl=t("What's next?"),Dt=u(),Na=r("p"),Ol=t("This completes the \u{1F917} Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on."),zt=u(),Q=r("p"),Fl=t("For your next steps, take a look at our "),Oa=r("a"),Rl=t("How-to guides"),Ml=t(" and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If you\u2019re interested in learning more about \u{1F917} Datasets core concepts, grab a cup of coffee and read our "),Fa=r("a"),Hl=t("Conceptual Guides"),Gl=t("!"),this.h()},l(s){const p=Dp('[data-svelte="svelte-1phssyn"]',document.head);m=l(p,"META",{name:!0,content:!0}),p.forEach(e),$=h(s),d=l(s,"H1",{class:!0});var ca=o(d);g=l(ca,"A",{id:!0,class:!0,href:!0});var Te=o(g);E=l(Te,"SPAN",{});var Wl=o(E);j(q.$$.fragment,Wl),Wl.forEach(e),Te.forEach(e),A=h(ca),x=l(ca,"SPAN",{});var Ul=o(x);D=n(Ul,"Quickstart"),Ul.forEach(e),ca.forEach(e),f=h(s),k=l(s,"P",{});var Nt=o(k);K=n(Nt,"This quickstart is intended for developers ready to dive into the code and see an end-to-end example of how to integrate \u{1F917} Datasets into their model training workflow. If you are a beginner or a non-developer, we recommend starting with our "),O=l(Nt,"A",{href:!0});var Yl=o(O);Ut=n(Yl,"tutorials"),Yl.forEach(e),Yt=n(Nt,", where you will get a more thorough introduction."),Nt.forEach(e),Ne=h(s),ma=l(s,"P",{});var Jl=o(ma);Jt=n(Jl,"Each dataset is unique, and depending on the task, some datasets may require additional steps to prepare the data. But you can always use \u{1F917} Datasets tools to load and process a dataset. Choose the type of dataset you want to work with, and let\u2019s get started!"),Jl.forEach(e),Oe=h(s),vs=l(s,"DIV",{class:!0});var Ql=o(vs);B=l(Ql,"DIV",{class:!0});var Ra=o(B);Z=l(Ra,"A",{class:!0,href:!0});var Ot=o(Z);fa=l(Ot,"DIV",{class:!0});var Kl=o(fa);Qt=n(Kl,"NLP"),Kl.forEach(e),Kt=h(Ot),ga=l(Ot,"P",{class:!0});var Zl=o(ga);Zt=n(Zl,"Tokenize a dataset and train a model to determine whether a pair of sentences have the same meaning."),Zl.forEach(e),Ot.forEach(e),Xt=h(Ra),X=l(Ra,"A",{class:!0,href:!0});var Ft=o(X);ba=l(Ft,"DIV",{class:!0});var Xl=o(ba);sn=n(Xl,"Audio"),Xl.forEach(e),an=h(Ft),ja=l(Ft,"P",{class:!0});var so=o(ja);en=n(so,"Resample an audio dataset and get it ready for a model to classify what type of banking issue a speaker is calling about."),so.forEach(e),Ft.forEach(e),tn=h(Ra),ss=l(Ra,"A",{class:!0,href:!0});var Rt=o(ss);_a=l(Rt,"DIV",{class:!0});var ao=o(_a);nn=n(ao,"Vision"),ao.forEach(e),rn=h(Rt),ya=l(Rt,"P",{class:!0});var eo=o(ya);ln=n(eo,"Apply a data augmentation to an image dataset and get it ready for a model to diagnose disease in bean plants."),eo.forEach(e),Rt.forEach(e),Ra.forEach(e),Ql.forEach(e),Fe=h(s),j(ds.$$.fragment,s),Re=h(s),wa=l(s,"P",{});var to=o(wa);on=n(to,"Start by installing \u{1F917} Datasets:"),to.forEach(e),Me=h(s),j($s.$$.fragment,s),He=h(s),ms=l(s,"P",{});var Mt=o(ms);pn=n(Mt,"To work with audio datasets, install the "),va=l(Mt,"A",{href:!0});var no=o(va);cn=n(no,"Audio"),no.forEach(e),un=n(Mt," feature:"),Mt.forEach(e),Ge=h(s),j(ks.$$.fragment,s),Le=h(s),fs=l(s,"P",{});var Ht=o(fs);hn=n(Ht,"To work with image datasets, install the "),$a=l(Ht,"A",{href:!0});var ro=o($a);dn=n(ro,"Image"),ro.forEach(e),mn=n(Ht," feature:"),Ht.forEach(e),Ve=h(s),j(xs.$$.fragment,s),Be=h(s),as=l(s,"H2",{class:!0});var Gt=o(as);gs=l(Gt,"A",{id:!0,class:!0,href:!0});var lo=o(gs);Ba=l(lo,"SPAN",{});var oo=o(Ba);j(Es.$$.fragment,oo),oo.forEach(e),lo.forEach(e),fn=h(Gt),Wa=l(Gt,"SPAN",{});var po=o(Wa);gn=n(po,"NLP"),po.forEach(e),Gt.forEach(e),We=h(s),W=l(s,"P",{});var Ma=o(W);bn=n(Ma,"The fastest and easiest way to get started is by loading an existing dataset from the "),qs=l(Ma,"A",{href:!0,rel:!0});var io=o(qs);jn=n(io,"Hugging Face Hub"),io.forEach(e),_n=n(Ma,". There are thousands of datasets to choose from, spanning many tasks. In this guide, you\u2019ll load the "),As=l(Ma,"A",{href:!0,rel:!0});var co=o(As);yn=n(co,"Microsoft Research Paraphrase Corpus (MRPC)"),co.forEach(e),wn=n(Ma," training dataset to train a model to determine whether a pair of sentences mean the same thing. As with all text datasets, you\u2019ll need to tokenize the dataset to prepare it for training."),Ma.forEach(e),Ue=h(s),es=l(s,"P",{});var Ce=o(es);Ua=l(Ce,"STRONG",{});var uo=o(Ua);vn=n(uo,"1"),uo.forEach(e),$n=n(Ce,". Load the MRPC dataset by providing the "),ka=l(Ce,"A",{href:!0});var ho=o(ka);kn=n(ho,"load_dataset()"),ho.forEach(e),xn=n(Ce," function with the dataset name, dataset configuration (not all datasets will have a configuration), and dataset split:"),Ce.forEach(e),Ye=h(s),j(Ts.$$.fragment,s),Je=h(s),R=l(s,"P",{});var ua=o(R);Ya=l(ua,"STRONG",{});var mo=o(Ya);En=n(mo,"2"),mo.forEach(e),qn=n(ua,". Next, load a pretrained "),Cs=l(ua,"A",{href:!0,rel:!0});var fo=o(Cs);An=n(fo,"BERT"),fo.forEach(e),Tn=n(ua," model and its corresponding tokenizer from the "),Ps=l(ua,"A",{href:!0,rel:!0});var go=o(Ps);Cn=n(go,"\u{1F917} Transformers"),go.forEach(e),Pn=n(ua," library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),ua.forEach(e),Qe=h(s),j(Ss.$$.fragment,s),Ke=h(s),z=l(s,"P",{});var cs=o(z);Ja=l(cs,"STRONG",{});var bo=o(Ja);Sn=n(bo,"3"),bo.forEach(e),Dn=n(cs,". Tokenize the dataset with the tokenizer you just loaded. You should also truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: "),Qa=l(cs,"CODE",{});var jo=o(Qa);zn=n(jo,"input_ids"),jo.forEach(e),In=n(cs,", "),Ka=l(cs,"CODE",{});var _o=o(Ka);Nn=n(_o,"token_type_ids"),_o.forEach(e),On=n(cs,", and an "),Za=l(cs,"CODE",{});var yo=o(Za);Fn=n(yo,"attention_mask"),yo.forEach(e),Rn=n(cs,". These are the inputs to your model."),cs.forEach(e),Ze=h(s),bs=l(s,"P",{});var Lt=o(bs);Mn=n(Lt,"Use the "),xa=l(Lt,"A",{href:!0});var wo=o(xa);Hn=n(wo,"map()"),wo.forEach(e),Gn=n(Lt," function to speed up processing by applying your tokenization function to batches of examples in the dataset:"),Lt.forEach(e),Xe=h(s),j(Ds.$$.fragment,s),st=h(s),I=l(s,"P",{});var us=o(I);Xa=l(us,"STRONG",{});var vo=o(Xa);Ln=n(vo,"4"),vo.forEach(e),Vn=n(us,". Rename the "),se=l(us,"CODE",{});var $o=o(se);Bn=n($o,"label"),$o.forEach(e),Wn=n(us," column to "),ae=l(us,"CODE",{});var ko=o(ae);Un=n(ko,"labels"),ko.forEach(e),Yn=n(us,", which is the expected input name in "),zs=l(us,"A",{href:!0,rel:!0});var xo=o(zs);Jn=n(xo,"BertForSequenceClassification"),xo.forEach(e),Qn=n(us,":"),us.forEach(e),at=h(s),j(Is.$$.fragment,s),et=h(s),M=l(s,"P",{});var ha=o(M);ee=l(ha,"STRONG",{});var Eo=o(ee);Kn=n(Eo,"5"),Eo.forEach(e),Zn=n(ha,". Use the "),Ea=l(ha,"A",{href:!0});var qo=o(Ea);Xn=n(qo,"set_format()"),qo.forEach(e),sr=n(ha," function to set the dataset format according to the machine learning framework you\u2019re using and specify the columns you want to format. The "),qa=l(ha,"A",{href:!0});var Ao=o(qa);ar=n(Ao,"set_format()"),Ao.forEach(e),er=n(ha," function applies the formatting on-the-fly."),ha.forEach(e),tt=h(s),Aa=l(s,"P",{});var To=o(Aa);tr=n(To,"After setting the format, create a dataset using your framework\u2019s class or API:"),To.forEach(e),nt=h(s),j(js.$$.fragment,s),rt=h(s),Ns=l(s,"P",{});var Ll=o(Ns);te=l(Ll,"STRONG",{});var Co=o(te);nr=n(Co,"6"),Co.forEach(e),rr=n(Ll,". Start training with the machine learning framework you\u2019re using!"),Ll.forEach(e),lt=h(s),j(Os.$$.fragment,s),ot=h(s),ts=l(s,"H2",{class:!0});var Vt=o(ts);_s=l(Vt,"A",{id:!0,class:!0,href:!0});var Po=o(_s);ne=l(Po,"SPAN",{});var So=o(ne);j(Fs.$$.fragment,So),So.forEach(e),Po.forEach(e),lr=h(Vt),re=l(Vt,"SPAN",{});var Do=o(re);or=n(Do,"Audio"),Do.forEach(e),Vt.forEach(e),pt=h(s),U=l(s,"P",{});var Ha=o(U);pr=n(Ha,"Audio datasets are loaded just like text datasets. However, an audio dataset is preprocessed a bit differently. Instead of a tokenizer, you\u2019ll need a "),Rs=l(Ha,"A",{href:!0,rel:!0});var zo=o(Rs);ir=n(zo,"feature extractor"),zo.forEach(e),cr=n(Ha,". An audio input may also require resampling its sampling rate to match the sampling rate of the pretrained model you\u2019re using. In this guide, you\u2019ll prepare the "),Ms=l(Ha,"A",{href:!0,rel:!0});var Io=o(Ms);ur=n(Io,"MInDS-14"),Io.forEach(e),hr=n(Ha," dataset for a model train on and classify the banking issue a customer is having."),Ha.forEach(e),it=h(s),ns=l(s,"P",{});var Pe=o(ns);le=l(Pe,"STRONG",{});var No=o(le);dr=n(No,"1"),No.forEach(e),mr=n(Pe,". Load the MInDS-14 dataset by providing the "),Ta=l(Pe,"A",{href:!0});var Oo=o(Ta);fr=n(Oo,"load_dataset()"),Oo.forEach(e),gr=n(Pe," function with the dataset name, dataset configuration (not all datasets will have a configuration), and a dataset split:"),Pe.forEach(e),ct=h(s),j(Hs.$$.fragment,s),ut=h(s),H=l(s,"P",{});var da=o(H);oe=l(da,"STRONG",{});var Fo=o(oe);br=n(Fo,"2"),Fo.forEach(e),jr=n(da,". Next, load a pretrained "),Gs=l(da,"A",{href:!0,rel:!0});var Ro=o(Gs);_r=n(Ro,"Wav2Vec2"),Ro.forEach(e),yr=n(da," model and its corresponding feature extractor from the "),Ls=l(da,"A",{href:!0,rel:!0});var Mo=o(Ls);wr=n(Mo,"\u{1F917} Transformers"),Mo.forEach(e),vr=n(da," library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task."),da.forEach(e),ht=h(s),j(Vs.$$.fragment,s),dt=h(s),C=l(s,"P",{});var G=o(C);pe=l(G,"STRONG",{});var Ho=o(pe);$r=n(Ho,"3"),Ho.forEach(e),kr=n(G,". The "),Bs=l(G,"A",{href:!0,rel:!0});var Go=o(Bs);xr=n(Go,"MInDS-14"),Go.forEach(e),Er=n(G," dataset card indicates the sampling rate is 8kHz, but the Wav2Vec2 model was pretrained on a sampling rate of 16kHZ. You\u2019ll need to upsample the "),ie=l(G,"CODE",{});var Lo=o(ie);qr=n(Lo,"audio"),Lo.forEach(e),Ar=n(G," column with the "),Ca=l(G,"A",{href:!0});var Vo=o(Ca);Tr=n(Vo,"cast_column()"),Vo.forEach(e),Cr=n(G," function and "),Pa=l(G,"A",{href:!0});var Bo=o(Pa);Pr=n(Bo,"Audio"),Bo.forEach(e),Sr=n(G," feature to match the model\u2019s sampling rate."),G.forEach(e),mt=h(s),j(Ws.$$.fragment,s),ft=h(s),N=l(s,"P",{});var hs=o(N);ce=l(hs,"STRONG",{});var Wo=o(ce);Dr=n(Wo,"4"),Wo.forEach(e),zr=n(hs,". Create a function that will preprocess the audio "),ue=l(hs,"CODE",{});var Uo=o(ue);Ir=n(Uo,"array"),Uo.forEach(e),Nr=n(hs," with the feature extractor, and truncate and pad the sequences into tidy rectangular tensors. The most important thing to remember here is to call the audio "),he=l(hs,"CODE",{});var Yo=o(he);Or=n(Yo,"array"),Yo.forEach(e),Fr=n(hs," in the feature extractor since the "),de=l(hs,"CODE",{});var Jo=o(de);Rr=n(Jo,"array"),Jo.forEach(e),Mr=n(hs," - the actual speech signal - is the model input."),hs.forEach(e),gt=h(s),Y=l(s,"P",{});var Ga=o(Y);Hr=n(Ga,"Once you have the "),me=l(Ga,"CODE",{});var Qo=o(me);Gr=n(Qo,"preprocess_function"),Qo.forEach(e),Lr=n(Ga,", use the "),Sa=l(Ga,"A",{href:!0});var Ko=o(Sa);Vr=n(Ko,"map()"),Ko.forEach(e),Br=n(Ga," function to speed up processing by applying the function to batches of examples in the dataset."),Ga.forEach(e),bt=h(s),j(Us.$$.fragment,s),jt=h(s),P=l(s,"P",{});var L=o(P);fe=l(L,"STRONG",{});var Zo=o(fe);Wr=n(Zo,"5"),Zo.forEach(e),Ur=n(L,". Use the "),Da=l(L,"A",{href:!0});var Xo=o(Da);Yr=n(Xo,"rename_column()"),Xo.forEach(e),Jr=n(L," function to rename the "),ge=l(L,"CODE",{});var sp=o(ge);Qr=n(sp,"intent_class"),sp.forEach(e),Kr=n(L," column to "),be=l(L,"CODE",{});var ap=o(be);Zr=n(ap,"labels"),ap.forEach(e),Xr=n(L,", which is the expected input name in "),Ys=l(L,"A",{href:!0,rel:!0});var ep=o(Ys);sl=n(ep,"Wav2Vec2ForSequenceClassification"),ep.forEach(e),al=n(L,":"),L.forEach(e),_t=h(s),j(Js.$$.fragment,s),yt=h(s),Qs=l(s,"P",{});var Vl=o(Qs);je=l(Vl,"STRONG",{});var tp=o(je);el=n(tp,"6"),tp.forEach(e),tl=n(Vl,". Create a dataset using your framework\u2019s class or API, and start training!"),Vl.forEach(e),wt=h(s),rs=l(s,"H2",{class:!0});var Bt=o(rs);ys=l(Bt,"A",{id:!0,class:!0,href:!0});var np=o(ys);_e=l(np,"SPAN",{});var rp=o(_e);j(Ks.$$.fragment,rp),rp.forEach(e),np.forEach(e),nl=h(Bt),ye=l(Bt,"SPAN",{});var lp=o(ye);rl=n(lp,"Vision"),lp.forEach(e),Bt.forEach(e),vt=h(s),J=l(s,"P",{});var La=o(J);ll=n(La,"Image datasets are loaded just like text datasets. However, instead of a tokenizer, you\u2019ll need a "),Zs=l(La,"A",{href:!0,rel:!0});var op=o(Zs);ol=n(op,"feature extractor"),op.forEach(e),pl=n(La," to preprocess the dataset. An image input is a sequence of pixel values that represent an image. Applying some data augmentation to an image in computer vision is common to make the model more robust against overfitting. You\u2019re free to use any data augmentation library you want, and then you can apply the augmentations to your dataset with \u{1F917} Datasets. In this guide, you\u2019ll load the "),Xs=l(La,"A",{href:!0,rel:!0});var pp=o(Xs);il=n(pp,"Beans"),pp.forEach(e),cl=n(La," dataset and get it ready for the model to train on and identify disease from the leaf images."),La.forEach(e),$t=h(s),ls=l(s,"P",{});var Se=o(ls);we=l(Se,"STRONG",{});var ip=o(we);ul=n(ip,"1"),ip.forEach(e),hl=n(Se,". Load the Beans dataset by providing the "),za=l(Se,"A",{href:!0});var cp=o(za);dl=n(cp,"load_dataset()"),cp.forEach(e),ml=n(Se," function with the dataset name and a dataset split:"),Se.forEach(e),kt=h(s),j(sa.$$.fragment,s),xt=h(s),S=l(s,"P",{});var V=o(S);ve=l(V,"STRONG",{});var up=o(ve);fl=n(up,"2"),up.forEach(e),gl=n(V,". Now you can add some data augmentations to your dataset with any library ("),aa=l(V,"A",{href:!0,rel:!0});var hp=o(aa);bl=n(hp,"Albumentations"),hp.forEach(e),jl=n(V,", "),ea=l(V,"A",{href:!0,rel:!0});var dp=o(ea);_l=n(dp,"imgaug"),dp.forEach(e),yl=n(V,", "),ta=l(V,"A",{href:!0,rel:!0});var mp=o(ta);wl=n(mp,"Kornia"),mp.forEach(e),vl=n(V,") you like. Here, you\u2019ll use "),na=l(V,"A",{href:!0,rel:!0});var fp=o(na);$l=n(fp,"torchvision"),fp.forEach(e),kl=n(V," to randomly change the color properties of an image:"),V.forEach(e),Et=h(s),j(ra.$$.fragment,s),qt=h(s),os=l(s,"P",{});var De=o(os);$e=l(De,"STRONG",{});var gp=o($e);xl=n(gp,"3"),gp.forEach(e),El=n(De,". Create a function to apply your transform to your dataset and generate your model input: "),ke=l(De,"CODE",{});var bp=o(ke);ql=n(bp,"pixel_values"),bp.forEach(e),Al=n(De,"."),De.forEach(e),At=h(s),j(la.$$.fragment,s),Tt=h(s),ps=l(s,"P",{});var ze=o(ps);xe=l(ze,"STRONG",{});var jp=o(xe);Tl=n(jp,"4"),jp.forEach(e),Cl=n(ze,". Use the "),Ia=l(ze,"A",{href:!0});var _p=o(Ia);Pl=n(_p,"set_transform()"),_p.forEach(e),Sl=n(ze," function to apply the transformation on-the-fly to your dataset:"),ze.forEach(e),Ct=h(s),j(oa.$$.fragment,s),Pt=h(s),pa=l(s,"P",{});var Bl=o(pa);Ee=l(Bl,"STRONG",{});var yp=o(Ee);Dl=n(yp,"5"),yp.forEach(e),zl=n(Bl,". Create a dataset using your framework\u2019s class or API, and start training!"),Bl.forEach(e),St=h(s),is=l(s,"H2",{class:!0});var Wt=o(is);ws=l(Wt,"A",{id:!0,class:!0,href:!0});var wp=o(ws);qe=l(wp,"SPAN",{});var vp=o(qe);j(ia.$$.fragment,vp),vp.forEach(e),wp.forEach(e),Il=h(Wt),Ae=l(Wt,"SPAN",{});var $p=o(Ae);Nl=n($p,"What's next?"),$p.forEach(e),Wt.forEach(e),Dt=h(s),Na=l(s,"P",{});var kp=o(Na);Ol=n(kp,"This completes the \u{1F917} Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on."),kp.forEach(e),zt=h(s),Q=l(s,"P",{});var Va=o(Q);Fl=n(Va,"For your next steps, take a look at our "),Oa=l(Va,"A",{href:!0});var xp=o(Oa);Rl=n(xp,"How-to guides"),xp.forEach(e),Ml=n(Va," and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If you\u2019re interested in learning more about \u{1F917} Datasets core concepts, grab a cup of coffee and read our "),Fa=l(Va,"A",{href:!0});var Ep=o(Fa);Hl=n(Ep,"Conceptual Guides"),Ep.forEach(e),Gl=n(Va,"!"),Va.forEach(e),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(Lp)),c(g,"id","quickstart"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#quickstart"),c(d,"class","relative group"),c(O,"href","./tutorial"),c(fa,"class","w-full text-center bg-gradient-to-br from-emerald-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),c(ga,"class","text-gray-700"),c(Z,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),c(Z,"href","/docs/datasets/quickstart#nlp"),c(ba,"class","w-full text-center bg-gradient-to-br from-violet-400 to-orange-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),c(ja,"class","text-gray-700"),c(X,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),c(X,"href","/docs/datasets/quickstart#audio"),c(_a,"class","w-full text-center bg-gradient-to-br from-cyan-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),c(ya,"class","text-gray-700"),c(ss,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),c(ss,"href","/docs/datasets/quickstart#vision"),c(B,"class","w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3 md:gap-y-4 md:gap-x-5"),c(vs,"class","mt-4"),c(va,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Audio"),c($a,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Image"),c(gs,"id","nlp"),c(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gs,"href","#nlp"),c(as,"class","relative group"),c(qs,"href","https://huggingface.co/datasets"),c(qs,"rel","nofollow"),c(As,"href","https://huggingface.co/datasets/glue/viewer/mrpc"),c(As,"rel","nofollow"),c(ka,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),c(Cs,"href","https://huggingface.co/bert-base-uncased"),c(Cs,"rel","nofollow"),c(Ps,"href","https://huggingface.co/transformers/"),c(Ps,"rel","nofollow"),c(xa,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.map"),c(zs,"href","https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification"),c(zs,"rel","nofollow"),c(Ea,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.set_format"),c(qa,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.set_format"),c(_s,"id","audio"),c(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_s,"href","#audio"),c(ts,"class","relative group"),c(Rs,"href","https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor"),c(Rs,"rel","nofollow"),c(Ms,"href","https://huggingface.co/datasets/PolyAI/minds14"),c(Ms,"rel","nofollow"),c(Ta,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),c(Gs,"href","https://huggingface.co/facebook/wav2vec2-base"),c(Gs,"rel","nofollow"),c(Ls,"href","https://huggingface.co/transformers/"),c(Ls,"rel","nofollow"),c(Bs,"href","https://huggingface.co/datasets/PolyAI/minds14"),c(Bs,"rel","nofollow"),c(Ca,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.cast_column"),c(Pa,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Audio"),c(Sa,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.map"),c(Da,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.rename_column"),c(Ys,"href","https://huggingface.co/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(Ys,"rel","nofollow"),c(ys,"id","vision"),c(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ys,"href","#vision"),c(rs,"class","relative group"),c(Zs,"href","https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor"),c(Zs,"rel","nofollow"),c(Xs,"href","https://huggingface.co/datasets/beans"),c(Xs,"rel","nofollow"),c(za,"href","/docs/datasets/pr_4440/en/package_reference/loading_methods#datasets.load_dataset"),c(aa,"href","https://albumentations.ai/"),c(aa,"rel","nofollow"),c(ea,"href","https://imgaug.readthedocs.io/en/latest/"),c(ea,"rel","nofollow"),c(ta,"href","https://kornia.readthedocs.io/en/latest/"),c(ta,"rel","nofollow"),c(na,"href","https://pytorch.org/vision/stable/transforms.html"),c(na,"rel","nofollow"),c(Ia,"href","/docs/datasets/pr_4440/en/package_reference/main_classes#datasets.Dataset.set_transform"),c(ws,"id","whats-next"),c(ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ws,"href","#whats-next"),c(is,"class","relative group"),c(Oa,"href","./how_to"),c(Fa,"href","./about_arrow")},m(s,p){a(document.head,m),i(s,$,p),i(s,d,p),a(d,g),a(g,E),_(q,E,null),a(d,A),a(d,x),a(x,D),i(s,f,p),i(s,k,p),a(k,K),a(k,O),a(O,Ut),a(k,Yt),i(s,Ne,p),i(s,ma,p),a(ma,Jt),i(s,Oe,p),i(s,vs,p),a(vs,B),a(B,Z),a(Z,fa),a(fa,Qt),a(Z,Kt),a(Z,ga),a(ga,Zt),a(B,Xt),a(B,X),a(X,ba),a(ba,sn),a(X,an),a(X,ja),a(ja,en),a(B,tn),a(B,ss),a(ss,_a),a(_a,nn),a(ss,rn),a(ss,ya),a(ya,ln),i(s,Fe,p),_(ds,s,p),i(s,Re,p),i(s,wa,p),a(wa,on),i(s,Me,p),_($s,s,p),i(s,He,p),i(s,ms,p),a(ms,pn),a(ms,va),a(va,cn),a(ms,un),i(s,Ge,p),_(ks,s,p),i(s,Le,p),i(s,fs,p),a(fs,hn),a(fs,$a),a($a,dn),a(fs,mn),i(s,Ve,p),_(xs,s,p),i(s,Be,p),i(s,as,p),a(as,gs),a(gs,Ba),_(Es,Ba,null),a(as,fn),a(as,Wa),a(Wa,gn),i(s,We,p),i(s,W,p),a(W,bn),a(W,qs),a(qs,jn),a(W,_n),a(W,As),a(As,yn),a(W,wn),i(s,Ue,p),i(s,es,p),a(es,Ua),a(Ua,vn),a(es,$n),a(es,ka),a(ka,kn),a(es,xn),i(s,Ye,p),_(Ts,s,p),i(s,Je,p),i(s,R,p),a(R,Ya),a(Ya,En),a(R,qn),a(R,Cs),a(Cs,An),a(R,Tn),a(R,Ps),a(Ps,Cn),a(R,Pn),i(s,Qe,p),_(Ss,s,p),i(s,Ke,p),i(s,z,p),a(z,Ja),a(Ja,Sn),a(z,Dn),a(z,Qa),a(Qa,zn),a(z,In),a(z,Ka),a(Ka,Nn),a(z,On),a(z,Za),a(Za,Fn),a(z,Rn),i(s,Ze,p),i(s,bs,p),a(bs,Mn),a(bs,xa),a(xa,Hn),a(bs,Gn),i(s,Xe,p),_(Ds,s,p),i(s,st,p),i(s,I,p),a(I,Xa),a(Xa,Ln),a(I,Vn),a(I,se),a(se,Bn),a(I,Wn),a(I,ae),a(ae,Un),a(I,Yn),a(I,zs),a(zs,Jn),a(I,Qn),i(s,at,p),_(Is,s,p),i(s,et,p),i(s,M,p),a(M,ee),a(ee,Kn),a(M,Zn),a(M,Ea),a(Ea,Xn),a(M,sr),a(M,qa),a(qa,ar),a(M,er),i(s,tt,p),i(s,Aa,p),a(Aa,tr),i(s,nt,p),_(js,s,p),i(s,rt,p),i(s,Ns,p),a(Ns,te),a(te,nr),a(Ns,rr),i(s,lt,p),_(Os,s,p),i(s,ot,p),i(s,ts,p),a(ts,_s),a(_s,ne),_(Fs,ne,null),a(ts,lr),a(ts,re),a(re,or),i(s,pt,p),i(s,U,p),a(U,pr),a(U,Rs),a(Rs,ir),a(U,cr),a(U,Ms),a(Ms,ur),a(U,hr),i(s,it,p),i(s,ns,p),a(ns,le),a(le,dr),a(ns,mr),a(ns,Ta),a(Ta,fr),a(ns,gr),i(s,ct,p),_(Hs,s,p),i(s,ut,p),i(s,H,p),a(H,oe),a(oe,br),a(H,jr),a(H,Gs),a(Gs,_r),a(H,yr),a(H,Ls),a(Ls,wr),a(H,vr),i(s,ht,p),_(Vs,s,p),i(s,dt,p),i(s,C,p),a(C,pe),a(pe,$r),a(C,kr),a(C,Bs),a(Bs,xr),a(C,Er),a(C,ie),a(ie,qr),a(C,Ar),a(C,Ca),a(Ca,Tr),a(C,Cr),a(C,Pa),a(Pa,Pr),a(C,Sr),i(s,mt,p),_(Ws,s,p),i(s,ft,p),i(s,N,p),a(N,ce),a(ce,Dr),a(N,zr),a(N,ue),a(ue,Ir),a(N,Nr),a(N,he),a(he,Or),a(N,Fr),a(N,de),a(de,Rr),a(N,Mr),i(s,gt,p),i(s,Y,p),a(Y,Hr),a(Y,me),a(me,Gr),a(Y,Lr),a(Y,Sa),a(Sa,Vr),a(Y,Br),i(s,bt,p),_(Us,s,p),i(s,jt,p),i(s,P,p),a(P,fe),a(fe,Wr),a(P,Ur),a(P,Da),a(Da,Yr),a(P,Jr),a(P,ge),a(ge,Qr),a(P,Kr),a(P,be),a(be,Zr),a(P,Xr),a(P,Ys),a(Ys,sl),a(P,al),i(s,_t,p),_(Js,s,p),i(s,yt,p),i(s,Qs,p),a(Qs,je),a(je,el),a(Qs,tl),i(s,wt,p),i(s,rs,p),a(rs,ys),a(ys,_e),_(Ks,_e,null),a(rs,nl),a(rs,ye),a(ye,rl),i(s,vt,p),i(s,J,p),a(J,ll),a(J,Zs),a(Zs,ol),a(J,pl),a(J,Xs),a(Xs,il),a(J,cl),i(s,$t,p),i(s,ls,p),a(ls,we),a(we,ul),a(ls,hl),a(ls,za),a(za,dl),a(ls,ml),i(s,kt,p),_(sa,s,p),i(s,xt,p),i(s,S,p),a(S,ve),a(ve,fl),a(S,gl),a(S,aa),a(aa,bl),a(S,jl),a(S,ea),a(ea,_l),a(S,yl),a(S,ta),a(ta,wl),a(S,vl),a(S,na),a(na,$l),a(S,kl),i(s,Et,p),_(ra,s,p),i(s,qt,p),i(s,os,p),a(os,$e),a($e,xl),a(os,El),a(os,ke),a(ke,ql),a(os,Al),i(s,At,p),_(la,s,p),i(s,Tt,p),i(s,ps,p),a(ps,xe),a(xe,Tl),a(ps,Cl),a(ps,Ia),a(Ia,Pl),a(ps,Sl),i(s,Ct,p),_(oa,s,p),i(s,Pt,p),i(s,pa,p),a(pa,Ee),a(Ee,Dl),a(pa,zl),i(s,St,p),i(s,is,p),a(is,ws),a(ws,qe),_(ia,qe,null),a(is,Il),a(is,Ae),a(Ae,Nl),i(s,Dt,p),i(s,Na,p),a(Na,Ol),i(s,zt,p),i(s,Q,p),a(Q,Fl),a(Q,Oa),a(Oa,Rl),a(Q,Ml),a(Q,Fa),a(Fa,Hl),a(Q,Gl),It=!0},p(s,[p]){const ca={};p&2&&(ca.$$scope={dirty:p,ctx:s}),ds.$set(ca);const Te={};p&2&&(Te.$$scope={dirty:p,ctx:s}),js.$set(Te)},i(s){It||(y(q.$$.fragment,s),y(ds.$$.fragment,s),y($s.$$.fragment,s),y(ks.$$.fragment,s),y(xs.$$.fragment,s),y(Es.$$.fragment,s),y(Ts.$$.fragment,s),y(Ss.$$.fragment,s),y(Ds.$$.fragment,s),y(Is.$$.fragment,s),y(js.$$.fragment,s),y(Os.$$.fragment,s),y(Fs.$$.fragment,s),y(Hs.$$.fragment,s),y(Vs.$$.fragment,s),y(Ws.$$.fragment,s),y(Us.$$.fragment,s),y(Js.$$.fragment,s),y(Ks.$$.fragment,s),y(sa.$$.fragment,s),y(ra.$$.fragment,s),y(la.$$.fragment,s),y(oa.$$.fragment,s),y(ia.$$.fragment,s),It=!0)},o(s){w(q.$$.fragment,s),w(ds.$$.fragment,s),w($s.$$.fragment,s),w(ks.$$.fragment,s),w(xs.$$.fragment,s),w(Es.$$.fragment,s),w(Ts.$$.fragment,s),w(Ss.$$.fragment,s),w(Ds.$$.fragment,s),w(Is.$$.fragment,s),w(js.$$.fragment,s),w(Os.$$.fragment,s),w(Fs.$$.fragment,s),w(Hs.$$.fragment,s),w(Vs.$$.fragment,s),w(Ws.$$.fragment,s),w(Us.$$.fragment,s),w(Js.$$.fragment,s),w(Ks.$$.fragment,s),w(sa.$$.fragment,s),w(ra.$$.fragment,s),w(la.$$.fragment,s),w(oa.$$.fragment,s),w(ia.$$.fragment,s),It=!1},d(s){e(m),s&&e($),s&&e(d),v(q),s&&e(f),s&&e(k),s&&e(Ne),s&&e(ma),s&&e(Oe),s&&e(vs),s&&e(Fe),v(ds,s),s&&e(Re),s&&e(wa),s&&e(Me),v($s,s),s&&e(He),s&&e(ms),s&&e(Ge),v(ks,s),s&&e(Le),s&&e(fs),s&&e(Ve),v(xs,s),s&&e(Be),s&&e(as),v(Es),s&&e(We),s&&e(W),s&&e(Ue),s&&e(es),s&&e(Ye),v(Ts,s),s&&e(Je),s&&e(R),s&&e(Qe),v(Ss,s),s&&e(Ke),s&&e(z),s&&e(Ze),s&&e(bs),s&&e(Xe),v(Ds,s),s&&e(st),s&&e(I),s&&e(at),v(Is,s),s&&e(et),s&&e(M),s&&e(tt),s&&e(Aa),s&&e(nt),v(js,s),s&&e(rt),s&&e(Ns),s&&e(lt),v(Os,s),s&&e(ot),s&&e(ts),v(Fs),s&&e(pt),s&&e(U),s&&e(it),s&&e(ns),s&&e(ct),v(Hs,s),s&&e(ut),s&&e(H),s&&e(ht),v(Vs,s),s&&e(dt),s&&e(C),s&&e(mt),v(Ws,s),s&&e(ft),s&&e(N),s&&e(gt),s&&e(Y),s&&e(bt),v(Us,s),s&&e(jt),s&&e(P),s&&e(_t),v(Js,s),s&&e(yt),s&&e(Qs),s&&e(wt),s&&e(rs),v(Ks),s&&e(vt),s&&e(J),s&&e($t),s&&e(ls),s&&e(kt),v(sa,s),s&&e(xt),s&&e(S),s&&e(Et),v(ra,s),s&&e(qt),s&&e(os),s&&e(At),v(la,s),s&&e(Tt),s&&e(ps),s&&e(Ct),v(oa,s),s&&e(Pt),s&&e(pa),s&&e(St),s&&e(is),v(ia),s&&e(Dt),s&&e(Na),s&&e(zt),s&&e(Q)}}}const Lp={local:"quickstart",sections:[{local:"nlp",title:"NLP"},{local:"audio",title:"Audio"},{local:"vision",title:"Vision"},{local:"whats-next",title:"What's next?"}],title:"Quickstart"};function Vp(F){return zp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zp extends Cp{constructor(m){super();Pp(this,m,Vp,Gp,Sp,{})}}export{Zp as default,Lp as metadata};
