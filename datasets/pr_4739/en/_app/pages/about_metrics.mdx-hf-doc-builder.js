import{S as $t,i as kt,s as Pt,e as s,k as f,w as ve,t as o,M as Tt,c as i,d as a,m as p,a as n,x as _e,h as r,b as c,G as t,g as d,y as we,q as be,o as ye,B as ge,v as xt}from"../chunks/vendor-hf-doc-builder.js";import{T as Mt}from"../chunks/Tip-hf-doc-builder.js";import{I as at}from"../chunks/IconCopyLink-hf-doc-builder.js";function Dt(ee){let h,k,m,_,E;return{c(){h=s("p"),k=o("Metrics will soon be deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at our newest library \u{1F917} "),m=s("a"),_=o("Evaluate"),E=o("! In addition to metrics, we\u2019ve also added more tools for evaluating models and datasets."),this.h()},l(v){h=i(v,"P",{});var w=n(h);k=r(w,"Metrics will soon be deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at our newest library \u{1F917} "),m=i(w,"A",{href:!0,rel:!0});var P=n(m);_=r(P,"Evaluate"),P.forEach(a),E=r(w,"! In addition to metrics, we\u2019ve also added more tools for evaluating models and datasets."),w.forEach(a),this.h()},h(){c(m,"href","https://huggingface.co/docs/evaluate/index"),c(m,"rel","nofollow")},m(v,w){d(v,h,w),t(h,k),t(h,m),t(m,_),t(h,E)},d(v){v&&a(h)}}}function Bt(ee){let h,k,m,_,E,v,w,P,Ee,te,T,ae,x,Ae,C,$e,ke,oe,A,M,Q,S,Pe,N,Te,Y,xe,re,D,Me,I,De,Be,se,b,Se,O,Ue,Le,R,Ge,Ce,ie,$,B,J,U,Ne,K,Ie,ne,u,Oe,V,Re,He,X,je,Fe,L,We,qe,Z,ze,Qe,le,H,Ye,ce,y,Je,j,Ke,Ve,F,Xe,Ze,de,W,et,he;return v=new at({}),T=new Mt({props:{warning:!0,$$slots:{default:[Dt]},$$scope:{ctx:ee}}}),S=new at({}),U=new at({}),{c(){h=s("meta"),k=f(),m=s("h1"),_=s("a"),E=s("span"),ve(v.$$.fragment),w=f(),P=s("span"),Ee=o("All about metrics"),te=f(),ve(T.$$.fragment),ae=f(),x=s("p"),Ae=o("\u{1F917} Datasets provides access to a wide range of NLP metrics. You can load metrics associated with benchmark datasets like GLUE or SQuAD, and complex metrics like BLEURT or BERTScore, with a single command: "),C=s("a"),$e=o("load_metric()"),ke=o(". Once you\u2019ve loaded a metric, easily compute and evaluate a model\u2019s performance."),oe=f(),A=s("h2"),M=s("a"),Q=s("span"),ve(S.$$.fragment),Pe=f(),N=s("span"),Te=o("ELI5: "),Y=s("code"),xe=o("load_metric"),re=f(),D=s("p"),Me=o("Loading a dataset and loading a metric share many similarities. This was an intentional design choice because we wanted to create a simple and unified experience. When you call "),I=s("a"),De=o("load_metric()"),Be=o(", the metric loading script is downloaded and imported from GitHub (if it hasn\u2019t already been downloaded before). It contains information about the metric such as it\u2019s citation, homepage, and description."),se=f(),b=s("p"),Se=o("The metric loading script will instantiate and return a "),O=s("a"),Ue=o("Metric"),Le=o(" object. This stores the predictions and references, which you need to compute the metric values. The "),R=s("a"),Ge=o("Metric"),Ce=o(" object is stored as an Apache Arrow table. As a result, the predictions and references are stored directly on disk with memory-mapping. This enables \u{1F917} Datasets to do a lazy computation of the metric, and makes it easier to gather all the predictions in a distributed setting."),ie=f(),$=s("h2"),B=s("a"),J=s("span"),ve(U.$$.fragment),Ne=f(),K=s("span"),Ie=o("Distributed evaluation"),ne=f(),u=s("p"),Oe=o("Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),V=s("code"),Re=o("f(AuB) = f(A) + f(B)"),He=o("), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),X=s("code"),je=o("f(AuB) \u2260 f(A) + f(B)"),Fe=o("), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),L=s("a"),We=o("F1"),qe=o(" scores of each data subset as your "),Z=s("strong"),ze=o("final metric"),Qe=o("."),le=f(),H=s("p"),Ye=o("A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),ce=f(),y=s("p"),Je=o("\u{1F917} Datasets solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),j=s("a"),Ke=o("Metric.compute()"),Ve=o(" the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),F=s("a"),Xe=o("Metric.compute()"),Ze=o(" will perform the final metric evaluation."),de=f(),W=s("p"),et=o("This solution allows \u{1F917} Datasets to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),this.h()},l(e){const l=Tt('[data-svelte="svelte-1phssyn"]',document.head);h=i(l,"META",{name:!0,content:!0}),l.forEach(a),k=p(e),m=i(e,"H1",{class:!0});var G=n(m);_=i(G,"A",{id:!0,class:!0,href:!0});var ot=n(_);E=i(ot,"SPAN",{});var rt=n(E);_e(v.$$.fragment,rt),rt.forEach(a),ot.forEach(a),w=p(G),P=i(G,"SPAN",{});var st=n(P);Ee=r(st,"All about metrics"),st.forEach(a),G.forEach(a),te=p(e),_e(T.$$.fragment,e),ae=p(e),x=i(e,"P",{});var me=n(x);Ae=r(me,"\u{1F917} Datasets provides access to a wide range of NLP metrics. You can load metrics associated with benchmark datasets like GLUE or SQuAD, and complex metrics like BLEURT or BERTScore, with a single command: "),C=i(me,"A",{href:!0});var it=n(C);$e=r(it,"load_metric()"),it.forEach(a),ke=r(me,". Once you\u2019ve loaded a metric, easily compute and evaluate a model\u2019s performance."),me.forEach(a),oe=p(e),A=i(e,"H2",{class:!0});var ue=n(A);M=i(ue,"A",{id:!0,class:!0,href:!0});var nt=n(M);Q=i(nt,"SPAN",{});var lt=n(Q);_e(S.$$.fragment,lt),lt.forEach(a),nt.forEach(a),Pe=p(ue),N=i(ue,"SPAN",{});var tt=n(N);Te=r(tt,"ELI5: "),Y=i(tt,"CODE",{});var ct=n(Y);xe=r(ct,"load_metric"),ct.forEach(a),tt.forEach(a),ue.forEach(a),re=p(e),D=i(e,"P",{});var fe=n(D);Me=r(fe,"Loading a dataset and loading a metric share many similarities. This was an intentional design choice because we wanted to create a simple and unified experience. When you call "),I=i(fe,"A",{href:!0});var dt=n(I);De=r(dt,"load_metric()"),dt.forEach(a),Be=r(fe,", the metric loading script is downloaded and imported from GitHub (if it hasn\u2019t already been downloaded before). It contains information about the metric such as it\u2019s citation, homepage, and description."),fe.forEach(a),se=p(e),b=i(e,"P",{});var q=n(b);Se=r(q,"The metric loading script will instantiate and return a "),O=i(q,"A",{href:!0});var ht=n(O);Ue=r(ht,"Metric"),ht.forEach(a),Le=r(q," object. This stores the predictions and references, which you need to compute the metric values. The "),R=i(q,"A",{href:!0});var mt=n(R);Ge=r(mt,"Metric"),mt.forEach(a),Ce=r(q," object is stored as an Apache Arrow table. As a result, the predictions and references are stored directly on disk with memory-mapping. This enables \u{1F917} Datasets to do a lazy computation of the metric, and makes it easier to gather all the predictions in a distributed setting."),q.forEach(a),ie=p(e),$=i(e,"H2",{class:!0});var pe=n($);B=i(pe,"A",{id:!0,class:!0,href:!0});var ut=n(B);J=i(ut,"SPAN",{});var ft=n(J);_e(U.$$.fragment,ft),ft.forEach(a),ut.forEach(a),Ne=p(pe),K=i(pe,"SPAN",{});var pt=n(K);Ie=r(pt,"Distributed evaluation"),pt.forEach(a),pe.forEach(a),ne=p(e),u=i(e,"P",{});var g=n(u);Oe=r(g,"Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),V=i(g,"CODE",{});var vt=n(V);Re=r(vt,"f(AuB) = f(A) + f(B)"),vt.forEach(a),He=r(g,"), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),X=i(g,"CODE",{});var _t=n(X);je=r(_t,"f(AuB) \u2260 f(A) + f(B)"),_t.forEach(a),Fe=r(g,"), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),L=i(g,"A",{href:!0,rel:!0});var wt=n(L);We=r(wt,"F1"),wt.forEach(a),qe=r(g," scores of each data subset as your "),Z=i(g,"STRONG",{});var bt=n(Z);ze=r(bt,"final metric"),bt.forEach(a),Qe=r(g,"."),g.forEach(a),le=p(e),H=i(e,"P",{});var yt=n(H);Ye=r(yt,"A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),yt.forEach(a),ce=p(e),y=i(e,"P",{});var z=n(y);Je=r(z,"\u{1F917} Datasets solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),j=i(z,"A",{href:!0});var gt=n(j);Ke=r(gt,"Metric.compute()"),gt.forEach(a),Ve=r(z," the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),F=i(z,"A",{href:!0});var Et=n(F);Xe=r(Et,"Metric.compute()"),Et.forEach(a),Ze=r(z," will perform the final metric evaluation."),z.forEach(a),de=p(e),W=i(e,"P",{});var At=n(W);et=r(At,"This solution allows \u{1F917} Datasets to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),At.forEach(a),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(St)),c(_,"id","all-about-metrics"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#all-about-metrics"),c(m,"class","relative group"),c(C,"href","/docs/datasets/pr_4739/en/package_reference/loading_methods#datasets.load_metric"),c(M,"id","eli5-loadmetric"),c(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M,"href","#eli5-loadmetric"),c(A,"class","relative group"),c(I,"href","/docs/datasets/pr_4739/en/package_reference/loading_methods#datasets.load_metric"),c(O,"href","/docs/datasets/pr_4739/en/package_reference/main_classes#datasets.Metric"),c(R,"href","/docs/datasets/pr_4739/en/package_reference/main_classes#datasets.Metric"),c(B,"id","distributed-evaluation"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#distributed-evaluation"),c($,"class","relative group"),c(L,"href","https://huggingface.co/metrics/f1"),c(L,"rel","nofollow"),c(j,"href","/docs/datasets/pr_4739/en/package_reference/main_classes#datasets.Metric.compute"),c(F,"href","/docs/datasets/pr_4739/en/package_reference/main_classes#datasets.Metric.compute")},m(e,l){t(document.head,h),d(e,k,l),d(e,m,l),t(m,_),t(_,E),we(v,E,null),t(m,w),t(m,P),t(P,Ee),d(e,te,l),we(T,e,l),d(e,ae,l),d(e,x,l),t(x,Ae),t(x,C),t(C,$e),t(x,ke),d(e,oe,l),d(e,A,l),t(A,M),t(M,Q),we(S,Q,null),t(A,Pe),t(A,N),t(N,Te),t(N,Y),t(Y,xe),d(e,re,l),d(e,D,l),t(D,Me),t(D,I),t(I,De),t(D,Be),d(e,se,l),d(e,b,l),t(b,Se),t(b,O),t(O,Ue),t(b,Le),t(b,R),t(R,Ge),t(b,Ce),d(e,ie,l),d(e,$,l),t($,B),t(B,J),we(U,J,null),t($,Ne),t($,K),t(K,Ie),d(e,ne,l),d(e,u,l),t(u,Oe),t(u,V),t(V,Re),t(u,He),t(u,X),t(X,je),t(u,Fe),t(u,L),t(L,We),t(u,qe),t(u,Z),t(Z,ze),t(u,Qe),d(e,le,l),d(e,H,l),t(H,Ye),d(e,ce,l),d(e,y,l),t(y,Je),t(y,j),t(j,Ke),t(y,Ve),t(y,F),t(F,Xe),t(y,Ze),d(e,de,l),d(e,W,l),t(W,et),he=!0},p(e,[l]){const G={};l&2&&(G.$$scope={dirty:l,ctx:e}),T.$set(G)},i(e){he||(be(v.$$.fragment,e),be(T.$$.fragment,e),be(S.$$.fragment,e),be(U.$$.fragment,e),he=!0)},o(e){ye(v.$$.fragment,e),ye(T.$$.fragment,e),ye(S.$$.fragment,e),ye(U.$$.fragment,e),he=!1},d(e){a(h),e&&a(k),e&&a(m),ge(v),e&&a(te),ge(T,e),e&&a(ae),e&&a(x),e&&a(oe),e&&a(A),ge(S),e&&a(re),e&&a(D),e&&a(se),e&&a(b),e&&a(ie),e&&a($),ge(U),e&&a(ne),e&&a(u),e&&a(le),e&&a(H),e&&a(ce),e&&a(y),e&&a(de),e&&a(W)}}}const St={local:"all-about-metrics",sections:[{local:"eli5-loadmetric",title:"ELI5: `load_metric`"},{local:"distributed-evaluation",title:"Distributed evaluation"}],title:"All about metrics"};function Ut(ee){return xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nt extends $t{constructor(h){super();kt(this,h,Ut,Bt,Pt,{})}}export{Nt as default,St as metadata};
